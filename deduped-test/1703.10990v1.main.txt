##### Contents

-    1 Introduction
-    2 5D AGT conjecture
    -    2.1 Review of the simplest 5D AGT correspondence
    -    2.2 Reargument of Ding-Iohara-Miki algebra and AGT
        correspondence
-    3 Kac determinant and singular vecter of the algebra @xmath
    -    3.1 Kac determinant of the algebra @xmath
    -    3.2 Proof of Theorem 3.1
    -    3.3 Singular vectors and generalized Macdonald functions
-    4 Crystalization of 5D AGT conjecture
    -    4.1 Crystallization of the deformed Virasoro algebra and AGT
        correspondence.
    -    4.2 Crystallization of @xmath case of DIM algebra
    -    4.3 Crystallization of @xmath case of DIM algebra
    -    4.4 Other types of limit
-    5 R-Matrix of DIM algebra
    -    5.1 Explicit calculation of R-Matrix
    -    5.2 General formula for R-matrix
-    6 Properties of generalized Macdonald functions
    -    6.1 Partial orderings
    -    6.2 Realization of rank @xmath representation by generalized
        Macdonald function
    -    6.3 Limit to @xmath deformation
-    A Macdonald functions and Hall-Littlewood functions
-    B Definition of DIM algebra and level @xmath representation
-    C Proofs and checks in Section 4
    -    C.1 Other proofs of Lemma 4.22
    -    C.2 Explicit form of @xmath
    -    C.3 Check of ( 4.41 )
    -    C.4 Comparison of formulas ( 4.98 ) and ()
-    D Examples of R-matrix

## 1 Introduction

1.1. The Jack symmetric polynomials [ 1 , 2 ] are a system of orthogonal
polynomials expressing the excited states of an integrable
one-dimensional quantum many-body system with the trigonometric type
potential called the Calogero-Sutherland model [ 3 , 4 ] . ¹ ¹ 1 To be
precise, the Jack polynomials are eigenfunctions of a Hamiltonian @xmath
which is obtained by a certain transformation of the Calogero-Sutherland
Hamiltonian. Here @xmath is a parameter appearing in the
Calogero-Sutherland model. The excited states can be constructed from
the Jack polynomials. These are one-parameter deformations of the Schur
symmetric polynomials. In general, being integrable means that the model
has sufficiently many conserved quantities, and that system can be
analytically solved. Like the Calogero-Sutherland model, many of the
integrable systems are not physical models of particles existing in the
real world. However, the mathematical structure of the integrable
models, e.g., excellent solvability, can be used to advantage in many
fields of mathematics.

Let us consider symmetric functions which are defined as a projective
limit of symmetric polynomials with finite variables [ 5 , Chap. 1] . In
the case of the Jack polynomials, the infinite-variable limit exists and
is called the Jack symmetric functions. The Jack functions are
parametrized by partitions or Young diagrams, and has the complex
parameter @xmath (see also Footnote 1 ). Actually we can consider the
parameter @xmath as an indeterminate, and then the Jack functions are
defined over the field @xmath . The surprising result due to Mimachi and
Yamada is that the Jack functions associated to rectangular Young
diagrams have a one-to-one correspondence with singular vectors of the
Virasoro algebra [ 6 ] . The Virasoro algebra is constructed by the
infinitesimal conformal transformations in two dimensions, and is the
Lie algebra generated by @xmath （ @xmath ） and the central element
@xmath satisfying the relations

  -- -------- -- -------
     @xmath      (1.1)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (1.2)
  -- -------- -- -------

This is an essential algebra to two-dimensional conformal field theories
required for string theory and statistical mechanics. To obtain the
irreducible representations of the Virasoro algebra is important not
only in representation theory but also in the conformal field theories.
The irreducibility of highest weight representations can be determined
by special vectors called singular vectors in the highest weight
representation. Although the singular vectors have an integral
representation, the expression formula of the Jack functions by the
Dunkl operator [ 7 ] is more useful. Further, various properties of Jack
functions are known. Thus, the expression of the singular vectors by the
Jack functions is very convenient and beneficial.

As a @xmath -difference deformation of the Jack polynomials, there is a
system of orthogonal polynomials with rich theory called the Macdonald
polynomials [ 5 ] . For later use let us introduce the notation for
Macdonald symmetric function, which is the infinite-variable version of
the Macdonald polynomial. We denote by @xmath the Macdonald symmetric
function associated to the partition @xmath . Here @xmath and @xmath are
free parameters, and they can be considered as complex numbers or
indeterminates. In this paper, we regard the power sum symmetric
functions @xmath as variables of the Macdonald functions (for more
detail, see Appendix A ). The Macdonald polynomials are also
simultaneous eigen-functions of commuting @xmath -difference operators,
now called Macdonald difference operators. Let us also mention that they
are related to the Ruijsenaars model [ 8 ] which is a relativistic
extension of the Calogero-Sutherland model. The @xmath -deformation like
the Macdonald functions makes theory clearer and often mathematically
easier to handle. For example, the Jack functions can be characterized
as the Hamiltonian @xmath (see Footnote 1 ), but they have ​​degenerate
eigenvalues, and difficulties arise when we prove their orthogonality
and ​​coincidence with the singular vectors. In the theory of the
Macdonald functions, this degeneracy problem can be eliminated and the
discussion is clearer. Also, the Hamiltonian @xmath has an infinite
number of commuting operators. However, it is difficult to write down
these operators explicitly [ 9 ] , and in the Macdonald theory we have
an explicit formula for the commuting family of difference operators
having @xmath as simultaneous eigenfunctions. For the above reason, it
can be said that Macdonald’s theory is more beautiful.

In the @xmath ( @xmath ) limit with @xmath fixed, the Macdonald
functions are reduced to the Jack functions. On the other hand, in
@xmath limit with @xmath fixed, they are reduced to the symmetric
functions called the Hall-Littlewood functions. The Hall-Littlewood
functions have a close connection to the character of the general linear
group over finite fields, and they are also a generalization of the
Schur functions [ 5 , Chap. III] . It is one of the advantages that it
is possible to unify and generalize the two generalizations of the Schur
functions. Some applications in knot invariants [ 10 , 11 , 12 ] and
stochastic processes [ 13 ] are also known. The Macdonald functions are
one of the important symmetric functions for modern mathematics.

Awata, Kubo, Odake and Shiraishi introduced in [ 14 ] a @xmath
-deformation of the Virasoro algebra, which is named the deformed
Virasoro algebra. This deformed algebra is designed so that singular
vectors of Verma modules correspond to Macdonald symmetric functions
@xmath . The deformed Virasoro algebra is an associative algebra defined
over the base field @xmath , where @xmath and @xmath are the same
parameters as in @xmath . The generators are denoted by @xmath ( @xmath
), and the defining relation is

  -- -------- -- -------
     @xmath      (1.3)
  -- -------- -- -------

where @xmath and @xmath are the structure constants defined by

  -- -------- -- -------
     @xmath      (1.4)
  -- -------- -- -------

It is shown that the singular vectors of the deformed Virasoro algebra
coincide with the Macdonald functions associated with rectangular Young
diagrams. It is also possible to obtain the Jack and Macodnald functions
associated with general partitions from the singular vectors of the
@xmath -algebra and the deformed @xmath -algebra (which is the
(deformed) Virasoro algebra when @xmath ) [ 15 , 16 , 17 ] . To be
exact, singular vectors of the (deformed) @xmath -algebra can be
realized by @xmath families of bosons under the free field
representation. By a certain projection to one of these bosons, we can
obtain the Jack (or Macdonald) functions associated with Young diagrams
with @xmath edges (see Figure 1 ).

1.2. The representation theory of the Virasoro algebra plays an
essential role in the two-dimensional conformal field theories. In 2009,
while studying the low energy effective theory of M5-branes, Alday,
Gaiotto and Tachikawa discovered the correspondence between the
correlation functions of two-dimensional conformal field theories and
the partition functions of four-dimensional supersymmetric gauge
theories (AGT conjecture) [ 18 ] . Gauge theory has a long history and
is an attractive theory studied by a lot of mathematicians and
physicists. Although it is difficult to calculate the partition
functions of gauge theories in general, Nekrasov gave an explicit
formula (Nekrasov formula) for the instanton partition function of
four-dimensional @xmath supersymmetric gauge theory in 2002 [ 19 ] . The
Nekrasov formula @xmath is written by the summation of the terms @xmath
parametrized by tuples of Young diagrams:

  -- -------- -- -------
     @xmath      (1.5)
  -- -------- -- -------

These terms are given in a factorized form, and as @xmath increases, the
amount of calculation becomes enormous. However, it can be calculated by
a simple combinatoric method. The discovery of [ 18 ] is the following
relation between two-dimensional and four-dimensional field theories.
The Nekrasov formula for the four-dimensional @xmath gauge theory with
four matters in (anti-)fundamental representation (actually, it is the
Nekrasov formula of the @xmath gauge theory divided by the @xmath factor
@xmath ) coincides with the four-point conformal block of the
two-dimensional conformal field theory.

Basics of the conformal field theories were established by Belavin,
Polyakov and Zamolodchikov (BPZ) in 1984 [ 20 ] . They described the
critical phenomenon of the two-dimensional Ising model which is a model
of the ferromagnet, and so on. The primary fields @xmath are operators
on the representation space of the Virasoro algebra such that

  -- -------- -- -------
     @xmath      (1.6)
  -- -------- -- -------

The primary fields are the main research object in the conformal field
theories. Here @xmath is called the conformal dimension of the primary
field. Furthermore, in the conformal field theories, it is a fundamental
problem to calculate the correlation functions of the primary fields.
Generally, in the quantum field theories, the calculations of
correlation functions are difficult, and usually it is often solved by
approximation. BPZ succeeded in determining the exact forms of
correlation functions in the conformal field theories. In particular,
they derived differential equations with regular singularities for the
correlation functions.

However, the research by BPZ was performed mainly for primary fields
with the special conformal dimension, i.e. the minimal models, and they
did not investigate the correlation functions in general forms. Even if
we derive the differential equations of the correlation functions, it is
difficult to find their solutions. From the standpoint of conformal
field theories, the AGT conjecture that states the agreement between the
Nekrasov formulas and the conformal blocks (originally in the Liouville
theory, that is the theory having the primary field with generic
conformal dimensions ² ² 2 Also the AGT conjecture using the Minimal
models is studied in [ 21 ] . To be exact, contribution of the
Heisenberg algebra is added to the Minimal models. ) is studied under
the expectation that general formulas for the correlation functions can
be obtained.

Various extensions were made immediately after the AGT conjecture was
discovered. First of all, the original AGT conjecture deals with the
four-dimensional gauge theory in the case that the number of
(anti-)fundamental matters @xmath is @xmath . Immediately after this
original conjecture [ 18 ] , the cases with @xmath were studied in [ 22
] . These cases can be obtained from the case of @xmath by applying the
same degenerate limits to the Nekrasov formula and the conformal block.
Especially when @xmath , the conformal block degenerates to the inner
product of the vector @xmath called the Whittaker vector of the Virasoro
algebra. ³ ³ 3 Also in the @xmath case, the degenerate conformal blocks
can be realized by the inner product of certain vectors that are the
general form of the vector @xmath . Moreover, it is also expected that
the four-dimensional gauge theories with the higher gauge group @xmath
correspond with the @xmath -algebra [ 23 ] .

The Jack functions and the Macdonald functions also play an important
role in the AGT conjecture. For example, the expansion coefficients of
the Whittaker vector @xmath by the Jack functions are clarified [ 24 ] .
In addition, it is known that a good basis called AFLT basis [ 25 , 26 ,
27 ] can be regarded as a sort of generalization of the Jack functions.
The AFLT basis is a basis in the representation space of the algebra
@xmath , which is first introduced by Alba, Fateev, Litvinov and
Tarnopolskiy, and the conformal block can be combinatorially expanded by
this basis. The AFLT basis is an orthogonal basis which parametrized by
pairs of Young diagrams @xmath . In the @xmath algebra case, it is
parametrized by @xmath -tuples of Young diagrams and exists in the
representation space of the algebra @xmath . By inserting the identity
@xmath with respect to the AFLT basis @xmath , the calculation of
correlation functions @xmath is attributed to that of the matrix element
@xmath , where @xmath is a sort of the primary field defined by some
relations with generators of the Virasoro algebra and the Heisenberg
algebra. Then the three-point functions @xmath are factorized and
coincide with the significant factors called the Nekrasov factors, which
compose the Nekrasov formula. Namely, if we expand the correlation
functions by using the AFLT basis, then the form of its expansion is
quite the same as that of the Nekrasov formula ( 1.5 ). Further, the
conformal block of the algebra @xmath coincides with the partition
function @xmath of @xmath gauge theory. Actually, such a good basis does
not exist in the representation space of the Virasoro algebra. Since the
@xmath factor contributes and complicates the AGT conjecture, we need
the adjustment by the Heisenberg algebra. Since the AFLT basis
correspond to the torus fixed points in the instanton moduli space, it
is also called the fixed point basis.

In [ 28 ] , the original AGT conjecture is ”proved” with the help of the
AFLT basis (the generalized Jack functions) and the free field
representation. ⁴ ⁴ 4 The AGT conjecture are proved in the case of
@xmath in [ 29 , 30 ] by using Zamolodchikov reccursion relation. Some
proofs from geometric representation theory are also given in [ 31 , 32
, 33 ] . However, this ”proof” is based on another conjecture. To
explain it in more detail, recall that the free field representation of
the conformal blocks can be written by the Dotsenko-Fateev integral
@xmath , where @xmath means some integrals of the integrand @xmath .
Then @xmath can be expanded by a sum of the products of the generalized
Jack functions @xmath and their dual functions @xmath , which are
parametrized by tuples of Young diagrams. This expansion formula is
called the Cauchy formula. At that time, it was conjectured that the
integral value of each term @xmath directly corresponds to @xmath in the
Nekrasov formula. This is the scenario of the ”proof.” Although this
proof is straightforward without using recurrence formulas etc, since
the integral value of the generalized Jack functions is still a
conjecture, it is necessary to prove it in order to complete this proof.
For that, we need to investigate more properties of the generalized Jack
functions.

@xmath -deformed version of the AGT conjecture is also provided. ⁵ ⁵ 5
Elliptic deformations of the AGT conjecture are also proposed in [ 34 ,
35 ] . That is, the deformed Virasoro/ @xmath -algebra is related to
five-dimensional gauge theories (5D AGT conjecture) [ 36 , 37 ] . In the
simplest case, it is shown that the inner product of the Whittaker
vector of the deformed Virasoro algebra coincides with the instanton
partition function (K-theoretical partition function) of the
five-dimensional @xmath pure @xmath gauge theory. Also the same approach
as [ 28 ] is taken in the @xmath -deformed case. In other words, it is
conjectured that the @xmath -deformed Dotsenko-Fateev integral
corresponds to the partition function with @xmath matters, and this
conjecture is checked by using the generalized Macdonald functions [ 38
] . The @xmath -deformed version of the AFLT basis [ 39 ] (that is, the
generalized Macdonald functions) exists in the representation space of
the level @xmath representation of the Ding-Iohara-Miki algebra (DIM
algebra).

The DIM algebra (explained in Appendix B ) has the face of a @xmath
-deformation of the @xmath algebra as introduced by Miki in [ 40 ] , and
the deformed Virasoro/ @xmath -algebra appear in its representation [ 41
] . Since the DIM algebra has a lot of background, there are a lot of
other names such as quantum toroidal @xmath algebra [ 42 , 43 ] ,
quantum @xmath algebra [ 44 ] , elliptic Hall algebra [ 45 ] and so on.
The DIM algebra has a Hopf algebra structure which does not exist in the
deformed Virasoro/ @xmath -algebra, and the DIM algebra is associated
with the Macdonald functions having rich theory. ⁶ ⁶ 6 In the study of
the algebraic structure of the operator @xmath that is free field
representation of the Macdonald’s difference operator, it is discovered
that @xmath form a part of representation of the DIM algebra [ 46 ] .
See also Fact B.2 . Unlike the case of the generalized Jack functions,
the generalized Macdonald functions can be constructed by the coproduct
of the DIM algebra [ 39 ] . It is a surprising phenomenon that the
structure of the coproduct of the DIM algebra has information on the
partition functions of the five-dimensional gauge theories. Furthermore,
in the @xmath -deformed case, Awata-Kanno’s and Iqbal-Kozkaz-Vafa’s
refined topological vertices [ 47 , 48 ] are also reproduced by the
matrix elements of some intertwining operator of the DIM algebra, and
the coincidence between the correlation function of the DIM algebra and
the 5D Nekrasov formula is proved [ 49 ] .

The AGT conjecture with respect to the @xmath -deformed AFLT basis [ 39
] (recalled in Section 2.2 ) is almost parallel to the undeformed case,
and it suffices to consider the algebra @xmath , denoted by @xmath ,
which is generated by certain operators @xmath ( @xmath , @xmath )
obtained by the level @xmath representation of the DIM algebra. The
level @xmath representation is that on a Fock module @xmath with the
highest weight @xmath . The vertex operator @xmath on this Fock module
is defined by the relation (Definition 2.16 )

  -- -------- -- -------
     @xmath      (1.7)
  -- -------- -- -------

where @xmath . @xmath can be regarded as an analog of the Virasoro
primary field. The generalized Macdonald functions are defined to be the
eigenfunctions of the generator @xmath constructed by the copoduct of
the DIM algebra. Then, it is conjectured that the matrix elements of
@xmath with respect to the generalized Macdonald functions reproduce the
five-dimensional Nekrasov factors. Under this conjecture, the four-point
conformal block of @xmath corresponds to the 5D @xmath Nekrasov formula
with @xmath matters.

1.3. The first main theorem in this thesis is the formula for the Kac
determinant of the algebra @xmath (Theorem 3.1 ):

###### Theorem.

  -- -- -------- -- -------
        @xmath      (1.8)
        @xmath
  -- -- -------- -- -------

where @xmath , @xmath , and @xmath denotes the number of the @xmath
-tuples of Young diagrams of the size @xmath . For the definition of
@xmath , see Notations in the latter part of this section.

This determinant can be proved by using the fact that the generators
@xmath can be decomposed into the deformed @xmath -algebra part and the
@xmath part by a linear transformation of the bosons, and using the
screening currents of the deformed @xmath -algebra. By this formula, we
can solve the conjecture [ 39 , Conjecture 3.4] that the following PBW
type vectors of the algebra @xmath (Definition 2.8 ) are a basis:

  -- -------- -- -------
     @xmath      (1.9)
  -- -------- -- -------

We also discover that singular vectors of the algebra @xmath correspond
to some generalized Macdonald functions as the second main theorem. By
this result, we can get singular vectors from generalized Macdonald
functions. ⁷ ⁷ 7 Whether the singular vectors considered in this thesis,
e.g., @xmath can express all singular vectors of the algebra @xmath is
incompletely understood. However, the Kac determinant can be proved by
the only vanishing points given by the singular vectors @xmath (see (
3.25 )) corresponding to the simple roots, because the determinant has
@xmath weyl group invariance. The singular vectors are intrinsically the
same as those of the deformed @xmath -algebra. However, as the
projection of the bosons is necessary for the correspondence with the
ordinary Macdonald functions, the result of this thesis that does not
need projections can be thought to be a generalization of [ 17 ] . As a
corollary of this fact, we can find a new relation of the ordinary
Macdonald functions and the generalized Macdonald functions by the
projection of the bosons. Furthermore, since screening operators are
written by integrals, we can also get an integral representation of
generalized Macdonald functions.

Concretely, the vector @xmath defined to be

  -- -------- -- --------
     @xmath      (1.10)
  -- -------- -- --------

is a singular vector. Here @xmath denotes the screening operator, the
@xmath -tuple of parameters @xmath is a function of @xmath , and for
non-negative integers @xmath , @xmath ,

  -- -------- -- --------
     @xmath      (1.11)
  -- -------- -- --------

(for more details, see Section 3.3 ). The singular vector @xmath
coincides with the generalized Macdonald function with the @xmath -tuple
of Young diagrams in Figure 2 (Theorem 3.4 . (A). (main theorem)).

In fact, Figure 1 means the same Young diagram being on the rightmost
side in Figure 2 . Hence the projection of this generalized Macdonald
function corresponds to the ordinary Macdonald functions associated with
the rightmost Young diagram with @xmath edges in Figure 2 (Corollary 3.5
).

When the condition @xmath for the number of screening currents and
parameter @xmath in @xmath is removed, the above figure is not a Young
diagram. However it turns out that the vector @xmath coincides with the
generalized Macdonald function obtained by cutting off the protruding
part and moving boxes to the Young diagrams on the left side. For
example, if @xmath for all @xmath , the corresponding @xmath -tuple of
Young diagrams of the generalized Macdonald function is Figure 3
(Theorem 3.4 . (B). (main theorem)).

1.4. Furthermore, we investigate behavior in the limit to the
Hall-Littlewood functions, @xmath , of the deformed Virasoro algebra and
the algebra @xmath . Also 5D AGT conjecture is studied in this limit.
The reason of considering such a limit is that the situation becomes
simple and some problems are solved. In particular, the simplest 5D AGT
conjecture can be proved, and PBW type vectors can be expressed in terms
of Hall-Littlewood functions. By virtue of the theory of Hall-Littlewood
functions, we can obtain and prove an explicit formula (Theorem 4.23 )
for the four-point correlation function of a certain operator @xmath ,
which is the limit @xmath of the vertex operator @xmath associated with
@xmath . Here, @xmath is the Fock module with the highest weight @xmath
.

###### Theorem.

  -- -------- -- --------
     @xmath      (1.12)
  -- -------- -- --------

Here for a partition @xmath , @xmath , and @xmath is the same one in (
1.8 ).

The function @xmath can be calculated by the generalized Hall-Littlewood
functions in the same way as [ 39 ] . However, we can obtain this
formula by inserting the identity with respect to the PBW type vectors.

We call this Hall-Littlewood limit @xmath ‘crystallization’ after the
use of the quantum groups [ 50 ] , where the parameter @xmath represents
the temperature in the RSOS model [ 51 ] which has symmetry of the
deformed Virasoro algebra, and the limit @xmath can be considered as the
zero temperature limit. Although our studies are mathematically
different from the notion of the original crystal base of quantum
groups, the physical meaning and the motivation to simplify phenomena
are the same. To investigate their mathematical relationship is an
interesting open problem. On the other hand, little is known about the
physical meaning of the Hall-Littlewood limit in the gauge thoery at
present.

1.5. In this thesis, the R-matrix of the DIM algebra is also
investigated. The result with respect to the R-matrix is based on the
collaborative researches [ 52 , 53 ] , and only works of the author is
described. In general, a R-matrix is defined as a solution of the
Yang-Baxter equation, and is closely related to the solvable lattice
models, the knot invariants and so on. Further, it is well-known that
R-matrices can be constructed by Hopf algebras such as the quantum
groups. In general, a Hopf algebra @xmath with the coproduct @xmath is
called quasi-cocommutative if there exists an invertible element @xmath
in the algebra @xmath such that

  -- -------- -- --------
     @xmath      (1.13)
  -- -------- -- --------

This @xmath is called the universal R-matrix. If @xmath also satisfies
the relations

  -- -------- -- --------
     @xmath      (1.14)
  -- -------- -- --------

(see definition of @xmath in Section 5 ) then @xmath is called
quasi-triangular and @xmath satisfies the Yang-Baxter equation @xmath .
The DIM algebra is known to be quasi-triangular [ 42 ] . In this thesis,
the representation matrix of the universal R-matrix @xmath is explicitly
calculated. In the tensor product of the level 1 representation of the
DIM algebra (we denote it by @xmath ), it is block-diagonalized at each
level of the free boson Fock space. Also, it can be seen that the action
of @xmath on the generalized Macdonald functions corresponds to the
exchange of spectral parameters, partitions, and variables in the
generalized Macdonald functions. Moreover, by using the renormalized
generalized Macdonald functions (the integral form @xmath ), it can be
conjectured that

  -- -------- -- --------
     @xmath      (1.15)
  -- -------- -- --------

where @xmath is the vector obtained by exchanging partitions, variables
and spectral parameters in @xmath (see definitions in Section 5.1 ). As
a consequence, we have conjecture (Conjecture 5.1 ) of the explicit
formula for the representation matrix @xmath of the universal R-matrix
in the basis of @xmath :

###### Conjecture.

  -- -------- -- --------
     @xmath      (1.16)
  -- -------- -- --------

In [ 52 , 53 ] , the RTT relation of the DIM algebra is also studied
using this R-matirx.

1.6. This thesis is organized as follows. In Section 2 , two examples of
the 5D AGT conjecture are reviewed. One is the correspondence between
the Whittaker vector of the deformed Virasoro algebra and the partition
function of the 5D pure gauge theory. The other is the conjecture on the
AFLT basis using the level @xmath representation of the DIM algebra. In
Section 3 , we give a factorized formula for the Kac determinant of the
algebra @xmath . Its proof depends on some results of the deformed
@xmath -algebra. The relationship between the singular vectors and the
generalized Macdonald functions is also revealed. In Section 4 , we
investigated the @xmath limit of the deformed Virasoro algebra, the
algebra @xmath and the 5D AGT conjecture. In particular, the simplest 5D
AGT conjecture is proved in this limit. In Section 5 , the explicit form
of the representation of the universal R-matrix of the DIM algebra is
calculated. Its general form is also conjectured in terms of the
generalized Macdonald functions. In Section 6 , properties of the
generalized Macdonald functions are studied. First, to state the
existence theorem of the generalized Macdonald functions, we need
partial orderings among @xmath -tuples of partitions. In this thesis, by
using the partial orderings @xmath (see Definition 2.10 ) and @xmath
(see Definition 6.1 ), the existence theorem is proved. However, in [ 39
] , another ordering @xmath is used and the proof of existence theorem [
39 , Proposition 3.8] is omitted. We justify the theorem [ 39 ,
Proposition 3.8] by comparing @xmath and @xmath in Subsection 6.1 . In
Subsection 6.2 , we also investigate the action of the generators @xmath
and higher rank Hamiltonians on the generalized Macdonald functions.
Their actions are based on a conversion rule called spectral duality
that exchanges the level @xmath representation and the rank @xmath
representation of the DIM algebra. Furthermore, in Subsection 6.3 , the
@xmath limit is also studied. Since the generalized Jack functions have
degenerate eigenvalues, their Cauchy formula used in the senario of
proof of the AGT conjecture [ 28 ] is non-trivial. By taking the limit
from the Macdonald functions, we can justify the orthogonality of the
generalized Jack functions and show the Cauchy formula. In Appendix A ,
the definition and basic facts of the ordinary Macdonald functions and
the Hall-Littlwood functions are briefly reviewed following [ 5 ] . In
Appendix B , the definition of the DIM algebra and the level @xmath
representation are explained following mainly [ 46 , 54 , 55 ] .
Moreover we also describe the definition of another representation of
the DIM algebra called level @xmath representation or the rank @xmath
representation. In Appendix C , we present some proofs and checks of
conjectures in Section 4 . At last in Appendix D , we give explicit
examples of R-matrix at level 2.

## Notations

-   @xmath denote the set of positive integers, integers, rational
    numbers, real numbers, complex numbers, respectively.

-   @xmath denotes the set of non-negative integers.

-   @xmath denotes the set of integers except @xmath .

-   @xmath denotes the Kronecker delta.

-   @xmath denotes the ring of polynomials in @xmath over a field @xmath
    .

-   @xmath denotes the cardinality of set.

-   Functions @xmath depending on multiple variables @xmath ( @xmath )
    are occasionally written as @xmath or @xmath for abbreviation.

-   For a partition @xmath , @xmath and @xmath denote the power sum
    symmetric function and the monomial symmetric function,
    respectively.

-   For @xmath , @xmath denotes the elementary symmetric function.

Let us explain the notation of partitions and Young diagrams.

A partition @xmath is a non-increasing sequence of integers @xmath . We
write @xmath . The length of @xmath , denoted by @xmath , is the number
of elements @xmath with @xmath . Partitions are identified if all
elements except @xmath are the same. For example, @xmath . @xmath
denotes the number of elements that are equal to @xmath in @xmath , and
we occasionally write partitions as @xmath . For example, @xmath .

The partitions are identified with the Young diagrams, which are the
figures written by putting @xmath boxes on the @xmath -th row and
aligning the left side. For example, if @xmath , its Young diagram is

[]

.

The conjugate of a partition @xmath , denoted by @xmath , is the
partition whose Young diagram is the transpose of the diagram @xmath .
For example, The conjugate of @xmath is @xmath . For a partition @xmath
and a coordinate @xmath , define

  -- -------- -- --------
     @xmath      (1.17)
  -- -------- -- --------

@xmath is called arm length and @xmath is called leg length. In the
diagram, they mean the numbers of boxes in right side from or below the
box being in the @xmath -th row and @xmath -th column. For example, if
@xmath , then @xmath , @xmath .

[]

Note that they can take negative values as @xmath , @xmath . For a
partition @xmath , we define @xmath . This means the sum of the numbers
obtained by attaching a zero to box in the top row of the Young diagram
of @xmath , a @xmath to each box in the second row, and so on.

For @xmath -tuple of partitions @xmath , define @xmath . If @xmath , we
occasionally use the symbol ” @xmath ” as @xmath .

## Acknowledgments

The author would like to express his deepest gratitude to his supervisor
Hidetoshi Awata for a great deal of advice. Without his guidance and
persistent help, this thesis would not have been possible. The author
shows his greatest appreciation to Hiroaki Kanno for his insightful
comments and suggestions, and H. Fujino, T. Matsumoto, A. Mironov, Al.
Morozov, And. Morozov and Y. Zenkevich for the collaborative researches.
Some of the results in this thesis are based on the collaborations with
them. The author also would like to thank M. Hamanaka, K. Iwaki, T.
Shiromizu, S. Yanagida and friends for valuable discussions and
supports. The author is supported in part by Grant-in-Aid for JSPS
Fellow 26-10187.

## 2 5D AGT conjecture

### 2.1 Review of the simplest 5D AGT correspondence

We start with recapitulating the result of the Whittaker vector of the
deformed Virasoro algebra and the simplest five-dimensional AGT
correspondence.

###### Definition 2.1.

Let @xmath and @xmath be independent parameters and @xmath . The
deformed Virasoro algebra is the associative algebra over @xmath
generated by @xmath ( @xmath ) with the commutation relation

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

where the structure constant @xmath is defined by

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

The relation ( 2.1 ) can be written in terms of the generating function
@xmath as

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

where @xmath .

The deformed Virasoro algebra is introduced in [ 14 ] . Let @xmath be
the highest weight vector such that @xmath , @xmath ( @xmath ), and
@xmath be the Verma module generated by @xmath . Similarly, @xmath is
the vector satisfying the condition that @xmath , @xmath ( @xmath ).
@xmath is the dual module generated by @xmath . The PBW type vectors
@xmath for partitions @xmath form a basis over @xmath . Also, @xmath
form a basis over @xmath . Here @xmath is a partition or a Young
diagram. The bilinear form @xmath is uniquely defined by @xmath . This
bilinear form is called the Shapovalov form. The Whittaker vector @xmath
is defined as follows.

###### Definition 2.2 ([36]).

For a generic parameter @xmath , define the Whittaker vector ⁸ ⁸ 8 The
vector @xmath is also called the Gaiotto state or the irregular vector.
@xmath by the relations

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

Similarly, the dual Whittaker vector @xmath is defined by the condition
that

  -- -- -- -------
           (2.5)
  -- -- -- -------

This vector is in the form @xmath and its norm is calculated as @xmath ,
where @xmath denotes the inverse matrix element of the Shapovalov matrix
@xmath .

It is useful to consider the free field representation of the deformed
Virasoro algebra. By the Heisenberg algebra generated by @xmath ( @xmath
) and @xmath with the relations

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

the generating function @xmath can be represented as

  -- -------- -------- -- -------
     @xmath   @xmath      (2.7)
     @xmath   @xmath      (2.8)
  -- -------- -------- -- -------

Here @xmath . Let @xmath be the highest weight vector in the Fock module
of the Heisenberg algebra such that @xmath ( @xmath ), and @xmath . Then
@xmath . Furthermore, @xmath can be regarded as the highest weight
vector @xmath of the deformed Virasoro algebra with highest weight
@xmath . In [ 36 ] , Awata and Yamada conjectured an explicit formula
for @xmath in terms of Macdonald functions under the free field
representation, and Yanagida proved it in [ 56 ] . The simplest
five-dimensional AGT conjecture is that the inner product @xmath
coincides with the five-dimensional (K-theoretic) Nekrasov formula for
pure @xmath gauge theory [ 47 , 57 , 58 ] :

  -- -------- -------- -- --------
     @xmath   @xmath      (2.9)
     @xmath   @xmath      (2.10)
  -- -------- -------- -- --------

where @xmath and @xmath are the arm length and the leg length defined in
Introduction, and @xmath is the conjugate of @xmath .

###### Fact 2.3.

For @xmath ,

  -- -------- -- --------
     @xmath      (2.11)
  -- -------- -- --------

This fact is conjectured in [ 36 ] and proved in [ 59 , 60 ] when the
parameter @xmath is generic.

### 2.2 Reargument of Ding-Iohara-Miki algebra and AGT correspondence

We now turn to the DIM algebra [ 61 , 40 ] . Let us recall the AFLT
basis in the 5D AGT correspondence of the @xmath gauge theory along [ 39
] . In this section, we use @xmath kinds of bosons @xmath ( @xmath ,
@xmath ) and @xmath with the relations

  -- -------- -- --------
     @xmath      (2.12)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (2.13)
  -- -------- -- --------

Here @xmath is the substitution of zero mode @xmath , which is realized
in two different ways in Sections 3 and 4 , respectively. Let us define
the vertex operators @xmath and @xmath .

###### Definition 2.4.

Set

  -- -------- -------- -- --------
     @xmath   @xmath      (2.14)
     @xmath   @xmath      (2.15)
  -- -------- -------- -- --------

###### Definition 2.5.

Define generators @xmath by

  -- -------- -------- -- --------
     @xmath   @xmath      (2.16)
  -- -------- -------- -- --------

where @xmath denotes the usual normal ordered product, and

  -- -------- -- --------
     @xmath      (2.17)
  -- -------- -- --------

The generator @xmath arises from the level @xmath representation of
Ding-Iohara-Miki algebra [ 46 , 41 ] , and is obtained by acting the
coproduct of the DIM algebra to the vertex operator @xmath @xmath times
(see Appendix B ). The other generators @xmath appear in the commutation
relations of generators @xmath ( @xmath ). When we just consider the AGT
conjecture, it suffices to deal with the subalgebra @xmath in some
completion of the endomorphism of the algebra of @xmath -tensored Fock
modules for our Heisenberg algebra.

###### Notation 2.6.

We denote the algebra @xmath by @xmath .

###### Proposition 2.7.

If @xmath , the commutation relations of the generators are

  -- -------- -- --------
     @xmath      (2.18)
     @xmath
     @xmath      (2.19)
     @xmath      (2.20)
  -- -------- -- --------

where @xmath is the multiplicative delta function and the structure
constant @xmath is defined by

  -- -------- -- --------
     @xmath      (2.21)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (2.22)
  -- -------- -- --------

These relations are equivalent to

  -- -------- -- --------
     @xmath      (2.23)
     @xmath      (2.24)
     @xmath      (2.25)
  -- -------- -- --------

The proof is similar to the calculation of the deformed Virasoro algebra
or the deformed @xmath -algebra. In the formula ( 2.18 ), we use

  -- -------- -- --------
     @xmath      (2.26)
  -- -------- -- --------

For an @xmath -tuple of parameters @xmath , define @xmath and @xmath to
be the highest weight vectors such that @xmath ( @xmath , @xmath ),
@xmath and @xmath . @xmath is the highest weight module generated by
@xmath , and @xmath is the dual module generated by @xmath . The
bilinear form (Shapovalov form) @xmath is uniquely determined by the
condition @xmath .

###### Definition 2.8.

For an @xmath -tuple of partitions @xmath , set

  -- -------- -- --------
     @xmath      (2.27)
     @xmath      (2.28)
  -- -------- -- --------

The PBW theorem cannot be used because the algebra @xmath is not a Lie
algebra, but in [ 39 ] it was conjectured that the PBW type vectors
@xmath and @xmath are a basis over @xmath and @xmath , respectively.
This conjecture can be solved by the Kac determinant of the algebra
@xmath , which is proved in Section 3 . In this section, we consider
another type of the PBW basis, since it has good expression in @xmath
limit in terms of the Hall-Littlewood functions (see Section 4.3 ).

###### Definition 2.9.

For @xmath , set

  -- -------- -- --------
     @xmath      (2.29)
     @xmath      (2.30)
  -- -------- -- --------

Let us review the AFLT basis in @xmath , which is also called
generalized Macdonald functions. In order to state its existence
theorem, let us prepare the following ordering.

###### Definition 2.10.

For @xmath -tuple of partitions @xmath and @xmath ,

  -- -------- -------- -- --------
     @xmath   @xmath      (2.31)
              @xmath
  -- -------- -------- -- --------

Here @xmath . Note that the second condition can be replaced with @xmath
.

We can state the existence theorem of generalized Macdonald functions in
the basis of products of Macdonald functions @xmath , where @xmath are
Macdonald symmetric functions defined in Appendix A with substituting
the bosons @xmath for the power sum symmetric functions @xmath .

###### Proposition 2.11.

For each @xmath -tuple of partitions @xmath , there exists a unique
vector @xmath such that

  -- -------- -- --------
     @xmath      (2.32)
     @xmath      (2.33)
  -- -------- -- --------

where @xmath is a constant, @xmath is the eigenvalue of @xmath .
Similarly, there exists a unique vector @xmath such that

  -- -------- -- --------
     @xmath      (2.34)
     @xmath      (2.35)
  -- -------- -- --------

Then the eigenvalues are

  -- -------- -- --------
     @xmath      (2.36)
  -- -------- -- --------

Although the ordering of Definition 2.10 is different from the one in [
39 ] , the eigenfunctions @xmath are quite the same. The proof is
similar to the one in Section 6.1 , which follows from triangulation of
@xmath . By this proposition, it can be seen that @xmath is a basis over
@xmath , and the eigenvalues of @xmath are non-degenerate. In Section
6.1 , a more elaborated ordering is introduced and a relationship
between these orderings is explained. In Section 6.3 , it is shown that
these vectors @xmath correspond to the generalized Jack functions
defined in [ 28 ] in the @xmath limit. To use generalized Macdonald
functions in the AGT correspondence, we need to consider its integral
form. In this paper, we adopt the following renormalization, which is
slightly different from that of [ 39 ] .

###### Definition 2.12.

Define the vectors @xmath and @xmath , called the integral forms, by the
condition that

  -- -------- -- --------
     @xmath      (2.37)
                 (2.38)
  -- -------- -- --------

###### Conjecture 2.13.

The coefficients @xmath and @xmath are polynomials in @xmath , @xmath
and @xmath with integer coefficients.

###### Example 2.14.

If @xmath , the transition matrix @xmath is as follows:

  -- -------- --
     @xmath
  -- -------- --

  -- -------- --
     @xmath
  -- -------- --

  -- -------- --
     @xmath
  -- -------- --

By using these integral forms, the five dimensional AGT conjecture can
be stated in the following form. (c.f. [ 39 , Conjecture 3.11 and
Conjecture 3.13] )

###### Conjecture 2.15.

The norm of @xmath reproduces the Nekrasov factor:

  -- -------- -- --------
     @xmath      (2.39)
  -- -------- -- --------

where @xmath .

###### Definition 2.16.

Call the linear operator @xmath the vertex operator if it satisfies

  -- -------- -- --------
     @xmath      (2.40)
  -- -------- -- --------

and @xmath . Then the relations for the Fourier components are

  -- -------- -- --------
     @xmath      (2.41)
  -- -------- -- --------

for @xmath .

###### Example 2.17.

If @xmath , it is known that @xmath exists and is given by

  -- -------- -- --------
     @xmath      (2.42)
  -- -------- -- --------

where @xmath is the operator from @xmath to @xmath satisfying the
relation @xmath .

###### Conjecture 2.18.

The matrix elements of @xmath with respect to generalized Macdonald
functions are

  -- -------- -------- -- --------
     @xmath   @xmath      (2.43)
              @xmath
  -- -------- -------- -- --------

Under these conjectures, we can obtain a formula for multi-point
correlation functions of @xmath by inserting the identity @xmath . In
particular, the formula for the four-point functions agrees with the 5D
@xmath Nekrsov formula with @xmath matters. An M-theoretic derivation of
this formula is also given by [ 62 ] .

## 3 Kac determinant and singular vecter of the algebra @xmath

### 3.1 Kac determinant of the algebra @xmath

In this section, we give the formula for the Kac determinant of the
algebra @xmath and prove it. Moreover, it is shown that singular vectors
correspond to the generalized Macdonald functions. In order to prove the
Kac determinant, we need screening currents of the algebra @xmath . To
construct them, it is necessary to realize the operator @xmath and the
highest wight vector @xmath in terms of the charge operator @xmath and
@xmath ( @xmath ). Let @xmath be the operator satisfying the relation

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

@xmath be the highest weight vector in the Fock module of the Heisenberg
algebra such that @xmath for @xmath . For an @xmath -tuple of complex
parameters @xmath with @xmath , we realize the highest wight vector
@xmath and @xmath as

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

where @xmath is defined by @xmath . Then they satisfy the required
relation @xmath . Similarly, let @xmath be the dual highest weight
vector, and @xmath . These highest wight vectors are normalized by
@xmath , and satisfy the condition of the Shapovalov form @xmath . ⁹ ⁹ 9
The parameters @xmath and @xmath are assumed to be generic in this
section.

We obtain the formula for the Kac determinant with respect to the PBW
type vectors @xmath . ¹⁰ ¹⁰ 10 The formulas for the Kac determinant of
the deformed Virasoro and the deformed @xmath -algebra are proved in [
63 , 64 ] .

###### Theorem 3.1.

Let @xmath . Then

  -- -------- -------- -- -------
     @xmath   @xmath      (3.3)
              @xmath      (3.4)
  -- -------- -------- -- -------

where @xmath , @xmath , and @xmath is the number of entries in @xmath
equal to @xmath . @xmath denotes the number of @xmath -tuples of Young
diagrams of size @xmath , i.e., @xmath . In particular, if @xmath ,

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

###### Corollary 3.2.

If @xmath and @xmath for any numbers @xmath , @xmath and integers @xmath
, @xmath , then the PBW type vectors @xmath (resp. @xmath ) are a basis
over @xmath (resp. @xmath ).

It can be seen that the representation of the algebra @xmath on the Fock
Module @xmath is irreducible if and only if the parameters @xmath
satisfy the condition that @xmath and @xmath . The proof of Theorem 3.1
is given in the next section.

### 3.2 Proof of Theorem 3.1

It is known that the level @xmath representation of the DIM algebra
introduced in the last section can be regarded as the tensor product of
the deformed @xmath -algebra and the Heisenberg algebra associated with
the @xmath factor [ 41 ] . This fact is obtained by a linear
transformation of bosons. The point of proof of Theorem 3.1 is to
construct singular vectors by using screening currents of the deformed
@xmath -algebra under the decomposition of the generators @xmath into
the deformed @xmath -algebra part and the @xmath part. In general, a
vector @xmath in the Fock module @xmath is called the singular vector of
the algebra @xmath if it satisfies

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

for all @xmath and @xmath . The singular vectors obtained by the
screening currents are intrinsically the same one of the deformed @xmath
-algebra. From this singular vector, we can get the vanishing line of
the Kac determinant in the similar way of the deformed @xmath -algebra.

At first, in the @xmath case, we introduce the following bosons.
U(1) part boson

  -- -------- -- -------
     @xmath      (3.7)
     @xmath      (3.8)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

Orthogonal component of @xmath for @xmath

  -- -------- -- --------
     @xmath      (3.10)
  -- -------- -- --------

Fundamental boson of the deformed @xmath -algebra part

  -- -------- -- --------
     @xmath      (3.11)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (3.12)
  -- -------- -- --------

Then they satisfy the following relations

  -- -------- -- --------
     @xmath      (3.13)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (3.14)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (3.15)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (3.16)
  -- -------- -- --------

where @xmath is @xmath or @xmath if the proposition @xmath is true or
false, respectively. ¹¹ ¹¹ 11 Note that @xmath correspond to the
fundamental bosons @xmath in [ 17 ] Using these bosons, we can decompose
the generator @xmath into the U(1) part and the deformed @xmath -algebra
part. That is to say,

  -- -------- -- --------
     @xmath      (3.17)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (3.18)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (3.19)
  -- -------- -- --------

  -- -- -- --------
           (3.20)
  -- -- -- --------

@xmath is the generator of the deformed @xmath -algebra. Let us
introduce the new parameters @xmath and @xmath defined by

  -- -------- -- --------
     @xmath      (3.21)
  -- -------- -- --------

Then the inner product of PBW type vectors can be written as

  -- -------- -- --------
     @xmath      (3.22)
  -- -------- -- --------

Hence, its determinant is also in the form

  -- -------- -- --------
     @xmath      (3.23)
  -- -------- -- --------

where @xmath is a polynomial in @xmath ( @xmath ) which is independent
of @xmath . Now in [ 17 ] , the screening currents of the deformed
@xmath -algebra are introduced:

  -- -------- -- --------
     @xmath      (3.24)
  -- -------- -- --------

where @xmath is the root boson defined by @xmath and @xmath . The bosons
@xmath and @xmath commute with @xmath , and it is known that the
screening charge @xmath commutes with the generators @xmath . Therefore,
@xmath commutes with any generator @xmath , and it can be considered as
the screening charges of the algebra @xmath . Define parameters @xmath ,
@xmath by @xmath and @xmath , and set @xmath . Then @xmath . For any
number @xmath , the vector arising from the screening current @xmath ,

  -- -------- -- --------
     @xmath      (3.25)
  -- -------- -- --------

is a singular vector. @xmath is in the Fock module @xmath with the
parameter @xmath satisfying @xmath for @xmath and @xmath , @xmath . The
@xmath vectors obtained by this singular vector

  -- -------- -- --------
     @xmath      (3.26)
  -- -------- -- --------

contribute the vanishing point @xmath in the polynomial @xmath .
Similarly to the case of the deformed @xmath -algebra (see [ 64 ] ), by
the @xmath Weyl group invariance of the eigenvalues of @xmath , the
polynomial @xmath has the factor @xmath ( @xmath ). Considering the
degree of polynomials @xmath , we can see that when @xmath , the Kac
determinant is

  -- -------- -------- -- --------
     @xmath   @xmath
              @xmath      (3.27)
  -- -------- -------- -- --------

where @xmath is a rational function in parameters @xmath and @xmath and
independent of the parameters @xmath . If @xmath , @xmath is clearly in
the form

  -- -------- -- --------
     @xmath      (3.28)
  -- -------- -- --------

Next, the prefactor @xmath can be computed in general @xmath case by
introducing another boson

  -- -------- -- --------
     @xmath      (3.29)
  -- -------- -- --------

The commutation relation of the boson @xmath is

  -- -------- -- --------
     @xmath      (3.30)
  -- -------- -- --------

Define the determinants @xmath and @xmath with @xmath given by the
expansions

  -- -------- -- --------
     @xmath      (3.31)
  -- -------- -- --------

where @xmath and @xmath are

  -- -------- -- --------
     @xmath      (3.32)
     @xmath      (3.33)
  -- -------- -- --------

By using these determinants, the Kac determinant can be written as

  -- -------- -- --------
     @xmath      (3.34)
  -- -------- -- --------

Here @xmath is the determinant of the diagonal matrix @xmath . This
factor is independent of the parameters @xmath , and we have @xmath . In
( 3.27 ), the factor depending on @xmath in @xmath was already
clarified. Hence, we can determine the prefactor @xmath by computing the
leading term in @xmath . That is, the prefactor @xmath can be written as

  -- -------- -- --------
     @xmath      (3.35)
  -- -------- -- --------

where we introduce the function @xmath which gives the leading term of
@xmath as the polynomial in @xmath , and @xmath . To calculate this
leading term, define the operators @xmath , @xmath by

  -- -------- -- --------
     @xmath      (3.36)
  -- -------- -- --------

@xmath is arising from only the operator @xmath in @xmath . Then

  -- -------- -- --------
     @xmath      (3.37)
  -- -------- -- --------

Let the matrices @xmath and @xmath be given by

  -- -- -- --------
           (3.38)
  -- -- -- --------

where @xmath and @xmath are defined in the usual way:

  -- -------- -- --------
     @xmath      (3.39)
     @xmath      (3.40)
  -- -------- -- --------

Then @xmath is expressed as

  -- -------- -- --------
     @xmath      (3.41)
  -- -------- -- --------

Since the matrix @xmath is lower triangular with respect to the partial
ordering @xmath ¹² ¹² 12 Here the partial orderings @xmath and @xmath
are defined as follows:

@xmath @xmath (3.42) @xmath (3.43)

@xmath @xmath (3.44) @xmath (3.45)

Then we have @xmath unless @xmath . and its diagonal elements are

  -- -------- -- --------
     @xmath      (3.46)
  -- -------- -- --------

we have

  -- -------- -------- -- --------
     @xmath   @xmath      (3.47)
              @xmath      (3.48)
              @xmath      (3.49)
  -- -------- -------- -- --------

The transition matrix @xmath is upper triangular with respect to the
partial ordering @xmath , and all diagonal elements are @xmath . Thus
@xmath . Similarly by considering the base transformation to @xmath , it
can be seen that

  -- -------- -- --------
     @xmath      (3.50)
  -- -------- -- --------

Therefore the prefactor @xmath is

  -- -------- -- --------
     @xmath      (3.51)
  -- -------- -- --------

Hence Theorem 3.1 is proved.

### 3.3 Singular vectors and generalized Macdonald functions

In this subsection, the singular vectors of the algebra @xmath are
discussed. Trivially, when @xmath , the Kac determinant ( 3.3 )
degenerates, and it can be easily seen that the vectors @xmath are
singular vectors. Since the screening operator @xmath is the same one of
the deformed @xmath -algebra, the situation of the singular vectors of
@xmath except contribution arising when @xmath is the same as the
deformed @xmath -algebra.

We discover that singular vectors obtained by the screening currents
@xmath correspond to generalized Macdonald functions. ¹³ ¹³ 13 The
relation between singular vectors of the @xmath algebra and the AFLT
basis is investigated in [ 65 ] . First, we have the following simple
theorem.

###### Theorem 3.3.

For a number @xmath , if @xmath and the other @xmath are generic, there
exists a unique singular vector @xmath in @xmath , and it corresponds to
the generalized Macdonald function @xmath with

  -- -------- -- --------
     @xmath      (3.52)
  -- -------- -- --------

That is,

  -- -------- -- --------
     @xmath      (3.53)
  -- -------- -- --------

###### Proof.

Existence and uniqueness are understood by the formula for the Kac
determinant ( 3.3 ) in the usual way. Actually, the unique singular
vector @xmath is the one of ( 3.25 ). Since the screening charges
commute with @xmath , the singular vector is an eigenfunction of @xmath
of the eigenvalue @xmath . Using the relations @xmath for @xmath and
@xmath , @xmath , we have

  -- -------- -- --------
     @xmath      (3.54)
  -- -------- -- --------

where @xmath is the eigenvalue of the generalized Macdonald functions
introduced in ( 2.36 ). Thus, the singular vector @xmath and the
generalized Macdonald function @xmath are in the same eigenspace of
@xmath . Moreover, by comparing the eigenvalues @xmath , it can be shown
that the dimension of the eigenspace of the eigenvalue @xmath is @xmath
even when @xmath . Therefore, this theorem follows. ∎

Let us consider more complicated cases. For variables @xmath ( @xmath ),
define the function @xmath by

  -- -------- -- --------
     @xmath      (3.55)
  -- -------- -- --------

Then it satisfies @xmath . We focus on the following singular vectors

  -- -------- -- --------
     @xmath      (3.56)
  -- -------- -- --------

where the parameter @xmath is @xmath , @xmath , and for non-negative
integers @xmath and @xmath ( @xmath ),

  -- -------- -- --------
     @xmath      (3.57)
  -- -------- -- --------

Then the singular vector @xmath is in the Fock module @xmath of the
highest weight @xmath defined by @xmath , @xmath , @xmath and

  -- -------- -- --------
     @xmath      (3.58)
  -- -------- -- --------

Now we obtain the following main theorem with respect to the generalized
Macdonald functions and the singular vectors of the DIM algebra. This
theorem can be regarded as a generalization of the result in [ 17 ] .

###### Theorem 3.4.

Let parameters @xmath satisfy @xmath for all @xmath .

(A). If @xmath for all @xmath , then the singular vector @xmath
coincides with the generalized Macdonald function @xmath with @xmath :

  -- -------- -- --------
     @xmath      (3.59)
  -- -------- -- --------

See Figure 2 in Introduction.

(B). If @xmath for all @xmath , the singular vector @xmath coincides
with the generalized Macdonald function associated with the tuple of
Young diagrams @xmath :

  -- -------- -- --------
     @xmath      (3.60)
  -- -------- -- --------

See Figure 3 in Introduction.

###### Proof.

The proof is quite similar to that of Theorem 3.3 . The eigenvalue of
this singular vector is

  -- -------- -- --------
     @xmath      (3.61)
  -- -------- -- --------

On the other hand, the eigenvalue of the generalized Macdonald function
in the case (A) is calcurated as follows. Firstly,

  -- -------- -- --------
     @xmath      (3.62)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (3.63)
  -- -------- -- --------

Hence, by using the equation @xmath , we can see that

  -- -------- -------- -- --------
     @xmath   @xmath      (3.64)
              @xmath      (3.65)
  -- -------- -------- -- --------

This is equal to the eigenvalue of the singular vector. Also, it can be
seen that the dimension of the eigenspace of the eigenvalue @xmath is
@xmath even when @xmath .

If the condition @xmath does not hold, Figure 2 is not a Young diagram.
In this case, the singular vector @xmath corresponds to the generalized
Macdonald function with the @xmath -tuple of Young diagram obtained by
cutting off the protruding parts and moving the boxes to the Young
diagram in the left side. That is, the case (B) . The proof in the case
(B) is exactly the same as the case (A) , so it is omitted. ∎

It is known that projections of the singular vectors @xmath in the case
(A) onto the diagonal components of the boson @xmath correspond to
ordinary Macdonald functions [ 17 , (35)] . Hence, ordinary Macdonald
functions are obtained by the projection of generalized Macdonald
functions.

###### Corollary 3.5.

When @xmath for all @xmath ,

  -- -------- -- --------
     @xmath      (3.66)
  -- -------- -- --------

Here, @xmath denotes the ordinary power sum symmetric functions.

## 4 Crystalization of 5D AGT conjecture

### 4.1 Crystallization of the deformed Virasoro algebra and AGT
correspondence.

Next, we consider a crystallization of the results of Subsection 2.1 ,
namely the behavior in the @xmath limit of the deformed Virasoro algebra
and the simplest 5D AGT correspondence. ¹⁴ ¹⁴ 14 The results of this
section are based on the sub-thesis [ 66 ] . In this limit, the scaled
generators

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

satisfy the commutation relation

  -- -------- -------- -- -------
     @xmath   @xmath      (4.2)
     @xmath   @xmath      (4.3)
     @xmath   @xmath      (4.4)
     @xmath   @xmath
              @xmath      (4.5)
  -- -------- -------- -- -------

In [ 67 ] , the above algebra is introduced and its free field
representation is given. Let the bosons @xmath ( @xmath ) satisfy the
relations @xmath , @xmath . These bosons can be regarded as the @xmath
limit of the bosons @xmath and @xmath in ( 2.6 ), i.e., @xmath , @xmath
. Then @xmath is represented as

  -- -------- -- -------
     @xmath      (4.6)
  -- -------- -- -------

where

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

and @xmath is 1 or 0 if the proposition @xmath is true or false,
respectively. By this free field representation, we can write the PBW
type vectors in terms of Hall-Littlewood functions @xmath defined in
Appendix A :

  -- -------- -------- -- -------
     @xmath   @xmath      (4.8)
     @xmath   @xmath      (4.9)
  -- -------- -------- -- -------

Here @xmath is an abbreviation for @xmath , and @xmath and @xmath are
the same highest weight vectors in Section 2.1 such that @xmath and
@xmath .

These expressions are the consequences of Jing’s operators (Fact A.1 ).
Because of ( A.7 ), they are diagonalized as

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

where @xmath is defined in Appendix A . Since @xmath is non-degenerate,
there is no singular vector in the limit @xmath . The disappearance of
singular vectors can be understood by the fact that the highest weight
which has singular vectors diverges at @xmath . The Whittaker vector of
this algebra is similarly defined.

###### Definition 4.1.

Define the Whittaker vector @xmath by the relation

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

Similarly, the dual Whittaker vector @xmath is defined by

  -- -- -- --------
           (4.12)
  -- -- -- --------

Then the crystallized Whittaker vector is in the simple form

  -- -------- -- --------
     @xmath      (4.13)
  -- -------- -- --------

and its inner product is

  -- -------- -- --------
     @xmath      (4.14)
  -- -------- -- --------

On the other hand, recalling the Nekrasov formula @xmath given in ( 2.9
) of Subsection 2.1 , we can take the crystal limit with the following
trick.

###### Proposition 4.2.

The renormalization @xmath controls divergence in the @xmath limit (
@xmath , @xmath : fixed):

  -- -------- -- --------
     @xmath      (4.15)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (4.16)
  -- -------- -- --------

###### Proof.

Removing parts which have singularity in the Nekrasov factor, we have

  -- -------- -------- -- --------
     @xmath   @xmath      (4.17)
     @xmath   @xmath      (4.18)
  -- -------- -------- -- --------

Hence,

  -- -------- -- --------
     @xmath      (4.19)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (4.20)
  -- -------- -- --------

If @xmath or @xmath for any integer @xmath , @xmath , then @xmath at
@xmath . Therefore, the sum with respect to partitions @xmath , @xmath
can be rewritten as the sum with respect to integers @xmath , @xmath ,
i.e.,

  -- -------- -------- -- --------
     @xmath   @xmath      (4.21)
     @xmath   @xmath      (4.22)
  -- -------- -------- -- --------

After some simple calculation, we get ( 4.16 ). ∎

Using these calculations, we can get the following theorem which is an
analog of the simplest 5D AGT relation (Fact 2.3 ), and prove it more
easily than the generic case.

###### Theorem 4.3.

  -- -------- -- --------
     @xmath      (4.23)
  -- -------- -- --------

Note that the left hand side is independent of @xmath .

###### Proof.

@xmath can be rewritten as

  -- -------- -- --------
     @xmath      (4.24)
  -- -------- -- --------

which has simple poles at @xmath with @xmath , @xmath and @xmath . Then

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (4.25)
     @xmath   @xmath   @xmath      (4.26)
              @xmath   @xmath      (4.27)
  -- -------- -------- -------- -- --------

Note that

  -- -------- -- --------
     @xmath      (4.28)
  -- -------- -- --------

Thus

  -- -------- -- --------
     @xmath      (4.29)
  -- -------- -- --------

is an odd function in @xmath . Therefore

  -- -- -- --------
           (4.30)
  -- -- -- --------

Residues at all singularities in @xmath of @xmath vanish, but @xmath .
Hence @xmath is independent of @xmath . Therefore,

  -- -------- -- --------
     @xmath      (4.31)
  -- -------- -- --------

∎

In this paper, we discuss the crystallization only of the deformed
Virasoro algebra. It is expected that the limit can be taken for the
general deformed @xmath -algebra. However in the case of @xmath , an
essential singularity seems to appear, and at present we do not know how
to take an appropriate limit. To find an appropriate limit procedure and
apply the AGT conjecture for the deformed @xmath -algebra [ 68 ] we need
further studies. In the crystallized case, the screening current
diverges, which is one of the reasons why in this limit singular vectors
disappear. Hence it may be difficult to apply the AGT correspondence
studied by [ 37 ] .

### 4.2 Crystallization of @xmath case of DIM algebra

Next, we discuss a crystallization of the results of Subsection 2.2 . In
this subsection and the next subsection, unlike Section 3 , the
operators @xmath are assumed to be independent of the parameter @xmath
in order to avoid difficulty in taking the @xmath limit. Let us realize
the operators @xmath and the vector @xmath as

  -- -------- -- --------
     @xmath      (4.32)
  -- -------- -- --------

Then they also satisfy relation @xmath . Similarly @xmath . Moreover, we
consider the case that the parameters @xmath are independent of @xmath .
The case where the parameters @xmath depend on @xmath is briefly
described in Section 4.4 .

At first, let us demonstrate the @xmath limit in the @xmath case. In
this subsection, we use the same bosons @xmath and @xmath as subsection
4.1 . Since singularity in @xmath can be removed by normalization @xmath
, define the vertex operator @xmath by

  -- -------- -- --------
     @xmath      (4.33)
  -- -------- -- --------

If @xmath , @xmath are ordinary Macdonald functions, and their integral
forms @xmath have, at @xmath , the relation

  -- -- -- --------
           (4.34)
           (4.35)
  -- -- -- --------

Hence, the matrix elements @xmath can be written in terms of integrals
by virtue of Jing’s operators @xmath and @xmath defined in ( A.8 ) and (
A.9 ). Using the usual normal ordered product @xmath with respect to the
bosons @xmath , ¹⁵ ¹⁵ 15 Let @xmath be the Heisenberg algebra generated
by the bosons @xmath ( @xmath ), @xmath and @xmath . @xmath is the
algebra obtained by making @xmath commutative. The normal ordered
product @xmath is defined to be the linear map from @xmath to @xmath
such that for @xmath ,

(4.36)

and @xmath . In the next subsection, the same symbol @xmath denotes the
normal ordered product with respect to the bosons @xmath which is
defined similarly. we have

  -- -------- -- --------
     @xmath
     @xmath      (4.37)
     @xmath
     @xmath      (4.38)
  -- -------- -- --------

Thus

  -- -------- -- --------
     @xmath      (4.39)
  -- -------- -- --------

where @xmath , @xmath , @xmath , and the integration contour is @xmath .
This integral reproduces the @xmath limit of the Nekrasov factor.

###### Definition 4.4.

Set

  -- -------- -------- -- --------
     @xmath   @xmath      (4.40)
              @xmath
  -- -------- -------- -- --------

where @xmath is the set of boxes in @xmath whose arm length @xmath is
not zero. For example, if @xmath , @xmath . This Nekrasov factor has the
property @xmath for any @xmath .

Therefore, the conjecture in the crystallized case of @xmath is

  -- -------- -- --------
     @xmath      (4.41)
  -- -------- -- --------

The case of some particular partitions can be checked by calculating the
contour integral (Appendix C.3 ).

### 4.3 Crystallization of @xmath case of DIM algebra

Next, let us consider the @xmath limit in the case of @xmath . In this
case, the generator @xmath of the Heisenberg algebra is renormalized as

  -- -------- -- --------
     @xmath      (4.42)
  -- -------- -- --------

and the generator @xmath is used as it is. By this normalization, it is
possible to take the limit. Also, the algebraic structure of @xmath and
@xmath does not change. Then @xmath and @xmath have the form

  -- -------- -- --------
     @xmath      (4.43)
     @xmath      (4.44)
     @xmath      (4.45)
  -- -------- -- --------

Moreover, let us use the bosons @xmath ( @xmath ) and @xmath with the
relation

  -- -------- -- --------
     @xmath      (4.46)
  -- -------- -- --------

and regard @xmath , @xmath . Let us define the generator at @xmath .

###### Definition 4.5.

Set

  -- -------- -- --------
     @xmath      (4.47)
  -- -------- -- --------

###### Proposition 4.6.

Definition 4.5 is well-defined, i.e., @xmath has no singularity at
@xmath , and its free field representation is

  -- -------- -- --------
     @xmath      (4.48)
  -- -------- -- --------

where @xmath is defined in Section 4.1 and

  -- -------- -- --------
     @xmath      (4.49)
     @xmath      (4.50)
  -- -------- -- --------

###### Proof.

Define @xmath and @xmath by

  -- -------- -- --------
     @xmath      (4.51)
  -- -------- -- --------

we can see @xmath is well-behaved in the limit @xmath by the form of
@xmath . If @xmath ,

  -- -------- -- --------
     @xmath      (4.52)
  -- -------- -- --------

if @xmath ,

  -- -------- -- --------
     @xmath      (4.53)
  -- -------- -- --------

and if @xmath ,

  -- -------- -- --------
     @xmath      (4.54)
  -- -------- -- --------

Thus @xmath is well-defined and ( 4.48 ) is the natural free field
representation. ∎

For the second generator, the following rescale is suitable.

###### Definition 4.7.

Set

  -- -------- -- --------
     @xmath      (4.55)
  -- -------- -- --------

###### Proposition 4.8.

The free field representation of @xmath is given by

  -- -------- -- --------
     @xmath      (4.56)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (4.57)
     @xmath
  -- -------- -- --------

This proposition is easily obtained by calculating @xmath . We can
calculate the commutation relation of these generators as follows.

###### Proposition 4.9.

The generators @xmath and @xmath satisfy the relations

  -- -------- -------- -- --------
     @xmath   @xmath      (4.58)
              @xmath      (4.59)
              @xmath      (4.60)
              @xmath      (4.61)
  -- -------- -------- -- --------

  -- -------- -------- -- --------
     @xmath   @xmath      (4.62)
              @xmath      (4.63)
              @xmath      (4.64)
  -- -------- -------- -- --------

  -- -------- -- --------
     @xmath      (4.65)
  -- -------- -- --------

###### Proof.

These are obtained by the following relation of generating functions:

  -- -------- -------- -- --------
     @xmath   @xmath      (4.66)
     @xmath   @xmath      (4.67)
     @xmath               (4.68)
     @xmath   @xmath      (4.69)
     @xmath   @xmath      (4.70)
     @xmath   @xmath      (4.71)
  -- -------- -------- -- --------

where

  -- -------- -- --------
     @xmath      (4.72)
  -- -------- -- --------

and for ( 4.68 ) we used the formula

  -- -------- -- --------
     @xmath      (4.73)
  -- -------- -- --------

∎

The algebra generated by @xmath and @xmath is closely related to the
Hall-Littlewood functions. In particular, the PBW type vectors can be
written as the product of two Hall-Littlewood functions.

###### Definition 4.10.

For a pair of partitions @xmath , set

  -- -------- -- --------
     @xmath      (4.74)
     @xmath      (4.75)
  -- -------- -- --------

We have the expression of these vectors in terms of the Hall-Littlewood
functions.

###### Proposition 4.11.

  -- -------- -- --------
     @xmath      (4.76)
     @xmath      (4.77)
  -- -------- -- --------

where

  -- -------- -------- -- --------
     @xmath   @xmath      (4.78)
     @xmath   @xmath      (4.79)
  -- -------- -------- -- --------

The vectors @xmath do not have such a good expression. This proposition
is proved by the theory of Jing’s operator. Then the vectors @xmath are
partially diagonalized as the following proposition. Furthermore, with
the help of Hall-Littlewood functions, we can calculate the Shapovalov
matrix @xmath and its inverse @xmath .

###### Proposition 4.12.

We can express @xmath by the inner product @xmath of Hall-Littlewood
functions defined in Appendix A :

  -- -------- -------- -- --------
     @xmath   @xmath      (4.80)
              @xmath
     @xmath   @xmath      (4.81)
              @xmath
  -- -------- -------- -- --------

###### Proof.

( 4.80 ) follows from Proposition 4.11 . ( 4.81 ) can be obtained by the
equation

  -- -- -- --------
           (4.82)
  -- -- -- --------

which is shown by inserting the complete system with respect to @xmath
into the equation @xmath . ∎

Existence of the inverse matrix @xmath leads linear independence of
@xmath . Since there are the same number of linear independent vectors
as the dimension of each level of @xmath , we can see that @xmath forms
a basis over @xmath .

###### Proposition 4.13.

If @xmath is not a root of unity and @xmath , @xmath (resp. @xmath ) is
a basis of @xmath (resp. @xmath ).

Next, let us introduce generalized Hall-Littlewood functions which are
specialization of generalized Macdonald functions and give some
crystallized versions of the AGT conjecture.

###### Definition 4.14.

Define the vectors @xmath and @xmath as the @xmath limit of generalized
Macdonald functions, i.e.,

  -- -------- -- --------
     @xmath      (4.83)
  -- -------- -- --------

We call the vectors @xmath generalized Hall-Littlewood functions.

These are the eigenvectors of @xmath :

  -- -------- -- --------
     @xmath      (4.84)
  -- -------- -- --------

Moreover the eigenvalues are

  -- -------- -- --------
     @xmath      (4.85)
  -- -------- -- --------

However there are too many degenerate eigenvalues to ensure the
existence of generalized Hall-Littlewood functions. It is difficult to
characterize @xmath as the eigenfunction of only @xmath . For example,
@xmath and @xmath have the relation @xmath , but @xmath . ¹⁶ ¹⁶ 16
Definition 4.14 is given under the hypothesis that the vector @xmath has
no singulality in the limit @xmath . If we can show the existence therem
of both generalized Macdonald and generalized Hall-Littlewood functions
by using the same partial ordering and the same basis, this hypothesis
is guaranteed.

###### Example 4.15.

Let us define the transition matrices @xmath and @xmath by the
expansions

  -- -------- -- --------
     @xmath      (4.86)
     @xmath      (4.87)
  -- -------- -- --------

Then up to the degree 2 the matrix elements @xmath are given by

  -- -------- --
     @xmath
  -- -------- --

  -- -------- --
     @xmath
  -- -------- --

Up to the degree 2 the matrix elements @xmath are given by

  -- -------- --
     @xmath
  -- -------- --

  -- -------- --
     @xmath
  -- -------- --

Similarly to the case of generic @xmath , we define the integral forms
of generalized Hall-Littlewood functions and give a conjecture of their
norms.

###### Definition 4.16.

The integral forms @xmath and @xmath are defined by

  -- -------- -- --------
     @xmath      (4.88)
     @xmath      (4.89)
  -- -------- -- --------

Note that the coefficients @xmath and @xmath can be zero at @xmath .

###### Conjecture 4.17.

  -- -------- -- --------
     @xmath      (4.90)
  -- -------- -- --------

Next, let us define the vertex operator at crystal limit.

###### Definition 4.18.

The vertex operator @xmath is the linear operator satisfying the
relations

  -- -------- -- --------
     @xmath      (4.91)
     @xmath      (4.92)
     @xmath      (4.93)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (4.94)
  -- -------- -- --------

The existence of such an operator is shown by the renormalization @xmath
. In the relation of @xmath and @xmath , it is understood that this
renormalization is appropriate by considering the shift of @xmath such
that @xmath and not all @xmath are commutative and the relation does not
diverge. We give some simple properties of the vertex operator @xmath .

###### Proposition 4.19.

  -- -------- -- --------
     @xmath      (4.95)
  -- -------- -- --------

For any @xmath ,

  -- -- -- --------
           (4.96)
  -- -- -- --------

These follow from the commutation relations of @xmath . Especially note
that the three-point function which has generators @xmath on the left
side, i.e., @xmath , remains only in the case of special Young diagrams
@xmath with only one vertical column.

###### Conjecture 4.20.

The matrix elements of @xmath with respect to the integral form @xmath
are

  -- -------- -------- -- --------
     @xmath   @xmath      (4.97)
              @xmath
  -- -------- -------- -- --------

Under these conjectures 4.17 and 4.20 , we obtain the formula for
correlation functions of the vertex operator @xmath . For example, the
function corresponding to the four-point conformal block is

  -- -------- -------- -- --------
     @xmath   @xmath
              @xmath
              @xmath      (4.98)
  -- -------- -------- -- --------

( 4.98 ) is the AGT conjecture in the limit @xmath with help of the AFLT
basis. However, in the crystallized case, we can prove another formula
for this four-point correlation function by using the PBW type basis. At
first, let us show the following two lemmas.

###### Lemma 4.21.

The matrix elements with respect to PBW type vector @xmath and @xmath
are

  -- -------- -- --------
     @xmath      (4.99)
  -- -------- -- --------

###### Proof.

For @xmath , by ( 4.93 ) and the relation @xmath ,

  -- -------- -------- -- ---------
     @xmath   @xmath      (4.100)
              @xmath
              @xmath
  -- -------- -------- -- ---------

Repeating this calculation, we get

  -- -------- -- ---------
     @xmath      (4.101)
  -- -------- -- ---------

where @xmath . When @xmath , by similar calculation

  -- -------- -------- -- ---------
     @xmath   @xmath
              @xmath      (4.102)
  -- -------- -------- -- ---------

By using above two formulas ( 4.101 ) and ( 4.102 ), if we write @xmath
,

  -- -------- -------- -- ---------
     @xmath   @xmath
              @xmath
              @xmath
              @xmath
              @xmath      (4.103)
  -- -------- -------- -- ---------

∎

We have explicit form of formulas for some parts of the inverse
Shapovalov matrix.

###### Lemma 4.22.

  -- -------- -- ---------
     @xmath      (4.104)
  -- -------- -- ---------

###### Proof.

In this proof, we put @xmath . Hall-Littlewood function @xmath is the
elementary symmetric function @xmath times @xmath . Elementary symmetric
functions have the generating function

  -- -------- -- ---------
     @xmath      (4.105)
  -- -------- -- ---------

Hence by the @xmath case of Fact A.2 ,

  -- -------- -------- -- ---------
     @xmath   @xmath      (4.106)
              @xmath
              @xmath
  -- -------- -------- -- ---------

Therefore, the lemma follows from Proposition 4.12 . ∎

We give other proofs of this lemma in Appendix C.1 , and the form of
@xmath can be found in Appendix C.2 . By the property ( 4.95 ),
Proposition 4.12 and Lemmas 4.21 and 4.22 , we can show the following
theorem.

###### Theorem 4.23.

  -- -------- -------- -- ---------
     @xmath   @xmath
              @xmath
              @xmath      (4.107)
  -- -------- -------- -- ---------

In this way, the explicit formula for the correlation function can be
obtained, where we don’t use any conjecture. The formulas ( 4.98 ) and (
4.23 ) are compared in Appendix C.4 . We expect that these works will be
generalized to @xmath case.

### 4.4 Other types of limit

Finally, we present other types of the crystal limit. In this paper, we
investigated the crystal limit while the parameters @xmath , @xmath and
@xmath are fixed. However, it is also important to study the cases when
these parameters depend on @xmath . For example, let us consider the
case that @xmath , @xmath , @xmath ( @xmath ) and @xmath , @xmath ,
@xmath are independent of @xmath or fixed in the limit @xmath . Let
@xmath for all @xmath and @xmath . Then the Nekrasov formula for generic
@xmath case ( @xmath )

  -- -------- -- ---------
     @xmath      (4.108)
  -- -------- -- ---------

depends only on the partitions of the shape @xmath in the limit @xmath ,
where @xmath is fixed, and coincides with the partition function of the
pure gauge theory ( 4.16 ):

  -- -------- -- ---------
     @xmath      (4.109)
  -- -------- -- ---------

where @xmath . Hence, we expect that the vector

  -- -------- -- ---------
     @xmath      (4.110)
  -- -------- -- ---------

corresponds to the Whittaker vector in the section 4.1 in this limit,
though we were not able to properly explain it. In this way, by
considering the various other values of @xmath and @xmath , we can find
special behavior of @xmath and the conformal block @xmath and may prove
the relation. These are our future studies.

## 5 R-Matrix of DIM algebra

### 5.1 Explicit calculation of R-Matrix

In general, a bialgebra @xmath is called quasi-cocommutative if there
exists an invertible element @xmath such that for all @xmath ,

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

This @xmath is called the universal R-matirx. The DIM algebra is
quasi-cocommutative [ 42 ] . In this section, we explicitly calculate
the representation of @xmath . Moreover, the expression of its
representation matrix is generally conjectured. ¹⁷ ¹⁷ 17 The results in
this Section are contribution of the author in the collaborations [ 52 ,
53 ] .

The point of calculation is to make use of the condition that the
generalized Macdonald functions are the eigenfunctions of @xmath , to
reduce the degree of freedom of the matrix in advance. In this section,
we formally write the universal @xmath -matrix as @xmath and set @xmath
, @xmath , @xmath . Occasionally, we explicitly write the variable of
the generalized Macdonald functions like

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

and the PBW basis of the bosons is written as

  -- -------- -- -------
     @xmath      (5.3)
  -- -------- -- -------

Firstly, by definition of @xmath , we have

  -- -------- -------- -- -------
     @xmath   @xmath      (5.4)
              @xmath
  -- -------- -------- -- -------

Here @xmath . The formula for the coproduct of the DIM algebra and the
definition of the representation @xmath are given in Appendix B . Hence,
@xmath are eigenfunctions of @xmath . By comparing the forms of @xmath
and @xmath , the eigenfunctions of @xmath are obtained by replacing
@xmath with @xmath and @xmath with @xmath . Moreover, by checking their
eigenvalues, it can be seen that @xmath are proportional to @xmath :

  -- -- -- -------
           (5.5)
  -- -- -- -------

where @xmath are proportionality constants. By this property, the
representation matrix of @xmath is block-diagonalized at each level of
the boson @xmath . The proportionality constants @xmath can be
calculated by using the generalized Macdonald functions in the @xmath
case.

Similarly to the @xmath case, from the relation

  -- -------- -- -------
     @xmath      (5.6)
  -- -------- -- -------

we have

  -- -------- -- -------
     @xmath      (5.7)
  -- -------- -- -------

where @xmath and @xmath are constants. Since the generalized Macdonald
functions satisfy

  -- -------- -- -------
     @xmath      (5.8)
  -- -------- -- -------

the proportionality constants have the relation @xmath .

For example let us describe the calculation of the representation matrix
at level 1. The following are examples of @xmath at level 1:

  -- -------- -- -------
     @xmath      (5.9)
  -- -------- -- -------

  -- -------- -- --------
     @xmath      (5.10)
  -- -------- -- --------

By the above discussion, if we set the matrix

  -- -------- -- --------
     @xmath      (5.11)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (5.12)
  -- -------- -- --------

then the representation matrix of @xmath in the basis of the generalized
Macdonald functions is the transposed matrix of @xmath :

  -- -- -- --------
           (5.13)
  -- -- -- --------

The constants @xmath are determined as follows. At first, since scalar
multiples of R-matrices are also R-matrices, we can normalize as @xmath
. This means that @xmath . Next, we consider the base change from @xmath
to the bosons @xmath :

  -- -------- -- --------
     @xmath      (5.14)
  -- -------- -- --------

Then @xmath is in the form

  -- -------- -- --------
     @xmath      (5.15)
  -- -------- -- --------

where @xmath is a function of @xmath . Since for the action of @xmath to
@xmath , variables @xmath and @xmath should not appear, we get equations
@xmath . By solving these equations, we can see that

  -- -------- -- --------
     @xmath      (5.16)
  -- -------- -- --------

Substituting this value into the matrix ( 5.15 ), we have

  -- -------- -- --------
     @xmath      (5.17)
  -- -------- -- --------

In this way, we obtain the explicit expression @xmath of representation
matrix of universal @xmath at level 1. Of course, it is possible to
calculate the representation matrix of @xmath in the same way, but by
using symmetry with respect to @xmath at different @xmath , we can
easily understand the forms of @xmath and @xmath :

  -- -------- -- --------
     @xmath      (5.18)
  -- -------- -- --------

Indeed, we can check that they satisfy the Yang-Baxter equation

  -- -------- -- --------
     @xmath      (5.19)
  -- -------- -- --------

Incidentally, in the basis of the generalized Macdonald functions,

  -- -------- -- --------
     @xmath      (5.20)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (5.21)
  -- -------- -- --------

The representation matrix of @xmath is the @xmath matrix block at the
lower right corner of @xmath or @xmath . For example, in the basis of
generalized Macdonald functions, its representation matrix is

  -- -------- -- --------
     @xmath      (5.22)
  -- -------- -- --------

Next, let us explain the case at level 2. The generalized Macdonald
functions at level 2 in the @xmath case are expressed as

  -- -------- -- --------
     @xmath
                 (5.23)
  -- -------- -- --------

where @xmath denotes the product of ordinary Macdonald functions @xmath
, and the matrix @xmath is given in Appendix D . In the same manner, we
can get the representation matrix of @xmath . At first, @xmath at level
2 is in the form

  -- -------- -- --------
     @xmath      (5.24)
  -- -------- -- --------

Then we can find the proportionality constants such that all @xmath are
zero just by solving equations @xmath ( @xmath ). We have also checked
that the representation matrix @xmath obtained in this way satisfies the
Yang-Baxter equation up to level 3. The explicit expressions of @xmath
at level 2 are written in Appendix D .

### 5.2 General formula for R-matrix

The proportionality constants are in the form

  -- -------- -- --------
     @xmath      (5.25)
     @xmath      (5.26)
     @xmath      (5.27)
     @xmath      (5.28)
  -- -------- -- --------

These proportionality constants can be simplified by using the integral
forms of the generalized Macdonald functions @xmath , which are defined
in Section 2.2 . Define its opposite version by

  -- -------- -- --------
     @xmath      (5.29)
  -- -------- -- --------

and the constants @xmath by

  -- -------- -- --------
     @xmath      (5.30)
  -- -------- -- --------

By these renormalized functions, the relation ( 5.5 ) can be written as

  -- -------- -- --------
     @xmath      (5.31)
  -- -------- -- --------

Then it is conjectured that

  -- -------- -- --------
     @xmath      (5.32)
  -- -------- -- --------

This equation has been checked at @xmath . Therefore, the representation
matrix @xmath of @xmath in the basis of the integral forms can be
expressed as the following conjecture.

###### Conjecture 5.1.

  -- -------- -- --------
     @xmath      (5.33)
  -- -------- -- --------

Since the formula ( 5.33 ) means the expansion coefficients of @xmath in
front of @xmath , by using the transition matrix defined by

  -- -------- -- --------
     @xmath      (5.34)
  -- -------- -- --------

and its opposite version @xmath , the R-matrix can be calculated by the
matrix operation

  -- -------- -- --------
     @xmath      (5.35)
  -- -------- -- --------

This formula gives a much simpler way to get explicit expressions as
compared with deducing them from the universal R-matrix [ 42 ] .
Incidentally, the proportionality constants are conjectured to be

  -- -------- -- --------
     @xmath      (5.36)
  -- -------- -- --------

where @xmath is the Nekrasov factor defined in ( 2.10 ), Section 2.1 .
The second equality follows from the formula (eq.(2.34) in [ 47 ] ,
eq.(102) in [ 69 ] )

  -- -------- -- --------
     @xmath      (5.37)
  -- -------- -- --------

where @xmath is the framing factor [ 70 ] . The equation ( 5.36 ) has
been checked up to level 3.

## 6 Properties of generalized Macdonald functions

### 6.1 Partial orderings

The existence theorem of generalized Macdonald functions can be stated
by the ordering @xmath in Definition 2.10 . In this subsection, we
introduce a more elaborated ordering. Using this ordering, we can find
more elements which is @xmath in the transition matrix @xmath , where
@xmath , and get more strict condition to existence theorem.

###### Definition 6.1.

For @xmath -tuples of partitions @xmath and @xmath ,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath
  -- -------- -------- -------- --

for all @xmath . Here @xmath denote that @xmath for all @xmath .

###### Example 6.2.

If @xmath and the number of boxes is @xmath , then

  -- -- --

  -- -- --

Here @xmath stands for @xmath .

By using the following conjecture, we can state the existence theorem.

###### Conjecture 6.3.

Let @xmath . In the action of @xmath ( @xmath ) on Macdonald functions
@xmath , there only appear partitions @xmath contained in @xmath , i.e.,

  -- -------- -- -------
     @xmath      (6.1)
  -- -------- -- -------

###### Theorem 6.4.

Under the Conjecture 6.3 , for an @xmath -tuple of partitions @xmath ,
there exists an unique vector @xmath such that

  -- -------- -- -------
     @xmath      (6.2)
     @xmath      (6.3)
  -- -------- -- -------

###### Proof.

At first, @xmath satisfies

  -- -------- -- -------
     @xmath      (6.4)
     @xmath
  -- -------- -- -------

If we act @xmath on the product of the Macdonald functions, then

  -- -------- -------- -- -------
     @xmath   @xmath      (6.5)
              @xmath
  -- -------- -------- -- -------

where @xmath is a polynomial of degree @xmath of @xmath . ¹⁸ ¹⁸ 18 That
is to say, for the operator @xmath such that @xmath , the polynomial
@xmath satisfies @xmath . Hence

  -- -- -- -------
           (6.6)
  -- -- -- -------

Therefore one can easily diagonalize it and we have this theorem. ∎

In the basis of monomial symmetric functions @xmath , we have

  -- -------- -------- -- -------
     @xmath   @xmath      (6.7)
              @xmath
              @xmath
  -- -------- -------- -- -------

where

  -- -------- -- -------
     @xmath      (6.8)
  -- -------- -- -------

( @xmath ). Thus the partial ordering @xmath defined as follows also
triangulates @xmath .

  -- -------- -- -------
     @xmath      (6.9)
  -- -------- -- -------

It can be shown that the partial ordering @xmath is equivalent to the
ordering @xmath introduced in [ 39 ] . Therefore Theorem 6.4 supports
the existence theorem in [ 39 , Proposition3.8] .

### 6.2 Realization of rank @xmath representation by generalized
Macdonald function

A representation of the DIM algebra called rank @xmath representation is
provided in [ 44 ] in terms of a basis @xmath called AFLT basis. This
rank @xmath representation corresponds to the @xmath -fold tensor
product of the level (0,1) representation described in Appendix B . The
level (0,1) representation can be considered as the spectral dual to the
level (1,0) representation which is realized by the Heisenberg algebra.
In this subsection, based on this spectral duality we present
conjectures for explicit expressions of the action of @xmath on the
generalized Macdonald functions, which are defined to be eigenfunctions
of the Hamiltonian @xmath . We can also conjecture the eigenvalues of
higher rank Hamiltonians on the generalized Macdonald functions from
those of the spectral dual generators provided in [ 44 ] . Our
conjectures mean that the generalized Macdonald functions concretely
realize the spectral dual basis to @xmath in [ 44 ] .

#### Action of @xmath on generalized Macdonald function

Although we already define the integral forms @xmath of the generalized
Macdonald functions, let us use another renormalization @xmath of them,
which is defined by

  -- -------- -- --------
     @xmath      (6.10)
  -- -------- -- --------

where @xmath is the Nekrasov factor. This renormalization is the almost
same as @xmath . Their difference is conjectured to be the scalar
multiplication of only monomials in parameter @xmath , @xmath and @xmath
.

It is expected that the basis @xmath corresponds to the AFLT basis ¹⁹ ¹⁹
19 Originally, the AFLT basis is defined by the property that their
inner products and matrix elements of vertex operators reproduce the
Nekrasov factor. In [ 39 ] the integral forms @xmath were already
conjectured to be the AFLT basis in this original sense. in [ 44 ] and
realizes the rank @xmath representation through the spectral duality
@xmath . That is to say, for any generator @xmath in the DIM algebra,
the action of @xmath on the integral forms @xmath is in the same form as
one of @xmath on the basis @xmath [ 44 ] , where @xmath . Indeed, we can
check that the action of @xmath on the generalized Macdonald functions
is as the following conjecture. Let us denote adding a box to or
removing it from the Young diagram @xmath through @xmath and @xmath
respectively. We also use the notation @xmath for the triple @xmath ,
where @xmath is the coordinate of the box of the Young diagram @xmath .

###### Conjecture 6.5.

  -- -------- -- --------
     @xmath      (6.11)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (6.12)
     @xmath      (6.13)
  -- -------- -- --------

and for the triple @xmath , we put

  -- -------- -- --------
     @xmath      (6.14)
  -- -------- -- --------

These actions of @xmath in this conjecture come from the corresponding
actions of the generators @xmath and @xmath in [ 44 ] respectively,
i.e., @xmath and @xmath in our notation, which are the spectral duals of
@xmath and @xmath . Incidentally, introducing the coefficients @xmath by

  -- -- -- --------
           (6.15)
  -- -- -- --------

i.e., @xmath , we can further conjecture that

  -- -------- -- --------
     @xmath      (6.16)
  -- -------- -- --------

Conjecture 6.5 with respect to @xmath and the formula ( 6.16 ) are
checked on a computer for @xmath for @xmath , for @xmath for @xmath and
for @xmath for @xmath . Conjecture 6.5 with respect to @xmath is also
checked for the same size of @xmath .

#### Higher Hamiltonian

For each integer @xmath , the spectral dual of @xmath is @xmath defined
by @xmath and

  -- -------- -- --------
     @xmath      (6.17)
  -- -------- -- --------

According to [ 40 ] , @xmath are spectral dual to @xmath and
consequently mutually commuting; @xmath . Thus the generalized Macdonald
functions @xmath are automatically eigenfunctions of all @xmath , i.e.,
@xmath , and @xmath can be regarded as higher Hamiltonians for the
generalized Macdonald functions. Since @xmath are the spectral duals to
@xmath ; @xmath , their eigenvalues are expected to be

###### Conjecture 6.6.

  -- -------- -- --------
     @xmath      (6.18)
  -- -------- -- --------

where @xmath is defined in ( B.21 ).

The eigenvalues @xmath correspond to those of the rank @xmath
representation of the generators @xmath in [ 44 ] . In the @xmath case,
the conjecture ( 6.18 ) can be proved. We have checked it for @xmath for
@xmath , for @xmath for @xmath , for @xmath for @xmath and for @xmath
for @xmath in the @xmath case.

### 6.3 Limit to @xmath deformation

In this subsection, we show that the generalized Macdonald functions are
reduced to the generalized Jack functions introduced by Morozov and
Smirnov in [ 28 ] in the @xmath limit (see also the sub-thesis [ 71 ] ).
Although the scenario of proof of AGT correspondence is given in [ 28 ]
, the orthogonality of the generalized Jack functions are non-trivial
since there are degenerate eigenvalues. The Cauchy formula used in the
scenario of proof can not be proved without the orthogonality. However,
the eigenvalues of generalized Macdonald functions are non-degenerate.
Hence, we can prove the orthogonality of the generalized Jack functions
by using the limit in this section. When taking this limit, we set
@xmath @xmath , @xmath , @xmath and take the limit @xmath with @xmath
fixed. To expose the @xmath dependence of the generators @xmath in the
Heisenberg algebra, they are realized in terms of @xmath kinds of power
sum symmetric functions @xmath with @xmath in the ring of symmetric
functions @xmath in the variables @xmath ( @xmath , @xmath ):

  -- -------- -- --------
     @xmath      (6.19)
  -- -------- -- --------

Since the operators @xmath become parameter @xmath in the representation
space @xmath , they are transformed as

  -- -------- -- --------
     @xmath      (6.20)
  -- -------- -- --------

from the beginning in this section. With the above transformation, the
generator @xmath can be regarded as the operator over the ring of
symmetric functions @xmath . Define the isomorphism @xmath by

  -- -------- -- --------
     @xmath      (6.21)
  -- -------- -- --------

where @xmath is defined in Section 5.1 . The inner product @xmath over
@xmath is defined by

  -- -------- -- --------
     @xmath      (6.22)
  -- -------- -- --------

This inner product naturally realize the one over the Fock module @xmath
, i.e.,

  -- -------- -- --------
     @xmath      (6.23)
  -- -------- -- --------

Let @xmath be the adjoint operator of @xmath with respect to the inner
product. Since the generalized Macdonald functions @xmath have
non-degenerate eigenvalues the orthogonality clearly follows:

  -- -------- -- --------
     @xmath      (6.24)
  -- -------- -- --------

where @xmath is defined to be the eigenfunctions of the adjoint operator
@xmath of the eigenvalue @xmath .

Now let us take the @xmath limit. At first, consider the @xmath
expansion of @xmath . By @xmath and @xmath , @xmath , we have

  -- -------- -------- -- --------
     @xmath   @xmath      (6.25)
              @xmath
              @xmath
              @xmath
  -- -------- -------- -- --------

Hence the @xmath expansion is

  -- -------- -------- -- --------
     @xmath   @xmath      (6.26)
              @xmath
              @xmath
  -- -------- -------- -- --------

Thus we get

  -- -------- -------- -- --------
     @xmath   @xmath      (6.27)
              @xmath
  -- -------- -------- -- --------

where

  -- -------- -- --------
     @xmath      (6.28)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (6.29)
  -- -------- -- --------

For @xmath , we define operators @xmath by

  -- -------- -- --------
     @xmath      (6.30)
  -- -------- -- --------

With respect to @xmath , @xmath and @xmath , all homogeneous symmetric
functions belong to the same eigenspace. Hence, the eigenfunctions of
@xmath are the same to those of @xmath . In addition, we have

  -- -------- -- --------
     @xmath      (6.31)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (6.32)
  -- -------- -- --------

Consequently the limit @xmath of the generalized Macdonald functions are
eigenfunctions of the differential operator @xmath . As a matter of
fact, @xmath plus the momentum @xmath corresponds to the differential
operator of [ 28 , 72 ] , the eigenfunctions of which are called
generalized Jack symmetric functions. ²⁰ ²⁰ 20 To be adjusted to the
notation of [ 28 , 72 ] , we need to transform the subscripts: @xmath ,
@xmath .

As in [ 39 , Proposition 3.7] , we can triangulate @xmath similarly.
Moreover if @xmath ²¹ ²¹ 21 We write @xmath (resp. @xmath ) if and only
if @xmath and

@xmath (6.33)

@xmath (6.34)

for all @xmath and @xmath . and @xmath is generic, then @xmath . (
@xmath , @xmath are eigenvalues of @xmath .) Therefore we get the
existence theorem of the generalized Jack symmetric functions.

###### Proposition 6.7.

There exists a unique symmetric function @xmath satisfying the following
two conditions:

  -- -------- -- --------
     @xmath      (6.35)
     @xmath      (6.36)
  -- -------- -- --------

where @xmath denotes the product of monomial symmetric functions @xmath
. ( @xmath is the usual monomial symmetric function of variables @xmath
.)

From the above argument and the uniqueness in this proposition we get
the following important result.

###### Proposition 6.8.

The limit of the generalized Macdonald symmetric functions @xmath to
@xmath -deformation coincide with the generalized Jack symmetric
functions @xmath . That is

  -- -------- -- --------
     @xmath      (6.37)
  -- -------- -- --------

###### Remark 6.9.

For the dual functions @xmath and @xmath , a similar proposition holds.

By the orthogonality ( 6.24 ), Proposition 6.8 and the fact that the
scalar product @xmath reduces to the scalar product @xmath which is
defined by

  -- -------- -- --------
     @xmath      (6.38)
  -- -------- -- --------

we obtain the orthogonality of the generalized Jack symmetric functions.

###### Proposition 6.10.

If @xmath , then

  -- -------- -- --------
     @xmath      (6.39)
  -- -------- -- --------

By this proposition, we can prove the Cauchy formula for generalized
Jack symmetric functions in the usual way. For example, in the @xmath
case, we have

  -- -------- -- --------
     @xmath      (6.40)
  -- -------- -- --------

where @xmath . This is the necessary formula in the scenario of proof of
the AGT conjecture [ 28 ] .

We give examples of Proposition 6.8 in the case @xmath . The generalized
Macdonald symmetric functions of level 1 and 2 have the forms:

  -- -- -- --------
           (6.41)
  -- -- -- --------

  -- -------- -- --------
     @xmath      (6.42)
  -- -------- -- --------

  -- -------- --
     @xmath
  -- -------- --

Also the generalized Jack symmetric functions have the forms:

  -- -- -- --------
           (6.43)
  -- -- -- --------

  -- -------- -- --------
     @xmath      (6.44)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (6.45)
  -- -------- -- --------

If we take the limit @xmath of @xmath , then @xmath appears.

## Appendix

## Appendix A Macdonald functions and Hall-Littlewood functions

In this subsection, we briefly review some properties of Hall-Littlewood
functions and Macdonald functions following [ 5 , Chap. III, VI] .

Let @xmath be the ring of symmetric polynomials of @xmath variables and
@xmath be the ring of symmetric functions. The inner product @xmath over
@xmath is defined such that for power sum symmetric functions @xmath (
@xmath ),

  -- -------- -- -------
     @xmath      (A.1)
  -- -------- -- -------

where @xmath is the number of entries in @xmath equal to @xmath . For a
partition @xmath , Macdonald functions @xmath are uniquely determined by
the following two conditions [ 5 ] :

  -- -------- -- -------
     @xmath      (A.2)
     @xmath      (A.3)
  -- -------- -- -------

Here @xmath is the monomial symmetric function and @xmath is the
ordinary dominance partial ordering, which is defined as follows:

  -- -------- -- -------
     @xmath      (A.4)
  -- -------- -- -------

In this paper, we regard power sum symmetric functions @xmath ( @xmath )
as the variables of Macdonald functions, i.e., @xmath . Here @xmath is
an abbreviation for @xmath . In this paper, we often use the symbol
@xmath , which is the polynomial of bosons @xmath obtained by replacing
@xmath in Macdonald functions with @xmath .

Next, let the Hall-Littlewood function @xmath be given by @xmath . If
@xmath , then for a partition @xmath of length @xmath , the
Hall-Littlewood polynomial @xmath with @xmath is expressed by

  -- -------- -- -------
     @xmath      (A.5)
  -- -------- -- -------

where @xmath . Note that @xmath . The action of the symmetric group
@xmath of degree @xmath is defined by @xmath for @xmath .

It is convenient to introduce functions @xmath , which are defined by
scalar multiples of @xmath as follows:

  -- -------- -- -------
     @xmath      (A.6)
  -- -------- -- -------

where @xmath . They are diagonalized as

  -- -------- -- -------
     @xmath      (A.7)
  -- -------- -- -------

These functions @xmath can be constructed by using Jing’s operators
@xmath and @xmath [ 73 ] , which is defined by

  -- -------- -------- -- -------
     @xmath   @xmath      (A.8)
     @xmath   @xmath      (A.9)
  -- -------- -------- -- -------

where @xmath is the bosons realized by

  -- -------- -- --------
     @xmath      (A.10)
  -- -------- -- --------

###### Fact A.1 ([73]).

Let @xmath be the vector such that @xmath ( @xmath ). Then for a
partition @xmath , we have

  -- -- -------- -------- -- --------
        @xmath   @xmath      (A.11)
        @xmath   @xmath      (A.12)
  -- -- -------- -------- -- --------

Furthermore, the following specialization formula is known.

###### Fact A.2 (Chap. III, §4, Example 3 in [5]).

Let @xmath be indeterminate. Under the specialization

  -- -------- -- --------
     @xmath      (A.13)
  -- -------- -- --------

The Hall-Littlewood function @xmath is specialized as

  -- -------- -- --------
     @xmath      (A.14)
  -- -------- -- --------

## Appendix B Definition of DIM algebra and level @xmath representation

In this section, we recall the definition of the DIM algebra and the
level @xmath representation. For the notations, we follow [ 46 ] . The
DIM algebra has two parameters @xmath and @xmath . Let @xmath be the
formal series

  -- -------- -- -------
     @xmath      (B.1)
  -- -------- -- -------

Then this series satisfies @xmath .

###### Definition B.1.

Define the algebra @xmath to be the unital associative algebra over
@xmath generated by the currents @xmath , @xmath and the central element
@xmath satisfying the defining relations

  -- -------- -- -------
     @xmath      (B.2)
     @xmath      (B.3)
     @xmath      (B.4)
     @xmath      (B.5)
     @xmath      (B.6)
  -- -------- -- -------

Note that @xmath are central elements in @xmath . Let us contain the
invertible elements @xmath and @xmath in the definition of @xmath .
Further, set @xmath . This algebra @xmath is an example of the family
topological Hopf algebras introduced by Ding and Iohara [ 61 ] . This
family is a sort of generalization of the Drinfeld realization of the
quantum affine algebras. However, Miki introduce a deformation of the
@xmath algebra in [ 40 ] , which is the quotient of the algebra @xmath
by the Serre-type relation. Hence we call the algebra @xmath the
Ding-Iohara-Miki algebra (DIM algebra). Since the algebra @xmath has a
lot of background, there are a lot of other names such as quantum
toroidal @xmath algebra [ 42 , 43 ] , quantum @xmath algebra [ 44 ] ,
elliptic Hall algebra [ 45 ] and so on. This algebra has a Hopf algebra
structure. The formulas for its coproduct are

  -- -------- -- -------
     @xmath      (B.7)
     @xmath      (B.8)
     @xmath      (B.9)
  -- -------- -- -------

and @xmath , where @xmath and @xmath . Since we do not use the antipode
and the counit in this thesis, we omit them. The DIM algebra @xmath can
be realized by the Heisenberg algebra defined in Section 2.1 .

###### Fact B.2 ([46, 41]).

The morphism @xmath defined as follows is a representation of the DIM
algebra:

  -- -------- -- --------
     @xmath      (B.10)
     @xmath      (B.11)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (B.12)
     @xmath      (B.13)
     @xmath      (B.14)
     @xmath      (B.15)
  -- -------- -- --------

Not that the zero mode @xmath of @xmath can be essentially identified
with the Macdonald difference operator [ 5 , 14 ] . By using the
coproduct of @xmath , we can consider its tensor representations. For an
@xmath -tuple of parameters @xmath , define the morphism @xmath by

  -- -------- -- --------
     @xmath      (B.16)
  -- -------- -- --------

where @xmath is inductively defined by @xmath , @xmath and @xmath . The
representation @xmath is called the level @xmath representations. In
Section 2.2 , for simplicity, we write the @xmath -th bosons as

  -- -------- -- --------
     @xmath      (B.17)
  -- -------- -- --------

The generator @xmath is defined by

  -- -------- -- --------
     @xmath      (B.18)
  -- -------- -- --------

The @xmath is also called the level @xmath representation or the
horizontal representation in [ 49 , 74 , 52 , 53 ] in order to
distinguish another representation of the DIM algebra, which is called
the level @xmath representation or the vertical representation. To
define the level @xmath representation, we introduce some notations. For
a partition @xmath and a number @xmath , we set

  -- -------- -- --------
     @xmath      (B.19)
     @xmath      (B.20)
     @xmath      (B.21)
     @xmath      (B.22)
     @xmath      (B.23)
     @xmath      (B.24)
  -- -------- -- --------

Here, @xmath and @xmath are considered as elements in @xmath .

###### Fact B.3 ([54, 55]).

Let @xmath be an indeterminate parameter and @xmath be the Fock module
generated by the highest weight vector @xmath . The morphism @xmath
defined as follows gives a representation of the DIM algebra:

  -- -------- -- --------
     @xmath      (B.25)
     @xmath      (B.26)
     @xmath      (B.27)
     @xmath      (B.28)
  -- -------- -- --------

and @xmath , where @xmath denotes the ordinary Macdonald function @xmath
associated with the partition @xmath .

In this thesis, the vectors @xmath are realized by the Macdonald
functions @xmath along . However, they can also be regarded as the
abstract vectors labeled by Young diagrams. Note that the factors ( B.19
)-( B.22 ) can be written by the contribution from the edges of the
Young diagram @xmath , i.e., the positions which we can add a box in or
remove it from. The tensor representation of @xmath is also called the
rank @xmath representation, which is given in [ 44 ] in terms of the
edge contribution. This representation is connected to the level @xmath
representation under the change of basis and the automorphism of the DIM
algebra, that is defined as follows. This connection is called the
spectral duality [ 74 , Section 5] .

###### Fact B.4 ([40]).

There exists an automorphism @xmath such that

  -- -------- -- --------
     @xmath      (B.29)
  -- -------- -- --------

@xmath and @xmath .

Although the algebra in [ 40 ] slightly differs from the algebra @xmath
in the Serre-type relation, this fact holds. This automorphism is of
order four. By using this automorphism, we can check the correspondence
between two representations of the DIM algebra. In Section 6.2 , we
briefly explain the spectral duality and check it with respect to the
generators @xmath and @xmath .

## Appendix C Proofs and checks in Section 4

### c.1 Other proofs of Lemma 4.22

In this subsection, let us explain other proofs of Lemma 4.22 by the
method of contour integrals.

The generating function of elementary symmetric functions and Jing’s
operator makes the equation

  -- -------- -- -------
                 (C.1)
     @xmath
     @xmath
  -- -------- -- -------

where we put @xmath , @xmath , and @xmath . It suffices to show that
@xmath , which is proved by a recursive relation of @xmath as follows.
The contour integral @xmath surrounding origin is represented as that
surrounding @xmath . Since @xmath , the residue of @xmath at @xmath is
@xmath . Hence, the only residue at @xmath is left, and it is

  -- -------- -- -------
     @xmath      (C.2)
  -- -------- -- -------

By change of variable @xmath ,

  -- -------- -- -------
     @xmath      (C.3)
  -- -------- -- -------

Therefore

  -- -------- -- -------
     @xmath      (C.4)
  -- -------- -- -------

Thus the Lemma 4.22 is proved.

Although it is slightly hard, one can also prove this lemma by reversing
the order of integration, i.e., first perform over a variable @xmath
surrounding origin. Indeed @xmath has the pole only at @xmath , and its
residue satisfies

  -- -------- -- -------
     @xmath      (C.5)
  -- -------- -- -------

where for @xmath , @xmath and @xmath . By the assumption that @xmath for
@xmath with @xmath , we inductively get

  -- -- -- -------
           (C.6)
  -- -- -- -------

By virtue of the equation

  -- -------- -- -------
     @xmath      (C.7)
  -- -------- -- -------

which is also proved by induction with respect to @xmath , it can be
seen that @xmath .

### c.2 Explicit form of @xmath

The formula for @xmath is given in the Lemma 4.22 and the last
subsection. We also have an explicit form of @xmath and @xmath . By the
Proposition 4.12 , it suffices to give the explicit form of @xmath .

###### Proposition C.1.

  -- -------- -- -------
     @xmath      (C.8)
  -- -------- -- -------

###### Proof.

The proof is similar to the previous subsection. Set

  -- -------- -- -------
     @xmath      (C.9)
  -- -------- -- -------

Then @xmath by Jing’s operator. Integration of @xmath around @xmath
makes recursive relation

  -- -------- -- --------
     @xmath      (C.10)
  -- -------- -- --------

and leads this Proposition. ∎

For general partitions @xmath and @xmath , we can get the integral
representation of @xmath . However, it is very hard to give their
explicit formula.

### c.3 Check of (4.41)

The integral formula ( 4.41 ) can be checked by the similar way to
subsections C.1 and C.2 .

Let us set

  -- -------- -------- -- --------
     @xmath   @xmath      (C.11)
     @xmath   @xmath
  -- -------- -------- -- --------

Then @xmath , @xmath . The integration of @xmath around @xmath give the
relation

  -- -------- -- --------
     @xmath      (C.12)
  -- -------- -- --------

On the other hand, the integration of @xmath around @xmath makes

  -- -------- -- --------
     @xmath      (C.13)
  -- -------- -- --------

Thus

  -- -------- -------- -- --------
     @xmath   @xmath      (C.14)
     @xmath
  -- -------- -------- -- --------

These agree with the right hand side of ( 4.41 ).

### c.4 Comparison of formulas (4.98) and (4.23)

In this subsection, we compare two formulas ( 4.98 ) and ( 4.23 ) which
are obtained by the other basis.

Comparing the coefficients of @xmath , we have the equation

  -- -------- -- --------
     @xmath      (C.15)
  -- -------- -- --------

Note that the left hand side is the summation with respect to pairs of
partitions @xmath and the right hand side is the summation with respect
to single partitions @xmath . The right hand side depends only on the
ratio @xmath though the left hand side doesn’t look that way.

For a single partition @xmath , let us define @xmath to be the set of
all pairs of partitions @xmath such that a permutation of the sequence
@xmath coincides with @xmath . For example, if @xmath ,

  -- -------- -- --------
     @xmath      (C.16)
  -- -------- -- --------

Then we obtain a strange factorization formula with respect to the
partial summation of left hand side in ( C.15 )

  -- -- -- --------
           (C.17)
  -- -- -- --------

where @xmath and @xmath is introduced in Definition 4.4 . If we prove
that left hand side of ( C.17 ) depends only on @xmath , ( C.17 ) is
easily seen by checking the case of @xmath . This equation almost
reproduces each term of the right hand side in ( C.15 ). Hence ( C.15 )
may be proved by this equation. If ( C.15 ) holds, the AGT conjecture at
@xmath with the help of the AFLT basis ( 4.98 ) is completely proved.

## Appendix D Examples of R-matrix

Examples of the generalized Macdonald functions at level 3 in the @xmath
case:

  -- -------- -- -------
     @xmath      (D.1)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (D.2)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (D.3)
     @xmath      (D.4)
     @xmath      (D.5)
  -- -------- -- -------

The representation matrix of @xmath in the basis of the generalized
Macdonald functions at level 2:

  -- -------- -- -------
     @xmath      (D.6)
     @xmath      (D.7)
  -- -------- -- -------

  -- -------- -- --------
     @xmath      (D.8)
     @xmath      (D.9)
     @xmath      (D.10)
     @xmath      (D.11)
     @xmath      (D.12)
     @xmath      (D.13)
     @xmath      (D.14)
     @xmath      (D.15)
     @xmath      (D.16)
     @xmath      (D.17)
  -- -------- -- --------

Representation Matrix of @xmath in the basis of bosons @xmath at level
2:

  -- -------- -- --------
     @xmath      (D.18)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (D.19)
     @xmath      (D.20)
     @xmath      (D.21)
     @xmath      (D.22)
     @xmath      (D.23)
     @xmath      (D.24)
     @xmath      (D.25)
     @xmath      (D.26)
     @xmath      (D.27)
  -- -------- -- --------

where @xmath .
