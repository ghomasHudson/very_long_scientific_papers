## Part I: Universal Entropic Inference

### Chapter 1 Foundations of Probability Theory

This chapter reviews Caticha’s design derivation of probability theory [
21 ] and reiterates related concepts from [ 44 ] . A formal equivalence
between, the “degree of rational belief” one ought to have that a
proposition is true, and, the “probability” a proposition is true, is
found. Cox derived probability theory as a measure of reasonable
expectation by solving functional equations over Boolean equivalent
propositions in 1946 [ 29 ] . His derivation was appreciated by Jaynes [
22 ] , who instead designed probability as an objective measure of
plausibility. The approach taken by Caticha differs in a few ways from
Cox and Jaynes; the goal is to design a tool for inductive inference
such that objective statements may be made about one’s rational, but
informationally subjective, beliefs. The main differences will be noted
within the derivation below.

#### 1.1 Designing a tool for inference

When faced with incomplete information, complete and precise deduction
may not always be possible. In these instances, we require a tool to
perform inductive reasoning such that trustworthy inferences may be
made. Usual deduction runs a straightforward course, “If @xmath then
@xmath ”, and “If @xmath then @xmath ”, one may deduce: “If @xmath then
@xmath ”. Deduction fails to address less certain situations, such as
“If @xmath suggests @xmath and @xmath suggests @xmath ” then, by what
measure should we infer, “ @xmath suggests @xmath ”? Although
correlations between the propositions @xmath are known to exist, there
is no rule for manipulating these “suggestions” logically. We are
therefore motivated to design a tool capable of giving rational
assessments in situations with limited information. Not knowing if we
should believe a proposition to be true or false, we seek to design
rules for making inferences that maintain a high standard of honesty in
our beliefs, for the purpose of doing science [ 45 ] . We therefore seek
a tool capable of quantifying the degree of rational belief (DoRB) we
ought to hold that a proposition is true. ¹ ¹ 1 The question of whether
degrees of rational belief are scientific is discussed in a comment at
the end of the derivation.

In Cox’s derivation of probability, he let @xmath denote “propositions”
rather than “events”. He states, “speaking of events easily invokes the
notion of sequence in time, and this may become a source of confusion. A
proposition may, of course, assert the occurrence of an event, but it
may just as well assert something else, for example, something about a
physical constant.” [ 29 ] . Propositions are self-contained statements
or assertions that may be true or false – and therefore they may take on
a wide variety of character; for example @xmath could represent “ the
position of a particle is @xmath ”, @xmath “the gene sequence of a
banana is @xmath ”, to anything @xmath one might wish to infer “ @xmath
”.

In everyday situations it is apparent that some propositions are more
believable than others. By design we would like our tool for inference
to quantify the believability of a proposition. If @xmath is more
believable than @xmath and @xmath more believable that @xmath , then we
wish degrees of rational belief (DoRBs) to represent this information,
as well as @xmath being quantifiably more believable than @xmath . We
are therefore inclined to represent DoRBs by real numbers such that they
may be ranked in a transitive manner. It is then by design that a DoRB
should be a function that maps arbitrary propositions @xmath to values
on the real line such that the believability of propositions are ranked.
We will denote this function with square brackets @xmath such that
@xmath represents the DoRB an arbitrary proposition @xmath is true. That
is [ 21 ] ,

Degrees of rational belief (or, as we shall later call them,
probabilities) are represented by real numbers.

When utilizing DoRBs, to be maximally informed [ 45 ] , we should make
inferences only once all of the relevant information has been taken into
account. It is clear that the knowledge of one proposition may support
or oppose another proposition. For instance, if we know @xmath “It is
cloudy today”, then @xmath logically supports the belief in the claim
@xmath “ It will rain today”. To remain honest about our beliefs, if we
know @xmath to be true, we should logically assign a larger DoRB to
@xmath rather than if we had not known @xmath to be true. We introduce a
vertical bar “ @xmath ” between propositions @xmath to denote a new
proposition, which is read: “ @xmath is true given @xmath is true”, “
@xmath given @xmath ”, or “ @xmath conditional on @xmath ”. As @xmath is
a proposition itself, we may assign it a DoRB such that @xmath is read
“the degree of rational belief @xmath is true given that @xmath is true”
or for short “the DoRB of @xmath given @xmath ”.

Conditional propositions allow us to construct the upper and lower limit
a DoRB may take. Because @xmath is true, the DoRB of @xmath must
represent the “most” believable situation for logical and rational
consistency – we denote the value of this DoRB as @xmath . Conversely,
the DoRB that not- @xmath , denoted @xmath , is true given @xmath is
true is @xmath is false such that @xmath represents complete rational
disbelief or the “least” degree of rational belief. The amount of
rational belief we assign to a certainly true proposition @xmath or a
certainly false proposition @xmath is independent of the specific
proposition @xmath . This lets @xmath and @xmath be single, but
distinct, numbers such that transitivity may be obeyed.

Compound propositions may be built by considering Boolean logic
operations between propositions: the “and” conjunction, denoted @xmath ,
constructs propositions of the form @xmath (later denoted as @xmath or
with a comma @xmath ) that are true iff both @xmath and @xmath are true,
as well as the Boolean disjunction “or”, denoted @xmath , that
constructs propositions of the form @xmath that are true when @xmath ,
@xmath , or both are true while being false iff both are false. Because
applying arbitrary conjunctions and disjunctions between propositions
results in a new proposition itself, “ @xmath ”, the set of all
propositions is closed under Boolean operations. This realization is
fundamental in moving forward because anything we learn about the
functional form of @xmath and @xmath leads to us learning more about the
desired functional form of @xmath for arbitrary propositions @xmath .

#### 1.2 Functional form of [@xmath or @xmath]

Consider the DoRB a proposition @xmath is true. For consistency, it must
be that @xmath , which means that finding the functional form of @xmath
will ultimately give insight into the functional form of @xmath .
Because of this, we expect the functional form of @xmath to somehow be
related to the functions: @xmath and @xmath . We let @xmath denote the
functional dependence of a DoRB over a disjunction,

  -- -------- -- -------
     @xmath      (1.1)
  -- -------- -- -------

with that of the constituting disjunct DoRBs. First consider the special
case when @xmath and @xmath are mutually exclusive (i.e. given @xmath is
false and vice versa), then,

  -- -------- -- -------
     @xmath      (1.2)
  -- -------- -- -------

for brevity. The functional form of @xmath can be found by considering
the associative property of three propositions,

  -- -------- -- -------
     @xmath      (1.3)
  -- -------- -- -------

where again we will let @xmath be mutually exclusive propositions. The
DoRB function must arrive at the same result for the logically
equivalent forms of @xmath . This imposes,

  -- -------- -- -------
     @xmath      (1.4)
  -- -------- -- -------

@xmath may be applied two-fold in each instance, first,

  -- -------- -- -------
     @xmath      (1.5)
  -- -------- -- -------

and then again,

  -- -------- -- -------
     @xmath      (1.6)
  -- -------- -- -------

with equality due to the equivalency of the propositions and for self
consistency. This is the well known Associativity Functional Equation [
46 ] ; it has the general solution

  -- -------- -- -------
     @xmath      (1.7)
  -- -------- -- -------

where @xmath is an arbitrary monotonic function and @xmath is a constant
[ 21 , 46 ] . It should be noted that in [ 29 , 22 ] , the Associativity
Functional Equation is used to specify the form of [ @xmath and @xmath ]
rather than [ @xmath or @xmath ], the @xmath function of ours being
related to theirs by its logarithm. This is consistent because the
logarithm of a (positive) monotonic function is monotonic itself (theirs
is a positive exponential). Because @xmath is a monotonic function, it
necessarily preserves transitivity. Without loss of generality we may
take @xmath of both sides

  -- -------- -- -------
     @xmath      (1.8)
  -- -------- -- -------

and regraduate (monotonically rescale) our DoRB function by letting,

  -- -------- -- -------
     @xmath      (1.9)
  -- -------- -- -------

such that the mutually exclusive disjunction rule for DoRBs takes the
convenient form,

  -- -------- -- --------
     @xmath      (1.10)
  -- -------- -- --------

The special case of mutually exclusive propositions can be lifted by
considering the logical identities: @xmath , @xmath , and @xmath .
Because @xmath are propositions themselves we may use equation ( 1.10 )
to find,

  -- -------- -- --------
     @xmath      
     @xmath      (1.11)
  -- -------- -- --------

which means the general sum rule for DoRBs is,

  -- -------- -- --------
     @xmath      (1.12)
  -- -------- -- --------

#### 1.3 Functional form of [@xmath and @xmath]

Consider we wish to find the DoRB of a proposition @xmath . Again this
means that finding the functional form of @xmath will give insight into
the functional form of @xmath . Due to this, we again expect the
functional form of @xmath to be related to the functions: @xmath , which
we express as,

  -- -------- -- --------
     @xmath      (1.13)
  -- -------- -- --------

The function @xmath can be deconstructed from four arguments down to two
arguments by following eliminative induction [ 21 , 47 ] . The
functional dependencies that capture conjunction “and” as it pertains to
the DoRB of @xmath and @xmath depend both on the DoRB of @xmath as well
as our DoRB of @xmath given @xmath is true – @xmath ,

  -- -------- -- --------
     @xmath      (1.14)
  -- -------- -- --------

while all other candidates fail to perform as desired. The functional
form of @xmath can be found by considering the distributive property of
propositions,

  -- -------- -- --------
     @xmath      (1.15)
  -- -------- -- --------

For our inference tool to not be self-refuting, @xmath must arrive at
the same result for the logically equivalent forms of @xmath , which
imposes the following relation,

  -- -------- -- --------
     @xmath      (1.16)
  -- -------- -- --------

Letting @xmath and @xmath be mutually exclusive for simplicity, we have
@xmath such that @xmath , so,

  -- -------- -- --------
     @xmath      (1.17)
  -- -------- -- --------

Letting @xmath , @xmath and @xmath recasts @xmath into a more obvious
form,

  -- -------- -- --------
     @xmath      (1.18)
  -- -------- -- --------

This functional equation is linear in the second argument meaning it has
the general solution [ 46 ] ,

  -- -------- -- --------
     @xmath      (1.19)
  -- -------- -- --------

Because @xmath , if we let @xmath and @xmath we have that @xmath implies
@xmath , so,

  -- -------- -- --------
     @xmath      (1.20)
  -- -------- -- --------

In the special case that the propositions in question are independent,
@xmath and @xmath , one finds the functional form of @xmath must be
@xmath . The general solution for conjunction is therefore proportional
to the simple product of its constituents, @xmath . This means,

  -- -------- -- --------
     @xmath      (1.21)
  -- -------- -- --------

where @xmath is a constant, and the last equality is due to the
commutativity of @xmath ( 1.20 ).

###### Cleaning up the constants:

Lastly we find values for @xmath , @xmath , and @xmath . Because a
proposition or its negation is certainly true, @xmath , we have,

  -- -------- -- --------
     @xmath      (1.22)
  -- -------- -- --------

This equation is satisfied for @xmath or @xmath because by design @xmath
. Because the DoRBs are real numbers by design, @xmath is finite such
that it is not a non-number, like infinity. This then allows DoRBs to be
understood as a fraction of certainty, something that would be
impossible if the DoRB of a true proposition was infinite. The only
reasonable solution to the above equation is therefore @xmath .

We find @xmath by evaluating the DoRB of @xmath and using the product
rule,

  -- -------- -- --------
     @xmath      (1.23)
  -- -------- -- --------

and therefore @xmath because @xmath . The product rule becomes,

  -- -------- -- --------
     @xmath      (1.24)
  -- -------- -- --------

We may further regraduate the DoRB @xmath for convenience such that
@xmath is normalized to unity, and therefore, @xmath for any proposition
@xmath .

#### 1.4 Corollaries and Comments

From now on we will use the standard comma notation between propositions
to denote an “and” conjunction. The sum rule,

  -- -------- -- --------
     @xmath      (1.25)
  -- -------- -- --------

as well as the product rule,

  -- -------- -- --------
     @xmath      (1.26)
  -- -------- -- --------

were found to be the desired strategies designed for manipulating
degrees of rational belief . These rules are formally equivalent to the
sum and product probability rules, and thus, this derivation provides an
interpretation for probability as “a degree of rational belief” wherever
and whenever probability theory is utilized . This is perhaps the first
suggestion that Statistical and Quantum Theory are theories of inference
, that is, they are theories that quantify, rank, and guide one’s
intuition of what one ought to believe is true rather than providing an
ultimate description of nature; however, this is not revealed to be the
case until the maximum entropy method and the Entropic Dynamics
formulation of Quantum Mechanics are derived. Future chapters in this
thesis will provide the foundation for laws of Physics as applications
of inference . Note that at this point it is straightforward to
introduce probability densities @xmath for continuous propositions
@xmath that provide probabilities when taken with the product of their
measure @xmath . When there is no room for misinterpretation I may
simply call @xmath a probability or probability distribution rather than
a probability density. It is interesting to note that when one is
summing @xmath that it is a sum over propositions. In the continuous
case @xmath is, in the strictest interpretation, an integration over
propositions . I believe this interpretation is interesting in the later
chapters pertaining to Entropic Dynamics.

###### Corollaries

If we know @xmath to be true and potentially correlated to @xmath , then
as a matter of self honesty, we should include this information when
considering the probability of @xmath (whenever available). The
probability of @xmath conditional on @xmath , @xmath may be rewritten
using the product rule (where logically @xmath ), as

  -- -------- -- --------
     @xmath      (1.27)
  -- -------- -- --------

This is Bayes Theorem, and therefore it is designed to reflect informed
and honest judgment concerning the DoRB @xmath is true given @xmath is
true. In certain instances when the truth of a proposition is learned,
one is obligated, for the purpose of making informed judgments and self
honesty, to update an old probability @xmath to a new probability,

  -- -------- -- --------
     @xmath      (1.28)
  -- -------- -- --------

from ( 1.27 ). It is thus natural to define information operationally
@xmath as the rationale that causes a probability distribution to change
(inspired by [ 21 ] ), ( 1.28 ) being one example. In another example,
if it is unclear whether @xmath is true or @xmath is true, then one can
use the fact that @xmath is certainly true, and make the following
inference about @xmath ,

  -- -------- -- --------
     @xmath      (1.29)
  -- -------- -- --------

This process is called marginalization , in particular above, @xmath has
been marginalized over and gives a best guess for @xmath . If, later,
the value of @xmath is learned, one may use ( 1.28 ), or revert back to
@xmath if one inquires about the probability of @xmath and @xmath .

The expectation value of an outcome is defined to be such that the
variance about its value is minimal [ 21 ] , and therefore the expected
value is,

  -- -------- -- --------
     @xmath      (1.30)
  -- -------- -- --------

It should be noted that in general @xmath is not within @xmath , and
thus it cannot in general represent an element of reality even if @xmath
do. In this sense, expectation values, like DoRBs, pertain to the
characterization of knowledge about reality, rather than reality itself,
and therefore they are potentially useful epistemic quantities.

A particularly interesting notion in probability theory is information
geometry. There are several derivations of information geometry reviewed
and given in [ 21 ] , one of them in particular is as “a measure of
distinguishably between probability distributions”. The measure of
distinguishably is the invariant interval,

  -- -------- -- --------
     @xmath      (1.31)
  -- -------- -- --------

where @xmath is the @xmath th parameter of @xmath , which label the
family of probability distributions @xmath that populate a statistical
manifold @xmath having coordinates @xmath . In particular, the
information metric @xmath assigns geometry to @xmath , and it is unique
up to an overall scale factor @xmath [ 48 , 49 ] ,

  -- -------- -- --------
     @xmath      (1.32)
  -- -------- -- --------

Entropic Dynamics will take advantage of Information Geometry in the
derivation of Quantum Mechanics in Chapter 4 .

###### Comment: Are rational beliefs scientific?

In short, “Yes” [ 44 ] . In science we have access to data,
observations, and mathematical tools for accurately describing these
data and observations. We are particularly complacent when a
mathematical tool is able to predict new data and observations; however,
if it fails to do so, we either modify it, abandon it, or claim it was
being used in a region outside its operation. The set of mathematical
tools that together form the body of scientific description are rigorous
because they are subject to testing, peer-review, and may be adapted to
reflect new evidence. This practice preserves honesty as well as allows
one to make the most informed predictions. It is therefore clear that
rational practices allow one to form rational beliefs , and certainly,
science aims to discredit the alternative. If the physics of today is
changed tomorrow, our scientific process allows us to adjust accordingly
such that we have the most informed description of nature as possible.
This process may be exactly described by manipulating DoRBs, or as we
find, probability.

###### Comment: Why not simply use Kolmogorov’s Axioms?

The axiomatic derivation of probability given by Kolmogorov in 1933 [ 50
] , is arguably incomplete. In it, probability is defined as a map from
a set @xmath of events @xmath (which may be sets themselves) to the
positive real line: @xmath . The remaining axioms state that probability
is normalized @xmath and that under exclusive addition @xmath . This
axiomatic characterization of probability is not favored on two
accounts.

The first is that it fails to provide an interpretation for probability
. Because at most the interpretation of probability in the Kolmogorov
picture is a map from a set of events to the real line between @xmath
and @xmath , it is isomorphic to other instances represented in this
interval, which may lead to misidentification. I dedicated an article [
6 ] to argue against the notions of a “generalized complex probability”
or “complex quasi-probability”, because they fail to rank propositions
by more or less probable – the whole point of having probability in the
first place. Some have poorly named these instances quasi- probabilities
, because when viewed axiomatically, they appear to simply relax
Kolmgorov’s third axiom that probabilities are real but fail to mention
their inability to rank the DoRB, plausibility, or probability a
proposition is true. I do give an explanation for how probabilities may
be complex and still rank degrees of rational belief, but the complex
representation of actual probabilities is ultimately not particularly
useful other than for showing how quasi-probability distributions might
fail to rank (a miniature no-go theorem of sorts), so the full
development is omitted here.

A second flaw is that the axiomatic characterization lacks an updating
procedure or a reason to update. This flaw may actually be regarded as
an instance of the first flaw, since ultimately we would like to know
what probability is and how to use it. In the Kolmogorov paper [ 50 ] ,
conditional probability (or Bayes Theorem) is defined and then used to
“derive” the product rule, neither of which make reference to the
original axioms in an obvious way – in some sense joint probability
distributions are neglected all together in the axioms. In our
derivation above, probability is a tool designed for the pragmatic
purpose of making inferences; far more desirable for doing science than
the axiomatic approach, which appears to design probability for the
purpose of describing its set theoretic foundations.

#### 1.5 Conclusions and the desire for an updating tool

Probability theory has been derived as the unique tool for manipulating
degrees of rational belief, and thus, the “degree of rational belief one
ought to assign to a proposition being true” is formally equal to “the
probability” of that proposition. A probability distribution is
therefore a statement of rational beliefs, or as it is sometimes called,
a “state of knowledge”. Although we have derived rules for manipulating
probability distributions using the sum and product rules, we have not
given a definitive prescription for assigning numerical values to
individual probability distributions [ 29 ] . This leads to the problem
of priors [ 21 , 29 ] , a problem Jaynes discusses at length [ 51 ] .
Ultimately it is found that the proper assignment of prior distributions
is on the onus of the agent to use the tools of inference correctly –
they must be reasonable and use the information at hand, which may
include, but is not limited to, the symmetries in the problem. Updating
probability distributions has been motivated by practicality as well as
self honesty, but a general, less ad hoc method for updating probability
distributions is desired. In the next Chapter we will design a rigorous
tool for updating probability distributions as well as a tool for
updating density matrices – they are the standard and quantum relative
entropies, respectively.

### Chapter 2 The Shared Inferential Foundation of the Standard and
Quantum Relative Entropies

As is desired in the comments and conclusions of Chapter 1 , we design
an inferential updating procedure for probability distributions and
density matrices such that inductive inferences may be made. This
chapter follows [ 1 ] .

The inferential updating tools found in this derivation take the form of
the standard and quantum relative entropy functionals, and thus we find
that these functionals are designed for the purpose of updating
probability distributions and density matrices, respectively. Previously
formulated design derivations that find the relative entropy to be a
tool for inference originally required five design criteria (DC) [ 31 ,
32 , 33 ] , this was reduced to four in [ 34 , 35 , 36 ] , and then down
to three in [ 30 , 7 ] (reviewed in [ 21 ] ). We reduced the number of
required DC down to two while also providing the first design derivation
of the quantum relative entropy— using the same design criteria and
inferential principles in both instances .

The designed quantum relative entropy takes the form of Umegaki’s
quantum relative entropy, and thus it has the “proper asymptotic form of
the relative entropy in quantum (mechanics)” [ 52 , 53 , 54 ] .
Recently, Wilming et al. [ 55 ] gave an axiomatic characterization of
the quantum relative entropy that “uniquely determines the quantum
relative entropy”. Our derivation differs from their’s, again in that we
design the quantum relative entropy for a purpose, but also that our DCs
are imposed on what turns out to be the functional derivative of the
quantum relative entropy rather than on the quantum relative entropy
itself. The use of a quantum entropy for the purpose of inference has a
large history: Jaynes [ 24 , 22 ] invented the quantum maximum entropy
method [ 10 ] , and this method was found to be useful by [ 56 , 57 , 58
, 59 , 60 , 61 , 62 , 63 ] and many others. However, we find the quantum
relative entropy to be the suitable entropy for updating density
matrices, rather than the von Neuman entropy [ 42 ] , as is suggested in
[ 64 ] . Thus, [ 1 ] gives the desired motivation for why the
appropriate quantum relative entropy for updating density matrices, from
prior to posterior, should be logarithmic in form while also providing a
solution for updating non-uniform prior density matrices [ 64 ] . The
relevant results of these papers may be found using the quantum relative
entropy with suitably chosen prior density matrices.

One of the primary conclusions from [ 31 , 32 ] is that because the
relative entropies were reached by design, they may be interpreted as
such, “the relative entropies are tools for updating”, which means we no
longer need to rely on an interpretation of entropy as a measure of
disorder, an amount of missing information [ 65 ] , or amount of
uncertainty [ 24 ] . This shifts the focus on entropy away from measures
of ‘‘information” or ‘‘uncertainty”, which may be misleading, ¹ ¹ 1 The
following example from [ 21 ] shows that interpreting entropy as an
“amount of missing information” (or uncertainty) can be misleading:
Given someone who normally carries their keys in their pocket, the
probable location of their keys is described by narrow probability
distribution in their pocket. If this person makes a measurement,
something that’s normal utility is to gain information, but finds that
their keys are not in their pocket, then the newly assigned probability
distribution for the location of the keys is widespread. Thus,
counterintuitively, by gaining the knowledge that their keys were
outside of their pocket, they have lost information (as the entropy has
increased). This leads [ 21 ] to define information as that which causes
probability distributions to change (independent of whether or not that
change leads entropy to increase or decrease). and rather toward a
notion that entropies are objective tools for inference (as was utilized
by Jaynes [ 24 , 10 , 22 ] ). In this sense, the relative entropies were
built for the purpose of saturating their own interpretation [ 21 , 34 ]
² ² 2 Although, Skilling only accomplished this for “intensities” rather
than probabilities. , and, therefore, the quantum relative entropy is
the tool designed for updating density matrices .

The remainder of the chapter is organized as follows: first, we will
discuss some universally applicable principles of inference and motivate
the design of an entropy functional able to rank probability
distributions. This entropy functional will be designed such that it is
consistent with inference by applying a few reasonable design criteria,
which are guided by the principle of minimal updating. Using the same
principles of inference and design criteria, we find the form of the
quantum relative entropy suitable for inference. An example of updating
2 @xmath 2 prior density matrices with respect to expectation values
over spin matrices that do not commute with the prior, via the quantum
maximum entropy method, is given in Appendix C .

#### 2.1 The Design of Entropic Inference

Inference is the appropriate updating of probability distributions when
new information is received. Bayes rule and Jeffreys rule are both
equipped to handle information in the form of data; however, the
updating of a probability distribution due to the knowledge of an
expectation value was realized by Jaynes [ 24 , 22 , 10 ] through the
method of maximum entropy. The two methods of inference were thought to
be separate until the work of [ 7 , 8 , 9 ] , which showed Bayes Rule
and Jeffreys Rule are the derived forms of inference using the method of
maximum entropy when the expectation value constraints are in the form
of data. In the spirit of the derivation we will pretend the maximum
entropy method is not known and show how it may be derived as an
application of inference.

Given a probability distribution @xmath over a general set of
propositions @xmath , if new information is learned, we are obligated to
assign a new probability distribution @xmath that somehow reflects this
new information while also respecting our prior probability distribution
@xmath . The main question we must address is: “Given some information,
to what posterior probability distribution @xmath should we update our
prior probability distribution @xmath ?”, that is,

  -- -------- --
     @xmath   
  -- -------- --

This specifies the problem of inductive inference. Since “information”
has many colloquial, and potentially counterintuitive, definitions (as
discussed earlier), we remove potential confusion by defining
information operationally @xmath as the rationale that causes a
probability distribution to change (inspired by and adapted from [ 21 ]
). As probabilities are “degrees of rational belief”, a probability
should only change value if there exists a rationale to do so. Thus,
information can take many forms, for instance, learning the truth of a
proposition or the value of an expectation value, but , obtaining “new”
information also depends on one’s current state of knowledge @xmath [ 21
] .

The motivation for designing entropy is to build a function that allows
for a systematic search of a preferred posterior distribution in the
presence of new information for the purpose of inductive inference [ 21
] . It is best described in [ 21 ] :

  “The central idea, first proposed in [ 34 ] , is disarmingly simple:
  to select the posterior, first rank all candidate distributions in
  increasing order of preference and then pick the distribution that
  ranks the highest. Irrespective of what it is that makes one
  distribution preferable over another (we will get to that soon
  enough), it is clear that any ranking according to preference must be
  transitive: if distribution @xmath is preferred over distribution
  @xmath , and @xmath is preferred over @xmath , then @xmath is
  preferred over @xmath . Such transitive rankings are implemented by
  assigning to each @xmath a real number @xmath , which is called the
  entropy of @xmath , in such a way that if @xmath is preferred over
  @xmath , then @xmath . The selected distribution (one or possibly
  many, for there may be several equally preferred distributions) is
  that which maximizes the entropy functional.”

Because we wish to update from prior distributions @xmath to posterior
distributions @xmath by ranking, the entropy functional @xmath is a real
function of both @xmath and @xmath . In the absence of new information,
there is no available rationale to prefer any @xmath to the original
@xmath , and thereby the relative entropy should be designed such that
the selected posterior is equal to the prior @xmath (in the absence of
new information). The prior information encoded in @xmath is valuable
and we should not change it unless we are informed otherwise. Due to our
definition of information, and our desire for objectivity, we state the
predominate guiding principle for inductive inference:

###### The Principle of Minimal Updating (PMU):

A probability distribution should only be updated to the extent required
by the new information .

This simple statement provides the foundation for inference [ 21 ] . If
the updating of probability distributions is to be done objectively,
then possibilities should not be needlessly ruled out or suppressed.
Being informationally stingy, that we should only update probability
distributions when the information requires it, pushes inductive
inference toward objectivity. Thus, using the PMU helps formulate a
pragmatic (and objective) procedure for making inferences using
(informationally) subjective probability distributions [ 44 ] .

This method of inference is only as universal and general as its ability
to apply equally well to any specific inference problem. The notion of
“specificity” is the notion of independence; a specific case is only
specific in that it is separable from other specific cases. The notion
that systems may be “sufficiently independent” plays a central and
deep-seated role in science and the idea that some things can be
neglected and that not everything matters, is implemented by imposing
criteria that tells us how to handle statistically independent systems
and independent domains within a single system [ 21 ] . Ironically, the
universally shared property by all specific inference problems is their
ability to be independent of one another—they share independence. Thus,
a universal inference scheme based on the PMU permits:

###### Properties of Independence (PI):

Subdomain Independence: When information is received about one set of
propositions, it should not affect or change the state of knowledge
(probability distribution) of the other propositions (else information
was also received about them too);

  -- -- --
        
  -- -- --

Subsystem Independence: When two systems are a priori believed to be
independent and we only receive information about one, then the state of
knowledge of the other system remains unchanged.

The PIs are special cases of the PMU that describe situations when
someone should not update pieces of their probability distribution. The
PIs are ultimately implemented through design criteria in this design
derivation of the entropy @xmath . The process of constraining the form
of @xmath by imposing design criteria may be viewed as a process of
eliminative induction from which a single form for the entropy remains.
Thus, the justification behind the surviving entropy is not that it
leads to demonstrably correct inferences, but, rather, that all other
candidate entropies demonstrably fail to perform as desired [ 21 ] .
Rather than the design criteria instructing one how to update, they
instruct in what instances one should not update. That is, rather than
justifying one way to skin a cat over another, we tell you when not to
skin it, which is operationally unique—namely you don’t do it—luckily
enough for the cat.

##### The Design Criteria and the Standard Relative Entropy

The following design criteria (DC), guided by the PMU, are imposed and
one finds that the standard relative entropy is a tool designed for
inductive inference. The form of this presentation is inspired by [ 21 ]
.

###### DC1: Subdomain Independence

We keep DC1 from [ 7 , 66 , 21 ] and follow it below. DC1 imposes the
first instance of when one should not update—the Subdomain PI. Suppose
the information to be processed does not refer to a particular subdomain
@xmath of the space @xmath of @xmath ’s. In the absence of new
information about @xmath , the PMU insists we do not change our minds
about probabilities that are conditional on @xmath . Thus, we design the
inference method so that @xmath , the prior probability of @xmath
conditional on @xmath , is not updated and therefore the selected
conditional posterior is

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

(The notation will be as follows: we denote priors by @xmath , candidate
posteriors by lower case @xmath , and the selected posterior by upper
case @xmath .) Caticha emphasizes that the point is not that we make the
unwarranted assumption that keeping @xmath unchanged is guaranteed to
lead to correct inferences. It need not; induction is risky. The point
is, rather, that, in the absence of any evidence to the contrary, there
is no reason to change our minds and the prior information takes
priority. [ 21 ]

###### DC1 Implementation

Consider the set of microstates @xmath belonging to either of two
non-overlapping domains @xmath or its complement @xmath , such that
@xmath and @xmath . For convenience, let @xmath . Consider the following
constraints:

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

such that @xmath (however this will not be imposed directly with a
Lagrange multiplier as it would lead to inter domain dependencies), and
the following “local” expectation value constraints over @xmath and
@xmath ,

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

where @xmath is a scalar function of @xmath and @xmath . As we are
searching for the candidate distribution which maximizes @xmath while
obeying ( 2.2 ) and ( 2.3 ), we maximize the entropy @xmath with respect
to these expectation value constraints using the Lagrange multiplier
method,

  -- -------- --
     @xmath   
              
  -- -------- --

and, thus, the entropy is maximized when the following differential
relationships hold:

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (2.4)
     @xmath   @xmath   @xmath      (2.5)
  -- -------- -------- -------- -- -------

Equations ( 2.2 )–( 2.5 ), are @xmath equations we must solve to find
the four Lagrange multipliers @xmath and the @xmath probability values
@xmath associated to the @xmath microstates @xmath .

If the subdomain constraint DC1 is imposed in the most restrictive case,
then it will hold in general. The most restrictive case requires
splitting @xmath into a set of domains @xmath such that each @xmath
singularly includes one microstate @xmath . This gives,

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

Because the entropy @xmath is a functional over the probability of each
microstate’s posterior and prior distribution, its variational
derivative is also a function of said probabilities in general,

  -- -------- -- -------
     @xmath      (2.7)
  -- -------- -- -------

DC1 is imposed by constraining the form of @xmath to ensure that changes
in @xmath have no influence over the value of @xmath in domain @xmath ,
through @xmath , for @xmath . If there is no new information about
propositions in @xmath , its distribution should remain equal to @xmath
by the PMU. We further restrict @xmath such that an arbitrary variation
of @xmath (a change in the prior state of knowledge of the microstate
@xmath ) has no effect on @xmath for @xmath and therefore DC1 imposes
@xmath , as is guided by the PMU. At this point, it is easy to
generalize the analysis to continuous microstates such that the indices
become continuous @xmath , sums become integrals, and discrete
probabilities become probability densities @xmath .

###### Remark

We are designing the entropy for the purpose of ranking posterior
probability distributions (for the purpose of inference); however, the
highest ranked distribution is found by setting the variational
derivative of @xmath equal to the variations of the expectation value
constraints by the Lagrange multiplier method,

  -- -------- -- -------
     @xmath      (2.8)
  -- -------- -- -------

Therefore, the real quantity of interest is @xmath rather than the
specific form of @xmath . All forms of @xmath that give the correct form
of @xmath are equally valid for the purpose of inference. Thus, every
design criteria may be made on the variational derivative of the entropy
rather than the entropy itself, which we do. When maximizing the
entropy, for convenience, we will let,

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

and further use the shorthand @xmath , in all cases.

###### Dc1’:

In the absence of new information, our new state of knowledge @xmath is
equal to the old state of knowledge @xmath .

This is a special case of DC1, which is implemented differently in [ 21
] . The PMU is in principle a statement about informational
honestly—that is, one should not “jump to conclusions” in light of new
information, and in the absence of new information, one should not
change their state of knowledge. If no new information is given, the
prior probability distribution @xmath does not change, that is, the
posterior probability distribution @xmath is equal to the prior
probability. If we maximize the entropy without applying constraints,

  -- -------- -- --------
     @xmath      (2.10)
  -- -------- -- --------

then DC1’ imposes the following condition on @xmath :

  -- -------- -- --------
     @xmath      (2.11)
  -- -------- -- --------

for all @xmath in this case. This special case of the DC1 and the PMU
turns out to be incredibly constraining as we will see over the course
of DC2.

###### Comment

If the variable @xmath is continuous, DC1 requires that when information
refers to points infinitely close but just outside the domain @xmath ,
that it will have no influence on probabilities conditional on @xmath .
This may seem surprising as it may lead to updated probability
distributions that are discontinuous. Is this a problem? No. [ 21 ]

In certain situations (e.g., physics) we might have explicit reasons to
believe that conditions of continuity or differentiability should be
imposed and this information might be given to us in a variety of ways.
The crucial point, however – and this is a point that we keep and will
keep reiterating – is that unless such information is explicitly given,
we should not assume it. If the new information leads to
discontinuities, so be it. [ 21 ]

###### DC2: Subsystem Independence

DC2 imposes the second instance of when one should not update – the
Subsystem PI. We emphasize that DC2 is not a consistency requirement .
The argument we deploy is not that both the prior and the new
information tells us the systems are independent, in which case
consistency requires that it should not matter whether the systems are
treated jointly or separately. Rather, DC2 refers to a situation where
the new information does not say whether the systems are independent or
not, but information is given about each subsystem. The updating is
being designed so that the independence reflected in the prior is
maintained in the posterior by default via the PMU and the second clause
of the PIs. [ 21 ]

The point is not that when we have no evidence for correlations we draw
the firm conclusion that the systems must necessarily be independent.
They could indeed have turned out to be correlated and then our
inferences would be wrong. Again, induction involves risk. The point is
rather that if the joint prior reflects independence and the new
evidence is silent on the matter of correlations, then the prior
independence takes precedence. As before, in this case subdomain
independence, the probability distribution should not be updated unless
the information requires it. [ 21 ]

###### DC2 Implementation

Consider a composite system, @xmath . Assume that all prior evidence led
us to believe the subsystems are independent. This belief is reflected
in the prior distribution: if the individual system priors are @xmath
and @xmath , then the prior for the whole system is their product @xmath
. Further suppose that new information is acquired such that @xmath
would by itself be updated to @xmath and that @xmath would itself be
updated to @xmath . By design, the implementation of DC2 constrains the
entropy functional such that, in this case, the joint product prior
@xmath updates to the selected product posterior @xmath [ 21 ] .

The argument below is considerably simplified if we expand the space of
probabilities to include distributions that are not necessarily
normalized. This does not represent any limitation because a
normalization constraint can always be applied. We consider a few
special cases below:

Case 1: We receive the extremely constraining information that the
posterior distribution for system @xmath is completely specified to be
@xmath while we receive no information at all about system @xmath . We
treat the two systems jointly. Maximize the joint entropy @xmath subject
to the following constraints on the @xmath :

  -- -------- -- --------
     @xmath      (2.12)
  -- -------- -- --------

Notice that the probability of each @xmath within @xmath is being
constrained to @xmath in the marginal. We therefore need one Lagrange
multiplier @xmath for each @xmath to tie each value of @xmath to @xmath
. Maximizing the entropy with respect to this constraint is,

  -- -------- -- --------
     @xmath      (2.13)
  -- -------- -- --------

which requires that

  -- -------- -- --------
     @xmath      (2.14)
  -- -------- -- --------

for arbitrary variations of @xmath . By design, DC2 is implemented by
requiring @xmath in this case, therefore,

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

This equation must hold for all choices of @xmath and all choices of the
prior @xmath as @xmath is independent of @xmath . Suppose we had chosen
a different prior @xmath that disagrees with @xmath . For all @xmath and
@xmath , the multiplier @xmath remains unchanged as it constrains the
independent @xmath . This means that any dependence that the right-hand
side might potentially have had on @xmath and on the prior @xmath must
cancel out . This means that

  -- -------- -- --------
     @xmath      (2.16)
  -- -------- -- --------

Since any value of @xmath gives the same @xmath , we may choose a
convenient constant prior set equal to one, @xmath , therefore

  -- -------- -- --------
     @xmath      (2.17)
  -- -------- -- --------

in general. This gives

  -- -------- -- --------
     @xmath      (2.18)
  -- -------- -- --------

The left-hand side above does not depend on @xmath , and therefore
neither does the right-hand side. An argument exchanging systems @xmath
and @xmath gives a similar result.

Case 1—Conclusion: When system 2 is not updated the dependence on @xmath
and @xmath drops out,

  -- -------- -- --------
     @xmath      (2.19)
  -- -------- -- --------

and vice-versa when system 1 is not updated,

  -- -------- -- --------
     @xmath      (2.20)
  -- -------- -- --------

As we seek the general functional form of @xmath , and because the
@xmath dependence drops out of ( 2.19 ) and the @xmath dependence drops
out of ( 2.20 ) for arbitrary @xmath and @xmath , the explicit
coordinate dependence in @xmath consequently drops out of both such
that,

  -- -------- -- --------
     @xmath      (2.21)
  -- -------- -- --------

Thus, @xmath must only depend on coordinates through the probability
distributions themselves.

Case 2: Now consider a different special case in which the marginal
posterior distributions for systems @xmath and @xmath are both
completely specified to be @xmath and @xmath , respectively. Maximize
the joint entropy @xmath subject to the following constraints on the
@xmath ,

  -- -------- -- --------
     @xmath      (2.22)
  -- -------- -- --------

Again, this is one constraint for each value of @xmath and one
constraint for each value of @xmath , which, therefore, require the
separate Lagrange multipliers @xmath and @xmath . Maximizing @xmath with
respect to these constraints is then,

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.23)
                                @xmath   
  -- -------- -------- -------- -------- --------

leading to

  -- -------- -- --------
     @xmath      (2.24)
  -- -------- -- --------

The updating is being designed so that @xmath , as the independent
subsystems are being updated based on expectation values that are silent
about correlations. DC2 thus imposes,

  -- -------- -- --------
     @xmath      (2.25)
  -- -------- -- --------

Write ( 2.25 ) as,

  -- -------- -- --------
     @xmath      (2.26)
  -- -------- -- --------

The left-hand side is independent of @xmath so we can perform a
technique similar to before. Suppose we had chosen a different
constraint @xmath that differs from @xmath and a new prior @xmath that
differs from @xmath except at the point @xmath . At the value @xmath ,
the multiplier @xmath remains unchanged for all @xmath , @xmath , and
thus @xmath . This means that any dependence that the right-hand side
might potentially have had on @xmath and on the choice of @xmath ,
@xmath must cancel out, leaving @xmath unchanged. That is, the Lagrange
multiplier @xmath cancels out these dependences such that

  -- -------- -- --------
     @xmath      (2.27)
  -- -------- -- --------

Because @xmath is independent of arbitrary variations of @xmath and
@xmath on the left hand side (LHS) above—it is satisfied equally well
for all choices. The form of @xmath is apparent if @xmath as @xmath
similar to Case 1 as well as DC1’. Therefore, the Lagrange multiplier is

  -- -------- -- --------
     @xmath      (2.28)
  -- -------- -- --------

A similar analysis carried out for @xmath leads to

  -- -------- -- --------
     @xmath      (2.29)
  -- -------- -- --------

Case 2—Conclusion: Substituting back into ( 2.25 ) gives us a functional
equation for @xmath ,

  -- -------- -- --------
     @xmath      (2.30)
  -- -------- -- --------

The general solution for this functional equation is derived in the
Appendix A.3 , and is

  -- -------- -- --------
     @xmath      (2.31)
  -- -------- -- --------

where @xmath are constants. The constants are fixed by using DC1’.
Letting @xmath gives @xmath by DC1’, and, therefore,

  -- -------- -- --------
     @xmath      (2.32)
  -- -------- -- --------

so we are forced to conclude @xmath for arbitrary @xmath . Letting
@xmath such that we are really maximizing the entropy (although this is
purely convention) gives the general form of @xmath to be

  -- -------- -- --------
     @xmath      (2.33)
  -- -------- -- --------

As long as @xmath , the value of @xmath is arbitrary. The general form
of the entropy designed for the purpose of inference of @xmath is found
by integrating @xmath , and, therefore,

  -- -------- -- --------
     @xmath      (2.34)
  -- -------- -- --------

The constant in @xmath , @xmath , will always drop out when varying
@xmath . The apparent extra term ( @xmath ) from integration cannot be
dropped while simultaneously satisfying DC1’, which requires @xmath in
the absence of constraints or when there is no change to one’s
information. In previous versions where the integration term ( @xmath )
is dropped, one obtains solutions like @xmath (independent of whether
@xmath was previously normalized or not) in the absence of new
information. Obviously, this factor can be taken care of by
normalization, and, in this way, both forms of the entropy are equally
valid; however, this form of the entropy better adheres to the PMU
through DC1’ when normalization is not applied. Given that we may
regularly impose normalization, we may drop the extra @xmath term and
@xmath . For convenience then, ( 2.34 ) becomes

  -- -------- -- --------
     @xmath      (2.35)
  -- -------- -- --------

Given that normalization is applied regularly, the same selected
posterior @xmath maximizes both @xmath and @xmath , so in future use,
the star notation will be dropped.

###### Remarks

It can be seen that the relative entropy is invariant under coordinate
transformations. This implies that a system of coordinates carry no
information, which is to say that changing coordinates is simply a
change in the label of the proposition, while the set of propositions
ultimately stay the same [ 21 ] .

The general solution to the maximum entropy procedure with respect to
@xmath linear constraints in @xmath , @xmath , and normalization gives a
canonical-like selected posterior probability distribution,

  -- -------- -- --------
     @xmath      (2.36)
  -- -------- -- --------

The positive constant @xmath may always be absorbed into the Lagrange
multipliers so we may let it equal unity without loss of generality.
DC1’ is fully realized when we maximize with respect to a set of
constraints on @xmath that are already satisfied by @xmath . If all of
the constraints are already held by @xmath , their corresponding
Lagrange multipliers are forcibly zero (as can be seen in ( 2.36 ) using
( 2.34 )), in agreement with Jaynes, as the expectation values @xmath
are monotonic in their Lagrange multipliers and are satisfied when set
equal to zero. This gives the expected result @xmath as there is no new
information. Our design has arrived at a refined maximum entropy method
[ 24 ] , whose universality can be seen by following [ 7 , 8 ] . We will
follow [ 7 , 8 ] using density matrices in the next chapter.

#### 2.2 The Design of the Quantum Relative Entropy

In the last section, we assumed that the universe of discourse (the set
of relevant propositions or microstates) @xmath was known. In quantum
physics, things are a bit more ambiguous because many probability
distributions, or many experiments, can be associated with a given
density matrix. As any probability distribution from a given density
matrix, @xmath , ³ ³ 3 @xmath is the trace. may be ranked using the
standard relative entropy, it is unclear why we would choose one
universe of discourse over another. In lieu of this, so that one
universe of discourse is not given preferential treatment, we consider
ranking entire density matrices against one another. Probability
distributions of interest may be found from the selected posterior
density matrix. This moves our universe of discourse from sets of
propositions @xmath to Hilbert  space(s).

When the objects of study are quantum systems, we desire an objective
procedure to update from a prior density matrix @xmath to a posterior
density matrix @xmath . We will apply the same intuition used for
ranking probability distributions (Section 2.1 ) and implement the PMU,
PI, and design criteria to the ranking of density matrices.

##### 2.2.1 Designing the Quantum Relative Entropy

In this section, we design the quantum relative entropy using the same
inferentially guided design criteria as were used in the standard
relative entropy.

###### DC1: Subdomain Independence

The goal is to design a function @xmath that is able to rank density
matrices. This insists that @xmath be a real scalar valued function of
the posterior @xmath , and prior @xmath density matrices, which we will
call the quantum relative entropy or simply the entropy in this section.
An arbitrary variation of the entropy with respect to @xmath is,

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.37)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where @xmath is the transpose of @xmath . We wish to maximize this
entropy with respect to expectation value constraints, such as @xmath on
@xmath . Using the Lagrange multiplier method to maximize the entropy
with respect to @xmath and normalization, is setting the variation equal
to zero,

  -- -------- -- --------
     @xmath      (2.38)
  -- -------- -- --------

where @xmath and @xmath are the Lagrange multipliers for the respective
constraints. Because @xmath is a real number, we require @xmath to be
real, and because @xmath is Hermitian, @xmath and @xmath are Hermitian.
Furthermore, because expectation values are real, @xmath ’s are
Hermitian and their corresponding Lagrange multipliers are real too.
Arbitrary variations of @xmath give,

  -- -- -- --------
           (2.39)
  -- -- -- --------

where @xmath is the identity matrix. For these arbitrary variations, the
variational derivative of @xmath satisfies,

  -- -------- -- --------
     @xmath      (2.40)
  -- -------- -- --------

at the maximum; however, forcing @xmath to be Hermitian gives the same
result (after a bit more algebra) as is demonstrated in Appendix B . As
in the remark earlier, all forms of @xmath that give the correct form of
@xmath under variation are equally valid for the purpose of inference.
For notational convenience, we let

  -- -------- -- --------
     @xmath      (2.41)
  -- -------- -- --------

which is a matrix valued function of the posterior and prior density
matrices. The form of @xmath is already “local” in @xmath (the
variational derivative is with respect to the whole density matrix and
the RHS is a function of the density matrices), so we don’t need to
constrain it further as we did in the original DC1.

###### Dc1’:

In the absence of new information, the new state @xmath is equal to the
old state @xmath .

Applied to the ranking of density matrices, in the absence of new
information, the density matrix @xmath should not change, that is, the
posterior density matrix @xmath is equal to the prior density matrix.
Maximizing the entropy without applying any constraints gives,

  -- -------- -- --------
     @xmath      (2.42)
  -- -------- -- --------

and, therefore, DC1’ imposes the following condition in this case:

  -- -------- -- --------
     @xmath      (2.43)
  -- -------- -- --------

As in the original DC1’, if @xmath is known to obey some expectation
value @xmath , and then if one goes out of their way to constrain @xmath
to that expectation value and nothing else, it follows from the PMU that
@xmath , as no information has been gained. This is not imposed directly
but can be verified later due to the monotonocity of expectation values
in their corresponding Lagrange multipliers.

###### DC2: Subsystem Independence

The discussion of DC2 is the same as the standard relative entropy DC2 –
it is not a consistency requirement, and the updating is designed so
that the independence reflected in the prior is maintained in the
posterior by default via the PMU when the information provided is silent
about correlations.

###### DC2 Implementation

Consider a composite system living in the Hilbert space @xmath . Assume
that all prior evidence led us to believe the systems were independent.
This is reflected in the prior density matrix: if the individual system
priors are @xmath and @xmath , then the joint prior for the whole system
is @xmath . Further suppose that new information is acquired such that
@xmath would itself be updated to @xmath and that @xmath would be itself
be updated to @xmath . By design, the implementation of DC2 constrains
the entropy functional such that in this case, the joint product prior
density matrix @xmath updates to the product posterior @xmath so that
inferences about one do not affect inferences about the other.

The argument below is considerably simplified if we expand the space of
density matrices to include density matrices that are not necessarily
normalized. This does not represent any limitation because normalization
can always be imposed as one additional constraint. We consider a few
special cases below:

Case 1: We receive the extremely constraining information that the
posterior distribution for system @xmath is completely specified to be
@xmath while we receive no information about system @xmath . We treat
the two systems jointly. Maximize the joint entropy @xmath , subject to
the following constraints on the @xmath ,

  -- -------- -- --------
     @xmath      (2.44)
  -- -------- -- --------

where @xmath is the partial trace over the vectors in @xmath . Notice
that all of the @xmath elements in @xmath of @xmath are being
constrained. We therefore need a Lagrange multiplier which spans @xmath
and has @xmath components – it is a square Lagrange multiplier matrix
@xmath . This is readily seen by observing the component form
expressions of the Lagrange multipliers @xmath . Maximizing the entropy
with respect to this @xmath independent constraint is

  -- -------- -- --------
     @xmath      (2.45)
  -- -------- -- --------

but reexpressing this with its transpose @xmath , gives

  -- -------- -- --------
     @xmath      (2.46)
  -- -------- -- --------

where we have relabeled @xmath , for convenience, as the name of the
Lagrange multipliers are arbitrary. This variation is,

  -- -------- -- --------
     @xmath      (2.47)
  -- -------- -- --------

where @xmath denotes, equally well, the tensor, direct, or Kronecker
product. Similar to Appendix B , but without assuming @xmath is
Hermitian, one finds,

  -- -------- -- --------
     @xmath      (2.48)
  -- -------- -- --------

and therefore one concludes @xmath must be Hermitian because the RHS is
Hermitian. DC2 is implemented by requiring @xmath , such that the
function @xmath is designed to reflect subsystem independence in this
case; therefore, we have

  -- -------- -- --------
     @xmath      (2.49)
  -- -------- -- --------

Had we chosen a different prior @xmath , for all @xmath the LHS @xmath
remains unchanged given that @xmath is independent of scalar functions
of @xmath , e.g., @xmath . These scalar functions could be
simultaniously lumped into @xmath and @xmath and keep @xmath fixed. The
potential dependence on scalar functions of @xmath can be removed by
imposing DC2 in a subsystem independent situation where @xmath in @xmath
need not be fixed under variations of @xmath , and then use DC2 to
impose that it is fixed. ⁴ ⁴ 4 The resulting equation from such a
situation, when for instance maximizing the entropy of an independent
joint prior while imposing a constraint in @xmath , @xmath , facilitated
by a scalar Lagrange multiplier @xmath , is,

@xmath

after imposing DC2. For subsystem independence to be imposed here,
@xmath must be independent of variations in @xmath , and, therefore, in
a general subsystem independent case, @xmath is independent of scalar
functions of @xmath . This means that any dependence that the right-hand
side of ( 2.49 ) might potentially have had on @xmath must drop out ,
meaning,

  -- -------- -- --------
     @xmath      (2.50)
  -- -------- -- --------

Since @xmath is arbitrary, suppose further that we choose a unit prior,
@xmath , and note that @xmath , @xmath , and @xmath are block diagonal
in @xmath . ⁵ ⁵ 5 By being “block diagonal in @xmath ” we mean that:

(2.51)

This gives

  -- -------- -- --------
     @xmath      (2.52)
  -- -------- -- --------

Because the LHS of the above equation is block diagonal in @xmath , the
RHS is block diagonal in @xmath and, because the function @xmath is
understood to be a power series expansion in its arguments,

  -- -------- -- --------
     @xmath      (2.53)
  -- -------- -- --------

This gives

  -- -------- -- --------
     @xmath      (2.54)
  -- -------- -- --------

and, therefore, the @xmath factors out and @xmath . A similar argument
exchanging systems @xmath and @xmath shows @xmath .

Case 1—Conclusion: The analysis leads us to conclude that when the
system 2 is not updated, the dependence on @xmath drops out,

  -- -------- -- --------
     @xmath      (2.55)
  -- -------- -- --------

and, similarly,

  -- -------- -- --------
     @xmath      (2.56)
  -- -------- -- --------

Case 2: Now consider a different special case in which the marginal
posterior density matrices for systems @xmath and @xmath are both
completely specified to be @xmath and @xmath , respectively. Maximize
the joint entropy, @xmath , subject to the following constraints on the
@xmath ,

  -- -------- -- --------
     @xmath      (2.57)
  -- -------- -- --------

Here, each expectation value constrains the entire space @xmath , where
@xmath lives. The Lagrange multipliers must span their respective
spaces, so we implement the constraint with the Lagrange multiplier
operators @xmath , and,

  -- -------- -- --------
     @xmath      (2.58)
  -- -------- -- --------

For arbitrary variations of @xmath , we have

  -- -------- -- --------
     @xmath      (2.59)
  -- -------- -- --------

By design, DC2 is implemented by requiring @xmath in this case;
therefore, we have

  -- -------- -- --------
     @xmath      (2.60)
  -- -------- -- --------

Write ( 2.60 ) as

  -- -------- -- --------
     @xmath      (2.61)
  -- -------- -- --------

The LHS is independent of changes that might occur in @xmath on the RHS
of ( 2.61 ). This means that any variation of @xmath and @xmath must be
canceled out by @xmath – it removes the dependence of @xmath and @xmath
in @xmath . Therefore, any dependence that the RHS might potentially
have had on @xmath , @xmath must drop out in a general subsystem
independent case, leaving @xmath unchanged. Consequently,

  -- -------- -- --------
     @xmath      (2.62)
  -- -------- -- --------

Because @xmath is independent of arbitrary variations of @xmath and
@xmath on the LHS above—it is satisfied equally well for all choices.
The form of @xmath reduces to the form of @xmath from Case 1 when @xmath
and, similarly, DC1’ gives @xmath . Therefore, the Lagrange multiplier
is

  -- -------- -- --------
     @xmath      (2.63)
  -- -------- -- --------

A similar analysis is carried out for @xmath leading to

  -- -------- -- --------
     @xmath      (2.64)
  -- -------- -- --------

Case 2—Conclusion: Substituting back into ( 2.60 ) gives us a functional
equation for @xmath ,

  -- -------- -- --------
     @xmath      (2.65)
  -- -------- -- --------

which is

  -- -------- -- --------
     @xmath      (2.66)
  -- -------- -- --------

The general solution to this matrix valued functional equation is
derived in Appendix A.5 and is

  -- -------- -- --------
     @xmath      (2.67)
  -- -------- -- --------

where tilde @xmath is a “super-operator” having constant coefficients
and twice the number of indicies as @xmath and @xmath as discussed in
the Appendix (i.e., @xmath and similarly for @xmath ). DC1’ imposes

  -- -------- -- --------
     @xmath      (2.68)
  -- -------- -- --------

which is satisfied in general when @xmath , and, now,

  -- -------- -- --------
     @xmath      (2.69)
  -- -------- -- --------

Recall that the RHS of ( 2.65 ) is equal to the RHS of ( 2.66 ), which
is stated here:

  -- -------- -- --------
     @xmath      (2.70)
  -- -------- -- --------

We may fix the constant @xmath by substituting ( 2.69 ) into both sides
of the above equation. This gives,

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- -- --------
     @xmath      (2.71)
  -- -------- -- --------

where @xmath acts on the joint space of @xmath and @xmath and @xmath ,
@xmath acts on single subspaces @xmath , @xmath , respectively. Using
the log tensor product identity, @xmath , which can be seen by series
expansion, the RHS of ( 2.71 ) becomes

  -- -------- -- --------
     @xmath      (2.72)
  -- -------- -- --------

Note that arbitrarily letting @xmath in ( 2.71 ), ( 2.72 ) gives

  -- -------- -- --------
     @xmath      (2.73)
  -- -------- -- --------

or arbitrarily letting @xmath gives

  -- -------- -- --------
     @xmath      (2.74)
  -- -------- -- --------

As @xmath , @xmath , and @xmath are constant tensors, inspecting the
above equalities determines the form of the tensor to be @xmath where
@xmath is a scalar constant and @xmath is the super-operator identity
over the appropriate (joint) Hilbert space.

Because our goal is to maximize the entropy function, we let the
arbitrary constant @xmath and distribute @xmath identically, which gives
the final functional form,

  -- -------- -- --------
     @xmath      (2.75)
  -- -------- -- --------

“Integrating” @xmath gives a general form for the quantum relative
entropy,

  -- -------- -- --------
     @xmath      (2.76)
  -- -------- -- --------

where @xmath is Umegaki’s form of the relative entropy [ 67 , 68 , 69 ]
, the extra @xmath from integration is an artifact present for the
preservation of DC1’, and @xmath is a constant in the sense that it
drops out under arbitrary variations of @xmath . This entropy leads to
the same inferences as Umegaki’s form of the entropy with an added bonus
that @xmath in the absence of constraints or changes in
information—rather than @xmath which would be given by maximizing
Umegaki’s form of the entropy. In this sense, the extra @xmath better
adheres the quantum relative entropy to the PMU (DC1’), in cases when
normalization is not applied. In the spirit of this derivation, we will
keep the @xmath term there, but, for all practical purposes of
inference, as long as there is a normalization constraint, it plays no
role, and we find (letting @xmath and @xmath ),

  -- -------- -- --------
     @xmath      (2.77)
  -- -------- -- --------

which is Umegaki’s form of the quantum relative entropy. @xmath is an
equally valid entropy because, given normalization is applied, the same
selected posterior @xmath maximizes both @xmath and @xmath . The tool
for inferentially updating density matrices was designed from the first
principles of inference, as was suggested to be possible in private
communications with Caticha [ 45 ] .

##### 2.2.2 Remarks

Due to the universality and the equal application of the PMU (the same
design criteria for both the standard and quantum case), the quantum
relative entropy reduces to the standard relative entropy when @xmath or
when the experiment being performed @xmath is known. Because the two
entropies are derived in parallel, we expect the well-known inferential
results and consequences of the relative entropy to have a quantum
relative entropy analog, and this is indeed what we see in the next
chapter.

Maximizing the quantum relative entropy with respect to some constraints
@xmath , where @xmath are a set of arbitrary Hermitian operators, and
normalization @xmath , gives the following general solution for the
posterior density matrix:

  -- -------- -- --------
     @xmath      (2.78)
  -- -------- -- --------

where @xmath are the Lagrange multipliers of the respective constraints
and normalization @xmath may be factored out of the exponential in
general because the identity matrix commutes universally. If @xmath , it
is well known that the analysis arrives at the same expression for
@xmath after normalization, as it would if the von Neumann entropy were
used, and thus one can find expressions for thermalized quantum states
@xmath ( @xmath is the inverse temperature and @xmath is the Hamiltonian
operator). The remaining problem is to solve for the @xmath Lagrange
multipliers using their @xmath associated expectation value constraints.
In principle, their solution is found by computing @xmath , which is a
sum over the eigenvalues of @xmath , and then using standard methods,

  -- -------- -- --------
     @xmath      (2.79)
  -- -------- -- --------

and inverting to find @xmath , which has a unique solution due to the
joint concavity (convexity depending on the sign convention) of the
quantum relative entropy [ 52 , 53 ] when the constraints are linear in
@xmath . The simple proof that ( 2.79 ) is monotonic in @xmath , and
therefore that it is invertible, is that its derivative @xmath . Between
the Zassenhaus formula [ 70 ] ,

  -- -------- -- --------
     @xmath      (2.80)
  -- -------- -- --------

and Horn’s inequality [ 71 , 72 , 73 ] , the solutions to ( 2.79 ) lack
a certain calculational elegance because it is difficult to express the
eigenvalues of @xmath (in the exponential) in simple terms of the
eigenvalues of the @xmath ’s and @xmath , in general, when the matrices
do not commute. A pedagogical exercise is, starting with a prior that is
a mixture of spin-z up and down @xmath ( @xmath ), to maximize the
quantum relative entropy with respect to an expectation of a general
Hermitian operator with which the prior density matrix does not commute.
This 2 by 2 spin example is given in the Appendix C .

### Chapter 3 Inferential Applications of the Quantum Relative Entropy

This chapter follows [ 2 ] , which develops applications of the quantum
relative entropy (QRE) to describe quantum measurement within the
standard formalism of Quantum Mechanics (QM). The main advantage of our
technique is the conceptual clarity it achieves by reformulating quantum
measurement from the point of view of inference.

In QM the wavefunction has two modes of evolution [ 42 , 40 ] : one is
the continuous unitary evolution given by the dynamical Schrödinger
equation, while the other is the discrete collapse of the wavefunction
that occurs when a detection is made. The collapse postulate is
generally implemented ad hoc to empirically represent the affect of
detection on a quantum system.

In a von Neumann measurement scheme, the goal is to make measurements on
a pure state of interest @xmath (in the Hilbert space @xmath ). This is
accomplished by first entangling @xmath with a pointer variable state
@xmath (in the Hilbert space @xmath ) via a unitary time evolution, and
then by making detections of @xmath , to “measure” @xmath . The
entangled states in a von Neumann measurement scheme take the form of a
biorthogonal state or Schmidt decomposition, @xmath , such that the
probability of @xmath and @xmath is @xmath . A detection of @xmath
“collapses the wavefunction”, which is implemented via the ad hoc change
of state @xmath . Thus, by preparing the system of interest as an
entangled state @xmath , one can “measure” the @xmath ’s by making
detections of @xmath ’s.

A positive operator-valued measure (POVM) measurement [ 74 , 75 , 76 ,
77 ] can be thought of as a generalization of a von Neumann measurement
for the purpose of measuring pure or mixed states. The density matrix of
interest @xmath , which may be a pure or mixed state, is entangled with
a pointer variable @xmath , which is detected for the purpose of
“measuring” the state in @xmath . If the pointer variable was detected
at @xmath , the resulting density matrix,

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

quantifies the collapse of @xmath under a POVM measurement. A POVM is a
set of positive operators @xmath that are labeled by the pointer
variable values @xmath . POVMs sum to identity @xmath and are commonly
decomposed @xmath into the operators @xmath (and its adjoint @xmath ),
which are called the ‘‘measurement” or ‘‘Kraus” operators, and such
decompositions are not unique in general. ¹ ¹ 1 An example POVM for a
two state system is @xmath and @xmath . In the POVM measurement above,
because the @xmath ’s are entangled with @xmath , a detection of @xmath
, leads to the relevant measurement operators, @xmath , to be applied to
@xmath , resulting in ( 3.1 ) after normalization. The POVM measurement
scheme ( 3.1 ) is also known as the Quantum Bayes Rule ² ² 2 Note that
the Quantum Bayes Rule developed here solves a different problem than
was explored in [ 78 ] . Here a single density matrix is being
inferentially updated rather than the probability corresponding to the
form of @xmath copies of an unknown density matrix, i.e. quantum state
tomography. This type of quantum tomography may be replicated using the
standard maximum relative entropy method as it is a special case of (the
standard) Bayes Rule. [ 37 , 38 , 39 ] , or as the fundamental theorem
of quantum measurement [ 79 ] . For the remainder of the thesis, we will
refer to ( 3.1 ) as the Quantum Bayes Rule (QBR).

The QBR is a generalization of Lüders Rule [ 40 ] in which @xmath ’s are
projectors. Here we will derive the QBR from entropic arguments, which
we claim eliminates the need for ad hoc collapse postulates in QM. Our
result supports the interpretation that entropy may be used to
inferentially collapse density matrices using projectors (Lüders Rule),
as was derived in [ 11 , 12 ] ; however, our result is generalized to
the QBR and beyond. Rather than appealing to group theoretic arguments [
11 , 12 ] , our derivations are seemingly simpler as they only require
solving Lagrange multiplier problems. Because the quantum relative
entropy was derived as the tool for density matrix inference in [ 1 ] ,
our result discusses collapse from a purely inferential perspective, and
thus the quantum measurement proceedure gains more clarity. The present
derivation of the Quantum Bayes Rule using the quantum maximum entropy
method parallels the standard (probability) maximum entropy derivation
of Bayes Rule in [ 7 ] , and so, their derivation will be reviewed for
pedagogy.

As both forms of the standard and quantum relative entropy resemble one
another and were derived in parallel [ 1 ] , they inevitably share
analogous solutions and face similar limitations; however, because we
are dealing with density matrices, these limitations have consequences
in quantum mechanical experiments. In standard probability theory, there
is a phrase, “The maximum entropy method cannot fix flawed information”
[ 21 ] , and a similar theme permeates the inference procedure for
density matrices. Because the entropy was designed to update from a
prior density matrix @xmath to a posterior density matrix @xmath , the
form of @xmath must accurately describe the prior state of knowledge of
the system if @xmath is going to objectively represent the updated state
of knowledge for that quantum system. For instance, if our prior
knowledge tells us that a particle is located within a certain interval,
it makes no sense to impose that the particle has an average position
anywhere but within that interval. We derive this type of logical
compatibility in the quantum maximum entropy method, which we name the
Prior Density Matrix Theorem (PDMT). The PDMT states that a prior
density matrix can only be updated in the Hilbert space it originally
spans. The PDMT is purely mathematical and follows from logic analogous
to that of Bayes Theorem – if a prior state assigns zero probability to
some state, @xmath , Bayes Theorem gives @xmath . The PDMT is a purely
inferential consequence of entropic updating, yet it sheds light on some
of the nontrivial notions of quantum measurement and QM in general.

A special case of the PDMT implies that if the prior state is a pure
state @xmath , then the only inferential update one can make is
normalization using the quantum maximum entropy method. Thus, to be able
to reproduce standard quantum mechanical projection measurements, one
must first ad hocly allow the pure state to decohere (or partially
decohere) within the measurement device such that it can be updated
nontrivially. This is a reformulation of Lüders notion [ 40 ] that the
function of a measurement device must be to project the pure state into
a mixed state @xmath (where @xmath are rank-1 orthonormal projectors),
except our argument is formulated purely in terms of entropy and
inference. This concept is not as foreign or as objectionable as it may
seem if we consider the well known results of the quantum two slit
experiment. If a “which slit” detection of the particle is made, then
the resulting probability distribution is a decohered sum of
distributions on the screen (after many trials), whereas omitting this
detection allows for interference effects. Decoherence of the pure state
was required for a which slit inference. Once the particle hits the
screen, to detect its state, it decoheres (potentially again) on the
detection screen. This imprints a mixed state realization of the
incoming pure or decohered state on the screen @xmath , which may be
detected and collapse the state. ³ ³ 3 Note that both @xmath and @xmath
have the same positional probability distribution @xmath , but in
general they evolve differently in time. When making measurements in QM,
it is important to include both the system of interest and the auxiliary
pointer variable (and/or the measurement device), such that the
appropriate prior density matrix is generated. In this sense, “collapse
of the wavefunction” is better stated as the “collapse of the mixed
state after decoherence” – which then, as we will see, is nothing more
than standard probability updating. In conclusion, it is illogical for a
prior pure state to directly collapse into a different pure state
because, in some sense, we are already maximally informed about the
prior (pure) state of the system, which may be viewed as a (priorly)
collapsed state itself.

In preparation for the derivation of the Quantum Bayes Rule using the
quantum maximum entropy method, the derivation of Bayes Rule using the
standard maximum entropy method is reviewed below [ 7 ] . We will
introduce the PDMT and apply the quantum maximum entropy method to
derive the aforementioned cases of interest.

###### Maximum Entropy and Bayes

When the information provided is in the form of data, entropic updating
is consistent with Bayes Rule,

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

where Bayes Rule is the first equal sign and Bayes Theorem is the second
equal sign [ 21 ] . This leads to the realization that Bayesian and
entropic inference methods are consistent with one another [ 7 ] .

The posterior distribution @xmath can only be realized once the data
about @xmath ’s has been processed. This implies the state space of
interest is the product space of @xmath with a joint prior @xmath .
Suppose we collect data and observe the value @xmath . The data
constrains the joint posterior distribution @xmath to reflect the fact
that the value of @xmath is known to be @xmath , that is,

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

however; this data constraint is not enough to specify the full joint
posterior distribution,

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

because @xmath is not determined. The above equation is known as
Bayesian conditionalization.

As there are many distributions that satisfy this data constraint, we
rank candidate distributions using the maximum entropy method. Note that
the data constraint ( 3.3 ) in principle constrains each @xmath in
@xmath . This means a Lagrange multiplier @xmath is required to tie down
each @xmath of the marginal distribution @xmath . Maximizing the entropy
with respect to these constraints and normalization is,

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

where @xmath is the standard relative entropy and @xmath is the Lagrange
multiplier that imposes normalization. This leads to the following joint
posterior distribution,

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

The Lagrange multiplier @xmath is found by imposing normalization,

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (3.7)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

The Lagrange multiplier @xmath is found by considering the data
constraint ( 3.3 ),

  -- -------- -- -------
     @xmath      (3.8)
  -- -------- -- -------

Substituting in the Lagrange multiplier gives the joint posterior
distribution,

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

Integrating over @xmath gives the marginalized posterior distribution,

  -- -------- -- --------
     @xmath      (3.10)
  -- -------- -- --------

which is Bayes Rule. Jeffreys Rule is the generalization of Bayes Rule
when the data is uncertain and it is also consistent with the entropic
inference. ⁴ ⁴ 4 That is, set @xmath in place of ( 3.3 ). Further review
can be found in [ 21 ] . The universality of this entropic inference
method is emphasized by its consistency with other forms of inference
like Bayesian inference [ 7 , 8 , 9 ] .

###### A comment on biased priors

Entropic inference of this nature is only as useful as we are objective
about our subjectivity. One should be careful not to apply nonsensical
constraints, for instance, attempting to impose impossible expectation
values (like that the average roll of a six sided die be seven). In such
a case, the maximum entropy method provides “no solution” to the
optimization problem due to its irrationality [ 23 ] . Consider a set of
microstates @xmath , given a situation @xmath , that have a prior
probability @xmath , representing impossibility. In this subdomain
@xmath and situation @xmath , the values of @xmath are believed to be
impossible, and furthermore, one can see that it is impossible to update
this prior to anything but @xmath using the maximum entropy method for
any amount of new information (as can be seen in ( 2.36 )). In the same
way, a delta function prior distribution @xmath , which claims complete
certainty at @xmath , cannot be updated. We shall call priors that have
domains of unupdatablity @xmath ‘‘biased”, whether or not the prior was
attained by objective evidence or personal bias ⁵ ⁵ 5 “ I’m not biased
if I’m right!” - N. Carrara as the philosophical divide between the two
is, in all but the most extreme cases, a bit subjective. ⁶ ⁶ 6 For
instance, can an objective experimentalist be completely certain that
their measurement device hasn’t misfired?

A biased state of knowledge pertaining to a situation @xmath does not
imply bias for a new situation @xmath , so a realization that a
nonbiased probability should be assigned to the region @xmath admits the
system is now in a new situation @xmath . An example of this from
Statistical Mechanics (and also QM) occurs if the distance between the
walls of an infinite potential box is enlarged such that previous zero
probability regions now gain possibility. In this sense, and others,
that entropic updates are purely epistemic. If the physical situation
has changed, the situation @xmath should be updated as well. ⁷ ⁷ 7 In
the next chapter we see this kind of “situational” entropic updating in
Entropic Dynamics. Differences in situations @xmath in Entropic Dynamics
are labeled by differences in time @xmath .

#### 3.1 Quantum Entropic Inference

Before deriving the QBR, we must first discuss the ramifications of
“biased” prior density matrices.

##### 3.1.1 Prior density matrices

If the prior density matrix @xmath is a pure state, then we consider it
to be a completely “biased” density matrix because no amount of
information can update it, i.e., @xmath , without changing the
situation, using entropic methods. An example using a pure spin state
prior is discussed below to introduce the predicament, although the
analysis holds for all prior density matrices that are 1-dimensional
projectors.

Consider the biased prior density matrix @xmath – the positive spin-
@xmath eigenstate. To perform the calculation with any rigor using this
biased prior, we must unbias it slightly by allowing it to span more
than 1-dimension, where we introduce @xmath . We will use this @xmath
-prior @xmath for the prior, and then take the limit @xmath when
appropriate. In attempting to force the issue, consider maximizing the
relative entropy subject to an expectation value constraint of a general
2 by 2 Hermitian matrix @xmath , the expectation value of a weighted sum
of Pauli matrices and identity, such that @xmath would require a nonzero
component along spin down @xmath to meet the expectation value
constraint, in contrast to @xmath . Maximizing the entropy subject to
this constraint, normalization, and using the @xmath -prior gives an
@xmath -posterior,

  -- -------- -- --------
     @xmath      (3.11)
  -- -------- -- --------

The Lagrange multiplier that imposes normalization may be found by
diagonalizing the exponent @xmath ,

  -- -------- -- --------
     @xmath      (3.12)
  -- -------- -- --------

which suggests a convenient representation of the posterior density
matrix using @xmath ,

  -- -------- -- --------
     @xmath      (3.13)
  -- -------- -- --------

In the limit @xmath , the respective eigenvalues of @xmath , @xmath ,
approach a constant and @xmath while their respective eigenvectors
straighten out @xmath , and @xmath . Therefore the posterior density
matrix @xmath is equal to the biased prior density matrix and has not
been updated. Because the pure state fails to update, it is biased,
analogous to a delta function probability distribution in the standard
maximum entropy case. One might expect an infinitely large Lagrange
multiplier @xmath to be able to overcome the negative infinities one
obtains when taking @xmath . However, because @xmath is monotonic in
@xmath , an infinitely large Lagrange multiplier would imply the system
is being constrained to a maximally large expectation value (which
represents a very small subset of expectations) and usually implies that
the posterior is actually known with certainty (in which case there is
no need to use the maximum entropy method). The mixed spin state example
in Appendix C , is not biased, and therefore it avoided issues
surrounding biased prior density matrices. Below we will discuss the
general case and its implications.

Consider an @xmath th order biased prior represented in its eigenbasis
@xmath in an @xmath dimensional Hilbert space ( @xmath is a purestate).
Given an @xmath dimensional constraint @xmath (however the analysis
holds for @xmath of any rank), the prescription is to add some @xmath ’s
to @xmath such that @xmath spans @xmath , and,

  -- -------- -- --------
     @xmath      (3.14)
  -- -------- -- --------

has @xmath diagonal @xmath terms. Because density matrices are
Hermitian, and have a sum representation, @xmath , they may always be
rearranged and relabeled into the form above without loss of generality.
Thus, @xmath may always be written as a direct sum @xmath , where @xmath
is the first @xmath block of @xmath and @xmath is the remaining block
proportional to @xmath . Expressing the @xmath constraint matrices
@xmath in the eigenbasis of @xmath , and summing it, is,

  -- -------- -- --------
     @xmath      (3.15)
  -- -------- -- --------

which is a general representation of the matrix that resides in the
exponential of a posterior density matrix, @xmath , having an @xmath
-prior density matrix @xmath . Similarly partitioning @xmath by letting
@xmath be its first @xmath block, the characteristic polynomial equation
of @xmath may be written in the following form,

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.16)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where the @xmath ’s are the remaining @xmath independent coefficients.
For any finite @xmath , we may divide the characteristic equation by the
leading @xmath term, which in the limit of @xmath , reduces the
characteristic equation to the @xmath block characteristic equation,

  -- -------- -- --------
     @xmath      (3.17)
  -- -------- -- --------

for all finite @xmath . The eigenvectors associated to these @xmath
-finite eigenvalues span the @xmath vector space. As this is true for
all finite eigenvalues, the remaining @xmath eigenvalues are not finite
and indeed are all equal to negative infinity, due to the @xmath ’s as
@xmath . The remaining eigenvectors with the associated infinite
eigenvalues therefore span the remaining @xmath vector space, but are
not unique because they have degenerate eigenvalues. The eigenvectors
for the finite and infinite eigenvalues span disjoint subspaces and
therefore so do the unitary matrices which diagonalize them @xmath as
the unitary operators consist of columns of their associated
eigenvectors. This disjointness is independence in the sense that the
unitary operator @xmath is block diagonal, and therefore @xmath . The
posterior density matrix is therefore,

  -- -------- -- --------
     @xmath      (3.18)
  -- -------- -- --------

completely independent of the @xmath pieces of the constraints in @xmath
, and @xmath is the original @xmath th order biased prior. The lack of
an ability to update biased priors in the @xmath region is not a failure
of the method of maximum entropy, but rather a failure to choose
appropriate constraints given an @xmath th order biased prior density
matrix.

In general, any prior density matrix that does not span the entire
Hilbert space is an @xmath th order biased prior density matrix. This
insists the following, which we state as a theorem:

###### Prior Density Matrix Theorem (PDMT):

An @xmath th order biased prior density matrix @xmath can only be
inferentially updated in the eigenspace that it spans.

The immediate consequence of the PDMT is that entropic updating can only
cause epistemic and inferential changes to @xmath . The inability to
update a pure state nontrivially ⁸ ⁸ 8 The trivial update being pure
state normalization. , like in the pure state spin @xmath example, shows
just this. The only way to change the state of a 1-dimensional projector
(pure state) prior density matrix is to physically rotate the state by
applying dynamical unitary operators @xmath via the Schrödinger equation
because no inferential entropic update is possible. ⁹ ⁹ 9 Entropic
Dynamics, however, is an application of the standard maximum entropy
method and information geometry that does yield Schrödinger evolution.
However, if @xmath is a pure state and one knows that it will evolve
unitarily to @xmath , then one may evolve it. In this instance the
quantum maximum entropy method is not needed because the posterior state
is known; however, the quantum maximum entropy method has not failed
because the PDMT indeed gives @xmath as its solution.

Once the Quantum Bayes Rule is derived using entropy, we will see that
the Schrödinger equation and the quantum maximum entropy method
complement one another in QM – the first being responsible for
continuous dynamical “physical” changes to the system and the second
being discontinuous inferential updates within the space originally
spanned by @xmath , all being part of the measurement process. This is
not to say that the quantum maximum entropy method cannot in general
mimic the results of unitary state evolution, it could very well be the
special case that a unitary operator acting on an @xmath @xmath evolves
@xmath only within @xmath (rather than rotating the eigenvectors off
their hyperplane), and thus the PDMT does not prevent this update.

If one is serious about the assignment of a biased prior density matrix
then the following realization is needed, “Because pure states are
completely biased, the quantum maximum entropy method cannot update to a
new posterior at that time ”. If however the pure state is changed
“physically” by the addition of new microstates via interaction,
allowing it to decohere [ 80 , 41 ] (and the references therein), or
change by some other process, then at a later time one could employ a
method similar to [ 81 , 82 ] , that is, apply @xmath and its transpose
on either side of @xmath ,

  -- -------- -- --------
     @xmath      (3.19)
  -- -------- -- --------

to represent a new prior density matrix that, decohered, evolved
unitarily, or is part of a new experimental configuration. Now, if the
prior is an @xmath mixed state, it is possible to inferentially update
it non-trivially. If the quantum relative entropy is going to be used
for making inferences that pertain to pure state quantum measurement,
the function of a measurement device must be to project the system into
a basis @xmath , which evades the potential trivial results of the PDMT.
The proper prior for quantum measurement thus depends on both the system
of interest and the measurement device. This is a reformulation of
Lüders’ notion, except here, it has been motivated and expressed in the
language of entropic updating.

There are a few things to take away from this section. The quantum
maximum entropy method only updates a density matrix inferentially, as
can be seen by its lack of ability to rotate biased priors into
non-biased states or other biased priors states. This is exactly what we
expect as the unupdatability of biased priors exists in standard
probability theory. The solution to the biased prior problem (in the
standard and quantum maximum entropy method) is, if appropriate: to
change the constraint(s), the prior, both, or neither and accept the
consequences of its solution. This reasoning guides us in choosing
appropriate priors in subsequent derivations throughout this chapter.

#### 3.2 The Quantum Bayes Rule

Notationally, we will let density matrices living in a Hilbert space
@xmath to be denoted @xmath . Density matrices may of course be
expressed in any basis within these Hilbert spaces. Below, @xmath ’s are
the arbitrary variables (positions, momenta, ect.) that will be detected
and @xmath ’s are the arbitrary variables that will be inferred. We find
it convenient to denote the diagonal @xmath block matrix of @xmath with
an equal sign such that @xmath and similarly @xmath , and on occasion
probabilities @xmath . Also, a tilde above a density matrix will
represent a mixed representation of the density matrix in question
@xmath , with @xmath . Here we will review the standard POVM measurement
scheme that leads to the QBR.

###### Introduction – Quantum Bayes Rule:

Following [ 79 ] , consider a prior density matrix @xmath which is
entangled with a pointer variable such that @xmath . The system and the
pointer variable are entangled in the following way: given an initial
state of the pointer variable @xmath , the joint system is entangled
with a unitary operator @xmath ,

  -- -------- -- --------
     @xmath      (3.20)
  -- -------- -- --------

where,

  -- -------- -- --------
     @xmath      (3.21)
  -- -------- -- --------

and where,

  -- -------- -- --------
     @xmath      (3.22)
  -- -------- -- --------

is the @xmath sub-block matrix [ 79 ] . The prior density matrix of the
joint system is therefore,

  -- -------- -- --------
     @xmath      (3.23)
  -- -------- -- --------

where @xmath are the measurement operators of the POVM @xmath . Due to
Naimark’s theorem [ 83 , 84 ] , making a projective measurement of the
pointer variable @xmath can be used to perform a POVM measurement on
@xmath . Projecting the pointer variable requires the following action
on @xmath ,

  -- -------- -- --------
     @xmath      (3.24)
  -- -------- -- --------

which collapses the state in @xmath to @xmath and implies the new state
of the system is,

  -- -------- -- --------
     @xmath      (3.25)
  -- -------- -- --------

after normalization. Again this is known as the Quantum Bayes Rule (QBR)
[ 37 , 38 , 39 ] , the fundamental theorem of quantum measurement [ 79 ]
, or the POVM measurement formalism [ 74 , 75 , 76 , 77 ] .

In the remainder of this section we will derive inference rules using
the quantum maximum entropy method in order of increasing generality. In
each case, the relevant prior density matrix will be generated using
standard methods of decoherence in QM. First, we will consider the
simplest case – it is the inference of a single system collapsing into
one of its eigenstates. We will then reconsider this simple case in the
presence of measurement uncertainty – we call this case a “simple
partial (or uncertain) collapse”. The next level of generality is to
consider inferences on systems that are jointly coupled or entangled,
i.e., a system of interest entangled with a pointer variable. In such a
case, one must obtain the appropriate joint prior density matrix for
inference from QM. Inferences that lead the pointer variable to collapse
to a definite state yield a derivation of the QBR from the quantum
maximum entropy method. As before, if there is measurement uncertainty
in the pointer variable, one has a partial (or uncertain) collapse of
the pointer variable state. This yields a posterior state that we call
the Quantum Jeffreys Rule (QJR).

In the joint entangled system setting, we may wish to make detections of
both the system of interest and the pointer variable. We find that the
order of detection does not affect the final joint probability @xmath ,
which is consistent with the results of the delayed choice experiment –
a similar argument that argues from the point of view of epistemic
probability theory is given in [ 85 ] . Finally this section is
concluded with a simple example of quantum control in the quantum
maximum entropy method setting. It is an instance of the QJR, and it is
shown, for a joint entangled system, that capturing the pointer particle
inside a thermal box with temperature @xmath , or @xmath , leads to
changes in the statistics of the system of interest – thus a bit of
control may be exerted by the experimentalist by choosing the
temperature of the thermal box. Further generalizations are discussed in
the following section.

###### Simple Collapse:

This entropic update is a special case of ( 3.25 ) when the @xmath ’s
are all projectors rather than a more general POVM, i.e. we are going to
reproduce Lüder’s Rule. We must first argue for the appropriate form of
a prior density matrix that is undergoing measurement.

As we are simply doing a projective measurement on @xmath , “another”
pointer variable is not needed to generate the POVM. We follow the
intuition that if we are going to make inferences on the basis of
detection, the prior density matrix should appropriately reflect the
fact that it has interacted with a measurement device. A projective
measurement on the @xmath ’s in experiment requires entangling @xmath to
detector states @xmath and letting them decohere within the detector.
This avoids the potentially trivial results of the PDMT. For
concreteness we may imagine that @xmath is the pure state density matrix
of a particle that went though a two slit apparatus (no “which slit”
measurement has been made) and is impinging onto a screen, CCD array, or
a similar device designed to detect @xmath . Let the pure state be
@xmath . The pure state evolves with the detector states,

  -- -------- -- --------
     @xmath      (3.26)
  -- -------- -- --------

The unitary operators which couple the states are,

  -- -------- -- --------
     @xmath      (3.27)
  -- -------- -- --------

with the sub-block matrices,

  -- -------- -- --------
     @xmath      (3.28)
  -- -------- -- --------

We define a good detector as one in which the @xmath th pointer variable
state only entangles with the local state of the detector @xmath , which
is an argument for the sub-block matrix to take a simple form,

  -- -------- -- --------
     @xmath      (3.29)
  -- -------- -- --------

This then gives a fully entangled (von Neumann measurement) state,

  -- -------- --
     @xmath   
  -- -------- --

Tracing over the detector states @xmath represents that the system has
interacted with the measurement device (in this case the screen), but
its value has yet to be registered by either the device or the observer,
and that we have neglected to keep track of the detector states,

  -- -------- -- --------
     @xmath      (3.30)
  -- -------- -- --------

This is a mixed state realization of the original two slit pure state
probability @xmath on the screen. This is not original and may be
obtained following [ 40 ] using projectors @xmath or more directly [ 80
, 41 ] . Now that the form of the prior density matrix has been argued
for, we may utilize the quantum maximum entropy method.

In principle, when the result of a projective measurement ( @xmath ) is
registered, the state of the system is known with certainty. This is
represented by the following constraint on the posterior probability
distribution,

  -- -------- -- --------
     @xmath      (3.31)
  -- -------- -- --------

which is an expectation value on the posterior density matrix @xmath ,
stating that the system was detected in the state @xmath with certainty.
Because this constraint must be imposed for every @xmath , there is one
Lagrange multiplier @xmath for each @xmath . Maximizing the quantum
relative entropy with respect to this constraint and normalization is
setting

  -- -------- -- --------
     @xmath      (3.32)
  -- -------- -- --------

where @xmath is the quantum relative entropy. The posterior which
maximizes the quantum relative entropy subject to these constraints is,

  -- -------- -- --------
     @xmath      (3.33)
  -- -------- -- --------

Because the constraint and prior commute, the posterior density matrix
takes a simple form,

  -- -------- -- --------
     @xmath      (3.34)
  -- -------- -- --------

The normalization constraint gives,

  -- -------- -- --------
     @xmath      (3.35)
  -- -------- -- --------

and the expectation value constraint ( 3.31 ) gives,

  -- -------- -- --------
     @xmath      (3.36)
  -- -------- -- --------

The final form of the posterior density matrix is found by substituting
for @xmath , and the result is a collapsed state,

  -- -------- -- --------
     @xmath      (3.37)
  -- -------- -- --------

as expected. Written in a suggestive “Bayes update”, or “projective
collapse” form,

  -- -------- -- --------
     @xmath      (3.38)
  -- -------- -- --------

the result is more easily comparable to Lüders strong collapse rule and
the QBR. Note the tilde on @xmath indicates that it is the appropriate
prior for inference as the state has decohered in the detector. Although
@xmath are numerically equal probabilities, substitution of this above
is incorrect because @xmath is the state at the time prior to
interacting with the measurement device. Although this is perhaps a bit
fussy, it makes explicit why secure channels exist in quantum
cryptography – the statistics and dynamics of a quantum system change
when it is measured ( @xmath ) because the state must decohere before it
is inferentially updated ( @xmath ) due to the measurement. Thus,
“wavefunction collapse” is really decoherence and then collapse of the
mixed state. Note that this result is completely consistent with the use
of Bayesian inference to update probability distributions upon the
observation of data in QM, i.e., @xmath .

Equation ( 3.38 ) is the special case of the QBR ( 3.1 ) when the
measurements are projective. We call this inference a “simple collapse”
and reserve the title of QBR for the general result that will be derived
later. Note that this derivation does not require first solving for the
“weak” collapse and taking the limit [ 11 ] . This is because [ 1 ]
gives the general solution to @xmath (equation ( 2.78 )) when the
constraints are linear in @xmath .

###### Simple Partial (or Uncertain) Collapse:

A state may partially collapse if after detection the state still has a
certain probability of being in one state or another, due to measurement
uncertainty. This leads to a quantum analog of Jeffreys rule; however,
we shall reserve the title of the “Quantum Jeffreys Rule” for the
general result derived later. Given the similarly prepared prior density
matrix @xmath , we maximize the entropy with respect to a set of
constraints @xmath to codifying a lack of certainty in the final state
outcome (perhaps a narrow Gaussian distribution rather than exact
knowledge in ( 3.31 )). Maximizing the entropy with respect to these
constraints and normalization again gives the posterior,

  -- -------- -- --------
     @xmath      (3.39)
  -- -------- -- --------

and normalization implies,

  -- -------- -- --------
     @xmath      (3.40)
  -- -------- -- --------

Satisfying the remaining expectation value constraint ( @xmath ) gives
@xmath for each @xmath , and therefore,

  -- -------- -- --------
     @xmath      (3.41)
  -- -------- -- --------

which is a “partial” or “uncertain” collapse, reproducing [ 11 , 12 ] .

###### The Appropriate Joint Prior for the QBR:

The previous sections derive the collapse of a single simple system from
the quantum maximum entropy method. There, the appropriate prior density
matrix was generated from standard decoherence arguments in QM. Here,
the inference is over a joint entangled system, and therefore, the
appropriate prior density matrix is itself a joint prior density matrix.
The form of the joint prior density matrix for QBR depends on the
physical interactions between the system of interest, the pointer
variable, and later the detector used on the pointer variable. Thus, the
appropriate joint prior density matrix is generated from standard
methods in QM.

Notice that if @xmath is an @xmath th order biased prior, then,

  -- -------- -- --------
     @xmath      (3.42)
  -- -------- -- --------

is an @xmath th order biased prior, meaning that @xmath can only be
inferentially updated in that subspace (which may or may not be
desirable). This is potentially problematic if @xmath because @xmath is
a pure state and cannot be updated non-trivially due to the PDMT. The
unitary operators above represent the interaction between the pointer
variable and the system of interest, which entangles them according to (
3.23 ), and effectively generates the statistical dependencies that
appear in the joint prior probability density @xmath of @xmath , as well
as the likelihood function @xmath .

We follow the intuition that if we are going to make inferences on the
basis of detection, the prior density matrix should appropriately
reflect the fact that it has interacted with a measurement device. This
interaction will be modeled by entangling the pointer variable and
detector states @xmath , which act as a local environment states within
the detector, via a unitary evolution (following [ 41 ] and the notation
in [ 79 ] , but a simple projection argument from Lüders on the pointer
variable states of @xmath would also suffice),

  -- -------- -- --------
     @xmath      (3.43)
  -- -------- -- --------

where,

  -- -------- -- --------
     @xmath      (3.44)
  -- -------- -- --------

and the sub-block matrices are,

  -- -------- -- --------
     @xmath      (3.45)
  -- -------- -- --------

We define a good detector as one in which the @xmath th pointer variable
state only entangles with the local state of the detector @xmath , which
is an argument for the sub-block matrix to take a simple form,

  -- -------- -- --------
     @xmath      (3.46)
  -- -------- -- --------

The entangled density matrix becomes,

  -- -------- -- --------
     @xmath      
     @xmath      (3.47)
  -- -------- -- --------

The local environment of the detector states in which the pointer
variable reside, are traced over, as we do not keep track of their
evolution. This is to say, a small period of time after the projective
measurement has been made, the pointer variable states transition to a
mixed state, which gives a standard (classical) probability distribution
of the pointer variable states over the detector. The prior density
matrix after projective measurement has been made is thus a block
diagonal sum of states,

  -- -------- -- --------
     @xmath      (3.48)
  -- -------- -- --------

which we claim is the appropriate density matrix for POVM inference.
This form of the prior is no longer biased, even if @xmath was itself
biased.

###### The constraints leading to the QBR:

In principle, when the result of a projective measurement on the pointer
variable is registered, the state of the pointer variable is known with
certainty. This is represented by the following constraint on the
posterior probability distribution,

  -- -------- -- --------
     @xmath      (3.49)
  -- -------- -- --------

which resembles ( 3.3 ) (the symbol @xmath is the standard matrix
product). Notice that this information alone is not enough to fully
constrain @xmath as there are many @xmath which satisfy this constraint.
We therefore employ the quantum maximum entropy method and constrain the
posterior subject to normalization and the data constraint, and with
respect to the appropriate prior @xmath ,

  -- -------- -- --------
     @xmath      (3.50)
  -- -------- -- --------

which gives,

  -- -------- -- --------
     @xmath      (3.51)
  -- -------- -- --------

Because the prior density matrix is block diagonal @xmath we have,

  -- -------- -- --------
     @xmath      (3.52)
  -- -------- -- --------

Imposing normalization gives,

  -- -------- -- --------
     @xmath      (3.53)
  -- -------- -- --------

The data expectation value constraint forces,

  -- -------- -- --------
     @xmath      
     @xmath      (3.54)
  -- -------- -- --------

meaning, @xmath . Substituting in for the Lagrange multipliers gives the
final form of the posterior density matrix ,

  -- -------- -- --------
     @xmath      (3.55)
  -- -------- -- --------

The marginal posterior is the Quantum Bayes Rule,

  -- -------- -- --------
     @xmath      (3.56)
  -- -------- -- --------

which is equivalent to the standard POVM measurement formulation,

  -- -------- -- --------
     @xmath      (3.57)
  -- -------- -- --------

because @xmath and @xmath . The posterior probability of @xmath indeed
gives the standard Bayes Rule,

  -- -------- -- --------
     @xmath      (3.58)
  -- -------- -- --------

As stated in [ 39 ] , the off diagonal elements, @xmath , have a more
exotic updating rule. One may make further inferences about @xmath in (
3.56 ), for instance, an inference leading to its “simple collapse” (
3.38 ).

###### Quantum Jeffreys Rule (QJR):

In the same way as before, we may easily generalize this rule to cases
in which the final state of the pointer variable is uncertain and
encoded by a probability distribution @xmath rather than one exhibiting
certainty @xmath . Simply replacing the expectation value constraint (
3.49 ) by,

  -- -------- -- --------
     @xmath      (3.59)
  -- -------- -- --------

and performing the quantum maximum entropy method gives the marginal
posterior,

  -- -------- -- --------
     @xmath      (3.60)
  -- -------- -- --------

which we call the Quantum Jeffreys Rule.

###### What about inferring @xmath from @xmath measurements?

Above we have derived the QBR and the QJR from the quantum maximum
entropy method where it was assumed that a density matrix in @xmath is
to be inferred from detections of @xmath ’s. It is possible, however, to
consider the odd situation of detecting @xmath ’s and making inferences
about the pointer variables in @xmath . Through this computation we find
that the resulting joint probability distribution @xmath of @xmath is
independent of the order in which @xmath ’s and @xmath ’s are detected,
which is in agreement with results of the delayed choice experiment.

Consider rewriting a similarly entangled pointer variable prior density
matrix @xmath from ( 3.42 ) as,

  -- -------- -- --------
     @xmath      (3.61)
  -- -------- -- --------

where the @xmath ’s are a bit messy but obtained from moving and
relabeling the components of the unitary matrices. If another projection
measurement device is designed to detect @xmath states, rather than
@xmath states, an analogous argument can be used to decohere the joint
prior density matrix in the @xmath ’s,

  -- -------- -- --------
     @xmath      (3.62)
  -- -------- -- --------

rather than the @xmath ’s. It should be noted that in general @xmath ,
from the previous section, as they are block diagonal in different
Hilbert spaces – this joint prior density matrix is block diagonal in
@xmath . The same analysis from the previous section is made: using
@xmath as the prior, maximize the entropy with respect to normalization
and the @xmath data constraint,

  -- -------- -- --------
     @xmath      (3.63)
  -- -------- -- --------

and solve for the Lagrange multipliers. This gives

  -- -------- -- --------
     @xmath      (3.64)
  -- -------- -- --------

such that the marginal posterior is

  -- -------- -- --------
     @xmath      (3.65)
  -- -------- -- --------

which is equivalent to,

  -- -------- -- --------
     @xmath      (3.66)
  -- -------- -- --------

and is interpreted as the posterior density matrix of the pointer
variable after a complementary @xmath measurement operator has been
applied and @xmath has been detected. One may make further inferences on
@xmath , for instance, an inference leading to its “simple collapse” (
3.38 ).

The posterior density matrices, @xmath and @xmath , are related in the
following way: Notice that although @xmath in general, their components
along the diagonal-diagonal are equal, @xmath , because both density
matrices are decohered in one way or another from equivalently entangled
prior density matrices @xmath . This means

  -- -------- -- --------
     @xmath      (3.67)
  -- -------- -- --------

and because @xmath and @xmath ,

  -- -------- -- --------
     @xmath      (3.68)
  -- -------- -- --------

and likewise @xmath , we see all of the probability relationships hold
and may be used interchangeably. It should also be noted that in
general: @xmath and @xmath because their off diagonal components may
differ. The joint posterior density matrices @xmath and @xmath , and
their posterior marginals potentially differ in how they will evolve in
time. Because the use of appropriate measurement devices leads to @xmath
, there is no interpretational issue in the delayed choice experiment
because collapse of both @xmath and @xmath only occurs after decoherence
of both @xmath and @xmath . The time order of the decoherence becomes
irrelevant because the joint probabilities are equal. Essentially what
has happened in the delayed choice experiment is that you do not know if
you have done a “which slit” measurement or not, which is like having a
“mixed state of potential measurement outcomes”, but, this is precisely
what a POVM measurement represents.

###### Quantum Jeffreys Rule via thermal baths:

Rather than detecting the result of a projective measurement on the
pointer variable state, we consider the posterior density matrix one
would obtain if the pointer variable is sent into a thermal box – the
result can be naturally generated in the quantum maximum entropy method.
We will see inferences of this type allow the experimenter some control
over the distribution of the final states of @xmath by adjusting the
initial temperature of the thermal box that will be used to capture the
pointer particle. Here we will let the Hilbert space @xmath of the
pointer variable be spanned by @xmath , the energy basis eigenstates of
the pointer variable in the thermal box having a Hamiltonian @xmath .
The joint prior density matrix is prepared similar to how it was
prepared above, where @xmath , such that,

  -- -------- -- --------
     @xmath      (3.69)
  -- -------- -- --------

The following energy expectation value is used to represent the
constraint of a pointer variable in a thermal box,

  -- -------- -- --------
     @xmath      (3.70)
  -- -------- -- --------

Again notice that this information alone is not enough to fully
constrain @xmath as there are many @xmath which satisfy this constraint.
We therefore employ the quantum maximum entropy method; that is, to
maximize the quantum relative entropy subject to normalization, this
constraint, and with respect to the prior @xmath ,

  -- -------- -- --------
     @xmath      (3.71)
  -- -------- -- --------

which gives,

  -- -------- -- --------
     @xmath      (3.72)
  -- -------- -- --------

Because the prior density matrix is block diagonal @xmath we have that,

  -- -------- -- --------
     @xmath      (3.73)
  -- -------- -- --------

Imposing normalization gives,

  -- -------- -- --------
     @xmath      (3.74)
  -- -------- -- --------

The expectation value constraint forces,

  -- -------- -- --------
     @xmath      
     @xmath      (3.75)
  -- -------- -- --------

meaning one can solve @xmath by inverting the above equation after
computing @xmath as is done in Statistical Mechanics. The marginal
posterior is a realization of the Quantum Jeffreys Rule using
thermalization,

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.76)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

in which the state @xmath of the system may be controlled by forcing the
pointer variable into a box with temperature @xmath or @xmath as the
resulting joint posterior density matrices differ. In particular, if
@xmath this implies @xmath and therefore @xmath with @xmath being the
ground state energy of the particles in the thermal box. This is similar
in nature to a postselection measurement scheme from [ 43 ] , which will
be reviewed in Chapter 5 , except here we used a thermal box and the
quantum maximum entropy.

#### 3.3 Generalizations

General inferences of @xmath on the basis of a prior state of knowledge
@xmath and arbitrary expectation value constraints @xmath give the
following general updating rule,

  -- -------- -- --------
     @xmath      (3.77)
  -- -------- -- --------

from @xmath in light of new information about @xmath . This is of-course
the general solution to the quantum maximum entropy method, but now it
is clear it may be interpreted as the solution for general purpose
inference in QM when applied correctly.

##### 3.3.1 Non-commutativity for Simultaneous and Sequential Updates

The constraints and priors used in the derivation of the QBR and QJR
factored in the exponential because they took the form @xmath , in part
because of commutation of @xmath , even when the individual operators
@xmath do not commute themselves. Thus general inferences involving
expectation values of non-commuting operators and prior density matrices
generalizes these rules and the solution is found by diagonalizing the
exponential and using convex optimization methods (i.e. the general
solutions for ( 3.77 )). Perhaps the simplest example of such a
situation is the mixed spin state example given in Appendix C .

The above inferences have all been instances of simultaneous updating,
so a note should be made on sequential updating. Consider updating
@xmath with respect to a set of constraints ( @xmath ) such that @xmath
via the quantum maximum entropy method. If one receives new information,
( @xmath ), and we consider @xmath to be the new prior, we may update
@xmath . These sequential updates give the following inference chain
@xmath . If one were instead to apply these constraints in an opposite
order, even if the operators in ( @xmath ) and ( @xmath ) commute, in
general @xmath . The posterior @xmath is guaranteed to satisfy 2, but
not 1, whereas the posterior @xmath is guaranteed to satisfy 1 but not
2. The third case is the simultaneous update using both pieces of
information @xmath . This gives @xmath in one update, which again is not
equal to @xmath or @xmath in general as it is guaranteed to satisfy both
constraints 1 and 2 simultaneously. This type of non-commutation appears
also in the standard maximum entropy method [ 8 , 9 , 21 ] , so its
manifestation in the quantum maximum entropy method is not surprising.
The conclusion using the quantum maximum entropy method is the same;
sequential and simultaneous updating ultimately refer to different
states of knowledge, and therefore one should expect the entropic method
to give different results. The fact that the applied order of
constraints does not in general commute is therefore a feature [ 8 ] of
the quantum maximum entropy method – if new information rules out old
expectation value constraints, then sequential updating to @xmath or
@xmath is appropriate, whereas, if both constraints are expected to hold
in the posterior, then one should simultaneously update the state to
@xmath .

##### 3.3.2 Canonically Modified Quantum Bayes Rule

Finally, to the joint prior density matrix @xmath one may simultaneously
apply a data constraint to the pointer variable (that would result in a
Quantum Bayes Rule) while simultaneously constraining the @xmath space
with respect to an expectation value @xmath . This results in a state
that cannot alone be inferred from either a von Neumann maximum entropy
procedure or a Quantum Bayes procedure alone. The resulting state is,
analogous to its probabilistic form in [ 8 ] , a canonically modified
Quantum Bayes Rule. Maximizing the quantum relative entropy with respect
to these constraints and normalization gives,

  -- -------- -- --------
     @xmath      (3.78)
  -- -------- -- --------

Imposing the data constraint, @xmath , gives,

  -- -- -- --------
           (3.79)
  -- -- -- --------

which means the data Lagrange multiplier is,

  -- -- -- --------
           (3.80)
  -- -- -- --------

We therefore arrive at a canonically modified Quantum Jeffreys Rule,

  -- -------- -- --------
     @xmath      (3.81)
  -- -------- -- --------

The special case, if @xmath , gives a canonically modified Quantum Bayes
Rule,

  -- -------- -- --------
     @xmath      (3.82)
  -- -------- -- --------

If the expectation value constraint is not imposed, or if @xmath , then
the canonical factors drop out and the Quantum Bayes Rule is recovered.
Note that the values of @xmath are mutually compatible with the data
constraint and the prior as to guarantee a logical solution – this
logical restriction is present equally well in the canonically modified
Bayes Rule [ 8 ] .

#### 3.4 Conclusions

In this chapter we applied the quantum maximum entropy method and
derived the Lüders collapse (and partial collapse) rules, the QBR, and
the QJR. Furthermore, it was discussed why the order of sequential
updates do not commute in general, even when the operators within the
sequentially implemented expectation value constraints do. Having
derived a Quantum Bayes Rule from the quantum maximum entropy method,
the suggestion of such a possibility in [ 64 ] is met. The subject
matter reviewed in this chapter therefore unifies topics in Quantum
Information and quantum measurement through quantum entropic inference
just as much as Caticha and Giffin unified topics in information theory
and Bayesian inference through standard entropic inference [ 7 , 8 , 9 ]
.

The article [ 2 ] shows how the quantum maximum entropy method can
eliminate ad hoc collapse postulates in QM, in agreement with [ 11 , 12
] ; however decoherence is still required. As is demonstrated by the
arguments leading up to the PDMT and as the derivation of the above
inference rules have showed, the phrase “collapse of the wavefunction”
should be replaced by “decoherence and the collapse of the mixed state”.
This is in agreement with Lüders’ notion that the application of a
measurement device is to mix the incoming state @xmath if collapse is
going to occur. In this sense, a pure state is already a collapsed
state, so trying to directly collapse this state to a different pure
state is illogical (by the PDMT) unless the state first decoheres –
“uncollapsing” the state.

The main conclusion from [ 7 , 8 , 9 ] is that the standard maximum
entropy method is the “universal method for inference” because it can
reproduce Bayesian inference as a special case, while also being able to
make inferences not achievable through Bayesian inference or maximum
entropy inference (having prior’s set to 1 in the relative entropy) –
the method updates @xmath for arbitrary @xmath . The main conclusion
from this chapter is analogous – the quantum maximum entropy method is
the “universal method of density matrix inference”, the method updates
@xmath for arbitrary @xmath . Although pure state prior density matrices
fail to update @xmath due to the PDMT, there is no contradiction,
because one may still correctly unitarily evolve pure state priors
@xmath , and so one may simply accept the non-update as one does for
biased probability distributions @xmath in the standard maximum entropy
procedure.

Because Entropic Dynamics (reviewed next chapter) is able to derive
Quantum Mechanics and a density matrix formulation using the standard
maximum entropy method and information geometry, it implies that the
quantum maximum entropy method is really a special case of the standard
maximum entropy, and so, it retains its title of the universal method of
inference. The quantum maximum entropy method thus retains the more
specialized title of: “the universal method of density matrix
inference”. As the quantum maximum entropy method has utilized nothing
but techniques from the standard quantum mechanical formalism, it, and
its results, may be appended to the standard formalism.

## Part II: Entropic Dynamics and the Solution to the Quantum
Measurement Problem

### Chapter 4 Entropic Dynamics

The previous chapters review the design of probability, relative
entropy, and quantum relative entropy. By developing the foundations,
and designing the tools of inference thereafter, we were able to make
progress. In particular, we found that the standard and quantum relative
entropies are designed for the purpose of inference, each formulated
from the same Principle of Minimum Updating and design criteria, and
that the Quantum Bayes Rule is a special case of quantum entropic
inference. We formulated the Prior Density Matrix Theorem (PDMT), which
states that a quantum system can only be inferentially updated in the
regions spanned by its eigenspace, using the quantum maximum entropy
method. This stems from the logic that if a pure state, or the set of
them in @xmath , is known with certainty, than any information that
would lead to a deviation from this fact is moot. Hence, the unitary
evolution generated from the Schrödinger Equation (SE), which is capable
of rotating the eigenvectors of @xmath out of their original eigenspace,
is not in general an update that can be implemented by the quantum
maximum entropy method due to the PDMT. We would like to understand
unitary evolution from the point of view of inference; however, the
quantum maximum entropy method cannot provide the type of understanding
we seek.

The material in Chapter 2 and 3 is all predicated on the assumed
existence of the standard quantum formalism. While the standard quantum
formalism is great in practice, the formalism itself is empty – there is
no agreed upon interpretation of QM as it is difficult to formulate why
the oddities of QM are the way they are. This is likely because in most
approaches to the foundations of QM, one starts with the formalism and
then appends an interpretation to it, almost as an afterthought.
Entropic Dynamics (ED) [ 13 , 25 , 21 , 26 , 27 , 28 ] , on the other
hand, starts with the interpretation, that is, one specifies what the
ontic elements of the theory are, and only then one develops a formalism
appropriate to describe and predict those ontic elements. Laws of
physics are derived as applications of standard entropic inference, and
thus, ED differs from most theories in physics.

This framework is extremely constraining. For example, there is no room
for “quantum probabilities” in Entropic Dynamics. Probabilities are
neither classical nor quantum, they are tools for reasoning that have
universal applicability, as is touched upon in previous chapters. The
wavefunction should therefore be an epistemic object ( i.e. @xmath is a
probability) and its time evolution – the updating of @xmath – should
not be arbitrary. Given that probability theory has universal
applicability, the dynamics of probabilities in QM should be dictated by
the usual rules of inference. Entropic Dynamics is indeed able to
formulate QM as an instance of standard probability updating through
entropic inference and information geometry.

It should be noted that ED is not the “be all end all” in physics –
rather it generates models for inference that happen to be consistent
with physics. Along a similar line of thought, at this point in the
development of ED, “the discoveries” are the inferential constraints and
pertinent information required to obtain physics from probability
theory, rather than the physics equations themselves [ 45 ] . Past,
current, and future research in ED involves: reformulating other fields
of physics as inference [ 86 , 87 , 88 ] , refining and strengthening
methods in ED [ 89 , 13 ] , addressing the classical limit [ 90 ] ,
giving a derivation of the exact renormalization group [ 91 ] , deriving
the Black and Scholes equation [ 92 ] , addressing and differentiating
between the QM in ED and its Bohmian limit [ 93 , 94 ] , and using ED to
address measurement problems and no-go theorems in Quantum Mechanics [
14 , 3 , 4 ] – the later being the central focus of subsequent chapters.

This chapter is a review of the ED formalism presented in [ 25 , 26 , 89
, 13 ] . It follows sections of [ 3 , 4 ] as well as discussions with A.
Caticha [ 45 ] . The newest formulation of ED is [ 13 ] and the most
primitive is [ 26 ] . Novel material appears in Section 4.2 – it is a
straightforward application of ED that produces mixed quantum states.

###### Ontological Positions and Epistemic Inferables in Entropic
Dynamics:

An assumption that permeates theories of physics is the existence of
particles. Whether or not this assumption is “true”, it is nonetheless
useful [ 44 ] . As ED is an application of inference toward deriving
laws of physics, we need something “physical” or “ontic” to actually
make inferences about, otherwise ED may be critiqued as ungrounded. We
therefore assume the existence of particles and understand them to be
the primitive ontological elements we wish to make inferences about – at
least at the level of this effective theory or model. The natural
follow-up question is, “What inferences will we be making?”.

For the results of ED to be verifiable, we must be able to perform
experiments, collect data, and make inferences. Given that we live in
space, it is impossible to point to a detector that is not located in
some region of space. Therefore on some level, any detection made by a
detector gives some amount of positional information. Furthermore, we
know a detection has taken place when our detector changes, which may be
indicated to the observer in any number of convenient, and usually
macroscopic, ways (digital text, needle positioning, flashing lights,
signal amplification,…). The detector is itself in principle a
construction of particles, located in space , where changes are enacted
upon to reach a newly distinguished “detector” state. Following this
line of thought, and after contemplating the nature of what we might
mean by a particle, define: a particle is a piece of ontology at a
position . In this sense, and like Bohmian mechanics, particle positions
will play the role of beables [ 15 ] in ED.

Because the observer ultimately holds the final desiderata for
inference, they hold the degrees of rational belief, all verifiable
quantities in Physics are inferred and are therefore categorized as what
we call inferables [ 3 ] . As ED is founded on probability and
probability updating, Hermitian observables, as well as the
wavefunction, are epistemic and do not have an ontological
predisposition in ED. Standard Hermitian “observables” are more aptly
referred to as “inferables” as their values will be inferred from
positional measurements or positional correlations in a probability
distribution [ 14 , 3 ] . ¹ ¹ 1 As well as other measurement quantities
of interest like the complex valued, and a bit mysterious, Weak Values
from [ 43 ] . The thought that perhaps the only type of measurements we
do in physics are position measurements, and that other quantities are
inferred from position, is not original: Bell [ 15 , 17 , 95 ] , Feynman
[ 96 ] , Caticha [ 14 , 3 , 97 , 45 ] , and other physicists have
expressed this shared philosophy on experiment. As position is the only
ontological variable, and the fact that objects other than position may
be treated as epistemic inferables , the interpretation of QM is
drastically different in ED. This ends up providing a solution to the
quantum measurement problem [ 14 , 3 ] while also not being ruled out by
the Bell-Kochen-Specker [ 17 , 18 , 19 ] no-go theorem (as well as the
Bell [ 20 ] and no @xmath -epistemic theorem [ 16 ] ) [ 4 ] . ² ² 2 The
Bell-Kochen-Specker no-go theorem rules out interpretations of QM that
assign definite values to operators that simultaneously belong to
noncommuting sets of internally complete sets of commuting observables.
A common interpretation of the Bell-Kochen-Specker theorem is that
observables in QM are contextual , meaning that an observable’s
“character”, “aspect”, or “value” depend on the remaining set of
observables it is considered along-side-with in a measurement setting.

#### 4.1 From Entropic Dynamics to Quantum Mechanics

This remainder of section 4.1 is a review of [ 13 , 89 ] . In the
context from above, Entropic Dynamics seeks to generate useful and
dynamical inferential models toward understanding the things we can
infer from nature. These models eventually take the form of Physics
equations, but because they were generated from the foundation of
probability and probability updating, we can claim to know “what we
needed to know” in our model to solve the problem.

Here we are interested in the constraints and assumptions required to
derive QM from the first principles of inference and probability
updating using Entropic Dynamics. The first step is to state the
universe of discourse, the set of possible microstates or the subject
matter, one would like to infer on the basis of incomplete information –
these are the ontological positions of @xmath particles in a flat
Euclidean space @xmath (metric @xmath ). Our knowledge of the positions
of particles is characterized by a probability density @xmath , where
@xmath is a coordinate in a @xmath dimensional configuration space
@xmath of particle coordinates @xmath , and where @xmath denotes the
@xmath th spatial axis of the @xmath th particle’s position, or for
short @xmath . From the onset, particles have definite yet unknown
positions and are treated as the “physical” or “ontological” quantities
of interest – the proposition @xmath in @xmath reads: “the configuration
space coordinate @xmath correctly represents the ontological positions
of the particles in 3D space”. Expectation values of over @xmath ,
@xmath , or simply integrations @xmath , are therefore integrations over
the propositions @xmath rather than the actual ontological positions of
the particles since a particle can only have one position . ³ ³ 3 Later
this implies that the eigenvalues @xmath of the position operator @xmath
are themselves propositions rather than ontological particle positions.
However for brevity, we will simply call @xmath “the position of the
particles”.

The fact that positions are “ontological” and probabilities are
epistemic, immediately separates the interpretation of QM in ED from
other mainstream interpretations of QM, like: the Copenhagen
interpretation whose ontological values are created by the measurement
device, the Bohmian interpretation in which wavefunctions are
configuration space ontological fields, and the Many Worlds
interpretation that perhaps makes the largest assumption in physics
possible – that there exists infinitely many ontological and branching
universes. Our assumption is simple, “Particles have ontological
positions” and the designed features of probability updating work.

Now that the microstates have been specified, we are inclined to ask how
the position of these particles change. In particular, we wish to know
how probable it is for @xmath , that is, we seek a transition
probability of the form @xmath to quantify this uncertainty while being
consistent with the notion that particles have definite yet unknown,
ontological positions . We therefore make the following assumptions: 1)
particles move along continuous trajectories, 2) particles have a
tendency to be correlated and thus undergo interparticle correlated
drift based their configuration, 3) particles have a tendency to undergo
uncorrelated individual drifts depending on their location in 3D space.
Once the form of the transition probability @xmath is found, it will be
used to inferentially update @xmath . This crucial step is also the
reason why ED naturally avoids the PDMT.

###### 1) Continuous motion:

The first assumption is implemented by making large @xmath improbable.
This is done by imposing that each particle have small variances, @xmath
, in particle coordinates,

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

where motion is continuous in the limit @xmath . We use @xmath Lagrange
multipliers @xmath to impose these @xmath constraints. The Lagrange
multipliers @xmath eventually turn out to be proportional to the masses
of the particles.

###### 2) Interparticle correlation and drift:

Interparticle correlation and drift is implemented in the following way.
Letting @xmath be a scalar function over the configuration space of our
@xmath particles, we design @xmath such that its configuration space
partial derivatives @xmath regulate the expected drift of the particles,
i.e., @xmath is a “drift potential”. We impose this regulation be
distributed over the @xmath particles, and thus impose it with a single
constraint over the set of particles,

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

where @xmath is a small constant. Designed over configuration space,
@xmath allows for interparticle correlations and drift, and eventually,
entanglement. A point is made in [ 13 ] to note that the origins of
@xmath are unexplained, which is an interesting topic for future
research. ⁴ ⁴ 4 One potential explanation is in terms of the entropy of
some other microscopic ( @xmath ) variables [ 26 ] . Not giving the
precise (perhaps microscopic) origins of @xmath , but still using it for
the purpose of modeling probabilistic updates in the Entropic Dynamics
approach, is analogized to not giving the precise microscopic origins
forces, but still finding their use in Newton’s law. We will use the
Lagrange multiplier @xmath to impose this constraint.

###### 3) Uncorrelated individual particle drift:

Imposing the first two constraints without this one leads to an
interesting evolution, but richer forms of dynamics are found by further
imposing this constraint. As uncorrelated individual particle drifts are
unconcerned with the drifts of other particles, we introduce a field
@xmath , with @xmath in 3D space, to regulate the expected independent
drift of the @xmath particles. We further assume that the field @xmath
can be redefined by different amounts @xmath at each location such that
what is called the “0” field value at one location may not be the “0”
field value at another location. This is a local gauge symmetry, and the
way to compare field values at different locations is through a
connection field @xmath that reveals how the field value at @xmath is
related to the field value at @xmath in 3D space. The connection field
is constructed such that gauge transformations in @xmath also shift the
connection field by @xmath , and thus @xmath remains invariant. The
gauge invariant particle drift constraints regulate each particle’s
individual expected drift,

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

and thus we require @xmath Lagrange multipliers @xmath to impose these
@xmath constraints. If @xmath has the topology of an angle, it solves
Wallstrom’s objection to Nelson’s stochastic mechanics [ 98 , 99 , 100 ,
89 ] , and the Lagrange multipliers @xmath eventually turn out to be
proportional to the (quantized) electric changes of the particles. This
will be discussed a bit more later when it is more relevant.

###### Maximum Entropy:

There are many probability distributions @xmath that satisfy the above
expectation value constraints ( 4.1 - 4.3 ). We therefore use the
standard maximum entropy method [ 21 , 24 , 10 , 22 ] to rank the
candidate distributions. Without any prior knowledge, the prior
transition distribution @xmath is a very broad normalizable Gaussian
distribution to encode that, given nothing is known about particle
motion (equations ( 4.1 - 4.3 ) are yet to be imposed), particles may
jump anywhere with near to equal probability – there is no reason to
believe otherwise.

Maximizing the relevant relative entropy with respect to @xmath ,

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

subject to the expectation value constraints, ( 4.1 - 4.3 ), and
normalization, via the Lagrange multiplier method forces the probability
updating scheme to evolve probabilities in a way that is consistent with
the notion of particles having definite yet unknown, and ontological,
positions . ⁵ ⁵ 5 This, and the assumption that particles have
ontological positions in ED, is of tantamount importance for the
remainder of the discussion in this thesis. The maximum entropy update
gives,

  -- -- -- -------
           (4.5)
  -- -- -- -------

after completing the square. Because @xmath is nearly constant over
regions of interest, it has been absorbed into the normalization
constant @xmath . The expected drift of the particles is,

  -- -------- -- -------
     @xmath      (4.6)
  -- -------- -- -------

with @xmath . We will absorb @xmath as a scaling constant into @xmath
without loss of generality. A generic displacement can be expressed as
the expected drift plus a fluctuation,

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

respectively, where,

  -- -------- -- -------
     @xmath      (4.8)
  -- -------- -- -------

One finds that for large @xmath the dynamics is dominated by
fluctuations, @xmath , which are of order @xmath whereas the expected
drifts are on the order of @xmath . Large @xmath implies small @xmath ,
meaning that the large @xmath limit is the continuous limit of particle
motion, and that in this limit, fluctuations dominate.

##### 4.1.1 Entropic Time

So far, there has been no explicit mention of time; rather, the only
assumptions made are that particles have a tendency to change
ontological positions and follow drift gradients. But aren’t these
changes in position exactly what we as observers refer as a mechanism
for keeping track of time classically? We therefore introduce time as a
bookkeeping parameter to index change. If we have some initial knowledge
of the positions of particles @xmath , we may consider how our
distribution changes once our particles have undergone a fluctuation and
drift by considering,

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

We have let @xmath label the distribution @xmath and let @xmath label
the distribution @xmath “after” the distribution been updated
“entropically” by @xmath ( 4.5 ). Equation ( 4.9 ), and its
preliminaries, are what separate Entropic Dynamics from the standard
“time independent” probability updating one is usually accustomed to
seeing in the maximum entropy approach. It is natural to call equation (
4.9 ) the “entropic dynamics update”.

Before continuing, we need to introduce the concept of duration @xmath
in our transition probabilities @xmath . Because short steps imply short
time periods, and because fluctuations @xmath dominate for short steps (
@xmath ), the notion of continuous motion must be implemented at the
level of short steps. This implies the form of @xmath , where later it
will be revealed that the particle specific constant @xmath is the mass
of the @xmath th particle and the constant @xmath , that fixes units, is
Planck’s constant. We have that “equal fluctuations” of a particle are
equal measures of times.

The information metric of the transition probability in configuration
space coordinates is,

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

In the limit of short steps, one finds,

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

which diverges as @xmath goes to zero. This is somewhat expected due to
the nature of the information metric being a measure for statistical
distinguishability. In this limit @xmath is sharply peaked and thus
@xmath and @xmath overlap less and are thus “more distinguishable”. If
we choose the arbitrary scale constant @xmath such that it is
proportional to @xmath , then this metric can be recast as a “mass
tensor”

  -- -------- -- --------
     @xmath      (4.12)
  -- -------- -- --------

The inverse mass tensor is therefore,

  -- -------- -- --------
     @xmath      (4.13)
  -- -------- -- --------

These tensors will become particularly relevant later in the derivation.

Using our notion of time, we may reformulate the previously defined
quantities. Recasting @xmath as,

  -- -------- -- --------
     @xmath      (4.14)
  -- -------- -- --------

with

  -- -------- -- --------
     @xmath      (4.15)
  -- -------- -- --------

allows @xmath to be interpreted as the drift velocity of the particle,
where

  -- -------- -- --------
     @xmath      (4.16)
  -- -------- -- --------

The new form of,

  -- -------- -- --------
     @xmath      (4.17)
  -- -------- -- --------

makes clear time has been designed such that equal measures of a
particle’s fluctuation occur over equal durations of time. The
transition probability is,

  -- -------- -- --------
     @xmath      (4.18)
  -- -------- -- --------

which specifies the entropic dynamics update in ( 4.9 ).

Equation ( 4.9 ) is the integral form of the Fokker-Planck (diffusion)
equation and may be recast as the differential Fokker-Planck equation (a
derivation is reviewed in [ 21 ] ),

  -- -------- -- --------
     @xmath      (4.19)
  -- -------- -- --------

where the current velocity @xmath is the current = drift @xmath osmotic
velocities of the probability flow in configuration space, respectively.
Specifically:

  -- -------- -- --------
     @xmath      (4.20)
  -- -------- -- --------

is a function defined in terms of previously defined variables. In this
sense @xmath is something like a “current potential” for the current
velocity @xmath that tells us how @xmath is going to change in time by (
4.19 ). At this point, @xmath ’s only time dependence is through @xmath
, but it is important to evaluate what we have been able to derive using
ED so far:

ED has managed to show that the the Fokker-Planck equation ( 4.19 ) may
be interpreted as a mechanism of entropic probability updating . The
“current potential” @xmath , as argued above, is thus a mechanism or
function that guides probability updating, and in this sense, it is
purely epistemic – it is informative. To derive QM, we need an
additional mechanism for updating the constraints ( 4.2 ) and ( 4.3 ) in
the entropic dynamics update ( 4.9 ). We also let @xmath to be labeled
by time such that @xmath has further functionality in its ability to
update and mediate correlations in @xmath – that is, we let @xmath be
dynamically informative.

Note that nothing prevents us from rewriting ( 4.19 ) as a functional
derivative @xmath , where,

  -- -------- -- --------
     @xmath      (4.21)
  -- -------- -- --------

has an integration functional constant of @xmath , named @xmath . At
this point the dynamics of @xmath , and consequently @xmath , are
unknown and we need a natural way to tie down the functional form of the
time dependence in @xmath . The form of @xmath should originate from
considerations of the system of particles being modeled and how we might
expect it to respond to changes in @xmath or @xmath . The current
potential @xmath gives the modeler freedom to model different systems in
ED. Previous versions of ED that eventually lead to QM impose that the
dynamics of @xmath are determined by changes @xmath that keep @xmath ,
that is, @xmath plays the role of a Hamiltonian. We will take the newer
approach [ 13 ] , as it better adheres to the probabilistic foundations
in ED.

##### 4.1.2 Geometry of e-phase space

In the search for the constraints on @xmath that lead to QM, we look for
inspiration from information geometry, and impose that @xmath has the
topology of an angle (i.e. now @xmath has the topology of an angle).
Imposing that @xmath has the topology of an angle solves Wallstrom’s
objection ⁶ ⁶ 6 Wallstrom’s objection is that stochastic mechanics leads
to phases and wavefunctions that are either both multi-valued or both
single-valued. Both alternatives are unsatisfactory because on one hand
QM requires single-valued wavefunctions, while on the other hand
single-valued phases exclude states that are physically relevant (e.g.,
states with non-zero angular momentum). [ 89 ] to Nelson’s stochastic
mechanics [ 98 , 99 , 100 , 89 ] . Working with spin- @xmath particles
in which a “spin frame field” also contributes to the updating of @xmath
, @xmath becomes one of the orientation angles of the spin frame field [
45 , 13 ] (forthcoming [ 101 ] ) and the argument for @xmath having the
topology of an angle becomes more palatable. To obtain a mechanism for
updating @xmath we will extend the information metric from a discrete
simplex @xmath over @xmath } to the @xmath -dimensional ensemble phase
space (e-phase space) with extended coordinates @xmath , and then take
the continuous limit and utilize the simplectic structure of the e-phase
space. This derivation is similar in nature to [ 102 , 103 ] ; however,
the derivation presented in [ 13 ] is motivated for the purpose of doing
inference in Entropic Dynamics. First, however, in preparation for this
derivation, we will review a relevant derivation of the information
metric on a discrete statistical manifold [ 104 , 13 ] .

Due to normalization, the statistical manifold over a set of discrete
probabilities @xmath is the simplex @xmath , that is @xmath is the
equation of a simplex. One may consider changing coordinates to @xmath
and then the normalization condition takes the form @xmath , which
suggests the @xmath coordinates parameterize the surface of a sphere.
This suggestion can be taken seriously, and from it, declare that the
simplex is a @xmath sphere embedded in a @xmath -dimensional spherically
symmetric space. The generic form of a length invariant in a spherical
symmetric space takes the form,

  -- -------- -- --------
     @xmath      (4.22)
  -- -------- -- --------

where @xmath and @xmath are two arbitrary smooth and positive functions
of @xmath . Changing back to the original @xmath coordinates and letting
the probabilities be normalized to unity gives the information metric up
to an overall scale,

  -- -------- -- --------
     @xmath      (4.23)
  -- -------- -- --------

In the present derivation, we wish to consider an information metric
that is extended from the simplex to the 2 @xmath -dimensional e-phase
space @xmath by imposing the following conditions: (A) that the extended
space is compatible with the information metric on the simplex, and (B)
that @xmath has the topological structure of an angle. Condition (B)
suggests the following polar coordinate representation,

  -- -------- -- --------
     @xmath      (4.24)
  -- -------- -- --------

such that the equation of the simplex is maintained,

  -- -------- -- --------
     @xmath      (4.25)
  -- -------- -- --------

while also suggesting the space is over the surface of a sphere in 2
@xmath dimensions. To satisfy (A) we will follow the same algorithm that
was used to find the information metric. That is, we take the “spherical
suggestion” seriously and declare spherical symmetry in the space of
@xmath ,

  -- -------- -- --------
     @xmath      (4.26)
  -- -------- -- --------

Transforming back to the coordinates @xmath and setting @xmath , gives,
again up to an arbitrary proportionality constant,

  -- -------- -- --------
     @xmath      (4.27)
  -- -------- -- --------

Taking the continuous limit gives the desired form of the e-phase space
metric,

  -- -------- -- --------
     @xmath      (4.28)
  -- -------- -- --------

The square displacement may be interpreted as a measure of
distinguishability between @xmath and @xmath in the e-phase space. As is
proven and reviewed in [ 13 ] , this metric has an extra symmetry: a
complex structure and thus has a symplectic 2-form. The symplectic
2-form leads to the canonical Hamilton-Jacobi formalism with @xmath and
@xmath being the canonically conjugate variables. The explicit
derivation is not entirely relevant for the remainder of this thesis so
it will be omitted, but it may be found in [ 13 ] .

The availability of complex structure allows for the change of
coordinates,

  -- -------- -- --------
     @xmath      (4.29)
  -- -------- -- --------

under which the metric takes a simple form,

  -- -------- -- --------
     @xmath      (4.30)
  -- -------- -- --------

In these coordinates, @xmath is the canonically conjugate momentum to
@xmath as they satisfy the following Poisson brackets:

  -- -------- -- --------
     @xmath      (4.31)
  -- -------- -- --------

and,

  -- -------- -- --------
     @xmath      (4.32)
  -- -------- -- --------

At this point we may let the arbitrary scale constant @xmath as is
common in spherical embedding derivations of the information metric [ 21
] .

##### 4.1.3 Synthesis and the Schrödinger Equation

By finding the flow in e-phase space that preserves the symplectic
symmetry, we may find a Hamiltonian and find the appropriate dynamics
for @xmath , thus, the constraints ( 4.2 ) and ( 4.3 ) may be updated at
each time step of ( 4.9 ). As it stands, the geometry of the e-phase
space is independent from ED. We must incorporate the following features
for the geometry of e-phase space to be useful and consistent with ED:
the notion of ontological particle positions, entropic time, and the
entropic dynamics update ( 4.9 ) that leads to the Fokker-Planck
equation ( 4.19 ). The synthesis of these topics with the e-phase space
geometry gives the fully equivalent Schrödinger Equation (SE) with the
added interpretation that @xmath , @xmath , and @xmath are epistemic,
whereas particle positions are ontic.

As positions are the ontic variables, and because there is no notion of
time in the e-phase space as of yet, consider the potential positional
displacements @xmath of particles and evaluate the changes to @xmath and
@xmath :

  -- -------- -- --------
     @xmath      (4.33)
  -- -------- -- --------

and therefore,

  -- -------- -- --------
     @xmath      (4.34)
  -- -------- -- --------

The form of the metric under these variations is found by substitution,

  -- -------- -- --------
     @xmath      (4.35)
  -- -------- -- --------

where,

  -- -------- -- --------
     @xmath      (4.36)
  -- -------- -- --------

or in the complex coordinates,

  -- -------- -- --------
     @xmath      (4.37)
  -- -------- -- --------

This is a tensor in the e-phase space that measures distinguishability
under small displacements. To introduce entropic time, we follow [ 13 ]
and demand that duration be a measure of fluctuations, because
fluctuations ( 4.17 , 4.18 ) dominate the dynamics as @xmath . That is,
in probability @xmath converges to @xmath for small @xmath ,

  -- -------- -- --------
     @xmath      (4.38)
  -- -------- -- --------

(from [ 98 ] section 5), so therefore in probability ,

  -- -------- -- --------
     @xmath      (4.39)
  -- -------- -- --------

where @xmath . For small duration, the expected square displacement in
the e-phase space is effectively proportioned by the particles’ expected
fluctuations, or equally well, the reciprocal of their masses .

The final synthesizing step is to demand that @xmath have the form of
the Fokker-Planck equation. This simply requires the identification of
the e-Hamiltonian @xmath with the general form of @xmath ’s, from ( 4.21
), that are compatible with @xmath . As this construction is built for
small times, @xmath turns out to be the free particle e-Hamiltonian. To
account for additional interactions, a potential term @xmath is added to
the free e-Hamiltonian and thus the full e-Hamiltonian is,

  -- -------- -- --------
     @xmath      (4.40)
  -- -------- -- --------

which is allowed, and consistent with, the form of ( 4.21 ). Due to the
symplectic structure of the e-phase space and its synthesis into the ED
analysis above, the e-Hamilton equations

  -- -------- -- --------
     @xmath      (4.41)
  -- -------- -- --------

correctly satisfy all of the required ED constraints and mechanisms. In
the complex coordinates @xmath we have,

  -- -------- -- --------
     @xmath      
     @xmath      (4.42)
  -- -------- -- --------

(the second equality is reached through integration by parts) and
therefore the e-Hamilton equation(s) are,

  -- -------- -- --------
     @xmath      (4.43)
  -- -------- -- --------

which is the Schrödinger Equation (SE),

  -- -------- -- --------
     @xmath      (4.44)
  -- -------- -- --------

In standard notation this is,

  -- -------- -- --------
     @xmath      (4.45)
  -- -------- -- --------

where one identifies @xmath as Planck’s constant, @xmath as the particle
masses, and @xmath as proportional to particle charges. The condition
for compatibility of the probabilistic and linear structure of the SE
that leads to full equivalence between ED and the QM of charged
particles is that charges are quantized [ 13 , 89 ] . Entropic Dynamics
has managed to derive general unitary evolution of pure states as an
application of the standard maximum entropy method and information
geometry, as was desired in the introduction.

At this point the standard Hilbert space formalism may be adopted to
represent the epistemic state @xmath as a vector for convenience,

  -- -------- -- --------
     @xmath      (4.46)
  -- -------- -- --------

The expression of @xmath in another basis may be interpreted as a
potentially convenient way of expressing position space wavefunctions in
ED.

#### 4.2 Mixed States in ED

The derivation of mixed quantum states in ED involves a straightforward
application of probability theory. First, consider the derivation of
pure state QM up to equation ( 4.9 ),

  -- -------- -- --------
     @xmath      (4.47)
  -- -------- -- --------

Recall that the transition probability is,

  -- -------- -- --------
     @xmath      (4.48)
  -- -------- -- --------

having the expected drifts,

  -- -------- -- --------
     @xmath      (4.49)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (4.50)
  -- -------- -- --------

The evolution of @xmath in ( 4.47 ) is generated based on the implicit
assumption that if the potentials @xmath , contributing to the expected
drift of the particle @xmath , are known with certainty – and @xmath is
given – that the form of @xmath is known with certainty. That is, given
that the initial conditions @xmath are known, we may find @xmath .

Let’s suppose, that in addition to our uncertainty in the definite yet
unknown particle positions, that there exists an uncertainty in the set
of initial conditions under which the system has been prepared. Let
@xmath represent the probability the particles evolve according to the
@xmath th set of initial conditions @xmath . The probability
distribution of interest is the joint probability @xmath , so we seek
updates of the form @xmath . This can be accomplished by considering

  -- -------- -- --------
     @xmath      (4.51)
  -- -------- -- --------

however, the situational probabilities @xmath factor out,

  -- -------- -- --------
     @xmath      (4.52)
  -- -------- -- --------

which means that for all @xmath ,

  -- -------- -- --------
     @xmath      (4.53)
  -- -------- -- --------

We therefore may evaluate the entropic dynamics update of @xmath
particles in the @xmath th preparation @xmath independently from the
other preparations. Because the original entropic dynamics that leads to
pure state QM is indeed a special case of this entropic dynamics when
the initial conditions @xmath are known with certainty, we may follow
the exact derivation of pure state QM from ( 4.9 ) onwards and arrive at
the SE for the @xmath th preparation,

  -- -------- -- --------
     @xmath      (4.54)
  -- -------- -- --------

Because each of the @xmath preparations are over exactly the same @xmath
particles, intrinsic particle qualities having to do with @xmath , such
as masses @xmath and the quantized charges @xmath , are independent of
the initial conditions; i.e, we may not know @xmath but we do know that
the masses and charges are independent of @xmath . The index @xmath is
left on the external vector potential and the scalar potential @xmath to
include instances in which the observer in question does not know, as
part of @xmath , the potential used in the preparation procedure of
@xmath . Because the inference of @xmath is over the same @xmath
particles, the scalar potential is expected to factor @xmath into an
internal potential @xmath (like particle-particle Coulomb interactions
between the @xmath particles), which are independent of @xmath , and the
externally prescribed potential @xmath that does depend on @xmath . If
the observer in question is the one applying the external potential(s),
then the @xmath indices on the potentials drop out, but @xmath retains
its index through @xmath and @xmath .

At this point the standard Hilbert space formalism may be adopted to
represent the epistemic state @xmath as a vector,

  -- -------- -- --------
     @xmath      (4.55)
  -- -------- -- --------

If the observer in question is interested in calculating the probability
that the set of @xmath particles are located at the point @xmath in
configuration space, it is

  -- -------- -- --------
     @xmath      (4.56)
  -- -------- -- --------

and we may extract the density matrix @xmath as a matter of convenience.
Expectation values are treated in the usual way by introducing the
trace,

  -- -------- -- --------
     @xmath      (4.57)
  -- -------- -- --------

Probabilities are expectation values of projectors @xmath . One should
note that in general,

  -- -------- -- --------
     @xmath      (4.58)
  -- -------- -- --------

meaning the probability @xmath that the particles were prepared in
@xmath , is not equal in general to the probability that the state could
end up being inferred as @xmath , due to the fact that @xmath in
general.

The time derivative of this density matrix is the quantum Liouville
equation @xmath , thus, using ED, it can be interpreted as an
application of inference. This completes the derivation of density
matrices in ED with static probabilities @xmath , which is sufficient
for this thesis. A future topic of research is to recast the general
dynamics of density matrices as an application of inference using ED.

Now that density matrices are specified in ED, one may go ahead and use
the quantum relative entropy,

  -- -------- -- --------
     @xmath      (4.59)
  -- -------- -- --------

for the purpose of updating density matrices as we did in earlier
Chapters. It is now clear that because QM was derived from standard
probability theory, that the quantum maximum entropy method cannot be a
generalization of “probability updating” and that density matrices
cannot be “generalizations of probability”. Rather than density matrices
being generalizations, they are “particularizations”, as their function
is specifically designed for describing quantum systems [ 45 ] . All
probability updating in this instance has been accomplished by the use
of the standard relative entropy and information geometry in ED. This
will be discussed further in the next chapter on Quantum Measurement in
ED.

### Chapter 5 Solution to the Quantum Measurement Problem in Entropic
Dynamics

This chapter follows [ 14 , 3 ] and expands upon a few sections in [ 3 ]
. ¹ ¹ 1 Reference [ 3 ] extends the treatment to von Neumann, weak
measurements, and fully specifies the solution to the preferred basis
problem. In the previous chapter, Quantum Mechanics (QM) was derived as
an application of probability updating and information geometry in
Entropic Dynamics (ED). ED itself is an inference framework that is
general enough to process arbitrary information in the form of dynamical
constraints on the probability distribution in question. In standard QM,
wavefunctions follow one of two modes of dynamical evolution that are
usually considered to be detached from one another: the Schrödinger
Equation (SE), which evolves states unitarily from one pure state to
another, and its discontinuous collapse once a detection has been made [
42 ] . In ED, these modes of evolution are both described as instances
of entropic probability updating and are therefore two sides of the same
coin [ 14 ] . In the sense of [ 7 , 8 , 9 ] , there is only one
universal probability updating mechanism (entropic updating), and
therefore in ED there is no reason to privilege one instance of
probability updating (SE or collapse) over another.

As is mentioned in the preamble of the previous chapter, in an inference
framework such as ED, the common reference to “observables” is
misguided. “Observables” should be replaced by Bell’s term “beables” [
15 ] for ontic elements such as particle positions, and “inferables” for
those epistemic elements associated to probability distributions [ 3 ] .
² ² 2 Although, beables are inferables too. This distinction between
ontological and epistemic variables is essential toward the future
development of this thesis, and indeed using more rigorous verbiage
leads to a clearer interpretation of QM [ 15 , 95 ] . Ultimately, it is
these notions that prevent ED from being ruled out by any of the
aforementioned QM no-go theorems [ 4 ] , as well as providing clear
interpretations of the quantum measurement process in ED [ 14 , 3 ] .

In ED it is possible to infer “observables” other than position, e.g.
momentum, energy, and spin [ 14 , 3 ] . While positions are the only
ontic elements, other “observables” are purely epistemic; they are
properties of a probability distribution (or equally well an epistemic
wavefunction), not of the particle [ 14 ] . These ideas can be pushed to
an extreme when discussing the notion of a Weak Value [ 43 , 105 , 106 ]
of an operator. The Weak Value of an operator @xmath in which the system
is prepared in an initial state @xmath and “post-selected” in state
@xmath is defined to be,

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

The Weak Value of an operator may take values far outside the acceptable
range of eigenvalues while also being complex in general. They are
therefore not Hermitian observables, and yet, they can still be inferred
. Interpreting Weak Values as being part of the ontology leads to
paradoxes, which are usually advertised in the titles of various Weak
Value articles: “How the result of a measurement of a component of the
spin of a spin-1/2 particle can turn out to be 100” [ 43 ] , ‘‘The
quantum pigeonhole principle and the nature of quantum correlations” ³ ³
3 From [ 107 ] , “The classical pigeonhole principle states that if
@xmath objects are placed in @xmath separate boxes, then at least one
box must contain more than one object. In [ 108 ] it is argued that
quantum systems do not obey this principle. ” [ 108 , 107 ] , as well as
the “Quantum Cheshire Cats” [ 109 ] in which properties of a particle
(its spin in this case) seemingly travels along one arm of an
interferometer while the particle itself travels along the other.
Although stimulating, many of these paradoxes can be resolved by
treating a Weak Value as nothing more than a potentially interesting
epistemic inferable, which we will do later in this chapter. It should
be noted that weak measurement and Weak Values have been used as a
practical amplification technique given a large number of measurements
are made [ 43 , 106 ] . At this point we will take a step back from
Entropic Dynamics and review the quantum measurement problem(s).

#### 5.1 The Quantum Measurement Problems

There are two measurement problems outlined in [ 41 ] : the first is the
problem of definite outcomes , and the second is the problem of
preferred basis (or degenerate basis). We will introduce them here:

The problem of definite outcomes stems from the difference between how
Quantum Mechanics (QM) describes the world, and how the world is
described in everyday experience. In QM, particles evolve from one pure
state to another, and in some sense never “settle down” to a definite
final state, in all but the most trivial cases. This is in stark
contrast to our everyday experience and the detected results of quantum
mechanical experiments. Although the experimental results match the
predictions of QM in probability, they fail to match in formalism.
“Wavefunction collapse” is usually tacked onto the formalism ad hocly to
cover the blemish of QM’s lack of definite outcomes. The quantum
formalism fails to predict when its unitary evolution will halt ⁴ ⁴ 4
“Halt”, precisely in the sense of computability theory. and collapse the
state, as was given in the 2012 paper “Quantum measurement occurrence is
undecidable” [ 110 ] .

The second measurement problem, the problem of preferred basis or
degenerate basis , is a bit more technical in nature. In the von Neumann
measurement scheme, the system we would like to measure, @xmath , is
entangled with a “pointer variable” that indicates the state of a
measurement device. The pointer variable is treated quantum mechanically
and correlated with the system of interest in such a way that by
detecting its state, we may infer the state @xmath with certainty.
Starting the pointer variable in a “ready state” @xmath , it is
entangled with the system of interest via a unitary time evolution,

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

This is the von Neumann measurement procedure. A detection that finds
the pointer variable in state @xmath seemingly allows the observer to
infer that the system of interest has become the state @xmath ; however,
the catch is that the system and measurement device evolve into a
special entangled state called a “biorthogonal” state [ 41 ] , which has
the form of a Schmidt decomposition. The problem is, without a proper
specification of the ontology, the right hand side of ( 5.2 ) can be
expanded in other bases, and due to possible degeneracies in the
probabilities of the measurement outcomes and the unspecified ontology
of the pointer variable, it can be unclear what state was actually
detected, and therefore, what state was inferred – this specifies the
problem of preferred basis [ 41 ] . These degenerate basis raise the
question of which basis should be the preferred basis , i.e. the basis
which is actually present in the ontology of the quantum experiment. A
pedagogical example is that an entangled spin state @xmath in @xmath may
be expanded into the eigenbasis of @xmath , @xmath , which both have an
identical probability spectrum. There is no ontological matter of fact
about which outcome has been obtained in the measurement process (
@xmath or @xmath ) because both instances are probabilistically
indistinguishable. There is no reason to believe that @xmath pointer
states should be more ontological than @xmath pointer states, or the
reverse, and take the privileged role of the preferred basis in general
measurement setups. Thus, because the standard quantum formalism fails
to specify the preferred basis , quantum measurement procedures are
insufficient in general.

#### 5.2 Position and Inference: The Solution to the Quantum Measurement
Problem in Entropic Dynamics

The two major quantum measurement problems outlined in [ 41 ] , and
introduced above, are the problem of definite outcomes (collapse) and
the problem of preferred basis or degenerate basis . Solutions to the
measurement problem are often called “interpretations of QM”, the idea
being that all such “interpretations” agree on the formalism and thus
the experimental predictions [ 111 ] . Other solutions involve making
modifications to Quantum Mechanics [ 111 ] . Entropic Dynamics presents
a third type of solution to the quantum measurement problem(s). By
showing that QM is a subset of the available inference applications in
entropic inference, representing collapse through inferential Bayesian
updates is self-contained within ED’s theoretical framework [ 14 , 3 ] .
⁵ ⁵ 5 Reference [ 3 ] extends the treatment to von Neumann, weak
measurements, and fully specifies the solution to the preferred basis
problem. Furthermore, stating that the positions are the ontological
variables of interest, and updating probabilities in a way that is
consistent with this notion, ⁶ ⁶ 6 Specifically equations ( 4.1 ) - (
4.5 ), and ( 4.9 ). solves the problem of preferred basis. This matches
the conclusion in [ 112 ] that the preferred basis must be supplied as
an additional postulate outside of quantum mechanical law, and indeed,
this is how the preferred basis is prescribed in ED [ 14 , 3 ] . Thus,
Entropic Dynamics solves the quantum measurement problems in a way that
afflicts other interpretations of QM (and their solutions to the quantum
measurement problems).

How it is that ED is not ruled out by the Bell-Kochen-Specker Theorem ⁷
⁷ 7 The Bell-Kochen-Specker Theorem is a no-go theorem that rules out
interpretations of QM that assign ontological status to the eigenvalues
of operators that simultaneously belong to multiple (noncommuting) sets
of internally complete sets of commuting observables. The interpretation
of the Bell-Kochen-Specker theorem is that operators in QM are
contextual , meaning that their character or value depend on the
remaining set of commuting observables in a measurement setting. and
other no-go theorems is discussed in the next chapter. The remaining
measurement problems in ED are to describe collapse and the inference of
inferables other than position. Due to the nature of “inferables” in ED,
we are able to interpret Weak Values in the weak measurement scheme as
interesting epistemic inferables, which will be discussed later.

##### 5.2.1 Detection as an Entropic Update: The “Collapse” of the
Wavefunction

The “problem” of wavefunction collapse is never truly encountered in ED
for the same reason that it is not encountered in epistemic “classical”
probability theory. No one asks how the probability distribution of a
die role collapses during measurement; this just follows from the
inductive logic expressed by Bayes Rule, which again, is a special case
of entropic probability updating [ 7 , 8 , 9 ] . If a particle with
wavefunction @xmath is detected at @xmath with certainty, the prior
probability @xmath is updated to @xmath – the entropic dynamics updating
scheme comes to a halt and addresses the new data via the standard
maximum entropy method reviewed in previous chapters. What should be
emphasized is that, in Entropic Dynamics, both the unitary evolution of
the wavefunction and its collapse are probability updates compatible
with entropic inference – probability is always updated with respect to
the available information. A question of interest in Entropic Dynamics
is “What is the inference procedure for detecting particles?”. This
requires the specification of measurement devices in ED.

As is discussed in the preamble of the previous chapter, detectors are
themselves made of particles with definite positions. Let the internal
state of a detector be represented by the positional configuration of
its constituting particles @xmath (the bold face is to differentiate
this @xmath from the differential @xmath ). As the particles of interest
in ED also have ontological positions @xmath , the detector’s particles
with ontological positions @xmath may be described equally well in ED,
and positional correlations between the system and measurement device
@xmath may be generated and capitalized upon by the observer for the
purpose of inference. In principle, we have access to the joint
probability @xmath as it can be generated from the SE. Correlations of
this type are possible in ED because detectors ultimately consist of
particles, and thus, their inner workings may be described purely from
that basis. For instance, one may describe a voltmeter as measuring
voltage in terms of the displacements of the voltmeter’s internal
particle configurations (a current that generates an amplified signal in
the sense below). The inclusion of the positional detector states @xmath
into the analysis further actualizes the notion of a detector given in [
14 , 3 ] ; however, the final results are the same.

The internal mechanisms of a detector that lead to a detection can be
expressed “classically” as an amplification [ 14 ] , and so, we may let
the signal amplification of a detector be represented (for the purpose
of inference) by an “amplification” likelihood function @xmath that
gives the probability of a (usually macroscopic) detection signal @xmath
when its particles have transitioned from a ready configuration @xmath
to a final configuration @xmath . The likelihood functions @xmath are
capable of representing a large class of detectors from CCD cameras, to
bubble chambers, or even our own eyes, each having various likelihood
functions specific to the effectiveness of the amplification process of
the detector in question. The full measurement inference includes
amplified detection signals @xmath , detector states @xmath , and the
system of interest @xmath . This is incorporated into the inference
procedure by considering a larger space of variables @xmath .

Here is a quick summary of the discussion so far: the particles of
interest with configuration space coordinate @xmath are correlated with
the positions @xmath of particles that are considered to be part of the
“detector”. These correlations result in the joint probability @xmath ,
that may be generated using QM. Furthermore, for the convenience of the
observer, the final result of @xmath is amplified through the detector,
which, by design, gives the signal @xmath . The effectiveness of
conveying the result of @xmath through @xmath is represented by the
amplification likelihood function @xmath . The entire inference process
may be stated by the joint probability, @xmath . The conditional
probability @xmath is independent of @xmath in general because, by
construction, @xmath is some probabilistic function of @xmath , and thus
@xmath cannot give any more information about @xmath than can @xmath
through the conditional probability @xmath that was generated using QM.
Thus the joint probability of interest takes a slightly simplified form
@xmath .

An ideal device has amplified signals @xmath that are in a one to one
correspondence with @xmath such that @xmath , and @xmath . Ideal
amplification of this type is impossible to implement in practice when
the number of relevant detector particles, having the configuration
space coordinate @xmath , is macroscopic in number. However, if the
relevant number of detector particles is manageable, for instance when
detector particles are initialized in an unstable equilibrium that when
perturbed results in a chain reaction that amplifies its signal, the
number of relevant macroscopic (amplified) detector signals @xmath are
manageable. We will assume these probability relationships to hold for
now as it leads to the description of an ideal measurement device.

The purpose of @xmath is to provide the observer with a convenient
macroscopic interface for detection so the observer doesn’t need to
concern themselves with the potentially complicated internal detector
states @xmath . These states may be marginalized over and indeed give
@xmath , where @xmath is equal in value to the correlations originally
presented by @xmath , if the efficiency of the detector amplification
@xmath is ideal. The proof can be seen by writing the marginalization in
the following form,

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (5.3)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

Here we see the likelihood function for the detector particle
(configuration) is @xmath , which is equal to that of its amplified
signal, and in some sense, the correlations between @xmath have
propagated to @xmath through the amplification process. Given an
idealized amplification process, the probability that the particle was
at @xmath given the amplified signal @xmath is,

  -- -------- -- -------
     @xmath      (5.4)
  -- -------- -- -------

by Bayes rule, which may be generated using entropic methods [ 7 , 8 , 9
] (reviewed in the introduction of Chapter 3 ). An idealized inference
process allows one to detect the presence of a particle in an
infinitesimally small region centered around @xmath , that is, ( 5.4 )
is @xmath . This requires both the amplification likelihood function to
be ideal @xmath as well as an ideal detector particle configuration
conditional probability @xmath . Correlations such as @xmath are sought
after by the observer and this is indeed why the von Neumann measurement
process is valuable.

In most physical situations there is a lack of efficiency in the
amplification process such that @xmath . There are many such examples of
non-idealized amplification likelihood functions @xmath : a Gaussian (or
other) distribution for @xmath having an average value @xmath , a CCD
camera that clicks with certainty if @xmath is within a certain interval
⁸ ⁸ 8 Unlike the maximally efficient detector whose likelihood function
is a delta, the likelihood function for a CCD camera @xmath is an
indicator function. , or any such detector that has signals @xmath that
are a surjective function of the detector particle configurations (in
practice @xmath maps @xmath from its high dimensional space to @xmath ’s
lower dimensional space). In any of these cases, the Bayes update
becomes an instance of Jeffreys Rule,

  -- -------- --
     @xmath   
  -- -------- --

such that a detection at @xmath specifies @xmath with probability @xmath
. Even if @xmath and @xmath are equally correlated @xmath , here and in
the ideal case, the lack of efficiency in signal amplification on part
of the detector @xmath leads to further uncertainty in the final
position of @xmath ( LABEL:Jeff ). It should be noted that the type of
uncertainty presented in signal amplification @xmath is not necessarily
quantum mechanical in origin. Jauch was able to show this
experimentally, his measurements of position had less uncertainty than
did the particle’s quantum mechanical statistical uncertainty [ 113 ] ,
i.e., you can detect the position of a particle with a finer precision
than the original quantum statistical uncertainty permits. This
statement is as simple as differentiating between the statistical
uncertainty of a coin flip and our ability to detect that it indeed
landed on heads or tails with certainty. Detection and collapse in ED
are therefore characterized by ( 5.4 ) and ( LABEL:Jeff ).

###### Compatibility with the the quantum maximum entropy method:

The explanation presented in [ 3 ] for collapse, and reiterated above,
is completely consistent with the quantum maximum entropy method [ 2 ]
from Chapter 3 . This is because the data update of the density matrix
@xmath using the quantum maximum entropy method is simply a
probabilistic update of its components @xmath . In the language of
density matrices, the prior pure state @xmath , with probability @xmath
, needs to decohere @xmath with the measurement device before it could
be collapsed using the quantum maximum entropy method to avoid the PDMT.
This step of decoherence may be expressed using the standard QM tools
from Chapter 3 in the ED of mixed states formalism, or one can simply
skip to a more probabilistic explanation as was given in the above
discussion as the positional detector states @xmath are marginalized
over. The states @xmath are in principle the same detector states from
Chapter 3 . If however, the prior probability between @xmath and the
macroscopic detector states @xmath are already known @xmath , one could
simply use the standard maximum entropy method with the appropriate data
constraint @xmath , to derive the appropriate Bayes or Jeffreys rule [
14 , 7 , 8 , 9 ] – all of which are available tools within the ED
framework. Depending on the type of detector, the final inferred
state(s) of the particle(s) in question may continue to evolve under the
same or a new Hamiltonian, by some other inferential means, or perhaps
be thrown away entirely in preparation for the next experiment.

##### 5.2.2 Unitary Measurement Devices

A question of interest answered in [ 14 , 114 ] is, “How can we measure
(infer) observables (or in the present case “inferables”) other than
position (given that position is the preferred basis) in ED?”. The
subtext has been added for clarity and flow in this thesis, and the
surprisingly simple arguments of [ 14 , 114 ] are reviewed here.

Consider the wavefunction of a single particle living on a discrete
lattice,

  -- -------- -- -------
     @xmath      (5.6)
  -- -------- -- -------

As the particles have ontological positions in ED, @xmath gives the
probability the particle is at location @xmath . In a more
“complicated” measurement, the particle is subject to additional
interactions right before reaching the position detector. Let such a
setup @xmath be described by a particular unitary time evolution @xmath
that is designed to take particles from an initial state @xmath to the
position @xmath on the discrete lattice with certainty – that is, @xmath
. The unitary measurement device is the analog of a light prism; it
takes well defined momentum states of particles and deflects them onto a
screen for position detection. Since the set @xmath is orthonormal and
complete, the set @xmath is also orthonormal and complete. To figure out
the effect of @xmath on some generic initial state vector @xmath ,
expand the state of interest in the basis of the inferables of interest
@xmath , where @xmath . Then the state @xmath evolves according to
@xmath into a new state at a later time @xmath ,

  -- -------- -- -------
     @xmath      (5.7)
  -- -------- -- -------

which, invoking the Born rule for position measurements, implies that
the probability of finding the particle at the position @xmath at a
later time is @xmath , which is equal to the probability of @xmath at an
earlier time. These positional probabilities then may evolve with @xmath
on the screen in the above section and one may collapse the system and
infer an original positional state @xmath . That is, directly from [ 3 ]
(and inspired by [ 14 , 114 ] ), “From a physics perspective there is
nothing more to say but we can adopt a different language : we can say
that the particle has been “measured” as if it had earlier been in the
state @xmath ” – although, in-fact, it was not, because the full state
at that time was a superposition of @xmath ’s. Thus, the setup @xmath is
a device that in principle “measures” all operators of the form @xmath
where the eigenvalues @xmath are arbitrary scalars. Note that there is
no implication that the particle previously had or now currently has the
value @xmath . In the context of this thesis and in ED, the states
@xmath and operator @xmath are inferables of the theory that are
inferred from detections of the preferred ontological position basis.

If one wants to infer a continuous variable from a state like @xmath one
uses a unitary device @xmath with the property @xmath . A change of
variable is required @xmath , where @xmath is the appropriate monotonic
correspondence function of the unitary device (for simplicity consider
@xmath , where @xmath converts and scales what positions on a screen
correspond to what values of @xmath at an earlier time). This amounts
to,

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (5.8)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

such that @xmath and therefore @xmath at the later time @xmath . Again
Bayes Rule may be used as an instance of collapse as is specified above.
Observables of the form @xmath are thus inferred from position.

##### 5.2.3 Von Neumann Measurements in Entropic Dynamics

This subsection discusses the method for generating correlations between
the detector states @xmath and the inferables of interest using a von
Neumann measurement procedure in ED. As well, this discussion provides
an example that shows how both the measurement problems of definite
outcomes and preferred basis are resolved within the ED framework.

Given a state of interest @xmath , which has been expanded in the basis
of the inferables of interest @xmath , a von Neumann measurement is one
in which @xmath evolves with an auxiliary “pointer variable” (or
detector) state, @xmath , of which both become entangled, such that
@xmath may be inferred from detections of @xmath . In ED, the natural
pointer variables are the positions of (detector) particles as position
is the preferred basis. These “pointer variables” @xmath may be
interpreted as the internal positional states of a detector or as the
position of a single particle being fed into a detector, from Section
5.2.1 . The resulting state from a von Neumann measurement is the
entangled state,

  -- -------- -- -------
     @xmath      (5.9)
  -- -------- -- -------

that’s unitary evolution will be discussed now.

Initially, the state of interest is @xmath and the pointer variable is
in its “ready” state @xmath . The initial joint system is represented by
the tensor product of the initial states @xmath . The pure state evolves
with the detector states @xmath via a unitary evolution to a final
entangled state @xmath ,

  -- -------- -- --------
     @xmath      (5.10)
  -- -------- -- --------

The general form of this unitary operator is,

  -- -------- -- --------
     @xmath      (5.11)
  -- -------- -- --------

with the sub-block matrices,

  -- -------- -- --------
     @xmath      (5.12)
  -- -------- -- --------

We define a good detector as one in which the @xmath th state only
entangles with the @xmath th positional state of the detector, which is
an argument for the sub-block matrix to take a simple form,

  -- -------- -- --------
     @xmath      (5.13)
  -- -------- -- --------

This then gives a fully entangled (von Neumann measurement) state at a
later time,

  -- -------- -- --------
     @xmath      (5.14)
  -- -------- -- --------

Because position plays the role of the preferred basis in ED, the state
does not have degenerate bases and the measurement problem of preferred
basis is solved – particle or particles are ontologically present at one
of the locations @xmath . This solution is equally valid for the beable
particle positions in Bohmian Mechanics. Although ED does assume that
particles have definite yet unknown (and ontological) positions from the
start, this assumption is carried throughout, and in-fact, helps guide
the derivation of QM from ED. The probability updating scheme evolves
probabilities @xmath in a way that is consistent with notion that
particles having definite yet unknown, and ontological, positions as can
be seen in the discussion before, around, and through equations ( 4.1 )
- ( 4.5 ), and ( 4.9 ).

Given the von Neumann measurement above in ED, the joint probability is,

  -- -------- -- --------
     @xmath      (5.15)
  -- -------- -- --------

which is normalized @xmath . Given ideal amplification @xmath from
Section 5.2.1 , and that the value of the data is @xmath , the
(standard) entropic updating procedure (from the introduction in Chapter
3 or [ 7 , 8 , 9 ] ) gives Bayes Theorem,

  -- -------- -- --------
     @xmath      (5.16)
  -- -------- -- --------

which is collapse in Entropic Dynamics [ 14 , 3 ] . For a von Neumann
measurement, a detection of @xmath allows a precise inference of the
inferable in question. The resulting state is accurately describable by
a particular eigenvector @xmath , which is understood to be a convenient
representation of its position space wavefunction @xmath .

Again we emphasize that in Entropic Dynamics, both the unitary evolution
of the wavefunction and its collapse are both probability updates
compatible with entropic inference. The state evolves unitarily when the
information available is with respect to the continuous positional
expected drift of the particles (i.e. it undergoes the entropic dynamics
update with updating drift potentials), and if there is new information
in the form of data, the entropic dynamics update halts, addresses the
data, and updates the current probability distribution accordingly.
Thus, the measurement problem of definite outcomes is solved using
inference methods that are completely contained within the inference
framework that is Entropic Dynamics. Both of the quantum measurement
problems outlined in [ 41 ] are therefore solved in Entropic Dynamics.

These notions can easily be extended to “weak measurements”, which we
will discuss now.

#### 5.3 Weak Measurements and Weak Values in Entropic Dynamics

This section discusses the method for generating correlations between
the detector states @xmath and inferables of interest, in ED.

##### 5.3.1 Weak Measurements in Entropic Dynamics

To avoid clutter we will set @xmath and save the normalization factors
until the end of the calculation. An ideal pointer variable, or detector
state, in ED has a definite initial state in position space,

  -- -------- -- --------
     @xmath      (5.17)
  -- -------- -- --------

i.e., @xmath and @xmath approaches a delta function. We consider a more
general case in which the initial state is,

  -- -------- -- --------
     @xmath      (5.18)
  -- -------- -- --------

where @xmath is some finite volume, which reproduces the ideal
measurement device when @xmath after normalization. Using the
completeness relation @xmath , the state of the pointer particle can be
represented in momentum space as,

  -- -------- -- --------
     @xmath      (5.19)
  -- -------- -- --------

In this section we will consider @xmath to be the position of a single
particle in 3D space. The quantum system to be entangled with the
measurement device is a preselected or prepared superposition state,
that when expanded in the basis of the inferable of interest @xmath , is

  -- -------- -- --------
     @xmath      (5.20)
  -- -------- -- --------

A particularly telling situation is one in which the measuring device
and system of interest are coupled by a coupling or interaction
Hamiltonian,

  -- -------- -- --------
     @xmath      (5.21)
  -- -------- -- --------

where @xmath is the canonical conjugate of the pointer variable, which
thereby generates translations in the pointer variable @xmath , and
@xmath is a coupling constant with compact support near the time of
measurement that integrates to a finite number [ 43 ] . We can assume
that the coupling Hamiltonian will dominate over the full Hamiltonian
for the, assumed small, period of measurement. The time evolution of our
entangled system is,

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- -- --------
     @xmath      (5.22)
  -- -------- -- --------

which, in the position space representation, is

  -- -------- -- --------
     @xmath      (5.23)
  -- -------- -- --------

This is a superposition of potentially overlapping Gaussian
distributions that have peaks at the eigenvalues of @xmath . When the
Gaussian wavefunctions overlap we have a so-called “weak measurement” [
43 ] ; when the Gaussian distributions are neatly resolved we have a
“strong” or von Neumann measurement ( @xmath ) [ 43 ] . The joint
probability of the system is,

  -- -------- -- --------
     @xmath      (5.24)
  -- -------- -- --------

In the present case, we may infer the probability the system of interest
is accurately describable by a particular eigenvector @xmath by making
detections of the position of the pointer particle and entropically
collapse the system following Section 5.2.1 . The result is given by
Bayes Rule,

  -- -------- -- --------
     @xmath      (5.25)
  -- -------- -- --------

Thus a detection of @xmath (generally macroscopic) allows us to infer
the probability that the system is accurately describable by @xmath .
Bayes Rule leads to a “weak” collapse of the state of knowledge of
@xmath because the initial state of the pointer variable @xmath is
itself uncertain. As is pointed out in [ 43 ] , by making repeated
measurements on identically prepared weak measurement states, one can
infer @xmath with certainty. In the limit @xmath and a detection @xmath
, we can infer the eigenstate @xmath with probability 1 using ( 5.25 ) –
this results in a full state collapse in ED.

So far we have taken into account the uncertainty in the preparation of
the pointer variable but we have assumed that the pointer variable
@xmath has been amplified with 100% efficiency. If the detector is not
completely efficient such that @xmath then there is a second source of
uncertainty that contributes in addition to the initial uncertainty of
the pointer variable in ( 5.18 ). The probability the state is
accurately described by @xmath , if the detection is noisy, is given by
Jeffreys Rule,

  -- -------- -- --------
     @xmath      (5.26)
  -- -------- -- --------

which we see only gives the same state of knowledge as ( 5.25 ) if
@xmath is an ideal amplification, otherwise the detector introduces
extra uncertainty (as is discussed in Section 5.2.1 ). For instance, a
CCD camera’s likelihood function might be uniform over the width of the
corresponding pixel, which changes the probability of @xmath after a
detection @xmath of that pixel.

It should be noted a more robust set of operators @xmath can be measured
in ED by compounding a unitary device @xmath , from ( 5.7 ), with the
weak measurement. This is a combination of a unitary measurement device
with the weak measurement scheme, and doing this maps a non-position
pointer variable @xmath to a position pointer variable @xmath for
inference. Consider the coupling Hamiltonian

  -- -------- -- --------
     @xmath      (5.27)
  -- -------- -- --------

where @xmath is the Fourier conjugate to @xmath such that @xmath (for
instance @xmath where @xmath has arbitrary units compatible with @xmath
). Evolution under this Hamiltonian entangles the states,

  -- -------- -- --------
     @xmath      (5.28)
  -- -------- -- --------

which, in the @xmath space representation,

  -- -------- --
     @xmath   
  -- -------- --

Sending the pointer variable state through a unitary measurement device
@xmath with the property @xmath and a simple device correspondence
function @xmath [ 14 ] , gives,

  -- -------- -- --------
     @xmath      (5.30)
  -- -------- -- --------

This is again a superposition of potentially overlapping Gaussian
distributions having peaks at the eigenvalues of @xmath . Detections of
@xmath at @xmath allow us to infer the most likely @xmath at @xmath ,

  -- -------- -- --------
     @xmath      (5.31)
  -- -------- -- --------

Measurement device uncertainty can be included using the same arguments
from ( 5.26 ). The mechanics for the Stern-Gerlach experiment is well
known [ 115 , 43 , 105 ] ; however a brief discussion of the inference
of spin from position detections (including weak measurements) is given
in Appendix D .

##### 5.3.2 Exotic Inferables and Weak Values in Entropic Dynamics

The Weak Value @xmath of a Hermitian operator @xmath was first
introduced by Aharonov, Albert, and Vaidman [ 43 ] (AAV) in 1989 as an
interesting application of a weak measurement that gives nonintuitive
results (recent review [ 106 ] ). What is particularly significant in
AAV’s paper [ 43 ] is not that they defined an odd quantity associated
to @xmath , but rather they found, after a series of approximations (see
[ 105 ] ) a way in which @xmath could be “measured.” A Weak Value is a
complex number which may lie outside the set of eigenvalues of @xmath
when @xmath is sufficiently small. Due to this, the interpretation of
Weak Values has had a “colorful history” [ 106 ] , much of which can be
summarized by the question, “Are Weak Values ontic properties of a
particle?”. The answer given by ED is negative in that respect – Weak
Values are inferables of the theory.

###### Postselection:

The method for inferring a Weak Value involves two steps: first the
system must undergo a weak measurement to couple @xmath as in ( 5.23 )
and then postselect the final state of the system of interest. A
postselection is in principle no different than a preselection (in the
sense of [ 116 ] ), which is the appropriate filtering and selecting of
an initial quantum state except it happens after, rather than before,
the system is correlated with the pointer variable [ 43 ] . Because the
system of interest’s final state is known @xmath when successfully
postselected to @xmath , the probability of the pointer variable taking
a value @xmath is,

  -- -------- -- --------
     @xmath      (5.32)
  -- -------- -- --------

in agreement with the analysis in [ 117 ] that concludes that
postselections may be represented simply through conditional
probability.

###### Weak Values:

In ED, variables other than position either: are inferred from position,
are useful parameters in the position probability distribution of the
particle, or may be a particularly convenient basis for representing the
position space probability distribution @xmath . From this perspective
it is clear that inferring @xmath does not indicate that the particle is
ontically expressing @xmath , in the same way as inferring the momentum
from position detections does not imply it is ontic. Consider the final
state of an entangled joint system from ( 5.22 ) that has been
postselected into a peculiar state @xmath , such that,

  -- -------- -- --------
     @xmath      
     @xmath      (5.33)
  -- -------- -- --------

which in the position space representation is,

  -- -- -- --------
           (5.34)
  -- -- -- --------

given the postselection is in the desired range of validity [ 105 , 106
] . In the ED framework we are interested in the probability
distribution of @xmath postselected into @xmath ,

  -- -------- -- --------
     @xmath      (5.35)
  -- -------- -- --------

and note the real part of the weak value @xmath is taken as a feature of
the probability distribution of @xmath . Because @xmath appears as a
parameter in the probability distribution of @xmath , we may consider
@xmath and invert the problem to ask, “what is the probability the
parameter @xmath has a certain value given a detection of the pointer
particle at @xmath ” – that is we may use @xmath and the parameter
estimation scheme in [ 21 ] to find @xmath in agreement with [ 43 ] .
Equally well we can find the imaginary part of the Weak Value by using a
unitary measurement device ( 5.7 ). Consider using @xmath with a simple
correspondence function @xmath ,

  -- -------- -- --------
     @xmath      (5.36)
  -- -------- -- --------

and after completing the square and normalizing one finds,

  -- -- -- --------
           (5.37)
  -- -- -- --------

where @xmath is the imaginary part of @xmath and again after many
detections, @xmath and thus @xmath may be inferred in full.

There are several potentially interesting Weak Values, but in particular
consider that the operator @xmath postselected in the zero momentum
state,

  -- -------- -- --------
     @xmath      (5.38)
  -- -------- -- --------

is proportional to the full wavefunction, where @xmath is a constant
that will be removed after normalization [ 118 ] . Lundeen et al show
that the real and imaginary parts of @xmath are proportional to the
position and momentum shifts of the pointer variable and claim they are
“directly measuring the wavefunction”. From the ED perspective it is
more appropriate to say that the value of the wavefunction at each
@xmath is being “directly” inferred. If the complex valued @xmath is
inferred with certainty then it is possible to also solve for the phase
of the wavefunction @xmath (up to an additive constant @xmath ), and
thus, @xmath is also an inferable. This provides a link to Wiseman’s use
of Weak Values to measure the probability current in Bohmian Mechanics [
119 ] .

Especially in cases as exotic as the above, ED takes the standpoint that
Weak Values and quantities other than position (energy, momentum, spin,
etc.) are best considered as epistemic “inferables” rather than ontic
beables or observables.

#### 5.4 POVM Measurements in Entropic Dynamics

A POVM measurement can be looked at as a generalization of a von Neumann
measurement, where detections result in the inference of a density
matrix rather than a pure state. As POVM measurement can be described
using the designed quantum maximum entropy method [ 1 , 2 ] , it can
naturally be incorporated into Entropic Dynamics. In particular, the
Section on the Quantum Bayes Rule (QBR) from Chapter 3 can be added into
the ED framework wholesale with the requirement that the @xmath ’s and
@xmath ’s which appear in the QBR are in-fact particle positions. The
resulting density matrix in the Quantum Bayes Rule ( 3.56 ), @xmath , is
the inferable that results from the position detection @xmath after a
POVM measurement. This is analogous to the inferable @xmath that results
from the detection of @xmath in the von Neumann measurement scheme (
5.14 ).

#### 5.5 Conclusion

Both of the quantum measurement problems outlined in [ 41 ] are
completely solved within the Entropic Dynamics framework [ 14 , 3 ] .
Both the unitary evolution of the wavefunction and its collapse are
probability updates compatible with entropic inference. The state
evolves unitarily when the information available corresponds to the
continuous positional motion of the particles (i.e. it undergoes the
entropic dynamics update ( 4.9 ) with updating drift potentials ( 4.41
)), and if there is new information in the form of data, the entropic
dynamics update halts, addresses the data, and updates the current
probability distribution accordingly. Entropic updates with respect to
data yield Bayesian probability updates [ 7 , 8 ] . Thus, the
measurement problem of definite outcomes is solved using inference
techniques that are completely contained within the inference framework
that is Entropic Dynamics.

The measurement problem of preferred basis is solved within ED due to
the assumption, and ontological treatment throughout the derivation and
measurement process, of definite yet unknown particle positions.
“Observables” other than positions are more aptly called “inferables” as
their values are inferred on the basis of position detections. The title
of “observables” is downgraded to the title of “inferables”, which plays
a central role into why ED is not ruled out by the Bell-Kochen-Specker
theorem next chapter.

### Chapter 6 Entropic Dynamics and the sense in which no-go theorems
can go again

This chapter follows [ 4 ] and continues to get back to the original
quote by Jaynes (A) in the introduction. Now that we have derived
precise notions of probability, probability updating, density matrix
updating, Quantum Mechanics (QM) and quantum measurement in Entropic
Dynamics (ED), we can address age-old no-go theorems and tactfully avoid
pitfalls that some interpretations of QM fail to do. In particular, we
will discuss the following relevant no-go theorems: Bell’s theorem, the
Bell-Kochen-Specker (BKS) theorem, and the more recent @xmath -epistemic
no-go theorem by Pusey-Barret-Rudolph (PBR), in the context of ED.

On one hand, Quantum Mechanics is hugely successful in its ability to
predict the set of eigenvalues, expectation values, and operators for a
particle system of interest. On the other hand, the states of interest
hold intrinsic unpredictability, quantified by a probability
distribution, except for a few trivial cases. This unpredictable nature,
coupled with a desire to solve the quantum measurement problem(s) which
are left open by QM’s standard formalism, leaves a space for the many
interpretations of QM to coexist inharmoniously within the community – a
community, no doubt, easily bothered by disharmony of any type.

The community reduces and organizes this disharmony by ruling out
interpretations and foundational theories of QM that disagree with the
predictable findings of QM. This is done by first making a few seemingly
reasonable assumptions a theory of QM may obey, and then by showing
these assumptions lead to contradictions in the formalism, construct a
no-go theorem. This is the basis of Bell’s theorem [ 20 ] , the BKS
theorem [ 17 , 18 , 19 ] , as well as the findings of
Pusey-Barret-Rudolph (PBR) [ 16 ] (reviewed in [ 120 ] ) on the
epistemic interpretation of the wavefunction.

Bell’s theorem and the BKS theorem are intimately connected -- the
failure of one sometimes implying the failure of the other, i.e., they
are both capable of ruling out local hidden variable theories. ¹ ¹ 1 A
review of many joint proofs of the BKS and Bell theorems, together with
references to the original papers, can be found in [ 121 ] . The
interpretation of the BKS theorem is that operators in QM are contextual
, meaning that their character (or value) depend on the remaining set of
commuting observables in a measurement setting. After considering the
final results of these no-go theorems, theories and interpretations of
QM are sometimes classified using tables, for instance the 2 by 2 table:

  -----------------------------------------------------------------------
                          @xmath -ontic           @xmath -epistemic
  ----------------------- ----------------------- -----------------------
  contextual              A                       B

  noncontextual           C                       D
  -----------------------------------------------------------------------

,

might be followed by statements like, “theories of type “C” or “D” which
have noncontextual operators are ruled out by the BKS theorem and “B” is
ruled out by PBR”. A reader may be inclined to conclude that QM must be
a theory of type “A” (potential interpretation of Bohmian mechanics or
Many Worlds). The 2 by 2 table is by its nature an over simplification;
it fails to span the entire set of plausible theories, and consequently
interpretations, of QM. This is due the fact that theorems are proofs by
contradiction, and only theories which strictly adhere to their
assumptions are ruled out.

In particular, this chapter shows that Entropic Dynamics (ED) is a
theory of QM that lies on the line between theories “B” and “D”, while
not being ruled out by any of the aforementioned no-go theorems. We
classify ED as a hybrid-contextual theory of QM because the positions of
particles are treated noncontextually, as they are the preferred basis [
14 , 3 ] , while it is shown that all other observables are treated
contextually – the main result of [ 4 ] . Although being
“hybrid-contextual”, ED is not ruled out by the BKS theorem. Concepts in
ED are naturally communicated in the language of probability, and for
this reason, the operator language used in contextuality proofs do not
naturally coincide with the language in ED – this will be touched upon
more later. We will discuss the @xmath -epistemic no-go theorem (PBR),
Bell’s Theorem, and finally the BKS theorem in the context of ED.

#### 6.1 @xmath-epistemic?

In the previous chapters we claimed that @xmath is an epistemic object
that helps represent our current knowledge of the system in question.
This seemingly runs into conflict with the @xmath -epistemic no-go
theorem from [ 16 ] ; however, there is no issue. An excellent review of
the @xmath -epistemic/ontic dichotomy is presented in [ 120 ] ; however,
the @xmath -epistemic classifications there (and in [ 16 ] ) do not meet
the exact @xmath -epistemic classification that ED formulates from the
first principles of probability and probability updating.

The first assumption in [ 16 ] is 1) that “a @xmath -epistemic system
has ‘physical states’ upon which inferences may be made” (paraphrased).
The “physical states” of a system are denoted by @xmath . ED agrees with
this assumption, and the variables which are “physical” in ED are alone
the definite yet unknown positions of particles. The second assumption
2) is that “systems which are prepared independently have independent
physical states” (PIP). The PBR theorem finds a contradiction between
assumption 2) and QM for systems which are prepared independently and
then measured in an entangled basis, i.e., the physical states have
dependencies in the entangled basis that are seemingly not present in
their preparation. Thus, throwing away assumption 2) seemingly implies
that @xmath must be ontic by their definition.

The second assumption (called the PIP) is considered to have a weak
point [ 120 ] , “In my view, the weakest part of the PIP is the CPA,
i.e., the idea that there should be no global properties of a system
that are not reducible to properties of its subsystems when it is
prepared in a product state”. It is further stated “… the only time
global properties would necessarily have to play a role is when a joint
measurement is made, e.g. a measurement in an entangled basis” (on which
the PBR theorem depends) and, “ It would still be possible to work with
separate systems completely independently of each another, in blissful
ignorance of the global properties, until we decide to do an experiment
that necessarily involves bringing the systems together.”. The
description of bringing the systems together in space is not present in
the mathematical formalism of the PBR proof. The measurement process in
the PBR is treated like a “black box”, having inputs and outputs where
nothing is discussed about what happens in the middle. Caticha [ 45 ]
finds this to be a weak point in the PBR, it fails to take Bell’s advice
about measurement – we should be careful and describe the full inference
procedure. Rather than discussing the second assumption and the entire
PBR proof, we will discuss how the notions of “physicality” and @xmath
-epistemology in [ 16 ] and ED differ, which is reason enough for our
version of @xmath -epistemic states to not be ruled out.

The conclusion of [ 16 ] is that wavefunctions are “physical properties”
of quantum systems because the second assumption that “systems which are
prepared independently have independent physical states” fails to hold
up in their definition of @xmath -epistemic states. By their definition,
a “property” @xmath is a function of the “physical states” @xmath of the
probability distribution @xmath in question. The set of properties
@xmath are considered to be “physical properties” iff @xmath and @xmath
do not overlap in @xmath . This guarantees that a measurement of @xmath
allows @xmath or @xmath to be inferred uniquely. The following classical
particle example is given in the PBR paper [ 16 ] :

  “… if an experimenter knows only that the system has energy @xmath ,
  and is otherwise completely uncertain, the experimenter’s knowledge
  corresponds to a distribution @xmath uniform over all points in phase
  space with @xmath . Seeing as the energy is a physical property of the
  system, different values of the energy @xmath and @xmath correspond to
  disjoint regions of phase space, hence the distributions @xmath and
  @xmath have disjoint supports. On the other hand, if two probability
  distributions @xmath and @xmath have overlapping supports, i.e. there
  is some region @xmath of phase space where both distributions are
  non-zero, then the labels @xmath and @xmath cannot refer to a physical
  property of the system.”

From this classical example we see that the physicality of a “property”
@xmath is determined on the basis of whether or not any physical states
@xmath are shared between @xmath and @xmath . These definitions are
considered in QM where one would like to know if the wavefunction is a
“physical property” of the system ( @xmath -ontic) or if it is not a
physical property ( @xmath -epistemic). From [ 16 ] :

  “Suppose that, for any pair of distinct quantum states @xmath and
  @xmath , the distributions @xmath and @xmath do not overlap: then, the
  quantum state @xmath can be inferred uniquely from the physical state
  of the system and hence satisfies the above definition of a physical
  property. Informally, every detail of the quantum state is “written
  into” the real physical state of affairs. But if @xmath and @xmath
  overlap for at least one pair of quantum states, then @xmath can
  justifiably be regarded as “mere” information.”

In ED, the only “physical states” and “physical properties” are the
positions of particles @xmath – all other “properties” @xmath are
“inferables”. Independent of whether two probability distributions are
overlapping in @xmath , such that @xmath or @xmath can (or cannot) be
uniquely inferred from position detections, the “properties” @xmath and
@xmath are epistemic inferables (positions are ontic however). In ED,
the ‘‘physicality” of a property is not determined by one’s ability to
make an inference with certainty. The previous chapter shows this. ² ² 2
It also shows that the wavefunction itself is an inferable through
position detections using the weak measurement and Weak Value scheme.
Where [ 16 ] finds contradiction between the notion that prepared
quantum states cannot in general be uniquely inferred from measurements
of @xmath , ED finds no contradiction – probability theory is designed
for the purpose of addressing one’s lack of complete information, and
thus, indeterminacy is commonplace in the ED framework.

In [ 120 ] the following relevant comment is made about Bohmian
Mechanics when addressed in the context of the PBR theorem:

  “The particle positions are supposed to be the things in the theory
  that provide a direct picture of what reality looks like to us, e.g.
  when we observe the pointer of a measurement device pointing to a
  specific value then it is the positions of the particles that make up
  the pointer that determine this. Nevertheless, the wavefunction is
  still needed as part of the ontology because it determines how the
  particles move via the guidance equation. The response of a
  measurement device to an interaction with a system it is measuring
  depends on the wavefunction of the system as well as the particle
  positions, so the wavefunction is still part of the ontic state, even
  if it is in some sense less primitive than the particle positions.”

Consider the following: In ED, “interactions” occur between the
positions of the particles in the system of interest and positions of
the particles in the detector. These interactions cause changes in the
detector particle’s positions, and then we make inductive inferences.
The realization that these interacting fields are intermediary , that
they are predicated in experiment between particle-based-apparatuses and
the particles of interest themselves, speaks to an interpretation that
their function may simply be a convenient representation, or set of
mechanisms, for describing peculiar positional correlations between
particles [ 45 ] . The particles themselves are “doing whatever they are
doing” and our model does its best to make inferences on the basis of
the available information. The phase @xmath is guided by the e-Hamilton
equations ( 4.41 ) and Hamiltonian, which informs us about the expected
drift of the particles through the expectation value constraints ( 4.2 ,
4.3 ), rather than dictating the motion of the particles directly –
@xmath is a location in e-phase space. The ontology of any
intermediating fields can only be specified as far as the epistemic
correlations (conditional dependencies) they build between ontic
particle positions in @xmath . The conclusion from this assessment is
not that “fields are not real”, but rather, that there is space for the
inquisition “must we demand in our model that these fields are real?” -
the answer in ED is “no, particles with peculiar probabilistic
correlations is enough”. In the absence of QED in ED, this is just a
conjecture, but regardless, the conjecture works well enough for the
model of QM in ED [ 45 ] .

As the leading assumptions of what entails a @xmath -epistemic state
differ, the @xmath -epistemic no-go theorem does not apply, which is
admitted as a possible exemption to their no-go theorem in the
conclusion of [ 16 ] . We are therefore justified in treating @xmath
epistemically by our own definition – that @xmath is a convenient
representation for epistemic probability distributions @xmath and how
@xmath is inferentially updated, that has nothing to do with probability
distribution overlap or preparation. Although the PBR theorem may be
valuable for “ontological models” [ 120 ] , it is not particularly
valuable here.

#### 6.2 Hidden Variables, Realism, and Non-locality

The subject of hidden variables, realism, and non-locality in ED has
been touched upon in [ 21 , 14 ] and it will be further explored here.
In Bell’s landmark paper [ 20 ] , he found a contradiction between QM
and hidden variable theories which claimed local realism. It was
accomplished by considering a hidden variable @xmath , which if known,
would give the outcome of an experiment (an eigenvalue of an operator)
with certainty @xmath . By integrating over the probability of a hidden
variable,

  -- -------- -- -------
     @xmath      (6.1)
  -- -------- -- -------

he showed that such expectation values do not always agree with the
expectations values of QM, for general @xmath .

In ED there is no such hidden variable. The particle dynamics is
non-deterministic as can be seen by the Brownian like paths particles
take due to the form of the transition probability @xmath in ( 4.18 ),
or after “energy conservation”, that the particles are undergoing a
non-dissipative diffusion. The process that is deterministic in ED is
the evolution of the probability distribution as it follows the
e-Hamilton equations from ( 4.41 ) given the appropriate constraints,
boundary, and initial conditions are known. The phase of the
wavefunction @xmath updates the probability distribution of particle
locations rather than guiding each particle at every point. In the same
fashion as above, “The particles themselves are “doing whatever they are
doing” and our model does its best to make inferences on the bases of
the available information. The phase @xmath is guided by the e-Hamilton
equations ( 4.41 ), and Hamiltonian, which informs us about the expected
drift of the particles … ”. The nonlocal nature of probability as a
means for quantifying knowledge (of the future, past, or present)
accounts for the “quantum mechanical” nonlocal correlations between
particles in ED.

As spin states are regularly used in Bell-type experiments, before ED
can give a full account of the Bell experiment, the ED of spin must be
developed fully. At this point it seems like spin states are represented
by “spin frames” [ 45 , 101 ] (Forthcoming by Caticha, Cararra) that
provide additional information about the positions of particles in
@xmath much in the same fashion that @xmath does, and therefore, spin
should be an epistemic inferable. Note that by detecting positions in a
von Neumann or weak measurement setting, the spin may be inferred from
position detections as is reviewed in Appendix D .

Using a multiple observer probability analysis, Bell’s theorem was
investigated in epistemic frameworks of QM [ 5 ] . As any collapse is an
epistemic change in the system, each viable observer is obligated to
assign distributions that coincide with their current state of knowledge
of the system. If Alice and Bob are stationed at space-like separated
measurement devices, they have access to different information
throughout the experiment due to the observed order of events being
different. I find that, at best, the Bell and the related CHSH
inequality [ 122 , 123 ] can only be ‘‘nonlocally violated
counterfactually” as the CHSH is generated from the posing of an if-then
question due to the asymmetry of each observer’s local information. ³ ³
3 i.e. if Alice’s measurement setting and outcome is @xmath and @xmath ,
then Bob would expect the measurement outcomes to be … ; however Bob
does not actually know Alice’s measurement setting and outcome until
after she communicates it. As the CHSH and Bell inequalities are
expectation values, they are themselves epistemic inferables. The final
result of [ 5 ] is that probabilities in QM over nonlocal measurement
settings must have counterfactual (if-then) dependencies on their
measurement settings as it cannot be verified otherwise by any local
observer. This provides support for epistemic interpretations of the
wavefunction and their use as a tool for inference.

#### 6.3 BKS type Theorems

The BKS theorem shed light on the incompatibility of hidden variable
theories and Quantum Mechanics [ 17 , 18 ] . Years later Mermin [ 19 ]
demonstrated what is considered to be the simplest expression of what is
usually an algebra and geometry intensive BKS theorem using observables.
BKS proofs have been generalized to the @xmath -qubit Pauli group [ 124
] , and [ 125 ] gives a BKS proof using continuous position and momentum
observables. In [ 124 ] , they give a simple algorithm to convert
observable based BKS proofs to a large number of projector based BKS
proofs, so here we will focus on the simpler observable based proofs.

The class of hidden variable theories excluded by the BKS theorem
satisfy the following seemingly reasonable conditions. The value of an
operator is definite yet unknown such that we may assign it a
preexisting value (its eigenvalue) by applying what is called a
valuation [ 116 , 19 , 124 ] . The reason for introducing valuations is
to make a connection to hidden variable theories ( 6.1 ) in which, given
the hidden variable @xmath is known, @xmath is known too. The alleged
power of the proof below is that it is independent of the state @xmath .
This immediately conflicts with the inferential ideology of QM in ED,
however, we will continue to introduce the proof. The valuation of an
operator @xmath at any time is one of its eigenvalues,

  -- -------- -- -------
     @xmath      (6.2)
  -- -------- -- -------

It is also assumed that functional relationships between the operators
@xmath should hold throughout the valuation process,

  -- -------- -- -------
     @xmath      (6.3)
  -- -------- -- -------

as the values of the operators are supposed to take definite values.
Thus it is found that operators must commute @xmath when taking
valuations for ( 6.3 ) to hold. Mermin demonstrates the contradiction of
equations ( 6.2 ) and ( 6.3 ) with Quantum Mechanics by considering what
is now know as the Peres-Mermin Square:

  ZI   IX   ZX
  ---- ---- ----
  IZ   XI   XZ
  ZZ   XX   YY

.

Each table entry is an observable from the 2-qubit Pauli group
consisting of a joint eigenbasis consisting of 4 eigenvectors. As a
notational convenience we will omit tensor products when there is no
room for confusion and let @xmath such that an arbitrary table entry
@xmath represents @xmath , following the notational structure in [ 124 ]
. The standard matrix product of the operators along a given row or
column is the rank 4 identity @xmath (in this notation) with the
exception of the last row, which is @xmath . Consider the valuation of
the standard matrix product of the elements of the first row,

  -- -------- -- -------
     @xmath      (6.4)
  -- -------- -- -------

Supposing ( 6.3 ) is true then

  -- -------- -- -------
     @xmath      (6.5)
  -- -------- -- -------

The valuation of the @xmath th element @xmath in the table is @xmath ,
and therefore ( 6.3 ) imposes a constraint on the individual valuations
@xmath , which is only satisfied if either 0 or 2 of the valuations are
@xmath . This cuts the number of possibilities from @xmath to @xmath .
Let @xmath be the product of the operators in the @xmath th row and
@xmath the product of the operators in the @xmath th column such that
above @xmath @xmath @xmath @xmath is the standard matrix product between
the listed operators. Mermin showed his square indeed leads to a
contradiction when considering the product of the row and column
valuations,

  -- -------- -- -------
     @xmath      (6.6)
  -- -------- -- -------

whereas applying ( 6.3 ) to each row and column, @xmath , gives,

  -- -------- -- -------
     @xmath      (6.7)
  -- -------- -- -------

which is a contradiction. This is due to the fact that not all of the
elements in Mermin’s square commute and therefore all of the observables
cannot simultaneously be assigned definite eigenvalues. Quantum
mechanical formalism and experiment agree with ( 6.6 ) and not with (
6.7 ) and thus ( 6.3 ) is ruled out. Bell makes a point that it may be
overconstraining for the valuation to produce identical values when
different sets of commuting observables are being considered, just to
refute it by noting that a space-like separated observer could change
which set of commuting observables he/she wishes to measure mid-flight.
A hidden variable theory would then have to explain this nonlocal change
in the valuation meaning that the BKS theory only refutes local hidden
variables theories [ 17 ] .

##### 6.3.1 Interpreting the Contradiction: Contextuality

The standard interpretation of the contradiction by Bell, Kochen,
Specker, Mermin and others is that quantum mechanical observables are
contextual , meaning that the operator’s “aspect”, “character”, or
“value” depend on the remaining set of commuting observables under which
it is considered, that is, which observables it would be measured along
side with. Any observable that does not depend on the remaining set of
commuting observables in this way is called noncontextual , which, for
example, are the individual observables @xmath from the Mermin square
and ( 6.7 ).

In more recent years the interpretation of the BKS theorem, which in
principle would rule out all local hidden variable theories obeying (
6.2 ) and ( 6.3 ), has been under scrutiny, in essence, for having a
more restricted interpretation than the theorem claims. The work by [
126 , 127 , 128 ] opens a loophole due to the impracticality of infinite
measurement precision, and thus the BKS theorem is “nullified” in their
language. Appleby (and others) find the “nullified” critique to be too
harsh of a criticism [ 129 ] . De Ronde [ 130 ] points out that
epistemic and ontic contextuality are consistently being scrambled into
a omelet when perhaps the yoke and egg whites should be cooked
separately. He defines “ontic contextuality” as the formal algebraic
inconsistency of the operator and valuation formalism of Quantum
Mechanics within the BKS theorem – having nothing to do with
measurement. Its epistemic counterpart is more aligned with the
principles of Bohr in that Quantum Mechanics involves an interaction
between the system and measurement apparatus whose outcomes are
inevitably communicated in classical terms – the context is given by the
measurement device. The difference is subtle but, as noted, “ontic
contextuality” is defined to be independent of the differing
interpretations of quantum mechanics whereas epistemic contextuality
need not be. Our treatment of contextuality does separate in this
fashion; however, de Ronde’s usage of the word “ontic” refers to the
quantum formalism, whereas our usage only refers to ontic particle
positions in ED.

##### 6.3.2 Critiques on representing onticity with valuations in QM

As shown, the assumptions ( 6.2 ) and ( 6.3 ) lead to contradictions.
The main critique we present is, “how do we know that the valuation of
an observable @xmath accurately represents the notion of definite,
preexisting values of an operator, that would be obtained if a
measurement is carried out?”. The alleged strength of the BKS theorem is
that the analysis has been done independent of the particular state
@xmath and thus it should hold for all @xmath in general. This is
troubling for a number of reasons, the first being that a particular
@xmath may not have components along every eigenvector of an operator
@xmath , in which case a zero probability event could be assigned a
definite existence, and one would never know because @xmath , which all
of the observables in question pertain to, has not been specified. This
issue here is an interplay between the ontic and epistemic contextuality
given by de Ronde, because only sensible valuations may be given if the
state of the system is known – in general the density matrix @xmath .

If the valuation process is to be applicable to arbitrary “observables”
independent of the state at hand, then one runs into another logical
inconsistency when attempting to apply valuations to a density matrix,
@xmath , because it represents the probabilistic state of a system. It
makes little sense to have different sets of commuting observables
@xmath which are required to span the same Hilbert space as the state in
question @xmath (or @xmath ). Furthermore, the valuation of a density
matrix @xmath gives one of its eigenvalues, @xmath , which are
probabilities themselves and are never directly observed, but are
usually inferred from the frequency of a large number of independent
trials. One cannot possibly claim that a system is ontically expressing
a definite preexisting probability value @xmath . Probability by its
nature is a measure of the uncertainty of a state @xmath rather than a
value (physically) carried by the state @xmath – which is as epistemic
as it gets! If Alice knowingly prepares one system and Bob does not know
which system Alice has prepared, then it is clear that @xmath ’s cannot
have a definite existence because both Alice and Bob disagree about said
values over the same single “ontic” system of interest. Furthermore,
when a measurement is made to determine the state, the probability value
updates (the eigenvalue changes) and in this sense the assignment of an
eigenvalue @xmath through valuation represents nothing physical about
the state of the system’s definite, preexisting values that would in
principle be obtained if a measurement was carried out. If this one
@xmath valuation counter example can be found, it is unclear how many
other observables would also be counter examples. In general the
eigenvalues of operators do not represent definite, preexisting
(noncontextual) values of an operator that would be obtained if a
measurement was carried out.

Due to these critiques, and that in ED one may infer eigenvalues from
position detections, it is difficult to know what precisely a valuation
procedure represents meta-physically, besides the simple choice of a
matrix element. As discussed, the valuation of an operator may not
always represent an ontic value of an observable, and therefore we
suggest relaxing this notion and replacing it by the more general
statement, “The valuation of an operator (or set of operators)
represents a quantity that in principle may be inferred”, or in the
language of [ 3 ] , “The valuation of an observable is an inferable of
the theory”. In this sense, the problem presented by the BKS theorem
never truly arises in the ED framework – operators, simply put, are
epistemic inferables.

##### 6.3.3 ED: A hybrid-contextual theory

It should be noted that in Entropic Dynamics, the idea of valuation is
very unnatural. An inference based theory allows us to state, quantify,
and represent how much we do not know about the state of a system
through a probability distribution, upon which we use the rules of
inference and probability updating to determine what we do. The
resulting interpretation of the BKS theorem (contextuality) when
performed in the ED framework “is that operators and their eigenvalues
do not in general pertain to the ontology of particles” – they are, in
general, inferables. Although a precisely prepared state @xmath can be
measured (inferred) with certainty using a unitary measurement device
from Section 5.2.2 , it is important to recall the note, “… there is no
implication that the particle previously had or now currently has the
value @xmath ” [ 14 , 3 ] .

Strictly speaking, the BKS theorem discards realist theories in which
all of the considered operators are treated ontically through their
valuation. This leaves open the possibility for a hybrid-contextual
theory in which only a subset of commuting observables are definite yet
unknown , or noncontextual, while other variables (or sets of commuting
observables) are contextual inferables.

The only operators that are required to undergo valuation in ED are the
@xmath -particle position coordinates with their corresponding @xmath
operators @xmath as they are the preferred basis. In the language of
valuations, we have,

  -- -------- -- -------
     @xmath      (6.8)
  -- -------- -- -------

for a particular coordinate @xmath . Position operators trivially obey (
6.3 ),

  -- -------- -- -------
     @xmath      (6.9)
  -- -------- -- -------

for any function @xmath , because all position operators mutually
commute. No parity contradiction in the sense of [ 19 , 124 ] can be
reached. At the end of the day, the BKS proofs are proofs by
contradiction which means that a set of counter examples has been found
which rule out the general applicability of assigning definite yet
unknown values to all operators all the time. However, as seen above,
there are instances in which there is no contraction and the assignment
of definite yet unknown values in this instance is consequently not
ruled out.

Operators other than position, @xmath , need not be noncontextual in ED
as they are considered to be epistemic in nature. In this case, one
should not claim @xmath , one of its eigenvalues @xmath , or a state
@xmath , to have a definite existence outside of characterizing our
knowledge of the definite yet unknown positions of particles @xmath .
That being said, the operators @xmath can in general be expanded and
interpreted in the position basis . When applying positional valuations
to @xmath , we find that they are naturally contextual in the sense that
equation ( 6.3 ) fails to hold in general. Although the following
argument does hold true mathematically, the valuation process is
unwarranted in ED as ED never claims the operator @xmath any of its
eigenvalues, @xmath , or the results of their valuations, to be ontic –
it is meaningless to talk about the noncontextuality of non-position
variables. We will proceed anyhow for completeness.

###### Proof that position valuations of non-position operators are
contextual:

If one were to perform the valuation of an arbitrary (non-position)
operator @xmath in ED before measurement, because the positions of
particles are the preferred basis, the only valuation function worth
considering, @xmath , is the one that considers definite particle
positions. ⁴ ⁴ 4 The Hermitian @xmath operators are arbitrary: they
could be tensor products of 1D projectors, standard QM observables of
interest like @xmath or @xmath , or subsets of them. Thus, we should
consider valuation of the diagonal matrix elements of @xmath in the
position basis (here let @xmath for the @xmath particles and @xmath
operators that are tensor multiplied in @xmath ),

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.10)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where in this case it is supposed that the definite yet unknown value of
@xmath is @xmath . The positional valuation @xmath is not one of the
eigenvalues or “observables” of @xmath , but in ED, @xmath is an
inferable and so is @xmath (through for instance the use of a Weak Value
[ 107 ] ). The positional valuation @xmath is some real number that in
principle may be assigned to the @xmath th position coordinate, and
potentially has nothing to do with the state of the system @xmath as is
remarked in the critiques in Section 6.3.2 . The position valuation
@xmath may be interpreted as an expected value of the operator @xmath if
the position of the particle(s) were known to be exactly at the value
@xmath in configuration space, that is, @xmath . The actual location of
the particle is not known in general and should be weighted by the
appropriate @xmath if one is considering expectation values of @xmath .
Parity type proofs of the BKS theorem require @xmath to be
simultaneously part of an even number of sets of commuting observables [
124 ] . This means an operator @xmath is simultaneously diagonalized in
(at-least two) different basis,

  -- -------- -- --------
     @xmath      (6.11)
  -- -------- -- --------

where, for example in the Peres-Mermin square, @xmath refers to the
eigenvectors of the commuting set of variables from the @xmath th row
and @xmath refers to the eigenvectors of the commuting set of variables
from the @xmath th column; however, the argument is valid for all
(non-spin) KS-sets. The largest number of distinct sets of eigenvectors
is equal to the number of sets of commuting observables in the BKS proof
(the number of rows and columns of the Peres-Mermin square). Using this
notation we may denote the product of the operators in a commuting set
by,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the number of commuting observables in the set of
commuting observables @xmath . In general, the application of ( 6.3 ) to
the position valuations of @xmath will not hold,

  -- -------- -- --------
     @xmath      (6.13)
  -- -------- -- --------

because it would require,

  -- -------- -- --------
     @xmath      (6.14)
  -- -------- -- --------

which holds in a few trivial cases (identity, null, orthogonal), but not
in general. Note that this equation is the analog of ( 6.5 ), expect
with position valuations, and here, equation ( 6.3 ) already fails to
hold in general. This poses no issue in ED because @xmath or the
individual @xmath need only exist epistemically, so their valuations
(matrix elements) need not agree - the product of matrix elements need
not be the matrix element of the product so imposing equality is
nonsensical. Equations like ( 6.3 ) do not hold true in general because,
if valuations are interpreted as inferables (Section 6.3.2 ), then
expecting something like @xmath to hold true is analogous to expecting
expectation values like @xmath to hold true, which of-course is not true
in general.

Furthermore, “ontic contextuality” in the sense of de Ronde
(contextuality due to the operator formalism of QM) is preserved among
non-position observables (for noncontextual position). Furthermore, if (
6.3 ) is applied to the product of all of the commuting sets of
observables,

  -- -------- -- --------
     @xmath      (6.15)
  -- -------- -- --------

for situations when the LHS is less than zero or it is simply not equal
to the RHS (compounding from ( 6.14 )). This calculation shows that
definite (noncontextual) positions before measurement do not imply
definite (noncontextual) @xmath , and therefore, we are further
justified in treating the operators @xmath contextually - which means we
should not apply valuations to them, or if we do, we should not expect (
6.3 ) to hold. The current form of ED would potentially be ruled out if
@xmath were required to be ontological - but this is not the case.

Because position operators always mutually commute with one another, and
are therefore all simultaneously diagonalizable in the same set of
position eigenvectors (i.e. @xmath ), they may be treated
noncontextually together. If an operator is a (tensor or dot) product of
contextual and noncontextual operators, it remains contextual as it is
epistemic (or equally well due to the lack of equality in ( 6.14 )).
This can be seen by applying position space valuations to the continuous
operators defined in [ 125 ] . Spin in ED is epistemic (forthcoming [ 45
, 101 ] ) so their noncontextual valuations are not required as well and
thus spin will be contextual in general. As noted in the critiques, the
valuation of an operator may not always express the definite yet unknown
values of an observable – it may be best to relax this notion such that
the valuation of an operator represents a quantity that in principle may
be inferred, an inferable, in general.

###### Inferring contextual operators:

In the previous section, the positional valuations of the operators are
a bit obscure, partially because the measurement process in ED was not
included (it is as if positions measurements were made before applying a
unitary measurement device, so the eigenvalues of @xmath ’s were not
actually being inferred). A question of interest is, how, if everything
is to be measured or inferred using a (non-contextual) position basis
(Chapter 5 and [ 14 , 3 ] ), is the contextual nature of a set of
contextual operators @xmath non-contradictory? This question is
especially tricky because it mixes the epistemic and ontic notions of
contextuality in the sense of [ 130 ] , who, quote Mermin , “the whole
point of an experimental test of BKS [theorem] misses the point.”. ED
perhaps sheds some light onto Mermin’s statement about the lack of an
experimental test of the BKS.

Suppose Alice prepares a two particle system and sends it to Bob who has
a compound unitary measurement device ( 5.7 ) for each set of commuting
observables (each row and column) of the Mermin square (for simplicity),
but really this is applicable to any construction of sets of commuting
observables. Because Bob can only measure one row or column for a given
pair of particles sent from Alice, him choosing the @xmath th row or
column means he has chosen and applied the unitary measurement device
@xmath to the incoming state and mapped it to position coordinates for
detection and inference. This measurement device is designed to infer
one set of commuting observables at a time from position detections.
That is, the physical application of @xmath picks, @xmath , the @xmath
th set of commuting observables, ⁵ ⁵ 5 It should be noted that the
square of @xmath ’s is depicted for the purpose of illustrating that
measurement devices pick a set of commuting observables. This and the
previous sections refer to arbitrary (non-spin) KS-sets, not just those
that can be arranged in squares.

@xmath @xmath @xmath @xmath @xmath @xmath @xmath @xmath @xmath @xmath
@xmath @xmath @xmath @xmath

@xmath @xmath @xmath @xmath ,

and at a later time one may apply valuation(s) to the associated
position operators if one wishes because the operators are diagonal in
the position basis (at that later time) @xmath . The positions may be
detected and the associated commuting set of @xmath may be inferred .
The notion of detectors picking sets of commuting observables is
mentioned in [ 19 ] , but here the process is specified to show how
contextuality is preserved. Observables @xmath not in row @xmath (after
@xmath is applied) are not in general diagonal in @xmath as they did not
originally commute with all of the observables in @xmath by
construction, and naturally, their eigenvalues are not being inferred.
It is as if picking a set of commuting observables, by the application
of a unitary measurement device, rotates the entire KS-set (the square
in this illustration) about the picked “axis” (the picked set of
commuting observables), the picked set of commuting observables now
being diagonal in position. Picking other sets of commuting observables
requires using a different unitary measurement device @xmath , which
rotates the whole KS-set differently (about a different “axis” for the
sake of the analogy). Thus, mixtures of noncontexutal position operators
with epistemic contextual operators are contextual as a unitary
measurement device will be needed for the inference of the contextual
operators, which “rotates” the KS-set.

The operators @xmath are treated contextually, or hybrid contextually in
the preferred basis. As only one set of commuting observables may be
picked at a time by Bob, the quantum mechanical expectation values match
that as inferred by Bob (and are therefore in the form of ( 6.6 )).
Alice, being in the dark, does not know which row Bob will pick and is
free to assign a probability Bob picks the @xmath th row or column, and
after learning the chosen row or column may she update her probability
accordingly [ 5 ] .

#### 6.4 Discussion

The most natural inferential tool in ED is probability. The critiques
given in Section 6.3.2 are further motivation for the use of probability
to make rational inferences, while the interpretation of valuation
functions as assigning ontic values, which inevitably lead to paradox in
the BKS theorem, is not generally applicable. There, reason was given
for the need of a more general interpretation of the valuation of an
operator, which was stated, “The valuation of an operator (or set of
operators) represents a quantity that in principle may be inferred”.
Because the probability of a state is only defined in terms of its set
of commuting observables, and because there is no way to generate a
unique joint probability distribution among non-commuting observables [
131 ] , a rational discussion on the potential simultaneous onticity of
non-commuting observables is not possible. As ED formulates QM as an
application of entropic inference while assuming the position of
particles to be the only ontic variables, the positions form the
preferred basis for inference, and therefore ED is a “hybrid-contextual”
theory of QM that does not violate any of the relevant aforementioned
theorems. Although it is unverifiable if reality truly consists of
particles with ontic position, the concept and model is demonstratively
useful for making predictions and for analyzing the results of
experiment. In a pragmatic approach [ 44 , 45 ] , when a model turns out
to be trustworthy and reliable, we recognize its success and say the
model or theory is “true” and the ontic elements are “real”.

### Chapter 7 Conclusions

Presented in this thesis is the synthesis of probability and entropic
inference with Quantum Mechanics and quantum measurement. The standard
and quantum relative entropies were shown to be designed for the purpose
of updating probability distributions and density matrices,
respectively, from the same inferential origins. The Quantum Bayes Rule
and collapse were derived as an application of the quantum relative
entropy maximization, which unifies topics in quantum measurement and
Quantum Information through entropic inference – similar to the
unification of Bayesian probability updating and the standard maximum
entropy method in [ 7 , 8 , 9 ] . Because the quantum maximum entropy
method is able to simultaneously process inferences that neither a
Quantum Bayes procedure nor a maximum von Neumann entropy procedure can
process alone, the designed quantum maximum entropy method may be
considered “a universal method of density matrix inference”. Because the
quantum maximum entropy method only utilizes the standard quantum
mechanical formalism, it, and its results, may be appended to the
standard quantum mechanical formalism. In this sense, the quantum
measurement problem of collapse is solved within the quantum mechanical
formalism, but the quantum measurement problem of preferred basis is
not.

The derived interpretations of probability and its entropic updating
allows one to formulate Quantum Mechanics as an application of entropic
inference called Entropic Dynamics. In Entropic Dynamics, particles have
definite yet unknown positions and probabilities are purely epistemic.
This separates Entropic Dynamics from other theories and interpretations
of Quantum Mechanics because one may address the quantum measurement
problem and quantum no-go theorems in a new light. Pivotal to both of
these discussions is the concept that observables in Quantum Mechanics
may better be stated as inferables , that is, quantities one may wish to
infer. Entropic Dynamics suggests this change of language, observables
@xmath inferables , when it derives Quantum Mechanics as an application
of inference; and although on the surface this change of language seems
purely semantical, it has deep implications for the interpretation of
Quantum Mechanics. The combination in Entropic Dynamics of epistemic
inferables that lack an ontological predisposition, the positions of
particles that are ontological, and use of epistemic probability theory,
end up resolving the measurement problems of preferred basis and
definite outcomes, as well as the no-go theorems of @xmath -epistemic,
Bell’s inequality, and the BKS theorem, in effect, without ever being a
problem in Entropic Dynamics in the first place [ 14 , 3 , 4 ] . Because
Entropic Dynamics derives Quantum Mechanics from the standard maximum
entropy method, as well as the beginnings of a density matrix formalism
added here, the quantum maximum entropy method can be derived from
Entropic Dynamics; and therefore, the standard maximum entropy method
retains its title as the “universal method of inference” [ 7 , 8 , 9 ] .

In the spirit of E.T. Jaynes’ quote (A) presented in the introduction to
this thesis, through the precise design of probability theory, the
maximum entropy method, and the use of Entropic Dynamics, we have
managed to push the theory of Quantum Mechanics past some of the
derailing ambiguities of its standard formalism. The culmination of the
articles reviewed in this thesis has led to a better understanding of
Quantum Mechanics and quantum measurement through inference and Entropic
Dynamics.
