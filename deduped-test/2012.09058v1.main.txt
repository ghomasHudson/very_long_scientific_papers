### Acknowledgements

I would like to thank all who contributed to achieving this amazing
goal. Heartfelt thanks to my advisors, Prof. Barbara Caputo, Prof. Elisa
Ricci, and Samuel Rota Bulò. At the beginning of the Ph.D. I was a
tenacious but pretty messy and badly organized student. Day by day, with
huge patience, countless suggestions, and precious advice, they turned
that student into a researcher. I am deeply thankful to Barbara, for
introducing me to research, for transferring me her passion and
dedication, and for teaching me that a clear plan is better than a bunch
of ideas. A huge thanks to Elisa for inspiring me with her behavior,
making me understand the importance of stubbornness, and how to face
deadlines and pressure while always keeping the same positive attitude.
A special thanks to Samuel: discussing problems and ideas with you,
trying to follow your thoughts has been an amazing, advanced school for
growing my scientific perspectives. All of you taught me how to identify
interesting research questions, and how to think for answering them. You
showed me how to make the best out of all experiences, how to celebrate
successes and how to embrace and react to failures. I enjoyed every
moment of this Ph.D. and I will always be grateful to you for shaping me
as the researcher I am today.

I would like to express my gratitude to Prof. Bernt Schiele and Prof.
Timothy Hospedales for having taken the time to accurately read this
thesis. It was a great honor for me to receive their positive and
valuable feedback.

I am grateful to Stefano Messelodi, for welcoming me to Fondazione Bruno
Kessler, allowing me to work in such an engaging environment.
Appreciation is also due to Hakan Karaoguz and Prof. Patric Jensfelt for
hosting me in the RPL lab in Stockholm, introducing me to the challenges
of robot vision. Additionally, I wish to thank Prof. Zeynep Akata and
all members of the EML lab in Tübingen for showing me different
perspectives and, recently, welcoming me for a new exciting experience.

This journey wouldn’t have been the same without some good fellows
sharing the way. I thank all members of the VANDAL lab in Rome and
Turin, with a big thanks to Fabio and Dario for bearing me in my attempt
to become a better supervisor. Thanks also to Fabio (the first), Paolo,
Valentina, Antonio, Silvia, and Mirco for sharing with me lab life and
conference adventures. I am grateful to all members of TeV and MHUG labs
in Trento, with a special mention to Pilz, Simo, Swathi, Levi, Enrico,
Aliaks, and Sub: thanks for sharing with me lab life, stressful and
joyful times, conferences, and beers. Heartfelt thanks to all my
co-authors, Lorenzo in particular for his fundamental support, smart
insights, and nice moments together.

Research is a part but not all of my life. I wish to thank all my
long-time friends in Monte, with extra gratitude to Robi, Alex and
Diego. Whenever I return to my hometown you always make me feel as if I
have never been away. I love that feeling.

I would like to thank my family, from my cousins to my grandparents, for
never making me feel alone. To my parents, Rinaldo and Anna: thank you
for always supporting me and for the values you taught me. I do not
think I can express in words how much I owe you. Thanks to my sister,
Serena, for understanding me and making me always remember what really
matters. I am proud of you.

Finally, I want to thank Elisa, my girlfriend. These years were not easy
for us: a long distance in between, occasional stress, pressures. You
have always been patient, helping me, pushing me, and believing in me
far more than what I do. I love you.

###### Contents

-    1 Introduction
    -    1.1 Overview
        -    1.1.1 Domain shift: generalizing to new visual domains
        -    1.1.2 Semantic shift: breaking model’s semantic limits
        -    1.1.3 Recognizing unseen categories in unseen domains
    -    1.2 Contributions
    -    1.3 Outline
    -    1.4 Publications
-    2 Recognition across New Visual Domains
    -    2.1 Problem statement
    -    2.2 Related Works
    -    2.3 Preliminaries: Domain Alignment Layers
    -    2.4 Latent Domain Discovery
        -    2.4.1 Problem Formulation
        -    2.4.2 Multi-domain DA-layers
        -    2.4.3 Domain prediction
        -    2.4.4 Training the network
        -    2.4.5 Experimental results
        -    2.4.6 Conclusions
    -    2.5 Domain Generalization
        -    2.5.1 Problem Formulation
        -    2.5.2 Starting point: Domain Generalization with Weighted
            BN
        -    2.5.3 WBN Experiments: Domain Generalization in Semantic
            Place Categorization
        -    2.5.4 From BN to Classifiers: Best Sources Forward
        -    2.5.5 Experiments: Domain Generalization in Computer Vision
        -    2.5.6 Conclusions
    -    2.6 Continuous Domain Adaptation
        -    2.6.1 The KTH Handtool Dataset
        -    2.6.2 Problem Formulation
        -    2.6.3 ONDA: ONline Domain Adaptation with
            Batch-Normalization
        -    2.6.4 Experimental results
        -    2.6.5 Conclusions
    -    2.7 Predictive Domain Adaptation
        -    2.7.1 Problem Formulation
        -    2.7.2 AdaGraph: Graph-based Predictive DA
        -    2.7.3 Model Refinement through Joint Prediction and
            Adaptation
        -    2.7.4 Experimental results
        -    2.7.5 Conclusions
-    3 Recognizing New Semantic Concepts
    -    3.1 Problem statement
    -    3.2 Related Works
    -    3.3 Sequential and Memory Efficient Learning of New Datasets
        -    3.3.1 Problem Formulation
        -    3.3.2 Affine Weight Transformation through Binary Masks
        -    3.3.3 Learning Binary Masks
        -    3.3.4 Experimental results
        -    3.3.5 Conclusions
    -    3.4 Incremental Learning in Semantic Segmentation
        -    3.4.1 Problem Formulation
        -    3.4.2 Modeling the Background for Incremental Learning in
            Semantic Segmentation
        -    3.4.3 Experimental results
        -    3.4.4 Conclusions
    -    3.5 Open World Recognition
        -    3.5.1 Problem Formulation
        -    3.5.2 Preliminaries
        -    3.5.3 Deep Nearest Non-Outlier
        -    3.5.4 Boosting Deep Open World Recognition
        -    3.5.5 Experimental results
        -    3.5.6 Towards Autonomous Visual Systems: Web-aided OWR
        -    3.5.7 Conclusions
-    4 Towards Recognizing Unseen Categories in Unseen Domains
    -    4.1 Problem statement
    -    4.2 Related Works
    -    4.3 Recognizing Unseen Categories in Unseen Domains
        -    4.3.1 Preliminaries
        -    4.3.2 Simulating Unseen Domains and Concepts through Mixup
        -    4.3.3 Experimental results
        -    4.3.4 Conclusions
-    5 Conclusions and Future Works
    -    5.1 Summary of contributions
    -    5.2 Open problems and future directions
-    A Recognition across New Visual Domains
    -    A.1 Latent Domain Discovery
        -    A.1.1 mDA layers formulas
        -    A.1.2 Training loss progress
        -    A.1.3 Additional Results on PACS
    -    A.2 Predictive Domain Adaptation
        -    A.2.1 Metadata Details
        -    A.2.2 Additional Analysis
-    B Recognizing New Semantic Concepts
    -    B.1 Incremental Learning in Semantic Segmentation
        -    B.1.1 How should we use the background?
        -    B.1.2 Per class results on Pascal-VOC 2012
        -    B.1.3 Validation protocol and hyper-parameters
-    C Towards Recognizing Unseen Categories in Unseen Domains
    -    C.1 Recognizing Unseen Categories in Unseen Domains
        -    C.1.1 Hyperparameter choices
        -    C.1.2 ZSL+DG: analysis of additional baselines
        -    C.1.3 ZSL+DG: ablation study
        -    C.1.4 ZSL results

###### List of Figures

-    1.1 Overview of our research problem. Suppose we are given an
    initial training set composed of images of a set of classes (e.g.
    elephant , horse ) acquired in a given domain (e.g. real photos ).
    Two main discrepancies can occur at test time: either images contain
    the same semantics but in different domains (e.g. paintings ,
    bottom-left) or they contain images of the same domain but depicting
    different semantic concepts (e.g. dog and giraffe , top-right). In
    the first case we talk about domain shift problem, while the second
    considers the semantic shift problem. The goal of this thesis is to
    address the two problems together (bottom-right), i.e. recognizing
    new semantic concepts (e.g. dog , giraffe ) in new visual domains (
    paintings ).
-    2.1 The idea behind the proposed framework for latent domain
    discovery. In this section, we introduce a novel deep architecture
    which, given a set of images, automatically discovers multiple
    latent domains and use this information to align the distributions
    of the internal CNN feature representations of sources and target
    domains for the purpose of domain adaptation. In this way, more
    accurate target classifiers can be learned.
-    2.2 Schematic representation of our method applied to the AlexNet
    architecture (left) and of an mDA-layer (right).
-    2.7 Distribution of the assignments produced by the domain
    prediction branch for each latent domain in all possible settings of
    the PACS dataset. Different colors denote different source domains.
-    2.12 Top-6 images associated to each latent domain for the
    different sources/target combinations. Each row corresponds to a
    different latent domain.
-    2.25 Distribution of the assignments produced by the domain
    prediction branch in all possible multi-target settings of the PACS
    dataset. Different colors denote different source domains (red: Art,
    yellow: Cartoon, blue: Photo, green: Sketch).
-    2.38 Distribution of the assignments produced by the domain
    prediction branch trained with the additional constraint on the
    entropy loss in all possible multi-target settings of the PACS
    dataset. Different colors denote different source domains (red: Art,
    yellow: Cartoon, blue: Photo, green: Sketch).
-    2.41 Distribution of the assignments produced by the domain
    prediction branch for each latent domain in all target settings of
    the Digits-five dataset. Different colors denote different source
    domains (black: MNIST, blue: MNIST-m, green: USPS, red: SVHN,
    yellow: Synthetic numbers).
-    2.42 Office31 dataset. Performance at varying number of domain
    labels ( @xmath ) for source samples.
-    2.43 The domain generalization problem. At training time (orange
    block) images of multiple source domains (e.g. A,B,C) are available.
    These images are used to train different models with parameters
    @xmath . Our approach automatically computes a model D which
    accurately classifies images of a novel domain (not available during
    training) by combining the models of the known domains.
-    2.47 Example of the proposed WBN framework. (a) AlexNet with BN
    layers after each fully connected. (b) The same network employing
    Domain Alignment layers for domain adaptation, where different BN
    are used for source and target domains. (c) Our approach for DG with
    WBN layers.
-    2.48 Distribution of the values of the weights computed with
    AlexNet+WBN for the scenario Lj.N as target in Table 2.9 . Different
    colors represent different original source domains.
-    2.50 Intuition behind the proposed BSF framework. Different
    domain-specific classifiers and the classifiers fusion are learned
    at training time on source domains, in a single end-to-end trainable
    architecture . When a target image is processed, our deep model
    optimally combines the source models in order to compute the final
    prediction.
-    2.51 Simplified architecture of the proposed BSF framework. The
    input image is fed to a series of domain-specific classifiers and to
    the domain prediction branch. The latter produces the assignment
    @xmath which is fed to the domain prediction loss. The same @xmath
    is modulated by @xmath before being used to combine the output of
    each classifier. The final output of the architecture @xmath , is
    fed to the classification loss.
-    2.52 Rotated-MNIST dataset: analysis of the assignments computed by
    the domain prediction branch.
-    2.53 Our ONDA approach for performing kitting in arbitrary
    conditions. Given a training set, we can train a robot vision model
    offline. As the robot performs the task, we gradually adapt the
    visual model to the current working conditions, in an online fashion
    and without requiring target data during the offline training phase.
-    2.54 The 2-arm stationary robot platform.
-    2.55 The statistics of the BN layers are initialized offline, by
    training the network on the images of the source domain. At
    deployment time, the input frames are processed using the global
    estimate of the statistics (red lines). However the robot collects
    each @xmath input frames to compute partial BN statistics, using
    these estimated values to gradually update the BN statistics for the
    current scenario.
-    2.58 Experiments on isolated shifts. The labels of the x-axes
    denote the conditions of target domain, with the first line
    indicating the light condition, the second the camera and the third
    the background. We underlined the changes between the source and
    target domains.
-    2.61 Accuracy vs number of updates of ONDA for different values of
    (a) @xmath and (b) @xmath in a sample scenario. The red line denotes
    the BN lower bound of the starting model, while the yellow line the
    DIAL upper bound.
-    2.62 Predictive Domain Adaptation. During training we have access
    to a labeled source domain (yellow block) and a set of unlabeled
    auxiliary domains (blue blocks), all with associated metadata. At
    test time, given the metadata corresponding to the unknown target
    domain, we predict the parameters associated to the target model.
    This predicted model is further refined during test, while
    continuously receiving data of the target domain.
-    2.63 AdaGraph framework (Best viewed in color). Each BN layer is
    replaced by its GBN counterpart. The parameters used in a GBN layer
    are computed from a given metadata and the graph. Each domain in the
    graph (circles) contains its specific parameters (rectangular
    blocks). During the training phase (blue part), a metadata (i.e.
    @xmath , blue block) is mapped to its domain (z). While the
    statistics of GBN are determined only by the one of @xmath ( @xmath
    ), scale and bias are computed considering also the graph edges.
    During test, we receive the metadata for the target domain ( @xmath
    , red block) to which no node is linked. Thus we initialize @xmath
    and we compute its parameters and statistics exploiting the
    connection with the other nodes in the graph ( @xmath ).
-    2.64 Portraits dataset: comparison of different models in the PDA
    scenario with respect to the average accuracy on a target decade,
    fixed the same region of source and target domains. The models are
    based on ResNet-18.
-    3.1 Idea behind our BAT approach. A network pre-trained on a given
    recognition task A (i.e. ImageNet) can be extended to tackle other
    recognition tasks B (e.g. digits) and C (e.g. traffic sign) by
    simply transforming the network weights (orange cubes) through
    task-specific binary masks (colored grids).
-    3.2 Overview of the proposed BAT model (best viewed in color).
    Given a convolutional kernel, we exploit a real-valued mask to
    generate a domain-specific binary mask. An affine transformation
    directly applied to the binary masks, which changes their range
    (through a scale parameter @xmath ) and their minimum value (through
    @xmath ). A multiplicative mask applied to the original kernels and
    the pre-trained kernel themselves are scaled by the factors @xmath
    and @xmath respectively. All the different masks are summed to
    produce the final domain-specific kernel.
-    3.3 Percentage of 1s in the binary masks at different layers depth
    for Piggyback (left) and our full model (center) and values of the
    parameters @xmath , @xmath , @xmath computed by our full model
    (right) for all datasets of the Imagenet-to-Sketch benchmark and the
    ResNet-50 architecture.
-    3.4 Percentage of 1s in the binary masks at different layers depth
    for Piggyback (left) and our full BAT model (center, ours ) and
    values of the parameters @xmath , @xmath , @xmath computed by our
    full model (right) for all datasets of the Imagenet-to-Sketch
    benchmark with the DenseNet-121 architecture.
-    3.5 Illustration of the semantic shift of the background class in
    incremental learning for semantic segmentation. Yellow boxes denote
    the ground truth provided in the learning step, while grey boxes
    denote classes not labeled. As different learning steps have
    different label spaces, at step @xmath old classes (e.g. person )
    and unseen ones (e.g. car ) might be labeled as background in the
    current ground truth. Here we show the specific case of single class
    learning steps, but we address the general case where an arbitrary
    number of classes is added.
-    3.6 Overview of MiB . At learning step @xmath an image is processed
    by the old (top) and current (bottom) models, mapping the image to
    their respective output spaces. As in standard ICL methods, we apply
    a cross-entropy loss to learn new classes (blue block) and a
    distillation loss to preserve old knowledge (yellow block). In this
    framework, we model the semantic changes of the background across
    different learning steps by (i) initializing the new classifier
    using the weights of the old background one (left), (ii) comparing
    the pixel-level background ground truth in the cross-entropy with
    the probability of having either the background (black) or an old
    class (pink and grey bars) and (iii) relating the background
    probability given by the old model in the distillation loss with the
    probability of having either the background or a novel class (green
    bar).
-    3.7 Qualitative results on the 100-50 setting of the ADE20K dataset
    using different incremental methods. The image demonstrates the
    superiority of our approach on both new (e.g. building , floor ,
    table ) and old (e.g. car , wall , person ) classes. From left to
    right: image, FT, LwF [ li2017learning ] , ILT [
    michieli2019incremental ] , LwF-MC [ rebuffi2017icarl ] , MiB , and
    the ground-truth. Best viewed in color.
-    3.8 In the open-world scenario a robot must be able to classify
    correctly known objects, ( apple and mug ), and detect novel
    semantic concepts (e.g. banana ). When a novel concept is detected,
    it should learn the new class from an auxiliary dataset, updating
    its internal knowledge.
-    3.9 Overview of the B-DOC global to local clustering. The global
    clustering (left) pushes sample representations closer to the
    centroid (star) of the class they belong to. The local clustering
    (right), instead, forces the neighborhood of a sample in the
    representation space to be semantically consistent, pushing away
    samples of other classes.
-    3.10 Overview of how B-DOC learns the class-specific rejection
    thresholds. The small circles represent the samples in the held out
    set. The dashed circles, having radius the maximal distance (red),
    represent the limits beyond which a sample is rejected as a member
    of that class. As it can be seen, the class-specific threshold is
    learned to reduce the rejection errors.
-    3.15 Comparison of NNO [ bendale2015towards ] , DeepNNO and B-DOC
    on RGB-D Object dataset [ lai2011large ] . The numbers in
    parenthesis denote the average accuracy among the different
    incremental steps.
-    3.20 Comparison of NNO [ bendale2015towards ] , DeepNNO and B-DOC
    on Core50 [ lomonaco2017core50 ] . The numbers in parenthesis denote
    the average accuracy among the different incremental steps.
-    3.23 Comparison of NNO [ bendale2015towards ] , DeepNNO and B-DOC
    on CIFAR-100 dataset [ krizhevsky2009learning ] . The numbers in
    parenthesis denote the average accuracy among the different steps.
-    3.24 CIFAR-100 results in the closed world scenario.
-    3.26 CIFAR-100 results of DeepNNO in the closed world scenario for
    different values of @xmath .
-    3.28 Overview of the open world recognition task within a robotic
    platform. Given an image of an object, a classification algorithm
    assigns to it a class label. If the object is recognized as novel,
    the object label and relative are obtained through external resource
    (e.g. a human and/or the Web). Finally, the images are used to
    incrementally updated the knowledge base of the robot.
-    3.29 CIFAR-100: performances of Web-aided OWR in the open world
    scenario, with 50 unknown classes.
-    3.31 Qualitative results of deployment of DeepNNO on a robotic
    platform. The robot recognizes an object as unknown (i.e. the red
    hammer, bottom) and adds it to the knowledge base through the
    incremental learning procedure (top right).
-    4.1 Our ZSL+DG problem. During training we have images of multiple
    categories (e.g. elephant , horse ) and domains (e.g. photo ,
    cartoon ). At test time, we want to recognize unseen categories
    (e.g. dog , giraffe ), as in ZSL, in unseen domains (e.g. paintings
    ), as in DG, exploiting side information describing seen and unseen
    categories.
-    4.2 Our CuMix Framework. Given an image (bottom, horse , photo ),
    we randomly sample one image from the same (middle, photo ) and one
    from another (top, cartoon ) domain. The samples are mixed through
    @xmath (white blocks) both at image and feature level, with their
    features and labels projected into the embedding space @xmath (by
    @xmath and @xmath respectively) and there compared to compute the
    final objective. Note that @xmath varies during training (top part),
    changing the mixing ratios in and across domains.
-    4.3 ZSL results on CUB, SUN, AWA and FLO datasets with ResNet-101
    features.
-    A.3 Digits-five: plots of the domain (orange) and classification
    (blue) losses during the training phase.
-    A.6 Digits-five: plots of the cross-entropy loss on source samples
    (orange) and entropy loss on target sample (blue) for the semantic
    classifier during the training phase.
-    A.9 Digits-five: plots of the entropy loss on single sample (blue)
    and on the average batch assignments (orange) for the domain
    classifier during the training phase.
-    A.13 Portraits dataset: performances of AdaGraph with respect to
    the number of auxiliary domains available for different
    source-target pairs. The years reported in the captions indicate the
    starting year of source and target decades.

###### List of Tables

-    2.1 Digits datasets: comparison of different models in the
    multi-source scenario. MNIST (M) and MNIST-m (Mm) are taken as
    source domains, USPS (U) as target.
-    2.2 Digits-five [ xu2018deep ] setting, comparison of different
    single source and multi-source DA models. The first row indicates
    the target domain with the others used as sources.
-    2.3 PACS dataset: comparison of different methods using the ResNet
    architecture. The first row indicates the target domain, while all
    the others are considered as sources.
-    2.4 PACS dataset: comparison of different methods using the ResNet
    architecture on the multi-source multi-target setting. The first row
    indicates the two target domains.
-    2.5 Office-31 dataset: comparison of different methods using
    AlexNet. In the first row we indicate the source (top) and the
    target domains (bottom).
-    2.6 Office-31: comparison with state-of-the-art algorithms. In the
    first row we indicate the source (top) and the target domains
    (bottom).
-    2.7 Office-Caltech dataset: comparison with state-of-the-art
    algorithms. In the first row we indicate the source (top) and the
    target domains (bottom).
-    2.8 DG accuracy on COLD over different lighting conditions. First
    row indicates the target sequence, with the first letters denoting
    the laboratory and the last the illumination condition (C=Cloudy,
    S=Sunny, N=Night). Vertical lines separate domains of the same
    laboratory. * indicates the algorithm uses domain knowledge.
-    2.9 DG accuracy on COLD over different environments/sensors. First
    row indicates the target sequence, with the first letters denoting
    the laboratory and the last the illumination condition (C=Cloudy,
    S=Sunny, N=Night). Vertical lines separate domains with same
    illumination condition. * indicates the algorithm uses domain
    knowledge.
-    2.10 VPC dataset: average accuracy per class.
-    2.11 VPC dataset: comparison with state of the art.
-    2.12 SPED dataset: comparison of different models.
-    2.13 Rotated-MNIST dataset: comparison with previous methods.
-    2.14 PACS dataset: comparison with previous methods.
-    2.15 PACS dataset: sensitivity analysis.
-    2.16 Example Images from KTH Handtool Dataset
-    2.17 Portraits dataset. Ablation study.
-    2.18 CompCars dataset [ yang2015large ] . Comparison with state of
    the art. @xmath denotes Decaf features as input, @xmath denotes
    VGG-Full.
-    2.19 CarEvolution [ RematasICCVWS13 ] : comparison with state of
    the art.
-    2.20 Portraits dataset [ yang2015large ] : performances of the
    refinement strategy on the continuous adaptation scenario
-    3.1 Accuracy of ResNet-50 architectures in the ImageNet-to-Sketch
    scenario.
-    3.2 Accuracy of DenseNet-121 architectures in the
    ImageNet-to-Sketch scenario.
-    3.3 Accuracy of VGG-16 architectures in the ImageNet-to-Sketch
    scenario.
-    3.4 Results in terms of @xmath and @xmath scores for the Visual
    Decathlon Challenge.
-    3.5 Impact of the parameters @xmath , @xmath , @xmath and @xmath of
    our model using the ResNet-50 architectures in the
    ImageNet-to-Sketch scenario. ✓ denotes a learned parameter, while
    @xmath denotes [ mallya2018piggyback ] obtained as a special case of
    our model.
-    3.6 Impact of the parameters @xmath , @xmath , @xmath and @xmath of
    our model using the DenseNet-121 architectures in the
    ImageNet-to-Sketch scenario. ✓ denotes a learned parameter, while
    @xmath denotes [ mallya2018piggyback ] obtained as a special case of
    our model.
-    3.7 Mean IoU on the Pascal-VOC 2012 dataset for the disjoint
    incremental class learning scenarios.
-    3.8 Mean IoU on the Pascal-VOC 2012 dataset for the overlapped
    incremental class learning scenario.
-    3.9 Ablation study of the proposed method on the Pascal-VOC 2012
    overlapped setup. CE and KD denote our cross-entropy and
    distillation losses, while init our initialization strategy.
-    3.10 Mean IoU on the ADE20K dataset for different incremental class
    learning scenarios, adding 50 classes at each step.
-    3.11 Mean IoU on the ADE20K dataset for a multi-step incremental
    class learning scenario, adding 50 classes in 5 steps.
-    3.12 Ablation study of B-DOC on the global (GC), local clustering
    (LC) and Triplet loss on the OWR metric. The right column shows the
    average OWR-H over all steps.
-    3.13 Rejection rates of different techniques for detecting the
    unknowns. The results are computed using the same feature extractor
    on the RGB-D Object dataset.
-    4.1 Domain Generalization accuracies on PACS with ResNet-18.
-    4.2 Ablation on PACS dataset with ResNet-18 as backbone.
-    4.3 ZSL+DG scenario on the DomainNet dataset with ResNet-50 as
    backbone.
-    A.1 PACS dataset: comparison of different methods using the ResNet
    architecture. The first row indicates the target domain, while all
    the others are considered as sources. The numbers in parenthesis
    indicate the results using a target validation set for model
    selection.
-    A.2 PACS dataset: comparison of different methods using the ResNet
    architecture on the multi-source multi-target setting. The first row
    indicates the two target domains. The numbers in parenthesis
    indicate the results using a target validation set for model
    selection.
-    A.3 CompCars dataset [ yang2015large ] . Results with ResNet-18
    architecture.
-    B.1 Comparison of different implementations of LwF-MC on the
    Pascal-VOC 2012 overlapped setup.
-    B.2 Comparison of different implementations of LwF-MC on the 50-50
    setting of the ADE20K dataset.
-    B.3 Per Class Mean IoU on 19-1 setting of Pascal-VOC 2012. disjoint
    setup
-    B.4 Per Class Mean IoU on 19-1 setting of Pascal-VOC 2012.
    overlapped setup
-    B.5 Per Class Mean IoU on 15-5 setting of Pascal-VOC 2012. disjoint
    setup
-    B.6 Per Class Mean IoU on 15-5 setting of Pascal-VOC 2012.
    overlapped setup
-    B.7 Per Class Mean IoU on 15-1 setting of Pascal-VOC 2012. disjoint
    setup
-    B.8 Per Class Mean IoU on 15-1 setting of Pascal-VOC 2012.
    overlapped setup
-    C.1 ZSL+DG scenario on the DomainNet dataset with ResNet-50 as
    backbone.
-    C.2 Results on DomainNet dataset with Real-Painting as sources and
    ResNet-50 as backbone.
-    C.3 ZSL results.

## Chapter 1 Introduction

### 1.1 Overview

A long-standing goal of artificial intelligence and robotics is the
implementation of agents that are able to interact in the real world. In
order to achieve this goal, a crucial step lays in making the agents
understand the current state of the surrounding environment, by
providing them with both powerful sensors and the ability to process the
information the sensors give them. To this extent, visual cameras are
one of the most powerful and information-rich sensors. Indeed,
applications requiring visual abilities are countless: from self-driving
cars to detecting and handling objects for service robots in homes, from
kitting in industrial workshops, to robots filling shelves and shopping
baskets in supermarkets, etc., they all imply interacting with a wide
variety of objects, which requires a deep understanding of how these
objects look like, their visual properties and associated
functionalities.

Due to the central role that vision has in the path towards developing
agents with intelligent, autonomous behaviors, a lot of research efforts
have been spent on improving computer and robot vision systems. Within
this context, in recent years these fields have seen unprecedented
advancements thanks to deep learning architectures [
Goodfellow-et-al-2016 ] . Deep models are very effective in learning
discriminative representations from input data, and their applications
touch on many different fields, such as natural language processing [
mikolov2013distributed , collobert2008unified , deng2018nlp ,
young2018nlp ] , speech recognition [ hinton2012deep , deng2013speech ,
deng2013speechmicr ] and reinforcement learning [ lillicrap2015rl ,
mnih2015rl , gu2017rl ] . In the context of computer vision,
Convolutional Neural Networks (CNNs) [ lecun1998gradient ] are the
leading paradigm. These networks are particularly effective in
processing grid-like input data [ Goodfellow-et-al-2016 ] a category to
which images belong. The successes of CNNs in computer vision are
countless: they have achieved outstanding results in many visual tasks,
ranging from object classification [ krizhevsky2012imagenet , he2016deep
] and detection [ girshick2014rich , ren2017faster ] , to more complex
ones such as image captioning [ karpathy2015deep , you2016image ] ,
visual question answering [ antol2015vqa , xu2016ask ] and motion
transfer [ siarohin2019animating , chan2019everybody ] .

Despite their effectiveness, CNNs have some drawbacks. First, they are
data-hungry, i.e. very large labeled datasets are usually required for
training them [ russakovsky2015imagenet ] . This is a major issue since
it is hard to obtain a large amount of labeled data for any possible
application scenario. For instance, this often happens in robotics,
where data acquisition and annotation are especially time-consuming and
often infeasible.

Another major limitation of deep architectures is that their
effectiveness is limited to the particular set of knowledge present in
their training set, relying on the closed world assumption (CWA) [
sunderhauf2018limits ] . This assumption rarely holds in practice and,
due to the large variability of the real world, training and test images
may differ significantly in terms of visual appearance, or may even
contain different semantic categories. As a simple example, let us
consider the scenario represented in Figure 1.1 . If we train a system
to recognize animals (e.g. elephant and horses ) in a given visual
domain (e.g. real photos ) it will inherently assume that (i) those
animals are the only animals we want to recognize and (ii) that they
will always appear under the distribution of real images. What will
eventually come as no surprise is that the model will struggle in
distinguishing the same animals in a different visual domain (e.g.
paintings ) and it will never be able to recognize animals (e.g. dog and
giraffe ) not present in its initial training set. This was a toy
example but, in reality, applications where we would like to adapt a
model to new input distributions and/or semantics, are countless. For
example, given a robot manipulation task we cannot forecast a priori all
the possible conditions (e.g. environments, lighting) it will be
employed in. Moreover, we might have data only for a subset of objects
we would like to recognize, at least initially. Similar reasoning
applies to autonomous driving, where it is nearly impossible to collect
data for every possible driving condition (e.g. weather, road), and the
semantic categories we want to recognize might change with the location
(e.g. region-specific animals) or purpose of the vehicle (e.g. garbage
collector).

The goal of this thesis is to address these two problems together. In
particular, we want to extend the effectiveness of deep architectures to
visual domains and semantic concepts not included in the initial
training set, with the long-term goal of building visual recognition
systems capable of recognizing new semantic concepts in new visual
domains.

#### 1.1.1 Domain shift: generalizing to new visual domains

To recognize new semantic concepts in new visual domains, the first
problem we must face is generalizing to new visual domains, by
overcoming the domain shift problem. To this extent, Domain Adaptation
(DA) methods [ csurka2017domain , wang2018deep ] are specifically
designed to transfer knowledge from a source domain, where a large
amount of labeled data are available, to a domain of interest, i.e. the
target domain where few or no labeled data are available. While standard
approaches usually focus on a single-source and single-target scenario [
ganin2014unsupervised , long2017deep ] , a large variety of settings
exist depending on the information we have about our source and target
domains. For instance, we might have multiple sources and/or multiple
target domains, as in multi-source DA [ xu2018deep , MDAN_ICLRW18 ] ,
and multi-target DA [ chen2019blending , gholami2020unsupervised ] . In
these cases, a naive application of single- source/target domain
adaptation algorithms would not suffice, consequently leading to poor
results. Moreover, the domains might be either explicitly divided or
unified in a mixed dataset. Thus, we must discover the various domains
required for effectively addressing the domain shift problem [
gong2013reshaping , xiong2014latent , hoffman2012discovering ] . While
standard DA assumes that data of the target domain are available during
the initial training phase, a more realistic scenario is that,
initially, we do not have any image of the target domain at all. This
problem arises in practice every time our systems are employed in unseen
environments such as novel viewpoints, illumination, or weather
conditions. There are three possible ways to tackle this problem,
depending on the information we have on our target domain.

In case we have no information about our target but we have multiple
source domains, we can address this problem by disentangling
domain-specific and domain-agnostic components, thereby building a model
robust to any possible target domain shift. This is the goal of domain
generalization (DG) that has recently raised a lot of interest in the
community [ li2017deeper , li2019episodic , carlucci2019domain ] .
Differently, if we have no information about our target and a single
source domain, we cannot disentangle domain and semantic specific
components. In this scenario, the only feasible strategy is to
dynamically adapt our model as we receive target domain data at test
time, in a continuous fashion. This setting is called Continuous DA and
multiple works tried to address it before the deep learning breakthrough
by e.g. manifold-based techniques [ hoffman2014continuous ] and low-rank
exemplar SVM [ li2018domain ] .

Eventually, we could have information about the target domain shift in
the form of metadata describing the visual inputs we should expect. This
scenario is called Predictive DA (PDA) and assumes the presence of a
single source domain and multiple auxiliary ones and that each domain
has its own respective metadata [ yang2016multivariate ] . Understanding
how a metadata links to the domain-specific parameters, allows us to
infer a model for any target domain given its respective description.

The first part of this thesis describes how we provided solutions for
the domain-shift problem, regardless of the information we have about
our source/target domain. We started from the latent domain discovery
problem, where we assume to have data of both source and target domains
but with the two being mixtures of multiple hidden domains. In this
particular scenario, we show how a weighted version of
batch-normalization (BN) [ ioffe2015batch ] , coupled with a domain
discovery branch can equip a deep architecture with the ability to
discover latent domains for DA [ mancini2018boosting ,
mancini2019inferring ] . We will show how, the same domain classifier
can be applied to the more complex DG task, where no data is available
about our target domain. In particular, the similarity among the domains
can be used either within the network (i.e. through BN layers [
mancini2018robust ] ) or at classification level [ mancini2018best ] to
effectively tackle DG. Finally, we will extend BN-based DA algorithms to
the PDA scenario by relating domains and their specific parameters
through a graph, where each node is a domain (with attached parameters)
and the weight of each edge depends on the similarity among the domains,
as given by the available metadata [ mancini2019adagraph ] . Moreover,
we provide a simple extension of BN to tackle the Continuous DA problem,
showing the effectiveness of this algorithm both on challenging robotics
scenarios [ mancini2018kitting ] and as a tool to refine the target
model predicted by our PDA algorithm [ mancini2019adagraph ] .

#### 1.1.2 Semantic shift: breaking model’s semantic limits

The second major problem we must tackle, if we want to recognize new
semantic concepts in unseen domains, is to understand how to integrate
novel knowledge within our deep architecture, thereby overcoming the
semantic shift problem. To this extent, multiple works have tried to
extend the knowledge base of a pre-trained deep model, and, depending on
the information we have regarding the new concepts, we can split them
into three main categories.

In the case where we have data available for our new concepts, we are in
the incremental learning scenario [ rebuffi2017icarl ,
kirkpatrick2017overcoming , li2017learning ] . In incremental learning
(IL), we have a pre-trained model and we receive data of the new
classes/tasks in successive learning stages without having access to the
original training set. The goal is to sequentially learn new
classes/tasks as new data are available while not forgetting previous
knowledge, thereby addressing the catastrophic forgetting problem.

A special case is when we want our model to not only acquire new
knowledge but also to detect unseen concepts. This is the goal of
open-world recognition (OWR), where the task is to classify images if
they belong to the categories of the training set, to spot samples
corresponding to unknown classes, and based on such unknown class
detections update the model to progressively include the novel
categories [ bendale2015towards ] .

A second scenario assumes that just one or few samples are available for
the novel semantic concepts. This is the case of one and few-shot
learning [ fei2006one , vinyals2016onematching , snell2017prototypical ,
sung2018fewlearning ] , where we make use of the available training data
to build a model capable of inferring the classifier for the novel
classes, given a little amount of data. Solutions to this problem
usually rely on classifier regression [ kozerawski2018clear ] , weight
imprinting [ qi2018wi1 , snell2017prototypical ] and meta-learning
techniques [ finn2017model , sun2019fewmeta ] .

Finally, we might face the extreme case where no training data is
available for the new categories we want to recognize. This research
thread is Zero-Shot Learning [ lampert2013awa , akata2013label ,
xian2018zeroshotgood ] where the goal is to recognize semantic concepts
that were not seen during training, given external information about the
novel classes. This information is available either in the form of
manually annotated attributes, visual descriptions, or word embeddings [
akata2015evaluation , xian2018zeroshotgood ] .

In the second part of the thesis, we explore ways to include novel
semantic concepts within a pre-trained architecture. In particular, we
start by considering multi-task/domain learning, where the goal is to
sequentially learn multiple classifiers for different domains/tasks from
a single pre-trained model. To this extent, we propose an algorithm
based on task-specific binary masks applied on top of the parameters of
the pre-trained model. We show how while requiring very few additional
parameters, our algorithm achieves performance comparable to
task-specific fine-tuned models.

Furthermore, we move towards the incremental class learning scenario,
considering OWR. For this, we develop the first end-to-end trainable
architecture for OWR [ mancini2019knowledge ] , based on a deep
extension of non-parametric classifiers, i.e. NCM and NNO [
mensink2012metric , guerriero2018deep , bendale2015towards ] . We also
show how we can improve the performances of this algorithm by
considering clustering strategies that can push samples closer to their
class-specific centroid while distancing them from the ones of other
classes [ fontanel2020boosting ] .

Finally, we explore the application of incremental class learning (ICL)
techniques in the task of semantic segmentation [ cermelli2020modeling ]
. Here we discover that the performance of standard approaches is
hampered by the semantic content of the background class, which changes
among different incremental steps. We call this problem background
semantic shift and we provide the first solution to it through a simple
yet effective modification of the logits used within standard
distillation and entropy-based losses.

#### 1.1.3 Recognizing unseen categories in unseen domains

An open research question is whether we can address the domain and
semantic shift problems together, producing a deep model able to
recognize new semantic concepts in possibly unseen domains. In the third
part of this thesis, we will start analyzing how we can merge these two
worlds, providing a first attempt in this direction in an offline but
quite extreme setting. In particular, we consider a scenario where,
during training, we are given a set of images of multiple domains and
semantic categories and our goal is to build a model that can to
recognize images of unseen concepts, as in ZSL, in unseen domains, as in
DG. This new problem, which we called ZSL+DG, poses novel research
questions which go beyond the ones of DG and ZSL problems, if taken in
isolation. For instance, we can rely on the fact that multiple source
domains permit to disentangle semantic and domain-specific information,
as in DG. Despite this, we have no guarantee that the disentanglement
will hold for the unseen semantic categories at test time. Additionally,
while in ZSL it is reasonable to assume that the learned mapping between
images and semantic attributes will generalize also to images of unseen
concepts, in ZSL+DG we have no guarantee that this will happen for
images of unseen domains.

To tackle this problem, we propose a solution based on a variant of the
well-known mixup regularization strategy [ zhang2017mixup ] . In
particular, we show how we can use mixup to simulate features of novel
domains and semantic concepts during training, achieving
state-of-the-art performances in both DG, ZSL, and in the novel ZSL+DG
scenario [ mancini2020dgzsl ] . Up to our knowledge, this is the first
algorithm able to work in both worlds, recognizing unseen semantic
concepts in unseen domains.

### 1.2 Contributions

Focusing on visual recognition, this thesis contributes towards
developing deep learning architectures able to cope with test images
containing both different visual domains (i.e. domain shift) as well as
new semantic concepts (i.e. semantic shift) unseen during the initial
training phase. To this extent, we can divide the main contributions
into three parts. The first contains techniques able to attack the
well-known domain shift problem of classical DA by considering
non-canonical scenarios where the amount of information regarding either
the source(s) or the target(s) domains varies. The second part contains
algorithms that are able to extend pre-trained architectures with new
semantic concepts (i.e. tasks or classes) using external datasets not
available during the initial training phase. The goal of these
algorithms is to produce models capable of recognizing previously unseen
concepts without hampering the performances on old ones. In the third
part, we start exploring the recognition of unseen semantic concepts in
unseen visual domains, presenting one of the first works merging these
two worlds. In the following, we will describe the specific
contributions presented in each part.

Modeling the Domain Shift In the context of attacking the domain shift
problem, we will present:

-   The first deep learning model capable of discovering latent domains
    in unsupervised domain adaptation, when the source domain is
    composed of a mixture of multiple visual domains [
    mancini2018boosting , mancini2019inferring , mancini2019discovering
    ] . Specifically, the architecture is based on two main components,
    i.e. a side branch that automatically computes the assignment of
    each sample to its latent domain and novel layers that exploit
    domain membership information to appropriately align the
    distribution of the CNN internal feature representations to a
    reference distribution.

-   Two domain similarity-based frameworks for Domain Generalization [
    mancini2018robust , mancini2018best ] . The frameworks rely on the
    idea that, given a set of different classification models associated
    with known domains (e.g. corresponding to multiple environments,
    robots), the best model for a new sample in the novel domain can be
    computed directly at test time by optimally combining the known
    models. While in [ mancini2018robust ] the combination is held out
    through the statistics of batch-normalization layers [
    ioffe2015batch ] , in [ mancini2018best ] a similar principle is
    applied at classification level.

-   A simple yet effective algorithm for Continuous DA in Robotics [
    mancini2018kitting ] . The algorithm is based on an online update of
    standard batch-normalization layers. We show the effectiveness of
    our algorithm on a newly collected dataset with challenging robotic
    scenarios, containing various illumination conditions, backgrounds,
    and viewpoints.

-   The first deep learning model that can tackle Predictive DA [
    mancini2019adagraph ] . In this scenario no target data are
    available and the system has to learn to generalize from annotated
    source images plus unlabeled samples with associated metadata from
    auxiliary domains. We inject metadata information within a deep
    architecture by encoding the relation between different domains
    through a graph. Given the target domain metadata, our approach
    produces the target model by a weighted combination of the
    domain-specific parameters associated to the graph nodes. We also
    propose to refine the predicted target model through the incoming
    stream of target data directly at test time, extending [
    mancini2018kitting ] .

Modeling the Semantic Shift . In the context of including new semantic
concepts to a pre-trained architecture, we will present:

-   An effective algorithm performing multi-domain learning [
    mancini2018adding , mancini2020boostingmva ] . The algorithm builds
    on previous works by masking the weights of a pre-trained
    architecture through task/domain-specific binary filters [
    mallya2018piggyback ] . However, we take into account more
    elaborated affine transformations of the binary masks, showing that
    our generalization achieves significantly higher levels of
    adaptation to new tasks, with performances comparable to fine-tuning
    strategies while requiring slightly more than 1 bit per network
    parameter per additional task. With this strategy, we achieve
    results close to the state of the art in the Visual Domain Decathlon
    challenge [ rebuffi2017learning ] .

-   An incremental class learning algorithm for semantic segmentation
    which explicitly models the background semantic shift problem [
    cermelli2020modeling ] . In particular, we identify and analyze the
    problem of semantic shift of the background class in incremental
    learning for semantic segmentation. This problem arises since the
    background class might contain both old as well as still unseen
    classes. This exacerbates the catastrophic forgetting problem and
    hampers the ability to learn novel concepts. To tackle this issue,
    we propose a new distillation-based algorithm with an objective
    function and a classifier initialization strategy that explicitly
    model the semantic shift of the background class. The proposed
    algorithm largely outperforms standard incremental learning methods
    in different benchmarks.

-   The first deep architecture that can to perform open-world
    recognition (OWR) [ mancini2019knowledge ] . The proposed deep
    network is based on a deep extension of a non-parametric model, [
    bendale2015towards ] and it can detect whether a perceived object
    belongs to the set of categories known by the system and learns
    without the need to retrain the whole system from scratch. In a
    first study [ mancini2019knowledge ] , we considered both the cases
    where annotated images about the new category can be provided by an
    ’oracle’ (i.e. human supervision), or by autonomous mining of the
    Web. In a second instance [ fontanel2020boosting ] , we show how
    clustering-based techniques can boost the performances of this OWR
    framework.

Modeling the Semantic and Domain Shift together . In the context of
merging the two worlds, we will describe the new ZSL+DG problem [
mancini2020dgzsl ] where, at test time, images of unseen domains as well
as unseen classes must be correctly classified. Additionally, we will
present the first holistic method capable of addressing ZSL and DG
individually and both combined together (ZSL+DG). Our method is based on
simulating new domains and categories during training by mixing the
available training domains and classes both at the image and feature
levels. The mixing strategy becomes increasingly more challenging during
training, in a curriculum fashion. The extensive experimental analysis
show the effectiveness of our approach in all settings: ZSL, DG, and
ZSL+DG.

### 1.3 Outline

Chapter 2 will discuss the domain shift problem. It will first give an
overview of the problems (Section 2.1 ) and the related works (Section
2.2 ), delving into the details of the Domain Alignment Layers of [
carlucci2017just , carlucci2017autodial ] , which serve as a starting
point for our works. In Section 2.4 , we will describe our multi-domain
Alignment Layers which allows us to model multiple but mixed source
domains through weighted normalization and a domain classifier for
unsupervised domain adaptation. In Section 2.5 , 2.6 and 2.7 we will
consider the case where no target data are available. In particular, in
Section 2.5 we will extend the multi-domain Alignment Layers to the
domain generalization scenario and we show how the domain classifier can
be used as a proxy to merge activations from layers beyond normalization
ones for effective DG. In Section 2.6 , we present ONDA , a continuous
DA approach which makes use of continuous update of normalization
statistics as target data arrive. Finally, in Section 2.7 , we present
AdaGraph , a first deep learning-based approach for predictive domain
adaptation which merges normalization statistics of different layers
based on the given vectorized description of the target domain.

Chapter 3 will lead us to the semantic shift problem. It will start by
presenting a general problem definition (Section 3.1 ) with an overview
of the related works (Section 3.2 ). It will then describe BAT (Section
3.3 ), an approach for multi-domain learning where task-specific binary
masks are affinely transformed to obtain a good trade-off among
performances and parameters. In Section 3.4 , we identify the
background-shift problem on incremental class learning for semantic
segmentation and we describe MiB , the first method addressing it, by
changing how background probabilities are treated in standard entropy
losses. Finally, in Section 3.5 , we will describe the DeepNNO , a first
deep approach for Open World Recognition, and how we can improve this
model with clustering and learned rejection thresholds.

Chapter 4 will discuss the importance of tackling both domain and
semantic shift together (Section 4.1 ) and the works that pushed towards
this direction (Section 4.2 ). We will then present a new task,
zero-shot learning under domain generalization and a first holistic
method, CuMix , addressing domain and semantic shift together, using
increasingly more complex mixing of samples and features.

The thesis concludes by summarizing the findings, open problems, and
possible future direction of research in Chapter 5 .

### 1.4 Publications

In the following, the author’s publications are listed in chronological
order. Note that some articles (marked with *) have not been included in
the thesis.

-   * M. Mancini, S. Rota Bulò, E. Ricci, B. Caputo
    Learning Deep NBNN Representations for Robust Place Categorization
    IEEE Robotics and Automation Letters, May 2017, vol. 3, n. 2., pp.
    1794-1801. Presented at IEEE/RSJ International Conference on
    Intelligent Robots and Systems (IROS) 2017.

-   M. Mancini, L. Porzi, S. Rota Bulò, B. Caputo, E. Ricci
    Boosting Domain Adaptation by Discovering Latent Domains
    IEEE International Conference on Computer Vision and Pattern
    Recognition (CVPR) 2018. (spotlight)

-   M. Mancini, S. Rota Bulò, B. Caputo, E. Ricci
    Robust Place Categorization with Deep Domain Generalization
    IEEE Robotics and Automation Letters, July 2018, vol. 3, n. 3., pp.
    2093-2100.

-   M. Mancini, E.Ricci, B. Caputo, S. Rota Bulò
    Adding New Tasks to a Single Network with Weight Transformations
    using Binary Masks
    European Computer Vision Conference Workshop on Transferring and
    Adapting Source Knowledge in Computer Vision 2018. (best paper award
    honorable mention)

-   M. Mancini, S. Rota Bulò, B. Caputo, E. Ricci
    Best sources forward: domain generalization through source-specific
    nets
    IEEE International Conference on Image Processing (ICIP) 2018.

-   M. Mancini, H. Karaoguz, E. Ricci, P. Jensfelt, B. Caputo
    Kitting in the Wild through Online Domain Adaptation
    IEEE/RSJ International Conference on Intelligent Robots and Systems
    (IROS) 2018.

-   M. Mancini, H. Karaoguz, E. Ricci, P. Jensfelt, B. Caputo
    Knowledge is Never Enough: Towards Web Aided Deep Open World
    Recognition
    IEEE International Conference on Robotics and Automation
    (ICRA) 2019.

-   M. Mancini, S. Rota Bulò, B. Caputo, E. Ricci
    AdaGraph: Unifying Predictive and Continuous Domain Adaptation
    through Graphs
    IEEE/CVF International Conference on Computer Vision and Pattern
    Recognition (CVPR) 2019. ( oral )

-   * M. Mancini, L. Porzi, F. Cermelli, B. Caputo
    Discovering Latent Domains for Unsupervised Domain Adaptation
    through Consistency
    International Conference on Image Analysis and Processing
    (ICIAP) 2019.

-   * F. Cermelli, M. Mancini, E. Ricci, B. Caputo
    The RGB-D Triathlon: Towards Agile Visual Toolboxes for Robots
    IEEE/RSJ International Conference on Intelligent Robots and Systems
    (IROS) 2019.

-   M. Mancini, L. Porzi, S. Rota Bulò, B. Caputo, E. Ricci
    Inferring Latent Domains for Unsupervised Deep Domain Adaptation
    IEEE Transactions on Pattern Analysis & Machine Intelligence 2019.

-   * L. O. Vasconcelos, M. Mancini, D. Boscaini, B. Caputo, E. Ricci
    Structured Domain Adaptation for 3D Keypoint Estimation
    International Conference on 3D Vision (3DV) 2019. ( (oral )

-   F. Cermelli, M. Mancini, E. Ricci, B. Caputo
    Modeling the Background for Incremental Learning in Semantic
    Segmentation
    IEEE/CVF International Conference on Computer Vision and Pattern
    Recognition (CVPR) 2020.

-   M. Mancini, E.Ricci, B. Caputo, S. Rota Bulò
    Boosting Binary Masks for Multi-Domain Learning through Affine
    Transformations .
    Machine Vision and Applications, June 2020, vol. 31, n. 6, pp. 1-14.

-   D. Fontanel, F. Cermelli, M. Mancini, S. Rota Buló, E. Ricci, B.
    Caputo
    Boosting Deep Open World Recognition by Clustering
    IEEE Robotics and Automation Letters, October 2020, vol. 5, no. 4,
    pp. 5985-5992. Presented at IEEE/RSJ International Conference on
    Intelligent Robots and Systems (IROS) 2020.

-   M. Mancini, Z. Akata, E. Ricci, B. Caputo
    Towards Recognizing Unseen Categories in Unseen Domains .
    European Computer Vision Conference (ECCV) 2020.

-   * L. O. Vasconcelos, M. Mancini, D. Boscaini, S. Rota Buló, B.
    Caputo, E. Ricci
    Shape Consistent 2D Keypoint estimation under Unsupervised Domain
    Adaptation .
    International Conference on Pattern Recognition (ICPR) 2020.

## Chapter 2 Recognition across New Visual Domains

This chapter presents various strategies to tackle the domain shift
problem in the presence of different information regarding the source
and target domains. We start by providing a general formulation of the
problem (Sec. 2.1 ). We then review related literature (Sec. 2.2 ),
analyzing the Domain Alignment layers for DA (Sec. 2.3 ), introduced in
previous works [ carlucci2017just , carlucci2017autodial ,
li2016revisiting ] . In the remaining sections, we describe how we
extended the Domain Alignment layers to address non-canonical DA
settings. We start with the latent-domain discovery problem (Sec. 2.4 ),
where we have multiple source/target domains but mixed, i.e. we do not
know to which domain each sample belongs to. We describe the first deep
learning solution to this problem [ mancini2018boosting ,
mancini2019inferring ] based on a weighted computation of the
batch-normalization statistics [ ioffe2015batch ] both at training (in
case of mixed source domains) and at inference time (in case of mixed
targets). In Sec. 2.5 , we show how a similar approach can be applied to
tackle the domain generalization problem [ mancini2018robust ] ,
removing the assumption of having target data at training time.
Additionally, we show how to extend the same idea beyond
batch-normalization layers, mixing activations of domain-specific
classification modules [ mancini2018best ] . In Sec. 2.6 , we take a
step further, removing the assumption of having multiple source domains
during training, developing a model able to adapt to arbitrary target
domains at inference time, dynamically updating its internal knowledge,
in a continuous fashion [ mancini2018kitting ] . Finally, in Sec. 2.7 ,
we provide a solution to the Predictive DA scenario, where we must use
multiple auxiliary domains with associated metadata during training to
learn the relationship among metadata and domains. We then exploit this
knowledge to generate a model for the target domain given just its
description in terms of metadata. Our solution, called AdaGraph [
mancini2019adagraph ] , is based on multiple domain-specific
batch-normalization layers connected through a graph that we use at
inference time to produce a model for the target domain. AdaGraph is the
first deep learning-based approach to tackle the Predictive DA problem.
In [ mancini2019adagraph ] , we also extend the continuous DA approach
in [ mancini2018kitting ] to dynamically refine the predicted models at
test time.

### 2.1 Problem statement

As described in Section 1.1.1 , the goal of DA algorithms is to transfer
knowledge from a large labeled dataset, i.e. the source domain, to a
small and/or unlabeled one, i.e. the target . In particular, throughout
this work, we will focus on the case where the target domain is either
fully unsupervised or not present at all during training.

The first case is the Unsupervised Domain Adaptation problem (UDA).
Formally, we can define the UDA problem as follows. Let us denote with
@xmath our input space (e.g. the image space), with @xmath our output
space (e.g. the set of possible semantic classes) and with @xmath the
set of possible visual domains (e.g. environments, illumination
conditions). Denoting with @xmath the set of our source domain(s), we
can define our supervised training set as @xmath where @xmath , @xmath
and @xmath . Moreover, let us define our unsupervised target dataset as
@xmath , with @xmath , and @xmath . Note that we assume source and
target domains to differ, i.e. @xmath . Moreover, due to the domain
shift, each domain has different joint distribution defined over @xmath
: we have @xmath with @xmath , @xmath and @xmath . Our goal is to learn
a mapping @xmath which is effective for each of our target domain(s)
@xmath .

From our formulation, we have the standard single-source/target scenario
when @xmath , while the multi-source scenario when @xmath . In both
cases, @xmath is assumed available during training. In case both @xmath
and @xmath are available but at least one of them is composed of an
unknown mixture of domains (i.e. @xmath with unknown @xmath and/or
@xmath ), we are in the latent domain discovery scenario and we have no
domain identifier @xmath in the triplets of @xmath and @xmath .

In case @xmath is not available during training but @xmath , we are in
the Domain Generalization (DG) scenario. In this setting, we can exploit
the presence of multiple source domains, even latent, to disentangle
domain and semantic specific components from our inputs, producing a
model robust to any possible target domain.

In @xmath is not available during training and @xmath , we cannot
disentangle domain-specific and semantic-specific information. However,
we can still cope with the domain shift problem in different ways,
depending on the information we have about our target. If no information
is available, we can only adapt our model at test time, while
classifying samples of the target domain. This is known as the
Continuous/Online DA scenario.

Lastly, another scenario is Predictive DA (PDA) . In this case, we have
a set of auxiliary domains @xmath forming an additional training dataset
@xmath . Moreover, the domain identifiers @xmath are expressed as
metadata . Using the auxiliary set @xmath and the domain metadata, we
can learn a mapping among metadata and domain-specific parameters. Then,
given target metadata @xmath , we can infer its domain-specific
parameters, reducing the domain shift problem.

In the following section, we will review the relevant literature for DA
and each of the previously mentioned problem. As a final remark, it is
worth highlighting that, in this chapter, we assume source and target
domains sharing the same output space @xmath . In Chapter 3 we will
consider the case where the visual domains are shared among train and
test data (i.e. @xmath ) but the semantic classes differ and/or varies
over time. Finally, in Chapter 4 we will consider the scenario where
both the output and the domain space differ among train and test.

### 2.2 Related Works

In this section we will review previous works on DA. We start by
reviewing DA methods, based on both hand-crafted and deep features, in
standard scenarios where target domain data are available. We then
review previous works tackling the domain shift problem without target
domain data, starting from DG techniques and covering less explored
directions, such as Continuous and Predictive DA.

DA methods with hand-crafted features. Earlier DA approaches operate on
hand-crafted features and attempt to reduce the discrepancy between the
source and the target domains by adopting different strategies. For
instance, instance-based methods [ huang2007correcting , yamada2012no ,
gong2013connecting ] develop from the idea of learning
classification/regression models by re-weighting source samples
according to their similarity with the target data. A different strategy
is exploited by feature-based methods, coping with domain shift by
learning a common subspace for source and target data such as to obtain
domain-invariant representations [ gong2012geodesic , long2013transfer ,
fernando2013unsupervised ] . Parameter-based methods [ yang2007adapting
] address the domain shift problem by discovering a set of shared
weights between the source and the target models. However, they usually
require labeled target data which is not always available.

While most earlier DA approaches focus on a single-source and
single-target setting, some works have considered the related problem of
learning classification models when the training data spans multiple
domains [ mansour2009domain , duan2009domain , sun2011two ] . The common
idea behind these methods is that when source data arises from multiple
distributions, adopting a single source classifier is suboptimal and
improved performance can be obtained by leveraging information about
multiple domains. However, these methods assume that the domain labels
for all source samples are known in advance. In practice, in many
applications the information about domains is hidden and latent domains
must be discovered into the large training set. Few works have
considered this problem in the literature. Hoffman et al. [
hoffman2012discovering ] address this task by modeling domains as
Gaussian distributions in the feature space and by estimating the
membership of each training sample to a source domain using an iterative
approach. Gong et al. [ gong2013reshaping ] discover latent domains by
devising a nonparametric approach which aims at simultaneously achieving
maximum distinctiveness among domains and ensuring that strong
discriminative models are learned for each latent domain. In [
xiong2014latent ] domains are modeled as manifolds and source images
representations are learned decoupling information about semantic
category and domain. By exploiting these representations the domain
assignment labels are inferred using a mutual information based
clustering method.

Deep Domain Adaptation. Most recent works on DA consider deep
architectures and robust domain-invariant features are learned using
either supervised neural networks [ long2015learning ,
tzeng2015simultaneous , ganin2014unsupervised , ghifary2016deep ,
bousmalis2016domain , carlucci2017autodial ] , deep autoencoders [
zeng2014deep ] or generative adversarial networks [
bousmalis2016unsupervised , shrivastava2017learning ] . Research efforts
can be grouped in terms of the number of source domains available at
training time.

In the single-source DA setting, we can identify two main strategies.
The first deals with features and aims at learning deep domain invariant
representations. The idea is to introduce in the learning architecture
different measures of domain distribution shift at a single or multiple
levels [ LongZ0J17 , sun2016return , carlucci2017autodial ,
carlucci2017just ] and then train the network to minimize these measures
while also reducing a task-specific loss, for instance for
classification or detection. In this way the network produces features
invariant to the domain shift, but still discriminative for the task at
hand. Besides distribution evaluations, other domain shift measures used
similarly are the error in the target sample reconstruction [
ghifary2016deep ] , or various coherence metrics on the pseudo-labels
assigned by the source models to the target data [ sener2016learning ,
haeusser2017associative , saito2017asymmetric ] . Finally, a different
group of feature-based methods rely on adversarial loss functions [
tzeng2015simultaneous , ganin2016domain ] . The method proposed in [
sankaranarayanan2018generate ] , that push the network to be unable to
discriminate whether a sample coming from the source or from the target,
is an interesting variant of [ ganin2016domain ] , where the domain
difference is still measured at the feature level but passing through an
image reconstruction step. Besides integrating the domain discrimination
objective into end-to-end classification networks, it has also been
shown that two-step networks may have practical advantages [
tzeng2017adversarial , angeletti2018adaptive ] .

The second popular deep adaptive strategy focuses on images . The
described adversarial logic that demonstrated its effectiveness for
feature-based methods, has also been extended to the goal of reducing
the visual domain gap. Powerful GAN [ goodfellow2014generative ] methods
have been exploited to generate new images or perturb existing ones to
resemble the visual style of a certain domain, thus reducing the
discrepancy at pixel level [ bousmalis2017unsupervised ,
shrivastava2017learning ] . Most of the works based on image adaptation
aim at generating either target-like source images or source-like target
images, but it has been recently shown that integrating both the
transformation directions is highly beneficial [ russo2018sbadagan ] .

In practical applications one may be offered more than one source
domain. This has triggered the study of multi-sources DA algorithms. The
multi-source setting was initially studied from a theoretical point of
view, focusing on theorems indicating how to optimally sub-select the
data to be used in learning the source models [ crammer2008learning ] ,
or proposing principled rules for combining the source-specific
classifiers and obtain the ideal target class prediction [
mansour2009domain ] . Several other works followed this direction in the
shallow learning framework. When dealing with shallow-methods the naïve
model learned by collecting all the source data in single domain without
any adaptation was usually showing low performance on the target. It has
been noticed that this behavior changes when moving to deep learning,
where the larger number of samples as well as their variability supports
generalization and usually provides good results on the target. Only
very recently two methods presented multi-source deep learning
approaches that improve over this reference. The approach proposed in [
xu2018deep ] builds over [ ganin2016domain ] by replicating the
adversarial domain discriminator branch for each available source.
Moreover these discriminators are also used to get a perplexity score
that indicates how the multiple sources should be combined at test time,
according to the rule in [ mansour2009domain ] . A similar multi-way
adversarial strategy is used in [ MDAN_ICLRW18 ] , but this work comes
with a theoretical support that frees it from the need of respecting a
specific optimal source combination and thus from the need of learning
the source weights.

While recent deep DA methods significantly outperform approaches based
on hand-crafted features, the vast majority of them only consider
single-source, single-target settings. Moreover, almost all work
presented in the literature so far assume to have direct access to
multiple source domains, where in many practical applications such
knowledge might not be directly available, or costly to obtain in terms
of time and human annotators. To our knowledge, our works [
mancini2018boosting , mancini2019inferring ] are the first works
proposing a deep architecture for discovering latent source domains and
exploiting them for improving classification performance on target data.

Domain Generalization. Opposite to domain adaptation [ csurka2017domain
] , where it is assumed that target data are available in the training
phase, the key idea behind DG is to learn a domain agnostic model to be
applied to any unseen target domain. Although less researched than
domain adaptation, the need for DG algorithms has been recognized for
quite some time in the literature [ muandet2013domain ] .

Previous DG methods can be broadly grouped into four main categories.
The first category comprises methods which attempt to learn
domain-invariant feature representations by considering specific
alignment losses, such as maximum mean discrepancy (MMD), adversarial
loss or self-supervised losses. Notable approaches in this category are
[ muandet2013domain , li2018domainadv , carlucci2019domain ] . The
second category of methods [ li2017deeper , khosla2012undoing ] develop
from the idea of creating deep architectures where both domain-agnostic
and domain-specific parameters are learned on source domains. After
training, only the domain-agnostic part is retained and used for
processing target data. The third category devise specific optimization
strategies or training procedures in order to enhance the generalization
ability of the source model to unseen target data. For instance, in [
li2018learning ] a meta-learning approach is proposed for DG.
Differently, in [ li2019episodic ] an episodic training procedure is
presented to learn models robust to the domain shift. The latter
category comprises methods which introduce data and feature augmentation
strategies to synthesise novel samples and improve the generalization
capability of the learned model [ shankar2018generalizing ,
volpi2018generalizing , volpi2019addressing ] . These strategies are
mostly based either on adversarial training [ shankar2018generalizing ,
volpi2018generalizing ] or data augmentation [ volpi2019addressing ] .

Beyond DG: Domain Adaptation without Target Data. DG assumes that
multiple source domains are available, in some applications this
assumption might not hold. This calls for DA methods able to cope with
the domain shift when i) only one source domain is available and ii) no
target data are available in the training phase. Depending on their
available information, these methods can work by exploiting e.g. the
stream of incoming target samples, or side information describing
possible future target domains. Note that, differently from DG, these
methods produce models which are not robust to any possible target
domain, but must be re-adapted if the target domain changes

The first scenario is typically referred as continuous [
hoffman2014continuous ] or online DA [ mancini2018kitting ] . To address
this problem, in [ hoffman2014continuous ] a manifold-based DA technique
is employed, such as to model an evolving target data distribution. In [
li2018domain ] Li et al. propose to sequentially update a low-rank
exemplar SVM classifier as data of the target domain become available.
In [ lampert2015predicting ] , the authors propose to extrapolate the
target data dynamics within a reproducing kernel Hilbert space.

The second scenario corresponds to the problem of Predictive DA (PDA).
PDA is introduced in [ yang2016multivariate ] , where a multivariate
regression approach is described for learning a mapping between domain
metadata and points in a Grassmanian manifold. Given this mapping and
the metadata for the target domain, two different strategies are
proposed to infer the target classifier. In Section 2.7 , we show how it
is possible to address this task with deep architectures, using
batch-normalization layers [ ioffe2015batch ] .

Other closely related tasks are the problems of zero shot domain
adaptation and domain generalization. In zero-shot domain adaptation [
peng2018zero ] the task is to learn a prediction model in the target
domain under the assumption that task-relevant source-domain data and
task-irrelevant dual-domain paired data are available. Domain
generalization methods [ muandet2013domain , li2017deeper ,
dinnocente2018domain , motiian2017unified ] attempt to learn
domain-agnostic classification models by exploiting labeled source
samples from multiple domains but without having access to target data.
Similarly to Predictive DA, in domain generalization multiple datasets
are available during training. However, in PDA data from auxiliary
source domains are not labeled.

### 2.3 Preliminaries: Domain Alignment Layers

Batch-normalization [ ioffe2015batch ] (BN) is a common strategy used in
deep architectures for stabilizing the optimization problem, making the
gradients more well-behaved, and enabling a faster and more effective
training [ santurkar2018helpbn1 , bjorck2018helpbbn2 ] . BN works by
normalizing the input features to a fixed, target distribution, i.e. a
standard Gaussian. Recent works [ li2016revisiting , carlucci2017just ,
carlucci2017autodial ] have shown how we can use BN layers to perform
domain adaptation in a traditional batch setting. In the following, we
will denote BN layers with domain-specific statistics as Domain
Alignment layers (DA-layers).

DA-layers [ li2016revisiting , carlucci2017just , carlucci2017autodial ]
are motivated by the observation that, in general, activations within a
neural network follow domain-dependent distributions. As a way to reduce
domain shift, the activations are thus normalized in a domain-specific
way, shifting them according to a parameterized transformation in order
to match their first and second-order moments to those of a reference
distribution, which is generally chosen to be normal with zero mean and
unit standard deviation. While most previous works only considered
settings with two domains, i.e. source and target, the basic idea can be
applied to any number of domains, as long as the domain membership of
each sample point is known. Specifically, denoting as @xmath the
distribution of activations for a given feature channel and domain
@xmath , an input @xmath to the DA-layer can be normalized according to

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

where @xmath , @xmath are mean and variance of the input distribution,
respectively, and @xmath is a small constant to avoid numerical issues.
In practice, when the statistics @xmath and @xmath are computed over the
current mini-batch, we obtain the application of standard batch
normalization separately to the sample points of each domain.

The main idea behind these works is to create a deep architecture with
one parallel branch per domain, where all branches share the same
parameters but embed different, domain-specific, BN layers (i.e.
different statistics within DA-layers). The domain-specific BN layers
align the distributions of features of different domains to the same
reference distribution, achieving the desired domain adaptation effect.
In the following sections, we will show how variants of DA-layers can be
successfully applied in multiple distinct DA scenarios, even without the
presence of target domain data during the initial training phase.

### 2.4 Latent Domain Discovery 111M. Mancini, L. Porzi, S. Rota Bulò,
B. Caputo, E. Ricci. Boosting Domain Adaptation by Discovering Latent
Domains. IEEE International Conference on Computer Vision and Pattern
Recognition (CVPR) 2018.222M. Mancini, L. Porzi, S. Rota Bulò, B.
Caputo, E. Ricci. Inferring Latent Domains for Unsupervised Deep Domain
Adaptation. IEEE Transactions on Pattern Analysis & Machine Intelligence
2019.

As stated in Section 2.2 , the problem of Unsupervised DA has been
widely studied and both theoretical results [ ben2010theory ,
mansour2009domain ] and several algorithms have been developed, both
considering shallow models [ huang2007correcting , gong2013connecting ,
gong2012geodesic , long2013transfer , fernando2013unsupervised ] and
deep architectures [ long2015learning , tzeng2015simultaneous ,
ganin2014unsupervised , long2016unsupervised , ghifary2016deep ,
carlucci2017autodial , bousmalis2016domain ] . While deep neural
networks tend to produce more transferable and domain-invariant features
with respect to shallow models, previous works have shown that the
domain shift is only alleviated but not entirely removed [
donahue2014decaf ] .

Most previous works on UDA focus on a single-source and single-target
scenario. However, in many computer vision applications labeled training
data are often generated from multiple distributions, i.e. there are
multiple source domains. Examples of multi-source DA problems arise when
the source set corresponds to images taken with different cameras,
collected from the web or associated to multiple points of views. In
these cases, a naive application of single-source domain adaptation
algorithms would not suffice, leading to poor results. Analogously,
target samples may arise from more than a single distribution and
learning multiple target-specific models may improve significantly the
performance. Therefore, in the past several research efforts have been
devoted to develop domain adaptation methods considering multiple source
and target domains [ mansour2009domain , duan2009domain , sun2011two ,
xu2018deep ] . However, these approaches assume that the multiple
domains are known. A more challenging problem arises when training data
correspond to latent domains, i.e. we can make a reasonable estimate on
the number of source and target domains available, but we have no
information, or only partial, about domain labels. To address this
problem, known in the literature as latent domain discovery , previous
works have proposed methods which simultaneously discover hidden source
domains and use them to learn the target classification models [
hoffman2012discovering , gong2013reshaping , xiong2014latent ] .

This section introduces the first approaches [ mancini2018boosting ,
mancini2019inferring ] based on deep neural networks able to
automatically discover latent domains in multi-source, multi-target UDA
setting. Our method is inspired from the Domain Alignment Layers
described in Section 2.3 , introduced by [ carlucci2017autodial ,
carlucci2017just ] . Our approach develops from the same intuition of
Domain Alignment Layers, i.e. aligning representations of source and
target distributions to a reference Gaussian. However, to address the
additional challenges of discovering and handling multiple latent
domains, we propose a novel architecture which is able to (i) learn a
set of assignment variables which associate source and target samples to
a latent domain and (ii) exploit this information for aligning the
distributions of the internal CNN feature representations and learn
robust target classifiers (Fig. 2.1 ). Our experimental evaluation shows
that the proposed approach alleviates the domain discrepancy and
outperforms previous UDA techniques on popular benchmarks, such as
Office-31 [ saenko2010adapting ] , PACS [ li2017domain ] and
Office-Caltech [ gong2012geodesic ] .

To summarize, the contributions presented in this section are threefold.
Firstly, we introduce a novel deep learning approach for unsupervised
domain adaptation which operates in a multi-source, multi-target
setting. Secondly, we describe a novel architecture which is not only
able to handle multiple domains, but also permits to automatically
discover them by grouping source and target samples. Thirdly, our
experiments demonstrate that this framework is superior to many
state-of-the-art single- and multi-source/target UDA methods.

#### 2.4.1 Problem Formulation

We assume to have data belonging to one of several domains.
Specifically, as in Section 2.1 , we consider @xmath source domains,
characterized by unknown probability distributions @xmath defined over
@xmath , where @xmath is the input space (e.g. images) and @xmath the
output space (e.g. object or scene categories) and, similarly, we assume
@xmath target domains characterized by @xmath . Note that, for
simplicity, we wrote @xmath as @xmath . The numbers of source and target
domains are not necessarily known a-priori, and are left as
hyperparameters of our method.

During training we are given a set of labeled sample points from the
source domains, and a set of unlabeled sample points from the target
domains, while we can have partial or no information about the domain of
the source sample points. We model the source data as a set @xmath of
i.i.d. observations from a mixture distribution @xmath , where @xmath is
the unknown probability of sampling from a source domain @xmath .
Similarly, the target sample @xmath consists of i.i.d. observations from
the marginal @xmath of the mixture distribution over target domains.
Furthermore, we denote by @xmath and @xmath , the source data and label
sets, respectively. We assume to know the domain label for a (possibly
empty) sub-sample @xmath from the source domains and we denote by @xmath
the domain labels in @xmath of the sample points in @xmath . Note that,
differently from the general formulation in Section 2.1 , here neither
@xmath and @xmath might have domain labels available.

Our goal is to learn a predictor that is able to classify data from the
target domains. The major difficulties that this problem poses, and that
we have to deal with, are: (i) the distributions of source and target
domains can be drastically different, making it hard to apply a
classifier learned on one domain to the others, (ii) we lack direct
observation of target labels, and (iii) the assignment of each source
and target sample point to its domain is unknown, or known for a very
limited number of source sample points.

Several previous works [ long2015learning , tzeng2015simultaneous ,
ganin2014unsupervised , ghifary2016deep , bousmalis2016domain ,
carlucci2017autodial ] have tackled the related problem of domain
adaptation in the context of deep neural networks, dealing with (i) and
(ii) in the single domain case for both source and target data (i.e.
@xmath and @xmath ). In particular, some recent works have demonstrated
a simple yet effective approach based on the replacement of standard BN
layers with specific Domain Alignment layers [ carlucci2017just ,
carlucci2017autodial ] . These layers reduce internal domain shift at
different levels within the network by normalizing features in a
domain-dependent way, matching their distributions to a pre-determined
one. We revisit this idea in the context of multiple, unknown source and
target domains and introduce a novel Multi-domain DA layer (mDA-layer)
in Section 2.4.2 , which is able to normalize the multi-modal feature
distributions encountered in our setting. To do this, our mDA-layers
exploit a side-output branch attached to the main network (see Section
2.4.3 ), which predicts domain assignment probabilities for each input
sample. Finally, in Section 2.4.4 we show how the predicted domain
probabilities can be exploited, together with the unlabeled target
samples, to construct a prior distribution over the network’s parameters
which is then used to define the training objective for our network.

#### 2.4.2 Multi-domain DA-layers

In Section 2.3 , we described Domain Alignment Layers and how they are a
simple yet effective solution for doman adaptation. However, applying
them as described in Eq. ( 2.1 ) requires full domain knowledge, because
for each domain @xmath , @xmath and @xmath need to be calculated on a
data sample belonging to the specific domain @xmath . In our case,
however, we do not know the domain of the source/target sample points,
or we have only partial knowledge about that. To tackle this issue, we
propose to model the layer’s input distribution as a mixture of
Gaussians, with one component per domain ³ ³ 3 Interestingly, [
deecke2018mode ] showed how a similar strategy can be effective even
within a single domain. . Specifically, we define a global input
distribution @xmath , where @xmath is the probability of sampling from
domain @xmath , and @xmath is the domain-specific distribution for
@xmath , namely a normal distribution with mean @xmath and variance
@xmath . Given a mini-batch @xmath , a maximum likelihood estimate of
the parameters @xmath and @xmath is given by

  -- -------- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath   @xmath      (2.2)
  -- -------- -------- -------- -------- -- -------

where

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

and @xmath is the conditional probability of @xmath belonging to domain
@xmath , given @xmath . Clearly, the value of @xmath is known for all
sample points for which we have domain information. In all other cases,
the missing domain assignment probabilities are inferred from data,
using the domain prediction network branch which will be detailed in
Section 2.4.3 . Thus, from the perspective of the alignment layer, these
probabilities become an additional input, which we denote as @xmath for
the predicted probability of @xmath belonging to @xmath .

By substituting @xmath for @xmath in ( 2.3 ), we obtain a new set of
empirical estimates for the mixture parameters, which we denote as
@xmath and @xmath . These parameters are used to normalize the layer’s
inputs according to

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

where @xmath , @xmath , @xmath and @xmath is the set of source/target
latent domains. As in previous works [ carlucci2017autodial ,
carlucci2017just , ioffe2015batch ] , during back-propagation we
calculate the derivatives through the statistics and weights,
propagating the gradients to both the main input and the domain
assignment probabilities.

#### 2.4.3 Domain prediction

Our mDA-layers receive a set of domain assignment probabilities for each
input sample point, which needs to be predicted, and different
mDA-layers in the network, despite having different input distributions,
share consistently the same domain assignment for the sample points. As
a practical example, in the typical case in which mDA-layers are used in
a CNN to normalize convolutional activations, the network would predict
a single set of domain assignment probabilities for each input image,
which would then be fed to all mDA-layers and broadcasted across all
spatial locations and feature channels corresponding to that image. We
compute domain assignment probabilities using a distinct section of the
network, which we call the domain prediction branch, while we refer to
the main section of the network as the classification branch. The two
branches share the bottom-most layers and parameters as depicted in
Figure 2.2 .

The domain prediction branch is implemented as a minimal set of layers
followed by two softmax operations with @xmath and @xmath outputs for
the source and target latent domains, respectively (more details follow
in Section 2.4.5 ). The rationale of keeping the domain prediction
separated between source and target derives from the knowledge that we
have about the source/target membership of a sample point that we
receive in input, while it remains unknown the specific source or target
domain it belongs to. Furthermore, for each sample point @xmath with
known domain membership @xmath , we fix in each mDA-layer @xmath if
@xmath , otherwise @xmath .

We split the network into a domain prediction branch and classification
branch at some low level layer. This choice is motivated by the
observation [ aljundi2016lightweight ] that features tend to become
increasingly more domain invariant going deeper into the network,
meaning that it becomes increasingly harder to compute a domain
membership as a function of deeper features. In fact, as pointed out in
[ carlucci2017autodial ] , this phenomenon is even more evident in
networks that include DA-layers.

#### 2.4.4 Training the network

In order to exploit unlabeled data within our discriminative setting, we
follow the approach sketched in [ carlucci2017autodial ] , where
unlabeled data is used to define a regularizer over the network’s
parameters. By doing so, we obtain a loss for @xmath that takes the
following form:

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

where @xmath is a loss term that penalizes based on the final
classification task, while @xmath accounts for the domain classification
task.

Classification loss @xmath . The classification loss consists of two
components, accounting for the the supervised sample from the source
domain @xmath and the unlabeled target sample @xmath , respectively:

  -- -------- -------- -- -------
     @xmath   @xmath      (2.6)
  -- -------- -------- -- -------

The first term on the right-hand-side is the average log-loss related to
the supervised examples in @xmath , where @xmath denotes the output of
the classification branch of the network for a source sample, i.e. the
predicted probability of @xmath having class @xmath . The second term on
the right-hand-side of ( 2.6 ) is the entropy @xmath of the
classification distribution @xmath , averaged over all unlabeled target
examples @xmath in @xmath , scaled by a positive hyperparameter @xmath .

Domain loss @xmath . Akin to the classification loss, the domain loss
presents a component exploiting the supervision deriving from the known
domain labels in @xmath and a component exploiting the domain
classification distribution on all sample points lacking supervision.
However, the domain loss has in addition a term that tries to balance
the distribution of sample points across domains, in order to avoid
predictions to collapse into trivial solutions such as constant
assignments to a single domain. Accordingly, the loss takes the
following form:

  -- -------- -- -------
     @xmath      (2.7)
  -- -------- -- -------

Here, @xmath and @xmath denote the outputs of the domain prediction
branch for data points from the source and target domains, respectively,
while @xmath and @xmath denote the distributions of predicted domain
classes across @xmath and @xmath , respectively, i.e.

  -- -------- --
     @xmath
  -- -------- --

The first term in ( 2.7 ) enforces the correct domain prediction on the
sample points with known domain and it is scaled by a positive
hyperparameter @xmath . The terms scaled by the positive hyperparameter
@xmath enforce domain predictions with low uncertainty for the data
points with unknown domain labels, by minimizing the entropy of the
output distribution. Finally, the terms scaled by the positive
hyperparameter @xmath enforce balanced distributions of predicted domain
classes across the source and target sample, by maximizing the entropy
of the averaged distribution of domain predictions. Interestingly, since
the classification branch has a dependence on the domain prediction
branch via the mDA-layers, by optimizing the proposed loss, the network
learns to predict domain assignment probabilities that result in a low
classification loss. In other words, the network is free to predict
domain memberships that do not necessarily reflect the real ones, as
long as this helps improving its classification performance.

We optimize the loss in ( 2.5 ) with stochastic gradient descent. Hence,
the samples @xmath , @xmath , @xmath that are considered in the
computation of the gradients are restricted to a random subsets
contained in the mini-batch. In Section 2.4.5 we provide more details on
how each mini-batch is sampled. We call our model m ulti- D omain A
lignment layers for latent domain discovery (mDA).

#### 2.4.5 Experimental results

##### Datasets

In our evaluation we consider several common DA benchmarks: the
combination of USPS [ friedman2001elements ] , MNIST [ lecun1998gradient
] and MNIST-m [ ganin2014unsupervised ] ; the Digits-five benchmark in [
xu2018deep ] ; Office-31 [ saenko2010adapting ] ; Office-Caltech [
gong2012geodesic ] and PACS [ li2017deeper ] .

MNIST, MNIST-m and USPS are three standard datasets for digits
recognition. USPS [ friedman2001elements ] is a dataset of digits
scanned from U.S. envelopes, MNIST [ lecun1998gradient ] is a popular
benchmark for digits recognition and MNIST-m [ ganin2014unsupervised ]
its counterpart obtained by blending the original images with colored
patches extracted from BSD500 photos [ arbelaez2011contour ] . Due to
their different representations (e.g. colored vs gray-scale), these
datasets have been adopted as a DA benchmark by many previous works [
ganin2014unsupervised , bousmalis2016domain , bousmalis2016unsupervised
] . Here, we consider a multi source DA setting, using MNIST and MNIST-m
as sources and USPS as target, training on the union of the training
sets and testing on the test set of USPS.

Digits-five is an experimental setting proposed in [ xu2018deep ] which
considers 5 datasets of digits recognition. In addition to MNIST, MNST-m
and USPS, it includes SVHN [ netzer2011reading ] and Synthetic numbers
datasets [ ganin2016domain ] . SVHN [ netzer2011reading ] contains
pictures of real-world house numbers, collected from Google Street View.
Synthetic numbers [ ganin2016domain ] is built from computer generated
digits, including multiple sources of variations (i.e. position,
orientation, background, color and amount of blur), for a total of 500
thousands images. We follow the experimental setting described in [
xu2018deep ] : the train/test split comprises a subset of 25000 images
for training and 9000 for testing for each of the domains, except for
USPS for which the entire dataset is used. As in [ xu2018deep ] , we
report the results when either SVHN or MNIST-m are used as targets and
all the other domains are taken as sources.

Office-31 is a standard DA benchmark which contains images of 31 object
categories collected from 3 different sources: Webcam (W), DSLR camera
(D) and the Amazon website (A). Following [ xiong2014latent ] , we
perform our tests in the multi-source setting, where each domain is in
turn considered as target, while the others are used as source.

Office-Caltech [ gong2012geodesic ] is obtained by selecting the subset
of @xmath common categories in the Office31 and the Caltech256 [
griffin2007caltech ] datasets. It contains @xmath images, about half of
which belong to Caltech256. The different domains are Amazon (A), DSLR
(D), Webcam (W) and Caltech256 (C). In our experiments we consider the
set of source/target combinations used in [ gong2013reshaping ] .

PACS [ li2017deeper ] is a recently proposed DA benchmark which is
especially interesting due to the significant domain shift between its
domains. It contains images of 7 categories ( dog, elephant, giraffe,
guitar, horse ) and 4 different visual styles: i.e. Photo (P), Art
paintings (A), Cartoon (C) and Sketch (S). We employ the dataset in two
different settings. First, following the experimental protocol in [
li2017deeper ] , we train our model considering 3 domains as sources and
the remaining as target, using all the images of each domain.
Differently from [ li2017deeper ] we consider a DA setting (i.e. target
data is available at training time) and we do not address the problem of
domain generalization. Second, we use 2 domains as sources and the
remaining 2 as targets, in a multi-source multi-target scenario. In this
setting the results are reported as average accuracy between the 2
target domains.

In all experiments and settings, we assume to have no domain labels
(i.e. @xmath ), unless otherwise stated.

##### Networks and training protocols

We apply our approach to four different CNN architectures: the MNIST and
SVHN networks described in [ ganin2014unsupervised , ganin2016domain ] ,
AlexNet [ krizhevsky2012imagenet ] and ResNet [ he2016deep ] . We choose
AlexNet due to its widespread use in many relevant DA works [
ganin2014unsupervised , carlucci2017autodial , long2015learning ,
long2016unsupervised ] , while ResNet is taken as an exemplar for modern
state-of-the-art architectures employing batch-normalization layers.
Both AlexNet and ResNet are first pre-trained on ImageNet and then
fine-tuned on the datasets of interest. The MNIST and SVHN architectures
are chosen for fair comparison with previous works considering digits
datasets [ ganin2016domain , xu2018deep ] . Unless otherwise noted, we
optimize our networks using Stochastic Gradient Descent with momentum
@xmath and weight decay @xmath .

For the evaluation on MNIST, MNIST-m and USPS datasets, we employ the
MNIST network described in [ ganin2014unsupervised ] , adding an
mDA-layer after each convolutional and fully-connected layer. The domain
prediction branch is attached to the output of conv1 , and is composed
of a convolution with the same meta-parameters as conv2 , a global
average pooling, a fully-connected layer with 100 output channels and
finally a fully-connected classifier. Following the protocol described
in [ carlucci2017autodial , ganin2014unsupervised ] , we set the initial
learning rate @xmath to 0.01 and we anneal it through a schedule @xmath
defined by @xmath where @xmath , @xmath and @xmath is the training
progress increasing linearly from 0 to 1. We rescale the input images to
@xmath pixels, subtract the per-pixel image mean of the dataset and feed
the networks with random crops of size @xmath . A batch size of 128
images per domain is used.

For the Digits-five experiments we employ the SVHN architecture of [
ganin2016domain ] , which is the same architecture adopted by [
xu2018deep ] , augmented with mDA-layers and a domain prediction branch
in the same way as the MNIST network described in the previous
paragraph. We train the architecture for 44000 iterations, with a batch
size of 32 images per domain, an initial learning rate of @xmath which
is decayed by a factor of 10 after 80% of the training process. We use
Adam as optimizer with a weight decay @xmath , and pre-process the input
images like in the MNIST, MNIST-m, USPS experiments.

For the experiments on Office-31 and Office-Caltech we employ the
AlexNet architecture. We follow a setup similar to the one proposed in [
carlucci2017autodial , carlucci2017just ] , fixing the parameters of all
convolutional layers and inserting mDA-layers after each fully-connected
layer and before their corresponding activation functions. The domain
prediction branch is attached to the last pooling layer pool5 , and is
composed of a global average pooling, followed by a fully connected
classifier to produce the final domain probabilities. The training
schedule and hyperparameters are set following [ carlucci2017autodial ]
.

For the experiments on the PACS dataset we consider the ResNet
architecture in the 18-layers setup described in [ he2016deep ] ,
denoted as ResNet18. This architecture comprises an initial @xmath
convolution, denoted as conv1 , followed by 4 main modules, denoted as
conv2 – conv5 , each containing two residual blocks. To apply our
approach, we replace each Batch Normalization layer in the residual
blocks of the network with an mDA-layer. The domain prediction branch is
attached to conv1 , after the pooling operation. The branch is composed
of a residual block with the same structure as conv2 , followed by
global average pooling and a fully connected classifier. In the
multi-target experiments we add a second, identical domain prediction
branch to discriminate between target domains. We also add a standard BN
layer after the final domain classifiers, which we found leads to a more
stable training process in the multi-target case. In both cases, we
adopt the same training meta-parameters as for AlexNet, with the
exception of weight-decay which is set to @xmath and learning rate which
is set to @xmath . The network is trained for 600 iterations with a
batch size of 48, equally divided between the domains, and the learning
rate is scaled by a factor 0.1 after 75% of the iterations.

Regarding the hyperparameters of our method, we set the number of source
domains @xmath equal to @xmath , where @xmath is the number of different
datasets used in each single experiment. In the multi-source
multi-target scenarios, since we always have the domains equally split
between source and target, we consider @xmath equal @xmath for both
source and target. Following [ carlucci2017autodial ] , in the
experiments with AlexNet we fix @xmath with @xmath . Similarly, for the
experiments on digits classification, we set @xmath and @xmath for
MNIST, MNIST-m and USPS, and @xmath and @xmath for Digits-five, with
@xmath if @xmath , which we found leading to a more stable minimization
of the loss of the domain branch. In the experiments involving ResNet18
we select the values @xmath and @xmath through cross-validation,
following the procedure adopted in [ long2013transfer ,
carlucci2017autodial ] . Similarly, in the multi-target ResNet18
experiments we select @xmath . When domain labels are available for a
subset of source samples, we fix @xmath .

We implement ⁴ ⁴ 4 Code available at:
https://github.com/mancinimassimiliano/latent_domains_DA.git all the
models with the Caffe [ jia2014caffe ] framework and our evaluation is
performed using an NVIDIA GeForce 1070 GTX GPU. We initialize both
AlexNet and ResNet18 from models pre-trained on ImageNet, taking AlexNet
from the Caffe model zoo, and converting ResNet18 from the original
Torch model ⁵ ⁵ 5
https://github.com/HolmesShuan/ResNet-18-Caffemodel-on-ImageNet . For
all the networks and experiments, we add mDA layers and their variants
in place of standard BN layers.

##### Results

In this section, we first analyze the proposed approach, demonstrating
the advantages of considering multiple sources/targets and discovering
latent domains. We then compare the proposed method with
state-of-the-art approaches. For all the experiments we report the
results in terms of accuracy, repeating the experiments at least 5 times
and averaging the results. In the multi-target experiments, the reported
accuracy is the average of the accuracies over the target domains. As
for standard deviations, since we do not tune the hyperparameters of our
model and baselines by employing the accuracy on the target domain,
their values can be high in some settings. For this reason, in order to
provide a more appropriate analysis of the significance of our results,
we propose to adopt the following approach. In particular, let us model
the accuracy of an algorithm as a random variable @xmath with unknown
distribution. The accuracy of a single run of the algorithm is an
observation from this distribution. Therefore, in order to compare two
algorithms we consider the two sets of associated observations @xmath
and @xmath and estimate the probability that one algorithm is better
than the other as:

  -- -------- --
     @xmath
  -- -------- --

where @xmath is the Dirac function. In the following we use this metric
to compare our approach with respect to a baseline where no latent
domain discovery process is implemented (specifically, the method DIAL [
carlucci2017just ] , see below) considering five runs for each
experiment. For sake of clarity, we denote this probability estimate as
@xmath .

In the following we first analyze the performances of the proposed
approach with @xmath (denoted as mDA @xmath ), i.e. the algorithm we
presented in [ mancini2018boosting ] , and then we describe the impact
of the loss term we introduce in this section setting @xmath (denoted
simply as mDA).

Experiments on the Digits datasets

In a first series of experiments, reported in Table 2.1 , we test the
performance of our approach on the MNIST, MNIST-m to USPS benchmark
(M-Mm to U). The comparison includes: (i) the baseline network trained
on the union of all source domains ( Unified sources ); (ii) training
separate networks for each source, and selecting the one the performs
the best on the target ( Best single source ); (iii) DIAL [
carlucci2017just ] , trained on the union of the sources ( DIAL [
carlucci2017just ] - Unified sources ); (iv) DIAL, trained separately on
each source and selecting the best performing model on the target ( DIAL
[ carlucci2017just ] - Best single source ). We also report the results
of our approach in the ideal case where the multiple source domains are
known and we do not need to discover them ( Multi-source DA ). For our
approach with @xmath , we consider several different values of @xmath ,
i.e. the number of discovered source domains.

By looking at the table several observations can be made. First, there
is a large performance gap between models trained only on source data
and DA methods, confirming that deep architectures by themselves are not
enough to solve the domain shift problem [ donahue2014decaf ] . Second,
in analogy with previous works on DA [ mansour2009domain ,
duan2009domain , sun2011two ] , we found that considering multiple
sources is beneficial for reducing the domain shift with respect to
learning a model on the unified source set. Finally, and more
importantly, when the domain labels are not available, our approach is
successful in discovering latent domains and in exploiting this
information for improving accuracy on target data, partially filling the
performance gap between the single source models and Multi-source DA .
Interestingly, the performance of our algorithm changes only slightly
for different values of @xmath , motivating our choice to always fix
@xmath to the known number of domains in the next experiments.
Importantly, comparing our approach with DIAL we achieve higher accuracy
in most of the runs, i.e. @xmath . In this experiment, the introduction
of the loss term forcing a uniform assignment among clusters (denoted as
mDA) leads to comparable performances to our method with @xmath . This
behaviour can be ascribed to the fact that the separation among
different domains is quite clear in this case and adding constraints to
the domain discovery process is not required. In the following, we show
that the proposed loss is beneficial in more challenging datasets.

In a second set of experiments (Table 2.2 ), we compare our approach
with previous and recently proposed single and multi-source unsupervised
DA approaches. Following [ xu2018deep ] , we perform experiments on the
Digits-five dataset, considering two settings with SVHN and MNIST-m as
targets. As in the previous case, we evaluate the performance of the
baseline network (with and without BN layers) and of DIAL when trained
on the union of the sources, and, as an upper bound, our Multi-source DA
with perfect domain knowledge. Moreover, we consider the Deep Cocktail
Network (DCTN) [ xu2018deep ] multi-source DA model, as well as the
“source only” baseline and the single source DA models reported in [
xu2018deep ] : Reverse gradient (RevGrad) [ ganin2014unsupervised ] and
Domain Adaptation Networks (DAN) [ long2015learning ] . For all single
source DA models we consider two settings: “Unified Sources”, where all
source domains are merged, and “Multi-Source”, where a separate model is
trained for each source domain, and the final prediction is computed as
an ensemble. As we can see, the Unified Sources DIAL already achieves
remarkable results in this setting, outperforming DCTN, and Multi-source
DA only provides a modest performance increase. As expected, the
performance of our approach lies between these two ( @xmath equal to
0.56 and 0.64 for SVHN and MNIST-m respectively, with @xmath ).

##### Experiments on PACS

Comparison with state of the art. In our main PACS experiments we
compare the proposed approach with the baseline ResNet18 network, and
with ResNet18 + DIAL [ carlucci2017just ] , both trained on the union of
source sets. As in the digits experiments, we also report the
performance of our method when perfect domain knowledge is available
(Multi-source DA). Table 2.3 shows our results. In general, DA models
are especially beneficial when considering the PACS dataset, and
multi-source DA networks significantly outperform the single source one.
Remarkably, our model is able to infer domain information automatically
without supervision. In fact, its accuracy is either comparable with
Multi-source DA (Photo, Art and Cartoon) or in between DIAL and
Multi-source DA (Sketch). The average @xmath is @xmath . Looking at the
partial results, it is interesting to note that the improvements of our
approach and Multi-source DA w.r.t. DIAL are more significant when
either the Sketch or the Cartoon domains are employed as target set
(average @xmath ). Since these domains are less represented in the
ImageNet database, we believe that the corresponding features derived
from the pre-trained model are less discriminative, and DA methods based
on multiple sources become more effective. Setting @xmath , allows to
obtain a further boost of performances in the Sketch scenario, where the
source domains are closer in appearances. In the other settings, the
domain shift is mostly among the Sketch domain and all the others and it
can be easily captured by our original formulation in [
mancini2018boosting ] .

To analyze the performances of our approach in a multi-source
multi-target scenario, we perform a second set of experiments on the
PACS dataset considering 2 domains as sources and the other 2 as
targets. The results, shown in Table 2.4 , comprise the same baselines
as in Table 2.3 . Note that, apart from the difficulty of providing
useful domain assignments both in the source and target sets during
training, the domain prediction step is required even at test time, thus
having a larger impact on the final performances of the model. The
performance gap between DIAL and our approach increases in this setting
compared to Table 2.3 . Our hypothesis is that not accounting for
multiple domains has a larger impact on the unlabeled target than on the
labeled source. Looking at the partial results, when Photo is considered
as one of the target domains there are no particular differences in the
final performances of the various DA models: this may be caused by the
bias of the pre-trained network towards this domain. However, when the
other domains are considered as targets, the gain in performances
produced by our model are remarkable. When Sketch is one of the target
domains, our model completely fills the gap between the unified
source/target DA method and the multi-source multi-target upper bound
with a gain of more then 7% when Art and Cartoon considered as other
target. Setting @xmath in this setting allows to obtain a further boost
of performances. This is evident in the scenario where Photo and Art are
both the source or target domains, with Cartoon-Sketch correspond to the
other pair. In this scenario the source/target pairs are quite close and
enforcing a uniform assignment among the latent domains provides a
better estimate of each of them.

Ablation study. We exploit the challenging multisource-multitarget
scenario of Table 2.4 in order to assess the impact of the various
components of our algorithm. In particular we show how the performance
are affected if (i) a random domain is assigned to each sample; (ii) no
loss is applied to the domain prediction branch; (iii) no entropy loss
is applied to the classification of unlabeled target samples. From Table
2.4 we can easily notice that if we drop either the domain prediction
branch (random assignment) or the losses on top of it ( @xmath ), the
performances of the model become comparable to the ones obtain by the
DIAL baseline. This shows not only the importance of discovering latent
domains, but also that both the domain branch and our losses allow to
extract meaningful subsets from the data. Moreover, this demonstrates
the fact that our improvements are not only due to the introduction of
multiple normalization layers, but also to the latent domain discovering
procedure. For what concerns the classification branch, without the
entropy component on unlabelled target samples ( @xmath ), the
performance of the model significantly decreases (i.e. from 82.6 to 76.4
in average). This confirms the findings of previous works [
carlucci2017autodial , carlucci2017just ] about the impact that this
loss for normalization based DA approaches. In particular, assuming that
source and target samples of different domains are independently
normalized, the entropy loss generates a gradient flow through unlabeled
samples based in the direction of its most confident prediction. This is
particularly important to learn useful features even for the target
domain/s, for which no supervision is available.

In-depth analysis. The ability of our approach to discover latent
domains is further investigated on PACS. First, in Figure 2.7 , we show
how our approach assigns source samples to different latent domains in
the single target setting. The four plots correspond to a single run of
the experiments of Table 2.3 . Interestingly, when either Cartoon
(Figure (c)c ) or Sketch (Figure (d)d ) is the target, samples from
Photo and Art tend to be associated to the same latent domain and,
similarly, when either Photo (Figure (a)a ) or Art (Figure (b)b ) is the
target, samples from Cartoon and Sketch are mostly grouped together.
These results confirms the ability of our approach to automatically
assign images of similar visual appearance to the same latent
distribution. In Figure 2.12 , we show the top-6 images associated to
each latent domain for each sources/target setting. In most cases,
images associated to the same latent domain have similar appearance,
while there is high dissimilarity between images associated to different
latent domains. Moreover, images assigned to the same latent domain tend
to be associated with one of the original domains. For instance, the
first row of Figure (a)a contains only images from Art, while the third
contains only images from Sketch. Note that no explicit domain
supervision is ever given to our method in this setting.

In Figure 2.25 , we show the histograms of the domain assignment
probabilities predicted by our model with @xmath in the various
multi-source, multi-target settings of Table 2.3 . As the figures shows,
in most cases the various pairs of target domains tend to be very well
separated: this justifies the large gain of performances produced by our
model in this scenario. The only cases where the separation is less
marked is when Art and Photo, which have very similar visual appearance,
are considered as targets. On the other hand, source domains are not
always as clearly separated as the targets. In particular the pairs
Photo-Cartoon, Art-Photo and Art-Cartoon, tend to receive similar
assignments when they are considered as source. A possible explanation
is that the supervised source loss could have a stronger influence on
the domain assignment than the unsupervised target one. In any case,
note that these results do not detract from the validity of our
approach. In fact, our main objective is to obtain a good classification
model for the target set, independently from the actual domain
assignments we learn.

In Figure 2.38 , the same analysis is performed on our method with the
additional constraint of having a uniform assignment distribution among
domains. As the figure shows, this constraint allows to obtain a clearer
domain separation in most of the cases, overcoming the difficulties that
the domain prediction branch experienced in separating domain pairs such
as Photo-Cartoon and Photo-Art.

We perform a similar analysis in another dataset, Digits-five. The
results are reported in Figure 2.41 . As the figure shows, when SVHN is
the target domain, one of the latent domains (latent domain 1) receives
very confident assignments for the samples of the MNIST dataset. The
samples of the other source datasets receive assignment spread through
all the latent domains, with the exceptions of USPS which receives the
most confident predictions for the second latent domain and MNIST-m,
which partially influences the first latent domain, the one with
confidence assignments to MNIST. One latent domain does not receive
assignments form any of the sources (latent domain three): this might
happen if the entropy term overcomes the uniform assignment constraints
in the early stages of training. Similarly, when MNIST-m is the target
domain, the first two latent domains receive confident assignments for
samples belonging to MNIST and SVHN datasets respectively, while the
third and the fourth receive higher assignments for samples of the
remaining source domains.

##### Experiments on Office-31

In our Office-31 experiments we consider the following baselines,
trained on the union of the source sets: (i) a plain AlexNet network;
(ii) AlexNet with BN inserted after each fully-connected layer; and
(iii) AlexNet + DIAL [ carlucci2017just ] . Additionally, we consider
single source domain adaptation approaches, using the results reported
in [ xu2018deep ] . The methods are Transfer Component Analysis (TCA) [
pan2010domain ] , Geodesic Flow Kernel (GFK) [ gong2012geodesic ] , Deep
Domain Confusion (DDC) [ tzeng2015simultaneous ] , Deep Reconstruction
Classification Networks (DRCN) [ ghifary2016deep ] and Residual Transfer
Network (RTN) [ long2016unsupervised ] , as well as the Reversed
Gradient (RevGrad) [ ganin2014unsupervised ] and Domain Adaptation
Network (DAN) [ long2015learning ] algorithms considered in the digits
experiments. For these algorithms we report the performances obtained in
the “Best single source” and “Unified sources“ settings, as available
from [ xu2018deep ] . As in the previous experiments, Multi-source DA
with perfect domain knowledge can be regarded as a performance upper
bound for our method. Finally, we include results reported in [
xu2018deep ] for different multi-source DA models: Deep Cocktail Network
(DCTN) [ xu2018deep ] , the two shallow methods in [ xie2015learning ]
(sFRAME) and [ gopalan2011domain ] (SGF), and an ensemble of baseline
networks trained on each source domain separately (Source only). These
results are summarized in Table 2.5 .

We note that, in this dataset, the improvements obtained by adopting a
multi-source model instead of a single-source one are small. This is in
accordance with findings in [ li2017deeper ] , where it is shown that
the domain shift in Office-31, when considering deep features, is indeed
quite limited if compared to PACS, and it is mostly linked to changes in
the background (Webcam-Amazon, DSLR-Amazon) or acquisition camera
(DSLR-Webcam). This is further supported by the smaller gap between DIAL
and our method in this case compared to the previous experiments
(average @xmath of 0.54). In this setting, introducing our uniform loss
term does not provides boost in performances. We ascribe this behaviour
to the fact that in this scenario, each batch is built with a
non-uniform number of samples per domain (following [
carlucci2017autodial ] ) while our current objective assumes a balanced
sampling among domains.

In a final Office-31 experiment, we consider a setting where the true
domain of a subset of the source samples is known at training time.
Figure 2.42 shows the average accuracy obtained when a different amount
of domain labels are available. Interestingly, by increasing the level
of domain supervision the accuracy quickly saturates towards the value
of Multi-source DA, completely filling the gap with as few as @xmath of
the source samples.

##### Comparison with S.o.t.A. on inferring latent domains

In this section we compare the performance of our approach with previous
works on DA which also consider the problem of inferring latent domains
[ hoffman2012discovering , xiong2014latent , gong2013reshaping ] . Since
there are no previous works adopting deep learning models (i) in a
multi-source setting and (ii) discovering hidden domains. Therefore, the
methods we compare to all employ handcrafted features. For these
approaches we report results taken from the original papers.
Furthermore, we evaluate the method of Gong et al. [ gong2013reshaping ]
using features from the last layer of the AlexNet architecture. For a
fair comparison, when applying our method we freeze AlexNet up to fc7 ,
and apply mDA layers only after fc7 and the classifier.

We first consider the Office-31 dataset, as this benchmark has been used
in [ hoffman2012discovering , xiong2014latent ] , showing the results in
Table 2.6 . Our model outperforms all the baselines, with a clear margin
in terms of accuracy. Importantly, even when the method in [
gong2013reshaping ] is applied to features derived from AlexNet, still
our approach leads to higher accuracy. For the sake of completeness, in
the same table we also report results from previous multi-source DA
methods [ gopalan2013unsupervised , nguyen2015dash , lin2017cross ]
based on shallow models. While these approaches significantly outperform
[ hoffman2012discovering ] and [ xiong2014latent ] , still their
accuracy is much lower than ours. Moreover, introducing our novel loss
term provides higher performances with respect to the our approach with
@xmath .

To provide a comparison in a multi-target scenario, we also consider the
Office-Caltech dataset, comparing our model with [
hoffman2012discovering , gong2013reshaping ] . Following [
gong2013reshaping ] , we test both single target (Amazon) and
multi-target (Amazon-Caltech and Webcam-DSLR) scenarios. As for the PACS
multi-source/multi-target case, the assignment of each sample to the
source or target set is assumed to be known, while the assignment to the
specific domain is unknown. We again want to remark that, since we do
not assume to know the target domain to which a sample belongs, the task
is even harder since we require a domain prediction step also at test
time. As in the Office-31 experiments, our approach outperforms all
baselines, including the method in [ gong2013reshaping ] applied to
AlexNet features. In this scenario, introducing our uniform loss
provides a boost in performances in the multi-target setting, where the
two source/target pairs have similar appearance. This is inline to what
reported for the multitarget experiments on PACS (Table 2.4 ).

#### 2.4.6 Conclusions

In this section, we presented a novel deep DA model for automatically
discovering latent domains within visual datasets. The proposed deep
architecture is based on a side-branch that computes the assignment of
source and target samples to their associated latent domain. These
assignments are then used within the main network by novel domain
alignment layers which reduce the domain shift by aligning the feature
distributions of the discovered sources and the target domains. Our
experimental results demonstrate the ability of our model to efficiently
exploit the discovered latent domains for addressing challenging domain
adaptation tasks. Future works could investigate other architectural
design choices for the domain prediction branch, as well as the
possibility to integrate it into other CNN models for unsupervised
domain adaptation [ ganin2014unsupervised ] . In the next section, we
will remove the assumption of having target data during training,
focusing on the domain generalization scenario. We will show how mDA
layers can be extended to effectively address the domain generalization
problem.

### 2.5 Domain Generalization

In the previous section, we showed how it is possible to overcome the
domain shift problem effectively even when our source/target domain is a
mixture of multiple ones. However, it relies on a fundamental
assumption: the presence of target data during training. Unfortunately,
this assumption is not always satisfied in practice.

Let us consider the problem of semantic place categorization from visual
data [ wu2009visual ] . This task is important in robotics, since
correctly identifying the semantic category of a place allows the robot
to improve its localization, mapping and exploration [
stachniss2006speeding , kostavelis2015semantic ] capabilities. We have
three strategies to address this problem. The first is using labeled
datasets of training images [ wu2011centrist , fazl2012histogram ,
urvsivc2016part , mancini2017learning ] . While the resulting models are
very accurate when test samples are similar to training data, their
performance significantly degrade when the robot collects images with
very different visual appearance [ pronobis2010realistic ] .

A second strategy could be exploiting domain adaptation (DA) techniques
[ prasath2012transfer , kira2014transfer , costante2013transfer ] .
These methods develop models which are meant to be effective in the
scenario where the robot will operate, i.e. the target domain. While
domain adaptation algorithms provide effective solutions, they require
some prior knowledge of the target domain at training time, e.g. to have
access to target data. Unfortunately, this information may not always be
available. Consider for instance an household robot: since the number of
possible customers is huge, it is inconceivable to collect data for each
possible house and application scenario.

In this context, a more relevant problem to address is domain
generalization (DG) . As described in previous sections, opposite to DA,
where target data are exploited to produce a classifier accurate under
specific working conditions, the idea behind DG is to learn a domain
agnostic model applicable to any unseen target domain. In other words,
the goal of DG is building a model which is as general as possible, e.g.
employable by different robots and in various environmental conditions.

In this section, we build on the mDA layers presented in Section 2.4 and
we first propose a novel deep learning framework for DG, namely We call
this approach WBN ( W eighted B atch N ormalization for Domain
Generalization) [ mancini2018robust ] . The approach develops from the
idea that, given data from multiple source domains and the associated
models, the best model for the target domain can be generated on-the-fly
when a novel sample arrives by optimally combining the precomputed
models from source domains (see Fig. 2.43 ). To implement this idea we
design a novel CNN architecture which relies on two main components.
First, inspired by recent works on domain adaptation [ carlucci2017just
, li2016revisiting ] , we construct multiple source models by embedding
into a common CNN few domain-specific Batch Normalization layers. In
this way, different classifiers can be built keeping the number of
parameters limited. Second, we design a lateral network branch which
computes the likelihood that a certain instance belongs to a given
domain. When applied to a novel target sample, this branch calculates
its probabilities to be part of the different source domains. These
values are used to construct the target classifier performing a
combination of known source models. This is similar to the idea of mDA
layers, with the difference that (i) no target data are available during
training and (ii) the domain assignment branch is used to compute the
similarity of target samples with source domains.

In the second part of this section, we extend this approach by
considering domain-specific classifiers, and classifying each incoming
target image by optimally fusing the prediction scores of the
source-specific classifiers. As in WBN, this is achieved through an
end-to-end trainable deep architecture with two main components. The
first implements the source-specific classifiers, while the second
module is a network branch which computes the similarities of an input
sample to all source domains, such as to assign weights to the source
classifiers and properly merge their predictions. The second module is
also designed in order to easily permit, if needed, the integration of a
domain agnostic classifier which, acting in synergy with the
domain-specific models, can further improve generalization. We call this
approach B est S ources F orward for Domain Generalization (BSF) [
mancini2018best ] .

To this aim, the novel Weighted Batch Normalization (WBN) layers are
introduced. We demonstrate the effectiveness of the proposed DG approach
with extensive experiments on three datasets, namely the COsy
Localization Database (COLD) [ pronobis2009ijrr ] , the Visual Place
Categorization (VPC) dataset [ wu2009visual ] and the Specific PlacEs
Dataset (SPED) [ chen2017deep ] . Moreover, we show how the proposed
framework can be employed where no prior information about source
domains is available at training time: given a training set, our model
can be used to automatically cluster training data and learning multiple
models, discovering latent domains and associated classifiers.

To summarize, the main contributions of this section are: (i) an
extension of the mDA framework, WBN, which exploits the similarity of
target samples with the given source domains to address the DG problem;
(ii) we introduce the problem of domain generalization for semantic
place recognition, showing how WBN is effective in addressing it, even
without exact domain knowledge; (iii) we extend of WBN, by considers
source-specific classifiers in place of domain-specific alignment
layers, showing its effectiveness in standard DG benchmarks in computer
vision.

#### 2.5.1 Problem Formulation

The goal of DG is to extend the knowledge acquired from a set of source
domains to any unknown target domain. In this context, the source sets
correspond, e.g., to data acquired by multiple robots in different
environments while the unknown target to any unseen environment.
Formally, following the notation in Section 2.1 , we have our training
set defined as @xmath where @xmath , @xmath and @xmath , with @xmath .
Note that no target domain data @xmath is available during training.
Moreover, we assume @xmath , analyzing in Sections 2.6 and 2.7 the case
where @xmath but other information is available. Our goal is to learn a
predictor @xmath able to work in any possible target domain @xmath
unseen during training, i.e. @xmath . It is worth highlighting that,
differently from the Latent Domain Discovery problem presented in
Section 2.4 , here we might have full knowledge about the domain labels.

#### 2.5.2 Starting point: Domain Generalization with Weighted BN 666M.
Mancini, S. Rota Bulò, B. Caputo, E. Ricci. Robust Place Categorization
with Deep Domain Generalization. IEEE Robotics and Automation Letters,
July 2018, vol. 3, n. 3., pp. 2093-2100.

A clear issue with DA methods, including DA-layers and mDA, is that they
require the presence of a target set @xmath in the training phase. This
implies that data collected in the scenario of interest should be
available for learning the classification model. However, a more
realistic situation, especially in robotics, is when we employ our
system in completely unseen environments/domains. As an example,
consider a service robot: it is unfeasible to collect data for all
possible working environments. Therefore, it is important to drop the
assumption of having target data beforehand while designing deep models
addressing the domain shift problem. In this subsection, we start by
removing target data from DA-layers and mDA layers.

From the formulation of DA-layers defined in Eq. ( 2.1 ), we can obtain
multiple, domain-specific models by considering separate BN statistics
for each of the source domains during training. In particular, given the
features of a sample @xmath at a given layer and spatial location
(omitted for simplicity) as well as its domain label @xmath , we can
apply the domain-specific BN as follows:

  -- -------- -- -------
     @xmath      (2.8)
  -- -------- -- -------

The problem with this formulation is that, at test time, no statistics
from the unseen target domains are available. To solve this problem, we
restore to a soft-version of Eq. ( 2.8 ). Let us write Eq. ( 2.8 ) as:

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

The hard-assignment in Eq. ( 2.9 ), used at training time, can be
replaced with a weighted version at test time, modeling the uncertainty
we have about our target domain. In particular, we can write:

  -- -------- -- --------
     @xmath      (2.10)
  -- -------- -- --------

where @xmath is the probability of sample @xmath to belong to domain
@xmath , with @xmath and @xmath . The intuition behind this choice is
deriving a classification model for the target domain as a combination
of models from the source domains, with the weights derived from the
similarity of the target domain data to the source domains.

In order to compute the weights @xmath , we restore to the same domain
classification module described in Section 2.4.3 , employing a separate
network branch which originates from the first few convolutional layers
of the main network (see Fig. (c)c ). This choice is motivated by the
fact that end-to-end training is allowed and the number of parameters is
kept limited. The specific architecture of the branch may be variable,
with the only restriction that its final output must be a probability
vector of dimension @xmath , corresponding to the number of known
domains.

Denoting the classification branch as @xmath and @xmath , during
training we minimize a simplified version of Eq. ( 2.5 ), namely:

  -- -------- -- --------
     @xmath      (2.11)
  -- -------- -- --------

the loss is the sum of two terms, one considering place label
information for accurate recognition, the other enforcing the lateral
branch to successfully compute the correct domain, with @xmath balances
the contribution of the semantic classification and the domain
prediction terms. At test time, the domain assignment produced by @xmath
for target samples will be used to obtain the domain similarity @xmath
of Eq. ( 2.10 ).

Finally, we would like to highlight that this framework can be easily
extended to perform DG in the lack of domain labels, following what
described in Section 2.4 . In particular, we can rely on the
soft-assignment strategy to compute the latent domain statistics, as in
Eq. ( 2.2 ). As in the previous section, the intuition is that, since
similar input images will tend to produce similar outputs in the lateral
network branch, implicitly visual data will be automatically clustered,
enabling a latent domain discovery process. In this scenario, we let the
domain assignment network be guided by the semantic loss while computing
the statistics using Eq. ( 2.2 ). In Fig. 2.47 we show the difference
between this model and standard DA-layers.

#### 2.5.3 WBN Experiments: Domain Generalization in Semantic Place
Categorization

Datasets. In our experiments we use three robot vision datasets, namely
the widely adopted COLD [ pronobis2009ijrr ] and VPC [ wu2009visual ]
datasets, and the recent SPED dataset [ chen2017deep ] .

The COLD Database contains three datasets of indoor scenes acquired in
different laboratories and from different robots. The COLD-Freiburg (Fr)
has 26 image sequences collected in the Autonomous Intelligent Systems
Laboratory at the University of Freiburg, with a camera mounted on an
ActivMedia Pioneer-3 robot. COLD-Ljubljana (Lj) contains 18 sequences
acquired from an iRobot ATRV-Mini platform at the Visual Cognitive
Systems Laboratory of University of Ljubljana. In the COLD-Saarbrücken
(Sa) an ActivMedia PeopleBot has been employed to gather 29 sequences
inside the Language Technology Laboratory at the German Research Center
for Artificial Intelligence in Saarbrücken.

The VPC dataset contains images acquired from several rooms of 6
different houses with multiple floors. The images are acquired by means
of a camcorder placed on a rolling tripod, simulating a mobile robotic
platform. The dataset contains 11 semantic categories, but only 5 are
common to all houses: bedroom, bathroom, kitchen, living room and
dining-room. Following previous works [ wu2009visual , fazl2012histogram
, yang2012object ] , we use the common categories in our experiments.

SPED is a large scale dataset introduced in the context of place
recognition. It contains images of 2543 outdoor cameras collected from
the Archive of Many Outdoor Scenes (AMOS) [ jacobs2007consistent ]
during February and August 2014 ⁷ ⁷ 7 The full dataset was not available
at the time we proposed WBN, but the authors provided us a subset with
about 500 images per camera corresponding to 900 categories.

Networks and training protocols. For COLD and VPC we perform experiments
with two common architectures: AlexNet [ krizhevsky2012imagenet ] and
ResNet [ he2016deep ] . For AlexNet we use the standard architecture
pre-trained on Imagenet [ deng2009imagenet ] . In all the experiments,
we fine-tune the last two fully-connected layers, rescaling the input
images to 227 @xmath 227 pixels. For ResNet we consider the 10 layers
version of the architecture, again pre-trained on ImageNet. In all the
experiments, we rescale the input images to 224x224 pixels, fine-tuning
the network starting from the last residual block. Both the networks are
trained with a weight decay of 0.0005 and an initial learning rate of
0.001, while the initial learning-rate of the final classifier is set to
0.01. The learning rate is dropped of a 0.1 factor after @xmath of the
iterations. For the experiments on COLD, we use a batch size of 256 for
AlexNet and 64 for ResNet, training the networks for 1000 iterations.
For VPC, we set the batch size to 128 and 64 for AlexNet and ResNet
respectively, training the networks for 2000 iterations. The training
parameters are the same for our method and the baselines and fine-tuning
is performed for all the models.

WBN can be applied to common CNNs by simply replacing standard BN layers
with our WBN layers. While for ResNet BN layers are already employed,
this is not true for AlexNet. For these experiments we employ a variant
of AlexNet where BN layers are inserted after each fully-connected
layer.

For SPED we use AlexNet and the AMOSNet architecture, following [
chen2017deep ] . AMOSNet is very similar to AlexNet, with the first
fully-connected layer replaced by a convolutional layer and a pooling
operation. We follow the same protocol of [ chen2017deep ] , using the
same hyperparameters for training. We train both networks from scratch,
applying BN or WBN layers after each layer with parameters, except the
classifier. The implementation details of the domain assignment branch
follows the one of Section 2.4.5 and we set @xmath for all the
experiments.

The evaluation was performed using a NVIDIA GeForce 1070 GTX GPU,
implementing all the models with the popular Caffe [ jia2014caffe ]
framework. For the baseline AlexNet architecture we take the pre-trained
model available in Caffe, while for ResNet we consider the model from [
simon2016cnnmodels ] . The code implementing the WBN layers is publicly
available ⁸ ⁸ 8 https://github.com/mancinimassimiliano/caffe

Results on COLD. We first perform experiments on the COLD database,
where the goal is to demonstrate the effectiveness of WBN in learning
effective classification models in case of varying environmental
conditions (e.g. illuminations, laboratories).. For each laboratory and
illumination condition we consider the standard sequences 1 of part A,
except for Saarbrücken Cloudy, for which we take sequence 2 due to known
acquisition issues ⁹ ⁹ 9 http://www.cas.kth.se/COLD/bugs.php and
Saarbrücken Sunny, for which we take part B since sunny sequences for
part A are not available. We consider the 4 classes shared between the
sequences: printer area, corridor, bathroom and office (obtained by
merging 1-person and 2-persons office). We report the results as the
average accuracy per class. In these experiments we consider both
AlexNet and ResNet comparing WBN with baseline models obtained adding
traditional BN layers to the same architectures.

We test two different variants of the proposed approach. In the first
case (WBN @xmath ) we consider the presence of domain priors at training
time, as in Section 2.5.2 . In the second variant, WBN, we do not assume
to have knowledge about domains at training time, thus our model just
relies on the soft-assignment. We highlight that WBN with
soft-assignment is similar to the mDA layers of Section 2.4 except that
(i) no loss is applied on the domain prediction branch and (ii) no
target data are available during training, thus no statistics are
available for them and we must rely on the domain prediction branch also
at test time.

Firstly, we consider different lighting conditions, i.e. we assume that
the domain shift is due to changes of illuminations. To this extent we
train the network on sequences of the same laboratory, training on two
lighting conditions (e.g. sunny and cloudy ) and testing on the third
(e.g. night ). The results are reported in Table 2.8 .

As expected, when knowledge about domains is available (WBN @xmath ),
improved classification accuracy can be obtained, in general, with
respect to a domain agnostic classifier. Interestingly, for both
networks the result of WBN without domain priors is either comparable or
surpasses the baseline in almost all settings. This suggests that the
network is able to latently discover clusters of samples and effectively
using this information for learning robust classification models.

Secondly, we perform a similar analysis to Table 2.8 but considering
changes of robotic platform/environment. We keep constant the lighting
condition, training on two laboratories and testing on the third. Table
2.9 shows the obtained results. Again, in most cases exploiting domain
priors brings benefits in term of performances, for both networks. The
results of Tables 2.8 and 2.9 show that the benefits of our WBN layer,
with and without domain loss, are not limited to a particular type of
domain shift (i.e. changes in robots, environment or illumination
condition), demonstrating that our approach provides a general and
effective strategy to address domain variations. In both experiments,
there are few cases in which standard BN achieves comparable or slightly
superior results w.r.t. WBN. A possible reason is that in some
situations the ability of our model to generalize to novel settings may
be hindered by the small number or by the specific characteristics of
the available source domains.

In order to verify the ability of WBN to discover latent domains, Fig.
2.49 shows the distribution of the values @xmath computed for the images
of the original source domains associated to one of the experiments in
Table 2.9 . The plots associated to other experiments are similar and we
do not report them due to lack of space. Since we consider two latent
domains in these experiments and @xmath , we report only the values
computed for @xmath . Different colors represent the original source
domains. As the figure shows, the lateral branch computes different
assignments for the samples of the different original source domains. As
a result, the latent source domains extracted by WBN tend to correspond
to the original ones used by WBN @xmath .

In another series of experiments we consider the scenario where both
illumination and laboratory change. We performed 27 different
experiments, corresponding to the case where Saarbrücken is considered
as target domain. Figure 2.49 report the histogram of the gains in
accuracy of our approach AlexNet+WBN* w.r.t. AlexNet+BN. As shown in
Fig. 2.49 , in most of the cases our model leads to an increase in
accuracy between 1-5%. In only 5 out of 27 experiments, our model does
not produce benefits.

Comparison with SOTA on VPC. In order to compare our model with the
state-of-the art approaches in robotics, we consider the VPC dataset.
VPC has been used in previous works to test the DG abilities of
different methods. Following the standard experimental protocol of [
wu2009visual ] , we evaluate our model using 5 houses for training and 1
for test, averaging the results between the 6 configurations. For each
house we report the average accuracy per class. Table 2.10 compares the
result of our models with baseline deep architectures, with and without
traditional BN layers. We consider both the case where domain
information is available (WBN @xmath ) and where it is not (WBN).
Analogously to what observed in the experiments on COLD dataset, the
accuracy increases when WBN is adopted, both in case of AlexNet and
ResNet architectures. Interestingly, having domain priors during
training produce a boost of performances for ResNet, while for AlexNet
this is not the case. This suggests that different features have a
different impact on our model. Features of the very last layers, as in
AlexNet, may not be enough domain discriminative, especially in case of
limited shift within the source domains. In those cases, a
soft-assignment can provide a more effective strategy for clustering
samples.

Finally, Table 2.11 compares the results obtained with WBN with those of
state-of-the-art methods. Specifically we consider the method in [
wu2009visual ] , where SIFT [ lowe2004distinctive ] and CENTRIST (CE)
features [ wu2011centrist ] are provided as input to a nearest neighbor
classifier, and the approach in [ fazl2012histogram ] , where the same
classifier is employed but using Histogram of Oriented Uniform Patterns
(HOUP) as input. For sake of completeness, we also report the results
obtained by exploiting also the temporal information between images. For
this setting, we report the performances of the CENTRIST-based approach
of [ wu2011centrist ] coupled with Bayesian Filtering (BF) and the
results of [ yang2012object ] which used again a Bayesian Filter
together with object templates. As shown in the Table, applying
deep-learning techniques already guarantees an increase in performances
of about @xmath with respect to the state of the art. Introducing WBN
inside the network, allows a further accuracy gain.

Experiments on a large-scale scenario: SPED. In this section we show the
results obtained when WBN is applied to a large scale dataset of outdoor
scenes, i.e. the SPED dataset. In order to utilize SPED as a DG
benchmark, we split the dataset in two sets, February and August,
considering the months of data acquisition. Since no other automatic
training data splits are possible using timestamps, in these experiments
we do not use domain supervision and only consider WBN with two latent
domains. The choice of having two domains is motivated by the fact that
the dataset contains images collected at different times of the day and
thus we assume that the latent domains automatically discovered by our
method correspond to "night" and "day".

Results are shown in Table 2.12 . WBN provides a clear gain in all
considered settings and for all considered architectures. The
improvement of 4% obtained in the case "August-to-February" for both
networks is remarkable given the very large number of classes and the
lack of domain supervision.

#### 2.5.4 From BN to Classifiers: Best Sources Forward 101010M.
Mancini, S. Rota Bulò, B. Caputo, E. Ricci. Best sources forward: domain
generalization through source-specific nets. IEEE International
Conference on Image Processing (ICIP) 2018.

In Section 2.5.2 , we discussed how to address DG given a domain
classification branch and domain-specific (either latent or explicit)
normalization layers. However, the same approach can be applied, in
principle, to other parts of the network. In this subsection, we
describe how the same methodology can be applied to domain-specific
classification layers (Fig. 2.50 ).

The approach devised in the previous section requires three components:
(i) a way to estimate domain membership of a sample both at training and
at test time, (ii) a distinction between domain-specific and
domain-agnostic network elements, and (iii) a strategy to merge
domain-specific activations within the network. The first point can be
easily addressed through a domain classifier, as described in Sections
2.5.2 and 2.4.3 .

For what concerns the second point, we can write our classification
model as @xmath , where @xmath denotes the set of parameters to learn
and each @xmath are the parameters corresponding to a specific domain
@xmath . Moreover, let us consider @xmath , where @xmath indicates the
parameters shared by all domain-specific models and @xmath the
domain-specific ones. Under this formulation, in Section 2.5.2 , @xmath
were all the parameters of the network while @xmath the domain-specific
BN statistics. In this section, we change perspective and we assume
@xmath to be a feature extractor and @xmath to be domain-specific
semantic classification heads. Note that the formulation is general and
can be applied to multiple/different levels of the network.

Now that we have defined the domain-specific component we must define
how to merge activations of the domain-specific layers. During training,
the most simple strategy would be to rely on the domain-label of the
sample, namely:

  -- -------- -- --------
     @xmath      (2.12)
  -- -------- -- --------

where we wrote @xmath as @xmath for simplicity. Similarly to Eq. ( 2.9
), also this equation cannot be applied at test time, when the domain
membership of a sample is unknown and falls out the space of the
available source domains @xmath . Similarly to what we did in Section
2.5.2 , we can use a soft version of Eq. ( 2.12 ) at test time:

  -- -------- -- --------
     @xmath      (2.13)
  -- -------- -- --------

where @xmath is the probability of sample @xmath to belong to domain
@xmath , as computed by our domain classifier. The model is trained with
the same semantic and domain classification loss defined Eq. ( 2.11 ).

We highlight that, differently from Sections 2.4 and 2.5.2 , here the
merging of the domain-specific activations/components is held-out after
and not within the feature extraction process. Moreover, we found a
simple modification being beneficial in this scenario. In particular, we
introduce an hyperparameter @xmath and we re-write the classification
function as follows:

  -- -------- -- --------
     @xmath      (2.14)
  -- -------- -- --------

In practice, @xmath allows to merge domain-specific component both
exploiting the similarity among domains with its first term (as in Eq. (
2.12 )), while considering domain agreements on the predictions,
weighting them equally with the second term. This allows the model to be
robust to inaccurate domain assignment at test time while increasing the
feedback to domain-specific models for source sets with few samples. In
practice, during training we randomly switch with probability @xmath
between using the given domain label as @xmath or assigning to all
domain-classifier a uniform weight @xmath . At test time, we use Eq. (
2.14 ) with @xmath obtained from the domain prediction branch. As the
experiments show, this choice allows us to obtain a more robust final
classification model. Figure 2.51 provides an overview of our model.

#### 2.5.5 Experiments: Domain Generalization in Computer Vision

Datasets. We test the performance of BSF on two publicly available
benchmarks. The first is rotated-MNIST [ ghifary2015domain ] , a dataset
composed by different domains originated applying different degrees of
rotations to images of the original MNIST digits dataset [
lecun1998gradient ] . We follow the experimental protocol of [
motiian2017unified ] , randomly extracting 1000 images per class from
the dataset and rotating them respectively of 0, 15, 30, 45, 60 and 75
degrees counterclockwise. As previous works, we consider one domain as
target and the rest as sources.

The second is PACS [ li2017deeper ] the same database we used on the
latent domain discovery section. Differently from Section 2.4.5 , we
consider a domain generalization setting (i.e. no target data available
during training). Following the experimental protocol of [ li2017deeper
] , we train our model considering three domains as source datasets and
the remaining one as target.

Networks and training protocols. In our evaluation we set the parameters
@xmath and @xmath . For the experiments on the rotated-MNIST dataset, we
employ the LeNet architecture [ lecun1998gradient ] following [
motiian2017unified ] . The network is trained from scratch, using a
batch size of 250 with an equal number of samples for each source
domain. We train the network for 10000 iterations, using Stochastic
Gradient Descent (SGD) with an initial learning rate of 0.01, momentum
0.9 and weight decay 0.0005. The learning rate is decayed through an
inverse schedule, following previous works [ ganin2014unsupervised ] .
For the domain prediction branch, we take as input the image and perform
two convolutions, with the same parameters of the first two
convolutional layers of the main network. Each convolution is followed
by a ReLU non linearity and a pooling operation. The domain prediction
branch follows the implementations of the previous sections. It
terminates with a global average pooling followed by a fully connected
layer which outputs the final weights. To ensure that @xmath , we apply
the softmax operator after the fully connected layer.

For PACS, we trained the standard AlexNet architecture, starting from
the ImageNet pre-trained model. We use a batch size of 192, with 64
samples for each source domain. The initial learning rate is set to
@xmath with a weight decay of @xmath and a momentum of 0.9. We train the
network for 3000 iterations, decaying the initial learning rate by a
factor of 10 after 2500 iterations, using SGD. For the domain prediction
branch, we use the features of pool5 as input, performing a global
average pooling followed by a fully-connected layer and a softmax
operator which outputs the domain weights.

Our evaluation is performed using a NVIDIA GeForce 1070 GTX GPU,
implementing all the models with the popular Caffe [ jia2014caffe ]
framework. For the baseline AlexNet architecture we take the pre-trained
model available in Caffe.

Results on Rotated-MNIST. We first test the effectiveness of our model
on the rotated-MNIST benchmark. We compare BSF with the CCSA method in [
motiian2017unified ] and the multi-task autoencoders in [
ghifary2015domain ] (MTAE) and [ rifaiexplicit ] (CAE). The results from
baseline methods are taken directly from [ motiian2017unified ] .

As shown in Table 2.13 , BSF outperforms all the baselines. A remarkable
gain in accuracy is achieved in the @xmath case. We ascribe this gain to
the capability of our deep network to assign, for each target image,
more importance to the source domains corresponding to the closest
orientations, increasing the weights of the associated classifiers.
Indeed, since @xmath is in the middle of the range between all possible
orientations, it is likely that a stronger classifier can be constructed
since we can exploit all the source models appropriately re-weighted. To
further verify the effectiveness of our framework and its ability to
properly combine source-specific models, we also compute for target
samples with different orientations the number of assignments to each
source domain. In this experiment one target sample @xmath is assigned
to a source domain by computing the @xmath . The results are shown in
Fig. 2.52 (the number of assignments are normalized for each row). The
figure clearly shows that the proposed domain prediction branch tends to
associate a target sample to the source domains corresponding to the
closest orientations. Consequently, our deep network classifies target
samples constructing a model from the most related source classifiers.
This results into more accurate predictions than previous
domain-agnostic models due to the specialization of source classifiers
on specific orientations.

Results on PACS. We also perform experiments on the PACS dataset. We
compare BSF with both previous approaches using precomputed features (in
this case DECAF-6 features [ donahue2014decaf ] ) as input and
end-to-end trainable deep models. For the baselines with pre-computed
features, we report the results of MTAE [ ghifary2015domain ] , low-rank
exemplar SVMs (LRE-SVM) [ xu2014exploiting ] , and uDICA [
muandet2013domain ] ), while for the end-to-end trainable deep models,
we report the results of the domain agnostic model coupled with tensor
factorization of [ li2017deeper ] (TF-CNN) and the meta-learning
approach MLDG [ li2017learning ] . For a fair comparison the deep models
[ li2017deeper , li2017learning ] and our network are all based on the
same architecture, i.e. AlexNet. Table 2.14 shows the results of our
comparison. The performance of previous methods are taken directly from
previous papers [ li2017deeper , li2017learning ] . For our approach and
[ li2017deeper ] we also report results obtained without fine-tuning.
Our model outperforms all previous methods. These results are remarkable
because, differently from the rotated-MNIST dataset, in PACS the domain
shift is significant and it is not originated by simple image
perturbations. Therefore, the association between a target sample and
the given source domains is more subtle to capture. For sake of
completeness we also report the performances obtained with the standard
AlexNet network. These results shows that state of the art deep models
have excellent generalization abilities, typically outperforming shallow
models. However, designing deep networks specifically addressing the DG
problem as we do leads to higher accuracy.

We also perform a sensitivity analysis to study the impact of the
parameter @xmath on the performance and demonstrate the benefit of
adding a domain-agnostic classifier. We consider the proposed approach
without fine-tuning.

As shown in Table 2.15 , considering only the source-specific
classifiers ( @xmath ) leads, on average, to the best performances,
surpassing in the majority of the cases a domain agnostic classifier
obtained by setting @xmath . This confirms our original intuition that
addressing DG by fusing multiple source models is an effective strategy.
However, there are few situations where using only source models can
lead to a decrease in accuracy ( e.g. in the setting Cartoon) and
incorporating a domain-agnostic component, even with reduced weight as
@xmath , improves generalization accuracy.

#### 2.5.6 Conclusions

In this section, we presented two deep learning models for addressing
DG. The first, WBN, exploits a weighted formulation of BN to learn
robust classifiers that can be applied to previously unseen target
domains. We showed how this approach is effective in the context of
semantic place categorization in robotics, achieving state-of-the-art
performance on the VPC benchmark. The effectiveness of WBN is also
confirmed by experiments on a large scale dataset of outdoor scenes.

The second, BSF, addresses the problem of DG by exploiting multiple
domain-specific classifiers. In particular, it extends the principles of
WBN, with a domain prediction branch choosing the optimal combination of
source classifiers to use at test time, based on the similarity between
the input image and the samples from the source domains. Differently
from WBN, it goes beyond domain-specific BN layers but explores
domain-specific classification modules. Moreover, a domain agnostic
component is also introduced in our framework further improving the
performance of the method. Experiments demonstrate the effectiveness of
BSF which outperformed the state-of-the-art models on two benchmarks (at
the time of submission).

With WBN and BSF, we have merged domain-specific models either at the
BNs-level or at the classifiers one, due to the ease of linearly
combining their parameters/statistic (WBN) and predictions (BSF). In
future works, it would be interesting to blend domain-specific models at
different levels of the networks, as explored in other works in contexts
such as multi-task learning [ misra2016cross ] , life-long learning [
aljundi2017expert ] and motion control [ zhao2017blending1 ,
zhang2018blending2 ] .

A drawback of both WBN and BSF is the assumption that multiple and
diverse source domains are available at training time. This may be not
always possible due to costly or even unfeasible data collection
processes. Other recent approaches overcome this issue by considering
external sources of knowledge such as automatically-generated training
data [ mccormac2017scenenet ] and online annotators [ song2015robot ] .
Generating synthetic data for the target task could be a huge advantage
for training deep models, but requires the knowledge of the target task
beforehand, something that is not assumed by our model. A possible
solution to this issue consists of endowing the robot with the ability
of access, on-demand, additional information about the target data.
Indeed, the generality of our framework allows the integration of
external sources of knowledge (e.g. generating multiple domains through
web queries or synthetic data). Finally, a major drawback of DG models
is the need for multiple labeled source domains during training. In the
next sections, we will show how we can drop the assumption of having
multiple source domains by extending the DA-layers models to the
Continuous DA and Predictive DA scenarios.

### 2.6 Continuous Domain Adaptation111111M. Mancini, H. Karaoguz, E.
Ricci, P. Jensfelt, B. Caputo. Kitting in the Wild through Online Domain
Adaptation. IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS) 2018.

Despite the remarkable performances achieved by DA algorithms in
computer vision [ li2016revisiting , carlucci2017autodial ] , and their
growing popularity in robot vision [ angeletti2018adaptive ] they
require the presence of images from the target domain in advance during
training. This is a huge limitation, especially in robotics, due to the
likely unpredictable conditions of the environment in which a robot will
be employed. In the previous section, we have seen how we can sidestep
the need for target data during training in case we are given a set of
multiple labeled source domains, addressing the DG scenario. However,
this setting has also a limitation: the need of collecting (and
labeling) data of multiple source domains. In this section, we want to
overcome this issue, performing adaption given just a single source
domain during training, without any target domain data. This setting,
Continuous DA [ hoffman2014continuous ] , requires to cope with the
domain shift directly at test time, as the model processes data of the
target domain.

Here, we consider a realistic application scenario for Continuous DA
algorithms: the task of kitting. This task is the process of grouping
related parts such as gathering components of a personal computer (PC)
into one bin for assembly [ banerjee2015ontology ] . The kitting task
requires the recognition of the parts in the environment, the ability to
pick objects from the bins, and placing them at the correct location [
holz2015skill ] . All of these subtasks are very challenging on their
own but the recognition of the parts is crucial for the robot to
sequentially perform the other subtasks. Already in today’s factory
settings, object recognition tasks possess challenges such as
environmental effects (illumination, viewpoint, etc), varying object
material properties, and cluttered scenes [ liu2012fast ] . In order to
simplify the recognition task, some approaches use machine vision in
rather isolated settings for decreasing the environmental variability [
schyja2012modular ] . Liu et. al [ liu2012fast ] proposed a specially
designed camera system and estimation based on 3D CAD models to robustly
detect and verify the type and the pose of the object. Kaiba et. al. [
kaipa2015resolving ] proposed an interactive method where a remote human
operator resolves ambiguities in the perception system. Unfortunately,
none of the above methods are generic enough to be applied in a truly
unconstrained setting. In this section, we are primarily concerned with
solving the object recognition problem for kitting using vision in the
wild, i.e. in non-isolated settings exhibiting large variations. Right
now, most of the robots in the manufacturing industry are operating in
isolation, primarily because of safety concerns. However, many future
scenarios have robots and humans working closer together, moving robots
into new areas of applications, beyond mass production and preprogrammed
behavior. For this to happen, not only safety but perception will be a
major challenge.

In this section, we describe two main contributions. The first is a
kitting dataset that contains images of objects taken under varying
illumination, viewpoint, and background conditions from a robotic
platform. This dataset provides the community with a novel tool for
studying the robustness of robot vision algorithms to drastic changes in
the appearance of the input images and assess progress in the field. We
are not aware of existing, publicly available kitting databases covering
this range of visual variability.

Second, we describe an approach for achieving online adaptation of a
deep model [ mancini2018kitting ] . Differently from classical DA
approaches, this algorithm can adapt a deep model to any target domain
on the fly, without requiring any target domain data before-hand. We
benchmark the performance of our algorithm on the proposed dataset,
showing how this model is able to produce large improvements on the
target domain performances compared to the base architecture trained on
the source domain, and matching what would be achieved by having all
data from the target available beforehand.

#### 2.6.1 The KTH Handtool Dataset

The KTH Handtool Dataset ¹² ¹² 12
https://www.nada.kth.se/cas/data/handtool/ is collected for evaluating
the object recognition/detection performance of robot vision methods in
varying viewpoint, illumination and background settings, all crucial
abilities for robot kitting in unconstrained, real-world settings.
Instead of having general household objects, the dataset consists of
hand tools in order to represent a workshop setting in a factory. It
consists of 9 different hand tools for 3 different categories; hammer,
plier and screwdriver. The images are collected with a 2-arm stationary
robot platform shown in Fig. 2.54 . Dataset consists of 3 different
illuminations, 2 different cameras (One Kinect camera and one webcam)
with different viewpoints and 2 different background settings that
correspond to 12 (3x3x2) domains in total. For each hand tool,
approximately 40 images with different poses are collected for each
camera and domain setting. Table 2.16 shows example images from
different domains. In total, approximately 4500 RGB images are available
in the dataset.

#### 2.6.2 Problem Formulation

Suppose we collected a set of images using a robotic platform with the
aim of training a robot vision model with it. Since the image collection
has been acquired in the real world, the resulting model will be biased
towards the particular conditions (e.g. illumination, environmental)
under which the images have been acquired. Because of this, if we employ
such a system and the current working conditions are different from
those of the training set, the performances of the model will degrade
due to the presence of a substantial shift between the training and test
data. In this situation, to increase the generalization capabilities of
the robot we can remove the acquisition bias either by collecting more
training data in a large variety of conditions, which is extremely
expensive, or by developing algorithms able to bridge the gap between
the training and test data, aligning the original model to the novel
scenario. The latter is the goal of domain adaptation. Formally, we
assume to have a source domain @xmath , where @xmath is an image and
@xmath the associated semantic label. Opposite to traditional domain
adaptation in a batch setting, during training we have only access to
the source domain @xmath and we do not have any data or prior
information about the target domain @xmath , apart from the set of
semantic labels which is assumed to be shared. When the robot is active,
the current working conditions will compose the target domain and we
will have access to the automatically acquired sequence of images @xmath
. In this scenario, in order to adapt the network parameters @xmath to
this novel domain, we must exploit the incoming test images collected by
the robot on the fly.

#### 2.6.3 ONDA: ONline Domain Adaptation with Batch-Normalization

Starting from the idea of Domain Alignment Layers (Section 2.3 ), we can
follow the same principle of obtaining a target-specific model but
considering an online setting. In particular, instead of having a fixed
target set available during training, we propose to exploit the stream
of data acquired while the robot is acting in the environment and
continuously update the BN statistics. In this way, we can gradually
adapt the deep network to a novel scenario.

Specifically, we start by training the network on the source domain
@xmath , initializing the BN statistics at time @xmath as @xmath .
Assuming that the set of network parameters @xmath are shared between
the source and target domain except for the BN statistics, we can adapt
the network classifier @xmath by updating the BN statistics with the
estimates computed from the sequence @xmath . Defining as @xmath the
number of target images to use for updating online the BN statistics, we
can compute a partial estimate @xmath of the BN statistics as:

  -- -------- --
     @xmath
  -- -------- --

where @xmath is the distribution of activations for a given feature
channel and domain @xmath , following the notation in Section 2.3 . The
global statistics at time @xmath can be updated as follows:

  -- -------- -------- --
     @xmath   @xmath
     @xmath   @xmath
  -- -------- -------- --

where @xmath is the hyperparameter regulating the decay of the moving
average.

The above formulation achieves a similar adaptation effect of DAL layers
[ li2016revisiting , carlucci2017just , carlucci2017autodial ] but with
three main advantages. First, no samples of the target domain, neither
labeled nor unlabeled, are used during training. Thus, no further data
acquisition and annotation efforts are required. Second, since we do not
exploit target data for training, contrary to standard DA algorithms, we
have no bias towards a particular target domain. Third, since the
adaptation process is online, the model can adapt itself to multiple
sequential changes of the working conditions, being able to tackle
unexpected environmental variations (e.g. sudden illumination changes).

The reader might wonder if other possible choices may be considered for
initializing @xmath , such as exploiting a first calibration phase where
the robot collects images of the target domain in order to produce a
first estimate of the BN statistics. Here we choose to use the
statistics estimated on the source domain because 1) we want a model
ready to be employed, without requiring any additional preparation at
test time; 2) the robot may occur in multiple domains during employment
and if a shift occurs (e.g. illumination condition changes) our method
will automatically adapt the visual model to the novel domain starting
from the current estimated statistics: initializing @xmath allows to
check the performance of the algorithm even for multiple sequential
shifts and long-term applications. Obviously our method can benefit from
a calibration phase of the statistics closer to the target working
conditions: we plan to analyze these aspects in the future.

#### 2.6.4 Experimental results

Networks and training protocols. We perform our experiments with the
AlexNet [ krizhevsky2012imagenet ] architecture pre-trained on ImageNet
[ deng2009imagenet ] . We train 3 additional models: a variant of
AlexNet with BN, the DA architecture DIAL from [ carlucci2017just ] and
our ONline DA model (ONDA). Following [ carlucci2017just ] , we add BN
layers or its variants after each fully-connected layer. Both the
standard AlexNet, AlexNet with BN and DIAL are trained with a batch size
of 128. We implemented [ carlucci2017just ] by splitting the batch size
between images of source and target domain proportionally to the number
of images for each set, as in [ carlucci2017just ] , without employing
the entropy-loss for target images [ carlucci2017just ,
carlucci2017autodial ] . We highlight that DIAL is our upper-bound in
this case, since it shares the same philosophy of ONDA but with the
assumption that images of the target domain are available at training
time.

As preprocessing, we rescale all the images in order to ensure a
shortest side of 256 pixels, preserving the aspect ratio and subtracting
the mean value per channel computed over the ImageNet database. As input
to the network we use a random crop of 227 @xmath 227 at training time,
employing a central crop with the same dimensions during test. No
additional data augmentation is performed. For all the variants of the
architecture, we fine-tune the last layers for 30 epochs with an initial
learning rate of 0.001 for fc6 , fc7 and of 0.01 for the classifier,
with a weight decay of 0.0005 and momentum 0.9. We scale the initial
learning rates by a factor of 0.1 after 25 epochs.

In order to apply ONDA, we start from the weights of AlexNet with BN,
training on the given source domain. Then, we perform one iteration over
the target domain, without updating any parameter other than the BN
statistics. As a trade-off between stability of the statistics and
fastness of adaptation we set @xmath and @xmath . We will detail the
impact of these choices in the following subsections.

In all the experiments, we consider the task of object recognition in
the fine-grained setting, with all the 9 classes considered as
classification objective. We report the average accuracy between 5 runs,
shuffling the order of the input images in each run of our model.

##### Domain Adaptation results

In this subsection, we will present the results of our algorithm. In
order to analyze the particular effect that each possible change may
have to the adaptation capabilities of our model, we isolate the sources
of shift. To this extent, we consider two sample starting source
domains: in the first case (Figure (a)a ), the acquisition conditions
are artificial light, Kinect camera and white background; in the second
case we consider cloudy illumination, webcam and brown background
(Figure (b)b ). From these source domains we start by changing only one
of the acquisition conditions (left part of the figures) and gradually
increasing the number of changes to 2 and 3 conditions (middle and right
parts respectively). We report the results for our model after 25%, 50%
and 90% of the target data processed.

As the figures show, our model is able to fill the gap between the BN
baseline (red bars) and the DA upper bound DIAL (green bars) in almost
all settings. Only in few cases, where the shift between the
performances of BN and DIAL is lower, this does not happen (i.e. Figure
(a)a , target artificial-Kinect-brown and directed-Kinect-White). In all
the other settings the gains are remarkable: considering both figures,
the average difference between the performance of BN and ONDA-90 are of
15%, 18% and 20% for the single, double and triple shift cases
respectively. We stress that the gain increases with the amount of shift
between the source and target domains, underlying the importance of
applying DA adaptation methods in changing environments. As expected,
the statistics computed in the first stages (i.e. ONDA-25) are not
always sufficiently representative of the true estimate since they may
be still biased by the statistics computed over the source domain.
However the estimate becomes more precise as more images of the target
domain are processed (i.e. ONDA-50 and ONDA-90), gradually covering the
gap with the estimate computed by DIAL. The fastness of adaptation and
the quality of the estimates depend on the two hyperparameters @xmath
and @xmath . In the next subsection we will analyze their impact to the
final performances of the algorithm.

Ablation study. In this subsection we analyze the impact of the two
hyperparameters, the update frequency @xmath and the decay @xmath , on
the number of images needed by ONDA to estimate the statistics for the
target domain. We use a sample scenario of Figure (b)b , where cloudy
illumination, webcam camera and brown background are the source domain
conditions and artificial light, Kinect camera and white background are
the target domain ones. In the first experiment, we fix @xmath to 10,
varying the value of @xmath . We start by a single pre-trained model of
AlexNet with BN repeating the experiments for 5 runs, shuffling the
order of the input data, and reporting the average accuracy for each
update step.

Results are shown in Figure (a)a : increasing the value of @xmath to 0.2
(green line) or 0.5 (black line) allows the model to achieve a faster
adaptation to the target conditions, with the drawback of a noisier
estimation of the statistics. Thus, increasing @xmath leads to an
unstable convergence of the performance. On the other hand, choosing too
low values of @xmath (e.g. 0.05 or 0.01, purple and gold lines
respectively) allows a more stable convergence of the model, but with
the drawback of slower adaptation to the novel conditions.

Regarding the hyperparameter @xmath , we follow the same protocol of the
first experiment, fixing @xmath to 0.1 and varying the number of images
collected before updating the statistics, @xmath , reporting how the
accuracy changes with respect to the number of frames processed. As
Figure (b)b shows, low values of @xmath (e.g. @xmath ) allows a faster
adaptation, due to the higher update frequency, but at the price of a
noisier estimation of the statistics, which is harmful to the final
accuracy achieved by the model. At the same time, high values of @xmath
(e.g. 20, 30) allow for a more precise estimate of the statistics,
highlighted by the smoothness of the respective lines in the graph, with
the drawback of a lower speed of adaptation to the novel domain, caused
by the lower update frequency.

The speed of adaptation and the final quality of the BN statistics is
obviously a consequence of the values chosen for both hyperparameters.
Obviously @xmath and @xmath are not independent from each other: for a
lower @xmath a lower @xmath should be selected in order to preserve the
final performance of the algorithm and conversely for a higher @xmath ,
a higher @xmath will allow a faster adaptation of the model. As a
trade-off between fast adaptation and good results, we found
experimentally that choosing @xmath and @xmath worked well for both
short and long term experiments.

#### 2.6.5 Conclusions

In this section, we presented a novel dataset for addressing the kitting
task in robotics. The dataset takes into account multiple variations of
acquisition conditions such as camera, illumination, and background
changes which may occur during the robot employment. This dataset is
intended for testing the robustness of robot vision algorithms to
changing environments, providing a novel benchmark for assessing the
robustness of robot vision systems.

Additionally, we described ONDA, an algorithm capable of performing
online adaptation of deep models to any unseen visual domain. The
algorithm, based on the update of the statistics of batch-normalization
layers, can continuously adapt the model to the current environmental
conditions of the robot, providing more robustness to unexpected working
conditions. Experiments on the newly proposed dataset, confirm the
ability of ONDA to fill the gap between a standard architecture, trained
only on source data, and its domain adapted counterpart, without
requiring any additional target data during training.

It is worth highlighting how, despite its effectiveness and the fact of
requiring a single source domain (differently from the DG approaches in
Section 2.5 ), the method has two main drawbacks. Since it adapts to the
stream of the target samples, its adaptation is gradual and it cannot
work under abrupt changes of the input distribution. As a consequence,
it can only address one target domain shift at the time, contrary to DG
approaches, which build a single model for multiple target domains. In
the next section, we will show how we can merge the benefits of DG and
Continuous DA, proposing the first deep model for the task of Predictive
DA.

Finally, as future works, we plan to enlarge the dataset, including more
sources of variations and more objects. We further plan to provide a
deeper analysis of our algorithm with more architectures, as well as
exploring possible extensions that could exploit knowledge coming from
previously met scenarios.

### 2.7 Predictive Domain Adaptation 131313M. Mancini, S. Rota Bulò, B.
Caputo, E. Ricci. AdaGraph: Unifying Predictive and Continuous Domain
Adaptation through Graphs. IEEE/CVF International Conference on Computer
Vision and Pattern Recognition (CVPR) 2019.

An underline common thread linking the sections of this chapter is the
importance of being able to overcome the domain shift problem even under
incomplete (Section 2.4 ) or absent (Sections 2.5 and 2.6 ) information
about our target domain during training. In particular, although it
might be reasonable for some applications to have target samples
available during training, in most cases data collection and labeling
might be too costly (e.g. robotics) or even unfeasible (e.g. hazardous
environments). Therefore, we argued that it is important to build models
able to perform domain adaptation even without target data at training
time.

For this reason, in Sections 2.5 and 2.6 we focused on scenarios that do
not assume the presence of target data during training, namely DG and
Continuous DA. In both scenarios, different information is used to
overcome the domain shift. In the first, DG, the presence of multiple
labeled source domains allows us to build models disentangling
domain-specific and semantic-specific information, possibly generalizing
to unseen input distributions. In the second, Continuous DA, target data
received at test time are used to update the model gradually. Both
scenarios have some inherent drawbacks. In DG, we require the presence
of multiple labeled source domain, something that might be hard to
obtain. In Continuous DA, instead, the model gradually adapts to the
target distribution and, consequently, (i) it cannot work under abrupt
changes of domains and (ii) it can address only one target domain shift
at the time.

In this section, we want to take a step forward by (i) dropping the
assumption of having multiple labeled source domains (opposite to DG)
and (ii) adding the possibility to rapidly adapt the model to multiple
target domains (opposite to Continuous DA). Following this idea,
previous studies proposed the Predictive Domain Adaptation (PDA)
scenario [ yang2016multivariate ] , where neither the data nor the
labels from the target are available during training. Only annotated
source samples are available, together with additional information from
a set of auxiliary domains, in form of unlabeled samples and associated
metadata ( e.g. corresponding to the image timestamp or camera pose,
etc).

In this section we describe AdaGraph [ mancini2019adagraph ] , a deep
architecture for PDA. As for the works presented in previous sections,
we learn a set of domain-specific models by considering a common
backbone network with domain-specific alignment layers embedded into it
[ carlucci2017autodial , carlucci2017just , li2016revisiting ] .
However, differently from the previous works, we propose to exploit
metadata and auxiliary samples by building a graph which explicitly
describes the dependencies among domains. Within the graph, nodes
represent domains, while edges encode relations between domains, imposed
by their metadata. Thanks to this construction, when metadata for the
target domain are available at test time, the domain-specific model can
be recovered. We further exploit target data directly at test time by
devising an approach for continuously updating the deep network
parameters once target samples are made available (Figure 2.62 ). We
demonstrate the effectiveness of our method with experiments on three
datasets: the Comprehensive Cars (CompCars) [ yang2015large ] , the
Century of Portraits [ ginosar2015century ] and the CarEvolution
datasets [ RematasICCVWS13 ] , showing that our method outperforms
state-of-the-art PDA approaches. Finally, we show that the proposed
approach for continuous updating of the network parameters can be used
for continuous domain adaptation, producing more accurate predictions
than previous methods [ hoffman2014continuous , li2018domain ] .

To summarize, the contributions presented in this section are: (i) the
first deep architecture for addressing the problem of PDA; (ii) a
strategy for injecting metadata information within a deep network
architecture by encoding the relation between different domains through
a graph; (iii) a simple strategy for refining the predicted target model
which exploits the incoming stream of target data directly at test time.

#### 2.7.1 Problem Formulation

Our goal is to produce a model that is able to accomplish a task in a
target domain @xmath for which no data is available during training,
neither labeled nor unlabeled. The only information we can exploit is a
characterization of the content of the target domain in the form of
metadata @xmath plus a set of known domains @xmath , each of them having
associated metadata. All the domains in @xmath carry information about
the task we want to accomplish in the target domain. In particular,
since in this work we focus on classification tasks, we assume that
images from the domains in @xmath and @xmath can be classified with
semantic labels from a same set @xmath . As opposed to standard DA
scenarios, the target domain @xmath does not necessarily belong to the
set of known domains @xmath . Also, we assume that @xmath can be
partitioned into a labeled source domain @xmath and @xmath unlabeled
auxiliary domains @xmath .

In the specific, in this section we focus on the predictive DA (PDA)
problem, aimed at regressing the target model parameters using data from
the domains in @xmath . We achieve this objective by (i) interconnecting
each domain in @xmath using the given domain metadata; (ii) building
domain-specific models from the data available in each domain in @xmath
; (iii) exploiting the connection between the target domain and the
domains in @xmath , inferred from the respective metadata, to regress
the model for @xmath .

A schematic representation of the method is shown in Figure 2. We
propose to use a graph because of its seamless ability to encode
relationships within a set of elements (domains in our case). Moreover,
it can be easily manipulated to include novel elements (such as the
target domain @xmath ).

#### 2.7.2 AdaGraph: Graph-based Predictive DA

We model the dependencies between the various domains by instantiating a
graph composed of nodes and edges. Each node represents a different
domain and each edge measures the relatedness of two domains. Each edge
of the graph is weighted, and the strength of the connection is computed
as a function of the domain-specific metadata. At the same time, in
order to extract one model for each available domain, we employ recent
advances in domain adaptation involving the use of domain-specific
batch-normalization layers [ li2018adaptive , carlucci2017just ] . With
the domain-specific models and the graph we are able to predict the
parameters for a novel domain that lacks data by simply (i)
instantiating a new node in the graph and (ii) propagating the
parameters from nearby nodes, exploiting the graph connections.

Connecting domains through a graph. Let us denote the space of domains
as @xmath and the space of metadata as @xmath . As stated in Section
2.7.1 , in the PDA scenario, we have a set of known domains @xmath and a
bijective mapping @xmath relating domains and metadata. For simplicity,
we regard as unknown a metadata @xmath that is not associated to domains
in @xmath , i.e. such that @xmath .

Here we structure the domains as a graph @xmath , where @xmath
represents the set of vertices corresponding to domains and @xmath the
set of edges, i.e. relations between domains. Initially the graph
contains only the known domains so @xmath . In addition, we define an
edge weight @xmath that measures the relation strength between two
domains @xmath by computing a distance between the respective metadata,
i.e.

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

where @xmath is a distance function on @xmath .

Let @xmath be the space of possible model parameters and assume we have
properly exploited the domain data from each domain in @xmath to learn a
set of domain-specific models (we will detail this procedure in the next
subsection). We can then define a mapping @xmath , relating each domain
to its set of domain-specific parameters. Given some metadata @xmath we
can recover an associated set of parameters via the mapping @xmath
provided that @xmath . In order to deal with metadata that is unknown,
we introduce the concept of virtual node. Basically, a virtual node
@xmath is a domain for which no data is available but we have metadata
@xmath associated to it, namely @xmath . For simplicity, let us directly
consider the target domain @xmath . We have @xmath and we know @xmath .
Since no data of @xmath is available, we have no parameters that can be
directly assigned to the domain. However, we can estimate parameters for
@xmath by using the domain graph @xmath . Indeed, we can relate @xmath
to other domains @xmath using @xmath defined in ( 2.15 ) by opportunely
extending @xmath with new edges @xmath for all or some @xmath (e.g. we
could connect all @xmath that satisfy @xmath for some @xmath ). The
extended graph @xmath with the additional node @xmath and the new edge
set @xmath can then be exploited to estimate parameters for @xmath by
propagating the model parameters from nearby domains. Formally we
regress the parameters @xmath through the formula

  -- -------- -- --------
     @xmath      (2.16)
  -- -------- -- --------

where we normalize the contribution of each edge by the sum of the
weights of the edges connecting node @xmath . With this formula we are
able to provide model parameters for the target domain @xmath and, in
general, for any unknown domain by just exploiting the corresponding
metadata.

We want to highlight that this strategy only depends extending the graph
with a virtual node @xmath and computing the relative edges. While the
relations of @xmath with other domains can be inferred from given
metadata, as in ( 2.15 ), there could be cases in which no metadata is
available for the target domain. In such situations, we can still
exploit the incoming target image @xmath to build a probability
distribution over nodes in @xmath , in order to assign the new data
point to a mixture of known domains. To this end, let use define @xmath
the conditional probability of an image @xmath , where @xmath is the
image space, to be associated with a domain @xmath . From this
probability distribution, we can infer the parameters of a
classification model for @xmath through:

  -- -------- -- --------
     @xmath      (2.17)
  -- -------- -- --------

where @xmath is well defined for each node linked to a known domain,
while it must be estimated with ( 2.16 ) for each virtual domain @xmath
for which @xmath .

In practice, the probability @xmath is constructed from a metadata
classifier @xmath , trained on the available data, that provides a
probability distribution over @xmath given @xmath , which can be turned
into a probability over @xmath through the inverse mapping @xmath .

Extracting node specific models. We have described how to regress model
parameters for an unknown domain by exploiting the domain graph. Now, we
focus on the actual problem of training domain-specific models using
data available from the known domains @xmath . Since @xmath entails a
labeled source domain @xmath and a set of auxiliary domains @xmath , we
cannot simply train independent models with data from each available
domain due to the lack of supervision on domains in @xmath for the
target classification task. For this reason, we need to estimate the
model parameters for the unlabeled domains @xmath by exploiting DA
techniques.

To achieve this, we start from the domain alignment layers presented in
[ li2018adaptive , carlucci2017autodial , carlucci2017just ] and
described in Section 2.3 . In this scenario, the set of parameters for a
domain @xmath , @xmath is composed of different parts. Formally for each
domain we have @xmath , where @xmath holds the domain-agnostic
components and @xmath the domain-specific ones. In our case @xmath
comprises parameters from standard layers (i.e. the convolutional and
fully connected layers of the architecture), while @xmath comprises
parameters and statistics of the domain-specific BN layers.

We start by using the labeled source domain @xmath to estimate @xmath
and initialize @xmath . In particular, we obtain @xmath by minimizing
the standard cross-entropy loss:

  -- -------- -- --------
     @xmath      (2.18)
  -- -------- -- --------

where @xmath is the classification model for the source domain, with
parameters @xmath .

To extract the domain-specific parameters @xmath for each @xmath , we
employ 2 steps: the first is a selective forward pass for estimating the
domain-specific statistics while the second is the application of a loss
to further refine the scale and bias parameters. Formally, we replace
each BN layer in the network with a GraphBN counterpart (GBN), where the
forward pass is defined as follows:

  -- -------- -- --------
     @xmath      (2.19)
  -- -------- -- --------

where @xmath and @xmath are the node specific scale and bias parameters
of the BN layers. Basically in a GBN layer, the set of BN parameters and
statistics to apply is conditioned on the node/domain to which @xmath
belongs. While this equation is similar to Eq. ( 2.1 ), we highlight
that, differently from it and [ carlucci2017just , carlucci2017autodial
] , here we use domain-specific scale and bias parameters, not only
statistics. During training, as for standard BN, we update the
statistics by leveraging their estimate obtained from the current batch
@xmath :

  -- -------- -- --------
     @xmath      (2.20)
  -- -------- -- --------

where @xmath is the set of elements in the batch belonging to domain
@xmath . As for the scale and bias parameters, we optimize them by means
of a loss on the model output. For the auxiliary domains, since the data
are unlabeled, we use an entropy loss, while a cross-entropy loss is
used for the source domain:

  -- -------- -------- -- --------
     @xmath   @xmath      (2.21)
     @xmath   @xmath      (2.22)
  -- -------- -------- -- --------

where @xmath represents the whole set of domain-specific parameters and
@xmath the trade off between the cross-entropy and the entropy loss.

While ( 2.21 ) allows to optimize the domain-specific scale and bias
parameters, it does not take into account the presence of the
relationship between the domains, as imposed by the graph. A way to
include the graph within the optimization procedure is to modify ( 2.19
) as follows:

  -- -------- -- --------
     @xmath      (2.23)
  -- -------- -- --------

with:

  -- -------- -- --------
     @xmath      (2.24)
  -- -------- -- --------

for @xmath . Basically we use a scale and bias parameters during the
forward pass which are influenced by the graph edges, as described in (
2.24 ).

Taking into account the presence of @xmath during the forward pass is
beneficial for mainly two reasons. First, it allows to keep a
consistency between how those parameters are computed at test time and
how they are used at training time. Second, it allows to regularize the
optimization of @xmath and @xmath , which may be beneficial in cases
where a domain contains few data. While the same procedure may be
applied also for @xmath , in our current design we avoid mixing them
during training. This choice is linked to the fact that each image
belongs to a single domain and keeping the statistics separate allows us
to estimate them more precisely.

At test time, once we have initialized the domain-specific parameters of
@xmath using either ( 2.16 ) or ( 2.17 ), the forward pass of each GBN
layer is computed through ( 2.23 ). In Figure 2.63 , we sketch the
behaviour of AdaGraph both at training and test time.

#### 2.7.3 Model Refinement through Joint Prediction and Adaptation

While the approach described in the previous section allows to perform a
blind adaptation of a model to a target domain given metadata, it is not
completely true that we have no information about the images of the
target domain. In fact, while at training time we have no access to
target data, at test time target samples are gradually made available.
While we could passively classify the target data stream, this would not
be an effective choice, since the information coming directly from
target images is valuable and can be leveraged to refine our model. This
will be extremely important, e.g. in the case of an inaccurate estimate
of the target model parameters or in the presence of noisy metadata. In
those cases, exploiting the stream of incoming images would compensate
for the initial error.

To this end, we equip our model with a simple yet effective strategy to
perform continuous domain adaptation. Following recent works [
li2018adaptive ] and our ONDA framework, we start with the observation
that a simple way to continuously adapt a model to the incoming stream
of target data, is just by updating the BN statistics. Formally, let us
suppose our target domain is composed by a set of @xmath observations
@xmath . Since we will receive one data sample at time, we provide our
model with a memory. This memory has a fixed size @xmath (e.g. @xmath in
all our experiments) and allows to store a sequence of @xmath target
samples. Once these samples have been collected, we use them to compute
a local estimate of the GBN statistics for the target domain. This
estimate will be added to the global estimation of the statistics used
by the GBN layers of our model, in the same way BN statistics are
updated during training. After the update, we free the memory and
restart collecting samples of the target domain. Obviously the presence
of a memory can be used not only to estimate the statistics for updating
the GBN layers, but also as a starting point for more complex
optimization strategies. In this work, we exploit the memory to further
refine the regressed scale and bias parameters. In particular, we follow
recent BN-based DA algorithms [ carlucci2017autodial , carlucci2017just
] and employ an entropy loss on the target domain data collected in the
memory. This loss is applied to the "output" normalized through the
statistics computed using the samples within the memory, in order to
ensure a consistency with the training phase for the update of the
statistics and the parameters of each GBN layer.

#### 2.7.4 Experimental results

##### Experimental setting

Datasets. We analyze the performance of AdaGraph on three datasets: the
Comprehensive Cars (CompCars) [ yang2015large ] , the Century of
Portraits [ ginosar2015century ] and the CarEvolution [ RematasICCVWS13
] .

The Comprehensive Cars (CompCars) [ yang2015large ] dataset is a
large-scale database composed of 136,726 images spanning a time range
between 2004 and 2015. As in [ yang2016multivariate ] , we use a subset
of 24,151 images with 4 types of cars ( MPV , SUV , sedan and hatchback
) produced between 2009 and 2014 and taken under 5 different view points
(front, front-side, side, rear, rear-side). Considering each view point
and each manufacturing year as a separate domain we have a total of 30
domains. As in [ yang2016multivariate ] we use a PDA setting where 1
domain is considered as source, 1 as target and the remaining 28 as
auxiliary sets, for a total of 870 experiments. In this scenario, the
metadata are represented as vectors of two elements, one corresponding
to the year and the other to the view point, encoding the latter as in [
yang2016multivariate ] .

Century of Portraits (Portraits) [ ginosar2015century ] is a large scale
collection of images taken from American high school yearbooks. The
portraits are taken over 108 years (1905-2013) across 26 states. We
employ this dataset in a gender classification task, in two different
settings. In the first setting we test our PDA model in a leave-one-out
scenario, with a similar protocol to the tests on the CompCars dataset.
In particular, to define domains we consider spatio-temporal information
and we cluster images according to decades and to spatial regions (we
use 6 USA regions, as defined in [ ginosar2015century ] ). Filtering out
the sets where there are less than 150 images, we obtain 40 domains,
corresponding to 8 decades (from 1934 on) and 5 regions ( New England ,
Mid Atlantic , Mid West , Pacific , Southern ). We follow the same
experimental protocol of the CompCars experiments, i.e. we use one
domain as source, one as target and the remaining 38 as auxiliaries. We
encode the domain metadata as a vector of 3 elements, denoting the
decade, the latitude (0 or 1, indicating north/south) and the east-west
location (from 0 to 3), respectively. Additional details can be found in
the appendix. In a second scenario, we use this dataset for assessing
the performance of our continuous refinement strategy. In this case we
employ all the portraits before 1950 as source samples and those after
1950 as target data.

CarEvolution [ yang2015large ] is composed of car images collected
between 1972 and 2013. It contains 1008 images of cars produced by three
different manufacturers with two car models each, following the
evolution of the production of those models during the years. We choose
this dataset in order to assess the effectiveness of our continuous
domain adaptation strategy. A similar evaluation has been employed in
recent works considering online DA [ li2018domain ] . As in [
li2018domain ] , we consider the task of manufacturer prediction where
there are three categories: Mercedes , BMW and Volkswagen . Images of
cars before 1980 are considered as the source set and the remaining are
used as target samples.

Networks and Training Protocols. To analyze the impact on performance of
our main contributions we consider the ResNet-18 architecture [
he2016deep ] and perform experiments on the Portraits dataset. In
particular, we apply our model by replacing each BN layer with its
AdaGraph counterpart. We start with the network pre-trained on ImageNet,
training it for 1 epoch on the source dataset, employing Adam as
optimizer with a weight decay of @xmath and a batch size of 16. We
choose a learning rate of @xmath for the classifier and @xmath for the
rest of the architecture. We train the network for 1 epoch on the union
of source and auxiliary domains to extract domain-specific parameters.
We keep the same optimizer and hyperparameters except for the learning
rates, decayed by a factor of 10. The batch size is kept to 16, but each
batch is composed by elements of a single pair year-region belonging to
one of the available domains (either auxiliary or source). The order of
the pairs is randomly sampled within the set of allowed ones.

In order to fairly compare with previous methods we also consider Decaf
features [ donahue2014decaf ] . In particular, in the experiments on the
CompCars dataset, we use Decaf features extracted at the fc7 layer. Note
that these features are comparable to the ones used in [
yang2016multivariate ] (i.e. penultimate layer of the VGG-F model in [
chatfield2014return ] ). Similarly, for the experiments on CarEvolution,
we follow [ li2018domain ] and use Decaf features extracted at the fc6
layer. In both cases, we apply our model by adding either a BN layer or
our AdaGraph approach directly to the features, followed by a ReLU
activation and a linear classifier. For these experiments we train the
model on the source domain for 10 epochs using Adam as optimizer with a
learning rate of @xmath , a batch size of 16 and a weight decay of
@xmath . The learning rate is decayed by a factor of 10 after 7 epochs.
For CompCars, when training with the auxiliary set, we use the same
optimizer, batch size and weight decay, with a learning rate @xmath for
1 epoch. Domain-specific batches are randomly sampled, as for the
experiments on Portraits.

For all the experiments we use as distance measure @xmath with @xmath
and set @xmath equal to @xmath , both in the training and in the
refinement stage. At test time, we classify each input image as it
arrives, performing the refinement step after the classification. The
buffer size in the refinement phase is equal to 16 and we set @xmath ,
the same used for updating the GBN components while training with the
auxiliar domains.

We implemented ¹⁴ ¹⁴ 14 The code is available at
https://github.com/mancinimassimiliano/adagraph our method with the
PyTorch [ paszke2019pytorch ] framework and our evaluation is performed
using a NVIDIA GeForce 1080 Ti GTX GPU.

##### Results

In this section we report the results of our evaluation, showing both an
empirical analysis of the proposed contributions and a comparison with
state-of-the-art-approaches.

Analysis of AdaGraph. We first analyze the performance of our approach
by employing the Portraits dataset. In particular, we evaluate the
impact of (i) introducing a graph to predict the target domain BN
statistics ( AdaGraph BN ), (ii) adding scale and bias parameters
trained in isolation ( AdaGraph SB ) or jointly ( AdaGraph Full ) and
(iii) adopting the proposed refinement strategy ( AdaGraph + Refinement
). As baseline ¹⁵ ¹⁵ 15 We do not report the results of previous
approaches [ yang2016multivariate ] since the code is not publicly
available. we consider the model trained only on the source domain and,
as an upper bound, a corresponding DA method which is allowed to use
target data during training. In our case, the upper bound corresponds to
a model similar to the method proposed in [ carlucci2017autodial ] .

The results of our ablation are reported in Table 2.17 , where we report
the average classification accuracy corresponding to two scenarios:
across decades (considering the same region for source and target
domains) and across regions (considering the same decade for source and
target dataset). The first scenario corresponds to 280 experiments,
while the second to 160 tests. As shown in the table, by simply
replacing the statistics of BN layers of the source model with those
predicted through our graph a large boost in accuracy is achieved (
@xmath in the across decades scenario and @xmath in the across regions
one). At the same time, estimating the scale and bias parameters without
considering the graph is suboptimal. In fact there is a misalignment
between the forward pass of the training phase (i.e. considering only
domain-specific parameters) and how these parameters will be combined at
test time (i.e. considering also the connection with the other nodes of
the graph). Interestingly, in the across regions setting, our full model
slightly drops in performance with respect to predicting only the BN
statistics. This is probably due to how regions are encoded in the
metadata (i.e. considering geographical location), making it difficult
to capture factors (e.g. cultural, historical) which can be more
discriminative to characterize the population of a region or a state.
However, as stated in Section 2.7.3 , employing a continuous refinement
strategy allows the method to compensate for prediction errors. As shown
in Table 2.17 , with a refinement step ( AdaGraph + Refinement ) the
accuracy constantly increases, filling the gap between the performance
of the initial model and our DA upper bound.

It is worth noting that applying the refinement procedure to the source
model ( Baseline + Refinement ) leads to better performance (about
@xmath in the across decades scenario and @xmath for across regions
one). More importantly, the performance of the Baseline + Refinement
method is always worse than what obtained by AdaGraph + Refinement ,
because our model provides, on average, a better starting point for the
refinement procedure.

Figure 2.64 shows the results associated to the across decades scenario.
Each bar plot corresponds to experiments where the target domain is
associated to a specific year. As shown in the figure, on average, our
full model outperforms both AdaGraph BN and AdaGraph SB , showing the
benefit of the proposed graph strategy. The results in the figure
clearly also show that our refinement strategy always leads to a boost
in performance.

Comparison with the state of the art. Here we compare the performances
of our model with state-of-the-art PDA approaches. We use the CompCars
dataset and we benchmark against the Multivariate Regression (MRG)
methods proposed in [ yang2016multivariate ] . We apply our model in the
same setting as [ yang2016multivariate ] and perform 870 different
experiments, computing the average accuracy (Table 2.18 ). Our model
outperforms the two methods proposed in [ yang2016multivariate ] by
improving the performances of the Baseline network by @xmath . AdaGraph
alone outperforms the Baseline model when it is updated with our
refinement strategy and target data ( Baseline + Refinement ). When
coupled with a refinement strategy, our graph-based model further
improves the performances, filling the gap between AdaGraph and our DA
upper bound. It is interesting to note that our model is also effective
when there are no metadata available in the target domain. In the table,
AdaGraph (images) corresponds to our approach when, instead of
initializing the BN layer for the target exploiting metadata, we employ
the current input image and a domain classifier to obtain a probability
distribution over the graph nodes, as described in Section 2.7.2 . The
results in the Table show that AdaGraph (images) is more accurate than
AdaGraph (metadata) .

Exploiting AdaGraph Refinement for Continous Domain Adaptation. In
Section 2.7.3 , we have shown a way to boost the performances of our
model by leveraging the stream of incoming target data and refine the
estimates of the target BN statistics and parameters. Throughout the
experimental section, we have also demonstrated how this strategy
improves the target classification model, with performances close to DA
methods which exploit target data during training.

In this section we show how this approach can be employed as a
competitive method in the case of continuous domain adaptation [
hoffman2014continuous ] . We consider the CarEvolution dataset and
compare the performances of our proposed strategy with two state of the
art algorithms: the manifold-based adaptation method in [
hoffman2014continuous ] and the low-rank SVM strategy presented in [
li2018domain ] . As in [ li2018domain ] and [ hoffman2014continuous ] ,
we apply our adaptation strategy after classifying each novel image and
compute the overall accuracy. The images of the target domain are
presented to the network in a chronological order i.e. from 1980 to
2013. The results are shown in Table 2.19 . While the integration of a
BN layer alone leads to better performances over the baseline, our
refinement strategy produces an additional boost of about 3%. If scale
and bias parameters are refined considering the entropy loss, accuracy
further increases.

We also test the proposed model on a similar task considering the
Portraits dataset. The results of our experiments are shown in Table
2.20 . Similarly to what observed on the previous experiments,
continuously adapting our deep model as target data become available
leads to better performance with respect to the baseline. The refinement
of scale and bias parameters contributes to a further boost in accuracy.

#### 2.7.5 Conclusions

We present the first deep architecture for Predictive Domain Adaptation,
AdaGraph. We leverage metadata information to build a graph where each
node represents a domain, while the strength of an edge models the
similarity among two domains according to their metadata. We then
propose to exploit the graph for the purpose of DA and we design novel
domain-alignment layers. This framework yields the new state of the art
on standard PDA benchmarks. We further present an approach to exploit
the stream of incoming target data such as to refine the target model.
We show that this strategy itself is also an effective method for
continuous DA, outperforming state-of-the-art approaches, and our
previous ONDA model. In future works, it would be interesting to explore
methodologies to incrementally update the graph and to automatically
infer relations among domains, even in the absence of metadata.
Moreover, the connections among the nodes can be used in few-shot
scenarios, using the relations among domains to provide additional
feedback to nodes of domains with few samples.

This section concludes our works which considered the domain shift
problem in isolation both in the presence and in absence of target data
and under different settings. In the next chapters, we will describe our
works that tackled the semantic shift problem in isolation first
(Chapter 3 ) and coupled with the domain shift one lately (Chapter 4 ).

## Chapter 3 Recognizing New Semantic Concepts

This chapter analyzes different problems concerning the extension of a
pre-trained architecture to new visual concepts in an incremental
fashion, varying the knowledge we want to add and what we want to
recognize. As in the previous chapter, we start by providing a general
formulation of the problem (Sec. 3.1 ) and reviewing previous works on
incremental learning of classes/tasks and in an open world (Sec. 3.2 ).
In Sec. 3.3 we show how we can extend a model perform the same task
(i.e. classification) across multiple visual domains with different
output spaces (e.g. digits recognition, street signal classification)
through affinely transformed binary mask [ mancini2020boostingmva ] .
This approach extends previous works on multi-domain learning [
mallya2018piggyback ] , achieving the highest (at the time of
acceptance) trade-off among learning new tasks effectively, and using a
low number of parameters. In Sec. 3.4 we focus on the incremental class
learning problem when new classes are added to the same classification
head of old ones but in the context of semantic segmentation [
cermelli2020modeling ] . Here we show how there is an inherent problem
in this setting caused by the semantic shift of the background class
across different incremental steps. We show how this problem can be
addressed by a simple modification of the cross-entropy and distillation
losses employed in previous approaches [ li2017learning ] . Finally, in
Sec. 3.5 , we analyze the problem of open-world recognition, where the
goal is to not only include new classes incrementally but also to detect
if an image belongs to an unknown category. We analyze the problem in
robotics scenarios, starting by implementing the first deep approach for
this problem [ mancini2019knowledge ] . The approach extends standard
non-parametric methods [ bendale2015towards ] and is further improved in
a subsequent work by clustering-based losses and class-specific
rejection options [ fontanel2020boosting ] . In [ mancini2019knowledge ]
we also discuss how the approach could be employed in a realistic
scenario by obtaining datasets with new knowledge directly from the web,
a first step towards having agents able to automatically expand their
visual recognition capabilities by reasoning on what they see in the
real world.

### 3.1 Problem statement

Overview. In Chapter 2 we have analyzed multiple algorithms being able
to overcome the domain shift problem in various scenarios. However,
while the domain shift is a crucial issue for the applicability of
visual systems in real scenarios, it deals with one side of the problem:
changes in the input distribution without changes in the semantic space.
In this chapter, we are interested in tackling the opposite problem.
Given a model trained to recognize a set of classes in a given domain,
we want to extend its output space, equipping it with the ability to
recognize semantic concepts not included in the initial training set.

The methodologies used to add new knowledge to a pre-trained model can
be roughly divided into three main categories, depending on the
information we have about the training classes. In the first category,
we receive data for the novel concepts we want our model to recognize.
This scenario is usually called continual/incremental learning [
de2019continual ] and requires to add new knowledge to the model without
access to the initial training set and, more importantly, without
forgetting previous knowledge [ mccloskey1989catastrophic ,
kemker2018measuring , goodfellow2013empirical ] . In the second
category, we have models using few sample images of the classes of
interests at test time, using the initial training set to learn how to
compare this support set composed of few images with a query image.
These models fall in the few-shot learning paradigm and require to
receive, at test time, sample images of the classes we want to recognize
[ snell2017prototypical , finn2017model ] . The last category of methods
learn to recognize concepts beyond the initial training set without any
image available but using class descriptions (e.g. binary attributes [
lampert2013awa ] , word embeddings [ mikolov2013distributed ] ). In this
scenario, called zero-shot learning [ xian2018zeroshotgood ] , a model
has to map images in a given semantic embedding space where all classes
(seen and unseen) are projected. In this way, it is possible to compare
images with unseen and/or seen concepts to perform the final
classification.

In this thesis, we will consider both incremental and zero-shot learning
models. In particular, in this chapter, we will consider scenarios where
the domain shift is not present, i.e. training and test domains are the
same, but the semantic space of the model is incrementally extended over
time, as for the first category. In Chapter 4 , we will show how we can
obtain a model attacking both domain- and semantic shift, recognizing
unseen categories (as in zero-shot learning) in unseen domains (as in
domain generalization).

Incremental Learning. Let us formalize the incremental learning problem.
Assuming we have a model pre-trained on a set @xmath with @xmath and
@xmath . Note that @xmath is the input space, as in Section 2.1 , while
@xmath is the output space of the initial training set (i.e. the set of
classes in @xmath ). Using this set we can obtain a function @xmath
parametrized by @xmath and mapping images into the initial output space.
To include new concepts in @xmath , we receive a new dataset containing
the new concepts of interests. Since we might perform multiple training
steps, let us denote with @xmath the dataset we receive at time @xmath .
Note that, while the input space does not change (i.e. @xmath ) the
output space does and we have @xmath with @xmath if @xmath . After
@xmath training learning steps, our goal is to obtain a model @xmath
where the output space @xmath comprises all the concepts seen until the
training step T, i.e. @xmath .

Under this definition, we have different problems, depending on how the
output space is built [ chaudhry2018riemannian ] . The first distinction
is on the number of classification heads. We have single-head models,
where there is a single classification head for all the concepts in
@xmath , and multi-head , in case we have one head per set of classes
@xmath . In this latter scenario, despite some exceptions [
aljundi2019task , rao2019continual ] , it is common to give as input to
the prediction function the information about the output space of
interests, i.e. @xmath with @xmath . We will analyze this scenario in
the context of Multi-Domain Learning [ rebuffi2017learning ,
bilen2017universal ] , in Section 3.3 .

Considering the single-head scenario, the second distinction relates to
the limits of @xmath . In case @xmath is closed-ended , we have the
standard incremental class learning scenario and we ask our model to
recognize which to which class in @xmath our image belongs. We will
analyze this setting in Section 3.4 , in the task of semantic
segmentation. In case @xmath is open-ended , i.e. the model includes a
rejection option for known classes, we are in the open world recognition
one, and our model is asked to recognize the class of an image and,
eventually, detecting if it belongs to an unknown concept. This scenario
will be the focus of Section 3.5 .

In the following section, we will report the relevant literature for
incremental learning, multi-domain learning and open world recognition.

### 3.2 Related Works

Incremental Learning. The problem of catastrophic forgetting [
mccloskey1989catastrophic ] has been extensively studied for image
classification tasks [ de2019continual ] . Previous works can be grouped
in three categories [ de2019continual ] : replay-based [
rebuffi2017icarl , castro2018end , shin2017continual , hou2019learning ,
wu2018memory , ostapenko2019learning ] , regularization-based [
kirkpatrick2017overcoming , chaudhry2018riemannian , zenke2017continual
, li2017learning , dhar2019learning ] , and parameter isolation-based [
mallya2018packnet , mallya2018piggyback , rusu2016progressive ] . In
replay-based methods, examples of previous tasks are either stored [
rebuffi2017icarl , castro2018end , hou2019learning , wu2019large ] or
generated [ shin2017continual , wu2018memory , ostapenko2019learning ]
and then replayed while learning the new task. Parameter isolation-based
methods [ mallya2018packnet , mallya2018piggyback , rusu2016progressive
] assign a subset of the parameters to each task to prevent forgetting.
Regularization-based methods can be divided in prior-focused and
data-focused. The former [ zenke2017continual , chaudhry2018riemannian ,
kirkpatrick2017overcoming , aljundi2018memory ] define knowledge as the
parameters value, constraining the learning of new tasks by penalizing
changes of important parameters for old ones. The latter [
li2017learning , dhar2019learning ] exploit distillation [
hinton2015distilling ] and use the distance between the activations
produced by the old network and the new one as a regularization term to
prevent catastrophic forgetting.

Despite these progresses, very few works have gone beyond image-level
classification. A first work in this direction is [
shmelkov2017incremental ] which considers ICL in object detection
proposing a distillation-based method adapted from [ li2017learning ]
for tackling novel class recognition and bounding box proposals
generation. In this work we also take a similar approach to [
shmelkov2017incremental ] and we resort on distillation. However, here
we propose to address the problem of modeling the background shift which
is peculiar of the semantic segmentation setting.

To our knowledge, the problem of ICL in semantic segmentation has been
addressed only in [ ozdemir2018learn , ozdemir2019extending ,
tasar2019incremental , michieli2019incremental ] . Ozdemir et al. [
ozdemir2018learn , ozdemir2019extending ] describe an ICL approach for
medical imaging, extending a standard image-level classification method
[ li2017learning ] to segmentation and devising a strategy to select
relevant samples of old datasets for rehearsal. Taras et al. proposed a
similar approach for segmenting remote sensing data. Differently,
Michieli et al. [ michieli2019incremental ] consider ICL for semantic
segmentation in a particular setting where labels are provided for old
classes while learning new ones. Moreover, they assume the novel classes
to be never present as background in pixels of previous learning steps.
These assumptions strongly limit the applicability of their method.

Here we propose a more principled formulation of the ICL problem in
semantic segmentation. In contrast with previous works, we do not limit
our analysis to medical [ ozdemir2018learn ] or remote sensing data [
tasar2019incremental ] and we do not impose any restrictions on how the
label space should change across different learning steps [
michieli2019incremental ] . Moreover, we are the first to provide a
comprehensive experimental evaluation of state of the art ICL methods on
commonly used semantic segmentation benchmarks and to explicitly
introduce and tackle the semantic shift of the background class, a
problem recognized but largely overseen by previous works [
michieli2019incremental ] .

Multi-domain Learning. Another challenge in incremental learning is
extending a pre-trained model to address new tasks, each with different
output space. Indeed, the need for visual models capable of addressing
multiple domains received a lot of attention in recent years for what
concerns both multi-task learning [ zamir2018taskonomy , liu2019end ,
cermelli2019rgb ] and multi-domain learning [ rebuffi2017learning ,
rosenfeld2017incremental ] . Multi-task learning focuses on learning
multiple visual tasks (e.g. semantic segmentation, depth estimation [
liu2019end ] ) with a single architecture. On the other hand, the goal
of multi-domain learning is building a model able to address a task
(e.g. classification) in multiple visual domains (e.g. real photos,
digits) without forgetting previous domains and by using fewer
parameters possible. An important work in this context is [
bilen2017universal ] , where the authors showed how multi-domain
learning can be addressed by using a network sharing all parameters
except for batch-normalization (BN) layers [ ioffe2015batch ] . In [
rebuffi2017learning ] , the authors introduced the Visual Domain
Decathlon Challenge, a first multi-domain learning benchmark. The first
attempts in addressing this challenge involved domain-specific residual
components added in standard residual blocks, either in series [
rebuffi2017learning ] or in parallel [ rebuffi2018efficient ] , In [
rosenfeld2017incremental ] the authors propose to use controller modules
where the parameters of the base architecture are recombined
channel-wise, while in [ liu2019end ] exploits domain-specific attention
modules. Other effective approaches include devising instance-specific
fine-tuning strategies [ guo2019spottune ] , target-specific
architectures [ morgado2019nettailor ] and learning covariance
normalization layers [ li2019efficient ] .

In [ mallya2018packnet ] only a reserved subset of network parameters is
considered for each domain. The intersection of the parameters used by
different domains is empty, thus the network can be trained end-to-end
for each domain. Obviously, as the number of domain increases, fewer
parameters are available for each domain, with a consequent limitation
on the performances of the network. To overcome this issue, in [
mallya2018piggyback ] the authors proposed a more compact and effective
solution based on directly learning domain-specific binary masks. The
binary masks determine which of the network parameters are useful for
the new domain and which are not, changing the actual composition of the
features extracted by the network. This approach inspired subsequent
works, improving both either the power of the binary masks [
mancini2018adding ] or the number of bits required, masking directly an
entire channel [ berriel2019budget ] .

In our work [ mancini2020boostingmva ] , we take inspiration from these
last research trends. In particular, we generalize the design of the
binary masks employed in [ mallya2018piggyback ] and [ mancini2018adding
] considering neither simple multiplicative binary masks nor simple
affine transformations of the original weights [ mancini2018adding ] but
a general and flexible formulation capturing both cases. Experiments
show how our approach in [ mancini2020boostingmva ] leads to a boost in
the performances while using a comparable number of parameters per
domain. Moreover, our approach achieves performances comparable to more
complex models [ rebuffi2018efficient , morgado2019nettailor ,
li2019efficient , li2019efficient ] in the challenging Visual Domain
Decathlon challenge, largely reducing the gap of binary-mask based
methods with the current state of the art. Note that while learning to
address the same task (i.e. classification) in multiple visual domains,
these trend of works addresses catastrophic forgetting by adding
domain-specific parameters, extending the semantic extent of a
pre-trained model by exploiting isolated set of parameters. In fact, if
the initial network parameters remain untouched, the catastrophic
forgetting problem is avoided but at the cost of the additional
parameters required. The extreme case is the work of [
rusu2016progressive ] in the context of reinforcement learning, where a
parallel network is added each time a new domain is presented with side
domain connections, exploited to improve the performances on novel
domains. Differently to [ rusu2016progressive ] , the mask-based
approaches [ mallya2018packnet , mallya2018piggyback , mancini2018adding
, mancini2020boostingmva ] require a much lower overhead in terms of
total parameters, showing comparable or even superior results to
task-specific fine-tuned models [ mallya2018packnet ,
mallya2018piggyback , mancini2020boostingmva ] .

Open World Recognition. The necessity of breaking the closed-world
assumption (CWA) for robot vision systems [ sunderhauf2018limits ] has
lead various research efforts on understanding how to extend pre-trained
models with new semantic concepts while retain previous knowledge and
detecting possibly unknown ones. There are two components towards this
goal: the first is incrementally adding new categories to the
pre-trained model, while the second is maintaining a right estimation of
the uncertainty on the predictions allowing to reject inputs of unseen
classes. Due to the central role this task has in real-world
applications, recent years have seen a growing interests among robotic
vision researches on topics such as continual [ lesort2020continual ]
and incremental learning [ valipour2017incremental ,
camoriano2017incremental , cermelli2019rgb ] . In [ pasquale2015teaching
] , the authors study how to update the visual recognition system of a
humanoid robot on multiple training sessions. In [
camoriano2017incremental ] , a variant of the Regularized Least Squares
algorithm is introduced to add new classes to a pre-trained model. In [
parisi2018lifelong ] , a growing dual-memory is proposed to dynamically
learn novel object instances and categories. In [ lagunes2019learning ]
the authors proposed to learn an embedding in order to perform fast
incremental learning of new objects. Another solution to this problem
can exploit the help of a human-robot interaction, as in [
valipour2017incremental ] where a robot incrementally learns to detect
new objects as they are manually pointed by a human.

While these approaches focus on incremental and continual learning,
acting in the open world requires both detecting unknown concepts
automatically and adding them in subsequent learning stages. Towards
this objective, in [ bendale2015towards ] the authors introduced the OWR
setting, as a more general and realistic scenario for agents acting in
the real world. In [ bendale2015towards ] , the authors extend the
Nearest Class Mean (NCM) classifier [ mensink2012metric ,
guerriero2018deep ] to act in the open set scenario, proposing the
Nearest Non-Outlier algorithm (NNO). In order to estimate whereas a test
sample belongs to the known or unknown set of categories, this method
introduces a rejection threshold that, after the first initialization
phase, is kept fixed for subsequent learning episodes. In [ de2016online
] , the authors proposed to tackle OWR with the Nearest Ball Classifier,
with a rejection threshold based on the confidence of the predictions.
In [ mancini2019knowledge ] , we extended the NNO algorithm of [
bendale2015towards ] by employing an end-to-end trainable deep
architecture as feature extractor, with a dynamic update strategy for
the rejection threshold. Moreover, our work was the first to consider
the collection of datasets containing new knowledge using web resources,
towards agent able to automatically include new knowledge with limited
to none human supervision. In the subsequent work [ fontanel2020boosting
] , we showed how we can improve the performances of NCM based
classifier for OWR through a global to local clustering loss. Moreover,
differently for previous works, we designed class-specific rejection
threshold rather are explicitly learned rather than fixed based on
heuristic strategies.

### 3.3 Sequential and Memory Efficient Learning of New Datasets 111M.
Mancini, E.Ricci, B. Caputo, S. Rota Bulò. Adding New Tasks to a Single
Network with Weight Transformations using Binary Masks. European
Computer Vision Conference Workshop on Transferring and Adapting Source
Knowledge in Computer Vision 2018.222M. Mancini, E.Ricci, B. Caputo, S.
Rota Bulò. Boosting Binary Masks for Multi-Domain Learning through
Affine Transformations. Machine Vision and Applications 2020.

In this section, we focus on the problem of multi-domain learning [
rebuffi2017learning , bilen2017universal ] . Following the problem
statement of [ rebuffi2017learning ] , the goal of multi-domain learning
is to train a model to address multiple classification tasks using as
few parameters per each of them. In the following, we focus on the case,
considered also in [ rebuffi2017learning ] where we adapt an initial
pre-trained model to address novel tasks sequentially. This capability
is crucial for increasing the knowledge of an intelligent system and
developing effective incremental [ ring1997child , kuzborskij2013n ] ,
life-long [ thrun1995lifelong , thrun2012learning , silver2013lifelong ]
learning algorithms. While fascinating, achieving this goal requires
facing multiple challenges. First, learning a new task should not
negatively affect the performance on old tasks. Second, it should be
avoided adding multiple parameters to the model for each new task
learned, as it would lead to poor scalability of the framework. In this
context, while deep learning algorithms have achieved impressive results
on many computer vision benchmarks [ krizhevsky2012imagenet , he2016deep
, girshick2014rich , long2015fully ] , mainstream approaches for
adapting deep models to novel tasks tend to suffer from the problems
mentioned above. In fact, fine-tuning a given architecture to new data
does produce a powerful model on the novel task, at the expense of a
degraded performance on the old ones, resulting in the well-known
phenomenon of catastrophic forgetting [ french1999catastrophic ,
goodfellow2013empirical ] . At the same time, replicating the network
parameters and training a separate network for each task is a powerful
approach that preserves performances on old tasks, but at the cost of an
explosion of the network parameters [ rebuffi2017learning ] .

Different works addressed these problems by either considering losses
encouraging the preservation of the current weights [ li2017learning ,
kirkpatrick2017overcoming ] or by designing task-specific network
parameters [ rusu2016progressive , rebuffi2017learning ,
rosenfeld2017incremental , mallya2018packnet , mallya2018piggyback ] .
Interestingly, in [ mallya2018piggyback ] the authors showed that an
effective strategy for achieving good sequential multi-task learning
performances with a minimal increase in term of network size is to
create a binary mask for each task. In particular, this mask is then
multiplied by the main network weights, determining which of them are
useful for addressing the new task.

In this section, we take inspiration from these last work. and we
formulate the sequential multi-task learning as the problem of learning
a perturbation of a baseline , pre-trained network, in a way to maximize
the performance on a new task. Importantly, the perturbation should be
compact in the sense of limiting the number of additional parameters
required with respect to the baseline network. To this extent, we apply
an affine transformation to each convolutional weight of the baseline
network, which involves both a learned binary mask and few additional
parameters. The binary mask is used as a scaled and shifted additive
component and as a multiplicative filter to the original weights. Figure
3.1 shows an example application of our proposed algorithm. Given a
network pre-trained on a particular task (i.e. ImageNet [
russakovsky2015imagenet ] , orange blocks) we can transform its original
weights through binary masks (colored grids) and obtain a network which
effectively addresses a novel tasks (e.g. digit [ netzer2011reading ] or
traffic sign [ stallkamp2012man ] recognition) We name our solution BAT
(Binary-mask Affinely Transformed for multi-domain learning). This
solution allows us to achieve two main goals: 1) boosting the
performance of each task-specific network that we train, by leveraging
the higher degree of freedom in perturbing the baseline network, while
2) keeping a low per-task overhead in terms of additional parameters
(slightly more than 1 bit per parameter per task).

We assess the validity of BAT, and some variants thereof, on standard
benchmarks including the Visual Decathlon Challenge [
rebuffi2017learning ] . The experimental results show that our model
achieves performances comparable with fine-tuning separate networks for
each recognition task on all benchmarks, while retaining a very small
overhead in terms of additional parameters per task. Notably, we achieve
results comparable to state-of-the-art models on the Visual Decathlon
Challenge [ rebuffi2017learning ] but without requiring multiple
training stages [ li2019efficient ] or a large number of task-specific
parameters [ guo2019spottune , rebuffi2018efficient ] .

#### 3.3.1 Problem Formulation

We address the problem of sequential learning of new tasks, i.e. we
modify a baseline network such as, e.g. ResNet-50 pre-trained on the
ImageNet classification task, so to maximize its performance on a new
task, while limiting the amount of additional parameters needed. The
solution we propose exploits the key idea from Piggyback [
mallya2018piggyback ] of learning task-specific masks, but instead of
pursuing the simple multiplicative transformation of the parameters of
the baseline network, we define a parametrized, affine transformation
mixing a binary mask and real parameters that significantly increases
the expressiveness of the approach, leading to a rich and nuanced
ability to adapt the old parameters to the needs of the new tasks. This
in turn brings considerable improvements on all the conducted
experiments, as we will show in the experimental section, while
retaining a reduced, per-task overhead.

Let us assume to be given a pre-trained, baseline network @xmath
assigning a class label in @xmath to elements of an input space @xmath
(e.g. images). ³ ³ 3 We focus on classification tasks, but the proposed
method applies also to other tasks. The parameters of the baseline
network are partitioned into two sets: @xmath comprises parameters that
will be shared for other domains, whereas @xmath entails the rest of the
parameters (e.g. the classifier). Our goal is to learn for each domain
@xmath , with a possibly different output space @xmath , a classifier
@xmath . Here, @xmath entails the parameters specific for the @xmath th
domain, while @xmath holds the shareable parameters of the baseline
network mentioned above.

Each domain-specific network @xmath shares the same structure of the
baseline network @xmath , except for having a possibly differently sized
classification layer. For each convolutional layer ⁴ ⁴ 4 Fully-connected
layers are a special case. of @xmath with parameters @xmath , the
domain-specific network @xmath holds a binary mask @xmath , with the
same shape of @xmath , that is used to mask original filters. The way
the mask is exploited to specialize the network filters produces
different variants of our model, which we describe in the following.

#### 3.3.2 Affine Weight Transformation through Binary Masks

Following previous works [ mallya2018piggyback ] , we consider
domain-specific networks @xmath that are shaped as the baseline network
@xmath and we store in @xmath a binary mask @xmath for each
convolutional kernel @xmath in the shared set @xmath . However,
differently from [ mallya2018piggyback ] , we consider a more general
affine transformation of the base convolutional kernel @xmath that
depends on a binary mask @xmath as well as additional parameters.
Specifically, we transform @xmath into

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

where @xmath are additional domain-specific parameters in @xmath that we
learn along with the binary mask @xmath , @xmath is an opportunely sized
tensor of @xmath s, and @xmath is the Hadamard (or element-wise)
product. The transformed parameters @xmath are then used in the
convolutional layer of @xmath . We highlight that the domain-specific
parameters that are stored in @xmath amount to just a single bit per
parameter in each convolutional layer plus a few scalars per layer,
yielding a low overhead per additional domain while retaining a
sufficient degree of freedom to build new convolutional weights. Figure
3.2 provides an overview of the transformation in ( 3.1 ).

Our model, can be regarded as a parametrized generalization of [
mallya2018piggyback ] , since we can recover the formulation of [
mallya2018piggyback ] by setting @xmath and @xmath . Similarly, if we
get rid of the multiplicative component, i.e. we set @xmath , we
obtained the following transformation

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

which corresponds to a simpler but still effective version of our method
(presented in [ mancini2018adding ] ) and will be taken into account in
our analysis.

We want to highlight that each model (i.e. [ mallya2018piggyback ] ,
BAT, and its simplified version) has different representation
capabilities. In fact, in [ mallya2018piggyback ] , the domain-specific
parameters can take only two possible values: either @xmath (i.e. if
@xmath ) or the original pre-trained weights (i.e. if @xmath ). On the
other hand, the scalar components of our simple model [
mancini2018adding ] allow both scaling (i.e. with @xmath ) and shifting
(i.e. with @xmath ) the original network weights, with the additive
binary mask adding a bias term (i.e. @xmath ) selectively to a group of
parameters (i.e. the one with @xmath ). BAT generalizes [
mallya2018piggyback ] and [ mancini2018adding ] by considering the
multiplicative binary-mask term @xmath as an additional bias component
scaled by the scalar @xmath . In this way, our model has the possibility
to obtain parameter-specific bias components, something that was not
possible neither in [ mallya2018piggyback ] nor in [ mancini2018adding ]
. The additional degrees of freedom makes the search space of our method
larger with respect to [ mallya2018piggyback , mancini2018adding ] ,
with the possibility to express more complex (and tailored)
domain-specific transformations. Thus, as we show in the experimental
section, the additional parameters that we introduce with our method
bring a negligible per-domain overhead compared to [ mallya2018piggyback
] and [ mancini2018adding ] , which is nevertheless generously balanced
out by a significant boost of the performance of the domain-specific
classifiers.

Finally, following [ bilen2017universal ] , we opt also for
domain-specific batch-normalization parameters (i.e. mean, variance,
scale and bias), unless otherwise stated. Those parameters will not be
fixed (i.e. they do not belong to @xmath ) but are part of @xmath , and
thus optimized for each domain. In the cases where we have a
convolutional layer followed by batch normalization, we keep the
corresponding parameter @xmath fixed to @xmath , because the output of
batch normalization is invariant to the scale of the convolutional
weights.

#### 3.3.3 Learning Binary Masks

Given the training set of the @xmath domain, we learn the
domain-specific parameters @xmath by minimizing a standard supervised
loss, i.e. the classification log-loss. However, while the
domain-specific batch-normalization parameters can be learned by
employing standard stochastic optimization methods, the same is not
feasible for the binary masks. Indeed, optimizing the binary masks
directly would turn the learning into a combinatorial problem. To
address this issue, we follow the solution adopted in [
mallya2018piggyback ] , i.e. we replace each binary mask @xmath with a
thresholded real matrix @xmath . By doing so, we shift from optimizing
discrete variables in @xmath to continuous ones in @xmath . However, the
gradient of the hard threshold function @xmath is zero almost
everywhere, making this solution apparently incompatible with
gradient-based optimization approaches. To address this issue we
consider a strictly increasing, surrogate function @xmath that will be
used in place of @xmath only for the gradient computation, i.e.

  -- -------- --
     @xmath
  -- -------- --

where @xmath denotes the derivative of @xmath with respect to its
argument. The gradient that we obtain via the surrogate function has the
property that it always points in the right down hill direction in the
error surface. Let @xmath be a single entry of @xmath , with @xmath and
let @xmath be the error function. Then

  -- -------- --
     @xmath
  -- -------- --

and, since @xmath by construction of @xmath , we obtain the sign
agreement

  -- -------- --
     @xmath
  -- -------- --

Accordingly, when the gradient of @xmath with respect to @xmath is
positive (negative), this induces a decrease (increase) of @xmath . By
the monotonicity of @xmath this eventually induces a decrease (increase)
of @xmath , which is compatible with the direction pointed by the
gradient of @xmath with respect to @xmath .

In the experiments, we set @xmath , i.e. the identity function,
recovering the workaround suggested in [ hin12 ] and employed also in [
mallya2018piggyback ] . However, other choices are possible. For
instance, by taking @xmath , i.e. the sigmoid function, we obtain a
better approximation that has been suggested in [ goodman1994learning ,
bengio2013estimating ] . We test different choices for @xmath in the
experimental section.

#### 3.3.4 Experimental results

Datasets. In the following we test our method on two different
multi-task benchmarks, where the multiple tasks regard different
classification objectives and/or domains. For the first benchmark we
follow [ mallya2018piggyback ] , and we use 6 datasets: ImageNet [
russakovsky2015imagenet ] , VGG-Flowers [ nilsback2008automated ] ,
Stanford Cars [ krause20133d ] , Caltech-UCSD Birds (CUBS) [
wah2011caltech ] , Sketches [ eitz2012humans ] and WikiArt [
saleh2015large ] . VGG-Flowers [ nilsback2008automated ] is a dataset of
fine-grained recognition containing images of 102 categories,
corresponding to different kind of flowers. There are 2’040 images for
training and 6’149 for testing. Stanford Cars [ krause20133d ] contains
images of 196 different types of cars with approximately 8 thousand
images for training and 8 thousands for testing. Caltech-UCSD Birds [
wah2011caltech ] is another dataset of fine-grained recognition
containing images of 200 different species of birds, with approximately
6 thousands images for training and 6 thousands for testing. Sketches [
eitz2012humans ] is a dataset composed of 20 thousands sketch drawings,
16 thousands for training and 4 thousands for testing. It contains
images of 250 different objects in their sketched representations.
WikiArt [ saleh2015large ] contains painting from 195 different artists.
The dataset has 42’129 images for training and 10628 images for testing.
These datasets contain a lot of variations both from the category
addressed (i.e. cars [ krause20133d ] vs birds [ wah2011caltech ] ) and
the appearance of their instances (from natural images [
russakovsky2015imagenet ] to paintings [ saleh2015large ] and sketches [
eitz2012humans ] ), thus representing a challenging benchmark for
sequential multi-task learning techniques.

The second benchmark is the Visual Decathlon Challenge [
rebuffi2017learning ] . This challenge has been introduced in order to
check the capability of a single algorithm to tackle 10 different
classification tasks. The tasks are taken from the following datasets:
ImageNet [ russakovsky2015imagenet ] , CIFAR-100 [
krizhevsky2009learning ] , Aircraft [ maji2013fine ] , Daimler
pedestrian classification (DP) [ munder2006experimental ] , Describable
textures (DTD) [ cimpoi2014describing ] , German traffic signs (GTS) [
stallkamp2012man ] , Omniglot [ lake2015human ] , SVHN [
netzer2011reading ] , UCF101 Dynamic Images [ bilen2016dynamic ,
soomro2012ucf101 ] and VGG-Flowers [ nilsback2008automated ] . A more
detailed description of the challenge and the datasets can be found in [
rebuffi2017learning ] . For this challenge, an independent scoring
function is defined [ rebuffi2017learning ] . This function @xmath is
expressed as:

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

where @xmath is the test error of the baseline in the domain @xmath ,
@xmath is the test error of the submitted model and @xmath is a scaling
parameter ensuring that the perfect score for each task is 1000, thus
with a maximum score of 10000 for the whole challenge. The baseline
error is computed doubling the error of 10 independent models fine-tuned
on the single tasks. This score function takes into account the
performances of a model on all 10 classes, preferring models with good
performances on all of them compared to models outperforming by a large
margin the baseline in just few of them. Following [ berriel2019budget ]
, we use this metric also for the first benchmark, keeping the same
upper-bound of 1000 points for each task. Moreover, as in [
berriel2019budget ] , we report the ratio among the score obtained and
the parameters used, denoting it as @xmath . This metric allows to
capture the trade-off among the performances and model size.

Networks and training protocols. For the first benchmark, we use 3
networks: ResNet-50 [ he2016deep ] , DenseNet-121 [ huang2017densely ]
and VGG-16 [ simonyan2014very ] , reporting the results of Piggyback [
mallya2018piggyback ] , PackNet [ mallya2018packnet ] and both the
simple [ mancini2018adding ] and full version of our model (BAT).

Following the protocol of [ mallya2018piggyback ] , for all the models
we start from the networks pre-trained on ImageNet and train the
task-specific networks using Adam [ kingma2014adam ] as optimizer except
for the classifiers where SGD [ bottou2010large ] with momentum is used.
The networks are trained with a batch size of 32 and an initial learning
rate of 0.0001 for Adam and 0.001 for SGD with momentum 0.9. Both the
learning rates are decayed by a factor of 10 after 15 epochs. In this
scenario we use input images of size @xmath pixels, with the same data
augmentation (i.e. mirroring and random rescaling) of [
mallya2018packnet , mallya2018piggyback ] . The real-valued masks are
initialized with random values drawn from a uniform distribution with
values between @xmath and @xmath . Since our model is independent on the
order of the tasks, we do not take into account different possible
orders, reporting the results as accuracy averaged across multiple runs.
For simplicity, in the following we will denote this scenario as
ImageNet-to-Sketch .

For the Visual Decathlon we employ the Wide ResNet-28 [
zagoruyko2016wide ] adopted by previous methods [ rebuffi2017learning ,
rosenfeld2017incremental , mallya2018piggyback ] , with a widening
factor of 4 (i.e. 64, 128 and 256 channels in each residual block).
Following [ rebuffi2017learning ] we rescale the input images to @xmath
pixels giving as input to the network images cropped to @xmath . We
follow the protocol in [ mallya2018piggyback ] , by training the simple
and full versions of our model for 60 epochs for each task, with a batch
size of 32, and using again Adam for the entire architecture but the
classifier, where SGD with momentum is used. The same learning rates of
the first benchmark are adopted and are decayed by a factor of 10 after
45 epochs. The same initialization scheme is used for the real-valued
masks. No hyperparameter tuning has been performed as we used a single
training schedule for all the 10 tasks, except for the ImageNet
pre-trained model, which was trained following the schedule of [
rebuffi2017learning ] . As for data augmentation, mirroring has been
performed, except for the datasets with digits (i.e. SVHN), signs
(Omniglot, GTS) and textures (i.e. DTD) as it may be rather harmful (as
in the first 2 cases) or unnecessary.

In both benchmarks, we train our network on one task at the time,
sequentially for all tasks. For each task we introduce the task specific
binary masks and additional scalar parameters, as described in section
3.3.2 . Moreover, following previous approaches [ rebuffi2017learning ,
rebuffi2018efficient , mallya2018piggyback , rosenfeld2017incremental ]
, we consider a separate classification layers for each task. This is
reflected also in the computation of the parameters overhead required by
our model, we do not consider the separate classification layers,
following comparison systems [ rebuffi2017learning ,
rebuffi2018efficient , mallya2018piggyback , rosenfeld2017incremental ]
.

##### Results

ImageNet-to-Sketch. In the following we discuss the results obtained by
our model on the ImageNet-to-Sketch scenario. We compare our method with
Piggyback [ mallya2018piggyback ] , PackNet [ mallya2018packnet ] and
two baselines considering the network only as feature extractor,
training only the task-specific classifier, and individual networks
separately fine-tuned on each task. PackNet [ mallya2018packnet ] adds a
new task to an architecture by identifying important weights for the
task, optimizing the architecture through alternated pruning and
re-training steps. Since this algorithm is dependent on the order of the
task, we report the performances for two different orderings [
mallya2018piggyback ] : starting from the model pre-trained on ImageNet,
in the first setting ( @xmath ) the order is
CUBS-Cars-Flowers-WikiArt-Sketch while for the second ( @xmath ) the
order is reversed. For our model, we evaluate both the full and the
simple version, including task-specific batch-normalization layers.
Since including batch-normalizatin layers affects the performances, for
the sake of presenting a fair comparison, we report also the results of
Piggyback [ mallya2018piggyback ] obtained as a special case of our
model with separate BN parameters per task for ResNet-50 and
DenseNet-121. Moreover, we report the results of the Budget-Aware
adapters ( @xmath ) method in [ berriel2019budget ] . This method relies
on binary masks applied not per-parameter but per-channel, with a budget
constraint allowing to further squeeze the network complexity. As in our
method, also in [ berriel2019budget ] task-specific BN layers are used.

Results are reported in Tables 3.1 , 3.2 and 3.3 . We see that both
versions of our model are able to fill the gap between the classifier
only baseline and the individual fine-tuned architectures, almost
entirely in all settings. For larger and more diverse datasets such as
Sketch and WikiArt, the gap is not completely covered, but the distance
between our models and the individual architectures is always less than
1%. These results are remarkable given the simplicity of our method, not
involving any assumption of the optimal weights per task [
mallya2018packnet , li2017learning ] , and the small overhead in terms
of parameters that we report in the row "# Params" (i.e. @xmath for
ResNet-50, @xmath for DenseNet-121 and @xmath for VGG-16), which
represents the total number of parameters (counting all tasks and
excluding the classifiers) relative to the ones in the baseline network
⁵ ⁵ 5 If the base architecture contains @xmath parameters and the
additional bits introduced per task are @xmath then @xmath , where
@xmath denotes the number of tasks (included the one used for
pre-training the network) and the 32 factor comes from the bits required
for each real number. The classifiers are not included in the
computation. .

For what concerns the comparison with the other algorithms, our model
consistently outperforms both the basic version of Piggyback and PackNet
in all the settings and architectures, with the exception of Sketch for
the DenseNet and VGG-16 architectures and CUBS for VGG-16, in which the
performances are comparable with those of Piggyback. When task-specific
BN parameters are introduced also for Piggyback (Tables 3.1 and 3.2 ),
the gap in performances is reduced, with comparable performances in some
settings (i.e. CUBS) but with still large gaps in others (i.e. Flowers,
Stanford Cars and WikiArt). These results show that the advantages of
our model are not only due to the additional BN parameters, but also to
the more flexible and powerful affine transformation introduced.

This statement is further confirmed with the VGG-16 experiments of Table
3.3 . For this network, when the standard Piggyback model is already
able to fill the gap between the feature extractor baseline and the
individual architectures, our model achieves either comparable or
slightly superior performances (i.e. CUBS, WikiArt and Sketch). However
in the scenarios where Piggyback does not reach the performances of the
independently fine-tuned models (i.e. Stanford Cars and Flowers), our
model consistently outperform the baseline, either halving (Flowers) or
removing (Stanford Cars) the remained gap. Since this network does not
contain batch-normalization layers, it confirms the generality of our
model, showing the advantages of both our simple and full versions, even
without task-specific BN layers.

For what concerns the comparison with @xmath , the performances of our
model are either comparable or superior in most of the settings.
Remarkable are the gaps in the WikiArt dataset, with our full model
surpassing @xmath by 3% with ResNet-50 and 4% for DenseNet-121. Despite
both Piggyback and @xmath use fewer parameters than our approach, our
full model outperforms both of them in terms of the final score (Score
row) and the ratio among the score and the parameters used (Score/Params
row). This shows that our model is the most powerful in making use of
the binary masks, achieving not only higher performances but also a more
favorable trade-off with the model size.

Finally, both Piggyback, @xmath and our model outperform PackNet and, as
opposed to the latter method, do not suffer from the heavily dependence
on the ordering of the tasks. This advantage stems from having a
sequential multi-task learning strategy that is task independent, with
the base network not affected by the new tasks that are learned.

Visual Decathlon Challenge. In this section we report the results
obtained on the Visual Decathlon Challenge. We compare our model with
the baseline method Piggyback [ mallya2018piggyback ] (PB), the
budget-aware adapters of [ berriel2019budget ] ( @xmath ), the improved
version of the winner entry of the 2017 edition of the challenge [
rosenfeld2017incremental ] (DAN), the network with task-specific
parallel adapters [ rebuffi2018efficient ] (PA), the task-specific
attention modules of [ liu2019end ] (MTAN), the covariance normalization
approach [ li2019efficient ] (CovNorm) and SpotTune [ guo2019spottune ]
. We additionally report the baselines proposed by the authors of the
challenge [ rebuffi2017learning ] . For the latter, we report the
results of 5 models: the network used as feature extractor (Feature), 10
different models fine-tuned on each single task (Fine-tune), the network
with task-specific residual adapter modules [ rebuffi2017learning ]
(RA), the same model with increased weight decay (RA-decay) and the same
architecture jointly trained on all 10 tasks, in a round-robin fashion
(RA-joint). The first two models are considered as references. For the
parallel adapters approach [ rebuffi2018efficient ] we report also the
version with a post training low-rank decomposition of the adapters
(PA-SVD). This approach extracts a task specific and a task agnostic
component from the learned adapters with the task specific components
which are further fine-tuned on each task. Additionally we report the
novel results of the residual adapters [ rebuffi2017learning ] as
reported in [ rebuffi2018efficient ] (RA-N).

Similarly to [ rosenfeld2017incremental ] we tune the training schedule,
jointly for the 10 tasks, using the validation set, and evaluate the
results obtained on the test set (via the challenge evaluation server)
by a model trained on the union of the training and validation sets,
using the validated schedule. As opposed to methods like [
rebuffi2017learning ] we use the same schedule for the 9 tasks (except
for the baseline pre-trained on ImageNet), without adopting
task-specific strategies for setting the hyperparameters. Moreover, we
do not employ our algorithm while pre-training the ImageNet architecture
as in [ rebuffi2017learning ] . For fairness, we additionally report the
results obtained by our implementation of [ mallya2018piggyback ] using
the same pre-trained model, training schedule and data augmentation
adopted for our algorithm (PB ours).

The results are reported in Table 3.4 in terms of the @xmath -score
(see, Eq. ( 3.3 )) and @xmath . In the first part of the table are shown
the baselines (i.e. fine-tuned architectures and using the network as
feature extractor) while in the middle the sequential learning models
against which we compare. In the last part of the table we report, for
fairness, the methods that do not consider a sequential learning setting
since they either train on all the datasets jointly (RA-joint) or have a
multi-process step considering the all tasks (PA-SVD).

From the table we can see that the full form of our model (F) achieves
very high results, being the third best performing method in terms of
@xmath -score, behind only CovNorm and SpotTune and being comparable to
PA. However, SpotTune uses a large amount of parameters (11x) and PA
doubles the parameters of the original model. CovNorm uses a very low
number of parameters, but requires a two-stage pipeline. On the other
hand, our model does not require neither a large number of parameters
(such as SpotTune and PA) nor a two-stage pipeline (as CovNorm) while
achieving results close to the state of the art (215 points below
CovNorm in terms of @xmath -score). Compared to binary mask based
approaches, our model surpasses PiggyBack of more than 600 points,
@xmath of 300 and BAT simple of more than 200. It is worth highlighting
that these results have been achieved without task-specific
hyperparameter tuning , differently from previous works e.g. [
rebuffi2017learning , rebuffi2018efficient , li2019efficient ] .

Analyzing the @xmath score, BAT is the third best performing model,
behind @xmath and CovNorm. We highlight however that CovNorm requires a
two-stage pipeline to reduce the amount of parameters needed, while
@xmath is explicitly designed with the purpose of limiting the budget
(i.e. parameters, flops) required by the model.

Ablation Study

In the following we will analyze the impact of the various components of
our model. In particular we consider the impact of the parameters @xmath
, @xmath , @xmath , @xmath and the surrogate function @xmath on the
final results of our model for the ResNet-50 and DenseNet-121
architectures in the ImageNet-to-Sketch scenario. Since the
architectures contains batch-normalization layers, we set @xmath for our
simple and full versions and @xmath when we analyze the special case [
mallya2018piggyback ] . For the other parameters we adopt various
choices: either we fix them to a constant in order not take into account
their impact, or we train them, to assess their particular contribution
to the model. The surrogate function we use is the identity function
@xmath , unless otherwise stated (i.e. with Sigmoid ). The results of
our analysis are shown in Tables 3.5 and 3.6 .

As the Tables shows, while the BN parameters allow a boost in the
performances of Piggyback, adding @xmath to the model does not provide
further gain in performances. This does not happen for the simple
version of our model: without @xmath our model is not able to fully
exploit the presence of the binary masks, achieving comparable or even
lower performances with respect to the Piggyback model. We also notice
that a similar drop affecting our Simple version when bias was omitted.

Noticeable, the full versions with @xmath suffer a large decrease in
performances in almost all settings (e.g. ResNet-50 Flowers from 96.7%
to 91.0%), showing that the component that brings the largest benefits
to our algorithm is the addition of the binary mask itself scaled by
@xmath (i.e. @xmath ). This explains also the reason why the simple
version achieves a performance similar to the full version of our model.
We finally note that there is a limited contribution brought by the
standard Piggyback component (i.e. @xmath ), compared to the new
components that we have introduced in the transformation: in fact, there
is a clear drop in performance in various scenarios (e.g. CUBS, Cars)
when we set either @xmath or @xmath , thus highlighting the importance
of those components. Consequently, as @xmath is introduced in our Simple
model, the boost of performances is significant such that neither the
inclusion of @xmath , nor considering channel-wise parameters @xmath
provide further gains. Slightly better results are achieved in a larger
datasets, such as WikiArt, with the additional parameters giving more
capacity to the model, thus better handling the larger amount of
information available in the dataset.

As to what concerns the choice of the surrogate @xmath , no particular
advantage has been noted when @xmath with respect to the standard
straight-through estimator ( @xmath ). This may be caused by the noisy
nature of the straight-through estimator, which has the positive effect
of regularizing the parameters, as shown in previous works [
bengio2013estimating , neelakantan2015adding ] .

We also note that for DenseNet-121, as opposed to ResNet-50, setting
@xmath to zero degrades the performance only in 1 out of 5 datasets
(i.e. CUBS) while the other 4 are not affected, showing that the
effectiveness of different components of the model is also dependent on
the architecture used.

Parameter Analysis We analyze the values of the parameters @xmath ,
@xmath and @xmath of one instance of our full model in the
ImageNet-to-Sketch benchmark. We use both the architectures employed in
that scenario (i.e. ResNet-50 and DenseNet-121) and we plot the values
of @xmath , @xmath and @xmath as well as the percentage of 1s present
inside the binary masks for different layers of the architectures.
Together with those values we report the percentage of 1s for the masks
obtained through our implementation of Piggyback. Both the models have
been trained considering task-specific batch-normalization parameters.
The results are shown in Figures 3.3 and 3.4 . In all scenarios our
model keeps almost half of the masks active across the whole
architecture. Compared to the masks obtained by Piggyback, there are 2
differences: 1) Piggyback exhibits denser masks (i.e. with a larger
portion of 1s), 2) the density of the masks in Piggyback tends to
decreases as the depth of the layer increases. Both these aspects may be
linked to the nature of our model: by having more flexibility through
the affine transformation adopted, there is less need to keep active
large part of the network, since a loss of information can be recovered
through the other components of the model, as well as constraining a
particular part of the architecture. For what concerns the value of the
parameters @xmath , @xmath and @xmath for both architectures @xmath and
@xmath tend to have larger magnitudes with respect to @xmath . Also, the
values of @xmath and @xmath tend to have a different sign, which allows
the term @xmath to span over positive and negative values. We also
notice that the transformation of the weights are more prominent as the
depth increases, which is intuitively explained by the fact that
baseline network requires stronger adaptation to represent the
higher-level concepts pertaining to different tasks. This is even more
evident for WikiArt and Sketch due to the variability that these
datasets contain with respect to standard natural images.

#### 3.3.5 Conclusions

This section presented a simple yet powerful method for sequentially
learning new tasks, given a fixed, pre-trained deep architecture. In
particular, we generalize previous works on multi-domain learning
applying binary masks to the original weights of the network [
mallya2018piggyback ] by introducing an affine transformation that acts
upon such weights and the masks themselves. Our generalization allows
implementing a large variety of possible transformations, better
adapting to the specific characteristics of each task. These advantages
are shown experimentally on two public benchmarks fully confirm the
power of our approach which fills the gap between the binary-mask based
and state-of-the-art methods on the Visual Decathlon Challenge.

Interesting future directions are extending this approach to several
life-long learning scenarios (from incremental class learning to
open-world recognition) and exploiting the relationship between
different task through cross-task affine transformations, in order to
reuse knowledge obtained from different tasks by the model.

While in this section we considered multi-domain learning, an inherently
multi-head problem, single-head incremental learning scenarios are
considered more challenging in the community, due to the more severe
presence of the catastrophic forgetting problem [ chaudhry2018riemannian
] . In the next section, we will study the problem of incremental class
learning in semantic segmentation, a single-head problem mostly
unexplored in the community.

### 3.4 Incremental Learning in Semantic Segmentation 666F. Cermelli, M.
Mancini, E. Ricci, B. Caputo. Modeling the Background for Incremental
Learning in Semantic Segmentation. IEEE/CVF International Conference on
Computer Vision and Pattern Recognition (CVPR) 2020.

In Section 3.3 , we focused on the problem of multi-domain learning,
where the goal is to equip a model to tackle multiple tasks at the same
time. Both in our BAT approach and previous works [ rebuffi2017learning
, rebuffi2018efficient , rosenfeld2017incremental , li2019efficient ]
this is achieved by learning task-specific parameters which are included
in the original pre-trained model. This kind of scenario falls into the
multi-head incremental learning setting (i.e. one network per task/set
of concepts) and it is considered to be an easier problem than the
single-head counterpart [ chaudhry2018riemannian ] . In the single-head
scenario, we have a unique model classifying all semantic concepts
together and, since all concepts share the same output space, this makes
the catastrophic forgetting problem more severe. In this section, we
will describe a solution to a classical single-head scenario, i.e.
incremental class learning, for an unexplored task: semantic
segmentation.

Semantic segmentation is a fundamental problem in computer vision. In
the last years, thanks to the emergence of deep neural networks and to
the availability of large-scale human-annotated datasets [
pascal-voc-2012 , zhou2017scene ] , the state of the art has improved
significantly [ long2015fully , chen2018encoder , zhao2017pyramid ,
lin2017refinenet , zhang2018exfuse ] . Current approaches are derived by
extending deep architectures from image-level to pixel-level
classification, taking advantage of Fully Convolutional Networks (FCNs)
[ long2015fully ] . Over the years, semantic segmentation models based
on FCNs have been improved in several ways, e.g. by exploiting
multiscale representations [ lin2017refinenet , zhang2018exfuse ] ,
modeling spatial dependencies and contextual cues [ chen2017rethinking ,
chen2017deeplab , chen2018encoder ] or considering attention models [
chen2016attention ] .

Still, existing semantic segmentation methods are not designed to
incrementally update their inner classification model when new
categories are discovered. While deep nets are undoubtedly powerful, it
is well known that their capabilities in an incremental learning setting
are limited [ kemker2018measuring ] . In fact, deep architectures
struggle in updating their parameters for learning new categories whilst
preserving good performance on the old ones ( catastrophic forgetting [
mccloskey1989catastrophic ] ).

As described in Section 3.2 , the problem of incremental learning has
been traditionally addressed in object recognition [ li2017learning ,
kirkpatrick2017overcoming , chaudhry2018riemannian , rebuffi2017icarl ,
hou2019learning ] and detection [ shmelkov2017incremental ] , but less
attention has been devoted to semantic segmentation. Here we fill this
gap, proposing an incremental class learning (ICL) approach for semantic
segmentation. Inspired by previous methods on image classification [
li2017learning , rebuffi2017icarl , castro2018end ] , we cope with
catastrophic forgetting by resorting to knowledge distillation [
hinton2015distilling ] . However, we argue (and experimentally
demonstrate) that a naive application of previous knowledge distillation
strategies would not suffice in this setting. In fact, one peculiar
aspect of semantic segmentation is the presence of a special class, the
background class, indicating pixels not assigned to any of the given
object categories. While the presence of this class marginally
influences the design of traditional, offline semantic segmentation
methods, this is not true in an incremental learning setting. As
illustrated in Fig. 3.5 , it is reasonable to assume that the semantics
associated to the background class changes over time. In other words,
pixels associated to the background during a learning step may be
assigned to a specific object class in subsequent steps or vice-versa,
with the effect of exacerbating the catastrophic forgetting. To overcome
this issue, we revisit the classical distillation-based framework for
incremental learning [ li2017learning ] by introducing two novel loss
terms to properly account for the semantic distribution shift within the
background class, thus introducing the first ICL approach tailored to
semantic segmentation. We name this method as MiB ( M odel i ng the B
ackground for incremental learning in semantic segmentation). We
extensively evaluate MiB on two datasets, Pascal-VOC [ pascal-voc-2012 ]
and ADE20K [ zhou2017scene ] , showing that our approach, coupled with a
novel classifier initialization strategy, largely outperform traditional
ICL methods.

To summarize, the contributions described in this section are as
follows:

-   We study the task of incremental class learning for semantic
    segmentation, analyzing in particular the problem of distribution
    shift arising due to the presence of the background class.

-   We propose a new objective function and introduce a specific
    classifier initialization strategy to explicitly cope with the
    evolving semantics of the background class. We show that our
    approach greatly alleviates the catastrophic forgetting, leading to
    the state of the art.

-   We benchmark MiB over several previous ICL methods on two popular
    semantic segmentation datasets, considering different experimental
    settings. We hope that our results will serve as a reference for
    future works.

#### 3.4.1 Problem Formulation

Before delving into the details of ICL for semantic segmentation, we
first introduce the task of semantic segmentation. Let us denote as
@xmath the input space (i.e. the image space) and, without loss of
generality, let us assume that each image @xmath is composed by a set of
pixels @xmath with constant cardinality @xmath . The output space is
defined as @xmath , with the latter denoting the product set of @xmath
-tuples with elements in a label space @xmath . Given an image @xmath
the goal of semantic segmentation is to assign each pixel @xmath of
image @xmath a label @xmath , representing its semantic class.
Out-of-class pixels can be assigned a special class, i.e. the background
class @xmath . Given a training set @xmath , the mapping is realized by
learning a model @xmath with parameters @xmath from the image space
@xmath to a pixel-wise class probability vector, i.e. @xmath . The
output segmentation mask is obtained as @xmath , where @xmath is the
probability for class @xmath in pixel @xmath .

In the ICL setting, training is realized over multiple phases, called
learning steps , and each step introduces novel categories to be learnt.
In other terms, during the @xmath learning step, the previous label set
@xmath is expanded with a set of new classes @xmath , yielding a new
label set @xmath . Following the notation in Section 3.1 , at learning
step @xmath we are also provided with a training set @xmath that is used
in conjunction to the previous model @xmath to train an updated model
@xmath . As in standard ICL, we assume the sets of labels @xmath that we
obtain at the different learning steps to be disjoint, except for the
special void/background class @xmath .

#### 3.4.2 Modeling the Background for Incremental Learning in Semantic
Segmentation

A naive approach to address the ICL problem consists in retraining the
model @xmath on each set @xmath sequentially. When the predictor @xmath
is realized through a deep architecture, this corresponds to fine-tuning
the network parameters on the training set @xmath initialized with the
parameters @xmath from the previous stage. This approach is simple, but
it leads to catastrophic forgetting. Indeed, when training using @xmath
no samples from the previously seen object classes are provided. This
biases the new predictor @xmath towards the novel set of categories in
@xmath to the detriment of the classes from the previous sets. In the
context of ICL for image-level classification, a standard way to address
this issue is coupling the supervised loss on @xmath with a
regularization term, either taking into account the importance of each
parameter for previous tasks [ kirkpatrick2017overcoming ,
shin2017continual ] , or by distilling the knowledge using the
predictions of the old model @xmath [ li2017learning , rebuffi2017icarl
, castro2018end ] . We take inspiration from the latter solution to
initialize the overall objective function of our problem. In particular,
we minimize a loss function of the form:

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

where @xmath is a standard supervised loss (e.g. cross-entropy loss),
@xmath is the distillation loss and @xmath is a hyper-parameter
balancing the importance of the two terms.

As stated in Sec. 3.4.1 , differently from standard ICL settings
considered for image classification problems, in semantic segmentation
we have that two different label sets @xmath and @xmath share the common
void/background class @xmath . However, the distribution of the
background class changes across different incremental steps. In fact,
background annotations given in @xmath refer to classes not present in
@xmath , that might belong to the set of seen classes @xmath and/or to
still unseen classes i.e. @xmath with @xmath (see Fig. 3.5 ). In the
following, we show how we account for the semantic shift in the
distribution of the background class by revisiting standard choices for
the general objective defined in Eq. ( 3.4 ).

Revisiting Cross-Entropy Loss. In Eq.( 3.4 ), a possible choice for
@xmath is the standard cross-entropy loss computed over all image
pixels:

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

where @xmath is the ground truth label associated to pixel @xmath and
@xmath .

The problem with Eq.( 3.5 ) is that the training set @xmath we use to
update the model only contains information about novel classes in @xmath
. However, the background class in @xmath might include also pixels
associated to the previously seen classes in @xmath . Here we argue
that, without explicitly taking into account this aspect, the
catastrophic forgetting problem would be even more severe. In fact, we
would drive our model to predict the background label @xmath for pixels
of old classes, further degrading the capability of the model to
preserve semantic knowledge of past categories. To avoid this issue, we
propose to modify the cross-entropy loss in Eq.( 3.5 ) as follows:

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

where:

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

Our intuition is that by using Eq.( 3.6 ) we can update the model to
predict the new classes and, at the same time, account for the
uncertainty over the actual content of the background class. In fact, in
Eq.( 3.6 ) the background class ground truth is not directly compared
with its probabilities @xmath obtained from the current model @xmath ,
but with the probability of having either an old class or the background
, as predicted by @xmath (Eq.( 3.7 )). A schematic representation of
this procedure is depicted in Fig. 3.6 (blue block). It is worth noting
that the alternative of ignoring the background pixels within the
cross-entropy loss is a sub-optimal solution. In fact, this would not
allow to adapt the background classifier to its semantic shift and to
exploit the information that new images might contain about old classes.

Revisiting Distillation Loss. In the context of incremental learning,
distillation loss [ hinton2015distilling ] is a common strategy to
transfer knowledge from the old model @xmath into the new one,
preventing catastrophic forgetting. Formally, a standard choice for the
distillation loss @xmath is:

  -- -------- -- -------
     @xmath      (3.8)
  -- -------- -- -------

where @xmath is defined as the probability of class @xmath for pixel
@xmath given by @xmath but re-normalized across all the classes in
@xmath i.e.:

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

The rationale behind @xmath is that @xmath should produce activations
close to the ones produced by @xmath . This regularizes the training
procedure in such a way that the parameters @xmath are still anchored to
the solution found for recognizing pixels of the previous classes, i.e.
@xmath .

The loss defined in Eq.( 3.8 ) has been used either in its base form or
variants in different contexts, from incremental task [ li2017learning ]
and class learning [ rebuffi2017icarl , castro2018end ] in object
classification to complex scenarios such as detection [
shmelkov2017incremental ] and segmentation [ michieli2019incremental ] .
Despite its success, it has a fundamental drawback in semantic
segmentation: it completely ignores the fact that the background class
is shared among different learning steps. While with Eq.( 3.6 ) we
tackled the first problem linked to the semantic shift of the background
(i.e. @xmath contains pixels of @xmath ), we use the distillation loss
to tackle the second: annotations for background in @xmath with @xmath
might include pixels of classes in @xmath .

From the latter considerations, the background probabilities assigned to
a pixel by the old predictor @xmath and by the current model @xmath do
not share the same semantic content. More importantly, @xmath might
predict as background pixels of classes in @xmath that we are currently
trying to learn. Notice that this aspect is peculiar to the segmentation
task and it is not considered in previous incremental learning models.
However, in our setting we must explicitly take it into account to
perform a correct distillation of the old model into the new one. To
this extent we define our novel distillation loss by rewriting @xmath in
Eq.( 3.9 ) as:

  -- -------- -- --------
     @xmath      (3.10)
  -- -------- -- --------

Similarly to Eq.( 3.8 ), we still compare the probability of a pixel
belonging to seen classes assigned by the old model, with its
counterpart computed with the current parameters @xmath . However,
differently from classical distillation, in Eq.( 3.10 ) the
probabilities obtained with the current model are kept unaltered, i.e.
normalized across the whole label space @xmath and not with respect to
the subset @xmath (Eq.( 3.9 )). More importantly, the background class
probability as given by @xmath is not directly compared with its
counterpart in @xmath , but with the probability of having either a new
class or the background , as predicted by @xmath (see Fig. 3.6 , yellow
block).

We highlight that, with respect to Eq.( 3.9 ) and other simple choices
(e.g. excluding @xmath from Eq.( 3.9 )) this solution has two
advantages. First, we can still use the full output space of the old
model to distill knowledge in the current one, without any constraint on
pixels and classes. Second, we can propagate the uncertainty we have on
the semantic content of the background in @xmath without penalizing the
probabilities of new classes we are learning in the current step @xmath
.

Classifiers’ Parameters Initialization. As discussed above, the
background class @xmath is a special class devoted to collect the
probability that a pixel belongs to an unknown object class. In
practice, at each learning step @xmath , the novel categories in @xmath
are unknowns for the old classifier @xmath . As a consequence, unless
the appearance of a class in @xmath is very similar to one in @xmath ,
it is reasonable to assume that @xmath will likely assign pixels of
@xmath to @xmath . Taking into account this initial bias on the
predictions of @xmath on pixels of @xmath , it is detrimental to
randomly initialize the classifiers for the novel classes. In fact a
random initialization would provoke a misalignment among the features
extracted by the model (aligned with the background classifier) and the
random parameters of the classifier itself. Notice that this could lead
to possible training instabilities while learning novel classes since
the network could initially assign high probabilities for pixels in
@xmath to @xmath .

To address this issue, we propose to initialize the classifier’s
parameters for the novel classes in such a way that given an image
@xmath and a pixel @xmath , the probability of the background @xmath is
uniformly spread among the classes in @xmath , i.e. @xmath , where
@xmath is the number of new classes (notice that @xmath ). To this
extent, let us consider a standard fully connected classifier and let us
denote as @xmath the classifier parameters for a class @xmath at
learning step @xmath , with @xmath and @xmath denoting its weights and
bias respectively. We can initialize @xmath as follows:

  -- -------- -------- -- --------
     @xmath   @xmath      (3.11)
     @xmath   @xmath      (3.12)
  -- -------- -------- -- --------

where @xmath are the weights and bias of the background classifier at
the previous learning step. The fact that the initialization defined in
Eq.( 3.11 ) and ( 3.12 ) leads to @xmath is easy to obtain from @xmath .

As we will show in the experimental analysis, this simple initialization
procedure brings benefits in terms of both improving the learning
stability of the model and the final results, since it eases the role of
the supervision imposed by Eq.( 3.6 ) while learning new classes and
follows the same principles used to derive our distillation loss (Eq.(
3.10 )).

#### 3.4.3 Experimental results

##### ICL Baselines

We compare MiB against standard ICL baselines, originally designed for
classification tasks, on the considered segmentation task, thus
segmentation is treated as a pixel-level classification problem.
Specifically, we report the results of six different
regularization-based methods, three prior-focused and three
data-focused.

In the first category, we chose Elastic Weight Consolidation (EWC) [
kirkpatrick2017overcoming ] , Path Integral (PI) [ zenke2017continual ]
, and Riemannian Walks (RW) [ chaudhry2018riemannian ] . They employ
different strategies to compute the importance of each parameter for old
classes: EWC uses the empirical Fisher matrix, PI uses the learning
trajectory, while RW combines EWC and PI in a unique model. We choose
EWC since it is a standard baseline employed also in [
shmelkov2017incremental ] and PI and RW since they are two simple
applications of the same principle. Since these methods act at the
parameter level, to adapt them to the segmentation task we keep the loss
in the output space unaltered (i.e. standard cross-entropy across the
whole segmentation mask), computing the parameters’ importance by
considering their effect on learning old classes.

For the data-focused methods, we chose Learning without forgetting (LwF)
[ li2017learning ] , LwF multi-class (LwF-MC) [ rebuffi2017icarl ] and
the segmentation method of [ michieli2019incremental ] (ILT). We denote
as LwF the original distillation based objective as implemented in Eq.(
3.4 ) with basic cross-entropy and distillation losses, which is the
same as [ li2017learning ] except that distillation and cross-entropy
share the same label space and classifier. LwF-MC is the single-head
version of [ li2017learning ] as adapted from [ rebuffi2017icarl ] . It
is based on multiple binary classifiers, with the target labels defined
using the ground truth for novel classes (i.e. @xmath ) and the
probabilities given by the old model for the old ones (i.e. @xmath ).
Since the background class is both in @xmath and @xmath we implement
LwF-MC by a weighted combination of two binary cross-entropy losses, on
both the ground truth and the probabilities given by @xmath . Finally,
ILT [ michieli2019incremental ] is the only method specifically proposed
for ICL in semantic segmentation. It uses a distillation loss in the
output space, as in our adapted version of LwF [ li2017learning ] and/or
another distillation loss in the features space, attached to the output
of the network decoder. Here, we use the variant where both losses are
employed. As done by [ shmelkov2017incremental ] , we do not compare
with replay-based methods (e.g. [ rebuffi2017icarl ] ) since they
violate the standard ICL assumption regarding the unavailability of old
data.

In all tables we report other two baselines: simple fine-tuning (FT) on
each @xmath (e.g. Eq.( 3.5 )) and training on all classes offline
(Joint). The latter can be regarded as an upper bound. All results are
reported as mean Intersection-over-Union (mIoU) in percentage, averaged
over all the classes of a learning step and all the steps.

##### Implementation Details

For all methods we use the Deeplab-v3 architecture [ chen2017rethinking
] with a ResNet-101 [ he2016deep ] backbone and output stride of 16.
Since memory requirements are an important issue in semantic
segmentation, we use in-place activated batch normalization, as proposed
in [ rota2018place ] . The backbone has been initialized using the
ImageNet pre-trained model [ rota2018place ] . We follow [
chen2017rethinking ] , training the network with SGD and the same
learning rate policy, momentum and weight decay. We use an initial
learning rate of @xmath for the first learning step and @xmath for the
followings, as in [ shmelkov2017incremental ] . We train the model with
a batch size of 24 for 30 epochs for Pascal-VOC 2012 and 60 epochs for
ADE20K in every learning step. We apply the same data augmentation of [
chen2017rethinking ] and we crop the images to @xmath during both
training and test. For setting the hyper-parameters of each method, we
use the protocol of incremental learning defined in [ de2019continual ]
, using 20% of the training set as validation. The final results are
reported on the standard validation set of the datasets.

##### Pascal-VOC 2012

PASCAL-VOC 2012 [ pascal-voc-2012 ] is a widely used benchmark that
includes 20 foreground object classes. Following [
michieli2019incremental , shmelkov2017incremental ] , we define two
experimental settings, depending on how we sample images to build the
incremental datasets. Following [ michieli2019incremental ] , we define
an experimental protocol called the disjoint setup: each learning step
contains a unique set of images, whose pixels belong to classes seen
either in the current or in the previous learning steps. Differently
from [ michieli2019incremental ] , at each step we assume to have only
labels for pixels of novel classes, while the old ones are labeled as
background in the ground truth. The second setup, that we denote as
overlapped , follows what has been done in [ shmelkov2017incremental ]
for detection: each training step contains all the images that have at
least one pixel of a novel class, with only the latter annotated. It is
important to note a difference with respect to the previous setup:
images may now contain pixels of classes that we will learn in the
future, but labeled as background. This is a more realistic setup since
it does not make any assumption on the objects present in the images.

Following previous works [ shmelkov2017incremental ,
michieli2019incremental ] , we perform three different experiments
concerning the addition of one class ( 19-1 ), five classes all at once
( 15-5 ), and five classes sequentially ( 15-1 ), following the
alphabetical order of the classes to split the content of each learning
step.

Addition of one class (19-1) . In this experiment, we perform two
learning steps: the first in which we observe the first 19 classes, and
the second where we learn the tv-monitor class. Results are reported in
Table 3.7 for the disjoint scenario and in Table 3.8 for the overlapped
. Without employing any regularization strategy, the performance on past
classes drops significantly. FT, in fact, performs poorly, completely
forgetting the first 19 classes. Unexpectedly, using PI as a
regularization strategy does not provide benefits, while EWC and RW
improve performance of nearly 15%. However, prior-focused strategies are
not competitive with data-focused ones. In fact, LwF, LwF-MC, and ILT,
outperform them by a large margin, confirming the effectiveness of this
approch on preventing catastrophic forgetting. While ILT surpasses
standard ICL baselines, our model is able to obtain a further boost.
This improvement is remarkable for new classes, where we gain @xmath in
mIoU, while do not experience forgetting on old classes. It is
especially interesting to compare MiB with the baseline LwF which uses
the same principles of our method but without modeling the background.
Compared to LwF we achieve an average improvement of about @xmath , thus
demonstrating the importance of modeling the background in ICL for
semantic segmentation. These results are consistent in both the disjoint
and overlapped scenarios.

Single-step addition of five classes ( 15-5 ). In this setting we add,
after the first training set, the following classes: plant, sheep, sofa,
train, tv-monitor . As before, results are reported in Table 3.7 for the
disjoint scenario and in Table 3.8 for the overlapped . Overall, the
behavior on the first 15 classes is consistent with the 19-1 setting: FT
and PI suffer a large performance drop, data-focused strategies (LwF,
LwF-MC, ILT) outperform EWC and RW by far, while MiB gets the best
results, obtaining performances closer to the joint training upper
bound. For what concerns the disjoint scenario, our method improves over
the best baseline of @xmath on old classes, of @xmath on novel ones and
of @xmath in all classes. These gaps increase in the overlapped setting
where MiB surpasses the baselines by nearly @xmath in all cases, clearly
demonstrating its ability to take advantage of the information contained
in the background class.

Multi-step addition of five classes ( 15-1 ). This setting is similar to
the previous one except that the last 5 classes are learned
sequentially, one by one. From Table 3.7 and Table 3.8 , we can observe
that performing multiple steps is challenging and existing methods work
poorly for this setting, reaching performance inferior to 7% on both old
and new classes. In particular, FT and prior-focused methods are unable
to prevent forgetting, biasing their prediction completely towards new
classes and demonstrating performances close to 0% on the first 15
classes. Even data-focused methods suffer a dramatic loss in
performances in this setting, decreasing their score from the single to
the multi-step scenarios of more than 50% on all classes. On the other
side, MiB is still able to achieve good performances. Compared to the
other approaches, MiB outperforms all baselines by a large margin in
both old ( @xmath on the disjoint and @xmath on the overlapped ), and
new (nearly @xmath on both setups) classes. As the overall performance
drop ( @xmath on all classes) shows, the overlapped scenario is the most
challenging one since it does not impose any constraint on which classes
are present in the background.

Ablation Study. In Table 3.9 we report a detailed analysis of our
contributions, considering the overlapped setup. We start from the
baseline LwF [ li2017learning ] which employs standard cross-entropy and
distillation losses. We first add to the baseline our modified
cross-entropy ( CE ): this increases the ability to preserve old
knowledge in all settings without harming ( 15-1 ) or even improving (
19-1 , 15-5 ) performances on the new classes. Second, we add our
distillation loss ( KD ) to the model. Our KD provides a boost on the
performances for both old and new classes. The improvement on old
classes is remarkable, especially in the 15-1 scenario (i.e. 22.8%). For
the novel classes, the improvement is constant and is especially
pronounced in the 15-5 scenario (7%). Notice that this aspect is
peculiar of our KD since standard formulation work only on preserving
old knowledge. This shows that the two losses provide mutual benefits.
Finally, we add our classifiers’ initialization strategy ( init ). This
component provides an improvement in every setting, especially on novel
classes: it doubles the performance on the 19-1 setting ( @xmath vs
@xmath ) and triplicates on the 15-1 ( @xmath vs @xmath ). This confirms
the importance of accounting for the background shift at the
initialization stage to facilitate the learning of new classes.

##### Ade20k

ADE20K [ zhou2017scene ] is a large-scale dataset that contains 150
classes. Differently from Pascal-VOC 2012, this dataset contains both
stuff (e.g. sky, building, wall) and object classes. We create the
incremental datasets @xmath by splitting the whole dataset into disjoint
image sets, without any constraint except ensuring a minimum number of
images (i.e. 50) where classes on @xmath have labeled pixels. Obviously,
each @xmath provides annotations only for classes in @xmath while other
classes (old or future) appear as background in the ground truth. In
Table 3.10 and Table 3.11 we report the mean IoU obtained averaging the
results on two different class orders: the order proposed by [
zhou2017scene ] and a random one. In this experiments, we compare MiB
with data-focused methods only (i.e. LwF, LwF-MC, and ILT) due to their
gap in performance with prior-focused ones.

Single-step addition of 50 classes ( 100-50 ). In the first experiment,
we initially train the network on 100 classes and we add the remaining
50 all at once. From Table 3.10 we can observe that FT is clearly a bad
strategy on large scale settings since it completely forgets old
knowledge. Using a distillation strategy enables the network to reduce
the catastrophic forgetting: LwF obtains @xmath on past classes, ILT
@xmath , and LwF-MC @xmath . Regarding new classes, LwF is the best
strategy, exceeding LwF-MC by @xmath and ILT by @xmath . However, MiB is
far superior to all others, improving on the first classes and on the
new ones. Moreover, we can observe that we are close to the joint
training upper bound, especially considering new classes, where the gap
with respect to it is only @xmath . In Figure 3.7 we report some
qualitative results which demonstrate the superiority of MiB compared to
the baselines.

Multi-step addition of 50 classes ( 100-10 ). We then evaluate the
performance on multiple incremental steps: we start from 100 classes and
we add the remaining classes 10 by 10, resulting in 5 incremental steps.
In Table 3.11 we report the results on all sets of classes after the
last learning step. In this setting the performance of FT, LwF and ILT
are very poor because they strongly suffers catastrophic forgetting.
LwF-MC demonstrates a better ability to preserve knowledge on old
classes, at the cost of a performance drop on new classes. Again, MiB
achieves the best trade-off between learning new classes and preserving
past knowledge, outperforming LwF-MC by @xmath considering all classes.

Three steps of 50 classes ( 50-50 ). Finally, in Table 3.10 we analyze
also the performance on three sequential steps of 50 classes. Previous
ICL methods achieve different trade-offs between learning new classes
and not forgetting old ones. LwF and ILT obtain a good score on new
classes, but they forget old knowledge. On the contrary, LwF-MC
preserves knowledge on the first 50 classes without being able to learn
new ones. MiB outperforms all the baselines by a large margin with a gap
of @xmath on the best performing baseline, achieving the highest mIoU on
every step. Remarkably, the highest gap is on the intermediate step,
where there are classes that we must both learn incrementally and
preserve from forgetting on the subsequent learning step.

#### 3.4.4 Conclusions

In this section, we studied the incremental class learning problem for
semantic segmentation, analyzing the realistic scenario where the new
training set does not provide annotations for old classes, leading to
the semantic shift of the background class and exacerbating the
catastrophic forgetting problem. We address this issue by proposing a
novel objective function and a classifiers’ initialization strategy
which allows our network to explicitly model the semantic shift of the
background, effectively learning new classes without deteriorating its
ability to recognize old ones. Results show that MiB outperforms
regularization-based ICL methods by a large margin, considering both
small and large scale datasets. We believe that our problem formulation,
our approach and our extensive comparison with previous methods will
encourage future works on this novel research topic, especially in the
direction of effectively including the semantic shift in the background
class in ICL  models in semantic segmentation.

In Sections 3.3 and Sections 3.4 , we focused on the multi-domain and
incremental learning problem respectively, incrementally adding new
semantic task/concepts to a pre-trained model. However, in both these
tasks, the underlying assumption is that the images will contain only
objects we have seen during training or that we can safely consider as
background. A more realistic problem is equipping models with the
ability to not only recognizing semantic concepts and incrementally
learn new ones, but also detecting if an image contains a previously
unseen semantic category. In the next section, we will show how we can
address this problem in the framework of open-world recognition.

### 3.5 Open World Recognition 777M. Mancini, H. Karaoguz, E. Ricci, P.
Jensfelt, B. Caputo. Knowledge is Never Enough: Towards Web Aided Deep
Open World Recognition. IEEE International Conference on Robotics and
Automation (ICRA) 2019.888D. Fontanel, F. Cermelli, M. Mancini, S. Rota
Buló, E. Ricci, B. Caputo. Boosting Deep Open World Recognition by
Clustering. IEEE Robotics and Automation Letters 2020.

In the previous sections, we have discussed how new knowledge in terms
of classification tasks (Section 3.3 ) and semantic concepts (Section
3.4 ) can be added to a pre-trained model. In particular, in Section 3.4
, we showed how it is possible to have a model whose output space
contains all the concepts incrementally learned by the model. However,
all the models discussed so-far rely on a simple assumption: all the
categories we are interested in recognize are contained in our output
space. This closed-world assumption (CWA) is unrealistic for agents
acting in the real-world. Indeed it is impossible to capture all
existing semantic concepts in a single training set unless we are in a
very constrained scenario. In this section, we take a step forward and
we show how we can break the CWA developing two visual systems able to
work in the open world .

To clarify our goal, let us consider the example shown in Fig. 3.8 . The
robot has a knowledge base composed by a limited number of classes.
Given an image containing an unknown concept (e.g. banana), we want the
robot to detect it as unknown and being able to add it to its knowledge
base in subsequent learning stages. To accomplish this goal, it is very
important for a robot vision system to have two crucial abilities: (i)
it must be able to recognize already seen concepts and detect unknown
ones (i.e. open set recognition), and (ii) it must be able to extend its
knowledge base with new classes (i.e. incremental learning), without
forgetting the already learned ones and without access to old training
sets, avoiding catastrophic forgetting [ mccloskey1989catastrophic ] ).
While open set recognition [ scheirer2012toward , fragoso2013evsac ,
li2005open ] and incremental learning [ rebuffi2017icarl ,
camoriano2016incremental , camoriano2017incremental ,
valipour2017incremental ] are well-studied problems in the literature,
few works proposed a solution to solve them together [
bendale2015towards , de2016online ] . Standard approaches for open world
recognition (OWR) equip the nearest class mean (NCM) classification
algorithm with a rejection option based on an estimated threshold. While
standard approaches [ bendale2015towards , de2016online ] use shallow
features, in this section we take a step forward, proposing two deep
models for open world recognition.

The first model we will discuss builds on recent work by Guerriero et
al. [ guerriero2018deep ] and is (up to our knowledge) the first deep
open world recognition architecture in the literature. This approach
couples the flexibility of non-parametric classification methods,
necessary to add incrementally new classes over time and able to
estimate a probability score for each known class supporting the
detection of new classes (Nearest Non Outlier, NNO [ bendale2015towards
] ), with the powerful intermediate representations learned by deep
networks. We enable end-to-end training of the architecture through an
online approximate estimate and update function for the mean prototype
representing each known class and for the threshold allowing to detect
novel classes in a life-long learning fashion. We name this approach
DeepNNO ( Deep N earest N on- O utlier) [ mancini2019knowledge ] .

The second model improves DeepNNO by forcing the deep architecture used
as feature extractor to cluster appropriately samples belonging to the
same class, while pushing away samples of other classes. For this
reason, it introduces a global clustering loss term that aims at keeping
closer the features of samples belonging to the same class to their
class centroid. Furthermore, we show how the soft nearest neighbor loss
[ salakhutdinov2007learning , frosst2019analyzing ] can be successfully
employed as a local clustering loss term in order to force pair of
samples of the same class to be closer in the learned metric space than
relative sample points of other classes. Moreover, differently from
DeepNNO and previous shallow works [ bendale2015towards ] we avoid to
estimate a global rejection threshold on the model predictions based on
heuristic rules but we (i) define an independent threshold for each
class and (ii) we explicitly learn the thresholds by using a
margin-based loss function which balances rejection errors on samples of
a reserved memory held-out from the training. We name this approach
B-DOC (B-DOC ) [ fontanel2020boosting ] .

We evaluate DeepNNO and B-DOC on Core50 [ lomonaco2017core50 ] , RGB-D
Object Dataset [ lai2011large ] and CIFAR-100 [ krizhevsky2009learning ]
datasets, showing experimentally that DeepNNO outperforms previous OWR
methods and B-DOC show increased effectiveness in both detecting novel
classes and adding new classes to the set of known ones.

The outline of this section is as follows. We start by giving a more
formal definition of the OWR problem (Section 3.5.1 ) and some
preliminaries on the NCM [ mensink2012metric , guerriero2018deep ] and
NNO [ bendale2015towards , de2016online ] algorithms which serve as
starting point for our approaches (Section 3.5.2 ). We then describe
DeepNNO (Section 3.5.2 ) and B-DOC (Section 3.5.2 ), showing their
results on the aforementioned benchmarks (Section 3.5.2 ). We conclude
by providing a perspective toward autonomous visual systems with
preliminary experiments on Web-aided OWR (Section 3.5.6 ) and the
conclusions (Section 3.5.7 ).

#### 3.5.1 Problem Formulation

The goal of OWR is producing a model capable of (i) recognizing known
concepts (i.e. classes seen during training), (ii) detecting unseen
categories (i.e. classes not present in any training set used for
training the model) and (iii) incrementally add new classes as new
training data is available. Formally, let us denote as @xmath and @xmath
the input space (i.e. image space) and the closed world output space
respectively (i.e. set of known classes). Moreover, since our output
space will change as we receive new data containing novel concepts, we
will denote as @xmath the set of classes seen after the @xmath
incremental step, with @xmath denoting the category present in the first
training set. Additionally, since we aim to detect if an image contains
an unknown concept, in the following we will denote as @xmath the
special unknown class, building the output space as @xmath . We assume
that, at each incremental step, we have access to a training set @xmath
, with @xmath , @xmath , and @xmath , where @xmath is the set of
categories contained in the training set @xmath . Note that, without
loss of generality, in each incremental step, we assume to see a new set
of classes @xmath if @xmath . The set of known classes at step @xmath is
computed as @xmath and given a sequence of @xmath incremental steps, our
goal is to learn a model mapping input images to either their
corresponding label in @xmath or to the special class @xmath . In the
following we will split the classification model into two components: a
feature extractor @xmath that maps the samples into a feature space and
a classifier @xmath that maps the features into a class label, i.e.
@xmath with @xmath .

#### 3.5.2 Preliminaries

Standard approaches to tackle the OWR problem apply non-parametric
classification algorithms on top of learned metric spaces [
bendale2015towards , de2016online ] . A common choice for the classifier
@xmath is the Nearest Class Mean (NCM) [ mensink2012metric ,
guerriero2018deep ] . NCM works by computing a centroid for each class
(i.e. the mean feature vector) and assigning a test sample to the
closest centroid in the learned metric space. Formally, we have:

  -- -------- -- --------
     @xmath      (3.13)
  -- -------- -- --------

where @xmath is a distance function (e.g. Euclidean) and @xmath is the
mean feature vector for class @xmath . The standard NCM formulation
cannot be applied in the OWR setting since it lacks the inherent
capability of detecting images belonging to unknown categories. To this
extent, in [ bendale2015towards ] the authors extend the NCM algorithm
to the OWR setting by defining a rejection criterion for the unknowns.
In this extension, called Nearest Non-Outlier (NNO), class scores are
defined as:

  -- -------- -- --------
     @xmath      (3.14)
  -- -------- -- --------

where @xmath is the rejection threshold and @xmath is a normalization
factor. The final classification is held-out as:

  -- -------- -- --------
     @xmath      (3.15)
  -- -------- -- --------

Following [ mensink2012metric ] , in [ bendale2015towards ] the features
are linearly projected into a metric space defined by a matrix @xmath
(i.e. @xmath ), with @xmath learned on the first training set @xmath and
kept fixed during the successive learning steps. The main limitation of
this approach is that new knowledge will be incorporated in the
classifier @xmath without updating the feature extractor @xmath
accordingly. In the next section, we show how the performance of NNO can
be significantly improved by using as @xmath a deep architecture trained
end-to-end in each incremental step.

#### 3.5.3 Deep Nearest Non-Outlier

The DeepNNO algorithm is obtained from NNO with the following
modifications: (i) the feature extractor function is replaced with deep
representations derived from neural network layers; (ii) an online
update strategy is adopted for the mean vectors @xmath ; (iii) an
appropriate loss is optimized using stochastic gradient descent (SGD)
methods in order to compute the feature representations and the
associated class specific means.

First, inspired by the recent work [ guerriero2018deep ] , we replace
the feature extractor function @xmath with deep representations derived
from a neural network @xmath and define the class-specific probability
scores as follows:

  -- -------- -- --------
     @xmath      (3.16)
  -- -------- -- --------

Note that, differently from [ bendale2015towards ] , we do not consider
explicitly the matrix @xmath since this is replaced by the network
parameters @xmath . Furthermore, we avoid to use a clamping function as
this could hamper the gradient flow within the network. This formulation
is similar to the NNO version proposed in [ de2016online ] which have
been showed to be more effective than that in [ bendale2015towards ] for
online scenarios.

In OWR the classification model must be updated as new samples arrive.
In DeepNNO this translates into incrementally updating the feature
representations @xmath and defining an appropriate strategy for updating
the class mean vectors. Given a mini-batch of samples @xmath , we
compute the mean vectors through:

  -- -------- -- --------
     @xmath      (3.17)
  -- -------- -- --------

where @xmath represents the number of samples belonging to class @xmath
seen by the network until the current update step @xmath , @xmath
represents the number of samples belonging to class @xmath in the
current batch and @xmath represents the current mini-batch mean vector
relative to the features of class @xmath .

Given the class-probability scores in DeepNNO we define the following
prediction function:

  -- -------- -- --------
     @xmath      (3.18)
  -- -------- -- --------

where @xmath is a threshold which, similarly to the parameter @xmath in
Eqn.( 3.14 ), regulates the number of samples that are assigned to a new
class. While in [ bendale2015towards ] @xmath is a user defined
parameter which is kept fixed, in this subsection we argue that a better
strategy is to dynamically update @xmath since the feature extractor
function and the mean vectors change during training. Intuitively, while
training the deep network, an estimate of @xmath can be obtained by
looking at the probability score given to the ground truth class. If the
score is higher than the threshold, the value of @xmath can be
increased. Oppositely, the value of the threshold is decreased if the
prediction is rejected. Specifically, given a mini-batch @xmath we
update @xmath as follows:

  -- -------- -- --------
     @xmath      (3.19)
  -- -------- -- --------

where @xmath is the number of classes in @xmath represented by at least
one sample in @xmath and @xmath is the weighted average probability
score of instances of class @xmath within the batch. Formally we
consider:

  -- -------- -- --------
     @xmath      (3.20)
  -- -------- -- --------

where @xmath is a normalization factor and:

  -- -------- -- --------
     @xmath      (3.21)
  -- -------- -- --------

where @xmath and @xmath are scalar parameters which allow to assign
different importance to samples for which the scores given to the ground
truth class are respectively rejected or not by the current threshold
@xmath .

To train the network, we employ standard SGD optimization, minimizing
the binary cross entropy loss over the training set:

  -- -------- -- --------
     @xmath      (3.22)
  -- -------- -- --------

where:

  -- -------- -- --------
     @xmath      (3.23)
  -- -------- -- --------

After computing the loss, we use standard backpropagation to update the
network parameters @xmath . After updating @xmath , we use the samples
of the current batch to update both the class mean estimates @xmath and
the threshold @xmath , using Eqn.( 3.17 ) and Eqn.( 3.19 ) respectively.

To allow our model for incremental learning of our deep neural network,
we exploit two additional components. Following standard rehearsal-based
approaches for incremental learning [ rebuffi2017icarl ,
chaudhry2018riemannian , castro2018end ] , the first is a memory which
stores the most relevant samples of each class in @xmath . The relevance
of a sample @xmath is determined by its distance @xmath to the class
mean @xmath i.e. the lower is the distance, the higher is the relevance
of the sample. The memory is used to augment the training set @xmath ,
allowing to update the mean estimates of the classes in @xmath as the
network is trained using samples of novel ones. In order to avoid an
unbounded growth, the size of the memory is kept fixed and it is pruned
after each incremental step to make room for instances of novel classes.
The pruning is performed by removing, for each class in @xmath , the
instances with lowest relevance.

The second component is a batch sampler which makes sure that a given
ratio of the batch is composed by samples taken from the memory,
independently from the memory size. This allows to avoid biasing the
incremental learning procedure towards novel categories, in the case
their number of samples is much larger than the memory size.
Additionally, we add a distillation loss [ hinton2015distilling ] which
act as regularizer and avoids the forgetting of previously learned
features. Denoting as @xmath the network trained on the set of known
classes, the distillation loss is defined as:

  -- -------- -- --------
     @xmath      (3.24)
  -- -------- -- --------

The overall loss is thus defined as:

  -- -------- -- --------
     @xmath      (3.25)
  -- -------- -- --------

where @xmath is an hyperparameter balancing the contribution of @xmath
within @xmath .

#### 3.5.4 Boosting Deep Open World Recognition

Despite its experimental effectiveness (see Section 3.5.5 ), DeepNNO has
two main drawbacks. First, the learned feature representation @xmath is
not forced to produce predictions clearly localized in a limited region
of the metric space. Indeed, constraining the feature representations of
a given class to a limited region of the metric space allows to have
both more confident predictions on seen classes and producing clearer
rejections also for images of unseen concepts. Second, having an
heuristic strategy for setting the threshold is sub-optimal with no
guarantees on the robustness of the choice. In the following, we will
detail how we provide solutions to both problems in B-DOC .

To obtain feature representations clearly localized in the metric space
based on their semantic, we propose to use a pair of losses enforcing
clustering. In particular, we use a global term which forces the network
to map samples of the same class close to their centroid (Fig. 3.9 ,
left) and a local clustering term which constrains the neighborhood of a
sample to be semantically consistent, i.e. to contain samples of the
same class (Fig. 3.9 , right). In the following we describe the two
clustering terms.

Global Clustering . The global clustering term aims to reduce the
distance between the features of a sample with the centroid of its
class. To model this, we took inspiration from what has been proposed in
[ mensink2012metric ] and we employ a cross-entropy loss with the
probabilities obtained through the distances among samples and class
centroids. Formally, given a sample @xmath and its class label @xmath ,
we define the global clustering term as follows:

  -- -------- -- --------
     @xmath      (3.26)
  -- -------- -- --------

The class-specific score @xmath is defined as:

  -- -------- -- --------
     @xmath      (3.27)
  -- -------- -- --------

where @xmath is a temperature value which allows us to control the
behavior of the classifier. We set @xmath as the variance of the
activations in the feature space, @xmath , in order to normalize the
representation space and increase the stability of the system. During
training, @xmath is the variance of the features extracted from the
current batch while, at the same time, we keep an online global estimate
of @xmath that we use at test time. The class mean vectors @xmath with
@xmath as well as @xmath are computed in an online fashion, as in
DeepNNO.

Local Clustering . To enforce that the neighborhood of a sample in the
feature space is semantically consistent (i.e. given a sample @xmath of
a class @xmath , the nearest neighbours of @xmath belong to @xmath ), we
employ the soft nearest neighbour loss [ salakhutdinov2007learning ,
frosst2019analyzing ] . This loss has been proposed to measure the
class-conditional entanglement of features in the representation space.
In particular, it has been defined as:

  -- -------- -- --------
     @xmath      (3.28)
  -- -------- -- --------

where T refers to the temperature value, @xmath is the current training
batch, and @xmath is the set of samples in the training batch belonging
to class @xmath . Instead of performing multiple learning steps to
optimize the value of T as proposed in [ frosst2019analyzing ] , we use
as @xmath as we do in Eq. 3.27 .

Intuitively, given a sample @xmath of a class @xmath , a low value of
the loss indicates that the nearest neighbours of @xmath belong to
@xmath , while high values indicates the opposite (i.e. nearest
neighbours belong to classes @xmath with @xmath ). Minimizing this
objective allows to enforce the semantic consistency in the neighborhood
of a sample in the feature space.

Knowledge distillation and full objective. As highlighted in the Section
3.5.3 , to avoid forgetting old knowledge, we want the feature extractor
to preserve the behaviour learned in previous learning steps. To this
extent, as in DeepNNO, we introduce (i) a memory which stores the most
relevant samples for classes in @xmath and (ii) a distillation loss
which enforces consistency among the features extracted by @xmath and
ones obtained by the feature extractor of the previous learning step,
@xmath . The distillation loss is computed as in Eq. ( 3.24 ). As
before, this loss is minimized only for incremental training steps,
hence, only when @xmath . Additionally, we apply also the same balanced
batch sampling scheme of DeepNNO.

Overall, given a batch of samples @xmath , we train the network to
minimize the following loss:

  -- -------- -- --------
     @xmath      (3.29)
  -- -------- -- --------

with @xmath and @xmath hyperparameters weighting the different
components.

Learning to detect the unknown . In order to extend the NCM-based
classifier of B-DOC to work on the open set scenario, we explicitly
learn class-specific rejection criterions. As illustrated in Fig. 3.10 ,
for each class @xmath we define the class-specific threshold as the
maximal distance @xmath for which the sample belongs to @xmath . Under
this definition, the B-DOC classifier is:

  -- -------- -- --------
     @xmath      (3.30)
  -- -------- -- --------

with @xmath . Instead of heuristically estimating or fixing a maximal
distance, we explicitly learn it for each class be freezing the feature
extractor @xmath and minimizing the following objective over the
thresholds @xmath :

  -- -------- -- --------
     @xmath      (3.31)
  -- -------- -- --------

where @xmath if @xmath and @xmath otherwise. The @xmath loss leads to an
increase of @xmath if the distance from a sample belonging to the class
@xmath and the class centroid @xmath is greater than @xmath . Instead,
if a sample not belonging to @xmath has a distance from @xmath less then
@xmath , it increases the value of @xmath .

Overall, the training procedure of B-DOC is made of two steps: in the
first we train the feature extractor on the training set minimizing Eq.
3.29 , while in the second we learn the distances @xmath on a set of
samples which we held-out from training set. To this extent, we split
the samples of the memory in two parts, one used for updating the
feature extractor @xmath and the centroids @xmath and the other part for
learning the @xmath values.

#### 3.5.5 Experimental results

In this subsection, we first introduce the experimental setting and the
metrics used for the evaluation, then we report results of DeepNNO and
B-DOC , showing ablation studies for each of their components.

Datasets and Baselines. We assess the performance of our models on three
datasets: RGB-D Object [ lai2011large ] Core50 [ lomonaco2017core50 ]
and CIFAR-100 [ krizhevsky2009learning ] . The RGB-D Object dataset [
lai2011large ] is one of the most used dataset to evaluate the ability
of a model to recognize daily-life objects. It contains 51 different
semantic categories that we split in two parts in our experiments: 26
classes are considered as known categories, while the other 25 are the
set of unknown classes. Among the 26 classes, we consider the first 11
classes as the initial training set and we incrementally add the
remaining classes in 4 steps of 5 class each. As proposed in [
lai2011large ] , we sub-sample the dataset taking one every fifth frame.
For the experiments, we use the first train-test split among the
original ones defined by the authors [ lai2011large ] . In each split
one object instance from each class is chosen to be used in the test set
and removed from the training set. This split provides nearly 35,000
training images and 7,000 test images.

Core50 [ lomonaco2017core50 ] is a recently introduced benchmark for
testing continual learning methods in an egocentric setting. The dataset
contains images of 50 objects grouped into 10 semantic categories. The
images have been acquired on 11 different sequences with varying
conditions. Following the standard protocol described in [
lomonaco2017core50 ] , we select the sequences 3, 7, 10 for the
evaluation phase and use the remaining ones to train the model. Due to
these differences in conditions between the sequences, Core50 represents
a very challenging benchmark for object recognition. As in the RGB-D
Object dataset, we split it into two parts: 5 classes are considered
known and the other 5 as unknown. In the known set, the first 2 classes
are considered as the initial training set. The others are incrementally
added 1 class at a time.

CIFAR-100 [ krizhevsky2009learning ] is a standard benchmark for
comparing incremental class learning algorithms [ rebuffi2017icarl ] .
It contains 100 different semantic categories. We split the dataset into
50 known and 50 unknown classes and considering 20 classes as the
initial training set. Then, we incrementally add the remaining ones in
steps of 10 classes. We evaluate the performance of DeepNNO and B-DOC in
the OWR scenario comparing it with NNO [ bendale2015towards ] , using
the simplified implementation in [ de2016online ] . We further compare
our methods with two standard incremental class learning algorithms,
namely LwF [ li2017learning ] (in the MC variant of [ rebuffi2017icarl ]
) and iCaRL [ rebuffi2017icarl ] . Both LwF and iCaRL are designed for
the closed world scenario, thus we use their performances as reference
in that setting, without open-ended evaluation. For each dataset, we
have randomly chosen five different sets of known and unknown classes.
After fixing them, we run the experiments three times for each method.
The results are obtained by averaging the results among each run and
order.

Networks architectures and training protocols. We use a ResNet-18
architecture [ he2016deep ] for all the experiments. For RGB-D Object
dataset and Core50, we train the network from scratch on the initial
classes for 12 epochs and for 4 epochs in the incremental steps. For
CIFAR-100, instead, we set the epochs to 120 for the initial learning
stage and to 40 for each incremental step. In the case of NNO we use the
features extracted from the pre-trained network to compute the
class-specific mean vectors of novel categories, but we do not update
the weight matrix @xmath and the threshold parameter @xmath , as in [
bendale2015towards ] . For DeepNNO we use an initial learning rate of
@xmath in all settings, for B-DOC we use a learning rate of @xmath for
the RGB-D Object dataset and CIFAR-100, and @xmath for Core50, with a
batch size of 128 for RGB-D Object dataset and of 64 for CIFAR-100 and
Core50. We train the networks using SGD with momentum @xmath and a
weight decay of @xmath on all datasets. We resize the images of RGB-D
Object dataset to @xmath pixels, the ones of CIFAR-100 to 32x32 and the
images of Core50 to @xmath pixels. We perform random cropping and
mirroring for all the datasets. In all experiments, we set @xmath ,
@xmath and @xmath for DeepNNO, while @xmath for B-DOC . For both methods
we consider a fixed size memory of 2000 samples, constructing each batch
by drawing 40% of the instances from the memory. Note that, in B-DOC 20%
of the samples present in the memory are never seen during training, but
are used only to learn the class-specific threshold values @xmath . For
this set of held-out samples, we also perform color jittering varying
brightness, hue and saturation.

Metrics We use 3 standard metrics for comparing the performances of OWR
methods. For the closed world we show the global accuracy with and
without rejection option. Specifically, in the closed world without
rejection setting, the model is tested only on the known set of classes,
excluding the possibility to classify a sample as unknown . This
scenario measures the ability of the model to correctly classify samples
among the given set of classes. In the closed world with rejection
scenario, instead, the model can either classify a sample among the
known set of classes or categorize it as unknown . This scenario is more
challenging than the previous one because samples belonging to the set
of known classes might be misclassified as unknowns . For the open world
we use the standard OWR metric defined in [ bendale2015towards ] as the
average between the accuracy computed on the closed world with rejection
scenario and the accuracy computed on the open set scenario (i.e. the
accuracy on rejecting samples of unknown classes). Since the latter
metric creates biases on the final score (i.e. a method rejecting every
sample will achieve a 50% accuracy), we introduced the OWR-H as the
harmonic mean between the accuracy on open set and the closed world with
rejection scenarios to mitigate this bias.

##### Quantitative results

We report the results on the RGB-D Object dataset in Fig. 3.15 .
Considering the closed world without rejection, reported in Fig. (a)a .
This scenario is used to asses the ability of a method to learn novel
classes while preserving old knowledge, without considering the open-set
scenario. As a first observation, we note that both our deep methods
outperform NNO by a large margin (i.e. 9.2% DeepNNO and 14.8% B-DOC in
accuracy on average), showing the importance of end-to-end trained deep
representations for OWR. Remarkably, B-DOC outperforms DeepNNO by 5.6%
of accuracy on average. The reason for the improvement comes from the
introduction of the global and local clustering loss terms, which allows
the model to better aggregate samples of the same class and to better
separate them from samples of other classes. Comparing our models with
the incremental class learning approaches LwF and iCaRL, we can see that
both of them are highly competitive, surpassing LwF with a large gap
while being either comparable (B-DOC ) or slightly inferior (DeepNNO)
with the more effective iCaRL. We believe these are remarkable results
given that the main goal of our models is not to purely extend their
knowledge over time with new concepts.

For what concerns the comparison on the closed world with rejection,
shown in Fig. (b)b , again DeepNNO and B-DOC surpass NNO in terms of
performance. However, the results of B-DOC are remarkable, demonstrating
how it is achieves higher confidence on the known classes, being able to
reject a lower number of known samples. In particular, B-DOC is more
confident on the first incremental steps, and obtains, on average, an
accuracy of 10.3% more than DeepNNO.

The findings are confirmed also on OWR metrics. Again, both DeepNNO and
B-DOC surpass NNO, showing the importance of end-to-end trained
representations and updated thresholds in achieving a higher
performance, even in the presence of unknowns. Even on the OWR metrics,
B-DOC surpasses DeepNNO. From the results of OWR, reported in Fig. (c)c
, we see that B-DOC reaches performance similar to DeepNNO in the first
steps, while it outperforms it in the latest ones. However, considering
the OWR-H (Fig. (d)d ), B-DOC is better in all the incremental steps.
This is because its learned rejection criterion, coupled with the
clustering losses, allows B-DOC to achieve a better trade-off between
the accuracy of open set and closed world with rejection. Overall, B-DOC
improves on average by 4.8% and 5.2% with respect to DeepNNO in the OWR
and OWR-H metrics respectively. We will provide a deeper analysis on the
rejection criterion of DeepNN and B-DOC with ablation studies in the
next subsections.

In Fig. 3.20 we report the results on the Core50 [ lomonaco2017core50 ]
dataset. Similarly to the RGB-D Object dataset, DeepNNO and B-DOC
achieve very competitive results with respect to incremental class
learning algorithms designed for the closed world scenario, with B-DOC
remarkably outperforming iCaRL by 4.7% of accuracy in the last
incremental step. Similarly, B-DOC achieves a superior performance in
both closed world, without and with rejection option with respect to the
other OWR algorithms, outperforming NNO by 13.01% and DeepNNO by 7.74%
on average in the first (Fig. (a)a ) and by more than @xmath for both
NNO and DeepNNO in the latter (Fig. (b)b ). In particular, it is worth
noting how the challenges of Core50 (i.e. train and test acquisitions
under different conditions) does not allow DeepNNO and NNO to properly
model the confidence threshold, rejecting most of the sample of the
known classes. Indeed, by including the rejection option the accuracy
drops to 27.2% and 26.3% respectively for DeepNNO and NNO, while B-DOC
reaches an average accuracy of 38.0%.

In Fig. (c)c and Fig. (d)d , we report the OWR performances (standard
and harmonic) on Core50. While DeepNNO surpasses the performance of NNO
in both metrics (5.4% in standard OWR and 3.1% in OWR-H), B-DOC performs
even better, outperforming DeepNNO by 3.4% and 7.2% in average
respectively in standard OWR and OWR-H metrics, confirming the
effectiveness of the clustering losses and the learned class-specific
maximal distances.

Finally, in Fig. 3.23 we report the results on the CIFAR-100 dataset in
terms of the OWR (Fig. (a)a ) and OWR-H metrics (Fig. (b)b ). Even in
this benchmark, confirms the finding of previous analysis: end-to-end
trained methods with updated thresholds (DeepNNO and B-DOC ) are more
effective than shallow methods (NNO). Similarly to previous analyses,
B-DOC outperforms, on average both DeepNNO and NNO, with lower
performances only in the initial training stage. However, in the
incremental learning steps B-DOC clearly outperforms both methods,
demonstrating its ability to learning and recognizing in an open-world
without forgetting old classes. The relative gaps are still remarkable.
DeepNNO improves over NNO by 6.3% and  4% in OWR and OWR-H metrics
respectively. However, in the incremental steps, the average improvement
of B-DOC over NNO are of  10% in both OWR and OWR-H metrics, while over
DeepNNO are of 2% for the OWR and 4.5% for the OWR-H metric.

##### Ablation study of DeepNNO

DeepNNO improves over NNO by introducing two main aspects: end-to-end
trained deep representations with an updated rejection threshold, and a
distillation loss to preserve old knowledge. In the following we analyze
in detail the reasons behind the improvement of DeepNNO with respect to
NNO on the CIFAR-100 dataset, focusing first on the importance of
learning deep representations with updated thresholds and then on the
impact of the distillation loss.

Deep representation and updated threshold. We start by performing
experiments in the closed world scenario, i.e. measuring the
performances considering only the set of known classes. In particualr,
we compare the performance of DeepNNO with NNO and DeepNNO without
rejection option (i.e. DeepNNO-no rejection ). The latter baseline
method is the upper bound of DeepNNO in terms of performances in the
closed world, since it does not reject any instance of known classes
(i.e. it does not identify samples of known classes as unknowns). This
baseline is used to demonstrate the validity of the method in Eq. ( 3.19
) for setting the threshold @xmath . The results are shown in Fig. 3.25
where the numbers between parenthesis denote the average accuracy among
the different incremental steps. From Fig. 3.25 it is possible to draw
two observations. First, there is a large gap between the performances
of DeepNNO and NNO, with our model outperforming its non-deep
counterpart by more than @xmath on average and by more than @xmath after
all the incremental steps.

The improved performance of DeepNNO can be ascribed to the fact that, by
dynamically updating the learned feature representations, DeepNNO is
able to better adapt the learned classifier to novel semantic concepts.
Second, DeepNNO achieves results close to DeepNNO without rejection.
This indicates that, thanks to the proposed approach for setting the
threshold @xmath , DeepNNO only rarely identifies samples of known
classes as belonging to an unknown category. We believe this is mainly
due to the introduction of the different weighting factors @xmath and
@xmath while updating @xmath . This observation is confirmed by results
shown in Fig. 3.27 which analyzes the effect of varying @xmath with
@xmath fixed to 1. As @xmath decreases, the accuracy decreases as well,
due to the higher value reached by @xmath which leads to wrongly reject
many samples, classified as instances of unknown classes. We want to
highlight however that for more complex and realistic scenario, the
threshold obtained by DeepNNO does not generalize as well and more the
more principled strategy of B-DOC results more effective, as we will
show in the next subsection.

As a second experiment, we compare the performances of DeepNNO and NNO
in the open world recognition scenario varying the number of known and
unknown classes. The results are shown in Fig. 3.25 , from which it is
easy to see that DeepNNO outperforms its non-deep counterpart by a large
margin. In fact, in this scenario, our model achieves a standard OWR
accuracy 9% higher than standard NNO on average, considering 50 unknown
classes. Moreover, this margin increases during the training: after all
the incremental steps our model outperforms NNO by a margin close to
15%. It is worth noting that the advantages of our model are independent
on the number of unknown classes, since DeepNNO constantly outperforms
NNO in all settings.

Distillation loss. Another important component of DeepNNO is the
distillation loss. This loss guarantees the right balance between
learning novel concepts and preserving old features. To analyze its
impact, in Fig. 3.27 we report the performances of DeepNNO in the closed
world scenario for different values of @xmath . From the figure it is
clear that, without the regularization effect of the distillation loss,
the accuracy significantly drops. On the other hand, a high value of
@xmath leads to poor performances and low confidence on the novel
categories. Properly balancing the contribution of classification and
distillation loss the best performance can be achieved. The use of the
distillation loss is thus crucial for limiting the catastrophic
forgetting, as previously verified in [ li2017learning ,
rebuffi2017icarl ] .

##### Ablation study of B-DOC

B-DOC is mainly built on three components, i.e. global clustering loss
(GC), local clustering loss (LC) and the learned class-specific
rejection thresholds. In the following we analyze the contribution of
each of them. We start from the two clustering losses and then we
compare the choice we made for the rejection with other common choices.

Global and local clustering. In Table 3.12 we compare the two clustering
terms considering the open world recognition metrics in the RGB-D Object
dataset. By analyzing the two loss terms separately we see that, on
average, they show similar performance. In particular, using only the
global clustering (GC) term we achieve slightly better performance on
the first three incremental steps, while on the fourth the local
clustering (LC) term is better. However, the best performance on every
step is achieved by combining the global and local clustering terms (GC
+ LC). This demonstrates that the two losses provide different
contributions, being complementary to learn a representation space which
properly clusters samples of the same classes while better detecting
unknowns.

Lastly, since the B-DOC loss functions and triplet loss [
balntas2016learning ] share the same objective, i.e. building a metric
space where samples sharing the same semantic are closer then ones with
different semantics, we report in Table 3.12 also the results achieved
by replacing our loss terms with a triplet loss [ balntas2016learning ]
. As the Table shows, the triplet loss formulation (Triplet) fails in
reaching competitive results with respect to our full objective function
in Eq. ( 3.29 ), with a gap of more than 5% in both standard OWR metric
and OWR harmonic mean. Notably, it achieves lower results also with
respect to all of the loss terms in isolation and the superior
performances of LC confirm the advantages of SNNL-based loss functions
with respect to triplets, as shown in [ frosst2019analyzing ] .

Detecting the Unknowns. In Table 3.13 we report a comparison of
different strategies to reject samples on the RGB-D Object dataset [
lai2011large ] . In particular, using the same feature extractor, we
compare the proposed method to learn the class-specific maximal
distances (i.e. Eq. ( 3.31 )) with three baselines: (i) the online
update strategy of DeepNNO (Eq. ( 3.19 )), (ii) we learn class-specific
maximal distances but during training (i.e. without our two-stage
pipeline) and (iii) we learn a single maximal distance which applies to
all classes using our two-stage training strategy.

The comparison is performed considering the difference of the rejection
rates on the known and unknown samples. For the known class samples, we
report the percentage of correctly classified samples in the
closed-world that are rejected when the rejection option is included. We
intentionally remove the wrongly classified samples since we want to
isolate rejection mistakes from classification ones. On the unknown
samples, we report the open-set accuracy, i.e. the percentage of
rejected samples among all the unknown ones. In the third column, we
report the difference among the open-set accuracy and the rejection rate
on known samples. Ideally, the difference should be as close as possible
to 100%, since we want a 100% rejection rate on unknown class samples
and 0% on the known class ones.

From the table, we can see that the highest gap is achieved by the
class-specific maximal distance with the two-stage pipeline we proposed,
which rejects 27.4% of known class samples and 65.2% on the unknown
ones. The gap with the other strategies is remarkable. Using the two
stage-pipeline but a class-generic maximal distance leads to a low
rejection rate, both on known and unknown samples, achieving a
difference of 22.6%, which is 15.2% less than using a class-specific
distance. On the other hand, estimating the confidence threshold as
proposed in DeepNNO or without our two-stage pipeline provides a very
high rejection rate, both on known and unknown classes, which lead to a
difference of 14.4% and 15.6% for DeepNNO and the single-stage strategy
respectively, the lowest two among the four strategies. In fact,
computing the thresholds using only the training set biases the
rejection criterion on the overconfidence that the method has acquired
on this set. Consequently, this makes the model considering the
different test data distribution (caused by e.g. different object
instances) as a source for rejection even if the actual concept present
in the input is known. Using the two-stage process we can overcome this
bias, tuning the rejection criterion on unseen data on which the model
cannot be overconfident.

#### 3.5.6 Towards Autonomous Visual Systems: Web-aided OWR

The OWR frameworks considered so far assumes the existence of an
’oracle’, providing annotated images for each new class. In a robotic
scenario, this has been often translated into having a human in the
loop, with the robot asking for images and labels. This scenario somehow
limits the autonomy of a robot system, that without the presence of a
teacher, would find itself stuck when detecting a new object. Moreover,
especially in robotics applications, this assumption is highly
unrealistic since: i) the labels of samples of unknown categories are,
by definition, unknown; ii) images of the unknown classes for
incrementally updating the model are usually unavailable, since it is
impossible to have a pre-loaded database containing all possible classes
existing in the real world. In the last part of this section we want to
describe a simple general pipeline to address the aforementioned issues
with first pilot experiments showing its possible application.

We start by considering the problem of retrieving the correct label of
an unknown object. To this extent, we exploit standard search tools used
by humans. First, once an object is recognized as unknown, we query the
Google Image Search engine ⁹ ⁹ 9 https://images.google.com/ to retrieve
the closest keyword to the current image. Obviously the retrieved label
might not be correct e.g. due to low resolution of the image or a non
canonical pose of the object. We tackle this issue through an additional
human verification step, leaving the investigation of this problem to
future works. As subsequent step, we use the retrieved keyword to
automatically download images from the web. These weakly-annotated and
noisy images represent new training data for the novel category which
can be used to incrementally train the deep network. Fig. 3.28 shows an
overview of our pipeline. Interestingly, this simple framework mimics
the human ability to learn not only from situated experiences, but also
from visual knowledge externalized on artifacts (e.g. like drawings), or
indeed Web resources.

We conduct a first series of preliminary experiments, using web-images
in the incremental learning steps of DeepNNO, to validate the
feasibility of this pipeline. The results of our experiments are shown
in Fig. 3.30 for CIFAR-100 and in Fig. 3.30 for Core50. As expected,
considering images from the Web instead of images from the datasets lead
to a decrease in terms of performance. However, the accuracy of the
Web-based DeepNNO is still good, especially when compared with its
non-deep counterpart.

On the CIFAR-100 experiments we achieve a remarkable performance, with
Web DeepNNO outperforming NNO by 3.5% on average and by more than 5%
after all the incremental steps, with respect to the standard OWR
metric. We highlight that these results have been achieved exploiting
only noisy and weakly labeled Web images, without any filtering
procedure or additional optimization constraints. On the Core50
experiments, the gap Between DeepNNO and NNO is lower, as shown in Fig,
(c)c and (d)d and this impacts also the results of the Web-based version
of DeepNNO, achieving a modest improvement with respect to NNO. We
ascribe this behavior to the fact that there is a large appearance gap
between Core50 images gathered in an egocentric setting and Web images,
thus both the rejection threshold and the semantic centroids of new
classes are not able to well model the underline data distribution, with
deteriorated final results. We believe that this issue can be addressed
in future works by e.g. imposing some constraints on the quality of
downloaded images and by coupling DeepNNO with domain adaptation
techniques [ patel2015visual , carlucci2017autodial , mancini2018kitting
, mancini2018boosting ] in order to reduce the domain shift between
downloaded images and training data.

To validate the applicability of the pipeline in a real scenario, we
tested the Web-aided version of DeepNNO by integrating it into a visual
object detection framework and running it on a Yumi 2-arm manipulator
equipped with a Kinect. We have used the Faster-RCNN framework in [
ren2015faster ] with the ResNet-101 architecture [ he2016identity ] as
backbone. We pre-trained the network on the COCO dataset [
lin2014microsoft ] , after replacing the standard fully-connected
classifier with the proposed DeepNNO. We performed an open world
detection experiment by placing multiple objects (known and unknowns) in
the workspace of the robot. Whenever a novel object is detected, the
robot tries to get the corresponding label from Google Image Search,
using the cropped image of the unknown object. In case the label is not
correct, a human operator cooperates with the robot and provides the
right label. The provided label is used by the robot to automatically
download the images associated to the novel class from the Web sources.
These images and the original one where the object has been detected in
the workspace, are then used to update the classification model.

Figure 3.31 shows a qualitative result associated to our experiment. The
robot was able to correctly detect the red hammer as unknown, add it in
its knowledge base and recognize it in subsequent learning steps. ¹⁰ ¹⁰
10 A full example is available in the supplementary material of [
mancini2019knowledge ] . Despite the simplicity of the workspace, we
want to highlight that the robot was able to recognize the hammer
without any explicitly labeled training data for the class of interest.

We want to point out that here we are not claiming that our framework is
incorporating new knowledge into a visual robotic system in a completely
autonomous and fully effective way. Indeed (i) the human verification
step on the retrieved keyword is necessary and (ii) web supervision [
divvala2014webly , chen2015webly ] requires to address challenges such
as noisy labels [ niu2018noisy ] and domain shift [ xu2016weblyda ] ,
which we did not take into account. Nevertheless, we still believe our
experiments show how our pipeline is a feasible starting point which is
worth exploring in future research directions toward autonomous learners
in the real world.

#### 3.5.7 Conclusions

In this section, we presented two approaches to tackle the open world
recognition problem in robot vision. We base our approaches on an NCM
classifier built on top of end-to-end trainable deep features (DeepNNO),
and we further boost the OWR performances of this framework by training
the deep architecture to minimize a global to local semantic clustering
loss (B-DOC ) which allows reducing distances of samples of the same
class in the feature space while separating them from points belonging
to other classes, better detecting unknown concepts. In B-DOC we also
avoid heuristic estimates of a rejection criterion for detecting
unknowns by explicitly learning class-specific distances beyond which a
sample is rejected. Quantitative and qualitative analysis on standard
recognition benchmarks shows the efficacy of the proposed approaches and
choices, outperforming the previous state-of-the-art OWR algorithm.
Finally, we also showed preliminary experiments with a simple pipeline
for allowing the robot to autonomously learn new semantic concepts,
without the aid of an oracle providing it with a training set containing
the desired target classes.

Future works will further investigate webly supervised approaches with
the goal of pushing the envelope in life-long learning of autonomous
systems. In particular, when training images are autonomously retrieved
from the Web, they come with inherent noisy labeling (e.g. wrong
semantic) and domain shift (e.g. white backgrounds). Attacking all these
problems would allow active visual systems to get closer to full
autonomy. In an intermediate direction, it would be interesting to
analyze the OWR problem in an active learning context [
parisi2019rethinking ] , letting the robot decide when to ask for human
help for either collecting data or label new concepts.

This section concludes our line of works on incrementally injecting new
knowledge in a pre-trained deep model under various scenarios, with
(ICL) or without (multi-domain learning) shared output spaces with old
knowledge, and with (ICL, multi-domain learning) or without (OWR)
closed-world assumption. Additionally, we identified problems (e.g.
semantic shift of the background class) and posed challenges (web-aided
OWR) not tackled in the community. Nevertheless, differently from
Chapter 2 , here we consider the training and test distributions to
belong to be equal, without any domain shift problem. On the other hand,
differently from the techniques presented in Chapter 2 , this chapter
described techniques that allow modifying the output space of a
pre-trained architecture. In the next chapter, we will merge these two
worlds together, describing the first method capable of recognizing
unseen semantic concepts in unseen visual domains.

## Chapter 4 Towards Recognizing Unseen Categories in Unseen Domains

While in the previous chapters we considered methods extending a
pretrained model either to new input distributions or to new semantic
concepts, an open research question is whether we can address the two
problems together, producing a deep model able to recognize new semantic
concepts (i.e. addressing the semantic shift) in possibly unseen domains
(i.e. addressing the domain shift). In this chapter, we start analyzing
how we can merge these two worlds, providing a first attempt in this
direction in an offline but quite extreme setting. In particular, we
considered a scenario where, during training, we are given a set of
images of multiple domains and semantic categories and our goal is to
build a model able to recognize images of unseen concepts, as in
zero-shot learning (ZSL), in unseen domains, as in domain-generalization
(DG). This novel problem, which we called ZSL under DG (ZSL+DG), poses
novel research questions going beyond the ones posed by the DG and ZSL
problems if taken in isolation. For instance, similarly to DG, we can
rely on the fact that the multiple source domains permit to disentangle
semantic and domain-specific information. However, differently from DG,
we have no guarantee that the disentanglement will hold for the unseen
semantic categories at test time. Moreover, while in ZSL it is
reasonable to assume that the learned mapping between images and
semantic attributes will generalize also to test images of the unseen
concepts, in ZSL+DG we have no guarantee that this will happen for
images of unseen domains. In Section 4.1 we provide a formal definition
of the problem, while in Sec. 4.2 we review the related works in the
zero-shot learning literature and domain generalization. In Section 4.3
we provide a first solution to this problem by designing a curriculum
strategy based on the mixup [ zhang2017mixup ] algorithm. In particular,
we use mixup both at the input and feature level to simulate the domain
shift and semantic shift the network will encounter at test time.
Experiments show how this approach is effective in both ZSL, DG, and the
two tasks together, producing one of the first attempts for recognizing
unseen categories in unseen domains.

### 4.1 Problem statement

Overview. As highlighted in Chapter 1 , most existing deep visual models
are based on the assumptions that (a) training and test data come from
the same underlying distribution, i.e. domain shift, and (b) the set of
classes seen during training constitute the only classes that will be
seen at test time, i.e. semantic shift. These assumptions rarely hold in
practice and, in addition to depicting different semantic categories,
training and test images may differ significantly in terms of visual
appearance in the real world.

Up to now, we have presented approaches that tackle these problems in
isolation. In particular, in Chapter 2 , we have considered the case
where training and test distribution changes, addressing the domain
shift problem, starting from the assumption of having target data
available (Section 2.4 ), and removing it in the more complex domain
generalization (Section 2.5 ), continuous (Section 2.6 ) and predictive
domain adaptation (Section 2.7 ). However, in all the works we assumed
the output space to be constant after the initial training stage and
shared between training and test times.

On the other hand, in Chapter 3 , we considered the case where the
semantic space of a model is extended over time, as new training data
arrives, but without the presence of the domain shift problem. In fact,
while in Multi-Domain Learning (Section 3.3 ), a single model is asked
to tackle different classification tasks in different visual domains, we
have full supervision in each of the domains, and no unseen domain is
received at test time. Similarly, in Incremental Learning (Section 3.4 )
and Open World Recognition (Section 3.5 ), we consider a single data
distribution during all training steps and little to no shift at test
time.

In this chapter we focus on a different problem, considering the two
shifts occurring jointly at test time. In particular, our goal is
recognizing new semantic categories in new domains, without any of the
categories and domains being present in our initial training set. In
terms of the domain shift, we will consider the problem from a DG
perspective (i.e. data of the target domain are not present during
training while multiple sources are available). For the semantic shift,
we will consider the problem as Zero-Shot Learning (ZSL) [
xian2018zeroshotgood ] . In ZSL, the goal is to recognize objects unseen
during training given no data but external information about the novel
classes provided in forms of semantic attributes [ lampert2013awa ] ,
visual descriptions [ akata2015evaluation ] or word embeddings [
mikolov2013efficient ] . We consider this problem because allows us to
decouple semantic and domain shift, without considering other problems
(e.g. catastrophic forgetting, see Section 3.2 ). Moreover, we will
start by considering a ZSL scenario (i.e. at test time we want to
recognize only unseen classes) and not the generalized ZSL one [
xian2018zeroshotgood ] (where both seen and unseen categories must be
recognized) because this allows us to sidestep the inherent bias our
model would have on seen classes, focusing solely on the domain and
semantic shifts.

To clarify the setting, let us consider the case depicted in Fig. 4.1 .
A system trained to recognize elephants and horses from realistic images
and cartoons might be able to recognize the same categories in another
visual domain, like art paintings (Fig. 4.1 , bottom) or it might be
able to describe other quadrupeds in the same training visual domains
(Fig. 4.1 , top). On the other hand, how to deal with the case where new
animals are shown in a new visual domain is not clear. We want to remark
that, while the one of Fig. 4.1 is a toy example, the need for a
holistic approach jointly recognizing unseen categories in unseen
domains comes from the large variability of the real world itself. Since
it is impossible to construct a training set containing such
variability, we cannot train a model to be robust to all the possible
environments and semantic inputs it might encounter. Addressing these
two problems together, allows our models to be more robust to these
variabilities. Applications, where we need such robustness, are
countless. For example, given a robot manipulation task we cannot
forecast a priori all the possible conditions (e.g. environments,
lighting) it will be employed in. Moreover, we might have data only for
a subset of objects we want to recognize while only descriptions for the
others.

To our knowledge, our work [ mancini2020dgzsl ] is the first attempt to
answer this question, proposing a method that is able to recognize
unseen semantic categories in unseen domains . In particular, our goal
is to jointly tackle ZSL and DG (see Fig. 4.1 ). ZSL algorithms usually
receive as input a set of images with their associated semantic
descriptions, and learn the relationship between an image and its
semantic attributes. Likewise, DG approaches are trained on multiple
source domains and at test time are asked to classify images, assigning
labels within the same set of source categories but in an unseen target
domain. Here we want to address the scenario where, during training, we
are given a set of images of multiple domains and semantic categories
and our goal is to build a model able to recognize images of unseen
concepts, as in ZSL, in unseen domains, as in DG.

To achieve this, we need to address challenges usually not present when
these two classical tasks, i.e. ZSL and DG, are considered in isolation.
For instance, while in DG we can rely on the fact that the multiple
source domains permit to disentangle semantic and domain-specific
information, in ZSL+DG we have no guarantee that the disentanglement
will hold for the unseen semantic categories at test time. Moreover,
while in ZSL it is reasonable to assume that the learned mapping between
images and semantic attributes will generalize also to test images of
the unseen concepts, in ZSL+DG we have no guarantee that this will
happen for images of unseen domains.

To overcome these issues, during training we simulate both the semantic
and the domain shift we will encounter at test time. Since explicitly
generating images of unseen domains and concepts is an ill-posed
problem, we sidestep this issue and we synthesize unseen domains and
concepts by interpolating existing ones. To do so, we revisit the mixup
[ zhang2017mixup ] algorithm as a tool to obtain partially unseen
categories and domains. Indeed, by randomly mixing samples of different
categories we obtain new samples which do not belong to a single one of
the available categories during training. Similarly, by mixing samples
of different domains, we obtain new samples which do not belong to a
single source domain available during training.

Under this perspective, mixing samples of both different domains and
classes allows to obtain samples that cannot be categorized in a single
class and domain of the one available during training, thus they are
novel both for the semantic and their visual representation. Since
higher levels of abstraction contain more task-related information, we
perform mixup at both image and feature level, showing experimentally
the need for this choice. Moreover, we introduce a curriculum-based
mixing strategy to generate increasingly complex training samples. We
show that our CuMix ( Cu rriculum Mix up for recognizing unseen
categores in unseen domains) model obtains state-of-the-art performances
in both ZSL and DG in standard benchmarks and it can be effectively
applied to the combination of the two tasks, recognizing unseen
categories in unseen domains. ¹ ¹ 1 The code is available at
https://github.com/mancinimassimiliano/CuMix

To summarize, the contributions of this chapter are: (i) We introduce
the ZSL+DG scenario, a first step towards recognizing unseen categories
in unseen domains. (ii) We describe CuMix , the first holistic method
able to address ZSL, DG, and the two tasks together. Our method is based
on simulating new domains and categories during training by mixing the
available training domains and classes both at image and feature level.
The mixing strategy becomes increasingly more challenging during
training, in a curriculum fashion. (iii) Through our extensive
evaluations and analysis, we show the effectiveness of CuMix in all
three settings: namely ZSL, DG and ZSL+DG.

Problem statement. In this chapter, we will considered the ZSL+DG
problem. Differently from the incremental learning methods presented in
3 , here we assume that our new semantic concepts is not available in
the form of a training set, but is contained in a semantic descriptor
which we receive at test time. Using the semantic descriptors for
training classes, we can learn how to match visual features,
generalizing their available during training, we can match In the ZSL+DG
problem, the goal is to recognize unseen categories (as in ZSL) in
unseen domains (as in DG). Formally, let @xmath denote the input space
(e.g. the image space), @xmath the set of possible classes and @xmath
the set of possible domains. During training, we are given a set @xmath
where @xmath , @xmath and @xmath . Note that @xmath and @xmath and, as
in standard DG, we have multiple source domains (i.e. @xmath , with
@xmath ) with different distributions i.e. @xmath , @xmath . For
simplicity, in this section we assume to have exact knowledge about the
domain label of each sample.

In the ZSL+DG problem, the goal is to recognize unseen categories (as in
ZSL) in unseen domains (as in DG). Formally, let @xmath denote the input
space (e.g. the image space), @xmath the set of possible classes and
@xmath the set of possible domains. During training, we are given a set
@xmath where @xmath , @xmath and @xmath . Note that @xmath and @xmath
and, as in standard DG, we have multiple source domains (i.e. @xmath ,
with @xmath ) with different distributions i.e. @xmath , @xmath . Given
@xmath our goal is to learn a function @xmath mapping an image @xmath of
domains @xmath to its corresponding label in a set of classes @xmath .
Note that in standard ZSL, while the set of train and test domains are
shared, i.e. @xmath , the label sets are disjoint i.e. @xmath , thus
@xmath is a set of unseen classes. On the other hand, in DG we have a
shared output space, i.e. @xmath , but a disjoint set of domains between
training and test i.e. @xmath , thus @xmath is a set of unseen domains.
Since the goal of our work is to recognize unseen classes in unseen
domains, we unify the settings of DG and ZSL, considering both semantic-
and domain shift at test time i.e. @xmath and @xmath .

### 4.2 Related Works

In this section, we review related works in ZSL, and works trying to
perform DA and/or DG with techniques linked to the mixup algorithm which
serves as the base for our method. We will also describe works
addressing ZSL under domain shift and/or DG with different semantic
spaces, highlighting the differences with our setting.

Zero-Shot Learning (ZSL). Traditional ZSL approaches attempt to learn a
projection function mapping images/visual features to a semantic
embedding space where classification is performed. This idea is achieved
by directly predicting image attributes e.g. [ lampert2013awa ] or by
learning a linear mapping through margin-based objective functions [
akata2013label , akata2015evaluation ] . Other approaches explored the
use of non-linear multi-modal embeddings [ xian2016latent ] ,
intermediate projection spaces [ zhang2015zero , zhang2016zero ] or
similarity-based interpolation of base classifiers [
changpinyo2016synthesized ] . Recently, various methods tackled ZSL from
a generative point of view considering Generative Adversarial Networks [
xian2018feature ] , Variational Autoencoders (VAE) [
schonfeld2019generalized ] or both of them [ xian2019fvaegan ] . While
none of these approaches explicitly tackled the domain shift, i.e.
visual appearance changes among different domains/datasets, various
methods proposed to use domain adaptation technique, e.g. to refine the
semantic embedding space, aligning semantic and projected visual
features [ schonfeld2019generalized ] or, in transductive scenarios, to
cope with the inherent domain shift existing among the appearance of
attributes in different categories [ Kodirov_2015_ICCV ,
fu2015transductive , gan2016learning ] . For instance, in [
schonfeld2019generalized ] a distance among visual and semantic
embedding projected in the VAE latent space is minimized. In [
Kodirov_2015_ICCV ] the problem is addressed through a regularised
sparse coding framework, while in [ fu2015transductive ] a multi-view
hypergraph label propagation framework is introduced.

Recently, works have considered also coupling ZSL and DA in a
transductive setting. For instance, in [ zhuo2019unsupervised ] a
semantic guided discrepancy measure is employed to cope with the
asymmetric label space among source and target domains. In the context
of image retrieval, multiple works addressed the sketch-based image
retrieval problem [ yelamarthi2018sketch , Dutta_2019_CVPR ] , even
across multiple domains. In [ thong2019open ] the authors proposed a
method to perform cross-domain image retrieval by training
domain-specific experts. While these approaches integrated DA and ZSL,
none of them considered the more complex scenario of DG, where no target
data are available.

Simulating the Domain Shift for Domain Generalization. As highlighted in
Section 2.2 , multiple research efforts have been recently devoted into
addressing the domain generalization problem. Here we will recall some
of them that are linked to the idea behind of the approach we will
present in the next section. For a more detailed overview of DG works,
we ask the reader to refer to Section 2.2 .

In particular, since we mix samples to simulate new domains, our
approach is linked with data and feature augmentation strategies for DG
[ shankar2018generalizing , volpi2018generalizing , volpi2019addressing
] . Among them, we can distinguish two main categories:
adversarial-based [ shankar2018generalizing , volpi2018generalizing ,
zhou2020deep , zhou2020learning ] , trying to simulate novel domains
through adversarial perturbations of the original input, and data
augmentation-based [ volpi2019addressing ] , which determines which
augmentations to perform in order to improve the generalization
capabilities of the model. Differently from these methods, we will
specifically employ mixup to perturb input and feature representations.

Similarly, the fact that mixed samples are made increasingly more
difficult during training, has a link with episodic strategies for
domain generalization, such as [ li2019episodic ] . In [ li2019episodic
] , the authors describe a DG procedure which is based on multiple
domain-specific and one domain-agnostic networks. During training, a
domain-specific feature extractor receives as input images of different
domains (i.e. with a different distributions) that the domain agnostic
predictor is asked to correctly classify. Vice-versa, the
domain-agnostic feature extractor must learn to extract features which
even a domain-specific classifier of a different domain (with respect to
the one of the input image) should correctly classify. In this way, the
domain-agnostic components learn to cope with domain shift in their
inputs, similarly to what they will experience at test time. In our
method, we will not require domain-specific components, but we will
simulate the domain shift by gradually increasing the challenge posed by
the mixed samples.

Recently, works have considered mixup in the context of domain
adaptation [ xu2019adversarial ] to e.g. reinforce the judgments of a
domain discrimination. However, we employ mixup from a different
perspective i.e. simulating semantic and domain shift we will encounter
at test time. To this extent, we are not aware of previous methods using
mixup for DG and ZSL.

Finally, works have recently considered the heterogeneous domain
generalization (HDG) problem [ li2019episodic , li2019feature ] . The
goal of HDG is to train a feature extractor able to produce useful
representations for novel domains and novel categories [ li2019feature ]
. The novel domains have their specific output space (as in MDL, see
Section 3.3 ). Despite data of novel domains and classes are not present
during the feature extractor training phase, data of the novel domains
are required to train a classifier for the new domains/categories on top
of the agnostic feature extractor. Our ZSL+DG is different since we
assume that a model is trained once and uses side information (e.g. word
embeddings) to classify unseen categories in unseen domains at the test
time, without any training samples for new domains and categories.

### 4.3 Recognizing Unseen Categories in Unseen Domains222M. Mancini, Z.
Akata, E. Ricci, B. Caputo. Towards Recognizing Unseen Categories in
Unseen Domains. European Computer Vision Conference (ECCV) 2020.

#### 4.3.1 Preliminaries

From the definitions of Section 4.1 , we recall that our goal is to
learn a function @xmath mapping an image @xmath of unseen domains @xmath
to its corresponding label in a set of unseen classes @xmath .

In the following we divide the function @xmath into three parts: @xmath
, mapping images into a feature space @xmath , i.e. @xmath , @xmath
going from @xmath to a semantic embedding space @xmath , i.e. @xmath ,
and an embedding function @xmath where @xmath during training and @xmath
at test time. Note that @xmath is a learned classifier for DG while it
is a fixed semantic embedding function in ZSL, mapping classes into
their vectorized representation extracted from external sources. Given
an image @xmath , the final class prediction is obtained as follows:

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

In this formulation, @xmath can be any learnable feature extractor (e.g.
a deep neural network), while @xmath any ZSL predictor (e.g. a semantic
projection layer, as in [ xian2019semantic ] or a compatibility function
among visual features and labels, as in [ akata2013label ,
akata2015evaluation ] ). The first solution to address the ZSL+DG
problem could be training a classifier using the aggregation of data
from all source domains. In particular, for each sample we could
minimize a loss function of the form:

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

with @xmath an arbitrary loss function, e.g. the cross-entropy loss. In
the following, we show how we can use the input to Eq. ( 4.2 ) to
effectively recognize unseen categories in unseen domains.

#### 4.3.2 Simulating Unseen Domains and Concepts through Mixup

The fundamental problem of ZSL+DG is that, during training, we have
neither access to visual data associated to categories in @xmath nor to
data of the unseen domains @xmath . One way to overcome this issue in
ZSL is to generate samples of unseen classes by learning a generative
function conditioned on the semantic embeddings in @xmath [
xian2018feature , xian2019fvaegan ] . However, since no description is
available for the unseen target domain(s) in @xmath , this strategy is
not feasible in ZSL+DG. On the other hand, previous works on DG proposed
to synthesize images of unseen domains through adversarial strategies of
data augmentation [ volpi2018generalizing , shankar2018generalizing ] .
However, these strategies are not applied to ZSL since they cannot
easily be extended to generate data for unseen semantic categories
@xmath .

To circumvent this issue, we introduce a strategy to simulate, during
training, novel domains and semantic concepts by interpolating from the
ones available in @xmath and @xmath . Simulating novel domains and
classes allows to train the network to cope with both semantic- and
domain shift, the same situation our model will face at test time. Since
explicitly generating inputs of novel domains and categories is a
complex task, in this section we propose to achieve this goal, by mixing
images and features of different classes and domains, revisiting the
popular mixup [ zhang2017mixup ] strategy.

In practice, given two elements @xmath and @xmath of the same space
(e.g. @xmath ), mixup [ zhang2017mixup ] defines a mixing function
@xmath as follows:

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

with @xmath sampled from a beta distribution, i.e. @xmath , with @xmath
an hyperparameter. Given two samples @xmath and @xmath randomly drawn
from a training set @xmath , a new loss term is defined as:

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

where @xmath is the one-hot vectorized representation of label @xmath .
Note that, when mixing two samples and label vectors with @xmath , a
single @xmath is drawn and applied within @xmath in both image and label
spaces. The loss defined in Eq.( 4.4 ) forces the network to disentangle
the various semantic components (i.e. @xmath and @xmath ) contained in
the mixed inputs (i.e. @xmath and @xmath ) plus the ratio @xmath used to
mix them. This auxiliar task acts as a strong regularizer that helps the
network to e.g. being more robust against adversarial examples [
zhang2017mixup ] . Note however that the function @xmath creates input
and targets which do not represent a single semantic concept in @xmath
but contains characteristics taken from multiple samples and categories,
synthesising a new semantic concept from the interpolation of existing
ones.

For recognizing unseen concepts in unseen domains at test time, we
revisit @xmath to obtain both cross-domain and cross-semantic mixes
during training, simulating both semantic- and domain shift. While
simulating the semantic shift is a by-product of the original mixup
formulation, here we explicitly revisit @xmath in order to perform
cross-domain mixups. In particular, instead of considering a pair of
samples from our training set, we consider a triplet @xmath , @xmath and
@xmath . Given @xmath , the other two elements of the triplet are
randomly sampled from @xmath , with the only constraint that @xmath and
@xmath . In this way, the triplet contains two samples of the same
domain (i.e. @xmath ) and a third of a different one (i.e. @xmath ).
Then, our mixing function @xmath is defined as follows:

  -- -------- -- -------
     @xmath      (4.5)
  -- -------- -- -------

with @xmath sampled from a Bernoulli distribution @xmath and @xmath
representing either the input @xmath or the vectorized version of the
label @xmath , i.e. @xmath . Note that we introduced a term @xmath which
allows to perform either intra-domain (with @xmath ) or cross-domain
(with @xmath ) mixes.

To learn a feature extractor @xmath and a semantic projection layer
@xmath robust to domain- and semantic shift, we propose to use @xmath to
simulate both samples and features of novel domains and classes during
training. Namely, we simulate the semantic- and domain shift at two
levels, i.e. image and class levels. Given a sample @xmath we define the
following loss:

  -- -------- -- -------
     @xmath      (4.6)
  -- -------- -- -------

where @xmath , @xmath , @xmath are randomly sampled from @xmath , with
@xmath and @xmath . The loss term in Eq. ( 4.6 ) enforces the feature
extractor to effectively process inputs of mixed domains/semantics
obtained through @xmath . Inspired by [ verma2019manifold ] , we design
an additional loss acting at the classification level, by enforcing the
semantic consistency of mixed features in @xmath . This loss term is
defined as:

  -- -- -- -------
           (4.7)
  -- -- -- -------

where, as before, @xmath , with @xmath and @xmath and @xmath is a
generic loss function e.g. the cross-entropy loss. This second loss term
forces the classifier @xmath and the semantic projection layer @xmath to
be robust to features with mixed domains and semantics.

While we can simply use a fixed mixing function @xmath , as defined in
Eq. ( 4.5 ), for Eq. ( 4.6 ) and Eq. ( 4.7 ), we found that it is more
beneficial to devise a dynamic @xmath which changes its behaviour during
training, in a curriculum fashion. Intuitively, minimizing the two
objectives defined in Eq.( 4.6 ) and Eq.( 4.7 ) requires our model to
correctly disentangle the various semantic components used to form the
mixed samples. While this is a complex task even for intra-domain mixes
(i.e. when only the semantic is mixed), mixing samples across domains
makes the task even harder, requiring to isolate also domain-specific
factors. To effectively tackle this task, we choose to act on the mixing
function @xmath . In particular, we want our @xmath to create mixed
samples with progressively increased degree of mixing both with respect
to content and domain, in a curriculum-based fashion.

During training we regulate both @xmath (weighting the probability of
cross-domain mixes) and @xmath (modifying the probability distribution
of the mix ratio @xmath ), changing the probability distribution of the
mixing ratio @xmath and of the cross-domain mix @xmath . In particular,
given a warm-up step of @xmath epochs and being @xmath the current epoch
we set @xmath , with @xmath as hyperparameter, while @xmath . As a
consequence, the learning process is made of three phases, with a smooth
transition among them. We start by solving the plain classification task
on a single domain (i.e. @xmath , @xmath , @xmath ). In the subsequent
step ( @xmath ) samples of the same domains are mixed randomly, with
possibly different semantics (i.e. @xmath , @xmath ). In the third phase
( @xmath ), we mix up samples of different domains (i.e. @xmath ),
simulating the domain shift the predictor will face at test time. Figure
4.2 , shows a representation of how @xmath varies during training (top,
white block).

Final objective. The full training procedure, is represented in Figure
4.2 . Given a training sample @xmath , we randomly draw other two
samples, @xmath and @xmath , with @xmath and @xmath , feed them to
@xmath and obtain the first mixed input. We then feed @xmath , @xmath ,
@xmath and the mixed sample through @xmath , to extract their respective
features. At this point we use features extracted from other two
randomly drawn samples (in the figure, and just for simplicity, @xmath
and @xmath with same mixing ratios @xmath and @xmath ), to obtain the
feature level mixed features needed to build the objective in Eq.( 4.7
). Finally, the features of @xmath and the two mixed variants at image
and feature level, are fed to the semantic projection layer @xmath ,
which maps them to the embedding space @xmath . At the same time, the
labels in @xmath are projected in @xmath through @xmath . Finally, the
objectives defined in Eq.( 4.2 ),Eq.( 4.6 ) and Eq.( 4.7 ) functions are
then computed in the semantic embedding space. Our final objective is:

  -- -------- -- -------
     @xmath      (4.8)
  -- -------- -- -------

with @xmath and @xmath hyperparameters weighting the importance of the
two terms. As @xmath in both @xmath , @xmath and @xmath , we use the
standard cross-entropy loss, even if any ZSL objective can be applied.
Finally, we highlight that the optimization is performed batch-wise,
thus also the sampling of the triplet considers the current batch and
not the full training set @xmath . Moreover, while in Figure 4.2 we show
for simplicity that the same samples are drawn for @xmath and @xmath ,
in practice, given a sample, the random sampling procedure of the other
two members of the triplet is held-out twice, one at the image level and
one at the feature level. Similarly, the sampling of the mixing ratios
@xmath and cross domain factor @xmath of @xmath is held-out sample-wise
and twice, one at image level and one at feature level. As in Eq. ( 4.3
), @xmath and @xmath are kept fixed across mixed inputs/features and
their respective targets in the label space.

Discussion. We present similarities between our CuMix framework with DG
and ZSL methods. In particular, presenting the classifier with noisy
features extracted by a non-domain specialist network, has a similar
goal as the episodic strategy for DG described in [ li2019episodic ] .
On the other hand, here we sidestep the need to train domain experts by
directly presenting as input to our classifier features of novel domains
that we obtain by interpolating the available sources samples. Our
method is also linked to mixup approaches developed in DA [
xu2019adversarial ] . Differently from them, we use mixup to simulate
unseen domains rather then to progressively align the source to the
given target data.

Our method is also related to ZSL frameworks based on feature generation
[ xian2018feature , xian2019fvaegan ] . While the quality of our
synthesized samples is lower since we do not exploit attributes for
conditional generation, we have a lower computational cost. In fact,
during training we simulate the test-time semantic shift without
generating samples of unseen classes. Moreover, we do not require
additional training phases on the generated samples or the availability
of unseen class attributes to be available beforehand.

#### 4.3.3 Experimental results

##### Datasets and implementation details

We assess CuMix in three scenarios: ZSL, DG and the proposed ZSL+DG
setting.

ZSL . We conduct experiments on four standard benchmarks:
Caltech-UCSD-Birds 200-2011 (CUB) [ welinder2010cub ] , SUN attribute
(SUN) [ patterson2012sun ] , Animals with Attributes (AWA) [
lampert2013awa ] and Oxford Flowers (FLO) [ nilsback2008flo ] . CUB
contains 11,788 images of 200 bird species, with 312 attributes, SUN
14,430 images of 717 scenes annotated with 102 attributes, and AWA
30,475 images of 50 animal categories with 85 attributes. Finally, FLO
is a fine-grained dataset of flowers, containing 8,189 images of 102
categories. As semantic representation, we use the visual descriptions
of [ reed2016learning ] , following [ xian2018feature , xian2019semantic
] . For each dataset, we use the train, validation and test split
provided by [ xian2018zeroshotgood ] . In all the settings we employ
features extracted from the second-last layer of a ResNet-101 [
he2016deep ] pre-trained on ImageNet as image representation. For CuMix
, we consider @xmath as the identity function and as @xmath a simple
fully connected layer, performing our version of mixup directly at the
feature level while applying our alignment loss in the embedding space.
All hyperparameters have been set following [ xian2018zeroshotgood ] .

DG . We perform experiments on the PACS dataset [ li2017deeper ] with
9,991 images of 7 semantic classes in 4 different visual domains, art
paintings , cartoons , photos and sketches . For this experiment we use
the standard train and test split defined in [ li2017deeper ] , with the
same validation protocol. We use as base architecture a ResNet-18 [
he2016deep ] pre-trained on ImageNet. For our model, we consider @xmath
to be the ResNet-18 while @xmath to be the identity function. We use the
same training hyperparameters and protocol of [ li2019episodic ] .

ZSL+DG . Since no previous work addressed the problem of ZSL+DG, there
is no benchmark on this task. As a valuable benchmark, we choose
DomainNet [ peng2019moment ] , a recently introduced dataset for
multi-source domain adaptation [ peng2019moment ] with a large variety
of domains, visual concepts and possible descriptions. It contains
approximately 600’000 images from 345 categories and 6 domains, clipart
, infograph , painting , quickdraw , real and sketch .

To convert this dataset from a DA to a ZSL scenario, we need to define
an unseen set of classes. Since CuMix uses a network pre-trained on
ImageNet [ russakovsky2015imagenet ] , the set of unseen classes can not
contain any of the classes present in ImageNet following the good
practices in [ xian2017zero ] . We build our validation + test set with
100 classes that contain at least 40 images per domain and that has no
overlap with ImageNet. We reserve 45 of these classes for the unseen
test set, matching the number used in [ thong2019open ] , and the
remaining 55 classes for the unseen validation set. The remaining 245
classes are used as seen classes during training.

We set the hyperparameters of each method by training on all the images
of the seen classes on a subset of the source domains and validating on
all the images of the validation set from the held-out source domain.
After the hyperparameters are set, we retrain the model on the training
set, i.e. 245 classes, and validation set, i.e. 55 classes, of a total
number of 300 classes. Finally, we report the final results on the 45
unseen classes. As semantic representation we use word2vec embeddings [
mikolov2013efficient ] extracted from the Google News corpus and L2
-normalized, following [ thong2019open ] . For all the baselines and our
method, we employ as base architecture a ResNet-50 [ he2016deep ]
pre-trained on ImageNet, using the same number of epochs and SGD with
momentum as optimizer, with the same hyperparameters of [ thong2019open
] .

##### Results

ZSL. In the ZSL scenario, we choose as baselines standard inductive
methods plus more recent approaches. In particular we report the results
of ALE [ akata2013label ] , SJE [ akata2015evaluation ] , SYNC [
changpinyo2016synthesized ] , GFZSL [ verma2017simple ] and SPNet [
xian2019semantic ] . ALE [ akata2013label ] and SJE [
akata2015evaluation ] are linear compatibility methods using a ranking
loss and the structural SVM loss respectively. SYNC [
changpinyo2016synthesized ] learns a mapping from the feature space and
the semantic embedding space by means of phantom classes and a weighted
graph. GFZSL [ verma2017simple ] employs a generative framework where
each class-conditional distribution is modeled as a multivariate
Gaussian. Finally, SPNet [ xian2019semantic ] learns a semantic
projection function from the feature space through the image embedding
space by minimizing the standard cross-entropy loss.

Our results grouped by datasets are reported in Figure 4.3 . Our model
achieves performance either superior or comparable to the state of the
art in all benchmarks but AWA. We believe that in AWA learning a better
alignment between visual features and attributes may not be as effective
as improving the quality of the visual features. Especially, although
the names of the test classes do not appear in the training set of
ImageNet, for AWA being a non-fine-grained dataset, the information
content of the test classes is likely represented by the ImageNet
training classes. Moreover, for non-fine-grained datasets, finding
labeled training data may not be as challenging as it is in fine-grained
datasets. Hence, we argue that zero-shot learning is of higher practical
interest in fine-grained settings. Indeed CuMix is effective in
fine-grained scenarios (i.e. CUB, SUN, FLO) where it consistently
outperforms the state-of-the-art approaches.

These results show that our model based on mixup achieves competitive
performances on ZSL by simulating the semantic shift the classifier will
experience at test time. To this extent, our approach is the first to
show that mixup can be a powerful regularization strategy for the
challenging ZSL setting.

DG. The second series of experiments consider the standard DG scenario.
Here we test our model on the PACS dataset using a ResNet-18
architecture. As baselines for DG we consider the standard model trained
on all source domains together (AGG), the adversarial strategies in [
ganin2016domain ] (DANN) and [ shankar2018generalizing ] , the meta
learning-based strategy MLDG [ li2018learning ] and MetaReg [
balaji2018metareg ] . Moreover we consider the episodic strategy
presented in [ li2019episodic ] (Epi-FCR).

As shown in Table 4.1 , our model achieves competitive results
comparable to the state-of-the-art episodic strategy Epi-FCR [
li2019episodic ] . Remarkable is the gain obtained with respect to the
adversarial augmentation strategy CrossGrad [ shankar2018generalizing ]
. Indeed, synthesizing novel domains for domain generalization is an
ill-posed problem, since the concept of unseen domain is hard to
capture. However, with CuMix we are able to simulate inputs/features of
novel domains by simply interpolating the information available in the
samples of our sources. Despite containing information available in the
original sources, our approach produces a model more robust to domain
shift.

Another interesting comparison is against the self-supervised approach
JiGen [ carlucci2019domain ] . Similarly to [ carlucci2019domain ] we
employ an additional task to achieve higher generalization abilities to
unseen domains. While in [ carlucci2019domain ] the JigSaw puzzles [
noroozi2016unsupervised ] are used as a secondary self-supervised task,
here we employ the mixed samples/features in the same manner. The
improvement in the performances of CuMix highlights that recognizing the
semantic of mixed samples acts as a more powerful secondary task to
improve robustness to unseen domains.

Finally, it is worth noting that CuMix performs a form of episodic
training, similar to Epi-FCR [ li2019episodic ] . However, while Epi-FCR
considers multiple domain-specific architectures (to simulate the domain
experts needed to build the episodes), we require a single domain
agnostic architecture. We build our episodes by making the mixup among
images/features of different domains increasingly more drastic. Despite
not requiring any domain experts, CuMix achieves comparable performances
to Epi-FCR, showing the efficacy of our strategy to simulate unseen
domain shifts.

Ablation study. In this section, we ablate the various components of
CuMix. We performed the ablation on the PACS benchmark for DG, since
this allows us to show how different choices act on the generalization
to unseen domains. In particular, we ablate the following implementation
choices: 1) mixing samples at the image level, feature level or both 2)
impact of our curriculum-based strategy for mixing features and samples.

As shown in Table 4.2 , mixing samples at feature level produces a clear
gain on the results with respect to the baseline, while mixing samples
only at image level can even harm the performance. This happens
particularly in the sketch domain, where mixing samples at feature level
produces a gain of  2% while at image level we observe a drop of  10%
with respect to the baseline. This could be explained by mixing samples
at image level producing inputs that are too noisy for the network and
not representative of the actual shift experienced at test time. Mixing
samples at feature level instead, after multiple layers of abstractions,
allows to better synthesize the information contained in the different
samples, leading to more reliable features for the classifier. Using
both of them we obtain higher results in almost all domains.

Finally, we analyze the impact of the curriculum-based strategy for
mixing samples and features. As the table shows, adding the curriculum
strategy allows to boost the performances for the most difficult cases
(i.e. sketches) producing a further accuracy boost. Moreover, applying
this strategy allows to stabilize the training procedure, as
demonstrated experimentally.

ZSL+DG. On the proposed ZSL+DG setting we use the DomainNet dataset,
training on five out of six domains and reporting the average per-class
accuracy on the held-out one. We report the results for all possible
target domains but one, i.e. real photos, since our backbone has been
pre-trained on ImageNet, thus the photo domain is not an unseen one.
Since no previous methods addressed the ZSL+DG problem, in this section
we consider simple baselines derived from the literature of both ZSL and
DG. The first baseline is a standard ZSL model without any DG algorithm
(i.e. the standard AGG): as ZSL method we consider SPNet [
xian2019semantic ] . The second baseline is a DG approach coupled with a
ZSL algorithm. To this extent we select the state-of-the-art Epi-FCR as
the DG approach, coupling it with SPNet. As reference, we also evaluate
the performance of standard mixup coupled with SPNet.

As shown in Table 4.3 , CuMix achieves competitive performances in
ZSL+DG setting when compared to a state-of-the-art approach for DG
(Epi-FCR) coupled with a state-of-the-art one for ZSL (SPNet),
outperforming this baseline in almost all settings but sketch and, in
average by almost @xmath . Particularly interesting are the results on
the infograph and quickdraw domains. These two domains are the ones
where the shift is more evident as highlighted by the lower results of
the baseline. In these scenarios, our model consistently outperforms the
competitors, with a remarkable gain of more than 1.5% in average
accuracy per class with respect to the ZSL only baseline. We want to
highlight also that DomainNet is a challenging dataset, where almost all
standard DA approaches are ineffective or can even lead to negative
transfer [ peng2019moment ] . CuMix however is able to overcome the
unseen domain shift at test time, improving the performance of the
baselines in all scenarios. Our model consistently outperforms SPNet
coupled with the standard mixup strategy in every scenario. This
demonstrates the efficacy of the choices in CuMix for revisiting mixup
in order to recognize unseen categories in unseen domains.

#### 4.3.4 Conclusions

In this section, we proposed the novel ZSL+DG scenario. In this setting,
during training, we are given a set of images of multiple domains and
semantic categories and our goal is to build a model able to recognize
unseen concepts, as in ZSL, in unseen domains, as in DG. To solve this
problem we design CuMix, the first algorithm which can be holistically
and effectively applied to DG, ZSL, and ZSL+DG. CuMix is based on
simulating inputs and features of new domains and categories during
training by mixing the available source domains and classes, both at
image and feature level. Experiments on public benchmarks show the
effectiveness of CuMix, achieving state-of-the-art performances in
almost all settings in all tasks. Future works might investigate
alternative data-augmentation schemes in the ZSL+DG setting as well as
the use of novel formulations of the mixing functions. Moreover, it
would be interesting to extend CuMix to the more realistic
Generalized-ZSL scenario, where the model must recognize both seen and
unseen categories.

## Chapter 5 Conclusions and Future Works

### 5.1 Summary of contributions

In this thesis, we analyzed the capability of deep neural networks to
generalize to unseen input distributions and to include knowledge not
present in their initial training set, with the final goal of building
deep models able to recognize new/unseen categories in unseen visual
domains.

In Chapter 2 , we started by analyzing the problem from a perspective of
the input the network receives, considering scenarios where training
(source) and test (target) output spaces do not change but their input
distribution does. In particular, in Section 2.4 we considered the
problem of latent domain discovery in domain adaptation. In this
setting, we assume the availability of unlabeled target data during
training and that either source or target domains (or both) are a
mixture of multiple latent domains. In this context, we proposed the
first deep neural network able to work in this scenario. Our
architecture is made of two main components, namely novel multi-domain
alignment layers (mDA) and a domain prediction branch. The mDA layers
perform batch-normalization (BN) [ ioffe2015batch ] , extending previous
works on domain adaptation [ carlucci2017autodial , carlucci2017just ,
li2016revisiting ] , through weighted statistics, computed using the
domain probabilities extracted by the domain prediction branch. The
domain prediction branch relies on the assumption that similar inputs
should produce similar activations, and it is trained through a simple
entropy-loss, without requiring any domain label. Our results show that
our framework can successfully enable the deep model to discover latent
domains and outperform standard single-source methods.

As a second step, we removed the assumption of having target data
available during training by considering the domain generalization (DG)
scenario (Section 2.5 ). Here we build on the idea that improvements on
the performances of a DG model can be achieved by modeling the
similarity of a target sample to the available source domains. We thus
develop a simple extension to the latent domain discovery framework
which makes use of domain labels (if available) and of the domain
prediction branch at test time, to decide which of the source domains
should contribute more in the final decision. In particular, we use
domain-specific BN layers, weighting their activations using the
similarity that the target sample has with the source domains.
Experiments on robotics scenarios show the effectiveness of the approach
in multiple place categorization benchmarks under various domain shifts
(e.g. light conditions, seasons, environments) with and without the
presence of domain labels (Section 2.5.2 ). Subsequently, we extend this
solution to levels of the network different from BN layers. In
particular, we consider merging the activation of domain-specific
classifiers at test time, with their importance weighted again by the
similarity of a target sample to the source domains. We also explore the
use of different kinds of merging strategies, balancing domain-specific
features with domain-agnostic ones. Results show the effectiveness of
the approach against domain generalization baselines in standard
benchmarks (Section 2.5.4 ).

While DG is one of the domain adaptation scenario where target data are
not present, other settings can be considered, depending on the amount
of information we have about our target domain. Studying other aspects
of the problem, in Section 2.6 we focused on the continuous domain
adaptation scenario, where a single source domain is available during
training (without any target data) and adaptation must be performed
exploiting the incoming stream of target samples at test time, without
access to the original training set. In this context, we develop an
extension to the domain alignment layers [ carlucci2017autodial ,
carlucci2017just , li2016revisiting ] to tackle this problem. In
particular, we show how updating the statistics of BN layers using the
incoming stream of target data can be a simple yet effective strategy
for tackling this problem. We assess the performance of our model, ONDA,
on a robotic object classification task, collecting and releasing a
dataset for studying this unexplored problem, containing multiple
objects in various acquisition conditions.

Finally, we considered the predictive domain adaptation (PDA) scenario
where, during training, we have a single labeled source domain and
multiple unlabeled auxiliary domains, each of them with a description
(i.e. metadata) attached. The goal of this problem is to build a model
able to address the classification task in the target domain by using
just the target-specific metadata. We develop the first deep learning
model for this problem, AdaGraph (Section 2.7 ), which builds on a
graph, where each node is a domain with attached its domain-specific
parameters, and its edge is the distance among the metadata of the two
connected domains. At test time, given the target domain metadata, we
obtain the target-specific parameters through a weighted combination of
its closest nodes in the graph. Due to their simplicity and the easiness
of linearly combining them, we use domain-specific BN layers as
domain-specific parameters. To improve the estimated target statistics,
we also incorporate a continuous domain adaptation strategy into the
framework, extending the previously described ONDA algorithm.
Experiments show how our model outperforms standard PDA approaches, with
the continuous update strategy being surpassing state-of-the-art
approaches in continuous domain adaptation.

In Chapter 3 , we moved to the problem of extending the output space of
a pre-trained architecture to new semantic categories. We started by
analyzing the problem of multi-domain learning, where the goal is to add
new (classification) tasks to a pre-trained model without harming the
performance on old tasks and with as few task-specific parameters as
possible. Our contribution (Section 3.3 ) has been showing how affinely
transformed task-specific binary masks applied to the original network
weights can allow a network to learn multiple models with (i)
performances close to networks fine-tuned for the specific tasks and
(ii) with very little overhead in terms of the number of parameters
required for each task. We assess the performance of our model on the
challenging Visual Domain Decathlon, showing performance comparable with
more complicated/multi-stages state-of-the-art approaches.

In Section 3.4 , we focused on a different problem, incremental class
learning. In this task, we want to add new knowledge to a pre-trained
model without having access to the original training set, thus
addressing the catastrophic forgetting problem. We analyzed this task in
semantic segmentation discovering how the performance of standard
incremental learning algorithms is hampered by the change of the
semantic of the background class in different learning steps, a problem
which we named the background shift. Indeed, in a given learning step,
the background might contain pixels of classes learned in previous steps
as well as pixels of classes we will learn in future ones. We showed how
a simple modification of standard cross-entropy and distillation losses,
taking explicitly into account the different meaning of the background
across different learning steps and coupled with an ad-hoc
initialization procedure, can effectively address both catastrophic
forgetting and background shift, even in large-scale scenarios (e.g.
ADE-20k).

In the final section of Chapter 3 , we studied a more challenging
problem, open-world recognition (OWR). In this task, we must not only be
able to add new concepts to a pre-trained model, but also to detect
unknown concepts if received as inputs. In this scenario, we developed
DeepNNO (Section 3.5.3 ), the first end-to-end trainable model in OWR,
extending standard non-parametric algorithms [ bendale2015towards ] with
losses and training schemes preventing catastrophic forgetting. We then
showed how clustering-based objectives and trainable class-specific
rejection thresholds can further boost the performances of deep OWR
model (Section 3.5.4 ). The experiments on standard datasets and
robotics scenarios showed the efficacy of the two approaches and the
importance of each design choice. Moreover, we described and tested a
simple pipeline for Web-aided OWR, where knowledge about new classes is
not given by an external ’oracle’ but automatically retrieved from web
queries (Section 3.5.6 ). We believe our algorithms and our web-based
pipeline constitute a first meaningful step towards autonomously
learning real-world agents.

Finally, in Chapter 4 , we merged the two worlds, analyzing whether it
is possible to build a model recognizing unseen classes in unseen
domains. In particular, we described the problem of Zero-Shot Learning
(ZSL) under Domain Generalization (DG), where, during training, we are
given images of a set of classes in multiple source domains and, at test
time, we are asked to recognize different, unseen categories depicted in
unseen visual domains. In this scenario, we must learn how to map images
into a semantic embedding space of class descriptions (e.g. word
embeddings) making sure that the mapping generalizes both to unseen
semantic classes (addressing the semantic shift problem) and to unseen
domains (addressing the domain shift problem). We developed the first
simple solution to this problem based on mixup [ zhang2017mixup ] . In
particular, our idea was to simulate the shifts we will encounter at
test time by simulating samples (and features) of new domains and/or
categories by mixing the domains and classes available at training time.
Moreover, we made the mixes increasingly more challenging during
training by increasing both the probability of having a high mixing
ratio and of cross-domain mixing. Our approach, named CuMix, showed
remarkable results on ZSL, DG, and the proposed ZSL+DG, being not only
the first holistic approach for ZSL and DG but also the first model that
effectively recognizes unseen categories in unseen domains.

### 5.2 Open problems and future directions

While in this thesis we studied how to build deep learning models
generalizing to either new visual domains (Chapter 2 ) or new semantic
concepts (Chapter 3 ) or both (Chapter 4 ), multiple problems remain to
be addressed and multiple directions to be explored towards having
visual systems recognizing new semantic concepts in arbitrary visual
domains.

Starting with the proposed solutions, for each of the proposed
algorithms, we briefly discussed possible immediate extensions as well
as interesting research directions worth to be explored. For instance,
in Chapter 2 we discussed multiple solutions involving domain-specific
parameters whose activations are merged by using the weights obtained a
domain prediction branch. For all these solutions, it might be
interesting to investigate how to strengthen the domain classifier. For
instance, one can avoid the use of a domain-specific classifier but rely
on the distances of the activations from the various domain-specific
distributions (e.g. statistics of BN layers) as a measure of domain
similarity. On the other hand, stronger clustering objectives could be
applied as objectives for the domain prediction branch to strengthen the
discovering of the latent domains. Moreover, it would be interesting to
investigate if the algorithms of Chapter 2 can be extended to use
parameters beyond standard BN layers, as preliminary experiments with
classifiers in Section 2.5.4 . To this extent, also the affinely
transformed binary masks of Section 3.3 can be a good starting point for
having simple and easy to combine domain-specific parameters.

When tackling Predictive DA with AdaGraph (Section 2.7 ), we assign to
each domain a node in our graph. However, the current formulation has
two main drawbacks. First, it is not scalable if the set of possible
metadata descriptors increases; second it considers all the metadata
equally important for addressing the domain shift problem. Future works
might explore different strategies for including metadata-specific
information as well as modeling the importance of the different metadata
components. For instance, a possible solution could be to employ
domain-specific alignment layers per each metadata group (e.g.
viewpoint, year of production) and learning how to optimally recombine
their activations for the final prediction.

Other interesting research directions can be drawn from the works in
Chapter 3 . For instance, the first question is whether the multi-domain
algorithm presented in Section 3.3 can be applied to other scenarios
(e.g. incremental class learning) and even to tackle the domain shift
problem (e.g. PDA, DG). In the latter case, one can use one binary-mask
per network parameter per domain, combining them at test time based on
the target domain metadata (PDA) or similarity to the source domains
(DG). Another interesting question is whether the model can exploit the
relationships among the single task/domains through side connections, in
such a way that each task/domain benefits from the others.

For what concerns incremental learning in semantic segmentation, it
would be interesting to quantify the background shift and understanding
whether the different kinds of shift (e.g. background containing old
classes vs background containing classes we will learn in the future)
require more specific solutions than the general one we designed in
Section 3.4 . On the other hand, it would be interesting to verify if
the effectiveness of our MiB algorithm (or simple extensions)
generalizes to other problems where the semantic of the background is
uncertain, such as incremental learning in object detection [
shmelkov2017incremental ] and instance segmentation [ porzi2019seamless
] as well as non-incremental tasks such as weakly-supervised semantic
segmentation [ bearman2016whats ] , generalized zero-shot learning [
xian2019semantic ] , and dataset merging [ dmitriev2019learning ] .

In OWR, an interesting future work would be to quantify the robustness
of OWR algorithms to the domain shift. In this context, we could verify
how much their capabilities of detecting unknowns and recognizing known
concepts are affected by changes in the input distributions. Moreover, a
very important research direction would be improving the various
components of the web-based pipeline sketched in Section 3.5.6 . For
instance, we could develop a tool for automatic and robust labeling of
the detected unknowns. Moreover, we could design algorithms for
filtering the noisy web images retrieved and/or dealing with both noisy
labels and domain shift while learning the new categories. We believe
the latter being a promising direction towards having robotics visual
systems learning fully autonomously from the environment they interact
with.

Finally, in Chapter 4.3 , we introduce a new research problem (ZSL+DG)
and algorithm (CuMix) with the aim of encouraging the community towards
developing models tackling both domain and semantic shift together.
However, we believe that our ZSL+DG problem is just the beginning of
this journey. Indeed, in principle, we would like the semantic space of
our models to consider both seen and unseen categories (as in
Generalized ZSL [ xian2018zeroshotgood ] ) in arbitrary domains, while
requiring the minimum number of source domains possible (even just one,
as recent works in single-source domain generalization [
volpi2018generalizing , volpi2019addressing , qiao2020learning ] ).
Moreover, we might receive data for new classes over time, as in
incremental learning. In this case, we would like our model to recognize
old seen, new seen, and still unseen categories at test time. This would
require our model to address both semantic shift, with the relative bias
among the set of classes (as in GZSL), the catastrophic forgetting
problem [ french1999catastrophic ] and related ones, such as our
identified background shift. Moreover, if the data for the new classes
come from new domains, we want our model to also address the domain
shift problem. Despite that with CuMix and ZSL+DG we focused on a subset
of these problems, We believe that the contributions of Chapter 4 and
the findings of this thesis, will push researchers into exploring ways
to overcome both domain and semantic shift together, towards building
visual algorithms able to cope with the large and unpredictable
variability of the real world.

## Appendix A Recognition across New Visual Domains

### a.1 Latent Domain Discovery

#### a.1.1 mDA layers formulas

From Section 2.4 , we have the output of our mDA layer denoted by

  -- -------- -- -------
     @xmath      (A.1)
  -- -------- -- -------

where, for simplicity:

  -- -------- -- -------
     @xmath      (A.2)
  -- -------- -- -------

and the statistics are given by

  -- -------- -------- -------- -------
     @xmath   @xmath            (A.3)
              @xmath   @xmath
  -- -------- -------- -------- -------

where @xmath .

From the previous equations we can derive the partial derivative of the
loss function with respect to both the input @xmath and the domain
assignment probabilities @xmath . Let us denote @xmath the partial
derivative of the loss function @xmath with respect to the output @xmath
of the mDA layer. We have:

  -- -------- -------- -------- -------
     @xmath   @xmath            (A.4)
              @xmath   @xmath
  -- -------- -------- -------- -------

and

  -- -------- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath   @xmath      (A.5)
  -- -------- -------- -------- -------- -- -------

Thus, the partial derivative of @xmath w.r.t. the input @xmath is:

  -- -------- -- -------
     @xmath      (A.6)
  -- -------- -- -------

where:

  -- -------- -------- -------- -------
     @xmath   @xmath            (A.7)
              @xmath   @xmath
  -- -------- -------- -------- -------

For the domain assignment probabilities @xmath we have:

  -- -------- -------- -- --------
     @xmath   @xmath      (A.8)
     @xmath   @xmath      (A.9)
     @xmath   @xmath      (A.10)
  -- -------- -------- -- --------

Thus, the partial derivative of @xmath w.r.t. @xmath is:

  -- -------- -- --------
     @xmath      (A.11)
  -- -------- -- --------

where @xmath and @xmath are defined as in ( A.7 ).

#### a.1.2 Training loss progress

In this section, we plot the losses as the training progresses for the
Digits-five experiments. The plots are shown in Figure A.3 . For both
MNIST-m and SVHN, the classification loss smoothly decreases, while the
domain loss first decreases and then stabilizes around a fixed value.
This is a consequence of the introduced balancing term on the domain
assignments, which enforces the entropy to be low for the assignment of
a single sample, but high for the assignments averaged across the entire
batch. In Figures A.6 and A.9 we plot the single components of the
classification and domain loss respectively. For the semantic part
(Figure A.6 ), both the entropy loss on target sample and the
cross-entropy loss on source samples decrease smoothly. For the domain
assignment part (Figure A.9 ), we can see how the entropy loss on single
samples rapidly decreases, while the average batch assignment keeps an
high entropy, as expected. We highlight that when SVHN is used as
target, the source domains are a bit closer to each other in appearance,
thus the average batch entropy has a slightly lower value (i.e. the
assignments are less balanced) with respect to the MNIST-m as target
case.

Finally, it is worth noticing that the domain loss reaches a stable
value earlier than the classification components. This is a design
choice, since we want to learn a semantic predictor on stable and
confident domain assignments.

#### a.1.3 Additional Results on PACS

A crucial problem in domain adaptation rarely addressed in the
literature is how to tune model hyper-parameters. In fact, setting the
hyper-parameters values based on the performance on the source domain is
sub-optimal, due to the domain shift. Furthermore, assuming the presence
of a validation set for the target domain is not realistic in practice [
morerio2017minimal ] : in unsupervised domain adaptation we only assume
the presence of a set of unlabelled target data. Despite recent research
in this direction [ morerio2017minimal ] , there is no clear solution to
this problem in the literature. This problem is more severe in our case,
since it is not trivial to define a validation set for the latent domain
discovery problem, due to the assumption that multiple source and target
domains are mixed.

Nonetheless, for the sake of completeness, we analyze the performances
of our model and the baselines if we assume the presence of a target
validation set to perform model selection. We consider the PACS dataset,
in both the single and multi-target scenarios. The results are reported
in parenthesis in Table A.1 and in Table A.2 . While both our model and
the baselines obviously benefit from the validation set, the overall
trends remain the same, with our model achieving higher performances
with respect to the baseline and close to the multi-source upper bound.
Notice that a validation set is especially beneficial in the case of
consistent domain shift: for instance, all the methods increase their
results by almost 5% in Table A.1 when Sketch is the target domain.

As a final note, we underline that the use of a validation set on the
target domain for unsupervised domain adaptation is not a common
practice in the community, thus these results can be regarded as an
upper bound with respect to our model.

### a.2 Predictive Domain Adaptation

#### a.2.1 Metadata Details

CompCars. For the experiments with the CompCars dataset [ yang2015large
] , we have two domain information: the car production year and the
viewpoint. We encode the metadata through a 2-dimensional integer vector
where the first integer encodes the year of production (between 2009 and
2014) and the second the viewpoint. While encoding the production year
is straightforward, for the viewpoint we use the same criterion adopted
in [ yang2016multivariate ] , i.e. we encode the viewpoint through
integers between 1-5 in the order: Front , Front-Side , Side , Rear-Side
, Rear .

Portraits. For the experiments with the Portraits dataset [
ginosar2015century ] , we have again two domain information: the year
and the region where the picture has been taken. To allow for a bit more
precise geographical information we encode the metadata through a
3-dimensional integer vector.

As for the CompCars dataset, the first integer encodes the decade of the
image (8 decades between 1934 and 2014), while the second and third the
geographical position. For the geographical position we simplify the
representation through a coarse encoding involving 2 directions:
est-west (from 0 to 1) and north-south (from 0 to 3). In particular we
assign the following value pairs ([north-south, east-west]):
Mid-Atlantic @xmath , Midwestern @xmath , New England @xmath , Pacific
@xmath and Southern @xmath . Each component of the vector has been
normalized in the range 0-1.

#### a.2.2 Additional Analysis

##### ResNet-18 on CompCars

Here we apply AdaGraph to the ResNet-18 architecture in the CompCars
dataset [ yang2015large ] . As for the other experiments, we apply
AdaGraph by replacing each BN layer of the network with its GBN
counterpart.

The network is initialized with the weights of the model pretrained on
ImageNet. We train the network for 6 epochs on the source dataset,
employing Adam as optimizer with a weight decay of @xmath and a
batch-size of 16. The learning rate is set to @xmath for the classifier
and @xmath for the rest of the network and it is decayed by a factor of
10 after 4 epochs. We extract domain-specific parameters by training the
network for 1 epoch on the union of source and auxiliary domains,
keeping the same optimizer and hyperparameters. The batch size is kept
to 16, building each batch with elements of a single pair production
year-viewpoint belonging to one of the domains available during training
(either auxiliary or source).

The results are shown in Table A.3 . As the table shows, AdaGraph
largely increases the performance of the Baseline model. Coherently with
previous experiments, our refinement strategy is able to further
increase the performances of AdaGraph , filling almost entirely the gap
with the DA upper bound.

##### Performances vs Number of Auxiliary Domains

In this section, we analyze the impact of varying the number of
available auxiliary domains on the performances of our model. We employ
the ResNet-18 architecture on the Portraits dataset, with the same
setting and set of hyperparameters described in the experimental
section. However, differently from the previous experiments, we vary the
number of available auxiliary domains, from 1 to 38. We repeat the
experiments 20 times, randomly sampling the available auxiliary domains
each time.

The results are shown in Figure A.13 . As expected, increasing the
number of auxiliary domains leads to an increase in the performance of
the model. In general, as we have more than 20 domains available, the
performance of our model are close to the DA upper bound. While these
results obviously depend on the relatedness between the auxiliary
domains and the target, the plots show that having a large set of
auxiliary domains may not be strictly necessary for achieving good
performances.

## Appendix B Recognizing New Semantic Concepts

### b.1 Incremental Learning in Semantic Segmentation

#### b.1.1 How should we use the background?

As highlighted in Section 3.4 , an important design choice for
incremental learning in semantic segmentation is how to use the
background. In particular, since the background class is present both in
old and new classes, it can be considered either in the supervised
cross-entropy loss, in the distillation component or in both. For our
MiB method and all the baselines (LwF [ li2017learning ] , Lwf-MC [
rebuffi2017icarl ] , ILT [ michieli2019incremental ] ), we considered
the latter case (i.e. background in both). However, a natural question
arises on how different choices for the background would impact the
final results. In this section we investigate this point.

We start from the LwF-MC [ rebuffi2017icarl ] baseline, since it is
composed of multiple binary classifiers and allows to easy decouple
modifications on the background from the other classes. We then test two
variants:

-    LwF-MC-D ignores the background in the classification loss, using
    as target for the background the probability given by @xmath .

-    LwF-MC-C ignores the background in the distillation loss, using
    only the supervised signal from the ground-truth.

In Table B.1 and B.2 we report the results of the two variants for the
overlapped scenarios of the Pascal VOC dataset and the 50-50 scenario of
ADE20K respectively. Together with the two variants, we report the
results of our method (MiB), the offline training upper-bound (Joint)
and the LwF-MC version employed in Section 3.4.3 which uses the
background in both binary cross-entropy and distillation, blending the
two components with a hyper-parameter.

As the tables show, the three variants of Lwf-MC exhibit different
trade-offs among learning new knowledge and remembering the past one. In
particular, LwF-MC-C learns very well new classes, being always the most
performing variant on the last incremental step. However, it suffers a
significant drop in the old knowledge, showing its inability to tackle
the catastrophic forgetting problem.

LwF-MC-D shows the opposite trend. It maintains very well the old
knowledge, being the best variant in old classes for every setting.
However, it is very intransigent [ chaudhry2018riemannian ] i.e. it is
not able to correctly learn new classes, thus obtaining the worst
performances on them.

As expected, LwF-MC which considers the background in both cross-entropy
and distillation achieves a trade-off among learning new knowledge, as
in LwF-MC-C, while preserving the old one, as in LwF-MC-D.

As the tables show, our MiB approach models the background more
effectively, achieving the best trade-off among learning new knowledge
and preserving old concepts. In particular, our method is the best by a
margin in all scenarios for the new classes, while for old ones it is
either better or comparable to the performance of the intransigent
LwF-MC-D method. The only scenarios where it shows lower performances
are the multi-step ones. Indeed in these scenarios, the multiple
learning episodes make preserving old knowledge harder, and an
intransigent method is less prone to forgetting since it is biased to
old classes. However, the intransigence is not the right solution if the
number of old and new classes are balanced, as in the 50-50 scenario of
ADE20k, since the overall performances will be damaged.

#### b.1.2 Per class results on Pascal-VOC 2012

From Table B.3 to B.8 , we report the results for all classes of the
Pascal-VOC 2012 dataset. As the tables show, MiB achieves the best
results in the majority of classes (i.e. at least 14/20 in the 19-1
scenarios, 13/20 in the 15-5 and 16/20 in the 15-1 ones) being either
the second best or comparable to the top two in all the others.
Remarkable cases are the ones where we learn classes that are either
similar in appearance (e.g. bus and train) or appear in similar contexts
(e.g. sheep and cow): for those pairs, our model outperforms the
competitors by a margin in both old classes (i.e. bus and cow in the
15-5 and 15-1 scenarios) and new ones (i.e. sheep and train). These
results show the capability of MiB to not only learn new knowledge while
preserving the old one, but also to learn discriminative features for
difficult cases during different learning steps.

#### b.1.3 Validation protocol and hyper-parameters

In this work, we follow the protocol of [ de2019continual ] for setting
the hyper-parameters in continual learning. The protocol works in three
steps and does not require any data of old tasks. First, we split the
training set of the current learning step into train and validation
sets. We use @xmath of the data for training and @xmath for validation.
Note that the validation set contains only labels for the current
learning step.

Second, we set general hyper-parameters values (e.g. learning rate) as
the ones achieving the highest accuracy in the new set of classes with
the fine-tuned model. Since we tested multiple methods, we wanted to
ensure fairness in terms of hyper-parameters used, without producing
biased results. To this extent, this step is held out only once starting
from the fine-tuned model and fixing the hyper-parameters for all the
methods. In particular, we set the learning rate as @xmath for the
incremental steps in all datasets and settings.

As a final step, we set the hyper-parameters specific of the continual
learning method as the highest values (to ensure minimum forgetting)
with a tolerated decay on the performance on the new classes with
respect to the ones achieved by the fine-tuned model (to ensure maximum
learning). We set the tolerated decay as @xmath of the original
performances, exploring hyper-parameters values of the form @xmath ,
with @xmath and @xmath . We perform this validation procedure in the
first learning step of each scenario, keeping the hyper-parameters fixed
for the subsequent ones. Since this procedure is costly, we perform it
only for the Pascal-VOC dataset, keeping the hyper-parameters for the
large-scale ADE20k. As a result, for the prior focused methods, we
obtain a weight of @xmath for EWC [ kirkpatrick2017overcoming ] and PI [
zenke2017continual ] and @xmath for RW [ chaudhry2018riemannian ] in all
scenarios. For the data-focused methods we obtain a weight of @xmath for
the distillation loss of LwF [ li2017learning ] , @xmath for the one in
LwF-MC [ rebuffi2017icarl ] and @xmath for both distillation losses in
ILT [ michieli2019incremental ] , in all settings. For our MiB method,
we obtain a distillation loss weight of @xmath for all scenarios except
for the 15-1 in Pascal VOC, where the weight is set to @xmath .

## Appendix C Towards Recognizing Unseen Categories in Unseen Domains

### c.1 Recognizing Unseen Categories in Unseen Domains

#### c.1.1 Hyperparameter choices

In this section, we will provide additional details on the
hyperparameter choices and validation protocols, not included in Section
4.3 .

ZSL . For each dataset, we use the train, validation and test split
provided by [ xian2018zeroshotgood ] . In all the settings we employ
features extracted from the second-last layer of a ResNet-101 [
he2016deep ] pretrained on ImageNet as image representation, without
end-to-end training. For CuMix , we consider @xmath as the identity
function and as @xmath a simple fully connected layer, performing the
mixing directly at the feature-level while applying our alignment loss
in the embedding space (i.e. @xmath and @xmath coincide in this case and
are applied only once.) All hyperparameters have been set dataset-wise
following [ xian2018zeroshotgood ] , using the available validation
sets. For all the experiments, we use SGD as optimizer with an initial
learning rate equal to 0.1, momentum equal to 0.9, a weight-decay set to
0.001 for all settings but AWA, where is set 0. The learning-rate is
downscaled by a factor of ten after 2/3 of the total number of epochs
and @xmath . In particular, for CUB and FLO we train our model for
@xmath epochs, setting @xmath and @xmath for CUB, and @xmath and @xmath
for FLO. For AWA, we train our network for @xmath epochs, with @xmath
and @xmath . For SUN, we train our network for @xmath epochs, with
@xmath and @xmath . In all settings, the batch-size is set to 128.

DG. We use as base architecture a ResNet-18 [ he2016deep ] pretrained on
ImageNet. For our model, we consider @xmath to be the ResNet-18, @xmath
to be the identity function and @xmath will be a learned,
fully-connected classifier. We use the same training hyperparameters and
protocol of [ li2019episodic ] , setting @xmath , @xmath , @xmath and
@xmath .

ZSL+DG. For all the baselines and our method we employ as base
architecture a ResNet-50 [ he2016deep ] pretrained on ImageNet, using
SGD with momentum as optimizer, with a learning rate of @xmath for the
ZSL classifier and @xmath for the ResNet-50 backbone, a weight decay of
@xmath and momentum @xmath . We train the models for 8 epochs (each
epoch counted on the smallest source dataset), with a batch-size
containing 24 sample per domain. We decrease the learning rates by a
factor of @xmath after 6 epochs. For our model, we consider the backbone
as @xmath and a simple fully-connected layer as @xmath . We set @xmath ,
@xmath for all the experiments, while @xmath in @xmath and @xmath in
@xmath depending on the scenario.

#### c.1.2 ZSL+DG: analysis of additional baselines

In Table 4.3 , we showed the performance of our method in the new ZSL+DG
scenario on the DomainNet dataset [ peng2019moment ] , comparing it with
three baselines: SPNet [ xian2019semantic ] , simple mixup [
zhang2017mixup ] coupled with SPNet and SPNet coupled with EpiFCR [
li2019episodic ] , an episodic-based method for DG. We reported the
results of these baselines to show 1) the performance of a
state-of-the-art ZSL method (SPNet), 2) the impact of mixup alone (
mixup +SPNet) and 3) the results obtained by coupling state-of-the-art
models for DG and for ZSL together (EpiFCR+SPNet). We chose SPNet and
EpiFCR as state-of-the-art references for ZSL and DG respectively
because they are very recent approaches achieving high performances on
their respective scenarios.

In this section, we motivate our choices by showing that other baselines
of ZSL and DG achieve lower performances in this new scenario. In
particular we show the performances of two standard ZSL methods, ALE [
akata2013label ] and DEVISE [ frome2013devise ] and a standard DG/DA
method, DANN [ ganin2016domain ] . We choose DANN since it is a strong
baseline for DG on residual architectures, as shown in [ li2019episodic
] . As in Section 4.3 , we show the performances of the ZSL methods
alone, ZSL methods coupled with DANN, and with EpiFCR. For all methods,
we keep the same training hyperparameters, tuning only the
method-specific ones. The results are reported in Table C.1 . As the
table shows, CuMix achieves superior performances even compared to these
additional baselines. Moreover, these baselines achieve lower results
than the EpiFCR method coupled with SPNet, as expected. It is also worth
highlighting how coupling ZSL methods with DANN for DG achieves lower
performances than the ZSL methods alone in this scenario. This is in
line with the results reported in [ peng2019moment ] , where standard
domain alignment-based methods are shown to be not effective in the
DomainNet dataset, leading also to negative transfer in some cases [
peng2019moment ] .

Finally, we want to highlight that coupling EpiFCR with any of the ZSL
baselines, is not a straightforward approach, but requires to actually
adapt this method, re-structuring the losses. In particular, we
substitute the classifier originally designed for EpiFCR with the
classifier specific of the ZSL method we apply on top of the backbone.
Moreover, we additionally replace the classification loss with the loss
devised for the particular ZSL method. For instance, for EpiFCR+SPNet,
we use as classifier the semantic projection network, using the
cross-entropy loss in [ xian2019semantic ] as classification loss.
Similarly, for EpiFCR+DEVISE and EpiFCR+ALE, we use as classifier a
bi-linear compatibility function [ xian2018zeroshotgood ] coupled with a
pairwise ranking objective [ frome2013devise ] and with a weighted
pairwise ranking objective [ akata2013label ] respectively.

#### c.1.3 ZSL+DG: ablation study

In order to further investigate our design choices on the ZSL+DG
setting, we conducted experiments on a challenging scenario where we
consider just two domains as sources, i.e. Real and Painting. The
results are shown in Table C.2 . On average our model improves SPNet by
2% and SPNet + Epi-FCR by 1.1%. Our approach without curriculum largely
outperforms standard image-level mixup [ zhang2017mixup ] (more than
2%). Applying mixup at both feature and image level but without
curriculum is effective but achieves still lower results with respect to
our CuMix strategy (as in Tab. 2). Interestingly, if we apply the
curriculum strategy but switching the order of semantic and domain
mixing (CuMix reverse), this achieves lower performances with respect to
CuMix, which considers domain mixing harder than semantic ones. This
shows that, in this setting, it is important to correctly tackle
intra-domain semantic mixing before including inter-domain ones.

#### c.1.4 ZSL results

In this section, we report the ZSL results of Figure 4.3 in tabular
form. The results are shown in Table C.3 . Here, we also report the
results of a baseline which uses just the cross-entropy loss term
(similarly to [ xian2019semantic ] ), without the mixing term employed
in our CuMix method. As the table shows, our baseline is weak,
performing below most of the ZSL methods in all scenarios but FLO.
However, adding our mixing strategy allows to boost the performances in
all scenarios, achieving state-of-the-art performances in most of them.
We also want to highlight that in Table C.3 , as in Figure 4.3 , we do
not report the results of methods based on generating features of unseen
classes for ZSL [ xian2018feature , xian2019fvaegan ] . This choice is
linked to the fact that these methods can be used as data augmentation
strategies to improve the performances of any ZSL method, as shown in [
xian2018feature ] . While using them can improve the results of all the
baselines as well as CuMix , this falls out of the scope of our work.
