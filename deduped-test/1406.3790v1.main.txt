##### Acknowledgements. This thesis summarizes my five and half years
study and research experience in VLSI Research Laboratory at Hong Kong
University of Science and Technology. I would like to take this
opportunity to thank all people who have helped, accompanied and
supported me during my PhD study in HKUST. My foremost thanks belong to
my supervisor, Prof. Chi-Ying Tsui, for his patient guidance,
encouragement, understanding, and support all over the time. His
brilliant insights on VLSI and highly motivation on NoC led me to the
wonderful world of on-chip networks. He always inspires me and has
provided me with valuable advice in my study. His brilliance, enthusiasm
and hard working towards research, and nice guidance not only have
benefited my current research but also have a lasting influence in my
professional career and personal development. I am deeply grateful to
Prof. Chin Tau Lea, Prof. Jiang Xu, Prof. Chi-Keung Tang and Prof.
Albert C. S. Chung for serving on my Thesis Committee, and I am full of
gratitude to Prof. Oliver Chiu-Sing CHOY from Chinese University of Hong
Kong for being my Thesis External Examiner. They managed to take time to
serve on my committee in their tight schedule. I would also like to
thank Prof. Radu Marculescu from Carnegie Mellon University for hosting
my Fulbright visit. I benefit a lot through working together with Prof.
Radu Marculescu, Prof. Diana Marculescu in CMU and Prof. Paul Bogdan
from University of Southern California. Their valuable feedback helped
improve my work in many ways. I would like to thank Mr. Leo Fok and Mr
Siu Fai Luk for providing me with great technical and administrative
support these years. I thank everyone in VLSI lab (especially Dr. Jie
Jin, Dr. Liu Feng, Dr. Shao Hui, Mr Yunxiao ling, Mr Yingfei Teh, Mr
Xing Li, Mr Youzhe Fan, Mr Jingyang Zhu and Mr Syed Abbas Mohsin) for
creating a family environment and the friends I met in HKUST for their
friendship and help. Last, but not the least, I would like to express my
deepest gratitude to my parents and girlfriend for their unconditional
love and support through all these years during my Ph.D. study.

###### Contents

-    1 Introduction
    -    1.1 Challenges in computing platform design
    -    1.2 NoCs for multi-core communication
    -    1.3 Application-specific NoC design flow
    -    1.4 NoC characterization
        -    1.4.1 NoC topology
        -    1.4.2 NoC switching technique
        -    1.4.3 NoC router design
    -    1.5 Thesis overview
-    2 SVR-NoC: A Learning Based NoC Latency Performance Model
    -    2.1 Introduction
    -    2.2 Background
    -    2.3 Proposed NoC analytical model
        -    2.3.1 Basic assumptions and notations
        -    2.3.2 End-to-end delay formulation
        -    2.3.3 Link dependency analysis
        -    2.3.4 GE-type traffic modeling
        -    2.3.5 WH router modeling
        -    2.3.6 Discussion on extensions to VC router modeling
    -    2.4 SVR-NoC latency model
        -    2.4.1 Channel and source queuing regression models
        -    2.4.2 Overall SVR-NoC methodology
        -    2.4.3 Feature extraction for training data
        -    2.4.4 Support vector regression for @xmath and @xmath
    -    2.5 Experimental results
        -    2.5.1 Experimental setup and training data preparation
        -    2.5.2 Proposed queuing model accuracy
        -    2.5.3 SVR prediction accuracy
        -    2.5.4 Runtime comparison
    -    2.6 Conclusion
-    3 Thermal-aware and Application-specific Routing Algorithm Design
    -    3.1 Introduction
    -    3.2 Related Work
    -    3.3 Methodology overview
        -    3.3.1 Assumptions and preliminaries
        -    3.3.2 Motivation for thermal-aware routing
        -    3.3.3 Thermal-aware routing design flow
    -    3.4 Main algorithm
        -    3.4.1 Application-specific path set finding algorithm
        -    3.4.2 Optimal traffic allocation
    -    3.5 Router microarchitecture
    -    3.6 Experimental results
        -    3.6.1 Simulation environment setup
        -    3.6.2 Adaptivity improvement
        -    3.6.3 Peak energy simulation results
        -    3.6.4 Simulation results with different processor/router
            energy ratio
    -    3.7 Conclusion
-    4 Fault-tolerant Routing Scheme for NoCs using Dynamic
    Reconfiguration of Partial-Faulty Routing Resources
    -    4.1 Introduction
    -    4.2 Background
    -    4.3 Fault-tolerant NoC design
        -    4.3.1 Fault-diagnosis and hybrid fault model
        -    4.3.2 Dynamic buffer swapping (DBS)
        -    4.3.3 Dynamic MUX swapping (DMS)
        -    4.3.4 Handling hard link faults
        -    4.3.5 Deadlock issue and handling strategy
    -    4.4 Simulation Results
        -    4.4.1 Simulation environment setup
        -    4.4.2 Comparison of connectivity of the network
        -    4.4.3 Improvement in packet acceptance rate
        -    4.4.4 Effects on latency performance
    -    4.5 Conclusion
-    5 FSNoC: A Flit-level Speedup Scheme For NoCs Using
    Self-Reconfigurable Bi-directional Channels
    -    5.1 Introduction
    -    5.2 Motivations of FSNoC
        -    5.2.1 A motivational example
        -    5.2.2 Single router throughput comparisons
    -    5.3 Implementation of FSNoC
        -    5.3.1 Inter-router channel direction control
        -    5.3.2 Router datapath design of FSNoC
    -    5.4 FSNoC for long-wire links
        -    5.4.1 Long-wire links with repeaters
        -    5.4.2 Long-wire links with pipeline registers
    -    5.5 Experimental results
        -    5.5.1 Simulation setup
        -    5.5.2 Simulation results on synthetic traffics
        -    5.5.3 Simulation results on real benchmarks
        -    5.5.4 Simulation results for FSNoC with link pipelines
        -    5.5.5 Implementation overhead
    -    5.6 Conclusion
-    6 A Traffic-aware Adaptive Routing Algorithm on a Highly
    Reconfigurable Network-on-Chip Architecture
    -    6.1 Introduction
    -    6.2 Background
    -    6.3 New NoC architecture
        -    6.3.1 Motivation for adding hub routers
        -    6.3.2 Overview of proposed NoC architecture
        -    6.3.3 Router implementation
    -    6.4 Routing algorithm design
        -    6.4.1 Choosing the express hub and EVC paths
        -    6.4.2 Adaptive routing for BiNoC
    -    6.5 Simulation results
        -    6.5.1 Simulation setup
        -    6.5.2 Comparison of adaptive routing for BiNoC
        -    6.5.3 Effect of adding EVC and hub routers
        -    6.5.4 Results using real world workloads
        -    6.5.5 Overhead evaluation
    -    6.6 Conclusion
-    7 Conclusions and future work
    -    7.1 Research summary
    -    7.2 Future research directions

###### List of Figures

-    1.1 The plots of transistor counts against dates and Moore’s Law
    (courtesy of [ 1 ] )
-    1.2 The illustration of the processor performance growth since the
    late 1970s (from [ 2 ] )
-    1.3 Power density trends in Intel’s CPU (from [ 3 ] )
-    1.4 Growth in clock rate of microprocessors (from [ 2 ] )
-    1.5 Manycore system roadmap for improving computing platform
    performance (from [ 4 ] )
-    1.6 The trend of on-chip interconnections (from [ 5 ] )
-    1.7 The NoC-based Video Object Plane Decoder (VOPD) platform in [ 6
    ]
-    1.8 NoC synthesis flow for application specific multicore systems
-    1.9 The NoC router architecture with virtual channels and the
    open-loop source queuing delay measurement
-    2.1 The NoC performance model used in the synthesis inner loop for
    large design space exploration
-    2.2 An example showing the flow delay: a) the core communication
    graph, b) the links involved to calculate the end-to-end delay of
    flow @xmath c) the acquisition time @xmath and transfer time @xmath
    for link @xmath in WH model, d) @xmath and @xmath in VC model
-    2.3 Modeling of the GE packet generation process in [ 7 ]
-    2.4 An illustration of the channel service time of link @xmath
    under WH routing
-    2.5 The transition diagram to calculate the average degree of
    virtual channel multiplexing [ 7 ]
-    2.6 SVR-NoC methodology overview
-    2.7 Illustration of the channel and source queuing feature vectors
-    2.8 The soft margin loss setting in @xmath -SVR [ 8 ]
-    2.9 An example RMSE surface in the grid search
-    2.10 Analytical model accuracy comparisons
-    2.11  a) Regression curve of the channel queuing time (CQ)
    b)Comparisons of the learning and queuing model
-    2.12 Comparisons with the queuing model under different traffic
    pattern and NoC configurations
-    2.13 Comparisons with the queuing model under different traffic
    pattern and NoC configurations -2
-    2.14 Latency prediction performance under trained traffic patterns
-    2.15 Latency prediction for trained synthetic traffics -2
-    2.16 Latency prediction for trained synthetic traffics -3
-    2.17 Latency prediction for untrained synthetic traffics
-    2.18 Latency prediction for untrained synthetic traffics -2
-    2.19 Flow latency comparison for DVOPD benchmark
-    2.20 Run time comparisons of the learning model and simulation
-    3.1 An example of: a) Core communication graph (CCG), b) NoC
    architecture, c) Channel dependency graph (CDG) and its strongly
    connected components(SCC) after mapping
-    3.2 A motivation example of allocating traffic among paths: a) the
    core communication graph(CCG), b) routing paths allocation on @xmath
    NoC, c) two strategies of using the routing paths, d) thermal
    profile comparison (left: strategy one, right: strategy two)
-    3.3 Proposed design flow for the thermal-aware routing
-    3.4 Application specific and deadlock free path finding algorithm
-    3.5 Proof of deadlock free inheritance
-    3.6 Block diagram of the router supporting ratio-based routing:
    a)Router microarchitecture, b)Routing computation unit,
    c)Ratio-based output port selection
-    3.7 Adaptivity comparisons for different benchmarks
-    3.8 Peak energy simulation for real benchmarks
-    3.9 Latency and peak energy simulation for a) Random, b)
    Transpose-1 and c) Transpose-2 traffic
-    3.10 Latency and peak energy simulation for a) Hotspot-Rs, b)
    Hotspot-Tr and c) Bursty traffic
-    3.11 Latency and peak energy simulation for a) Hotspot-C and b)
    Butterfly traffic
-    3.12 An example of the NoC energy profile under hotspot-4c
    traffic: a) proposed routing b) oddeven routing c) negativefirst
    routing d) XY routing
-    4.1 A Comparison of various fault model
-    4.2 Architecture of fault-tolerant NoC [ 9 , 10 ]
-    4.3 Dynamic Buffer Swapping (DBS) technique
-    4.4 A pathological case for packets interleaving
-    4.5 DMS hardware implementation
-    4.6 Deadlock situation for DBS operation
-    4.7 Connectivity comparison of the hybrid fault model
-    4.8 Comparison between buffer swapping strategy
-    4.9 Latency comparison of various fault model and schemes
-    5.1 Typical four-stage pipelined router micro-architecture using
    VC-based flow-control
-    5.2 The comparisons of different architectures a) example traffic
    flows b) timing diagram under typical uni-directional NoC c) timing
    diagram for typical NoC with 2X internal bandwidth d) timing diagram
    for BiNoCs
-    5.3 The proposed flit-level speedup switching timing diagram for
    the case in Fig. 5.2 -a
-    5.4 Profiling results on an @xmath mesh under different traffic
    patterns: a) Histogram of percentage of times that no flows in the
    opposite link direction at run time b) Breakdown of scenarios III
    and IV under the condition that there is no contention in the
    opposite link
-    5.5 The histogram of @xmath on an @xmath mesh under @xmath
    saturation injection rate
-    5.6 Single router throughput comparisons of different router
    architectures: a) Throughput versus packet arrival rate @xmath under
    @xmath . b) Throughput versus packet arrival rate @xmath given
    @xmath c) The throughput comparisons of four NoC architectures under
    a variety of @xmath combinations.
-    5.7  a) The channel direction control (CDC) module b) The timing
    diagram of link direction switching under the arrival of three
    packets (A,B,C) at router R1 and R2
-    5.8 Proposed buffer organization
-    5.9 Switch allocator logic (modified from [ 106 ] )
-    5.10 FSNoC design for long wire link with repeaters
-    5.11  a) FSNoC design for long wire links with pipeline
    registers, b) the detail schematics for data link pipeline
    registers, c) two types of conflicts during the the link traversal
    (LT) stage
-    5.12 Simulation results for six synthetic traffics on an @xmath
    mesh under XY routing
-    5.13 The impacts of buffer depths and packet size on overall
    throughput
-    5.14 Latency comparisons for a) four architectures under oddeven
    (OE) routing algorithm b) FSNoC and BiNoC with WH router
    architecture and VC router architecture with different VC numbers c)
    NoC architectures with 4L links
-    5.15 Benchmark throughput comparison
-    5.16 Latency comparisons for the SPEC-web benchmarks
-    5.17 Histogram of the delivery time for Telecom
-    5.18 Latency comparisons for NoCs with 2-stage pipelined links
-    6.1  a) Proposed NoC architecture for an @xmath mesh with
    bi-directional links, EVCs and hub routers b) The hub router
    architecture with each VC corresponding to a node in the region c)
    An example of routing from (1,3) to (3,1) using hub routers
-    6.2 Router architecture of a) BiNoC; b) BiNoC + EVC bypass node; c)
    BiNoC+EVC source/sink node
-    6.3 The express paths to the hub router
-    6.4 a)Router architecture of node (1,2) b) Contention between EVC
    and Hub paths
-    6.5 K-step (K=3) speculative routing
-    6.6 Comparison of the fitness functions for @xmath meshes using
    bi-directional channels
-    6.7 Comparison of different adaptive routing schemes for @xmath
    meshes
-    6.8 Comparison of different NoC architectures under various mesh
    size and traffic patterns.
-    6.9 Latency comparison for benchmarks
-    6.10 Percentage of packets using EVCs and hub routers
-    6.11 Energy comparison for benchmarks
-    6.12 Area overhead comparison

###### List of Tables

-    2.1 Summary and comparison of NoC latency models
-    2.2 Parameters and notations in NoC latency model
-    2.3 Training features for regression
-    3.1 Notations of application specific path set finding algorithm
-    3.2 Parameters and notations in LP formulation
-    3.3 Execution time for different mesh sizes
-    3.4 Peak energy reduction under various @xmath (for Random,
    Transpose-1, Transpose-2 and Hotspot-C traffic)
-    3.5 Peak energy reduction under various @xmath (for Butterfly,
    Hotspot-Tr,Hotspot-Rs and Bursty traffic)
-    3.6 Peak energy reduction under various @xmath (for real benchmark
    traffic)
-    4.1 The proposed hybrid model for permanent faults
-    4.2 Handshake flow control for DBS
-    4.3 Packet acceptance rate comparison
-    4.4 Latency and packet acceptance rate comparison for various
    traffics
-    5.1 NoC architectures used in experiments
-    5.2 Area breakdown of different NoC architectures
-    5.3 Power breakdown of different NoC architectures
-    6.1 Notations in the routing for proposed NoC

## Chapter 1 Introduction

### 1.1 Challenges in computing platform design

Computer and IC technology have made dramatic progress in the past few
decades since the first generation of electronic computer was built [ 2
] . As indicated by the Moore’s Law [ 11 ] (shown in Fig. 1.1 ), the
transistor count on an integrated circuits(ICs) doubles every two years.
Accordingly, by the year of 2012, the state-of-the-art processors
already contain billions of transistors (such as the Intel’s 10-core
Xeon CPU with 2.5 billion transistors [ 12 ] and Nvidia’s 7.08 billion
transistors GPU [ 13 ] ). The steady technological improvements,
together with the enhancement from better computer architectures, have
contributed to a consistent performance improvement every year [ 2 ] .
Fig. 1.2 depicts the comparisons of computer processor performance
relative to a VAX-11/780 processor which are measured using the standard
SPEC benchmarks over the years [ 2 ] . As shown in the figure, before
1980s, the growth in performance is largely driven by the technology
advancement, which gives about @xmath performance increase per year [ 2
] . Then, this growth rate is improved to about @xmath due to the
introduction of more advanced architectural and computer organizational
concepts, such as the emergence of the RISC (Reduced Instruction Set
Computer) based microprocessors [ 2 ] . However, this trend begins to
slow down again due to the constraints of power [ 3 ] [ 2 ] . As shown
in Fig. 1.3 , the power and power density on chip have been dramatically
increased with the technology scaling down by the year of 2010. This is
because as the technology process improves to a new generation, the
increase in the number of transistors and the operating frequency
overwhelms the decrease in load capacitance per transistor and the
running voltage, which results in an overall growth in power density and
energy [ 2 ] . For example, the Intel processor Core2 Duo consumes as
high as 130W power which is about 20 times power of the 10 years ago
Pentium processor in market [ 14 ] . The high power density causes
severe reliability issues and inevitably makes the die cost unaffordable
due to the high cooling requirements and costs. In order to avoid
reaching the power limit, the clock frequency growth with time has to be
slowed down. This trend is shown in Fig. 1.4 , where the clock frequency
remains around @xmath after year 2003 [ 2 ] . Instead of continuing the
aggressive clock frequency scaling, the computer architects have
proposed a new direction to achieve maximal performance under these
tight constraints and budgets, i.e., by employing more processor cores
on chip for the computation tasks [ 4 ] . Consequently, the embedded
systems have led to the multi-processor System-on-Chip (MPSoC) design
and the high performance computer architectures have evolved into Chip
Multi-processor (CMP) platforms, which involve tens or hundreds
processor elements, memory blocks, ASIC acceleration engines to be
inter-connected together on chip. Each processing element performs its
tasks in a parallel way taking the advantage of the parallelism either
in task, thread or system level. As shown in Fig. 1.5 , many recent
chips have already switched to the paradigm of multi-core based platform
for this purpose. For example, in [ 15 ] , a @xmath -core heterogeneous
digital baseband IC for MIMO 4G Software Defined Radio (SDR) is
proposed. Their proposed NoC-based prototype doubles the throughput and
consumes only @xmath power over the previous MPSoC solutions. Another
example is the Intel @xmath -tile Teraflops processor [ 16 ] which is a
homogeneous NoC-based CMP platform and delivers up to @xmath TFlops of
performance. Recently, photonic on-chip network based multi-core systems
have also been widely studied [ 17 , 18 ] , which further attempts to
optimize the traditional metal-based interconnect performance in terms
of delay and power for future SoCs with thousands PEs.

### 1.2 NoCs for multi-core communication

For multicore based computing platform, an efficient way to manage the
communication among the on-chip resources become critically important.
The peer-to-peer interconnection consumes large wire area, which leads
to large area and fan-outs [ 5 ] . The bus based architecture suffers
from its limited bandwidth as well as scalability [ 5 ] . The
scalability issue of these two schemes brings significant overhead in
power and transmission delay when the chip feature size reduces beyond
45nm. To satisfy the communication requirements with hundreds or
thousands processor elements (PEs), Network-on-Chip has been proposed as
an efficient and scalable solution. Borrowing the concepts in Internet
and wireless network, NoC use routers to route packets instead of wires
[ 6 ] . The latency and throughput performance is improved due to the
higher bandwidth offered by the network. Meanwhile, the power
consumption can be significantly reduced by breaking long links between
the processors and avoiding high fan-outs in the outputs. In summary,
Fig. 1.6 shows the trend of on-chip interconnection and compares the
total wire length under different technology nodes [ 5 ] . As shown in
the figure, for the technology nodes beyond 50nm, NoCs are more
preferred over the other two paradigms in order to provide scalable
communications for more than @xmath wire lengths.
In Fig. 1.7 , we show an example of a MPSoC design using the mesh
topology NoC for the video object plane decoder (VOPD) application [ 6 ]
. The whole application is characterized by an application task graph [
6 ] , where the vertices in the graph represent certain computation
tasks need to be performed and the edges indicate the communication
bandwidth (MB/s) between two adjacent tasks. As shown in the figure, for
the NoC-based VOPD platform, the whole system consists of twelve tiles
organized in a rectangular mesh topology. Each tile is made up of a
processor element (PE) and a router. The PE executes certain tasks in
the application task graph while each router has five input/output ports
that are connected to the four neighboring routers as well as the local
PE. At run time, the packets are routed based on the routing algorithm
which is designed to determine the order of the routers to be traversed
for a specific communication flow. For the NoC-based multicore system,
besides the latency and throughput improvement, it also brings the
following advantages:
1) High reliability: For Multicore systems, the complex system is highly
susceptible to faults [ 19 ] . Compared to point-to-point dedicated
links and buses, NoC can achieve higher reliability by providing
redundant paths among the cores. If some of the routers fell into
permanent or temporary faults, the other routers can be utilized to
re-route the packets to the destinations and hence packet acceptance
rate will not drop dramatically.
2) Modular design and IP re-use: NoC provides sufficient bandwidth for
communication, while the processors can be designed without considering
the network; therefore it supports modularity design and IP reuse [ 20 ]
. Moreover, the global clock synchronization is not necessary in NoC
which increase the overall system yield [ 21 , 14 ] .
3) Global asynchronous, locally synchronous (GALS) design: For multicore
systems, it is difficult to distribute a single clock over thousands
processor cores. To deal with these issues, NoC offers a good platform
for the GALS design style [ 21 ] because each tile (processor elements
and the router) can work separately within its own clock domain [ 14 ] .
By employing GALS design, multiple Voltage-Frequency islands can be
developed in different regions of NoC so as to achieve lower power
consumption [ 14 ] .
4) Power and area efficiency: Compared to the buses, the arbitration
time for contention is much smaller as each router only needs to handle
local contention scenario [ 22 ] . Therefore, large buffers to store the
unserved packets are not needed in NoC routers, which result in a more
compact router design and reduces the area/power overhead [ 22 ] . For
power dissipation, because the buses are connected to all the PEs in the
system, while the links in NoC only need to connect two neighboring
routers (or a router and a PE) [ 22 ] . Therefore, with proper
floorplanning, NoCs uses shorter wire length and occupies less load per
transition [ 22 ] . Moreover, NoCs provide a variety of efficient power
management strategies to further reduce power. This is because the NoC
can be partitioned into sub-networks and each region can be powered-off
or slowed down via dynamic voltage and frequency scaling (DVFS)
individually [ 22 ] . High power efficiency can be achieved without
significant degradation in the overall system performance.

### 1.3 Application-specific NoC design flow

In a typical NoC-based multicore system design, we begin with a
specification of performance requirements combined with some cost
constraints. These performance metrics, such as the latency/throughput,
power consumption and hotspot temperature, drive the choice of NoC
design parameters. A typical NoC synthesis flow works as follows
(summarized from [ 6 ] ):
1) Task scheduling and mapping: The first thing to determine is to
allocate and schedule the tasks on the available processors. Usually a
task graph is utilized to characterize the traffic patterns and the
communication volumes of each traffic flow in the application (as shown
in Fig. 1.7 ). Given the processors in the platform, task scheduling and
mapping algorithms are developed to decide which processor that a
specific task should be executed on as well as the order of the tasks to
be executed on the same processor. In this step, bandwidth utilization,
total delay and power consumption are major design objectives while
physical bandwidth as well as hard or soft deadline of some particular
tasks are the constraints that need to be considered.
2) Core mapping: After the tasks are scheduled and mapped onto
processors, the next step is to place these processors onto the NoC
architecture. A core mapping algorithm is developed for this purpose.
The core communication graph derived in the previous step determines the
placement of tiles in this step. A mapping solution with high
throughput, low latency and low power is usually desired while it should
not exceed the capacities of the physical link bandwidth.
3) Routing algorithm design: After the task and processor mapping,
routing algorithm is developed to decide the physical paths for sending
the packets from the sources to the destinations. It will greatly affect
the packet latency between the two cores as well as the overall chip
power and thermal profile. For the routing algorithm design, one
important issue is to avoid deadlock. The deadlock refers to the
situation that the whole system stalls due to the circular dependencies
[ 23 ] . More specifically, for the deadlock scenario, it happens at run
time, where flits from some packets occupies some resources in the
router (such as the buffer). At the same time, they request to use other
resources (such as the buffer in the downstream node), so the
dependencies of the channels may have chances to form a cycle. In this
case, all these packets are stalled in place and can not proceed to the
destination anymore [ 23 ] .

As there are a lot of possible design choices in each of the three steps
above, NoC-based multicore system design produces a large space to be
explored. Therefore, it is of utmost importance to provide an accurate
performance evaluation with respect to the specific configurations in
the synthesis inner loop. Both analytical models and simulations can be
used in the NoC performance evaluations. To fully understand and model
the details of the network situations occurred at run time, NoC
simulators are developed and widely adopted with high fidelity. On the
other hand, since NoC designs have many power, area and latency
trade-offs in topology, task and core mapping algorithms etc. , analytic
models have also been deployed to allow fast design space explorations [
24 ] . In general, it is more reasonable to work with simple analytical
models first in the synthesis loops, while more detailed simulations
become necessary to accurately characterize the exact performance of the
network after only a few candidates being remained [ 24 ] . In Fig. 1.8
, we summarized the synthesis flow for the NoC design. The usage of
analytical models is highlighted within the inner loop in the figure.

### 1.4 NoC characterization

In order to better understand the terminologies, concepts and algorithms
developed in this thesis, in the following, we briefly review the basic
ideas and the models that characterizes an NoC platform.

#### 1.4.1 NoC topology

Network topology refers to the arrangement of various elements (links,
nodes, etc. ) of the network [ 20 ] . Essentially, it is the topological
structure of the multi-core platform and is dependent on the placement
of the network’s components, including the locations of the processors
and the routers [ 20 ] . There are various NoC topologies, such as mesh,
torus [ 25 ] , butterfly [ 26 ] , 3D-mesh [ 27 ] and fat trees [ 28 ] .
In this thesis, we assumed the underlying NoC system is composed of a 2D
mesh network (as shown in Fig. 1.7 ). The reason for using the 2D mesh
network is due to its regularity and layout efficiency on silicon
surface [ 25 ] . Moreover, the 2D mesh topology also matches well with
the current IC manufacturing technology for the layout consideration,
especifally for most IC components which have rectangular shape [ 14 ] .
Therefore, this topology has attracted wide attention in most
state-of-the-art NoC-based multicore prototypes ( e.g., MIT’s 16-tile
RAW chip [ 29 ] and Intel’s 80-tile TFLOPS chip [ 16 ] ).
Another advantage of the mesh based topology is the high scalability to
merge or combine building blocks which are developed with regular shapes
[ 14 ] . When the complexity of the embedded systems is increasing, more
PEs are trying to be put together on the chip. With regular shape, the
additional PE blocks can be easily integrated on the original design [
14 ] which eases the voltage/frequency island based control on NoC.

#### 1.4.2 NoC switching technique

In general, based on the flow control granularity, the NoC routers can
be classified into three types, namely the circuit switching, virtual
cut-through switching and the wormhole switching [ 6 ] . The reviews are
done based on [ 6 , 30 , 14 ] :
In the circuit switching paradigm, two PEs set up a specific
communications channel (named as circuit path) in NoC first before they
begin to transfer packets to each other. The circuit switching ensures
the bandwidth for the channel settled and keeps connected during the
whole communication period of the specific flow [ 30 ] . However, it is
sometimes inefficient in using the channel bandwidth because the unused
links reserved for one connection cannot be used by others when the
circuit is set up [ 6 , 30 ] .
In virtual cut-through switching [ 31 ] , the buffers are designed to be
capable of storing the whole maximum packet. However, the whole packet
is only stored into a router buffer if the downstream router buffer is
already occupied by other packets [ 14 ] . Otherwise, the flits once
arrived at the current buffer can be routed directly without the need to
wait for the arriving of other flits in the same packet [ 14 ] . Hence,
in virtual cut-through switching, if the packet stall [ 20 ] happens due
to the failure of allocating a downsteam channel, the packet stays in
the current node will not block any other packets [ 14 ] . Compared to
the circuit switching, as the flits can be forwarded immediately, the
network latency under no congestion is reduced; however, the virtual
cut-through switching still requires the buffer size to be large enough
in order to store the whole packet under congestion [ 14 , 6 , 30 ] .
In order to overcome the limitations in the circuit and virtual cut
through switching, the wormhole switching techniques have been proposed
and widely used in the communication networks [ 30 ] as it requires
fewer buffer resources than previous two techniques [ 6 , 30 ] . In
particular, in wormhole NoC, each message consists of several packets.
Furthermore, the packet is divided into several flits, which are the
minimal flow control units in the routing. The header flit is utilized
to settle the routing paths in the routers, while the body and tail
flits simply follow the paths reserved by its header. When the tail flit
leaves the router, it will release all the resources it reserved for the
packet so that the consequent packets can use them again [ 6 , 30 ] .
Therefore, one major advantage of wormhole routing is that it does not
need a large enough buffer to hold the whole packet, which drastically
reduces the overall latency [ 14 ] .

#### 1.4.3 NoC router design

The typical structure of an on-chip router for a mesh NoC is shown in
Fig. 1.9 , where we show the basic control and data path with four
virtual channels (VCs) in each input port [ 20 , 6 ] . For the wormhole
router, it can be viewed as a special case of virtual channel routers
where the number of VCs per input port equals to one. As shown in Fig.
1.9 , the router control path usually consists of the routing
computation (RC) module, the virtual channel allocation (VA) module and
the switch allocation (SA) module. Its data path usually consists of the
buffers (virtual channels or a single buffer), the switching crossbar
and the output registers. Of note, the RC module is used to compute the
output port according to the destination address recorded in the header
flit in front of the buffer. The VA module is used to allocate the
downstream virtual channels to the packet in the current virtual channel
buffer. After a virtual channel is allocated to the packet, the switch
allocation module works to arbitrate for the usage of the switch fabric
(crossbar) entries among the packets. On top of this baseline
architecture, many modifications on the router datapath and control path
have been proposed to reduce the power consumption and latency ( e.g.,
the speculative router [ 20 ] and the lookahead router with bypass
architectures [ 32 ] ).

The network interface (NI) is needed between the router and PE to
convert and transfer messages. The standard measurement setup for
interconnection networks is shown in Fig. 1.9 -b [ 20 ] . To measure the
performance of an interconnection network, we need to attach terminals (
i.e., PEs) to the local port of the adjacent router in the network. The
NI is usually modeled with infinite buffer size to isolate the NoCs from
the processors [ 20 ] . It is important that in this open-loop
measurement, the monitors in NI is placed ahead of the source queue
instead of after the queue (the monitor records the injection and
ejection times of the packets to PE) [ 20 ] . In this way, the packets
that have been generated and are still waiting to be injected into the
network are considered [ 20 ] . Therefore, the overall packet latency
under this set-up will not only include the NoC traverse time but also
the source queuing time [ 20 ] . Various workload can be applied to the
NI, including the traffics generated from real processor models, the
trace-based workload as well as the synthetic workload without using any
processor information [ 20 ] .

### 1.5 Thesis overview

In our work, we investigated the issues for a high performance NoC
design, from analytical performance modeling to the routing algorithm
design and NoC architecture optimization. In particular, we looked at
the following areas: NoC performance modeling for the design space
exploration (Chapter 2); routing algorithm design for the
thermal-awareness and reliability objectives (Chapter 3 and 4); flexible
NoC architecture design using self-reconfigurable bi-directional
channels (Chapter 5 and Chapter 6). We aim to address several key
problems in NoC design from both the algorithmic point of view as well
as the hardware and architectural level optimization. To be more
precise, the outline and contributions of this thesis are summarized
below:
Chapter 2: SVR-NoC: A learning based NoC latency model
In this chapter, instead of using conventional queuing-theory-based NoC
latency model, which have several assumptions that comprise the overall
prediction accuracy. We proposed a learning based NoC latency regression
model, namely SVR-NoC, to accurately evaluate a candidate design in the
inner loop for the design space exploration. Compared to the previous
models, we showed that better accuracy over the queuing models and at
the same time @xmath speedup over the detailed simulations can be
achieved using the SVR-NoC model on both the synthetic traffic patterns
and real application traces. Therefore, the SVR-NoC can benefit the
exploration of numerous design configurations in the offline phase.
Chapter 3: A thermal-aware routing algorithm for application-specific
Network-on-Chips (NoCs)
For NoC-based multi-core systems, the routing algorithm significantly
affects the overall performance and needs to be tackled in the offline
design phase to meet the certain design constraints. Among all the
routing considerations, temperature is one of the most critical ones as
the uneven temperature across the chip will introduce thermal hotspots
and degrade the performance dramatically. Towards this end, in this
chapter, we propose an offline thermal-aware routing algorithm to evenly
distributed the traffic across the chip so to reduce the hotspot
temperature. Specifically, we propose a deadlock free adaptive routing
algorithm which provides maximal number of paths to route packets for
the given application. Then, a linear programming based algorithm is
used to find the optimal ratio to send packets among the paths. We show
that as much as @xmath peak energy reduction can be achieved over a set
of applications while the latency/throughput performance is maintained
by using the proposed method.
Chapter 4: Fault-tolerant NoC routing algorithms design
In this chapter, we investigate and propose a highly resilient routing
algorithm to tackle the router and link faults at run time. More
specifically, we classify the permanent faults in NoC into two types (
i.e., link faults and buffer faults). For the link faults, a highly
resilient routing algorithm is used to re-route the packets from faulty
links. While for the buffer faults, we propose two new schemes, namely
dynamic buffer swapping and dynamic MUX swapping to handle the errors in
the buffers and crossbar Muxes, respectively. We show that, higher
packet acceptance rate as well as better latency and throughput
performance can be achieved for a set of test traffics.
Chapter 5: FSNoC: A flit level speedup scheme for NoCs using
self-reconfigurable bi-directional channels
Besides the optimization in the algorithm level, we also explored to
improve NoC performance from the architectural level. In this chapter,
we propose FSNoC, a new NoC router architecture that supports switching
two flits from the same packet simultaneously by using the
bi-directional channels. Compared to previous router architectures using
bi-directional links, better link bandwidth utilization can be achieved
and therefore FSNoC can lead to higher performance in latency and
throughput. The channel direction control protocol as well as the router
micro-architecture which supports flit-level parallel transmission have
been proposed. We demonstrate the performance improvement of FSNoC using
both synthetic traffic patterns as well as the traces from realistic
applications. The hardware overhead of FSNoC in terms of area and power
is also reported and analyzed in detail in this chapter.
Chapter 6: A traffic-aware adaptive routing algorithm on a highly
flexible NoC architecture
In this chapter, we aims to add more flexible in the overall NoC
architecture and propose a new platform which consists of i)
self-reconfigurable bi-directional channels, ii) express virtual
channels and iii) regional hub routers to improve the system
performance. A fitness-based and traffic-aware adaptive routing
algorithm is designed which is suitable for the proposed platform and
chooses the routing path dynamically to adapt the traffic conditions at
run time. Combining the routing algorithm and the platform, more than
80% improvement in saturation throughput can be obtained, while
involving less than 15% overhead in power dissipation.
Chapter 7: Conclusion and future work
This chapter summarizes the works done in the whole thesis and discusses
several future research directions.

## Chapter 2 SVR-NoC: A Learning Based NoC Latency Performance Model

In this Chapter, we propose SVR-NoC, a learning based Network-on-Chip
(NoC) latency model using support vector regression (SVR). Different
from the state-of-the-art NoC analytical models, which use queuing
models to compute the average channel waiting time and the source
queuing time, the proposed SVR-NoC model predicts the NoC latency based
on learning the typical training data. More specifically, given the
application communication graph, the NoC architecture and the routing
algorithm, we first analyze the links dependency and then determines the
ordering of latency analysis. The channel and source queue waiting times
are then estimated using a new generalized @xmath queuing model, which
can tackle bursty arrival times with general service time distributions.
To improve the prediction accuracy, the queuing theory based delays are
included as one of the features in the learning process. We propose a
systematic learning framework that uses the kernel-based support vector
regression method to collect training data and predict the traffic flow
latency. The proposed learning-based model can be used to analyze
various traffic scenarios for NoC platforms with arbitrary buffer and
packet length. Experimental results on both synthetic and real
applications demonstrate the accuracy and scalability of the proposed
SVR-NoC model as well as a @xmath X speedup over simulation-based
evaluation methods.

### 2.1 Introduction

The NoC complexity as well as its tight requirements on the power,
latency, and throughput have become major challenges in the design of
NoC-based multi-core systems [ 35 ] . As shown in Fig. 2.1 , a typical
NoC-based system design requires many synthesis steps including task
allocation, mapping, core placement and routing. Specifically, based on
the pre-characterized application traffic, the designers first need to
schedule and map the tasks on the available processor elements (PEs).
After the task scheduling and mapping are done, core placement and
routing path allocation need to be explored. Each of the above steps can
produce numerous design choices. Therefore, a performance analysis tool
is needed to evaluate whether the chosen NoC configuration for the input
application leads a better design over others while satisfying the
design constraints at the same time. Detailed network simulations can
provide performance evaluation results with high fidelity. However, they
suffer from long evaluation times and so are only suitable for
estimating a small subset of alternatives in the final prototyping
stage. Because of this, NoC performance models are widely adopted to
guide the pruning of the design space during the system synthesis [ 33 ]
.

Among all the NoC performance metrics, latency is recognized as one of
the most critical design parameters since it determines the whole system
throughput under specific workloads [ 24 ] . In this work, we propose a
latency model to predict the average delay of flows in an NoC-based
multi-core system for the design space exploration. In order to derive a
latency model, most previous researches are based on the queuing-theory
formalisms and treat each input channel in the NoC router as an @xmath [
34 ] , @xmath [ 33 ] , @xmath [ 24 ] . Indeed, these models provide
accurate performance estimations when the following assumptions hold: i)
The packet length satisfies an exponential distribution, and therefore
the packet service time in the router is exponentially distributed as
well [ 34 ] . ii) The traffic inter-arrival time is assumed to follow a
Poisson distribution at all traffic sources [ 33 , 35 ] . However, it
has been observed that in many NoC systems, the behavior of the traffic
follows the fractal/long-range-dependent (LRD) pattern [ 37 ] and the
distributions of the service time are correlated as well. Consequently,
the accuracy of the queuing theory-based model is compromised in these
cases.

In this chapter, we attempt to develop an NoC latency model which is
suitable for the synthesis inner loop and has higher prediction accuracy
by using a new approach based on the machine learning techniques. More
specifically, we first propose a new queuing-theory-based delay
evaluation methodology which can work for a variety of NoC
configurations and traffic scenarios. The proposed performance queuing
(PQ) model is based on a @xmath queuing formalism and generalizes the
previous NoC PQ models as follows: i) The existing traffic arrival
modeling using Poisson approximations is extended to a generalized
exponential (GE) packet inter-arrival distribution which can account for
burst traffic patterns. ii) The packet service process within each
router is modeled with a general distribution to account for the service
time correlation between routers and traffic flows. iii) A more general
NoC architecture model is used so that routers with finite buffer depth
are accommodated, thus enabling the consideration of arbitrary buffer
depth and packet length combinations. iv) By considering the link
dependencies, the proposed framework is completely generic and can be
applied to any NoC topology with different task mapping and routing
algorithms. Then, to relax some of the assumptions in the PQ model (such
as the GE traffic arrival process) and further improve the modeling
accuracy, we propose SVR-NoC, which is a support vector regression (SVR)
based NoC latency model. In SVR-NoC, the delay predictions based on
enhanced queuing-theory-based PQ model are included as part of the
features in the learning process. In the training stage, the training
data-set is formed by collecting the latency simulation results of the
same NoC platform on various synthetic traffic patterns. We employ
Support vector regression [ 38 ] techniques to learn the channel queuing
and the source queuing models as functions of their feature sets,
respectively. During the learning process, cross validation is used to
avoid training data over-fitting. In the prediction stage, the learned
SVR model is used to estimate the average waiting time at each input
buffer channel and then the overall packet flow latency for the new
application patterns.

The rest of the chapter is organized as follows. In Section 2.2, we
review the previous arts and highlight our contributions. In Section
2.3, we present the proposed generic queuing-theory based latency model.
Section 2.4 details the learning-based SVR-NoC latency model. The
experimental results of the proposed NoC latency model on both synthetic
and real applications are shown in section 2.5. Finally, Section 2.6
concludes this chapter.

### 2.2 Background

The analytical models for evaluating the NoC average latency can be
classified into three groups: probabilistic models [ 39 ] , network
calculus models [ 40 ] , and queuing theory models [ 33 , 35 , 24 ] . In
[ 39 ] , a probabilistic analysis framework was developed to model a
single wormhole router performance. However, additional effort is needed
to extend to network of routers. In [ 40 ] , the network calculus
approach was adopted to characterize the NoC performance. However, the
average delay prediction error is larger than that of the queuing models
[ 40 ] . Therefore, most of the previous efforts are based on queuing
models to evaluate the NoC delay.

For the general class of queuing-theory-based NoC models, most of the
early works consider the modeling of wormhole (WH) routers under the
assumption of Poisson arrival time distribution and memoryless packet
service time distribution. For example, in [ 34 ] , an M/M/1
approximation of link delay is used to analyze the capacity and flow
allocation. Although generally tractable, the accuracy of M/M/1 models
can be significantly compromised as the assumption of exponential
arrival and service time distributions may not hold in many real
applications [ 41 , 37 ] . Several works have been proposed to improve
the estimation accuracy by generalizing the arrival and service time
distributions. In [ 42 ] , an analytical model based on M/G/1/K queue is
proposed to account for finite size input buffers in local area networks
(LANs). However, this model is based on the Laplace-Stieltjes transform
and is too complicated to be used in the NoC synthesis loop. In [ 35 ] ,
an M/G/1 based latency model for NoC analysis is proposed. It only
assumes that the arrival rate of the header flits (as opposed to the
entire packet) follows a Poisson distribution. In [ 24 ] , a
fixed-priority G/G/1 based NoC latency model, which attempts to model
the burst arrival times with a 2-state Markov-modulated Poisson process
(MMPP), was proposed. However, this approach targets a specific
priority-based router architecture, while many NoC routers may utilize a
more fair arbitration such as the round robin (RR) scheme. In [ 36 ] ,
an M/M/m/K queue-based analytical model is proposed to analyze the delay
of NoCs with variable virtual channels per link. This approach assumes
negligible flit buffers ( i.e., single flit buffer) such that a packet
reaches its destination before its tail leaves the source host. In [ 33
] , an @xmath queuing mode is proposed for both wormhole and virtual
channel NoCs. However, this approach assumes that the granularity of the
buffers is in terms of packets instead of flits and therefore a single
channel buffer can hold up to @xmath packets during the analysis. This
may not be the case for NoCs whose buffers are rather small (only
several flits) to save area and power [ 24 ] .

Machine learning is a technique that has been extensively used in the
domain of pattern recognition or artificial intelligence where it is
usually difficult to derive the exact mathematical relationship between
the outputs and inputs in these problem formulations [ 38 ] . In NoC
performance modeling and analysis, most of the learning models focus on
using learning-based model to improve the area/power model accuracy. For
example, in [ 43 ] , the multivariate adaptive regression splines (MARS)
technique is used to develop a non-parametric NoC router power and area
regression model. Compared to the conventional ORION2.0 model [ 44 ] ,
the learning-based model improves the prediction accuracy over a variety
of NoC implementations. Later, in [ 45 ] , the model accuracy is further
enhanced by explicitly modeling of control and data paths in the
regression analysis. Besides area/power modeling, the learning
techniques have also been used in optimizing NoC runtime performance,
such as the reinforcement learning based DVFS control [ 46 ] , the
Q-learning based congestion-aware routing [ 47 ] and the neural network
based optimal dynamic routing [ 48 ] . However, for the latency
performance metric analysis, machine learning techniques for improving
modeling accuracy over the conventional queuing model have not been
thoroughly studied yet.

In this chapter, we propose a new learning-based NoC latency model which
generalizes the previous work by considering: i) the arrival traffic
burstiness, ii) the general service time distribution, iii) the finite
buffer depth and arbitrary packet length. For clarity purposes, in Table
2.1 , we summarize and compare our proposed queuing model and the
learning-based SVR-NoC model against other models proposed to date ² ² 2
In the table, PB ratio is defined as the ratio of average packet size (
@xmath flits) to the buffer depth ( @xmath flits) . As shown in the
table, our proposed model offers a much broader coverage for various
temporal and spatial traffic patterns, as well as NoC architectures.
This provides more flexibility for the designers to explore the NoC
design space.

To the best of our knowledge, this chapter brings the following new
contributions over the previous efforts:

-   We proposed a new queuing-theory based NoC traffic model which is
    topology-independent and can be used to analyze a variety of traffic
    scenarios as well as arbitrary buffer size and packet length
    combinations.

-   In addition to the proposed queuing model, we propose and develop a
    learning-based framework for NoC latency analysis. The model has
    fewer assumptions related to the packet length and traffic
    distribution as well as the router architectures.

-   We show the accuracy and scalability of the proposed SVR model using
    both the synthetic traffic and real applications. The speedup of the
    learning model over the conventional simulations can significantly
    benefit the NoC synthesis and optimization process.

### 2.3 Proposed NoC analytical model

#### 2.3.1 Basic assumptions and notations

We assume that the target applications have been scheduled and mapped
onto the target NoC platform and the source and destination tile
addresses for each specific flow @xmath in the application are known.
Also, borrowing from the idea of modeling bursty traffic in hyper-cube
multi-computers [ 49 ] , we assume that the packet inter-arrival times
of flow @xmath have been characterized using a general exponential (GE)
distribution [ 7 ] (discussed later in Section 2.3.4) with mean @xmath
and a square coefficient of variation (SCV) @xmath . Therefore, the
@xmath characterization of the traffic model is an input to our
analytical framework. Moreover, a deadlock-free and deterministic
routing algorithm is used to guarantee that no cycles are formed by the
link dependencies. Without loss of generality, in this work we use X-Y
routing. Other deadlock-free and deterministic routing schemes can also
be used. We also adopt a wormhole router architecture, where there
exists a single buffer at each input port. For simplicity, we assume
that the packets have a fixed size of @xmath (flits) as in [ 41 ] .
However, this assumption can be relaxed to cover arbitrary packet length
distribution. Other assumptions are that the traffic sources (i.e., the
source PEs) have an infinite queue size and the destinations immediately
consume the arriving flits. To facilitate the discussion, the symbols in
Table 2.2 are used consistently in this chapter, which follows the name
conventions in [ 36 , 42 ] .

#### 2.3.2 End-to-end delay formulation

In a WH or VC NoC, the end-to-end flow latency @xmath of a specific flow
@xmath (shown in Fig. 2.2 -a and -b) is made up of three parts [ 36 , 42
] : 1) the queuing time at the source @xmath , 2) the packet transfer
time @xmath and 3) the path acquisition time ( @xmath ). It is expressed
as [ 36 ] :

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

In order to calculate @xmath , we need to consider the path acquisition
time of every link @xmath residing in the routing path of @xmath (Fig
2.2 -b shows the links used for the flow @xmath ) [ 36 ] and therefore:
@xmath , where @xmath is the time for a packet header to contend a
channel (in WH routing) or a VC (in VC routing) with other flows for
link @xmath and @xmath is routing path length (number of hops) of @xmath
.

If we denote the time to transmit the header flit by @xmath , then the
packet transfer time @xmath can be rewritten as [ 36 ] :

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

where the first term denotes the header flit transmission time over the
network, and the second term approximates the packet serialization time
of the body and tail flits. The notations of the queuing delay @xmath ,
@xmath and @xmath are also illustrated in Fig. 2.2 for both the WH and
VC NoCs.

In order to derive @xmath , @xmath and @xmath for each link, which are
the elements of the routing path of @xmath , need to be obtained. Two
issues hereby arise. First, it is important to determine the order of
analyzing the links of the path. Second, the detail model to obtain
@xmath and @xmath of each link has to be developed. In particular, we
need to differentiate between the VC routing and the WH routing, which
are shown in Fig. 2.2 -c and -d, respectively. As shown in the figure,
the VC router complicates the @xmath and @xmath modeling in the
following two aspects: i) For the link contention time @xmath , the
packet can randomly choose among @xmath VCs instead of @xmath and
therefore the simple single server queuing model in WH models can not be
used directly. ii) For the flit transfer time @xmath , we need to
consider the additional time required due to the flow multiplexing among
the VCs over the same physical link as described in [ 36 ] .

In the following subsections, we first present the procedure to
determine the orders of the analysis of the links. After that, we will
then elaborate on the formulation of the WH and VC router models.

#### 2.3.3 Link dependency analysis

To account for the impact of the congestion at the downstream routers on
the blocking of a particular router, it is important to obtain the
dependency among all the links. Fig. 2.2 -b shows an example. Due to
back-pressure, the waiting times @xmath and @xmath of link @xmath (
i.e., @xmath ) will affect the time to serve a packet in the buffer head
of link @xmath ( i.e., @xmath ). Therefore, @xmath is said to be
dependent on @xmath . Similarly, @xmath and @xmath depend on those
downstream links of @xmath which is determined by the application
mapping and routing. Thus it is required to finish the analysis of all
the downstream links of link @xmath in the flow @xmath before we can
calculate the queuing service and waiting time of link @xmath . To
obtain the link dependencies, a link dependency graph (LDG) is built
first and the topological sort algorithm as used in [ 42 ] is then used
to order the links. The detail of link dependency analysis is presented
in Algorithm 2 . The vertices in LDG correspond to the link channels in
the NoC while a directed edge joining two vertices reflects that there
is a dependency between these two links (e.g., @xmath and @xmath in Fig.
2.2 ). When the routing or the task mapping solution changes, the LDG
needs to be rebuilt. The LDG is built by checking every flow @xmath in
the application communication set @xmath . An edge is added between two
vertices of the LDG if there exists a flow @xmath that the two links
corresponding to the two vertices are two adjacent links on the routing
path. The order of the link analysis are then obtained by applying the
topological sort algorithm on the LDG.

1: Input: @xmath the application flow set

2: Output: @xmath the ordered list of links for queuing analysis

3: Container: @xmath the link dependency graph

4: for all link @xmath do

5: @xmath

6: end for {initialize the @xmath }

7: for all flow @xmath do

8: @xmath ;

9: @xmath ;

10: for @xmath do

11: @xmath ; {the upstream link}

12: @xmath {the downstream link}

13: @xmath

14: end for {considering all the links in the path of @xmath }

15: end for {considering all flows in application}

16: return @xmath ;

Algorithm 1 Link dependency analysis

#### 2.3.4 GE-type traffic modeling

Many applications in NoCs show burst patterns of traffic over a wide
range of time scales. Therefore, the Generalized Exponential (GE)
distribution is utilized to model the arrival traffic at the source PEs
and the links [ 7 ] . In the following, we briefly review the GE type
distribution proposed in [ 7 , 49 ] . Under GE distribution, the
cumulative distribution function (cdf) of the inter-arrival time @xmath
is given by [ 7 ] :

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

where @xmath and ( @xmath ) are the mean and square coefficient of
variation (SCV) of @xmath . The GE packet generation process is shown in
Fig. 2.3 from [ 7 , 49 ] . As shown in the figure, a packet experiences
a zero service time to reach the departure point ( i.e., the point to
generate the packet) with probability @xmath . For the rest ( i.e., with
probability @xmath ), the packet needs to traverse the system with an
exponentially distributed service time with mean equal to @xmath . The
burst of packets consist of a packet which comes from the exponential
branch (M branch) in addition to a number of continuous packets arriving
through the direct branch [ 7 ] . The GE distribution is a versatile and
simple distribution which helps to make the queuing formulation
analytically tractable [ 7 ] . Moreover, it has been demonstrated that
the GE distribution also provides efficient approximations for short or
long-range dependent traffic in supercomputers [ 7 ] .

#### 2.3.5 WH router modeling

In this section, we present the techniques to estimate the three key
components of the WH analytical models, i.e., the flit transfer time
@xmath , the path acquisition time @xmath and the source queuing time
@xmath .

1) Flit transfer time : The flit transfer time @xmath of a link @xmath
is defined as the time taken for the header flit to leave the buffer
head in the upstream node and reach the front of the buffer in the
current link @xmath after being granted the link access. Fig. 2.4
illustrates this timing concept. It is equivalent to the time taken from
Point A to Point D. More specifically, it consists of two parts, the
first part is the time to leave the upstream router ( i.e., the time
from Point A to Point C in Fig. 2.4 ). For WH routing, this is a
constant value depending on the number of pipeline stages ( @xmath ) in
the router and the link. The second part accounts for the time the
header flit takes to arrive at the front of the buffer of link @xmath (
@xmath ). It is illustrated by the time to travel from Point C to D in
Fig. 2.4 . This time value can be approximated by the waiting time of a
queuing system (such as the M/M/1/K ) with capacity equals to the buffer
size @xmath . The mean flit arrival rate of this queuing system is given
by:

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

where @xmath is a binary value indicating whether the channel is an
element of the routing path set @xmath [ 35 ] :

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

The mean time to serve a flit ( @xmath ) in this queuing system is
calculated by the weighted average of the service time for all flows
passing through link @xmath and is given by:

  -- -------- -- -------
     @xmath      (2.7)
  -- -------- -- -------

Eqn. 2.7 shows that for a packet with @xmath flits, it takes @xmath
cycles for a header in the buffer to win the next link of flow @xmath .
Therefore, @xmath cycles are needed in total for the service of the
header flit. After that, the body and tail flits are transferred in
@xmath cycle without any additional delay. The mean flit service time
over the whole packet is thus computed as @xmath .

After the mean arrival rate ( @xmath ) and the mean service time (
@xmath ) at this queue are obtained, the M/M/1/K queuing formulation
presented in [ 34 ] can be applied to approximate @xmath . @xmath is
then obtained as @xmath .

2) Path acquisition time : The path acquisition time @xmath of flow
@xmath at its @xmath hop @xmath ( i.e., @xmath ) is defined as the
waiting time of the header flit to be granted the access to the buffers
in @xmath after contention with other flows routing towards the same
output direction [ 36 ] . It is usually modeled as the waiting time of a
queuing system as in [ 36 , 42 ] . Examples of models used are the G/G/1
[ 24 ] , M/G/1/K [ 33 , 50 ] , GE/G/1 [ 7 ] and MMPP/G/1 [ 51 ] queues.
For the fair allocation policies such as the round-robin arbitration [
52 ] , each flow has the same priority and takes turns to use the output
link. Therefore, the system capacity @xmath of the queuing system to
derive @xmath is the number of flows that contend for the same link. In
Fig 2.4 , @xmath for @xmath . The arrival process of the queuing system
is the merging of all flows that route to @xmath . More precisely, the
mean arrival rate is @xmath . The higher moments of the arrival process
of this queue are calculated according to the specific queuing model
used. For example, if the GE/G/1 queuing model is employed to derive
@xmath , the squared coefficient of variance (SCV) of the arrival flows
are required in order to apply the queuing formula. The SCV of the
traffic to @xmath can be approximated as:

  -- -------- -- -------
     @xmath      (2.8)
  -- -------- -- -------

In [ 51 , 49 , 53 ] , detailed mathematical manipulations are shown to
obtain a more accurate derivation of @xmath . Moreover, the method can
be used to generate the estimations of other higher moments that are
required by the more generalized queuing models, such as the MMPP/G/1 or
MAP/G/1 queues. However, the computation complexity of this method is
high as more traffic details are considered. Anyway, it provides a way
to model more complex traffic phenomena such as self-similarity and LRD
in the queuing models. The service time of the queue that is used to
compute @xmath is illustrated in Fig. 2.4 (referenced from [ 42 , 49 ]
). Without loss of generality, we assume that a packet has @xmath flits
and it spreads over several adjacent links along the path. Here we
assume the input buffer depth is @xmath flits. The service time accounts
for the time that a packet occupies the link @xmath [ 42 ] . For
example, in Fig. 2.4 , assume a header flit in Point A is granted the
current link @xmath access. Then, the service process of this packet
begins when the header flit leaves A and ends when the tail flit departs
A so as to release @xmath for other flows. If the downstream links are
not congested, this service time simply equals to the packet length (
i.e., @xmath cycles) because the whole packet can traverse in a
continuous way [ 42 ] . However, when there is severe blockage along the
path, the worst-case scenario occurs when the packet head reaches Point
B (Fig. 2.4 ) and the accumulated buffer spaces from Point C to Point B
are just enough to hold the whole packet [ 49 , 42 ] . The time delay
for the packet at point A to reach point B, @xmath can be given by [ 42
] :

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

where @xmath represents the effective number of hops that a packet may
span [ 49 , 42 ] . Specifically, for a flow @xmath with a Manhattan
distance from the source to the destination tile equal to @xmath , if
@xmath is smaller than the number of remaining hops @xmath , @xmath
equals to @xmath ; otherwise, it equals to @xmath [ 49 , 42 ] . Of note,
different from [ 42 ] , we have included the router pipeline delay
@xmath in Eqn. 2.9 to reflect a more tight packet transfer time to Point
B.

The service time @xmath for the flow @xmath is then bounded by @xmath
and @xmath under different congestion conditions and can be approximated
as [ 42 , 49 , 50 ] :

  -- -- -- --------
           (2.10)
  -- -- -- --------

The rationale of Eqn. 2.10 is explained as follows [ 42 , 49 , 50 ] :
when the downstream channels along the routing path is not congested (
e.g., @xmath ), the link service time @xmath is approximated by the
packet length @xmath . At the other extreme, when there exists severe
blockage at the subsequent hops ( i.e., @xmath ), @xmath is approximated
by @xmath as the congestion delay in the subsequent links dominates the
current channel service time.
From Eqn. 2.10 , once the downstream links transfer time @xmath and
contention time @xmath are known, the overall link service time @xmath
is then the weighted average of the service time of all flows passing
through the link @xmath , which yields the mean service time @xmath as:

  -- -------- -- --------
     @xmath      (2.11)
  -- -------- -- --------

For M/G/1/K, G/G/1 and MMPP/G/1 models, the SCV of the service time for
link @xmath is also required. It can be approximated in a similar way as
[ 24 ] and is given by:

  -- -------- -- --------
     @xmath      (2.12)
  -- -------- -- --------

Based on the above discussions, after obtaining the arrival and the
service process characteristics, in this work, we first use the M/G/1/K
queuing formula to obtain the waiting time @xmath and then extend to
@xmath by considering the second moment (SCVs) of the GE traffic input
of the queuing system:

  -- -------- -- --------
     @xmath      (2.13)
  -- -------- -- --------

where @xmath is the M/G/1/K queue based waiting time, @xmath and @xmath
are the SCVs of the service and inter-arrival time, respectively.

3) Source queuing time: The source queue at the network interface (NI)
is modeled as a queuing system with infinite capacity, and hence an
M/M/1/ @xmath [ 36 ] or a GE/G/1/ @xmath [ 7 ] queuing model can be
used. For example, if GE/G/1 model is used, the queuing time is given by
[ 7 ] :

  -- -------- -- --------
     @xmath      (2.14)
  -- -------- -- --------

where the arrival process ( @xmath , @xmath ) and the mean service time
@xmath at the source node @xmath (link @xmath ) are calculated in
similar manners as presented for the router channels.

#### 2.3.6 Discussion on extensions to VC router modeling

When there are multiple virtual channels sharing a single physical port,
the inputs received from a physical link will be multiplexed among all
the available VCs [ 55 , 36 ] . In order to model the effect of VC
multiplexing, we need to scale the mean packet waiting time by a factor
@xmath , which represents the average degree of VC multiplexing that
takes place at a given link channel as in [ 54 , 55 , 56 , 7 ] . The
mean message delay for the flow @xmath is rewritten as:

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

To obtain the value of @xmath , the VC state transition diagram (STD) of
a physical channel is used for the analysis [ 54 ] . As shown in Fig.
2.5 , state @xmath ( @xmath ) in the STD represents that there are
@xmath VCs currently being occupied by packets in a physical channel.
The transition rate from state @xmath to @xmath is denoted as @xmath ,
which reflects the mean packet arrival rate at the given input channel
when the STD is at state @xmath [ 54 ] . It not only depends on the
current state (i.e., the number of VCs being used) in STD, but also is
related to the traffic arrival model. In [ 7 ] and [ 51 ] , @xmath is
derived based on the maximum entropy principles [ 57 ] for the GE and
MMPP traffic arrival models. Similarly, the rate from state @xmath to
@xmath is @xmath , where @xmath is the mean service time at each
physical channel. Based on the STD, the state probability, @xmath , (
@xmath ), that represents @xmath VCs are busy at a given channel, can be
determined by solving the steady-state equations of the Markov chain in
Fig. 2.5 [ 51 , 7 ] . The average degree of VC multiplexing that takes
place at a given physical channel is then given by [ 55 ] : @xmath .
Consequently, the source queuing time @xmath and the path acquisition
time @xmath in the VC channel model will equal to those calculated in
the wormhole model multiplying @xmath as shown in Eqn. 2.15 .

### 2.4 SVR-NoC latency model

To further improve the accuracy of the analytical performance model, we
introduce a learning based performance model [ 58 ] . The main
characteristics of this learning model is that in addition to the using
of collected traffic data statistics as training features, we also
include the prediction results of the queuing model presented in section
2.3 as part of the features. More precisely, the SVR-NoC model learns
and refines the proposed queuing model based on the typical training
data from the simulation of the target NoC platform. In this section, we
first define two regression functions that need to be learned and then
elaborate on the support vector regression techniques for obtaining
these two models, respectively.

#### 2.4.1 Channel and source queuing regression models

Based on Eqn. 2.1 , we define two queuing delays that make up of the
traffic flow latency. The first is the source queuing delay @xmath which
is the waiting time of the packets at the source queue @xmath before
injected into the network ( i.e., @xmath ). The second is the channel
waiting time @xmath which is the total packet transfer time at the
direction @xmath of router @xmath and equals to @xmath in Eqn. 2.1 . In
this work, we model these two components via two regression functions
@xmath and @xmath . We denote the feature sets used in learning @xmath
and @xmath by two vectors @xmath and @xmath , respectively. The proposed
supervised SVR-NoC learning framework is applied on the training data
set to formulate the following models in which the estimated queuing
delays are specified as functions of the selected features:

  -- -------- -- --------
     @xmath      (2.16)
  -- -------- -- --------

Using Eqn. 2.16 , given the feature sets, we can estimate @xmath and
@xmath and substitute them in Eqn. 2.1 to obtain the latency of each
specific flow. Once the @xmath metric is estimated, the channel buffer
utilization and the network throughput can also be obtained as in [ 35 ]
.

#### 2.4.2 Overall SVR-NoC methodology

In SVR-NoC, the step to obtain the estimates of the channel and source
queuing delay using the proposed PQ model is summarized in Algorithm 2.
In the training stage, we first apply the link dependency analysis
presented in section 2.3.3 to determine the correct link ordering for
performance analysis. Then, for each link @xmath in the ordered list
@xmath , we calculate the arrival traffic model ( @xmath ) according to
the routing algorithm and applications. As shown in Eqn. 2.11 , the link
transfer time @xmath only depends on its downstream link contention time
@xmath , which should have been analyzed in the previous loop. On the
other hand, the path acquisition time @xmath depends not only on its
downstream link contention time @xmath and transfer time @xmath but also
on the current link’s @xmath (Eqn. 2.9 and Eqn. 2.10 ). Therefore,
@xmath is calculated first. After @xmath is obtained, we then calculate
the mean and SCV of the link service time ( @xmath ). If this link
connects two routers, we utilize GE/G/1/K queue to obtain @xmath .
Otherwise, we calculate the source queuing time @xmath . Once all the
@xmath variables are known, the predicted channel waiting time @xmath
can be obtained as @xmath while the source waiting time @xmath .

1: Input: @xmath the application flow set; @xmath the ordered list of
links for queuing analysis

2: Output: @xmath the estimated channel waiting time for link @xmath ;
@xmath the estimated source waiting time

3: for all link @xmath do

4: ( @xmath ) = traffic_model( @xmath )

5: @xmath ;

6: for all @xmath and @xmath do

7: @xmath ;

8: end for

9: ( @xmath )= @xmath ;

10: if @xmath then

11: @xmath

12: @xmath

13: else

14: @xmath

15: @xmath

16: end if

17: end for

Algorithm 2 Analytical working flow to obtain @xmath and @xmath

Figure 2.6 shows the overall SVR-NoC framework, which consists of two
parts, namely the training stage and prediction engine . In the training
stage, different traffic patterns are fed into the system level NoC
simulator to collect the statistics of the training data of the channels
in the routers and the source queues. The channel queue feature set
@xmath includes the packet arrival rates @xmath and the forwarding
probabilities @xmath in the router as well as the calculated channel
waiting time @xmath from the proposed analytical model. The source queue
feature set @xmath includes the injection rates @xmath at the traffic
source, the neighboring channel waiting times from the analytic model
@xmath as well as the calculated source queuing time @xmath . Both
@xmath and @xmath are extracted from the simulation results to form the
training data set. Support vector regression is then carried out to
obtain the @xmath and @xmath models, respectively.

After the training stage, the obtained SVR models are used in the
prediction engine to estimate the latency performance for different
traffic patterns. The prediction engine can be embedded in the NoC
synthesis framework for the inner-loop design evaluation. Of note, the
features in @xmath and @xmath are computed from the input core
communication graph (CCG) as well as the proposed queuing model. Then,
the @xmath and @xmath functions are used to evaluate the channel and
source queuing delay, respectively. Finally, the traffic flow latency
and the overall latency can be computed according to Eqn. 2.1 .

#### 2.4.3 Feature extraction for training data

##### Channel queuing feature vector

The average waiting time in the channel @xmath of router @xmath ( @xmath
) depends not only on its packet arrival rate @xmath but also on the
contention among the channels within the same router, as well as the
traffic conditions in the neighboring routers (such as the congestion in
the downstream router). In the analytical models, these contentions are
usually calculated explicitly by computing the contention matrix using
the application traffic arrival rate matrices and the forwarding
probability matrices [ 35 , 33 ] . In contrast, we aim at learning the
impact of these contentions implicitly by providing the SVR engine with
sufficient samples. Therefore, for the channel queuing feature vector,
it is made up of three parts: 1)we first include the arrival rate @xmath
of the current channel to indicate the traffic workload injected to the
channel. 2)The forwarding probability from current channel to different
output directions @xmath and the amount of traffic from other input
channels to the same output direction @xmath ( @xmath ) are included to
reflect the contentions situations. 3) Finally, the estimated channel
service time @xmath and the channel waiting time @xmath from queuing
models are included to provide an estimation for @xmath . Of note, both
the arrival rate matrix @xmath and the forwarding probability matrix
@xmath at node @xmath can be calculated offline based on the application
CCG. Specifically, as in [ 35 ] , @xmath is given by

  -- -------- -- --------
     @xmath      (2.17)
  -- -------- -- --------

where @xmath indicates whether the channel is part of the routing path
set @xmath and is equal to (as in [ 35 ] ):

  -- -------- -- --------
     @xmath      (2.18)
  -- -------- -- --------

The forwarding probability @xmath denotes the portion of traffic that
arrives at channel @xmath of node @xmath and is forwarded to the output
direction @xmath . It is given by (as in [ 35 ] ):

  -- -------- -- --------
     @xmath      (2.19)
  -- -------- -- --------

where @xmath is a binary indicator that returns 1 if the routing path
@xmath contains the channel @xmath through the output direction @xmath .
In SVR-NoC, in order to reduce the size of training data that needs to
be collected from the simulations, we propose to include an estimation
of the channel queuing time @xmath in the channel queuing feature set.
Of note, @xmath is obtained from the proposed GE/G/1/K queuing model and
is independent of the training set simulation results. In Table 2.3 , we
list all the elements in the channel queuing feature vector @xmath . In
Fig. 2.7 -a, we show an example of the forwarding probability and
aggregated contention traffic. More specifically, as shown in Fig. 2.7 ,
for the current west input channel ( @xmath ), when considering the east
output direction, the forward probability @xmath represents the portion
of traffic that will be forwarded towards east direction. In addition,
@xmath in the figure indicates the traffic that contends with @xmath to
the east output from the south input channel. By including all other
input channels, the input feature vector @xmath can be formed as in
Table 2.3 .

##### Source queuing feature vector

The time that a packet needs to wait in a particular source queue
depends on the traffic generation rate at the processor and the network
congestion status, which are reflected by the arrival rate and waiting
time in the attached router, respectively. Therefore, the packet
generation rate @xmath is first included in @xmath as shown in Table 2.3
. The source generation rate @xmath can be computed by:

  -- -------- -- --------
     @xmath      (2.20)
  -- -------- -- --------

As shown in Table 2.3 , to achieve a better inference on the level of
network congestion and a higher accuracy, we also need to include the
average waiting time @xmath of the channels in the downstream links in
@xmath , where @xmath is an output direction that the current source
queue @xmath has traffic to forward to. Similar to the channel queuing
feature set, we include the calculated source queuing time @xmath in the
feature set (shown in Table 2.3 ) to improve the learning function
accuracy. In Fig. 2.7 -b, the source traffic rate @xmath as well as the
waiting time @xmath in the west down-streaming link are shown, which
corresponds to @xmath and @xmath in Table 2.3 , respectively.

#### 2.4.4 Support vector regression for @xmath and @xmath

After obtaining the feature vectors @xmath and @xmath , we apply @xmath
-SVR [ 38 ] to learn the two nonlinear models @xmath and @xmath ,
respectively. The objective of @xmath -SVR is to find a function @xmath
that deviates from the actual target values @xmath ( i.e., @xmath and
@xmath ) in the data-set by at most @xmath [ 38 ] . There are three
steps in the @xmath -SVR [ 8 ] : (1) Primal form optimization, (2) Dual
problem formulation and (3) Implicit mapping via kernels. The primal
formulation is a straightforward representation of the regression
problem whereas the latter two steps provide the a practical
transformation considering the non-linear extension via kernel tricks.
We present the @xmath -SVR formulation of the channel average waiting
time function @xmath while similar procedures can be applied for
obtaining @xmath .

##### Primal form formulation

Without loss of generality, we begin by assuming @xmath is a linear
function. This assumption will be relaxed later to highly non-linear
functions by using the Radial Basis kernels [ 38 ] discussed in step 3.
Given a set of @xmath training data points, i.e., @xmath , where @xmath
is the @xmath sample feature vector with a dimensionality of @xmath (
@xmath as shown in Table 2.3 ) . Under the linear model assumption,
@xmath can be expressed as [ 8 ] :

  -- -------- -- --------
     @xmath      (2.21)
  -- -------- -- --------

where @xmath , @xmath . Let @xmath be the maximum prediction error (
i.e. , @xmath and @xmath represents the L2 norm) that we can tolerate.
We introduce two slack variables, @xmath and @xmath , to represent the
error larger and smaller than the target value by more than @xmath ,
respectively [ 38 ] . They are defined as the “soft margin loss”. Figure
2.8 -a illustrates the soft margin loss for @xmath . We want to minimize
the ”soft margin loss” and hence we add penalty if @xmath or @xmath is
larger than zero. From Figure 2.8 -b, only the points outside the @xmath
region will be penalized in a linear fashion [ 38 ] . In order to
prevent data over-fitting when learning @xmath from the training
samples, a regularization term proportional to @xmath is added into the
objective function [ 8 ] . To find the optimal @xmath , the problem is
formulated as [ 8 ] :

  -- -------- -- --------
     @xmath      (2.22)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (2.23)
  -- -------- -- --------

where @xmath is the regularization parameter that determines the
tradeoff between the fitting accuracy over the training samples (the
second term in Eqn. 2.22 ) and the regularization term for preventing
data over-fitting (the first term in Eqn. 2.22 ) [ 8 ] .

##### Dual problem expansion

The primal form optimization problem in Eqn. 2.22 and Eqn. 2.23 is
difficult to solve, therefore we solve its corresponding Lagrangian
formulation @xmath by introducing a set of Lagrangian multipliers @xmath
, @xmath , @xmath , @xmath [ 8 , 38 ] , where:

  -- -------- -- --------
     @xmath      (2.24)
  -- -------- -- --------

It follows that the partial derivatives of @xmath with respect to the
primal variables @xmath are zero for the optimal point in the primal
form [ 8 ] . Hence, the primal variables @xmath can be represented by
@xmath , @xmath after setting:

  -- -------- -- --------
     @xmath      (2.25)
  -- -------- -- --------

By substituting the primal variables in Eqn. 2.22 and Eqn. 2.23 with
their dual variable representations, the dual optimization problem can
be obtained [ 38 ] as follows:

  -- -------- -- --------
     @xmath      (2.26)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (2.27)
  -- -------- -- --------

where @xmath is the dot product of two vectors. This is a quadratic
programming problem and can be solved efficiently in polynomial time.
@xmath is then obtained as [ 38 ] :

  -- -------- -- --------
     @xmath      (2.28)
  -- -------- -- --------

From Eqn. 2.28 , it can been seen that the complexity of the regression
function @xmath depends only on the training samples @xmath that have
nonzero @xmath terms (which are defined as the support vectors) [ 8 ] .
Hence the SVR model benefits from keeping only a few training samples
for very efficient predictions [ 8 , 38 ] .

##### Kernel trick for nonlinear extension

In [ 8 , 38 ] , it has been proven that the linear model formulation in
Eqn. 2.26 and Eqn. 2.27 is still valid if we substitute the dot product
operation @xmath with a number of various kernel functions @xmath :

  -- -------- -- --------
     @xmath      (2.29)
  -- -------- -- --------

From the analytical delay model, it is suggested that the router delay
is a non-linear function of the extracted features. Therefore we use the
most common Radial Basis Function (RBF) kernel [ 8 ] ( @xmath ) in this
work, where @xmath is a tuning parameter to replace the linear dot
production operation in Eqn. 2.26 . As the RBF kernel is highly
nonlinear [ 8 ] , this kernel trick extends the @xmath formulation from
the previous linear assumption into a non-linear function.

##### Cross-validation and grid-search

When solving the optimization problem in Eqn. 2.26 - 2.27 , the tuning
parameters @xmath , @xmath , together with the @xmath in the RBF kernel
are assumed to be fixed. They can be set to any arbitrary values. To
find out these parameters with highest model accuracy, we need to search
over different combinations of @xmath . In this work, we adopt a @xmath
-fold cross-validation approach [ 8 , 38 ] . A @xmath -fold
cross-validation separates the training data into @xmath subsets. @xmath
subsets are used in the training regression stage and the remaining
subset is used as the testing set. @xmath runs are carried out with
different subsets as the testing set. The cross-validation accuracy is
then equal to the root mean square error (RMSE) of all the @xmath runs [
8 , 38 ] . In SVR-NoC, we carry out the parameter search on @xmath using
a @xmath -fold cross-validation as suggested in [ 59 ] . Fig. 2.9 shows
an example root mean square error surface for @xmath =0.5 with different
@xmath combinations. In this work, various combinations of @xmath values
are tried and the one with the best cross-validation accuracy ( i.e.,
the lowest RMSE) is selected. The range and resolution of @xmath ,
@xmath and @xmath are chosen according to [ 59 , 60 ] .

### 2.5 Experimental results

#### 2.5.1 Experimental setup and training data preparation

We use Booksim2.0 [ 61 ] to simulate the router and NoC performance on
mesh NoCs. For each mesh size (ranging from @xmath to @xmath ), various
synthetic traffic patterns including uniform random , transpose and
shuffle and tornado [ 20 ] are used as inputs to the target router
architecture to obtain the training data-set. To verify the model, after
the training stage, the learned model is used to predict the delay with
different injection rates for these three patterns. Also, we use this
learned model to predict the delay for other traffic patterns that are
different from the training sets. Here we use bitreversal and
bitcomplement [ 20 ] traffics. The training data set contains runs with
different packet injection rates under each traffic pattern, which
provides sufficient training data for the SVR learning engine as well as
reasonable training time. The SVR-NoC framework is implemented in MATLAB
based on Libsvm and LS-SVM library [ 59 , 60 ] . Both synthetic and real
application traffic patterns are used in the evaluation. The real
application in the experiment includes DVOPD (Dual Object Plane Decoder)
[ 62 ] which is a video MPSoC benchmark mapped onto a @xmath mesh NoC.

#### 2.5.2 Proposed queuing model accuracy

We first compare and evaluate the proposed analytical model using two
synthetic traffic patterns ( i.e., , random and shuffle [ 20 ] traffic)
with Poisson packet injection rates at each source node. The packet
length @xmath is assumed to be fixed of @xmath flits. The buffer depth
@xmath is adopted to be @xmath flits per input port. We compare the
proposed model with two state-of-the-art NoC analtyical models. The
reference model @xmath is adopted from [ 24 ] , where the priority G/G/1
queue has been modified to a generalized G/G/1 queue formula to model
the round-robin arbitration in NoC. The reference model @xmath is
implemented as proposed in [ 35 ] which is based on a generalized M/G/1/
queue. The comparison results of three analytical models against the
simulations under @xmath mesh size are summarized in Fig. 2.10 .

As shown in Fig. 2.10 , compared to the two reference models, which
over- and under-estimate the network saturation injection points by
@xmath , the proposed model achieves less than @xmath error in
predicting the network saturation point ( i.e., the injection rate where
the overall latency starts increasing dramatically) for both the random
and shuffle traffic patterns. Consequently, the proposed analytic model
provides a better estimation of the channel and source queuing time and
helps to relieve the burden of the learning process.

#### 2.5.3 SVR prediction accuracy

We show the @xmath regression accuracy for the training data-set in
Figure 2.11 -a. As shown in the figure, our SVR predictor is very
accurate for fitting the channel regression curve. The root mean square
error (RMSE) between the predicted and the actual values is less than
@xmath and the squared correlation coefficient is higher than @xmath .

##### Average latency prediction

Although the proposed queueing model provides a comprehensive estimation
of different NoC configurations and improves the accuracy under a
variety of traffic patterns. From our experimental results, it is still
observed the accuracy of the queueing model decreases under some traffic
patterns due to the approximations and assumptions made in the
derivation. To illustrate this, we first compare the SVR-NoC with the
proposed analytical model. In this comparison, @xmath mesh NoC with
random, tornado and shuffle traffic are considered. We consider
different buffer depth @xmath and packet length @xmath combinations in
the evaluation. The average latencies obtained by different methods are
summarized in Fig. 2.11 -b, Fig. 2.12 and Fig . 2.13 . As shown in
Figure 2.12 , the analytical model incurs more than @xmath error in
predicting the network criticality. Especially for the uneven traffic
such as tornado, the error is extremely large for all the packet length
and buffer depth combinations. The inaccuracy is due to the assumption
and simplification made in the derivation of the PQ model. On the other
hand, the SVR-NoC can predict the network saturation point very
accurately, with less than @xmath error for both the random and
transpose traffic patterns.

Figure 2.14 - 2.16 show the prediction results for various traffic
patterns and router architectures. The traffic patterns in these figures
are used in the training stage. There are three router architectures
considered. They are wormhole routers with: 1) Packet length @xmath (
flits ) and buffer depth @xmath flits ; 2) @xmath and @xmath and 3)
@xmath and @xmath . As can be seen in Figure 2.14 - 2.16 , the errors of
SVR-NoC in predicting the network saturation point are within @xmath for
all the traffic patterns and router architectures.

##### SVR-NoC for untrained traffic and real applications

Next, we show the latency prediction performance of the SVR-NoC for the
other traffic patterns that are not included in the training data set.
Figure 2.17 - 2.18 show the comparison results for @xmath , @xmath and
@xmath meshes. As shown in the figures, the SVR-NoC still demonstrates
high accuracy in the bit-reversal and bit-complement unseen patterns
with less than @xmath error in predicting the network criticality
status. We also use a real benchmark, Dual Object Plane Decoder (DVOPD)
[ 62 ] for the comparison and demonstrate the accuracy of SVR-NoC.
Figure 2.19 shows the comparison of the latency obtained by the
simulation and that predicted by SVR-NoC for each traffic flow. As shown
in Figure 2.19 , the maximum difference between the simulation results
and the SVR-NoC regression results is less than @xmath . Similar
observations can be obtained for several different mappings as well as
other application benchmarks.

#### 2.5.4 Runtime comparison

The training time of SVR-NoC for an @xmath mesh with @xmath samples
takes about @xmath hours. However, the training process is a one-time
effort and it does not incur additional overhead during the inner-loop
performance prediction. The average simulation time for an @xmath mesh
for @xmath cycles is about fifteen minutes while the SVR-NoC predictive
framework takes about eight seconds. The latency estimation method using
an analytical model [ 33 ] takes about six seconds. We can see that the
SVR-NoC has a similar run-time with the analytical model and has a
@xmath speedup over the simulation-based prediction. In Fig. 2.20 , we
show the run time comparisons of the simulations and learning models
under different mesh sizes. As can be seen from the figure, the learning
model achieves orders of magnitude improvement in the running time,
which can benefit the exploration of a large design space in the design
phase.

### 2.6 Conclusion

In this chapter, we propose a machine-learning based approach for NoC
performance prediction. We demonstrate the modeling accuracy by using
independent test data sets. The SVR-NoC model achieves less than @xmath
error for various traffic patterns with about @xmath speedup compared to
the simulation-based estimation.

## Chapter 3 Thermal-aware and Application-specific Routing Algorithm
Design

In this chapter, we propose a routing algorithm to reduce the hotspot
temperature for application-specific Network-on-chip (NoC). Using the
traffic information of the applications as well as the mapping of the
tasks onto the NoC, we develop a routing scheme which can achieve a
higher adaptivity than the generic ones and at the same time distribute
the traffic more uniformly. A set of admissible paths which is deadlock
free for all the communications is first obtained. To reduce the hotspot
temperature, we find the optimal distribution ratio of the communication
traffic among the set of candidate paths. The problem of finding this
optimal distribution ratio is formulated as a linear programming (LP)
problem and is solved offline. A router microarchitecture which supports
our ratio-based selection policy is also proposed. From the simulation
results, the peak energy reduction considering the energy consumption of
both the processors and routers can be as high as 20% for synthetic
traffic and real benchmarks.

### 3.1 Introduction

For the design of sophisticated MPSoC on high performance NoC, power and
temperature have become the dominant constraints [ 63 ] . Higher power
consumption leads to higher temperature and at the same time, the uneven
power consumption distribution across the chip will create thermal
hotspots. These thermal hotspots have adverse effect on the carrier
mobility, the meantime between failure (MTBF), and also the leakage
current of the chip. As a result, they will degrade the performance and
reliability dramatically. Consequently, it is highly desirable to have
an even power and thermal profile across the chip [ 64 ] . This imposes
as a design constraint for NoC to avoid uneven power consumption profile
so as to reduce the hotspot temperature.

In a typical NoC-based MPSoC design, we need to allocate and schedule
the tasks on the available processors and map these processors onto the
NoC architecture first. After the task and processor mapping, routing
algorithm is developed to decide the physical paths for sending the
packets from the sources to the destinations. Each phase of the NoC
design affects the total power consumption and also the power profile
across the chip. Previously, task mapping and processor core floorplan
algorithms [ 65 ] have been proposed to achieve a thermal balanced NoC
design. However, the routing algorithm is rarely exploited for this
purpose. Since communication network (including the routers and the
physical links) consumes a significant part of the chip’s total power
budget ( @xmath of total tile power [ 66 ] ), the decision on the
routing path of the packets will greatly affect the power consumption
distribution and hence the overall chip hotspot temperature [ 63 ] .
Therefore it is important to consider the thermal constraint in the
routing phase of the NoC design.

In this chapter, we tackle the routing problem to achieve an even
temperature distribution for an application-specific MPSoC. Given an
application described by the task flow graph and a target NoC topology,
we assume that the tasks are already scheduled and allocated to the
processors and the processor mapping is also done. We then utilize the
traffic information of the application specified in the task flow graph,
which can be obtained through profiling [ 67 , 68 ] , to decide how to
split the traffic among different physical routes for an even power
profile.

Similar to [ 69 ] , we use the peak power (or the peak energy under a
given time window) metrics to evaluate the effectiveness in reducing the
hotspot temperature. Through simulation-based evaluation, we demonstrate
that the proposed algorithm can reduce the peak energy of the tiles by
@xmath while improving or maintaining the throughput and latency
performance.

### 3.2 Related Work

In the area of temperature-aware NoC design, many previous works focus
on the power consumption distribution of the processor cores. In [ 70 ]
, a dynamic task migration algorithm was proposed to reduce the hotspot
temperature due to the processor core ( i.e., PE). In [ 71 ] , a thermal
management hardware infrastructure was implemented to adjust the
frequency and voltage of the processing elements according to the
temperature requirements at run time.

Since the power consumption of the routers is as significant as the
processor core, the thermal constraint should also be addressed in the
routing algorithm design. There have been a lot of works on NoC routing
algorithms for various purposes including low power routing [ 71 ] ,
fault-tolerant routing [ 19 ] and congestion avoidance routing to
improve latency [ 72 ] . However, there are only a few works [ 69 , 63 ,
73 ] taking temperature issue into account.

In [ 69 ] , an ant-colony-based dynamic routing algorithm was proposed
to reduce the peak power. Heavy packet traffics are distributed on the
chip based on this dynamic routing algorithm to minimize the occurrence
of hot spots. However this dynamic routing algorithm is generic in
nature and does not take into account the specific traffic information
of the applications. Therefore it may not be able to achieve an optimal
path distribution. Special control packets are sent among the routers to
implement the algorithm which increases the power overhead. Also, two
additional forward and backward ant units are needed in the router which
results in a large area overhead. Moreover, this work only minimizes the
peak power of the routers but does not consider the effect of the
processor core power on the temperature. In [ 63 ] , a run-time
thermal-correlation-based routing algorithm is proposed. When the peak
temperature of the chip exceeds a threshold, the NoC is under thermal
emergency and the dynamic algorithm will throttle the load or re-route
the packets using the paths that have the least thermal correlation with
the run time hottest regions. However, the algorithm also does not
consider the specific traffic information of the applications. It may be
inefficient if multiple hotspots occur at the same time. Also it does
not clearly describe how to do the re-routing while still guaranteeing
the deadlock free property. In [ 73 ] , a new routing-based traffic
migration algorithm VDLAPR and the buffer allocation scheme are proposed
to trade-off between the load balanced and the temperature balanced
routing for 3D NoCs. In particular, the VDLAPR algorithm is designed for
3D NoCs by distributing the traffic among various layers. For routing
within each layer, a thermal-aware routing algorithm such as the one
introduced in this chapter is still needed.

In this chapter, we focus on the thermal-aware routing for application
specific NoC [ 74 ] . To guarantee deadlock free property, generic
routing schemes use algorithms such as X-Y routing, odd-even routing or
forbidden turn routing [ 6 ] . However, this will limit the flexibility
of re-distributing the traffic to achieve an even power consumption
profile. Here, we utilize the characterized application traffic
information to achieve a larger path set for routing and at the same
time provide deadlock avoidance. Higher adaptivity and hence better
performance can be achieved since more paths can be used for the
re-distribution of the traffic. Given the set of possible routing paths,
we formulate the problem of allocating the optimal traffic among all
paths as a mathematical programming problem. At run time, the routing
decisions will be made distributively according to the calculated
traffic splitting ratios. We demonstrate the effectiveness of the
proposed routing strategy on peak energy reduction using both synthetic
and real application traffics.

### 3.3 Methodology overview

#### 3.3.1 Assumptions and preliminaries

We aim to achieve an even power consumption distribution and reduce the
hotspot temperature for application specific MPSoC using NoC connection.
We assume that the given application is specified by a task flow graph
which characterizes the communication dependencies and the bandwidth
requirements among the tasks of the application. In this chapter, we use
a tile-based mesh topology for the NoC due to its popularity and
simplicity. However, our methodology can be easily extended to other
irregular topologies. Based on the task flow graph and the target
topology, task allocation and processor mapping are determined before
the routing phase. The energy consumption of each processor core is then
estimated according to the tasks mapped to it. Taking all the above
information as input, we address the issue of reducing the peak
temperature via routing algorithm design. We adopt an adaptive routing
strategy instead of static routing scheme because it provides path
diversity to distribute traffic among the sources and the destinations [
6 ] and achieves better performance in terms of latency, throughput and
congestion avoidance. We also assume minimum path routing is used.

To facilitate the discussion of the proposed methodology and algorithms,
we borrow some of the definitions presented and discussed in [ 23 ] .
{definition} A Task graph @xmath is a directed graph where the vertex
@xmath represents a task in the application and the edge @xmath
represents the communication from @xmath to @xmath . {definition} A core
communication graph @xmath is a directed graph where the vertex @xmath
represents the @xmath processor in the mesh and the edge @xmath
represents the communication from @xmath and @xmath . Given an
application-specific task graph and its corresponding mapping function,
@xmath can be generated accordingly.

{definition}

A channel dependency graph @xmath is a directed graph where the vertex
@xmath represents a physical link @xmath in the mesh topology pointing
from tile @xmath to tile @xmath . An edge @xmath exists if there is a
dependency from @xmath to @xmath which indicates a path passing through
the two links @xmath and @xmath consecutively.

From Duato’s theorem [ 75 ] , a routing algorithm is deadlock free for
an application if there are no cycles in its channel dependency graph
(CDG). {definition} Strongly connected components (SCC): A directed
graph is strongly connected if there is a path from each vertex to every
other. The strongly connected components of a graph are its maximally
connected sub-graphs.

We illustrate the definitions of CCG, CDG and SCC by an example shown in
Figure 3.1 . Here we use a @xmath mesh NoC (Figure 3.1 -b as the
communication backbone for an application characterized by the core
communication graph shown in Figure 3.1 -a. For each communication pair
@xmath in the CCG, assume the Manhattan distance (i.e. the minimum
number of hops) between the source and destination tile pair @xmath is
@xmath and the distance between the two tiles in the @xmath -direction
is @xmath , the total number of minimum length paths connecting the two
tiles is given by @xmath ¹ ¹ 1 @xmath is the combination notation, where
@xmath since we need to traverse along the @xmath direction @xmath times
among total @xmath number of hops. Let @xmath denote the @xmath path
connecting the two tiles. If @xmath traverses two network links @xmath
and @xmath consecutively, then an edge is added to connect the two nodes
@xmath and @xmath in the CDG. By inspecting all the feasible paths, we
can construct the whole channel dependency graph. In Figure 3.1 , taking
the communication pair @xmath as an example, two minimum length paths
exist: @xmath and @xmath .In the CDG, we add two edges connecting @xmath
and @xmath , respectively. As shown in Figure 3.1 -c, there are in total
10 strongly connected components in the resulting CDG which can be found
by the Tarjan’s algorithm [ 76 ] . We can find two circles in Figure 3.1
-c, i.e., @xmath and @xmath , where some edges are required to be
removed by the path set finding algorithm in Section 4.1 to avoid
deadlocks. {definition} Routing adaptivity: Routing adaptivity is
defined as the ratio of the total number of available paths provided by
a routing algorithm to the number of all possible minimum length paths
between the source and destination pairs. Higher adaptivity will improve
the capability of avoiding congestion and redistributing the traffic to
reduce thermal hotspot. However, we need to meet the deadlock-free
constraints while improving the routing adaptivity.

For a given application, we first use the task graph @xmath (Definition
1) to characterize its traffic patterns as well as the communication
volume between the tasks. Then based on the mapping algorithm on the
target NoC platform, the task graph @xmath is transformed to the core
communication graph @xmath (Definition 2). The channel dependency graph
@xmath (Definition 3) is built based on the routing paths assigned to
each edge in @xmath . In this chapter, we propose to find the strongly
connected components @xmath (Definition 4) of the underlying @xmath to
remove the circular dependency among the channel resources to avoid
potential deadlock. Finally, routing adaptivity (Definition 5) is the
metric that reflects the capability of redistributing traffic for
different cycle breaking algorithms in @xmath .

#### 3.3.2 Motivation for thermal-aware routing

For adaptive routing, normally there will be more than one path
available for every communication pair. If traffic is distributed
equally on all the paths, some of the routers may have more paths
passing through them and hence more packets to receive and send. We show
the need to allocate traffic properly among the paths by an example
illustrated in Figure 3.2 .

In Figure 3.2 , three communication pairs occur concurrently: from tile
P3 to P1, tile P1 to P5 and tile P4 to P0. We assume 1000 packets are
generated for each pair within a time window and sent over the network
in this example. The energy consumption of processing a single packet in
a router is denoted as @xmath . Two routing strategies are compared.
Strategy 1 uses uniform traffic distribution among all paths between the
source and the destination nodes for routing. Strategy 2 allocates
traffic non-uniformly among the candidate paths. The total number of
packets handled by each router is different in these two strategies. The
router energy distribution is shown in Figure 3.2 -c. By properly
allocating the traffic, we can reduce the peak energy of the tile by 16%
and the energy difference among the tiles by 37.5%. In this simple
example, we assume the average energy consumption of the processor core
and the router are about the same. We use Hotspot [ 77 ] to simulate and
evaluate the thermal profile. As shown in Figure 3.2 -d, strategy 2
indeed makes the thermal profile more uniform and reduces the hotspot
temperature.

#### 3.3.3 Thermal-aware routing design flow

From the above example, we can see that we need to find a set of paths
for routing the packets for every communication pair and allocate the
traffic properly among these paths so as to achieve a uniform power
consumption profile. One critical issue of determining the path set is
to provide deadlock avoidance. In generic routing algorithms, deadlock
is prevented by disallowing various turns [ 6 ] . For application
specific NoCs, it will be too conservative and unnecessarily prohibit
some legitimate paths to be used [ 23 ] as some disallowed turns can
actually be used because the application does not have traffics
interacting with these turns to form circular dependencies in the CDG.
By using the information specified by the task flow graph and the
derived communication graph, we can find more paths available for
routing and increase the flexibility of distributing traffic among the
sources and the destinations. Here we use a similar approach as [ 23 ]
to find the set of admissible paths for each communication pair while
still satisfying the deadlock free requirement.

After obtaining the admissible path set for routing, we find the optimal
traffic allocation to each path based on the bandwidth requirement to
achieve an even power distribution profile. The problem is formulated as
a mathematical programming problem and solved by a LP solver. These
phases of design are done offline at the design time. After that, the
optimal distribution ratio of each path is obtained. For each router and
a particular source-destination communication pair, the ratios of the
paths passing through it are combined into the probabilities of using
each port to route to the destination. At run time, these probability
values are stored into the routing table in each router. For each
incoming packet, the router will inquiry its routing table and return
the candidate output ports for this packet according to the input
direction and the destination. The final output port will be chosen
according to the probability value of each candidate.

Figure 3.3 summarizes the whole design flow of our proposed methodology.
In the next section, the details of the algorithm will be presented.

### 3.4 Main algorithm

In this section, we present the details of the offline routing
algorithms. We first discuss the algorithm of finding the set of
admissible paths for each source-destination communication pair. The
admissible paths avoid the circular dependency among any paths and hence
provide the deadlock free property. Then we present the optimal traffic
allocation problem formulation.

#### 3.4.1 Application-specific path set finding algorithm

In [ 23 ] , a dynamic routing algorithm that increases the average
routing adaptivity while maintaining deadlock free is proposed. The
average routing adaptivity is often used to represent the degree of
adaptiveness and flexibility of a routing algorithm. Here we use a
similar approach. Instead of maximizing the average routing adaptivity,
we aim to maximize the flexibility to re-divert the traffic to even out
the power consumption distribution. Therefore we have to consider the
bandwidth requirement of each communication also since the amount of
packet processed will directly affect the energy consumption.

Figure 3.4 shows the main flow of our path finding algorithm. Similar to
[ 23 ] , based on the application’s task flow graph, we examine all the
paths between the source and destination pairs to build the application
channel dependency graph (ACDG). Most likely, there will be cycles in
the ACDG so that some edges have to be removed to break these cycles to
guarantee deadlock free. In [ 23 ] , a branch and bound algorithm is
used to select the set of edges to remove all the cycles while
maintaining all the connectivity and maximizing the average adaptivity.
The average adaptivity @xmath is defined as:

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

where

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

The notations used in the above equations are summarized in Table 3.1 .
Connectivity is guaranteed for every communication @xmath by making sure
that at least one path exists between the source and destination nodes.
So we have @xmath . In this chapter, instead of finding all cycles in
the ACDG and breaking each cycle respectively, as done in [ 23 ] , we
apply Tarjan’s algorithm [ 76 ] to find all the strongly connected
components (SCC) and try to eliminate cycles within each nontrivial
components (components containing more than one vertex). One important
feature of SCC is that cycles of a directed graph are contained in the
same components. We then eliminate cycles within each nontrivial
components to achieve deadlock free. Tarjan’s algorithm is used because
the complexity is lower ( @xmath ). In many cases, several edges are
shared among different cycles (as illustrated in Figure 3.3 ,the two
edges @xmath and @xmath are shared among several cycles). If we inspect
each cycle separately, we may consider these edges more than once. On
the other hand, when we use Tarjan’s algorithm, cycles with common edges
are in the same component and hence decision can be made more
efficiently if we remove some shared edges to break these cycles
simultaneously. When we select edges to break the cycle, instead of
optimizing the average routing adaptivity, we maximize the following
objective function:

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

Here we weight the routing adaptivity of each communication with its
corresponding bandwidth requirement ( @xmath is the bandwidth of
communication @xmath ). The rationale is that for the communications
that have large bandwidth requirement, more packets need to be processed
and routed, the impact on the power consumption distribution is higher.
Therefore we should have higher flexibility for these communications to
re-divert the traffic and hence higher adaptivity.

#### 3.4.2 Optimal traffic allocation

In the following, we use the notations summarized in Table 3.2 and the
energy model described in section 3.4.2 to obtain the linear programming
(LP) formulation of the optimal traffic allocation problem.

##### Energy consumption model

We assume the energy consumption of each processor @xmath ( @xmath ) is
available after task mapping. Wormhole routing is used in our routing
scheme. In wormhole routing, each packet is divided into several flits
which are the minimum units for data transmission and flow control. For
every data packet, the head flit sets up the path directions for the
body and the tail flits. Thus, @xmath , @xmath and @xmath only incur
when the head flit is processed by the router. Total energy consumption
for processing a single packet in router @xmath is given by:

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

Let @xmath denote the number of packets that received by router @xmath ,
then the total router energy consumption is equal to @xmath . The total
energy of each tile @xmath ( @xmath ) is equal to @xmath .

##### LP problem formulation

Given the set of admission paths for every communication pair which is
deadlock free, we want to obtain the ratio of traffic allocated to each
path so as to minimize the maximum energy consumption among all the
tiles. We formulate the following linear programming problem to solve
the optimal path ratios:

1) Variables @xmath : ratio of traffic allocated to the @xmath path
@xmath between tiles @xmath and @xmath among all the @xmath paths, where
@xmath .

2) Objective functions : The energy consumption of the @xmath tile,
@xmath , is given by

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

where @xmath is the total number of packets received by router @xmath
per unit time and is equal to the summation of the number of the packets
from all paths that pass through tile @xmath , i.e., @xmath . @xmath is
given by:

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

In order to balance the tile energy @xmath , our objective function is
written in a min-max form as follows:

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

It is equivalent to:

  -- -------- -- -------
     @xmath      (3.8)
  -- -------- -- -------

3) Problem constraints : The objective function is optimized subject to
the following constraints:

3-1) Traffic split constraints : summation of all the traffic allocation
ratios between a given pair @xmath should equal to 1.

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

  -- -------- -- --------
     @xmath      (3.10)
  -- -------- -- --------

3-2) Bandwidth constraints : the aggregate bandwidth used for a specific
link should not exceed the link capacity.

The communication bandwidth = packet injection rate @xmath packet size
@xmath clock frequency. Assume @xmath is the cycle time and @xmath is a
physical link in the mesh NoC, a path @xmath will traverse this link if
@xmath . So we have

  -- -------- -- --------
     @xmath      (3.11)
  -- -------- -- --------

Given the application’s task flow graph and the task mapping, the packet
injection rate @xmath can be calculated by summing the bandwidth
requirement from all the communication pairs where the source tasks are
mapped onto tile @xmath and destination tasks are mapped onto tile
@xmath .

The above formulation is a typical linear programming problem and can be
solved efficiently using MATLAB CVX optimization toolbox [ 78 ] .

##### Using the path ratios in the routers

After solving the LP problem, we obtain a set of admissible paths and
their corresponding traffic allocation ratios. To use these information
in the implementation of the NoC routing, we can use two schemes. The
simplest way is source routing. In the source processor, the header flit
of each packet contains the entire path information. The source
processor @xmath decides which path to use to send the packet to
destination @xmath according to the traffic allocation ratios @xmath of
the set of admissible paths. Intel Teraflops chip [ 6 ] uses this
source-based routing scheme as the router will be simpler. However, this
will create a large overhead on the effective bandwidth as the packet
needs to contain additional payload to record the entire routing path.

A more efficient way of implementing the thermal aware routing is to use
routing tables stored in each router. One of the major advantages of
table-based routing is that it can be dynamically reconfigured or
reloaded [ 23 ] to allow modifications in the communication
requirements. However since the routing decision is made within each
router without knowing the entire path information, the path
traffic-allocation ratios can not be directly used. In the following, we
will show how to convert these ratios into local probability values for
the router to select which output port to send out the packet.

To support the thermal-aware routing, the routing table is organized as
follows: for each router @xmath in the mesh topology and each of its
input direction @xmath , there is a routing table @xmath [ 23 ] . For
each output port @xmath , there is a set of corresponding entries in the
routing table. Each entry consist of the tile id numbers of the source (
@xmath ), and destination ( @xmath ) pair, and the probability values
@xmath of using @xmath to route to @xmath . Formally, @xmath .

When using routing tables within the routers, the final routing path for
a packet of a specific traffic flow is composed by the output port
selected in each router along the path distributively. One problem may
hereby arise: will the selected output ports in the routers form a new
path that is not included in the admissible path set and hence introduce
the possibility of deadlock? In the following, we prove that path
generated at run-time using the table-based routing is indeed deadlock
free provided the path set used for generating the routing table @xmath
contains no circles. {lemma} (deadlock-free property in distributive
routing): For the table-based routing, after generating the distributed
routing table for each router according to the deadlock free path set
@xmath , the actual route generated at run time will not contain a
disallowed path not included in @xmath . Deadlock-free property is hence
inherited from the path set @xmath .

(by contradiction): We prove that a disallowed path @xmath can not exist
if all the entries of the routing table are created according to @xmath
. Assume @xmath is an actual path used to route a packet from node
@xmath to @xmath at run time (Figure 3.5 ). @xmath is obtained by
sequentially looking up the routing tables from routers @xmath to @xmath
. Assume this path @xmath is a disallowed path which may cause circular
dependencies with the other routing paths in @xmath , then at least one
edge @xmath is an element of @xmath in Table- 3.1 in order to generate
the circular dependencies. However, since @xmath is built from the
routing tables in router @xmath , which means there is an entry in the
routing table of @xmath using link @xmath as input and link @xmath as
output. It is already known the entries of the routing tables are
created according to the paths in the admissible path set @xmath .
Therefore, there is another path @xmath while @xmath traversing through
@xmath from the input link @xmath to the output link @xmath , hence
@xmath also contains the edge @xmath in its CDG. On the other hand,
since @xmath , it will create cycles in the CDG and @xmath is actually
not a deadlock-free path set. This contradicts with the assumption.

Now the issue is how to obtain the probability values @xmath for each
output @xmath from the path traffic allocation ratios. For each router
@xmath and each of its input port @xmath , we obtain the subset of
admissible paths ( i.e., @xmath ) that pass through @xmath using input
port @xmath . For a given destination tile @xmath , using minimum path
routing, only two candidate output ports ( @xmath and @xmath ) are
feasible. The probabilities of selecting ports @xmath and @xmath are
calculated by comparing the aggregate traffic of the paths inside @xmath
that use ports @xmath and @xmath , respectively, to route to the tile
@xmath . Using the notations in Table 3.1 , let paths @xmath and @xmath
, we have:

  -- -------- -- --------
     @xmath      (3.12)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (3.13)
  -- -------- -- --------

After considering all the routers and the communication pairs, the port
probability values are obtained and stored into the routing tables
offline.

In order to maintain the global path traffic allocation ratios, each
entry in the routing table @xmath needs to distinguish the source tile
location @xmath of the packet. Thus the number of entries in the routing
table is increased. We can reduce the table size by grouping the entries
of different sources but the same destinations together. The new format
of the routing table becomes @xmath . In this case, the port probability
value @xmath in @xmath ) is calculated as:

  -- -------- -- --------
     @xmath      (3.14)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (3.15)
  -- -------- -- --------

Although the exact path traffic allocation ratios cannot be maintained,
the traffic distribution on each router and each link can still be
maintained which is more important for uniform energy distribution
purpose. From the simulation results which will be presented in Section
6, this routing table implementation achieves similar improvement in
peak energy reduction and latency performance compared to the routing
table using the source-destination pair.

### 3.5 Router microarchitecture

The block diagram of our proposed router to support thermal aware
routing is illustrated in Figure 3.6 -a. For minimum path routing, if
the input direction and the destination are fixed, there are at most two
candidate output ports, @xmath and @xmath , and @xmath . The routing
selection unit in the router selects the output port @xmath by comparing
the probability value @xmath with a random number @xmath in @xmath . It
will select port @xmath @xmath , otherwise port @xmath will be chosen.

A pseudo random number generator using linear feedback shift register
(LFSR) is employed to generate the random number. The header flits from
each input port are first decoded by a parser (the HPU module shown in
Figure 3.6 -b) to extract the destinations. Then by checking @xmath ,
two candidate output ports are returned with the corresponding
probability values. The output port selection unit then make a decision
on the output port. In addition to the probability selection,
backpressure information ( Bp_1 and Bp_2 in Figure 3.6 -c) from
downstream routers are also used. If one candidate output port is not
available due to limited buffer space, the backpressure signal will
disable this output port from the selection.

### 3.6 Experimental results

#### 3.6.1 Simulation environment setup

We implemented the proposed thermal-aware routing strategy for
mesh-based NoC architecture. A C++ program is developed to analyze the
traffic parameters automatically and generate the corresponding LP
formulation for a given application. The LP problem is then solved by
CVX [ 78 ] which is a toolbox embedded in MATLAB for convex
optimization. In order to evaluate the NoC performance, a systemC based
cycle-accurate simulator was developed combining the features of several
widely adopted academic tools (including Noxim [ 79 ] , Nirgam [ 80 ]
and Booksim [ 61 ] ). We used both synthetic traffic and real benchmarks
to evaluate the performance and compare with other deadlock free routing
algorithms (westfirst, northlast, negativefirst, oddeven and X-Y routing
[ 6 ] ). The real benchmarks include PIP (Picture-In-picture) [ 68 ] ,
MWD (Multi-Window Display) [ 68 ] , MPEG4 [ 81 ] , VOPD (Video Object
Plane Decoder) [ 68 ] , MMS_1(Multimedia system mapping to 16 cores) [
67 ] , MMS_2(Multimedia system mapping to 25 cores) [ 82 ] and DVOPD
(Dual Video Object Plane Decoder) [ 62 ] . For synthetic traffic, we use
8 different traffic scenarios, namely Uniform random, Transpose-1,
Transpose-2 [ 83 ] , Hotspot in center (Hotspot-C) [ 84 ] , Hotspot in
top-right corner (hotspot-Tr) [ 23 ] , Hotspot in right-side
(Hotspot-Rs) [ 23 ] , Butterfly [ 79 ] and Bursty traffic [ 79 ] .
@xmath , @xmath and @xmath NoC meshes are used for different benchmarks
and @xmath meshes are used for all synthetic traffics.

#### 3.6.2 Adaptivity improvement

First we compare the routing adaptivity with other routing algorithm
under several real benchmarks. The results are shown in Figure 3.7 . It
can be seen that 20%-30% more paths are available if we take the
application traffic into consideration.

#### 3.6.3 Peak energy simulation results

Next we evaluate the performance of the proposed algorithm in peak
energy reduction. In the experiment, energy parameters ( @xmath etc.)
are adopted from Noxim and Booksim simulator. After @xmath warm-up
cycles, we carry out energy simulation using a fixed time window for all
cases. Different simulations were done for each packet injection rate to
obtain the average peak energy consumption. The simulation results for
the real benchmarks and the synthetic benchmarks are shown in Figure 3.8
and Figure 3.9 - 3.11 , respectively.

In the figures, the proposed source-destination routing table and
destination-only routing table schemes are denoted as opt_source_dest
and opt_dest, respectively. The theoretical optimal energy consumption
which is obtained by solving the LP formulation using CVX is also
included as a reference and is denoted as opt_peak.

From Figures 3.8 and 3.9 , we can see that comparing with other adaptive
routing algorithms, the proposed routing algorithm can achieve more than
20% peak energy reduction in most traffics. At the same time, the
latency performance can also be improved or maintained for all the
cases.

In Figure 3.12 , we illustrate a scenario of the tile energy
distribution under Hotspot-C traffic ( i.e., four center cores are
traffic hotspots). Figure 3.12 -a is the energy distribution using the
proposed thermal-aware routing algorithm, and Figures 3.12 -b, -c and -d
show the energy distribution using odd-even, negative-first and XY
routing, respectively. For all the four cases, the total energy
consumption of the NoC is same, i.e., @xmath . However, the peak tile
energy of the proposed, odd-even, negative-first and XY routing are
@xmath , @xmath , @xmath and @xmath respectively. From the figures, we
can see that our proposed scheme indeed leads to a more uniform energy
profile across the NoC chip.

In Table 3.3 , we summarize the execution time of our off-line routing
algorithm (including path generation and LP solving) under various mesh
size and communication density @xmath ( @xmath is defined as the ratio
of the total number of communications pairs to the number of processors
in mesh). It can be seen that the execution time is reasonable. For
larger mesh size ( @xmath or more) and more communication pairs (100 or
more), the number of minimum paths increase dramatically. It takes
longer time (2 -3 hours in average) to obtain the traffic allocation
ratios. Since the traffic allocation is determined offline, in most
cases, the time cost is still affordable. In case we want to reduce the
execution time, we can restrict the number of minimum-length paths to a
smaller subset.

#### 3.6.4 Simulation results with different processor/router energy
ratio

The energy consumption of a tile consists of that of the processor and
the router. For different applications, the power contribution of the
router and the processor varies greatly. Our thermal-aware routing
algorithm can only re-distribute the router power. We want to evaluate
the effectiveness of our algorithm on the peak energy reduction when the
ratio of the energy contribution from the router and the processor
varies. Let @xmath . The experimental results shown in the previous
sections assume @xmath . In this sub-section, we simulate the peak
energy reduction using our routing scheme for different @xmath values.
Tables 3.4 , 3.5 and 3.6 summarize the results for synthetic traffic and
real benchmarks, respectively. From the results we can see that when
@xmath increases, the peak energy reduction is smaller as the relative
contribution of the router energy reduces. Overall, we can achieve an
average 6%-17% peak energy reduction over other two existing routing
schemes when @xmath ranges from @xmath to @xmath across all the
benchmarks.

### 3.7 Conclusion

NoC has been widely adopted to handle the complicate communications for
future MPSoCs. As temperature becomes one key constraint in NoC, in this
chapter, we propose an application-specific and thermal-aware routing
algorithm to distribute the traffic more uniformly across the chip. A
deadlock free path set finding algorithm is first utilized to maximize
the routing adaptivity. A linear programming (LP) problem is formulated
to allocate traffic properly among the paths. A table-based router is
also designed to select output ports according to the traffic allocation
ratios. From the simulation results, the peak energy reduction can be as
high as @xmath for both synthetic traffic and real benchmarks.

## Chapter 4 Fault-tolerant Routing Scheme for NoCs using Dynamic
Reconfiguration of Partial-Faulty Routing Resources

In this chapter, we propose a fault-tolerant framework for
Network-on-Chips (NoC) to achieve maximum performance under fault. A
fine-grained fault model is first introduced. Different from the
traditional link or node NoC fault models which assume the faulty
resource is totally unfunctional, we distinguish the faulty components
and handle them according to their fault classes. By doing so, we can
avoid unnecessary partitioning of the network and hence achieve a higher
connectivity under high fault rate. In particular, two new dynamic
re-configuration schemes at the router level, namely Dynamic Buffer
Swapping (DBS) and Dynamic MUX Swapping (DMS), are proposed to deal with
the buffer and crossbar faults, which are the main sources of failure in
the router. In these schemes, the healthy resources in the router are
maximally utilized to mitigate the faults. A deadlock recovery scheme is
designed to handle the potential deadlock hazard due to the proposed DBS
operation. Experimental results show that we can achieve higher packet
acceptance rate as well as lower latency compared with state-of-the-art
fault-tolerant routing schemes

### 4.1 Introduction

For NoC-based Multiprocessor-System-on-Chip (MPSOC), the complex system
is highly susceptible to the prominence of faults [ 86 ] . A significant
error tolerance to both permanent and temporary faults is thus strongly
desired. Temporary errors are mainly due to the on-chip crosstalks and
coupling noise while permanent faults are mostly caused by the worn-out
devices, manufacturing defects and accelerated aging effects [ 10 ] .
The NoC architecture offers a good opportunity of achieving high
reliability since it provides multiple paths between the
source-destination communication pairs and allows reconfiguration under
faulty conditions.

To achieve fault tolerance in NoC, a proper fault model is required for
the clear identification and isolation of the defective components so
that the routing algorithm can maximally utilize the remaining
functionality to achieve the optimal performance. Traditional NoC fault
model treats the router fault as a node fault [ 85 ] or a link fault [
86 , 9 , 19 ] and assumes total outage of the faulty resource. However,
a fine-grained functional fault model will be more beneficial because
graceful degradation of network operation by exploiting the partially
functional resources can be achieved [ 10 ] . In this chapter, we
identify the major fault components in the NoC and propose a
fine-grained permanent fault model, which distinguishes the partial
router faults (buffer or crossbar faults) from the hard link faults.
Hard link faults may lead to a partitioned network. On the other hand, a
faulty router with an appropriate reconfiguration strategy, may be able
to tolerate the buffer or crossbar errors and still supports the full
connectivity in the NoC with reduced capacity. This motivates a need to
develop an integrated fault handling methodology for different fault
classes. An example comparing different fault models given the same
fault pattern is shown in Fig. 4.1 . As shown in the figure, if we adopt
the generic node or link fault model (Fig. 4.1 -a and -b) which do not
consider whether the fault is caused by the buffer or the crossbar, we
may isolate many nodes from the network. To increase the connectivity
under faulty situation, [ 19 ] adopts a static offline port swap
algorithm to reduce the number of faulty links (replacing port P_a with
port P_b in this example). However, the resultant network is still
partitioned, because the port fault is treated as a hard link fault
under this algorithm.

In this chapter, we propose a dynamic swapping scheme which shares the
healthy resource in the router for different ports at run-time. As shown
in Fig. 4.1 -d, by leveraging the other healthy buffers or multiplexers
(Muxes) and dynamically re-allocating the resources, the network
connectivity can remain intact which result in better performance in
terms of packet acceptance rate. Two new techniques (DBS and DMS) are
proposed to handle the buffer and crossbar faults, respectively. An
approach that combines the turn model based routing and an efficient
deadlock recovery scheme is also proposed to resolve the potential
deadlock issue when the routers are working in the DBS mode.

### 4.2 Background

The growing concern on reliable NoC has prompted extensive researches in
this area, from self-detect and self-diagnosis routers [ 9 ] to
fault-tolerant routing algorithm design [ 86 ] . In [ 9 ] , a router
with built-in-self-test (BIST) and self-diagnose circuits is proposed to
detect and locate the faults in the FIFOs and the crossbar MUXes. After
identifying the faults, the FIFO faults and the MUX faults are treated
as input and output link faults, respectively. This fault categorization
method reduces the connectivity of the network and hence degrades the
performance if there are a large number of faults. In [ 10 ] , a
detailed online fault diagnosis framework is proposed to diagnose the
link, crossbar and arbitration/allocation logic faults. The remaining
healthy paths are identified from the faulty paths and the routing
algorithm utilizes these healthy paths to achieve graceful degradation.
However, the faulty paths can still lead to network partitioning as they
are determined statically. Moreover, this work is designed for
buffer-less router and may not be suitable for buffered router
architecture, which is required for high throughput and low latency
design.

There have been many fault-tolerant routing algorithms proposed to deal
with faulty components in NoC. In [ 85 ] , the faulty router is treated
as a node fault and the routing algorithm can be dynamically
reconfigured to re-route the packets following the contour around the
faulty nodes. This region-based routing algorithm is only suitable for
one-faulty-router topology. Also, the node fault model may cause the
faulty router totally isolated from the network, while in real
situation, the faults may only affect a certain part of the router and
the rest of it can still function correctly. In [ 86 ] , the NoC faults
are modeled as link level hard failures and a highly resilient routing
algorithm is designed to reconfigure the routing table in an offline
process. The algorithm is static in nature and there exists a trade-off
between the network connectivity and the deadlock avoidance by
selectively removing the turn rules. In [ 87 ] , a deflection routing
algorithm is proposed to dynamically re-route the packets towards the
destinations based on the stress factors which reflect the traffic
condition. The NoC faults are treated as the link faults and it does not
clearly describe how to avoid deadlock and livelock based on the stress
factors.

### 4.3 Fault-tolerant NoC design

In the context of NoC, the most common source of failures includes link
errors and single-event upsets within the router [ 88 ] . Since the
input buffers and the crossbar occupy the majority of the router area (
@xmath for the FIFO buffer and @xmath for the crossbar MUX in [ 9 ] ) ,
most of the faults in the router may occur there [ 19 ] . In this
chapter, first we propose a hybrid fine-grained fault model to
distinguish the nature and consequence of different faults and briefly
describe the fault-diagnosis technique to distinguish the types of fault
[ 89 ] . Then, we introduce the DBS and DMS techniques which dynamically
reconfigure the routers to handle buffer faults and crossbar faults
while still maintaining the maximum service of the router. After that,
the scheme to deal with link faults is discussed. Finally, we introduce
the deadlock recovery mechanism used in our framework.

#### 4.3.1 Fault-diagnosis and hybrid fault model

##### Fault-diagnosis infrastructure

The fault diagnosis techniques not only should detect the occurrence of
faults but also should indicate precisely the type as well as the
position of the faults. In this work, we assume the NoC fault diagnostic
technique proposed in [ 9 , 10 ] is used (shown in Fig. 4.2 ), which
provides the detail information of the faults in router. In the
following, we briefly review the state-of-the-art fault-diagnosis
structures in NoC [ 9 , 10 , 19 ] .

As shown in Fig. 4.2 , the cyclic redundancy check (CRC) units are used
for the link fault diagnosis. The packet correctness is verified by the
CRC modules associated with each input port. The link transmission
errors is characterized by a corresponding checksum mismatch [ 10 ] .
Once a switch-to-switch transmission error is detected, the BIST
controller in the upstream router will start the link diagnosis by
sending the test patterns to the downstream neighbor and find out the
specific link wires that have faults. Similarly, the intra-router error
will be discovered if the packets pass the CRC check at the input of the
router but fail at the output side [ 10 ] . On the detection of the
internal fault, the router will switch to the test mode to diagnose the
specific faulty components such as the input buffers or switch
crossbars. As illustrated in Fig 4.2 , the Test Pattern Generator (TPG)
will generate the test vectors for the buffers (TPG1) and crossbar MUX
(TPG2) respectively, while the Signature Analyzer will compare the
results and identify the specific faulty components in either FIFO
buffers or the crossbar. The diagnosis results are finally forwarded to
the DBS and DMS units for router reconfiguration based on the proposed
hybrid fault model.

##### A novel hybrid fault model

Table 4.1 summarizes our proposed hybrid fault model. The permanent
faults are divided into two classes: the static and the dynamic fault.
Faults occurred at the interconnect wires is denoted as link fault and
is classified as a static fault, which physically disconnects two nodes.
Of course in real situation, not all the wires are faulty and we can
still exploit the healthy wires to maintain the connectivity similar to
the approaches proposed in [ 90 ] . In this chapter, we mainly focus on
the recovery of the router faults and hence we assume a totally-failure
link fault. To handle the static fault, a fault-aware routing algorithm
is exploited to bypass the error links. On the other hand, if the fault
occurs within the buffer or the crossbar, we can mitigate them by
reusing the other healthy resources in the router. All the datapath will
remain intact with a small penalty of reduced bandwidth. This kind of
fault is categorized as dynamic faults in our model.

#### 4.3.2 Dynamic buffer swapping (DBS)

Dynamic buffer swapping (DBS) algorithm is proposed to handle the input
buffer faults. For traditional fault model, the buffer fault is treated
as a hard link fault and data cannot be transmitted even the link is
healthy. In our approach, through proper swapping of another substitute
buffer to hold the packets destined for the faulty buffer, a graceful
performance degradation can be achieved by fully utilizing the link. By
adopting DBS algorithm, no extra buffers or virtual channels are needed
in the router, which satisfies the stringent area constraint of NoC [ 85
] .

##### DBS operation

DBS technique operates with three states. As shown in Figure 4.3 ,
assume fault occurs at the north input buffer of router R1, packet 0 is
blocked in router R2. We denote this situation as state S0. After
receiving the transmission request token, R1 enters the port-swap
scheduling state S1. In S1, R1 will arrange another input port buffer to
substitute the north buffer based on a predefined swapping strategy.
Assume the east input buffer is chosen in this example. Then a traffic
control token is propagated to router R4 indicating that the
transmission link (R1-R4) will be shut down in the next state for DBS
operation. Upon the reception of this flow control token, R4 will stop
allocating and granting new requests to traverse through its west output
port after the tail flit of packet 2 finishes routing. R1 will also
continuously monitor its east input buffer. Once the buffer serves the
tail flit of packet 2, it will enter the buffer swapping state S2. In
this state, the communication between R2-R1 link is enabled while link
R4-R1 is currently disabled. The packet 0 can thus be transmitted to
R1’s east input buffer and further be forwarded towards its
destinations. After packet 0 departs R1, R1 returns to normal state S0
and link R4-R1 is re-enabled again. The router will keep running at
state S0 until another transmission request is asserted to use R1’s
north input buffer.

##### Avoid packet interleaving in DBS

One situation needs to be addressed and avoided is the packet-level
interleaving when DBS operation is performed. In the previous example,
in S1, R4 will control its traffic towards its west output port
temporarily. Without this control, if R4 continues its transmission for
the following packets (packet 3 in Fig. 4.4 for instance) and R1 jumps
to state S2 for buffer swapping operation, the east buffer of R1 will
store part of the flits from packet 3 and the whole packet 0 from R2.
The remaining flits of packet 3 are still in R4 waiting to be routed.
Thus packet 0 will interleave with packet 3 in the east input buffer of
R1. The header of packet 0 will erase the output reservation information
created for packet 3. After packet 0 is sent and R1 returns to the
initial state S0, the residual flits of packet 3 will enter R1 from R4.
These flits lose the routing information in R1 which is previously
reserved by their header and have to be discarded.

Figure 4.4 illustrates the above situation. In order to avoid
interleaved packets due to the DBS operation, a simple but effective
flow control scheme is developed. Two handshake signals, current
transmit status (CTS) and current port status (CPS), are associated with
each port. Both CTS and CPS signals are sent by the faulty routers and
read by their neighbors. The CTS (1bit) token manages the link
transmission. The router can send the packets across the link only when
it receives a ’1’ in the CTS. The CPS (2 bits) token is responsible for
intra-router transmission. If “00” is read from one port, the port can
be used as an normal output port for all the packets. When “01” is read,
the router ( R4 in Fig. 4.3 ) will begin monitor this port and grant it
only to the packets which have been partially transmitted across the
link (packet 1 and 2 in Fig. 4.3 ) to avoid the packet-level
interleaving. Once the residual tail flit ( @xmath in Fig. 4.3 ) is
received, the faulty router enters buffer swapping state, and sends CPS
“10” to its swapped port (east output port of R1 in Fig. 4.3 ) and the
corresponding neighbor (R4 in Fig. 4.3 ) will not transmit packets to
this link during the whole state. Within each router, three registers
are used to record the faulty port (FP_R), the scheduled swapping port
(SP_R) and the disabled port (DP_R), respectively. The status of these
tokens for the example shown in Fig. 4.3 are summarized in Table 4.2 .

##### buffer swapping strategy design

In DBS we need to schedule a healthy port to swap with the faulty port.
As shown in Fig. 4.3 , a buffer swapper is needed to match the physical
link connections with the swapped healthy FIFO input. We can use a
five-by-five port swapper to connect each physical link with every input
buffer, similar to the swapper used in [ 19 ] . This architecture
provides the highest flexibility for buffer swap scheduling. A
round-robin DBS scheduling can be adopted by dynamically using different
buffers to swap with the faulty port so to even out the bandwidth
penalty for each input. For example, if the north port is faulty and in
the previous swapping state, we use the east port to hold the packets
from the north, then we will choose to use the south port to handle the
next request for the north port.

A more area-efficient implementation for buffer swapping is to use a
dedicated swapping strategy. This will reduce the area overhead of the
port swapper as we do not need to have a crossbar-like connection
architecture. The faulty port can only choose its predefined neighbor
for swapping operation. From the simulation results, which will be
presented in the next section, it is shown that this swapping strategy
achieves only a little performance penalty compared with the round-robin
DBS.

#### 4.3.3 Dynamic MUX swapping (DMS)

The crossbar in the router is usually implemented in the form of five
@xmath MUXs [ 9 ] . As shown in Fig. 4.5 ,

if one of the MUX malfunctions, the data will not be presented to the
output link correctly. Here we present a dynamic MUX swapping (DMS)
technique to mitigate the MUX faults and make all the output links
available for routing. The proposed ring-based crossbar swapper is shown
in Fig. 4.5 . The ring topology allows each MUX to be shared among its
neighboring output ports. If the MUX corresponding to the east output
port is faulty, the MUX for the north port can be time shared with the
north and east ports by turning on-off the switches in MUX swapper
accordingly. By doing so, all the output links can be maintained.

#### 4.3.4 Handling hard link faults

For the link wire faults, since the physical connectivity between the
routers has been damaged, we need to use a robust routing algorithm to
bypass the faulty links. In this work, we use a similar approach as
proposed in [ 86 , 19 ] to route the packets under the existence of hard
link faults.

#### 4.3.5 Deadlock issue and handling strategy

##### Deadlock in proposed framework

To ensure deadlock free routing, in normal operation, Oddeven (OE)
routing algorithm [ 91 ] is used in our framework. However when faults
occur, due to the DBS or the re-routing, a deadlock may be introduced.

Figure 4.6 gives an example of two deadlock situations. In Fig. 4.6 -a,
a local deadlock will occur between the two nodes R1 and R2 if the south
port of R2 and north port of R1 are faulty and all the packets in other
ports choose these two ports as outputs. In this case, R1 (R2) will be
stuck at state S1 waiting for the original packets in the substitute
port of R2 (R1) being completely transmitted. In Fig. 4.6 -b, a global
blockage situation is illustrated assuming the south to east turn is
forbidden. If the scheduler in R2 arranges its west input buffer to hold
packets from north and the scheduler in R1 arranges its east input
buffer to substitute the faulty south buffer, a circular dependency on
buffer resources can thus be generated as in the figure. According to [
92 ] , this kind of circular dependency on the NoC resources may bring
deadlocks and stall the whole system.

For DMS operation, we only time-share the MUX between two output ports
and do not change the dependency of the buffers in the routing
algorithm. Hence it will not create deadlock if the original routing
algorithm is deadlock-free.

##### Strategies to relieve deadlock

We employ a detection scheme, similar to the DISHA approach in computer
network [ 93 ] to detect the blockage in the network. When the router
receives the header flit of a packet, an embedded counter is triggered
to count the number of cycles it stays in the buffer. The counter
continues to increment until the tail flit has been successfully
transmitted or its value exceeds a predefined threshold @xmath . In the
latter case, the router will enter the deadlock handling mode. After
that the counter would be reset and waits for next packet.
In this chapter, we propose a set of strategies to relieve the deadlock
situation. We mainly deal with the packets in the faulty routers in DBS
mode because DMS and the routing algorithm will not generate circular
dependency. First, we set the SP_R register to the next port so as to
initialize another swapping operation using a different substitute port.
In Fig. 4.6 -b, by using the east input buffer of R2 to swap with the
north faulty buffer, we can break the original dependency and avoid the
blockage. If all the ports in the router are not available for swapping
as the case in Fig. 4.6 -a, we will reset the routing computation (RC),
switch allocation (SA) information of these packets to request a
different output port. In figure 4.6 -a, if packet 1 chooses the west
instead of south as output direction and leaves R2 successfully, then
the deadlock problem can be solved accordingly.

If the re-computation step fails to relieve the blockage, and the local
processing element (PE) has available memory spaces in network interface
(NI), next, we will send these blocked packets to the local PE, and make
the PE re-send these packets to the destination. In order to avoid
creating a new circular dependency between the PE and other routers,
this operation is carried out only when NI has free memory slots. If the
memory in NI is full, we need to drop the current packet and require a
higher level end-to-end re-transmission protocol to re-send the packets
[ 10 ] . For hard link faults, as mentioned in [ 86 ] , in order to
improve network connectivity, we need to selectively remove the turn
rules of some routers at the cost of increasing the hazard of deadlock.
The scheme discussed above can also be applied to the packets that
violate these turn rules so as to return to normal status.

### 4.4 Simulation Results

#### 4.4.1 Simulation environment setup

In order to evaluate the NoC performance, a systemC based NoC simulator
was developed. The energy parameters are adopted from Noxim [ 79 ] . The
mesh topology is used for the simulation due to its popularity and
simplicity. @xmath meshes are used in the simulation for all the
synthetic traffics. Several real benchmarks are also used for evaluating
the latency effects. They are MMS (Multimedia system mapped on @xmath
mesh) in [ 94 ] , DVOPD (Dual Video Object Plane Decoder mapped on
@xmath mesh) and MPEG4 (MPEG4 codec) in [ 68 ] . The simulation is
carried out for @xmath cycles after @xmath warm up cycles.

#### 4.4.2 Comparison of connectivity of the network

We define fault rate @xmath as follows: @xmath ; and the parameter
@xmath indicates the portion of the partial router faults in the
injected fault pattern: @xmath ; the connectivity of a network is
defined as the ratio of number of node pairs that can communicate to
total number of node pairs in the network. Figure 4.7 is the analytical
results for the connectivity for @xmath mesh.

For each fault rate, we randomly generate 30 fault patterns. Then the
connectivity is analyzed by checking whether each node pair can
communicate under the given pattern. From the simulation, we demonstrate
by distinguishing the dynamic faults with static faults, network
connectivity can be improved more than 20% compared to the link fault
model.

#### 4.4.3 Improvement in packet acceptance rate

In table 4.3 , we show the simulation results for the packet acceptance
rate compared with two state-of-art works in [ 86 , 87 ] . We randomly
generate 20 fault patterns for each fault rate and use random traffic to
evaluate this merit. The packets between two nodes which can not
communicate anymore will be counted as dropped packets as well. Under
high fault rate, the connectivity drops dramatically in [ 86 , 87 ] and
hence reduces the packet acceptance rate. On the contrary, by performing
the DBS and DMS operations, the probability of a successful transmission
between any two nodes is greatly improved. Hence, @xmath - @xmath higher
acceptance rate can be achieved.

#### 4.4.4 Effects on latency performance

In Fig. 4.8 , we make a comparison of the proposed round-robin and
dedicated DBS operation for @xmath mesh. Both two strategies outperform
the resilient routing in [ 86 ] and a 1.15 @xmath improvement in the
saturation point can be observed.

Fig. 4.9 is the simulation results for @xmath mesh under the hybrid
fault situation. As discussed earlier, DMS will not generate deadlocks
so the performance of DMS operation is the closest to the baseline
routers. While the DBS operation may bring deadlocks and the latency
depends on the predefined threshold of the deadlock detection scheme (
@xmath and @xmath ) . It is noticeable that the performance in our work
is consistently better than [ 86 ] when faults can be categorized as
dynamic faults.

In Table 4.4 , we summarize the latency and acceptance rate performance
for the traffic scenarios other than the uniform random case. Two
different fault numbers, @xmath and @xmath are used for each traffic. We
can see that for different traffic scenarios, an average @xmath
reduction in latency and @xmath improvement in reliability can be
achieved.

### 4.5 Conclusion

In this chapter, we propose a fine-grained fault model for network on
chips to maximally maintain the network connectivity. Based on the fault
model, we distinguish with different fault components and use specific
strategies to handle these faults. DBS and DMS techniques are proposed
to make the partial router still functional and hence improve the system
robustness. Simulation results demonstrate higher packet acceptance rate
and better latency performance can be achieved.

## Chapter 5 FSNoC: A Flit-level Speedup Scheme For NoCs Using
Self-Reconfigurable Bi-directional Channels

In this chapter, we tackle the problem of optimizing the bandwidth
utilization of the Network-on-Chips (NoC). More specifically, we propose
a flit-level speedup scheme to enhance the NoC performance utilizing
self-reconfigurable bidirectional channels. For the NoC intra-router
bandwidth, in addition to the traditional efforts on allowing flits from
different packets using the idling internal bandwidth of the crossbar,
our proposed flit-level speedup scheme also allows flits within the same
packet to be transmitted simultaneously. For inter-router transmission,
a novel distributed channel configuration protocol is developed to
dynamically control the link directions. In this way, the bisection
bandwidth between the routers can adapt to the changing network status.
We describe the implementation of the proposed flit-level speedup NoC on
a two-dimensional mesh. An input buffer architecture which supports
reading and writing two flits from the same virtual channel at one time
is proposed. The switch allocator is also designed to support flit-level
parallel arbitration. Extensive simulations on both the synthetic
traffic and real application benchmarks, show performance improvement in
throughput and latency over the existing architectures using
bi-directional channels.

### 5.1 Introduction

NoCs utilize the interconnected routers instead of buses or
point-to-point wires to send and receive packets between processor
elements (PE). Due to the high communication bandwidth provided, it
overcomes the scalability limitations of the buses and bring significant
performance improvement in terms of communication latency, power
consumption and reliability etc [ 95 ] . A typical NoC consists of the
processing elements (PEs), network interfaces (NIs) and the routers. The
latter two make up of the communication infrastructures in order to
support high-bandwidth communication. The NIs are used to packetize the
message and transform each packet from the clients (PE cores) into
fixed-length flow-control units ( i.e., flits). The flits are then sent
to the attached router, whose function is to route packets from the
input ports to an appropriate output port. In a typical four-stage
pipelined, virtual-channel (VC) based router [ 6 ] architecture (shown
in Fig. 5.1 ), the router needs to compute the next hop direction (RC
stage) and allocates a virtual channel (VA stage) for every incoming
header flit. Then the body and tail flits follow the path settled and
only need to contend for the switch entries (SA stage). The winning
flits will traverse the router to the output port (ST stage) and move to
the next hop routers (LT stage). In general, the bandwidth provided by
NoC and its utilization have a significant impact on the overall
performance [ 20 ] . As shown in Fig. 5.1 , the NoC bandwidth can be
categorized into two types: the inter-router bandwidth (channel
bandwidth) and the intra-router bandwidth (switch bandwidth). The
inter-router bandwidth is a set of physical interconnection wires
between the routers (or router and NI) to handle either the incoming or
outgoing traffic from the neighboring routers (or PEs). The intra-router
bandwidth is dictated by the datapath within the router ( e.g., the
input buffers, the crossbar fabric and the output registers).

In order to improve the usage of internal bandwidth, in [ 20 ] , NoC
with input speedup is proposed by providing some excess bandwidth in the
cross-point to relieve the contentions in the routers. In [ 96 ] , a
fine-grained buffer utilization scheme ViChaR is proposed which target
dynamic adaption of the router buffers by providing unified buffer
resources across all input ports. Due to the efficient usage of the
buffers, a 25% performance increase can be achieved over generic
routers.

In addition to the efforts on optimizing the internal bandwidth
utilization, several approaches have been proposed recently to
dynamically adjust the inter-router channel resources to match the run
time workload requirements. In [ 97 ] , based on the observation that
NoC link resources are generally over-provisioned, a dynamic channel
width configuration algorithm is proposed to adjust the link bandwidth
for the current application to optimize the energy-performance
trade-offs. In [ 98 ] , an adaptive physical channel regulator (APCR) is
proposed to provide finer granularity of using link channels. The flit
size in APCR router is less than the physical channel width (phit).
Therefore, the APCR scheme allows flits from different packets to share
the same physical bandwidth at run time. However, this concurrent
transmission of multiple flits from different packets are implemented at
the cost of increased physical link bandwidth. In [ 99 ] , a
heterogeneous NoC architecture is proposed where wider links are
allocated to routers that require more resources. At run time, two flits
are combined when possible and then sent through the wider link in order
to reduce the delay.

Besides the above approaches which are based on the uni-directional
links, new progresses have been made by utilizing the
self-reconfigurable bi-directional channels. This is motivated by the
observation that, between two routers, quite often one uni-directional
link may be overflowed with heavy traffic while the link for the
opposite direction remains idle [ 100 , 101 , 102 ] . The inefficient
link bandwidth usage in uni-directional NoC may limit the system latency
and throughput. In [ 100 ] , the link directions are configured in the
architecture level with route re-allocation to meet the bandwidth and
QoS requirement. In [ 101 , 102 ] , NoC architectures with run time
reconfigurable bi-directional channels were proposed. The direction of
each link is decided at run time depending on the traffics of the two
routers using either an external bandwidth arbiter [ 102 ] or a channel
direction control (CDC) protocol [ 101 ] . For both approaches, the link
direction is reversed if there are more than two packets requesting to
traverse along one direction and no packets requesting in the opposite
direction. Besides the performance improvement in latency and
throughput, NoCs with bi-directional channels have also demonstrated the
advantages in mitigating the static and dynamic channel failures [ 103 ]
, providing QoS guarantees [ 104 ] as well as better energy-performance
trade-offs [ 105 ] .

In this chapter, based on the self-reconfigurable bi-directional
channels, we propose a flit-level speedup scheme to further enhance the
NoC performance. More specifically, by allowing flits within the same
packet to be transmitted simultaneously using the inter- and intra-
router bandwidth, the channel utilization is improved and therefore
better latency/throughput performance can be achieved. Moreover, a new
channel direction control method is proposed which masks inter-router
signaling delays within the router pipeline depth and is efficient for
configuring the directions of long wire links using repeaters or
pipelined registers. The input buffers which support reading and writing
two flits from the same virtual channel is proposed. The switch
allocator is also designed to support flit-level parallel arbitration.
Both the synthetic and real-world traffic patterns have been used to
evaluate the performance of proposed flit-level speedup scheme. The
hardware overhead is also compared to the conventional NoC and BiNoC
designs. The results demonstrate better latency and throughput
performance can be achieved with moderate power and area overhead than
the traditional NoC and BiNoC schemes.

Compared to the previous works, this chapter makes the following key
contributions:

1.  A fine-grained, flit-level transmission scheme is proposed which
    provides more flexibility in using NoC bi-directional channels. We
    demonstrate the performance enhancement using both synthetic traffic
    and application benchmarks.

2.  A new channel direction control protocol for bi-directional channels
    is proposed. It is shown this protocol can be used for long wire
    links with repeaters or pipeline registers.

3.  The FSNoC router architecture that supports flit-level parallel
    transmission is proposed. The performance overhead in terms of area
    and power are analyzed and compared with state-of-the-art BiNoC
    implementations.

The remainder of this chapter is organized as follows. Section 5.2 gives
the motivation of adding more flexibility in BiNoC and compares the
single router throughput under different transmission schemes. Then, in
Section 5.3, we present the proposed flit-level speedup scheme. We
elaborate on the channel directional control protocols to support
parallel flit transmission in FSNoC, followed by a in-depth discussion
of the hardware implementations. Section 5.4 extends the FSNoC design
for the existence of long wire links made with repeaters or pipeline
registers. Our simulation results are presented in Section 5.5. Finally,
section 5.6 concludes the whole work.

### 5.2 Motivations of FSNoC

#### 5.2.1 A motivational example

To illustrate the difference in bandwidth utilization under different
NoC architectures, we consider a case of three packets A, B, C
traversing from a router R1 to a router R2 as shown in Fig. 5.2 -a. We
denote the physical bandwidth of Channel1 pointing from router the R1 to
R2 as @xmath (flits) and the number of input ports in the routers as
@xmath . The packets in flow A, B and C have fixed length (in this
example it is equal to @xmath flits) including the header and the tail
flit. A fair round-robin policy is adopted in the virtual channel and
switch arbitration. At the beginning, the packets A, B and C need to
contend for the VCs in R2. As there are only two VCs available at each
port, we assume packets A and B successfully acquire the virtual
channels while packet C has to wait until A or B finishes transmission
and releases the occupied VC. We assume the packet stalling [ 20 ] occur
in R2 during the transmission of packet A and B. The blockage times for
packets A and B in ports Out1 and Out2 are @xmath and @xmath cycles,
respectively. Moreover, for the sake of simplicity, in this example, we
assume the processing time of a packet transfer is only one cycle in the
routers R1 and R2. For the state-of-the-art NoC architectures, there are
three ways to utilize the NoC inter- and intra-router bandwidth:

1) Typical uni-directional NoC switching : In a typical uni-directional
NoC router [ 6 ] , the internal crossbar size is @xmath while the
inter-router bandwidth @xmath . Therefore, two kinds of bandwidth
limitations exist in the transmission as shown in Fig. 5.2 -b. The first
limitation is the inter-router bandwidth limitation, where as there is
only one unidirectional link between R1 and R2 ( @xmath ), packets A and
B have to alternately use this single capacity link ( i.e., Channel1 ).
The second is the intra-router bandwidth limitation where in R2, the two
virtual channels in the west input need to compete one of the @xmath
entries in the crossbar during the SA phase. Therefore packet A and B/C
can not be transmitted at the same time.

2) Uni-directional NoC with input speedup switching : The input speedup
scheme provides @xmath internal bandwidth in the input crossbar side and
increases the crossbar size to @xmath . The intra-router bandwidth
limitation can be overcome as shown in Fig. 5.2 -c, where both the VCs
in R2 can use the crossbar entries simultaneously. Thus the latency of
packet C is reduced to 20 cycles. However, the inter-router bandwidth
limitation still exists in the transmission.

3) Bi-directional NoC switching : In BiNoCs [ 101 ] , since there are no
traffic flows from R2 to R1, the link originally pointing from R2 to R1
can be reverted which doubles the inter-router bandwidth from R1 to R2 (
i.e., @xmath ). Consequently, as shown in Fig. 5.2 -d, both packets A
and B can be transmitted at the same time and the delay of packet C can
be further reduced to 17 cycles.

In BiNoC switching, after packets A and B using Channel1 simultaneously,
the transmission of flits in packet C only needs one flit-width channel
(shown in Fig. 5.2 -d). In other words, half of the effective bandwidth
between R1 and R2 remains unused. Further performance gain can be
achieved if we increase the inter-router bandwidth utilization under
this case. Towards this end, we propose a flit-level speedup scheme
which allows flits within the same packet to use the doubled bandwidth
in addition to those from different packets. Fig. 5.3 depicts the timing
diagram that allows flits within the same packet (A,B,C) to be
transmitted simultaneously. As can be seen from the figure, the latency
of packet A/B is reduced due to two flits are sent together towards
output port Out2/Out3 . On the other hand, the latency of packet C is
improved because its waiting time is greatly reduced as the packet B in
front of it leaves and releases the VC in R2 earlier than in the BiNoC
case. In this example, these two effects further improve the latency of
the three packets by 12.5%, 27.2% and 29.4%, respectively.

The profiling results in Fig. 5.4 confirms the high opportunity of using
the proposed flit-level speedup scheme. More specifically, this is based
on the following observations:

1) The uneven traffic between neighboring routers : For a channel
between two routers, while there is packet sending from one end to the
other, there is a significant portion of time that no packet is coming
from the opposite direction. Fig. 5.4 -a shows the profiling results on
an @xmath mesh NoC architecture under @xmath of the saturation injection
rate of three synthetic traffic patterns ( i.e., uniform random, hotspot
and transpose). As can be seen from the figure, in uneven traffic
patterns ( e.g. transpose), for more than 80% of the links, the
probability that there is no packets in the opposite direction is larger
than @xmath at run time. Therefore, for these links, more than @xmath of
time, the effective bandwidth during transmission can be doubled via
alternating the opposite link directions. Even for uniform random
traffic, most of the links (67.50% in Fig. 5.4 -a) have more than @xmath
possibility to double the bandwidth.

2) The breakdown of the scenarios when there are no flows in the
opposite direction: Under this condition, we can further distinguish
into four scenarios for the router under consideration. They are: I) the
router has no packet to send through the link, II) the router has only
one packet with a single flit to be sent through the link, III) the
router has one packet with multiple flits to be sent through the link,
IV) the router has multiple packets requesting the output link. For
scenarios I and II, we do not need to alternate the opposite link as the
original link bandwidth is sufficient for the transmission. However, for
the last two scenarios, where scenario III and IV corresponds to
flit-level speedup and bi-directional switching, respectively, we can
increase bandwidth and hence the performance if we can reverse the
direction of the opposite link and send two flits through the two links
at the same time. Figure 5.4 -b shows the portion of time that scenario
III and IV occur among all of the above four scenarios under various
injection rates and traffic patterns for an @xmath mesh NoC
architecture. Under low injection rate, since there is little congestion
in the network, scenarios I and II dominate the occurrence. On the other
hand, when the injection rate increases, the probability that we can use
the double capacity of the bi-directional link increases. As shown in
the figure, scenario III occurs frequently in run time. However the
state-of-art bidirectional switching schemes will only work for scenario
IV [ 101 ] [ 102 ] .

#### 5.2.2 Single router throughput comparisons

FSNoC is further motivated by the throughput comparisons of a single
router. We design an event-driven simulator for a single router to model
four different switching schemes: 1) Typical uni-directional NoC, 2)
Typical NoC with 2X input speedup, 3) Bi-directional NoC and 4) Proposed
FSNoC. The simulator is based on two parameters @xmath and @xmath . The
@xmath parameter represents the probability that there is no flows in
the opposite link (as shown in Fig. 5.4 -a) and therefore both two link
channels of that direction are enabled for sending data. The @xmath
parameter describes the average packet arrival rate at each crossbar
input ^(a) ^(a) a In a VC-based router architecture, the internal
physical bandwidth of a router is mainly dictated by its crossbar input
bandwidth. Therefore we abstract the packet arrival process @xmath with
respect to each crossbar input entry instead of each VC in the input
port. . Both @xmath and @xmath can be obtained by profiling the target
applications on specific NoC architectures and are treated as input to
the simulator. For example, let @xmath denote the mean packet arrival
rate of router @xmath over all crossbar inputs in @xmath . For three
traffic patterns (random, transpose and hotspot) and under @xmath
saturation injection rate, the histogram of @xmath is shown in Fig. 5.5
. As can be seen from the figure, the overall average packet rate @xmath
for random, hotspot and transpose traffic patterns are @xmath , @xmath
and @xmath (packets/cycle), respectively. Similarly, the @xmath values
under the same configurations for these three traffics are @xmath ,
@xmath and @xmath . These @xmath and @xmath values can be passed to the
simulator to evaluate the single router throughput under a specific
traffic pattern. Although being simplified, the single router throughput
analysis can provide some useful insights on the performance impacts of
FSNoC and other architectures.

In Fig. 5.6 -a and -b, The simulated throughputs versus different packet
arrival rates @xmath are shown with @xmath and @xmath , respectively. It
is observed that when the packet arrival rate approximates a very large
value ( i.e., @xmath ), the BiNoC and FSNoC have the same saturation
throughput. This is because under this situation, all the crossbar
entries tend to be occupied by different packets and scenario III
discussed in sub-section 5.2.1 dominates the time that can use @xmath
bandwidth. However, as profiled in Fig. 5.5 , many routers tend to have
a smaller @xmath ( e.g., @xmath ) at run time, which provides the
opportunities for FSNoC to enhance the system performance. The
throughput under a variety of @xmath and @xmath combinations can be
obtained by fitting the simulation curves in Fig. 5.6 -a and -b as shown
in Fig. 5.6 -c. As can be observed from the figure, the FSNoC achieves
about @xmath improvement over the BiNoCs for a wide range of @xmath and
@xmath values( e.g., @xmath and @xmath ).

Based on the above observations, we propose FSNoC which improves the NoC
performance by adding more flexibility in using the bandwidth. For the
inter-router transmission, bidirectional links are employed to provide a
double bandwidth at run time. For the intra-router transmission, a new
input buffer organization and a switch allocator are designed to allow
flits within the same packet to participate in the routing pipelines.

### 5.3 Implementation of FSNoC

In this section, we present the details of our proposed flit-level
speedup scheme. The channel direction control protocol which adapts the
link directions based on the run time traffic conditions will be
described first. Then we present the router datapath design to support
flit-level parallel transmission.

#### 5.3.1 Inter-router channel direction control

In FSNoC with bi-directional channels, both links connecting to one
direction of the router can be reconfigured as sending or receiving.
However, in order to keep the original order of the flits within the
same packet, we define one link as the master and the other as the
slave. As shown in Fig. 5.7 -a, the master link of a router is also the
slave link of its neighbor and vice versa. If there is traffic in both
directions between two neighboring routers, the master and slave links
acts as the sending and receiving links, respectively. In order to keep
the order of flits within the same packet during the flit-level parallel
transmission, when writing two flits from the same VC into the output
channel, the output controller will always put the first flit on the
master link and the second flit on its slave link. On the receiving
side, we always assemble the flit appeared on the slave link ahead of
the flit on the master link into the VC buffer.

Based on the master and slave link definition, we develop a new channel
direction control (CDC) protocol to configure the links at run time so
as to adapt for various traffic conditions. Previously, there are two
ways to do the direction control, i.e., the centralized and distributed
control. For example, [ 102 ] employs a separate bandwidth allocator
between two adjacent routers so as to determine the link directions
simultaneously for both routers, while [ 101 ] uses a distributed
control mechanism without the need of an extra controller. In this work,
the proposed CDC protocol is also based on a distributed scheme since it
is more modular and easier to implement. Therefore, we mainly compare
the proposed CDC protocol with that used in [ 101 ] .

1: Input: Virtual channel status @xmath

2: Output: Pressure signal @xmath and local pressure register

3: bank @xmath

4: if @xmath then

5: for all each input direction @xmath do

6: @xmath

7: for each VC channel @xmath do

8: @xmath ;

9: end for

10: end for

11: end if {initialize the pressure counter}

12: always @xmath

13: for all input direction @xmath do

14: for all VC channel @xmath do

15: if @xmath == @xmath then

16: @xmath

17: @xmath

18: @xmath == @xmath ;

19: else if @xmath == @xmath then

20: @xmath

21: @xmath

22: @xmath == @xmath ;

23: end if {add or decrease the request}

24: end for

25: end for

26: for each output direction @xmath do

27: if @xmath then

28: @xmath ;

29: else

30: @xmath ;

31: end if

32: @xmath

33: end for

Algorithm 3 Operation of the RE module

For the CDC proposed in [ 101 ] , there are two Finite State Machines
(FSMs) in each router, one for the master link with high priority (
i.e., HP-FSM) and the other for the slave link with low priority ( i.e.,
LP-FSM). The direction of each link depends on the state of the
corresponding FSM. Since the incoming request signal from the
neighboring router takes two cycles to arrive at the current router, a
”wait” state is required when the FSM transits from a ”receiving” state
to a ”sending” state which will introduce @xmath dead cycles in the
channel direction reversal process [ 101 ] .

In this work, the proposed channel control scheme is shown in Fig. 5.7
-a, which embeds the inter-rouer request signal delays within the
four-stages pipeline depth and uses a single module to decide the output
width ( @xmath or @xmath flits) for each direction instead of two
separate FSMs. Two modules, namely Request extractor (RE) and Output
width controller (OWC), are introduced to work together with the
conventional four pipeline stages in the datapath. The RE module
monitors the input channel status and generate the pressure signals to
the corresponding OWC modules in the current and the neighboring router.
The pressure signal of an output direction, which is named as Req_out in
Fig. 5.7 -a, represents the number of virtual channels (VCs) that
requests to send out towards this direction after being granted a VC in
the downstream router. Similar to [ 101 ] , in order to provide clean
signals from router to router in the transmission, we assume both the
data and control signals are doubly registered (shown in Fig. 5.7 -a).
Therefore, it takes two cycles for Req_out to arrive at the neighboring
router and become the input request signal ( e.g., Req_in_d shown in
Fig. 5.7 -a) of the OWC module in the neighboring router. The OWC module
within each router then works with the switch allocator to process the
local pressure signal Req_out_d as well as the received neighbor
pressure signal Req_in_d and determines the width of each output
direction ( @xmath flits, @xmath flit or @xmath flit) accordingly.
During the crossbar traversal (CT) and link traversal (LT) stage, the
direction control signals (DS) of the master and slave link are
generated based on the output width calculated in the SA stage and are
used to configure the write and read operations.

In the proposed CDC protocol, the pressure signal needs two cycles to
arrive at the neighboring router and is required to be used in the
SA/OWC stage. In order to guarantee both the OWC modules in router R1
and R2 receive the Req_in_d and Req_out_d signals generated at the same
time, the RE module extracts the pressure signals ( i.e., Req_out) at
the RC stage instead of the VA stage. The effect of generating the
signal in the RC stage is that the VC allocation result is updated one
clock cycle later when a packet first enters the virtual channel buffer.
However, it only affects the performance when the neighboring router
requests to send two flits, while the newly arrived packet in the
current router wins the VC allocation after the RC stage. In the next
cycle, during the switch allocation (SA) stage, the Req_out_d signal
received by the OWC module may still be @xmath as it was generated two
cycles ago and the latest VA results has not yet been updated. In this
case, the current router may treat this as there is no request for
sending and the link will then be configured as receiving for both
channels while it should be configured as one sending and one receiving.
The correctness of the router operation is not compromised, only some of
the priority of sending out a flit in current router is lost in this
special case. From our simulation results, it is shown that there is no
significant difference in the latency and throughput when the pressure
signal is generated at the routing computation stage compared with that
generated at the VC allocation stage.

Fig 5.7 -b shows a typical timing diagram of the link direction
switching under the proposed CDC protocol. Assume two packets @xmath and
@xmath arrived at @xmath are granted downstream VCs in cycle @xmath and
@xmath , respectively. They are forwarded towards router @xmath . Also,
we assume a packet @xmath arrives at router @xmath and is routed to
@xmath after being allocated a VC in cycle @xmath . In cycle @xmath ,
the RE module in @xmath extracts a pressure signal @xmath and sends it
to the OWC modules in @xmath and @xmath . At the same time, since there
are no requests from @xmath to @xmath , the pressure signal generated at
@xmath is still @xmath . In cycle @xmath , the OWC module in @xmath
receives the local pressure signal @xmath from the RE module and the
neighbor pressure signal @xmath from @xmath . The OWC in @xmath then
enables both channels corresponding to the master and slave link during
the switch arbitration (SA) for packet @xmath and @xmath . On the other
hand, the OWC module in @xmath disables its master and slave channel
during the SA stage to prepare for receiving two flits at one time.
After cycle @xmath , each flit from @xmath and @xmath can traverse the
crossbar and the link using the @xmath bandwidth together. In cycle
@xmath when the neighboring pressure signal @xmath received by @xmath
turns to be @xmath due to packet @xmath being granted VC in cycle @xmath
, the slave link channel in @xmath is disabled in the @xmath stage and
re-configured to the direction of read from cycle @xmath . In contrast,
the master link channel in @xmath is enabled again during the SA stage
for packet @xmath . Therefore, flits from packet @xmath can begin to
traverse the crossbar from cycle @xmath smoothly.

The operation of the request extractor is described in Algorithm 3 . For
each VC, its status consists of four states: 1) @xmath : the channel is
waiting for the VC allocation in the corresponding output direction. 2)
@xmath : the packet has just been granted the VC in the downstream
router and has not been included in the @xmath yet. 3) @xmath : the VC
occupies the downstream VC for transmission. 4) @xmath : The tail flit
has left the buffer in the last cycle and released the downstream VC. In
the beginning, all the channel status are reset to @xmath and the
request registers @xmath are cleared to @xmath (line 3-10). At run time,
for a specific channel @xmath of direction @xmath , if its status is
@xmath , then we need to get the output direction @xmath of this VC and
increased the pressure value @xmath by one (line 14-17). On the other
hand, if the VC status is @xmath , we need to decrease the pressure
value of direction @xmath by one (line 18-22). Finally, the pressure
values in @xmath are passed to the OWC modules in both current and
neighbor router (line 31).

Algorithm 4 describes the operation of the output width controller. It
dynamically configures the data width of each output direction (0, 1 or
2 flits) by enabling/disabling the master link ( @xmath ) and slave link
( @xmath ), respectively. By default, the master link is enabled for
write operations while the slave link is disabled for read. At run time,
if the local pressure signal ( @xmath ) indicates that there are @xmath
requests from different input VCs towards the same direction @xmath and
there are no flows in the opposite direction (dictated by @xmath ), the
controller will enable both the master and the slave link channels
corresponding to output direction @xmath to participate in the switch
arbitration (line 18-19). Similarly, if the local pressure equals to
@xmath while the neighbor pressure signal equals to @xmath , the slave
link will be enabled to support flit-level parallel transmission (line
14-16). On the other hand, if @xmath equals to @xmath and the @xmath is
larger than @xmath , both the master and slave links will be disabled to
let the neighboring router output using the @xmath bandwidth (line
12-13). In addition to generate @xmath and @xmath signals, the OWC
module also determines the input switch arbitration mode @xmath . By
default, @xmath for each VC is set to be @xmath (line 4-8), which means
only one flit from this VC can participate in the SA stage every cycle.
However, under the case of flit-level speedup, the VC can send two flits
at the same time. Therefore, under this condition, the switch
arbitration mode of the requesting VC is set to be @xmath (line 17),
which allows two flits within the same packet to participate in the
arbitration for both the master and slave links of the corresponding
output direction.

1: Input: Pressure signal @xmath and @xmath

2: Output: Master/slave link enable @xmath and @xmath , Switch
allocation mode @xmath

3: always @xmath

4: for all input direction @xmath do

5: for all VC channel @xmath do

6: @xmath

7: end for

8: end for

9: for all output direction @xmath do

10: @xmath

11: @xmath

12: if @xmath == 0 && @xmath then

13: @xmath

14: else if @xmath == 1 && @xmath == @xmath then

15: @xmath

16: @xmath

17: @xmath

18: else if @xmath == 2  && @xmath == @xmath then

19: @xmath

20: end if

21: end for

Algorithm 4 Operation of the OWC module

#### 5.3.2 Router datapath design of FSNoC

Besides the RE and OWC modules added in the control path, the router
datapath also needs to be modified. In FSNoC, the routing computation
(RC) module and virtual channel allocator are identical to those in the
typical NoC [ 106 ] and BiNoC architecture [ 101 ] because the output
directions/VCs are computed/reserved in a same way in all architectures.
However, the FSNoC has a number of additional requirements in the input
buffer and switch allocator design to support efficient flit-level
parallel transmission.

##### Input buffer organization

In the proposed flit-level speedup scheme, it is possible that two
incoming flits are for the same VC buffer. Therefore, it is required to
support reading/writing two flits from/to the same VC buffer at the same
time. One way is to adopt multiple ports memory. However, this causes
significant overhead in terms of area and memory access time [ 20 ] .
Here we propose a novel input buffer organization, which is shown in
Figure 5.8 , to satisfy this requirement.

Figure 5.8 -a shows the input port configuration with @xmath virtual
channels. Two @xmath demux and @xmath flit assemblers are needed for
each port. The original virtual channel buffer is spitted into two
sub-buffers, s0 and s1, respectively. Both sub buffers share the same
buffer state information (the GROPC vector shown in Figure 5.8 -a are
defined as in [ 20 ] ). The incoming flits are written into s0 and s1
alternatively so that two consecutive flits are stored in different sub
buffers. Figure 5.8 -b shows the modified buffer pointer in each virtual
channel. For s0 and s1, each has a head-tail pointer pair pointing to
the start and end addresses of the flits of the packets. In addition,
two 1-bit registers, denoted as the Front and Back registers, are added
to indicate which sub-buffer holds the first and the last flit ( e.g.,
flits ”a” and ”f” in Figure 5.8 -b) of the VC channel.

The details of the flit assembler is described in Figure 5.8 -c. It is
responsible for assembling the flits into the sub-buffer of the VC in
the right order. When two flits are sent in the reconfigurable links,
the first flit is always connected to the master link of the sender (
i.e., the slave link of the receiver). To ensure the correct assembling
of the flits, the slave link is always connected to the sub-buffer that
is not pointed by the Back register. Figure 5.8 -b shows an example. The
Back register points to s1 since the last flit ”f” is stored in s1. The
assembler will connect the slave link to s0 and the master link to s1 in
the next operation. Similar operation is executed when only 1 flit is
written into the buffer from the input link. The read operation is
similar to the write operation based on the value of the Front register.
Figure 5.8 -d illustrates the updated pointers after reading a flit ”a”
from the buffer and writing flits ”g” and ”h” into it. As shown in the
figure, the head and tail pointers are updated in a circular manner
while the Front and Back registers are updated based on the number of
flits reading from or writing into the virtual channels.

##### Switch allocator design

The switch allocator assigns the master and slave link channel of each
output direction to the input VCs such that the VCs with granted switch
access can move flits to the corresponding links in the crossbar and
link traversal stage. Different from BiNoC, to support the flit-level
speedup in the NoC, the switch allocator should not only support
granting two requests from different VCs, but also allow a single VC to
arbitrate for both the master and slave links. Figure 5.9 -a shows the
building block of our switch allocator. This allocator is accomplished
with a two-stage arbitration architecture [ 20 ] , which is shown in
Fig. 5.9 -a. At the input stage, a @xmath arbiter in each input
direction is used to select @xmath requests out of many requests from
the @xmath virtual channels. Of note, the @xmath winners can come from
different VCs or from the same VC. This is determined by the SA
arbitration mode vector ( @xmath ) discussed in section 5.3.1 . If all
the VCs in the current input direction @xmath have the same mode @xmath
( i.e., @xmath , @xmath ), then the two winning requests come from
different VCs. On the other hand, if some of the VCs have the SA mode
@xmath , the two winning requests are combined together and returned to
one of these flit-level speedup VCs. At the output side of the switch
allocator, @xmath @xmath arbiters are utilized to decide the actual
number of the requests that is finally granted.

Figure 5.9 -b shows the detail design of the @xmath arbiter for the
input stage allocation. It consists of two @xmath round-robin arbiters
and a mode decision module named as @xmath . The @xmath module of
direction @xmath reads in the SA mode vector ( i.e., @xmath ) and
determines the selection mode for the current input port. If all the
entries of SA vector equal to @xmath , it means that there is no request
that requires two flits to be transmitted from the same VC and hence the
two @xmath arbiters will work independently to select two VCs using
different priority pointers . Otherwise, the @xmath will chooses one VC
with the @xmath mode and generate the priority pointer exactly pointing
to that VC for both @xmath arbiters.

Figure 5.9 -c shows the @xmath arbiter used in the output side. It
consists of two @xmath arbiters. The link allocation control signals (
@xmath and @xmath discussed in Algorithm 4 ) of each output direction
enable/disable the arbiters accordingly. Moreover, a module named as
@xmath (shown in Figure 5.9 -c) is used to grant two requests to the
same input port so to allow parallel transmission of two flits from the
same VC.

### 5.4 FSNoC for long-wire links

For many NoC designs, there exist long wire links due to the irregular
topology [ 107 ] or the short-cut paths created by long-range links [
108 ] . In order to address the issue of high latency and power
consumption, repeaters [ 109 ] or pipeline registers [ 108 , 107 ] are
usually inserted between the routers to break the long wires. In this
section, we extend the FSNoC design to support flit-level parallel
transmission over the long wires.

#### 5.4.1 Long-wire links with repeaters

For long-wire link with repeaters, it still takes one cycle to move a
flit from one router to the neighbor. In FSNoC, the control channels (
e.g., @xmath in Fig. 5.7 -a) between two routers are uni-directional.
Therefore, the repeaters can be inserted directly between the sender and
receiver as in the case of typical NoCs. On the other hand, for the data
channels, bi-direction repeaters (Bi-repeater) are needed to allow the
data transmission over both directions. Moreover, the direction
selection signal ( i.e. , @xmath in Fig. 5.7 -a) should be propagated to
configure the intermediate Bi-repeaters properly. Fig. 5.10 shows the
detail design of a data channel between two routers @xmath and @xmath .
Within each router, the OWC module determines the output channel width
as well as the @xmath signal in the SA stage based on the CDC protocol.
Then, the generated @xmath signal is pipelined together with the flits
in the crossbar traverse ( CT ) and link traverse ( LT ) stages before
reaching the output port to configure the inout directions. As shown in
Fig. 5.10 , the @xmath signal during the CT and LT stages are
represented as @xmath and @xmath , respectively. If there are @xmath
Bi-repeaters need to be inserted between router @xmath and @xmath to
address the long wire delays (Seen Fig. 5.10 for a case of two
Bi-repeaters), @xmath of them close to @xmath will be driven by the
@xmath signal from @xmath while the remaining are driven by the @xmath
from @xmath . For example, in Fig. 5.10 , Bi-repeater 1 and 2 are driven
by @xmath and @xmath , respectively. The one-cycle ahead signal @xmath
from @xmath and @xmath are connected to the registers in the Bi-repeater
@xmath and @xmath . Every cycle, at the clock rising edge, the @xmath
signals will be read into the registers @xmath simultaneously. They
become the @xmath signals for the current cycle. Then, the @xmath
signals configure the Bi-repeater to ensure the write/read directions
are the same as the driving router. As the CDC control protocol
discussed in Section 5.3 guarantees @xmath and @xmath are conflict-free,
the correct data transfer direction can be maintained in the
Bi-repeaters. Also, as the number of pipeline stages do not change, the
FSNoC with repeaters has the same latency as the original design for
data transmission.

#### 5.4.2 Long-wire links with pipeline registers

Besides repeaters, pipeline registers can also be used to break long
wire links which achieves better timing properties at the cost of longer
link traversal latency.

For the control channels, conventional registers can be added directly
as the signals are uni-directional. On the other hand, for the data
channels, the direction selection signals ( @xmath signals) are
pipelined together with the data flits to ensure the proper read and
write direction. This is shown in Fig. 5.11 -a, where a link pipeline
module (LPM) is inserted between routers @xmath and @xmath for a data
link. Two direction selection signals from @xmath and @xmath , i.e.,
@xmath and @xmath , are also connected to the LPM. The detail schematic
of the pipeline module is shown in Fig. 5.11 -b. In LPM, a flit register
( @xmath ) and two one-bit registers ( @xmath ) are used to buffer the
data flits and direction signals, respectively. Every cycle, based on
@xmath and @xmath , a @xmath selects whether the flit on @xmath or
@xmath should be written into the internal @xmath Similarly, at the
output side of @xmath , the previous one-cycle pipelined @xmath signals,
i.e., @xmath and @xmath , determine the link segment ( @xmath or @xmath
) that current flit in @xmath needs to be forwarded to.

Compared to FSNoC in section 5.3 , the additional link traversal cycles
due to link pipelining complicate the channel direction control in two
aspects. First, it brings the synchronization issue between the
neighboring and local pressure signals. As shown in Fig. 5.7 -a,
originally, it takes two cycles for both the neighbor pressure ( @xmath
) and local pressure signal ( @xmath ) to arrive at the @xmath module.
As the @xmath modules in two adjacent routers make the decision based on
same @xmath ) pair every cycle, it is ensured that no write conflicts (
i.e., write from both directions) occur by the proposed distributed CDC
protocol in Algorithm 4 . On the other hand, here, for the case of
@xmath -stage pipelined link, the pressure signal @xmath takes
additional @xmath cycles ( i.e., @xmath cycles) to arrive at the @xmath
in @xmath while it still needs two cycles to reach @xmath in @xmath .
The asynchronization of @xmath modules in @xmath and @xmath may create
incorrect output width combinations ( e.g., an output width of @xmath
flits for @xmath and @xmath flit for @xmath ) and overwrite useful flits
on the link. Therefore, in order to provide correct direction decisions,
@xmath additional pipeline registers should be added between the @xmath
and @xmath modules inside each router to keep the local pressure signals
synchronized with the neighbor pressures.

The second hurdle due to link pipeline is that more types of conflicts
need to be resolved in the CDC protocol. More precisely, two types of
conflicts on a pipelined link are identified, namely wire conflict and
register conflict . Taking a one-stage pipelined link in Fig. 5.11 -a
and -b as an example, the wire conflict happens if both ends try to
write data on the same wire segment. As shown in Fig. 5.11 -a and -c,
the output register of router @xmath will write a flit @xmath on the
link segment @xmath if the direction selection signal @xmath equals to
@xmath ( i.e., 0). On the other hand, the @xmath will write a flit
@xmath on the same link segment if the direction selection signal @xmath
equals to @xmath . Therefore, a wire conflict happens if both @xmath and
@xmath equal to @xmath at the same time. The register conflict refers to
the operation that try to write flits into the same @xmath in one cycle.
For example, if @xmath equals to @xmath , flit @xmath on @xmath will be
written into @xmath at the next cycle. Similarly, if @xmath equals to
@xmath , flit @xmath on @xmath will be written into @xmath . Thus, the
register conflict occurs if @xmath and @xmath equal to @xmath at the
same time. As discussed in section 5.3 , in FSNoC, the direction
selection signal @xmath and @xmath are generated in the same cycle by
the @xmath module of @xmath and @xmath , respectively. Therefore, they
cannot be @xmath simultaneously under the CDC protocol in Algorithm 4 ,
which means the register conflict will not occur. On the other hand, as
@xmath is generated and pipelined one cycle before @xmath , they may
both be equal to @xmath at run time and produce the wire conflicts as a
result. In order to resolve the wire conflicts in one-stage pipelined
link, the @xmath module needs to store the direction selection results
in the previous cycle and ensure the @xmath signal generated in current
cycle not conflict with that of the neighbor router in both current and
previous cycles. More generally, for a @xmath -stage pipelined link with
@xmath , more than one @xmath s are inserted between two routers. In
order to read and write correctly, we need to guarantee there are no
wire conflicts on the @xmath link segments as well as no register
conflicts in the @xmath @xmath modules. Therefore, the @xmath module in
each router needs to store the @xmath signals of the previous @xmath
cycles. For the master link, its direction will be reversed for the
other router if the current router have not written flits into this link
during the past @xmath cycles. Similarly, for the slave link, its
direction can be reversed for write operations if the current router
receives the neighboring pressure signal @xmath continuously during the
past @xmath cycles.

### 5.5 Experimental results

#### 5.5.1 Simulation setup

We evaluate and compare the proposed FSNoC architecture with other
designs using a cycle-accurate, SystemC-based simulator extended from
Noxim [ 79 ] . We assume the mesh-based topology for all the
comparisons. Table 5.1 shows four router architectures used for
comparison. Specifically, we compared the performance of the proposed
scheme with three different architectures, namely the Typical NoC,
Typical NoC with 2X input speedup, and BiNoC. For all architectures,
each input direction of the router has @xmath VCs and the buffer depth
of each VC is @xmath flits as in [ 101 ] . We assume the packet has a
constant length of @xmath flits. As shown in Table 5.1 , for
conventional unidirectional NoC, a @xmath crossbar is used to support
the @xmath -in and @xmath -out data transmission. The input speedup
scheme provides @xmath excess bandwidth in the input side and therefore,
a @xmath crossbar is needed. For the BiNoC and FSNoC architectures, as
we need to provide flexible data transmission over the @xmath inout
ports, a larger @xmath crossbar is required. In order to make a fair
comparison over the four architectures, each router architecture is
assumed to operate under the maxium frequency, which is obtained from
the synthesis results by Synopsys Design Compiler under TSMC @xmath nm
technology. The simulated latency is then calculated in the unit
normalized to the cycle @xmath of Typical NoC.

For four architectures, the Dimension ordered XY routing is adopted in
order to avoid deadlocks (except Fig. 5.14 -a, which evaluates the
effects of oddeven routing algorithm). Various traffic patterns were
used in the evaluation, including synthetic traffic and real benchmarks.
More specifically, two types of real benchmarks are used in the
evaluation. The first type is task-graph-based MPSoC applications,
including MWD (Multi-Window Display) [ 68 ] , MMS (Multimedia system) [
67 ] , MPEG4 (MPEG4 codec) [ 68 ] and DVOPD (Dual Video Object Plane
Decoder) [ 62 ] as well as three E3S [ 110 ] applications named
auto_indust, telecom and consumer. For these application-specific
benchmarks, we need to map the task graphs onto the mesh architecture
first. The second type of applications are extracted from SPEC-web [ 111
] benchmarks which model the memory access patterns for CMP
architectures. For SPEC-web applications, five 16-node multithreaded
workloads for IBM and Oracle database server ( i.e., DB2V and Oracle ),
the Apache HTTP server ( i.e., Apache ), scientific workloads of matrix
factorization ( i.e.,Sparse ) and the ocean dynamic simulation ( i.e.,
Ocean ) are extracted on a @xmath mesh. Moreover, application
consolidations [ 112 ] are evaluated by choosing four applications and
randomly mapping them onto an @xmath mesh ( i.e., Mixed ). In the
following, we first evaluate the performance of the NoC architectures
without link pipeline stages in sub-section 5.5.2 - 5.5.3 . Then, we
discuss the simulation results for FSNoC designs link pipelines.

#### 5.5.2 Simulation results on synthetic traffics

For the synthetic traffic comparison, six traffic patterns were used,
i.e., random, transpose, hotspot, shuffle, bit-reversal and butterfly [
20 , 79 ] . For the hotspot traffic, two central nodes in the mesh
network have @xmath higher probability to receive packets from other
nodes. Figures 5.12 summarizes the comparisons on the latency under the
XY routing algorithm. For all the cases, the proposed FSNoC out-performs
other three schemes. As observed from the figure, the improvement highly
depends on the traffic patterns. For example, comparing the results in
random traffic with transpose or bitreversal traffic, we can find the
uneven traffics achieve more latency improvement as these non-uniform
patterns provide more chances for the BiNoC and FSNoC architecture to
use the @xmath bandwidth during the transmission. Specifically, we can
compare the maximum packet injection rate sustainable by the network of
each architectures [ 23 ] . The maximum injection rate ( @xmath ) is
calculated as the rate (packets/cycle/node) at which the corresponding
throughput reaches @xmath of the saturation. For instance, for the
random traffic pattern, the @xmath of FSNoC, BiNoC, @xmath speedup NoC
and Typical NoC are @xmath , @xmath , @xmath and @xmath ( packets/cycle
), respectively. While for transpose traffic, the @xmath of four
architectures are @xmath , @xmath , @xmath , @xmath ( packets/cycle ).
In summary, the BiNoC architecture improves @xmath by @xmath over the
typical and @xmath input speedup schemes. In addition to that, the FSNoC
further increases the saturation injection point by @xmath .

We then evaluate the NoC performance regarding the influence of
different packet length ( @xmath flits) and buffer depth ( @xmath flits
per VC). Figure 5.13 shows the maximum throughput that four
architectures can obtain for the random traffic pattern on an @xmath
mesh. All the throughput values are normalized to that of a typical NoC
design with constant packet length of @xmath flits and buffer depth of
@xmath flits. As shown in the figure, the throughput improvement is more
obvious when a longer packet length is adopted as the single packet
transfer time can be more significantly reduced by allowing sending two
flits at one time. Overall, the proposed FSNoC design achieves the
highest throughput over a wide variety of packet length and buffer depth
combinations.

In Fig. 5.14 -a, we show the latency of four architectures running
oddeven routing algorithm under random and transpose traffic patterns.
For the uneven traffic patterns such as transpose, the oddeven routing
algorithm evenly distribute the load during the routing, therefore, the
improvement of FSNoC over BiNoC is smaller than the case of XY routing.
However, as can be seen from the figure, the FSNoC still achieves @xmath
improvement in the saturation injection point for the random and
transpose traffic. In Fig. 5.14 -b, we evaluate the influence of number
of virtual channels on the latency performance. For all architectures,
the buffer size per input port is fixed as in Table 5.1 . As shown in
Fig. 5.14 -b, FSNoC consistently achieves the highest performance for
both the wormhole (WH) architecture and two virtual channel
architectures. The idea of flit-level speedup can also be applied to
NoCs with four uni-directional links between router pairs ( i.e., @xmath
link architecture in [ 101 , 102 ] ). Because the @xmath link
architecture always provides @xmath bandwidth for each direction, it
gives the upper-bound performance that BiNoC can achieve. Conventional
@xmath link design only allows two different packets to use the @xmath
channel. Therefore, we can improve the performance by allowing two flits
from the same packet to use @xmath links ( i.e., flit-level speedup). In
Fig. 5.14 , we compare the latency performance of four architectures,
namely BiNoC, FSNoC, Typical NoC with @xmath link and FSNoC with @xmath
link, using random and transpose traffic patterns. As can be seen from
the figure, for uniform traffic patterns such as random, the @xmath link
architectures ( e.g., Typical @xmath and FSNoC @xmath ) achieve higher
performance than the FSNoC and BiNoC because the static bandwidth
assignment avoids the direction switching overhead existed in
bi-directional designs. For uneven traffic patterns such as transpose,
the Typical @xmath and FSNoC @xmath architectures have similar
performance as BiNoC and FSNoC, respectively. This is because the
direction switching is not so frequent in these patterns. Hence, the
FSNoC and BiNoC architectures work just like the @xmath architecture
under these cases.

#### 5.5.3 Simulation results on real benchmarks

We also evaluate the FSNoC performance using several real benchmarks. In
Fig. 5.16 , we compare the saturation throughput for seven MPSoC
applications. Of note, for these MPSoC benchmarks such as MMS, MWD , we
assume the packets are injected according to a Poisson process whose
mean rate is scaled by a parameter @xmath according to the communication
data volume on the edges of the task graphs. We then tune the scaling
factor @xmath to find out the maximum throughput that various
architectures can support for the comparison in Fig. 5.16 . All the
saturation throughputs are normalized to that of the typical NoC design.
As shown in the figure, while the bi-directional switching scheme
out-performs the two unidirectional schemes, our proposed flit-level
speedup scheme consistently further improves the performance. The
relative improvement of FSNoC over BiNoC ranges from 2% to 17% depending
on the applications. In Fig. 5.16 , we use the collected traces from
SPEC-web benchmarks to compare the average communication latency in the
CMP platforms. As shown in the figures, for all the SPEC applications,
the proposed FSNoC design achieves fairly good reduction in latency.
Especially for those applications with medium or high workload ( e.g.,
Oracle and Ocean in Fig. 5.16 ), the improvement is more significant as
the FSNoC significantly reduce the network congestion under these
workloads by providing more flexibility to use the inter-router
bandwidth.

In Figure 5.17 , we also show an example histogram of the packet
delivery time under 80% saturation injection factor for the telecom
application in E3S application. As shown in the figure, not only the
average latency but also the maximum delay are significantly reduced. As
the worst-case delay is more important for the applications with
real-time deadline constraints, the FSNoC also provides a good
prospective to satisfy the QoS requirements for these applications.

#### 5.5.4 Simulation results for FSNoC with link pipelines

Regarding the influences of long wire links, we next evaluate the
latency performance of four architectures with @xmath -stage pipeline
registers inserted in all links. In Fig. 5.18 , the latency simulation
results for four synthetic traffic patterns on an @xmath mesh are shown.
As can be seen from the figure, for these two traffic patterns, the
network latency has increased at light loads. For example, for the
random traffic, comparing Fig. 5.12 with Fig. 5.18 , it can be observed
the zero load latency has increased from @xmath cycles to @xmath cycles
because it takes @xmath cycles in total for a flit to reach the
neighboring router. Moreover, the saturation injection point ( i.e.,
@xmath ) has also decreased slightly for all the architectures. For
random traffic, the @xmath values of four architectures have reduced to
@xmath , @xmath , @xmath and @xmath (packet/cycle) , respectively. This
is because in order to avoid both wire and register conflicts described
in section 5.4.2 , the OWC module should determine the direction control
( @xmath ) signals by considering not only the current cycle pressure
information but also that of previous two cycles, which reduces the
chances to use the @xmath channel bandwidth. However, as can be seen
from the figure, the pipelined FSNoC still outperforms the other three
architectures for both random and transpose traffics.

#### 5.5.5 Implementation overhead

As discussed in [ 101 ] , although the @xmath link architectures have
almost the same overhead as BiNoC in the router design, the more
inter-router wiring resources required is not cheap as technologies keep
scaling down. Rather than increasing physical bandwidth by adding more
additional uni-directional links, NoCs based on bi-directional channels
are advocated to reduce the routing congestion and spaces. Therefore, in
the hardware overhead evaluation, we majorly implement and compare the
four router architectures described in Table 5.1 . We implement each
scheme in Verilog and synthesized the design using Synopsys Design
Vision based on TSMC 65nm library. According to our synthesis results,
for all the four architectures, the critical paths are located in the ST
stage. For both the BiNoC and FSNoC architectures, the maximum path
delay @xmath is dictated by the @xmath crossbar and hence they run at
the same maximum clock frequency. Table 5.2 and 5.3 summarize the area
and power breakdown of different NoC architectures. For FSNoC, the
direction control module (i.e., Dir. ctrl. in Table 5.2 and 5.3 )
includes the Request extractor (RE) and Output width controller (OWC).
As shown in the tables, the input buffers consume most of the router
area and power for all the four architectures. From the synthesis
results, the area overhead of 2X input speedup, BiNoC and the proposed
FSNoC over the conventional NoC are @xmath , @xmath and @xmath ,
respectively. For the power comparison, we assume a @xmath switching
activity factor of a random payload as in [ 101 ] . As can be seen from
the table, the power overhead of 2X input speedup, BiNoC and FSNoC over
conventional NoC are @xmath , @xmath and @xmath , respectively.

### 5.6 Conclusion

In this chapter, we have proposed a flit-level speedup scheme for
improving the NoC performance using self-reconfigurable bi-directional
links. In order to support transmitting two flits within the same packet
at the same cycle, a novel channel direction control protocol is
proposed to dynamically configure the link directions. The corresponding
design of the input buffer organization and the switch allocator are
also proposed. Also, we have extended the channel direction control
scheme to work under the existence of long wire links. From the
simulation results, significant improvement in latency and throughput
are achieved for both synthetic traffic and the real benchmarks.

## Chapter 6 A Traffic-aware Adaptive Routing Algorithm on a Highly
Reconfigurable Network-on-Chip Architecture

In this chapter, we propose a flexible NoC architecture and a dynamic
distributed routing algorithm which can enhance the NoC communication
performance with minimal energy overhead. In particular, our proposed
NoC architecture exploits the following two features: i)
self-reconfigurable bidirectional channels to increase the effective
bandwidth and ii) express virtual paths, as well as localized hub
routers, to bypass some intermediate nodes at run time in the network. A
deadlock-free and traffic-aware dynamic routing algorithm is further
developed for the proposed architecture, which can take advantage of the
increased flexibility in the proposed architecture. Both the channels
self-reconfiguration and routing decisions are made in a distributed
fashion, based on a function of the localized traffic conditions, in
order to maximize the performance and minimize the energy costs at the
macroscopic level. Our simulation results show that the proposed
approach can reduce the network latency by @xmath in most cases compared
to a conventional unidirectional mesh topology, while incurring less
than 15% power overhead.

### 6.1 Introduction

The NoC architecture and the corresponding routing strategy play an
important role in optimizing the system performance in terms of
throughput and latency. Indeed, enhancing the NoC architecture with
additional flexibility is appealing since it can offer more
opportunities to minimize the impact of traffic congestion by bypassing
the intermediate router pipeline stages for some packets at run time [
113 ] . At the same time, a routing strategy, which fully utilizes the
characteristics of both the underlying communication infrastructure and
the NoC traffic, is equally important since the routing decision made at
the run time is critical in optimizing the NoC performance and resources
utilization [ 114 , 37 ] .

For NoC architecture, the bandwidth and the average distance between the
nodes have a significant impact on the overall network performance. Most
of the existing NoC architectures have employed a two-dimensional mesh
topology with two unidirectional links connecting the neighboring tiles.
Although the mesh topology is well suited for silicon implementation [
115 ] , the channel bandwidth is not optimally used [ 102 ] and it may
suffer from long packet latencies due to the lack of short paths between
remotely located nodes [ 108 ] .

For routing strategy design, several adaptive routing algorithms have
been proposed for conventional unidirectional mesh NoCs [ 114 , 115 ,
112 , 116 ] . However, dynamic routing has been far less explored for
performance improvement.

In this chapter, we propose a reconfigurable NoC architecture which
combines the advantages of providing higher effective bandwidth between
the neighboring nodes and shorter paths between remotely located nodes [
117 ] . Our proposed NoC architecture exploits the following two
capabilities, namely: i) self reconfigurable bidirectional channels
(BiNoC) [ 102 , 100 ] and ii) static express virtual channels (EVCs) [
113 ] with regional hub routers, to dynamically create short-cut paths
for transmitting packets. In order to exploit the added flexibility
provided by the proposed architecture, we also propose a new
fitness-based adaptive routing algorithm for optimizing the network
performance.

Experimental results obtained for synthetic traffic and real benchmarks
show that our approach can improve the latency by as much as 80% , while
involving less than 15% overhead in power dissipation.

The remaining of this chapter is organized as follows: In section 6.2,
we review related work on NoC architecture and adaptive routing
algorithm design. In section 6.3, we describe the newly proposed NoC
architecture. Section 6.4 presents the adaptive routing algorithm for
the proposed NoC. The practical considerations and simulation results
are discussed in section 6.5. Finally, section 6.6 concludes this work.

### 6.2 Background

In NoC architecture design, many efforts are focused on increasing the
total bandwidth and reducing the average node distance by proposing new
topologies [ 118 , 119 ] . In [ 118 ] , a Diagonally-linked Mesh
topology is proposed which employs physical diagonal express links
between routers to reduce the distances between nodes. In [ 119 ] , a
new topology called Multidrop Express Channels (MECS) is proposed; this
uses a one-to-many communication model to enable a high degree of
connectivity. Compared to the mesh based NoC, these new topologies can
improve the network bandwidth and connectivity at the cost of adding
more physical resources such as the link wires and increased the router
complexity.

To improve the trade-off between the design complexity and the
performance, there are also approaches exploiting the mesh topology due
to its modularity and scalability. In [ 113 ] , the NoC architecture
with static and dynamic express channels (EVC) is proposed. Instead of
adding physical links, a flow control mechanism allows packets to
virtually bypass intermediate routers along their path. Static EVC
approach uses the express paths of uniform lengths and distinguishes the
nodes as either EVC source (sink) nodes or bypass nodes according to
whether the EVC originates (terminates) at those nodes. Dynamic EVCs
make every node in the network a source (sink) and allow EVCs of various
lengths to originate from a node [ 113 ] .

Besides the idea of bypassing intermediate routers, another important
direction in optimizing the mesh NoC architecture is to enable the
network to increase the effective bandwidth at run time. Indeed, very
often, there is much traffic in one direction, while the channel for the
opposite direction is idle. NoCs with run-time reconfigurable
bi-directional channels have been recently proposed to fully utilize the
bandwidth according to the run-time traffic conditions (BiNoC) [ 102 ,
100 , 101 , 120 ] . However, adaptive routing is less exploited in these
works for simplicity reasons.

Previous adaptive routing algorithms are mainly designed for
unidirectional NoCs. In [ 114 ] , a neighbors-on-path (NoP) selection
strategy is proposed to make each node routing selection based on the
condition of the nodes adjacent to the neighbors. In [ 115 ] , a
regional contention awareness scheme (RCA) was first proposed to utilize
both the local and non-local information to improve the load balancing
in NoCs. In [ 112 ] , the RCA scheme was improved by using the adaptive
routing scheme DBAR to leverage the local and non-local network
information. In [ 116 ] , a destination-based adaptive routing algorithm
(DAR) is proposed where every node estimates the delay to every other
node in the network. However, for a more flexible NoC platform such as
the ones employing bi-directional links and EVC channels, these schemes
need to be modified with a congestion fitness function to reflect the
run time traffic status.

Starting from these overarching ideas, in this work, we aim at improving
the NoC performance with respect to both architecture design and routing
algorithm development. Towards this end, we propose a flexible NoC
architecture which combines the advantages of higher effective bandwidth
and router bypass capability and develop an adaptive routing algorithm
to optimize the network run-time performance.

### 6.3 New NoC architecture

In this section, we first present the motivation for adding regional hub
routers into the NoC and then elaborate on the new reconfigurable NoC
architecture.

#### 6.3.1 Motivation for adding hub routers

Because of the complexity of the dynamic EVC approach [ 113 ] , in this
work, we adopt a 3-hop static EVC network; this provides a better trade
off between performance and design complexity. We also define a region
as a @xmath mesh whose boundary is made up of the EVC source/sink nodes.
A regional hub router is added into each region based on the following
key observations:

1) The regional traffic represents a significant portion of the total
traffic distribution. Here, we define the regional traffic as the
communication flow whose source and destination reside within the same
region. This is especially true for application-specific NoCs where
highly communicating nodes are usually mapped close to each other. For
example, in [ 101 ] , the regional traffic with more than @xmath of
nodes communicating with other nodes residing within a region of three
hops away is used in the evaluation.

2) While the static EVCs provide express paths crossing over NoC
regions, they typically lack sufficient paths needed to handle the
traffic within the NoC regions. If both the source and destination nodes
are bypass nodes in a static EVC, then the packets cannot use the EVCs
to bypass the intermediate nodes.

3) The static EVCs enable packets to bypass the intermediate nodes
following a fixed express path. Also, the EVCs are restricted to a
single dimension and cannot turn directly. Consequently, it will be
beneficial if we can dynamically build shortcut paths for regional
traffic rather than being limited by the single dimension requirement.

Therefore, in this work, we propose to add a light weight hub router to
minimize the burden of the regional traffic in NoCs. More precisely, we
additionally allocate a dedicated ( i.e., static) express path for each
node to route to the hub router and utilize the hub router to connect
the nodes within the same region; this way, when the hub router is used,
the intermediate pipeline stages of the normal routers can be bypassed
altogether.

#### 6.3.2 Overview of proposed NoC architecture

Figure 6.1 -a shows the proposed flexible NoC architecture for @xmath
meshes. In this example, the static EVC hop length is equal to 3. Each
region is a small @xmath mesh. Other static EVC hop length and region
size can be used for different sizes of the meshes. The solid line in
Figure 6.1 -a represents the physical channels between the tiles which
are made up of two bi-directional links. Both bidirectional links can be
reconfigured as sending or receiving. For each router, we define one
link as a high priority link (master link) and the other as a low
priority link (slave link) as in [ 101 ] . By default, the master link
is the sending link and the slave link is the receiving link when there
is traffic in both directions. As shown in Figure 6.1 -a, the master
link of one router corresponds to the slave link of its neighbor. The
dash line in the figure represents the 3-hop EVCs.

We further divide each @xmath region into four quadrants (see Figure 6.1
-a). In order to reduce the additional complexity added to the NoC
backbone, the regional hub router is a simple and generic four ports
router with four virtual channels at each input port as shown in Figure
6.1 -b. Every input port corresponds to a quadrant in the region and
each node in the quadrant is assigned a virtual channel at the input
port of the hub router. For example, VC0 of the first input port (i.e.
the quadrant 1 port) of the hub router is assigned to node (0,2) in
Figure 6.1 -a.

At run time, the hub router plays the role of setting a connection
between the source and destination pairs within different quadrants of
the same region. For example, in Figure 6.1 -c, if node (1,3) in
quadrant one needs to communicate with node (3,1) in the quadrant three,
it cannot use the static EVCs. Instead, it can communicate via the hub
router. The packet will first follow the dedicated express hub path
bypassing router (1,2) pipeline stages to reach the input port
corresponding to quadrant one of the hub router. After the hub router
grants its connection from quadrant one input port to the quadrant three
output port in the switch allocation stage, the packet will traverse the
crossbar in the hub router and bypass router (2,1) pipeline stages to
arrive at the destination node (3,1) directly. Compared with using
normal paths to send packets from (1,3) to (3,1), which requires five
router pipeline stages, only one router pipeline in the hub router is
needed.

In the proposed architecture, we utilize the virtual channel allocator
and switch allocator in the hub router to arbitrate the contentions. For
instance, if both nodes (1,3) and (1,0) want to send packets to (3,1)
using the hub router, after arriving at the hub router, these two
packets will request the same virtual channel which corresponds to node
(3,1) in the quadrant three output port. The virtual channel allocator
will grant one of the requests first, while the other packet needs to
wait in the input channel buffer until the virtual channel becomes
available again. Similarly, when there are multiple requests from
different input quadrant ports to the same output quadrant, for
instance, if both the packets from node (1,3) to (3,1) and node (1,1) to
(3,0) want to use the quadrant four output port in the hub router, the
switch allocator will only serve one request each time.

#### 6.3.3 Router implementation

In this section, we present the router design that supports the BiNoC
with EVC and region hub. First, we will discuss the additional hardware
required to support the BiNoC approach. Then the micro-architecture that
supports static EVCs will be presented. Finally, we present the
modifications required to support the region-hub routing.

Figure 6.2 -a shows the router design for BiNoC with the extra logic
required (see the shaded area) to support the bi-directional switching.
Compared to a unidirectional wormhole router, in BiNoC each input and
output port need to be modified to inout ports controlled by the
corresponding finite state machines (FSMs) [ 101 ] . Also, the arbiter
and the crossbar are modified to handle the requests received from the
master and slave ports. At run time, the FSM associated to every inout
port configures the port mode ( i.e., input or output) according to the
requests existing in current router and the backpressure feedback from
the neighboring routers as discussed in [ 101 ] .

Figure 6.2 -b shows the router micro-architecture that supports static
EVCs for BiNoC. In this work, we build the EVC path using the output
ports corresponding to the master links, because the master link has a
higher priority in transmitting flits according to the channel direction
controlled by the FSM. Compared to the BiNoC router in Figure 6.2 -a,
the extra logic need to support the EVC architecture is shaded in Figure
6.2 -b and Figure 6.2 -c. In static EVCs, there are two types of nodes,
namely the bypass nodes and the EVC source/sink nodes. In the bypass
nodes (as shown in Figure 6.2 -b), an EVC latch is added to hold the
packets using the EVC. Once a packet arrives at the EVC latch, the
packet will bypass all the intermediate pipeline stages of the current
router (such as switch allocation, crossbar traversal) and is directly
sent to the output port in the same dimension. In Figure 6.2 -b, the EVC
paths along the north/south direction that a packet will take to bypass
the router pipeline is highlighted. The architecture of the EVC
source/sink routers is shown in Figure 6.2 -c. Here, we add an EVC sink
buffer at the slave link port to hold the packets from the upstream
nodes. In addition, an EVC arbiter is added to handle the arbitration
requests from all the input channels (both the normal input and EVC sink
buffers). Once a packet successfully wins the allocation, an EVC flag in
the flit will be asserted. Based on this flag, the downstream router
will decide whether to put the flits into the EVC latch or into the
normal input buffer.

In the proposed regional hub routing, for every node, there is one
static express path assigned to connect to the hub router. Figure 6.3
shows the paths assigned to the four nodes in the quadrant one of Figure
6.1 . We add hub latches to multiplex with the normal buffers at the
input port and hub registers to multiplex with the output registers at
the output port (shown in Figure 6.3 ). The hub latch and register pair
forms an express path for a dedicated node to bypass the pipeline stage
within the router.

Figure 6.4 -a shows the router design for node (1,2) in Figure 6.3 which
connects to the hub router using an additional port. The extra logic
needed compared to Figure 6.2 -c is shaded. As shown in Figure 6.4 -a,
the EVC paths are highlighted in red lines while the express paths
formed by hub latch/register pairs are highlighted in blue lines.

At run time, there may be contentions between the hub latches or between
the EVC and hub latches. For example, for router (1,3) in Figure 6.4 -b,
the hub latch in the local port and the hub latch in the west port may
contend with the EVC latch in the north port to use the south output
link. In order to resolve such contentions, the express path arbiter in
Figure 6.4 -b is needed in every router to handle the hub and EVC latch
requests to make sure that, at any one time, only one packet can bypass
the pipeline stages and use the physical channel. In this work, we
allocate a higher priority to the EVC paths, because the hub paths only
serve the regional traffic, while the EVC paths usually work for the
traffic meant for longer distances. For example, the express path
arbiter will first grant the north EVC latch to use the south output
link. The packets from the west and local hub latches, as well as the
input buffers will wait in the output port. Then, the express path
arbiter will grant the requests from hub latches to use the output link
with a higher priority than the normal paths. In order to avoid the
starvation of flits in normal buffers and registers, the arbiter will
grant the requests from normal paths after serving the EVC and hub
latches for @xmath consecutive cycles as in [ 113 ] . In this work,
@xmath is set to be 30 according to our simulation results.

### 6.4 Routing algorithm design

In this section, we present the details of our adaptive routing
algorithm for the proposed flexible NoC. We first describe the scheme to
choose among the static EVCs, express hub paths and BiNoC for sending a
packet. Next, we present the adaptive routing algorithm when using the
bi-directional channels to route packets. We develop a fitness function
as a metric to evaluate the neighboring node availability based on the
channels occupancy. The fitness function not only captures the buffer
status but also takes the traffic dynamics, as well as the
characteristics of the bi-directional channel into account. Then, a
K-step speculative routing selection strategy is proposed to evaluate
each candidate direction, which dynamically selects the nodes with the
best fitness values that reside on a path towards the destination.

#### 6.4.1 Choosing the express hub and EVC paths

In the proposed NoC architecture, we can use EVCs, hub routers, as well
as the bi-directional channels to send packets around. The network
interface (NI) in the source tile determines whether to use the express
hub paths for the packet first. More specifically, the NI first compares
the source and destination address of the packet. If the source and
destination nodes are located in different quadrants of the same region,
and if the destination node cannot be reached by EVCs from the source
directly, then the NI will check the availability of the hub latch in
the local port of the neighboring router. If the hub latch is free, the
packet will be sent to it and follow the express hub path to route
towards the destination. Otherwise, the packet will be sent to the
normal buffer in the local port of the neighboring router.

In the EVC source/sink nodes, for each input channel, the routing
computation module decides whether to use the EVCs based on the EVC
latch availability and the average waiting time of the sink buffer in
the EVC sink node. If the waiting time is larger than a threshold T
(which means that there exists heavy congestion at the EVC sink node),
the EVC will not be chosen for routing.

At last, for the packets which cannot use EVCs or the express hub paths,
the adaptive routing algorithm proposed for bi-directional NoC is used
to find an output direction based on the run time traffic status.

#### 6.4.2 Adaptive routing for BiNoC

##### Channel and direction fitness

In NoCs utilizing bidirectional channels, for each direction, there are
two input/output channels corresponding to the master and slave links,
respectively. We use two metrics, namely channel fitness and direction
fitness, to reflect the suitability of an input channel to receive
packets.

In the channel fitness function, in order to predict the dynamics of the
network traffic which may exhibit a non-Markovian behavior [ 37 ] , we
include the channel average waiting time to reflect the channels
”memory”. The fitness value of an input channel is represented as:

  -- -------- -- -------
     @xmath      (6.1)
  -- -------- -- -------

where @xmath is a scaling parameter, @xmath is the average number of
free slots in the channel and @xmath is the average waiting time that a
packet spends in the channel. In this work, according to the simulation
results with different @xmath values ranging from @xmath to @xmath , we
observe @xmath equals to 0.8 offers the highest performance.

During the routing stage, since we have not yet determined whether the
master or the slave output port will be used, we need to evaluate the
input channels at both ports of the downstream router to get a more
appropriate metric for choosing the output direction. The direction
fitness function is used to indicate the overall availability of one
direction in the downstream router. One issue when designing the
direction fitness function is to consider the characteristics of the
bi-directional channels since the effective bandwidth between two
neighboring routers varies with the actual traffic over time. One simple
way is to add together the two channels fitness values regardless of
them being of master or slave type. However, because the master port has
a higher priority in sending and its input channel can only receive
flits when there is no sending request at the same port. Then, if we
only consider the channel fitness without regarding to the probability
of receiving flits using this channel or the actual channel utilization,
it may not lead to an optimal selection.

Input: Average waiting time: @xmath ; Average available slots @xmath ;
master port input channel block probability @xmath ; scaling parameter
@xmath
Output: Channel fitness: @xmath and the direction fitness function
@xmath

1: for all node @xmath do

2: for all @xmath do

3: @xmath

4: for all @xmath do

4: @xmath

5: if @xmath then

6: @xmath

7: else

8: @xmath

9: end if

10: end for {for all input direction @xmath }

11: end for {for all output direction @xmath }

12: end for {for all the nodes in the NoC}

Algorithm 5 Channel direction fitness calculation

In order to deal with this issue, we calculate the fitness of a
particular direction by distinguishing the input channels at the master
and slave ports. For the input channel in the slave port, because the
channel can always be used whenever there is a request from a neighbor,
its fitness directly contributes to the fitness function calculation.
For the input channels at the master inout port, we calculate the
average block probability @xmath due to the write conflict during a time
window. This is the probability when the FSM controller configures the
port into the output mode and lets the output register use the link for
sending packets instead of receiving the packets from the neighboring
nodes. The fitness of direction i under consideration is then:

  -- -------- -- -------
     @xmath      (6.2)
  -- -------- -- -------

In Algorithm 5 , we present the idea of calculating the fitness function
periodically for any given direction. The notations used for the
algorithm are listed in Table 6.1 . Two counters are needed in each
input channel to calculate the average waiting time of packets and the
number of available buffer slots during any time window. According to
these two metrics, the input channel fitness at the master and slave
inout ports can be calculated. An extra counter at the master port FSM
is used to record the average blocking probability @xmath . The fitness
of the direction under evaluation is then the weighted sum of the two
input channels fitness values.

##### Fitness based adaptive routing algorithm

In order to avoid deadlocks at run time, the odd-even turn rules [ 91 ]
are used in the minimal path based routing computation. Under this
assumption, for any packet in the current router, there are at most two
feasible output directions.

As discussed in [ 115 , 114 , 116 ] , if the adaptive routing algorithm
only considers the local congestion metric, it will be too slow to
re-act to congestion in the more distant parts of the network as it
relies on network backpressure to propagate the congestion state. In
this work, we propose a scheme that combines the local and remote
channel fitness status in order to make a better routing decision (see
Algorithm 2).

Different from previous approaches in [ 114 , 115 , 112 ] , which
utilize fixed local and non-local neighbors to evaluate each candidate
direction in the router, our proposed adaptive routing algorithm uses a
K-step speculative routing to dynamically select a set of nodes on the
routing path which have the best fitness value towards the destination
currently. Based on the fitness value of these nodes, the routing
algorithm decides the optimal output port for the current router.

Figure 6.5 shows a routing scenario for the proposed K-step speculative
routing with @xmath . The values on the link indicate the direction
fitness level measured at each router. When evaluating the east output
direction for the current node, node (1,1) is chosen first. Then, the
router compares the two candidate directions from node (1,1) to the
destination. Since the north output direction at node (1,1) has a larger
direction fitness value (5) than the east output direction. The north
neighboring node (1,2) is chosen second. Similarly, for node (1,2), its
east neighboring node (2,2) will be selected. Therefore, nodes (1,1),
(1,2) and (2,2) form a 3-step speculative path towards the destination
using the east output direction of the current router. Similarly, nodes
(0,2), (0,3) and (1,3) form a speculative path from the north output
direction towards the destination.

Input: Current node : Destination node ; Set of candidate output
directions @xmath
Output: Output direction @xmath
Function: @xmath : A set of candidate directions for node @xmath

1: for all direction @xmath do

2: @xmath ;

3: @xmath ;

4: @xmath ;

5: while @xmath do

6: @xmath ;

7: @xmath ;

8: if @xmath then

9: @xmath ;

10: @xmath ;

11: if @xmath then

12: @xmath ;

13: else

14: @xmath ;

15: end if

16: end if

17: @xmath ;

18: end while

19: end for

20: @xmath ;

Algorithm 6 K-step speculative selection algorithm

By adding together the direction fitness values of each path, the scores
of using the north and the east output port can be calculated. The
router then chooses the direction with the best score for the current
router. If the direction fitness value changes at run time, the nodes
selected for evaluating each candidate port will also change accordingly
to capture the network dynamics.

Algorithm 6 presents the details of the proposed @xmath -step
speculative routing algorithm. At run time, each router periodically
collects the channel and direction fitness values of the neighbors
located in a region less than @xmath -hops away. Then, for each output
candidate, we perform a @xmath -step look ahead routing towards the
destinations and select a set of @xmath consecutive nodes and directions
to form a speculative routing path towards the destinations using
current candidate direction. By comparing the total fitness values along
the paths between the two candidate directions, the direction with the
best path fitness value is chosen as the output port for the current
router.

In this work, we adopt a light weight traffic status propagation network
similar to [ 116 ] which is used to propagate the direction fitness
information to the nodes within @xmath -hops distance in the network.
More specifically, every @xmath cycles, the direction fitness value
updated by the current router is propagated @xmath hops away throughout
the network in a hop-by-hop manner. After collecting all the direction
fitness values from the neighboring routers within K-hops distance,
these values are stored in the router and used for the proposed K-step
speculative routing algorithm. For the case @xmath , we can further
reduce the network complexity by using a specific control channel
between the neighboring nodes as in [ 114 ] instead of a status
propagation network to exchange the traffic information. The signal sent
through the control channel contains the fitness of the direction @xmath
associated to the current router and its 1-hop neighbors. According to
the simulation results shown in section 5, there is not too much
performance improvement for @xmath compared to @xmath . Hence we propose
to set @xmath (i.e. look only two hops ahead) to achieve the best
trade-off between the performance and additional complexity.

### 6.5 Simulation results

#### 6.5.1 Simulation setup

We evaluate the proposed NoC architecture, as well as the adaptive
routing algorithm, using a cycle accurate NoC simulator implemented in
C++. Simulations were done for different mesh network sizes: @xmath and
@xmath . To investigate the benefits of our NoC architecture and the
performance improvements of our adaptive traffic aware routing
algorithm, we compare our approach with the following NoC architectures:
1) classical unidirectional NoC; 2) bi-directional NoC [ 101 ] ; 3)
bi-directional NoC with static EVCs [ 113 ] .

Throughout the experiments, we assume that each input channel has a
buffer depth of 6 flits for the NoC using bi-directional channels and a
buffer depth of 12 flits for the uni-directional NoC. Both synthetic
traffic ( i.e., uniform, transpose) and real benchmark traces were used
in the simulations. For uniform traffic, each node sends packets to
other nodes in the network with an equal probability. For transpose
traffic, each node at the @xmath mesh location sends packets to the node
at the @xmath location on the mesh.

For the real benchmark workloads, we consider several 16-node
multithreaded commercial and scientific workloads from DBmbench [ 121 ]
and SPECweb99 [ 111 ] . This type of traffic includes the memory
request/response coherence traffic between PEs and caches.

#### 6.5.2 Comparison of adaptive routing for BiNoC

We compare our proposed adaptive routing algorithm with some widely
adopted routing algorithms to evaluate the performance under
bi-directional channels. More precisely, we compare the benefits of
using a fitness function and a K-step speculative selection algorithm to
capture the network traffic status. First, we compare different fitness
functions used to evaluate the router channels. In this comparison, we
only gather the traffic status from the immediate neighbors and use
different fitness functions to evaluate the channels status. The
baseline for this comparison is the random selection strategy of the
Odd-even routing algorithm [ 91 ] . Two direction fitness functions are
used in the simulation:

1) Fitness function 1 is the direct sum of the input channel fitness
without distinguishing the master and slave channels

2) Fitness function 2 uses the link blocking probability to weight the
channels in the combination of the master and slave channels fitness
values.

Figure 6.6 shows the comparison results obtained for an @xmath mesh
under random and transpose traffic patterns. As shown in this figure,
using a fitness function to represent the channel traffic status
significantly improves the network critical load (by more than 22.5%).
Furthermore, if we consider the characteristic of the bidirectional
links by distinguishing the master and slave channels, an extra 5% - 8%
performance gain can be achieved.

Next, using the proposed fitness function for the bidirectional NoC, we
make a comparison of different adaptive routing schemes which consider
both the immediate neighbors and non-local nodes to evaluate the
directions in the routing selection phase. We evaluate our proposed
K-step speculative selection scheme with different K values (K=2, 3, 4).
At the same time we compare our scheme with two widely adopted schemes,
namely neighbor-on-path [ 114 ] and regional-contention-awareness
(RCA-1D) [ 115 ] under random, and transpose traffic patterns on an
@xmath mesh. In order to minimize the intra-region interference in
RCA-1D [ 112 , 116 ] , we modify the RCA-1D scheme to integrate the
destination into the selection function as in the DBAR approach [ 112 ]
.

As shown from Figure 6.7 , the proposed K-step speculative scheme
achieves the best performance results by selectively choosing the
intermediate nodes towards destination at run time. Also, it can be seen
that there is no significant improvement in the performance for a K
value larger than 2. Since the complexity of the fitness value
propagation increases dramatically when @xmath , we use @xmath
throughout the remaining simulations.

#### 6.5.3 Effect of adding EVC and hub routers

In this section, we evaluate the performance improvement resulted by
adding EVC and regional hub routers into the BiNoC. Both random
selection and K-step speculative selection strategies are used in these
simulations. The following architectures were evaluated and compared:
the typical NoC with unidirectional channels, the BiNoC proposed in [
101 ] , the BiNoC + EVC which combines the features of Bi-directional
switching in [ 101 ] and express virtual channels in [ 113 ] , and the
proposed NoC with regional hubs.

Figure 6.8 summarizes the simulation results. We compare the average
latency performance under various traffic injection rate for @xmath and
@xmath meshes. Several observations can be made based on these results:

1) BiNoC outperforms the typical NoC design. Adding EVC into the network
can further improve the network criticality by 9.8%-80% for different
traffic patterns and mesh sizes. Furthermore, if the regional hub router
is added, the throughput can be further improved by 9.3%-15%.

2) For all the NoC architectures using the adaptive routing algorithm,
the fitness function employed to evaluate the traffic status can
significantly improve the network performance. As shown in Figure 6.8 ,
using the 1 hop away neighboring info can improve the network
criticality by 13% - 30%, while using 2 hop neighbor info can further
improve the performance by 3% - 11% in most cases.

#### 6.5.4 Results using real world workloads

In this section, we show the latency comparison for various real world
benchmarks with an @xmath mesh size. We report seven experiments. The
first five experiments collect the traces from the same applications,
i.e., Apache, Oracle, Db2V, sparse and Ocean which were mapped onto an
@xmath mesh. The last two experiments named Mixed-1 and Mixed-2 combine
the traces from different applications and map them to the same NoC
platform. The simulation time of each benchmark is set to @xmath clock
circles with a @xmath cycles warmup period.

As shown in Figure 6.9 , compared with the baseline BiNoC design with
random selection strategy, which is normalized to 1, by adopting a
fitness function to evaluate the channels status can reduce the average
latency by @xmath . For example, after applying the fitness functions,
the latency of the sparse benchmark is reduced to @xmath compared with
BiNoC routing. The static EVC approach further improves the performance
by @xmath compared with its BiNoC counterparts for both random selection
or fitness function based selection strategies. As in the sparse
application, the EVC+BiNoC+2 hop info routing improves the latency by
@xmath over the BiNoC + 2 hop info routing. If a central hub router is
added into each region in our proposed NoC architecture, the performance
can be further improved by @xmath . As shown in Figure 6.9 , for the
sparse application, the proposed NoC architecture with @xmath
speculative routing improves the latency by @xmath .

Figure 6.10 shows the percentage of packets that utilize the EVCs and
hub routers when running different benchmarks. As shown, on average,
@xmath of packets utilize the express hub paths to send packets to the
destination directly, while @xmath packets utilize EVCs at least once at
run time.

Next, we evaluate the energy overhead of different NoC architectures
while running these applications. For this purpose, we have modified the
Orion power model [ 44 ] to consider the extra energy consumption over
the unidirectional NoC. For the BiNoC, the additional energy consumption
is mainly due to the larger crossbar size ( @xmath ), and the channel
direction control logic [ 101 ] . We modified the energy consumption of
the crossbar in the Orion power model and added the energy overhead for
the link direction switching according to [ 101 ] . For BiNoC + EVC
architecture, the additional energy consumption for the EVC latch and
the EVC arbiter is discussed in [ 113 ] and we add this to the Orion
model. For the energy consumption of the proposed architecture, the hub
latches in the normal router are modeled similarly to the EVC, while the
energy consumption of the hub router is modeled as a four-port, four-VC
normal router as in the Orion power model. For the calculation of the
fitness values for each channel, we need additional adders and
multipliers to calculate the fitness values every 100 cycles. This
energy overhead is modeled in a similar way as in [ 114 ] .

In Figure 6.11 , the energy consumption of the unidirectional NoC
architecture is normalized to one. The BiNoC approach has about 10%-20%
energy overhead due to a large crossbar size and the link direction
switching overhead [ 101 ] . The EVC and regional hub approach can
effectively reduce the energy by allowing some packets use the express
paths and bypassing the router pipeline stages. In Figure 6.11 , the
energy overhead for the proposed NoC architecture with 2-step selection
strategy is 13% on average.

#### 6.5.5 Overhead evaluation

We have modified the Orion 2.0 area model [ 44 ] to evaluate the area
overhead. In this evaluation, we target a 1.0GHz operating frequency
under the 65nm technology. When evaluating the BiNoC, the bi-directional
link direction control logic is considered as in [ 101 ] . As shown in
Figure 6.12 , the area numbers of source/sink nodes and bypass nodes in
the BiNoC+EVC architecture, as well as the normal router and hub router
in the proposed architecture are compared against the baseline BiNoC
design which is normalized to one. Router (1,2) in Figure 6.3 is chosen
when evaluating the normal routers in the proposed architecture, since
this router is the most complicated one with an extra port connected to
the hub router. In Figure 6.12 , compared to BiNoC baseline router, the
normal router of the proposed architecture has an overhead of about 13%.
Together with the additional hub router in each @xmath region, the
proposed NoC architecture has an area overhead of 19%.

### 6.6 Conclusion

In this chapter, we have proposed a flexible NoC architecture which
utilizes bidirectional channels, EVC channels and regional hub routers
to improve the NoC performance. We have also proposed a traffic-aware,
adaptive routing algorithm that considers the characteristic of the
bidirectional links. Simulation results on synthetic traffic, as well as
real world benchmarks, show that the proposed NoC architecture, together
with the routing algorithm, can significantly reduce the latency by as
much as @xmath with a small energy and area overhead.

## Chapter 7 Conclusions and future work

### 7.1 Research summary

The work discussed in this thesis focus on the issues of achieving a
high performance NoC design for future multicore systems. In particular,
starting from the offline design space exploration, we have proposed a
more accurate NoC model to predict the latency performance under various
routing algorithm and placement choices. We first identified the
limitations of conventional simulations and queuing-theory-based
performance evaluation methods. Motivated by combining the high accuracy
in simulations and fast speed in analytical models, we have applied
machine learning techniques to combine the advantages of simulations and
mathematical formalism. In Chapter 2, we have presented a support vector
regression based latency model for evaluating NoC performance in the
synthesis inner loop. Through learning from the typical training data,
the SVR-NoC achieves better accuracy and similar speedup performance
compared to the queuing models, which will benefit the exploration of
the design space offline.
Then, in Chapter 3 and Chapter 4, we have explored the NoC routing
algorithm designs for different objectives and constraints.
Specifically, we have discussed the routing algorithms for the
thermal-awareness and fault-tolerance purposes. In Chapter 3, in order
to reduce the hotspot temperature of the whole system while maintaining
the latency and throughput performance, we have proposed an
application-specific path set finding algorithm which has a high
adaptivity to distribute traffic and ensures deadlock-free property.
Then, a linear programming problem is formulated to compute the optimal
ratio of using the paths offline. The router architecture which supports
the proposed ratio-based adaptive routing is also discussed. In Chapter
4, we have proposed an adaptive routing algorithm to tackle run time
router faults. In this chapter, we proposed a dynamic buffer swapping
algorithm and a dynamic crossbar Mux swapping algorithm to maximally
maintain the network connectivity under the buffer and crossbar faults,
respectively. Higher packet acceptance rate and lower latency can be
achieved compared to previous re-routing based methods.
In Chapter 5 and Chapter 6, we have proceeded to explore the
architectures for optimizing the system performance. We have proposed
our new NoC design based on the usage of the emerging bi-directional
channels. More precisely, we have designed two new flexible NoC
architectures using bi-directional links to further improve the
throughput and latency performance. In Chapter 5, the overall NoC
topology is unchanged. We focused on the router design to improve the
usage of bi-directional channel bandwidth. Towards this end, we have
proposed a flit-level speedup scheme (FSNoC) to allow two flits from the
same packet to be transmitted simultaneously. In this way, higher
bandwidth utilization and throughput can be achieved compared to the
conventional NoCs equipped with bi-directional channels. In Chapter 6,
we have proposed a new NoC topology which takes the the advantage of
several express paths provided in the system: 1) bi-directional
channels, 2) express paths and 3) regional hub router based shortcut
paths. An adaptive routing algorithm is designed to dynamically choose
among these paths. We have demonstrated the effectiveness of the
proposed new architecture using both synthetic traffic and real
multicore workloads.

### 7.2 Future research directions

Additional work is needed to extend this thesis work. In future, we
expect to conduct the researches in the following directions:
1) Exploring the current learning based NoC models: We will explore and
compare several other learning methods besides support vector regression
( e.g., neural networks, Gaussian process). We will compare the model
accuracy with the SVR-based model. Also, we will explore the inclusion
of different features to improve the learning accuracy and reduce the
training data size as well as learning time. We plan to study the
importance of each features in the feature set and develop the criteria
of how to choose features in the training stage. Moreover, we will
develop a more flexible platform to extract arbitrary training data from
the simulation.
2) Applying learning techniques to model the worst-case delay: Besides
the average latency modeling in this work, we will also apply the
learning techniques to predict the worst-case delays for the NoCs with
real-time deadline requirement. We will explore the effects of different
learning methods on the worst case delay bound tightness.
3) Hardware implementation for the flexible NoC architecture: For our
proposed NoC architecture, we plan to implement a FPGA-based prototype
and carry out a thorough comparison and analysis on the power, area
overhead.
4) More applications based on the proposed NoC platform: In future, we
also expect to see more applications which is implemented based on the
proposed NoC architectures. Specifically, we have started to apply the
proposed NoC platform for a specific biological application, i.e., the
protein folding calculation and prediction.
5) Hardware implementations and detail evaluations of FSNoC under the
existence of long wire links: As the bi-directional repeaters introduced
in Chapter 5 may increase the critical path length and power overhead, a
more accurate hardware overhead evaluation of FSNoC should also consider
the physical VLSI implementation and optimization of the bi-repeaters.
6) Run-time thermal aware routing for reducing the hotspot temperature:
The proposed thermal aware routing is based on application task graph,
which only characterizes the average-case communication bandwidth
requirement. Therefore, the routing is done in the offline phase and
cannot capture the burst traffic arrivals at run time. In order to
consider the fluctuation of power/thermal profile at run time, a
fine-grained on-line routing algorithm is needed which determines the
routing path dynamically based on the current temperature of the routers
and PEs.