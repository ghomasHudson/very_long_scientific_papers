##### Contents

-    1 Introduction
-    2 Adaptive Prefix Coding
    -    2.1 Algorithm
    -    2.2 Analysis
-    3 Online Sorting with Few Comparisons
    -    3.1 Algorithm
    -    3.2 Lower bound
-    4 Online Sorting with Sublinear Memory
    -    4.1 Algorithm
    -    4.2 Lower bound
-    5 One-Pass Compression
    -    5.1 Algorithm
    -    5.2 Lower bounds
-    6 Stream Compression
    -    6.1 Universal compression
    -    6.2 Grammar-based compression
    -    6.3 Entropy-only bounds
-    7 Conclusions and Future Work

## Chapter 1 Introduction

Sequential-access data compression is by no means a new subject, but it
remains interesting both for its own sake and for the insight it
provides into other problems. Apart from the Data Compression
Conference, several conferences often have tracks for compression (e.g.,
the International Symposium on Information Theory, the Symposium on
Combinatorial Pattern Matching and the Symposium on String Processing
and Information Retrieval), and papers on compression often appear at
conferences on algorithms in general (e.g., the Symposium on Discrete
Algorithms and the European Symposium on Algorithms) or even theory in
general (e.g., the Symposium on Foundations of Computer Science, the
Symposium on Theory of Computing and the International Colloquium on
Algorithms, Languages and Programming). We mention these conference in
particular because, in this thesis, we concentrate on the theoretical
aspects of data compression, leaving practical considerations for later.

Apart from its direct applications, work on compression has inspired the
design and analysis of algorithms and data structures, e.g., succinct or
compact data structures such as indexes. Work on sequential data
compression in particular has inspired the design and analysis of online
algorithms and dynamic data structures, e.g., prediction algorithms for
paging, web caching and computational finance. Giancarlo, Scaturro and
Utro [ GSU ] recently described how, in bioinformatics, compression
algorithms are important not only for storage, but also for indexing,
speeding up some dynamic programs, entropy estimation, segmentation, and
pattern discovery.

In this thesis we study three kinds of sequential-access data
compression: adaptive prefix coding, one-pass compression with memory
bounded in terms of the alphabet size and context length, and
compression in the read/write streams model. Adaptive prefix coding is
perhaps the most natural form of online compression, and adaptive prefix
coders are the oldest and simplest kind of sequential compressors,
having been studied for more than thirty years. Nevertheless, in Chapter
2 we present the first one that is worst-case optimal with respect to
both the length of the encoding it produces and the time it takes to
encode and decode. In Chapter 3 we observe that adaptive alphabetic
prefix coding is equivalent to online sorting with binary comparisons,
so our algorithm from Chapter 2 can easily be turned into an algorithm
for online sorting. Chapter 4 is also about online sorting but, instead
of aiming to minimize the number of comparisons (which remains within a
constant factor of optimal), we concentrate on trying to use sublinear
memory, in line with research on streaming algorithms. We then study
compression with memory constraints because, although compression is
most important when space is in short supply and compression algorithms
are often implemented in limited memory, most analyses ignore memory
constraints as an implementation detail, creating a gap between theory
and practice. We first study compression in the case when we can make
only one pass over the data. One-pass compressors that use memory
bounded in terms of the alphabet size and context length can be viewed
as finite-state machines, and in Chapter 5 we use that property to prove
a nearly tight tradeoff between the amount of memory we can use and the
quality of the compression we can achieve. We then study compression in
the read/write streams model, which allows us to make multiple passes
over the data, change them, and even use multiple streams (see [ Sch07 ]
). Streaming algorithms have revolutionized the processing of massive
data sets, and the read/write streams model is an elegant conversion of
the streaming model into a model of external memory. By viewing
read/write stream algorithms as simply more powerful automata, that can
use passes and memory both polylogarithmic in the size of the input, in
Chapter 6 we prove lower bounds on the compression we can achieve with
only one stream. Specifically, we show that, although we can achieve
universal compression with only one pass over one stream, we need at
least two streams to achieve good grammar-based compression or
entropy-only bounds. We also combine previously known results to prove
that two streams are sufficient for us to compute the Burrows-Wheeler
Transform and, thus, achieve low-entropy bounds. As corollaries of our
lower bounds for compression, we obtain lower bounds for computing
strings’ minimum periods and for computing the Burrows-Wheeler Transform
[ BW94 ] , which came as something of a surprise to us. It seems no one
has previously considered the problem of finding strings’ minimum
periods in a streaming model, even though some related problems have
been studied (see, e.g., [ EMŞ04 ] , in which the authors loosen the
definition of a repeated substring to allow approximate matches). We are
currently investigating whether we can derive any more such results.

The chapters in this thesis were written separately and can be read
separately; in fact, it might be better to read them with at least a
small pause between chapters, so the variations in the models considered
do not become confusing. Of course, to make each chapter independent, we
have had to introduce some degree of redundancy. Chapter 2 was written
specifically for this thesis, and is based on recent joint work [ GN ]
with Yakov Nekrich at the University of Bonn; a summary was presented at
the University of Bielefeld in October of 2008, and will be presented at
the annual meeting of the Italy-Israel FIRB project “Pattern Discovery
in Discrete Structures, with Applications to Bioinformatics” at the
University of Palermo in February of 2009. Chapter 3 was also written
specifically for this thesis, but Chapter 4 was presented at the 10th
Italian Conference on Theoretical Computer Science [ Gag07b ] and then
published in Information Processing Letters [ Gag08 ] . Chapter 5 is a
slight modification of part of a paper [ GM07b ] written with Giovanni
Manzini at the University of Eastern Piedmont, which was presented in
2007 at the 32nd Symposium on Mathematical Foundations of Computer
Science. A paper [ GKN09 ] we wrote with Marek Karpinski (also at the
University of Bonn) and Yakov Nekrich that partially combines the
results in these two chapters, will appear at the 2009 Data Compression
Conference. Chapter 6 is a slight modification of a paper [ Gagb ] that
has been submitted to a conference, with a very brief summary of some
material from a paper [ GM07a ] written with Giovanni Manzini and
presented at the 18th Symposium on Combinatorial Pattern Matching.

## Chapter 2 Adaptive Prefix Coding

Prefix codes are sometimes called instantaneous codes because, since no
codeword is a prefix of another, the decoder can output each character
once it reaches the end of its codeword. Adaptive prefix coding could
thus be called “doubly instantaneous”, because the encoder must produce
a self-delimiting codeword for each character before reading the next
one. The main idea behind adaptive prefix coding is simple: both the
encoder and the decoder start with the same prefix code; the encoder
reads the first character of the input and writes its codeword; the
decoder reads that codeword and decodes the first character; the encoder
and decoder now have the same information, and they update their copies
of the code in the same way; then they recurse on the remainder of the
input. The two main challenges are, first, to efficiently update the two
copies of the code and, second, to prove the total length of the
encoding is not much more than what it would be if we were to use an
optimal static prefix coder.

Because Huffman codes [ Huf52 ] have minimum expected codeword length,
early work on adaptive prefix coding naturally focused on efficiently
maintaining a Huffman code for the prefix of the input already encoded.
Faller [ Fal73 ] , Gallager [ Gal78 ] and Knuth [ Knu85 ] developed an
adaptive Huffman coding algorithm — usually known as the FGK algorithm,
for their initials — and showed it takes time proportional to the length
of the encoding it produces. Vitter [ Vit87 ] gave an improved version
of their algorithm and showed it uses less than one more bit per
character than we would use with static Huffman coding. (For simplicity
we consider only binary encodings; therefore, by @xmath we always mean
@xmath .) Milidiú, Laber and Pessoa [ MLP99 ] later extended Vitter’s
techniques to analyze the FGK algorithm, and showed it uses less than
two more bits per character than we would use with static Huffman
coding. Suppose the input is a string @xmath of @xmath characters drawn
from an alphabet of size @xmath ; let @xmath be the empirical entropy of
@xmath (i.e., the entropy of the normalized distribution of characters
in @xmath ) and let @xmath be the redundancy of a Huffman code for
@xmath (i.e., the difference between the expected codeword length and
@xmath ). The FGK algorithm encodes @xmath as at most @xmath bits and
Vitter’s algorithm encodes it as at most @xmath bits; both take @xmath
total time to encode and decode, or @xmath amortized time per character.
Table 2.1 summarizes bounds for various adaptive prefix coders.

If @xmath is drawn from a memoryless source then, as @xmath grows,
adaptive Huffman coding will almost certainly “lock on” to a Huffman
code for the source and, thus, use @xmath bits. In this case, however,
the whole problem is easy: we can achieve the same bound, and use less
time, by periodically building a new Huffman code. If @xmath is chosen
adversarially, then every algorithm uses at least @xmath bits in the
worst case. To see why, fix an algorithm and suppose @xmath for some
integer @xmath , so any binary tree on @xmath leaves has at least two
leaves with depths at least @xmath . Any prefix code for @xmath
characters can be represented as a code-tree on @xmath leaves; the
length of the lexicographically @xmath th codeword is the depth of the
@xmath th leaf from the left. It follows that an adversary can choose
@xmath such that the algorithm encodes it as at least @xmath bits. On
the other hand, a static prefix coding algorithm can assign codewords of
length @xmath to the @xmath most frequent characters and codewords of
length @xmath to the two least frequent characters, and thus use at most
@xmath bits. Therefore, since the expected codeword length of a Huffman
code is minimum, @xmath and so @xmath .

This lower bound seems to say that Vitter’s upper bound cannot be
significantly improved. However, to force the algorithm to use @xmath
bits, it might be that the adversary must choose @xmath such that @xmath
is small. In a previous paper [ Gag07a ] we were able to show this is
the case, by giving an adaptive prefix coder that encodes @xmath in at
most @xmath bits. This bound is perhaps a little surprising, since our
algorithm was based on Shannon codes [ Sha48 ] , which generally do not
have minimum expected codeword length. Like the FGK algorithm and
Vitter’s algorithm, our algorithm used @xmath amortized time per
character to encode and decode. Recently, Karpinski and Nekrich [ KN ]
showed how to combine some of our results with properties of canonical
codes, defined by Schwartz and Kallick [ SK64 ] (see also [ Kle00 , TM00
, TM01 ] ), to achieve essentially the same compression while encoding
and decoding each character in @xmath amortized time and @xmath
amortized time, respectively. Nekrich [ Nek07 ] implemented their
algorithm and observed that, in practice, it is significantly faster
than arithmetic coding and slightly faster Turpin and Moffat’s GEO coder
[ TM01 ] , although the compression it achieves is not quite as good.
The rest of this chapter is based on joint work with Nekrich [ GN ] that
shows how, on a RAM with @xmath -bit words, we can speed up our
algorithm even more, to both encode and decode in @xmath worst-case
time. We note that Rueda and Oommen [ RO04 , RO06 , RO08 ] have
demonstrated the practicality of certain implementations of adaptive
Fano coding, which is somewhat related to our algorithm ,especially a
version [ Gag04 ] in which we maintained an explicit code-tree.

### 2.1 Algorithm

A Shannon code is one in which, if a character has probability @xmath ,
then its codeword has length at most @xmath . In his seminal paper on
information theory, Shannon [ Sha48 ] showed how to build such a code
for any distribution containing only positive probabilities.

###### Theorem 2.1 (Shannon, 1948).

Given a probability distribution @xmath with @xmath , we can build a
prefix code in @xmath time whose codewords, in lexicographic order, have
lengths @xmath .

We encode each character of @xmath using a canonical Shannon code for a
probability distribution that is roughly the normalized distribution of
characters in the prefix of @xmath already encoded. In order to avoid
having to consider probabilities equal to 0, we start by assigning every
character a count of 1. This means the smallest probability we ever
consider is at least @xmath , so the longest codeword we ever consider
is @xmath bits.

A canonical code is one in which the first codeword is a string of 0s
and, for @xmath , we can obtain the @xmath st codeword by incrementing
the @xmath th codeword (viewed as a binary number) and appending some
number of 0s to it. For example, Figure 2.1 shows the codewords in a
canonical code, together with their lexicographic ranks. By definition,
the difference between two codewords of the same length in a canonical
code, viewed as binary numbers, is the same as the difference between
their ranks. For example, the third codeword in Figure 2.1 is 0100 and
the sixth codeword is 0111, and @xmath , where @xmath means the argument
is to be viewed as a binary number. We use this property to build a
representation of the code that lets us quickly answer encoding and
decoding queries.

We maintain the following data structures: an array @xmath that stores
the codewords’ ranks in lexicographic order by character; an array
@xmath that stores the characters and their frequencies in order by
frequency; a dictionary @xmath that stores the rank of the first
codeword of each length, with the codeword itself as auxiliary
information; and a dictionary @xmath that stores the first codeword of
each length, with its rank as auxiliary information.

To encode a given character @xmath , we first use @xmath to find @xmath
’s codeword’s rank; then use @xmath to find the rank of the first
codeword of the same length and that codeword as auxiliary information;
then add the difference in ranks to that codeword, viewed as a binary
number, to obtain @xmath ’s codeword. For example, if the codewords are
as shown in Figure 2.1 and @xmath is the @xmath th character in the
alphabet and has codeword 0111, then

1.  @xmath ,

2.  @xmath ,

3.  @xmath .

To decode @xmath given a binary string prefixed with its codeword, we
first search in @xmath for the predecessor of the first @xmath bits of
the binary string, to find the first codeword of the same length and
that codeword’s rank as auxiliary information; then add that rank to the
difference in codewords, viewed as binary numbers, to obtain @xmath ’s
codeword’s rank; then use @xmath to find @xmath . In our example above,

1.  @xmath ,

2.  @xmath ,

3.  @xmath .

Admittedly, reading the first @xmath bits of the binary string will
generally result in the decoder reading past the end of most codewords
before outputting the corresponding character. We do not see how to
avoid this without potentially having the decoder read some codewords
bit by bit.

Querying @xmath and @xmath takes @xmath worst-case time, so the time to
encode and decode depends mainly on the time needed for predecessor
queries on @xmath and @xmath . Since the longest codeword we ever
consider is @xmath bits, each dictionary contains @xmath keys, so we can
implement each as an instance of the data structure described below, due
to Fredman and Willard [ FW93 ] . This way, apart from the time to
update the dictionaries, we encode and decode each character in a total
of @xmath worst-case time. Andersson, Miltersen and Thorup [ ABT99 ]
showed Fredman and Willard’s data structure can be implemented with AC
@xmath instructions, and Thorup [ Tho03 ] showed it can be implemented
with AC @xmath instructions available on a Pentium 4; admittedly,
though, in practice it might still be faster to use a sorted array to
encode and decode each character in @xmath time.

###### Lemma 2.2 (Fredman & Willard, 1993).

Given @xmath keys, we can build a dictionary in @xmath worst-case time
that stores those keys and supports predecessor queries in @xmath
worst-case time.

###### Corollary 2.3.

Given @xmath keys, we can build a dictionary in @xmath worst-case time
that stores those keys and supports predecessor queries in @xmath
worst-case time.

###### Proof.

We store the keys at the leaves of a search tree with degree @xmath ,
size @xmath and height at most 5. Each node stores an instance of
Fredman and Willard’s dictionary from Lemma 2.2 : each dictionary at a
leaf stores @xmath keys and each dictionary at an internal node stores
the first key in each of its children’s dictionaries. It is
straightforward to build the search tree in @xmath time and implement
queries in @xmath time. ∎

Since a codeword’s lexicographic rank is the same as the corresponding
character’s rank by frequency, and a character’s frequency is an integer
than changes only by being incremented after each of its occurrence, we
can use a data structure due to Gallager [ Gal78 ] to update @xmath and
@xmath in @xmath worst-case time per character of @xmath . We can use
@xmath binary searches in @xmath and @xmath time to compute the number
of codewords of each length; building @xmath and @xmath then takes
@xmath time. Using multiple copies of each data structure and standard
background-processing techniques, we update each set of copies after
every @xmath characters and stagger the updates, such that we need spend
only @xmath worst-case time per character and, for @xmath , the copies
we use to encode the @xmath th character of @xmath will always have been
last updated after we encoded the @xmath th character.

Writing @xmath for the @xmath th character of @xmath , @xmath for the
prefix of @xmath of length @xmath , and @xmath for the number of times
@xmath occurs in @xmath , we can summarize the results of this section
as the following lemma.

###### Lemma 2.4.

For @xmath , we can encode @xmath as at most

  -- -------- --
     @xmath   
  -- -------- --

bits such that encoding and decoding it take @xmath worst-case time.

### 2.2 Analysis

Analyzing the length of the encoding our algorithm produces is just a
matter of bounding the sum of the codewords’ lengths. Fortunately, we
can do this using a modification of the proof that adaptive arithmetic
coding produces an encoding not much longer than the one decrementing
arithmetic coding produces (see, e.g., [ HV92 ] ).

###### Lemma 2.5.

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Since @xmath and @xmath are the same multiset,

  -- -------- --
     @xmath   
  -- -------- --

Since

  -- -------- --
     @xmath   
  -- -------- --

is the number of distinct arrangements of the characters in @xmath , we
could complete the proof by information-theoretic arguments; however, we
will use straightforward calculation. Specifically, Robbins’ extension [
Rob55 ] of Stirling’s Formula,

  -- -------- --
     @xmath   
  -- -------- --

implies that

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the base of the natural logarithm. Therefore, since
@xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Combining Lemmas 2.4 and 2.5 and assuming @xmath immediately gives us
our result for this chapter.

###### Theorem 2.6.

We can encode @xmath as at most @xmath bits such that encoding and
decoding each character takes @xmath worst-case time.

## Chapter 3 Online Sorting with Few Comparisons

Comparison-based sorting is perhaps the most studied problem in computer
science, but there remain basic open questions about it. For example,
exactly how many comparisons does it take to sort a multiset? Over
thirty years ago, Munro and Spira [ MS76 ] proved distribution-sensitive
upper and lower bounds that differ by @xmath , where @xmath is the size
of the multiset @xmath and @xmath is the number of distinct elements
@xmath contains. Specifically, they proved that @xmath ternary
comparisons are sufficient and @xmath are necessary, where @xmath
denotes the entropy of the distribution of elements in @xmath and @xmath
denotes the multiplicity of the distinct element @xmath in @xmath .
Throughout, by @xmath we mean @xmath . Their bounds have been improved
in a series of papers, summarized in Table 3.1 , so that the difference
now is slightly less than @xmath , where @xmath is the base of the
natural logarithm.

Apart from the bounds shown in Table 3.1 , there have been many bounds
proven about, e.g., sorting multisets in-place or with minimum data
movement, or in the external-memory or cache-oblivious models. In this
chapter we consider online stable sorting; online algorithms sort @xmath
element by element and keep those already seen in sorted order, and
stable algorithms preserve the order of equal elements. For example,
splaysort (i.e., sorting by insertion into a splay tree [ ST85 ] ) is
online, stable and takes @xmath comparisons and time. In Section 3.1 we
show how, if @xmath , then we can sort @xmath online and stably using
@xmath ternary comparisons and @xmath time. In Section 3.2 we prove
@xmath comparisons are necessary in the worst case.

### 3.1 Algorithm

Our idea is to sort @xmath by inserting its elements into a binary
search tree @xmath , which we rebuild occasionally using the following
theorem by Mehlhorn [ Meh77 ] . We rebuild @xmath whenever the number of
elements processed since the last rebuild is equal to the number of
distinct elements seen by the time of the last rebuild. This way, we
spend @xmath total time rebuilding @xmath .

###### Theorem 3.1 (Mehlhorn, 1977).

Given a probability distribution @xmath on @xmath keys, with no @xmath ,
in @xmath time we can build a binary search tree containing those keys
at depths at most @xmath .

To rebuild @xmath after processing @xmath elements of @xmath , to each
distinct element @xmath seen so far we assign probability @xmath , where
@xmath denotes the first @xmath elements of @xmath ; we then apply
Theorem 3.1 . Notice the smallest probability we consider is at least
@xmath , so the resulting tree has height at most @xmath .

We want @xmath always to contain a node for each distinct element @xmath
seen so far, that stores @xmath as a key and a linked list of @xmath ’s
occurrences so far. After we use Mehlhorn’s theorem, therefore, we
extend @xmath and then replace each of its leaf by an empty AVL tree [
AL62 ] . To process an element @xmath of @xmath , we search for @xmath
in @xmath ; if we find a node @xmath whose key is equal to @xmath , then
we append @xmath to @xmath ’s linked list; otherwise, our search ends at
a node of an AVL tree, into which we insert a new node whose key is
equal to @xmath and whose linked list contains @xmath .

If an element equal to @xmath occurs by the time of the last rebuild
before we process @xmath , then the corresponding node is at depth at
most @xmath in @xmath , so the number of ternary comparisons we use to
insert @xmath into @xmath is at most that number plus 1; the extra
comparison is necessary to check that the algorithm should not proceed
deeper into the tree. Otherwise, since our AVL trees always contain at
most @xmath nodes, we use @xmath comparisons. Therefore, we use a total
of at most

  -- -------- --
     @xmath   
  -- -------- --

comparisons to sort @xmath and, assuming each comparison takes @xmath
time, a proportional amount of time. We can bound this sum using the
following technical lemma, which says the logarithm of the number of
distinct arrangements of the elements in @xmath is close to @xmath , and
the subsequent corollary. We write @xmath to denote the distinct
elements in @xmath .

###### Lemma 3.2.

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Robbins’ extension [ Rob55 ] of Stirling’s Formula,

  -- -------- --
     @xmath   
  -- -------- --

implies that

  -- -------- --
     @xmath   
  -- -------- --

Therefore, since @xmath , straightforward calculation shows that

  -- -- --
        
  -- -- --

is at least @xmath and at most @xmath . ∎

###### Corollary 3.3.

  -- -- --
        
  -- -- --

###### Proof.

Since @xmath and @xmath are the same multiset,

  -- -- --
        
  -- -- --

by Lemma 3.2 . If follows that

  -- -- --
        
  -- -- --

Our upper bound follows immediately from Corollary 3.3 .

###### Theorem 3.4.

When @xmath , we can sort @xmath online and stably using @xmath ternary
comparisons and @xmath time.

### 3.2 Lower bound

Consider any online, stable sorting algorithm that uses ternary
comparisons. Since the algorithm is online and stable, it must determine
each element’s rank relative to the distinct elements already seen,
before moving on to the next element. Since it uses ternary comparisons,
we can represent its strategy for each element as an extended binary
search tree whose keys are the distinct elements already seen. If the
current element is distinct from all those already seen, then the
algorithm reaches a leaf of the tree, and the minimum number of
comparisons it performs is equal to that leaf’s depth. If the current
element has been seen before, however, then the algorithm stops at an
internal node and the minimum number of comparisons it performs is 1
greater than that node’s depth; again, the extra comparison is necessary
to check that the algorithm should not proceed deeper into the tree.

Suppose @xmath is a power of 2; then, in any binary search tree on
@xmath keys, some key has depth @xmath (the inequality holds because any
distribution on @xmath elements has entropy at most @xmath ).
Furthermore, suppose an adversary starts by presenting one copy of each
of @xmath distinct element; after that, it considers the algorithm’s
strategy for the next element as a binary search tree, and presents the
deepest key. This way, the adversary forces the algorithm to use at
least @xmath comparisons.

###### Theorem 3.5.

We generally need @xmath ternary comparisons to sort @xmath online and
stably.

## Chapter 4 Online Sorting with Sublinear Memory

When in doubt, sort! Librarians, secretaries and computer scientists all
know that when faced with lots of data, often the best thing is to
organize them. For some applications, though, the data are so
overwhelming that we cannot sort. The streaming model was introduced for
situations in which the flow of data cannot be paused or stored in its
entirety; the model’s assumptions are that we are allowed only one pass
over the input and memory sublinear in its size (see, e.g., [ Mut05 ] ).
Those assumptions mean we cannot sort in general, but in this chapter we
show we can when the data are very compressible.

Our inspiration comes from two older articles on sorting. In the first,
“Sorting and searching in multisets” from 1976, Munro and Spira [ MS76 ]
considered the problem of sorting a multiset @xmath of size @xmath
containing @xmath distinct elements in the comparison model. They showed
sorting @xmath takes @xmath time, where @xmath is the entropy of @xmath
, @xmath means @xmath and @xmath is the frequency of the @xmath th
smallest distinct element. When @xmath is small or the distribution of
elements in @xmath is very skewed, this is a significant improvement
over the @xmath bound for sorting a set of size @xmath .

In the second article, “Selection and sorting with limited storage” from
1980, Munro and Paterson [ MP80 ] considered the problem of sorting a
set @xmath of size @xmath using limited memory and few passes. They
showed sorting @xmath in @xmath passes takes @xmath memory locations in
the following model (we have changed their variable names for
consistency with our own):

  In our computational model the data is a sequence of @xmath distinct
  elements stored on a one-way read-only tape. An element from the tape
  can be read into one of @xmath locations of random-access storage. The
  elements are from some totally ordered set (for example the real
  numbers) and a binary comparison can be made at any time between any
  two elements within the random-access storage. Initially the storage
  is empty and the tape is placed with the reading head at the
  beginning. After each pass the tape is rewound to this position with
  no reading permitted. …[I]n view of the limitations imposed by our
  model, [sorting] must be considered as the determination of the sorted
  order rather than any actual rearrangement.

An obvious question — but one that apparently has still not been
addressed decades later — is how much memory we need to sort a multiset
in few passes; in this chapter we consider the case when we are allowed
only one pass. We assume our input is the same as Munro and Spira’s, a
multiset @xmath with entropy @xmath containing @xmath distinct elements.
To simplify our presentation, we assume @xmath so @xmath . Our model is
similar to Munro and Paterson’s but it makes no difference to us whether
the tape is read-only or read-write, since we are allowed only one pass,
and whereas they counted memory locations, we count bits. We assume
machine words are @xmath bits long, an element fits in a constant number
of words and we can perform standard operations on words in unit time.
Since entropy is minimized when the distribution is maximally skewed,

  -- -------- --
     @xmath   
  -- -------- --

thus, under our assumptions, @xmath words take @xmath bits.

In Section 4.1 we consider the problem of determining the permutation
@xmath such that @xmath is the stable sort of @xmath (i.e., @xmath and,
if @xmath , then @xmath ). For example, if

  -- -------- --
     @xmath   
  -- -------- --

(with subscripts serving only to distinguish copies of the same distinct
element), then the stable sort of @xmath is

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

We give a simple algorithm that computes @xmath using one pass, @xmath
time and @xmath bits of memory. In Section 4.2 we consider the simpler
problem of determining a permutation @xmath such that @xmath is in
sorted order (not necessarily stably-sorted). We prove that in the worst
case it takes @xmath bits of memory to compute any such @xmath in one
pass.

### 4.1 Algorithm

The key to our algorithm is the fact @xmath , where @xmath is the sorted
list of positions in which the @xmath th smallest distinct element
occurs. In our example, @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Since each @xmath is a strictly increasing sequence, we can store it
compactly using Elias’ gamma code [ Eli75 ] : we write the first number
in @xmath , encoded in the gamma code; for @xmath , we write the
difference between the @xmath st and @xmath th numbers, encoded in the
gamma code. The gamma code is a prefix-free code for the positive
integers; for @xmath , @xmath consists of @xmath zeroes followed by the
@xmath -bit binary representation of @xmath . In our example, we encode
@xmath as

  -- -------- --
     @xmath   
  -- -------- --

###### Lemma 4.1.

We can store @xmath in @xmath bits of memory.

###### Proof.

Encoding the length of every list with the gamma code takes @xmath bits.
Notice the numbers in each list @xmath sum to at most @xmath . By
Jensen’s Inequality, since @xmath and @xmath is concave, we store @xmath
in at most @xmath bits. Therefore, storing @xmath as described above
takes

  -- -------- --
     @xmath   
  -- -------- --

bits of memory.

To reduce @xmath to @xmath — important when one distinct element
dominates, so @xmath is close to 0 and @xmath — we must avoid writing a
codeword for each element in @xmath . Notice that, for each run of
length at least 2 in @xmath (a run being a maximal subsequence of copies
of the same element) there is a run of 1’s in the corresponding list
@xmath . We replace each run of 1’s in @xmath by a single 1 and the
length of the run. For each except the last run of each distinct
element, the run-length is at most the number we write for the element
in @xmath immediately following that run; storing the last run-length
for every character takes @xmath bits. It follows that storing @xmath
takes

  -- -------- --
     @xmath   
  -- -------- --

bits, where @xmath is the number of runs of the @xmath th smallest
distinct element and @xmath is the total number of runs in @xmath .
Mäkinen and Navarro [ MN05 ] showed @xmath , so our bound is @xmath . ∎

To compute @xmath in one pass, we keep track of which distinct elements
have occurred and the positions of their most recent occurrences, which
takes @xmath words of memory. For @xmath , if @xmath is an occurrence of
the @xmath th smallest distinct element and that element has not
occurred before, then we start @xmath ’s encoding with @xmath ; if it
last occurred in position @xmath , then we append @xmath to @xmath ; if
it occurred in position @xmath but not @xmath , then we append @xmath to
@xmath ; if it occurred in both positions @xmath and @xmath , then we
increment the encoded run-length at the end of @xmath ’s encoding.
Because we do not know in advance how many bits we will use to encode
each @xmath , we keep the encoding in an expandable binary array [
CLRS01 ] : we start with an array of size 1 bit; whenever the array
overflows, we create a new array twice as big, copy the contents from
the old array into the new one, and destroy the old array. We note that
appending a bit to the encoding takes amortized constant time.

###### Lemma 4.2.

We can compute @xmath in one pass using @xmath bits of memory.

###### Proof.

Since we are not yet concerned with time, for each element in @xmath we
can simply perform a linear search — which is slow but uses no extra
memory — through the entire list of distinct elements to find the
encoding we should extend. Since an array is never more than twice the
size of the encoding it holds, we use @xmath bits of memory for the
arrays. ∎

To make our algorithm time-efficient, we use search in a splay tree [
ST85 ] instead of linear search. At each node of the splay tree, we
store a distinct element as the key, the position of that element’s most
recent occurrence and a pointer to the array for that element. For
@xmath , we search for @xmath in the splay tree; if we find it, then we
extend the encoding of the corresponding list as described above, set
the position of @xmath ’s most recent occurrence to @xmath and splay
@xmath ’s node to the root; if not, then we insert a new node storing
@xmath as its key, position @xmath and a pointer to an expandable array
storing @xmath , and splay the node to the root. Figure 4.3 shows the
state of our splay tree and arrays after we process the first @xmath
elements in our example; i.e., @xmath . Figure 4.3 shows the changes
when we process the next element, an @xmath : we double the size of
@xmath ’s array from @xmath to @xmath bits in order to append @xmath ,
set the position of @xmath ’s most recent occurrence to @xmath and splay
@xmath ’s node to the root. Figure 4.3 shows the final state of our
splay tree and arrays after we process the last element, an @xmath : we
append @xmath to @xmath ’s array (but since only @xmath of its @xmath
bits were already used, we do not expand it), set the position of @xmath
’s most recent occurrence to @xmath and splay @xmath ’s node to the
root.

###### Lemma 4.3.

We can compute @xmath in one pass using @xmath time and @xmath bits of
memory.

###### Proof.

Our splay tree takes @xmath words of memory and, so, does not change the
bound on our memory usage. For @xmath we search for the @xmath th
largest distinct element once when it is not in the splay tree, insert
it once, and search for it @xmath times when it is in the splay tree.
Therefore, by the Update Lemma [ ST85 ] for splay trees, the total time
taken for all the operations on the splay tree is

  -- -------- --
     @xmath   
  -- -------- --

where @xmath are any positive weights, @xmath is their sum and @xmath .
Setting @xmath for @xmath , this bound becomes

  -- -------- --
     @xmath   
  -- -------- --

Because appending a bit to an array takes amortized constant time, the
total time taken for operations on the arrays is proportional to the
total length in bits of the encodings, i.e., @xmath . ∎

After we process all of @xmath , we can compute @xmath from the state of
the splay tree and arrays: we perform an in-order traversal of the splay
tree; when we visit a node, we decode the numbers in its array and
output their positive partial sums (this takes @xmath words of memory
and time proportional to the length in bits of the encoding, because the
gamma code is prefix-free); this way, we output the concatenation of the
decoded lists in increasing order by element, i.e., @xmath . In our
example, we visit the nodes in the order @xmath ; when we visit @xmath
’s node we output

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Our results in this section culminate in the following theorem:

###### Theorem 4.4.

We can compute @xmath in one pass using @xmath time and @xmath bits of
memory.

We note @xmath can be recovered efficiently from our splay tree and
arrays: we start with an empty priority queue @xmath and insert a copy
of each distinct element, with priority equal to the position of its
first occurrence (i.e., the first encoded number in its array); for
@xmath , we dequeue the element with minimum priority, output it, and
reinsert it with priority equal to the position of its next occurrence
(i.e., its previous priority plus the next encoded number in its array).
This idea — that a sorted ordering of @xmath partially encodes it — is
central to our lower bound in the next section.

### 4.2 Lower bound

Consider any algorithm @xmath that, allowed one pass over @xmath ,
outputs a permutation @xmath such that @xmath is in sorted order (not
necessarily stably-sorted). Notice @xmath generally cannot output
anything until it has read all of @xmath , in case @xmath is the unique
minimum; also, given the frequency of each distinct element, @xmath
tells us the arrangement of elements in @xmath up to equivalence.

###### Theorem 4.5.

In the worst case, it takes @xmath bits of memory to compute any sorted
ordering of @xmath in one pass.

###### Proof.

Suppose each @xmath , so @xmath and the number of possible distinct
arrangements of the elements in @xmath is maximized,

  -- -------- --
     @xmath   
  -- -------- --

It follows that in the worst case @xmath uses at least

  -- -------- --
     @xmath   
  -- -------- --

bits of memory to store @xmath ; the inequality holds by Stirling’s
Formula,

  -- -------- --
     @xmath   
  -- -------- --

If @xmath then

  -- -------- --
     @xmath   
  -- -------- --

otherwise, since @xmath is maximized when @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

in both cases,

  -- -------- --
     @xmath   
  -- -------- --

## Chapter 5 One-Pass Compression

Data compression has come of age in recent years and compression
algorithms are now vital in situations unforeseen by their designers.
This has led to a discrepancy between the theory of data compression
algorithms and their use in practice: compression algorithms are often
designed and analysed assuming the compression and decompression
operations can use a “sufficiently large” amount of working memory;
however, in some situations, particularly in mobile or embedded
computing environments, the memory available is very small compared to
the amount of data we need to compress or decompress. Even when
compression algorithms are implemented to run on powerful desktop
computers, some care is taken to be sure that the
compression/decompression of large files do not take over all the RAM of
the host machine. This is usually accomplished by splitting the input in
blocks (e.g., bzip ), using heuristics to determine when to discard the
old data (e.g., compress , ppmd ), or by maintaining a “sliding window”
over the more recently seen data and forgetting the oldest data (e.g.,
gzip ).

In this chapter we initiate the theoretical study of space-conscious
compression algorithms. Although data compression algorithms have their
own peculiarities, this study belongs to the general field of
algorithmics in the streaming model (see, e.g., [ BBD @xmath 02 , Mut05
] ), in which we are allowed only one pass over the input and memory
sublinear (possibly polylogarithmic or even constant) in its size. We
prove tight upper and lower bounds on the compression ratio achievable
by one-pass algorithms that use an amount of memory independent of the
size of the input. By “one-pass”, we mean that the algorithms are
allowed to read each input symbol only once; hence, if an algorithm
needs to access (portions of) the input more than once it must store
it—consuming part of its precious working memory. Our bounds are
worst-case and given in terms of the empirical @xmath th-order empirical
entropy of the input string. More precisely we prove the following
results:

1.  Let @xmath , @xmath and @xmath be constants and let @xmath be a
    function independent of @xmath . In the worst case it is impossible
    to store a string @xmath of length @xmath over an alphabet of size
    @xmath in @xmath bits using one pass and @xmath bits of memory.

2.  Given a @xmath -bit encoding of @xmath , it is impossible to recover
    @xmath using one pass and @xmath bits of memory.

3.  Given @xmath , @xmath and @xmath , we can store @xmath in @xmath
    bits using one pass and @xmath bits of memory, and later recover
    @xmath using one pass and the same amount of memory.

While @xmath is often treated as constant in the literature, we treat it
as a variable to distinguish between, say, @xmath and @xmath bits.
Informally, (a) provides a lower bound to the amount of memory needed to
compress a string up to its @xmath th-order entropy; (b) tells us the
same amount of memory is required also for decompression and implies
that the use of a powerful machine for doing the compression does not
help if only limited memory is available when decompression takes place;
(c) establishes that (a) and (b) are nearly tight. Notice @xmath plays a
dual role: for large @xmath , it makes (a) and (b) inapproximability
results — e.g., we cannot use @xmath bits of memory without worsening
the compression in terms of @xmath by more than a constant factor; for
small @xmath , it makes (c) an interesting approximability result —
e.g., we can compress reasonably well in terms of @xmath using, say,
@xmath bits of memory. The main difference between the bounds in (a)–(b)
and (c) is a @xmath factor in the memory usage. Since @xmath is a
constant, @xmath and the bounds on the encoding’s length match. Note
that @xmath can be arbitrarily small, but the term @xmath cannot be
avoided (Lemma 5.5 ).

We use @xmath to denote the string that we want to compress. We assume
that @xmath has length @xmath and is drawn from an alphabet of size
@xmath . Note that we measure memory in terms of alphabet size so @xmath
is considered a variable. The @xmath th-order empirical entropy @xmath
of @xmath is defined as @xmath , where @xmath is the number of times
character @xmath occurs in @xmath ; throughout, we write @xmath to mean
@xmath and assume @xmath . It is well known that @xmath is the maximum
compression we can achieve using a fixed codeword for each alphabet
symbol. We can achieve a greater compression if the codeword we use for
each symbol depends on the @xmath symbols preceding it. In this case the
maximum compression is bounded by the @xmath th-order entropy @xmath
(see [ KM99 ] for the formal definition). We use two properties of
@xmath th-order entropy in particular:

-   @xmath ,

-   since @xmath , we have

      -- -------- --
         @xmath   
      -- -------- --

We point out that the empirical entropy is defined pointwise for any
string and can be used to measure the performance of compression
algorithms as a function of the string’s structure, thus without any
assumption on the input source. For this reason we say that the bounds
given in terms of @xmath are worst-case bounds.

Some of our arguments are based on Kolmogorov complexity [ LV08 ] ; the
Kolmogorov complexity of @xmath , denoted @xmath , is the length in bits
of the shortest program that outputs @xmath ; it is generally
incomputable but can be bounded from below by counting arguments (e.g.,
in a set of @xmath elements, most have Kolmogorov complexity at least
@xmath ). We use two properties of Kolmogorov complexity in particular:
if an object can be easily computed from other objects, then its
Kolmogorov complexity is at most the sum of theirs plus a constant; and
a fixed, finite object has constant Kolmogorov complexity.

### 5.1 Algorithm

Move-to-front compression [ BSTW86 ] is probably the best example of a
compression algorithm whose space complexity is independent of the input
length: keep a list of the characters that have occurred in decreasing
order by recency; store each character in the input by outputting its
position in the list (or, if it has not occurred before, its index in
the alphabet) encoded in Elias’ @xmath code, then move it to the front
of the list. Move-to-front stores a string @xmath of length @xmath over
an alphabet of size @xmath in @xmath bits using one pass and @xmath bits
of memory. When memory is scarce, we can use @xmath bits of memory by
storing only the @xmath most recent characters; it is easy to see that
this increases the number of bits stored by a factor of at most @xmath .
On the other hand, note that we can store @xmath in @xmath bits by
keeping a separate list for each possible context of length @xmath ;
this increases the memory usage by a factor of at most @xmath . In this
section we first use a more complicated algorithm to get a better upper
bound: given constants @xmath , @xmath and @xmath , we can store @xmath
in @xmath bits using one pass and @xmath bits of memory.

We start with the following lemma — based on a previous paper [ Gag06b ]
about compression algorithms’ redundancies — that says we can an
approximation @xmath of a probability distribution @xmath in few bits,
so that the relative entropy between @xmath and @xmath is small. The
relative entropy @xmath between @xmath and @xmath is the expected
redundancy per character of an ideal code for @xmath when characters are
drawn according to @xmath .

###### Lemma 5.1.

Let @xmath be a string of length @xmath over an alphabet of size @xmath
and let @xmath be the normalized distribution of characters in @xmath .
Given @xmath and constants @xmath and @xmath , we can store a
probability distribution @xmath with @xmath in @xmath bits using @xmath
bits of memory.

###### Proof.

Suppose @xmath . We can use an @xmath -time algorithm due to Misra and
Gries [ MG82 ] (see also [ DLM02 , KSP03 ] ) to find the @xmath values
of @xmath such that @xmath , where @xmath , using @xmath bits of memory;
or, since we are not concerned with time in this chapter, we can simply
make @xmath passes over @xmath to find these @xmath values. For each, we
store @xmath and @xmath ; since @xmath depends only on @xmath , in total
this takes @xmath bits. This information lets us later recover @xmath
where

  -- -------- --
     @xmath   
  -- -------- --

Suppose @xmath ; then @xmath . Since @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Now suppose @xmath ; then @xmath . Therefore

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath in both cases, @xmath . ∎

Armed with this lemma, we can adapt arithmetic coding to use @xmath bits
of memory with a specified redundancy per character.

###### Lemma 5.2.

Given a string @xmath of length @xmath over an alphabet of size @xmath
and constants @xmath and @xmath , we can store @xmath in @xmath bits
using @xmath bits of memory.

###### Proof.

Let @xmath be the normalized distribution of characters in @xmath , so
@xmath . First, as described in Lemma 5.1 , we store a probability
distribution @xmath with @xmath in @xmath bits using @xmath bits of
memory. Then, we process @xmath in blocks @xmath of length @xmath
(except @xmath may be shorter). For @xmath , we store @xmath as the
first @xmath bits to the right of the binary point in the binary
representation of

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a string of length @xmath chosen randomly according to
@xmath , @xmath means @xmath is lexicographically less than @xmath , and
@xmath and @xmath indicate the indices in the alphabet of the @xmath th
characters of @xmath and @xmath , respectively. Notice that, since
@xmath for any string @xmath of length @xmath , these bits uniquely
identify @xmath and, thus, @xmath . Also, since the probabilities in
@xmath are @xmath -bit numbers, we can compute @xmath from @xmath with
@xmath additions and @xmath multiplications using @xmath bits of memory.
(In fact, with appropriate data structures, @xmath additions and @xmath
multiplications suffice.) Finally, we store @xmath in @xmath bits. In
total we store @xmath in

  -- -- --
        
  -- -- --

bits using @xmath bits of memory. ∎

We boost our space-conscious arithmetic coding algorithm to achieve a
bound in terms of @xmath instead of @xmath by running a separate copy
for each possible @xmath -tuple, just as we boosted move-to-front
compression.

###### Lemma 5.3.

Given a string @xmath of length @xmath over an alphabet of size @xmath
and constants @xmath , @xmath and @xmath , we can store @xmath in @xmath
bits using @xmath bits of memory.

###### Proof.

We store the first @xmath characters of @xmath in @xmath bits then apply
Lemma 5.2 to subsequences @xmath , where @xmath consists of the
characters in @xmath that immediately follow occurrences of the
lexicographically @xmath th possible @xmath -tuple. Notice that although
we cannot keep @xmath in memory, enumerating them as many times as
necessary in order to apply Lemma 5.2 takes @xmath bits of memory. ∎

To make our algorithm use one pass and to change the @xmath factor to
@xmath , we process the input in blocks @xmath of length @xmath . Notice
each individual block @xmath fits in memory — so we can apply Lemma 5.3
to it — and @xmath .

###### Theorem 5.4.

Given a string @xmath of length @xmath over an alphabet of size @xmath
and constants @xmath , @xmath and @xmath , we can store @xmath in @xmath
bits using one pass and @xmath bits of memory, and later recover @xmath
using one pass and the same amount of memory.

###### Proof.

Let @xmath be a constant such that, by Lemma 5.3 , we can store any
substring @xmath of @xmath in @xmath bits using @xmath bits of memory.
We process @xmath in blocks @xmath of length @xmath (except @xmath may
be shorter). Notice each block @xmath fits in @xmath bits of memory.
When we reach @xmath , we read it into memory, apply Lemma 5.3 to it —
using

  -- -- --
        
  -- -- --

bits of memory — then erase it from memory. In total we store @xmath in

  -- -------- --
     @xmath   
  -- -------- --

bits using @xmath bits of memory.

Notice the encoding of each block @xmath also fits in @xmath bits of
memory. To decode each block later, we read its encoding into memory,
search through all possible strings of length @xmath in lexicographic
order until we find the one that yields that encoding — using @xmath
bits of memory — and output it. ∎

The method for decompression in the proof of Theorem 5.4 above takes
exponential time but is very simple (recall we are not concerned with
time here); reversing each step of the compression takes linear time but
is slightly more complicated.

### 5.2 Lower bounds

Theorem 5.4 is still weaker than the strongest compression bounds that
ignore memory constraints, in two important ways: first, even when
@xmath the bound on the compression ratio does not approach @xmath as
@xmath goes to infinity; second, we need to know @xmath . It is not hard
to prove these weaknesses are unavoidable when using fixed memory. In
this section, we use the idea from these proofs to prove a nearly
matching lower bound for compression: in the worst case it is impossible
to store a string @xmath of length @xmath over an alphabet of size
@xmath in @xmath bits, for any function @xmath independent of @xmath ,
using one encoding pass and @xmath bits of memory. We close with a
symmetric lower bound for decompression.

###### Lemma 5.5.

Let @xmath be a constant and let @xmath be a function independent of
@xmath . In the worst case it is impossible to store a string @xmath of
length @xmath in @xmath bits using one encoding pass and memory
independent of @xmath .

###### Proof.

Let @xmath be an algorithm that, given @xmath , stores @xmath using one
pass and memory independent of @xmath . Since @xmath ’s future output
depends only on its state and its future input, we can model @xmath with
a finite-state machine @xmath . While reading @xmath characters of
@xmath , @xmath must visit some state at least twice; therefore either
@xmath outputs at least one bit for every @xmath characters in @xmath —
or @xmath bits in total — or for infinitely many strings @xmath outputs
nothing. If @xmath is unary, however, then @xmath . ∎

###### Lemma 5.6.

Let @xmath be a constant, let @xmath be a function independent of @xmath
and let @xmath be a function independent of @xmath and @xmath . In the
worst case it is impossible to store a string @xmath of length @xmath
over an alphabet of size @xmath in @xmath bits for all @xmath using one
pass and @xmath bits of memory.

###### Proof.

Let @xmath be an algorithm that, given @xmath , @xmath , @xmath and
@xmath , stores @xmath using @xmath bits of memory. Again, we can model
it with a finite-state machine @xmath , with @xmath and @xmath ’s
Kolmogorov complexity @xmath . (Since @xmath , @xmath , @xmath , and
@xmath are all fixed, their Kolmogorov complexities are @xmath .)

Suppose @xmath is a periodic string with period @xmath whose repeated
substring @xmath has @xmath . We can specify @xmath by specifying @xmath
, the states @xmath is in when it reaches and leaves any copy of @xmath
in @xmath , and @xmath ’s output on that copy of @xmath . (If there were
another string @xmath that took @xmath between those states with that
output, then we could substitute @xmath for @xmath in @xmath without
changing @xmath ’s output.) Therefore @xmath outputs at least

  -- -------- --
     @xmath   
  -- -------- --

bits for each copy of @xmath in @xmath , or @xmath bits in total. For
@xmath , however, @xmath approaches 0 as @xmath goes to infinity. ∎

The idea behind these proofs is simple: model a one-pass algorithm with
a finite-state machine and evaluate its behaviour on a periodic string.
Nevertheless, combining it with the following simple results — based on
the same previous paper [ Gag06b ] as Lemma 5.1 — we can easily show a
lower bound that nearly matches Theorem 5.4 . (In fact, our proofs are
valid even for algorithms that make preliminary passes that produce no
output — perhaps to gather statistics, like Huffman coding [ Huf52 ] —
followed by a single encoding pass that produces all of the output; once
the algorithm begins the encoding pass, we can model it with a
finite-state machine.)

###### Lemma 5.7.

Let @xmath , @xmath and @xmath be constants and let @xmath be a randomly
chosen string of length @xmath over an alphabet of size @xmath . With
high probability every possible @xmath -tuple is followed by @xmath
distinct characters in @xmath .

###### Proof.

Consider a @xmath -tuple @xmath . For @xmath , let @xmath if the @xmath
th through @xmath st characters of @xmath are an occurrence of @xmath
and the @xmath th character in @xmath does not occur in @xmath ;
otherwise @xmath . Notice @xmath is followed by at most @xmath distinct
characters in @xmath and @xmath and @xmath for @xmath . Therefore, by
Chernoff bounds (see [ HR90 ] ) and the union bound, with probability
greater than

  -- -------- --
     @xmath   
  -- -------- --

every @xmath -tuple is followed by fewer than @xmath distinct
characters. ∎

###### Corollary 5.8.

Let @xmath , @xmath and @xmath be constants. There exists a string
@xmath of length @xmath over an alphabet of size @xmath with @xmath but
@xmath for @xmath .

###### Proof.

If @xmath is randomly chosen, then @xmath with probability greater than
@xmath and, by Lemma 5.7 , with high probability every possible @xmath
-tuple is followed by @xmath distinct characters in @xmath ; therefore
there exists an @xmath with both properties. Every possible @xmath
-tuple is followed by at most @xmath more distinct characters in @xmath
than in @xmath and, thus,

  -- -- --
        
  -- -- --

Consider what we get if, for some @xmath , we allow the algorithm @xmath
from Lemma 5.6 to use @xmath bits of memory, and evaluate it on the
periodic string @xmath from Corollary 5.8 . Since @xmath has period
@xmath and its repeated substring @xmath has @xmath , the finite-state
machine @xmath outputs at least

  -- -------- --
     @xmath   
  -- -------- --

bits for each copy of @xmath in @xmath , or @xmath bits in total.
Because @xmath , this yields the following nearly tight lower bound;
notice it matches Theorem 5.4 except for a @xmath factor in the memory
usage.

###### Theorem 5.9.

Let @xmath , @xmath and @xmath be constants and let @xmath be a function
independent of @xmath . In the worst case it is impossible to store a
string @xmath of length @xmath over an alphabet of size @xmath in @xmath
bits using one encoding pass and @xmath bits of memory.

###### Proof.

Let @xmath be an algorithm that, given @xmath , @xmath , @xmath and
@xmath , stores @xmath while using one encoding pass and @xmath bits of
memory; we prove that in the worst case @xmath stores @xmath in more
than @xmath bits. Again, we can model it with a finite-state machine
@xmath , with @xmath and @xmath . Let @xmath be a string of length
@xmath with @xmath and @xmath for @xmath , as described in Corollary 5.8
, and suppose @xmath for some @xmath . We can specify @xmath by
specifying @xmath , the states @xmath is in when it reaches and leaves
any copy of @xmath in @xmath , and @xmath ’s output on that copy.
Therefore @xmath outputs at least

  -- -------- --
     @xmath   
  -- -------- --

bits for each copy of @xmath in @xmath , or @xmath bits in total — which
is asymptotically greater than @xmath . ∎

With a good bound on how much memory is needed for compression, we turn
our attention to decompression. Good bounds here are equally important,
because often data is compressed once by a powerful machine (e.g., a
server or base-station) and then transmitted to many weaker machines
(clients or agents) who decompress it individually. Fortunately for us,
compression and decompression are essentially symmetric. Recall Theorem
5.4 says we can recover @xmath from a @xmath -bit encoding using one
pass and @xmath bits of memory. Using the same idea about finite-state
machines and periodic strings gives us the following nearly matching
lower bound:

###### Theorem 5.10.

Let @xmath , @xmath and @xmath be constants and let @xmath be a function
independent of @xmath . There exists a string @xmath of length @xmath
over an alphabet of size @xmath such that, given a @xmath -bit encoding
of @xmath , it is impossible to recover @xmath using one pass and @xmath
bits of memory.

###### Proof.

Let @xmath be a string of length @xmath with @xmath but @xmath for
@xmath , as described in Corollary 5.8 , and suppose @xmath for some
@xmath . Let @xmath be an algorithm that, given @xmath , @xmath , @xmath
, @xmath and a @xmath -bit encoding of @xmath , recovers @xmath using
one pass; we prove @xmath uses @xmath bits of memory. Again, we can
model @xmath with a finite-state machine @xmath , with @xmath equal to
the number of bits of memory @xmath uses and @xmath . We can specify
@xmath by specifying @xmath , the state @xmath is in when it starts
outputting any copy of @xmath in @xmath , and the bits of the encoding
it reads while outputting that copy of @xmath ; therefore

  -- -- --
        
  -- -- --

so

  -- -------- --
     @xmath   
  -- -------- --

The theorem follows because @xmath can be arbitrarily large compared to
@xmath . ∎

## Chapter 6 Stream Compression

Massive datasets seem to expand to fill the space available and, in
situations when they no longer fit in memory and must be stored on disk,
we may need new models and algorithms. Grohe and Schweikardt [ GS05 ]
introduced read/write streams to model situations in which we want to
process data using mainly sequential accesses to one or more disks. As
the name suggests, this model is like the streaming model (see, e.g., [
Mut05 ] ) but, as is reasonable with datasets stored on disk, it allows
us to make multiple passes over the data, change them and even use
multiple streams (i.e., disks). As Grohe and Schweikardt pointed out,
sequential disk accesses are much faster than random accesses —
potentially bypassing the von Neumann bottleneck — and using several
disks in parallel can greatly reduce the amount of memory and the number
of accesses needed. For example, when sorting, we need the product of
the memory and accesses to be at least linear when we use one disk [
MP80 , GKS07 ] but only polylogarithmic when we use two [ CY91 , GS05 ]
. Similar bounds have been proven for a number of other problems, such
as checking set disjointness or equality; we refer readers to
Schweikardt’s survey [ Sch07 ] of upper and lower bounds with one or
more read/write streams, Heinrich and Schweikardt’s recent paper [ HS08
] relating read/write streams to classical complexity theory, and Beame
and Huỳnh-Ngọc’s recent paper [ BH08 ] on the value of multiple
read/write streams for approximating frequency moments.

Since sorting is an important operation in some of the most powerful
data compression algorithms, and compression is an important operation
for reducing massive datasets to a more manageable size, we wondered
whether extra streams could also help us achieve better compression. In
this chapter we consider the problem of compressing a string @xmath of
@xmath characters over an alphabet of size @xmath when we are restricted
to using @xmath bits of memory and @xmath passes over the data. In
Section 6.1 , we show how we can achieve universal compression using
only one pass over one stream. Our approach is to break the string into
blocks and compress each block separately, similar to what is done in
practice to compress large files. Although this may not usually
significantly worsen the compression itself, it may stop us from then
building a fast compressed index [ FMMN07 ] (unless we somehow combine
the indexes for the blocks) or clustering by compression [ CV05 , FGG
@xmath 07 ] (since concatenating files should not help us compress them
better if we then break them into pieces again). In Section 6.2 we use a
vaguely automata-theoretic argument to show one stream is not sufficient
for us to achieve good grammar-based compression. Of course, by “good”
we mean here something stronger than universal compression: we want the
size of our encoding to be at most polynomial in the size of the
smallest context-free grammar than generates @xmath and only @xmath . We
still do not know whether any constant number of streams is sufficient
for us to achieve such compression. Finally, in Section 6.3 we show that
two streams are necessary and sufficient for us to achieve entropy-only
bounds. Along the way, we show we need two streams to find strings’
minimum periods or compute the Burrows-Wheeler Transform. As far as we
know, this is the first study of compression with read/write streams,
and among the first studies of compression in any streaming model; we
hope the techniques we use will prove to be of independent interest.

### 6.1 Universal compression

An algorithm is called universal with respect to a class of sources if,
when a string is drawn from any of those sources, the algorithm’s
redundancy per character approaches 0 with probability 1 as the length
of the string grows. The class most often considered, and which we
consider in this section, is that of stationary, ergodic Markov sources
(see, e.g., [ CT06 ] ). Since the @xmath th-order empirical entropy
@xmath of @xmath is the minimum self-information per character of @xmath
with respect to a @xmath th-order Markov source (see [ Sav97 ] ), an
algorithm is universal if it stores any string @xmath in @xmath bits for
any fixed @xmath and @xmath . The @xmath th-order empirical entropy of
@xmath is also our expected uncertainty about a randomly chosen
character of @xmath when given the @xmath preceding characters.
Specifically,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the number of times character @xmath occurs in @xmath ,
and @xmath is the concatenation of those characters immediately
following occurrences of @xmath -tuple @xmath in @xmath .

In a previous paper [ GM07b ] we showed how to modify the well-known
LZ77 compression algorithm [ ZL77 ] to use sublinear memory while still
storing @xmath in @xmath bits for any fixed @xmath and @xmath . Our
algorithm uses nearly linear memory and so does not fit into the model
we consider in this chapter, but we mention it here because it fits into
some other streaming models (see, e.g., [ Mut05 ] ) and, as far as we
know, was the first compression algorithm to do so. In the same paper we
proved several lower bounds using ideas that eventually led to our lower
bounds in Sections 6.2 and 6.3 of this chapter.

###### Theorem 6.1 (Gagie and Manzini, 2007).

We can achieve universal compression using one pass over one stream and
@xmath bits of memory.

To achieve universal compression with only polylogarithmic memory, we
use a recent algorithm due to Gupta, Grossi and Vitter [ GGV08 ] .
Although they designed it for the RAM model, we can easily turn it into
a streaming algorithm by processing @xmath in small blocks and
compressing each block separately.

###### Theorem 6.2 (Gupta, Grossi and Vitter, 2008).

In the RAM model, we can store any string @xmath in @xmath bits, for all
@xmath simultaneously, using @xmath time.

###### Corollary 6.3.

We can achieve universal compression using one pass over one stream and
@xmath bits of memory.

###### Proof.

We process @xmath in blocks of @xmath characters, as follows: we read
each block into memory, apply Theorem 6.2 to it, output the result,
empty the memory, and move on to the next block. (If @xmath is not given
in advance, we increase the block size as we read more characters.)
Since Gupta, Grossi and Vitter’s algorithm uses @xmath time in the RAM
model, it uses @xmath bits of memory and we use @xmath bits of memory.
If the blocks are @xmath , then we store all of them in a total of

  -- -------- --
     @xmath   
  -- -------- --

bits for all @xmath simultaneously. Therefore, for any fixed @xmath and
@xmath , we store @xmath in @xmath bits. ∎

A bound of @xmath bits is not very meaningful when @xmath is not fixed
and grows as fast as @xmath , because the second term is @xmath .
Notice, however, that Gupta et al. ’s bound of @xmath bits is also not
very meaningful when @xmath , for the same reason. As we will see in
Section 6.3 , it is possible for @xmath to be fairly incompressible but
still to have @xmath for @xmath . It follows that, although we can prove
bounds that hold for all @xmath simultaneously, those bounds cannot
guarantee good compression in terms of @xmath when @xmath .

By using larger blocks — and, thus, more memory — we can reduce the
@xmath redundancy term in our analysis, allowing @xmath to grow faster
than @xmath while still having a meaningful bound. We conjecture that
the resulting tradeoff is nearly optimal. Specifically, using an
argument similar to those we use to prove the lower bounds in Sections
6.2 and 6.3 , we believe we can prove that the product of the memory,
passes and redundancy must be nearly linear in @xmath . It is not clear
to us, however, whether we can modify Corollary 6.3 to take advantage of
multiple passes.

###### Open Problem 6.4.

With multiple passes over one stream, can we achieve better bounds on
the memory and redundancy than we can with one pass?

### 6.2 Grammar-based compression

Charikar et al. [ CLL @xmath 05 ] and Rytter [ Ryt03 ] independently
showed how to build a context-free grammar APPROX that generates @xmath
and only @xmath and is an @xmath factor larger than the smallest such
grammar OPT , which is @xmath bits in size.

###### Theorem 6.5 (Charikar et al., 2005; Rytter, 2003).

In the RAM model, we can approximate the smallest grammar with @xmath
using @xmath time.

In this section we prove that, if we use only one stream, then in
general our approximation must be superpolynomially larger than the
smallest grammar. Our idea is to show that periodic strings whose
periods are asymptotically slightly larger than the product of the
memory and passes, can be encoded as small grammars but, in general,
cannot be compressed well by algorithms that use only one stream. Our
argument is based on the following two lemmas.

###### Lemma 6.6.

If @xmath has period @xmath , then the size of the smallest grammar for
that string is @xmath bits.

###### Proof.

Let @xmath be the repeated substring and @xmath be the proper prefix of
@xmath such that @xmath . We can encode a unary string @xmath as a
grammar @xmath with @xmath productions of total size @xmath bits. We can
also encode @xmath and @xmath as grammars @xmath and @xmath with @xmath
productions of total size @xmath bits. Suppose @xmath , @xmath and
@xmath are the start symbols of @xmath , @xmath and @xmath ,
respectively. By combining those grammars and adding the productions
@xmath and @xmath , we obtain a grammar with @xmath productions of total
size @xmath bits that maps @xmath to @xmath . ∎

###### Lemma 6.7.

Consider a lossless compression algorithm that uses only one stream, and
a machine performing that algorithm. We can compute any substring from

-    its length;

-    for each pass, the machine’s memory configurations when it reaches
    and leaves the part of the stream that initially holds that
    substring;

-    all the output the machine produces while over that part.

###### Proof.

Let @xmath be the substring and assume, for the sake of a contradiction,
that there exists another substring @xmath with the same length that
takes the machine between the same configurations while producing the
same output. Then we can substitute @xmath for @xmath in @xmath without
changing the machine’s complete output, contrary to our specification
that the compression be lossless. ∎

Lemma 6.7 implies that, for any substring, the size of the output the
machine produces while over the part of the stream that initially holds
that substring, plus twice the product of the memory and passes (i.e.,
the number of bits needed to store the memory configurations), must be
at least that substring’s complexity. Therefore, if a substring is not
compressible by more than a constant factor (as is the case for most
strings) and asymptotically larger than the product of the memory and
passes, then the size of the output for that substring must be at least
proportional to the substring’s length. In other words, the algorithm
cannot take full advantage of similarities between substrings to achieve
better compression. In particular, if @xmath is periodic with a period
that is asymptotically slightly larger than the product of the memory
and passes, and @xmath ’s repeated substring is not compressible by more
than a constant factor, then the algorithm’s complete output must be
@xmath bits. By Lemma 6.6 , however, the size of the smallest grammar
that generates @xmath and only @xmath is bounded in terms of the period.

###### Theorem 6.8.

With one stream, we cannot approximate the smallest grammar with @xmath
.

###### Proof.

Suppose an algorithm uses only one stream, @xmath bits of memory and
@xmath passes to compress @xmath , with @xmath , and consider a machine
performing that algorithm. Furthermore, suppose @xmath is periodic with
period @xmath and its repeated substring @xmath is not compressible by
more than a constant factor. Lemma 6.7 implies that the machine’s output
while over a part of the stream that initially holds a copy of @xmath ,
must be @xmath . Therefore, the machine’s complete output must be @xmath
bits. By Lemma 6.6 , however, the size of the smallest grammar that
generates @xmath and only @xmath is @xmath bits. Since @xmath , the
algorithm’s complete output is superpolynomially larger than the
smallest grammar. ∎

As an aside, we note that a symmetric argument shows that, with only one
stream, in general we cannot decode a string encoded as a small grammar.
To prove this, instead of considering a part of the stream that
initially holds a copy of the repeated substring @xmath , we consider a
part that is initially blank and eventually holds a copy of @xmath . We
can compute @xmath from the machine’s memory configurations when it
reaches and leaves that part, so the product of the memory and passes
must be greater than or equal to @xmath ’s complexity. Also, we note
that Theorem 6.8 has the following corollary, which may be of
independent interest.

###### Corollary 6.9.

With one stream, we cannot find strings’ minimum periods.

###### Proof.

Consider the proof of Theorem 6.8 . If we could find @xmath ’s minimum
period, then we could store @xmath in @xmath bits by writing @xmath and
one copy of its repeated substring @xmath . ∎

We are currently working on a more detailed argument to show that we
cannot even check whether a string has a given period. Unfortunately, as
we noted earlier, our results for this section are still incomplete, as
we do not know whether multiple streams are helpful for grammar-based
compression.

###### Open Problem 6.10.

With @xmath streams, can we approximate the smallest grammar well?

### 6.3 Entropy-only bounds

Kosaraju and Manzini [ KM99 ] pointed out that proving an algorithm
universal does not necessarily tell us much about how it behaves on
low-entropy strings. In other words, showing that an algorithm encodes
@xmath in @xmath bits is not very informative when @xmath . For example,
although the well-known LZ78 compression algorithm [ ZL78 ] is
universal, @xmath while @xmath . To analyze how algorithms perform on
low-entropy strings, we would like to get rid of the @xmath term and
prove bounds that depend only on @xmath . Unfortunately, this is
impossible since, as the example above shows, even @xmath can be 0 for
arbitrarily long strings.

It is not hard to show that only unary strings have @xmath . For @xmath
, recall that @xmath . Therefore, @xmath if and only if each distinct
@xmath -tuple @xmath in @xmath is always followed by the same distinct
character. This is because, if a @xmath is always followed by the same
distinct character, then @xmath is unary, @xmath and @xmath contributes
nothing to the sum in the formula. Manzini [ Man01 ] defined the @xmath
th-order modified empirical entropy @xmath such that each context @xmath
contributes at least @xmath to the sum. Because modified empirical
entropy is more complicated than empirical entropy — e.g., it allows for
variable-length contexts — we refer readers to Manzini’s paper for the
full definition. In our proofs in this chapter, we use only the fact
that

  -- -------- --
     @xmath   
  -- -------- --

Manzini showed that, for some algorithms and all @xmath simultaneously,
it is possible to bound the encoding’s length in terms of only @xmath
and a constant that depends only on @xmath and @xmath ; he called such
bounds “entropy-only”. In particular, he showed that an algorithm based
on the Burrows-Wheeler Transform (BWT) [ BW94 ] stores any string @xmath
in at most @xmath bits for all @xmath simultaneously (since @xmath , we
could remove the @xmath term by adding 1 to the coefficient @xmath ).

###### Theorem 6.11 (Manzini, 2001).

Using the BWT, move-to-front coding, run-length coding and arithmetic
coding, we can achieve an entropy-only bound.

The BWT sorts the characters in a string into the lexicographical order
of the suffixes that immediately follow them. When using the BWT for
compression, it is customary to append a special character $ that is
lexicographically less than any in the alphabet. For a more thorough
description of the BWT, we again refer readers to Manzini’s paper. In
this section we first show how we can compute and invert the BWT with
two streams and, thus, achieve entropy-only bounds. We then show that we
cannot achieve entropy-only bounds with only one stream. In other words,
two streams are necessary and sufficient for us to achieve entropy-only
bounds.

One of the most common ways to compute the BWT is by building a suffix
array. In his PhD thesis, Ruhl introduced the StreamSort model [ Ruh03 ,
ADRR04 ] , which is similar to the read/write streams model with one
stream, except that it has an extra primitive that sorts the stream in
one pass. Among other things, he showed how to build a suffix array
efficiently in this model.

###### Theorem 6.12 (Ruhl, 2003).

In the StreamSort model, we can build a suffix array using @xmath bits
of memory and @xmath passes.

###### Corollary 6.13.

With two streams, we can compute the BWT using @xmath bits of memory and
@xmath passes.

###### Proof.

We can compute the BWT in the StreamSort model by appending $ to @xmath
, building a suffix array, and replacing each value @xmath in the array
by the @xmath st character in @xmath (replacing either 0 or 1 by $,
depending on where we start counting). This takes @xmath bits of memory
and @xmath passes. Since we can sort with two streams using @xmath bits
memory and @xmath passes (see, e.g., [ Sch07 ] ), it follows that we can
compute the BWT using @xmath bits of memory and @xmath passes. ∎

Now suppose we are given a permutation @xmath on @xmath elements as a
list @xmath @xmath , and asked to rank it, i.e., to compute the list
@xmath . This problem is a special case of list ranking (see, e.g., [
ABD @xmath 07 ] ) and has a surprisingly long history. For example,
Knuth [ Knu98 , Solution 24] described an algorithm, which he attributed
to Hardy, for ranking a permutation with two tapes. More recently, Bird
and Mu [ BM04 ] showed how to invert the BWT by ranking a permutation.
Therefore, reinterpreting Hardy’s result in terms of the read/write
streams model gives us the following bounds.

###### Theorem 6.14 (Hardy, c. 1967).

With two streams, we can rank a permutation using @xmath bits of memory
and @xmath passes.

###### Corollary 6.15.

With two streams, we can invert the BWT using @xmath bits of memory and
@xmath passes.

###### Proof.

The BWT has the property that, if a character is the @xmath th in @xmath
, then its successor in @xmath is the lexicographically @xmath th in
@xmath (breaking ties by order of appearance). Therefore, we can invert
the BWT by replacing each character by its lexicographic rank, ranking
the resulting permutation, replacing each value @xmath by the @xmath th
character of @xmath , and rotating the string until $ is at the end.
This takes @xmath memory and @xmath passes. ∎

Since we can compute and invert move-to-front, run-length and arithmetic
coding using @xmath bits of memory and @xmath passes over one stream, by
combining Theorem 6.11 and Corollaries 6.13 and 6.15 we obtain the
following theorem.

###### Theorem 6.16.

With two streams, we can achieve an entropy-only bound using @xmath bits
of memory and @xmath passes.

To show we need at least two streams to achieve entropy-only bounds, we
use De Bruijn cycles in a proof similar to the one for Theorem 6.8 . A
@xmath th-order De Bruijn cycle [ dB46 ] is a cyclic sequence in which
every possible @xmath -tuple appears exactly once. For example, Figure
6.1 shows a 3rd-order and a 4th-order De Bruijn cycle. (We need consider
only binary De Bruijn cycles.) Our argument this time is based on Lemma
6.7 and the following results about De Bruijn cycles.

###### Lemma 6.17.

If @xmath for some @xmath th-order De Bruijn cycle @xmath , then @xmath
.

###### Proof.

By definition, each distinct @xmath -tuple is always followed by the
same distinct character; therefore, @xmath and @xmath . ∎

###### Theorem 6.18 (De Bruijn, 1946).

There are @xmath @xmath th-order De Bruijn cycles.

###### Corollary 6.19.

We cannot store most @xmath th-order De Bruijn cycles in @xmath bits.

Since there are @xmath possible @xmath -tuples, @xmath th-order De
Bruijn cycles have length @xmath , so Corollary 6.19 means that we
cannot compress most De Bruijn cycles by more than a constant factor.
Therefore, we can prove a lower bound similar to Theorem 6.8 by
supposing that @xmath ’s repeated substring is a De Bruijn cycle, then
using Lemma 6.17 instead of Lemma 6.6 .

###### Theorem 6.20.

With one stream, we cannot achieve an entropy-only bound.

###### Proof.

As in the proof of Theorem 6.8 , suppose an algorithm uses only one
stream, @xmath bits of memory and @xmath passes to compress @xmath ,
with @xmath , and consider a machine performing that algorithm. This
time, however, suppose @xmath is periodic with period @xmath and that
its repeated substring @xmath is a @xmath th-order De Bruijn cycle,
@xmath , that is not compressible by more than a constant factor. Lemma
6.7 implies that the machine’s output while over a part of the stream
that initially holds a copy of @xmath , must be @xmath . Therefore, the
machine’s complete output must be @xmath bits. By Lemma 6.17 , however,
@xmath . ∎

Notice Theorem 6.20 implies a lower bound for computing the BWT: if we
could compute the BWT with one stream then, since we can compute
move-to-front, run-length and arithmetic coding using @xmath bits of
memory and @xmath passes over one stream, we could thus achieve an
entropy-only bound with one stream, contradicting Theorem 6.20 .

###### Corollary 6.21.

With one stream, we cannot compute the BWT.

Grohe and Schweikardt [ GS05 ] proved that, with @xmath streams, we
generally cannot sort @xmath numbers, each consisting of @xmath bits,
using @xmath bits of memory and @xmath passes. Combining this result
with the following lemma, we immediately obtain a lower bound for
computing the BWT with @xmath streams.

###### Lemma 6.22.

With two or more streams, sorting @xmath numbers, each of @xmath bits,
takes @xmath more bits of memory and @xmath more passes than computing
the BWT of a ternary string of length @xmath .

###### Proof.

We reduce the problem of sorting a sequence @xmath of @xmath -bit binary
numbers, @xmath , to the problem of computing the BWT of a ternary
string of length @xmath . Let @xmath denote the @xmath th bit of @xmath
. Using two streams, @xmath passes and @xmath memory, we replace each
@xmath by @xmath , writing 2 as a single character, @xmath and @xmath
each as @xmath bits, and @xmath as @xmath bits. Let @xmath be the
resulting string and consider the last @xmath characters of the BWT of
@xmath : they are a permutation of the characters followed by 2s in
@xmath , i.e., the bits of @xmath ; if @xmath or @xmath but @xmath then,
because @xmath is lexicographically less than @xmath , each bit of
@xmath comes before each bit of @xmath ; if @xmath then, for any @xmath
, because @xmath is lexicographically less than @xmath , the bit @xmath
comes before the bit @xmath . In other words, the last @xmath characters
of the BWT of @xmath are @xmath in sorted order. ∎

###### Corollary 6.23.

With @xmath streams, we cannot compute the BWT of a ternary string of
length @xmath using @xmath bits of memory and @xmath passes.

In another paper [ GM07a ] we improved the coefficient in Manzini’s
bound from @xmath to 2.7, using a variant of distance coding instead of
move-to-front and run-length coding. We conjecture this algorithm can
also be implemented with two streams.

###### Open Problem 6.24.

With @xmath streams, can we achieve the same entropy-only bounds that we
achieve in the RAM model?

The main idea of distance coding [ Bin00 ] is to write the starting
position of each maximal run (i.e., subsequence consisting of copies of
the same character), by writing the distance from the start of each
maximal run to the start of the next maximal run of the same character.
Notice we do not need to write the length of each run because the end of
each run (except the last) is the position before the start of the next
one. By symmetry, it makes essentially no difference to the length of
the encoding if we write the distance to the start of each maximal run
from the start of the previous maximal run of the same character, which
is not difficult with @xmath bits of memory and @xmath passes.

Kaplan, Landau and Verbin [ KLV07 ] showed how, using the BWT followed
by distance coding and arithmetic coding, we can store @xmath in @xmath
bits for any fixed @xmath and @xmath . This bound holds only when we use
an idealized arithmetic coder with @xmath total redundancy; if we use a
0th-order coder with per character redundancy @xmath , then the bound
becomes @xmath . In our paper we used a lemma due to Mäkinen and Navarro
[ MN05 ] bounding the number of runs in terms of the of the product of
the length and the 0th-order empirical entropy, to change the latter
bound into @xmath , which is an improvement when @xmath . Unfortunately,
the presence of the @xmath term prevents this from being an entropy-only
bound. To prove an entropy-only bound, we modified distance coding to
use an escape mechanism, which we have not verified can be implemented
in the read/write streams model.

## Chapter 7 Conclusions and Future Work

In this thesis we have tried to provide a fairly complete but coherent
view of our studies of sequential-access data compression, balancing
discussion of previous work with presentation of our own results. We
would like to highlight now what we consider our key ideas. The most
important innovation in Chapter 2 was probably our use of predecessor
queries for encoding and decoding with a canonical code. This, combined
with our use of Fredman and Willard’s data structure [ FW93 ] , Shannon
coding and background processing, allowed us to encode and decode each
character in constant worst-case time while producing an encoding whose
length was worst-case optimal. Chapters 3 and 4 were, admittedly,
somewhat tangential to our topic, but we included them to show how our
interests shifted from the model we considered in Chapter 2 to the one
we considered in Chapter 5 . The key idea in Chapter 5 was to view
one-pass algorithms with memory bounded in terms of the alphabet size
and context length as finite-state machines. This, combined with the
fact that short, randomly chosen strings almost certainly have low
empirical entropy, allowed us to prove a lower bound on the amount of
memory needed to achieve good compression, that nearly matched our upper
bound (which was relatively easy to prove, given Lemma 5.1 ). Finally,
the key idea in Chapter 6 was to to extend the automata-theoretic
arguments of Chapter 5 to algorithms that can make multiple passes and
use an amount of memory that depends on the length of the input. This
gave us our lower bound for achieving good grammar-based compression
with one stream, our lower bound for finding strings’ minimum periods
and, combined with properties of De Bruijn sequences, our lower bound
for achieving entropy-only bounds.

As we mentioned in the introduction, a paper [ GKN09 ] we wrote with
Marek Karpinski and Yakov Nekrich at the University of Bonn that
partially combining the results in Chapters 2 and 5 , will appear at the
2009 Data Compression Conference. This paper concerns fast adaptive
prefix coding with memory bounded in terms of the alphabet size and
context length, and shows that we can encode @xmath in @xmath bits while
using @xmath bits of memory and @xmath worst-case time to encode and
decode each character. Of course, we would like to improve these bounds,
and perhaps implement and test how our algorithm performs with large
alphabets such as Chinese, Unicode or the English vocabulary. We would
also like to implement our algorithm from Chapter 2 , testing several
implementations of dictionaries to determine which is the fastest in
practice; Fredman and Willard’s analysis has enormous constants hidden
in the asymptotic notation. Finally, we are preparing a paper with
Nekrich that will give efficient algorithms for adaptive alphabetic
prefix coding, adaptive prefix coding for unequal letter costs, and
adaptive length-restricted prefix coding (see [ Gag07a ] for
descriptions of these problems).

As we also mentioned in the introduction, we are currently investigating
whether we can prove any more results like the lower bound in Chapter 6
on finding strings’ minimum periods. We are working on the open problems
presented in Chapter 6 , about using multiple passes to obtain smaller
redundancy terms for universal compression with one stream,
approximating the smallest grammar with @xmath streams, and achieving
better entropy-only bounds with @xmath streams. Finally, we have been
collaborating with Paolo Ferragina at the University of Pisa and
Giovanni Manzini at the University of Eastern Piedmont on a paper [ FGM
] about BWT-based compression in the external memory model (see [ Vit08
] ) with limited random disk accesses. For the moment, however, our
curiosity about sequential-access data compression is mostly satisfied.

After proving our first results about adaptive prefix coding [ Gag07a ]
, we wrote several papers [ Gag06a , Gag06b , Gag08 , Gaga ] concerning
the number of bits needed to store a good approximation of a probability
distribution and, more generally, a Markov process. For one of these
papers [ Gag06b ] , about bounds on the redundancy in terms of the
alphabet size and context length, we proved versions of Lemmas 5.1 and
5.7 , which eventually led to Chapters 5 and 6 . We are now curious
whether our results can be combined with algorithms that build
sophisticated probabilistic models, either for data compression (see,
e.g., [ FGMS05 , FGM06 , FM08 ] ) or for inference (see, e.g., [ Ris86 ,
RST96 , AB00 , BY01 ] and subsequent articles). These algorithms work by
considering a class of probabilistic models that are, essentially,
Markov sources with variable-length contexts, and finding the model that
minimizes the sum of the length of the model’s description and the
self-information of the input with respect to the model; we note this
sum is something like the @xmath th-order modified empirical entropy.
Similar kinds of models are used in both applications because many
algorithms for inference are based on Rissanen’s Minimum Description
Length Principle [ Ris78 ] , which is based on ideas from data
compression.

How we minimize the sum of the length of the model’s description and the
self-information depends on how we represent the model. At least some of
the algorithms mentioned above assume that the length of description is
proportional to the number of contexts used. However, it seems that, if
some contexts occur frequently but the distributions of characters that
follow them are nearly uniform, and other occur rarely but are always
followed by the same character, then it might give better compression to
prune the former and keep the latter, which take only @xmath bits each
to store. Of course, this is just speculation at the moment.