## Chapter 1 Introduction

### 1.1 Formulation of the problem

My thesis studies asymptotic norm estimates for oscillatory integral
operators acting on the @xmath space of functions of one real variable.

More precisely, I fix a real @xmath function @xmath (called phase
function ) and consider a one-parameter family of operators of the form

  -- -------- -- -------
     @xmath      (1.1)
  -- -------- -- -------

Here @xmath is an unimportant @xmath cut-off function compactly
supported in a small neighborhood of the origin in @xmath .

The operators @xmath act on @xmath , and it is generally to be expected
that for @xmath the norm @xmath will decay. Typically, we will have

  -- -------- -- -------
     @xmath      (1.2)
  -- -------- -- -------

My thesis studies how the decay rate @xmath depends on the properties of
the phase function @xmath .

### 1.2 History and motivation

Hörmander [ 8 ] proved that if @xmath on the support of @xmath , then (
1.2 ) is true with @xmath , and this is best possible.

Typical example falling under the scope of this result is @xmath . For
this choice of the phase functions @xmath is a rescaled and cut-off
version of the Fourier transform.

Some problems of harmonic analysis naturally lead to more general phase
functions which do not necessarily satisfy Hörmander’s condition @xmath
.

For instance, such phase functions arise in studying smoothing
properties of generalized Radon transform associated with families of
curves having various geometric degeneracies. The most direct connection
exists between decay norm estimates for @xmath and smoothing properties
of the generalized Radon transform in the plane defined by

  -- -------- --
     @xmath   
  -- -------- --

Namely the decay estimate ( 1.2 ) implies that @xmath is a smoothing
operator of order @xmath , that is, it acts from the Sobolev space
@xmath into @xmath for any @xmath (see Phong and Stein [ 11 ] ).

### 1.3 Known results for degenerate phase function

As I mentioned above, it is desirable to be able to determine the
optimal exponent @xmath in ( 1.2 ) for phase functions with vanishing
@xmath , which are called degenerate .

My thesis addresses this problem in its local aspect. That is, I
concentrate on the properties of @xmath near the origin, and use the
freedom to choose the support of the cut-off function @xmath as small as
I want. The typical form of results that I will state is going to be
“There exists a small neighborhood of the origin @xmath such that if
@xmath , then …”.

As it was realized by Phong and Stein [ 9 ] , the optimal exponent
@xmath depends on the local properties of the phase function @xmath at
the origin via the Newton polygon of @xmath .

The Newton polygon of @xmath is defined as follows. In the positive
quadrant of the plane mark all the points with integer coordinates
@xmath such that the partial derivative

  -- -------- --
     @xmath   
  -- -------- --

After that, take the marked point for which @xmath is minimal and add
the vertical ray emanating from this point upward. Also, take the marked
point for which @xmath is minimal and add the horizontal ray emanating
from this point leftward. The Newton polygon is the convex hull of the
set consisting of all marked points and two added rays (see Fig. 1).

[]

Assume that the Newton polygon of @xmath is not empty, which means that
not all partial derivatives of @xmath vanish at the origin. Denote by
@xmath the parameter of intersection of the line @xmath with the
boundary of the Newton polygon. The number

  -- -------- --
     @xmath   
  -- -------- --

is called the Newton decay rate of @xmath (this definition differs by a
factor of 1/2 from [ 9 ] and [ 12 ] ).

Known norm estimates for @xmath relevant for my work are the following:

-   (Phong and Stein [ 9 ] ) Lower estimate @xmath .

-   (implicitly in Seeger [ 13 ] , [ 14 ] ) Almost sharp upper estimate
    @xmath for any @xmath .

-   (Phong and Stein [ 9 ] ) Sharp upper estimate @xmath under the
    additional assumption that @xmath is real analytic.

The upper estimates are true provided the support of @xmath is small
enough. The lower estimate is true for @xmath . In all three estimates
@xmath .

### 1.4 Main result of the thesis

The purpose of my thesis is to show that the sharp estimate proven by
Phong and Stein in the real analytic case continues to hold in the
@xmath case without loss of @xmath . There will be one possible
exception, when one loses at most a power of log.

Consider the formal Taylor series of @xmath at the origin

  -- -------- --
     @xmath   
  -- -------- --

I say that @xmath is exceptionally degenerate , if this series can be
factored in the ring of formal power series @xmath into the product

  -- -------- --
     @xmath   
  -- -------- --

where

-   @xmath ,

-   the series @xmath is of the form @xmath with @xmath , and

-   the series @xmath is invertible, that is its zeroth order term is
    nonzero.

Note that @xmath for such a phase function.

The main result of the thesis is the following

###### Theorem 1.1.

There exists a small neighborhood of the origin @xmath such that
(a) If @xmath is not exceptionally degenerate, and @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

(b) If @xmath is exceptionally degenerate, and @xmath , then

  -- -------- -- -------
     @xmath      (1.3)
  -- -------- -- -------

This theorem was proved in my paper [ 12 ] , modulo an inessential
improvement of the power of @xmath in ( 1.3 ). In Chapter 5 I will show
that in fact for @xmath estimate ( 1.3 ) can be improved to the sharp
one @xmath . I do not know if a similar improvement is possible for
@xmath .

## Chapter 2 Real analytic case

In this chapter I give an argument for the upper estimate of Phong and
Stein in the real analytic case. This argument is somewhat simpler than
the original proof. The main purpose is to familiarize the reader with
the ideas and the technology, which will later be partially recycled in
the proof of Theorem 1.1. I will explain what the main difficulty is
going to be in generalizing to the @xmath case. To begin with, I review
the lower bound. In this chapter no new results are proved.

### 2.1 Lower bound

The proof of the lower bound @xmath is obtained by looking at the
regions of the @xmath plane where @xmath and restricting the operator to
those regions.

#### 2.1.1 Typical example

I consider the example of the polynomial phase function whose Newton
polygon has only two corner points:

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

I also assume in this example that the segment joining points @xmath
intersects the bisectrix of the @xmath plane (Fig. 2). This example
captures the main idea of the proof in the general case.

[]

We can expand @xmath as follows:

  -- -------- --
     @xmath   
  -- -------- --

Notice that the exponent @xmath in Fig. 2, while the number of branches
@xmath is equal to the height of the triangle. The reader will see later
that these features naturally extend to general phase functions.

Now notice that for @xmath for generic @xmath different from all @xmath
I have

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

From the equation of the straight line passing through @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

putting @xmath I find

  -- -------- --
     @xmath   
  -- -------- --

I am looking for a region of the @xmath -plane where @xmath and @xmath .
According to ( 2.2 ), this happens for

  -- -------- --
     @xmath   
  -- -------- --

It follows that I can find a rectangle @xmath of size @xmath with sides
parellel to the coordinate axes with

  -- -------- --
     @xmath   
  -- -------- --

and such that @xmath on @xmath .

It remains to invoke the following straighforward

###### Lemma 2.1.

Suppose the kernel @xmath of an integral operator

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

is such that @xmath on a rectangle of size @xmath . Then the norm of the
operator on @xmath satisfies @xmath .

Using the lemma, I get

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

This is the correct answer, since I was looking at the Newton polygon of
@xmath , which differs from the Newton polygon of @xmath by a shift by
vector @xmath .

#### 2.1.2 General case

The general case turns out to be very similar to the example I have just
considered. The argument does not use real analyticity and works
generally in the @xmath case.

I have an asymptotic expansion

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

where @xmath runs through points inside the Newton polygon of @xmath . I
consider the edge of the Newton polygon intersecting the bisectrix (the
main edge ) and look at the region @xmath , where @xmath is the exponent
associated with this edge.

A simple check shows that in this region the terms in ( 2.5 ) coming
from all the edges but the main one and from the inside of the Newton
polygon are subleading. As a result, ( 2.2 ) is true as before, and (
2.4 ) follows. @xmath

### 2.2 Upper bound

As I have just shown, the lower bound follows by restricting the
operator to a rectangle where the phase function is effectively
constant, so that the oscillatory behavior is suppressed. The upper
bound, to which we proceed, is much trickier. It requires a
decomposition of the @xmath plane into much bigger rectangles, on which
the phase function does oscillate, in a controlled fashion.

#### 2.2.1 Elementary tools

The following 3 elementary results are needed in the proof of the upper
bound. In a sense, in most cases you just need to find the right
combination of the tools which works, and do the algebra corectly.

All rectangles below are assumed to have sides parallel to the
coordinate axes.

###### Lemma 2.2 (Size estimate).

Let @xmath be an integral operator of the form ( 2.3 ) . Assume that the
kernel @xmath is supported in a rectangle of size @xmath and bounded:
@xmath . Then @xmath is bounded on @xmath with the norm @xmath .

###### Lemma 2.3 (Oscillatory estimate).

Let @xmath be an oscillatory integral operator of the form ( 1.1 ) .
Assume that
(1) @xmath is supported in a rectangle @xmath of size @xmath ,
(2) @xmath in @xmath for @xmath ,
(3) @xmath in @xmath ,
(4) @xmath in @xmath for @xmath .
Then @xmath with @xmath depending only on @xmath .

###### Lemma 2.4 (Almost orthogonality).

Let @xmath be a family of rectangles and @xmath be a family of integral
operators of the form ( 2.3 ) such that
(1) the kernel of @xmath is supported in @xmath ,
(2) the family @xmath is almost orthogonal in the sense that for every
rectangle @xmath the number of rectangles whose horisontal or vertical
pojections intersect those of @xmath is bounded by a constant @xmath .
(3) @xmath are bounded on @xmath with @xmath independent of @xmath .
Then @xmath is bounded on @xmath with norm @xmath , where @xmath depends
on @xmath only.

The proof of the size estimate is straightforward (consider @xmath ).
The oscillatory estimate is a variant of the Operator van der Corput
lemma of Phong and Stein [ 9 ] . The lemma is proved by a standard
@xmath argument. The assumptions made are enough to show, integrating by
parts twice, that the kernel of @xmath has the bound

  -- -------- --
     @xmath   
  -- -------- --

which implies the necessary norm estimate. We omit the details. The
almost orthogonality is a trivial consequence of the Cotlar-Stein lemma.

#### 2.2.2 Example

Once again, I start with an example. This time I take the @xmath rather
then @xmath in the form of ( 2.1 )

  -- -------- --
     @xmath   
  -- -------- --

While I looked at the phase @xmath when proving the lower bound, it is
its second mixed derivative @xmath which is important for the upper
bound.

I also assume again that the segment joining points @xmath is the main
edge of the Newton polygon of @xmath , that is it intersects the
bisectrix of the @xmath plane (Fig. 2).

The proof starts by taking the dyadic decomposition of the @xmath plane
into rectangles @xmath of size @xmath . I take a suitable smooth
partition of unity fitted to this family of rectangles, and use it to
localize the operator @xmath to @xmath , that is, I consider the
operators

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is contained in the doubled rectangle @xmath (Fig. 3).

[]

I again look at the expansion

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

I notice that there are 3 important regions of parameters @xmath . Case
I: @xmath , where @xmath is a large constant. Case II: @xmath . Case
III: @xmath . These regions corespond to rectangles @xmath lying
respectively well above, well below, and around the curve @xmath (Fig.
4).

[]

Case I. By the size estimate (Lemma 2.2 ) I know the individual bounds

  -- -------- -- -------
     @xmath      (2.7)
  -- -------- -- -------

To apply the oscillatory estimate, I need to know how large @xmath is on
@xmath . It is easy to see from ( 2.6 ) that provided @xmath is chosen
large enough, I have the following estimates on @xmath

  -- -------- -- -------
     @xmath      (2.8)
  -- -------- -- -------

In this particular situation it is easy to check that the remaining
conditions of Lemma 2.3 are satisfied, so that I conclude

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

The most natural thing to do is to take a geometric mean @xmath of
estimates ( 2.7 ) and ( 2.9 ), choosing @xmath , so that the resulting
estimate will have the desired @xmath behavior @xmath . Remember that

  -- -------- --
     @xmath   
  -- -------- --

Doing the algebra, I obtain the following estimate

  -- -------- -- --------
     @xmath      (2.10)
  -- -------- -- --------

Now it’s time to invoke almost orthogonality. I split all the Case I
rectangles into families indexed by a natural number @xmath , putting
into the @xmath -th family all @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

For each @xmath , such a family is almost orthogonal, and so it follows
from ( 2.10 ) by Lemma 2.4 that

  -- -------- -- --------
     @xmath      (2.11)
  -- -------- -- --------

Notice that in general I have @xmath (Fig. 2). Assume for the moment
that @xmath . In this case @xmath , and I can sum estimate ( 2.11 ) over
@xmath from @xmath to infinity, to get

  -- -------- -- --------
     @xmath      (2.12)
  -- -------- -- --------

which is the required estimate.

If @xmath , I am going to start again from estimates ( 2.7 ) and ( 2.9 )
and use a completely different splitting into almost orthogonal
families. In this case, I will put into the @xmath -th family all @xmath
such that

  -- -------- --
     @xmath   
  -- -------- --

By almost orthogonality, it follows from ( 2.7 ) and ( 2.9 ) that

  -- -------- -- --------
     @xmath      (2.13)
  -- -------- -- --------

The decreasing and increasing progressions under the minimum sign
balance for

  -- -------- --
     @xmath   
  -- -------- --

Summing ( 2.13 ) in @xmath , I get

  -- -------- -- --------
     @xmath      (2.14)
  -- -------- -- --------

which is exactly what is required.

Case II. It comes as no suprise that this case is going to be absolutely
similar to Case I. The size estimate ( 2.7 ) stays the same, and the
appropriate oscillatory estimate obtained analogously to ( 2.9 ) comes
out to be

  -- -------- --
     @xmath   
  -- -------- --

If @xmath , I split into almost orthogonal families @xmath and arrive at
the analogue of ( 2.14 ).

If @xmath (notice that always @xmath ), I do the same manipulation which
led to ( 2.10 ), and get

  -- -------- --
     @xmath   
  -- -------- --

Notice that now @xmath , which is exactly compatible with having to sum
over @xmath . I split into the almost orthogonal fiamilies @xmath , and
get the analogues of ( 2.11 ) and ( 2.12 ). Case closed.

Case III. Here I cannot get any reliable estimates on @xmath on the
whole rectangle @xmath . Because of this, a further decomposition is
required. However, the present example is a bit too special to
demonstrate the method. I will deal with this situation in a more
general setting in the next section.

#### 2.2.3 General case

Now I am going to consider the general case of real analytic @xmath .
The basis of my consideration is going to be the following far-reaching
generalization of ( 2.6 ) known as Puiseux theorem . This result is
basically well known (see [ 9 ] ).

[]

First I look at the Newton polygon of @xmath . In general, the polygon
is going to have some number of finite edges and two infinite edges
(Fig. 5). With each finite edge @xmath joining points @xmath and @xmath
, @xmath , I associate numbers @xmath and @xmath , where

  -- -------- --
     @xmath   
  -- -------- --

Let also @xmath and @xmath be the @xmath and @xmath coordinates of the
infinite edges. Then the claim of the Puiseux theorem is that in a
neighborhood of the origin there exists a factorization

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

where @xmath are convergent fractional power series with fractionality
at most @xmath , @xmath , whose expansion starts with

  -- -------- --
     @xmath   
  -- -------- --

and where @xmath is a real analytic function with @xmath .

I see that looking at the Newton polygon alone gives me quite detailed
information on the structure of the zero set @xmath , as well as of its
level sets. Now I am going to proceed along the lines of the example
considered in the previous section.

I think of the @xmath plane, or rather of its positive quadrant, as
split into pieces by curves @xmath (Fig. 6). Note that the way I number
finite edges from right to left, numbers @xmath decrease with @xmath .

[]

Now I consider the dyadic partition of the positive quadrant into the
rectangles @xmath , and the corresponding smooth partition of the @xmath
into the operators @xmath . The rectangles @xmath fall into two
categories, the ones which lie far away from any of the curves @xmath ,
and the ones which lie close to one of these curves.

Far away rectangles.

Consider the rectangles lying between @xmath and @xmath . To simplify
the notation, I put @xmath , but I do not assume that I am dealing with
the rightmost finite edge. The corresponding pairs of @xmath are singled
out by the condition

  -- -------- -- --------
     @xmath      (2.16)
  -- -------- -- --------

( @xmath a large constant).

It follows from ( 2.15 ) by straightforward agebra that on such a
rectangle

  -- -------- --
     @xmath   
  -- -------- --

which is a complete analogue of estimate ( 2.8 ).

Now if @xmath , then I am again in the situation of Case I of the
example from the previous section. I will split the rectangles into
almost orthogonal families @xmath , resum, and get the estimate

  -- -------- -- --------
     @xmath      (2.17)
  -- -------- -- --------

where the sum is taken over @xmath satisfying ( 2.16 ), and

  -- -------- -- --------
     @xmath      (2.18)
  -- -------- -- --------

where @xmath is the point of intersection of the straight line passing
through the edge @xmath with the bisectrix (Fig. 7). Notice that I do
not assume that @xmath is the main edge of the Newton polygon.

[]

Analogously if @xmath , I find myself in the Case II situation. So I
will split the rectangles into the families @xmath , resum, and get the
estimate ( 2.17 ) with @xmath instead of @xmath .

If @xmath , I as before split into families @xmath , and get the same
estimates.

Fig. 8 shows the direction of resummation for all regions of the
quadrant. Here @xmath denotes the exponent, corresponding to the main
edge (the one intersecting the bisectrix).

[]

Since obviously @xmath for all edges, the above discussion results in
the needed estimate

  -- -------- --
     @xmath   
  -- -------- --

Rectangles which are close.

I look at the rectangles close to the curve @xmath , where @xmath is one
of exponents @xmath . These rectangles satisfy the condition @xmath and
form an almost orthogonal family. So it is sufficient to prove the bound

  -- -------- -- --------
     @xmath      (2.19)
  -- -------- -- --------

for each of these rectangles individually. The argument I give below is
different from and shorter than the original proof of this estimate
given by Phong and Stein (see [ 9 ] , pp. 126–148).

It is easy to see from ( 2.15 ) that on @xmath the @xmath has the
following behavior

  -- -------- -- --------
     @xmath      (2.20)
  -- -------- -- --------

where to simplify the notation I put @xmath , so that the edge in
question joins @xmath with @xmath , but I am not going to assume that
this is the rightmost edge. I also dropped the index @xmath from @xmath
, @xmath , and @xmath .

The branches @xmath are in general complex-valued. I introduce

  -- -------- --
     @xmath   
  -- -------- --

The @xmath are smooth analytic functions, and for @xmath I have

  -- -------- -- --------
     @xmath      (2.21)
  -- -------- -- --------

Some of the curves @xmath may intersect the rectangle @xmath (Fig. 9).
The geometry of the problem suggests to take a Whitney decomposition of
the set @xmath , where

  -- -------- --
     @xmath   
  -- -------- --

into rectangles @xmath of sixe @xmath such that the distance from @xmath
to Z is @xmath in anistotropic norm @xmath .

[]

The easiest way to arrange such a decomposition is to dilate the picture
vertically by the factor of @xmath , do the usual Whitney decomposition,
and contract back.

A quite obvious but important point follows from ( 2.21 ): For each
@xmath , the subfamily of the rectangles @xmath having @xmath is almost
orthogonal.

Now I am going to smoothly localize @xmath to @xmath ’s, denoting the
corresponding partial operators @xmath . This does not break almost
orthogonality, and I have

  -- -------- -- --------
     @xmath      (2.22)
  -- -------- -- --------

I now turn to estimating the norm of @xmath . Note that on @xmath I have

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the first nozero term in the expansion of @xmath (
@xmath if this expansion is identically zero).

It follows that on @xmath

  -- -------- --
     @xmath   
  -- -------- --

It is also not difficult to see that supplementary conditions of Lemma
2.3

  -- -------- --
     @xmath   
  -- -------- --

are satisfied on @xmath .

Now I am prepared to apply the oscillatory estimate to @xmath . Namely,
Lemma 2.3 gives

  -- -------- -- --------
     @xmath      (2.23)
  -- -------- -- --------

(The last inequality follows by taking the lower bound for @xmath
ignoring contributions of @xmath , and also by using @xmath .)

As always, I also have the following size estimate:

  -- -------- -- --------
     @xmath      (2.24)
  -- -------- -- --------

The natural decay in @xmath that I expect is @xmath , where @xmath has
the same meaning as in ( 2.17 ), in particular,

  -- -------- --
     @xmath   
  -- -------- --

So I take the geometric mean of ( 2.23 ) and ( 2.24 ) with the
corresponding exponents @xmath and @xmath . As the reader may check, I
get (see ( 2.18 ) for the definition of @xmath )

  -- -------- -- --------
     @xmath      (2.25)
  -- -------- -- --------

Now please note that I may assume @xmath . Indeed, if @xmath , then I
just switch to the adjoint of @xmath , which amounts to interchanging
roles of @xmath and @xmath and transforms @xmath .

Further, if @xmath strictly, then it is easy to see geometrically that
necessarily @xmath no matter how the edge of the Newton polygon lies. In
this situation I can substitute ( 2.25 ) into ( 2.22 ), sum in @xmath ,
and get the required estimate ( 2.19 ) (note that @xmath ).

The special case @xmath can happen only if @xmath and the Newton polygon
has only one finite edge joining points @xmath and @xmath . In this case
I avoid taking the geometric mean and substitute ( 2.23 ) and ( 2.24 )
directly into ( 2.22 ):

  -- -------- -- --------
     @xmath      (2.26)
  -- -------- -- --------

as a simple analysis shows. (Find @xmath for which the progressions
balance. Consider the cases @xmath and @xmath .) This is the right
estimate in this particular case. @xmath

#### 2.2.4 Discussion and outlook

This finishes the proof of the upper bound in the real analytic case.

The main components of the proof, such as

-   the use of the Puiseux expansion,

-   the resummation procedures used to estimate far away from the
    branches,

-   the Whitney decomposition method used close to the branches

are going to carry over to the @xmath case either verbatim or with small
modifications, as the reader will see in the coming chapters.

Jumping slightly ahead of time, I am going to say that the only crucial
difference, actually the one responsible for the presence of @xmath ’s
in Theorem 1.1, is going to come from the possible occurence of multiple
real nondifferentiable branches. Namely, in the @xmath case I may have a
situation like the one shown in Fig. 10, when the zero set of @xmath has
several branches, which, although being close to each other to
infinitely high order, are nevertheless non-coinciding and in fact
nondifferentiable. This of course would be impossible in the real
analytic case.

[]

The problem with such a situation is that I lose condition ( 2.21 ),
which was the basis of almost orthogonality, and the whole Whitney
decomposition procedure is going to become useless near these multiple
branches.

The way I am going to fight this difficulty will be to localize away
from the branches by a very narrow cutoff. The Whitney decomposition
will still work away from the branches, and near the branches I will
have to use a completely different argument, based on a method due to
Seeger [ 13 ] .

## Chapter 3 Smooth Puiseux theorem

The next 2 chapters are devoted to the proof of Theorem 1.1 . While in
the previous chapter, which was supposed to be expository, I was
allowing myself to be informal at times, from now on I will strive to
provide full details.

In this chapter I explain how the Puiseux expansion ( 2.15 ) generalizes
from the real analytic to the @xmath case.

### 3.1 Algebraic notation

Proving theorems about @xmath functions often involves an intermediate
step, when the analysis is done purely algebraically within the category
of formal power series. I am going to employ this very strategy. Here, I
will set up some algebraic notation.

First recall that for any ring @xmath , the symbols @xmath and @xmath
denote the rings of polynomials and, respectively, formal power series
in indeterminate @xmath with coefficients from @xmath . This notation
can be iterated, e.g. @xmath is the ring of polynomials in @xmath with
coefficients which are elements of @xmath , and @xmath is the ring of
double formal power series.

Factorization formulas for @xmath function, which I am going to prove in
this chapter, are going to be valid in a small neighborhood of the
origin. Since I do not care how small this neighborhood is, it will be
convenient to formulate the results for function-germs rather than
functions.

An identity involving several function-germs is defined to be true if
there exist functions from the equivalence classes of these germs such
that in the intersection of their domains of definition the identity is
true in the usual sense.

Basically, this convention will spare me the necessity to repeat the
phrase “There exists a small neighborhood of the origin @xmath such that
in @xmath …” every time.

I will make use of the following rings of germs of @xmath -valued
functions:

-   @xmath — continuous functions at the origin of @xmath ;

-   @xmath and @xmath — @xmath functions at the origin of @xmath and
    @xmath , respectively;

-   @xmath and @xmath — rings of one-sided germs; consist of (the
    equivalence classes of) functions @xmath defined in a left
    half-neighborhood of zero of the form @xmath , where @xmath can
    depend on @xmath , which are continuous, respectively @xmath , up to
    zero;

-   @xmath , @xmath , — the subring of @xmath consisting of germs @xmath
    , for which there exists a series @xmath , @xmath , such that @xmath
    in the sense that for any @xmath

      -- -------- --
         @xmath   
      -- -------- --

    Such an @xmath is uniquely determined and is called the asymptotic
    expansion of @xmath .

Notice that for the elements of @xmath , @xmath , and @xmath , I can
talk about their Taylor series at the origin. A germ whose Taylor series
is zero is called flat .

The rings of germs of @xmath -valued functions will be denoted by adding
an @xmath to the above notation, e.g. @xmath .

### 3.2 Puiseux decomposition of @xmath functions

Now I am going to state the main result of this chapter. The proof will
be given in the following sections.

Let @xmath . Denote by @xmath the Newton polygon of @xmath , and assume
that @xmath , so that @xmath is not flat.

Let @xmath run through all compact edges of the boundary of @xmath . For
each edge @xmath joining integer points @xmath and @xmath , where @xmath
, put

  -- -------- --
     @xmath   
  -- -------- --

Let also @xmath be the @xmath -coordinate of the vertical infinite edge,
and @xmath be the @xmath -coordinate of the horizontal infinite edge of
@xmath .

###### Proposition 3.1.

In the above conditions, the germ @xmath admits in the region @xmath a
factorization of the form

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

where
(1) @xmath , @xmath ,
(2) all @xmath , and @xmath as @xmath for any @xmath ,
(3) all @xmath for @xmath with asymptotic expansions of the form @xmath
as @xmath , where @xmath ,
(4) if @xmath is any of the functions @xmath , and if @xmath is the
product over all @xmath such that @xmath has exactly the same asymptotic
expansion as @xmath , then @xmath ,
(5) if in (4) I additionally assume that the asymptotic expansion of
@xmath is real, then @xmath is also real.

This result copies ( 2.15 ) in the part that concerns the number of
branches and the leading terms in their asymptotic expansion. However,
there are also substantial differences, such as:

-   in general, branches @xmath and @xmath infinitely tangent to
    coordinate axes but not coinciding with them are present;

-   I cannot claim that branches @xmath are differentiable; (4) is the
    best that is true in general.

These differences are for real, as very simple example show. For
instance, one can take @xmath with a choice of coefficients so that the
determinant oscillates around zero as @xmath .

To the best of my knowledge, Puiseux decompositions of @xmath functions
in the form of Proposition 3.1 or of a similar kind have not appeared in
the literature before. However, granted Lemma 3.2 below, the proof of
Proposition 3.1 follows a rather standard path, well known say in the
singularity theory of @xmath and analytic functions, see e.g. Arnold
et.al. [ 1 ] , or Artin [ 2 ] .

### 3.3 Preparation to the proof

The proof relies on the following result, which is well known in the
theory of plane algebraic curves under the same generic name of the
Puiseux theorem. A proof can be found in [ 17 ] , p. 98ff, or [ 3 ] ,
A.V.150.

###### Lemma 3.2.

Let @xmath be of the form

  -- -------- --
     @xmath   
  -- -------- --

where the zeroth order terms of all @xmath vanish. Let @xmath , @xmath ,
@xmath , @xmath be defined via the Newton polygon @xmath in the same way
as in the Proposition. Then there exists a factorization

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

where the series @xmath are of the form @xmath with @xmath .

The following lemma will be used to pass from factorizations of formal
power series (obtained via Lemma 3.2 ) to factorizations of
function-germs in the @xmath category. The proof uses standard
technology usually applied in such situations.

###### Lemma 3.3.

Let @xmath be of the form

  -- -------- --
     @xmath   
  -- -------- --

where @xmath for all @xmath . Let @xmath be the formal Taylor series of
@xmath at the origin. Let @xmath be a root of multiplicity @xmath ,
@xmath , of @xmath considered as a polynomial in @xmath , which means
that

  -- -------- --
     @xmath   
  -- -------- --

as elements of @xmath . Then there exist @xmath function-germs @xmath
such that
(1) all @xmath as @xmath ,
(2) all @xmath for @xmath ,
(3) @xmath ,
(4) if we additionally assume that @xmath and @xmath are real, then
@xmath is also real.

###### Proof.

Let @xmath be a @xmath function with the formal Taylor series @xmath ,
supplied by E. Borel’s theorem. Denote

  -- -------- --
     @xmath   
  -- -------- --

Let the (nonzero by assumption) series @xmath starts with a term @xmath
, @xmath , @xmath . Then I have @xmath as @xmath . On the other hand,
the functions @xmath are flat.

I will be looking for @xmath of the form

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is an unknown continuous @xmath -valued function-germ such
that @xmath as @xmath for any @xmath .

By Taylor’s formula, the equation @xmath can be written as

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

For small @xmath , this is equivalent to the equation @xmath for the
function @xmath given by

  -- -------- --
     @xmath   
  -- -------- --

Note that if @xmath , and @xmath is flat at the origin, while @xmath is
not flat, then @xmath is @xmath near the origin and is flat. So the
performed division by @xmath is legitimate, and @xmath .

On the complex circle @xmath , @xmath , the term @xmath will dominate
the other terms in @xmath if @xmath is sufficiently small. By Rouche’s
theorem it follows that the equation @xmath has for small fixed @xmath
exactly @xmath roots in the disc @xmath , which I denote @xmath , @xmath
. I can arrange so that @xmath are continuous in @xmath , and the
previous argument shows that @xmath for any @xmath .

We now prove (3). Since the functions @xmath enter the product in (3) in
a symmetric way, it is sufficient to prove that the elementary symmetric
polynomials @xmath in @xmath are in @xmath . By the Newton relations
(see [ 3 ] , A.IV.70), it is sufficient to prove the same for the
functions

  -- -------- --
     @xmath   
  -- -------- --

However, by Cauchy’s formula I have that for small @xmath

  -- -------- --
     @xmath   
  -- -------- --

from where it is clear that @xmath , since nothing dramatic happens to
@xmath on the circle @xmath .

To prove (4), I notice that under the additional assumption made I can
take @xmath to be real. Then @xmath , and therefore non-real roots
@xmath will appear in conjugate pairs. Then all @xmath will be real,
which implies (4). ∎

Now I am going to combine two previous lemmas to prove

###### Lemma 3.4.

Proposition 3.1 is true if @xmath .

###### Proof.

By Lemma 3.2 , the Taylor series @xmath of @xmath has a factorization (
3.2 ). Consider the function @xmath . Its Taylor series has the form
@xmath , and so factorizes as

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath be one of the series @xmath , and assume that among all the
@xmath there are exactly @xmath series coinciding with @xmath . Then
@xmath is a root of multiplicity @xmath of the polynomial @xmath , and
by Lemma 3.3 I conclude that there exist @xmath functions @xmath ,
@xmath , such that (1)–(3) from the formulation of the lemma are true.

In view of (3), we can divide @xmath by @xmath , and the result is again
a polynomial @xmath from @xmath . The Taylor polynomial of @xmath will
be @xmath divided by @xmath . Now we can apply Lemma 3.3 to @xmath
choosing a different @xmath etc.

By repeating this operation several times, I get a complete
factorization of @xmath . The required factorization of @xmath is then
obtained by the inverse substitution @xmath . The property (5) is
ensured by splitting off all real series @xmath before non-real ones in
the above argument. ∎

Proposition 3.1 will be reduced to Lemma 3.4 by means of the following
Malgrange preparation theorem (see [ 6 ] , p. 95).

###### Lemma 3.5.

Let @xmath , and assume that @xmath is not flat, so that @xmath , @xmath
, for some @xmath and @xmath . Then there is a factorization

  -- -------- --
     @xmath   
  -- -------- --

where
(1) @xmath , @xmath ,
(2) @xmath is of the form

  -- -------- --
     @xmath   
  -- -------- --

where all @xmath , @xmath .

### 3.4 Proof of the Proposition

Notice that the Newton polygon is invariant with respect to
multiplication by a nonzero @xmath function (see Phong and Stein [ 9 ] ,
p. 112). Therefore, for the functions @xmath such that @xmath is not
flat (which is equivalent to having @xmath ) the proposition follows
immediately from Lemmas 3.5 and 3.4 .

Assume now that @xmath . In this case we must somehow separate the roots
infinitely tangent to the @xmath -axis. This can be done as follows.
Since @xmath is not flat at the origin, there exists a rotated
orthogonal system of coordinates @xmath such that the restriction of
@xmath to the @xmath -axis is not flat. So we can apply Lemma 3.5 to
@xmath written in coordinates @xmath . Let @xmath be the arizing
polynomial.

If @xmath is the equation of the old @xmath -axis in the new
coordinates, then @xmath will be a root of multiplicity @xmath of @xmath
. So we can apply Lemma 3.3 and obtain @xmath roots @xmath , @xmath of
@xmath , such that @xmath .

Moreover, by Lemma 3.3 (3),(4) we will have that @xmath is in @xmath .
So we can divide @xmath by @xmath , and the quotient will be a @xmath
function, which is no longer flat on the old @xmath -axis.

Let @xmath be this last quotient written in the old system of
coordinates. Then the Newton polygon of @xmath is just @xmath shifted
@xmath units to the left. So we can factorize @xmath as in the case
@xmath described above.

It remains to get a factorization of @xmath in the old coordinates. It
is clear that the Taylor series of @xmath written in the coordinates
@xmath consists of one term @xmath . Interchanging the roles of @xmath
and @xmath brings us back to the case @xmath , and the required
factorization of the form @xmath can be obtained as described above.

## Chapter 4 Upper bound for smooth case

In this chapter I am going to prove Theorem 1.1 .

### 4.1 Beginning of the proof

The proof starts just like in the real analytic case.

I decompose the operator @xmath as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is defined as

  -- -------- --
     @xmath   
  -- -------- --

Here @xmath is a smooth dyadic partition of unity on @xmath , so that
the kernel of @xmath is supported on the rectangle @xmath . Three other
@xmath combinations refer to the quadrants defined by specific signs of
@xmath and @xmath . We restrict ourselves with the positive quadrant,
the other ones being exactly similar, and denote @xmath by simply @xmath
.

By Proposition 3.1 applied to @xmath , there is a neighborhood of the
origin @xmath such that in @xmath there exists a factorization of the
form ( 3.1 ). I assume that @xmath . The singular variety

  -- -------- --
     @xmath   
  -- -------- --

now splits into branches corresponding to the factors in the RHS of (
3.1 ). Note, however, that some of these branches may contain an
imaginary component.

Let @xmath denote the double of @xmath . I fix a large constant @xmath
such that if the pair @xmath satisfies the condition @xmath , then
@xmath on @xmath for all @xmath occurring as the lowest order terms of
the asymptotic expansions of @xmath in Proposition 3.1 .

Let me number the compact edges @xmath of the boundary of the Newton
polygon @xmath from right to left, so that

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the total number of compact edges. Also put @xmath if
@xmath , @xmath otherwise; @xmath if @xmath , @xmath otherwise.

Consider the following splitting of @xmath :

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

Here

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

  -- -------- --
     @xmath   
  -- -------- --

( @xmath stands for @xmath ) constitute the part of @xmath supported
relatively far away from @xmath . Further,

  -- -------- --
     @xmath   
  -- -------- --

constitute the part of @xmath supported near the branches of @xmath
which are infinitely tangent to the coordinate axes. Finally

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

are the part of @xmath supported near all other branches of @xmath .

In the following sections I will prove the upper bound claimed in
Theorem 1.1 for the norms of all operators in the RHS of ( 4.1 ): @xmath
and @xmath (Section 4.2), @xmath and @xmath (Section 4.3), and finally
@xmath (Sections 4.4 and 4.5). This will prove Theorem 1.1.

### 4.2 Estimates far away from @xmath

In this section, I prove that @xmath for each @xmath . The reader will
believe me that with minor modifications the argument given below will
also produce the same estimate for @xmath , @xmath .

The proof is very similar to the argument given in Section 2.2.3 for far
away rectangles. I will just provide some extra details about estimating
the size of @xmath on the support of @xmath and about checking
conditions of Lemma 2.3. In what concerns subsequent resummation of the
individual @xmath estimates, the argument goes through verbatim.

Take an operator @xmath entering the RHS of ( 4.2 ). I may reduce @xmath
if necessary so that on the part of @xmath inside @xmath the functions
@xmath do not differ much from the first terms of their asymptotic
expansions. Assume that @xmath is nonzero, which means that @xmath .
Then it is clear from the definition of the constant @xmath that the
factors in the RHS of ( 3.1 ) can estimated as follows for @xmath (see
Fig. 10a):

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

  -- -------- --
     @xmath   
  -- -------- --

( @xmath means @xmath , where @xmath is an unimportant constant
independent of @xmath ).

[]

Therefore it follows from ( 3.1 ) that on @xmath

  -- -------- -- -------
     @xmath      (4.5)
  -- -------- -- -------

The numbers @xmath , @xmath can be found from the Newton polygon @xmath
as described in Proposition 3.1. Using this information, I find that

  -- -------- --
     @xmath   
  -- -------- --

I further claim that on @xmath

  -- -------- -- -------
     @xmath      (4.6)
  -- -------- -- -------

Indeed, when differentiating the RHS of ( 3.1 ) in @xmath , the
derivative can fall one either @xmath , or @xmath , or one of the
remaining terms. In the first case, I simply get a bounded factor. In
the second case, I get an even better factor of @xmath for any @xmath ,
since the product in question is a @xmath function whose Taylor series
at the origin is @xmath . Finally, in the third case I get a factor of
the form @xmath or @xmath , which is @xmath in view of ( 4.4 ). This
argument works equally well for the second derivative, giving ( 4.6 ).

The rectangle @xmath is of size @xmath with @xmath , @xmath . So the
conditions of Lemma 2.3 are satisfied, and I obtain the oscillatory
estimate

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

On the other hand, the size estimate following from Lemma 2.2 is

  -- -------- -- -------
     @xmath      (4.8)
  -- -------- -- -------

The rest of the proof goes through exactly as described in Section
2.2.3. Namely, I am going to split the operators @xmath constituting
@xmath into almost orthogonal families @xmath , or @xmath , or @xmath ,
depending if @xmath is larger, smaller, or equal to @xmath . Then I am
going to resum and get the @xmath estimate for @xmath . I will not
repeat the details.

### 4.3 Estimates near the coordinate axes

In this section, I will prove the estimate @xmath . The same estimate
will be true for @xmath , since taking the adjoint of @xmath brings
@xmath to the form of @xmath . I may of course assume @xmath , since
otherwise @xmath and @xmath .

Notice that in the real analytic case there was no need to introduce
this special localization along the coordinate axis. In the notation of
Section 4.1, operator @xmath could be included into the @xmath part and
treated along the same lines as the @xmath . Analogously @xmath could be
united with @xmath . However, in the @xmath case the possible presence
of the branches infinitely tangent to coordinate axes asks for this
additional localization.

I represent @xmath as (see Fig. 10b)

[]

  -- -------- --
     @xmath   
  -- -------- --

and claim that
(1) @xmath
(2) @xmath for @xmath ,
(3) @xmath for some @xmath .
If I prove all these, the estimate @xmath will follow from the
Cotlar–Stein lemma.

I have

  -- -------- --
     @xmath   
  -- -------- --

where @xmath , so that the support of @xmath is contained in @xmath .
The property (2) is obvious. Further, the operator @xmath has the kernel

  -- -------- --
     @xmath   
  -- -------- --

I want to estimate this by the following variant of the standard van der
Corput lemma (see [ 16 ] , Corollary on p. 334).

###### Lemma 4.1.

Let @xmath be a positive integer, @xmath , @xmath , and assume that
@xmath on @xmath . If @xmath , assume additionally that @xmath is
monotonic on @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

Assume that @xmath . I apply this lemma with @xmath , @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

It is clear that @xmath . Further (recall that I denoted @xmath ),

  -- -------- --
     @xmath   
  -- -------- --

Of all the terms arising when I differentiate ( 3.1 ) @xmath times in
@xmath , the term in which all derivatives fall on @xmath will dominate
on the support of @xmath after a possible reduction of @xmath . It
follows that on the support of @xmath

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the common vertex of the horizontal infinite edge of
@xmath and its first compact edge @xmath .

By the previous remarks,

  -- -------- --
     @xmath   
  -- -------- --

In the case @xmath I have @xmath on the support of @xmath , so Lemma 4.1
gives

  -- -------- --
     @xmath   
  -- -------- --

I apply the following variant of the Schur test (see e.g. [ 7 ] ,
Theorem 5.2).

###### Lemma 4.2.

Let @xmath be an integral operator on @xmath with kernel @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Assume that the quantities

  -- -------- --
     @xmath   
  -- -------- --

are finite. Then @xmath is bounded with @xmath .

By this lemma and the estimate of @xmath I have just obtained,

  -- -------- --
     @xmath   
  -- -------- --

This of course implies the estimate

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

As usual, by Lemma 2.2 I also have a size estimate:

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

As the reader may check, taking the geometric mean of these two bounds
which kills the @xmath -factor gives exactly @xmath , with @xmath
defined as

  -- -------- --
     @xmath   
  -- -------- --

This implies (1) since all @xmath .

In proving (3), I may assume @xmath . Then @xmath on the support of
@xmath , whence by Lemma 4.1

  -- -------- --
     @xmath   
  -- -------- --

The support of @xmath is contained in the rectangle of size @xmath . Now
Lemma 2.2 gives a bound improved by a factor of @xmath :

  -- -------- --
     @xmath   
  -- -------- --

where I denoted @xmath . By multiplying the estimates ( 4.10 ) for
@xmath and @xmath , I get another bound:

  -- -------- --
     @xmath   
  -- -------- --

These two bounds have the form of ( 4.9 ) and ( 4.10 ) squared, but with
an additional factor exponentially decreasing in @xmath . Therefore it
is clear that this time taking the geometric mean killing the @xmath
-factor will give

  -- -------- --
     @xmath   
  -- -------- --

for some @xmath . This implies (3) and concludes the treatment of @xmath
.

### 4.4 Estimates near @xmath

This next two sections are devoted to proving upper bounds for @xmath .

Notice that the sum in ( 4.3 ) is almost orthogonal, since the @xmath -
and @xmath -supports of @xmath and @xmath are disjoint for @xmath larger
than a fixed constant. Therefore it suffices to estimate each @xmath
from the RHS of ( 4.3 ) individually.

Fix such a @xmath . For quite a while the proof is going to proceed
exactly like the argument in the part of Section 2.2.3 dealing with
close rectangles. Analogously to ( 4.5 ), on @xmath

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

where I ordered @xmath so that for @xmath we have @xmath in @xmath .

Let me quickly dispose of the case @xmath , in which I can apply Lemma
2.3 (the condition (4) is easily checked) and Lemma 2.2 to get the
oscillatory and size estimates

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

Now by taking the geometric mean killing the @xmath -factor, I obtain
the required estimate @xmath .

Now assume that @xmath . Denote @xmath , and let @xmath be the
asymptotic expansion of @xmath at zero. By E. Borel’s theorem, I can
find real functions @xmath such that @xmath and @xmath as @xmath .
Moreover, there is one case when I may and will take simply @xmath .
Namely, by Proposition 3.1, parts (4),(5), this is possible if the
series @xmath is real and different from any other @xmath .

Let @xmath be the union of the graphs of @xmath inside @xmath :

  -- -------- --
     @xmath   
  -- -------- --

It is not difficult to see that on @xmath

  -- -------- --
     @xmath   
  -- -------- --

This suggests to consider a Whitney-type decomposition of @xmath away
from @xmath into rectangles of size @xmath . The easiest way to do this
is to dilate the set @xmath along the @xmath -axis @xmath times, take
the standard Whitney decomposition into the dyadic squares away from
(the dilation of) @xmath , and contract everything to the original
scale. As a result, I get a covering

  -- -------- --
     @xmath   
  -- -------- --

where @xmath are rectangles of size @xmath , @xmath , such that the
distance from @xmath to @xmath in the anisotropic norm @xmath is of the
order @xmath .

I claim that the rectangles @xmath of fixed size form an almost
orthogonal family, i.e. that for each @xmath the number of rectangles
@xmath with @xmath such that either the @xmath - or the @xmath
-projections of @xmath and @xmath intersect is bounded by a fixed
constant independent of @xmath .

[]

Fig. 11

Consider the case of intersecting @xmath -projections (the other case is
similar). Then @xmath is contained in in the horizontal strip passing
through @xmath (see Fig. 11). By dilating along the @xmath -axis, I may
assume that @xmath . Since @xmath , there exists a point @xmath on the
graph of one of the functions @xmath such that @xmath . Let @xmath
denote the point where the graph of @xmath intersects the bottom of the
strip. Since @xmath , I have @xmath , and therefore @xmath . Thus all
possible rectangles @xmath are situated at a distance @xmath from no
more than @xmath points where the bottom of the horizontal strip
intersects @xmath . This implies that the number of @xmath in the
horizontal strip is bounded by a fixed constant, and the almost
orthogonality is verified.

Now let @xmath , where an @xmath is chosen so small that @xmath (in the
anisotropic norm). Consider a smooth partition of unity @xmath on @xmath
with @xmath , satisfying the natural differential inequalities. I am
going to decompose @xmath using this partition of unity. However, this
decomposition will not be useful near the real multiple branches of
@xmath , since I will not have good control on the size of @xmath there.
For now I am just going to localize away from those branches in the
following way.

Let @xmath denote the power exponent of the first nonzero term @xmath in
the asymptotic expansion of @xmath ; @xmath if this expansion is
identically zero. For a large fixed number @xmath I introduce the set

  -- -------- --
     @xmath   
  -- -------- --

where * indicates that the union is taken over all @xmath such that
@xmath and @xmath . By the choice of @xmath , this may happen only if
the series @xmath is real and there are several @xmath having @xmath as
their asymptotic expansion. One can say that @xmath is a tubular
neighborhood of width @xmath of the real multiple branches of @xmath
(see Fig. 12).

[]

Fig. 12

The purpose of introducing @xmath is that on @xmath I have (if @xmath is
large enough, which can be achieved by a further contraction of @xmath )

  -- -------- -- --------
     @xmath      (4.12)
  -- -------- -- --------

Now let @xmath be a smooth cut-off supported in the double of @xmath ,
@xmath on @xmath . I consider the decomposition

  -- -------- -- --------
     @xmath      (4.13)
  -- -------- -- --------

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

In the rest of this section I prove that @xmath . The operator @xmath
will be dealt with in the next section.

Let @xmath be one of the operators from the decomposition of @xmath ,
and assume that @xmath , i.e. that @xmath . Fix a point @xmath in this
last intersection. I claim that

  -- -------- -- --------
     @xmath      (4.14)
  -- -------- -- --------

for @xmath and @xmath . Indeed, let @xmath and @xmath be points of
@xmath for which the value of @xmath is respectively minimal and
maximal. Then

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

since @xmath . From this ( 4.14 ) follows.

Now from ( 4.11 ) and ( 4.12 ) we see that on @xmath

  -- -------- --
     @xmath   
  -- -------- --

It follows by Lemma 2.3 (the condition (4) needs to be checked, but this
is easy) that @xmath .

I can get a lower bound on @xmath by noting that @xmath . This gives

  -- -------- --
     @xmath   
  -- -------- --

and therefore

  -- -------- -- --------
     @xmath      (4.15)
  -- -------- -- --------

On the other hand, by Lemma 2.2 ,

  -- -------- -- --------
     @xmath      (4.16)
  -- -------- -- --------

Now it remains to resum the last two estimates by splitting the family
of operators @xmath into almost orthogonal families @xmath . This is
done exactly how I did it in Section 2.2.3 after Eq. ( 2.24 ) ^(*) ^(*)
* This resummation was unfortunately done in a wrong way in my paper [
12 ] . The part of that paper from Eq. (5.6) and until the end of
Section 5 has to be thrown out and substituted by the more careful
argument I give in Section 2.2.3 of this thesis. .

This ends the proof of @xmath .

### 4.5 Estimates near multiple real branches

To finish the proof of the theorem, I must estimate the operator @xmath
appearing in the decomposition ( 4.13 ) of @xmath .

In the estimates below I can assume that @xmath , since this can be
achieved by passing to the adjoint operator if necessary.

Further, I can assume that @xmath is chosen so large that the branches
of @xmath having different asymptotic expansions become completely
separated in the definition of @xmath . Since such branches can be
treated separately, I am reduced to the case when @xmath has the form

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

Here @xmath is a @xmath cut-off supported in @xmath , @xmath , @xmath ,
@xmath , and in factorization ( 3.1 ) exactly @xmath functions @xmath
have asymptotic expansion coinciding with that of @xmath . I will assume
that this happens for @xmath . I also re-denote @xmath .

I write @xmath as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath , and @xmath is the product of the rest of the terms in (
3.1 ).

Since all the branches of @xmath appearing in @xmath are well separated
from @xmath , there exists a constant @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

Moreover, it can be seen directly that if @xmath is exceptionally
degenerate, we have @xmath .

Further, by Proposition 3.1, parts (4), (5), I know that @xmath , so
that @xmath is @xmath in both variables on @xmath . It is clear that

  -- -------- -- --------
     @xmath      (4.17)
  -- -------- -- --------

I claim that, more generally,

  -- -------- -- --------
     @xmath      (4.18)
  -- -------- -- --------

Denote @xmath . The Taylor series of @xmath is

  -- -------- --
     @xmath   
  -- -------- --

It is clear that

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

Therefore the factorizations of @xmath , @xmath , which can be obtained
as described in the proof of Lemma 3.4 , will contain branches with the
asymptotic expansion @xmath , while the factorization of @xmath will not
contain such branches. This implies ( 4.18 ), provided that @xmath is
large enough, since @xmath can be expressed as

  -- -------- --
     @xmath   
  -- -------- --

with coefficients @xmath growing power-like as @xmath .

In addition, the above argument gives an estimate

  -- -------- -- --------
     @xmath      (4.19)
  -- -------- -- --------

for some constant @xmath ; @xmath if @xmath is exceptionally degenerate.

Denote @xmath . Consider the decomposition

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the characteristic function of the set @xmath , @xmath
is a constant.

I am going to prove the estimates:

  -- -------- -- --------
     @xmath      (4.20)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (4.21)
  -- -------- -- --------

The required bound for @xmath can then be derived as follows.

Consider first the exceptionally degenerate case, when @xmath . I have

  -- -------- --
     @xmath   
  -- -------- --

If it were not for the factor of @xmath , the terms in parentheses would
become equal for @xmath , and I would have the best possible estimate
@xmath . In the present situation I am going to lose something, and to
optimize the loss, I put @xmath with indeterminate @xmath and have the
estimate ^(†) ^(†) † Here I am being slightly more careful than in [ 12
] and earn a marginal improvement in the power of @xmath .

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

The optimal value of @xmath is @xmath , which gives

  -- -------- --
     @xmath   
  -- -------- --

in complete accordance with what is claimed in the theorem.

Assume now that @xmath is not exceptionally degenerate. In this case the
above argument gives in any case the estimate

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath , with some constant @xmath . (I do not pursue the
possibility of obtaining a @xmath factor here, since as I will see in a
moment, what I have is already good enough.)

I will need the following more general version of Lemma 2.2 , which can
be obtained immediately from Lemma 4.2 .

###### Lemma 4.3.

(Phong and Stein [ 10 ] , Lemma 1.6) Let @xmath be an integral operator
with kernel @xmath , and assume that
(1) @xmath ,
(2) for each @xmath , @xmath is supported in an @xmath -set of measure
@xmath ,
(3) for each @xmath , @xmath is supported in a @xmath -set of measure
@xmath .
Then @xmath .

By this lemma, I certainly have the estimate

  -- -------- --
     @xmath   
  -- -------- --

The idea is that now I can take the geometric mean of the last two
estimates killing the @xmath -factor and, if @xmath is very large, this
will introduce only a very small increase in the exponent of @xmath ,
actually tending to zero as @xmath . Thus I have

  -- -------- --
     @xmath   
  -- -------- --

I am going to show that in the case under consideration @xmath . This
allows me to choose and fix @xmath from the very beginning so large that
@xmath , thus proving the theorem.

I show that in fact @xmath . Indeed, since I already have @xmath
branches whose expansion starts with @xmath , I know that @xmath .
Therefore @xmath , and

  -- -------- --
     @xmath   
  -- -------- --

since @xmath . Besides that, the equality holds if and only if @xmath ,
@xmath , @xmath . But this corresponds exactly to the exceptionally
degenerate case, which is excluded.

I now turn to the proof of the claimed bounds for @xmath . The proof of
( 4.20 ) is easy and is based on the following well-known

###### Lemma 4.4.

(Christ [ 4 ] , Lemma 3.3) Let @xmath be such that @xmath on @xmath .
Then for any @xmath

  -- -------- --
     @xmath   
  -- -------- --

where the constant @xmath depends only on @xmath .

By this lemma, in view of ( 4.17 ) and ( 4.19 ), the kernel of @xmath is
supported in a @xmath -set of measure @xmath for each @xmath , and in an
@xmath -set of measure @xmath for each @xmath . Now ( 4.20 ) follows by
Lemma 2.7 .

Seeger’s method.

The proof of ( 4.21 ) constitutes the most intricate part of the whole
argument. It is carried out by a variation of a method developed in
Seeger [ 13 ] , Section 3. The key idea is to take an additional dyadic
localization in @xmath , @xmath . Let @xmath be fixed; all constants
below will however be independent of @xmath . Let @xmath be a vector
with integer components @xmath , @xmath some constant. Denote

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the characteristic function of the set @xmath for @xmath
, and of the set @xmath for @xmath .

For an appropriate fixed @xmath I have a decomposition

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

I am going to prove that for each @xmath

  -- -------- -- --------
     @xmath      (4.22)
  -- -------- -- --------

This will imply ( 4.21 ), since the number of @xmath in the
decomposition of @xmath is @xmath .

The kernel of the operator @xmath has the form

  -- -------- --
     @xmath   
  -- -------- --

Assuming that @xmath , and using Taylor’s formula in @xmath for @xmath ,
I have

  -- -------- -- --------
     @xmath      (4.23)
  -- -------- -- --------

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

Notice that @xmath . So the RHS of ( 4.23 ) looks like a polynomial in
@xmath with dyadically restricted coefficients. To handle such
polynomials, I need the following variant of Lemma 3.2 from [ 13 ] . I
chose to give a proof, since I have found one simpler than in [ 13 ] .

###### Lemma 4.5.

For an integer @xmath , an integer vector @xmath , @xmath , and a
constant @xmath consider the set @xmath of all polynomials of the form
@xmath with real coefficients @xmath satisfying

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

Then there exists a constant @xmath , independent of @xmath , and a set
@xmath of the form

  -- -------- -- --------
     @xmath      (4.24)
  -- -------- -- --------

such that
(1) @xmath , @xmath are negative integers, @xmath ,
(2) @xmath ; @xmath ; @xmath ,
(3) @xmath for @xmath for any @xmath .

###### Proof.

Put @xmath . Consider the convex set @xmath given as the intersection of
the half-planes lying above the lines @xmath , @xmath . The boundary of
@xmath consists of two infinite rays contained in straight lines @xmath
and @xmath , and of some (possibly zero) number of compact segments.

Let @xmath , @xmath be all the corner points of the boundary of @xmath
with the @xmath -coordinates @xmath . It is clear that @xmath . (In Fig.
12a @xmath , @xmath ; in Fig. 12b @xmath , @xmath .)

[]

An observation which will turn out to be important later: if @xmath ,
then the line @xmath cannot contain a compact segment of the boundary of
@xmath . To see this, it is sufficient to consider how the lines @xmath
, @xmath , pass with respect to the lines @xmath and @xmath .

I claim that for any @xmath and for large enough @xmath

  -- -------- -- --------
     @xmath      (4.25)
  -- -------- -- --------

First consider the case

  -- -------- -- --------
     @xmath      (4.26)
  -- -------- -- --------

Let @xmath be such that the boundary points @xmath and @xmath belong to
the line @xmath (Fig. 12c).

[]

By the above observation, @xmath . Since @xmath and @xmath lie above all
the other lines @xmath , I have for all @xmath

  -- -------- --
     @xmath   
  -- -------- --

From these two estimates it follows that

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

Using ( 4.26 ), I conclude

  -- -------- --
     @xmath   
  -- -------- --

This estimate clearly implies @xmath , provided that @xmath is large
enough.

Second, if @xmath , then the same argument as above shows @xmath .

Third, if @xmath , then if @xmath , I can show in the same way as above
that @xmath . If @xmath , then @xmath , and this region of @xmath ’s is
irrelevant.

So ( 4.25 ) is verified. Finally, it is not difficult to see that the
exceptional set in ( 4.25 ) satisfies (1)-(3). ∎

Now if I take out the factor of @xmath , the expression in the RHS of (
4.23 ) has the form of polynomial in @xmath falling under the scope of
the lemma with @xmath . So I have a set @xmath of the form ( 4.24 ) such
that

  -- -------- --
     @xmath   
  -- -------- --

I claim that this implies

  -- -------- -- --------
     @xmath      (4.27)
  -- -------- -- --------

Indeed, this will follow from Lemma 4.1 with @xmath , if I prove that
there exists a constant @xmath independent of @xmath and @xmath such
that for fixed @xmath and @xmath
(1) the number of intervals of monotonicity of @xmath considered as a
function of @xmath is less than @xmath ,
(2) the number of intervals comprising the @xmath -set where @xmath is
non-zero is less than @xmath .

To show (1), note that @xmath on @xmath . It follows that

  -- -------- --
     @xmath   
  -- -------- --

Therefore, @xmath vanishes at most @xmath times, which implies (1).

To show (2), it suffices to check that the number of intervals in the
set @xmath is bounded by a constant independent of @xmath and @xmath for
each @xmath . However, this last statement follows from ( 4.18 ).

Unfortunately, to prove the claimed norm estimate for @xmath , I will
need still another decomposition taking into account the form of the set
@xmath . Namely, for @xmath and an integer @xmath I put

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the characteristic function of the interval @xmath , and
consider the operators

  -- -------- --
     @xmath   
  -- -------- --

I am going to prove by induction in @xmath that for each @xmath

  -- -------- --
     @xmath   
  -- -------- --

The statement for @xmath implies the required estimate ( 4.22 ), since
@xmath , and the sum contains no more than @xmath terms.

For @xmath , I use the kernel of the operator @xmath , which has the
form

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the kernel of @xmath . If this expression is not zero,
then @xmath . In view of ( 4.27 ), and also because @xmath , Lemma 4.2
gives

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

which is even better by a factor of @xmath than what I need.

The induction step is performed by using the decomposition

  -- -------- --
     @xmath   
  -- -------- --

I will need the following variant of the Cotlar–Stein lemma, which can
be proved by an easy adaptation of the standard proof given in [ 16 ] ,
see e.g. Comech [ 5 ] , Appendix.

###### Lemma 4.6.

Let @xmath be a family of operators on a Hilbert space @xmath such that
(1) @xmath for @xmath ,
(2) @xmath with a constant @xmath independent of @xmath .
Then @xmath .

I have @xmath for @xmath . Let us estimate the sum

  -- -------- -- --------
     @xmath      (4.28)
  -- -------- -- --------

for a fixed @xmath . Since both @xmath and @xmath appear in the
decomposition of @xmath , I have @xmath . Further, the kernel of @xmath
has the form

  -- -------- --
     @xmath   
  -- -------- --

If this expression is different from zero, then

  -- -------- -- --------
     @xmath      (4.29)
  -- -------- -- --------

Assume first that

  -- -------- -- --------
     @xmath      (4.30)
  -- -------- -- --------

Then ( 4.29 ) implies @xmath , and I can use the estimate ( 4.27 ). By
Lemma 4.2 ,

  -- -------- --
     @xmath   
  -- -------- --

Therefore the part of the sum ( 4.28 ) over @xmath satisfying ( 4.30 )
is bounded by

  -- -------- --
     @xmath   
  -- -------- --

where I used the fact that by Lemma 4.5 (2) @xmath .

However, the number of @xmath which do not satisfy ( 4.30 ) is bounded
by a constant in view of Lemma 4.5 (2), so the corresponding part of (
4.28 ) is bounded by @xmath by the induction hypothesis.

By applying Lemma 4.6 , I complete the induction step. Theorem 1.1 is
now proven.

## Chapter 5 Stopping time

This chapter stands somewhat separately from the rest of the thesis.
Here I am developing a quite different method of proving upper norm
bounds. This method is incomplete as it stands, and it is unclear if it
is possible to make it complete. In its present form it is much less
powerful compared to methods based on the geometric analysis of the zero
set of @xmath which I used above. However, I can use this method to
prove that the estimate ( 1.3 ) from Theorem 1.1 can be improved to the
optimal @xmath in the case @xmath .

### 5.1 General idea

The main idea would be to try to organize an inductive process which
would “resolve the singularity” of @xmath by gradually decreasing the
space under its Newton polygon, eventually reducing me to the
non-degenerate case (Fig. 13).

[]

This idea was first applied to oscilatory integral operators by Phong
and Stein in [ 11 ] . Although the proof of that paper is incomplete as
it stands (almost orthogonality claim on p. 114 of [ 11 ] is
unjustified; see also Remark (c) on p. 150 of [ 9 ] ), the argument can
be saved at least in some partial cases [ 16 ] . Below I use a variation
of the method of [ 11 ] and [ 16 ] to get a somewhat sharper result.

Still a full realization of the above idea remains elusive. The
inductive process I can actually organize works well only for the
simplest Newton polygons consisting of just one edge joining 2 points on
the coordinate axes.

Unfortunately, this property may get destroyed already on the first step
of the inductive process (Fig. 14).

[]

However, it will not get destroyed provided that there are no integer
points lying strictly inside the triangle @xmath . The last condition is
satisfied in the following two cases:

-   @xmath or @xmath (Fig. 15)

-   @xmath (Fig. 16)

[]

These are exactly the cases when I am able to produce final results by
this method. In particular, the case @xmath settles @xmath in Theorem
1.1.

### 5.2 Results

I am going to prove the following

###### Theorem 5.1.

@xmath
I. Assume that @xmath is a smooth phase function in the square @xmath
and that @xmath satisfies

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

in Q. Assume also that @xmath is a smooth cutoff supported in @xmath .
Then the operator given by ( 1.1 ) is bounded on @xmath with

  -- -------- --
     @xmath   
  -- -------- --

II. If instead of ( 5.1 ) I assume that

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

in @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

I will need the following somewhat more quantitative auxiliary result,
which implies Part I immediately, and to which Part II will also be
later reduced.

###### Theorem 5.2.

Let @xmath be a smooth phase function in the square @xmath , @xmath a
real number. Assume that @xmath satisfies in @xmath

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

for @xmath Assume also that @xmath is a smooth cutoff supported in
@xmath . Then the operator given by ( 1.1 ) is bounded on @xmath with

  -- -------- --
     @xmath   
  -- -------- --

Notice that the number @xmath in these results is the Newton decay rate
corresponding to the Newton polygon with two vertices @xmath and @xmath
. Analogously @xmath is the right Newton decay rate for the @xmath –
@xmath Newton polygon. Notice that the @xmath exceptionally degenerate
phase functions of Theorem 1.1 satisfy conditions of Theorem 5.2, Part
II.

### 5.3 Proofs

Proof of Theorem 5.2. Induction on @xmath . For @xmath the result
follows from Lemma 2.3 .

Assume that @xmath . I divide @xmath into equal rectangles of size
@xmath , @xmath . If for some of these rectangles the condition @xmath
below is satisfied, I put it into a numbered collection of rectangles
@xmath . Otherwise I divide it further into equal rectangles of size
@xmath , now @xmath , etc. The stopping condition for a rectangle @xmath
of size @xmath is

  -- -------- -- --------
     @xmath      @xmath
  -- -------- -- --------

(star means the doubled rectangle, double star means the quadrupled
rectangle).

Eventually all @xmath up to a set of measure zero becomes decomposed
into rectangles @xmath . The exceptional set is the intersection of the
zero sets of @xmath , @xmath . This set is of measure zero, since @xmath
.

I claim that the covering @xmath has finite multiplicity, that is for
every @xmath there are only finitely many @xmath ’s such that

  -- -------- -- -------
     @xmath      (5.3)
  -- -------- -- -------

It is sufficient to prove that ( 5.3 ) implies @xmath . Now if @xmath ,
then it follows from ( 5.3 ) that

  -- -------- -- -------
     @xmath      (5.4)
  -- -------- -- -------

where @xmath denotes the “parent” of @xmath , that is the rectangle out
of which @xmath was obtained in the @xmath -dyadic division process
described above.

But it follows from ( 5.4 ) that already @xmath had to be retained and
not divided further. This contradiction shows that necessarily @xmath ,
from which finite multiplicity follows.

Because of finite multiplicity, I can localize the operator @xmath to
@xmath by a smooth partition of unity satisfying the “right”
differential bounds. Denote the part supported on @xmath by @xmath .

I claim that as well as the lower bound @xmath , the upper bound

  -- -------- -- --------
     @xmath      @xmath
  -- -------- -- --------

for EACH @xmath is true on @xmath , and in fact on @xmath .

The proof goes like this. Since @xmath was not retained, for each @xmath
there is a point @xmath in @xmath such that

  -- -------- -- -------
     @xmath      (5.5)
  -- -------- -- -------

Now by assumption

  -- -------- -- -------
     @xmath      (5.6)
  -- -------- -- -------

in the whole @xmath . Since the @xmath -size of @xmath is @xmath , it
follows from ( 5.5 ) and ( 5.6 ) by Newton-Leibnitz applied in the
@xmath -direction that

  -- -------- -- -------
     @xmath      (5.7)
  -- -------- -- -------

provided that @xmath . (Fig. 17)

[]

Now notice that for @xmath ( @xmath ) is true by assumption in the whole
@xmath , and that ( @xmath ) follows from ( @xmath ) and ( 5.7 ) by
Newton-Leibnitz applied in the @xmath -direction. So ( @xmath ) follows
by induction for all @xmath from @xmath to 0.

The main reason I need ( @xmath ) is to show that for each @xmath , the
subfamily of rectangles @xmath with @xmath is almost orthogonal.

Indeed, since @xmath on @xmath and @xmath on @xmath , by Lemma 4.4 there
are no more than const rectangles of the same @xmath -size @xmath with
intersecting @xmath -projections.

Analogously, since @xmath on @xmath and @xmath on @xmath , by Lemma 4.4
there are no more than const rectangles of the same @xmath -size @xmath
with intersecting @xmath -projections.

By almost orthogonality, I get

  -- -------- -- -------
     @xmath      (5.8)
  -- -------- -- -------

Now the idea is to rescale @xmath to a square of size @xmath by putting
(Fig. 18)

  -- -------- --
     @xmath   
  -- -------- --

[]

The norms on @xmath are related by

  -- -------- -- -------
     @xmath      (5.9)
  -- -------- -- -------

After the rescaling, I get that the phase @xmath satisfies the
conditions of Theorem 5.2 with @xmath instead of @xmath and with @xmath
instead of @xmath . Indeed, the main conditions

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

are satisfied. The auxiliary conditions are checked as follows:

1) @xmath for @xmath , where I used that by assumption @xmath on the
whole @xmath .

2) @xmath for @xmath by ( @xmath ), and for @xmath by knowing that
@xmath in the whole @xmath .

Thus it follows by the induction hypothesis that

  -- -------- --
     @xmath   
  -- -------- --

It follows from ( 5.8 ), ( 5.9 ), and the fact that @xmath that

  -- -------- --
     @xmath   
  -- -------- --

Two progressions balance for

  -- -------- --
     @xmath   
  -- -------- --

Notice that the second progression is indeed increasing:

  -- -------- --
     @xmath   
  -- -------- --

So it follows that

  -- -------- --
     @xmath   
  -- -------- --

This completes the induction step and the proof of the theorem. @xmath

Proof of Theorem 5.1, Part II. I am going to reduce this result to the
@xmath case of Theorem 5.2. This reduction is in fact very similar to
the proof of Theorem 5.2 itself.

I organize a dyadic decomposition of @xmath , this time into dyadic
squares @xmath of size @xmath , @xmath , with stopping condition

  -- -------- -- --------
     @xmath      @xmath
  -- -------- -- --------

That is, if @xmath is satisfied, the @xmath is retained, otherwise it is
further subdivided into 4 squares of equal size, etc.

As before,

  -- -------- --
     @xmath   
  -- -------- --

up to a set of measure zero. I show that @xmath form a covering of
finite multiplicity in the same way as before, and split

  -- -------- --
     @xmath   
  -- -------- --

Then I prove that on @xmath

  -- -------- -- --------
     @xmath      (5.10)
  -- -------- -- --------

The proof of these bounds is even simpler that that of @xmath . They
follow immediately by Newton-Leibnitz from the fact that @xmath was not
retained.

By Lemma 4.4 I conclude from ( 5.10 ) and ( 5.2 ) that for each @xmath
the @xmath with @xmath form an almost orthogonal family. This implies (
5.8 ).

To estimate @xmath , I rescale the operator to a square of size @xmath :

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- -- --------
     @xmath      (5.11)
  -- -------- -- --------

Assume that the stopping condition that was actually satisfied for
@xmath was @xmath (the case of @xmath being completely analogous because
of the @xmath - @xmath symmetry). Then after rescaling

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

Conditions

  -- -------- --
     @xmath   
  -- -------- --

are also easily checked. So we see that the @xmath satisfies the
assumptions of Theorem 5.2 for @xmath and @xmath .

It follows that

  -- -------- --
     @xmath   
  -- -------- --

Going back to ( 5.8 ) and ( 5.11 ),

  -- -------- --
     @xmath   
  -- -------- --

The progressions are balanced for @xmath , and thus

  -- -------- --
     @xmath   
  -- -------- --

The theorem is proved. @xmath
