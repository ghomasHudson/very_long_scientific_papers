##### Contents

-    1 Introductory remarks
-    2 Basic notions and definitions
    -    2.1 Point processes, Correlation functions
    -    2.2 Complex Gaussian distribution
    -    2.3 Gaussian analytic functions
    -    2.4 Stationary zero sets of Gaussian analytic functions
-    3 A recipe for stationary zero sets of random analytic functions
-    4 How to study the zeros of a polygaf?
-    5 Distribution of the zero set of a polygaf
-    6 Determinantal processes that are zeros of RAFs: Known results
-    7 Determinantal processes that are zeros of RAFs: An analogy
-    8 Matrix-valued GAFs and polygafs
-    9 Background: Results for Gaussian analytic functions
-    10 Our results: for polygafs
    -    10.1 Central moments of @xmath
    -    10.2 Estimating the second moment
    -    10.3 Estimating the values of non-split diagrams
-    11 Statements of the problems
-    12 Overcrowding - The planar case
    -    12.1 Ginibre ensemble
    -    12.2 Proof of Theorem 11.1
-    13 Overcrowding - The hyperbolic case
    -    13.1 The determinantal case
    -    13.2 All values of @xmath
-    14 Very large deviations for the planar GAF
-    15 Moderate deviations for the planar GAF

###### Acknowledgements. It is a truth universally acknowledged, that a
single author presenting a piece of research must owe a lot to the
inputs of many people. This definitely applies to this thesis, and I am
happy to acknowledge the help I have received from various quarters. As
regards my Ph.D., above all, I would like to thank my advisor, Yuval
Peres, for advising me all these years and especially for keeping up an
endless supply of problems to work on, when in a quest for research
problems I was still executing a random walk on many topics in
probability theory. I am just as grateful to Bálint Virág and Yuval
Peres for drawing me into the study of random analytic functions and for
generously sharing their insights and problems with me. I met random
matrices through Steve Evans, whose course on this subject in my first
semester at Berkeley remains one of the most useful courses I have ever
taken. Mikhail Sodin, through his papers as well as through direct
conversations, deeply influenced my perspective of the subject. I thank
my thesis committee members for reading the draft and making several
useful suggestions. I have also immeasurably benefited from the many
courses I took, most of them in the statistics and mathematics
departments. I learnt a great deal more of mathematics in these courses
than I could have on my own, for which I thank all the instructors. Also
I thank my fellow graduate students with whom I had many wonderful
discussions. Particularly, in the beginning years I learnt significantly
from Antar Bandyopadhyay, Noam Berger and Gábor Pete, and on the main
topic of this thesis I had many great discussions with Ron Peled and my
collaborator Ben Hough. I thank Sourav Chatterjee and Mikhail Sodin for
getting me interested in Normal approximation problems. I am very
grateful also to the department staff who have been immensely helpful
throughout. While I learnt a lot of mathematics after coming to
Berkeley, none of this would have been possible without the training in
mathematics and statistics that I received from my professors at the
Indian Statistical Institute. Without their dedicated teaching I would
not have become a probabilist. I particularly accuse Alladi Sitaram,
Sundaram Thangavelu and S.M. Srivastava of inspiring me into thinking
that I should become a researcher. No less important were the courses of
S.C.Bagchi, Arup Bose, V. Pati, S. Ramasubramaniam, T.S.S.R.K. Rao and
many others. Impersonal teachers have their own significance, and
include for example, the authors of many books, whose ideas I may have
absorbed and today assume to be my own. Among them I would particularly
like to mention the great probabilist and expositor, Mark Kac. When I
applied for my PhD I mentioned in my “Statement of Purpose” that what
got me interested in probability theory for the first time, was the
theorem by Kac that the average number of real roots of a random
polynomial with independent standard normal coefficients is asymptotic
to @xmath . It is a pleasure to me that this thesis can be seen as
continuing the same theme of zeros of random polynomials, but hopefully
does more than fill some much needed gaps! Even more influential on my
development as a person and on my attitude towards learning, were my
parents, my teachers at school and college, and my relatives and friends
(the public opinion is that I could stand much more development, but I
claim for myself all credit for that). It would be silly to even try to
adequately acknowledge in words, their roles in my making. My father’s
huge répertoire of stories fired my imagination in my early childhood
and made me think beyond everyday concerns. Apart from many other
things, my mother saw to it that I paid due attention to my studies,
till the time came when I realized that it was a pleasure. My brother
and sister, my aunts and uncles and my friends were quite as important
in shaping me. Almost none of them is a mathematician, but they have
that high respect (even without full comprehension) for knowledge that
is so widespread in India. I greatly value my friends for their great
company and for never quite giving up on me, even though I have always
been most irregular in returning their e-mails or phone calls. I am sure
that all of them will feel happy on seeing my thesis.

## Chapter \thechapter Introduction

### 1 Introductory remarks

Random analytic functions on the one hand and random matrices on the
other are two well studied topics in probability theory and mathematical
physics. One of the chief interests to a probabilist in these objects is
the kind of point processes one gets, by taking the set of zeros or the
set of eigenvalues, as the case may be. Both these kinds of point
processes typically have the property of “repulsion”, meaning that the
points distribute themselves more evenly than they would if they were
thrown down independently. That is an appealing feature because, while
there are ways to construct point processes that are more clumped than
independent points, there are not many natural ways in which to generate
point processes with less clumping.

This fact and several others (more empirical than mathematical) have led
to a folk wisdom that random analytic functions and random matrices
share many similarities. Differing responses to this statement have been
heard, including one that points out the obvious tautology here (after
all the characteristic polynomial of a random matrix is a random
polynomial), and another that says that there is not much similarity but
instead evokes an “anthropic” reasoning (the same set of people work on
both these fields). Without denying the validity of these explanations,
in this thesis we take a more positive approach attempting to provide a
unifying framework that includes both random matrices and random
analytic functions (see caveats below).

This is the simple but seemingly useful idea of considering random
matrix-valued analytic functions, and the set of points where it becomes
singular (i.e., the zeros of the determinant). The linear polynomials
reduce to random matrices and the @xmath matrices correspond to random
analytic functions.

A little explanation is in order. When we talk of random analytic
functions, we tacitly mean that we are somehow specifying the
distribution of coefficients or some closely related quantities
(otherwise any random set of points would be the zeros of a random
analytic function). Furthermore it is usually difficult to analyse a
random analytic function (especially to get exact properties) except in
the case of Gaussian coefficients. So the essence of the above paragraph
is that the determinant of a Gaussian matrix-valued analytic function is
a non-Gaussian analytic function in itself, but nevertheless amenable to
analysis because it is built out of Gaussian analytic functions.

Secondly, the earlier claim about random matrices falling within our
framework should be toned down. The chief, although not the whole,
emphasis in random matrix theory is on the study of Hermitian random
matrices and their (real) eigenvalues, for physical as well as
mathematical reasons. When we go to higher polynomials there is perhaps
no natural way to get the zeros to lie on the real line. This may
explain why these objects have not been studied before. What we study
here are zeros in the complex plane, for which of course there is no
such problem. Nevertheless we believe that it is also interesting
mathematically to study polynomials with random Hermitian or random
unitary coefficients (we do not do this here) even though the zeros are
spread out in the complex plane.

We now outline the contents of the thesis briefly.

-   In the remaining sections of this chapter, we give a quick
    introduction to the basic notions of a point process, correlation
    functions and Gaussian analytic functions. Most importantly, we
    recall the three canonical families of Gaussian analytic functions
    on the plane, the sphere and the unit disk (hyperbolic plane).

-   In Chapter Zeros of Random Analytic Functions we give a recipe for
    generating a slew of (non-Gaussian) random analytic functions whose
    zeros are stationary in the plane, the sphere and the unit disk. We
    make some basic computations on the distribution of zeros that will
    be used later.

-   In Chapter Zeros of Random Analytic Functions we recall the notion
    of a determinantal point process, and characterize the stationary
    determinantal point processes in the three fundamental domains. Of
    these the planar ones are known to be (limits of) the distribution
    of eigenvalues of certain random matrices (the Ginibre ensemble)
    while the processes on the sphere and disk are new (these processes
    themselves have been considered before in caillol , but an
    independent probabilistic meaning was not known).

-   In Chapter Zeros of Random Analytic Functions , we present an
    evocative analogy which suggests that the determinantal point
    processes on the sphere and the disk, introduced in Chapter Zeros of
    Random Analytic Functions , are in fact the singular points of
    certain random matrix-valued analytic functions that were introduced
    in Chapter Zeros of Random Analytic Functions .

-   In Chapter Zeros of Random Analytic Functions we prove that the
    stationary determinantal processes on the sphere introduced in
    Chapter Zeros of Random Analytic Functions are the singular points
    of the random matrix analytic function @xmath (in this case, more
    simply, the eigenvalues of @xmath ).

-   In Chapter Zeros of Random Analytic Functions we give partial proof
    that the determinantal processes on the disk introduced in Chapter
    Zeros of Random Analytic Functions are the singular points of the
    random matrix analytic function @xmath .

-   In Chapter Zeros of Random Analytic Functions we show asymptotic
    normality for smooth statistics applied to the zeros of random
    analytic functions introduced in Chapter Zeros of Random Analytic
    Functions following a method of Sodin and Tsirelson who showed the
    same for the canonical models of Gaussian analytic functions.

-   In Chapter Zeros of Random Analytic Functions and Chapter Zeros of
    Random Analytic Functions we move away from the line of presentation
    so far, and return to canonical Gaussian analytic functions. We deal
    with two large deviation type problems for zeros of the planar
    Gaussian analytic function, one posed by Yuval Peres, which we solve
    fully and another due to Mikhail Sodin, which we solve partially.

### 2 Basic notions and definitions

#### 2.1 Point processes, Correlation functions

A point process in a locally compact Polish space @xmath is a random
integer-valued positive Radon measure @xmath on @xmath . (Recall that a
Radon measure is a Borel measure which is finite on compact sets.) If
@xmath almost surely assigns at most measure @xmath to singletons, it is
a simple point process; in this case @xmath can be identified with a
random discrete subset of @xmath , and @xmath represents the number of
points of this set that fall in @xmath .

The distribution of a point process can, in most cases, be described by
its correlation functions (also known as joint intensities) w.r.t a
fixed Radon measure @xmath on @xmath .

###### Definition 2.1.

The correlation functions of a point process @xmath w.r.t. @xmath are
functions (if any exist) @xmath for @xmath , such that for any family of
mutually disjoint Borel subsets @xmath of @xmath , and for any
non-negative integers @xmath

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

where @xmath .

###### Remark 2.2.

It is a natural question to ask for conditions that guarantee the
existence of correlation functions and conditions under which they
determine the distribution of the point process. Such conditions do
exist, see Lenard’s lenard1 , lenard2 , lenard3 or the survey by
Soshnikov sos1 . But the conditions are too complicated and not relevant
for our purposes. In any case, when the joint distribution of @xmath is
determined by its moments, the correlation functions determine the
distribution of @xmath .

###### Remark 2.3.

For overlapping sets, the situation is more complicated. Restricting
attention to simple point processes, @xmath is not the intensity measure
of @xmath , but that of @xmath , the set of ordered @xmath -tuples of
distinct points of @xmath . Indeed, ( 2.1 ) implies (see lenard1 ;
lenard2 ; pervir ) that for any Borel set @xmath we have

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

Assuming that @xmath is simple, the correlation functions may be
interpreted as follows:

-   If @xmath is finite and @xmath then @xmath is the probability that
    @xmath .

-   If @xmath is open in @xmath and @xmath , if @xmath exist and are
    continuous, then

      -- -------- -- -------
         @xmath      (2.3)
      -- -------- -- -------

    Conversely, if for every @xmath , the right hand side of ( 2.3 )
    exists and is continuous in @xmath , @xmath , then it is the @xmath
    -point correlation functions of @xmath .

For us @xmath will always be an open subset of the plane (or the sphere
@xmath ) and @xmath will be a simple point process. @xmath may always be
taken to be the Lebesgue measure on @xmath , but we often find it
convenient to use some other measure that is mutually absolutely
continuous with the Lebesgue measure.

#### 2.2 Complex Gaussian distribution

A standard complex Gaussian is a complex-valued random variable with
probability density @xmath w.r.t the Lebesgue measure on the complex
plane. Equivalently, one may define it as @xmath , where @xmath and
@xmath are i.i.d. N( @xmath ) random variables.

Let @xmath , @xmath be i.i.d. standard complex Gaussians. Let @xmath
denote the column vector @xmath . Then if @xmath is an @xmath matrix,
@xmath is said to be an @xmath -dimensional complex Gaussian vector with
mean @xmath (an @xmath vector) and covariance @xmath (an @xmath matrix).
We denote its distribution by @xmath .

Here are some basic properties of complex Gaussian random variables.

-   If @xmath is a complex Gaussian, its distribution is determined by
    @xmath and @xmath . All moments of the form

      -- -------- --
         @xmath   
      -- -------- --

    vanish. This is the case even for @xmath .

-   If @xmath is a standard complex Gaussian, then @xmath and @xmath are
    independent, and have exponential distribution with mean @xmath and
    uniform distribution on the circle @xmath , respectively.

-   If @xmath , @xmath are i.i.d. @xmath , then by an easy application
    of Borel-Cantelli,

      -- -------- -- -------
         @xmath      (2.4)
      -- -------- -- -------

    In fact, equation ( 2.4 ) is valid for any i.i.d. sequence of
    complex-valued random variables @xmath , such that @xmath . Equation
    ( 2.4 ) is useful to compute the radii of convergence of random
    power series with independent coefficients.

-    Wick Expansions: The Wick or the Feynman diagram expansion is an
    expansion of @xmath functions of a Gaussian measure in an
    orthonormal basis consisting of polynomials of the underlying
    Gaussians. Following the presentation in the book by Janson janson ,
    we state the essential facts in the limited context that we shall
    need later. More details and complete proofs of the assertions can
    be found in janson .

    Let @xmath be i.i.d. standard complex Gaussians. Consider the
    collection of all monomials @xmath in these @xmath variables, and
    orthonormalise them by projecting the polynomials of degree @xmath
    on the orthogonal complement of the polynomials of degree @xmath .

    This procedure is the same as applying Gram-Schmidt to the monomials
    after arranging them in increasing order of the degree (how we order
    monomials of the same degree is immaterial because distinct
    monomials of the same degree are clearly orthogonal). Thus we get an
    orthonormal basis of all square integrable functions of the @xmath
    s, and the basis elements, termed Wick powers , are denoted by

      -- -------- --
         @xmath   
      -- -------- --

    the equality a consequence of the independence of @xmath s.

    These Wick polynomials are known explicitly (see janson )-

      -- -------- --
         @xmath   
      -- -------- --

    although this is not particularly important to us. It is quite well
    known that products of random variables that are jointly Gaussian
    can be described by summing over the weights of certain
    combinatorial entities. There is a similar formula (known as Wick
    formula or Feynman diagram formula) for expectation of product of
    Wick powers. We shall only need the following special case.

    Wick/Feynman diagram formula: Let @xmath have a complex Gaussian
    distribution with mean zero. Then

      -- -------- -- -------
         @xmath      (2.5)
      -- -------- -- -------

    where the sum is over all complete Feynman diagrams @xmath without
    self interaction (henceforth we shall just say Feynman diagram ). To
    define this, consider a collection of @xmath vertices with @xmath of
    the vertices labeled @xmath and @xmath of the vertices labeled
    @xmath , for @xmath . All the vertices labeled @xmath are also
    supposed to be distinguishable although we shall not introduce any
    more notation to distinguish them. Now, each @xmath is a matching of
    these vertices (a subgraph in which each vertex has degree @xmath ),
    such that each edge in @xmath connects a vertex labeled @xmath to a
    vertex labeled @xmath for some @xmath .

    The value @xmath of the diagram is the product of the weights of all
    the edges in @xmath , and the weight of an edge joining a vertex
    labeled @xmath to a vertex labeled @xmath ( @xmath ) is @xmath .

    ###### Example 2.4.
    Let @xmath . Then we must consider Feynman diagrams on the labels
    @xmath with @xmath vertices labeled @xmath , @xmath vertices labeled
    @xmath , @xmath vertices labeled @xmath and @xmath vertices labeled
    @xmath . Since a Feynman diagram (in our terminology as explained
    above) must connect @xmath s to @xmath s and vice-versa, and must
    give every vertex degree one, there are no Feynman diagrams unless
    @xmath and @xmath , in which case there are @xmath such diagrams.
    Thus

      -- -------- --
         @xmath   
      -- -------- --

#### 2.3 Gaussian analytic functions

Endow the space of analytic functions on a region @xmath with the
topology of uniform convergence on compact sets. This makes it a
complete separable metric space which is the standard setting for doing
probability theory (To see completeness, if @xmath is a Cauchy sequence,
then @xmath converges uniformly on compact sets to some continuous
function @xmath . Then it is easy to see that @xmath must be analytic
because its integral on any closed contour is zero since @xmath and the
latter vanishes for every @xmath , by analyticity of @xmath ).

###### Definition 2.5.

Let @xmath be a random variable taking values in the space of analytic
functions on a region @xmath . We say @xmath is a Gaussian analytic
function (GAF) on @xmath if @xmath has a mean zero complex Gaussian
distribution for every @xmath .

It is easy to see the following properties of GAFs.

-   @xmath are jointly Gaussian, i.e., the joint distribution of @xmath
    and finitely many derivatives of @xmath at finitely many points,

      -- -------- --
         @xmath   
      -- -------- --

    has a (mean zero) complex Gaussian distribution.

-   The distribution of a Gaussian analytic function is determined by
    its covariance kernel @xmath denoted by @xmath or just @xmath if
    there is no ambiguity as to which @xmath is under consideration.

#### 2.4 Stationary zero sets of Gaussian analytic functions

Our interest is in the zero set of a random analytic function. Unless
one’s intention is to model a particular physical phenomenon by a point
process, there is one criterion that makes some point processes more
interesting than others, namely, stationarity under a large group of
transformations (stationarity of a random process means invariance of
its distribution under a group action. It is also called invariance ,
especially when the stationarity is in “space” rather than “time”, but
we use both terms interchangeably). There are three particular two
dimensional domains on which the group of conformal automorphisms act
transitively (There are two others that we do not consider here, the
cylinder or the punctured plane, and the two dimensional torus). We
introduce these domains now.

-    The Complex Plane @xmath : The group of transformations

      -- -------- -- -------
         @xmath      (2.6)
      -- -------- -- -------

    where @xmath and @xmath , is nothing but the Euclidean motion group.
    These transformations preserve the Euclidean metric @xmath and the
    Lebesgue measure @xmath on the plane.

-    The Sphere @xmath : The group of transformations

      -- -------- -- -------
         @xmath      (2.7)
      -- -------- -- -------

    where @xmath and @xmath , is the group of linear fractional
    transformations mapping @xmath to itself bijectively. These
    transformations preserve the spherical metric @xmath and the
    spherical area measure @xmath . We call it the spherical metric
    because it is the push forward of the usual metric on the sphere
    inherited from @xmath , onto @xmath under the stereographic
    projection, and the measure is the push forward of the spherical
    area measure. The transformations ( 2.7 ) are just the rotations of
    the sphere under this identification with @xmath .

-    The Hyperbolic Plane @xmath : The group of transformations

      -- -------- -- -------
         @xmath      (2.8)
      -- -------- -- -------

    where @xmath and @xmath , is the group of linear fractional
    transformations mapping the unit disk @xmath to itself bijectively.
    These transformations preserve the hyperbolic metric @xmath and the
    hyperbolic area measure @xmath (this normalization differs from the
    usual one, with curvature @xmath , by a factor of @xmath , but it
    makes the analogy with the other two cases more formally similar).
    This is one of the many models discovered by Poincaré for the
    hyperbolic geometry of Bolyai, Gauss and Lobachevsky (see cfkp for
    an introduction).

Note that in each case, the group of transformations acts transitively
on the corresponding space, i.e., for every @xmath in the domain, there
is a transformation @xmath such that @xmath . This means that in these
spaces every point is just like every other point. Now we introduce
three families of GAFs whose relation to these symmetric spaces will be
made clear in Proposition 2.7 .

In each case, the domain of the random analytic function can be found
from ( 2.4 ). Indeed, ( 2.4 ) implies that when @xmath are i.i.d.
standard complex Gaussians, @xmath has the same radius of convergence as
@xmath .

-    The Complex Plane @xmath : Define for @xmath ,

      -- -------- -- -------
         @xmath      (2.9)
      -- -------- -- -------

    For every @xmath , this is a random analytic function in the entire
    plane.

-    The Sphere @xmath : Define for @xmath ,

      -- -------- -- --------
         @xmath      (2.10)
      -- -------- -- --------

    For every @xmath , this is a random analytic function on @xmath with
    a pole at @xmath (i.e., it is a polynomial).

-    The Hyperbolic Plane @xmath : Define for @xmath ,

      -- -------- -- --------
         @xmath      (2.11)
      -- -------- -- --------

    For every @xmath , this is a random analytic function in the unit
    disk @xmath .

###### Remark 2.6.

Although we wrote ( 2.9 ) for every @xmath , they are identical up to a
scaling of the complex plane. However, the functions in ( 2.10 ) and (
2.11 ) are truly different for different @xmath , i.e., there is no
transformation of the @xmath and @xmath , that makes @xmath and @xmath
the same, for @xmath . This is particularly obvious for the sphere,
because @xmath then denotes the number of zeros of @xmath .

We just quote the following proposition from ST1 . (The proof is
contained in the proof of Proposition 3.1 ). These random analytic
functions were discovered in several stages and (partially) by several
authors. The main contributions are due to Bogomolny, Bohigas and
Leboeuf bbl92 and bbl96 , Kostlan kostlan93 , Shub and Smale shubsmale .
Some of them are natural generalizations (to complex coefficients) of
random polynomials studied by Mark Kac in his founding papers starting
with kac . The special case @xmath , in the unit disk was derived also
by Diaconis and Evans diaeva as the limit of the logarithmic derivative
of characteristic polynomials of random unitary matrices. The uniqueness
in Proposition 2.7 also was perhaps known, but a much stronger form of
uniqueness (that the first intensity of zeros of any Gaussian analytic
function determines the distribution of the Gaussian analytic function
itself, up to multiplication by arbitrary deterministic non-vanishing
analytic functions) was found by Sodin sodin .

###### Proposition 2.7.

The zero sets of the GAF @xmath in equations ( 2.9 ), ( 2.10 ) and (
2.11 ) are invariant (in distribution) under the transformations defined
in equations ( 2.6 ), ( 2.7 ) and ( 2.8 ) respectively. This holds for
every allowed value of the parameter @xmath , namely @xmath for the
plane and the disk and @xmath for the sphere.

Moreover, these are the only Gaussian analytic functions (up to
multiplication by deterministic non vanishing analytic functions) with
stationary zero sets in these domains.

## Chapter \thechapter Stationary zero sets of random analytic functions

As we saw in Proposition 2.7 , on each of the three domains @xmath ,
there is a one parameter family of Gaussian analytic functions whose
zero sets are stationary under the corresponding group of isometries.
Moreover, these are the only Gaussian analytic functions on these
domains with these properties. Indeed Hannay hannay likens the
uniqueness of the Gaussian analytic function in ( 2.9 ) to that of the
Poisson process or the thermal blackbody radiation.

Here we stick to the three domains @xmath and ask for random analytic
functions whose zero sets are stationary. By Proposition 2.7 , we must
necessarily seek among non-Gaussian analytic functions. A natural idea
might by to replace i.i.d. Gaussians in the coefficients by i.i.d.
complex-valued random variables from some other distribution. However,
these seem difficult to analyse. Gaussian analytic functions have the
nice property that the evaluations of the function and its derivatives
are all Gaussian with distributions that we can explicitly work with and
this fails in other cases. In fact we do not know of another example of
a power series with i.i.d. coefficients whose zero set is stationary (on
any of these three domains). We resolve this deadlock by constructing
non-Gaussian analytic functions using Gaussian analytic functions as
building blocks.

### 3 A recipe for stationary zero sets of random analytic functions

Let @xmath be a (non-random) homogeneous polynomial in @xmath variables
with complex coefficients and let @xmath be any Gaussian analytic
function (not necessarily one of the canonical models defined in Section
2.4 ). Then if @xmath are i.i.d. copies of @xmath , then @xmath is a
random analytic function on the same domain as @xmath .

###### Proposition 3.1.

Let @xmath be a homogeneous polynomial of degree @xmath in @xmath
variables with complex coefficients, and let @xmath be one of the
canonical models of Gaussian functions in ( 2.9 ), ( 2.10 ) or ( 2.11 ).
If @xmath , @xmath are i.i.d. copies of @xmath , then the zero set of
the random analytic function

  -- -------- --
     @xmath   
  -- -------- --

is stationary under the same group of isometries as the zero set of
@xmath .

###### Proof.

First we recall the proof of invariance of the zero set of the Gaussian
analytic functions in ( 2.9 ), ( 2.10 ) and ( 2.11 ).. Fix an isometry
@xmath of @xmath (given in ( 2.6 ), ( 2.7 ) and ( 2.8 )). In each of the
three cases, there is a deterministic non-vanishing function @xmath such
that

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

where in fact

  -- -------- --
     @xmath   
  -- -------- --

Note that the equality in ( 3.1 ) is for the entire process, not just
for a fixed @xmath . Therefore, the zero set of @xmath is invariant in
distribution under the action of @xmath . (To prove equation ( 3.1 ),
just compute the covariance kernels of the Gaussian processes on the
left and right hand sides).

Coming back to @xmath , we see that

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

This implies that the zero set of @xmath is invariant in distribution
under the action of @xmath . ∎

This is a very simple observation, but note that while @xmath is built
in a simple manner out of copies of @xmath , the zero set of @xmath is
by no means a simple transformation of the zero sets of @xmath (except
in trivial cases such as when @xmath ). Thus the sets of zeros that we
get are genuinely new point processes, but have the advantage of being
based on Gaussian analytic functions, and therefore amenable to
analysis. We illustrate this next, by computing the first and second
correlations (joint intensities) for the zeros of @xmath . The tool that
we use to study functions such as @xmath is the Wick expansion,
suggested to us by Mikhail Sodin (see the paper by Sodin and Tsirelson
ST1 for a use of Wick expansions in the context of Gaussian analytic
functions). We call random analytic functions of the kind described in
Proposition 3.1 as polygafs .

### 4 How to study the zeros of a polygaf?

If @xmath is any analytic function (not random) on @xmath , let @xmath
denote the counting measure, with appropriate multiplicities, on the
zeros of @xmath . Then,

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

in the sense of distributions. This just means that for any @xmath ,

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

where @xmath is the Lebesgue measure. Therefore when @xmath is any
random analytic function, understanding the distribution of the zero set
depends on being able to do computations with @xmath (When @xmath is
Gaussian, there are other approaches to studying the zero set of @xmath
, but it appears that the approach outlined here is the only one that is
equally convenient for our more general setting. The other methods make
use of the probability density of @xmath evaluated at several points in
the domain etc, which are not available to us here).

Now from ( 4.2 ), if @xmath , @xmath are smooth functions with disjoint
supports in @xmath , we get that

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

In the last line we integrated by parts.

In 2.1 we defined the correlation functions in terms of the moments of
the joint counts of the number of points falling in several regions.
Fixing @xmath distinct points @xmath in @xmath and letting @xmath be a
bump function in a small neighbourhood of @xmath , by elementary measure
theoretical arguments one can deduce that the @xmath -point correlation
function @xmath of the zero set of @xmath , w.r.t Lebesgue measure is
given by

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

for distinct @xmath .

###### Remark 4.1.

Hammersley hamm gave a formula for the correlation functions of zeros of
random polynomials in terms of the distribution of the coefficients. (
4.3 ) is an alternative way of expressing the same. In this form, for
@xmath it is sometimes called Edelman-Kostlan formula.

The way to analyse @xmath is via Wick expansions that were outlined in
Chapter Zeros of Random Analytic Functions .

###### Example 4.2.

The particular example of Wick expansions that is of interest to us is
the following: Let @xmath be a homogeneous polynomial in @xmath
variables with complex coefficients. If @xmath are i.i.d. @xmath random
variables, then

  -- -------- --
     @xmath   
  -- -------- --

for every finite @xmath . Hence we can expand @xmath in Wick powers as

  -- -- -- -------
           (4.4)
  -- -- -- -------

where @xmath , @xmath and

  -- -------- --
     @xmath   
  -- -------- --

and the equality in ( 4.4 ) is in the @xmath sense (it could be better,
of course).

We record two observations for later use.

-   @xmath for all @xmath , because @xmath are also i.i.d. @xmath .

-   @xmath unless @xmath , where @xmath . To see this, note for any
    @xmath with @xmath , @xmath are also i.i.d. @xmath and hence, by the
    homogeneity of @xmath , it is also true that @xmath . Therefore,
    from the equation above for @xmath , we see that @xmath , which
    cannot be true unless @xmath or @xmath .

### 5 Distribution of the zero set of a polygaf

Now let @xmath be i.i.d. copies of @xmath , a Gaussian analytic function
on a domain @xmath (not necessarily one of the canonical GAFs on the
plane, sphere or disk). As before @xmath is a homogeneous polynomial.

Define @xmath . If @xmath is the covariance kernel of @xmath , then set

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath is the degree of @xmath . Then from ( 4.4 ) we
can write,

  -- -------- --
     @xmath   
  -- -------- --

We work with @xmath rather than @xmath , because @xmath are i.i.d
standard Gaussians for any @xmath , and so we can directly use the Wick
formulas that we stated in the previous chapter.

First Intensity: The first intensity (or @xmath -point correlation
function) as given by ( 4.3 ) is

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

Therefore,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

from ( 4.4 ). Therefore we obtain

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

As a special case, set @xmath , @xmath in formula ( 5.2 ) to deduce that
the intensity of zeros of @xmath is @xmath (This is known as the
Edelman-Kostlan formula). Thus we see that the intensity of zeros of
@xmath is @xmath times the intensity of zeroes of @xmath . This simple
relationship between the intensities is surprisingly not quite obvious
from the definition.

Two point Correlations: Again from ( 4.3 ) we get the @xmath -point
correlation. It is easy to see that

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

From ( 4.4 ) the right hand side can be written as

  -- -------- --
     @xmath   
  -- -------- --

This is precisely the situation elucidated in Example 2.4 . Thus, only
terms with @xmath survive.

Now make use of the observations made earlier- (1) @xmath , and (2)
@xmath unless @xmath . Grouping together terms by @xmath , we get,

  -- -------- -- -------
     @xmath      (5.3)
  -- -------- -- -------

where @xmath .

###### Remark 5.1.

Equation ( 5.3 ) has the appealing feature that the effects of the two
ingredients of @xmath , namely the polynomial @xmath and the Gaussian
analytic function @xmath , are clearly separated. @xmath depends only on
@xmath while @xmath depends only on @xmath . This observation is
crucially used in the next chapter, when we compute the correlations for
specific polynomials @xmath . One can write analogous but more
complicated expressions for higher correlations, with @xmath , @xmath
applied to a sum over Feynman diagrams.

## Chapter \thechapter Stationary determinantal point processes

In this chapter we move away from random analytic functions and talk
about a different class of point processes. In the next chapter, we
return to zeros of random analytic functions and show that there are
point processes in the intersection of the two classes.

One of the main qualitative properties of zero sets of random analytic
functions is that they have the property of “repulsion”, also called
“negative correlation”, at short ranges. This terminology is a little
misleading because correlations are never negative! The precise meaning
of negative correlations is that @xmath for @xmath that are sufficiently
close. There is another class of point processes that has this repulsion
property at all distances in a very strong sense. These point processes
were introduced by Macchi mac and are known as Determinantal (Fermionic)
point processes . See Figure 1 for a visual comparison of zeros and
eigenvalues with a Poisson process.

###### Definition 5.2.

A point process @xmath on @xmath is said to be a determinantal process
with kernel @xmath , if it is simple (i.e., there are no coincident
points almost surely) and its correlation functions w.r.t a measure
@xmath satisfy:

  -- -------- -- -------
     @xmath      (5.4)
  -- -------- -- -------

for every @xmath and @xmath . We shall always assume that @xmath is the
projection kernel of a closed subspace of @xmath , i.e., that @xmath for
a sequence of functions (there may be infinitely many of them) @xmath
orthonormal in @xmath . The distribution is determined by giving the
kernel @xmath or the Hilbert space on which it is a projection.

###### Remark 5.3.

This definition may look artificial at first sight, but the motivation
comes from quantum mechanics, where the probability densities are given
by the absolute square of a complex-valued function called the
amplitude. If @xmath (assumed to be orthogonal) are single particle wave
functions of electrons, the most natural @xmath -particle wave function
is not @xmath (as would have been the case for “independence”) because
firstly it has no symmetry in @xmath , and secondly it shows no
properties such as repulsion. Hence the idea is to anti-symmetrize this,
to get @xmath (This is for Fermions. For Bosons, one symmetrizes and
gets the permanent of @xmath , but that is another story). The
probability density is given by the absolute square of this, which can
be written as @xmath , with @xmath . To generalize this notion to
infinite particle systems, it is necessary to formulate the definition
in terms of correlation functions as done in Definition 5.2 . For
correlation functions to be non-negative in ( 5.4 ) a natural assumption
is to take @xmath to be a Hermitian non-negative definite kernel. Then
it turns out that @xmath is a necessary condition for there to exist a
determinantal process with kernel @xmath (see sos1 or hkpv ). And then,
any such process can be expressed as a mixture of determinantal
processes with projection kernels (see takshi1 , takshi2 , hkpv ). The
rank of the projection is the number of points in the process. Observe
that @xmath is negative. More general inequalities like this are a clear
consequence of the determinantal form of the correlation functions.

###### Remark 5.4.

Another justification for this definition is the huge number of
instances of determinantal processes that have arisen so far in random
matrix theory and combinatorics, many of them predating the definition.
To name a few,

-   Non-intersecting random walks on the line (Karlin and McGregor KarMg
    , johan ).

-   Eigenvalues of a random unitary matrix chosen according to the Haar
    measure (Dyson dys1 ). (There are many more random matrix examples).

-   Subset of edges of a finite graph present in a uniformly chosen
    spanning tree (Burton and Pemantle burpe ).

-   (An encoding of) Young diagrams sampled from the poissonised
    Plancherel measure of the symmetric group (Borodin, Okounkov and
    Olshanski borokools ).

The extensive survey of Soshnikov sos1 gives many more examples and
details.

While determinantal processes on @xmath or @xmath have been studied
extensively because they arise in random matrix theory and
combinatorics, in two dimensions they seem to have been largely
untouched.

To get a determinantal point process is trivial. Just take a reproducing
kernel Hilbert space @xmath of functions on @xmath , and let @xmath be
the reproducing kernel of @xmath . Then there exists a determinantal
point process with kernel @xmath (trivial when dim( @xmath ) @xmath ,
otherwise it can be constructed by taking limits of finite dimensional
ones. See sos1 or hkpv for details). However to deserve our attention,
the process must have some attractive features in addition to its
determinantal nature. We adhere to two guiding principles.

-   That the process be invariant in distribution (i.e., stationary)
    under a rich group of transformations.

-   That the process arise in a natural way probabilistically.

Although imprecise, the first principle suggests that we consider the
same three domains @xmath . And since we would ultimately want to relate
these to zeros of random analytic functions, we consider Hilbert spaces
of analytic functions on these spaces. We show that the Hilbert spaces
that give rise to stationary determinantal processes are precisely the
well known Bargmann-Fock spaces of analytic functions. But before going
into this, we should point out that these determinantal processes were
already studied by Caillol caillol under the name “One component plasma
on the sphere” (the two-component version was studied by Forrester,
Jancovici and Madore forjanmad ), and Jancovici and Téllez jantell from
the point of view of constructing Coulomb gases on these spaces. We
arrived at these processes prompted by a question of Bálint Virág as to
what natural determinantal processes can be defined on the two
dimensional sphere. As we remarked earlier, one can define a
determinantal process (which can be regarded as Coulomb gas at a
particular temperature @xmath ) by choosing one’s favourite Hilbert
space with a kernel. What is new here is that we show that these are
unique in a certain sense, and most importantly, we show in the next
chapter how to get these determinantal processes as zeros of random
analytic functions.

###### Theorem 5.5.

Let @xmath be one of @xmath and let @xmath be an arbitrary radially
symmetric Radon measure on @xmath . Let @xmath be complex analytic
functions on @xmath and belong to @xmath . Then the determinantal
process with kernel

  -- -------- -- -------
     @xmath      (5.5)
  -- -------- -- -------

is invariant in distribution under the corresponding group of isometries
if and only if it is one of the following.

-   @xmath , @xmath , @xmath , @xmath , where @xmath , and the kernel is

      -- -------- -- -------
         @xmath      (5.6)
      -- -------- -- -------

    The Hilbert space is the space of analytic functions in @xmath . We
    call this process Det- @xmath - @xmath .

-   @xmath , @xmath , @xmath , @xmath , where @xmath , and the kernel is

      -- -------- -- -------
         @xmath      (5.7)
      -- -------- -- -------

    The Hilbert space is the space of analytic functions in @xmath . We
    call this process Det- @xmath - @xmath .

-   @xmath , @xmath , @xmath , @xmath , where @xmath , and the kernel is

      -- -------- -- -------
         @xmath      (5.8)
      -- -------- -- -------

    The Hilbert space is the space of analytic functions in @xmath . We
    call this process Det- @xmath - @xmath .

###### Remark 5.6.

Note the similarity to the classification of Gaussian analytic function
with stationary zeros in Proposition 2.7 . Just as there, here too, Det-
@xmath processes are all identical up to scale, whereas the Det- @xmath
and Det- @xmath are genuine one parameter families of point processes.

There are (at least) two ways of using a positive definite kernel in
probability theory. One is to use it as the covariance kernel of a
Gaussian process and another is to use it as the kernel of a
determinantal process (it has to be a projection kernel for the latter).
We are not aware of any probabilistic connection between the two.

When we use the reproducing kernels of the Bargmann-Fock spaces as
covariance kernels of Gaussian processes, we get the Gaussian analytic
functions introduced in ( 2.9 ), ( 2.10 ) and ( 2.11 ). When we use them
as kernels for determinantal processes we get stationary point processes
in these domains.

###### Proof of Theorem 5.5.

Let

  -- -------- --
     @xmath   
  -- -------- --

Then @xmath is the unique invariant measure (up to multiplication by a
constant) on @xmath .

If @xmath is a determinantal process with kernel @xmath on @xmath , with
distribution invariant under the corresponding isometry group, then the
first intensity of the process, @xmath must be equal to @xmath for some
@xmath .

Express the correlation functions of @xmath w.r.t the measure @xmath
instead of the Lebesgue measure. Then the kernel becomes

  -- -------- --
     @xmath   
  -- -------- --

Invariance of the second correlation function implies that

  -- -------- -- -------
     @xmath      (5.9)
  -- -------- -- -------

for every isometry @xmath of @xmath , where by @xmath we mean that it
does not depend on @xmath .

The idea is this. We differentiate equation ( 5.9 ) w.r.t @xmath and
equate to zero. The derivatives w.r.t @xmath can be written as
derivatives w.r.t @xmath . That gives us differential equations for
@xmath that are easy to solve.

Firstly, fix any @xmath such that @xmath . Without loss of generality
take @xmath . Then there is a neighbourhood @xmath of @xmath in the
Complex plane such that if @xmath , and @xmath is close to identity,
then @xmath . Let @xmath be the disk of radius @xmath centered at @xmath
. Then @xmath cannot intersect both the positive and negative parts of
real axis because @xmath is convex and does not contain @xmath .
Moreover @xmath and @xmath intersect the real line at the same points.
Therefore by removing the positive or the negative half line, we can
define a continuous branch of logarithm on @xmath . Henceforth “ @xmath
” will denote this function.

Taking logarithms in Equation( 5.9 ) we get

  -- -------- -- --------
     @xmath      (5.10)
  -- -------- -- --------

is equal to const @xmath , not depending on @xmath . To differentiate
w.r.t @xmath we parameterize it with complex numbers as follows.

-   Complex plane Write @xmath , where @xmath , @xmath .

-   Sphere Write @xmath , where @xmath , @xmath .

-   Disk Write @xmath , where @xmath , @xmath .

Let us deal with the planar case first.

Complex plane Write @xmath , where @xmath . Then for @xmath is small
enough, if @xmath and @xmath , then @xmath . Apply @xmath to equation (
5.10 ) and evaluate at @xmath . We get

  -- -------- -------- -------- --
     @xmath   @xmath            
              @xmath   @xmath   
  -- -------- -------- -------- --

Where @xmath . This is well defined for @xmath and is analytic in @xmath
. Applying @xmath to @xmath we deduce that @xmath . By expanding Q
locally as a power series in @xmath , and observing the symmetry @xmath
, we conclude that @xmath for some @xmath holomorphic on @xmath . Then

  -- -- --
        
  -- -- --

where @xmath and @xmath is a constant.

Now again consider ( 5.10 ) and apply @xmath to it. We get

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Since this has to hold for every @xmath , we must have @xmath . That
means that @xmath for some @xmath . Therefore,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Making a change of variables we get @xmath . Then @xmath has to be
positive. This concludes the planar case.

Sphere In this case, we again differentiate Equation( 5.9 ) w.r.t @xmath
and their conjugates. However @xmath depends on the parameters and their
conjugates as well, and that makes the equation longer. A simplification
is obtained by noting the following:

Let @xmath be analytic in @xmath . Then with @xmath ,

  -- -------- -- --------
     @xmath      (5.11)
  -- -------- -- --------

Here @xmath denote the derivatives w.r.t the first and second arguments.

Apply @xmath to Equation ( 5.10 ) and evaluate at @xmath . Then ( 5.11 )
yields,

  -- -------- --
     @xmath   
  -- -------- --

where, this time @xmath . But all the considerations that applied to
@xmath in the Planar case also apply here and we get @xmath (perhaps
after a change of variables). Now @xmath will have to be a positive
integer (because integrating @xmath over the whole space should give
@xmath , where @xmath is the total number o points in the process).

Disk Analogous to the spherical case, here we observe that for any
@xmath analytic in @xmath and anti-analytic in @xmath ,

  -- -------- -- --------
     @xmath      (5.12)
  -- -------- -- --------

Applying @xmath to ( 5.10 ) and using equation ( 5.12 ) gives us

  -- -------- --
     @xmath   
  -- -------- --

where, this time @xmath . As before this leads us to @xmath and @xmath
will have to be positive.

This completes the proof of the theorem. ∎

## Chapter \thechapter Random matrix-valued analytic functions

In Chapter Zeros of Random Analytic Functions , we saw that by choosing
a Gaussian analytic function @xmath with a stationary zero set and a
(non-random) homogeneous polynomial @xmath , we could construct a random
analytic function with stationary zeros. There are two complementary
questions that arise naturally.

1.  Can one study these random analytic functions in this generality
    without having to appeal to special @xmath and @xmath ?

2.  Are there particular examples of @xmath and @xmath that are somehow
    special?

The answer to both these questions is yes. Regarding the first question,
we already saw in Chapter Zeros of Random Analytic Functions that the
correlation functions of the zero set can be computed in a general
fashion. We shall use these computations in Chapter Zeros of Random
Analytic Functions to prove asymptotic normality for the zero sets in
general. In the current chapter and the next two, we answer the second
question and show that zeros of random analytic functions sometimes (but
far from frequently, let alone always) turn out to be determinantal
point processes.

### 6 Determinantal processes that are zeros of RAFs: Known results

We remarked earlier that the focus in random matrix theory has been on
Hermitian random matrices. In the preface to his book “Random matrices”,
Mehta mehta says “The theory of non-Hermitian random matrices, though
not applicable to any physical problems, is a fascinating subject and
must be studied for its own sake. In this direction an impressive step
[has been taken by] Ginibre …” Ginibre found the exact distribution of
eigenvalues of three (two, strictly speaking) ensembles of non-Hermitian
random matrices. We quote the one that is relevant to us.

###### Theorem 6.1 (Ginibre(1965) gin ).

Let @xmath be an @xmath matrix with i.i.d. standard complex Gaussian
entries. Then the eigenvalues of @xmath have density

  -- -------- -- -------
     @xmath      (6.1)
  -- -------- -- -------

Equivalently, one may say that the eigenvalues of @xmath form a
determinantal point process with kernel

  -- -------- -- -------
     @xmath      (6.2)
  -- -------- -- -------

w.r.t the reference measure @xmath . The corresponding Hilbert space
@xmath .

Despite the enthusiastic response, as shown by Mehta’s quote above,
there do not seem to be any significant exact results beyond Ginibre’s.
The following beautiful result of Peres and Virág pervir seems to be the
next such.

###### Theorem 6.2 (Peres and Virág(2003) pervir ).

Let @xmath be the random power series whose coefficients are i.i.d.
standard complex Gaussians (this is the case @xmath in ( 2.11 ). Then
the zeros of @xmath form a determinantal point process on the unit disk
@xmath with the kernel (the Bergman kernel of the unit disk)

  -- -------- --
     @xmath   
  -- -------- --

w.r.t the reference measure @xmath on @xmath . The corresponding Hilbert
space @xmath is the space of all analytic functions in @xmath .

###### Remark 6.3.

Observing that Theorem 6.2 identifies the distribution of zeros of the
Gaussian analytic function (with @xmath ) defined in ( 2.11 ) as being
Det- @xmath (recall the definition of Det- @xmath from ( 5.8 )), one is
tempted to guess that the Gaussian analytic functions defined in ( 2.9 )
( 2.10 ) and ( 2.11 ) might have zeros distributed like the Det- @xmath
, Det- @xmath and Det- @xmath (defined in ( 5.6 ), ( 5.7 ) and 5.8 ),
respectively). However these canonical Gaussian analytic functions do
not have determinantal zero sets. Indeed, it was observed by Peres and
Virág in their paper that these zero sets do not have negative
correlations at large distances and hence, cannot be determinantal).
Therefore this beautiful might-have-been story is completely false!
Nevertheless, to quote Einstein, “The Lord is subtle, but not
malicious”. In the next section we shall see how a completely different
but equally compelling picture might well be true.

### 7 Determinantal processes that are zeros of RAFs: An analogy

First let us list all the Gaussian analytic functions whose zero sets we
know to be determinantal. This includes Theorem 6.2 and two trivial
cases (a one-point point process is always determinantal!).

@xmath @xmath : One zero, with standard complex Gaussian distribution on
@xmath . @xmath in @xmath .

@xmath @xmath : One zero, distributed uniformly on @xmath upon
stereographic projection from the plane. @xmath in @xmath

@xmath @xmath : Peres and Virág pervir : Infinitely many zeroes in the
disk. A determinantal point process with kernel @xmath w.r.t Lebesgue
measure on the unit disk. Equivalently, @xmath in @xmath .

Note that Ginibre’s result (Theorem 6.1 ) can be seen as regarding the
zeros of the random analytic function @xmath , which can be thought of
as a matrix version of the first of the above examples. This suggests
that we consider the matrix versions of the other two, i.e., we look at

-   @xmath , where @xmath are @xmath independent matrices with i.i.d.
    standard complex Gaussian entries.

-   @xmath , where @xmath are independent @xmath matrices with each one
    having i.i.d. complex Gaussian entries.

The analogy strongly suggests that the solutions to these equations
should give us the determinantal point processes corresponding to the
Bargmann-Fock spaces on the sphere and the unit disk (but only for
integer values of the parameter, since the size of the matrix, namely
@xmath , is discrete). Before going into the statements and proofs, we
make some big-picture remarks and connect these objects to the random
analytic functions studied in Chapter Zeros of Random Analytic Functions
.

###### Remark 7.1.

Note that here we are looking at the set of @xmath for which a random
matrix-valued analytic function ( @xmath or @xmath ) becomes singular.
This concept is an obvious generalization of both random matrices (which
correspond to the case when the analytic function is linear) and
Gaussian analytic functions (which correspond to the case when the
matrices have size @xmath ). In spite of this natural appeal, the
concept of a random matrix-valued analytic function does not seem to
have been considered in the literature.

One possible reason could be that the focus in random matrix theory has
been almost entirely on eigenvalues in one dimension (real line or the
circle) for physical reasons as well as the strong mathematical
connections with orthogonal polynomials, representation theory etc.
Moreover the eigenvalues have a physical meaning in quantum mechanics.
Note the difficulty of forcing the zeros to lie on the real line, except
by considering eigenvalues of a Hermitian matrix. Nevertheless, the idea
of matrixifying seems to be useful, not only as suggested above with
Gaussian matrix coefficients, but also polynomials with coefficients
that are Haar-distributed unitary matrices.

### 8 Matrix-valued GAFs and polygafs

Now we want to point out the connection with homogeneous polynomials
applied to i.i.d. copies of Gaussian analytic functions (polygafs, that
is).

Consider @xmath . This is the same as applying the homogeneous
polynomial @xmath ” @xmath ” in @xmath variables, to @xmath i.i.d.
copies of the Gaussian analytic function @xmath (which is the case
@xmath in ( 2.10 )).

Similarly @xmath is the homogeneous polynomial @xmath in @xmath
variables, applied to @xmath i.i.d. copies of the Gaussian analytic
function @xmath (which is the case @xmath in ( 2.11 )).

In other words, we have already shown in Proposition 3.1 that the zero
sets of these RAFs are stationary in @xmath . In the next two chapters
we investigate the distributions in greater depth. We shall show that in
the first case ( @xmath ) we do get determinantal processes, whereas in
the second, we show partial results in this direction. The precise
statements of the conjectures are as follows:

###### Conjecture 8.1.

Let @xmath be i.i.d @xmath matrices with i.i.d. standard complex
Gaussian entries. The zeros of @xmath form a determinantal point process
with kernel

  -- -------- --
     @xmath   
  -- -------- --

w.r.t the Lebesgue measure on @xmath . Equivalently, @xmath is the
projection kernel on the subspace of analytic functions in @xmath .

###### Conjecture 8.2.

Let @xmath be i.i.d. @xmath matrices with i.i.d. standard complex
Gaussian entries. The zeros of @xmath form a determinantal point process
on @xmath with kernel

  -- -------- --
     @xmath   
  -- -------- --

w.r.t the Lebesgue measure on @xmath . Equivalently, @xmath is the
projection kernel on the subspace of analytic functions in @xmath .

###### Remark 8.3.

The Det- @xmath process are obtained from Ginibre’s theorem 6.1 by
letting @xmath . So from the point of view of determinantal processes,
our problems can be stated as finding a probabilistic meaning to Det-
@xmath and Det- @xmath .

## Chapter \thechapter Matrix analytic functions on the sphere

In this section we prove Conjecture 8.1 stated at the end of Chapter
Zeros of Random Analytic Functions , i.e., we show that the processes
Det- @xmath - @xmath arise as the singular points of the matrix GAF
@xmath or equivalently, zeros of the polygaf @xmath . Recall that for
@xmath , @xmath is a positive integer (the number of points in the
process). We shall denote it by @xmath in this section.

###### Theorem 8.4.

Let @xmath be independent @xmath random matrices with i.i.d. standard
complex Gaussian entries. Then the set @xmath of zeros of @xmath , i.e.,
the eigenvalues of @xmath , has the distribution Det- @xmath - @xmath .

We need the following lemma.

###### Lemma 8.5.

Let @xmath be a point process on @xmath with @xmath points almost
surely. Assume that the @xmath -point correlation function (equivalently
the density) of @xmath has the form

  -- -------- --
     @xmath   
  -- -------- --

Here @xmath denotes the Vandermonde factor @xmath .

Suppose also that @xmath has a distribution invariant under
automorphisms of the sphere @xmath , i.e., under the transformations
@xmath , for any @xmath satisfying @xmath . Then

  -- -------- -- -------
     @xmath      (8.1)
  -- -------- -- -------

###### Proof of Lemma 8.5.

The claim is that the probability density of the @xmath points of @xmath
(in exchangeable random order) is

  -- -------- --
     @xmath   
  -- -------- --

First let us check that the density @xmath is invariant under the
isometries of @xmath . For this let @xmath , with @xmath satisfying
@xmath . Then,

  -- -------- -- -------
     @xmath      (8.2)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (8.3)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (8.4)
  -- -------- -- -------

From ( 8.2 ),( 8.3 ) and ( 8.4 ), it follows that

  -- -------- -- -------
     @xmath      (8.5)
  -- -------- -- -------

which shows the invariance of @xmath .

Invariance of @xmath means that @xmath with @xmath , and for every
@xmath , we have

  -- -------- -- -------
     @xmath      (8.6)
  -- -------- -- -------

Set @xmath . Then, from ( 8.6 ) and ( 8.5 ), we get

-   @xmath is a function of @xmath , @xmath , only.

-   @xmath for every @xmath .

We claim that these two statements imply that @xmath is a constant. To
see this fix @xmath , @xmath , such that @xmath for @xmath . Let @xmath
. Then @xmath and so @xmath is an isometry of @xmath . From the above
stated properties of @xmath , we deduce,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Take @xmath and @xmath . Then as @xmath , @xmath vary independently over
@xmath , the quantities @xmath vary over the intervals @xmath . By our
choice of @xmath s, this means that

  -- -- --
        
  -- -- --

@xmath is arbitrary, hence @xmath is constant. Therefore @xmath is
constant.

This shows that @xmath . ∎

###### Proof of Theorem 8.4.

Firstly we observe that the distribution of @xmath is invariant under
conformal automorphisms of @xmath . This is a direct consequence of
Proposition 3.1 , with @xmath ” @xmath ” and @xmath . Still we give
another simple direct proof. Let @xmath be such that @xmath . Set

  -- -------- --
     @xmath   
  -- -------- --

Then @xmath and @xmath are i.i.d. matrices with i.i.d. standard complex
Gaussian entries. Therefore the eigenvalue set of @xmath is also
distributed as @xmath .

Now @xmath is the set of solutions to the equation

  -- -------- --
     @xmath   
  -- -------- --

say @xmath . By our observation @xmath also has the same distribution as
the set of solutions to

  -- -------- --
     @xmath   
  -- -------- --

which is precisely @xmath . This proves the invariance.

Now by Lemma 8.5 , it suffices to show that the density of points in
@xmath is of the form given in ( 8.1 ).

We use the following well known matrix decomposition.

Schur decomposition: Any diagonalizable matrix @xmath can be written as

  -- -------- -- -------
     @xmath      (8.7)
  -- -------- -- -------

where @xmath is unitary, @xmath is strictly upper triangular and @xmath
is diagonal. Moreover the decomposition is almost unique, in the
following sense:

@xmath in addition to ( 8.7 ), with @xmath being respectively unitary,
strictly upper triangular, and diagonal, if and only if the element of
@xmath are a permutation of the elements of @xmath , and if this
permutation is identity, then @xmath and @xmath for some @xmath that is
both diagonal and unitary, i.e., @xmath Diag @xmath .

Corresponding to this matrix decomposition ( 8.7 ), Ginibre gin proved
the measure decomposition

Ginibre’s measure decomposition: If @xmath is decomposed as in ( 8.7 ),
with the elements of @xmath in a uniformly randomly chosen order, then

  -- -------- -- -------
     @xmath      (8.8)
  -- -------- -- -------

where @xmath is a finite measure on the unitary group @xmath such that
@xmath for every diagonal unitary @xmath .

Conditional on @xmath , the matrix @xmath has the density

  -- -------- --
     @xmath   
  -- -------- --

w.r.t. the Lebesgue measure on @xmath . From the measure decomposition (
8.8 ) we get the density of @xmath , @xmath , @xmath , @xmath to be

  -- -------- --
     @xmath   
  -- -------- --

(We have omitted constants entirely) Thus the density of @xmath is
obtained by integrating over @xmath . Now write @xmath where @xmath and
@xmath are diagonal matrices with the polar and radial parts of @xmath ,
respectively. Then

  -- -------- --
     @xmath   
  -- -------- --

As stated earlier, @xmath . The elements of @xmath are the same as
elements of @xmath , but multiplied by complex numbers of absolute value
@xmath . Hence, @xmath has the same “distribution” as @xmath . Thus
replacing @xmath by @xmath and @xmath by @xmath we see that the density
of @xmath is of the form @xmath . This is the form of the density
required to apply Lemma 8.5 . Thus we conclude that the eigenvalue
density is

  -- -------- -- -------
     @xmath      (8.9)
  -- -------- -- -------

To compute the constant, note that

  -- -------- --
     @xmath   
  -- -------- --

is an orthonormal set. Projection on the Hilbert space generated by
these functions gives a determinantal process whose kernel is as given
in the definition of @xmath . Writing out the density shows that this is
the same as the eigenvalue density that we have determined. Hence the
constants must match. ∎

## Chapter \thechapter Matrix analytic functions on the disk

Conjecture 8.2 at the end of Chapter Zeros of Random Analytic Functions
asserted that the singular points of the matrix GAF @xmath or
equivalently, the zeros of the polygaf @xmath are distributed as @xmath
. In other words they form a determinantal point process with kernel

  -- -------- --
     @xmath   
  -- -------- --

w.r.t the reference measure @xmath on @xmath . Here @xmath are i.i.d.
@xmath matrices with i.i.d. standard complex Gaussian entries.

As already emphasized, Proposition 3.1 applies to show that the zeros of
the polygaf @xmath are stationary on the unit disk. In this chapter we
shall prove that the first and second correlation functions agree with
those of the determinantal process with kernel @xmath .

First intensity: In the notation of Chapter Zeros of Random Analytic
Functions we have @xmath determinant , a homogeneous polynomial in
@xmath variables @xmath and @xmath i.i.d. copies of the power series
@xmath .

From ( 5.2 ), the intensity of zeros is @xmath . By an elementary
computation this comes out to be @xmath . Since this is the same as
@xmath , it follows that the polygaf under consideration has the same
intensity of zeros as the determinantal process with kernel @xmath (we
omit @xmath in the subscript often).

2-point correlations: We shall prove that

  -- -------- -- --------
     @xmath      (8.10)
  -- -------- -- --------

which shows that the 2-point correlations for the zeros of the polygaf
agree with those of the determinantal process with kernel @xmath .
(Henceforth correlations are expressed w.r.t. the Lebesgue measure).

We prove ( 8.10 ) by using Theorem 8.4 and a change of variables!

Let @xmath , where @xmath are all i.i.d. @xmath . Then @xmath . Consider
the polygaf @xmath . This is precisely the polygaf considered in Chapter
Zeros of Random Analytic Functions . Thus we know from Theorem 8.4
(which we proved by certain matrix decompositions, not at all by using
the formulas for correlation functions of polygafs) that the zeros of
@xmath are determinantal with kernel

  -- -------- --
     @xmath   
  -- -------- --

But the the general formula ( 5.3 ) for two-point correlations of zeros
of polygafs applies to @xmath also and hence it must be the case that

  -- -------- -- --------
     @xmath      (8.11)
  -- -------- -- --------

where @xmath depend only on @xmath , not on the GAFs that we feed in.

From ( 8.11 ) we get

  -- -------- -- --------
     @xmath      (8.12)
  -- -------- -- --------

This is because, both sides of ( 8.12 ) are analytic in @xmath and
anti-analytic in @xmath and moreover, ( 8.11 ) says that the two are
equal on the diagonal @xmath . Thus, by a standard (and elementary) fact
that can be found in any introductory book on several variable complex
analysis, see for example Rudin rud , the two sides must be equal for
all @xmath .

Now make the substitution @xmath to get

  -- -------- -- --------
     @xmath      (8.13)
  -- -------- -- --------

But again using ( 5.3 ) the left hand side is precisely what we get for
@xmath , when @xmath is the determinant of @xmath variables and @xmath
are i.i.d. copies of @xmath , with @xmath being i.i.d. standard complex
Gaussians. And the right side of ( 8.13 ) is @xmath .

We already know that @xmath . Therefore, the two point correlation
@xmath is

  -- -------- --
     @xmath   
  -- -------- --

## Chapter \thechapter Asymptotic Normality

### 9 Background: Results for Gaussian analytic functions

Sodin and Tsirelson ST1 proved asymptotic normality for smooth ( @xmath
) statistics applied to the zeros of the three canonical models of
Gaussian analytic functions in ( 2.9 ), ( 2.10 ) and ( 2.11 ), as the
density parameter @xmath . More precisely, they showed that for any real
valued @xmath , if

  -- -------- --
     @xmath   
  -- -------- --

then,

  -- -------- --
     @xmath   
  -- -------- --

and also that

  -- -------- --
     @xmath   
  -- -------- --

for a constant @xmath that is described explicitly and the same for all
the three geometries. (Note that as @xmath , the variance goes to zero!)
Here @xmath is the invariant measure on @xmath and @xmath is the
invariant Laplacian. In other words

  -- -------- -- -------
     @xmath      (9.1)
  -- -------- -- -------

and

  -- -------- -- -------
     @xmath      (9.2)
  -- -------- -- -------

### 10 Our results: for polygafs

In this article we modify the method of Sodin and Tsirelson to obtain
central limit theorems for smooth statistics of zeros of polygafs. One
point of this exercise is to demonstrate that the random analytic
functions can be studied at the level of generality introduced in
Chapter Zeros of Random Analytic Functions (Recall that polygafs include
matrix GAFs as very special cases).

###### Theorem 10.1.

Let @xmath where,

-   @xmath is a fixed non-random homogeneous polynomial in @xmath
    complex variables,

-   @xmath are i.i.d. GAFs in ( 2.9 ) or ( 2.10 ) or ( 2.11 ),

-   @xmath is a @xmath function on @xmath .

Set

  -- -------- --
     @xmath   
  -- -------- --

Then

  -- -------- --
     @xmath   
  -- -------- --

Moreover

  -- -------- -- --------
     @xmath      (10.1)
  -- -------- -- --------

for a constant @xmath that is described explicitly and is the same for
all the three geometries.

Let @xmath . Let @xmath denote a standard (real) normal random variable.
The idea of the proof is to show that

  -- -------- -- --------
     @xmath      (10.2)
  -- -------- -- --------

for @xmath . Then the moments of @xmath converge to those of @xmath and
convergence in distribution follows. To show ( 10.2 ), we need to
compute the moments of @xmath .

Recall the formula ( 4.2 )

  -- -------- -- --------
     @xmath      (10.3)
  -- -------- -- --------

From this we can also write

  -- -------- --
     @xmath   
  -- -------- --

where @xmath as defined in Section 5 . Then one can write the moments of
@xmath as

  -- -------- -- --------
     @xmath      (10.4)
  -- -------- -- --------

for @xmath .

#### 10.1 Central moments of @xmath

From the homogeneity of @xmath , we can write @xmath , where @xmath .
These @xmath are no longer analytic functions, but they are independent
complex Gaussian processes on @xmath with constant variance @xmath .
Thus for any fixed @xmath , we have that @xmath , @xmath are i.i.d.
standard complex Gaussians, and from ( 4.4 ) it follows that

  -- -------- --
     @xmath   
  -- -------- --

with coefficients @xmath that are the same as in ( 4.4 ) and depend only
on @xmath but not on @xmath or even the GAF @xmath . We get

  -- -------- --
     @xmath   
  -- -------- --

Here @xmath and likewise for @xmath .

Each of the expectations on the right hand side of the above equation
can be “evaluated” by the Feynman diagram formula ( 2.5 ). We denote by
@xmath , the quantity @xmath . Also we write @xmath for the value of a
Feynman diagram @xmath , with edge weights given by the covariance
matrix of the the Gaussian vector @xmath . Then, we get

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Here in the last sum, @xmath , @xmath vary over all possible Feynman
diagrams on labels @xmath and @xmath are such that the number of
vertices labeled @xmath in @xmath is @xmath and the number of vertices
labeled @xmath in @xmath is @xmath .

Put this together with ( 10.4 ) to deduce that @xmath is equal to

  -- -------- -- --------
     @xmath      (10.5)
  -- -------- -- --------

Here again, the sum is over all legal diagrams @xmath .

Our goal is to prove ( 10.2 ). To that end, we now consider the second
moment, which is obtained by setting @xmath . We get

  -- -------- -- --------
     @xmath      (10.6)
  -- -------- -- --------

where, each @xmath is now a Feynman diagram on vertices labeled @xmath .

Now suppose @xmath is even. Write ( 10.6 ) with @xmath replaced by
@xmath for @xmath and multiply them together. On the right hand side, we
get (the product of the values of Feynman diagrams is the value of the
union of the Feynman diagrams)

  -- -------- --
     @xmath   
  -- -------- --

where the sum is over all Feynman diagrams @xmath on vertices labeled by
@xmath , such that , when all the vertices labeled by @xmath are
identified for each @xmath , then @xmath has connected components @xmath
, @xmath .

To get @xmath , integrate against @xmath w.r.t Lebesgue measure over
@xmath . This yields that @xmath is equal to

  -- -------- -- --------
     @xmath      (10.7)
  -- -------- -- --------

Here again the sum is over all diagrams that have connected components
@xmath , @xmath (upon merging vertices labeled @xmath ).

Instead of pairing @xmath as @xmath , we could use any other matching.
Write the expression analogous to ( 10.7 ) for each matching, and add
them all up. Recall that the number of matchings of @xmath is @xmath .
Thus we deduce that @xmath is equal to

  -- -------- -- --------
     @xmath      (10.8)
  -- -------- -- --------

where the sum is over all diagrams @xmath , @xmath such that @xmath
“splits”, i.e., when vertices @xmath are merged, we get @xmath component
each of size @xmath .

Compare ( 10.8 ) with ( 10.5 ) (the expressions are incomplete without
the commentaries that follows after the equations!). The terms on the
right hand side of ( 10.5 ) that are absent in ( 10.8 ) are precisely
those, for which @xmath does not split into @xmath components. The proof
of ( 10.2 ) will be complete once we show that these terms together
contribute a negligible amount compared to @xmath .

#### 10.2 Estimating the second moment

This is the case @xmath , which we already dealt with in detail, in
Chapter Zeros of Random Analytic Functions . Particularly, from ( 5.3 )
we can write

  -- -------- -- --------
     @xmath      (10.9)
  -- -------- -- --------

where @xmath . It is convenient to express everything in terms of the
invariant quantities of @xmath . From ( 9.1 ) and ( 9.2 ) we rewrite (
10.9 ) as

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

Now split @xmath as @xmath . We get a sum of three integrals. The first
two integrals are equal by symmetry. We shall argue that the last
integral is negligible. Firstly note that

  -- -------- -- ---------
     @xmath      (10.10)
  -- -------- -- ---------

Thus @xmath is @xmath when @xmath and decays rapidly as @xmath moves
away from the diagonal. Since @xmath vanishes on the diagonal, it is
easy to calculate that

  -- -------- -- ---------
     @xmath      (10.11)
  -- -------- -- ---------

The first two integrals give us

  -- -------- -- ---------
     @xmath      (10.12)
  -- -------- -- ---------

Since @xmath is also invariant under isometries of @xmath , fixing
@xmath and integrating w.r.t @xmath we get

  -- -------- --
     @xmath   
  -- -------- --

Now from ( 10.10 ) it can be checked by direct computation that

  -- -------- -- ---------
     @xmath      (10.13)
  -- -------- -- ---------

From ( 10.11 ), ( 10.12 ) and ( 10.13 ), we get

  -- -------- -- ---------
     @xmath      (10.14)
  -- -------- -- ---------

This shows ( 10.1 ) with

  -- -------- --
     @xmath   
  -- -------- --

#### 10.3 Estimating the values of non-split diagrams

We want to show that the contribution of non-split diagrams to @xmath is
negligible. Note that this includes all the diagrams in the case when
@xmath is odd. Consider any @xmath -tuple of diagrams @xmath on labels
@xmath such that @xmath does not split into pairs when @xmath are
merged. We bound

  -- -------- --
     @xmath   
  -- -------- --

in absolute value by the obvious

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the support of @xmath . Then split the integral of
@xmath as a product of integrals over the connected components of @xmath
.

Without loss of generality, let @xmath be a connected component of
@xmath (when @xmath are merged). Since the weights of edges are bounded
by @xmath , deleting some edges will only increase the integral that we
want to bound. We delete enough edges to get a spanning tree @xmath on
@xmath . So we are left with an integral of the form

  -- -------- --
     @xmath   
  -- -------- --

Integrate inwards starting with the leaves. From ( 10.13 ) we get that

  -- -------- --
     @xmath   
  -- -------- --

Multiplying the contribution from each component, we get

  -- -------- --
     @xmath   
  -- -------- --

Non-split diagrams are precisely those that have less than @xmath
components, whence the right hand side is @xmath .

Now we want to bound the total contribution of all unsplit diagrams.
This can be done in the following manner.

Approximate @xmath by polynomials (by truncating the Wick expansion).
For integration of @xmath against a polynomial, our cruder bound on a
single Feynman diagram suffices, since there are only finitely many
terms and each of them goes to zero. So we get asymptotic normality for
@xmath integrated against polynomials. From this, one can deduce
asymptotic normality for @xmath . We skip the details (see ST1 ).

To summarize, we have argued that

  -- -------- --
     @xmath   
  -- -------- --

and proved that @xmath is of order @xmath . Thus ( 10.2 ) follows.

## Chapter \thechapter Overcrowding Problems

### 11 Statements of the problems

In this chapter we go back to the canonical models of Gaussian analytic
functions defined in Chapter Zeros of Random Analytic Functions .
Namely, consider the following Gaussian analytic functions (GAFs):

-    Planar GAF : The function defined in ( 2.9 ),

      -- -------- --
         @xmath   
      -- -------- --

    where @xmath are i.i.d. standard complex Gaussian random variables.

-    Hyperbolic GAFs : For each @xmath the function defined in ( 2.11 )

      -- -------- --
         @xmath   
      -- -------- --

    where as before @xmath are i.i.d. standard complex Gaussians. Almost
    surely, @xmath is an analytic function in the unit disk (and no
    more).

We denote the zero set by @xmath . Let @xmath denote the number of
points of @xmath in the disk of radius @xmath around @xmath (The GAF
will be clear from the context). By the invariance of the zero sets, the
results carry over to disks centered elsewhere.

Yuval Peres asked the following question and conjectured that the
probability decays as @xmath in the planar case (personal
communication).

Question: Fix @xmath , ( @xmath in the Hyperbolic case). Estimate @xmath
as @xmath .

One motivation for such a question is in Figure 2 . There one can see
the distribution of the zero set under certain conditions on the
coefficients that force large number of zeros in the disk of radius
@xmath (this is not the zero set conditioned to have overcrowding - that
seems harder to simulate). The picture suggests that the distribution of
the conditioned process may be worth studying on its own. A large
deviation estimate of the kind we derive will presumably be a necessary
step in such investigations.

The answer is different in the two settings. We prove-

###### Theorem 11.1.

Consider the planar GAF @xmath . For any @xmath , @xmath a constant
@xmath (depending on @xmath ) such that for every @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

In particular, @xmath .

###### Theorem 11.2.

Fix @xmath and consider the GAF @xmath . For any fixed @xmath , there
are constants @xmath (depending on @xmath and @xmath ) such that for
every @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

We prove Theorem 11.1 in Section 12 , Theorem 11.2 in Section 13 .

### 12 Overcrowding - The planar case

In this section we prove Theorem 11.1 . Before that we explain why one
expects the constant @xmath in the exponent in Theorem 11.1 , by analogy
with the Ginibre ensemble.

#### 12.1 Ginibre ensemble

The Ginibre ensemble is the determinantal point process (earlier we
denoted this by Det- @xmath ) in the plane with kernel

  -- -------- -- --------
     @xmath      (12.1)
  -- -------- -- --------

This process is of interest because it is the limit in distribution, as
@xmath , of the point process of eigenvalues of an @xmath matrix with
i.i.d. standard complex Gaussian entries (Theorem 6.1 ).

The Ginibre ensemble has many similarities to the zero set of @xmath .
In particular, the Ginibre ensemble is invariant in distribution under
Euclidean motions, has constant intensity @xmath in the plane and has
the same negative correlations as @xmath at short distances. Therefore
there are other similarities too, for instance, see denhan . There are
also differences between the two point processes. For instance, the
Ginibre ensemble has all correlations negative, whereas for the zero set
of @xmath , long-range two-point correlations are positive. However, in
our problem, since we are considering a fixed disk and looking at the
event of having an excess of zeros in it, it seems reasonable to expect
the same behaviour for both these point processes, since it is the short
range interaction that is relevant. In case of the Ginibre ensemble, the
overcrowding problem is easy to solve.

###### Theorem 12.1.

Let @xmath be the number of points of the Ginibre ensemble in the disk
of radius @xmath around @xmath (by translation invariance, the same is
true for any disk of radius @xmath ). Then for a fixed @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

By Kostlan kostlan , the set of absolute values of the points of the
Ginibre ensemble has the same distribution as the set @xmath , where
@xmath are independent, and @xmath has Gamma( @xmath ) distribution for
every @xmath . Hence @xmath , where @xmath are i.i.d. Exponential random
variables with mean @xmath , and it follows that

  -- -------- --
     @xmath   
  -- -------- --

as long as @xmath , because @xmath for @xmath . Therefore we get

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (12.2)
              @xmath   @xmath      (12.3)
              @xmath   @xmath      (12.4)
  -- -------- -------- -------- -- --------

Here and elsewhere we shall encounter the term @xmath . We compute its
asymptotics now.

  -- -------- --
     @xmath   
  -- -------- --

Integrate from @xmath to @xmath and note that

  -- -------- --
     @xmath   
  -- -------- --

to get

  -- -------- -- --------
     @xmath      (12.5)
  -- -------- -- --------

Thus ( 12.4 ) gives

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

To prove the inequality in the other direction, note that

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

In the second line, for the first summand we used the fact that @xmath
are stochastically increasing and for the second term we used the well
known fact @xmath and then the usual bound on the tail of a Poisson
random variable, namely @xmath .

Using the same idea to bound @xmath in the first summand, we obtain

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

In the last line we used @xmath . This completes the proof. ∎

#### 12.2 Proof of Theorem 11.1

Our method of proof is largely based on that of Sodin and Tsirelson ST3
. (They estimate the “hole probability”, @xmath as @xmath .)

###### Proof of Theorem 11.1.

Lower Bound Suppose the @xmath term dominates the sum of all the other
terms on @xmath , i.e., suppose

  -- -------- -- --------
     @xmath      (12.6)
  -- -------- -- --------

Then, by Rouche’s theorem @xmath and @xmath have the same number of
zeros in @xmath . Hence @xmath . Now we want to find a lower bound for
the probability of the event in ( 12.6 ). Note that the left side of (
12.6 ) is identically equal to @xmath .

Now suppose the following happen-

1.  @xmath @xmath .

2.  @xmath where @xmath will be chosen shortly.

3.  @xmath for every @xmath .

Then the right hand side of ( 12.6 ) is bounded by

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

if @xmath . Thus if the above three events occur with @xmath , then the
@xmath term dominates the sum of all the other terms on @xmath . Also
these events have probabilities as follows.

1.  @xmath @xmath .

2.  @xmath .

3.  The third event has probability as follows. Recall again that @xmath
    if @xmath and @xmath has exponential with mean @xmath . We apply
    this below with @xmath . This is clearly less than @xmath if @xmath
    . Therefore if @xmath is sufficiently large it is easy to see that
    for all @xmath , the same is valid. Thus

      -- -- -------- -------- --
            @xmath   @xmath   
            @xmath   @xmath   
            @xmath   @xmath   
            @xmath   @xmath   
      -- -- -------- -------- --

Since these three events are independent, we get the lower bound in the
theorem.

Upper Bound By Jensen’s formula, for any @xmath we have

  -- -------- -- --------
     @xmath      (12.7)
  -- -------- -- --------

Let @xmath . Sodin and Tsirelson ST3 show that

  -- -------- -- --------
     @xmath      (12.8)
  -- -------- -- --------

where @xmath .

Now suppose @xmath and @xmath for some @xmath . Then by ( 12.7 ) we have

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Thus @xmath is bounded by

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

From Lemma 12.2 , we deduce that for any @xmath , there is a constant
@xmath such that

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

From this, the upper bound follows. ∎

###### Lemma 12.2.

For any given @xmath , @xmath such that for every @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Let @xmath be the Poisson kernel on @xmath . Fix @xmath and let @xmath
and @xmath . Since @xmath is a subharmonic function, for any @xmath with
@xmath , we get

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

This implies @xmath .

Therefore if @xmath , then one of the following must happen. Either
@xmath } or @xmath .

Using ( 12.8 ), since @xmath for any @xmath , we see that

  -- -------- --
     @xmath   
  -- -------- --

for some constant @xmath depending on @xmath . Hence

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

where in the last line we have used Lemma 12.3 .

As @xmath , @xmath and hence the proof is complete.

∎

Now we prove the upper bound on the maximum modulus in a disk of radius
@xmath that was used in the last part of the proof of Lemma 12.2 . For
possible future use we prove a lower bound too.

###### Lemma 12.3.

Fix @xmath . There are constants @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Lower bound By Cauchy-Schwarz, @xmath . We shall choose @xmath later. We
will bound from below the probability that each of these summands is
less than @xmath .

Let @xmath denote the density of @xmath .

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Also if @xmath @xmath , then the second summand

  -- -------- --
     @xmath   
  -- -------- --

Also the event @xmath @xmath has probability at least @xmath .

Thus if we set @xmath for a sufficiently large @xmath , then both the
terms are less than @xmath with probability at least @xmath .

Upper bound By Cauchy’s theorem,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the curve @xmath , @xmath . Therefore,

  -- -------- --
     @xmath   
  -- -------- --

Thus we get

  -- -------- --
     @xmath   
  -- -------- --

@xmath are i.i.d. exponential random variables with mean @xmath .
Therefore,

  -- -------- --
     @xmath   
  -- -------- --

Using this bound for @xmath , we get

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

@xmath is minimized when @xmath and we get,

  -- -------- -- --------
     @xmath      (12.9)
  -- -------- -- --------

∎

### 13 Overcrowding - The hyperbolic case

#### 13.1 The determinantal case

We give a quick proof of Theorem 11.2 in the special case @xmath , as it
is much easier and moreover we get matching upper and lower bounds. The
proof is similar to the case of the Ginibre ensemble dealt with in
Theorem 12.1 and is based on the fact that the set of absolute values of
the zeros of @xmath is distributed the same as a certain set of
independent random variables. The reason for this similarity between the
two cases owes to the fact that both of them are determinantal. The zero
set of @xmath is a determinantal process with the Bergman kernel for the
unit disk, namely

  -- -------- --
     @xmath   
  -- -------- --

as discovered by Peres and Virág pervir .

###### Proof of Theorem 11.2 for @xmath.

By the result of Peres and Virág quoted in Theorem 6.2 , the set of
absolute values of the zeros of @xmath has the same distribution as the
set @xmath where @xmath are i.i.d. uniform @xmath random variables.
Therefore,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

To prove the inequality in the other direction, note that

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

This completes the proof of the theorem for @xmath . ∎

#### 13.2 All values of @xmath

###### Remark 13.1.

Overall, the idea of proof is the same as in that of Theorem 11.1 .
However we do not get matching upper and lower bounds in the present
case, the reason being that in the hyperbolic analogue of Lemma 12.3 ,
the leading term in the exponent of the upper bound does depend on
@xmath , unlike in the planar case. (An examination of the proof of
Theorem 11.1 reveals that we get a matching upper bound only because
replacing @xmath by @xmath does not affect the leading term in the
exponent in the upper bound in Lemma 12.3 ). However we still expect
that the lower bound in Theorem 11.2 is tight. (See remark after the
proof).

###### Proof of Theorem 11.2.

Lower Bound As before we find a lower bound for the probability that the
@xmath term dominates the rest. Note that if @xmath ,

  -- -------- -- --------
     @xmath      (13.1)
  -- -------- -- --------

Now suppose the following happen-

1.  @xmath @xmath .

2.  @xmath where @xmath will be chosen shortly.

3.  @xmath for every @xmath .

Then the right hand side of ( 13.1 ) is bounded by

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

if @xmath . Thus if the above three events occur with @xmath , then the
@xmath term dominates the sum of all the other terms on @xmath . Also
these events have probabilities as follows.

1.  @xmath @xmath .

2.  @xmath .

3.  The third event has probability as follows. Recall again that @xmath
    if @xmath and @xmath has exponential distribution with mean @xmath .
    We apply this below with @xmath . This is clearly less than @xmath .
    Thus

      -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
                  @xmath   @xmath   
                  @xmath   @xmath   
                  @xmath   @xmath   
                  @xmath   @xmath   
      -- -------- -------- -------- --

Since these three events are independent, we get the lower bound in the
theorem.

Upper Bound The proof will proceed along the same lines as in Theorem
11.1 . We need the following analogue of Lemma 12.3 .

###### Lemma 13.2.

Fix @xmath . Let @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

By Cauchy’s theorem, for every @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

From this we get

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath , we obtain

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

∎

Coming back to the proof of the upper bound in the theorem, fix @xmath
such that @xmath . Then by Jensen’s formula,

  -- -------- -- --------
     @xmath      (13.2)
  -- -------- -- --------

Now consider the first summand in the right hand side of ( 12.7 ).

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

Now suppose that @xmath @xmath where @xmath . This has probability at
least @xmath . Then,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

for some constants @xmath and @xmath .

Thus if @xmath then either @xmath or else @xmath for some @xmath . Thus

  -- -------- --
     @xmath   
  -- -------- --

This proves that

  -- -------- --
     @xmath   
  -- -------- --

Fix @xmath and @xmath close enough to @xmath such that @xmath . Then
with probability @xmath , we obtain from ( 13.2 ),

  -- -------- --
     @xmath   
  -- -------- --

Now the calculations in the proof of Lemma 12.2 show that

  -- -------- --
     @xmath   
  -- -------- --

Here @xmath is arbitrary and @xmath are as defined in Lemma 12.2 . By
the same computations as in that Lemma, we obtain, we obtain the
inequality

  -- -------- --
     @xmath   
  -- -------- --

Therefore, by ( 13.2 )

  -- -------- --
     @xmath   
  -- -------- --

where @xmath . However it is clear that this cannot be made to match the
lower bound by any choice of @xmath . ∎

###### Remark 13.3.

If we could prove

  -- -------- --
     @xmath   
  -- -------- --

that would have given us a matching upper bound. Now, one way for the
event @xmath to occur is to have @xmath which, by Lemma 13.2 has
probability at most @xmath . One way to proceed could be to show that if
the integral is smaller than @xmath , so is @xmath for @xmath
arbitrarily close to @xmath (with high probability). Alternately, if we
could bound the coefficients directly by the bound on the integral (as
in Lemma 13.2 ), that would also give us the desired bound. For these
reasons, and keeping in mind the case @xmath , where we do have a
matching upper bound, we believe that the lower bound in Theorem 11.2 is
tight.

## Chapter \thechapter Moderate and very large deviations for zeros of
the planar GAF

Inspired by the results obtained (using not entirely rigorous physical
arguments) by Jancovici, Lebowitz and Manificat janlebmag for Coulomb
gases in the plane (eg., Ginibre ensemble), M.Sodin sod2 conjectured the
following.

###### Conjecture 13.4 (Sodin).

Let @xmath be the number of zeroes of the planar GAF @xmath in the disk
@xmath . Then, as @xmath

  -- -------- -- --------
     @xmath      (13.3)
  -- -------- -- --------

The idea here is that the deviation probabilities undergo a qualitative
change in behaviour when the deviation under consideration becomes
comparable to the perimeter ( @xmath ) or to the area ( @xmath ) of the
domain.

Sodin and Tsirelson ST3 had already settled the case @xmath by showing
that for any @xmath , @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

Here we consider @xmath and prove that a “phase transition” in the
exponent occurs at @xmath . More precisely we prove that the conjecture
holds for @xmath and show the lower bound for @xmath .

###### Theorem 13.5.

Fix @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

###### Theorem 13.6.

Fix @xmath . Then for any @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

We prove Theorem 13.5 in Section 14 and Theorem 13.6 in Section 15 .
Taken together these show that the asymptotics of @xmath does undergo a
qualitative change at @xmath .

###### Remark 13.7.

Nazarov, Sodin and Volberg have recently proved all the remaining parts
of the conjecture (personal communication).

### 14 Very large deviations for the planar GAF

In this section we prove Theorem 13.5 .

###### Remark 14.1.

In the case @xmath , one side of the estimate as asked for in the
conjecture (with @xmath of the probability) follows trivially from the
results in Sodin and Tsirelson ST3 . They prove that for any @xmath ,
there exists a constant @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

When @xmath , clearly @xmath , whence from the above result it follows
that

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

This gives

  -- -------- -- --------
     @xmath      (14.1)
  -- -------- -- --------

The obviously loose inequality @xmath that we used, suggests that ( 14.1
) can be improved when @xmath to Theorem 13.5 .

###### Proof of Theorem 13.5.

Lower Bound Let @xmath . Suppose the @xmath term dominates the sum of
all the other terms on @xmath , i.e., suppose

  -- -------- -- --------
     @xmath      (14.2)
  -- -------- -- --------

Now we want to find a lower bound for the probability of the event in (
14.2 ). Note that the left side of ( 14.2 ) is identically equal to
@xmath .

Now suppose the following happen-

1.  @xmath @xmath .

2.  @xmath .

3.  @xmath for every @xmath .

Then the right hand side of ( 14.2 ) is bounded by

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Thus if the above three events occur, then the @xmath term dominates the
sum of all the other terms on @xmath . Also these events have
probabilities as follows.

1.  @xmath @xmath .

2.  @xmath .

3.  The third event has probability as follows. Recall again that @xmath
    if @xmath and @xmath is exponential with mean @xmath . We apply this
    below with @xmath . This is clearly less than @xmath if @xmath .
    Therefore if @xmath is sufficiently large it is easy to see that for
    all @xmath , the same is valid. Thus

      -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
                  @xmath   @xmath   
                  @xmath   @xmath   
                  @xmath   @xmath   
                  @xmath   @xmath   
      -- -------- -------- -------- --

Since these three events are independent, we get

  -- -------- -- --------
     @xmath      (14.3)
  -- -------- -- --------

Upper Bound We omit the proof of the upper bound, as it follows the same
lines as that of Theorem 11.1 and we have already seen such arguments
again in the proof of Theorem 11.2 (In those two cases as well as the
present case, we are looking at very large deviations, and that is the
reason why the same tricks work).

Moreover note that the lower bound along with ( 14.1 ) proves the
statement in the conjecture. ∎

### 15 Moderate deviations for the planar GAF

In this section we prove Theorem 13.6 .

###### Proof of Theorem 13.6.

Write @xmath . As usual, we bound @xmath from below by the probability
of the event that the @xmath term dominates the rest of the series.

Firstly, we need a couple of estimates. Consider @xmath as a function of
@xmath . This increases monotonically up to @xmath and then decreases
monotonically. @xmath is on the latter part. Write @xmath .

Firstly, observe that @xmath , for @xmath , whence @xmath . This implies
that

  -- -------- -- --------
     @xmath      (15.1)
  -- -------- -- --------

Secondly, note that for any @xmath ,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Now we set @xmath with @xmath so large that @xmath .

Then also note that if @xmath , it follows that

  -- -------- -- --------
     @xmath      (15.2)
  -- -------- -- --------

where we used ( 15.1 ) to replace @xmath by @xmath .

Thirdly, if @xmath with @xmath , then,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

If @xmath , where @xmath was as chosen before, then for @xmath , we get

  -- -------- -- --------
     @xmath      (15.3)
  -- -------- -- --------

From now on @xmath is fixed so that ( 15.2 ) and ( 15.3 ) are satisfied.

Next we divide the coefficients other than the @xmath one into groups:

-   @xmath for @xmath .

-   @xmath for @xmath }.

-   @xmath .

-   @xmath .

###### Remark 15.1.

As defined, there is an overlap between @xmath and @xmath . This is
inconsequential, but for definiteness, let us truncate the former
interval at @xmath (just as @xmath is understood to be truncated at
@xmath ).

Now consider the following events.

1.  @xmath for @xmath for @xmath }.

2.  @xmath for @xmath for @xmath .

3.  @xmath .

4.  @xmath for @xmath .

5.  @xmath .

Suppose all these events occur. Then

1.  The event @xmath for @xmath , @xmath gives

      -- -------- -------- -------- -- --------
         @xmath   @xmath   @xmath      (15.4)
                  @xmath   @xmath      (15.5)
                  @xmath   @xmath      (15.6)
                  @xmath   @xmath      (15.7)
      -- -------- -------- -------- -- --------

2.  The event @xmath for @xmath , @xmath gives

      -- -------- -------- -------- -- ---------
         @xmath   @xmath   @xmath      (15.8)
                  @xmath   @xmath      (15.9)
                  @xmath   @xmath      (15.10)
      -- -------- -------- -------- -- ---------

3.  The third event gives

      -- -------- -- ---------
         @xmath      (15.11)
      -- -------- -- ---------

    by assumption.

4.  The event @xmath for @xmath : Since @xmath ,

      -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
                  @xmath   @xmath   
                  @xmath   @xmath   
      -- -------- -------- -------- --

    Therefore we get (using @xmath @xmath )

      -- -------- -------- -------- -- ---------
         @xmath   @xmath   @xmath      (15.12)
                  @xmath   @xmath      (15.13)
      -- -------- -------- -------- -- ---------

Putting together the contributions from these four groups of terms, and
using @xmath , we get (for large values of @xmath )

  -- -------- --
     @xmath   
  -- -------- --

Now we compute the probabilities of the events enumerated above.

1.  The event @xmath for @xmath for @xmath . Now for a fixed @xmath , we
    deduce

      -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
                  @xmath   @xmath   
      -- -------- -------- -------- --

    Therefore

      -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
                  @xmath   @xmath   
      -- -------- -------- -------- --

    for some @xmath .

    Next we deal with @xmath .

      -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
                  @xmath   @xmath   
      -- -------- -------- -------- --

    Now the summation in the the last line has rapidly decaying terms
    and starts with @xmath which is smaller than @xmath . Thus

      -- -------- --
         @xmath   
      -- -------- --

    Thus the event in question has probability at least @xmath .

2.  The event @xmath for @xmath for @xmath . Following exactly the same
    steps as above we can prove that

      -- -------- --
         @xmath   
      -- -------- --

3.  The event @xmath . By Cauchy-Schwarz,

      -- -------- --
         @xmath   
      -- -------- --

    @xmath has @xmath distribution. Also @xmath , since the left hand is
    part of the Taylor series of @xmath . Therefore the event in
    question has probability,

      -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
                  @xmath   @xmath   
      -- -------- -------- -------- --

    where @xmath is the density of the @xmath distribution. This last
    follows because @xmath is increasing on @xmath and thus @xmath . for
    @xmath . Continuing,

      -- -------- -------- -------- -- ---------
         @xmath   @xmath   @xmath      (15.14)
                  @xmath   @xmath      (15.15)
      -- -------- -------- -------- -- ---------

    where we used Stirling’s approximation.

    The exponent needs simplification. Take the first and third terms in
    the exponent. We have @xmath . Recall that @xmath and that @xmath .
    Therefore by Taylor’s expansion of @xmath we get

      -- -------- -- ---------
         @xmath      (15.16)
      -- -------- -- ---------

    Now consider ( 15.15 ). Expand the fourth term in the exponential as
    @xmath . We get the following terms

    1.  @xmath , from the second and fourth terms (first piece of the
        fourth term) in the exponential in ( 15.15 ).

    2.  @xmath , from the sum of the first term in the expansion ( 15.16
        ) and the second piece of the fourth term in the exponential in
        ( 15.15 ).

    3.  @xmath , from the expansion ( 15.16 ).

    4.  Other terms such as @xmath etc. All these are of lower order
        than @xmath when @xmath .

    Hence,

      -- -------- --
         @xmath   
      -- -------- --

4.  The event @xmath for @xmath . This is just an event for a sequence
    of i.i.d. complex Gaussians. It has a fixed probability @xmath
    (say).

5.  The event @xmath also has a constant probability (not depending on
    @xmath , that is).

This completes the estimation of probabilities. Among these five events,
the third one, namely @xmath has the least probability (Recall that
@xmath ).

Also these events are all independent, being dependent on disjoint sets
of coefficients. Thus @xmath .

∎