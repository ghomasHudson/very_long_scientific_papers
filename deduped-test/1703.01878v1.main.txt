## Chapter 1 Introduction

  The whole is more than the sum of its parts.

  Aristotele

In recent years complex systems attracted increasing interest among
scientists of different fields like physics, computer science,
chemistry, biology, ecology, economy and social science, just to name a
few. This interest arises from the general idea that several natural
systems (such as gene regulatory networks, social systems, etc.) and
artificial systems (such as economical system, power grid, etc.) are
composed of a large number of simple elements which show a collective
emergence behavior that cannot be predicted from the description of each
particular element.
There is no universal definition of complex system, but most researchers
in the field would probably agree that it involves numerous components
(agents), which may be simple both in terms of their internal
characteristics and in the way they interact. These agents exhibit
emergence activities, i.e. not derivable from the summations of the
activity of the individual components, and a tangled hierarchical
self-organization which cannot be understood by referring to simple
hierarchical models.
Understanding the behavior of complex systems is of exceptional
relevance, from both practical and theoretical reasons. This problem has
been addressed by researchers which follow two basic approaches. The
first one involves the development of mathematical models which abstract
the most relevant features of the system to gain scientific insight into
its dynamic. The concepts used in the development of such models belong
to dynamical system theory, information theory, game theory, network
theory, numerical methods and cellular automata. The second approach,
instead, relies on the development of a more detailed and realistic
models in order to simulate the emergent behavior which appears when the
system is observed for a long time and length scales. The tools used in
this approach are computational methods, like agent-based simulation and
Monte Carlo simulation.
In this thesis we study the dynamical organization of complex systems
using methods based on information theory. To do so, we model a system
using a random field evolving in time. In particular, the dynamical
behavior of each system component is described by means of a discrete
stochastic process. We observe the system evolution in a given time
interval and we estimate the probability distributions of the variables
belonging to the random field. Then, exploiting measures based on
information theory, which are known as information-theoretic measures,
we look for relevant subsets (RSs). A RS is a group of variables which
have a strong mutual interaction and that weakly interact with variables
which are not in the relevant subset.
In order to find a measure for the interaction among variables, we
exploit a fundamental property of the dynamical system from a complex
system perspective which was introduced by Kauffman [ 14 ] . He stated
that there is a continuous exchange of information among the system
constituents and also between the environment and the system.
Historically, Tononi and Edelman [ 35 ] introduced an
information-theoretic measure, the Cluster Index, to study biological
neural networks which are close to a stationary state. More recently,
such Cluster Index was profitably applied to the study of dynamical
systems by Villani et al. [ 39 ] and was referred to as Dynamical
Cluster Index (DCI).
Starting from such ideas, we study the two measures that contribute to
the DCI, which are the Integration and the Mutual Information. We show
that the analysis of the different parts of the index is extremely
useful to better characterize the nature of the identified relevant
subsets. In many cases the detected RSs have an intricate nested
structure, so that it might not be clear which groups of variables are
really important. To overcome this problem, we introduce a sieving
procedure to extract only disjoint or partial overlapping RSs [ 8 ] .
Furthermore, we extend the DCI introducing causality [ 43 ] in order to
investigate the influence of found relevant subsets on the system
dynamics. To achieve this goal we use information theoretic concepts
like Entropy Rate and Transfer Entropy [ 28 ] , [ 18 ] . In particular,
on the basis of these measures, we introduce a directionality index [ 8
] to find the direction of information flows which can take place
between a RS and other parts of the system. The direction and the amount
of information flows are needed to select the leaders, i.e. the most
influenced RSs. We call these subsets functional dynamical structures
(FDS) to stress their functional role in the system dynamics.
Several different application domains are investigated to test the
effectiveness and the robustness of this measures. These domains
include: random boolean networks, mammalian cell cycle networks and the
Mitogen Activated Protein Kinase (MAPK) cascade signaling pathway in
eukaryotes.
Finally, it’s worth noting that our method does not require any previous
knowledge of underlying network topology of the complex system under
study, but relies only on the values assumed by the random fields during
the observation time interval.

### 1.1 Functional Dynamical Structures

The Dynamical Cluster Index is an extension of a measure introduced by
Tononi and Edelman [ 35 ] to detect clusters in biological neural
networks. In particular, Tononi et al. start from the idea that there
are functional bounded regions in the brain despite the fact that all
brain regions are widespread connected. These brain regions, called
functional clusters, are composed of neurons which interact more
strongly with neurons belonging to the same region than with neurons
belonging to another brain part. The functional aspect of these clusters
is suggested by the fact that there is a very fast signal transmission
among intra-cluster neurons and they can thus influence many aspects of
brain behavior. It’s worth noting that this method was applied to
networks considering only fluctuations around stable asymptotic state.
The time system evolution was indeed not studied. The first
generalization of Tononi’s method was made by Villani et al. [ 39 ] in
order to detect intermediate-level emergent structures in complex
systems. They extended the cluster index to the study of truly dynamical
systems and the DCI was introduced.
In this thesis, we further develop the method based on the Dynamical
Cluster Index and we also show new interesting applications that
highlight its effectiveness and help uncovering some of its features [ 7
, 40 ] . There are of course several methods to identify clusters of
nodes in a network based on its topology, but the DCI can be applied
also without any prior knowledge on the network topology. This property
is shared also by nonlinear correlation methods [ 13 ] ; with respect to
the latter, the advantage of the DCI is that it is not limited to binary
relations but it can be applied to clusters of any size. Finally, we
study the influence that detected relevant subsets have on the system by
focusing on the causal interactions among variables through the
different dynamical states. For this purpose, we introduce a
directionality index which, as previously mentioned, is an information
theoretic measure based on the transfer entropy. To the best of our
knowledge, such approach in unprecedented in the study of the dynamical
behavior of complex systems.

### 1.2 Contributions

The contribution of this thesis may be summarized as follows:

-   we studied the two measures that contribute to the DCI, which are
    the Integration and the Mutual Information. We showed that the
    analysis of the different parts of the index is extremely useful to
    better characterize the nature of the identified relevant subsets.
    This led to a first publication [ 7 ] ;

-   we extended the Dynamical Cluster Index methodology introducing a
    sieving algorithm. The results of this extension will appear in [ 40
    ] [ 8 ] ;

-   we further extended the DCI methodology introducing causality in
    order to investigate the influence of found relevant subsets on the
    system dynamics. The results will be presented at European
    Conference of Artificial Life [ 8 ] .

The results obtained in this thesis will be the subject of my oral
presentation at the Student Conference on Complexity Science (SCCS2015)
Conference in Granada in September 2015.

### 1.3 Outline of the Thesis

Chapter 2 reviews the fundamental information theoretic notions like
entropy, joint entropy, conditional entropy, relative entropy, mutual
information, integration and transfer entropy, which provide a basis for
the development of our indexes. Chapter 3 introduces our contribution to
the description of complex system behavior, namely the Dynamical Cluster
Index, the D-Index and the methodology to detect functional dynamical
structures [ 7 , 40 , 8 ] . Chapter 4 shows the results of the method
application to various models. In particular, they include what follows:
the random boolean network framework (in order to verify that the method
is able to identify subsets that make sense) and leaders-followers model
(to test the method robustness with respect to increasing noise levels).
Furthermore, two applications to real systems are presented, namely
mammalian cell cycle networks and the Mitogen Activated Protein Kinase
(MAPK) cascade signaling pathway in eukaryotes. In these two latter
applications, a peculiar discrete coding which describes the signs of
their first-order time derivatives is used. In chapter 5, the novelty of
the work and the future perspectives are presented.

## Chapter 2 Information Theory and Functional Structures

  This word ”information” in communications theory relates not so much
  to what you do say, as to what you could say.

  Warren Weaver

A complex system is composed of a huge amount of interacting parts. The
dynamical behavior of the whole system cannot be understood if the
analysis just focuses on the properties of the single parts. As a
consequence, statistical methods play a key role to understand how the
whole system works. In this framework, information theory provides a
collection of measures which can be useful to determine the degree of
interaction among interconnected elements. In this chapter, we provide a
brief background on information-theoretic measures. In particular, we
introduce the integration and the transfer entropy which are exploited
by our method. In particular, the first one is used to measure the
degree of interaction among the system elements while the latter is used
to find the direction of the information flow which takes place between
two subsets of the system.

### 2.1 Information and Entropy

Information theoretic measures give us a formal definition of the
dynamical structures notion. Even if the concept of information may be
referred to different meanings, we consider the formal presentation
proposed by Claude Shannon [ 31 ] , who developed a theory based on a
statistic description of a communication system. Shannon generalized the
information measure which was presented by Ralph Hartley [ 12 ] , who
stated that the information content of a message of length @xmath
composed of symbols chosen from an alphabet of cardinality @xmath is

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

where @xmath is an arbitrary base. Shannon generalized this concept
associating a probability distribution to the symbols source, i.e. a
symbol from the alphabet is chosen with a certain probability @xmath .
He stated that the information associated to the observation of a symbol
with occurrence probability @xmath is

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

Hence, the information that is received from a symbol observation
depends on its occurrence probability, i.e less likely is the symbol
appearance and more is the information that is gained from its
observation. As a consequence, a source is modeled by a discrete random
variable @xmath which can assume value on a non empty finite set @xmath
according to a probability distribution @xmath . If the source follows a
uniform distribution, the Shannon information coincides with the Hartley
information. In order to measure the average amount of transmitted
information, Shannon entropy [ 31 ] was introduced in 1949.

###### Definition 2.1.

The entropy @xmath of a discrete random variable @xmath with alphabet
@xmath and a probability mass function @xmath is defined as follows

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

As an alternative notation, we use @xmath . We consider the log in base
two (bits as units of entropy) and we define @xmath (since @xmath as
@xmath ).
We naturally extend the definition of entropy to a random vector, i.e. a
vector composed of two or more random variables.

###### Definition 2.2.

The entropy @xmath of a discrete random vector
@xmath with alphabet @xmath and a joint probability distribution @xmath
is defined as follows

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

By analogy with probability theory, we define the conditional entropy
using the notion on conditional probability which is a measure of the
probability of an event given the knowledge of the occurrence of another
event.

###### Definition 2.3.

The conditional entropy of two discrete random variables @xmath and
@xmath with alphabets @xmath and @xmath and a joint probability
distribution @xmath is defined as follows

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

The following theorem describes the relationship between joint entropy
and conditional entropy.

###### Theorem 2.1 (Chain Rule).

Given two discrete random variables @xmath and @xmath with alphabets
@xmath and @xmath and a joint probability distribution @xmath , then

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

We extend the previous result to a generic random vector.

###### Theorem 2.2.

Given a discrete random vector @xmath with alphabet @xmath and a joint
probability distribution @xmath , then

  -- -------- -- -------
     @xmath      (2.7)
  -- -------- -- -------

### 2.2 A Few Properties of Entropy

We present few properties of entropy which are exploited to construct
our method. In order to prove these properties, we introduce one of the
most important inequality in information theory.

###### Lemma 2.3 (Gibbs’ Inequality).

Given a random variables @xmath with alphabet @xmath and two probability
mass functions @xmath , then

  -- -------- -- -------
     @xmath      (2.8)
  -- -------- -- -------

with equality iff @xmath , for all @xmath .

Gibbs’ inequality allows us to introduce an upper bound for Shannon
entropy.

###### Theorem 2.4.

Given a discrete random variable @xmath with alphabet @xmath , which
contains a number of symbols equals to @xmath , and a probability mass
function @xmath , then

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

with equality iff @xmath is a uniform distribution.

###### Proof.

@xmath implies that @xmath . It follows that @xmath .
The equality holds iff @xmath because @xmath
@xmath

From Gibbs’ inequality we obtain

  -- -------- -- --------
     @xmath      (2.10)
  -- -------- -- --------

Let @xmath be uniformly distributed. Hence

  -- -------- -- --------
     @xmath      (2.11)
  -- -------- -- --------

Since

  -- -------- -- --------
     @xmath      (2.12)
  -- -------- -- --------

We obtain

  -- -------- -- --------
     @xmath      (2.13)
  -- -------- -- --------

Finally we prove that if @xmath is uniformly distributed then @xmath .

  -- -------- -- --------
     @xmath      (2.14)
  -- -------- -- --------

∎

For our purpose, we interpret the entropy as a measure of a lack of
knowledge about the system. In particular, we compute the probability of
each possible state of a dynamical system after observing its evolution
through a certain time interval. The gained information depends on the
observed trajectory. Therefore, we can see the entropy as the average
information which is gained by the system observation. This approach is
borrowed from statistical mechanics, where a macrostate of a system is
described associating a probability distribution over the possible
microstates which are unknown. In this contest, the entropy is seen as a
measure of the disorder of the systems: a connection between the
information theory and the thermodynamics can be thus grasped. [ 20 ] .

### 2.3 Mutual Information

In the study of complex systems, we have to deal with nonlinear systems
which do not satisfy the superposition principle. We are thus interested
in extensive measures which are useful to characterize a complex systems
at least in some particular cases. Intuitively, the information emitted
from two independent sources should be equal to the sum of the two
individual sources. The log-dependence in the definition of the Shannon
entropy makes the entropy of two independent random variables an
additive quantity. Hence, the system entropy is maximum when every pair
of system elements is independent. Otherwise, the correlation between
two or more elements of the systems causes an entropy reduction.
We introduce mutual information which can be used to measure the degree
of dependence of two random variables.

###### Definition 2.4.

The mutual information @xmath of two discrete random variables @xmath
and @xmath with alphabets @xmath and @xmath and a joint probability
distribution @xmath is defined as follows

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

Hence, the mutual information is symmetric under the exchange of its
arguments.
We present some relationships between entropy, conditional entropy and
mutual information. In order to prove these properties, we introduce
another extremely important information-theoretic inequality: the
Jensen’s inequality. We recall some mathematical notions which are used
in the inequality definition.

###### Definition 2.5 (Convex function).

Given a function @xmath . @xmath is said to be convex if for all @xmath

  -- -------- -- --------
     @xmath      (2.16)
  -- -------- -- --------

###### Theorem 2.5.

Given @xmath . Let be @xmath . @xmath is convex iff

  -- -------- -- --------
     @xmath      (2.17)
  -- -------- -- --------

###### Definition 2.6 (Standard Simplex).

  -- -------- -- --------
     @xmath      (2.18)
  -- -------- -- --------

###### Theorem 2.6 (Jensen’s Inequality).

Given @xmath , @xmath , @xmath . Let be @xmath convex, then

  -- -------- -- --------
     @xmath      (2.19)
  -- -------- -- --------

whith equality iff @xmath is strictly convex.

Jensen’s inequality allows us to introduce a lower bound for mutual
information.

###### Theorem 2.7.

Let @xmath and @xmath be two discrete random variables with alphabets
@xmath and @xmath and a joint probability distribution @xmath , then

  -- -------- -- --------
     @xmath      (2.20)
  -- -------- -- --------

###### Proof.

By definition

  -- -------- -- --------
     @xmath      (2.21)
  -- -------- -- --------

Since f(z) = log(z) is a concave function, we can apply Jensen’s
inequality changing the inequality direction

  -- -------- -- --------
     @xmath      (2.22)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (2.23)
  -- -------- -- --------

From the normalization axiom of probability theory, it follows

  -- -------- -- --------
     @xmath      (2.24)
  -- -------- -- --------

Hence

  -- -------- -- --------
     @xmath      (2.25)
  -- -------- -- --------

∎

There are the following relationships between entropy and mutual
information which can be proved using ( 2.19 ), ( 2.6 ).

  -- -------- -- --------
     @xmath      (2.26)
  -- -------- -- --------

From ( 2.6 ) it follows that

  -- -------- -- --------
     @xmath      (2.27)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (2.28)
  -- -------- -- --------

The following theorem states that, on average, the knowledge about a
random variable can reduce the uncertainty in another random variable.

###### Theorem 2.8.

Let @xmath and @xmath be two discrete random variables with alphabets
@xmath and @xmath and a joint probability distribution @xmath , then

  -- -------- -- --------
     @xmath      (2.29)
  -- -------- -- --------

with equality iff @xmath

It’s worth nothing that @xmath may be greater than @xmath . The equation
2.29 is true only for average quantity, i.e @xmath [ 5 ] .

The following theorem introduces an upper bound for mutual information.

###### Theorem 2.9.

Let @xmath and @xmath be two discrete random variables with alphabets
@xmath and @xmath and a joint probability distribution @xmath , then

  -- -------- -- --------
     @xmath      (2.30)
  -- -------- -- --------

###### Proof.

Entropy is non negative quantity so, in order to find the maximum of the
mutual information, we can find the maximum of the sum H(X)+H(Y) and the
minimum of the joint entropy H(X,Y).
From ( 2.6 ) we obtain

  -- -------- -- --------
     @xmath      (2.31)
  -- -------- -- --------

Hence, for every joint distribution p(x,y) holds

  -- -------- -- --------
     @xmath      (2.32)
  -- -------- -- --------

Hence

  -- -------- -- --------
     @xmath      (2.33)
  -- -------- -- --------

Finally we obtain

  -- -------- -- --------
     @xmath      (2.34)
  -- -------- -- --------

∎

Hence, from the equations 2.20 and 2.30 , we can state that I(X,Y) takes
values over the interval @xmath .

### 2.4 Integration

As we previously hinted at, mutual information can be used to measure
the degree of statistical dependence between two elements or subsets of
elements when they are associated to two random variables or two random
vectors respectively.
We can generalize this concept in order to measure the degree of
interaction among @xmath elements of the system. To do so, let us
consider a system @xmath composed of @xmath elements. The system
dynamics is described using a random field, i.e. @xmath , where @xmath
is a discrete random variable associated with the @xmath -th element.
@xmath assumes value in a finite alphabet @xmath . Let @xmath be a set
of @xmath elements of the system, such that @xmath , i.e. @xmath is a
subset of @xmath . Without loss of generality, let @xmath be the random
variables associated with the @xmath elements.
If all the random variables are mutually independent, the entropy of the
whole subset @xmath can be computed as follows

  -- -------- -- --------
     @xmath      (2.35)
  -- -------- -- --------

If there are interactions among the elements of @xmath , then a degree
of statistical dependence arises among the @xmath random variables. It
follows an entropy reduction that can be measured by means of
integration which is an information-theoretic measure defined as follows
[ 37 ]

###### Definition 2.7.

  -- -------- -- --------
     @xmath      (2.36)
  -- -------- -- --------

where @xmath is the random vector composed of all the variables which
are in @xmath .

The following theorem introduces a lower and an upper bounds for
integration.

###### Theorem 2.10.

Given a discrete random vector @xmath with alphabet @xmath and a joint
probability distribution @xmath . Let be @xmath , then

  -- -------- -- --------
     @xmath      (2.37)
  -- -------- -- --------

###### Proof.

From 2.7 we obtain

  -- -------- -- --------
     @xmath      (2.38)
  -- -------- -- --------

From 2.29 we can state that for @xmath

  -- -------- -- --------
     @xmath      (2.39)
  -- -------- -- --------

Hence

  -- -------- -- --------
     @xmath      (2.40)
  -- -------- -- --------

To prove that @xmath we use 2.9

  -- -------- -- --------
     @xmath      (2.41)
  -- -------- -- --------

If @xmath is uniformly distributed for @xmath then @xmath assumes its
maximum value (i.e. @xmath . In this case, the minimum value of @xmath
is achieved when @xmath assumes, with probability greater than zero,
only @xmath out of the @xmath possible values. Under this assumption it
follows that @xmath and hence

  -- -------- -- --------
     @xmath      (2.42)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (2.43)
  -- -------- -- --------

∎

For all of these features we can use integration and mutual information
to detect relevant subsets (RS) in complex dynamical systems. As we
previously hinted at, a RS is a subset of state variables which heavily
influence the system dynamics. It is composed of variables which have a
strong mutual interaction and that weakly interact with variables which
do not belong to the subset. Integration can be used as internal
criterion: it is a measure of the internal interaction among the
elements of a RS. At the same time, mutual information can be used as an
external criterion: it is a measure of the interaction between the RS
and the rest of the system (or between two relevant subsets). A
candidate relevant subset is characterized by a high integration value
and a low mutual information value.
It’s worth noting that both integration and mutual information scale
with the size of a relevant subset. We can see this behavior in 2.2 ,
2.3 , where integration and mutual information values are computed
analyzing the dynamics of a catalytic reaction network composed of 26
molecules (see chapter @xmath for more details). In order to compare RSs
of different sizes, a normalization procedure has been developed. It
will be presented in the next chapter.

### 2.5 Entropy Rate and Transfer Entropy

As previously explained, mutual information can be used to measure the
interaction between two relevant subsets. Since mutual information is a
symmetric measure, it does not provide indications on the information
flows directions. Furthermore, it does not take into account dynamical
information which are needed to detect the subsets which most influence
the system dynamics. We say that a subset @xmath affects the behavior of
a subset @xmath if the net information flow goes from @xmath to @xmath .
Hence, the direction and the amount of information flows is needed to
select the leaders, i.e. the most influential RSs. We call these subsets
Functional Dynamical Structures (FDS) to stress their functional role in
the system dynamics. A FDS sends a high amount of information to the
other subsets conditioning their behavior. In order to measure the
dynamical information flow between two relevant subsets, we used a new
measure based on the transfer entropy [ 28 ] . We recall some notions
from stochastic processes theory [ 2 ] and we define the entropy rate
which is used in the transfer entropy definition.

###### Definition 2.8 (Stochastic Process).

A stochastic process @xmath is a sequence of discrete random variables
with alphabet @xmath and a joint probability distribution @xmath for
@xmath .

###### Definition 2.9 (Markov Process).

A stochastic process @xmath is called a Markov Process or Markov Chain
if

  -- -------- -- --------
     @xmath      (2.44)
  -- -------- -- --------

for @xmath and for all @xmath .

###### Definition 2.10 (Stationary Markov Process).

A Markov Process @xmath is stationary if

  -- -------- -- --------
     @xmath      (2.45)
  -- -------- -- --------

i.e. @xmath is independent with respect to @xmath .

In order to introduce the notion of entropy rate, let us consider a
system @xmath composed of @xmath elements. The system dynamics is
described using a random field evolving in time, i.e. @xmath , where
@xmath is a discrete random variable associated with the @xmath -th
element at time @xmath . @xmath assumes value in a finite alphabet
@xmath . Hence, if we fix @xmath , @xmath is a random field while, if we
fix @xmath , @xmath is a stochastic process. Let assume that @xmath is a
stationary Markov process of order k. Under this assumption, the
probability to find @xmath in state @xmath at time @xmath satisfies the
following relation

  -- -------- -- --------
     @xmath      (2.46)
  -- -------- -- --------

We adopt the following shorthand notation proposed by Schreiber [ 28 ]

  -- -------- -- --------
     @xmath      (2.47)
  -- -------- -- --------

for words of length @xmath .

Let us now define the entropy rate as follows

###### Definition 2.11 (entropy rate).

Given a stationary Markov process @xmath of order @xmath , the entropy
rate @xmath is

  -- -------- -- --------
     @xmath      (2.48)
  -- -------- -- --------

for @xmath and for all @xmath .

Since @xmath , we can express @xmath as follows

  -- -------- -- --------
     @xmath      (2.49)
  -- -------- -- --------

where @xmath and @xmath are the entropies of the process @xmath
considering two delay vectors of dimension @xmath and @xmath
respectively.

In order to find the direction of the information flow which takes place
between two elements @xmath and @xmath , we have to generalize the
entropy rate. The idea is to measure the deviation from the following
generalized Markov property

  -- -------- -- --------
     @xmath      (2.50)
  -- -------- -- --------

If there is an information flow from @xmath to @xmath , then the
transition probabilities of @xmath depends on the states of @xmath . In
this case the entropy rate is

  -- -------- -- --------
     @xmath      (2.51)
  -- -------- -- --------

Otherwise, if there is not an information flow from @xmath to @xmath ,
then the transition probabilities of @xmath are independent from the
state of @xmath . In this case the entropy rate becomes

  -- -------- -- --------
     @xmath      (2.52)
  -- -------- -- --------

In order to measure the degree of statistical dependence of the
transitional probabilities of @xmath on that of @xmath , we can use the
transfer entropy which is defined as follows

  -- -------- -- --------
     @xmath      (2.53)
  -- -------- -- --------

At the same time, we can define the transfer entropy from @xmath to
@xmath as follows

  -- -------- -- --------
     @xmath      (2.54)
  -- -------- -- --------

We can express transfer entropy in terms of conditional entropies as
follows

  -- -------- -- --------
     @xmath      (2.55)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (2.56)
  -- -------- -- --------

It’s worth noting that the transfer entropy is explicitly non-symmetric:
@xmath is in general different from @xmath . In the next chapter, we
present an index to detect the directionality of information flow
between two relevant subsets.

## Chapter 3 Dynamical Cluster Index Methodology

  An idea which can be used once is a trick. If it can be used more than
  once it becomes a method.

  G. Polya and S. Szegö

### 3.1 The Dynamical Cluster Index

The DCI is an extension of the Functional Cluster Index (CI) introduced
by Edelman and Tononi in 1998 [ 35 ] and aimed at detecting functional
clusters in brain regions. In our work [ 40 ] , we relax the stationary
constraint and extend the CI to actual dynamical systems, so as to apply
it to a wide range of system classes, from abstract models to biological
models.
As in our work we rely on observational data, probabilities are
estimated by the relative frequencies of the values observed for each
variable.
Let us now consider a system @xmath composed of @xmath variables (e.g.
agents, chemicals, genes, artificial entities) and suppose that @xmath
is a subset composed of @xmath elements, with @xmath . The value @xmath
is defined as the ratio between two aforementioned measures: the
integration ( @xmath ) and the Mutual Information ( @xmath ).
As previously stated, @xmath measures the statistical independence of
the @xmath elements in @xmath and is defined as:

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

As previously stated, mutual information @xmath measures the mutual
dependence between subset @xmath and the rest of the system @xmath and
it is defined as:

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

hence, the DCI,

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

The value of DCI is not defined if @xmath . However, a vanishing mutual
information is a sign of separation of the investigated subset from the
rest of the system, and therefore the subset must be studied separately.
It is worth noting that the DCI scales with the subset size. In [ 40 ]
we show a procedure to normalize it, nevertheless a better approach is
that of assessing the statistical significance of the DCI of @xmath by
means of a statistical significance index [ 35 ] :

  -- -- -- -------
           (3.4)
  -- -- -- -------

where @xmath and @xmath are respectively the average and the standard
deviation of the @xmath of a sample of subsets of size @xmath extracted
from a reference system @xmath randomly generated according to the
frequency of each single state in @xmath and @xmath is the normalization
constant. It is worth noting that the aim of the reference system is
that of quantify the finite size effects affecting the information
theoretical measures on a random instance of a system with finite
dimensions.

### 3.2 DCI: remarks and observations

As previously stated, the two factors used to compute the @xmath of a
candidate RS are the integration of its elements and the mutual
information between the subset and the rest of the system. Therefore,
the @xmath of a subset @xmath of variables in a system—i.e., a possible
candidate RS—is then estimated by collecting a set of system states and
then by computing the entropy values of the possible combinations of
variables in @xmath . Therefore, all we need is just a collection of
observations of the system and, in principle, no information on the
topology or on the internal mechanics of the system are required. The
rationale behind the ratio between @xmath and @xmath is that a candidate
RS should express higher interactions among its components than with the
rest of the system. In spite of the simplicity of the concept, one must
carefully consider the meaning of the two quantities and the way they
are combined. In fact, the implicit assumption here is that it holds
@xmath and that the orders of magnitude of @xmath and @xmath are
reasonably close. However, there might be cases in which @xmath , which
denotes integrated subsets dynamically independent from the rest of the
system. Furthermore, we may be interested in finding the most integrated
subsystems among the ones that exchange information with the rest of the
system. Moreover, it is important to remark that insightful information
can be also provided by analyzing the two factors composing the @xmath ,
before computing their ratio. For this reason, an assessment of the
information brought by the individual values of @xmath and @xmath can
shed light on the potential of the application of these
information-theoretic measures to detect candidate RSs.

### 3.3 Integration vs. mutual information

To assess the contribution of @xmath and @xmath we first study their
statistical behavior with respect to subsystem size. We analyzed several
dynamical systems and the results obtained show that the general trend
respects the theoretical findings [ 35 ] , as shown in Figure 3.1 and
Figure 3.2 .

As we can observe, @xmath scales approximately linearly, whilst @xmath
is non-monotonic. We have previously proved that the maximal value of
@xmath for a subset of size @xmath equals @xmath . Hence, we can define
a rescaled version of @xmath , @xmath , useful to compare the
integration of subsets of different dimensions. Moreover, the addition
of a variable @xmath to the considered subset @xmath increases @xmath by
1 if @xmath deterministically depends on any variable in @xmath , while
it leaves @xmath unchanged if it assumes random values. These properties
may be useful to reckon the relative importance of integration values
computed for subsystems of different size.

#### 3.3.1 Experiments on a tuneable model

We are interested in understanding the individual informative
contribution of @xmath and @xmath . To this aim, we study these measures
on a simple model in which the integration among variables in a
subsystem under observation and its mutual information can be tuned by
acting on two parameters. The model abstracts from specific functional
relations among elements of the system and could resemble a basic
leader-followers model (see Figure 3.3 ). The system is composed of a
vector of @xmath binary variables @xmath , e.g., representing the
opinion in favor or against a given proposal. The model generates
independent observations of the system state, i.e. each observation is a
binary @xmath -vector generated independently of the others, on the
basis of the following rules:

-   Variables are divided into two groups, @xmath and @xmath ;

-   @xmath is called the leader and it is assigned a random value in
    @xmath ;

-   the value of the followers @xmath is set as a copy of @xmath with
    probability @xmath and randomly with probability @xmath ;

-   the values of elements of @xmath are assigned as a copy of a random
    element in @xmath with probability @xmath , or a random value with
    probability @xmath .

It is possible to tune the integration among elements in @xmath and the
mutual information between @xmath and @xmath by changing @xmath and
@xmath . Note that, given significant level of integration, we have two
notable cases:

-   @xmath isolated (possibly integrated) RS;

-   @xmath integrated and segregated cluster.

The possible scenarios which can be obtained by tuning @xmath and @xmath
can be conveniently illustrated by a 3-dimensional plot. Figure 3.4
shows the behavior of integration of @xmath as a function of @xmath and
@xmath . We can observe that it is a decreasing function of @xmath ,
while it is independent of @xmath (by definition, indeed). The behavior
of the mutual information between @xmath and @xmath is depicted in
Figure 3.5 . As we can observe, @xmath increases fast with @xmath , as
this parameters increases the correlation between variables in @xmath
and @xmath . Moreover, it also increases with @xmath , but the reason is
that the correlation among variables in @xmath increases the randomness
of variables in @xmath , which behaves similarly to the variables in
@xmath .

The case of @xmath corresponds to the situation in which @xmath is
almost completely independent of @xmath and can be easily detected by
observing only @xmath . Conversely, if we are interested in discovering
@xmath as significant RS, then we would consider cases in which both
@xmath and @xmath are significantly high. In our experiments, these
cases correspond to @xmath and @xmath . In these scenarios, we find that
it is possible to detect @xmath by using only @xmath , divided by @xmath
. It is important to mention that, when @xmath is slightly higher, it is
necessary to resort to the computation of the DCI to detect group @xmath
.

### 3.4 The Relevant Subset Detection Algorithm

As we previously hinted at, the dynamical cluster index is used to
detect the Candidate Relevant Subsets (CRSs) of a system. In order to
develop a methodology to detect CRSs, let us consider a system @xmath
composed of @xmath elements. The system dynamics is described using a
random field, i.e. @xmath , where @xmath is a discrete random variable
associated with the @xmath -th element. @xmath assumes value in a finite
alphabet @xmath . Let @xmath be a set of @xmath elements of the system,
such that @xmath , i.e. @xmath is a subset of @xmath . In order to get a
list of Candidate Relevant Subsets (CRSs), we compute the @xmath of
every possible subset of variables in @xmath and ranking the subsets by
@xmath values. The subsets occupying the first positions are most likely
to play a relevant role in system dynamics. For large-size systems,
exhaustive enumeration is computationally impractical as it requires to
enumerate the power set of @xmath . In this case, we resort to a genetic
algorithms.
The list of CRSs can be ranked according to the significance of their
@xmath . We can directly use this ranking to identify by hand the
relevant CRSs for the dynamics of the system. Nevertheless, in many
cases this analysis might return a huge list of entangled CRS, so that a
direct inspection is required for explaining their relevance. To this
aim, we present a DCI analysis post-processing sieving algorithm to
reduce the overall number of CRS to manually tackle. The main assumption
of the algorithm is that if a CRS @xmath is a proper subset of CRS
@xmath , i.e. @xmath , then only the subset with the higher DCI is
maintained between the two. Thus, only disjoint or partially overlapping
CRSs are retained: the used assumption implies that the remaining CRSs
are not further decomposable, forming in such a way the “building
blocks” of the dynamical organization of the system. The pseudo-code of
the algorithm is presented in Algorithm 1 .

Input: The array @xmath of all the @xmath ranked by their @xmath (DCI)

Output: A subset @xmath

@xmath

@xmath

Initialize auxiliary array @xmath

for @xmath to @xmath do

for @xmath to @xmath do

if @xmath then

if @xmath then

@xmath

end if

end if

end for

end for

for @xmath to @xmath do

if @xmath then

@xmath

end if

end for

Algorithm 1 Sieving algorithm

### 3.5 Temporal Correlation: the D Index

Although by the application of the DCI, CRSs are detected, this measure
does not provide indications neither on the quantity nor on the
direction of the information which flow among subsets. To this aim we
apply the directionality index proposed in [ 18 ] .

Let @xmath and @xmath be two random variables—or, equivalently, two sets
of variables. As mentioned in the previous chapter, we can define the
entropy rate of @xmath as the average number of bits needed to encode a
successive state of @xmath if all the previous states are known,
considering that the value of @xmath at @xmath depends either on @xmath
and @xmath at the time @xmath , eq. 3.5a , or just on the value of
@xmath at the time @xmath , eq. 3.5b . ¹ ¹ 1 Note that the temporal
dependency is not necessarily of unitary lag, i.e. @xmath . For a
complete assessment of the statistical dependency of @xmath on @xmath
one should sum over @xmath , where @xmath is the observation time limit.
Nevertheless, note that (i) in this work we are analyzing Markovian
systems, whose behavior depends only from the immediately previous step
and (ii) although TE is not a direct measure of causal effect, the use
of short history length alters the character of the measure towards
inferring causal effect [ 22 ] .

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (3.5a)
     @xmath   @xmath      (3.5b)
  -- -------- -------- -- --------

then, the transfer entropy @xmath is defined as the difference between
the aforementioned entropy rates. @xmath describes how the uncertainty
of @xmath is reduced by knowing the previous states of @xmath and @xmath
itself, eq. 3.6 .

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

Thus, @xmath can be described in term of entropy as:

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

and, since the @xmath describes the information moving from @xmath to
@xmath , and the transfer entropy is not symmetric, the information from
@xmath to @xmath is computed as well, eq. 3.8 .

  -- -------- -- -------
     @xmath      (3.8)
  -- -------- -- -------

Once that @xmath and @xmath are known, the directionality @xmath of the
information flow between @xmath and @xmath can be measured as:

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

where @xmath indicates that all the information moves from @xmath to
@xmath , i.e. absence of information flow from @xmath to @xmath and,
conversely, @xmath indicates that @xmath , with respect to @xmath , is
just an information receiver and not an information sender.
It is worthwhile to notice that @xmath does not provide any indication
about the amount of information exchanged between the variables, but it
only provides suitable indications on the direction of the information
flow.

## Chapter 4 Experimental Results

  It doesn’t matter how beautiful your theory is, it doesn’t matter how
  smart you are. If it doesn’t agree with experiment, it’s wrong.

  Richard P. Feynman

### 4.1 Boolean Networks

We consider a system composed of @xmath boolean nodes updated on the
basis of either a boolean function or a random boolean value generator.
Nodes update their state in parallel and synchronously. We illustrate
the results of @xmath instances of this network, defined in Table 4.1 ¹
¹ 1 Note that the size of these systems allows for an exhaustive
enumeration of all the possible groups, allowing their complete
assessment. It is worth remarking that each perturbation is introduced
after the system has recovered a stable dynamical situation. . The
@xmath instances share a common structure but differ in specific
dynamical organizations of some nodes.

-   In case 1 , we consider two integrated groups of three nodes
    (namely, group A and group B), by assigning at each node the @xmath
    function of the other two nodes in the group. In this case the
    seaving algorithm filtered the @xmath of the evaluated CRS, making
    it possible to easily identify the subsets responsible for the
    dynamical organization of the system. Only the two correct CRSs have
    high @xmath values, whereas all the other ones have @xmath values
    lower of more than 2 orders of magnitude. Moreover, no information
    exchange takes place between group A and group B, as they are
    structurally independent.

-    Case 2 derives from case @xmath by introducing in the first node of
    group B a further dependence from the last node of group A, hence
    introducing information transfer from group A to group B. The
    combination of the DCI analysis and the sieving algorithm correctly
    recalls the dynamical organization of the system—i.e. group A
    influences the behavior of (a part of) group B. We observe that the
    whole group B (Figure 4.1  b) was anyway ranked high w.r.t. its DCI
    significance, but it was discarded by the sieving algorithm because
    the dynamics of its first node is influenced also by group A and so
    the assessment of the whole group B is weakened. In general, the
    amount of this difference depends on the strength of the forces that
    influence the interface nodes and the elements interfacing different
    CRS can be detected by a simple comparison between the DCI analysis
    and the sieving algorithm outputs.

-    Case 3 derives from case 2 by introducing a further dependence of
    the first node of group A from the last node of group B: again, the
    combination of the DCI and the sieving algorithm detects the
    interface nodes and the underlining dynamical system organization.
    Note that the asymmetry in transfer entropies (and therefore in
    @xmath index) are due to differences in the initial conditions of
    the boolean network trajectories: a shift in initial conditions can
    change the direction of this asymmetry.

-   In case 4 five heterogeneously linked nodes are influenced by a
    triplet that is identical to that of group A. The combination of the
    dynamical rules of the nodes and their initial condition makes the
    dynamical behavior of the sixth node always in phase with the
    triplet, so our analysis individuates this quartet as the most
    relevant CRS. The other dynamical relations are not sufficiently
    strong to coordinate all the @xmath nodes, nevertheless their
    influence creates some masks ² ² 2 CRSs can be represented by rows,
    where entries corresponding to variables belonging to CRS are marked
    in black ( “masks”in the following) having high @xmath values,
    partially overlapped with the leading quartet. The overlap of these
    masks indirectly indicates the presence of a greater group with
    respect to the winning quartet, but having a less evident dynamical
    presence (see Figure 4.1 b).

-    Case 5 derives from case 1 by adding two nodes whose dynamical
    behavior directly depends on nodes of both group A and group B:
    these 8 nodes form therefore a group clearly different from the
    remaining 4 random nodes, as they are interdependent and ruled by
    deterministic functions. This group is identified by the plain DCI
    method, but the combination of DCI and sieving algorithm strikingly
    enlightens the interpretation of two triplets directly influencing a
    couple of nodes (Figure 4.1 a).

### 4.2 Gene regulatory networks

In this section we show the application of our method to two models of
regulatory networks: a model of mammalian cell cycle network (MCC in the
following), as “booleanized” in [ 6 ] —see Table 4.2 for the chosen
boolean model—, and a model of one of the major cellular signal
transduction pathways, known as the Mitogen Activated Protein Kinase
(MAPK) cascade [ 42 ] .

#### 4.2.1 Mammalian cell cycle networks

In [ 6 ] the authors provides a boolean dynamical model of the mammalian
cell cycle, able to reproduce the main characteristics of the succession
of molecular events leading to the reproduction of the genome of a cell
and its division into two daughter cells.
Mammalian cell division must be coordinated with the overall growth of
the organism; this coordination is achieved through extra-cellular
signals whose balance decides whether a cell will divide or remain in a
resting state. The positive signals or growth factors ultimately elicit
the activation of Cyclin D (CycD) in the cell. In the proposed model
CycD thus represents the input and its activity is considered constant.
By pointing the interested reader to [ 6 ] for the details, for now it
is enough to say that in absence of CycD the system presents a unique
stable attractor where only Rb, p27 and Cdh1 are active, whereas in its
presence E2F, CycE, CycA, Cdc20, Cdh1, UbcH10 and CycB cycle with a
period of length 7. We perturb both asymptotic states, obtaining in each
case only one group (composed of E2F, CycE, CycA, Cdc20, Cdh1, UbcH10
and CycB in the first case, and of Rb, E2F, p27, Cdc20, UbcH10 and CycB
in the second case). The leading groups of the two situations are
different, but in each case the other CRS identified overlap with these
groups and their sum cover the whole system, indicating the presence of
a single coordinated pattern. So, the analysis indicates that the
elements of the mammalian cell cycle network act as a single compact
engine, see Figure 4.2 b.

#### 4.2.2 Matabolic pathway MAPK

The MAPK pathway (evolutionarily conserved from yeasts to humans)
responds to a wide range of external stimuli, triggering growth, cell
division and proliferation [ 27 ] . [ 27 ] also introduce the models
considered in our analysis. The basic model is composed of reactions
that are quite well-established from an experimental viewpoint, and it
has the hierarchical structure shown in Figure 4.3 a. The three
chemicals identified as the core of this three-layered system are the
@xmath , @xmath and @xmath kinases (respectively @xmath , @xmath and
@xmath for short) [ 42 ] . @xmath is activated by means of a single
phosphorylation whereas @xmath and @xmath are both activated by double
phosphorylation. Parallel to the phosphorylation by kinases,
phosphatases present in the cellular volume can dephosphorylate the
phosphorylated kinases (Figure 4.3 a shows the schema of the @xmath
cascade where each layer of the cascade is dephosphorylated by a
specific phosphatase). Note that superimposed on the three-layered
structure of substrates-product reactions there is the properly called
@xmath signalling cascade, a linear chain of catalysis (dashed lines in
Figure 4.3 a) that transmit the external signal from @xmath to @xmath .
³ ³ 3 The symbol “ @xmath ” indicates the phophorylated version of the
molecule.

When the external signal and the concentrations of the phosphatases are
kept constant, a top-down reaction scheme as the one described in Figure
4.3 a leads to fixed-point solutions. On the other hand, oscillations
have been reported in the MAPK cascade [ 32 ] and, in order to account
for them, [ 27 ] adopt a models with feedback, one of which is described
in Figure 4.3 b. This variant (called S2 in the following) is
characterized by a configuration of the activating and inhibiting
interactions among layers that alters the  “layered” structure of the
basic model, which is no longer strictly hierarchical. This alternative
model is grounded on experimental data; we will not enter here a
discussion about the merits and limits of the model, referring the
interested reader to the original paper, but we will take it “for
granted” and we will apply our method to test whether it can discover
significant dynamical organization, without any prior knowledge of the
interactions, but on the sole basis of the dynamics of concentrations.
We simulate the [ 27 ] models with the CellDesigner software [ 9 , 10 ]
, keeping the P1, P2 and P3 phosphatases as constant (as they do)
obtaining the same asymptotic states shown by the authors. In order to
apply our method we perturb the asymptotic states of these models: in
particular, we focus our analysis on kinases perturbations. In
particular, we perform @xmath perturbation cycles, each cycle involving
the perturbation of each single kinase and the successive relaxing to a
stable situation before the subsequent perturbation [ 40 ] . The stable
situations that are reached can show both oscillating (S2 system) or
constant concentrations (basic system). Concentration changes are more
significant than their absolute values (it is important to monitor the
variables that are changing in coordinate way); therefore the continuous
concentration values are represented according to a three levels code
related to the sign of the time derivatives at time @xmath ( “decreasing
concentration”,  “no significant change”,  “increasing concentration”).
The combination of DCI and sieving algorithm applied to the basic MAPK
model detects two dynamical groups: the first including the first layer
of Figure 4.3 a and the second including the other two layers). The two
groups exchange information, the second transmitting more information to
the first one (see Figure 4.4 ). The introduction of the feedbacks
changes system dynamics: there are still two dynamically relevant
groups, now composed of the second layer and by the other two layers,
respectively. The analysis therefore suggests that the MAPK system may
be decomposed in two dynamically distinct parts.

## Chapter 5 Conclusion

  This is not the end. It is not even the beginning of the end. But it
  is, perhaps, the end of the beginning.

  Winston Churchill

In this thesis we introduced a methodology based on information theory
to identify functional dynamical structures in complex systems. To do so
we modeled a system using a random field evolving in time. In
particular, the dynamical behavior of each system component is described
by means of a discrete stochastic process. We observed the system
evolution in a given time interval and we estimated the probability
distributions of the variables belonging to the random field. Then,
exploiting information-theoretic measures (Dynamical Cluster Index and
D-index), we looked for functional dynamical structures.
The method does not require any previous knowledge of underlying network
topology of the complex system, but relies only on the values assumed by
the random fields during the observation time interval.
The effectiveness of the methodology has been validated on test cases
and subsequently applied to two prominent biological models, i.e. the
mammalian cell cycle network and Mitogen Activated Protein Kinase (MAPK)
cascade.

1.  Random boolean networks.

2.  Mammalian cell cycle networks.

3.  Mitogen Activated Protein Kinase (MAPK) cascade.

### 5.1 Contributions and Novelty of the Work

The main novelty of the thesis, in comparison to previous application of
the cluster index and of similar measures [ 35 ] is that we use it to
consider truly dynamical systems, and not only fluctuations around
stable asymptotic states. In principle, different kinds of data can be
considered. In the case of a deterministic dynamical system, attractors
are the main candidates to provide the required time series and we have
shown in case (a) that they can work effectively. Note however that the
method is ineffective in a situation that is sometimes encountered, i.e.
if there is just a single fixed point. Therefore we conclude that the
use of attractor states is only effective if the attractor landscape is
rich enough to show the main features of the system organization. This
has usually to be evaluated a posteriori, with the exception of trivial
cases like that of a single fixed point.
It is worth noting that the three-level coding used in case (c) regards
the similarities of the derivatives rather than those of the values of
the variables and that the models that were used in this work to
generate the time series are all based on first-order ordinary
differential or difference equations; it should be verified whether the
approach is valid also when higher-order dynamical systems are
considered. Of course, high-order ODE systems can be transformed in
first-order systems by adding variables, but the auxiliary variables
that are required might turn out to be unobservable.
When a system is subject to continuous external disturbances the time
series directly provide the required data and the experimental results
show that our treatment can reveal its organizational features even when
a high noise level is present.
Actually, the range of applicability of this method is quite broad and
it does not necessarily need to be limited to dynamical system. Indeed
the method just needs a set of frequencies of co-occurences of the
values of the system variables. So the method can be applied also to
many other systems, since all that is required is a series of ”cases”
associated to vectors of numerical variables that are not necessarily
ordered in time (think for example of different patients, each one
described by a vector of values of various symptoms).
A final comment is that the method is not a brute-force one: when there
is a clear organization in the system, e.g. in case (a), the
organization can be read out directly from the order list of candidate
relevant sets, but this does not always happen. The study of case (b)
and (c) shows that even in entangled real systems the method provides
useful clues to uncover the system organization. In order to describe
the dynamics of these entangled systems, we introduce a sieving
algorithm which selects disjoint or partially overlapping CRSs with the
most high DCI values and we used the D-index in order to recover the
direction of information flow among these CRSs.

### 5.2 Future Work

As far as future directions of this thesis are concerned, let us only
mention some major ones. Several aspects have been mentioned above and
will be subject to further analysis, including the effective analysis of
high-order dynamical systems with a simplified discrete coding and the
comparison among different kinds of time series data, i.e. attractors,
transients from arbitrary initial conditions and perturbations.
Our method needs discrete variables, so when dealing with continuous
data we have introduced a three-level coding that necessarily misses
some information. Different coding schemes could be compared, moreover
one might also consider the continuous generalization of the proposed
method.
Other research will be devoted to improvements of the method: it is
apparent that it faces a huge computational problem for large systems,
since the number of possible subsets increases extremely fast with the
number of variables. We used a genetic algorithm to efficiently search
the optimal candidate relevant subsets in terms of Dynamical Cluster
Index. However, the evaluation of the fitness function can be very
expensive when high dimensional systems are considered. In order to deal
with high dimensionality, a more efficient fitness function needs to be
developed and other heuristic algorithms need to be tested.
Several other specific cases need to be studied to confirm the validity
of the approach, mainly from real-world data. Let us mention that, among
others, we are considering applications to the study of innovation
processes, where data come from the real world and not from models. In
this respect, it is important to realize that the DCI methodology may be
integrated with other approaches: when dealing with real-world problems
and not from data generated by a perfectly known model, a typical
situation that is often encountered is that one knows some relationships
between variables, but not them all. In this case, the most promising
approach would be based on a combination of the a priori knowledge with
the DCI, in ways that still need to be tested.
Moreover, it should be recalled that the Dynamical Cluster Index is just
one out of several information-theoretic measures that might be applied
to analyze dynamical systems. It is worth noting that the integration
and the mutual Information can be useful even if used in isolation and
not combined together in the DCI. But other measures, e.g. those that
refer to joint distributions at different times, might prove to be
particularly useful for the study of dynamical systems.
Finally, it is worth noting that the information exchange assessment is
just preliminary; we are now working on a method to normalize and then
to evaluate the significance of the information transmitted among CRS.
