## Chapter 1 Introduction

In systems biology, it is becoming increasingly common to measure
biochemical entities at different levels of the same biological system.
Hence, data fusion problems, which focus on analyzing such data sets
simultaneously to arrive at a holistic understanding of the studied
system, are abundant in the life sciences. With the availability of a
multitude of measuring techniques, one of the central problems is the
heterogeneity of the data. In this thesis, we mainly discuss two types
of heterogeneity. The first one is the type of data , such as
metabolomics, proteomics and RNAseq data in genomics. These different
omics data reflect the properties of the studied biological system from
different perspectives. The second one is the type of scale , which
indicates the measurements obtained at different scales, such as binary,
ordinal, interval and ratio-scaled variables. In genomics, an example is
the measurements of gene-expression and point mutation status on the
same objects. The latter are binary data and gene-expression
measurements are quantitative data. Ideally, data fusion methods should
consider these two types of heterogeneity of such measurements and this
will be the topic of this thesis.

The goal of this thesis is to develop appropriate statistical methods
capable to fuse data sets of the two types of heterogeneity. Before
going into the details of the developed methods, we begin with a brief
introduction of the concept of data fusion in life sciences and the
characteristics of the two types of heterogeneity. Another important
concept in this thesis is the concave penalty, which is the basis of all
the developed methods. Although it is not directly related to the fusion
of heterogeneous data sets, it is also introduced in Chapter 1 . ¹ ¹ 1
This chapter is based on Smilde, A.K., Song, Y., Westerhuis, J.A.,
Kiers, H.A., Aben, N. and Wessels, L.F., 2019. Heterofusion: Fusing
genomics data of different measurement scales. arXiv preprint
arXiv:1904.10279.

### 1.1 Data fusion in life sciences

With the availability of comprehensive measurements collected in
multiple related data sets in the life sciences, the need for a
simultaneous analysis of such data to arrive at a global view on the
system under study is of increasing importance. There are many ways to
perform such a simultaneous analysis and these go also under very
different names in different areas of data analysis: data fusion, data
integration, global analysis, multi-set or multi-block analysis to name
a few. We will use the term data fusion in this thesis. Data fusion
plays an especially important role in the life sciences, e.g., in
genomics it is not uncommon to measure gene-expression (array data or
RNAseq data), methylation of DNA and copy number variation. Sometimes,
also proteomics and metabolomics measurements are available. All these
examples serve to show that having methods in place to integrate these
data is not a luxury anymore.

Without trying to build a rigorous taxonomy of data fusion it is
worthwhile to distinguish several distinctions in data fusion. The first
distinction is between model-based and exploratory data fusion. The
former uses background knowledge in the form of models to fuse the data;
one example being genome-scale models in biotechnology [ 1 ] . The
latter does not rely on models, since these are not available or poorly
known, and thus uses empirical modeling to explore the data. In this
thesis, we will focus on exploratory data fusion. The next distinction
is between low-, medium-, and high-level data fusion [ 2 ] . In
low-level data fusion, the data sets are combined at the lowest level,
that is, at the level of the (preprocessed) measurements. In
medium-level data fusion, each separate data set is first summarized,
e.g., by using a dimension reduction method or through variable
selection. The reduced data sets are subsequently subjected to the data
fusion. In high-level data fusion, each data set is used for prediction
or classification of an outcome and the prediction or classification
results are then combined, e.g, by using majority voting [ 3 ] . All
these types of data fusion have advantages and disadvantages which are
beyond the scope of this thesis. In this thesis, we will focus on low-
and medium-level fusion.

The final characteristic of data fusion relevant for this thesis is the
heterogeneity of the data sets to be fused. Different types of
heterogeneity can be distinguished. The first one is the type of data ,
such as metabolomics, proteomics and RNAseq data in genomics. Clearly,
these data relate to different parts of the biological system. The
second one is the type of scale in which the data are measured present
in the fusion problem. In genomics, an example is a data set where
gene-expressions are available and mutation data in the form of single
nucleotide polymorphisms(SNPs). The latter are binary data and
gene-expression measurements are quantitative data. They are clearly
measured at a different scale. Ideally, data fusion methods should
consider these two levels of heterogeneity in data analysis and this
will be the topic of this thesis. In the following section, we will show
the characteristics of these two types of heterogeneity and how they
affect the data analysis.

### 1.2 Two types of heterogeneity

#### 1.2.1 Heterogeneous measurement scales in multiple data sets

Multiple data sets measured on the same objects may have different types
of measurement scales. The history of measurement scales goes back a
long time. A seminal paper drawing attention to this issue appeared in
the 40-ties [ 4 ] . Since then numerous papers, reports and books have
appeared [ 5 , 6 , 7 , 8 , 9 , 10 ] . The measuring process assigns
numbers to aspects of objects (an empirical system ), e.g, weights of
persons. Hence, measurements can be regarded as a mapping from the
empirical system to numbers, and scales are properties of these
mappings. In measurement theory, there are two fundamental theorems [ 6
] : the representation theorem and the uniqueness theorem. The
representation theorem asserts the axioms to be imposed on an empirical
system to allow for a homomorphism of that system to a set of numerical
values. Such a homomorphism into the set of real numbers is called a
scale and thus represents the empirical system. A scale possesses
uniqueness properties: we can measure the weight of persons in kilograms
or in grams, but if one person weighs twice as much as another person,
this ratio holds true regardless the measurement unit. Hence, weight is
a so-called ratio-scaled variable and this ratio is unique. The
transformation of measuring in grams instead of kilograms is called a
permissible transformation since it does not change the ratio of two
weights. For a ratio-scaled variable, only similarity transformations
are permissible; i.e. @xmath where @xmath is the variable on the
original scale and @xmath is the variable on the transformed scale. This
is because

  -- -------- --
     @xmath   
  -- -------- --

Note that this coincides with the intuition that the unit of measurement
is immaterial.

The next level of scale is the interval-scaled measurement. The typical
example of such a scale is concentrations of metabolites in metabolomics
research and the permissible transformation is affine, i.e. @xmath . In
that case, the ratio of two intervals is unique because

  -- -------- --
     @xmath   
  -- -------- --

Stated differently, the zero point and the unit are arbitrary on this
scale.

Ordinal-scaled variables represent the next level of measurements.
Typical examples are scales of agreement in surveys: strongly disagree,
disagree, neutral, agree and strongly agree. There is a rank-order in
these answers, but no relationship in terms of ratios or intervals. The
permissible transformation of an ordinal-scale is a monotonic increasing
transformation since such transformations keep the order of the original
scale intact. Nominal-scaled variables are next on the list. These
variables are used to encode categories and are sometimes also called
categorical. Typical examples are gender, race, brands of cars and the
like. The only permissible transformation for a nominal-scaled variable
is the one-to-one mapping. A special case of a nominal-scaled variable
is the binary (0/1) scale. Binary data can have different meanings; they
can be used as categories (e.g. gender) and are then nominal-scale
variables. They can also be two points on a higher-level scale, such as
absence and presence (e.g. for methylation data).

The above four scales are the most used ones but others exist [ 5 , 6 ]
. Counts, e.g., have a fixed unit and are therefore sometimes called
absolute-scaled variables [ 8 ] . Another scale is the one for which the
power transformation is permissible; i.e. @xmath which is called a
log-interval scale because a logarithmic transformation of such a scale
results in an interval-scale. An example is density [ 6 ] . Sometimes
the scales are lumped in quantitative (i.e. ratio and interval) and
qualitative (ordinal and nominal) data.

An interesting aspect of measurement scales is to what extent meaningful
statistics can be derived from such scales (see Table 1 in [ 4 ] ). A
prototypical example is using a mean of a sample of nominal-scaled
variables which is generally regarded as being meaningless. This has
also provoked a lot of discussion [ 11 , 10 ] and there are nice
counter-examples of apparently meaningless statistics that still convey
information about the empirical system [ 12 ] . As always, the world is
not black or white.

In practice, we also use other taxonomies to classify the types of
measurements [ 13 ] . A commonly used one is the Discrete-Continuous
variable distinction according to whether or not the possible number of
values is countable. Therefore, binary, nominal and ordinal scaled
measurements are discrete while ratio and interval scaled measurements
are continuous. Another commonly used taxonomy is the
Quantitative-Qualitative variable distinction, which depends on whether
two different measurements differ in quality or in quantity. Thus
nominal scaled measurements are qualitative while ratio and interval
scaled measurements are quantitative. And the ordinal scaled
measurements have the characteristics of both quantitative and
quantitative variables.

#### 1.2.2 Heterogeneous information in multiple data sets

Multiple sets of measurements on the same objects obtained from
different platforms may reflect partially complementary information of
the studied system. Therefore, these multiple data sets may contain
heterogeneous information, the information that is common across all or
some of the data sets, and the information which is specific to each
data set (often called distinct). The challenge for the data fusion of
such data sets is how to separate the common and distinct information
existed in multiple data sets. These different sources of information
have to be disentangled from every data set to have a holistic
understanding of the studied system. Here we focus on using common and
distinct components to approximate the common and distinct variation
(information) existing in multiple data sets measured on the same
objects [ 14 ] . We will use a simultaneous component analysis (SCA)
model with structural sparsity patterns on the loading matrix [ 15 ] as
an example to show the idea.

A classical SCA model tries to discover the common subspace between
multiple data sets to represent the common information between these
data sets. Suppose the quantitative measurements from @xmath different
platforms on the same @xmath objects result into @xmath quantitative
data sets, @xmath , and the @xmath data set @xmath ( @xmath ) has @xmath
variables. After these data sets are column centered, we can decompose
them in the SCA model framework as @xmath , in which @xmath ( @xmath )
is the common score matrix; @xmath ( @xmath ) and @xmath ( @xmath ) are
the loading matrix and residual term respectively for @xmath and @xmath
is the number of components. The common column subspace, which is
spanned by the columns of the score matrix @xmath , represents the
common information between these @xmath data sets.

The drawback of the SCA model is that only the global common components,
which account for the common variation across all the data sets, is
modeled. However, the real situation in multiple data sets integration
can be far more complex as local common variation across some of the
data sets and distinct variation in each data set are expected as well.
With the help of the concept of structural sparsity on the loading
matrices of a SCA model, we can interpret the common and distinct
variation framework as follows. Suppose we construct a SCA model on
three column centered quantitative data sets @xmath , the common score
matrix is @xmath , the corresponding loading matrices are @xmath , and
@xmath , in which @xmath is the residual term for @xmath data set. If
the structured sparsity pattern in @xmath is expressed as follows,

  -- -------- --
     @xmath   
  -- -------- --

in which @xmath indicates the @xmath column of the @xmath loading matrix
@xmath , then we have the following relationships,

  -- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- --
     @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   
  -- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- --

Here @xmath indicates the @xmath column of the common score matrix
@xmath . The first component represents the global common variation
across three data sets; the @xmath , @xmath and @xmath components
represent the local common variation across two data sets and the @xmath
, @xmath and @xmath components represent the distinct variation specific
to each single data set. Therefore, the heterogeneity of data (common
and distinct information existed in multiple data sets) is disentangled
by the common and distinct components. In the above example, we only
show a single score and loading vector for any specific variation, but
that there can be multiple score and loading vcectors for each (global
common, local common or distinct variations).

Except for the above approach, there are also many other methods for
distinguishing common and distinct components. However, the above
approach has the advantage of estimating the model complexity directly
which is problematic in most other methods. Details will be shown in
Chapter 5 .

### 1.3 Using concave penalties to induce sparsity

Although concave penalties are not directly related to the fusion of
heterogeneous data sets, they are the basis of all the developed methods
in this thesis. Therefore, it is better to have a general introduction
to them at the beginning.

The comprehensive measurements in the current biological research always
result in high dimensional data sets, of which the number of variables
is much larger than the number of samples. For the analysis of such high
dimensional data sets, sparse parameter estimation (many estimated
parameters are exactly 0) is always desired since it makes both the data
analysis problem feasible and the results easier to be interpreted. Some
typical examples of sparse parameter estimation include the sparse
regression models [ 16 ] , the low rank matrix approximation problems [
17 , 14 , 18 ] , the structure learning problems in graphical models [
19 ] , and many others [ 20 , 21 , 22 ] .

We can use a linear regression model as an example to illustrate how to
achieve sparse parameter estimation through various penalties. Suppose
we have a univariate response variable @xmath and a multivariate
explanatory variable @xmath . A standard linear regression model can be
expressed as @xmath , in which @xmath is the coefficient vector and
@xmath is the error term following a normal distribution with mean
@xmath and variance @xmath , @xmath . After obtaining @xmath samples of
@xmath , we have the data sets @xmath , which can be further expressed
in their vector and matrix form as @xmath and @xmath . The optimization
problem associated with the standard linear model is @xmath , and the
analytical form solution is @xmath . Unfortunately, this model is
unidentifiable when @xmath and ill-conditioned when the explanatory
variables are correlated. Also the estimated coefficient vector @xmath
is always dense, which makes the interpretation difficult.

Cardinality constraint can be imposed on the linear regression model as
a hard constraint to induce a sparse parameter estimation of @xmath . If
we require only @xmath elements of @xmath to be nonzero, the associated
optimization problem can be expressed as @xmath , in which @xmath is the
cardinality constraint, @xmath indicates the pseudo @xmath norm and
counts the number of nonzero elements. Since the above optimization
problem is non-convex and difficult to solve, the cardinality constraint
is always replaced by its convex relaxation @xmath norm to induce the
sparsity, which results in the lasso regression model [ 23 ] . The
optimization problem associated with the lasso regression model is
@xmath in which @xmath is the tuning parameter and @xmath indicates the
@xmath norm. Efficient algorithms exist to solve this convex
optimization problem [ 24 , 25 ] . However, the @xmath norm penalty
shrinks all the elements of the coefficient vector @xmath to the same
degree. This leads to a biased estimation of the coefficients with large
absolute values. This behavior will further make the prediction error or
CV error based model selection procedure inconsistent [ 26 ] . Many
non-convex penalties, most of them are concave functions with respect to
the absolute value of @xmath , have been proposed to tackle the drawback
of the @xmath norm penalty [ 16 , 27 ] . They can shrink the parameters
in a nonlinear manner to achieve both nearly unbiased and sparse
parameter estimation. A frequentist version of the generalized double
Pareto (GDP) [ 27 ] shrinkage can serve as an example. The optimization
problem of a linear regression model with the GDP penalty can be
expressed as @xmath , in which @xmath is the @xmath element of @xmath ,
@xmath is a hyper-parameter, and @xmath is the concave GDP penalty on
@xmath . The thresholding properties of the cardinality constraint, the
@xmath norm penalty and the GDP penalty with different values of @xmath
are shown in Fig. 1.1 . The cardinality constraint shrinks all the
coefficients, whose absolute values are less than a selected threshold,
to 0 while keeping other coefficients unchanged. This thresholding
behavior is referred to as hard thresholding [ 23 ] . The @xmath norm
penalty shrinks all the coefficients to the same degree until some
coefficients are 0. And its thresholding behavior is referred to as soft
thresholding [ 23 ] . On the other hand, GDP penalty shrinks the
coefficients in a nonlinear manner according to the absolute values of
the corresponding coefficients. In this way, coefficients with small
absolute values are more likely to be shrunk to 0, while coefficients
with large absolute values tend to be shrunk less.

### 1.4 Scope and outline of the thesis

Principal component analysis (PCA) model is the basis of many commonly
used data fusion methods [ 28 ] . And both PCA and these data fusion
methods assume the used data sets are quantitative. Thus, before talking
about the data fusion of data sets with heterogeneous measurement
scales, we should first introduce the generalizations of PCA for
qualitative data sets. We review the extensions of PCA methods for the
analysis of multivariate binary data sets in Chapter 2 and develop a
robust logistic PCA method in Chapter 3 . After that, we are ready for
the data fusion of data sets with heterogeneous measurement scales. We
generalize the commonly used data fusion method, simultaneous component
analysis (SCA), in a probabilistic framework for the data fusion of the
multivariate quantitative and binary measurements data sets in Chapter 4
. Finally, it comes to the data fusion of data sets of the two types of
heterogeneity. We develop an exponential family SCA model for the data
fusion of multiple data sets of mixed data types, such as quantitative,
binary or count, and introduce the nearly unbiased group concave penalty
to induce structured sparsity on the loading matrix to separate common
(global and local) and distinct variation in such mixed types data sets
in Chapter 5 . Finally, the thesis closes in Chapter 6 with an outlook
into the future of fusing heterogeneous data sets.

## Chapter 2 PCA of binary genomics data

Genome wide measurements of genetic and epigenetic alterations are
generating more and more high dimensional binary data. The special
mathematical characteristics of binary data make the direct use of the
classical PCA model to explore low dimensional structures less obvious.
Although there are several PCA alternatives for binary data in the
psychometric, data analysis and machine learning literature, they are
not well known to the bioinformatics community. In this chapter we
introduce the motivation and rationale of some parametric and
nonparametric versions of PCA specifically geared for binary data. Using
both realistic simulations of binary data as well as mutation, CNA and
methylation data of the Genomic Determinants of Sensitivity in Cancer
1000 (GDSC1000) the methods are explored for their performance with
respect to finding the correct number of components, overfit, finding
back the correct low dimensional structure, variable importance etc. The
results show that if a low dimensional structure exists in the data that
most of the methods can find it. When assuming a probabilistic
generating process is underlying the data, we recommend to use the
parametric logistic PCA model (using the projection based approach),
while when such an assumption is not valid and the data is considered as
given, the nonparametric Gifi model is recommended. ¹ ¹ 1 This chapter
is based on Song, Y., Westerhuis, J.A., Aben, N., Michaut, M., Wessels,
L.F. and Smilde, A.K., 2017. Principal component analysis of binary
genomics data. Briefings in bioinformatics, 20(1), pp.317-329.

### 2.1 Background

Binary measurements only have two possible outcomes, such as presence
and absence, or true and false, which are usually labeled as “1” and
“0”. In many research problems, objects are characterized by multiple
binary features, each depicting a different aspect of the object. In
biological research, several examples of binary data sets can be found.
Genome wide measurements of genetic and epigenetic alterations are
generating more and more high dimensional binary data [ 29 , 30 ] . One
example is the high throughput measurements of point mutation. Here, a
feature is labeled as “1” when it is classified as mutated in a sample,
“0” when it is not. Another often observed binary data set is the
binarized version of copy number aberrations (CNA), which are gains and
losses of segments in chromosomal regions. Segments are labeled as “1”
when aberration is presents in a sample, otherwise “0” [ 31 ] . DNA
methylation data can also be discretized as binary features, where “1”
indicates a high level of methylation and “0” means a low level [ 30 ] .

Compared to commonly used quantitative data, binary data has some
special mathematical characteristics, which should be taken into account
during the data analysis. In binary measurements, “0” and “1” are
abstract representations of two exclusive categories rather than
quantitative values 0 and 1. These two categories can also be encoded to
any other two different labels, like “-1” and “1” or “-” and “+”,
without changing the meaning. Because “1” and “0” are only an abstract
representation of two categories, they cannot be taken interpreted as
quantitative data. Furthermore, the measurement error of binary data is
discrete in nature. Binary measurement error occurs when the wrong label
is assigned to an object, such as when a mutated gene is mis-classified
as wild type. Therefore, the by default used Gaussian error assumption
for continuous data in many statistical models is inappropriate for
binary data analysis. Another aspect of binary data is that there can be
an order in the two categories. For example, presence is often
considered more important than absence. Finally, binary data can be
generated from a discrete measurement process, but also a continuous
measurement process [ 32 ] .

PCA is one of the most popular methods in dimension reduction with
numerous applications in biology, chemistry and many other disciplines [
17 ] . PCA can map data points, which are in a high dimensional space,
to a low dimensional space with minimum loss of variation. The derived
low dimensional features, which provide a parsimonious representation of
the original high dimensional data, can be used in data visualization or
for further statistical analysis.

Classical linear PCA methods are appropriate for quantitative data. The
direct use of linear PCA on binary data does not take into account the
distinct mathematical characteristics of binary data. In this chapter,
we are going to introduce, compare and evaluate some of the PCA
alternatives for binary data. First, the theory of the different
approaches is introduced together with their model properties and how
the different models are assessed. Then we will introduce three binary
genomics data sets on which the models will be applied. Besides the real
data, realistic simulations of binary data are used to uncover some of
the properties of the different models.

### 2.2 Theory

There exist two separate directions in extending PCA for binary data;
parametric and nonparametric. Parametric approaches are represented by
logistic PCA methods, originating from the machine learning literature.
In these methods, PCA is extended to binary data from a probabilistic
perspective in a similar way as linear regression is extended to
logistic linear regression [ 33 , 34 , 35 ] . Nonparametric methods,
originating from the psychometric and data analysis communities, include
optimal scaling [ 36 ] , multiple correspondence analysis [ 37 ] and
many others [ 38 ] . In this direction, PCA is extended to binary data
from a geometric perspective without probabilistic assumptions. The
details for the motivation and rationale of above approaches for binary
data will be explained later in this section. We will start by
introducing classical PCA.

#### 2.2.1 Classical PCA

Classical PCA can be expressed as a projection based approach (finding
the low dimensional space that best represents a cloud of high
dimensional points) following Pearson [ 39 ] . The measurements of
@xmath quantitative variables on @xmath objects result into a matrix
@xmath ( @xmath ) with @xmath rows and @xmath columns. The column vector
form of the @xmath row of @xmath is @xmath , which is taken as a point
in @xmath dimensional space. Suppose that a low dimensional space is
spanned by the columns of an orthogonal loading matrix @xmath ( @xmath ,
@xmath . The orthogonal projection of @xmath on this low dimensional
space is @xmath . We find @xmath by minimizing the Euclidean distance
between the centered high dimensional points @xmath , and their low
dimensional projections:

  -- -- -------- -- -------- -------- -------
        @xmath      @xmath            (2.1)
                             @xmath   
  -- -- -------- -- -------- -------- -------

in which the column offset term @xmath ( @xmath ) is included to center
the samples, and @xmath is the identity matrix. The exact position of
the centered @xmath data point @xmath in this low dimensional space is
represented by its @xmath dimensional score vector @xmath , @xmath ,
where @xmath and @xmath are the estimated values of equation 2.1 . In
matrix form, we have @xmath , @xmath is the @xmath row of @xmath ;
@xmath is an @xmath dimensional vector of ones; the estimated offset
@xmath contains the column means of @xmath and @xmath is the column
centered @xmath .

Another approach to explain PCA is the reconstruction based approach [
40 ] . A high dimensional data point @xmath is approximated by a linear
function of the latent low dimensional score @xmath with orthogonal
coefficients @xmath , @xmath , @xmath , @xmath is the offset term. Now,
@xmath , @xmath and @xmath can be found simultaneously by minimizing the
Euclidean distance between centered @xmath , and their low dimensional
linear approximations @xmath :

  -- -- -------- -- -------- -------- -------
        @xmath      @xmath            (2.2)
                             @xmath   
  -- -- -------- -- -------- -------- -------

It is well known that the above two approaches for classical PCA are
equivalent and the global optimal solution can be obtained from the
@xmath truncated singular value decomposition (SVD) of the column
centered @xmath [ 41 ] . The solution @xmath contains the column means
of @xmath ; @xmath is the product of the first @xmath left singular
vectors and the diagonal matrix of first @xmath singular values; @xmath
contains the first @xmath right singular vectors.

Above, the classical PCA was derived from a geometrical perspective.
Bishop et al. [ 42 ] have derived another explanation for PCA from a
probabilistic perspective, called probabilistic PCA. A high dimensional
point @xmath can be regarded as a noisy observation of the true data
point @xmath , which lies in a low dimensional space. The model can be
expressed as @xmath and @xmath , @xmath is the offset term as before;
@xmath contains the coefficients; @xmath represents the low dimensional
score vector. The noise term @xmath is assumed to follow a multivariate
normal distribution with mean @xmath and constant variance @xmath ,
@xmath . Thus the conditional distribution of @xmath is a normal
distribution with mean @xmath and constant variance, @xmath . @xmath ,
@xmath and @xmath can be obtained by maximum likelihood estimation.

  -- -- -------- -------- -------- -------- -------
        @xmath            @xmath            (2.3)
                 @xmath                     
                                   @xmath   
  -- -- -------- -------- -------- -------- -------

The above maximum likelihood estimation is equivalent to the least
squares minimization in classical PCA from the perspective of
frequentist statistics [ 33 ] . One important implication is that all
the elements in the observed matrix @xmath are conditionally independent
of each other given the offset @xmath , the score matrix @xmath and the
loading matrix @xmath , which is the key point for the further extension
to binary data.

#### 2.2.2 Logistic PCA

The probabilistic interpretation of PCA under multivariate normal
distribution for the observed data provides a framework for the further
generalization to other data types [ 42 ] . As the Gaussian assumption
is only appropriate for continuous quantitative data, it is necessary to
replace the Gaussian assumption by the Bernoulli distribution for binary
observations in a similar way as from linear regression to logistic
linear regression [ 33 , 35 , 43 ] . The @xmath element in observed
matrix @xmath , @xmath , is a realization of the Bernoulli distribution
with parameter @xmath , which is the @xmath element in the probability
matrix @xmath . Specifically, the probability that @xmath equals “1” is
@xmath . Similar to probabilistic PCA, all the elements in the observed
matrix @xmath are conditionally independent of each other given the
parameter matrix @xmath ( @xmath ). The log likelihood for observation
@xmath given the probability matrix @xmath is as follows:

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

The log-odds of @xmath is @xmath , where @xmath , which is the natural
parameter of the Bernoulli distribution expressed in the exponential
family form. Thus @xmath and @xmath is called the logistic function. The
log likelihood for observation @xmath given log-odds @xmath is
represented as:

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

A low dimensional structure can be assumed to exist in the log-odds
@xmath ( @xmath ) as @xmath . Here @xmath is the object score matrix for
the log-odds @xmath ; @xmath is the loading matrix; @xmath is the
offset.

There are mainly two approaches to fit the model (equation 2.5 ),
logistic PCA [ 43 ] and projection based logistic PCA (logistic PPCA) [
35 ] . The main difference between these two approaches is whether
@xmath and @xmath are estimated simultaneously or sequentially. In the
logistic PCA model, the score matrix @xmath and loading matrix @xmath
are estimated simultaneously by alternating minimization [ 33 , 44 ] or
by a majorization-minimization (MM) algorithm [ 43 ] .

On the other hand, logistic PPCA only estimates @xmath directly. After
@xmath is estimated, @xmath is obtained by a projection based approach
in the same manner as classical PCA in equation 2.1 [ 35 ] . Score
matrix @xmath is the low dimensional representation of the log-odds
@xmath of the saturated model in the subspace spanned by @xmath .
Details of the log-odds @xmath from the saturated model will be shown
latter. In matrix form, @xmath , @xmath is the offset term. Then the
log-odds @xmath in equation 2.5 can be represented as @xmath . The
estimation of parameters @xmath and @xmath , can be obtained by
maximizing the conditional log likelihood @xmath in equation 2.5 . Then,
the solution for the score matrix is @xmath .

Compared to logistic PCA, logistic PPCA has fewer parameters to
estimate, and thus is less prone to overfitting. In addition, the
estimation of the scores of new samples in logistic PCA involves an
optimization problem while for logistic PPCA, it is a simple projection
of the new data on @xmath .

In a saturated model, there is a separate parameter for every individual
observation. The model is over-parameterized and has perfect fit to the
observed data. For quantitative data, the parameters of the saturated
model are simple the observed data. For example, the parameters of the
saturated PCA model on observed data matrix @xmath are the matrix @xmath
itself. For binary data, the parameter (probability of success) of a
saturated model for the observation “1” is 1; for the observation “0” is
0. Thus, the @xmath element in @xmath from the saturated logistic PCA
model is @xmath . It is negative infinity when @xmath ; positive
infinity when @xmath . In order to project @xmath onto the low
dimensional space spanned by @xmath , one needs a finite @xmath . In
logistic PPCA, positive and negative infinities in @xmath are
approximated by large numbers @xmath and @xmath . When @xmath is too
large, the elements in the estimated probability matrix @xmath for
generating the binary observation @xmath are close to 1 or 0; when
@xmath is close to 0, the elements in @xmath are close to 0.5. In the
original paper of logistic PPCA, CV is used to select @xmath [ 35 ] . In
this paper we select @xmath which corresponds to a probability of
success 0.95. This can be interpreted as using probabilities 0.95 and
0.05 to approximate the probabilities 1 and 0 in the saturated model.

#### 2.2.3 Theory of nonlinear PCA with optimal scaling

Another generalization of PCA to binary data is nonlinear PCA with
optimal scaling (the Gifi method). This method was primarily developed
for categorical data, of which binary data is a special case [ 45 , 36 ]
. The basic idea is to quantify the binary variables to quantitative
values by minimizing some loss functions. The quantified variables are
then used in a linear PCA model. The @xmath column of @xmath , @xmath ,
is encoded into an @xmath indicator matrix @xmath . @xmath has two
columns, “1” and “0”. If the @xmath object belongs to column “1”, the
corresponding element of @xmath is 1, otherwise it is 0. @xmath is the
@xmath object score matrix, which is the representation of @xmath in a
low dimensional Euclidean space; @xmath is a @xmath quantification
matrix, which quantifies this @xmath binary variable to a quantitative
value. For binary data, the rank of the quantification matrix @xmath is
constrained to 1. This is the PCA solution in the Gifi method. @xmath
can be expressed as @xmath , where @xmath is a two dimensional column
vector with binary quantifications and @xmath is the vector of weights
for @xmath principal components. The loss function is expressed as:

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

in which the score matrix @xmath is forced to be centered and
orthogonal, @xmath , @xmath , to avoid trivial solutions. The loss
function is optimized by alternating least squares algorithms. For
binary data, nonlinear PCA with optimal scaling is equivalent to
multiple correspondence analysis and to PCA on standardized variables [
38 ] .

### 2.3 Model properties

#### 2.3.1 Offset

Including the column offset term @xmath in component models also implies
that the column mean of score matrix is @xmath , i.e. @xmath .
Otherwise, the model is unidentifiable. In PCA and the Gifi method, the
estimated @xmath equals the column mean of @xmath . Therefore, including
@xmath in the model has the same effect as column centering of @xmath .
In logistic PPCA and logistic PCA, the @xmath element of @xmath , @xmath
, can be interpreted as the log-odds of the marginal probability of the
@xmath variable. When only the offset @xmath is included in the model,
@xmath , the @xmath element of the solution @xmath , @xmath , is the
log-odds of the empirical marginal probability of the @xmath variable
(the proportion of “1” in the @xmath column). When more components are
included, @xmath , the solution @xmath is not unique. If an identical
offset is required for comparing component models with a different
number of components, one can fix the offset term to the log-odds of the
empirical marginal probability during the maximum likelihood estimation.

#### 2.3.2 Orthogonality

Similar to PCA, the orthogonality constraint @xmath in logistic PPCA and
logistic PCA actually is inactive. If @xmath is not orthogonal, it can
be made orthogonal by subjecting @xmath to an SVD algorithm. @xmath
equals the right hand singular vectors and @xmath equals the product of
the left hand singular vectors and the diagonal matrix of singular
values. This extra step will not change the objective value. Table 2.1
gives the orthogonality properties of the scores and loadings of the
four methods discussed above.

#### 2.3.3 Nestedness

Linear PCA models are nested in the number of components, which means
the first @xmath principal components in the @xmath components model are
exactly the same as the @xmath components model. For the Gifi method,
this property only holds for the binary data case but not in general.
For logistic PPCA and logistic PCA, this property does not hold.

### 2.4 Model assessment

#### 2.4.1 Error metric

To make a fair comparison between linear PCA, the Gifi method, logistic
PPCA and logistic PCA, the training error is defined as the average
misclassification rate in using the derived low dimensional structure to
fit the training set @xmath . Each of the four methods provides an
estimation of the offset term, score matrix and loading matrix, @xmath ,
@xmath and @xmath . For linear PCA and the Gifi method, we take @xmath
as an approximation of the binary matrix @xmath ; for logistic PCA and
logistic PPCA, @xmath is used as an approximation for the probability
matrix @xmath , of which the observed matrix @xmath was generated. Since
both approximations are continuous, we need to select a threshold to
discretize them to binary fitting.

In the discretization process, two misclassification errors exist. “0”
can be misclassified as “1”, which we call @xmath and “1” can be
misclassified as “0”, which we call @xmath . @xmath is the number of
@xmath in this process, and @xmath is the number of @xmath ; @xmath is
the number of “0” in the observed binary matrix @xmath , and @xmath is
the number of “1” in @xmath . A commonly used total error rate is given
by @xmath , which gives equal weights to these two errors. However, this
can lead to undesirable results for imbalanced binary data, i.e. when
the proportions of “1” and “0” are extreme. Usually, imbalanced binary
data sets are common in real applications, where sometimes the
proportion of “1” in the observed matrix @xmath can be less than 5%. In
such a case, @xmath is more likely to occur than @xmath , and hence it
seems inappropriate to give them equal weights. In imbalanced cases, a
balanced error rate @xmath is more appropriate [ 46 ] . To decide
whether the predicted quantitative value represents a “0” or a “1”, a
threshold value has to be selected. This threshold value can be selected
by minimizing the balanced error rate in a training set after which it
can be applied to a test set in order to prevent biased (too optimistic)
results.

#### 2.4.2 Cross validation

The training error is an overly optimistic estimator of the
generalization error, which can be intuitively understood as the average
misclassification rate in predicting an independent test set. Thus, we
use cross validation (CV) to approximate the generalization error. In
this chapter, we use the CV algorithm named EM-Wold [ 47 , 48 ] . In
this approach, validation sets of elements of the matrix @xmath are
selected in a diagonal style rather than a row wise style. The left out
part is considered as missing. In this way the prediction of the left
out part is independent of the left out part itself. It is possible to
use this approach as all the component models in this paper can handle
missing data. A 7-fold CV procedure was used for all calculations in
this paper. In each of these folds, a component model is developed
taking the missing data into account. The model is then used to make a
prediction of the missing elements. This is repeated until all elements
of @xmath have been predicted in this way. The threshold of converting
the continuous predictions to binary predictions in CV was the same as
the one used in computing the training error.

### 2.5 Data Sets

#### 2.5.1 Real data sets

The data we used is from the Genomic Determinants of Sensitivity in
Cancer 1000 (GDSC1000) [ 30 ] . To facilitate the interpretability of
the results, only three cancer types are included in the data analysis:
BRCA (breast invasive carcinoma, 48 human cell lines), LUAD (lung
adenocarcinoma, 62 human cell lines) and SKCM (skin cutaneous melanoma,
50 human cell lines). Each cell line is a sample in the data analysis.
For these samples, three different binary data sets are available:
mutation, copy number aberration (CNA) and methylation data. For the
mutation data, there are 198 mutation variables. Each variable is a
likely cancer driver or suppressor gene. A gene is labeled as “1” when
it is classified as mutated in a sample and as “0” when classified as
wild type. The mutation data is very imbalanced (supplemental Fig. S2.1
a): roughly @xmath of the data matrix is labeled as “1”. The CNA data
has 410 observed CNA variables. Each variable is a copy number region in
a chromosome. It is labeled as “1” for a specific sample when it is
identified as aberrated and it is labeled as “0” otherwise. The CNA data
set is also imbalanced (supplemental Fig. S2.1 b): roughly @xmath of the
data matrix is labeled as “1”. For the methylation data, there are 38
methylation variables. Each variable is a CpG island located in gene
promotor region. In each variable, “1” indicates a high level of
methylation and “0” indicates a low level. The methylation data set is
relatively balanced compared to other data sets (supplemental Fig. S2.1
c): roughly @xmath of the data matrix is labeled as “1”.

#### 2.5.2 Simulated binary data sets

Binary data matrices with an underlying low dimensional structure can be
simulated either from a latent variable model or as the noise corrupted
version of a structured binary data set. In the first case the data
generating process is considered to provide a quantitative data set
while there is a binary read out. In the second case the data generating
process is considered to provide a binary data set. We use both of these
two approaches to study the properties of different binary PCA methods.

##### Simulated binary data based on the logistic PCA model

Data sets with different degrees of imbalance and with low dimensional
structures were simulated according to the logistic PCA model. The
offset term @xmath is used to control the degree of imbalance and the
log-odds @xmath is defined to have a low dimensional structure. The
observed binary matrix @xmath is generated from the corresponding
Bernoulli distributions.

Each element in the @xmath loading matrix @xmath is sampled from the
standard normal distribution. The Gram-Schmidt algorithm is used to
force @xmath . @xmath is set to 3. The simulated @xmath score matrix
@xmath has three group structures in the samples. @xmath samples are
divided into three groups of equal size. The three group means are set
manually to force sufficient difference between the groups. The first
two group means are set to @xmath and @xmath . The third group mean is
@xmath . The scores in first group are sampled from the multivariate
normal distribution @xmath , the scores in second group from @xmath and
the scores in the third group from @xmath . In this way, scores between
groups are sufficiently different and scores within the same group are
similar.

When the elements in @xmath are close to @xmath , the corresponding
probabilities are close to 0.5. In this case, the binary observations
are almost a random guess. When their absolute values are large, the
corresponding probabilities are close to 1 or 0, the binary observations
are almost deterministic. The scale of @xmath should be in a reasonable
interval, not too large and not too small. A constant @xmath is
multiplied to @xmath to control the scale for generating proper
probabilities. In addition, the offset term @xmath is included to
control the degree of imbalance in the simulated binary data set. After
@xmath is simulated as above, it is transformed to the probability
matrix @xmath by the logistic function @xmath and @xmath in @xmath is a
realization of Bernoulli distribution with parameter @xmath , which is
the @xmath element of probability matrix @xmath .

##### Simulated binary data based on noise corruption of pre-structured
binary data

Another approach of simulating binary data is by the noise corruption of
a pre-structured data set. Compared to the latent variable model, this
approach provides an intuitive understanding of the low dimensional
structure in the observed binary data. Pre-structured binary data set
@xmath has structured and unstructured parts. The goal of the current
simulation is to find the variables that belong to the structured part,
while the goal of the previous simulation is to see how well the whole
data set can be approximated. The structured part is simulated as
follows. @xmath different @xmath dimensional binary vectors are
simulated first, each element is sampled from the Bernoulli distribution
with probability @xmath , which is the degree of imbalance in the binary
data simulation. Each of these @xmath binary vectors is replicated 10
times to form the structured part. In the unstructured part of @xmath ,
all the elements are randomly sampled from Bernoulli distribution with
probability @xmath . The observed binary data @xmath is a noise
corrupted version of the pre-structured binary data set @xmath . If the
noise level is set to 0.1, all the elements in the binary data @xmath
have a probability of 0.1 to be bit-flipped. The observed binary matrix
@xmath , has @xmath groups of 10 highly correlated variables and the
other variables are not correlated. The @xmath groups are taken as the
low dimensional structure. The above simulation process is illustrated
in the supplemental Fig. S2.4.

### 2.6 Results

All the computations are done in R [ 49 ] . The linear PCA model is
fitted using the SVD method after centering the data [ 50 ] . The Gifi
method is fitted using the alternating least squares approach by Homals
package [ 36 ] . The logistic PCA and logistic PPCA models are fitted
using an MM algorithm with offset term [ 43 , 35 ] . The default
stopping criterion is used for all the approaches.

#### 2.6.1 Balanced simulation

The goal of this simulation is to evaluate the abilities of the four
approaches in finding back the embedded low dimensional structures in
the sample space and variable space. The simulation process is based on
the logistic PCA model. The offset term @xmath is set to @xmath to
simulate balanced binary data. The parameters are set to @xmath . The
simulated balanced binary data are shown in supplemental Fig. S2.2.
First a classical PCA on the simulated probability matrix @xmath and the
log-odds @xmath was performed. Fig. 2.1 shows the score plots of these
two PCA analyses. The difference between the score plots of linear PCA
on @xmath (Fig. 2.1 a) and on log-odds @xmath (Fig. 2.1 b) is obvious.
The scores of the linear PCA model on @xmath lie in the margin of the
figure, while for @xmath , they lie more in the center of the figure.
This difference is related to the nonlinear logistic function @xmath ,
which transforms @xmath to @xmath . Furthermore, PCA on the log-odds
matrix describes more variation in the first two PCs.

Logistic PCA, logistic PCA, Gifi and linear PCA are used to model the
binary matrix @xmath . Two principal components are used. Offset terms
are included in the model. The score plots produced by these different
approaches are shown in Fig. 2.2 . The similarity between Fig. 2.1 and
Fig. 2.2 indicates that the logistic PCA model approximates the
underlying log-odds @xmath from the binary observation @xmath , while
the other approaches approximate the probability matrix @xmath .

Another observation is that the score plots derived from logistic PPCA
(Fig. 2.2 a), Gifi (Fig. 2.2 c) and linear PCA (Fig. 2.2 d) are very
similar except for some scale differences. The similarity between the
Gifi and linear PCA for balanced binary data set is understandable. For
binary data, the Gifi method is equivalent to PCA on standardized binary
variables. Since the proportion of “1” and “0” of each binary variable
are similar in a balanced simulated data set, the column mean and
standard deviation of each binary variable are close to 0.5. Thus the
standardization of each binary variable will change 0 and 1 binary data
to -1 and 1 data. Therefore, except for the difference in scale, Gifi
and linear PCA are almost the same for balanced binary data. For
logistic PPCA, the score matrix @xmath is a low dimensional
representation of the log-odds @xmath from the saturated model, @xmath ,
and the @xmath is estimated by @xmath . This is equivalent to changing 0
and 1 to @xmath and @xmath . Thus, the true difference between linear
PCA and logistic PPCA is how to find the low dimension spanned by
loading matrix @xmath . Logistic PPCA finds it by minimizing the
logistic loss and linear PCA finds it by minimizing the least squares
loss.

The training error and CV error for different models are shown in Fig.
2.3 . We add the zero component model in which only the offset term
@xmath is included, as the baseline for evaluating the different methods
with different numbers of components. The estimated offset @xmath in the
zero component model is the column mean of @xmath for PCA and the Gifi
method, while it is the logit transform of the column mean of @xmath for
logistic PPCA and logistic PCA. All approaches successfully find the
three components truely underlying the data. It can also be observed
that logistic PCA is more eager to overfit the data. It shows a lower
balanced error rate, but a higher CV error rate for more than three
components compared to the other methods.

#### 2.6.2 Imbalanced simulation

The goal of the imbalanced simulation is to evaluate the effect of
imbalanced simulated data on the ability of the four approaches in
finding back the underlying low dimensional structures in variable
space. Since the offset @xmath in logistic PCA model can be interpreted
as the log-odds of marginal probabilities, we can use the log-odds of
the empirical marginal probabilities from the real data sets with
different degrees of imbalance as the offset in the simulation. The
simulation process is based on the logistic PCA model. The offset term
@xmath is set to log-odds of column means of real data to simulate
imbalanced binary data. The parameters @xmath are set to the size of
corresponding real data. The constant @xmath is selected as 20, @xmath
is set to 3. The simulated data is shown in supplemental Fig. S2.3. We
evaluate the effect of imbalanced binary data set on the different
models’ abilities of finding back the simulated low dimensional
structure. The CV error plots of different models are shown in Fig. 2.4
. All the approaches are successful in finding back three significant
PCs.

#### 2.6.3 Feature selection

For the assessment of feature importance, the binary data is simulated
by noise corruption of a pre-structured binary data set. @xmath is 198;
@xmath is 100; @xmath is set to 3. The degree of imbalance is set to
0.2, and the noise level is 0.1. The simulated data is shown in
supplemental Fig. S2.4. There are noisy corrupted structures in the
first 30 variables. For feature selection purposes we estimate the
importance of each feature in the model. This is performed as follows,
@xmath , where @xmath is the loading in the @xmath PCs for @xmath
variable, is taken as the importance measure. The process is repeated 15
times, the mean and standard deviation of the average squared loading
for the 100 variables are shown in Fig. 2.5 . It can be observed that
highly correlated binary variables have large loadings. The variance of
the loadings derived from logistic PCA is much higher than other
approaches. This indicates that the logistic PCA model cannot make
stable estimation of loadings.

#### 2.6.4 Real data

The binary mutation, CNA and methylation data sets are analysed using
the four different approaches. The score plots and error plots from
different approaches on the real mutation data set are shown in Fig. 2.6
. The CV results of PCA, Gifi and logistic PCA in Fig. 2.6 f do not
support the assumption that a low dimensional structure exists in the
mutation data. For the CV result of logistic PPCA (Fig. 2.6 f), the
minimum CV error was achieved using three components. However, this
minimum was only slightly lower than the zero component model. Although
the CV result of logistic PPCA is ambiguous, we can observe four
clusters in the score plot.

To explore the clusters in more detail, Fig. 2.7 shows the loading plot
and score plots with different mutation status of the logistic PPCA
model. With the corresponding loading values (Fig. 2.7 a) we determined
that these clusters were largely defined by TP53, BRAF and KRAS mutation
status. Interestingly, these genes also have the highest mutational
load, suggesting that variables with a higher aberration frequency
contain more information. Cluster 1 (c1 in Fig. 2.7 b) is BRAF-mutant
and TP53-mutant type; while cluster 2 (c2 in Fig. 2.7 b) is BRAF-mutant
and TP53-wild type. Cluster 3 (c3 in Fig. 2.7 c) mostly consists of
BRAF-wild and TP53-mutant cell lines, a configuration that often occurs
in all three analyzed cancer types. Cluster 4 (c4 in Fig. 2.7 c)
contains BRAF-wild and TP53-wild type cell lines, which again is a
configuration that occurs across cancer types. Finally, we observed
sub-clusters of LUAD cell lines towards the bottom of cluster 3 and 4,
which consist of KRAS-mutant cell lines (Fig. 2.7 d). As BRAF and KRAS
mutations both activate the MAPK pathway in a similar fashion, double
mutants are redundant and hence rarely observed. Our results are in line
with this mutual exclusivity pattern: with the exception of a single
BRAF/KRAS double mutant, we find BRAF mutations only in cluster 1 and 2
and KRAS mutations only in cluster 3 and 4. One notable exception of the
above characterization of the clusters is CAL-51 (labeled in Fig. 2.7
c). Given its TP53 wild-type status, CAL-51 would be expected in cluster
4, but it actually resides in the bottom-left of cluster 3. This shift
left is likely due to mutations in both SMARCA4 and PIK3CA, which have
the third and fourth most negative loading values on PC1.

The score plots and error plots from different approaches on CNA data
are shown in Fig. 2.8 . There is some evidence from the CV results from
all the models in Fig. 2.8 f for a five dimensional structure in the
data. However, in the score plots of Fig. 2.8 , the samples with
different cancer types are not well separated and there is no clear
evidence of natural clusters. Therefore, we do not zoom in further on
this data type.

The score plots and error plots from different approaches on methylation
data are shown in Fig. 2.9 . The three cancer types are well separated
in all the score plots. The similar and specific structure in the score
plots of logistic PPCA, the Gifi method and linear PCA may be related to
the unique structure of the methylation data (supplemental Fig. S2.1 c).
Different cancer types have very different methylation patterns,
represented by unique sets of features. In addition, there is some
evidence from the CV results from all the models in Fig. 2.9 f for a two
dimensional structure in the data.

We use the score plot derived from logistic PPCA model on methylation
data as an example to interpret the result. The first two principal
components from the logistic PPCA applied to the methylation data show
three clusters, which perfectly represent the three cancer types (Fig.
2.9 a). The corresponding loading values also roughly fall into three
cancer type specific clusters (Fig. 2.10 ), as most variables are
exclusively non-zero in a single cancer type. Notable exceptions are
GSTM1 and ARL17A, which are non-zero in two cancer types and hence each
reside between two clusters, and variables GSTT1 and DUSP22, which are
non-zero in all three cancer types and hence reside towards the center
of the plot.

### 2.7 Discussion

In this chapter, four methods were discussed that all aim to explore
binary data using low dimensional scores and loadings. It was shown that
each of the methods has different goals and therefore produces slightly
different results. Linear PCA (without the standardization processing
step) treats the binary data as quantitative data and tries to use low
rank score and loading matrices to fit the quantitative data. For binary
data, the quantification process in the Gifi method is simply a
standardization of the binary variables. After quantification, the Gifi
method tries to use low rank score and loading matrices to fit the
quantified binary variables. Both logistic PPCA and logistic PCA assume
that the binary data follows a Bernoulli distribution, and try to find
an optimal estimation of the log-odds matrix, which lies in the low
dimensional space. Logistic PCA tries to estimate the low dimensional
log-odds matrix directly; while logistic PPCA estimates this matrix by
the projection of the log-odds matrix from the saturated model on a
approximated low dimensional space.

For all the four approaches it is not the degree of imbalance which is
the problem. As shown in our simulation results, the low dimensional
structure of binary variables, which can be simulated from a latent
variable model or by noise corruption of a pre-structured binary data
set, is the key issue for the results of data analysis, rather than the
degree of imbalance of the data set. When there is a low dimensional
structure in our simulation process, all the approaches can successfully
find the correct number of components with different degrees of
imbalance.

In both the analysis of the simulated data and of the real data, the
performance of the linear PCA method, in the criteria of training error
and CV error, is similar to other specially designed algorithms for
binary data. In addition, since the global optimum of the linear PCA
model can always be achieved, the solution is very stable. However, the
linear PCA model on binary data obviously contradicts the mathematical
characteristics of binary data and the assumptions of the linear PCA
model itself. In addition, the fitted values, elements in the product of
score and loading matrix, can only be regarded as an approximation to
quantitative 0 and 1, and are thus difficult to interpret.

The results of linear PCA and the Gifi method are very similar,
especially when the degree of imbalance in each variable is
approximately equal. Furthermore, there are signs of overfitting in the
analysis of the CNA data by the Gifi model. However, compared to linear
PCA, the interpretability of the Gifi method is better. The mathematical
characteristics of binary data are taken into account from the
geometrical perspective and the solutions can be interpreted as an
approximation of the optimally quantified binary variables.

On the other hand, logistic PPCA and logistic PCA methods take into
account the mathematical characteristics of binary data from the
probabilistic perspective. Fitted values, elements in the product of the
derived score and loading matrices, can be interpreted as the log-odds
for generating the binary data, and the log-odds can again be
transformed to probability. The problem for logistic PCA is that it is
not robust with respect to the score and loading estimation, although it
is able to select the correct number of components [ 43 ] . Since both
score matrix @xmath and loading matrix @xmath are free parameters to fit
in the optimization, the estimation of @xmath will not hesitate to move
to infinity to minimize the loss function. This represents itself in
such a way that logistic PCA is prone to overfit (as can be seen from
the CV results) and the large variation in the loading estimation. The
non-robustness problem is mitigated in the logistic PPCA model. Since
only the loading matrix @xmath is freely estimated in logistic PPCA to
find the optimal model, while the score matrix @xmath is fixed given the
loadings, the logistic PPCA model is less prone to overfitting. Thus,
the estimation of the loadings of binary variables is more stable
compared to logistic PCA. Furthermore, since the fitted values are the
linear projection of the log-odds matrix of the saturated model, its
scale is constrained by the scale of the approximate log-odds matrix of
the saturated model, which can be specified in advance.

When assuming a probabilistic generating process is underlying the
binary data we recommend to use the parametric logistic PPCA model. When
such an assumption is not valid and the data is considered as given, the
nonparametric Gifi model is recommended.

### Acknowledgements

Y.S. gratefully acknowledges the financial support from China
Scholarship Council (NO.201504910809). The research leading to these
results has received funding from the European Research Council under
the European Union’s Seventh Framework Programme (FP7/2007-2013) / ERC
synergy grant agreement @xmath 319661 COMBATCANCER.

### 2.8 Supplementary information

## Chapter 3 Robust logistic PCA model

Although the projection based logistic PCA model works well in our
previous data analysis, the idea of projection in the context of
multivariate binary data is not straightforward. Furthermore, the
results of the model are related to the specification of the parameter
@xmath , which is the approximation of the infinity in the saturated
model. In addition, this approach is difficult to be generalized to the
data fusion of multiple data sets with different data types. Therefore,
in this chapter we focus on developing a robust version of logistic PCA
model without the involvement of projection.

The non-robustness of the standard logistic PCA model, manifested as
some estimated parameters diverge towards infinity, is because of the
used exact low rank constraint, which is specified as the multiplication
of low rank score and loading matrices. Therefore, we propose to fit a
logistic PCA model through non-convex singular value thresholding to
alleviate the non-robustness issue. An efficient MM algorithm is
implemented to fit the model and a missing value based CV procedure is
introduced for the model selection. In addition, we re-express the
logistic PCA model based on the latent variable interpretation of the
generalized linear model on binary data. The multivariate binary data
set is assumed to be the sign observation of an unobserved quantitative
data set, on which a low rank structure is assumed to exist. The latent
variable interpretation of the logistic PCA model not only makes the
assumption of low rank structure easier to understand, but also provides
us a way to define signal-to-noise ratio in multivariate binary data
simulation. Our experiments on realistic simulations of imbalanced
binary data and low signal-to-noise ratio show that the CV error based
model selection procedure is successful in selecting the proposed model.
And the selected model demonstrates superior performance in recovering
the underlying low rank structure compared to models with convex nuclear
norm penalty and exact low rank constraint. Finally, a binary CNA data
set is used to illustrate the proposed methodology in practice. ¹ ¹ 1
This chapter is based on Song, Y., Westerhuis, J.A. and Smilde, A.K.,
2019. Logistic principal component analysis via non-convex singular
value thresholding. arXiv preprint arXiv:1902.09486.

### 3.1 Background

Logistic PCA [ 34 , 43 ] is motivated from the probabilistic
interpretation of the classical PCA model with Gaussian distributed
error. The extension of the classical PCA model to the logistic PCA
model is similar to the extension of linear regression to logistic
linear regression. In the classical PCA model, the low rank constraint
is imposed on the conditional mean of the observed quantitative data
set, while in the logistic PCA model, the low rank constraint is imposed
on the logit transform of the conditional mean of the observed binary
data. Therefore, the logistic PCA model can also be re-expressed in a
similar way as the latent variable interpretation of the generalized
linear models (GLMs) on binary data [ 13 ] . In logistic PCA, the
observed binary data set can be assumed as the sign observation of an
unobserved quantitative data set, on which low rank structure is assumed
to exist. This intuitive latent variable interpretation not only
facilitates the understanding of the low rank structure in the logistic
PCA model, but also provides a way to define the signal-to-noise ratio
(SNR) in the simulation of multivariate binary data.

However, the standard logistic PCA model with the exact low rank
constraint, which is expressed as the multiplication of two low rank
matrices, is prone to overfitting, leading to divergence of some
estimated parameters towards infinity [ 43 , 51 ] . The same overfitting
problem also happens for the logistic linear regression model. If two
classes of the outcome are linearly separable with respect to an
explanatory variable, the corresponding coefficient of this variable
tends to go to infinity [ 13 ] . A common trick is adding a ridge
regression (quadratic) type penalty on the coefficient vector to
alleviate the overfitting issue. If we apply the same trick on the
logistic PCA model, the quadratic penalty on the loading matrix is
equivalent to a quadratic penalty on the singular values of a matrix,
which is the multiplication of the score and loading matrices. Details
will be shown later. Therefore, it is possible to derive a robust
logistic PCA model via the regularization of the singular values. [ 52 ]
proposed to use a nuclear norm penalty in the low rank matrix
approximation framework for the binary matrix completion problem. The
proposed method is similar to the logistic PCA model except that the
column offset term is not included and the exact low rank constraint is
replaced by its convex relaxation, the nuclear norm penalty. The nuclear
norm penalty, which is equivalent to applying a lasso penalty on the
singular values of a matrix, induces low rank estimation and constrains
the scale of non-zeros singular values simultaneously. However, a lasso
type penalty shrinks all parameters to the same degree, leading to
biased parameter estimation. This behavior will further make the CV
error or the prediction error based model selection procedure
inconsistent [ 26 ] . On the other hand, non-convex penalties, many of
which are concave functions, are capable to simultaneously achieve
nearly unbiased parameter estimation and sparsity [ 16 , 27 ] . Recent
research [ 53 , 54 ] has also shown the superiority of non-convex
singular value thresholding (applying non-convex penalties on the
singular values of a matrix) in recovering the true signal in a low rank
approximation framework under Gaussian noise. In this chapter, we
propose to fit the logistic PCA model via non-convex singular value
thresholding as a way to alleviate the overfitting problem and to induce
low rank estimation simultaneously. A MM algorithm is implemented to fit
the proposed model and an option for missing values is included. In the
developed algorithm, the updating of all the parameters has an
analytical form solution, and the loss function is guaranteed to
decrease in each iteration. After that, a missing value based CV
procedure is introduced for the model selection.

Based on the latent variable interpretation of the logistic PCA model,
realistic multivariate binary data sets (low SNR, imbalanced binary
data) are simulated to evaluate the performance of the proposed model
and the corresponding model selection procedure. It turns out that the
CV error based model selection procedure is successful in the selection
of the proposed model, and the selected model has superior performance
in recovering the underlying low rank structure compared to the model
with convex nuclear norm penalty and exact low rank constraint.
Furthermore, the performance of the logistic PCA model as a function of
the SNR in multivariate binary data simulation is fully characterized.
Finally, a binary CNA data set is used to illustrate the proposed
methodology in practise.

### 3.2 Latent variable interpretation of models on binary data

#### 3.2.1 Latent variable interpretation of the GLMs on binary data

A univariate binary response variable @xmath is assumed to follow a
Bernoulli distribution with parameter @xmath , @xmath . @xmath is a
multivariate explanatory variable and @xmath . For the GLMs on binary
data, we assume that the nonlinear transformation of the conditional
mean of @xmath is a linear function of @xmath , @xmath , in which @xmath
is the link function, @xmath is the conditional mean, and @xmath is a
@xmath dimensional coefficient vector. If the inverse function of @xmath
is @xmath , we have @xmath . If the logit link is used, @xmath , which
is the logistic linear regression model, and @xmath can be interpreted
as the log-odds, which is the natural parameter of Bernoulli
distribution expressed in exponential family distribution form. If the
probit link is used, @xmath , in which @xmath is the cumulative density
function (CDF) of the standard normal distribution, which is the probit
linear regression model.

The fact that the inverse link function @xmath can be interpreted as the
CDF of a specific probability distribution, motivates the latent
variable interpretation of the logistic or probit linear regression [ 13
] . @xmath can be assumed as the sign observation of a quantitative
latent variable @xmath , which has a linear relationship with the
explanatory variable @xmath . Taking the probit linear regression as an
example, the latent variable interpretation can be expressed as,

  -- -------- --
     @xmath   
  -- -------- --

in which @xmath is the latent variable, @xmath is the error term, and
@xmath is the indicator function. The probability for the observation
@xmath is @xmath . A similar latent variable interpretation can be
applied to the logistic linear regression model by assuming that the
error term @xmath follows the standard logistic distribution. The
probability density function of the logistic distribution can be
expressed as,

  -- -------- --
     @xmath   
  -- -------- --

in which @xmath and @xmath are the location and scale parameters. In the
standard logistic distribution, @xmath , @xmath . The inverse-logit
function @xmath is the CDF of the standard logistic distribution. The
assumption of @xmath for the @xmath is reasonable since we want to use
the linear function @xmath to capture the conditional mean of @xmath .
The assumption of @xmath for the @xmath seems restrictive, however
scaling the estimated @xmath by a positive constant as @xmath will not
change the conclusion of the model. Since the assumption of logistic
distributed noise is not very straightforward, the latent variable
interpretation of the logistic linear regression model is not widely
used.

The above latent variable interpretation of the GLMs on binary data is
naturally connected to the generating process of binary data [ 32 ] .
Binary data can be discrete in nature, for example when females and
males are classified as “1” and “0”. Another possibility is that there
is a continuous process underlying the binary observation. For example
in a toxicology study, the binary outcome of a subject being dead or
alive relates to the dosage of a toxin used and the subject’s tolerance
level. The tolerance varies for different subjects, and the status (dead
or alive) of a specific subject depends on whether its tolerance is
higher than the used dosage or not. Thus, a continuous tolerance level
is underlying the binary outcome [ 13 ] . If we assume our binary data
set is generated from a continuous process, it is natural to use the
latent variable interpretation of the probit link; or if it is assumed
from a discrete process, we can use the logit link, and interpret it
from the probabilistic perspective rather than the latent variable
perspective. However, usually, the difference between the results
derived from the GLMs using logit or probit link is negligible [ 13 ] .

#### 3.2.2 Latent variable interpretation of the logistic PCA model

The measurement of @xmath binary variables on @xmath samples results in
a binary matrix @xmath ( @xmath ), whose @xmath element @xmath equals
“1” or “0”. The logistic PCA model on @xmath can be interpreted as
follows. Conditional on the low rank structure assumption, which is used
to capture the correlations observed in @xmath , elements in @xmath are
independent realizations of the Bernoulli distributions, whose
parameters are the corresponding elements of a probability matrix @xmath
( @xmath ), @xmath . Assuming the natural parameter matrix, which is the
logit transform of the probability matrix @xmath , is @xmath ( @xmath ),
we have @xmath and @xmath , in which @xmath and @xmath are the
element-wise logit and inverse logit functions. The low rank structure
is imposed on @xmath in the same way as in a classical PCA model, @xmath
, in which @xmath ( @xmath ) is the @xmath dimensional column offset
term and can be interpreted as the logit transform of the marginal
probabilities of the binary variables. @xmath ( @xmath ) and @xmath (
@xmath ) are the corresponding low rank score and loading matrices, and
@xmath , @xmath , is the low rank. Therefore, for the logistic PCA
model, we have @xmath . On the other hand, in a classical PCA model, we
have @xmath , which is equivalent to using the identity link function.
Furthermore, unlike in the classical PCA model, the column offset @xmath
has to be included into the logistic PCA model to do the model based
column centering. The reason is that the commonly used column centering
processing step is not allowed to be applied on the binary data set as
the column centered binary data is not binary anymore.

The logistic PCA model can be re-expressed in the same way as the latent
variable interpretation of the GLMs on binary data. Our binary
observation @xmath is assumed to be the sign observation of an
underlying quantitative data set @xmath ( @xmath ), and for the @xmath
element, we have @xmath if @xmath and @xmath vice versa . The low rank
structure is imposed on the latent data set @xmath as @xmath , in which
@xmath ( @xmath ) is the error term, and its elements follow a standard
logistic distribution. The latent variable interpretation of the
logistic PCA model can be expressed as,

  -- -------- --
     @xmath   
  -- -------- --

Similar to the latent variable interpretation of the logistic linear
model, the assumption of @xmath is not restrictive, since scaling the
estimated @xmath by a positive constant @xmath will not change the
conclusions from the model. When the standard normal distributed error
is used in the above derivation, we get the probit PCA model. The latent
variable interpretation of the logistic or probit PCA not only
facilitates our understanding of the low rank structure underlying a
multivariate binary data, but also provides a way to define the SNR in
multivariate binary data simulation.

### 3.3 Logistic PCA via singular value thresholding

#### 3.3.1 The standard logistic PCA model

Assume the column centered @xmath is @xmath , @xmath . In the standard
logistic PCA model, the exact low rank constraint is imposed on @xmath
as the multiplication of two rank @xmath matrices @xmath and @xmath .
The negative log likelihood of fitting the observed @xmath conditional
on the low rank structure assumption on @xmath is used as the loss
function. We also introduce a weight matrix @xmath ( @xmath ) to tackle
the potential missing values in @xmath . The @xmath element of @xmath ,
@xmath , equals 0 when the corresponding element in @xmath is missing;
while it is 1 vice versa . The optimization problem of the standard
logistic PCA model can be expressed as,

  -- -------- -------- -------- -------
     @xmath   @xmath            (3.1)
                       @xmath   
                       @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- -------

in which the constraint @xmath is imposed to make @xmath identifiable.
Unfortunately, the classical logistic PCA model tends to overfit the
observed binary data. In order to decrease the loss function in equation
3.1 , @xmath tends to approach positive infinity when @xmath , and
negative infinity when @xmath . This overfitting problem will be
explored in more detail below. In logistic linear regression, this
overfitting problem can be solved by adding a quadratic penalty on the
coefficient vector to regularize the estimated parameters. A similar
idea can be applied to the logistic PCA model by taking it as a
regression type problem. The columns of the score matrix @xmath are
taken as the unobserved explanatory variables, while the loading matrix
@xmath are the coefficients. If we decompose @xmath into a @xmath
truncated SVD as @xmath , then @xmath and @xmath . It is easy to show
that the quadratic penalty @xmath , in which @xmath is the @xmath
singular value of @xmath . Therefore, it is possible to derive a robust
logistic PCA model by thresholding the singular values of @xmath .

#### 3.3.2 Logistic PCA via non-convex singular value thresholding

The most commonly used penalty function in thresholding singular values
is the nuclear norm penalty, and it has been used in solving many low
rank approximation problems [ 55 , 56 , 52 , 51 ] . If the SVD
decomposition of matrix @xmath is @xmath , the nuclear norm penalty can
be expressed as @xmath , in which @xmath is the @xmath singular value.
The nuclear norm penalty is the convex relaxation of the exact low rank
constraint and can be regarded as applying a lasso penalty on the
singular values of a matrix. Therefore, the nuclear norm penalty has the
same problem as the lasso penalty, it shrinks all singular values to the
same degree. This leads to a biased estimation of the large singular
values. This behavior will further make the prediction error or CV error
based model selection procedure inconsistent [ 26 ] . As an alternative,
non-convex penalties can shrink the parameters in a nonlinear manner to
achieve both nearly unbiased and sparse parameter estimation [ 16 , 27 ]
. Therefore, we propose to replace the exact low rank constraint in the
logistic PCA model by a concave penalty on the singular values of @xmath
to achieve a low rank estimation and to alleviate the overfitting issue.
We include the frequentist version of the generalized double Pareto
(GDP) [ 27 ] shrinkage, the smoothly clipped absolute deviation (SCAD)
penalty [ 16 ] and the @xmath penalty [ 57 ] as examples of concave
penalties in our implementation. The concave penalty on the singular
values of @xmath can be expressed as @xmath , in which @xmath is a
concave function in Table 3.1 , @xmath is the @xmath singular value of
@xmath . The thresholding properties of the exact low rank constraint,
the nuclear norm penalty, and various concave penalties with different
values of hyper-parameter are shown in Fig. 3.1 . Since the nuclear norm
penalty is a linear function of the singular values, it is both convex
and concave with respect to the singular values. Also, it is a special
case of the @xmath penalty when setting @xmath . Thus, the algorithm
developed in this chapter also applies to the model with a nuclear norm
penalty. The penalized negative log likelihood for fitting the observed
binary data @xmath of the logistic PCA with a concave penalty can be
shown as,

  -- -------- -------- -------- -------
     @xmath                     (3.2)
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- -------

in which @xmath and @xmath are as described above, and @xmath is the
tuning parameter.

### 3.4 Algorithm

Based on the MM principle [ 58 , 59 ] , a MM algorithm is derived to fit
the logistic PCA model via non-convex singular value thresholding. The
derived algorithm is guaranteed to decrease the objective function in
equation 3.2 during each iteration and the analytical form for updates
of all the parameters in each iteration and are presented below.
Although the following derivation focuses on using the logit link
function, the option for probit link is included in our implementation.

#### 3.4.1 The majorization of the negative log-likelihood

The negative log-likelihood @xmath can be majorized to a quadratic
function of @xmath by exploiting the upper-bound of the second order
gradient of @xmath . Suppose @xmath , in which @xmath and @xmath are the
@xmath elements of @xmath and @xmath , @xmath can be expressed as @xmath
. When the logit link is used, the following results can be easily
derived out, @xmath , @xmath . Assume that @xmath is upper bounded by a
constant @xmath . Since @xmath when the logit link is used [ 43 ] we can
set @xmath . Take @xmath as the general representation of @xmath ,
according to the Taylor’s theorem and the assumption that @xmath for
@xmath , we have the following inequality,

  -- -------- -------- -------- -------
     @xmath   @xmath            (3.3)
                       @xmath   
                       @xmath   
  -- -------- -------- -------- -------

where @xmath is the @xmath approximation of @xmath , @xmath is an
unknown constant, @xmath is an unknown constant doesn’t depend on @xmath
. Therefore, we have the following inequality about @xmath , @xmath .
Assume @xmath is the matrix forms of @xmath , @xmath . The inequality of
@xmath can be derived out as @xmath , in which @xmath indicates
element-wise matrix multiplication. Following [ 60 ] , we further
majorize the weighted least-squares upper bound into a quadratic
function of @xmath as

  -- -- -------- -------- -------
        @xmath            (3.4)
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- -------

#### 3.4.2 The majorization of the non-convex penalty

Suppose @xmath is the @xmath singular value of @xmath , and @xmath is a
concave function. From the definition of concavity [ 61 ] , we have
@xmath , in which @xmath is the @xmath singular value of the @xmath
approximation @xmath and @xmath is an unknown constant. Also, @xmath and
@xmath is the set of supergradients of function @xmath at @xmath . For
all the concave penalties used in Table 3.1 , their supergradient is
unique, thus @xmath . Therefore, @xmath can be majorized as follows

  -- -------- -------- -------- -------
     @xmath   @xmath            (3.5)
                       @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- -------

#### 3.4.3 Block coordinate descent

Summarizing the above two majorization steps, we have the following
majorized problem during the @xmath iteration.

  -- -------- -------- -------- -------
     @xmath   @xmath            (3.6)
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- -------

This majorized problem during the @xmath iteration can be solved by the
block coordinate descent algorithm.

##### Updating @xmath

When fixing @xmath in equation 3.6 , the analytical form solution of
@xmath is the column mean of @xmath , @xmath .

##### Updating @xmath

After deflating the offset set term @xmath in equation 3.6 , the
optimization problem of @xmath becomes @xmath , in which @xmath is the
column centering operator @xmath . This optimization problem is
equivalent to finding the proximal operator of the weighted sum of
singular values, for which the analytical form global solution exists [
62 ] . If the SVD decomposition of @xmath is @xmath , the analytical
form solution of @xmath is @xmath , in which @xmath , and @xmath is
@xmath element of the diagonal of @xmath .

##### Initialization

The initialization @xmath and @xmath can be set according to the user
imputed values, or by using the following random initialization
strategy. All the elements in @xmath can be sampled from the standard
uniform distribution and @xmath can be set to @xmath . In the following
algorithm, @xmath indicates the objective value in equation 3.2 during
the @xmath iteration, the relative change of the objective value is used
as the stopping criteria. @xmath indicates the tolerance for the
relative change of the objective value. Pseudocode of the algorithm
described above is shown in Algorithm 1 .

1: @xmath , @xmath , @xmath ;

2: @xmath , @xmath , @xmath ;

3: @xmath ;

4: Compute @xmath for missing values;

5: Initialize @xmath , @xmath ;

6: while @xmath do

7: @xmath ;

8: @xmath ;

9: @xmath ;

10: @xmath ;

11: @xmath ;

12: @xmath ;

13: @xmath ;

14: @xmath ;

15: @xmath ;

16: @xmath ;

17: @xmath ;

18: end while

19: @xmath ;

20: @xmath ;

Algorithm 1 An MM algorithm to fit the logistic PCA model via non-convex
singular value thresholding.

### 3.5 Real data set and simulation process

#### 3.5.1 Real data set

The CNA data sets in Chapter 1 is used as an example of real data sets
to show the results. The characterization of the CNA data set is shown
in supplemental Fig. S1 b.

#### 3.5.2 Simulation process

Multivariate binary data @xmath is simulated according to the logistic
PCA model in a similar way as Chapter 2 except that SNR is defined and
there are no group structures in the sample space. Based on the latent
variable interpretation of the logistic PCA model, we can define the SNR
as @xmath , in which @xmath is the error term, and its elements are
sampled from the standard logistic distribution. The column offset term
@xmath can be set in the same way as in Chapter 1 . However the rank
@xmath matrix @xmath is simulated in a slightly different way. We first
express @xmath in a SVD type as @xmath , in which @xmath , @xmath and
the diagonal of @xmath contains the singular values. Elements in @xmath
and @xmath are first sampled from @xmath . After that, the column mean
of @xmath is deflated to have @xmath , and the SVD is used to force
@xmath being orthogonal. Also, @xmath is forced to orthogonality by the
Gram-Schmidt algorithm. Then, the diagonal matrix @xmath , whose @xmath
diagonal elements are the sorted absolute values of the samples from
@xmath , is simulated. We express @xmath as @xmath , in which @xmath is
a constant used to adjust the SNR in the simulation of the multivariate
binary data. Then @xmath according to the logistic PCA model and @xmath
according to the latent variable interpretation. The probability matrix
@xmath is generated as @xmath , in which @xmath by the inverse logit
link function. Finally, the multivariate binary data set @xmath is
generated from Bernoulli distributions with the corresponding parameters
in @xmath .

### 3.6 Model assessment and model selection

In this chapter we focus on evaluating the model’s performance in
estimating the simulated parameters @xmath , @xmath , @xmath and @xmath
. Also, the CV error is defined based on the log likelihood of fitting
binary data rather than misclassifying the binary data.

#### 3.6.1 Model assessment

After a logistic PCA model is constructed on the simulated binary data,
we have the estimated parameters @xmath , @xmath and @xmath , and @xmath
and @xmath can also be computed. The model’s ability in recovering the
true @xmath can be evaluated by the relative mean squares error (RMSE),
which is defined as @xmath , where @xmath is the true parameter. The
RMSEs in estimating @xmath and @xmath are defined in the same way. In
some cases the mean Hellinger distance (MHD) to quantify the similarity
between the true probability matrix @xmath and the estimated @xmath is
used. Hellinger distance [ 63 ] is a symmetric measure to quantify the
similarity between two probability distributions. Assuming the parameter
of a Bernoulli distribution is @xmath and its estimation is @xmath , the
Hellinger distance is defined as @xmath . The mean Hellinger distance
between the probability matrix @xmath and its estimate @xmath is defined
as @xmath .

#### 3.6.2 Model selection

For the model selection on real data, a missing value based cross
validation (CV) procedure is proposed. The CV error is computed as
follows. First, elements in @xmath are split into the training and test
sets as follows: @xmath “1”s and “0”s of @xmath are randomly selected as
the test set @xmath , which are set to missing values, and the resulting
data set is taken as @xmath . After getting an estimation of @xmath from
a logistic PCA on the @xmath , we can index the elements, which are
corresponding to the test set @xmath , as @xmath . Then the CV error is
defined as the negative log-likelihood of using @xmath to fit @xmath .

There are two tuning parameters, @xmath and @xmath , during the model
selection of the logistic PCA model with a concave penalty. However, the
performance of the model is rather insensitive to the selection of
@xmath for some concave penalties, which will be shown below. After
fixing the value of @xmath , we can use a grid search to select a proper
value of @xmath based on the minimum CV error. First, a sequence of
@xmath values can be selected from a proper searching range, after which
logistic PCA models will be fitted with the selected @xmath values on
the training set @xmath . A warm start strategy, using the results of a
previous model as the initialization of the next model, is used to
accelerate the model selection process. The model with the minimum CV
error is selected and then it is re-fitted on the full data set @xmath .
Because the proposed model is non-convex and its result is sensitive to
the used initialization, it is recommended to use the results derived
from the selected model as the initialization of the model to fit the
full data sets.

### 3.7 Results

#### 3.7.1 Standard logistic PCA model tends to overfit the data

In this first section we will use the CNA data as an example. The
algorithm from [ 43 ] is implemented to fit the standard logistic PCA
model. Constraints, @xmath and @xmath , are imposed. Two different
standard logistic PCA models are constructed of the CNA data, both with
three components. The first model is obtained with low precision
(stopping criteria was set to @xmath ) while for the other model a high
precision was used ( @xmath ). The initialization was the same for these
two models. The low precision model converged already after 220
iterations, while the high precision model did not convergence even
after 50000 iterations. The difference between the final objective
values of these two models is not large, @xmath and @xmath respectively.
However, as shown in Fig. 3.2 , the scale of the loading plots derived
from these two models is very different. When a high precision stopping
criteria is used, some of the elements from the estimated loading matrix
from the standard logistic PCA model tend to become very large.

#### 3.7.2 Model selection of the logistic PCA model with a concave GDP
penalty

Here we use logistic PCA model with a concave GDP penalty as an example
to show the CV error based model selection procedure. The data set is
simulated as follows. The offset term @xmath is set to the logit
transform of the empirical marginal probabilities of the CNA data to
simulate an imbalanced binary data set. Other parameters used in the
simulation are @xmath , @xmath , @xmath and @xmath . First we will show
the model selection procedure of @xmath while the hyper-parameter @xmath
is fixed to @xmath . After splitting the simulated binary data set
@xmath into the training set @xmath and the test set @xmath , 30 @xmath
values are selected from the searching range @xmath with equal distance
in log-space. For each @xmath value, a logistic PCA with a GDP penalty (
@xmath , maximum iteration is 500) is constructed on @xmath and for each
model we evaluate its performance in estimating the simulated
parameters. As shown in the model selection results (Fig. 3.3 ), the
selected model with minimum CV error can also achieve approximately
optimal RMSEs in estimating the simulated @xmath , @xmath and @xmath .
However, the rank of the estimated @xmath from the selected model is 3,
which is different from the simulated rank @xmath . The reason will be
discussed later. The selected model is re-fitted on the full simulated
data @xmath , and the RMSEs of estimating @xmath , @xmath and @xmath are
0.0797, 0.2064 and 0.0421 respectively.

Next, we will show the model selection process of both @xmath and @xmath
. The simulated data @xmath is split into the @xmath and the @xmath in
the same way as described above. 30 @xmath values are selected from the
range @xmath equidistant in log-space. For each @xmath , 30 values of
@xmath are selected from a proper searching range, which is determined
by an automatic procedure. For each value of @xmath , the model
selection of @xmath is done on the @xmath in the same way as described
above, after which the selected model is re-fitted on the full data
@xmath . Therefore, for each value of @xmath , we have a selected model,
which is optimal with respect to the CV error. As shown in Fig. 3.4
(left), the difference between the RMSEs derived from these selected
models is very small. This can be caused by two reasons: the model is
insensitive to the selection of @xmath or the CV error based model
selection procedure is not successful in selecting @xmath . To clarify
the correct reason, we also fit @xmath models on the full data @xmath in
the same way as the above experiment. For each value of @xmath , the
model with minimum @xmath is selected. As shown in Fig. 3.4 (right), the
value of @xmath does not have a large effect on the RMSEs of the
selected models, which are optimal with respect to the @xmath .
Therefore, it can be concluded that the performance of the model is
insensitive to the model selection of @xmath . Therefore, the strategy
can be to set a default value for @xmath and focus on the selection of
@xmath .

#### 3.7.3 Model selection of the logistic PCA model with other concave
penalties

For the logistic PCA models with @xmath and SCAD penalty, the model is
selected and re-fitted on the full data sets in the same way as above.
Fig. 3.5 shows how the value of hyper-parameter @xmath or @xmath effects
the RMSEs in estimating @xmath , @xmath and @xmath from the logistic PCA
models, which are optimal with respect to CV error, with @xmath penalty
(left) and SCAD penalty (right). The model with @xmath penalty can
achieve similar performance as the model with GDP penalty when proper
value of @xmath is selected, while the model with SCAD penalty tends to
have very large RMSEs in estimating @xmath , @xmath and @xmath for all
the values of hyper-parameter @xmath .

#### 3.7.4 The performance of the logistic PCA model using different
penalties

In this section, we compare the performance of the logistic PCA models
with the exact low rank constraint, the nuclear norm penalty, SCAD (
@xmath ), @xmath ( @xmath ) and the GDP ( @xmath ) penalty. Random
initialization is used and the maximum number of iterations is set to
10000 for all the models. Furthermore, all models are fitted using both
@xmath and @xmath to test the model’s robustness to the stopping
criteria. For the standard logistic PCA model using the exact low rank
constraint, 5 components are used. For the models with GDP ( @xmath ),
nuclear norm, SCAD ( @xmath ) and @xmath ( @xmath ) penalties, the
models are selected (the model selection results are shown in Fig. 3.3
and the supplemental Fig. S3.1) and re-fitted on full data set in the
same way as was described above. In addition, according to the latent
variable interpretation of the logistic PCA model, the unobserved
quantitative data set @xmath is available in our simulation. We
constructed a 5 components PCA model (with offset term) on this latent
data @xmath , and this model is called the full information model. The
results of above experiment are shown in Table 3.2 . Since the logistic
PCA model with nuclear norm penalty is a convex problem, the global
solution can be achieved. The results from this model are taken as the
baseline to compare other approaches. The drawback of the model with the
nuclear norm penalty is that the proposed CV error based model selection
procedure tends to select a too complex model to compensate for the
biased estimation caused by the nuclear norm penalty (supplemental
Fig. S3.2, Table 3.2 ). Compared to the model with nuclear norm penalty,
the logistic PCA model with exact low rank constraint and SCAD penalty
tends to overfit the data, thus have bad performance in estimating the
simulated parameters @xmath , @xmath and @xmath . Also these models are
not robust to the stopping criteria. Compared to the model with nuclear
norm penalty, the logistic PCA models with a GDP penalty or a @xmath
penalty perform well in estimating the simulated parameters, and their
results are even close to the full information model.

The difference in the performance of the logistic PCA models with
different penalties (Table 3.2 ) are mainly related to how these
penalties shrink the singular values. Therefore, we also compared the
singular values of the simulated @xmath and their estimations from the
logistic PCA models with different penalties, and its estimation from
the full information model. The results are shown in Fig. 3.6 . The
simulated low rank is 5, however the last component is overwhelmed by
the noise. Furthermore, the @xmath component is less than 2 times noise
level and therefore cannot be expected to be distinguished from the
noise. From Fig. 3.6 (left) it becomes clear that the logistic PCA
models with exact low rank constraint and SCAD penalty clearly
overestimate the singular values of @xmath . And when the more strict
stopping criterion is used, the overestimation problem becomes even
worse. Fig. 3.6 (right) shows that the logistic PCA model with nuclear
norm penalty underestimated the singular values of @xmath , and includes
too many small singular values into the model. The logistic PCA model
with GDP penalty and @xmath penalty have very accurate estimation of the
first three and four singular values of @xmath . These results are in
line with their performance measures in Table 3.2 and their thresholding
properties in Fig. 3.1 . The bad performance of the model with exact low
rank constraint is mainly because the non-zero singular values are not
regularized at all (Fig. 3.1 ). Similarly, some of the non-zero singular
values (the values larger than @xmath ) are also not regularized at all
for the SCAD penalty (Fig. 3.1 ). This property can be more problematic
for the logistic PCA model with SCAD penalty because the selected model
depends on the model with the smallest @xmath value due to the used warm
start strategy during the model selection process. The low performance
of the model with nuclear norm penalty is because this penalty will over
shrink the larger singular values, and the model selected based on CV
error is too complex (Fig. 3.1 ). On the contrary, both the models with
@xmath and GDP penalties have nice thresholding properties and the
corresponding logistic models have superior performance. However, unlike
SCAD and GDP, the @xmath ( @xmath ) penalty has a small discontinuity
region, continuous thresholding can not be achieved, which could results
in instable prediction [ 16 ] . Therefore, even though the model with
@xmath penalty achieves slight better performance, we still recommend to
use the GDP penalty for the non-convex singular thresholding in the
logistic PCA model.

#### 3.7.5 Performance of the logistic PCA model as a function of SNR in
the binary simulation.

In the analysis of simulated quantitative data sets using the PCA model,
an increase in SNR makes the estimation of the true underlying low rank
structure easier. Unfortunately, this is not true in the estimation of
the true underlying logistic PCA model for simulated binary data. To
illustrate this, the following experiment was performed using logistic
PCA model with GDP penalty as an example. 30 SNR values are selected
from the interval @xmath equidistant in log-space. The simulated offset
term @xmath is set to @xmath to simulate balanced binary data sets, the
number of samples, variables, and the low rank are the same as the
experiment described above. For the binary data simulations with
different SNRs, only the constant @xmath , which is used to adjust the
SNR, changes with the SNR. All other parameters are kept the same. For
each simulated @xmath with a specific SNR, logistic PCA models with GDP
( @xmath ) penalty and with nuclear norm penalty are selected and
re-fitted. In addition, PCA models with different numbers of components
are fitted on the latent quantitative data set @xmath , and the model
with the minimum @xmath is selected. In addition, the null model, i.e.
the logistic PCA model with only the column offset term, is used to
provide a baseline for comparison of the different approaches. The above
experiments are repeated 10 times, and their results are shown in Fig.
3.7 . Results obtained from a similar experiment but performed on
imbalanced data simulation are shown in supplemental Fig. S3.2. There,
the simulated @xmath is set according to the marginal probabilities of
the CNA data set. Overall, the logistic PCA models with different
penalties can always achieve better performance than the null model, and
the model with a GDP penalty demonstrates superior performance with
respect to all the used metrics compared to the model with convex
nuclear norm penalty.

Fig. 3.7 (left and center) shows that with increasing SNR, the
estimation of the quantitative full model improves as expected. However,
for the parameters estimated from the binary data this is not the case.
First the estimation of the simulated parameters @xmath and @xmath
improves, but when the SNR increases even further, the estimation
deteriorates again leading to a bowl shaped pattern. This pattern has
been observed before in binary matrix completion using nuclear norm
penalty [ 52 ] . In order to understand this effect, considering the
S-shaped logistic curve (supplemental Fig. S3.3), the plot of the
function @xmath , in which @xmath and @xmath are a typical element of
@xmath and @xmath respectively. This curve almost becomes flat when
@xmath becomes very large. There is no resolution anymore in these flat
regimes. A large deviation in @xmath has almost no effect on the
logistic response. When the SNR becomes extremely large, the scale of
the simulated parameter @xmath is very extreme, then even if we have a
good estimation of the probability @xmath , the scale of estimated
@xmath can be far away from the simulated @xmath . That is why we
observed that even though the model is able to recovered the simulated
@xmath based on the logistic PCA model almost exactly (Fig. 3.7 right),
the estimation of @xmath and @xmath are not accurate (Fig. 3.7 left and
center). We refer to [ 52 ] for a detailed interpretation of this
phenomenon.

#### 3.7.6 Real data analysis

We demonstrate the proposed logistic PCA model with a GDP ( @xmath )
penalty and the corresponding model selection procedure on the CNA data
set. The model selection is done in the same way as was described above,
and the result is shown in supplemental Fig. S3.4. After that, the
selected 4 components model is re-fitted on the full data set. The score
and loading plot of the first 2 components are shown in Fig. 3.8 . As
was explained before in [ 64 ] , the CNA data set is not discriminative
for the three cancer types (illustrated in the score plot of Fig. 3.8
left). The structure in the loading plot (Fig. 3.8 right) mainly
explains the technical characteristics of the data. Fig. 3.8 (right)
shows that the gains and losses of the segments in the chromosomal
regions corresponding to the CNA measurements are almost perfectly
separated from each other in the first component. Therefore, the
variation explained of the first component is mainly because of the
difference of gains and losses in CNA measurements.

### 3.8 Discussion

To study the properties of the logistic PCA model with different
penalties, we need to have the ability to simulate the multivariate
binary data set with an underlying low rank structure, and the simulated
structure should have a proper SNR so that the model can find it back.
The latent variable interpretation of the logistic PCA model not only
makes the assumption of low rank structure easier to understand, but
also provides us a way to define SNR in multivariate binary data
simulation.

The standard logistic PCA model using the exact low rank constraint has
an overfitting problem. The overfitting issue manifests itself in a way
that some of the elements in the estimated loading matrix @xmath (the
orthogonality constraint is imposed on @xmath ) have the tendency to
approach infinity, and the non-zero singular values of the @xmath are
not upper-bounded when strict stopping criteria are used. This
overfitting issue can be alleviated by regularizing the singular values
of @xmath . Both convex nuclear norm penalties and some of the concave
penalties can induce low rank estimation and simultaneously constrain
the scale of the non-zero singular values. Therefore, logistic PCA
models with these penalties do not suffer from the overfitting problem.

However, the logistic PCA model with a GDP or a @xmath penalty has
several advantages compared to the model with the nuclear norm penalty.
Since the nuclear norm penalty applies the same degree of shrinkage on
all the singular values, the large singular values are shrunken too
much. Therefore, the implemented CV error based model selection
procedure tends to select a very complex model with too many components
to compensate for the biased estimations. On the contrary, both the GDP
penalty and the @xmath penalty achieve nearly unbiased estimation. Thus
the CV error based model selection is successful in selecting the
logistic PCA model with the a GDP penalty. Furthermore, the selected
logistic PCA model with a GDP or a @xmath penalty has shown superior
performance in recovering the simulated low rank structure compared to
the model with the nuclear norm penalty, and the exact low rank
constraint.

One exception of the used concave penalties is the SCAD penalty, which
leads to a logistic PCA model with poor performance. As stated in the
Section 3.7.4 , the poor performance of the model with the SCAD penalty
is mainly because of that some of the large singular values are not
regularized at all. And this drawback is exaggerated by the wart start
strategy used during the model selection process. The poor performance
of the models with the exact low rank constraint and the SCAD penalty
reminds us the importance of regularizing all the singular values
simultaneously in inducing the low rank structure for a logistic PCA
model.

### Acknowledgements

Y.S. gratefully acknowledges the financial support from China
Scholarship Council (NO.201504910809).

### 3.9 Supplementary information

## Chapter 4 Fusing binary and quantitative data sets

In the current era of systems biological research there is a need for
the integrative analysis of binary and quantitative genomics data sets
measured on the same objects. One standard tool of exploring the
underlying dependence structure present in multiple quantitative data
sets is simultaneous component analysis (SCA) model. However, it does
not have any provisions when a part of the data are binary. To this end,
we propose the generalized SCA (GSCA) model, which takes into account
the distinct mathematical properties of binary and quantitative
measurements in the maximum likelihood framework. Like in the SCA model,
a common low dimensional subspace is assumed to represent the shared
information between these two distinct types of measurements. However,
the GSCA model can easily be overfitted when a rank larger than one is
used, leading to some of the estimated parameters to become very large.
To achieve a low rank solution and combat overfitting, we propose to use
non-convex singular value thresholding. An efficient majorization
algorithm is developed to fit this model with different concave
penalties. Realistic simulations (low signal-to-noise ratio and highly
imbalanced binary data) are used to evaluate the performance of the
proposed model in recovering the underlying structure. Also, a missing
value based cross validation procedure is implemented for model
selection. We illustrate the usefulness of the GSCA model for
exploratory data analysis of quantitative gene expression and binary
copy number aberration (CNA) measurements obtained from the GDSC1000
data sets. ¹ ¹ 1 This chapter is based on Song, Y., Westerhuis, J.A.,
Aben, N., Wessels, L.F., Groenen, P.J. and Smilde, A.K., 2018.
Generalized Simultaneous Component Analysis of Binary and Quantitative
data. arXiv preprint arXiv:1807.04982.

### 4.1 Background

In biological research it becomes increasingly common to have
measurements of different aspects of information on the same objects to
study complex biological systems. The resulting coupled data sets should
be analyzed simultaneously to explore the dependency between variables
in different data sets and to reach a global understanding of the
underlying biological system. The SCA model is one of the standard
methods for the integrative analysis of such coupled data sets in
different areas, from psychology to chemistry and biology [ 65 ] . SCA
discovers the common low dimensional column subspace of the coupled
quantitative data sets, and this subspace represents the shared
information between them.

Next to the quantitative measurements (such as gene expression data), it
is common in biological research to have additional binary measurements,
in which distinct categories differ in quality rather than in quantity
(such as mutation data). Typical examples include the measurements of
point mutations, the binary version of CNA and DNA methylation
measurements [ 30 ] . Compared to quantitative measurement, a binary
measurement only has two mutually exclusive outcomes, such as presence
vs absence (or true vs false), which are usually labeled as “1” and “0”.
However, “1” and “0” indicate abstract representations of two categories
rather than quantitative values 1 and 0. As such, the special
mathematical properties of binary data should be taken into account in
the data analysis. In most biological data sets, the number of “0”s is
significantly larger than the number of “1”s for most binary variables
making the data imbalanced. Therefore, an additional requirement of the
data analysis method is that it should be able to handle imbalanced
data.

There is a need for statistical methods appropriate for doing an
integrative analysis of coupled binary and quantitative data sets in
biology research. The standard SCA models [ 65 , 66 ] that use column
centering processing steps and least-squares loss criteria are not
appropriate for binary data sets. Recently, iClusterPlus [ 67 ] was
proposed as a factor analysis framework to model discrete and
quantitative data sets simultaneously by exploiting the properties of
exponential family distributions. In this framework, the special
properties of binary, categorical, and count variables are taken into
account in a similar way as in generalized linear models. The common low
dimensional latent variables and data set specific coefficients are used
to fit the discrete and quantitative data sets. For the binary data set,
the Bernoulli distribution is assumed and the canonical logit link
function is used. The sum of the log likelihood is then used as the
objective function. Furthermore, the approach allows the use of a lasso
type penalty for feature selection. The Monte Carlo Newton–Raphson
algorithm for this general framework, however, involves a very slow
Markov Chain Monte Carlo simulation process. Both the high complexity of
the model and the algorithmic inefficiency limit its use for large data
sets and exploring its properties through simulations.

In this chapter, we generalize the SCA model to binary and quantitative
data from a probabilistic perspective similar as in Collins [ 68 ] and
Mo [ 67 ] . However, the generalized SCA model can easily lead to
overfitting by using a rank restriction larger than @xmath , leading to
some of the parameters to become very large. Therefore, a penalty on the
singular values of the matrix is used to simultaneously induce the low
rank structure in a soft manner and to control the scale of estimated
parameters. A natural choice is the convex nuclear norm penalty, which
is widely used in low rank approximation problems [ 69 , 51 , 70 ] .
However, the nuclear norm penalty shrinks all singular values to the
same degree, leading to biased estimates of the important latent
factors. Hence, we would like to reduce the shrinkage for the most
important latent factors while increasing the shrinkage for unimportant
latent factors. This nonlinear shrinkage strategy has shown its
superiority in recent work of low rank matrix approximation problems
under the presence of Gaussian noise [ 18 , 54 ] . Therefore, we will
explore the nonlinear shrinkage of the latent factors through concave
penalties in our GSCA model. The fitting of the resulting GSCA model is
a penalized maximum likelihood estimation problem. We derive a MM [ 58 ,
59 ] algorithm to solve it. Simple closed form updates for all the
parameters are derived for each step in the algorithm. A missing value
based cross validation procedure is also implemented to do model
selection. Our algorithm is easy to implement and is guaranteed to
decrease the loss function monotonically in each iteration.

### 4.2 The GSCA model

Before the GSCA model is introduced, consider the standard SCA model.
The quantitative measurements on the same @xmath objects from two
different platforms result into two data sets @xmath ( @xmath ) and
@xmath ( @xmath ), in which @xmath and @xmath are the number of
variables. Assume both @xmath and @xmath are column centered. The
standard SCA model can be expressed as

  -- -------- -------- -------- -------
     @xmath   @xmath            (4.1)
              @xmath   @xmath   
  -- -------- -------- -------- -------

where @xmath ( @xmath ) denotes the common component scores (or latent
variables), which span the common column subspace of @xmath and @xmath ,
@xmath ( @xmath ) and @xmath ( @xmath ) are the data set specific
loading matrices for @xmath and @xmath respectively, @xmath ( @xmath )
and @xmath ( @xmath ) are residuals, @xmath , @xmath , is an unknown low
rank. Orthogonality is imposed on @xmath as @xmath , where @xmath
indicates the @xmath identity matrix, to have a unique solution. @xmath
, @xmath and @xmath are estimated by minimizing the sum of the squared
residuals @xmath and @xmath .

#### 4.2.1 The GSCA model of binary and quantitative data sets

Following the probabilistic interpretation of the PCA model [ 42 ] , the
high dimensional quantitative data set @xmath can be assumed to be a
noisy observation from a deterministic low dimensional structure @xmath
( @xmath ) with independent and identically distributed measurement
noise, @xmath . Elements in @xmath ( @xmath ) follow a normal
distribution with mean 0 and variance @xmath , @xmath ). In the same
way, following the interpretation of the exponential family PCA on
binary data [ 68 ] , we assume there is a deterministic low dimensional
structure @xmath ( @xmath ) underlying the high dimensional binary
observation @xmath . Elements in @xmath follow the Bernoulli
distribution with parameters @xmath , @xmath . Here @xmath is the
element wise inverse link function in the generalized linear model for
binary data; @xmath and @xmath are the @xmath element of @xmath and
@xmath respectively. If the logit link is used, @xmath , while if the
probit link is used, @xmath , where @xmath is the cumulative density
function of the standard normal distribution. Although in this chapter,
we only use the logit link in deriving the algorithm and in setting up
the simulations, the option for the probit link is included in our
implementation. The two link functions are similar, but their
interpretations can be quite different [ 13 ] .

In the same way as in the standard SCA model, @xmath and @xmath are
assumed to lie in the same low dimensional subspace, which represents
the shared information between the coupled matrices @xmath and @xmath .
The commonly used column centering is not appropriate for the binary
data set as the centered binary data will not be “1” and “0” anymore.
Every column will still have only 2 values but these values are
different for different columns. Therefore, we include column offset
terms @xmath ( @xmath ) and @xmath ( @xmath ) for a model based
centering. The above ideas are modeled as

  -- -------- -------- -------- -------
     @xmath   @xmath            (4.2)
              @xmath   @xmath   
  -- -------- -------- -------- -------

where, @xmath ( @xmath ) is a @xmath dimensional vector of ones; the
parameters @xmath , @xmath and @xmath have the same meaning as in the
standard SCA model. Constraints @xmath and @xmath are imposed to have a
unique solution.

For the generalization to quantitative and binary coupled data, we
follow the maximum likelihood estimation framework. The negative log
likelihood for fitting coupled binary @xmath and quantitative @xmath is
used as the objective function. In order to implement a missing value
based cross validation procedure [ 71 ] , we introduce two weight
matrices @xmath ( @xmath ) and @xmath ( @xmath ) to handle the missing
elements. The @xmath element of @xmath , @xmath equals 0 if the @xmath
element in @xmath is missing, while it equals 1 vice versa . The same
rules apply to @xmath and @xmath . The loss functions @xmath for fitting
@xmath and @xmath for fitting @xmath are defined as follows:

  -- -------- -------- -------- -------
     @xmath   @xmath            (4.3)
              @xmath   @xmath   
  -- -------- -------- -------- -------

where @xmath indicates element-wise multiplication; @xmath is the
Frobenius norm of a matrix; @xmath is the pseudo @xmath norm of a
matrix, which equals the number of nonzero elements.

The shared information between @xmath and @xmath is assumed to be fully
represented by the low dimensional subspace spanned by the common
component score matrix @xmath . Thus, @xmath and @xmath are
conditionally independent given that the low dimensional structures
@xmath and @xmath lie in the same low dimensional subspace. Therefore,
the joint loss function is the direct sum of the negative log likelihood
functions for fitting @xmath and @xmath .

  -- -------- -------- -------- -------
     @xmath   @xmath            (4.4)
                       @xmath   
                       @xmath   
                       @xmath   
  -- -------- -------- -------- -------

#### 4.2.2 Concave penalties as surrogates for low rank constraint

To arrive at meaningful solutions for the GSCA model, it is necessary to
introduce penalties on the estimated parameters. If we take @xmath ,
@xmath , and @xmath , equation 4.2 in the GSCA model can be expressed as
@xmath . In the above interpretation of the GSCA model, the low rank
constraint on the column centered @xmath is expressed as the
multiplication of two rank @xmath matrices @xmath , @xmath , @xmath .
However, using an exact low rank constraint in the GSCA model has some
issues. First, the maximum likelihood estimation of this model easily
leads to overfitting. Given the constraint that @xmath , overfitting
represents itself in a way that some elements in @xmath tend to diverge
to plus or minus infinity. In addition, the exact low rank @xmath in the
GSCA model is commonly unknown and its selection is not straightforward.

In this chapter, we take a penalty based approach to control the scale
of estimated parameters and to induce a low rank structure
simultaneously. The low rank constraint on @xmath is obtained by a
penalty function @xmath , which shrinks the singular values of @xmath to
achieve a low rank structure. The most widely used convex surrogate of a
low rank constraint is the nuclear norm penalty, which is simply the sum
of singular values, @xmath [ 69 ] , where @xmath represents the @xmath
singular value of @xmath . The nuclear norm penalty was also used in a
related work [ 70 ] . Although the convex nuclear norm penalty is easy
to optimize, the same amount of shrinkage is applied to all the singular
values, leading to biased estimates of the large singular values. Recent
work [ 18 , 62 ] already showed the superiority of concave surrogates of
a low rank constraint under Gaussian noise compared to the nuclear norm
penalty. We take @xmath as the concave surrogate of a low rank
constraint on @xmath , where @xmath is a concave penalty function of
@xmath . After replacing the low rank constraint in equation 4.4 by
@xmath , the model becomes,

  -- -------- -------- -------- -------
     @xmath   @xmath            (4.5)
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- -------

The most commonly used non-convex surrogates of a low rank constraint
are concave functions, including @xmath (bridge penalty) [ 57 , 72 ] ,
smoothly clipped absolute deviation (SCAD) [ 16 ] , a frequentist
version of the generalized double Pareto (GDP) shrinkage [ 27 ] and
others [ 62 ] . We include the first three concave penalties in the
algorithm. Their formulas and their thresholding properties are shown in
Table 3.1 and Fig. 3.1 .

### 4.3 Algorithm

Based on the Majorization-Minimization (MM) principle [ 58 , 59 ] , an
MM algorithm is derived to fit the GSCA model with concave penalties.
The derived algorithm is guaranteed to decrease the objective function
in equation 4.5 during each iteration and the analytical form for
updates of all the parameters in each iteration exist.

#### 4.3.1 The majorization of the penalized negative lilkelihood

When fixing @xmath , we can majorize @xmath to a quadratic function of
the parameter @xmath . In addition, the concave penalty function @xmath
can be majorized to a linear function of the singular values of @xmath
by exploiting the concavity. The resulting majorized problem can be
analytically solved by weighted singular value thresholding [ 62 ] . The
derivation process is the same as the algorithm in Chapter 3 , therefore
we will only show the result here.

  -- -------- -------- -------- -------
     @xmath   @xmath            (4.6)
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- -------

in which @xmath , @xmath and @xmath ; @xmath is the upper-bound of the
second order gradient of @xmath during the @xmath iteration, and can
always set to @xmath ; @xmath is the approximation of parameter @xmath
during the @xmath iteration; @xmath is the @xmath singular value of
@xmath , which is an approximation of @xmath during the @xmath
iteration; @xmath is the approximation of @xmath during the @xmath
iteration; @xmath is a constant doesn’t depend on any unknown
parameters. Summarizing these two majorization steps, we have the
following majorized problem during the @xmath iteration.

  -- -------- -------- -------- -------
     @xmath   @xmath            (4.7)
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- -------

#### 4.3.2 Block coordinate descent

We optimize @xmath , @xmath and @xmath alternatingly while fixing the
other parameters. However, updating @xmath and @xmath depend on solving
the majorized problem in equation 4.7 rather than solving the original
problem in equation 4.5 . Because of the MM principle, this step will
also monotonically decrease the original loss function in equation 4.5 .

##### Updating @xmath

The analytical solution of @xmath in equation 4.7 is simply the column
mean of @xmath , @xmath .

##### Updating @xmath

After deflating the offset term @xmath , the loss function in equation
4.7 becomes @xmath , in which @xmath is the column centering matrix. The
solution of the resulting problem is equivalent to the proximal operator
of the weighted sum of singular values, which has an analytical form
solution [ 62 ] . Suppose @xmath is the SVD decomposition of @xmath ,
the analytical form solution of @xmath is @xmath , in which @xmath and
@xmath is the @xmath diagonal element in @xmath .

##### Updating @xmath

By setting the gradient of @xmath in equation 4.5 with respect to @xmath
to be 0, we have the following analytical solution of @xmath , @xmath .
When no low rank estimation of @xmath can be achieved, the constructed
model is close to a saturated model and the estimated @xmath is close to
0. In that case, when @xmath , the algorithm stops and gives a warning
that a low rank estimation has not been achieved.

##### Initialization and stopping criteria

Random initialization is used. All the elements in @xmath are sampled
from the standard uniform distribution, @xmath is set to 0 and @xmath is
set to 1. The relative change of the objective value is used as the
stopping criteria. Pseudocode of the algorithm described above is shown
in Algorithm 2 . @xmath is the tolerance of relative change of the loss
function.

1: @xmath , @xmath , penalty, @xmath , @xmath ;

2: @xmath , @xmath , @xmath ;

3: Compute @xmath , @xmath for missing values in @xmath and @xmath , and
@xmath ;

4: Initialize @xmath , @xmath , @xmath ;

5: @xmath ;

6: while @xmath do

7: @xmath ; @xmath ;

8: @xmath ;

9: @xmath ;

10: @xmath ;

11: @xmath ;

12: @xmath ;

13: @xmath ;

14: @xmath ;

15: @xmath ;

16: @xmath ;

17: @xmath ;

18: @xmath

19: @xmath ;

20: end while

Algorithm 2 A MM algorithm for fitting the GSCA model with concave
penalties.

### 4.4 Simulation

To see how well the GSCA model is able to reconstruct data generated
according to the model, we do a simulation study with similar
characteristics as a typical empirical data set. We first simulate the
imbalanced binary @xmath and quantitative @xmath following the GSCA
model with logit link and low signal-to-noise ratio (SNR). After that,
we evaluate the GSCA model with respect to 1) the quality of the
reconstructed low rank structure from the model, and 2) the
reconstruction of true number of dimensions.

#### 4.4.1 Data generating process

The SNR for generating binary data is defined according to the latent
variable interpretation of the logistic PCA model. Elements in @xmath
are independent and indirect binary observations of the corresponding
elements in an underlying quantitative matrix @xmath , @xmath if @xmath
and @xmath otherwise. @xmath can be expressed as @xmath , in which
@xmath , and elements in @xmath follow the standard logistic
distribution, @xmath . The SNR for generating binary data @xmath is
defined as @xmath . Assume the quantitative @xmath is simulated as
@xmath , in which @xmath and elements in @xmath follow a normal
distribution with 0 mean and @xmath variance, @xmath . The SNR for
generating quantitative @xmath is defined as @xmath .

After the definition of the SNR, we simulate the coupled binary @xmath
and quantitative @xmath as follows. @xmath represents the logit
transform of the marginal probabilities of binary variables and @xmath
represents the mean of the marginal distributions of quantitative
variables. They will be simulated according to the characteristics of a
real biological data set. The score matrix @xmath and loading matrices
@xmath , @xmath are simulated as follows. First, we express @xmath and
@xmath in a SVD type as @xmath and @xmath , in which @xmath , @xmath and
@xmath are diagonal matrices, @xmath and @xmath . All the elements in
@xmath , @xmath and @xmath are independently sampled from the standard
normal distribution. Then, @xmath , @xmath and @xmath are orthogonalized
by the QR algorithm. The diagonal matrix @xmath ( @xmath ) is simulated
as follows. @xmath elements are sampled from standard normal
distribution, their absolute values are sorted in decreasing order. To
satisfy the pre-specified @xmath and @xmath , @xmath is scaled by
positive scalars @xmath and @xmath as @xmath and @xmath . Then, binary
elements in @xmath are sampled from the Bernoulli distribution with
corresponding parameter @xmath , in which @xmath is inverse logit
function and @xmath . Quantitative data set @xmath is generated as
@xmath , in which @xmath and elements in @xmath are sampled from @xmath
. Take @xmath , @xmath . In order to make @xmath , we further deflate
the column offset of @xmath to the simulated @xmath , @xmath . This step
will not change the value of @xmath and @xmath , thus does not affect
the simulation of @xmath and @xmath .

#### 4.4.2 Evaluation metric and model selection

As for simulated data sets, the true parameters @xmath , @xmath and
@xmath are available. Therefore, the generalization error of the
constructed model can be evaluated by comparing the true parameters and
their model estimates. Thus, the evaluation metric is defined as the
relative mean squared error (RMSE) of the model parameters. The RMSE of
estimating @xmath is defined as @xmath , where @xmath represents the
true parameter and @xmath its GSCA model estimate. The RMSE of @xmath
and @xmath , are expressed as @xmath and @xmath and they are defined in
the same way as for @xmath .

For real data sets, missing value based cross validation (CV) is used to
estimate the generalization error of the constructed model. Also in
order to have an estimation of the uncertainty of the CV error, we will
use a K-fold CV procedure. To make the prediction of the left out fold
elements independent to the constructed model based on the reminding
folds, the data is partitioned into K folds of elements which are
selected in a diagonal style rather than row wise from @xmath and @xmath
respectively, similar to the leave out patterns described by Wold [ 47 ,
71 ] . The test set elements of each fold in @xmath and @xmath are taken
as missing values, and the remaining data are used to construct a GSCA
model. After estimation of @xmath and @xmath are obtained from the
constructed GSCA model, the negative log likelihood of using @xmath ,
@xmath to predict the missing elements (left out fold) is recorded. This
negative log likelihood is scaled by the number of missing elements.
This process is repeated K times until all the K folds have been left
out once. The mean of the K scaled negative log likelihoods is taken as
the CV error.

When we define @xmath and @xmath , the penalty term @xmath is not
invariant to the number of non-missing elements in @xmath , as the joint
loss function (equation 4.4 ) is the sum of the log likelihoods for
fitting all the non-missing elements in the data @xmath . Therefore, we
effectively follow a similar approach as Fan [ 16 ] by adjusting the
penalty strength parameter @xmath for the relative number observations.
By setting one fold of elements to be missing during the CV process,
@xmath rather than @xmath is used as the amount of penalty. During the
K-fold CV process, a warm start strategy, using the results of previous
constructed model as the initialization of next model, is applied. In
this way, the K-fold CV can be greatly accelerated.

In the model selection process, the tuning parameter @xmath and
hyper-parameters ( @xmath in @xmath and @xmath in SCAD and GDP) can be
selected by a grid search. However, previous work of using these penalty
functions in supervised learning context [ 57 , 16 , 27 ] and our
experiments have shown that the results are not very sensitive to the
selection of these hyper-parameters, and thus a default value can be
set. On the other hand, the selection of tuning parameter @xmath does
have a significant effect on the results, and should be optimized by the
grid search.

#### 4.4.3 Experiments

In the loading or score plots of the following experiments and real data
analysis, we will multiply the estimated score matrix @xmath by @xmath ,
and the estimated loading matrix @xmath by @xmath to make the loading
and score plots have the same scale.

##### Overfitting of the GSCA model with a fixed rank and no penalty

The real data sets from the Section 4.5 are used to show how the GSCA
model with a fixed rank and no penalty will overfit the data. The
algorithm (details are in the supplemental material) used to fit the
GSCA model (with an exact low rank constraint and orthogonality
constraint @xmath ) is a modification of the developed algorithm in
Section 4.3 . GSCA models with three components are fitted using
stopping criteria @xmath and @xmath . Exactly the same initialization is
used for these two models. As shown in Fig. 4.1 , different stopping
criteria can greatly affect the estimated @xmath from the GSCA models.
Furthermore, the number of iterations to reach convergence increases
from 141 to 23991.

Related phenomena have been observed in logistic linear regression model
and logistic PCA model [ 43 , 73 ] where some estimated parameters tend
to diverge towards infinity. The overfitting issue of the GSCA model
with exact low rank constraint can be interpreted in the same way by
taking the columns of score matrix @xmath as the latent variables and
the loading matrix @xmath as the coefficients to fit the binary @xmath .
This result suggests that if an exact low rank constraint is preferred
in the GSCA model, an extra scale penalty should be added on @xmath to
avoid overfitting.

##### Comparing the generalization errors of the GSCA models with
nuclear norm and concave penalties

To evaluate the performance of the GSCA model in recovering the
underlying structure, we set up the realistic simulation (strongly
imbalanced binary data and low SNR) as follows. The simulated @xmath and
@xmath have the same size as the real data sets in the Section 4.5 ,
@xmath , @xmath , @xmath . The logit transform of the empirical marginal
probabilities of the CNA data set in the Section 4.5 is set as @xmath .
Elements in @xmath are sampled from the standard normal distribution.
The simulated low rank is set to @xmath ; @xmath is set to 1; @xmath and
@xmath are set to 1. After the simulation of @xmath , there are two
columns only have “0” elements, which are removed as they provide no
information (no variation).

As the GSCA model with the nuclear norm penalty is a convex problem, a
global optimum can be obtained. The nuclear norm penalty is therefore
used as the baseline in the comparison with other penalties. An interval
from @xmath , which is large enough to achieve an estimated rank of at
most rank 1, to @xmath , which is small enough to achieve an estimated
rank of 159, is selected based on low precision models ( @xmath ). 30
log-spaced @xmath s are selected equally from the interval @xmath . The
convergence criterion is set as @xmath . The results are shown in Fig.
4.2 . With decreasing @xmath , the estimated rank of @xmath increased
from 0 to 159, and the estimated @xmath decreased from 2 to close to 0.
The minimum @xmath of 0.184 (the corresponding @xmath , @xmath , @xmath
and @xmath ) can be achieved at @xmath , which corresponds to @xmath and
@xmath . There are sharp transitions in all the three subplots near the
point @xmath . The reason is that when the penalty is not large enough,
the estimated rank becomes 159, and the constructed GSCA model is almost
a saturated model. Thus the model has high generalization error and the
estimated @xmath also becomes close to 0. Given that we only have
indirect binary observation @xmath and highly noisy observation @xmath
of the underlying structure @xmath , the performance of the GSCA model
with nuclear norm penalty is reasonable. However, results can be greatly
improved by using concave penalties.

For concave penalties, different values of the hyper-parameters, @xmath
in @xmath , @xmath in SCAD and GDP, are selected according to their
thresholding properties. For each value of the hyper-parameter, values
of tuning parameter @xmath are selected in the same manner as described
above. The minimum @xmath achieved and the corresponding @xmath and
@xmath for different values of hyper-parameter of the GSCA models with
different penalty functions are shown in Fig. 4.3 . Here all GSCA models
with concave penalties can achieve much lower RMSEs in estimating @xmath
, @xmath and @xmath compared to the convex nuclear norm penalty ( @xmath
in the plot). Among the three concave penalties used, @xmath and GDP
have better performance.

If we get access to the full information, the underlying quantitative
data @xmath rather than the binary observation @xmath , the SCA model on
@xmath and @xmath is simply a PCA model on @xmath . From this model, we
can get an estimation of @xmath , @xmath and @xmath . We compared the
results derived from the SCA model on the full information, the GSCA
models with nuclear norm, @xmath , SCAD ( @xmath ) and GDP ( @xmath )
penalties. All the models are selected to achieve the minimum @xmath .
The RMSEs of estimating @xmath , @xmath , @xmath , @xmath and @xmath and
the rank of estimated @xmath from different models are shown in Table
4.1 . Here we can see that the GSCA models with @xmath and GDP ( @xmath
) penalties have better performance in almost all criteria compared to
the nuclear norm and SCAD penalties, and even comparable with the SCA
model on full information. The singular values of the true @xmath ,
estimated @xmath from the above models and the noise terms @xmath are
shown in Fig. 4.4 . Only the first 15 singular values are shown to have
higher resolution of the details. Since the @xmath singular value of the
simulated data @xmath is smaller than the noise level, the best
achievable rank estimation is 9. Both the @xmath and GDP ( @xmath )
penalties successfully find the correct rank 9, and they have a very
good approximation of the first 9 singular values of @xmath . On the
other hand, the nuclear norm penalty shrinks all the singular values too
much. Furthermore, the SCAD penalty overestimates the first three
singular values and therefore shrinks all the other singular values too
much. These results are easily understandable if taking their
thresholding properties in Fig. 3.1 into account. Both the @xmath and
the GDP penalties have very good performance in this simulation
experiment.

##### Comparing the GSCA model with GDP penalty and the iClusterPlus
model

We compared our GSCA model with GDP penalty to the iClusterPlus model on
the simulated data sets. The parameters for the GSCA model with GDP
penalty is the same as described above. The running time is 60.61s when
@xmath , and 9.98s when @xmath . For the iClusterPlus model, 9 latent
variables are specified. The tuning parameter of the lasso type
constraint on the data specific coefficient matrices are set to 0. The
default convergence criterion is used, that is the maximum of the
absolute changes of the estimated parameters in two subsequent
iterations is less than @xmath . The running time of the iClusterPlus
model is close to 3 hours. The constructed iClusterPlus model provides
the estimation of column offset @xmath , the common latent variables
@xmath , and data set specific coefficient matrices @xmath and @xmath .
The estimated @xmath and @xmath are computed in the same way as defined
in the model section. The RMSEs in estimating @xmath , @xmath and @xmath
for iClusterPlus are 2.571, 2.473 and 3.060 respectively. Compared to
the results from the GSCA models in Table 4.1 , iClusterPlus is unable
to provide good results on the simulated data sets. Supplemental
Fig. S4.2 compares the estimated @xmath from the GSCA model with GDP
penalty and iClusterPlus model. As shown in supplemental
Fig. S4.2(right), the iClusterPlus model is unable to estimate the
offset @xmath correctly. Many elements of estimated @xmath are exactly
0, which corresponds to an estimated marginal probability of 0.5. In
addition, as shown in Fig. 4.5 (left), the singular values of the
estimated @xmath from the iClusterPlus model are clearly overestimated.
These undesired results from the iClusterPlus model are due mainly to
the imbalancedness of the simulated binary data set. If the offset term
@xmath in the simulation is set to 0, which corresponds to balanced
binary data simulation, and fix all the other parameters in the same way
as in the above simulation, the results of iClusterPlus and the GSCA
with GDP penalty are more comparable. In that case the RMSEs of
estimating @xmath , @xmath in the GSCA model with GDP penalty are 0.071
and 0.091 respectively, while the RMSEs of the iClusterPlus model are
0.107 and 0.142 respectively. As shown in Fig. 4.5 (right), the singular
values of estimated @xmath from the iClusterPlus model are much more
accurate compared to the imbalanced case. However, iClusterPlus still
overestimates the singular values compared to the GSCA model with GDP
penalty. This phenomenon is related to the fact that exact low rank
constraint is also used in the iClusterPlus model. These results suggest
that compared to iClusterPlus, the GSCA model with GDP penalty is more
robust to the imbalanced binary data and has better performance in
recovering the underlying structure in the simulation experiment.

##### The performance of the GSCA model for the simulation with
different SNRs

We will explore the performance of the GSCA model for the simulated
binary and quantitative data sets with varying noise levels in the
following experiment. Equal SNR levels are used in the simulation for
@xmath and @xmath . 20 log spaced SNR values are equally selected from
the interval @xmath . Then we simulated coupled binary data @xmath and
quantitative @xmath using the different SNRs in the same way as
described above. During this process, except for the parameters @xmath
and @xmath , which are used to adjust the SNRs, all other parameters
used in the simulation were kept the same. The GSCA models with GDP
penalty ( @xmath ), @xmath penalty ( @xmath ), nuclear norm penalty, and
the SCA model on the full information (defined above) are used in these
simulation experiments. For these three models, the model selection
process was done in the same way as described in above experiment. The
models with the minimum @xmath are selected. As shown in Fig. 4.6 , the
GSCA models with concave GDP and @xmath penalties always have better
performance than the convex nuclear norm penalty, and they are
comparable to the situation where the full information is available.
With the increase of SNR, the @xmath derived from the GSCA model, which
is used to evaluate the performance of the model in recovering the
underlying low dimensional structure, first decreases to a minimum and
then increases. As shown in bottom center and right, this pattern is
mainly caused by how @xmath changes with respect to SNRs. Although this
result counteracts the intuition that larger SNR means higher quality of
data, it is in line with our previous results in Chapter 3 .

##### Assessing the model selection procedure

The cross validation procedure and the cross validation error have been
defined in the model selection section. The GSCA model with GDP penalty
is used as an example to assess the model selection procedure. @xmath is
used as the stopping criteria for all the following experiments to save
time. The values of @xmath and @xmath are selected in the same way as
was described in Section 4.4 . Fig. 4.7 shows the minimum @xmath and
minimum CV error achieved for different values of the hyper-parameter
@xmath . The minimum CV error changes in a similar way as the minimum
@xmath with respect to the values of @xmath . However, taking into
account the uncertainty of estimated CV errors, the difference of the
minimum CV errors for different @xmath is very small. Thus, we recommend
to fix @xmath to be 1, rather than using cross validation to select it.
Furthermore, setting @xmath as the default value for the GDP penalty has
a probabilistic interpretation, see in [ 27 ] .

Whenever the GSCA model is used for exploratory data analysis, there is
no need to select @xmath explicitly. It is sufficient to find a proper
value to achieve a two or three component GSCA model, in order to
visualize the estimated score and loading matrices. If the goal is
confirmatory data analysis, it is possible to select the tuning
parameter @xmath explicitly by the proposed cross validation procedure.
Fig. 4.8 shows how the tuning parameter @xmath affects the CV errors,
@xmath and the estimated ranks. The minimum CV error obtained is close
to the Bayes error, which is the scaled negative log likelihood in cases
where the true parameters @xmath and @xmath are known. Even though,
inconsistence exists between CV error plot (Fig. 4.8 , left) and the
@xmath plot (Fig. 4.8 , center), the selected model corresponding to
minimum CV error can achieve very low @xmath and correct rank estimation
(Fig. 4.8 , right). Therefore, we suggest to use the proposed CV
procedure to select the value of @xmath at which the minimum CV error is
obtained. Finally, we fit a model on full data set without missing
elements using the selected value of @xmath and the outputs of the
selected model with minimum CV error as the initialization.

### 4.5 Empirical illustration

#### 4.5.1 Real data set

The Genomic Determinants of Sensitivity in Cancer 1000 (GDSC1000) [ 30 ]
contains 926 tumor cell lines with comprehensive measurements of point
mutation, CNA, methylation and gene expression. We selected the binary
CNA and quantitative gene expression measurements on the same cell lines
(each cell line is a sample) as an example to demonstrate the GSCA
model. To simplify the interpretation of the derived model, only the
cell lines of three cancer types are included: BRCA (breast invasive
carcinoma, 48 cell lines), LUAD (lung adenocarcinoma, 62 cell lines) and
SKCM (skin cutaneous melanoma, 50 cell lines). The CNA data set has 410
binary variables. Each variable is a copy number region, in which “1”
indicates the presence and “0” indicates the absence of an aberration.
Note that, the CNA data is very imbalanced: only @xmath the elements are
“1”. The empirical marginal probabilities of binary CNA variables are
shown in supplemental Fig. S4.1. The quantitative gene expression data
set contains 17,420 variables, of which 1000 gene expression variables
with the largest variance are selected. After that, the gene expression
data is column centered and scaled by the standard deviation of the each
variable to make it more consistent with the assumption of the GSCA
model.

#### 4.5.2 Exploratory data analysis of the coupled CNA and gene
expression data sets

We applied the GSCA model (with GDP penalty and @xmath =1) to the GDSC
data set of 160 tumor cell lines that have been profiled for both binary
CNA ( @xmath ) and quantitative gene expression ( @xmath ). The results
of model selection (supplemental Fig. S4.3) validate the existence of a
low dimensional common structure between CNA and gene expression data
sets. For exploratory purposes, we will construct a three component
model instead.

We first considered the score plot resulting from this GSCA model. The
first two PCs show a clear clustering by cancer type (Fig. 4.9 , left),
and in some cases even subclusters (i.e. hormone-positive breast cancer,
MITF-high melanoma). These results suggest that the GSCA model captures
the relevant biology in these data. Interestingly, when we performed PCA
on the gene expression data, we obtained score plots that were virtually
identical to those resulting from the GSCA model (supplemental
Fig. S4.4, left; modified RV coefficient of the scores matrices derived
from these two models: 0.9998), suggesting that this biological
relevance is almost entirely derived from the gene expression data.

We then wondered whether the GSCA model could leverage the gene
expression data to help us gain insight into the CNA data. To test this,
we first established how much insight could be gained from the CNA data
in isolation. Supplemental Fig. S4.5 shows the scores and loadings of
the first two components from a three component logistic PCA model [ 43
] applied to the CNA data. While these do seem to contain structure in
the loading plot, we believe that they mostly explain technical
characteristics of the data. For example, deletions and amplifications
are almost perfectly separated from each other by the PC1=0 line in the
loading plot (supplemental Fig. S4.6). Additionally, the scores on PC1
are strongly associated to the number of copy number aberrations (i.e.,
to the number of ones) in a given sample (supplemental Fig. S4.7).
Finally, the clusters towards the left of the loading plot suggested two
groups of correlated features, but these could trivially be explained by
genomic position, that is, these features correspond with regions on the
same chromosome arm, which are often completely deleted or amplified
(supplemental Fig. S4.8). Following these observations, we believe that
a study of the CNA data in isolation provides little biological insight.

On the other hand, using the GCSA model’s CNA loadings (Fig. 4.9 ,
center), we could more easily relate the features to the biology. Let us
focus on the features with extreme values on PC1 and for which the
corresponding chromosomal region contains a known driver gene. For
example, the position of MYC amplifications in the loading plot
indicates that MYC amplifications occur mostly in lung adenocarcinoma
and breast cancer samples (Fig. 4.9 , center; supplemental Fig. S4.9).
Similarly, ERBB2 amplifications occur mainly in breast cancer samples
(Fig. 4.9 , center; supplemental Fig. S4.9). Finally, PTEN deletions
were enriched in melanomas, though the limited size of the loading also
indicates that they are not exclusive to melanomas (Fig. 4.9 , center;
supplemental Fig. S4.9). Importantly, these three findings are in line
with known biology [ 74 , 75 , 76 ] and hence exemplify how GSCA could
be used to interpret the CNA data. Altogether, using the GSCA model, we
were able to 1) capture the biological relevance in the gene expression
data, and 2) leverage that biological relevance from the gene expression
to gain a better understanding of the CNA data.

### 4.6 Discussion

In this chapter, we generalized the standard SCA model to explore the
dependence between coupled binary and quantitative data sets. However,
the GSCA model with exact low rank constraint overfits the data, as some
estimated parameters tend to diverge to positive infinity or negative
infinity. Therefore, concave penalties are introduced in the low rank
approximation framework to achieve low rank approximation and to
mitigate the overfitting issues of the GSCA model. An efficient
algorithm framework with analytical form updates for all the parameters
is developed to optimize the GSCA model with any concave penalties. All
concave penalties used in our experiments have better performance with
respect to generalization error and estimated low rank of the
constructed GSCA model compared to the nuclear norm penalty. Both @xmath
and GDP penalties with proper model selection can recover the simulated
low rank structures almost exactly only from indirect binary observation
@xmath and noisy quantitative observation @xmath . Furthermore, we have
shown that the GSCA model outperforms the iClusterPlus model with
respect to speed and accuracy of the estimation of the model parameters.

The superior performance of the GSCA models with the concave penalties
compared to the models with an exact low rank constraint or a nuclear
norm penalty is related to their different thresholding properties. The
exact low rank constraint thresholds the singular values in a hard
manner and, therefore, only the largest @xmath singular values are kept.
On the other hand, the nuclear norm penalty works in a soft manner, in
which all the singular values are shrunk by the same amount of @xmath .
The thresholding properties of the concave penalties discussed in this
chapter lie in between these two approaches. As @xmath and @xmath , the
scale of the loadings is related to the scale of the singular values of
@xmath . Thus, we can shrink the singular values of @xmath to control
the scale of estimated loading matrices in an indirect way. The exact
low rank constraint kept the @xmath largest singular values but without
control of the scale of the estimated singular values, leading to
overfitting. On the other hand, nuclear norm penalty shrinks all the
singular values by the same amount of @xmath , leading to biased
estimation of the singular values. A concave penalty, like @xmath or
GDP, achieves a balance in thresholding the singular values. Among the
concave penalties we used in the experiment, the SCAD penalty does not
work well in the simulation study. The reason is that the SCAD penalty
does not shrink the large singular values, which therefore tend to be
overfitted, while the smaller singular values are shrunk too much.

Compared to the iClusterPlus method, only the option of binary and
quantitative data sets are included in our GSCA model, and at the moment
no sparsity can be imposed for the integrative analysis of binary and
quantitative data sets. However, the GSCA model with GDP penalty is
optimized by a more efficient algorithm, it is much more robust to the
imbalanced nature of the biological binary data and it provides a much
better performance for the simulation experiments in this chapter.
Furthermore, the exploratory analysis of the GDSC coupled CNA and gene
expression data sets provided important information on the binary CNA
data that was not obtained by a separate analysis.

### Acknowledgements

Y.S. gratefully acknowledges the financial support from China
Scholarship Council (NO.201504910809).

### 4.7 Supplementary information

#### 4.7.1 GSCA model with exact low rank constraint

The exact low rank constraint on @xmath can be expressed as the
multiplication of two low rank matrices @xmath and @xmath . The
optimization problem related to the GSCA model with exact low rank
constraint can be expressed as

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

The developed algorithm in the paper can be slightly modified to fit
this model. Similar to the paper, the above optimization problem can
majorized to the following problem.

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

The analytical solution of @xmath is also the column mean of @xmath .
After deflating out the offset term @xmath , the majorized problem
becomes @xmath subject to @xmath and @xmath . The global optimal
solution is the @xmath truncated SVD of @xmath . Other steps in the
algorithm to fit the GSCA model with exact low rank constraint are
exactly the same as in the paper to fit the GSCA model with concave
penalties.

#### 4.7.2 Supplemental figures

## Chapter 5 Fusing multiple data sets with two types of heterogeneity

Multiple sets of measurements on the same objects obtained from
different platforms may reflect partially complementary information of
the studied system. The integrative analysis of such data sets not only
provides us with the opportunity of a deeper understanding of the
studied system, but also introduces some new statistical challenges.
First, the separation of information that is common across all or some
of the data sets, and the information that is specific to each data set
is problematic. Furthermore, these data sets are often a mix of
quantitative and qualitative (binary or categorical) data types, while
commonly used data fusion methods require all data sets to be
quantitative. These two types of heterogeneity existed in multiple data
sets should be taken into account in the data fusion. In this chapter,
we propose an exponential family simultaneous component analysis (ESCA)
model to tackle the potential mixed data types problem of multiple data
sets. In addition, a structured sparse pattern of the loading matrix is
induced through a nearly unbiased group concave penalty to disentangle
the global, local common and distinct information of the multiple data
sets. A Majorization-Minimization based algorithm is derived to fit the
proposed model. Analytic solutions are derived for updating all the
parameters of the model in each iteration, and the algorithm will
decrease the objective function in each iteration monotonically. For
model selection, a missing value based cross validation procedure is
implemented. The advantages of the proposed method in comparison with
other approaches are assessed using comprehensive simulations as well as
the analysis of real data from a chronic lymphocytic leukaemia (CLL)
study. ¹ ¹ 1 This chapter is based on Song, Y., Westerhuis, J.A. and
Smilde, A.K., 2019. Separating common (global and local) and distinct
variation in multiple mixed types data sets. arXiv preprint
arXiv:1902.06241

### 5.1 Background

Multiple data sets measured on the same samples are becoming
increasingly common in different research areas, from biology, food
science to psychology. One typical example from biological research is
the GDSC1000 study, in which 926 cell lines are fully characterized with
respect to point mutation, copy number alternation (CNA), methylation,
gene expression and drug responses [ 30 ] . However, these comprehensive
measurements from the same cell lines not only provide the opportunity
for a deeper understanding of the studied biological system, but also
introduce statistical challenges.

The first challenge is how to separate the information that is common
across all or some of the data sets, and the information which is
specific to each data set (often called distinct). These different
sources of information have to be disentangled from every data set to
have a holistic understanding of the studied system. The second
challenge is that measurements from different platforms can be of
different data type, such as binary, quantitative or counts. These
different data types have different mathematical properties, which
should be taken into account in the data analysis. For example, the
measurement of a binary variable only has two possible exclusive
outcomes, often classified as “1” and “0”. Examples of binary data in
biology include point mutation, and the binarized CNA and methylation
data sets [ 30 ] . Taking binary measurements “1”, “0” as the
quantitative values 1, 0, and casting them into the classical data
fusion methods that assume data sets to be quantitative, clearly
neglects their binary nature.

In this chapter, we focus on the component or latent variable based data
fusion approaches, although other approaches exist such as undirected
graphical model based methods which are able to explore the association
between data sets of different data types [ 77 ] , or between variables
of different data types [ 78 , 79 ] . Two commonly used latent variable
based data fusion methods are simultaneous component analysis (SCA) [ 65
] and iCluster [ 80 ] , which both focus on using low dimensional
structures to approximate the common variation across all the data sets.
Both of these approaches have already been generalized to qualitative
data sets [ 67 , 64 ] . In addition, the concept of common and distinct
variation in data fusion has been framed in [ 81 , 14 ] , and several
methods [ 82 , 83 , 84 , 85 , 14 ] have been proposed. One typical
example is JIVE [ 82 ] . The JIVE model directly specifies the
components for the global common variation (variation across all the
data sets) and the distinct variation (variation specific to each data
set) in the model, and estimates them simultaneously. However, JIVE
model is incapable to tackle the qualitative data sets and the local
common variation (variation across some of the data sets) is ignored.
Direct generalization of JIVE to account for the local common variation
is infeasible as with the increase of the number of data sets, the
possible combinations of local common variation blows up exponentially.
Other methods [ 84 , 83 ] encounter similar problems with respect to the
estimation of local common variation. In addition, the model selection
procedure in these methods is still an unsolved issue [ 86 ] . A
promising solution was provided in [ 87 , 15 ] , in which a group
regularization procedure was applied to provide structured sparsity on
the loading matrix where the loadings of all variables of a given data
set are forced to 0 to disentangle the common (global and local) and
distinct variation indirectly. Details will be shown in the following
model section. In the SLIDE model [ 15 ] , first a series of structured
sparsity patterns on the loading matrix of a SCA model are learned using
a group lasso penalty. Then, these learned structured sparsity patterns
are imposed as hard constraints on the loading matrix of a SCA model,
and the appropriate model is selected by Bi-cross-validation [ 88 ] .
The Bayesian counterpart of the SLIDE model is the group factor analysis
model [ 87 ] , and the generalization of the group factor analysis to
mixed data types is the MOFA model [ 89 ] . These two Bayesian models
use an automatic relevance determination procedure to induce the
structured sparsity.

The first contribution in this chapter is the generalization of the SCA
model to the exponential family SCA (ESCA) model by exploiting the
exponential family distribution to account for potentially different
data types, such as binary, quantitative or count data. The
generalization is done in a similar way as the extension of principal
component analysis (PCA) to exponential family PCA [ 68 ] . The second
contribution is the use of a nearly unbiased group concave penalty to
induce a structured sparse pattern on the loading matrix of the ESCA
model to disentangle the common (global and local) and distinct
variation of multiple data sets of mixed data types. In the SLIDE model
[ 15 ] , the structured sparse pattern is induced by the group lasso
penalty, which shrinks in the group level (the groups) as a lasso
penalty, and in the individual level (the individual elements inside a
group) as a ridge regression penalty. However, a lasso type penalty
leads to biased parameter estimation, as the same degree of shrinkage is
applied to all the parameters. This will shrink the nonzero parameters
too much and as a result makes the prediction or cross validation error
based model selection procedures inconsistent [ 90 , 91 ] . On the other
hand, concave penalties, such as generalized double Pareto (GDP)
shrinkage [ 27 ] or bridge ( @xmath ) penalty [ 57 ] , are capable to
achieve nearly unbiased estimation of the parameters while producing
sparse solutions. Therefore, we replaced the group lasso penalty by a
group concave penalty on the loadings. The group concave penalty shrinks
the group level as a concave penalty, and it shrinks on the individual
level as ridge regression penalty. The third contribution lies in the
derived model fitting algorithm and the model selection procedure. A
Majorization-Minimization based algorithm is derived to fit the proposed
penalized ESCA (P-ESCA) model. Analytical form solutions for updating
all the parameters of the model in each iteration are derived, and the
algorithm will decrease the objective function in each iteration
monotonically. Furthermore, the missing value problem is tackled in the
developed algorithm, and this option is used in the cross validation
procedure for the model selection. The proposed model is similar to the
MOFA model, but differences exist in the way how the model is derived,
how the structured sparsity is achieved, and how the model is selected.
These differences are detailed out in the supplementary material.

Both the performance of the proposed P-ESCA model, and the effectiveness
of the model selection procedure are validated by extensive simulations
under different situations. The performance of the P-ESCA method is
compared with SLIDE and MOFAs. Finally, P-ESCA is exemplified by the
explorative analysis of the chronic lymphocytic leukaemia (CLL) data
sets [ 92 , 89 ] .

### 5.2 P-ESCA model

In this section, we will introduce the generalization of the ESCA model.
Then we will show how to use the group concave penalty to induce the
structured sparse pattern on the loading matrix of an ESCA model to
disentangle the common (global and local) variation and distinct
variation of multiple data sets.

#### 5.2.1 Exponential family SCA

The quantitative measurements from @xmath different platforms on the
same @xmath objects result into @xmath quantitative data sets, @xmath ,
and the @xmath data set @xmath ( @xmath ) has @xmath variables. In the
classical SCA model, we decompose the @xmath data sets as @xmath , in
which @xmath ( @xmath ) is a column vector with ones; @xmath ( @xmath )
is the column offset term; @xmath ( @xmath ) is the common score matrix;
@xmath ( @xmath ) and @xmath ( @xmath ) are the loading matrix and
residual term respectively for @xmath and @xmath is the number of
components. In addition, constraints @xmath and @xmath , in which @xmath
is the identity matrix, are imposed to make the model identifiable. The
SCA model tries to discover the common column subspace, which is spanned
by the columns of the score matrix @xmath , in @xmath data sets to
represent their common information. The column offset terms @xmath can
be removed by column centering of the corresponding data sets @xmath .
The parameters in the SCA model can be estimated by minimizing the sum
of squares @xmath , in which @xmath is the relative weight of the @xmath
data set @xmath .

The least squares loss criterion in the classical SCA model is only
appropriate for quantitative data sets. When some or all data sets are
of another data type, such as binary data, classical SCA model is not
appropriate anymore. Motivated by the previous research on exponential
family PCA model [ 34 ] , we use the exponential family distribution to
account for the different data types of multiple data sets, such as
Bernoulli distribution for binary data, Poisson distribution for count
data and Gaussian distribution for quantitative data.

Assume @xmath follows the exponential dispersion family distribution [
13 ] , and @xmath and @xmath are the natural parameter and the
dispersion parameter respectively. The probability density or mass
function can be specified as @xmath , in which @xmath is the
log-partition function, and @xmath is a function which does not depend
on the natural parameter @xmath . Supplemental Table S5.1 lists the
log-partition function @xmath and its first and second order derivatives
@xmath , @xmath for Gaussian, Bernoulli and Poisson distributions. The
relationship @xmath always hold in the exponential family distribution.
Supplemental Fig. S5.1 visualizes this relationship for the Gaussian,
Bernoulli and Poisson distributions. If the @xmath data set @xmath is
quantitative, according to the probabilistic interpretation of the PCA
model [ 42 ] , we assume there is a natural parameter matrix @xmath (
@xmath ) underlying @xmath , and the low dimensional structure exists in
@xmath , @xmath and @xmath , and elements in the error term @xmath
follows a normal distribution @xmath . The conditional mean of the
observed @xmath given the low dimensional structure assumption is @xmath
, in which @xmath is the first order derivative of the log-partition
function for the Gaussian distribution (supplemental Table S5.1). In
exponential family PCA, the same idea has been generalized to other
members of exponential family distributions by assuming @xmath and
@xmath , in which the function form of @xmath depends on the used
probability distribution (supplemental Table S5.1).

In the exponential family PCA model, the elements in @xmath are
conditionally independent given the low dimensional structure assumption
as @xmath . Take @xmath and @xmath as the @xmath element of @xmath and
@xmath respectively. The conditional log-likelihood of observing @xmath
is @xmath , in which @xmath indicates the inner product, for matrices,
@xmath ; @xmath , a constant that does not depend on the unknown
parameter @xmath ; @xmath and @xmath are the element-wise log-partition
function and the known dispersion parameter respectively for the @xmath
data set @xmath . In the ESCA model, we assume that the natural
parameter matrices @xmath lie in the same column subspace, which is
spanned by the common score matrix @xmath . To make the model
identifiable, constraints @xmath and @xmath are imposed. The
optimization problem associated with this ESCA model can be expressed as
follows,

  -- -------- -------- -------- -------
     @xmath   @xmath            (5.1)
                       @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- -------

#### 5.2.2 Separating common and distinct variation via structured
sparsity

The drawback of the SCA or ESCA models is that only the global common
components, which account for the common variation across all the data
sets, is allowed. However, the real situation in multiple data sets
integration can be far more complex as local common variation across
some of the data sets and distinct variation in each data set are
expected as well. Directly specifying the components in the ESCA model
for common (global and local) and distinct variation in the same way as
JIVE model [ 82 ] is infeasible, as the number of possible combinations
of local common variation will blow up exponentially with an increasing
number of data sets. A promising solution is using structured sparsity
on the loading matrix to disentangle the common (global and local) and
distinct variation indirectly [ 87 , 15 ] . Structured sparsity of the
data set specific loading matrices in component based data fusion
methods has been explored by [ 93 , 94 ] . The idea of using structured
sparsity to disentangle the common (global and local) and distinct
variation in multiple quantitative data sets is made explicit in [ 87 ,
15 ] . To illustrate the idea, we use an example with three quantitative
data sets. Suppose we construct a SCA model on three column centered
quantitative data sets @xmath , the common score matrix is @xmath , the
corresponding loading matrices are @xmath , and @xmath , in which @xmath
is the residual term for @xmath data set. If the structured sparsity
pattern in @xmath is expressed as follows,

  -- -------- --
     @xmath   
  -- -------- --

in which @xmath indicates the @xmath column of the @xmath loading matrix
@xmath , then we have the following relationships,

  -- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- --
     @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   
  -- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- -------- --

Here @xmath indicates the @xmath column of the common score matrix
@xmath . The first component represents the global common variation
across three data sets; the @xmath , @xmath and @xmath components
represent the local common variation across two data sets and the @xmath
, @xmath and @xmath components represent the distinct variation specific
to each single data set. In this way, the structured sparsity pattern in
the loading matrices @xmath can be used to separate the common (global
and local) and distinct variation of multiple quantitative data sets.

#### 5.2.3 Group concave penalty

In [ 93 , 94 , 15 ] , the structured sparsity is induced by a group
lasso penalty on the columns of @xmath . The used group lasso penalty is
@xmath , in which @xmath is the tuning parameter, @xmath indicates the
@xmath column of the @xmath loading matrix @xmath , and @xmath indicates
the @xmath norm of a vector. This group lasso penalty shrinks @xmath as
a lasso penalty and the elements inside @xmath as a ridge penalty.
However, lasso type penalty leads to biased parameter estimation as the
same degree of shrinkage is applied to all the parameters, which will
shrink the nonzero parameters too much and makes the prediction or cross
validation error based model selection procedures inconsistent [ 90 , 91
] . This leads in general to the selection of too complex models. The
SLIDE model [ 15 ] solves the model selection problem in a two stages
manner. First, varying degrees of regularization are imposed to induce a
series of structured sparse loading patterns. Then these structured
sparse patterns are taken as hard constraints on a new SCA model, in
which a Bi-cross validation procedure [ 88 ] is used for the final
selection. This two stages approach is similar to the often used
re-estimation trick in lasso regression. However, such a two-step
strategy cannot easily be generalized to the ESCA model. For example, if
a binary data set is used and the structured sparse pattern is imposed
as a hard constraint on the loading matrices in a ESCA model, the
estimated loadings of the binary data set can easily go to infinity [ 51
, 64 ] .

The above issue introduced by the biased estimation of lasso type
penalties can be alleviated by using concave penalties [ 57 , 27 ] ,
which can achieve sparse solutions and nearly unbiased parameter
estimation simultaneously. Therefore, in this chapter, we applied group
concave penalties, generalized double Pareto (GDP) shrinkage [ 27 ] and
bridge ( @xmath ) penalty [ 57 ] are included as special cases, on the
loading matrices of the ESCA model to induce structured sparse pattern.
Take @xmath , in which @xmath is the @xmath column of @xmath , and
@xmath is a general concave penalty function in Table 5.1 . The penalty
on @xmath can be expressed as @xmath , in which @xmath is the tuning
parameter. The group lasso penalty is a special case of the group @xmath
(bridge) penalty by setting @xmath . The thresholding properties of the
group @xmath penalty, group GDP penalty and group lasso can be found in
supplemental Fig. S5.2. In order to account for the situation that the
data sets have an unequal number of variables, we add the weights in the
same way as in the standard group lasso regression problem, i.e. @xmath
. The group concave penalty on @xmath can be expressed as @xmath . Based
on successful results in previous work [ 64 ] we will focus on the GDP
penalty, which is differentiable everywhere in its domain and its
performance is insensitive to the selection of the hyper-parameter
@xmath . Fig. 5.1 gives an example to show how the group GDP ( @xmath )
penalty induces structured sparsity pattern on the loading matrices
@xmath .

#### 5.2.4 Identifiability

The constraint @xmath makes the column offset terms @xmath identifiable.
The columns of the score matrix @xmath span the joint subspace @xmath ,
in which @xmath indicates the column subspace. The structured sparse
pattern on the loading matrices and the multiplication of the score and
loading matrices provide a way to separate the joint subspace @xmath
into subspaces , , corresponding to the global common, local common and
distinct variation, @xmath . If the orthogonality constraint @xmath is
imposed, the separated subspaces , , , corresponding to the global
common, local common and distinct variation, are orthogonal to each
other, and unique as @xmath . However, there is still a rotation freedom
for the components within the subspace corresponding to the global
common or local common or distinct variation.

#### 5.2.5 Regularized likelihood criterion

The regularized likelihood criterion of fitting the proposed P-ESCA
model can be derived as follows. To tackle the missing value problem,
@xmath weight matrices are introduced. For the @xmath data set @xmath ,
we introduce a same size weight matrix @xmath , in which @xmath if the
corresponding element in @xmath is missing, while @xmath vise versa .
This option is the basis for different missing value based cross
validation approaches. The corresponding optimization problem can be
expressed as follows,

  -- -------- -------- -------- -------
     @xmath   @xmath            (5.2)
                       @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- -------

in which @xmath indicates the element-wise matrix multiplication.

### 5.3 Algorithm

The original problem in equation 5.2 is difficult to optimize directly
because of the non-convex orthogonality constraint @xmath and the group
concave penalty @xmath . However, by using the Majorization-Minimization
(MM) principle, the original difficult problem can be majorized to a
simpler problem, for which analytical form solutions can be derived for
all the parameters. According to the MM principle, the derived algorithm
will monotonously decrease the loss function in each iteration. Further
details of the MM principle can be found in [ 58 , 59 ] .

#### 5.3.1 The majorization of the regularized likelihood criterion

Take @xmath as the loss function for fitting the @xmath data set @xmath
, and @xmath as the group concave penalty for the @xmath loading matrix
@xmath . We can majorize @xmath and @xmath respectively as follows.

##### The majorization of @xmath

Given @xmath , we have @xmath . The first and second gradients of @xmath
with respect to @xmath are @xmath and @xmath . Assume that @xmath is
upper bounded by a constant @xmath , which will be detailed below. If
@xmath represents the general representation of @xmath , then according
to the Taylor’s theorem and the assumption that @xmath for all @xmath ,
we have the following inequality,

  -- -------- -- -------
     @xmath      (5.3)
  -- -------- -- -------

Here @xmath is an approximation of @xmath at the @xmath iteration and
@xmath is an unknown constant. Combining the above inequality and the
majorization step [ 60 ] of transforming a weighted least square problem
to a least squares problem, we have the following inequality,

  -- -------- -------- -------- -------
     @xmath   @xmath            (5.4)
                       @xmath   
                       @xmath   
              @xmath   @xmath   
                       @xmath   
  -- -------- -------- -------- -------

in which @xmath is the approximation of @xmath during the @xmath
iteration. For the Bernoulli distribution, an elegant bound @xmath can
be used [ 43 ] ; for the Gaussian likelihood, @xmath ; for the Poisson
distribution, @xmath is unbounded, however, we can always set @xmath .

##### The majorization of @xmath

Assume @xmath is the @xmath approximation of @xmath , and @xmath .
According to the definition of a concave function [ 61 ] , we always
have the inequality @xmath , in which @xmath and @xmath is the set of
supergradients (the counterpart concept of the subgradient for a concave
function) of the function @xmath at @xmath . When the supergradient is
unique, then @xmath . Therefore, @xmath can be majorized as follows,

  -- -------- -------- -------- -------
     @xmath   @xmath            (5.5)
                       @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- -------

##### The majorization of the regularized likelihood criterion

Combining the above two majorization steps, we have majorized the
original complex problem in the equation 5.2 to a simper problem in each
iteration as follows,

  -- -------- -------- -------- -------
     @xmath   @xmath            (5.6)
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- -------

#### 5.3.2 Block coordinate descent

The majorized optimization problem in equation 5.6 can be solved by the
block coordinate descent approach, and the analytic solution can be
derived for all the parameters.

##### Updating @xmath

When fixing all other parameters except @xmath , the analytic solution
of @xmath in equation 5.6 is simply the column mean of @xmath , @xmath .

##### Updating @xmath

When fixing all other parameters except @xmath , and deflating the
offset term @xmath , the loss function in equation 5.6 becomes @xmath ,
in which @xmath is the column centering matrix. If we take @xmath , the
above equation can also be written in this way @xmath . To simplify the
equations, we set @xmath and @xmath . Then, we take @xmath as the row
concatenation of @xmath , @xmath , and take @xmath as the column
concatenation of @xmath , @xmath . After that, we have @xmath . Updating
@xmath equivalents to minimizing @xmath . Assume the SVD decomposition
of @xmath is @xmath , the analytic solution for @xmath is @xmath . The
derivation of the above solution is shown in the following paragraph.

To simplify the derivation, we take @xmath and @xmath . So the
optimization problem is @xmath . This equation can be expanded as @xmath
. Since @xmath , the above optimization problem equivalents to
maximizing a trace function problem, @xmath . Assume the SVD
decomposition of @xmath is @xmath , we have @xmath . According to the
Kristof theorem [ 41 ] , we have @xmath , in which @xmath is the @xmath
diagonal element of @xmath , and this upper-bound can be achieved by
setting @xmath .

##### Updating @xmath

Because @xmath , it is easy to prove that @xmath . Also, because of that
the least squares problems are decomposable, we have @xmath , in which
@xmath is the @xmath column of @xmath . In this way, we have the
following optimization problem,

  -- -------- -------- -------- -------
     @xmath   @xmath            (5.7)
                       @xmath   
                       @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- -------

The above optimization problem is equivalent to finding the proximal
operator of a @xmath (or Euclidean) norm, and the analytic solution
exists [ 25 ] . Take @xmath , the analytical solution of @xmath is
@xmath . To update the parameter @xmath , we can simply apply this
proximal operator to all the columns of @xmath .

##### Initialization and stopping criteria

The initialization of the parameters @xmath , @xmath , @xmath can be set
to the results of a classical SCA model on @xmath or to accept user
imputed initializations. The relative change of the objective function
is used as the stopping criteria. Pseudocode of the algorithm described
above is shown in Algorithm 3 , in which @xmath is the value of the
objective function in @xmath iteration, @xmath is the tolerance of
relative change of the objective function.

1: @xmath , @xmath , @xmath , @xmath , @xmath ;

2: @xmath , @xmath , @xmath ;

3: Compute @xmath for missing values in @xmath ;

4: Initialize @xmath , @xmath , @xmath ;

5: @xmath ;

6: @xmath ;

7: while @xmath do

8: for @xmath do

9: Estimate @xmath according to the data type of @xmath ;

10: @xmath ;

11: @xmath ;

12: @xmath ;

13: @xmath ;

14: end for

15: @xmath ;

16: @xmath ;

17: @xmath ;

18: @xmath ;

19: @xmath ;

20: for @xmath do

21: for @xmath do

22: @xmath ;

23: @xmath ;

24: @xmath ;

25: @xmath ;

26: end for

27: @xmath ;

28: end for

29: @xmath

30: @xmath ;

31: end while

32: Compute variation explained ratios.

Algorithm 3 An MM algorithm for fitting the P-ESCA model.

#### 5.3.3 Variation explained ratio of the P-ESCA model

For the quantitative data set @xmath , the parameters are @xmath ,
@xmath and @xmath . The total variation explained ratio of the model for
@xmath is defined as @xmath . And the variation explained ratio for the
@xmath component on @xmath is defined as @xmath . For the binary data
set, we use a similar strategy as the MOFA model [ 89 ] , where the
@xmath is taken as the pseudo @xmath during the @xmath iteration, and
@xmath rather than @xmath is used to compute the variation explained
ratios. The multiple data sets can also be taken as a single full data
set. In that case the @xmath values are taken as the weights for them,
and then we can compute the variation explained ratios of each component
for this full data set. The full single data set @xmath and the weight
matrix @xmath are the column concatenation of @xmath and @xmath , in
which @xmath is replaced by @xmath if the @xmath data set is not
quantitative. The offset term @xmath and the loading matrix @xmath are
the row concatenation of @xmath and @xmath and the score matrix @xmath .

### 5.4 Simulation process

To evaluate the proposed model and the model selection procedure, three
data sets of different data types with underlying global, local common
and distinct structures are simulated. The following simulations and
experiments focus on the quantitative and binary data types. We will
first show the simulation of the structure @xmath , in which @xmath is
the row concatenation of @xmath , @xmath . The structure @xmath can be
expressed in the SVD type as @xmath ( @xmath , @xmath ), in which @xmath
, @xmath is a diagonal matrix, and the structured sparse pattern exists
in the matrix @xmath . First, all the elements in @xmath and @xmath are
simulated from the standard normal distribution. To make sure that
@xmath , simulated @xmath is first column centered, and then it is
orthogonalized by the SVD algorithm to have @xmath . Also, @xmath is
orthogonalized by the QR algorithm to obtain @xmath . In this example 21
components are predefined, 7 groups of global, local common and
distinctive nature, 3 components each. The structure of these components
are set in @xmath as indicated below,

  -- -------- --
     @xmath   
  -- -------- --

in which @xmath indicates the loadings for the first three components
for data set 1, etc. After that, 21 values are sampled from @xmath , and
their absolute values are taken as the diagonal elements of @xmath .
Furthermore, an extra diagonal matrix @xmath , which has the same size
as matrix @xmath , is used to adjust the signal to noise ratios (SNRs)
in simulating different global, local common and distinct structures.
Then we have @xmath . In order to define the SNR, we have to specify the
noise term @xmath for the @xmath data set @xmath . If @xmath is
quantitative, all the elements in @xmath can be sampled from @xmath . If
@xmath is binary, according to the latent variable interpretation of
logistic PCA [ 95 ] , we assume there is a continuous latent matrix
@xmath underlying the binary observation @xmath , and the elements of
the noise term @xmath follow the standard logistic distribution. After
the specification of the noise terms, we can adjust the diagonal
elements in @xmath to satisfy the predefined SNRs in simulating the
global, local common and distinct structures. We restrict the diagonal
elements of @xmath for the same structure to share a single value to
have a unique solution. For example, for the global structure @xmath ,
the corresponding noise term is @xmath , and the SNR of the global
structure as defined as @xmath . The SNRs for the simulation of the
local common (C12, C13, C23) and distinct (D1, D2, D3) structures are
defined in the same way.

If @xmath is quantitative, we simply sample all the elements in @xmath
from the standard normal distribution. If @xmath is binary, the column
offset @xmath represents the logit transformation of the marginal
probabilities of binary variables. In our simulation, we will first
sample @xmath marginal probabilities from a Beta distribution. The Beta
distribution can be specified in the following way. For example, if we
have 100 samples of a binary variable and we assume the marginal
probability to be 0.1, this means we only observe @xmath “1”s. If we
model them as Binomial observations with parameter @xmath , and use a
uniform prior distribution for @xmath , then the posterior distribution
of @xmath is @xmath [ 96 ] . After generating @xmath marginal
probabilities from this Beta distribution, the logit transformation of
this vector of probabilities are set as @xmath . If @xmath is
quantitative, @xmath is simulated as @xmath , and all the elements of
@xmath are sampled from @xmath . If @xmath is binary, we have @xmath ,
and all the elements of @xmath are sampled from the Bernoulli
distributions, whose probabilities are the corresponding elements in the
inverse logit transformation of @xmath . An equivalent way to simulate
the binary @xmath is to first generate @xmath , in which all the
elements in @xmath are sampled from the standard logistic distribution.
Then, all the elements in @xmath are the binary observations of the
corresponding elements of @xmath , @xmath if @xmath , and @xmath vise
versa . In the following sections, we will use
Gaussian-Gaussian-Gaussian (G-G-G) to represent the simulation of three
quantitative data sets; Bernoulli-Bernoulli-Bernoulli (B-B-B) for the
simulation of three binary data sets; G-B-B for a quantitative data set
and two binary data sets and G-G-B for two quantitative data sets and a
binary data set.

### 5.5 Evaluation matrices and model selection

To evaluate the accuracy of the model in estimating the simulated
parameters, such as @xmath and @xmath , the relative mean squared error
(RMSE) is used. If, for example, the simulated parameter is @xmath ,
@xmath , and its estimation is @xmath , the RMSE is defined as @xmath .
All of the following evaluation matrices @xmath , @xmath and @xmath will
be used in the experimental section. To evaluate the recovered subspaces
with respect to the simulated global common, local common and distinct
structures, the modified RV coefficient [ 97 ] is used. If the simulated
global structure is @xmath , and its estimation is @xmath , the
similarity between the subspaces of @xmath and @xmath is calculated by
the modified RV coefficient.

For the real data sets, we can use the cross validation (CV) error as
the proxy of the prediction error to estimate the performance of the
model. The K-fold CV procedure used in Chapter 4 can be quite slow for
P-ESCA model. Therefore we set up the CV procedure as follows in a
similar way as Chapter 3 . From each data set @xmath , we will randomly
select 10% non-missing elements as @xmath , and these selected elements
in @xmath are set to missing values. The remaining elements form the
training set @xmath . For binary data, the selection of the test set
samples is performed in a stratified manner to tackle the situation of
unbalanced binary data. Here the test set consist of 10% “1”s and “0”s
which are randomly selected from @xmath as @xmath . A P-ESCA model is
constructed on the training sets @xmath , to obtain an estimation of
@xmath , in which @xmath . Then the parameters @xmath corresponding to
@xmath are indexed. The CV error for @xmath is obtained as the negative
log likelihood of using @xmath to predict @xmath .

If the data sets @xmath are of the same data type, a single tuning
parameter @xmath is used to replace the @xmath during the model
selection. First, @xmath are split into @xmath and @xmath in the same
way as described above. Then @xmath @xmath values are selected (with
equal distance in log-space) and for each @xmath value a P-ESCA model is
constructed on the training sets @xmath . A warm start strategy is used,
in which the outputs of a previous model are used to initialize the next
model with a slightly higher regularization strength. The warm start
strategy has a special meaning in the current context. If some component
loadings are shrunk to 0 in the previous model, they will also be 0 in
the next models with higher @xmath values. Thus, the search space of the
next model will be constrained based on the learned structured sparse
pattern in the previous model. In this way, with increasing @xmath ,
components are removed adaptively. We prefer to select the model with
the minimum CV error on @xmath and the corresponding value of @xmath is
@xmath . After that we re-fit a P-ESCA model with @xmath on the full
data sets @xmath and the outputs derived from the selected model with
minimum CV error are used for initialization in order to preserve the
learned structured sparse pattern.

If the data sets are of mixed data types, we prefer to use distinct
tuning parameters for each data type. Suppose we have three data sets
@xmath , of which @xmath is quantitative and @xmath are binary. We
specify two tuning parameters @xmath and @xmath for the loading matrices
corresponding to the quantitative and binary data sets. A heuristic
model selection approach, which has the same computational complexity as
tuning a single parameter, can be used for the model selection. The
splitting of @xmath into the training and test sets is the same as
discussed above. Then again, @xmath values of @xmath and @xmath are
selected with equal distance in log-space. For the first model, we fix
@xmath to be 0 or a very small value, and tune @xmath in the same way as
above. The model with the minimum CV error on the binary test sets
@xmath is selected, and the corresponding value of @xmath is @xmath .
After that, @xmath is fixed to @xmath , and the outputs of the above
selected model are set as the initialization for the models in the model
selection of @xmath , which is done in the same way as described above.
The model with the minimum CV error on the quantitative test set @xmath
is selected, and the corresponding value of @xmath is @xmath . After the
model selection, we re-fit the P-ESCA model on the full data sets @xmath
with the @xmath and @xmath and again the outputs of the selected model
in the model selection process are used for initialization.

### 5.6 Experiments

#### 5.6.1 Evaluating the dispersion parameter estimation procedure

The dispersion parameters of the Bernoulli and Poisson distributions can
always set to @xmath , while for the Binomial distribution with @xmath
experiments, it can always be set to @xmath . However, for a Gaussian
distribution, the dispersion parameter @xmath represents the variance of
the noise term, and is assumed to be known. Suppose we have a data set
@xmath , we prefer to use a PCA model to estimate the @xmath before
constructing a P-ESCA model. The rank of the PCA model is selected by a
missing value based cross validation procedure similar as described
above. Details of the @xmath estimation procedure are shown in the
supplementary material. After obtaining an estimation of @xmath , it can
be casted into the model or the data set can be scaled by @xmath , which
is the estimated standard deviation. We simulated G-G-G, G-G-B and G-B-B
data sets to test the @xmath estimation procedure. The parameters in the
simulation are set as @xmath , @xmath , @xmath , @xmath ; the SNRs of
the global, local common and distinct structures are all set to 1; the
marginal probability is set to @xmath to simulate unbalanced binary data
sets. The @xmath estimation procedure was repeated 3 times and the
average is taken as the estimation. As shown in supplemental Table S5.2,
the mean estimated dispersion parameters in different situations are
quite accurate, and the estimations derived from the 3 times repetitions
are very stable.

#### 5.6.2 An example of CV error based model selection

We use the simulated G-G-G data sets as an example to show how the model
selection is performed when multiple data sets are of the same data
type. The following parameters are used in the simulation, @xmath ,
@xmath , @xmath , @xmath ; the SNRs of global, local common and distinct
structures are all set to 1; all the dispersion parameters @xmath are
set to be 1. The signals, which are taken as the singular values of the
simulated structures, and the noise terms, which are taken as the
singular values of the corresponding residual terms, are characterized
in supplemental Fig. S5.3. The true variation explained ratios of each
component in every data set is computed using the simulated parameters,
and is visualized in supplemental Fig. S5.4. For the model selection
procedure, the maximum number of iterations is set to 500; the stopping
criteria is set to @xmath ; 30 @xmath values are selected from the
interval @xmath equidistant in log-space; 50 components are used in the
initialization. The values of @xmath in the P-ESCA model are set to the
estimated values from the above @xmath estimation procedure.

Fig. 5.2 shows how the CV errors, RMSEs and the RV coefficients change
with respect to @xmath when a P-ESCA model with a group GDP ( @xmath )
penalty is used. The top figures in Fig. 5.2 show that the CV errors
change in a similar way as the RMSEs. The model with minimum CV error
has low RMSEs in estimating the simulated parameters (Fig. 5.2 top
right) and correctly identifies the dimensions of the subspaces for the
global, local common and distinct structures (Fig. 5.2 bottom). However,
when the group lasso penalty is used this was not the case. Supplemental
Fig. S5.5 shows that when a group lasso penalty is used, the models with
minimal CV error do not coincide with the correct dimensions of the
subspaces. In the model with minimum CV error, almost all the components
are assigned to the global structure. This result relates to the fact
that the lasso type penalty over-shrinks the non-zero parameters, and
then the CV error based model selection procedure tends to select a too
complex model to compensate to the biased parameter estimation. On the
other hand, as the GDP penalty achieves nearly unbiased parameter
estimation, the CV error based model selection procedure correctly
identifies the correct model.

After the model selection, a high precision P-ESCA model ( @xmath ) with
a group GDP penalty is re-fitted on the full data sets with the value of
@xmath corresponding to the minimum CV error and the selected structured
sparse pattern. For this selected model, the RMSEs in estimating @xmath
, @xmath , @xmath , @xmath and @xmath are 0.0259, 0.0239, 0.0285, 0.0335
and 0.0096 respectively. The RV coefficients in estimating the global
common structure is 0.9985; local common structures , and , 0.9977,
0.9969, 0.9953; the distinct structures , and , 0.9961, 0.9937, 0.9779.
The variation explained ratios of each component on the three data sets
computed using the estimated parameters, visualized in Fig. 5.3 , are
very similar to the true ones in supplemental Fig. S5.4. These values
can be very useful in exploring the constructed model.

#### 5.6.3 Full characterization of the P-ESCA model when applied to
multiple quantitative data sets

When applied to multiple quantitative data sets, our model is similar as
the SLIDE model, except that we use different penalties and a different
model selection procedure. The details of the differences between the
two approaches are summarized in the supplementary material. Since the
concave GDP penalty is capable to achieve a nearly unbiased estimation
of the parameters, the P-ESCA model with a group GDP penalty is expected
to achieve similar performance to the two stages procedure used in the
SLIDE model. Therefore, we simulated seven realistic cases by adjusting
the SNRs of the simulated structures to compare the performance of these
two models and their model selection procedures. The SNRs of the
simulated structures corresponding to these seven cases are listed in
supplemental Table S5.3. Case 1: only the local common structures exist
and they have unequal SNRs; case 2: the JIVE case, only the global
common and distinct structures exist, and they are all of low SNRs;
case3: all the simulated structures are of low SNRs; case 4: global
common structure dominate the simulation; case 5: local common
structures dominate the simulation; case 6: distinct structures dominate
the simulation; case 7: none of the global, local common and distinct
structures exist.

The following parameters are used in the G-G-G data simulations, @xmath
, @xmath , @xmath , @xmath , all of the @xmath are set to 1. In order to
have exactly 3 components for all the simulated structures, we reject
the simulations of which the singular values of the three components of
any specific structure are not 2 times larger than the singular value of
the corresponding residual term. The P-ESCA model with a group GDP (
@xmath ) penalty is selected and re-fitted on the full data sets in the
same way as above. For the SLIDE model, the simulated data sets @xmath
are column centered and block-scaled by the Frobenius norm of each data
set. Then the SLIDE model is selected and fitted using the default
parameters. The deflated column offset term is taken as the estimated
@xmath . The derived loading matrices @xmath are re-scaled by the
corresponding Frobenius norm of each data set. G-G-G data sets are
simulated for all the 7 cases, and for each case, the simulation
experiment (data simulation, model selection, fitting the final model)
is repeated 10 times for both the P-ESCA model and the SLIDE model. The
mean RV coefficients in evaluating the estimated global, local common
and distinct structures and the corresponding mean estimated ranks are
shown in Table 5.2 , and the mean RMSEs in estimating the simulated
parameters are shown in supplemental Table S5.4. In all 7 cases, these
two methods have very accurate estimation of the subspaces corresponding
to the global, local common and distinct structures, and of the
simulated parameters @xmath , which is the column concatenation of
@xmath , @xmath , and @xmath , which is row concatenation of @xmath .
For some of the cases there is a slight advantage for the P-ESCA model.

#### 5.6.4 Full characterization of the P-ESCA model when applied to
multiple binary data sets

The performance of the proposed P-ESCA model is fully characterized with
respect to multiple binary data sets. Here we make a comparison to the
MOFA model, which is the Bayesian counterpart of P-ESCA. In the P-ESCA
model, the structured sparse pattern is induced through a group concave
penalty, and the model selection is done through missing value based
cross validation, while in the MOFA model, the structured sparse pattern
is induced through the automatic relevance determination approach and
the model is selected through maximizing the marginal likelihood. In
addition, MOFA model also shrinks a component to be 0 when its variation
explained ratios for all the data sets are less than a threshold, the
default value of which is 0. The details of the differences are
summarized in the supplementary material. For the model selection of the
P-ESCA model, the range of @xmath values is @xmath , and the other
parameters are the same as before. To give an impression of the model
selection process, we also characterized how the CV errors, RMSEs and
the RV coefficients change with respect to @xmath in the P-ESCA model
with a group GDP penalty on the simulated B-B-B data sets in
supplemental Fig. S5.6. For the MOFA model, the default parameters are
used, but as exact sparsity cannot be achieved by the automatic
relevance determination procedure used in the MOFA model, we take a
component for a single data set to be 0 when the variation explained
ratio of this component on this data set is less than @xmath .

In the seven B-B-B simulations cases, we set @xmath , and the marginal
probability to be @xmath to simulate very unbalanced binary data sets.
Other parameters are the same as in the G-G-G simulation cases. The mean
RV coefficients in evaluating the estimated global, local common and
distinct structures and the corresponding mean estimated ranks are shown
in Table 5.3 , and the mean RMSEs in estimating the simulated parameters
are shown in supplemental Table S5.5. Compared to the results derived
from the P-ESCA model on the G-G-G data sets (Table 5.2 ), the recovered
subspaces related to the global, local common and distinct structures
from P-ESCA model on B-B-B data sets are less accurate with respect to
RV coefficient and rank estimation, especially when the SNR of a
specific structure is much lower than others (in case 4, 5, 6). However,
given the fact that all the three data sets only have binary
observations, the recovered subspaces are accurate enough. Furthermore,
it is interesting to find that such low RMSEs in estimating @xmath ,
@xmath (supplemental Table S5.5) can be achieved solely from a model on
multiple binary data sets. Although these results are a little bit
counter intuitive, it is coordinate with the previous research [ 52 , 64
] . According to our previous research [ 64 ] , this result mainly
relates to the fact that the GDP penalty can achieve nearly unbiased
parameter estimation. On the other hand, the RMSEs in estimating the
simulated parameters from the MOFA model (supplemental Table S5.5) are
much larger. Especially for the estimation of the simulated column
offset term, all the elements in the estimated @xmath from the MOFA
model are very close to 0, and are far away from the simulated @xmath .
However, the recovered subspaces from the MOFA model are comparable to
the results derived from the P-ESCA model (Table 5.3 ).

#### 5.6.5 Full characterization of the P-ESCA model when applied to
multiple data sets of mixed data types

The proposed P-ESCA model is also fully characterized on the simulated
multiple data sets of mixed quantitative and binary data types. Both
G-B-B and G-G-B data sets are simulated for all the seven simulation
cases. We set @xmath , all of @xmath to be 1, the marginal probability
in simulating unbalanced data sets to be @xmath . Other parameters are
the same as above. The range of @xmath values for loadings related to
the quantitative data sets is @xmath , and for loadings related to
binary data sets is @xmath . The mean RV coefficients of the estimated
global, local common and distinct structures and the corresponding mean
ranks estimation from the P-ESCA and the MOFA model in the seven G-B-B
simulation cases are shown in Table 5.4 , for the G-G-B simulation the
results are shown in Table 5.5 . The mean RMSEs in estimating the
simulated parameters are shown in supplemental Table S5.6, for the G-B-B
simulations are in supplemental Table S5.7. Similar to the previous
results of B-B-B simulations, the P-ESCA model can achieve quite
accurate estimates of the subspaces related to the global, local common
and distinct structures (Table 5.4 , Table 5.5 ) when the SNRs of
different structures are relatively equal. However, when the SNR of a
specific structure is very low compared to others (in case 4, 5, 6), the
P-ESCA model has difficulty for its recovery. However, compared to the
MOFA model, P-ESCA can achieve better results with respect to the
recovered subspaces (Table 5.4 , Table 5.5 ) and estimation of the
simulated parameters (supplemental Table S5.6, Table S5.7) in G-B-B and
G-G-B simulations.

### 5.7 Real data analysis

We applied the P-ESCA model on the chronic lymphocytic leukaemia (CLL)
data set [ 92 , 89 ] , which was used in the chapter of the MOFA model,
to give an example of the real data analysis. For the 200 samples in the
CLL data set, not all of them are fully characterized for all the
measurements. Drug response data has 184 samples and 310 variables; DNA
methylation data, 196 samples and 4248 variables; transcriptome data,
136 samples and 5000 variables; mutation data, 200 samples and 69 binary
variables. The missing pattern of the CLL data sets is visualized in
supplemental Fig. S5.7. Except for the missing values related to the
samples that were not measured by a specific platform, there are also
some selected variables missing in the mutation data (supplemental
Fig. S5.7). All the quantitative data sets are first column centered and
scaled by the sample standard deviation of each variable. After that,
the dispersion parameters of the quantitative data sets are estimated by
the @xmath estimation procedure. Rank estimation of each single data set
was performed three times and results are shown in supplemental Table
S5.8. The P-ESCA model with a GDP ( @xmath ) is selected and re-fitted
on the CLL data sets in the same way as described above. The initial
number of components is set to 50. The selected model has 41 components,
and if we take each loading vector related to a single data set in a
component as a group, there are 51 non-zero loading groups. The model
selection results are shown in supplemental Fig. S5.8. Since the
variation explained ratios of 41 components are difficult to visualize,
we only show the components (Fig. 5.4 ), whose variation explained ratio
are larger than 2% for at least one data set. The above procedure
(processing, model selection, fitting the final model) is repeated 5
times to test its stability. The Pearson coefficient matrix for the 5
estimations of the @xmath and the RV coefficient matrices for the 5
estimations of the @xmath , @xmath and @xmath are shown in supplemental
Fig. S5.9.

In [ 89 ] , a 10 components MOFA model is selected on the CLL data sets.
The variation explained plots of the 10 components MOFA model,
reproduced from [ 89 ] , is shown in supplemental Fig. S5.10. There is
some overlap between the two models (Fig. 5.4 , supplemental
Fig. S5.10). Both models have one strong common component in which all
data sets participate, and a common component in which two (P-ESCA) or
three (MOFA) data sets participate. Furthermore the drug response and
the transcriptomic (mRNA) data have extra distinct components. The
variation explained is somewhat higher for the P-ESCA model which also
uses extra components. The amount of variation explained is the highest
for the drug response and mRNA data sets. The main difference between
the models is the fact that P-ESCA only finds a single component
relevant for the binary mutation data while MOFA finds two. The
comparison of the two models with respect to the estimated @xmath is
infeasible because the column offset term is not included in this 10
components MOFA model. In general the P-ESCA result is more complex than
the results in [ 89 ] in terms of number of selected components and
variation explained. However, this is mainly because, during the model
selection of [ 89 ] , the minimum variation explained threshold is set
to 2%. If we set the threshold to the default value 0%, and set the
initial number of components to be 50, and other parameters are kept the
same, a 50 components MOFA model is selected.

### 5.8 Discussion

In this chapter, we generalized an exponential family SCA (ESCA) model
for the data integration of multiple data sets of mixed data types.
Then, we introduced the nearly unbiased group concave penalty to induce
structured sparsity pattern on the loading matrices of the ESCA model to
separate the global, local common and distinct variation. An efficient
MM algorithm with analytical form updates for all the parameters was
derived to fit the proposed group concave penalty penalized ESCA
(P-ESCA) model. In addition, a missing value based cross validation
procedure is developed for the model selection. In many different
realistic simulations (different SNR levels, and combinations of
quantitative and or binary data sets of different), the P-ESCA model and
the model selection procedure work well with respect to recovering the
subspaces related to the global, local common and distinct structures,
and the estimation of the simulated parameters.

The performance of the P-ESCA model and the cross validation based model
selection procedure relate to the fact that the used group concave
penalty can achieve nearly unbiased estimation of the parameters while
generating sparse solutions. The nearly unbiased parameter estimation
makes the P-ESCA model have high accuracy in the estimation of the
simulated parameters, and the cross validation error based model
selection procedure is consistent. Another key point of the model
selection procedure is that the randomly sampled @xmath non-missing
elements are usually a typical set of elements from the population. This
makes the CV error a good proxy of the prediction error of the model.
The rank estimation in different repetitions of the model selection
procedure is robust and only differ slightly with respect to the very
weak components.

When applied to multiple quantitative data sets, the proposed P-ESCA
model can achieve slightly better performance than the SLIDE model in
recovering the subspaces of the simulated structures and in estimating
the simulated parameters. Also, since missing value problems (missing
values in a single data set, or missing complete samples in one or some
of the data sets) are very common in practice, the option of tackling
missing values is a big advantage. In the P-ESCA model and its model
selection procedure, the effect of missing values is masked by using the
weight matrices, making full use of the available data sets. When
applied to the multiple binary data sets or the mixed quantitative and
binary data sets, the proposed P-ESCA model has better performance than
the MOFA model in recovering the subspaces of the simulated structures
and in estimating the simulated parameters. Furthermore, the exact
orthogonality constraint can be achieved in the P-ESCA model, which is
crucial for the uniqueness of the recovered subspaces related to the
global, local common and distinct variation.

### Acknowledgements

Y.S. gratefully acknowledges the financial support from China
Scholarship Council (NO.201504910809).

### 5.9 Supplementary information

#### 5.9.1 Dispersion parameter estimation using PCA

The notations of this subsection is the same as the Chapter 5 . Before
constructing an ESCA or P-ESCA model, the dispersion parameter @xmath of
a quantitative data set @xmath , which is the variance of the residual
term, is assumed to be known. Assume the column centered quantitative
data set is @xmath ( @xmath ), and the PCA model of @xmath can be
expressed as @xmath . @xmath ( @xmath ) and @xmath ( @xmath ) are the
score and loading matrix respectively; @xmath ( @xmath ) is the residual
term and elements in @xmath , @xmath ; @xmath is the true low rank of
@xmath . In order to tackle the potential missing value problem, we also
introduce the weight matrix @xmath in the same way as above. The maximum
likelihood estimation of @xmath can be expressed as @xmath , in which
@xmath is the number of non-missing elements in @xmath . Since this is a
biased estimation of @xmath , we can adjust the estimation according to
the degree of freedom as @xmath . The parameters @xmath , @xmath and
@xmath are estimated as follows.

We select the rank @xmath using a similar model selection strategy as in
the main text. We first split @xmath into @xmath and @xmath in the same
way as in the main text. Then, a series of PCA models with different
number of components are constructed on @xmath , and the CV error is
defined as the least square error in fitting @xmath . After that @xmath
is set to the number of components of the model with the minimum CV
error. Then a rank @xmath PCA model is constructed on the full data
@xmath , and we get an estimate of @xmath and @xmath . Then @xmath is
set to @xmath . The EM type algorithm used to fit the PCA model with the
option of missing values is implemented in Matlab in the same way as in
[ 60 ] .

#### 5.9.2 The difference between the SLIDE model and the P-ESCA model
when applied to multiple quantitative data sets

The notations of this subsection is the same as the Chapter 5 .

-   Different processing steps. The SLIDE model does column centering
    and block scaling using the Frobenius norm of the corresponding data
    set to preprocess the data. Then the relative weights of the data
    sets in the SCA model are set to 1. On the other hand, we estimate
    the dispersion parameter (variation of the noise term) of each data
    set and the inverse of the estimated dispersion parameter is
    equivalent to the relative weight of the data sets in the SCA model.

-   Different penalty terms. The SLIDE model uses the group lasso
    penalty to induce the structured sparsity. Because of the block
    scaling processing step, there is no weight @xmath on the group
    lasso penalty to accommodate for the potential unequal number of
    variables in different data sets. On the other hand, the weighted
    group concave penalty is used in the P-ESCA model.

-   Option for missing values. The option of tackling the missing value
    problem is not included in the SLIDE model.

-   Different model selection procedures. The SLIDE model uses a two
    stages approach to do model selection, while our model selection
    approach is as described as in the main text.

#### 5.9.3 The difference between the MOFA model and the P-ESCA model

The notations of this subsection is the same as the Chapter 5 .

-   Different origins. Although these two methods are similar with
    respect to what they can do, they have different origins. The MOFA
    model is developed in the Bayesian probabilistic matrix
    factorization framework in the same line as the group factor
    analysis model and the factor analysis model, while the P-ESCA model
    is derived in the deterministic matrix factorization framework in
    the same line as the SLIDE model, the SCA model and the PCA model.

-   Different ways in inducing structured sparsity. In the P-ESCA model,
    the structured sparse pattern is induced through a group concave
    penalty, while in the MOFA model, it is induced through the
    automatic relevance determination approach. The group concave
    penalty can shrink a group of elements to be exactly 0, while the
    automatic relevance determination cannot achieve exact sparsity. In
    addition, MOFA model also shrinks a component to be 0 when its
    variation explained ratios for all the data sets are less than a
    threshold, whose default value is 0.

-   Different model selection procedures. The P-ESCA model is selected
    by a missing value based CV approach; while the selection of a MOFA
    model relies on maximizing the marginal likelihood. In theory,
    maximizing the marginal likelihood has no difficulty in tuning
    multiple parameters, while the CV based model selection procedure is
    infeasible for such task.

-   Orthogonality constraint. The orthogonality constraint @xmath can
    only be achieved in the P-ESCA model. Whether this property is
    meaningful or not depends on the specific research question.
    However, the constraint is crucial for the proof of the uniqueness
    of the recovered subspaces corresponding to the global, local common
    and distinct variation.

#### 5.9.4 Supplemental tables

#### 5.9.5 Supplemental figures

## Chapter 6 Outlook

In the proposed penalized exponential family SCA (P-ESCA) model (Chapter
5 ), a group concave penalty is used to induce group-wise sparse pattern
on the loading matrix to disentangle the global, local common and
distinct components. The P-ESCA model and the associated MM algorithm
can be further generalized to include other types of penalties to induce
more interesting sparse patterns on the loading matrix, which may be
useful for the data analyst. In addition, the currently developed model
selection procedure has difficulties in tuning multiple tuning
parameters. It will be worthwhile to explore other types of model
selection approaches to address this issue. Also, in the current thesis,
the parametric exponential family distribution is used to tackle the
heterogeneous measurement scales. There also exist other possible
non-parametric and semi-parametric approaches. It is worthwhile to
generalize them for the data fusion of multiple data sets with the two
types of heterogeneity. Furthermore, it is also interesting to
generalize the P-ESCA model for prediction tasks or to taking into
account the experimental design underlining the used multiple data sets.

### 6.1 Including other types of sparse patterns

The developed P-ESCA model and the associated MM algorithm have a lot of
potential for further generalization. The options for inducing
element-wise sparsity on the loading matrix or the composition of both
group-wise and element-wise sparsity or other types of penalties can be
easily included. Some examples will be shown in the following
subsections. These P-ESCA model extensions can be selected using the
developed missing value based CV procedure (Chapter 5 ). A possible
alternative model selection approach is the Bayesian optimization [ 98 ]
framework, which is appropriate for the tuning of multiple (usually less
than 20) continuous tuning parameters. Since this framework has been
successfully applied in the automatic tuning of various machine learning
algorithms [ 99 ] , it will be worthwhile to explore its usage for the
P-ESCA models with multiple tuning parameters.

The following notations are the same as the P-ESCA model in Chapter 5 .
Suppose the @xmath column of the @xmath loading matrix @xmath is @xmath
. The group concave penalty on the @xmath loading matrix @xmath is
imposed on the @xmath norm of @xmath as @xmath , in which @xmath is a
concave function. This group concave penalty (or concave @xmath norm
penalty) will shrink on the group level ( @xmath norm of @xmath ) as a
concave penalty, and in the element level (elements inside @xmath ) as a
ridge regression type penalty. Sometimes, we may need other types of
penalties, such as the element-wise sparsity on the elements of @xmath
or the composition of both element-wise and group-wise sparsity patterns
on @xmath . All these options can be easily included into the P-ESCA
model by a slightly modification of the developed MM algorithm.

#### 6.1.1 P-ESCA model with an element-wise concave penalty

When a single data set is used, the P-ESCA model with an element-wise
concave penalty is an approach for the sparse exponential family PCA
model. When multiple data sets are used, the model is an approach for
the sparse exponential family SCA model.

##### Element-wise concave penalty

An element-wise concave penalty can be imposed on the elements of @xmath
to induce the element-wise sparsity on @xmath . Suppose the @xmath
element of @xmath is @xmath , and its absolute value is @xmath , @xmath
. The concave penalty on @xmath can be expressed as @xmath , in which
@xmath is a concave function in Table 5.1 . The optimization problem
associated with this P-ESCA model with an element-wise concave penalty
can be expressed as follows,

  -- -------- -------- -------- -------
     @xmath   @xmath            (6.1)
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- -------

##### Algorithm

The algorithm developed in Section 5.3 can be modified only with respect
to the majorization step of the penalty function and the updating @xmath
step to fit the P-ESCA model with an element-wise penalty in equation
6.1 . Similar to the equation 5.5 , the element-wise penalty @xmath is a
concave function with respect to @xmath and can be majorized as @xmath ,
in which @xmath and @xmath is the absolute value of the @xmath element
of @xmath (the @xmath approximation of @xmath during the @xmath
iteration). After majorizing the original problem in equation 6.1 ,
updating the offset terms @xmath and score matrix @xmath in exactly the
same way as in Section 5.3 , the optimization problem associated with
the updating of @xmath is

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
     @xmath   @xmath   
  -- -------- -------- --

in which @xmath indicates the @xmath element of the matrix @xmath . The
above optimization problem is simple the proximal operator of the @xmath
norm, and the analytical solution exists [ 25 ] . Take @xmath and @xmath
, the analytical solution of @xmath is @xmath . To update the parameter
@xmath , we can simply apply this proximal operator to all the elements
of @xmath . The other parts of the algorithm are the same as in Section
5.3 .

#### 6.1.2 P-ESCA model with a concave L1 norm penalty

##### Concave L1 norm penalty

Another way to induce group sparsity on @xmath is through the concave
@xmath norm penalty [ 22 ] . Suppose the @xmath column of the @xmath
loading matrix @xmath is @xmath , and its @xmath norm is @xmath , @xmath
. The concave @xmath norm penalty on @xmath can be expressed as @xmath ,
in which weight @xmath is used to accommodate the potential different
number of variables in different data set, and @xmath is a concave
function in Table 5.1 . This concave @xmath norm penalty will shrink on
the group level ( @xmath norm of @xmath ) as a concave penalty, and in
the element level (elements inside @xmath ) as a lasso type penalty. The
optimization problem associated with this P-ESCA model with a concave
@xmath norm penalty can be expressed as follows,

  -- -------- -------- -------- -------
     @xmath   @xmath            (6.2)
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- -------

##### Algorithm

The algorithm developed in Section 5.3 can be modified only with respect
to the majorization step of the penalty function and the updating @xmath
step to fit the P-ESCA model with the concave @xmath norm penalty in
equation 6.2 . Similar to the equation 5.5 , the penalty function @xmath
is concave with respect to @xmath and can be majorized as @xmath , in
which @xmath and @xmath is the @xmath norm of the @xmath column of
@xmath (the @xmath approximation of @xmath during the @xmath iteration).
After majorizing the original problem in equation 6.2 , updating the
offset terms @xmath and score matrix @xmath in exactly the same way as
in Section 5.3 , the optimization problem associated with the updating
of @xmath is

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Take @xmath and @xmath , the analytical solution of @xmath is @xmath .
To update the parameter @xmath , we can simply apply this proximal
operator to all the elements of @xmath . The other parts of the
algorithm are the same as in Section 5.3 .

#### 6.1.3 P-ESCA model with a composite concave penalty

##### Composite concave penalty

There also exists a composite concave penalty to induce both group and
element-wise sparsity [ 22 ] . The composite concave penalty on @xmath
can be expressed as @xmath , in which weight @xmath is used to
accommodate the potential different number of variables in different
data set, @xmath and @xmath are two concave functions for the group
level and element level respectively. We will use the same concave
function @xmath from Table 5.1 for both @xmath and @xmath . This
composite concave penalty will shrink both on the group level ( @xmath )
and in the element level (elements inside @xmath ) as a concave penalty.
The optimization problem associated with this P-ESCA model with a
composite concave penalty can be expressed as follows,

  -- -------- -------- -------- -------
     @xmath   @xmath            (6.3)
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- -------

##### Algorithm

The algorithm developed in Section 5.3 can be modified only with respect
to the majorization step of the penalty function and the updating @xmath
step to fit the P-ESCA model with a composite concave penalty in
equation 6.3 . Here we take @xmath and @xmath . Since both @xmath and
@xmath are concave function and they are monotonically non-decreasing,
their composition is also a concave function with respect to @xmath .
Therefore, we can majorize the composite function @xmath in a similar
way as the equation 5.5 , @xmath , in which @xmath and @xmath is the
absolute value of the @xmath element of @xmath (the @xmath approximation
of @xmath during the @xmath iteration), @xmath . After majorizing the
original problem in equation 6.3 , updating the offset terms @xmath and
score matrix @xmath in exactly the same way as in Section 5.3 , the
optimization problem associated with the updating of @xmath is

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Take @xmath and @xmath , the analytical solution of @xmath is @xmath .
To update the parameter @xmath , we can simply apply this proximal
operator to all the elements of @xmath . The other parts of the
algorithm are the same as in Section 5.3 .

#### 6.1.4 P-ESCA model with other types of penalties

All the algorithms for the above P-ESCA models with different penalties
are based on the fact that the updating of @xmath in equation 5.7 can be
re-expressed as a problem of finding the proximal operator for the
@xmath norm or the @xmath norm penalty. Therefore, P-ESCA model can also
be extended to include other types of penalty whose proximal operator
has a simple or analytical solution. For example, there is no difficulty
in including concave penalties on the rows of the loading matrix @xmath
to induce row-wise sparsity, which could be useful for the feature
selection. Furthermore, we can also add cardinality constraints (pseudo
@xmath norm) on the number of nonzero elements, the number of nonzero
rows, or the number of nonzero columns of the loading matrix @xmath to
induce the desired sparsity pattern. These various @xmath norm penalties
are non-convex, however, there are heuristic solutions for the
corresponding proximal operator [ 100 ] . These various @xmath penalties
can be useful if all our data sets are quantitative. However, when
discrete data sets are used, the derived model with the @xmath norm type
of penalty will have problems in constraining the scale of estimated
parameters. The standard logistic PCA model, in which the exact low rank
constraint can be regarded as applying @xmath norm penalty on the
singular values, is a good example to illustrate this point.

### 6.2 Other directions of tackling heterogeneous measurement scales

In the current thesis, the heterogeneous measurement scales are
accounted for by assuming a parametric exponential family distribution
in a similar way as the generalized linear models. There also existed
other possible directions [ 38 , 101 , 36 ] to tackle the problems
induced by the heterogeneous measurement scales. One promising
alternative is the semi-parametric XPCA method [ 101 ] . In the
probabilistic interpretation of a PCA model on a matrix @xmath ( @xmath
), we assume we have @xmath samples from a @xmath dimensional
multivariate normal distribution and therefore normal marginal
distribution for each column. On the contrary, XPCA model is based on a
semi-parametric @xmath dimensional multivariate distribution, which is
the combination of nonparametric marginals of all the @xmath
quantitative or discrete columns and a Gaussian copula. The assumptions
of parametric marginal distributions (normal distribution for
quantitative data, Bernoulli distribution for binary data) for the
columns of the observed data set @xmath are relaxed in the XPCA model.
Therefore, when the exponential family distribution is not a good
approximation of the observed data, for example, the empirical
distribution of a quantitative variable is far from symmetric, XPCA
model has a clear advantage. Another interesting alternative is the
non-parametric representation matrices approach [ 38 ] , in which each
variable (continuous or discrete) is represented by a representation
matrix and the resulting representation matrices can be used in a three
way model for symmetric data. The advantage of the representation
matrices approach is that no probabilistic assumption is made for the
model.

### 6.3 Using data fusion for supervised learning

All the methods developed in this thesis are unsupervised learning
approaches. It is worthwhile to extend these methods in the supervised
learning framework for prediction tasks. A simple approach, same as the
extension of PCA to principal component regression model for prediction
tasks, is as follows. These various unsupervised data fusion methods are
taken as feature extraction approaches for multiple data sets. The
derived low dimensional score matrix can be regarded as the extracted
features and can be used as inputs for any other supervised learning
methods. However, the extracted low dimensional features are not
necessarily optimal for the prediction tasks. Therefore, when label
information is available, it is better to make full use of it to make
the extracted features more informative to the prediction tasks. The
P-ESCA model can be extended from this perspective in a similar way as
extending the PCA model to the partial least squares regression model [
102 ] . The extracted low dimensional structures from the P-ESCA model
should not only represent the multiple data sets well but also have high
covariance with the label information.

### 6.4 Incorporating the information of experimental design

Sometimes, the multiple sets of measurements on the same objects result
from carefully designed experimental studies rather than observational
studies. Such an experimental design always contains several factors,
such as different treatments or different time points or their
combinations, which are of interest with respect to the research
question. Therefore, these experimental factors are underlying the
multiple data sets on the same objects. To study the effects of these
experimental factors or to remove their effects on the explorative data
analysis, the used data fusion approaches should take the experimental
design structure into account. The proposed P-ESCA can be extended from
this direction by including extra low dimensional structures to account
for these experimental factors in a similar way as the
ANOVA-simultaneous component analysis (ASCA) model [ 103 ] .

\summary

Multiple high dimensional measurements from different platforms on the
same biological system are becoming increasingly common in biological
research. These different sources of measurements not only provide us
with the opportunity of a deeper understanding of the studied system,
but they also introduce some new statistical challenges. All these
challenges are related to the heterogeneity of the data sets. The first
type of heterogeneity is the type of data , such as metabolomics,
proteomics and RNAseq data in genomics. These different omics data
reflect the properties of the studied biological system from different
perspectives. The second type of heterogeneity is the type of scale ,
which indicates the measurements are obtained at different scales, such
as binary, ordinal, interval and ratio-scaled variables. Within this
thesis, various data fusion approaches are developed to tackle either
one or two types of heterogeneity that exist in multiple data sets.

In Chapter 2 , we reviewed and compared various parametric and
nonparametric extensions of principal component analysis (PCA)
specifically geared for binary data. The special mathematical
characteristics of binary data are taken into account from different
perspectives in these different extensions of PCA. We explored their
performance with respect to finding the correct number of components,
overfitting, retrieving the correct low dimensional structure, variable
importance, etc, using both realistic simulations of binary data as well
as mutation, copy number aberrations (CNA) and methylation data of the
GDSC1000 project. Our results indicate that if a low dimensional
structure exists in the data, most of the methods can find it. We
recommend to use the parametric logistic PCA model (projection based
approach) if the probabilistic generating process can be assumed
underlying the data, and to use the nonparametric Gifi model if such an
assumption is not valid and the data is considered as given.

In Chapter 3 , we developed a robust logistic PCA model via non-convex
singular value thresholding. The promising logistic PCA model for binary
data has an overfitting issue because of the used exact low rank
constraint. We proposed to fit a logistic PCA model via non-convex
singular value thresholding to alleviate the overfitting issue. An
efficient majorization-minimization (MM) algorithm is implemented to fit
the model and a missing value based cross validation (CV) procedure is
introduced for the model selection. Furthermore, we re-expressed the
logistic PCA model based on the latent variable interpretation of the
generalized linear models (GLMs) on binary data. The latent variable
interpretation of the logistic PCA model not only makes the assumption
of low rank structure easier to understand, but also provides us a way
to define signal to noise ratio (SNR) in the simulation of multivariate
binary data. Our experiments on realistic simulations of imbalanced
binary data and low SNR show that the CV error based model selection
procedure is successful in selecting the proposed model. And the
selected model demonstrates superior performance in recovering the
underlying low rank structure compared to models with exact low rank
constraint and convex nuclear norm penalty.

In the Chapter 4 , we developed a generalized simultaneous component
analysis (GSCA) model for the data fusion of binary and quantitative
data sets. Simultaneous component analysis (SCA) model is one of the
standard tools for exploring the underlying dependence structure present
in multiple quantitative data sets measured on the same objects.
However, it does not have any provisions when a part of the data are
binary. To this end, we propose the GSCA model, which takes into account
the distinct mathematical properties of binary and quantitative
measurements in the maximum likelihood framework. In the same way as in
the SCA model, a common low dimensional subspace is assumed to represent
the shared information between these two distinct types of measurements.
However, the GSCA model can easily be overfitted when a rank larger than
one is used, which can lead to the problem that some of the estimated
parameters can become very large. To achieve a low rank solution and
combat overfitting, we propose to use non-convex singular value
thresholding. An efficient majorization algorithm is developed to fit
this model with different concave penalties. Realistic simulations (low
SNR and highly imbalanced binary data) are used to evaluate the
performance of the proposed model in recovering the underlying
structure. Also, a missing value based CV procedure is implemented for
the model selection. We illustrate the usefulness of the GSCA model for
exploratory data analysis of quantitative gene expression and binary CNA
measurements obtained from the GDSC1000 data sets.

In Chapter 5 , we proposed a penalized exponential family SCA (P-ESCA)
model for the data fusion of multiple data sets with two types of
heterogeneity. Multiple sets of measurements on the same objects
obtained from different platforms may reflect partially complementary
information of the studied system. However, the heterogeneity of such
data sets introduces some new statistical challenges for their data
fusion. First, the separation of information that is common across all
or some of the data sets, and the information that is specific to each
data set is problematic. Furthermore, these data sets are often a mix of
quantitative and discrete (binary or categorical) data types, while
commonly used data fusion methods require all data sets to be
quantitative. Therefore, we proposed an exponential family simultaneous
component analysis (ESCA) model to tackle the potential mixed data types
problem of multiple data sets. In addition, a structured sparse pattern
of the loading matrix is induced through a nearly unbiased group concave
penalty to disentangle the global, local common and distinct information
of the multiple data sets. An efficient MM algorithm is derived to fit
the proposed model. Analytic solutions are derived for updating all the
parameters of the model in each iteration, and the algorithm will
decrease the objective function in each iteration monotonically. For
model selection, a missing value based CV procedure is implemented. The
advantages of the proposed method in comparison with other approaches
are assessed using comprehensive simulations as well as the analysis of
real data from a chronic lymphocytic leukaemia (CLL) study.

In Chapter 6 , we considered various extensions of the developed P-ESCA
model with respect to new penalties (element-wise, group-wise and their
composition) and new model selection approach. Also, we remarked the
potential of the semi-parametric XPCA model and non-parametric
representation matrices approach in tackling the data sets of
heterogeneous measurement scales. Furthermore, it is also interesting to
generalize the P-ESCA model for prediction tasks or to tacking into
account the experimental design underlining the used multiple data sets.

\samenvatting

In biologisch onderzoek wordt het steeds gebruikelijker meerdere
hoog-dimensionale metingen op verschillende platformen aan hetzelfde
biologische systeem uit te voeren. Deze van verschillende bronnen
afkomstige metingen bieden de mogelijkheid tot een beter begrip van het
bestudeerde systeem, maar brengen ook nieuwe statistische uitdagingen
met zich mee. Al deze uitdagingen houden verband met de heterogeniteit
van de dataverzamelingen. De eerste vorm van heterogeniteit ligt in het
type van de gegevens. Zo zijn er verschillende typen omics-data,
metabolomics, proteomics en RNAseq data in genomics, die ieder een eigen
perspectief op de eigenschappen van het biologische systeem bieden. De
tweede vorm van heterogeniteit ligt in de meetschaal van de data. De
data kunnen op verschillende schalen gemeten worden, zoals binair,
ordinale schaal, intervalschaal en ratioschaal. In dit proefschrift
worden verschillende datafusiemethoden ontwikkeld waarmee één of beide
soorten dataheterogeniteit aangepakt kunnen worden.

In hoofdstuk 2 wordt een aantal, zowel parametrische als
niet-parametrische, uitbreidingen van principale componenten analyse
(PCA) vergeleken die specifiek betrekking hebben op binaire data. In
deze uitbreidingen van PCA wordt op verschillende manieren rekening
gehouden met de speciale wiskundige karakteristieken van binaire
gegevens. We onderzochten de prestaties van deze uitbreidingen van PCA
met betrekking tot het vinden van het juiste aantal componenten,
overfitting, het vinden van de juiste laag-dimensionale structuur, het
belang van variabelen, enz. door gebruik van zowel realistische
simulaties van binaire data als van mutatiedata, ‘copy number
aberrations’ (CNA) en methylatiedata van het GDSC1000 project. Onze
resultaten laten zien dat als er een laag-dimensionale structuur in de
data aanwezig is, de meeste methoden deze kunnen vinden. Wij adviseren
om het parametrisch logistisch PCA-model (op projectie gebaseerde
benadering) te gebruiken als verondersteld wordt dat een stochastisch
proces aan de data ten grondslag ligt. Als dit niet het geval is en de
data als vast gegeven kan worden beschouwd, raden we aan het
niet-parametrische Gifi-model te gebruiken.

In hoofdstuk 3 hebben we een robuust logistisch PCA-model ontwikkeld met
behulp van een niet-convexe drempelwaardenfunctie voor de singuliere
waarden. Het veelbelovende logistische PCA-model voor binaire data heeft
een probleem met over-fitting vanwege de gebruikte randvoorwaarde van
een exacte lage rang. We stellen voor om het over-fitten te verminderen
door het logistische PCA-model te fitten met gebruik van een
niet-convexe drempelwaardenfunctie voor singuliere waarden. Een
efficiënt majorisatie-minimalisatie (MM) algoritme is geïmplementeerd om
het model te fitten en een op missende waarden gebaseerde kruisvalidatie
(KV) procedure is geïntroduceerd voor modelselectie. Bovendien hebben we
het logistische PCA-model uitgedrukt op basis van de latente variabelen
interpretatie van gegeneraliseerde lineaire modellen (GLMs). Niet alleen
maakt de aanname van een structuur met lage rang het model beter te
begrijpen, maar biedt ook een manier om de signaal-ruis verhouding in de
simulatie van multivariate binaire data te definiëren. Onze experimenten
met realistische simulaties van ongebalanceerde binaire data met een
lage signaal-ruis verhouding laten zien dat modelselectie gebaseerd op
KV-fouten goed in staat is het voorgestelde model te selecteren. Dit
geselecteerde model is uitstekend in staat de onderliggende lage-rang
structuur terug te vinden en werkt beter dan modellen met een exacte
lage-rang randvoorwaarde die een convexe spoornormboete gebruiken.

In hoofdstuk 4 ontwikkelden we een gegeneraliseerd simultaan componenten
analyse (GSCA) model voor de fusie van binaire en kwantitatieve
dataverzamelingen. Simultane componenten analyse (SCA) is één van de
standaard hulpmiddelen om de onderliggende afhankelijkheidsstructuur te
onderzoeken die aanwezig is in meerdere kwantitatieve data sets die aan
hetzelfde object gemeten zijn. Echter, SCA is niet geschikt als een deel
van de data binair is. Daarom stellen we een GSCA-model voor dat
rekening houdt met de specifieke mathematische eigenschappen van binaire
data en kwantitatieve metingen binnen het kader van grootste
aannemelijkheid. Op dezelfde manier als voor het SCA-model
veronderstellen we het bestaan van een laag-dimensionale deelruimte
waarin de gedeelde informatie van de twee typen van metingen wordt
gerepresenteerd. Het GSCA-model is evenwel geneigd tot overfitten
wanneer een rang groter dan 1 wordt gebruikt. Hierdoor kunnen sommige
parameters bijzonder groot geschat worden. Om een oplossing met lage
rang zonder overfitting te vinden, stellen we voor een niet-convexe
drempelwaardefunctie te gebruiken voor de selectie van singuliere
waarden. We ontwikkelden een efficiënt majorisatie algoritme om dit
model te fitten voor verschillende concave boetefuncties. Realistische
simulaties (lage signaal-ruis verhouding en sterk ongebalanceerde
binaire data) werden gebruikt om te beoordelen hoe goed het model de
onderliggende structuur kan reproduceren. Ook is een op missende waarden
gebaseerde kruisvalidatie geimplementeerd om modellen te selecteren. De
bruikbaarheid van het GSCA-model als exploratieve tool wordt
gedemonstreerd aan de hand van kwantitatieve genexpressiedata en binaire
CNA-metingen uit de GDSC1000 dataverzameling.

In hoofdstuk 5 stellen we een exponentieel SCA-model met boeteoptie
(P-ESCA) voor om meerdere dataverzamelingen met twee typen
heterogeniteit samen te voegen. Meerdere metingen aan hetzelfde object
maar uitgevoerd op verschillende platforms kunnen complementaire
informatie over het bestudeerde systeem opleveren. De heterogeniteit van
deze data biedt interessante nieuwe statistische uitdagingen als de
dataverzamelingen gefuseerd worden. Ten eerste is de scheiding van
informatie die gemeenschappelijk is voor alle (of enkele) van de
dataverzamelingen van de informatie die specifiek is voor iedere
dataverzameling afzonderlijk, lastig. Bovendien zijn deze
dataverzamelingen vaak een mix van kwantitatieve en discrete (binair of
categorisch) data typen, terwijl gebruikelijke datafusiemethoden
vereisen dat alle dataverzamelingen kwantitatief zijn. Met het door ons
voorgestelde exponentiële simultane componenten analyse (ESCA) model
kunnen we dergelijke gemengde dataverzamelingen wel analyseren. Om de
globale, lokaal gemeenschappelijke en verzameling-specifieke informatie
in de verschillende dataverzamelingen te ontwarren, hebben we op de
componentenladingsmatrix, via een groep van concave boetefuncties bijna
zonder systematische fout, een gestructureerd, bijna leeg patroon
opgelegd. Om het voorgestelde model te fitten hebben we een algoritme
gebaseerd op majorisatie-minimalisatie gemaakt. Dit algoritme gebruikt
analytische oplossingen om de modelparameters na iedere iteratie te
actualiseren; de doelfunctie wordt door het algoritme iedere iteratie
monotoon verminderd. Voor de modelselectie gebruiken we een op missende
waarden gebaseerde kruisvalidatie. De voordelen van de voorgestelde
methode in vergelijking met andere methoden werden beoordeeld met
uitgebreide simulaties en de analyse van echte data uit een chronische
lymfatische leukemie (CCL) studie.

In hoofdstuk 6 beschouwen we verschillende uitbreidingen van het
ontwikkelde P-ESCA-model met nieuwe boetesystemen (per element, per
groep en element-groep samenstelling) en een nieuwe benadering voor
modelselectie. We kijken ook naar de mogelijkheden die het
semi-parametrische XPCA-model en de niet-parametrische matrixmethode
bieden om data verzamelingen met heterogene meetschalen aan te pakken.

###### Acknowledgements. My four years of Ph.D. life in Amsterdam is
difficult but productive. In the beginning, I didn’t have much
confidence in myself for the transition from an experimentalist to a
data analyst (statistician). However, during this process, I learned a
lot on statistics, and have developed several statistical methods for
the data fusion of heterogeneous data sets. These cannot be done without
the help and support of a lot of people. Here I would like to express my
gratitude to you. First, I would like to say thanks to Age Smilde and
Johan Westerhuis for your four years’ supervision. I appreciate that you
accepted me to do my Ph.D. research in the BDA group. And thanks so much
for all the discussions we have during the last four years and your
insightful comments on my work. These helped me a lot in learning
statistics and doing research. I would also like to say thanks to Huub
Hoefsloot and Age Smilde for the matrix algebra course. Matrix algebra
is the foundation of statistics and numerical computation. A solid
understanding of this subject helps me a lot on the algorithm
development for my research. Also, I would like to acknowledge Gooitzen
Zwanenburg for your help in solving my problems on the computer,
software, Internet, the Dutch translation of the thesis summary, and
many other things. Furthermore, I want to thank Dicle, Chloie, Sandra,
and Maryam. I am glad to be in the same office as you. Thanks for the
discussion we have about life, research and many other topics. Also,
thanks for all the members in the BDA group for the coffee break
conversations, the presentations, and discussions during the group
meetings. Part of my Ph.D. research has collaborated with Lodewyk
Wessels, Nanne Aben from the Netherlands Cancer Institute (NKI) and
Patrick J.F. Groenen from the Erasmus University. Thanks to Lodewyk
Wessels and Nanne Aben for your real biological data sets, your
informative comments on our paper and all the biological
interpretations. Also, thanks to Patrick for your enlightening comments
on the algorithm section of our paper and for your work on the GDP
penalty, which is used throughout my thesis. I would also like to
acknowledge the China Scholarship Council for financial support. The
scholarship makes it possible for me to study in Amsterdam. Lastly, I
would like to thank my family and my girlfriend for your love and
encouragement.
Yipeng Song May 21, 2019
