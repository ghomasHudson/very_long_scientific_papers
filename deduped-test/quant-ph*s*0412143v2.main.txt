##### Contents

-    1 Limitations of Quantum Computers
    -    1.1 The Collision Problem
    -    1.2 Local Search
    -    1.3 Quantum Certificate Complexity
    -    1.4 The Need to Uncompute
    -    1.5 Limitations of Quantum Advice
-    2 Models and Reality
    -    2.1 Skepticism of Quantum Computing
    -    2.2 Complexity Theory of Quantum States
    -    2.3 Quantum Search of Spatial Regions
    -    2.4 Quantum Computing and Postselection
    -    2.5 The Power of History
-    3 The Complexity Zoo Junior
-    4 Notation
-    5 Oracles
-    6 Quantum Computers: @xmath Qubits
-    7 Further Concepts
-    I Limitations of Quantum Computers
    -    8 The Quantum Black-Box Model
    -    9 Oracle Separations
    -    10 Motivation
        -    10.1 Oracle Hardness Results
        -    10.2 Information Erasure
    -    11 Preliminaries
    -    12 Reduction to Bivariate Polynomial
    -    13 Lower Bound
    -    14 Set Comparison
    -    15 Open Problems
    -    16 Motivation
    -    17 Preliminaries
    -    18 Relational Adversary Method
    -    19 Snakes
    -    20 Specific Graphs
        -    20.1 Boolean Hypercube
        -    20.2 Constant-Dimensional Grid Graph
    -    21 Summary of Results
    -    22 Related Work
    -    23 Characterization of Quantum Certificate Complexity
    -    24 Quantum Lower Bound for Total Functions
    -    25 Asymptotic Gaps
        -    25.1 Local Separations
        -    25.2 Symmetric Partial Functions
    -    26 Open Problems
    -    27 Preliminaries
    -    28 Quantum Lower Bound
    -    29 Open Problems
    -    30 Preliminaries
        -    30.1 Quantum Advice
        -    30.2 The Almost As Good As New Lemma
    -    31 Simulating Quantum Messages
        -    31.1 Simulating Quantum Advice
    -    32 A Direct Product Theorem for Quantum Search
    -    33 The Trace Distance Method
        -    33.1 Applications
    -    34 Open Problems
-    II Models and Reality
    -    35 Bell Inequalities and Long-Range Threads
    -    36 Sure/Shor Separators
    -    37 Classifying Quantum States
    -    38 Basic Results
    -    39 Relations Among Quantum State Classes
    -    40 Lower Bounds
        -    40.1 Subgroup States
        -    40.2 Shor States
        -    40.3 Tree Size and Persistence of Entanglement
    -    41 Manifestly Orthogonal Tree Size
    -    42 Computing With Tree States
    -    43 The Experimental Situation
    -    44 Conclusion and Open Problems
    -    45 Summary of Results
    -    46 Related Work
    -    47 The Physics of Databases
    -    48 The Model
        -    48.1 Locality Criteria
    -    49 General Bounds
    -    50 Search on Grids
        -    50.1 Amplitude Amplification
        -    50.2 Dimension At Least 3
        -    50.3 Dimension 2
        -    50.4 Multiple Marked Items
        -    50.5 Unknown Number of Marked Items
    -    51 Search on Irregular Graphs
        -    51.1 Bits Scattered on a Graph
    -    52 Application to Disjointness
    -    53 Open Problems
    -    54 The Class @xmath
    -    55 Fantasy Quantum Mechanics
    -    56 Open Problems
    -    57 The Complexity of Sampling Histories
    -    58 Outline of Chapter
    -    59 Hidden-Variable Theories
        -    59.1 Comparison with Previous Work
        -    59.2 Objections
    -    60 Axioms for Hidden-Variable Theories
        -    60.1 Comparing Theories
    -    61 Impossibility Results
    -    62 Specific Theories
        -    62.1 Flow Theory
        -    62.2 Schrödinger Theory
    -    63 The Computational Model
        -    63.1 Basic Results
    -    64 The Juggle Subroutine
    -    65 Simulating @xmath
    -    66 Search in @xmath Queries
    -    67 Conclusions and Open Problems

###### List of Figures

-    1 Conway’s Game of Life
-    2 Known relations among @xmath complexity classes
-    3 Quantum states of one qubit
-    4 A snake of vertices flicks its tail
-    5 The coordinate loop in @xmath dimensions
-    6 Sure/Shor separators
-    7 Tree representing a quantum state
-    8 Known relations among quantum state classes
-    9 Quantum robot searching a 2D grid
-    10 The ‘starfish’ graph
-    11 Disjointness in @xmath communication
-    12 Simulating @xmath using postselection
-    13 Flow network corresponding to a unitary matrix

###### List of Tables

-    1 Query complexity and certificate complexity measures
-    2 Expressions for @xmath
-    3 Four objections to quantum computing
-    4 Summary of bounds for spatial search
-    5 Divide-and-conquer versus quantum walks
-    6 Four hidden-variable theories and the axioms they satisfy

###### Acknowledgements. My adviser, Umesh Vazirani, once said that he
admires the quantum adiabatic algorithm because, like a great squash
player, it achieves its goal while moving as little as it can get away
with.  Throughout my four years at Berkeley, I saw Umesh inculcate by
example his “adiabatic” philosophy of life: a philosophy about which
papers are worth reading, which deadlines worth meeting, and which
research problems worth a fight to the finish.  Above all, the concept
of “beyond hope” does not exist in this philosophy, except possibly in
regard to computational problems.  My debt to Umesh for his expert
scientific guidance, wise professional counsel, and generous support is
obvious and beyond my ability to embellish.  My hope is that I graduate
from Berkeley a more adiabatic person than when I came. Admittedly, if
the push to finish this thesis could be called adiabatic, then the
spectral gap was exponentially small.  As I struggled to make the
deadline, I relied on the help of David Molnar, who generously agreed to
file the thesis in Berkeley while I remained in Princeton; and my
committee—consisting of Umesh, Luca Trevisan, and Birgitta Whaley—which
met procrastination with flexibility. Silly as it sounds, a principal
reason I came to Berkeley was to breathe the same air that led Andris
Ambainis to write his epochal paper “Quantum lower bounds by quantum
arguments.”  Whether or not the air in 587 Soda did me any good, Part I
of the thesis is essentially a 150-page tribute to Andris—a colleague
whose unique combination of genius and humility fills everyone who knows
him with awe. The direction of my research owes a great deal as well to
Ronald de Wolf, who periodically emerges from his hermit cave to
challenge non-rigorous statements, eat dubbel zout, or lament American
ignorance.  While I can see eye-to-eye with Ronald about (say) the
@xmath versus @xmath problem, I still feel that Andrei Tarkovsky’s
Solaris would benefit immensely from a car chase. For better or worse,
my conception of what a thesis should be was influenced by Dave Bacon,
quantum computing’s elder clown, who entitled the first chapter of his
own 451-page behemoth “Philosonomicon.” I’m also indebted to Chris Fuchs
and his samizdat , for the idea that a document about quantum mechanics
more than 400 pages long can be worth reading most of the way through. I
began working on the best-known result in this thesis, the quantum lower
bound for the collision problem, during an unforgettable summer at
Caltech.  Leonard Schulman and Ashwin Nayak listened patiently to one
farfetched idea after another, while John Preskill’s weekly group
meetings helped to ensure that the mysteries of quantum mechanics, which
inspired me to tackle the problem in the first place, were never far
from my mind.  Besides Leonard, Ashwin, and John, I’m grateful to Ann
Harvey for putting up with the growing mess in my office.  For the
record, I never once slept in the office; the bedsheet was strictly for
doing math on the floor. I created the infamous Complexity Zoo web site
during a summer at CWI in Amsterdam, a visit enlivened by the presence
of Harry Buhrman, Hein Röhrig, Volker Nannen, Hartmut Klauck, and Troy
Lee.  That summer I also had memorable conversations with David Deutsch
and Stephen Wolfram.  Chapters I , II , and II partly came into being
during a semester at the Hebrew University in Jerusalem, a city where
“Aaron’s sons” were already obsessing about cubits three thousand years
ago.  I thank Avi Wigderson, Dorit Aharonov, Michael Ben-Or, Amnon
Ta-Shma, and Michael Mallin for making that semester a fruitful and
enjoyable one.  I also thank Avi for pointing me to the then-unpublished
results of Ran Raz on which Chapter II is based, and Ran for sharing
those results. A significant chunk of the thesis was written or revised
over two summers at the Perimeter Institute for Theoretical Physics in
Waterloo.  I thank Daniel Gottesman, Lee Smolin, and Ray Laflamme for
welcoming a physics doofus to their institute, someone who thinks the
string theory versus loop quantum gravity debate should be resolved by
looping over all possible strings.  From Marie Ericsson, Rob Spekkens,
and Anthony Valentini I learned that theoretical physicists have a
better social life than theoretical computer scientists, while from Dan
Christensen I learned that complexity and quantum gravity had better
wait before going steady. Several ideas were hatched or incubated during
the yearly QIP conferences; workshops in Toronto, Banff, and Leiden; and
visits to MIT, Los Alamos, and IBM Almaden.  I’m grateful to Howard
Barnum, Andrew Childs, Elham Kashefi, Barbara Terhal, John Watrous, and
many others for productive exchanges on those occasions. Back in
Berkeley, people who enriched my grad-school experience include Neha
Dave, Julia Kempe, Simone Severini, Lawrence Ip, Allison Coates, David
Molnar, Kris Hildrum, Miriam Walker, and Shelly Rosenfeld.  Alex
Fabrikant and Boriska Toth are forgiven for the cruel caricature that
they attached to my dissertation talk announcement, provided they don’t
try anything like that ever again.  The results on one-way communication
in Chapter I benefited greatly from conversations with Oded Regev and
Iordanis Kerenidis, while Andrej Bogdanov kindly supplied the explicit
erasure code for Chapter II .  I wrote Chapter I to answer a question of
Christos Papadimitriou. I did take some actual … courses at Berkeley,
and I’m grateful to John Kubiatowicz, Stuart Russell, Guido
Bacciagaluppi, Richard Karp, and Satish Rao for not failing me in
theirs.  Ironically, the course that most directly influenced this
thesis was Tom Farber’s magnificent short fiction workshop.  A story I
wrote for that workshop dealt with the problem of transtemporal
identity, which got me thinking about hidden-variable interpretations of
quantum mechanics, which led eventually to the collision lower bound.
 No one seems to believe me, but it’s true. The students who took my
“Physics, Philosophy, Pizza” course remain one of my greatest
inspirations.  Though they were mainly undergraduates with liberal arts
backgrounds, they took nothing I said about special relativity or
Gödel’s Theorem on faith.  If I have any confidence today in my teaching
abilities; if I think it possible for students to show up to class, and
to participate eagerly, without the usual carrot-and-stick of grades and
exams; or if I find certain questions, such as how a superposition over
exponentially many ‘could-have-beens’ can collapse to an ‘is,’ too
vertiginous to be pondered only by nerds like me, then those
pizza-eating students are the reason. Now comes the part devoted to the
mist-enshrouded pre-Berkeley years.  My initiation into the wild world
of quantum computing research took place over three summer internships
at Bell Labs: the first with Eric Grosse, the second with Lov Grover,
and the third with Rob Pike.  I thank all three of them for encouraging
me to pursue my interests, even if the payoff was remote and, in Eric’s
case, not even related to why I was hired.  Needless to say, I take no
responsibility for the subsequent crash of Lucent’s stock. As an
undergraduate at Cornell, I was younger than my classmates, invisible to
many of the researchers I admired, and profoundly unsure of whether I
belonged there or had any future in science.  What made the difference
was the unwavering support of one professor, Bart Selman.  Busy as he
was, Bart listened to my harebrained ideas about genetic algorithms for
SAT or quantum chess-playing, invited me to give talks, guided me to the
right graduate programs, and generally treated me like a future
colleague.  As a result, his conviction that I could succeed at research
gradually became my conviction too.  Outside of research, Christine
Chung, Fion Luo, and my Telluride roommate Jason Stockmann helped to
warm the Ithaca winters, Lydia Fakundiny taught me what an essay is, and
Jerry Abrams provided a much-needed boost. Turning the clock back
further, my earliest research foray was a paper on hypertext
organization, written when I was fifteen and spending the year at
Clarkson University’s unique Clarkson School program.  Christopher Lynch
generously agreed to advise the project, and offered invaluable help as
I clumsily learned how to write a C program, prove a problem @xmath
-hard, and conduct a user experiment (one skill I’ve never needed
again!).  I was elated to be trading ideas with a wise and experienced
researcher, only months after I’d escaped from the prison-house of high
school.  Later, the same week the rejection letters were arriving from
colleges, I learned that my first paper had been accepted to SIGIR, the
main information retrieval conference.  I was filled with boundless
gratitude toward the entire scientific community—for struggling, against
the warp of human nature, to judge ideas rather than the personal
backgrounds of their authors.  Eight years later, my gratitude and
amazement are undiminished. Above all, I thank Alex Halderman for a
friendship that’s spanned twelve years and thousands of miles, remaining
as strong today as it was amidst the Intellectualis minimi of Newtown
Junior High School; my brother David for believing in me, and for making
me prouder than he realizes by doing all the things I didn’t; and my
parents for twenty-three years of harping, kvelling, chicken noodle
soup, and never doubting for a Planck time that I’d live up to my
potential—even when I couldn’t, and can’t, share their certainty.

### Chapter \thechapter “Aren’t You Worried That Quantum Computing Won’t
Pan Out?”

For a century now, physicists have been telling us strange things: about
twins who age at different rates, particles that look different when
rotated 360 @xmath , a force that is transmitted by gravitons but is
also the curvature of spacetime, a negative-energy electron sea that
pervades empty space, and strangest of all, “probability waves” that
produce fringes on a screen when you don’t look and don’t when you do.
 Yet ever since I learned to program, I suspected that such things were
all “implementation details” in the source code of Nature, their study
only marginally relevant to forming an accurate picture of reality.
 Physicists, I thought, would eventually realize that the state of the
universe can be represented by a finite string of bits.  These bits
would be the “pixels” of space, creating the illusion of continuity on a
large scale much as a computer screen does.  As time passed, the bits
would be updated according to simple rules.  The specific form of these
rules was of no great consequence---since according to the Extended
Church-Turing Thesis, any sufficiently complicated rules could simulate
any other rules with reasonable efficiency. ¹ ¹ 1 Here “extended” refers
to the efficiency requirement, which was not mentioned in the original
Church-Turing Thesis.  Also, I am simply using the standard terminology,
sidestepping the issue of whether Church and Turing themselves intended
to make a claim about physical reality. So apart from practical
considerations, why worry about Maxwell’s equations, or Lorentz
invariance, or even mass and energy, if the most fundamental aspects of
our universe already occur in Conway’s Game of Life (see Figure 1 )?

Then I heard about Shor’s algorithm [ 219 ] for factoring integers in
polynomial time on a quantum computer.  Then as now, many people saw
quantum computing as at best a speculative diversion from the
“real work” of computer science.  Why devote one’s research career to a
type of computer that might never see application within one’s lifetime,
that faces daunting practical obstacles such as decoherence, and whose
most publicized success to date has been the confirmation that, with
high probability, @xmath [ 234 ] ?  Ironically, I might have agreed with
this view, had I not taken the Extended Church-Turing Thesis so
seriously as a claim about reality.  For Shor’s algorithm forces us to
accept that, under widely-believed assumptions, that Thesis conflicts
with the experimentally-tested rules of quantum mechanics as we
currently understand them.  Either the Extended Church-Turing Thesis is
false, or quantum mechanics must be modified, or the factoring problem
is solvable in classical polynomial time.  All three possibilities seem
like wild, crackpot speculations—but at least one of them is true!

The above conundrum is what underlies my interest in quantum computing,
far more than any possible application.  Part of the reason is that I am
neither greedy, nefarious, nor number-theoretically curious enough ever
to have hungered for the factors of a @xmath -digit integer.  I do think
that quantum computers would have benign uses, the most important one
being the simulation of quantum physics and chemistry. ² ² 2 Followed
closely by Recursive Fourier Sampling, parity in @xmath queries, and
efficiently deciding whether a graph is a scorpion. Also, as transistors
approach the atomic scale, ideas from quantum computing are likely to
become pertinent even for classical computer design.  But none of this
quickens my pulse.

For me, quantum computing matters because it combines two of the great
mysteries bequeathed to us by the twentieth century: the nature of
quantum mechanics, and the ultimate limits of computation.  It would be
astonishing if such an elemental connection between these mysteries shed
no new light on either of them.  And indeed, there is already a growing
list of examples [ 9 , 22 , 151 ] —we will see several of them in this
thesis—in which ideas from quantum computing have led to new results
about classical computation.  This should not be surprising: after all,
many celebrated results in computer science involve only deterministic
computation, yet it is hard to imagine how anyone could have proved them
had computer scientists not long ago “taken randomness aboard.” ³ ³ 3 A
few examples are primality testing in @xmath [ 17 ] , undirected
connectivity in @xmath [ 202 ] , and inapproximability of 3-SAT unless
@xmath [ 224 ] . Likewise, taking quantum mechanics aboard could lead to
a new, more general perspective from which to revisit the central
questions of computational complexity theory.

The other direction, though, is the one that intrigues me even more.  In
my view, quantum computing has brought us slightly closer to the elusive
Beast that devours Bohmians for breakfast, Copenhagenists for lunch, and
a linear combination of many-worlders and consistent historians for
dinner—the Beast that tramples popularizers, brushes off arXiv preprints
like fleas, and snorts at the word “decoherence”—the Beast so fearsome
that physicists since Bohr and Heisenberg have tried to argue it away,
as if semantics could banish its unitary jaws and complex-valued tusks.
 But no, the Beast is there whenever you aren’t paying attention,
following all possible paths in superposition.  Look, and suddenly the
Beast is gone.  But what does it even mean to look?  If you’re governed
by the same physical laws as everything else, then why don’t you evolve
in superposition too, perhaps until someone else looks at you and
thereby ‘collapses’ you?  But then who collapses whom first?  Or if you
never collapse, then what determines what you -you, rather than the
superposition of you’s, experience?  Such is the riddle of the Beast, ⁴
⁴ 4 Philosophers call the riddle of the Beast the “measurement
problem,” which sounds less like something that should cause insomnia
and delirious raving in all who have understood it.  Basically, the
problem is to reconcile a picture of the world in which “everything
happens simultaneously” with the fact that you (or at least I!) have a
sequence of definite experiences. and it has filled many with terror and
awe.

The contribution of quantum computing, I think, has been to show that
the real nature of the Beast lies in its exponentiality .  It is not
just two, three, or a thousand states held in ghostly superposition that
quantum mechanics is talking about, but an astronomical multitude, and
these states could in principle reveal their presence to us by factoring
a five-thousand-digit number.  Much more than even Schrödinger’s cat or
the Bell inequalities, this particular discovery ups the ante—forcing us
either to swallow the full quantum brew, or to stop saying that we
believe in it.  Of course, this is part of the reason why Richard
Feynman [ 108 ] and David Deutsch [ 90 ] introduced quantum computing in
the first place, and why Deutsch, in his defense of the many-worlds
interpretation, issues a famous challenge to skeptics [ 92 , p. 217] :
if parallel universes are not physically real, then explain how Shor’s
algorithm works .

Unlike Deutsch, here I will not use quantum computing to defend the
many-worlds interpretation, or any of its competitors for that matter.
 Roughly speaking, I agree with every interpretation of quantum
mechanics to the extent that it acknowledges the Beast’s existence, and
disagree to the extent that it claims to have caged the Beast.  I would
adopt the same attitude in computer science, if instead of freely
admitting (for example) that @xmath versus @xmath is an open problem,
researchers had split into “equalist,” “unequalist,” and
“undecidabilist” schools of interpretation, with others arguing that the
whole problem is meaningless and should therefore be abandoned.

Instead, in this thesis I will show how adopting a computer science
perspective can lead us to ask better questions —nontrivial but
answerable questions, which put old mysteries in a new light even when
they fall short of solving them.  Let me give an example.  One of the
most contentious questions about quantum mechanics is whether the
individual components of a wavefunction should be thought of as “really
there” or as “mere potentialities.”  When we don our computer scientist
goggles, this question morphs into a different one: what resources are
needed to make a particular component of the wavefunction manifest?
Arguably the two questions are related, since something “real” ought to
take less work to manifest than something “potential.”  For example,
this thesis gradually became more real as less of it remained to be
written.

Concretely, suppose our wavefunction has @xmath components, all with
equal amplitude.  Suppose also that we have a procedure to recognize a
particular component @xmath (i.e., a function @xmath such that @xmath
and @xmath for all @xmath ).  Then how often must we apply this
procedure before we make @xmath manifest; that is, observable with
probability close to @xmath ?  Bennett, Bernstein, Brassard, and
Vazirani [ 51 ] showed that @xmath applications are necessary, even if
@xmath can be applied to all @xmath components in superposition.  Later
Grover [ 139 ] showed that @xmath applications are also sufficient.  So
if we imagine a spectrum with “really there” ( @xmath application) on
one end, and “mere potentiality” ( @xmath applications) on the other,
then we have landed somewhere in between: closer to the “real” end on an
absolute scale, but closer to the “potential” end on the polynomial
versus exponential scale that is more natural for computer science.

Of course, we should be wary of drawing grand conclusions from a single
data point.  So in this thesis, I will imagine a hypothetical resident
of Conway’s Game of Life, who arrives in our physical universe on a
computational complexity safari—wanting to know exactly which intuitions
to keep and which to discard regarding the limits of efficient
computation.  Many popular science writers would tell our visitor to
throw all classical intuitions out the window, while quantum computing
skeptics would urge retaining them all.  These positions are actually
two sides of the same coin, since the belief that a quantum computer
would necessitate the first is what generally leads to the second.  I
will show, however, that neither position is justified.  Based on what
we know today, there really is a Beast, but it usually conceals its
exponential underbelly.

I’ll provide only one example from the thesis here; the rest are
summarized in Chapter Limits on Efficient Computation in the Physical
World .  Suppose we are given a procedure that computes a two-to-one
function @xmath , and want to find distinct inputs @xmath and @xmath
such that @xmath .  In this case, by simply preparing a uniform
superposition over all inputs to @xmath , applying the procedure, and
then measuring its result, we can produce a state of the form @xmath ,
for some @xmath and @xmath such that @xmath .  The only problem is that
if we measure this state, then we see either @xmath or @xmath , but not
both.  The task, in other words, is no longer to find a needle in a
haystack, but just to find two needles in an otherwise empty barn!
 Nevertheless, the collision lower bound in Chapter I will show that, if
there are @xmath inputs to @xmath , then any quantum algorithm for this
problem must apply the procedure for @xmath at least @xmath times.
 Omitting technical details, this lower bound can be interpreted in at
least seven ways:

1.  Quantum computers need exponential time even to compute certain
    global properties of a function, not just local properties such as
    whether there is an @xmath with @xmath .

2.  Simon’s algorithm [ 220 ] , and the period-finding core of Shor’s
    algorithm [ 219 ] , cannot be generalized to functions with no
    periodicity or other special structure.

3.  Any “brute-force” quantum algorithm needs exponential time, not just
    for @xmath -complete problems, but for many structured problems such
    as Graph Isomorphism, approximating the shortest vector in a
    lattice, and finding collisions in cryptographic hash functions.

4.  It is unlikely that all problems having “statistical zero-knowledge
    proofs” can be efficiently solved on a quantum computer.

5.  Within the setting of a collision algorithm, the components @xmath
    and @xmath in the state @xmath should be thought of as more
    “potentially” than “actually” there, it being impossible to extract
    information about both of them in a reasonable amount of time.

6.  The ability to map @xmath to @xmath , “uncomputing” @xmath in the
    process, can be exponentially more powerful than the ability to map
    @xmath to @xmath .

7.  In hidden-variable interpretations of quantum mechanics, the ability
    to sample the entire history of a hidden variable would yield even
    more power than standard quantum computing.

Interpretations (5), (6), and (7) are examples of what I mean by putting
old mysteries in a new light.  We are not brought face-to-face with the
Beast, but at least we have fresh footprints and droppings.

Well then. Am I worried that quantum computing won’t pan out?  My usual
answer is that I’d be thrilled to know it will never pan out, since this
would entail the discovery of a lifetime, that quantum mechanics is
false.  But this is not what the questioner has in mind.  What if
quantum mechanics holds up, but building a useful quantum computer turns
out to be so difficult and expensive that the world ends before anyone
succeeds?  The questioner is usually a classical theoretical computer
scientist, someone who is not known to worry excessively that the world
will end before @xmath exceeds @xmath .  Still, it would be nice to see
nontrivial quantum computers in my lifetime, and while I’m cautiously
optimistic, I’ll admit to being slightly worried that I won’t.  But when
faced with the evidence that one was born into a universe profoundly
unlike Conway’s—indeed, that one is living one’s life on the back of a
mysterious, exponential Beast comprising everything that ever could have
happened—what is one to do?  “Move right along… nothing to see here…”

### Chapter \thechapter Overview

  “Let a computer smear—with the right kind of quantum randomness—and
  you create, in effect, a ‘parallel’ machine with an astronomical
  number of processors … All you have to do is be sure that when you
  collapse the system, you choose the version that happened to find the
  needle in the mathematical haystack.”
  —From Quarantine [ 103 ] , a 1992 science-fiction novel by Greg Egan

Many of the deepest discoveries of science are limitations : for
example, no superluminal signalling, no perpetual-motion machines, and
no complete axiomatization for arithmetic.  This thesis is broadly
concerned with limitations on what can efficiently be computed in the
physical world.  The word “quantum” is absent from the title, in order
to emphasize that the focus on quantum computing is not an arbitrary
choice, but rather an inevitable result of taking our current physical
theories seriously.  The technical contributions of the thesis are
divided into two parts, according to whether they accept the quantum
computing model as given and study its fundamental limitations; or
question, defend, or go beyond that model in some way.  Before launching
into a detailed overview of the contributions, let me make some
preliminary remarks.

Since the early twentieth century, two communities---physicists ⁵ ⁵ 5 As
in Saul Steinberg’s famous New Yorker world map, in which 9 @xmath
Avenue and the Hudson River take up more space than Japan and China,
from my perspective chemists, engineers, and even mathematicians who
know what a gauge field is are all “physicists.” and computer
scientists—have been asking some of the deepest questions ever asked in
almost total intellectual isolation from each other.  The great joy of
quantum computing research is that it brings these communities together.
 The trouble was initially that, although each community would nod
politely during the other’s talks, eventually it would come out that the
physicists thought @xmath stood for “Non Polynomial,” and the computer
scientists had no idea what a Hamiltonian was.  Thankfully, the
situation has improved a lot—but my hope is that it improves further
still, to the point where computer scientists have internalized the
problems faced by physics and vice versa.  For this reason, I have
worked hard to make the thesis as accessible as possible to both
communities.  Thus, Chapter Limits on Efficient Computation in the
Physical World provides a “complexity theory cheat sheet” that defines
@xmath , @xmath , @xmath , and other computational complexity classes
that appear in the thesis; and that explains oracles and other important
concepts.  Then Chapter Limits on Efficient Computation in the Physical
World presents the quantum model of computation with no reference to the
underlying physics, before moving on to fancier notions such as density
matrices, trace distance, and separability.  Neither chapter is a
rigorous introduction to its subject; for that there are fine
textbooks—such as Papadimitriou’s Computational Complexity [ 188 ] and
Nielsen and Chuang’s Quantum Computation and Quantum Information [ 182 ]
—as well as course lecture notes available on the web.  Depending on
your background, you might want to skip to Chapters Limits on Efficient
Computation in the Physical World or Limits on Efficient Computation in
the Physical World before continuing any further, or you might want to
skip past these chapters entirely.

Even the most irredeemably classical reader should take heart: of the
@xmath proofs in the thesis, @xmath do not contain a single ket symbol.
⁶ ⁶ 6 To be honest, a few of those do contain density matrices—or the
theorem contains ket symbols, but not the proof . Many of the proofs can
be understood by simply accepting certain facts about quantum computing
on faith, such as Ambainis’s ⁷ ⁷ 7 Style manuals disagree about whether
Ambainis’ or Ambainis’s is preferable, but one referee asked me to
follow the latter rule with the following deadpan remark: “Exceptions to
the rule generally involve religiously significant individuals, e.g.,
‘Jesus’ lower-bound method.’ ” adversary theorem [ 27 ] or Beals et
al.’s polynomial lemma [ 45 ] .  On the other hand, one does run the
risk that after one understands the proofs, ket symbols will seem less
frightening than before.

The results in the thesis have all previously appeared in published
papers or preprints [ 1 , 2 , 4 , 5 , 7 , 8 , 9 , 10 , 11 , 13 ] , with
the exception of the quantum computing based proof that @xmath is closed
under intersection in Chapter II .  I thank Andris Ambainis for allowing
me to include our joint results from [ 13 ] on quantum search of spatial
regions.  Results of mine that do not appear in the thesis include those
on Boolean function query properties [ 3 ] , stabilizer circuits [ 14 ]
(joint work with Daniel Gottesman), and agreement complexity [ 6 ] .

In writing the thesis, one of the toughest choices I faced was whether
to refer to myself as ‘I’ or ‘we.’  Sometimes a personal voice seemed
more appropriate, and sometimes the Voice of Scientific Truth, but I
wanted to be consistent.  Readers can decide whether I chose humbly or
arrogantly.

#### 1 Limitations of Quantum Computers

Part I studies the fundamental limitations of quantum computers within
the usual model for them.  With the exception of Chapter I on quantum
advice, the contributions of Part I all deal with black-box or query
complexity, meaning that one counts only the number of queries to an
“oracle,” not the number of computational steps.  Of course, the queries
can be made in quantum superposition.  In Chapter I , I explain the
quantum black-box model, then offer a detailed justification for its
relevance to understanding the limits of quantum computers.  Some
computer scientists say that black-box results should not be taken too
seriously; but I argue that, within quantum computing, they are not
taken seriously enough.

What follows is a (relatively) nontechnical overview of Chapters I to I
, which contain the results of Part I .  Afterwards, Chapter I
summarizes the conceptual lessons that I believe can be drawn from those
results.

##### 1.1 The Collision Problem

Chapter I presents my lower bound on the quantum query complexity of the
collision problem.  Given a function @xmath from @xmath to @xmath (where
@xmath is even), the collision problem is to decide whether @xmath is
one-to-one or two-to-one, promised that one of these is the case.  Here
the only way to learn about @xmath is to call a procedure that computes
@xmath given @xmath .  Clearly, any deterministic classical algorithm
needs to call the procedure @xmath times to solve the problem.  On the
other hand, a randomized algorithm can exploit the “birthday paradox”:
only @xmath people have to enter a room before there’s a @xmath chance
that two of them share the same birthday, since what matters is the
number of pairs of people.  Similarly, if @xmath is two-to-one, and an
algorithm queries @xmath at @xmath uniform random locations, then with
constant probability it will find two locations @xmath such that @xmath
, thereby establishing that @xmath is two-to-one.  This bound is easily
seen to be tight, meaning that the bounded-error randomized query
complexity of the collision problem is @xmath .

What about the quantum complexity?  In 1997, Brassard, Høyer, and Tapp [
68 ] gave a quantum algorithm that uses only @xmath queries.  The
algorithm is simple to describe: in the first phase, query @xmath
classically at @xmath randomly chosen locations.  In the second phase,
choose @xmath random locations, and run Grover’s algorithm on those
locations, considering each location @xmath as “marked” if @xmath for
some @xmath that was queried in the first phase.  Notice that both
phases use order @xmath queries, and that the total number of
comparisons is @xmath .  So, like its randomized counterpart, the
quantum algorithm finds a collision with constant probability if @xmath
is two-to-one.

What I show in Chapter I is that any quantum algorithm for the collision
problem needs @xmath queries.  Previously, no lower bound better than
the trivial @xmath was known.  I also show a lower bound of @xmath for
the following set comparison problem : given oracle access to injective
functions @xmath and @xmath , decide whether

  -- -------- --
     @xmath   
  -- -------- --

has at least @xmath elements or exactly @xmath elements, promised that
one of these is the case.  The set comparison problem is similar to the
collision problem, except that it lacks permutation symmetry, making it
harder to prove a lower bound.  My results for these problems have been
improved, simplified, and generalized by Shi [ 218 ] , Kutin [ 161 ] ,
Ambainis [ 27 ] , and Midrijanis [ 176 ] .

The implications of these results were already discussed in Chapter
Limits on Efficient Computation in the Physical World : for example,
they demonstrate that a “brute-force” approach will never yield
efficient quantum algorithms for the Graph Isomorphism, Approximate
Shortest Vector, or Nonabelian Hidden Subgroup problems; suggest that
there could be cryptographic hash functions secure against quantum
attack; and imply that there exists an oracle relative to which @xmath ,
where @xmath is the class of problems having statistical zero-knowledge
proof protocols, and @xmath is quantum polynomial time.

Both the original lower bounds and the subsequent improvements are based
on the polynomial method , which was introduced by Nisan and Szegedy [
184 ] , and first used to prove quantum lower bounds by Beals, Buhrman,
Cleve, Mosca, and de Wolf [ 45 ] .  In that method, given a quantum
algorithm that makes @xmath queries to an oracle @xmath , we first
represent the algorithm’s acceptance probability by a multilinear
polynomial @xmath of degree at most @xmath .  We then use results from a
well-developed area of mathematics called approximation theory to show a
lower bound on the degree of @xmath .  This in turn implies a lower
bound on @xmath .

In order to apply the polynomial method to the collision problem, first
I extend the collision problem’s domain from one-to-one and two-to-one
functions to @xmath -to-one functions for larger values of @xmath .
 Next I replace the multivariate polynomial @xmath by a related
univariate polynomial @xmath whose degree is easier to lower-bound.  The
latter step is the real “magic” of the proof; I still have no good
intuitive explanation for why it works.

The polynomial method is one of two principal methods that we have for
proving lower bounds on quantum query complexity.  The other is
Ambainis’s quantum adversary method [ 27 ] , which can be seen as a
far-reaching generalization of the “hybrid argument” that Bennett,
Bernstein, Brassard, and Vazirani [ 51 ] introduced in 1994 to show that
a quantum computer needs @xmath queries to search an unordered database
of size @xmath for a marked item.  In the adversary method, we consider
a bipartite quantum state, in which one part consists of a superposition
over possible inputs, and the other part consists of a quantum
algorithm’s work space.  We then upper-bound how much the entanglement
between the two parts can increase as the result of a single query.
 This in turn implies a lower bound on the number of queries, since the
two parts must be highly entangled by the end.  The adversary method is
more intrinsically “quantum” than the polynomial method; and as Ambainis
[ 27 ] showed, it is also applicable to a wider range of problems,
including those (such as game-tree search) that lack permutation
symmetry.  Ambainis even gave problems for which the adversary method
provably yields a better lower bound than the polynomial method [ 28 ] .
 It is ironic, then, that Ambainis’s original goal in developing the
adversary method was to prove a lower bound for the collision problem;
and in this one instance, the polynomial method succeeded while the
adversary method failed.

##### 1.2 Local Search

In Chapters I , I , and I , however, the adversary method gets its
revenge.  Chapter I deals with the local search problem: given an
undirected graph @xmath and a black-box function @xmath , find a local
minimum of @xmath —that is, a vertex @xmath such that @xmath for all
neighbors @xmath of @xmath .  The graph @xmath is known in advance, so
the complexity measure is just the number of queries to @xmath .  This
problem is central for understanding the performance of the quantum
adiabatic algorithm, as well as classical algorithms such as simulated
annealing.  If @xmath is the Boolean hypercube @xmath , then previously
Llewellyn, Tovey, and Trick [ 169 ] had shown that any deterministic
algorithm needs @xmath queries to find a local minimum; and Aldous [ 24
] had shown that any randomized algorithm needs @xmath queries.  What I
show is that any quantum algorithm needs @xmath queries.  This is the
first nontrivial quantum lower bound for any local search problem; and
it implies that the complexity class @xmath (or “Polynomial Local
Search”), defined by Johnson, Papadimitriou, and Yannakakis [ 149 ] , is
not in quantum polynomial time relative to an oracle.

What will be more surprising to classical computer scientists is that my
proof technique, based on the quantum adversary method, also yields new
classical lower bounds for local search.  In particular, I prove a
classical analogue of Ambainis’s quantum adversary theorem, and show
that it implies randomized lower bounds up to quadratically better than
the corresponding quantum lower bounds.  I then apply my theorem to show
that any randomized algorithm needs @xmath queries to find a local
minimum of a function @xmath .  Not only does this improve on Aldous’s
@xmath lower bound, bringing us closer to the known upper bound of
@xmath ; but it does so in a simpler way that does not depend on random
walk analysis.  In addition, I show the first randomized or quantum
lower bounds for finding a local minimum on a cube of constant dimension
@xmath or greater.  Along with recent work by Bar-Yossef, Jayram, and
Kerenidis [ 43 ] and by Aharonov and Regev [ 22 ] , these results
provide one of the earliest examples of how quantum ideas can help to
resolve classical open problems.  As I will discuss in Chapter I , my
results on local search have subsequently been improved by Santha and
Szegedy [ 211 ] and by Ambainis [ 25 ] .

##### 1.3 Quantum Certificate Complexity

Chapters I and I continue to explore the power of Ambainis’s lower bound
method and the limitations of quantum computers.  Chapter I is inspired
by the following theorem of Beals et al. [ 45 ] : if @xmath is a total
Boolean function, then @xmath , where @xmath is the deterministic
classical query complexity of @xmath , and @xmath is the bounded-error
quantum query complexity. ⁸ ⁸ 8 The subscript ‘ @xmath ’ means that the
error is two-sided. This theorem is noteworthy for two reasons: first,
because it gives a case where quantum computers provide only a
polynomial speedup, in contrast to the exponential speedup of Shor’s
algorithm; and second, because the exponent of @xmath seems so
arbitrary.  The largest separation we know of is quadratic, and is
achieved by the @xmath function on @xmath bits: @xmath , but @xmath
because of Grover’s search algorithm.  It is a longstanding open
question whether this separation is optimal.  In Chapter I , I make the
best progress so far toward showing that it is.  In particular I prove
that

  -- -------- --
     @xmath   
  -- -------- --

for all total Boolean functions @xmath .  Here @xmath is the
bounded-error randomized query complexity of @xmath , and @xmath is the
zero-error quantum query complexity.  To prove this result, I introduce
two new query complexity measures of independent interest: the
randomized certificate complexity @xmath and the quantum certificate
complexity @xmath .  Using Ambainis’s adversary method together with the
minimax theorem, I relate these measures exactly to one another, showing
that @xmath .  Then, using the polynomial method, I show that @xmath for
all total Boolean @xmath , which implies the above result since @xmath .
 Chapter I contains several other results of interest to researchers
studying query complexity, such as a superquadratic gap between @xmath
and the “ordinary” certificate complexity @xmath .  But the main message
is the unexpected versatility of our quantum lower bound methods: we see
the first use of the adversary method to prove something about all total
functions, not just a specific function; the first use of both the
adversary and the polynomial methods at different points in a proof; and
the first combination of the adversary method with a linear programming
duality argument.

##### 1.4 The Need to Uncompute

Next, Chapter I illustrates how “the need to uncompute” imposes a
fundamental limit on efficient quantum computation.  Like a classical
algorithm, a quantum algorithm can solve a problem recursively by
calling itself as a subroutine.  When this is done, though, the quantum
algorithm typically needs to call itself twice for each subproblem to be
solved.  The second call’s purpose is to “uncompute” garbage left over
by the first call, and thereby enable interference between different
branches of the computation.  In a seminal paper, Bennett [ 52 ] argued
⁹ ⁹ 9 Bennett’s paper dealt with classical reversible computation, but
this comment applies equally well to quantum computation. that
uncomputation increases an algorithm’s running time by only a factor of
@xmath .  Yet in the recursive setting, the increase is by a factor of
@xmath , where @xmath is the depth of recursion.  Is there any way to
avoid this exponential blowup?

To make the question more concrete, Chapter I focuses on the recursive
Fourier sampling problem of Bernstein and Vazirani [ 55 ] .  This is a
problem that involves @xmath levels of recursion, and that takes a
Boolean function @xmath as a parameter.  What Bernstein and Vazirani
showed is that for some choices of @xmath , any classical randomized
algorithm needs @xmath queries to solve the problem.  By contrast,
@xmath queries always suffice for a quantum algorithm.  The question I
ask is whether a quantum algorithm could get by with fewer than @xmath
queries, even while the classical complexity remains large.  I show that
the answer is no: for every @xmath , either Ambainis’s adversary method
yields a @xmath lower bound on the quantum query complexity, or else the
classical and quantum query complexities are both @xmath .  The lower
bound proof introduces a new parameter of Boolean functions called the
“nonparity coefficient,” which might be of independent interest.

##### 1.5 Limitations of Quantum Advice

Chapter I broadens the scope of Part I , to include the limitations of
quantum computers equipped with “quantum advice states.”  Ordinarily, we
assume that a quantum computer starts out in the standard “all- @xmath
” state, @xmath .  But it is perfectly sensible to drop that assumption,
and consider the effects of other initial states.  Most of the work
doing so has concentrated on whether universal quantum computing is
still possible with highly mixed initial states (see [ 34 , 214 ] for
example).  But an equally interesting question is whether there are
states that could take exponential time to prepare, but that would carry
us far beyond the complexity-theoretic confines of @xmath were they
given to us by a wizard.  For even if quantum mechanics is universally
valid, we do not really know whether such states exist in Nature!

Let @xmath be the class of problems solvable in quantum polynomial time,
with the help of a polynomial-size “quantum advice state” @xmath that
depends only on the input length @xmath but that can otherwise be
arbitrary.  Then the question is whether @xmath , where @xmath is the
class of the problems solvable in quantum polynomial time using a
polynomial-size classical advice string. ¹⁰ ¹⁰ 10 For clearly @xmath and
@xmath both contain uncomputable problems not in @xmath , such as
whether the @xmath Turing machine halts. As usual, we could try to prove
an oracle separation.  But why can’t we show that quantum advice is more
powerful than classical advice, with no oracle?  Also, could quantum
advice be used (for example) to solve @xmath -complete problems in
polynomial time?

The results in Chapter I place strong limitations on the power of
quantum advice.  First, I show that @xmath is contained in a classical
complexity class called @xmath .  This means (roughly) that quantum
advice can always be replaced by classical advice, provided we’re
willing to use exponentially more computation time.  It also means that
we could not prove @xmath without showing that @xmath does not have
polynomial-size circuits, which is believed to be an extraordinarily
hard problem.  To prove this result, I imagine that the advice state
@xmath is sent to the @xmath machine by a benevolent “advisor,” through
a one-way quantum communication channel.  I then give a novel protocol
for simulating that quantum channel using a classical channel.  Besides
showing that @xmath , the simulation protocol also implies that for all
Boolean functions @xmath (partial or total), we have @xmath , where
@xmath is the deterministic one-way communication complexity of @xmath ,
and @xmath is the bounded-error quantum one-way communication
complexity.  This can be considered a generalization of the “dense
quantum coding” lower bound due to Ambainis, Nayak, Ta-Shma, and
Vazirani [ 32 ] .

The second result in Chapter I is that there exists an oracle relative
to which @xmath .  This extends the result of Bennett et al. [ 51 ] that
there exists an oracle relative to which @xmath , to handle quantum
advice.  Intuitively, even though the quantum state @xmath could in some
sense encode the solutions to exponentially many @xmath search problems,
only a miniscule fraction of that information could be extracted by
measuring the advice, at least in the black-box setting that we
understand today.

The proof of the oracle separation relies on another result of
independent interest: a direct product theorem for quantum search.  This
theorem says that given an unordered database with @xmath items, @xmath
of which are marked, any quantum algorithm that makes @xmath queries ¹¹
¹¹ 11 Subsequently Klauck, Špalek, and de Wolf [ 156 ] improved this to
@xmath queries, which is tight. has probability at most @xmath of
finding all @xmath of the marked items.  In other words, there are no
“magical” correlations by which success in finding one marked item leads
to success in finding the others.  This might seem intuitively obvious,
but it does not follow from the @xmath lower bound for Grover search, or
any other previous quantum lower bound for that matter.  Previously,
Klauck [ 155 ] had given an incorrect proof of a direct product theorem,
based on Bennett et al.’s hybrid method.  I give the first correct proof
by using the polynomial method, together with an inequality dealing with
higher derivatives of polynomials due to V. A. Markov, the younger
brother of A. A. Markov.

The third result in Chapter I is a new trace distance method for proving
lower bounds on quantum one-way communication complexity.  Using this
method, I obtain optimal quantum lower bounds for two problems of
Ambainis, for which no nontrivial lower bounds were previously known
even for classical randomized protocols.

#### 2 Models and Reality

This thesis is concerned with the limits of efficient computation in
Nature.  It is not obvious that these coincide with the limits of the
quantum computing model.  Thus, Part II studies the relationship of the
quantum computing model to physical reality.  Of course, this is too
grand a topic for any thesis, even a thesis as long as this one.  I
therefore focus on three questions that particularly interest me.
 First, how should we understand the arguments of “extreme” skeptics,
that quantum computing is impossible not only in practice but also in
principle?  Second, what are the implications for quantum computing if
we recognize that the speed of light is finite, and that according to
widely-accepted principles, a bounded region of space can store only a
finite amount of information?  And third, are there reasonable changes
to the quantum computing model that make it even more powerful, and if
so, how much more powerful do they make it?  Chapters II to II address
these questions from various angles; then Chapter II summarizes.

##### 2.1 Skepticism of Quantum Computing

Chapter II examines the arguments of skeptics who think that large-scale
quantum computing is impossible for a fundamental physical reason.  I
first briefly consider the arguments of Leonid Levin and other computer
scientists, that quantum computing is analogous to “extravagant” models
of computation such as unit-cost arithmetic, and should be rejected on
essentially the same grounds.  My response emphasizes the need to
grapple with the actual evidence for quantum mechanics, and to propose
an alternative picture of the world that is compatible with that
evidence but in which quantum computing is impossible.  The bulk of the
chapter, though, deals with Stephen Wolfram’s A New Kind of Science [
246 ] , and in particular with one of that book’s most surprising
claims: that a deterministic cellular-automaton picture of the world is
compatible with the so-called Bell inequality violations demonstrating
the effects of quantum entanglement.  To achieve compatibility, Wolfram
posits “long-range threads” between spacelike-separated points.  I
explain in detail why this thread proposal violates Wolfram’s own
desiderata of relativistic and causal invariance.  Nothing in Chapter II
is very original technically, but it seems worthwhile to spell out what
a scientific argument against quantum computing would have to
accomplish, and why the existing arguments fail.

##### 2.2 Complexity Theory of Quantum States

Chapter II continues the train of thought begun in Chapter II , except
that now the focus is more technical.  I search for a natural Sure/Shor
separator : a set of quantum states that can account for all experiments
performed to date, but that does not contain the states arising in
Shor’s factoring algorithm.  In my view, quantum computing skeptics
would strengthen their case by proposing specific examples of Sure/Shor
separators, since they could then offer testable hypotheses about where
the assumptions of the quantum computing model break down (if not how
they break down).  So why am I doing the skeptics’ work for them?
 Several people have wrongly inferred from this that I too am a skeptic!
 My goal, rather, is to illustrate what a scientific debate about the
possibility of quantum computing might look like.

Most of Chapter II deals with a candidate Sure/Shor separator that I
call tree states .  Any @xmath -qubit pure state @xmath can be
represented by a tree, in which each leaf is labeled by @xmath or @xmath
, and each non-leaf vertex is labeled by either a linear combination or
a tensor product of its subtrees.  Then the tree size of @xmath is just
the minimum number of vertices in such a tree, and a “tree state” is an
infinite family of states whose tree size is bounded by a polynomial in
@xmath .  The idea is to keep a central axiom of quantum mechanics—that
if @xmath and @xmath are possible states, so are @xmath and @xmath —but
to limit oneself to polynomially many applications of the axiom.

The main results are superpolynomial lower bounds on tree size for
explicit families of quantum states.  Using a recent lower bound on
multilinear formula size due to Raz [ 195 , 196 ] , I show that many
states arising in quantum error correction (for example, states based on
binary linear erasure codes) have tree size @xmath .  I show the same
for the states arising in Shor’s algorithm, assuming a number-theoretic
conjecture.  Therefore, I argue, by demonstrating such states in the lab
on a large number of qubits, experimentalists could weaken ¹² ¹² 12
Since tree size is an asymptotic notion (and for other reasons discussed
in Chapter II ), strictly speaking experimentalists could never refute
the hypothesis—just push it beyond all bounds of plausibility. the
hypothesis that all states in Nature are tree states.

Unfortunately, while I conjecture that the actual tree sizes are
exponential, Raz’s method is currently only able to show lower bounds of
the form @xmath .  On the other hand, I do show exponential lower bounds
under a restriction, called “manifest orthogonality,” on the allowed
linear combinations of states.

More broadly, Chapter II develops a complexity classification of quantum
states, and—treating that classification as a subject in its own
right—proves many basic results about it.  To give a few examples: if a
quantum computer is restricted to being in a tree state at every time
step, then it can be simulated in the third level of polynomial
hierarchy @xmath .  A random state cannot even be approximated by a
state with subexponential tree size.  Any “orthogonal tree state” can be
prepared by a polynomial-size quantum circuit.  Collapses of quantum
state classes would imply collapses of ordinary complexity classes, and
vice versa.  Many of these results involve unexpected connections
between quantum computing and classical circuit complexity.  For this
reason, I think that the “complexity theory of quantum states” has an
intrinsic computer-science motivation, besides its possible role in
making debates about quantum mechanics’ range of validity less
philosophical and more scientific.

##### 2.3 Quantum Search of Spatial Regions

A basic result in classical computer science says that Turing machines
are polynomially equivalent to random-access machines.  In other words,
we can ignore the fact that the speed of light is finite for complexity
purposes, so long as we only care about polynomial equivalence.  It is
easy to see that the same is true for quantum computing.  Yet one of the
two main quantum algorithms, Grover’s algorithm, provides only a
polynomial speedup. ¹³ ¹³ 13 If Grover’s algorithm is applied to a
combinatorial search space of size @xmath , then the speedup is by a
factor of @xmath —but in this case the speedup is only conjectured, not
proven. So, does this speedup disappear if we consider relativity as
well as quantum mechanics?

More concretely, suppose a “quantum robot” is searching a 2-D grid of
size @xmath for a single marked item.  The robot can enter a
superposition of grid locations, but moving from one location to an
adjacent one takes one time step.  How many steps are needed to find the
marked item?  If Grover’s algorithm is implemented naïvely, the answer
is order @xmath —since each of the @xmath Grover iterations takes @xmath
steps, just to move the robot across the grid and back.  This yields no
improvement over classical search.  Benioff [ 50 ] noticed this defect
of Grover’s algorithm as applied to a physical database, but failed to
raise the question of whether or not a faster algorithm exists.

Sadly, I was unable to prove a lower bound showing that the
naïve algorithm is optimal.  But in joint work with Andris Ambainis, we
did the next best thing: we proved the impossibility of proving a lower
bound, or to put it crudely, gave an algorithm.  In particular, Chapter
II shows how to search a @xmath grid for a unique marked vertex in only
@xmath steps, by using a carefully-optimized recursive Grover search.
 It also shows how to search a @xmath -dimensional hypercube in @xmath
steps for @xmath .  The latter result has an unexpected implication:
namely, that the quantum communication complexity of the disjointness
function is @xmath .  This matches a lower bound of Razborov [ 199 ] ,
and improves previous upper bounds due to Buhrman, Cleve, and Wigderson
[ 76 ] and Høyer and de Wolf [ 146 ] .

Chapter II also generalizes our search algorithm to handle multiple
marked items, as well as graphs that are not hypercubes but have
sufficiently good expansion properties.  More broadly, the
chapter develops a new model of quantum query complexity on graphs , and
proves basic facts about that model, such as lower bounds for search on
“starfish” graphs.  Of particular interest to physicists will be Section
47 , which relates our results to fundamental limits on information
processing imposed by the holographic principle.  For example, we can
give an approximate answer to the following question: assuming a
positive cosmological constant @xmath , and assuming the only
constraints (besides quantum mechanics) are the speed of light and the
holographic principle, how large a database could ever be searched for a
specific entry, before most of the database receded past one’s
cosmological horizon?

##### 2.4 Quantum Computing and Postselection

There is at least one foolproof way to solve @xmath -complete problems
in polynomial time: guess a random solution, then kill yourself if the
solution is incorrect.  Conditioned on looking at anything at all, you
will be looking at a correct solution!  It’s a wonder that this approach
is not tried more often.

The general idea, of throwing out all runs of a computation except those
that yield a particular result, is called postselection .  Chapter II
explores the general power of postselection when combined with quantum
computing.  I define a new complexity class called @xmath : the class of
problems solvable in polynomial time on a quantum computer, given the
ability to measure a qubit and assume the outcome will be @xmath (or
equivalently, discard all runs in which the outcome is @xmath ).  I then
show that @xmath coincides with the classical complexity class @xmath .

Surprisingly, this new characterization of @xmath yields an extremely
simple, quantum computing based proof that @xmath is closed under
intersection.  This had been an open problem for two decades, and the
previous proof, due to Beigel, Reingold, and Spielman [ 47 ] , used
highly nontrivial ideas about rational approximations of the sign
function.  I also reestablish an extension of the
Beigel-Reingold-Spielman result due to Fortnow and Reingold [ 115 ] ,
that @xmath is closed under polynomial-time truth-table reductions.
 Indeed, I show that @xmath is closed under @xmath truth-table
reductions, which seems to be a new result.

The rest of Chapter II studies the computational effects of simple
changes to the axioms of quantum mechanics.  In particular, what if we
allow linear but nonunitary transformations, or change the measurement
probabilities from @xmath to @xmath (suitably normalized) for some
@xmath ?  I show that the first change would yield exactly the power of
@xmath , and therefore of @xmath ; while the second change would yield
@xmath if @xmath , and some class between @xmath and @xmath otherwise.

My results complement those of Abrams and Lloyd [ 15 ] , who showed that
nonlinear quantum mechanics would let us solve @xmath - and even @xmath
-complete problems in polynomial time; and Brun [ 72 ] and Bacon [ 40 ]
, who showed the same for quantum computers involving closed timelike
curves.  Taken together, these results lend credence to an observation
of Weinberg [ 241 ] : that quantum mechanics is a “brittle” theory, in
the sense that even a tiny change to it would have dramatic
consequences.

##### 2.5 The Power of History

Contrary to widespread belief, what makes quantum mechanics so hard to
swallow is not indeterminism about the future trajectory of a particle.
 That is no more bizarre than a coin flip in a randomized algorithm.
 The difficulty is that quantum mechanics also seems to require
indeterminism about a particle’s past trajectory.  Or rather, the very
notion of a “trajectory” is undefined—for until the particle is
measured, there is just an evolving wavefunction.

In spite of this, Schrödinger [ 213 ] , Bohm [ 59 ] , Bell [ 49 ] , and
others proposed hidden-variable theories , in which a quantum state is
supplemented by “actual” values of certain observables.  These actual
values evolve in time by a dynamical rule, in such a way that the
predictions of quantum mechanics are recovered at any individual time.
 On the other hand, it now makes sense to ask questions like the
following: “Given that a particle was at location @xmath at time @xmath
(even though it was not measured at @xmath ), what is the probability of
it being at location @xmath at time @xmath ?”  The answers to such
questions yield a probability distribution over possible trajectories.

Chapter II initiates the study of hidden variables from the discrete,
abstract perspective of quantum computing.  For me, a hidden-variable
theory is simply a way to convert a unitary matrix that maps one quantum
state to another, into a stochastic matrix that maps the initial
probability distribution to the final one in some fixed basis.  I list
five axioms that we might want such a theory to satisfy, and investigate
previous hidden-variable theories of Dieks [ 97 ] and Schrödinger [ 213
] in terms of these axioms.  I also propose a new hidden-variable theory
based on network flows , which are classic objects of study in computer
science, and prove that this theory satisfies two axioms called
“indifference” and “robustness.” A priori , it was not at all obvious
that these two key axioms could be satisfied simultaneously.

Next I turn to a new question: the computational complexity of
simulating hidden-variable theories.  I show that, if we could examine
the entire history of a hidden variable, then we could efficiently solve
problems that are believed to be intractable even for quantum computers.
 In particular, under any hidden-variable theory satisfying the
indifference axiom, we could solve the Graph Isomorphism and Approximate
Shortest Vector problems in polynomial time, and indeed could simulate
the entire class @xmath (Statistical Zero Knowledge).  Combining this
result with the collision lower bound of Chapter I , we get an oracle
relative to which @xmath is strictly contained in @xmath , where @xmath
(Dynamical Quantum Polynomial-Time) is the class of problems efficiently
solvable by sampling histories.

Using the histories model, I also show that one could search an @xmath
-item database using @xmath queries, as opposed to @xmath with Grover’s
algorithm.  On the other hand, the @xmath bound is tight, meaning that
one could probably not solve @xmath -complete problems in polynomial
time.  We thus obtain the first good example of a model of computation
that appears slightly more powerful than the quantum computing model.

In summary, Chapter II ties together many of the themes of this thesis:
the black-box limitations of quantum computers; the application of
nontrivial computer science techniques; the obsession with the
computational resources needed to simulate our universe; and finally,
the use of quantum computing to shine light on the mysteries of quantum
mechanics itself.

### Chapter \thechapter Complexity Theory Cheat Sheet

  “If pigs can whistle, then donkeys can fly.”
  (Summary of complexity theory, attributed to Richard Karp)

To most people who are not theoretical computer scientists, the theory
of computational complexity—one of the great intellectual achievements
of the twentieth century—is simply a meaningless jumble of capital
letters.  The goal of this chapter is to turn it into a meaningful
jumble.

In computer science, a problem is ordinarily an infinite set of
yes-or-no questions: for example, “Given a graph, is it
connected?”  Each particular graph is an instance of the general
problem.  An algorithm for the problem is polynomial-time if, given any
instance as input, it outputs the correct answer after at most @xmath
steps, where @xmath and @xmath are constants, and @xmath is the length
of the instance, or the number of bits needed to specify it.  For
example, in the case of a directed graph, @xmath is just the number of
vertices squared.  Then @xmath is the class of all problems for which
there exists a deterministic classical polynomial-time algorithm.
 Examples of problems in @xmath include graph connectivity, and (as was
discovered two years ago [ 17 ] ) deciding whether a positive integer
written in binary is prime or composite.

Now, @xmath (Nondeterministic Polynomial-Time) is the class of problems
for which, if the answer to a given instance is ‘yes’, then an
omniscient wizard could provide a polynomial-size proof of that fact,
which would enable us to verify it in deterministic polynomial time.  As
an example, consider the Satisfiability problem: “given a formula
involving the Boolean variables @xmath and the logical connectives
@xmath (and, or, not), is there an assignment to the variables that
makes the formula true?”  If there is such an assignment, then a short,
easily-verified proof is just the assignment itself.  On the other hand,
it might be extremely difficult to find a satisfying assignment without
the wizard’s help—or for that matter, to verify the absence of a
satisfying assignment, even given a purported proof of its absence from
the wizard.  The question of whether there exist polynomial-size proofs
of un satisfiability that can be verified in polynomial time is called
the @xmath versus @xmath question.  Here @xmath is the class containing
the complement of every @xmath problem—for example, “given a Boolean
formula, is it not satisfiable?”

The Satisfiability problem turns out to be @xmath -complete, which means
it is among the “hardest” problems in @xmath : any instance of any
@xmath problem can be efficiently converted into an instance of
Satisfiability.  The central question, of course, is whether @xmath
-complete problems are solvable in polynomial time, or equivalently
whether @xmath (it being clear that @xmath ).  By definition, if any
@xmath -complete problem is solvable in polynomial time, then all of
them are.  One thing we know is that if @xmath , as is almost
universally assumed, then there are problems in @xmath that are neither
in @xmath nor @xmath -complete [ 162 ] .  Candidates for such
“intermediate” problems include deciding whether or not two graphs are
isomorphic, and integer factoring (e.g. given integers @xmath written in
binary, does @xmath have a prime factor greater than @xmath ?).  The
@xmath -intermediate problems have been a major focus of quantum
algorithms research.

#### 3 The Complexity Zoo Junior

I now present a glossary of @xmath complexity classes besides @xmath and
@xmath that appear in this thesis; non-complexity-theorist readers might
wish to refer back to it as needed.  The known relationships among these
classes are diagrammed in Figure 2 .  These classes represent a tiny
sample of the more than @xmath classes described on my Complexity Zoo
web page (www.complexityzoo.com).

@xmath (Polynomial Space) is the class of problems solvable by a
deterministic classical algorithm that uses a polynomially-bounded
amount of memory.  Thus @xmath , since a @xmath machine can loop through
all possible proofs.

@xmath (Exponential-Time) is the class of problems solvable by a
deterministic classical algorithm that uses at most @xmath time steps,
for some polynomial @xmath .  Thus @xmath .

@xmath (Bounded-Error Probabilistic Polynomial-Time) is the class of
problems solvable by a probabilistic classical polynomial-time
algorithm, which given any instance, must output the correct answer for
that instance with probability at least @xmath .  Thus @xmath .  It is
widely conjectured that @xmath [ 147 ] , but not even known that @xmath
.

@xmath (Probabilistic Polynomial-Time) is the class of problems solvable
by a probabilistic classical polynomial-time algorithm, which given any
instance, need only output the correct answer for that instance with
probability greater than @xmath .  The following problem is @xmath
-complete: given a Boolean formula @xmath , decide whether at least half
of the possible truth assignments satisfy @xmath .  We have @xmath and
also @xmath .

@xmath (pronounced “P to the sharp-P”) is the class of problems solvable
by a @xmath machine that can access a “counting oracle.”  Given a
Boolean formula @xmath , this oracle returns the number of truth
assignments that satisfy @xmath .  We have @xmath .

@xmath (Bounded-Error Quantum Polynomial-Time) is the class of problems
solvable by a quantum polynomial-time algorithm, which given any
instance, must output the correct answer for that instance with
probability at least @xmath .  More information is in Chapter Limits on
Efficient Computation in the Physical World .  We have @xmath [ 55 , 16
] .

@xmath (Exact Quantum Polynomial-Time) is similar to @xmath , except
that the probability of correctness must be @xmath instead of @xmath .
 This class is extremely artificial; it is not even clear how to define
it independently of the choice of gate set.  But for any reasonable
choice, @xmath .

@xmath ( @xmath with polynomial-size advice) is the class of problems
solvable by a @xmath algorithm that, along with a problem instance of
length @xmath , is also given an “advice string” @xmath of length
bounded by a polynomial in @xmath .  The only constraint is that @xmath
can depend only on @xmath , and not on any other information about the
instance.  Otherwise the @xmath ’s can be chosen arbitrarily to help the
algorithm.  It is not hard to show that @xmath .  Since the @xmath ’s
can encode noncomputable problems (for example, does the @xmath Turing
machine halt?), @xmath is not contained in any uniform complexity
class, where “uniform” means that the same information is available to
an algorithm regardless of @xmath .  We can also add polynomial-size
advice to other complexity classes, obtaining @xmath , @xmath , and so
on.

@xmath (Polynomial-Time Hierarchy) is the union of @xmath , @xmath ,
@xmath , etc.  Equivalently, @xmath is the class of problems that are
polynomial-time reducible to the following form: for all truth
assignments @xmath , does there exist an assignment @xmath such that for
all assignments @xmath , …, @xmath is satisfied, where @xmath is a
Boolean formula?  Here the number of alternations between “for all” and
“there exists” quantifiers is a constant independent of @xmath .  Sipser
[ 222 ] and Lautemann [ 163 ] showed that @xmath , while Toda [ 228 ]
showed that @xmath .

@xmath (Merlin Arthur) is the class of problems for which, if the answer
to a given instance is ‘yes,’ then an omniscient wizard could provide a
polynomial-size proof of that fact, which would enable us to verify it
in @xmath (classical probabilistic polynomial-time, with probability at
most @xmath of accepting an invalid proof or rejecting a valid one).  We
have @xmath .

@xmath (Arthur Merlin) is the class of problems for which, if the answer
to a given instance is ‘yes,’ then a @xmath algorithm could become
convinced of that fact after a constant number of rounds of interaction
with an omniscient wizard.  We have @xmath .  There is evidence that
@xmath [ 157 ] .

@xmath (Statistical Zero Knowledge) is the class of problems that
possess “statistical zero-knowledge proof protocols.”  We have @xmath .
 Although @xmath contains nontrivial problems such as graph isomorphism
[ 130 ] , there are strong indications that it does not contain all of
@xmath [ 63 ] .

Other complexity classes, such as @xmath , @xmath , @xmath , and @xmath
, will be introduced throughout the thesis as they are needed.

#### 4 Notation

In computer science, the following symbols are used to describe
asymptotic growth rates:

-   @xmath means that @xmath is at most order @xmath ; that is, @xmath
    for all @xmath and some nonnegative constants @xmath .

-   @xmath means that @xmath is at least order @xmath ; that is, @xmath
    .

-   @xmath means that @xmath is exactly order @xmath ; that is, @xmath
    and @xmath .

-   @xmath means that @xmath is less than order @xmath ; that is, @xmath
    but not @xmath .

The set of all @xmath -bit strings is denoted @xmath .  The set of all
binary strings, @xmath , is denoted @xmath .

#### 5 Oracles

One complexity-theoretic concept that will be needed again and again in
this thesis is that of an oracle .  An oracle is a subroutine available
to an algorithm, that is guaranteed to compute some function even if we
have no idea how.  Oracles are denoted using superscripts.  For example,
@xmath is the class of problems solvable by a @xmath algorithm that,
given any instance of an @xmath -complete problem such as
Satisfiability, can instantly find the solution for that instance by
calling the @xmath oracle.  The algorithm can make multiple calls to
the oracle, and these calls can be adaptive (that is, can depend on the
outcomes of previous calls).  If a quantum algorithm makes oracle calls,
then unless otherwise specified we assume that the calls can be made in
superposition.  Further details about the quantum oracle model are
provided in Chapter I .

We identify an oracle with the function that it computes, usually a
Boolean function @xmath .  Often we think of @xmath as defining a
problem instance, or rather an infinite sequence of problem instances,
one for each positive integer @xmath .  For example, “does there exist
an @xmath such that @xmath ?”  In these cases the oracle string , which
consists of @xmath for every @xmath , can be thought of as an input that
is @xmath bits long instead of @xmath bits.  Of course, a classical
algorithm running in polynomial time could examine only a tiny fraction
of such an input, but maybe a quantum algorithm could do better.  When
discussing such questions, we need to be careful to distinguish between
two functions: @xmath itself, and the function of the oracle string that
an algorithm is trying is to compute.

### Chapter \thechapter Quantum Computing Cheat Sheet

  “Somebody says … ‘You know those quantum mechanical amplitudes you
  told me about, they’re so complicated and absurd, what makes you think
  those are right?  Maybe they aren’t right.’  Such remarks are obvious
  and are perfectly clear to anybody who is working on this problem.  It
  does not do any good to point this out.”
  —Richard Feynman, The Character of Physical Law [ 109 ]

Non-physicists often have the mistaken idea that quantum mechanics is
hard.  Unfortunately, many physicists have done nothing to correct that
idea.  But in newer textbooks, courses, and survey articles [ 18 , 114 ,
175 , 182 , 235 ] , the truth is starting to come out: if you wish to
understand the central ‘paradoxes’ of quantum mechanics, together with
almost the entire body of research on quantum information and computing,
then you do not need to know anything about wave-particle duality,
ultraviolet catastrophes, Planck’s constant, atomic spectra,
boson-fermion statistics, or even Schrödinger’s equation.  All you need
to know is how to manipulate vectors whose entries are complex numbers.
 If that is too difficult, then positive and negative real numbers turn
out to suffice for most purposes as well.  After you have mastered these
vectors, you will then have some context if you wish to learn more about
the underlying physics.  But the historical order in which the ideas
were discovered is almost the reverse of the logical order in which they
are easiest to learn!

What quantum mechanics says is that, if an object can be in either of
two perfectly distinguishable states, which we denote @xmath and @xmath
, then it can also be in a linear “superposition” of those states,
denoted @xmath .  Here @xmath and @xmath are complex numbers called
“amplitudes,” which satisfy @xmath .  The asymmetric brackets @xmath are
called “Dirac ket notation”; one gets used to them with time.

If we measure the state @xmath in a standard way, then we see the “basis
state” @xmath with probability @xmath , and @xmath with probability
@xmath .  Also, the state changes to whichever outcome we see—so if we
see @xmath and then measure again, nothing having happened in the
interim, we will still see @xmath .  The two probabilities @xmath and
@xmath sum to @xmath , as they ought to.  So far, we might as well have
described the object using classical probabilities—for example, “this
cat is alive with probability @xmath and dead with probability @xmath ;
we simply don’t know which.”

The difference between classical probabilities and quantum amplitudes
arises in how the object’s state changes when we perform an operation on
it.  Classically, we can multiply a vector of probabilities by a
stochastic matrix , which is a matrix of nonnegative real numbers each
of whose columns sums to @xmath .  Quantum-mechanically, we multiply the
vector of amplitudes by a unitary matrix , which is a matrix of complex
numbers that maps any unit vector to another unit vector.
 (Equivalently, @xmath is unitary if and only if its inverse @xmath
equals its conjugate transpose @xmath .)  As an example, suppose we
start with the state @xmath , which corresponds to the vector of
amplitudes

  -- -------- --
     @xmath   
  -- -------- --

We then left-multiply this vector by the unitary matrix

  -- -------- --
     @xmath   
  -- -------- --

which maps the vector to

  -- -------- --
     @xmath   
  -- -------- --

and therefore the state @xmath to

  -- -------- --
     @xmath   
  -- -------- --

If we now measured, we would see @xmath with probability @xmath and
@xmath with probability @xmath .  The interesting part is what happens
if we apply the same operation @xmath a second time, without measuring.
 We get

  -- -------- --
     @xmath   
  -- -------- --

which is @xmath with certainty (see Figure 3 ).

Applying a “randomizing” operation to a “random” state produces a
deterministic outcome!  The reason is that, whereas probabilities are
always nonnegative, amplitudes can be positive, negative, or even
complex, and can therefore cancel each other out.  This interference of
amplitudes can be considered the source of all “quantum weirdness.”

#### 6 Quantum Computers: @xmath Qubits

The above description applied to “qubits,” or objects with only two
distinguishable states.  But it generalizes to objects with a larger
number of distinguishable states.  Indeed, in quantum computing we
consider a system of @xmath qubits, each of which can be @xmath or
@xmath .  We then need to assign amplitudes to all @xmath possible
outcomes of measuring the qubits in order from first to last.  So the
computer’s state has the form

  -- -------- --
     @xmath   
  -- -------- --

where

  -- -------- --
     @xmath   
  -- -------- --

What was just said is remarkable—for it suggests that Nature needs to
keep track of @xmath complex numbers just to describe a state of @xmath
interacting particles.  If @xmath , then this is already more complex
numbers than there are particles in the known universe.  The goal of
quantum computing is to exploit this strange sort of parallelism that is
inherent in the laws of physics as we currently understand them.

The difficulty is that, when the computer’s state is measured, we only
see one of the “basis states” @xmath , not the entire collection of
amplitudes.  However, for a few specific problems, we might be able to
arrange things so that basis states corresponding to wrong answers all
have amplitudes close to @xmath , because of interference between
positive and negative contributions.  If we can do that, then basis
states corresponding to right answers will be measured with high
probability.

More explicitly, a quantum computer applies a sequence of unitary
matrices called gates , each of which acts on only one or two of the
@xmath qubits (meaning that is a tensor product of the identity
operation on @xmath or @xmath qubits, and the operation of interest on
the remaining qubits).  As an example, the controlled-NOT or CNOT gate
is a two-qubit gate that flips a “target” qubit if a “control” qubit is
@xmath , and otherwise does nothing:

  -- -- --
        
  -- -- --

The unitary matrix corresponding to the CNOT gate is

  -- -------- --
     @xmath   
  -- -------- --

Adleman, DeMarrais, and Huang [ 16 ] showed that the CNOT gate, together
with the one-qubit gate

  -- -------- --
     @xmath   
  -- -------- --

constitute a universal set of quantum gates, in that they can be used to
approximate any other gate to any desired accuracy.  Indeed, almost any
set of one- and two-qubit gates is universal in this sense [ 94 ] .

A quantum circuit is just a sequence of gates drawn from a finite
universal set.  Without loss of generality, we can take the circuit’s
output to be the result of a single measurement after all gates have
been applied; that is, @xmath with probability @xmath .  (If a binary
output is needed, we simply throw away the last @xmath bits of @xmath .)
 It is known that allowing intermediate measurements does not yield any
extra computational power [ 55 ] .  The circuit is polynomial-size if
both @xmath and the number of gates are upper-bounded by a polynomial in
the length @xmath of the input.

We can now define the important complexity class @xmath , or
Bounded-Error Quantum Polynomial-Time.  Given an input @xmath , first a
polynomial-time classical algorithm @xmath prepares a polynomial-size
quantum circuit @xmath .  (The requirement that the circuit itself be
efficiently preparable is called uniformity .)  Then @xmath is applied
to the “all- @xmath ” initial state @xmath .  We say a language @xmath
is in @xmath if there exists an @xmath such that for all @xmath ,

1.  If @xmath then @xmath outputs ‘1’ with probability at least @xmath .

2.  If @xmath then @xmath outputs ‘0’ with probability at least @xmath .

By running @xmath multiple times and taking the majority answer, we can
boost the probability of success from @xmath to @xmath for any
polynomial @xmath .

@xmath was first defined in a 1993 paper by Bernstein and Vazirani [ 55
] . ¹⁴ ¹⁴ 14 As a historical note, Bernstein and Vazirani [ 55 ] defined
@xmath in terms of “quantum Turing machines.”  However, Yao [ 248 ]
showed that Bernstein and Vazirani’s definition is equivalent to the
much simpler one given here.  Also, Berthiaume and Brassard [ 57 ] had
implicitly defined @xmath (Exact Quantum Polynomial-Time) a year
earlier, and had shown that it lies outside @xmath and even @xmath
relative to an oracle. That paper marked a turning point.  Before,
quantum computing had been an idea , explored in pioneering work by
Deutsch [ 90 ] , Feynman [ 108 ] , and others.  Afterward, quantum
computing was a full-fledged model in the sense of computational
complexity theory, which could be meaningfully compared against other
models.  For example, Bernstein and Vazirani showed that @xmath :
informally, quantum computers are at least as powerful as classical
probabilistic computers, and at most exponentially more powerful.  (The
containment @xmath was later improved to @xmath by Adleman, DeMarrais,
and Huang [ 16 ] .)

Bernstein and Vazirani also gave an oracle problem called Recursive
Fourier Sampling ( @xmath ), and showed that it requires @xmath
classical probabilistic queries but only @xmath quantum queries.  This
provided the first evidence that quantum computers are strictly more
powerful than classical probabilistic computers, i.e. that @xmath .
 Soon afterward, Simon [ 220 ] widened the gap to polynomial versus
exponential, by giving an oracle problem that requires @xmath classical
probabilistic queries but only @xmath quantum queries.  However, these
results attracted limited attention because the problems seemed
artificial.

People finally paid attention when Shor [ 219 ] showed that quantum
computers could factor integers and compute discrete logarithms in
polynomial time.  The security of almost all modern cryptography rests
on the presumed intractability of those two problems.  It had long been
known [ 177 ] that factoring is classically reducible to the following
problem: given oracle access to a periodic function @xmath , where
@xmath is exponentially large, find the period of @xmath .  Shor gave an
efficient quantum algorithm for this oracle problem, by exploiting the
quantum Fourier transform , a tool that had earlier been used by Simon.
 (The algorithm for the discrete logarithm problem is more complicated
but conceptually similar.)

Other results in the “quantum canon,” such as Grover’s algorithm [ 139 ]
and methods for quantum error-correction and fault-tolerance [ 20 , 80 ,
132 , 159 , 225 ] , will be discussed in this thesis as the need arises.

#### 7 Further Concepts

This section summarizes “fancier” quantum mechanics concepts, which are
needed for Part II and for Chapter I of Part I (which deals with quantum
advice).  They are not needed for the other chapters in Part I .

Tensor Product. If @xmath and @xmath are two quantum states, then their
tensor product , denoted @xmath or @xmath , is just a state that
consists of @xmath and @xmath next to each other.  For example, if
@xmath and @xmath , then

  -- -- --
        
  -- -- --

Inner Product. The inner product between two states @xmath and @xmath is
defined as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denotes complex conjugate.  The inner product satisfies all
the expected properties, such as @xmath and

  -- -- --
        
  -- -- --

If @xmath then we say @xmath and @xmath are orthogonal .

General Measurements. In principle, we can choose any orthogonal basis
of states @xmath in which to measure a state @xmath .  (Whether that
measurement can actually be performed efficiently is another matter.)
 Then the probability of obtaining outcome @xmath is @xmath .  We can
even measure in a non -orthogonal basis, a concept called Positive
Operator Valued Measurements (POVM’s) that I will not explain here.
 None of these more general measurements increase the power of the
quantum computing model, since we can always produce the same effect by
first applying a unitary matrix (possibly using additional qubits called
ancillas ), and then measuring in a “standard” basis such as @xmath .

Mixed States. Superposition states, such as @xmath , are also called
pure states .  This is to distinguish them from mixed states , which are
the most general kind of state in quantum mechanics.  Mixed states are
just classical probability distributions over pure states.  There is a
catch, though: any mixed state can be decomposed into a probability
distribution over pure states in infinitely many nonequivalent ways.
 For example, if we have a state that is @xmath with probability @xmath
and @xmath with probability @xmath , then no experiment could ever
distinguish it from a state that is @xmath with probability @xmath and
@xmath with probability @xmath .  For regardless of what orthogonal
basis we measured in, the two possible outcomes of measuring would both
occur with probability @xmath .  Therefore, this state is called the
one-qubit maximally mixed state .

Density Matrices. We can represent mixed states using a formalism called
density matrices .  The outer product of @xmath with itself, denoted
@xmath , is an @xmath complex matrix whose @xmath entry is @xmath .  Now
suppose we have a state that is @xmath with probability @xmath , and
@xmath with probability @xmath .  We represent the state by a Hermitian
positive definite matrix @xmath with trace @xmath , as follows:

  -- -------- --
     @xmath   
  -- -------- --

When we apply a unitary operation @xmath , the density matrix @xmath
changes to @xmath .  When we measure in the standard basis, the
probability of outcome @xmath is the @xmath diagonal entry of @xmath .
 Proving that these rules are the correct ones, and that a density
matrix really is a unique description of a mixed state, are “exercises
for the reader” (which as always means the author was too lazy).
 Density matrices will mainly be used in Chapter I .

Trace Distance. Suppose you are given a system that was prepared in
state @xmath with probability @xmath , and @xmath with probability
@xmath .  After making a measurement, you must guess which state the
system was prepared in.  What is the maximum probability that you will
be correct?  The answer turns out to be

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the trace distance between @xmath and @xmath , defined
as @xmath where @xmath are the eigenvalues of @xmath .

Entanglement. Suppose @xmath is a joint state of two systems.  If @xmath
can be written as a probability distribution over pure states of the
form @xmath , then we say @xmath is separable ; otherwise @xmath is
entangled .

Hamiltonians. Instead of discrete unitary operations, we can imagine
that a quantum state evolves in time by a continuous rotation called a
Hamiltonian .  A Hamiltonian is an @xmath Hermitian matrix @xmath .  To
find the unitary operation @xmath that is effected by “leaving @xmath
on” for @xmath time steps, the rule ¹⁵ ¹⁵ 15 Here Planck’s constant is
set equal to @xmath as always. is @xmath .  The only place I use
Hamiltonians is in Chapter II , and even there the use is incidental.

## Part I Limitations of Quantum Computers

### Chapter \thechapter Introduction

  “A quantum possibility is less real than a classical reality, but more
  real than a classical possibility.”
  —Boris Tsirelson [ 229 ]

Notwithstanding accounts in the popular press, a decade of research has
made it clear that quantum computers would not be a panacea.  In
particular, we still do not have a quantum algorithm to solve @xmath
-complete problems in polynomial time.  But can we prove that no such
algorithm exists, i.e. that @xmath ?  The difficulty is that we can’t
even prove no classical algorithm exists; this is the @xmath versus
@xmath question.  Of course, we could ask whether @xmath assuming that
@xmath —but unfortunately, even this conditional question seems far
beyond our ability to answer.  So we need to refine the question even
further: can quantum computers solve @xmath -complete problems in
polynomial time, by brute force ?

What is meant by “brute force” is the following.  In Shor’s factoring
algorithm [ 219 ] , we prepare a superposition of the form

  -- -------- --
     @xmath   
  -- -------- --

where @xmath for some @xmath .  But as far as the key step of the
algorithm is concerned, the function @xmath is a “black box.”  Given any
superposition like the one above, the algorithm will find the period of
@xmath assuming @xmath is periodic; it does not need further information
about how @xmath was computed.  So in the language of Section 5 , we
might as well say that @xmath is computed by an oracle.

Now suppose we are given a Boolean formula @xmath over @xmath variables,
and are asked to decide whether @xmath is satisfiable.  One approach
would be to exploit the internal structure of @xmath : “let’s see, if I
set variable @xmath to TRUE, then this clause here is satisfied, but
those other clauses aren’t satisfied any longer … darn!”  However,
inspired by Shor’s factoring algorithm, we might hope for a cruder
quantum algorithm that treats @xmath merely as an oracle, mapping an
input string @xmath to an output bit @xmath that is @xmath if and only
if @xmath satisfies @xmath .  The algorithm would have to decide whether
there exists an @xmath such that @xmath , using as few calls to the
@xmath oracle as possible, and not learning about @xmath in any other
way.  This is what is meant by brute force.

A fundamental result of Bennett, Bernstein, Brassard, and Vazirani [ 51
] says that no brute-force quantum algorithm exists to solve @xmath
-complete problems in polynomial time.  In particular, for some
probability distribution over oracles, any quantum algorithm needs
@xmath oracle calls to decide, with at least a @xmath chance of being
correct, whether there exists an @xmath such that @xmath .  On a
classical computer, of course, @xmath oracle calls are necessary and
sufficient.  But as it turns out, Bennett et al.’s quantum lower bound
is tight, since Grover’s quantum search algorithm [ 139 ] can find a
satisfying assignment (if it exists) quadratically faster than any
classical algorithm.  Amusingly, Grover’s algorithm was proven optimal
before it was discovered to exist!

A recurring theme in this thesis is the pervasiveness of Bennett et
al.’s finding.  I will show that, even if a problem has considerably
more structure than the basic Grover search problem, even if “quantum
advice states” are available, or even if we could examine the entire
history of a hidden variable, still any brute-force quantum algorithm
would take exponential time.

#### 8 The Quantum Black-Box Model

The quantum black-box model formalizes the idea of a brute-force
algorithm.  For the time being, suppose that a quantum algorithm’s goal
is to evaluate @xmath , where @xmath is a Boolean function and @xmath is
an @xmath -bit string.  Then the algorithm’s state at any time @xmath
has the form

  -- -------- --
     @xmath   
  -- -------- --

Here @xmath is the index of an oracle bit @xmath to query, and @xmath is
an arbitrarily large string of bits called the “workspace,” containing
whatever information the algorithm wants to store there.  The state
evolves in time via an alternating sequence of algorithm steps and query
steps .  An algorithm step multiplies the vector of @xmath ’s by an
arbitrary unitary matrix that does not depend on @xmath .  It does not
matter how many quantum gates would be needed to implement this matrix.
 A query step maps each basis state @xmath to @xmath , effecting the
transformation @xmath .  Here @xmath is the string @xmath , with @xmath
exclusive-OR’ed into a particular location in @xmath called the “answer
bit.”  The reason exclusive-OR is used is that the query step has to be
reversible, or else it would not be unitary.

At the final step @xmath , the state is measured in the standard basis,
and the output of the algorithm is taken to be (say) @xmath , the first
bit of @xmath .  The algorithm succeeds if

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath .  Here the constant @xmath is arbitrary.  Then the
(bounded-error) quantum query complexity of @xmath , denoted @xmath , is
the minimum over all quantum algorithms @xmath that succeed at
evaluating @xmath , of the number of queries to @xmath made by @xmath .
 Here the ‘ @xmath ’ represents the fact that the error probability is
two-sided.  One can compare @xmath with @xmath , or zero-error quantum
query complexity; @xmath , or bounded-error classical randomized query
complexity; and @xmath , or deterministic query complexity, among other
complexity measures.  Chapter I will define many such measures and
compare them in detail.

As a simple example of the black-box model, let @xmath .  Then Grover’s
algorithm [ 139 ] implies that @xmath , while the lower bound of Bennett
et al. [ 51 ] implies that @xmath .  By comparison, @xmath .

The quantum black-box model has some simple generalizations, which
I will use when appropriate.  First, @xmath can be a partial function,
defined only on a subset of @xmath (so we obtain what is called a
promise problem ).  Second, the @xmath ’s do not need to be bits; in
Chapters I and I they will take values from a larger range.  Third, in
Chapter I the output will not be Boolean, and there will generally be
more than one valid output (so we obtain what is called a relation
problem ).

#### 9 Oracle Separations

  “I do believe it
  Against an oracle.”
  —Shakespeare, The Tempest

Several times in this thesis, I will use a lower bound on quantum query
complexity to show that a complexity class is not in @xmath “relative to
an oracle.”  The method for turning query complexity lower bounds into
oracle separations was invented by Baker, Gill, and Solovay [ 41 ] to
show that there exists an oracle @xmath relative to which @xmath .
 Basically, they encoded into @xmath an infinite sequence of
exponentially hard search problems, one for each input length @xmath ,
such that (i) a nondeterministic machine can solve the @xmath problem in
time polynomial in @xmath , but (ii) any deterministic machine would
need time exponential in @xmath .  They guaranteed (ii) by
“diagonalizing” against all possible deterministic machines, similarly
to how Turing created an uncomputable real number by diagonalizing
against all possible computable reals.  Later, Bennett and Gill [ 54 ]
showed that a simpler way to guarantee (ii) is just to choose the search
problems uniformly at random.  Throughout the thesis, I will cavalierly
ignore such issues, proceeding immediately from a query complexity lower
bound to the statement of the corresponding oracle separation.

The point of an oracle separation is to rule out certain approaches to
solving an open problem in complexity theory.  For example, the
Baker-Gill-Solovay theorem implies that the standard techniques of
computability theory, which relativize (that is, are “oblivious” to the
presence of an oracle), cannot be powerful enough to show that @xmath .
 Similarly, the result of Bennett et al. [ 51 ] that @xmath implies that
there exists an oracle @xmath relative to which @xmath .  While this
does not show that @xmath , it does show that any proof of @xmath would
have to use “non-relativizing” techniques that are unlike anything we
understand today.

However, many computer scientists are skeptical that anything can be
learned from oracles.  The reason for their skepticism is that over the
past @xmath years, they have seen several examples of non-relativizing
results in classical complexity theory.  The most famous of these is
Shamir’s Theorem [ 215 , 171 ] that @xmath , where @xmath is the class
of problems that have interactive proof systems , meaning that if the
answer for some instance is “yes,” then a polynomial-time verifier can
become convinced of that fact to any desired level of confidence by
exchanging a sequence of messages with an omniscient prover. ¹⁶ ¹⁶ 16
Arora, Impagliazzo, and Vazirani [ 36 ] claim the Cook-Levin Theorem,
that Satisfiability is @xmath -complete, as another non-relativizing
result.  But this hinges on what we mean by “non-relativizing,” far more
than the @xmath example. By contrast, oracles had been known relative to
which not even @xmath , let alone @xmath , is contained in @xmath [ 117
] .  So why should we ever listen to oracles again, if they got
interactive proofs so dramatically wrong?

My answer is threefold.  First, essentially all quantum algorithms that
we know today—from Shor’s algorithm, as discussed previously, to
Grover’s algorithm, to the quantum adiabatic algorithm ¹⁷ ¹⁷ 17 Given an
assignment @xmath to a 3SAT formula @xmath , the adiabatic algorithm
actually queries an oracle that returns the number of clauses of @xmath
that @xmath satisfies, not just whether @xmath satisfies @xmath or not.
 Furthermore, van Dam, Mosca, and Vazirani [ 233 ] have shown such an
oracle is sufficient to reconstruct @xmath .  On the other hand, the
adiabatic algorithm itself would be just as happy with a fitness
landscape that did not correspond to any 3SAT instance, and that is what
I mean by saying that it is an oracle algorithm at the core. [ 106 ] ,
to the algorithms of Hallgren [ 141 ] and van Dam, Hallgren, and Ip [
232 ] —are oracle algorithms at their core.  We do not know of any
non-relativizing quantum algorithm technique analogous to the
arithmetization technique that was used to prove @xmath .  If such a
technique is ever discovered, I will be one of the first to want to
learn it.

The second response is that without oracle results, we do not have even
the beginnings of understanding.  Once we know (for example) that @xmath
relative to an oracle, we can then ask the far more difficult
unrelativized question, knowing something about the hurdles that any
proof of @xmath would have to overcome.

The third response is that “the proof of the pudding is in the
proving.”  In other words, the real justification for the quantum
black-box model is not the a priori plausibility of its assumptions, but
the depth and nontriviality of what can be (and has been) proved in it.
 For example, the result that @xmath relative to an oracle [ 117 ] does
not tell us much about interactive proof systems.  For given an
exponentially long oracle string @xmath , it is intuitively obvious that
nothing a prover could say could convince a classical polynomial-time
verifier that @xmath is the all- @xmath string, even if the prover and
verifier could interact.  The only issue is how to formalize that
obvious fact by diagonalizing against all possible proof systems.  By
contrast, the quantum oracle separations that we have are not
intuitively obvious in the same way; or rather, the act of understanding
them confers an intuition where none was previously present.

### Chapter \thechapter The Collision Problem

The collision problem of size @xmath , or @xmath , is defined as
follows.  Let @xmath be a sequence of @xmath integers drawn from @xmath
, with @xmath even.  We are guaranteed that either

1.  @xmath is one-to-one (that is, a permutation of @xmath ), or

2.  @xmath is two-to-one (that is, each element of @xmath appears in
    @xmath twice or not at all).

The problem is to decide whether (1) or (2) holds.  (A variant asks us
to find a collision in a given two-to-one function.  Clearly a lower
bound for the collision problem as defined above implies an equivalent
lower bound for this variant.)  Because of its simplicity, the collision
problem was widely considered a benchmark for our understanding of
quantum query complexity.

I will show that @xmath , where @xmath is the bounded-error quantum
query complexity of function @xmath .  The best known upper bound, due
to Brassard, Høyer, and Tapp [ 68 ] , is @xmath (see Section 1.1 ).
 Previously, though, no lower bound better than the trivial @xmath bound
was known.  How great a speedup quantum computers yield for the problem
was apparently first asked by Rains [ 193 ] .

Previous lower bound techniques failed for the problem because they
depended on a function’s being sensitive to many disjoint changes to the
input.  For example, Beals et al. [ 45 ] showed that for all total
Boolean functions @xmath , @xmath , where @xmath is the block
sensitivity, defined by Nisan [ 183 ] to be, informally, the maximum
number of disjoint changes (to any particular input @xmath ) to which
@xmath is sensitive.  In the case of the collision problem, though,
every one-to-one input differs from every two-to-one input in at least
@xmath places, so the block sensitivity is @xmath .  Ambainis’s
adversary method [ 27 ] faces a related obstacle.  In that method we
consider the algorithm and input as a bipartite quantum state, and
upper-bound how much the entanglement of the state can increase via a
single query.  But under the simplest measures of entanglement, it turns
out that the algorithm and input can become almost maximally entangled
after @xmath queries, again because every one-to-one input is far from
every two-to-one input. ¹⁸ ¹⁸ 18 More formally, the adversary method
cannot prove any lower bound on @xmath better than @xmath , where @xmath
is the randomized certificate complexity of @xmath (to be defined in
Chapter I ).  But for the collision function, @xmath .

My proof is an adaptation of the polynomial method, introduced to
quantum computing by Beals et al. [ 45 ] .  Their idea was to reduce
questions about quantum algorithms to easier questions about
multivariate polynomials.  In particular, if a quantum algorithm makes
@xmath queries, then its acceptance probability is a polynomial over the
input bits of degree at most @xmath .  So by showing that any polynomial
approximating the desired output has high degree, one obtains a lower
bound on @xmath .

To lower-bound the degree of a multivariate polynomial, a key technical
trick is to construct a related univariate polynomial.  Beals et al. [
45 ] , using a lemma due to Minsky and Papert [ 178 ] , replace a
polynomial @xmath (where @xmath is a bit string) by @xmath (where @xmath
denotes the Hamming weight of @xmath ), satisfying

  -- -------- --
     @xmath   
  -- -------- --

and @xmath .

Here I construct the univariate polynomial in a different way.  I
consider a uniform distribution over @xmath -to-one inputs, where @xmath
might be greater than @xmath .  Even though the problem is to
distinguish @xmath from @xmath , the acceptance probability must lie in
the interval @xmath for all @xmath , and that is a surprisingly strong
constraint.  I show that the acceptance probability is close to a
univariate polynomial in @xmath of degree at most @xmath .  I then
obtain a lower bound by generalizing a classical approximation theory
result of Ehlich and Zeller [ 104 ] and Rivlin and Cheney [ 204 ] .
 Much of the proof deals with the complication that @xmath does not
divide @xmath in general.

Shortly after this work was completed, Shi [ 218 ] improved it to give a
tight lower bound of @xmath for the collision problem, when the @xmath
range from @xmath to @xmath rather than from @xmath to @xmath .  For a
range of size @xmath , his bound was @xmath .  Subsequently Kutin [ 161
] and Ambainis [ 29 ] showed a lower bound of @xmath for a range of size
@xmath as well.  By a simple reduction, these results imply a lower
bound of @xmath for the element distinctness problem —that of deciding
whether there exist @xmath such that @xmath .  The previous best known
lower bound was @xmath , and at the time of Shi’s work, the best known
upper bound was @xmath , due to Buhrman et al. [ 77 ] .  Recently,
however, Ambainis [ 30 ] gave a novel algorithm based on quantum walks
that matches the @xmath lower bound.

The chapter is organized as follows.  Section 10 motivates the collision
lower bound within quantum computing, pointing out connections to
collision-resistant hash functions, the nonabelian hidden subgroup
problem, statistical zero-knowledge, and information erasure.  Section
11 gives technical preliminaries, Section 12 proves the crucial fact
that the acceptance probability is “almost” a univariate polynomial, and
Section 13 completes the lower bound argument.  I conclude in Section 15
with some open problems.  In Section 14 I show a lower bound of @xmath
for the set comparison problem , a variant of the collision problem
needed for the application to information erasure.

#### 10 Motivation

In Chapter Limits on Efficient Computation in the Physical World I
listed seven implications of the collision lower bound; this section
discusses a few of those implications in more detail.  The implication
that motivated me personally—concerning the computational power of
so-called hidden-variable theories —is deferred to Chapter II .

##### 10.1 Oracle Hardness Results

The original motivation for the collision problem was to model
(strongly) collision-resistant hash functions in cryptography.  There is
a large literature on collision-resistant hashing; see [ 201 , 42 ] for
example.  When building secure digital signature schemes, it is useful
to have a family of hash functions @xmath , such that finding a distinct
@xmath pair with @xmath is computationally intractable.  A quantum
algorithm for finding collisions using @xmath queries would render all
hash functions insecure against quantum attack in this sense.  (Shor’s
algorithm [ 219 ] already renders hash functions based on modular
arithmetic insecure.)  My result indicates that collision-resistant
hashing might still be possible in a quantum setting.

The collision problem also models the nonabelian hidden subgroup problem
, of which graph isomorphism is a special case.  Given a group @xmath
and subgroup @xmath , suppose we have oracle access to a function @xmath
such that for all @xmath , @xmath if and only if @xmath and @xmath
belong to the same coset of @xmath .  Is there then an efficient quantum
algorithm to determine @xmath ?  If @xmath is abelian, the work of Simon
[ 220 ] , Shor [ 219 ] , and Kitaev [ 152 ] implies an affirmative
answer.  If @xmath is nonabelian, though, efficient quantum algorithms
are known only for special cases [ 105 , 138 ] .  An @xmath -query
algorithm for the collision problem would yield a polynomial-time
algorithm to distinguish @xmath from @xmath , which does not exploit the
group structure at all.  My result implies that no such algorithm
exists.

Finally, the collision lower bound implies that there exists an oracle
relative to which @xmath , where @xmath is the class of problems having
statistical zero-knowledge proof protocols.  For suppose that a verifier
@xmath and prover @xmath both have oracle access to a sequence @xmath ,
which is either one-to-one or two-to-one.  To verify with zero knowledge
that @xmath is one-to-one, @xmath can repeatedly choose an @xmath and
send @xmath to @xmath , whereupon @xmath must send @xmath back to @xmath
.  Thus, using standard diagonalization techniques, one can produce an
oracle @xmath such that @xmath .

##### 10.2 Information Erasure

Let @xmath with @xmath be a one-to-one function.  Then we can consider
two kinds of quantum oracle for @xmath :

1.  a standard oracle , one that maps @xmath to

    @xmath , or

2.  an erasing oracle (as proposed by Kashefi et al. [ 150 ] ), which
    maps @xmath to @xmath , in effect “erasing” @xmath .

Intuitively erasing oracles seem at least as strong as standard ones,
though it is not clear how to simulate the latter with the former
without also having access to an oracle that maps @xmath to @xmath .
 The question that concerns us here is whether erasing oracles are more
useful than standard ones for some problems.  One-way functions provide
a clue: if @xmath is one-way, then (by assumption) @xmath can be
computed efficiently, but if @xmath could be computed efficiently given
@xmath then so could @xmath given @xmath , and hence @xmath could be
inverted.  But can we find, for some problem, an exponential gap between
query complexity given a standard oracle and query complexity given an
erasing oracle?

In Section 14 I extend the collision lower bound to show an affirmative
answer.  Define the set comparison problem of size @xmath , or @xmath ,
as follows.  We are given as input two sequences, @xmath and @xmath ,
such that for each @xmath , @xmath .  A query has the form @xmath ,
where @xmath and @xmath , and produces as output @xmath if @xmath and
@xmath if @xmath . @xmath Sequences @xmath and @xmath are both
one-to-one; that is, @xmath and @xmath for all @xmath .  We are
furthermore guaranteed that either

1.  @xmath and @xmath are equal as sets (that is, @xmath ) or

2.  @xmath and @xmath are far as sets (that is,

    @xmath ).

As before the problem is to decide whether (1) or (2) holds.

This problem can be solved with high probability in a constant number of
queries using an erasing oracle, by using a trick similar to that of
Watrous [ 239 ] for verifying group non-membership.  First, using the
oracle, we prepare the uniform superposition

  -- -- --
        
  -- -- --

We then apply a Hadamard gate to the first register, and finally we
measure the first register.  If @xmath and @xmath are equal as sets,
then interference occurs between every @xmath pair and we observe @xmath
with certainty.  But if @xmath and @xmath are far as sets, then basis
states @xmath with no matching @xmath have probability weight at least
@xmath , and hence we observe @xmath with probability at least @xmath .

In Section 14 I sketch a proof that @xmath ; that is, no efficient
quantum algorithm using a standard oracle exists for this problem.
 Recently, Midrijanis [ 176 ] gave a lower bound of @xmath not merely
for the set comparison problem, but for the set equality problem (where
we are promised that @xmath and @xmath are either equal or disjoint).

#### 11 Preliminaries

Let @xmath be a quantum query algorithm as defined in Section 8 .  A
basis state of @xmath is written @xmath .  Then a query replaces each
@xmath by @xmath , where @xmath is exclusive-OR’ed into some specified
location of @xmath .  Between queries, the algorithm can perform any
unitary operation that does not depend on the input.  Let @xmath be the
total number of queries.  Also, assume for simplicity that all
amplitudes are real; this restriction is without loss of generality [ 55
] .

Let @xmath be the amplitude of basis state @xmath after @xmath queries
when the input is @xmath .  Also, let @xmath if @xmath , and @xmath if
@xmath .  Let @xmath be the probability that @xmath returns
“two-to-one” when the input is @xmath .  Then we obtain a simple variant
of a lemma due to Beals et al. [ 45 ] .

###### Lemma 1

@xmath is a multilinear polynomial of degree at most @xmath over the
@xmath .

Proof. We show, by induction on @xmath , that for all basis states
@xmath , the amplitude @xmath is a multilinear polynomial of degree at
most @xmath over the @xmath .  Since @xmath is a sum of squares of
@xmath ’s, the lemma follows.

The base case ( @xmath ) holds since, before making any queries, each
@xmath is a degree- @xmath polynomial over the @xmath .  A unitary
transformation on the algorithm part replaces each @xmath by a linear
combination of @xmath ’s, and hence cannot increase the degree.  Suppose
the lemma holds prior to the @xmath query.  Then

  -- -------- --
     @xmath   
  -- -------- --

and we are done.

#### 12 Reduction to Bivariate Polynomial

Call the point @xmath an @xmath - quasilattice point if and only if

1.  @xmath and @xmath are integers, with @xmath dividing @xmath ,

2.  @xmath ,

3.  @xmath , and

4.  if @xmath then @xmath .

For quasilattice point @xmath , define @xmath to be the uniform
distribution over all size- @xmath subfunctions of @xmath
-to-1 functions having domain @xmath and range a subset of @xmath .
 More precisely: to draw an @xmath from @xmath , we first choose a set
@xmath with @xmath uniformly at random.  We then choose a @xmath -to-1
function @xmath from @xmath to @xmath uniformly at random.  Finally we
let @xmath for each @xmath .

Let @xmath be the probability that algorithm @xmath returns @xmath when
the input is chosen from @xmath :

  -- -------- --
     @xmath   
  -- -------- --

We then have the following surprising characterization:

###### Lemma 2

For all sufficiently large @xmath and if @xmath , there exists a
bivariate polynomial @xmath of degree at most @xmath such that if @xmath
is a quasilattice point, then

  -- -------- --
     @xmath   
  -- -------- --

(where the constant @xmath can be made arbitrarily small by adjusting
parameters).

Proof. Let @xmath be a product of @xmath variables, with degree @xmath ,
and let @xmath be @xmath evaluated on input @xmath .  Then define

  -- -------- --
     @xmath   
  -- -------- --

to be the probability that monomial @xmath evaluates to @xmath when the
input is drawn from @xmath .  Then by Lemma 1 , @xmath is a polynomial
of degree at most @xmath over @xmath , so

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

for some coefficients @xmath .

We now calculate @xmath .  Assume without loss of generality that for
all @xmath , either @xmath or @xmath , since otherwise @xmath .

Define the “range” @xmath of @xmath to be the set of all @xmath such
that @xmath .  Let @xmath ; then we write @xmath . @xmath Clearly @xmath
unless @xmath , where @xmath is the range of @xmath .  By assumption,

  -- -------- --
     @xmath   
  -- -------- --

so the number of possible @xmath is @xmath and, of these, the number
that contain @xmath is @xmath .

Then, conditioned on @xmath , what is the probability that @xmath ?  The
total number of @xmath -to-1 functions with domain size @xmath is @xmath
since we can permute the @xmath function values arbitrarily, but must
not count permutations that act only within the @xmath constant-value
blocks of size @xmath .

Among these functions, how many satisfy @xmath ?  Suppose that, for each
@xmath , there are @xmath distinct @xmath such that @xmath .  Clearly

  -- -------- --
     @xmath   
  -- -------- --

Then we can permute the @xmath function values outside of @xmath
arbitrarily, but must not count permutations that act only within the
@xmath constant-value blocks, which have size either @xmath or @xmath
for some @xmath .  So the number of functions for which @xmath is

  -- -- --
        
  -- -- --

Putting it all together,

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where

  -- -------- --
     @xmath   
  -- -------- --

is a bivariate polynomial of total degree at most

  -- -------- --
     @xmath   
  -- -------- --

(Note that in the case @xmath for some @xmath , this polynomial
evaluates to @xmath , which is what it ought to do.)  Hence

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where

  -- -------- --
     @xmath   
  -- -------- --

Clearly

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath and @xmath , we also have

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

for all sufficiently large @xmath . Thus, since @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

and we are done.

#### 13 Lower Bound

We have seen that, if a quantum algorithm for the collision problem
makes few queries, then its acceptance probability can be approximated
by a low-degree bivariate polynomial.  This section completes the lower
bound proof by showing that no such polynomial exists.  To do so, it
generalizes an approximation theory result due to Rivlin and Cheney [
204 ] and (independently) Ehlich and Zeller [ 104 ] .  That result was
applied to query complexity by Nisan and Szegedy [ 184 ] and later by
Beals et al. [ 45 ] .

###### Theorem 3

@xmath

Proof. Let @xmath have range @xmath .  Then the quasilattice points
@xmath all lie in the rectangular region @xmath .  Recalling the
polynomial @xmath from Lemma 2 , define

  -- -------- --
     @xmath   
  -- -------- --

Suppose without loss of generality that we require

  -- -------- --
     @xmath   
  -- -------- --

(that is, algorithm @xmath distinguishes 1-to-1 from 2-to-1 functions
with error probability at most @xmath ).  Then, since

  -- -------- --
     @xmath   
  -- -------- --

by elementary calculus we have

  -- -------- --
     @xmath   
  -- -------- --

@xmath An inequality due to Markov (see [ 82 , 184 ] ) states that, for
a univariate polynomial @xmath , if @xmath for all @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

Clearly for every point @xmath , there exists a quasilattice point
@xmath for which

  -- -------- --
     @xmath   
  -- -------- --

For take @xmath —or, in the special case @xmath , take @xmath , since
there is only one quasilattice point with @xmath . Furthermore, since
@xmath represents an acceptance probability at such a point, we have

  -- -------- --
     @xmath   
  -- -------- --

Observe that for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

For consider a quasilattice point close to @xmath , and note that the
maximum-magnitude derivative is at most @xmath in the @xmath direction
and @xmath in the @xmath direction.

Let @xmath be a point in @xmath at which the weighted maximum-magnitude
derivative @xmath is attained.  Suppose first that the maximum is
attained in the @xmath direction.  Then @xmath (with @xmath constant) is
a univariate polynomial with

  -- -------- --
     @xmath   
  -- -------- --

for some @xmath .  So

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Similarly, suppose the maximum @xmath is attained in the @xmath
direction.  Then @xmath (with @xmath constant) is a univariate
polynomial with

  -- -------- --
     @xmath   
  -- -------- --

for some @xmath .  So

  -- -------- -------- --
     @xmath            
              @xmath   
  -- -------- -------- --

One can show that the lower bound on @xmath is optimized when we take
@xmath .  Then

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

and we are done.

#### 14 Set Comparison

Here I sketch a proof that @xmath , where @xmath is the set comparison
problem of size @xmath as defined in Section 10.2 .

The idea is the following.  We need a distribution of inputs with a
parameter @xmath , such that the inputs are one-to-one when @xmath or
@xmath —since otherwise the problem of distinguishing @xmath from @xmath
would be ill-defined for erasing oracles.  On the other hand, the inputs
must not be one-to-one for all @xmath —since otherwise the lower bound
for standard oracles would apply also to erasing oracles, and we would
not obtain a separation between the two.  Finally, the acceptance
probability must be close to a polynomial in @xmath .

The solution is to consider @xmath -to-one inputs, where

  -- -------- --
     @xmath   
  -- -------- --

is a quadratic with @xmath .  The total range of the inputs (on
sequences @xmath and @xmath combined) has size roughly @xmath ; thus, we
can tell the @xmath inputs apart from the @xmath inputs using an erasing
oracle, even though @xmath is the same for both.  The disadvantage is
that, because @xmath increases quadratically rather than linearly in
@xmath , the quasilattice points become sparse more quickly.  That is
what weakens the lower bound from @xmath to @xmath .  Note that, using
the ideas of Shi [ 218 ] , one can improve my lower bound on @xmath to
@xmath .

Call @xmath an @xmath -super-quasilattice point if and only if

1.  @xmath is an integer in @xmath ,

2.  @xmath and @xmath are integers in @xmath ,

3.  @xmath divides @xmath ,

4.  if @xmath then @xmath ,

5.  @xmath divides @xmath , and

6.  if @xmath then @xmath .

For super-quasilattice point @xmath , we draw input @xmath from
distribution @xmath as follows.  We first choose a set @xmath with
@xmath uniformly at random.  We then choose two sets @xmath with @xmath
, uniformly at random and independently.  Next we choose @xmath -1
functions @xmath @xmath and @xmath @xmath uniformly at random and
independently.  Finally we let @xmath and @xmath for each @xmath .

Define sets @xmath and @xmath .  Suppose @xmath and @xmath ; then by
Chernoff bounds,

  -- -------- --
     @xmath   
  -- -------- --

Thus, if algorithm @xmath can distinguish @xmath from @xmath with
probability at least @xmath , then it can distinguish @xmath from @xmath
with probability at least @xmath .  So a lower bound for the latter
problem implies an equivalent lower bound for the former.

Define @xmath to be the probability that the algorithm returns that
@xmath and @xmath are far on input @xmath , and let

  -- -------- --
     @xmath   
  -- -------- --

We then have

###### Lemma 4

For all sufficiently large @xmath and if @xmath , there exists a
trivariate polynomial @xmath of degree at most @xmath such that if
@xmath is a super-quasilattice point, then

  -- -------- --
     @xmath   
  -- -------- --

for some constant @xmath .

Proof Sketch. By analogy to Lemma 1 , @xmath is a multilinear polynomial
of degree at most @xmath over variables of the form @xmath and @xmath .
 Let @xmath where @xmath is a product of @xmath distinct @xmath
variables and @xmath is a product of @xmath distinct @xmath variables.
 Let @xmath .  Define

  -- -------- --
     @xmath   
  -- -------- --

then

  -- -------- --
     @xmath   
  -- -------- --

for some coefficients @xmath .  We now calculate @xmath .  As before we
assume there are no pairs of variables @xmath with @xmath .  Let @xmath
be the range of @xmath and let @xmath be the range of @xmath .  Then let
@xmath .  Let @xmath , @xmath , and @xmath .  By assumption

  -- -------- --
     @xmath   
  -- -------- --

so

  -- -------- --
     @xmath   
  -- -------- --

The probabilities that @xmath given @xmath and @xmath given @xmath can
be calculated similarly.

Let @xmath be the multiplicities of the range elements in @xmath , so
that

  -- -------- --
     @xmath   
  -- -------- --

Then

  -- -------- --
     @xmath   
  -- -------- --

and similarly for @xmath .

Putting it all together and manipulating, we obtain (analogously to
Lemma 1 ) that

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a trivariate polynomial in @xmath of total degree at
most @xmath .  Thus

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a polynomial of total degree at most @xmath .  The
argument that @xmath approximates @xmath to within a constant is
analogous to that of Lemma 2 .

The remainder of the lower bound argument follows the lines of Theorem 3
.

###### Theorem 5

@xmath .

Proof Sketch. Let @xmath for some @xmath .  Then the super-quasilattice
points @xmath all lie in @xmath .  Define @xmath to be

  -- -------- --
     @xmath   
  -- -------- --

Then @xmath for some constant @xmath , by Lemma 4 .

For every point @xmath , there exists a super-quasilattice point @xmath
such that @xmath , @xmath , and @xmath Hence, @xmath can deviate from
@xmath by at most

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath be a point in @xmath at which @xmath is attained.  Suppose
@xmath is attained in the @xmath direction; the cases of the @xmath and
@xmath directions are analogous.  Then @xmath is a univariate polynomial
in @xmath , and

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

One can show that the bound is optimized when we take @xmath .  Then

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

#### 15 Open Problems

In my original paper on the collision problem, I listed four open
problems: improving the collision lower bound to @xmath ; showing any
nontrivial quantum lower bound for the set equality problem; proving a
time-space tradeoff lower bound for the collision problem; and deciding
whether quantum query complexity and degree as a real polynomial are
always asymptotically equal.  Happily, three of these problems have
since been resolved [ 161 , 176 , 28 ] , but the time-space tradeoff
remains wide open.  We would like to say (for example) that if a quantum
computer is restricted to using @xmath qubits, then it needs @xmath
queries for the collision problem, ordinary Grover search being the best
possible algorithm.  Currently, we cannot show such a result for any
problem with Boolean output, only for problems such as sorting with a
large non-Boolean output [ 156 ] .

Another problem is to give an oracle relative to which @xmath , where
@xmath is Quantum Merlin Arthur as defined in [ 239 ] .  In other words,
show that if a function is one-to-one rather than two-to-one, then this
fact cannot be verified using a small number of quantum queries, even
with the help of a succinct quantum proof.

Finally, is it the case that for all (partial or total) functions @xmath
that are invariant under permutation symmetry, @xmath and @xmath are
polynomially related?

### Chapter \thechapter Local Search

This chapter deals with the following problem.

Local Search. Given an undirected graph @xmath and function @xmath ,
find a local minimum of @xmath —that is, a vertex @xmath such that
@xmath for all neighbors @xmath of @xmath .

We will be interested in the number of queries that an algorithm needs
to solve this problem, where a query just returns @xmath given @xmath .
 We will consider deterministic, randomized, and quantum algorithms.
 Section 16 motivates the problem theoretically and practically; this
section explains the results.

First, though, we need some simple observations.  If @xmath is the
complete graph of size @xmath , then clearly @xmath queries are needed
to find a local minimum (or @xmath with a quantum computer).  At the
other extreme, if @xmath is a line of length @xmath , then even a
deterministic algorithm can find a local minimum in @xmath queries,
using binary search: query the middle two vertices, @xmath and @xmath .
 If @xmath , then search the line of length @xmath connected to @xmath ;
otherwise search the line connected to @xmath .  Continue recursively in
this manner until a local minimum is found.

So the interesting case is when @xmath is a graph of ‘intermediate’
connectedness: for example, the Boolean hypercube @xmath , with two
vertices adjacent if and only if they have Hamming distance @xmath
.  For this graph, Llewellyn, Tovey, and Trick [ 169 ] showed a @xmath
lower bound on the number of queries needed by any deterministic
algorithm, using a simple adversary argument.  Intuitively, until the
set of vertices queried so far comprises a vertex cut (that is, splits
the graph into two or more connected components), an adversary is free
to return a descending sequence of @xmath -values: @xmath for the first
vertex @xmath queried by the algorithm, @xmath for the second vertex
queried, and so on.  Moreover, once the set of queried vertices does
comprise a cut, the adversary can choose the largest connected component
of unqueried vertices, and restrict the problem recursively to that
component.  So to lower-bound the deterministic query complexity, it
suffices to lower-bound the size of any cut that splits the graph into
two reasonably large components. ¹⁹ ¹⁹ 19 Llewellyn et al. actually give
a tight characterization of deterministic query complexity in terms of
vertex cuts. For the Boolean hypercube, Llewellyn et al. showed that the
best one can do is essentially to query all @xmath vertices of Hamming
weight @xmath .

Llewellyn et al.’s argument fails completely in the case of randomized
algorithms.  By Yao’s minimax principle, what we want here is a fixed
distribution @xmath over functions @xmath , such that any deterministic
algorithm needs many queries to find a local minimum of @xmath , with
high probability if @xmath is drawn from @xmath .  Taking @xmath to be
uniform will not do, since a local minimum of a uniform random function
is easily found.  However, Aldous [ 24 ] had the idea of defining @xmath
via a random walk , as follows.  Choose a vertex @xmath uniformly at
random; then perform an unbiased walk ²⁰ ²⁰ 20 Actually, Aldous used a
continuous-time random walk, so the functions would be from @xmath to
@xmath . @xmath starting from @xmath .  For each vertex @xmath , set
@xmath equal to the first hitting time of the walk at @xmath —that is,
@xmath .  Clearly any @xmath produced in this way has a unique local
minimum at @xmath , since for all @xmath , if vertex @xmath is visited
for the first time at step @xmath then @xmath .  Using sophisticated
random walk analysis, Aldous managed to show a lower bound of @xmath on
the expected number of queries needed by any randomized algorithm to
find @xmath . ²¹ ²¹ 21 Independently and much later, Droste et al. [ 99
] showed the weaker bound @xmath for any @xmath . (As we will see in
Section 17 , this lower bound is close to tight.)  Intuitively, since a
random walk on the hypercube mixes in @xmath steps, an algorithm that
has not queried a @xmath with @xmath has almost no useful information
about where the unique minimum @xmath is, so its next query will just be
a “stab in the dark.”

However, Aldous’s result leaves several questions about Local Search
unanswered.  What if the graph @xmath is a @xmath -D cube, on which a
random walk does not mix very rapidly?  Can we still lower-bound the
randomized query complexity of finding a local minimum?  More generally,
what parameters of @xmath make the problem hard or easy?  Also, what is
the quantum query complexity of Local Search ?

This chapter presents a new approach to Local Search , which I believe
points the way to a complete understanding of its complexity.  The
approach is based on Ambainis’s quantum adversary method [ 27 ] .
 Surprisingly, the approach yields new and simpler lower bounds for the
problem’s classical randomized query complexity, in addition to quantum
lower bounds.  Thus, along with recent work by Kerenidis and de Wolf [
151 ] and by Aharonov and Regev [ 22 ] , the results of this chapter
illustrate how quantum ideas can help to resolve classical open
problems.

The results are as follows.  For the Boolean hypercube @xmath , I show
that any quantum algorithm needs @xmath queries to find a local minimum
on @xmath , and any randomized algorithm needs @xmath queries (improving
the @xmath lower bound of Aldous [ 24 ] ).  The proofs are elementary
and do not require random walk analysis.  By comparison, the best known
upper bounds are @xmath for a quantum algorithm and @xmath for a
randomized algorithm.  If @xmath is a @xmath -dimensional grid of size
@xmath , where @xmath is a constant, then I show that any quantum
algorithm needs @xmath queries to find a local minimum on @xmath , and
any randomized algorithm needs @xmath queries.  No nontrivial lower
bounds (randomized or quantum) were previously known in this case. ²² ²²
22 A lower bound on deterministic query complexity was known for such
graphs [ 168 ] .

In a preprint discussing these results, I raised as my “most
ambitious” conjecture that the deterministic and quantum query
complexities of local search are polynomially related for every family
of graphs.  At the time, it was not even known whether deterministic and
randomized query complexities were polynomially related, not even for
simple examples such as the @xmath -dimensional square grid.
 Subsequently Santha and Szegedy [ 211 ] spectacularly resolved the
conjecture, by showing that the quantum query complexity is always at
least the @xmath root (!) of the deterministic complexity.  On the other
hand, in the specific case of the hypercube, my lower bound is close to
tight; Santha and Szegedy’s is not.  Also, I give randomized lower
bounds that are quadratically better than my quantum lower bounds;
Santha and Szegedy give only quantum lower bounds.

In another recent development, Ambainis [ 25 ] has improved the @xmath
quantum lower bound for local search on the hypercube to @xmath , using
a hybrid argument.  Note that Ambainis’s lower bound matches the upper
bound up to a polynomial factor.

The chapter is organized as follows.  Section 16 motivates lower bounds
on Local Search , pointing out connections to simulated annealing,
quantum adiabatic algorithms, and the complexity class @xmath of total
function problems.  Section 17 defines notation and reviews basic facts
about Local Search , including upper bounds.  In Section 18 I give an
intuitive explanation of Ambainis’s quantum adversary method, then state
and prove a classical analogue of Ambainis’s main lower bound theorem.
 Section 19 introduces snakes , a construction by which I apply the two
adversary methods to Local Search .  I show there that to prove lower
bounds for any graph @xmath , it suffices to upper-bound a combinatorial
parameter @xmath of a ‘snake distribution’ on @xmath .  Section 20
applies this framework to specific examples of graphs: the Boolean
hypercube in Section 20.1 , and the @xmath -dimensional grid in Section
20.2 .

#### 16 Motivation

Local search is the most effective weapon ever devised against hard
optimization problems.  For many real applications, neither backtrack
search, nor approximation algorithms, nor even Grover’s algorithm can
compare.  Furthermore, along with quantum computing, local search
(broadly defined) is one of the most interesting links between computer
science and Nature.  It is related to evolutionary biology via genetic
algorithms, and to the physics of materials via simulated annealing.
 Thus it is both practically and scientifically important to understand
its performance.

The conventional wisdom is that, although local search performs well in
practice, its central (indeed defining) flaw is a tendency to get stuck
at local optima.  If this were correct, one corollary would be that the
reason local search performs so well is that the problem it really
solves—finding a local optimum—is intrinsically easy.  It would thus be
unnecessary to seek further explanations for its performance.  Another
corollary would be that, for unimodal functions (which have no local
optima besides the global optimum), the global optimum would be easily
found.

However, the conventional wisdom is false.  The results of Llewellyn et
al. [ 169 ] and Aldous [ 24 ] show that even if @xmath is unimodal, any
classical algorithm that treats @xmath as a black box needs exponential
time to find the global minimum of @xmath in general.  My results extend
this conclusion to quantum algorithms.  In my view, the practical upshot
of these results is that they force us to confront the question: What is
it about ‘real-world’ problems that makes it easy to find a local
optimum?  That is, why do exponentially long chains of descending
values, such as those used for lower bounds, almost never occur in
practice, even in functions with large range sizes?  One possibility is
that the functions that occur in practice look “globally” like random
functions, but I do not know whether that is true in any meaningful
sense.

The results of this chapter are also relevant for physics.  Many
physical systems, including folding proteins and networks of springs and
pulleys, can be understood as performing ‘local search’ through an
energy landscape to reach a locally-minimal energy configuration.  A key
question is, how long will the system take to reach its ground state
(that is, a globally-minimal configuration)?  Of course, if there are
local optima, the system might never reach its ground state, just as a
rock in a mountain crevice does not roll to the bottom by going up
first.  But what if the energy landscape is unimodal?  And moreover,
what if the physical system is quantum?  My results show that, for
certain energy landscapes, even a quantum system would take exponential
time to reach its ground state, regardless of what external Hamiltonian
is applied to “drive” it.  So in particular, the quantum adiabatic
algorithm proposed by Farhi et al. [ 106 ] , which can be seen as a
quantum analogue of simulated annealing, needs exponential time to find
a local minimum in the worst case.

Finally, this chapter’s results have implications for so-called total
function problems in complexity theory.  Megiddo and Papadimitriou [ 174
] defined a complexity class @xmath , consisting (informally) of those
@xmath search problems for which a solution always exists.  For example,
we might be given a function @xmath as a Boolean circuit, and asked to
find any distinct @xmath pair such that @xmath .  This particular
problem belongs to a subclass of @xmath called @xmath (Polynomial
Pigeonhole Principle).  Notice that no promise is involved: the
combinatorial nature of the problem itself forces a solution to exist,
even if we have no idea how to find it.  In a recent talk, Papadimitriou
[ 187 ] asked broadly whether such ‘nonconstructive existence problems’
might be good candidates for efficient quantum algorithms.   In the case
of @xmath problems like the one above, the collision lower bound of
Chapter I implies a negative answer in the black-box setting.  For other
subclasses of @xmath , such as @xmath (Polynomial Odd-Degree Node), a
quantum black-box lower bound follows easily from the optimality of
Grover’s search algorithm.

However, there is one important subclass of @xmath for which no quantum
lower bound was previously known.  This is @xmath (Polynomial Local
Search), defined by Johnson, Papadimitriou, and Yannakakis [ 149 ] as a
class of optimization problems whose cost function @xmath and
neighborhood function @xmath (that is, the set of neighbors of a given
point) are both computable in polynomial time. ²³ ²³ 23 Some authors
require only the minimum neighbor of a given point to be computable in
polynomial time, which does not seem like the “right” idealization to
me.  In any case, for lower bound purposes we always assume the
algorithm knows the whole neighborhood structure in advance, and does
not need to make queries to learn about it. Given such a problem, the
task is to output any local minimum of the cost function: that is, a
@xmath such that @xmath for all @xmath .  The lower bound of Llewellyn
et al. [ 169 ] yields an oracle @xmath relative to which @xmath , by a
standard diagonalization argument along the lines of Baker, Gill, and
Solovay [ 41 ] .  Likewise, the lower bound of Aldous [ 24 ] yields an
oracle relative to which @xmath , where @xmath is simply the function
version of @xmath .  The results of this chapter yield the first oracle
relative to which @xmath .  In light of this oracle separation, I raise
an admittedly vague question: is there a nontrivial
“combinatorial” subclass of @xmath that we can show is contained in
@xmath ?

#### 17 Preliminaries

In the Local Search problem, we are given an undirected graph @xmath
with @xmath , and oracle access to a function @xmath .  The goal is to
find any local minimum of @xmath , defined as a vertex @xmath such that
@xmath for all neighbors @xmath of @xmath .  Clearly such a local
minimum exists.  We want to find one using as few queries as possible,
where a query returns @xmath given @xmath .  Queries can be adaptive;
that is, can depend on the outcomes of previous queries.  We assume
@xmath is known in advance, so that only @xmath needs to be queried.
 Since we care only about query complexity, not computation time, there
is no difficulty in dealing with an infinite range for @xmath —though
for lower bound purposes, it will turn out that a range of size @xmath
suffices.  I do not know of any case where a range larger than this
makes the Local Search problem harder, but I also do not know of a
general reduction from large to small range.

The model of query algorithms is the standard one.  Given a graph @xmath
, the deterministic query complexity of Local Search on @xmath , which
we denote @xmath , is @xmath where the minimum ranges over all
deterministic algorithms @xmath , the maximum ranges over all @xmath ,
and @xmath is the number of queries made to @xmath by @xmath before it
halts and outputs a local minimum of @xmath (or @xmath if @xmath fails
to do so).  The randomized query complexity @xmath is defined similarly,
except that now the algorithm has access to an infinite random string
@xmath , and must only output a local minimum with probability at least
@xmath over @xmath .  For simplicity, one can assume that the number of
queries @xmath is the same for all @xmath ; clearly this assumption
changes the complexity by at most a constant factor.

In the quantum model, an algorithm’s state has the form @xmath , where
@xmath is the label of a vertex in @xmath , and @xmath and @xmath are
strings representing the answer register and workspace respectively.
 The @xmath ’s are complex amplitudes satisfying @xmath .  Starting from
an arbitrary (fixed) initial state, the algorithm proceeds by an
alternating sequence of queries and algorithm steps .  A query maps each
@xmath to @xmath , where @xmath denotes bitwise exclusive-OR.  An
algorithm step multiplies the vector of @xmath ’s by an arbitrary
unitary matrix that does not depend on @xmath .  Letting @xmath denote
the set of local minima of @xmath , the algorithm succeeds if at the end
@xmath .  Then the bounded-error quantum query complexity, or @xmath ,
is defined as the minimum number of queries used by a quantum algorithm
that succeeds on every @xmath .

It is immediate that @xmath .  Also, letting @xmath be the maximum
degree of @xmath , we have the following trivial lower bound.

###### Proposition 6

@xmath and @xmath .

Proof. Let @xmath be a vertex of @xmath with degree @xmath .  Choose a
neighbor @xmath of @xmath uniformly at random, and let @xmath .  Let
@xmath , and @xmath for all neighbors @xmath of @xmath other than @xmath
.  Let @xmath be the neighbor set of @xmath (including @xmath itself);
then for all @xmath , let @xmath where @xmath is the minimum distance
from @xmath to a vertex in @xmath .  Clearly @xmath has a unique local
minimum at @xmath .  However, finding @xmath requires exhaustive search
among the @xmath neighbors of @xmath , which takes @xmath quantum
queries by Bennett et al. [ 51 ] .

A corollary of Proposition 6 is that classically, zero-error randomized
query complexity is equivalent to bounded-error up to a constant factor.
 For given a candidate local minimum @xmath , one can check using @xmath
queries that @xmath is indeed a local minimum.  Since @xmath queries are
needed anyway, this verification step does not affect the overall
complexity.

As pointed out by Aldous [ 24 ] , a classical randomized algorithm can
find a local minimum of @xmath with high probability in @xmath queries.
 The algorithm just queries @xmath vertices uniformly at random, and
lets @xmath be a queried vertex for which @xmath is minimal.  It then
follows @xmath to a local minimum by steepest descent.  That is, for
@xmath , it queries all neighbors of @xmath , halts if @xmath is a local
minimum, and otherwise sets @xmath to be the neighbor @xmath of @xmath
for which @xmath is minimal (breaking ties by lexicographic ordering).
 A similar idea yields an improved quantum upper bound.

###### Proposition 7

For any @xmath , @xmath .

Proof. The algorithm first chooses @xmath vertices of @xmath uniformly
at random, then uses Grover search to find a chosen vertex @xmath for
which @xmath is minimal.  By a result of Dürr and Høyer [ 102 ] , this
can be done with high probability in @xmath queries.  Next, for @xmath ,
the algorithm performs Grover search over all neighbors of @xmath ,
looking for a neighbor @xmath such that @xmath .  If it finds such a
@xmath , then it sets @xmath and continues to the next iteration.
 Otherwise, it repeats the Grover search @xmath times before finally
giving up and returning @xmath as a claimed local minimum.

The expected number of @xmath such that @xmath is at most @xmath .
 Since @xmath for all @xmath , clearly the number of such @xmath
provides an upper bound on @xmath .  Furthermore, assuming there exists
a @xmath such that @xmath , the expected number of repetitions of
Grover’s algorithm until such a @xmath is found is @xmath .  Since each
repetition takes @xmath queries, by linearity of expectation the total
expected number of queries used by the algorithm is therefore

  -- -------- --
     @xmath   
  -- -------- --

or @xmath .  To see that the algorithm finds a local minimum with high
probability, observe that for each @xmath , the probability of not
finding a @xmath such that @xmath , given that one exists, is at most
@xmath for a suitable constant @xmath .  So by the union bound, the
probability that the algorithm returns a ‘false positive’ is at most
@xmath .

#### 18 Relational Adversary Method

There are essentially two known methods for proving lower bounds on
quantum query complexity: the polynomial method of Beals et al. [ 45 ] ,
and the quantum adversary method of Ambainis [ 27 ] . ²⁴ ²⁴ 24 I am
thinking here of the hybrid method [ 51 ] as a cousin of the adversary
method. For a few problems, such as the collision problem [ 2 , 218 ] ,
the polynomial method succeeded where the adversary method failed.
 However, for problems that lack permutation symmetry (such as Local
Search ), the adversary method has proven more effective. ²⁵ ²⁵ 25
Indeed, Ambainis [ 28 ] has given problems for which the adversary
method provably yields a better lower bound than the polynomial method.

How could a quantum lower bound method possibly be applied classically?
 When proving randomized lower bounds, the tendency is to attack
“bare-handed”: fix a distribution over inputs, and let @xmath be the
locations queried so far by the algorithm.  Show that for small @xmath ,
the posterior distribution over inputs, conditioned on @xmath , is still
‘hard’ with high probability—so that the algorithm knows almost nothing
even about which location @xmath to query next.  This is essentially the
approach taken by Aldous [ 24 ] to prove a @xmath lower bound on @xmath
.

In the quantum case, however, it is unclear how to specify what an
algorithm ‘knows’ after a given number of queries.  So we are almost
forced to step back, and identify general combinatorial properties of
input sets that make them hard to distinguish.  Once we have such
properties, we can then try to exhibit them in functions of interest.

We will see, somewhat surprisingly, that this “gloved” approach is
useful for classical lower bounds as well as quantum ones.  In the
relational adversary method , we assume there exists a @xmath -query
randomized algorithm for function @xmath .  We consider a set @xmath of
@xmath -inputs of @xmath , a set @xmath of @xmath -inputs, and an
arbitrary real-valued relation function @xmath for @xmath and @xmath .
 Intuitively, @xmath should be large if @xmath and @xmath differ in only
a few locations.  We then fix a probability distribution @xmath over
inputs; by Yao’s minimax principle, there exists a @xmath -query
deterministic algorithm @xmath that succeeds with high probability on
inputs drawn from @xmath .  Let @xmath be the set of @xmath -inputs and
@xmath the set of @xmath -inputs on which @xmath succeeds.  Using the
relation function @xmath , we define a separation measure @xmath between
@xmath and @xmath , and show that (1) initially @xmath , (2) by the end
of the computation @xmath must be large, and (3) @xmath increases by
only a small amount as the result of each query.  It follows that @xmath
must be large.

The advantage of the relational method is that converts a
“dynamic” opponent—an algorithm that queries adaptively—into a
relatively static one.  It thereby makes it easier to focus on what is
unique about a problem, aspects of query complexity that are common to
all problems having been handled automatically.  Furthermore, one does
not need to know anything about quantum computing to understand and
apply the method.  On the other hand, I have no idea how one would come
up with it in the first place, without Ambainis’s quantum adversary
method [ 27 ] and the reasoning about entanglement that led to it.

The starting point is the “most general” adversary theorem in Ambainis’s
original paper (Theorem 6 in [ 27 ] ), which he introduced to prove a
quantum lower bound for the problem of inverting a permutation.  Here
the input is a permutation @xmath , and the task is to output @xmath if
@xmath and @xmath otherwise.  To lower-bound this problem’s query
complexity, what we would like to say is this:

Given any @xmath -input @xmath and any location @xmath , if we choose a
random @xmath -input @xmath that is ‘related’ to @xmath , then the
probability @xmath over @xmath that @xmath does not equal @xmath is
small.  In other words, the algorithm is unlikely to distinguish @xmath
from a random neighbor @xmath of @xmath by querying @xmath .

Unfortunately, the above claim is false.  Letting @xmath , we have that
@xmath for every @xmath -input @xmath , and thus @xmath .  Ambainis
resolves this difficulty by letting us take the maximum, over all @xmath
-inputs @xmath and @xmath -inputs @xmath that are related and differ at
@xmath , of the geometric mean @xmath .  Even if @xmath , the geometric
mean is still small provided that @xmath is small.  More formally:

###### Theorem 8 (Ambainis)

Let @xmath and @xmath be sets of inputs to function @xmath .  Let @xmath
be a symmetric real-valued function, and for @xmath , @xmath , and
location @xmath , let

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where the denominators are all nonzero.  Then the number of quantum
queries needed to evaluate @xmath with at least @xmath probability is
@xmath , where

  -- -------- --
     @xmath   
  -- -------- --

The best way to understand Theorem 8 is to see it used in an example.

###### Proposition 9 (Ambainis)

The quantum query complexity of inverting a permutation is @xmath .

Proof. Let @xmath be the set of all permutations @xmath such that @xmath
@xmath , and @xmath be the set of permutations @xmath such that @xmath .
 Given @xmath and @xmath , let @xmath if @xmath and @xmath differ only
at locations @xmath and @xmath , and @xmath otherwise.  Then given
@xmath with @xmath , if @xmath then @xmath , and if @xmath then @xmath .
 So @xmath .

The only difference between Theorem 8 and my relational adversary
theorem is that in the latter, we take the minimum of @xmath and @xmath
instead of the geometric mean.  Taking the reciprocal then gives up to a
quadratically better lower bound: for example, we obtain that the
randomized query complexity of inverting a permutation is @xmath .
 However, the proofs of the two theorems are quite different.

###### Theorem 10

Let @xmath be as in Theorem 8 .  Then the number of randomized queries
needed to evaluate @xmath with at least @xmath probability is @xmath ,
where

  -- -------- --
     @xmath   
  -- -------- --

Proof. Let @xmath be a randomized algorithm that, given an input @xmath
, returns @xmath with at least @xmath probability.  Let @xmath be the
number of queries made by @xmath .  For all @xmath , @xmath , define

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Now let @xmath be the distribution over @xmath in which each @xmath is
chosen with probability @xmath ; and let @xmath be the distribution over
@xmath in which each @xmath is chosen with probability @xmath .  Let
@xmath be an equal mixture of @xmath and @xmath .  By Yao’s minimax
principle, there exists a deterministic algorithm @xmath that makes
@xmath queries, and succeeds with at least @xmath probability given an
input drawn from @xmath .  Therefore @xmath succeeds with at least
@xmath probability given an input drawn from @xmath alone, or from
@xmath alone.  In other words, letting @xmath be the set of @xmath and
@xmath the set of @xmath on which @xmath succeeds, we have

  -- -------- --
     @xmath   
  -- -------- --

Define a predicate @xmath , which is true if @xmath has distinguished
@xmath from @xmath by the @xmath query and false otherwise.  (To
distinguish @xmath from @xmath means to query an index @xmath for which
@xmath , given either @xmath or @xmath as input.)  Also, for all @xmath
, define a score function

  -- -------- --
     @xmath   
  -- -------- --

This function measures how much “progress” has been made so far in
separating @xmath from @xmath -inputs, where the @xmath -inputs are
weighted by @xmath .  Similarly, for all @xmath define

  -- -------- --
     @xmath   
  -- -------- --

It is clear that for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

So we can denote the above sum by @xmath and think of it as a global
progress measure.  The proof relies on the following claims about @xmath
:

1.  @xmath initially.

2.  @xmath by the end.

3.  @xmath for all @xmath , where @xmath is the amount by which @xmath
    increases as the result of a single query.

It follows from (i)-(iii) that

  -- -------- --
     @xmath   
  -- -------- --

which establishes the theorem.  Part (i) is obvious.  For part (ii),
observe that for every pair @xmath with @xmath and @xmath , the
algorithm @xmath must query an @xmath such that @xmath .  Thus

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

It remains only to show part (iii).  Suppose @xmath for some @xmath ; we
will obtain a contradiction.  Let

  -- -------- --
     @xmath   
  -- -------- --

and let @xmath be the set of @xmath for which @xmath .  Since

  -- -------- --
     @xmath   
  -- -------- --

it follows by Markov’s inequality that

  -- -------- --
     @xmath   
  -- -------- --

Similarly, if we let @xmath be the set of @xmath for which @xmath , we
have

  -- -------- --
     @xmath   
  -- -------- --

In other words, at least @xmath of the increase in @xmath comes from
@xmath pairs such that @xmath , and at least @xmath comes from @xmath
pairs such that @xmath .  Hence, by a ‘pigeonhole’ argument, there
exists an @xmath and @xmath with @xmath that are distinguished by the
@xmath query.  In other words, there exists an @xmath with @xmath , such
that the @xmath index queried by @xmath is @xmath whether the input is
@xmath or @xmath .  Then since @xmath , we have @xmath , and hence

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

which equals @xmath .  Similarly @xmath since @xmath .  This contradicts
the definition

  -- -------- --
     @xmath   
  -- -------- --

and we are done.

#### 19 Snakes

For the lower bounds, it will be convenient to generalize random walks
to arbitrary distributions over paths, which we call snakes .

###### Definition 11

Given a vertex @xmath in @xmath and a positive integer @xmath , a snake
distribution @xmath (parameterized by @xmath and @xmath ) is a
probability distribution over paths @xmath in @xmath , such that each
@xmath is either equal or adjacent to @xmath , and @xmath .  Let @xmath
be the support of @xmath .  Then an element of @xmath is called a snake;
the part near @xmath is the tail and the part near @xmath is the head.

Given a snake @xmath and integer @xmath , we use @xmath as shorthand for
@xmath .

###### Definition 12

We say a snake @xmath is @xmath -good if the following holds.  Choose
@xmath uniformly at random from @xmath , and let @xmath be a snake drawn
from @xmath conditioned on @xmath for all @xmath .  Then

1.   Letting @xmath be the set of vertices @xmath in @xmath such that
    @xmath , we have

      -- -------- --
         @xmath   
      -- -------- --

2.   For all vertices @xmath , @xmath .

The procedure above—wherein we choose a @xmath uniformly at random, then
draw a @xmath from @xmath consistent with @xmath on all steps later than
@xmath —will be important in what follows.  I call it the snake @xmath
flicking its tail .  Intuitively, a snake is good if it is spread out
fairly evenly in @xmath —so that when it flicks its tail, (1) with high
probability the old and new tails do not intersect, and (2) any
particular vertex is hit by the new tail with probability at most @xmath
.

I now explain the ‘snake method’ for proving lower bounds for Local
Search .  Given a snake @xmath , we define an input @xmath with a unique
local minimum at @xmath , and @xmath -values that decrease along @xmath
from head to tail.  Then, given inputs @xmath and @xmath with @xmath ,
we let the relation function @xmath be proportional to the probability
that snake @xmath is obtained by @xmath flicking its tail.  (If @xmath
we let @xmath .)  Let @xmath and @xmath be inputs with @xmath , and let
@xmath be a vertex such that @xmath .  Then if all snakes were good,
there would be two mutually exclusive cases: (1) @xmath belongs to the
tail of @xmath , or (2) @xmath belongs to the tail of @xmath .  In case
(1), @xmath is hit with small probability when @xmath flicks its tail,
so @xmath is small.  In case (2), @xmath is hit with small probability
when @xmath flicks its tail, so @xmath is small.  In either case, then,
the geometric mean @xmath and minimum @xmath are small.  So even though
@xmath or @xmath could be large individually, Theorems 8 and 10 yield a
good lower bound, as in the case of inverting a permutation (see Figure
7.1).

One difficulty is that not all snakes are good; at best, a large
fraction of them are.  We could try deleting all inputs @xmath such that
@xmath is not good, but that might ruin some remaining inputs, which
would then have fewer neighbors.  So we would have to delete those
inputs as well, and so on ad infinitum.  What we need is basically a way
to replace “all inputs” by “most inputs” in Theorems 8 and 10 .

Fortunately, a simple graph-theoretic lemma can accomplish this.  The
lemma (see Diestel [ 98 , p.6] for example) says that any graph with
average degree at least @xmath contains an induced subgraph with minimum
degree at least @xmath .  Below I prove a weighted analogue of the
lemma.

###### Lemma 13

Let @xmath be positive reals summing to @xmath .  Also let @xmath for
@xmath be nonnegative reals satisfying @xmath and @xmath .  Then there
exists a nonempty subset @xmath such that for all @xmath , @xmath

Proof. If @xmath then the lemma trivially holds, so assume @xmath .  We
construct @xmath via an iterative procedure.  Let @xmath .  Then for all
@xmath , if there exists an @xmath for which

  -- -------- --
     @xmath   
  -- -------- --

then set @xmath .  Otherwise halt and return @xmath .  To see that the
@xmath so constructed is nonempty, observe that when we remove @xmath ,
the sum @xmath decreases by @xmath , while @xmath decreases by at most

  -- -------- --
     @xmath   
  -- -------- --

So since @xmath was positive to begin with, it must still be positive at
the end of the procedure; hence @xmath must be nonempty.

I can now prove the main result of the section.

###### Theorem 14

Suppose a snake drawn from @xmath is @xmath -good with probability at
least @xmath .  Then

  -- -------- --
     @xmath   
  -- -------- --

Proof. Given a snake @xmath , we construct an input function @xmath as
follows.  For each @xmath , let @xmath ; and for each @xmath , let
@xmath where @xmath is the distance from @xmath to @xmath in @xmath .
 Clearly @xmath so defined has a unique local minimum at @xmath .  To
obtain a decision problem, we stipulate that querying @xmath reveals an
answer bit ( @xmath or @xmath ) in addition to @xmath ; the algorithm’s
goal is then to return the answer bit.  Obviously a lower bound for the
decision problem implies a corresponding lower bound for the search
problem. Let us first prove the theorem in the case that all snakes in
@xmath are @xmath - good.  Let @xmath be the probability of drawing
snake @xmath from @xmath .  Also, given snakes @xmath and @xmath , let
@xmath be the probability that @xmath , if @xmath is drawn from @xmath
conditioned on agreeing with @xmath on all steps later than @xmath .
 Then define

  -- -------- --
     @xmath   
  -- -------- --

The first claim is that @xmath is symmetric; that is, @xmath .  It
suffices to show that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath .  We can assume @xmath agrees with @xmath on all steps
later than @xmath , since otherwise @xmath .  Given an @xmath , let
@xmath denote the event that @xmath agrees with @xmath (or equivalently
@xmath ) on all steps later than @xmath , and let @xmath (resp. @xmath )
denote the event that @xmath agrees with @xmath (resp. @xmath ) on steps
@xmath to @xmath .  Then

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Now let @xmath denote the event that @xmath , where @xmath is as in
Definition 12 .  Also, let @xmath be the input obtained from @xmath that
has answer bit @xmath , and @xmath be the input that has answer bit
@xmath .  To apply Theorems 8 and 10 , take @xmath and @xmath .  Then
take @xmath if @xmath holds, and @xmath otherwise.  Given @xmath and
@xmath with @xmath , and letting @xmath be a vertex such that @xmath ,
we must then have either @xmath or @xmath .  Suppose the former case;
then

  -- -------- --
     @xmath   
  -- -------- --

since @xmath is @xmath -good.  Thus @xmath equals

  -- -------- --
     @xmath   
  -- -------- --

Similarly, if @xmath then @xmath by symmetry.  Hence

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

the latter since @xmath and @xmath for all @xmath and @xmath .

In the general case, all we know is that a snake drawn from @xmath is
@xmath - good with probability at least @xmath .  Let @xmath denote the
event that @xmath is @xmath -good.  Take @xmath and @xmath , and take
@xmath as before.  Then since

  -- -------- --
     @xmath   
  -- -------- --

by the union bound we have

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

So by Lemma 13 , there exist subsets @xmath and @xmath such that for all
@xmath and @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

So for all @xmath with @xmath , and all @xmath such that @xmath , either
@xmath or @xmath .  Hence @xmath and @xmath .

#### 20 Specific Graphs

In this section I apply the ‘snake method’ developed in Section 19 to
specific examples of graphs: the Boolean hypercube in Section 20.1 , and
the @xmath -dimensional cubic grid (for @xmath ) in Section 20.2 .

##### 20.1 Boolean Hypercube

Abusing notation, let @xmath denote the @xmath -dimensional Boolean
hypercube—that is, the graph whose vertices are @xmath -bit
strings, with two vertices adjacent if and only if they have Hamming
distance @xmath .  Given a vertex @xmath , let @xmath denote the @xmath
bits of @xmath , and let @xmath denote the neighbor obtained by flipping
bit @xmath .  In this section I lower-bound @xmath and @xmath .

Fix a ‘snake head’ @xmath and take @xmath .  I define the snake
distribution @xmath via what I call a coordinate loop , as follows.
 Starting from @xmath , for each @xmath take @xmath with @xmath
probability, and @xmath with @xmath probability.  The following is a
basic fact about this distribution.

###### Proposition 15

The coordinate loop mixes completely in @xmath steps, in the sense that
if @xmath , then @xmath is a uniform random vertex conditioned on @xmath
.

One could also use the random walk distribution, following Aldous [ 24 ]
.  However, not only is the coordinate loop distribution easier to work
with (since it produces fewer self-intersections), it also yields a
better lower bound (since it mixes completely in @xmath steps, as
opposed to approximately in @xmath steps).

I first upper-bound the probability, over @xmath , @xmath , and @xmath ,
that @xmath (where @xmath is as in Definition 12 ).

###### Lemma 16

Suppose @xmath is drawn from @xmath , @xmath is drawn uniformly from
@xmath , and @xmath is drawn from @xmath .  Then @xmath .

Proof. Call a disagreement a vertex @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

Clearly if there are no disagreements then @xmath .  If @xmath is a
disagreement, then by the definition of @xmath we cannot have both
@xmath and @xmath .  So by Proposition 15 , either @xmath is uniformly
random conditioned on @xmath , or @xmath is uniformly random conditioned
on @xmath .  Hence @xmath .  So by the union bound,

  -- -------- --
     @xmath   
  -- -------- --

I now argue that, unless @xmath spends a ‘pathological’ amount of time
in one part of the hypercube, the probability of any vertex @xmath being
hit when @xmath flicks its tail is small.  To prove this, I define a
notion of sparseness , and then show that (1) almost all snakes drawn
from @xmath are sparse (Lemma 18 ), and (2) sparse snakes are unlikely
to hit any given vertex @xmath (Lemma 19 ).

###### Definition 17

Given vertices @xmath and @xmath , let @xmath be the number of steps
needed to reach @xmath from @xmath by first setting @xmath , then
setting @xmath , and so on.  (After we set @xmath we wrap around to
@xmath .)  Then @xmath is sparse if there exists a constant @xmath such
that for all @xmath and all @xmath ,

  -- -- --
        
  -- -- --

###### Lemma 18

If @xmath is drawn from @xmath , then @xmath is sparse with probability
@xmath .

Proof. For each @xmath , the number of @xmath such that @xmath is at
most @xmath .  For such a @xmath , let @xmath be the event that @xmath ;
then @xmath holds if and only if

  -- -------- --
     @xmath   
  -- -------- --

(where we wrap around to @xmath after reaching @xmath ).  This occurs
with probability @xmath over @xmath .  Furthermore, by Proposition 15 ,
the @xmath events for different @xmath ’s are independent.  So let

  -- -------- --
     @xmath   
  -- -------- --

then for fixed @xmath , the expected number of @xmath ’s for which
@xmath holds is at most @xmath .  Thus by a Chernoff bound, if @xmath
then

  -- -------- --
     @xmath   
  -- -------- --

for sufficiently large @xmath .  Similarly, if @xmath then

  -- -- --
        
  -- -- --

for sufficiently large @xmath .  By the union bound, then,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

for every @xmath triple simultaneously with probability at least @xmath
.  Summing over all @xmath ’s produces the additional factor of @xmath .

###### Lemma 19

If @xmath is sparse, then for every @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Proof. By assumption, for every @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Consider the probability that @xmath in the event that @xmath .  Clearly

  -- -------- --
     @xmath   
  -- -------- --

Also, Proposition 15 implies that for every @xmath , the probability
that @xmath is @xmath .  So by the union bound,

  -- -------- --
     @xmath   
  -- -------- --

Then @xmath equals

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

as can be verified by breaking the sum into cases and doing some
manipulations.

The main result follows easily:

###### Theorem 20

  -- -------- --
     @xmath   
  -- -------- --

Proof. Take @xmath .  Then by Theorem 14 , it suffices to show that a
snake @xmath drawn from @xmath is @xmath -good with probability at least
@xmath .  First, since

  -- -------- --
     @xmath   
  -- -------- --

by Lemma 16 , Markov’s inequality shows that

  -- -------- --
     @xmath   
  -- -------- --

Second, by Lemma 18 , @xmath is sparse with probability @xmath , and by
Lemma 19 , if @xmath is sparse then

  -- -------- --
     @xmath   
  -- -------- --

for every @xmath .  So both requirements of Definition 12 hold
simultaneously with probability at least @xmath .

##### 20.2 Constant-Dimensional Grid Graph

In the Boolean hypercube case, @xmath was defined by a ‘coordinate loop’
instead of the usual random walk mainly for convenience.  When we move
to the @xmath -dimensional grid, though, the drawbacks of random walks
become more serious: first, the mixing time is too long, and second,
there are too many self-intersections, particularly if @xmath .  The
snake distribution will instead use straight lines of randomly chosen
lengths attached at the endpoints, as in Figure 7.2.

Let @xmath be a @xmath -dimensional grid graph with @xmath .  That is,
@xmath has @xmath vertices of the form @xmath , where each @xmath is in
@xmath (assume for simplicity that @xmath is a @xmath power).  Vertices
@xmath and @xmath are adjacent if and only if @xmath for some @xmath ,
and @xmath for all @xmath (so @xmath does not wrap around at the
boundaries).

Take @xmath , and define the snake distribution @xmath as follows.
 Starting from @xmath , for each @xmath take @xmath identical to @xmath
, but with the @xmath coordinate @xmath replaced by a uniform random
value in @xmath .  Then take the vertices @xmath to lie along the
shortest path from @xmath to @xmath , ‘stalling’ at @xmath once that
vertex has been reached.  Call

  -- -------- --
     @xmath   
  -- -------- --

a line of vertices, whose direction is @xmath .  As in the Boolean
hypercube case, we have:

###### Proposition 21

@xmath mixes completely in @xmath steps, in the sense that if @xmath ,
then @xmath is a uniform random vertex conditioned on @xmath .

Lemma 16 in Section 20.1 goes through essentially without change.

###### Definition 22

Letting @xmath be as before, we say @xmath is sparse if there exists a
constant @xmath (possibly dependent on @xmath ) such that for all
vertices @xmath and all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

###### Lemma 23

If @xmath is drawn from @xmath , then @xmath is sparse with probability
@xmath .

Proof. Similar to Lemma 18 .  Let @xmath be a line of vertices with
direction @xmath , and notice that @xmath is the same for every vertex
@xmath in @xmath .  Let @xmath denote the event that @xmath for the
@xmath ’s in @xmath .  Then @xmath occurs with probability @xmath over
@xmath .  Furthermore, if @xmath then @xmath and @xmath are independent
events.  So let

  -- -------- --
     @xmath   
  -- -------- --

then for fixed @xmath , the expected number of lines for which @xmath
holds is at most @xmath .  Thus, by a Chernoff bound, if @xmath then

  -- -------- --
     @xmath   
  -- -------- --

which is at most @xmath for sufficiently large @xmath .  Similarly, if
@xmath then letting @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

for sufficiently large @xmath .  So with probability @xmath it holds
that for all @xmath , letting @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

###### Lemma 24

If @xmath is sparse, then for every @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where the big- @xmath hides a constant dependent on @xmath .

Proof. As in Lemma 19 , setting @xmath we obtain that @xmath equals

  -- -------- --
              
     @xmath   
     @xmath   
  -- -------- --

By the same proof as for Theorem 20 , taking @xmath yields the
following:

###### Theorem 25

Neglecting a constant dependent on @xmath , for all @xmath

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

### Chapter \thechapter Quantum Certificate Complexity

This chapter studies the relationships between classical and quantum
measures of query complexity.  Let @xmath be a Boolean function with
@xmath , that takes input @xmath .  Then the deterministic query
complexity @xmath is the minimum number of queries to the @xmath ’s
needed to evaluate @xmath , if @xmath is chosen adversarially and if
queries can be adaptive (that is, can depend on the outcomes of previous
queries).  Also, the bounded-error randomized query complexity, @xmath ,
is the minimum expected number of queries needed by a randomized
algorithm that, for each @xmath , outputs @xmath with probability at
least @xmath .  Here the ‘ @xmath ’ refers to two-sided error; if
instead we require @xmath to be output with probability @xmath for every
@xmath , we obtain @xmath , or zero-error randomized query complexity.

Analogously, @xmath is the minimum number of queries needed by a quantum
algorithm that outputs @xmath with probability at least @xmath for all
@xmath .  Also, for @xmath let @xmath be the minimum number of queries
needed by a quantum algorithm that outputs @xmath with probability
@xmath if @xmath , and with probability at least @xmath if @xmath
.  Then let @xmath .  If we require a single algorithm that succeeds
with probability @xmath for all @xmath , we obtain @xmath , or exact
quantum query complexity.  See Buhrman and de Wolf [ 78 ] for a more
detailed survey of these measures.

It is immediate that

  -- -------- --
     @xmath   
  -- -------- --

that @xmath , and that @xmath .  If @xmath is partial (i.e. @xmath ),
then @xmath can be superpolynomially smaller than @xmath ; this is what
makes Shor’s period-finding algorithm [ 219 ] possible.  For total
@xmath , by contrast, the largest known gap even between @xmath and
@xmath is quadratic, and is achieved by the @xmath function on @xmath
bits: @xmath (indeed @xmath ), whereas @xmath because of Grover’s search
algorithm [ 139 ] .  Furthermore, for total @xmath , Beals et al. [ 45 ]
showed that @xmath , while de Wolf [ 244 ] showed that @xmath .

The result of Beals et al. [ 45 ] relies on two intermediate complexity
measures, the certificate complexity @xmath and block sensitivity @xmath
, which are defined as follows.

###### Definition 26

A certificate for an input @xmath is a set @xmath such that for all
inputs @xmath of @xmath , if @xmath for all @xmath then @xmath .  Then
@xmath is the minimum size of a certificate for @xmath , and @xmath is
the maximum of @xmath over all @xmath .

###### Definition 27

A sensitive block on input @xmath is a set @xmath such that @xmath ,
where @xmath is obtained from @xmath by flipping @xmath for each @xmath
.  Then @xmath is the maximum number of disjoint sensitive blocks on
@xmath , and @xmath is the maximum of @xmath over all @xmath .

Clearly @xmath .  For total @xmath , these measures are all polynomially
related: Nisan [ 183 ] showed that @xmath , while Beals et al. [ 45 ]
showed that @xmath .  Combining these results with @xmath (from the
optimality of Grover’s algorithm), one obtains @xmath .

#### 21 Summary of Results

I investigate @xmath and @xmath , the bounded-error randomized and
quantum generalizations of the certificate complexity @xmath (see Table
8.1).  My motivation is that, just as @xmath was used to show a
polynomial relation between @xmath and @xmath , so @xmath and @xmath can
lead to new relations among fundamental query complexity measures.

What the certificate complexity @xmath measures is the number of queries
used to verify a certificate, not the number of bits used to communicate
it.  Thus, if we want to generalize @xmath , we should assume the latter
is unbounded.  A consequence is that without loss of generality, a
certificate is just a claimed value @xmath for the input @xmath ²⁶ ²⁶ 26
Throughout this chapter, I use @xmath to denote the ‘actual’ input being
queried, and @xmath to denote the ‘claimed’ input. —since any additional
information that a prover might provide, the verifier can compute for
itself.  The verifier’s job is to check that @xmath .  With this in mind
I define @xmath as follows.

###### Definition 28

A randomized verifier for input @xmath is a randomized algorithm that,
on input @xmath to @xmath , (i) accepts with probability @xmath if
@xmath , and (ii) rejects with probability at least @xmath if @xmath .
 (If @xmath but @xmath , the acceptance probability can be arbitrary.)
@xmath Then @xmath is the minimum expected number of queries used by a
randomized verifier for @xmath , and @xmath is the maximum of @xmath
over all @xmath .

I define @xmath analogously, with quantum instead of randomized
algorithms.  The following justifies the definition (the @xmath part was
originally shown by Raz et al. [ 197 ] ).

###### Proposition 29

Making the error probability two-sided rather than one-sided changes
@xmath and @xmath by at most a constant factor.

Proof. For @xmath , let @xmath be the event that verifier @xmath rejects
on input @xmath , and let @xmath be the event that @xmath encounters a
disagreement with @xmath on @xmath .  We may assume @xmath .  Suppose
that @xmath if @xmath and @xmath if @xmath .  We wish to lower-bound
@xmath for all @xmath such that @xmath .  Observe that

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

Hence for @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Now let @xmath be identical to @xmath except that, whenever @xmath
rejects despite having found no disagreement with @xmath , @xmath
accepts.  Clearly @xmath .  Also, in the case @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

The result follows since @xmath repetitions suffice to boost any
constant error probability to any other constant error probability.

For @xmath , suppose the verifier’s final state given input @xmath is

  -- -- --
        
  -- -- --

where @xmath is the reject state, @xmath is the accept state, and @xmath
for all @xmath .  Suppose also that @xmath and that @xmath whenever
@xmath , where @xmath is the probability of accepting.  Then the
verifier can make @xmath by performing the conditional rotation

  -- -------- --
     @xmath   
  -- -------- --

on the second register prior to measurement.  In the case @xmath , this
produces

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

It is immediate that @xmath , that @xmath , and that @xmath .  We also
have @xmath , since a randomized verifier for @xmath must query each
sensitive block on @xmath with @xmath probability.  This suggests
viewing @xmath as an ‘alloy’ of block sensitivity and certificate
complexity, an interpretation for which Section 25 gives some
justification.

The results of this chapter are as follows.  In Section 23 I show that
@xmath for all @xmath (partial or total), precisely characterizing
quantum certificate complexity in terms of randomized certificate
complexity. To do this, I first give a nonadaptive characterization of
@xmath , and then apply the adversary method of Ambainis [ 27 ] to
lower-bound @xmath in terms of this characterization.  Then, in Section
24 , I extend results on polynomials due to de Wolf [ 244 ] and to Nisan
and Smolensky (as described by Buhrman and de Wolf [ 78 ] ), to show
that @xmath for all total @xmath , where @xmath is the minimum degree of
a polynomial @xmath such that @xmath if and only if @xmath .  Combining
the results of Sections 23 and 24 leads to a new lower bound on quantum
query complexity: that @xmath for all total @xmath .  To my knowledge,
this is the first quantum lower bound to use both the adversary method
and the polynomial method at different points in the argument.

Finally, in Section 25 , I exhibit asymptotic gaps between @xmath and
other query complexity measures, including a total @xmath for which
@xmath , and a symmetric partial @xmath for which @xmath yet @xmath .  I
conclude in Section 26 with some open problems.

#### 22 Related Work

Raz et al. [ 197 ] studied a query complexity measure they called @xmath
, for Merlin-Arthur.  In my notation, @xmath equals the maximum of
@xmath over all @xmath with @xmath .  Raz et al. observed that @xmath ,
where @xmath is the number of queries needed given arbitrarily many
rounds of interaction with a prover.  They also used error-correcting
codes to construct a total @xmath for which @xmath but @xmath .  This
has similarities to the construction, in Section 25.2 , of a symmetric
partial @xmath for which @xmath but @xmath .  Aside from that and from
Proposition 29 , Raz et al.’s results do not overlap with the results
here.

Watrous [ 239 ] has investigated a different notion of ‘quantum
certificate complexity’—whether certificates that are quantum states can
be superpolynomially smaller than any classical certificate.  Also, de
Wolf [ 245 ] has investigated ‘nondeterministic quantum query
complexity’ in the alternate sense of algorithms that accept with zero
probability when @xmath , and with positive probability when @xmath .

#### 23 Characterization of Quantum Certificate Complexity

We wish to show that @xmath , precisely characterizing quantum
certificate complexity in terms of randomized certificate complexity.
 The first step is to give a simpler characterization of @xmath .

###### Lemma 30

Call a randomized verifier for @xmath nonadaptive if, on input @xmath ,
it queries each @xmath with independent probability @xmath , and rejects
if and only if it encounters a disagreement with @xmath .  (Thus, we
identify such a verifier with the vector @xmath .)  Let @xmath be the
minimum of @xmath over all nonadaptive verifiers for @xmath .  Then
@xmath .

Proof. Clearly @xmath .  For the upper bound, we can assume that a
randomized verifier rejects immediately on finding a disagreement with
@xmath , and accepts if it finds no disagreement.  Let @xmath .  Let
@xmath be an optimal randomized verifier, and let @xmath be the
probability that @xmath , when given input @xmath , finds a disagreement
with @xmath on the @xmath query.  By Markov’s inequality, @xmath must
have found a disagreement with probability at least @xmath after @xmath
queries.  So by the union bound

  -- -------- --
     @xmath   
  -- -------- --

for each @xmath .  Suppose we choose @xmath uniformly at random and
simulate the @xmath query, pretending that queries @xmath have already
been made and have returned agreement with @xmath .  Then we must find a
disagreement with probability at least @xmath .  By repeating this
procedure @xmath times, we can boost the probability to @xmath .  For
@xmath , let @xmath be the probability that @xmath is queried at least
once.  Then @xmath , whereas for each @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

It follows that, if each @xmath is queried with independent probability
@xmath , then the probability that at least one @xmath disagrees with
@xmath is at least

  -- -------- --
     @xmath   
  -- -------- --

To obtain a lower bound on @xmath , I will use the following simple
reformulation of Ambainis’s adversary method [ 27 ] .

###### Theorem 31 (Ambainis)

Given a function @xmath with @xmath , let @xmath be a function from
@xmath to nonnegative reals, and let @xmath be a relation such that
@xmath for all @xmath and @xmath whenever @xmath .  Let @xmath be such
that for every @xmath and @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Then @xmath .

I now prove the main result of the section.

###### Theorem 32

For all @xmath (partial or total) and all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Proof. Let @xmath be an optimal nonadaptive randomized verifier for
@xmath , and let

  -- -------- --
     @xmath   
  -- -------- --

First, @xmath .  We can run a “weighted Grover search,” in which the
proportion of basis states querying index @xmath is within a constant
factor of @xmath .  (It suffices to use @xmath basis states.)  Let
@xmath ; then for any @xmath , @xmath iterations suffice to find a
disagreement with @xmath with probability @xmath . Second, @xmath .
 Consider a matrix game in which Alice chooses an index @xmath to query
and Bob chooses @xmath ; Alice wins if and only if @xmath .  If both
players are rational, then Alice wins with probability @xmath , since
otherwise Alice’s strategy would yield a verifier @xmath with

  -- -------- --
     @xmath   
  -- -------- --

Hence by the minimax theorem, there exists a distribution @xmath over
@xmath such that for every @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath and let @xmath for each @xmath .  Also, let @xmath if and
only if @xmath for each @xmath and @xmath .  Then we can take @xmath and
@xmath in Theorem 31 .  So the quantum query complexity of
distinguishing @xmath from an arbitrary @xmath is @xmath .

#### 24 Quantum Lower Bound for Total Functions

The goal of this section is to show that

  -- -------- --
     @xmath   
  -- -------- --

for all total @xmath .  Say that a real multilinear polynomial @xmath
nondeterministically represents @xmath if for all @xmath , @xmath if and
only if @xmath .  Let @xmath be the minimum degree of a nondeterministic
polynomial for @xmath .  Also, given such a polynomial @xmath , say that
a monomial @xmath is covered by @xmath if @xmath contains every variable
in @xmath .  A monomial @xmath is called a maxonomial if it is not
covered by any other monomial of @xmath .  The following is a simple
generalization of a lemma attributed in [ 78 ] to Nisan and Smolensky.

###### Lemma 33 (Nisan-Smolensky)

Let @xmath nondeterministically represent @xmath .  Then for every
maxonomial @xmath of @xmath and @xmath , there is a set @xmath of
variables in @xmath such that @xmath , where @xmath is obtained from
@xmath by flipping the variables in @xmath .

Proof. Obtain a restricted function @xmath from @xmath , and a
restricted polynomial @xmath from @xmath , by setting each variable
outside of @xmath to @xmath .  Then @xmath cannot be constant, since its
representing polynomial @xmath contains @xmath as a monomial.  Thus
there is a subset @xmath of variables in @xmath such that @xmath , and
hence @xmath .

Using Lemma 33 , de Wolf [ 244 ] showed that @xmath for all total @xmath
, slightly improving the result @xmath due to Buhrman and de Wolf [ 78 ]
.  In Theorem 35 , I will give an analogue of this result for randomized
query and certificate complexities.  However, I first need a
probabilistic lemma.

###### Lemma 34

Suppose we repeatedly apply the following procedure: first identify the
set @xmath of maxonomials of @xmath , then ‘shrink’ each @xmath with
(not necessarily independent) probability at least @xmath .  Shrinking
@xmath means replacing it by an arbitrary monomial of degree @xmath .
 Then with high probability @xmath is a constant polynomial after @xmath
iterations.

Proof. For any set @xmath of monomials, consider the weighting function

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath be the set of monomials of @xmath .  Initially @xmath , and
we are done when @xmath .  The claim is that at every iteration, @xmath
.  For every @xmath is covered by some @xmath , but a given @xmath can
cover at most @xmath distinct @xmath with @xmath .  Hence

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

At every iteration, the contribution of each @xmath to @xmath has at
least @xmath probability of shrinking from @xmath to @xmath (or to
@xmath if @xmath ).  When this occurs, the contribution of @xmath is at
least halved.  Hence @xmath decreases by an expected amount at least
@xmath .  Thus after

  -- -------- --
     @xmath   
  -- -------- --

iterations, the expectation of @xmath is less than @xmath , so @xmath is
empty with probability at least @xmath .

I can now prove the main result. ²⁷ ²⁷ 27 The proof of Theorem 35 that I
gave previously [ 4 ] makes a claim that is both superfluous for proving
the theorem and false.  I am grateful to Gatis Midrijanis for pointing
this out to me.

###### Theorem 35

For total @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Proof. The algorithm is as follows.

  Repeat
  Choose a @xmath -input @xmath compatible with all queries made so far
  ²⁸ ²⁸ 28 Clearly, as long as @xmath is not a constant function, there
  exists a @xmath -input @xmath compatible with all queries made so far.
  Query a randomized @xmath -certificate for @xmath
  Until @xmath has been restricted to a constant function

Let @xmath be a polynomial that nondeterministically represents @xmath .
 Then the key fact is that for every @xmath -input @xmath , when we
query a randomized @xmath -certificate for @xmath we “hit” each
maxonomial @xmath of @xmath with probability at least @xmath .  Here
hitting @xmath means querying a variable in @xmath .  This is because,
by Lemma 33 , it is possible to change @xmath from @xmath to @xmath just
by flipping variables in @xmath .  So a randomized certificate would be
incorrect if it probed those variables with probability less than @xmath
.

Therefore, each iteration of the algorithm shrinks each maxonomial of
@xmath with probability at least @xmath .  It follows from Lemma 34 that
the algorithm terminates after an expected number of iterations @xmath .

Buhrman et al. [ 45 ] showed that @xmath .  Combining this with Theorems
32 and 35 yields a new relation between classical and quantum query
complexity.

###### Corollary 36

For all total @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

The best previous relation of this kind was @xmath , due to de Wolf [
244 ] .  It is worth mentioning another corollary of Theorems 32 and 35
, this one purely classical:

###### Corollary 37

For all total @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Previously, no relation between @xmath and @xmath better than @xmath was
known (although no asymptotic gap between @xmath and @xmath is known
either [ 210 ] ).

#### 25 Asymptotic Gaps

Having related @xmath and @xmath to other query complexity measures in
Section 24 , in what follows I seek the largest possible asymptotic gaps
among the measures.  In particular, I give a total @xmath for which
@xmath and hence @xmath , as well as a total @xmath for which @xmath .
 Although these gaps are the largest of which I know, Section 25.1 shows
that no ‘local’ technique can improve the relations @xmath and @xmath .
 Finally, Section 25.2 uses combinatorial designs to construct a
symmetric partial @xmath for which @xmath and @xmath are @xmath , yet
@xmath .

Wegener and Zádori [ 240 ] exhibited total Boolean functions with
asymptotic gaps between @xmath and @xmath .  In similar fashion, I give
a function family @xmath with an asymptotic gap between @xmath and
@xmath .  Let @xmath equal @xmath if and only if the Hamming weight of
its input is @xmath , @xmath , @xmath , or @xmath .  (The parameter
@xmath was found via computer search to produce a maximal separation.)
 Then for @xmath , let

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the first @xmath input bits, @xmath is the second @xmath
, and so on.  For @xmath , let

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Then since @xmath , we have @xmath .  On the other hand, @xmath but
@xmath , so

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Solving this recurrence yields @xmath .  We can now show a gap between
@xmath and @xmath .

###### Proposition 38

@xmath .

Proof. Since @xmath , it suffices to show that @xmath .  The randomized
verifier @xmath chooses an input variable to query as follows.  Let
@xmath be the claimed input, and let @xmath .  Let @xmath and @xmath .
 With probability @xmath , @xmath chooses an @xmath uniformly at random;
otherwise @xmath chooses an @xmath uniformly at random.  Here @xmath is
as follows.

  -------- -------- -------- -------- -------- -------- --------
  @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath
  @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath
  -------- -------- -------- -------- -------- -------- --------

Once @xmath is chosen, @xmath repeats the procedure for @xmath ,
and continues recursively in this manner until reaching a variable
@xmath to query.  One can check that if @xmath , then @xmath with
probability at least @xmath .  Hence @xmath with probability at least
@xmath , and @xmath .

By Theorem 32 , it follows that @xmath .  This offers a surprising
contrast with the query complexity setting, where the best known gap
between the deterministic and quantum measures is quadratic ( @xmath ).

The family @xmath happens not to yield an asymptotic gap between @xmath
and @xmath .  The reason is that any input to @xmath can be covered
perfectly by sensitive blocks of minimum size, with no variables left
over. In general, though, one can have @xmath .  As reported by Bublitz
et al. [ 74 ] , M. Paterson found a total Boolean function @xmath such
that @xmath and @xmath for all @xmath .  Composing @xmath recursively
yields @xmath and @xmath , both of which are the largest such gaps of
which I know.

##### 25.1 Local Separations

It is a longstanding open question whether the relation @xmath due to
Nisan [ 183 ] is tight.  As a first step, one can ask whether the
relations @xmath and @xmath are tight.  In this section I introduce a
notion of local proof in query complexity, and then show there is no
local proof that @xmath or that @xmath .  This implies that proving
either result would require techniques unlike those that are currently
known.  My inspiration comes from computational complexity, where
researchers first formalized known methods of proof, including
relativizable proofs [ 41 ] and natural proofs [ 200 ] , and then argued
that these methods were not powerful enough to resolve the field’s
outstanding problems.

Let @xmath and @xmath be query complexity measures obtained by
maximizing over all inputs—that is,

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Call @xmath a minimal block on @xmath if @xmath is sensitive on @xmath
(meaning @xmath ), and no sub-block @xmath is sensitive on @xmath .
 Also, let @xmath ’s neighborhood @xmath consist of @xmath together with
@xmath for every minimal block @xmath of @xmath .  Consider a proof that
@xmath for some nondecreasing @xmath .  I call the proof local if it
proceeds by showing that for every input @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

As a canonical example, Nisan’s proof [ 183 ] that @xmath is local.  For
each @xmath , Nisan observes that (i) a maximal set of disjoint minimal
blocks is a certificate for @xmath , (ii) such a set can contain at most
@xmath blocks, and (iii) each block can have size at most @xmath .
 Another example of a local proof is the proof in Section 23 that @xmath
.

###### Proposition 39

There is no local proof showing that @xmath or that @xmath for all total
@xmath .

Proof. The first part is easy: let @xmath if @xmath (where @xmath
denotes the Hamming weight of @xmath ), and @xmath otherwise.  Consider
the all-zero input @xmath .  We have @xmath , but @xmath , and indeed
@xmath for all @xmath . For the second part, arrange the input variables
in a lattice of size @xmath .  Take @xmath , and let @xmath be the
monotone Boolean function that outputs @xmath if and only if @xmath
contains a @xmath -square of size @xmath .  This is a square of @xmath
’s that can wrap around the edges of the lattice; note that only the
variables along the sides must be set to @xmath , not those in the
interior.  An example input, with a @xmath -square of size @xmath , is
shown below.

  -- -------- --
     @xmath   
  -- -------- --

Clearly @xmath , since there can be at most @xmath disjoint @xmath
-squares of size @xmath .  Also, @xmath for any @xmath that is @xmath
except for a single @xmath -square.  On the other hand, if we choose
uniformly at random among all such @xmath ’s, then at any lattice site
@xmath , @xmath .  Hence @xmath .

##### 25.2 Symmetric Partial Functions

If @xmath is partial, then @xmath can be much smaller than @xmath .
 This is strikingly illustrated by the collision problem: let @xmath if
@xmath is a one-to-one sequence and @xmath if @xmath is a two-to-one
sequence, promised that one of these is the case.  Then @xmath , since
every one-to-one input differs from every two-to-one input on at least
@xmath of the @xmath ’s.  On the other hand, Chapter I showed that
@xmath .

From the example of the collision problem, it is tempting to conjecture
that (say) @xmath whenever @xmath —that is, ‘if every @xmath -input is
far from every @xmath -input, then the quantum query complexity is
sublinear.’  Here I disprove this conjecture, even for the special case
of symmetric functions such as @xmath .  (Given a finite set @xmath , a
function @xmath where @xmath is called symmetric if @xmath implies
@xmath and @xmath for every permutation @xmath .)

The proof uses the following lemma, which can be found in Nisan and
Wigderson [ 185 ] for example.

###### Lemma 40 (Nisan-Wigderson)

For any @xmath , there exists a family of sets

  -- -------- --
     @xmath   
  -- -------- --

such that @xmath , @xmath for all @xmath , and @xmath for all @xmath .

A lemma due to Ambainis [ 26 ] is also useful.  Let @xmath where @xmath
be a partial Boolean function, and let @xmath be a real-valued
multilinear polynomial.  We say that @xmath approximates @xmath if (i)
@xmath for every input @xmath (not merely those in @xmath ), and (ii)
@xmath for every @xmath .

###### Lemma 41 (Ambainis)

At most @xmath distinct Boolean functions (partial or total) can be
approximated by polynomials of degree @xmath , where @xmath .

The result is an easy consequence of Lemmas 40 and 41 .

###### Theorem 42

There exists a symmetric partial @xmath for which @xmath and @xmath .

Proof. Let @xmath where @xmath , and let @xmath .  Let @xmath be as in
Lemma 40 .  We put @xmath in @xmath if and only if @xmath for some
@xmath .  Clearly @xmath , since if @xmath then every permutation of
@xmath differs from every permutation of @xmath on at least @xmath
indices.  The number of symmetric @xmath with @xmath as above is @xmath
.  We can convert any such @xmath to a Boolean function @xmath on @xmath
variables.  But Beals et al. [ 45 ] showed that, if @xmath , then @xmath
is approximated by a polynomial of degree at most @xmath .  So by Lemma
41 , if @xmath for every @xmath then

  -- -------- --
     @xmath   
  -- -------- --

and we solve to obtain @xmath .

#### 26 Open Problems

Is @xmath , where @xmath is the minimum degree of a polynomial
approximating @xmath ?  In other words, can one lower-bound @xmath using
the polynomial method of Beals et al. [ 45 ] , rather than the adversary
method of Ambainis [ 27 ] ?

Also, is @xmath ?  If so we obtain the new relations @xmath and @xmath .

### Chapter \thechapter The Need to Uncompute

Like a classical algorithm, a quantum algorithm can solve problems
recursively by calling itself as a subroutine.  When this is done,
though, the algorithm typically needs to call itself twice for each
subproblem to be solved.  The second call’s purpose is to uncompute
‘garbage’ left over by the first call, and thereby enable interference
between different branches of the computation.  Of course, a factor of
@xmath increase in running time hardly seems like a big deal, when set
against the speedups promised by quantum computing.  The problem is that
these factors of @xmath multiply, with each level of recursion producing
an additional factor.  Thus, one might wonder whether the
uncomputing step is really necessary, or whether a cleverly designed
algorithm might avoid it.  This chapter gives the first nontrivial
example in which recursive uncomputation is provably necessary.

The example concerns a long-neglected problem called Recursive Fourier
Sampling (henceforth @xmath ), which was introduced by Bernstein and
Vazirani [ 55 ] in 1993 to prove the first oracle separation between
@xmath and @xmath .  Many surveys on quantum computing pass directly
from the Deutsch-Jozsa algorithm [ 95 ] to the dramatic results of Simon
[ 220 ] and Shor [ 219 ] , without even mentioning @xmath .  There are
two likely reasons for this neglect.  First, the @xmath problem seems
artificial.  It was introduced for the sole purpose of proving an oracle
result, and is unlike all other problems for which a quantum speedup is
known.  (I will define @xmath in Section 27 ; but for now, it involves a
tree of depth @xmath , where each vertex is labeled with a function to
be evaluated via a Fourier transform.)  Second, the speedup for @xmath
is only quasipolynomial ( @xmath versus @xmath ), rather than
exponential as for the period-finding and hidden subgroup problems.

Nevertheless, I believe that @xmath merits renewed attention—for it
serves as an important link between quantum computing and the ideas of
classical complexity theory.  One reason is that, although other
problems in @xmath —such as the factoring, discrete logarithm, and
‘shifted Legendre symbol’ problems [ 232 ] —are thought to be
classically intractable, these problems are quite low-level by
complexity-theoretic standards.  They, or their associated decision
problems, are in @xmath . ²⁹ ²⁹ 29 For the shifted Legendre symbol
problem, this is true assuming a number-theoretic conjecture of Boneh
and Lipton [ 61 ] . By contrast, Bernstein and Vazirani [ 55 ] showed
that, as an oracle problem, @xmath lies outside @xmath and even @xmath
(the latter result is unpublished, though not difficult).  Subsequently
Watrous [ 239 ] gave an oracle @xmath , based on an unrelated problem,
for which @xmath . ³⁰ ³⁰ 30 Actually, to place @xmath outside @xmath
relative to an oracle, it suffices to consider the complement of Simon’s
problem (“Does @xmath only when @xmath ?”). Also, Green and Pruim [ 135
] gave an oracle @xmath for which @xmath .  However, Watrous’ problem
was shown by Babai [ 38 ] to be in @xmath , while Green and Pruim’s
problem is in @xmath .  Thus, neither problem can be used to place
@xmath outside higher levels of the polynomial hierarchy.

On the other hand, Umesh Vazirani and others have conjectured that
@xmath is not in @xmath , from which it would follow that there exists
an oracle @xmath relative to which @xmath .  Proving this is, in my
view, one of the central open problems in quantum complexity theory.
 Its solution seems likely to require novel techniques for
constant-depth circuit lower bounds. ³¹ ³¹ 31 For the @xmath function
can be represented by a low-degree real polynomial—this follows from the
existence of a polynomial-time quantum algorithm for @xmath , together
with the result of Beals et al. [ 45 ] relating quantum algorithms to
low-degree polynomials.  As a result, the circuit lower bound technique
of Razborov [ 198 ] and Smolensky [ 223 ] , which is based on the
nonexistence of low-degree polynomials, seems unlikely to work.  Even
the random restriction method of Furst et al. [ 120 ] can be related to
low-degree polynomials, as shown by Linial et al. [ 166 ] .

In this chapter I examine the @xmath problem from a different angle.
 Could Bernstein and Vazirani’s quantum algorithm for @xmath be improved
even further, to give an exponential speedup over the classical
algorithm?  And could we use @xmath , not merely to place @xmath outside
of @xmath relative to an oracle, but to place it outside of @xmath with
(say) a logarithmic number of alternations?

My answer to both questions is a strong ‘no.’  I study a large class of
variations on @xmath , and show that all of them fall into one of two
classes:

1.  a trivial class, for which there exists a classical algorithm making
    only one query, or

2.  a nontrivial class, for which any quantum algorithm needs @xmath
    queries, where @xmath is the height of the tree to be evaluated.
     (By comparison, the Bernstein-Vazirani algorithm uses @xmath
    queries, because of its need to uncompute garbage recursively at
    each level of the tree.)

Since @xmath queries always suffice classically, this dichotomy theorem
implies that the speedup afforded by quantum computers is at most
quasipolynomial.  It also implies that (nontrivial) @xmath is solvable
in quantum polynomial time only when @xmath .

The plan is as follows.  In Section 27 I define the @xmath problem, and
give Bernstein and Vazirani’s quantum algorithm for solving it.  In
Section 28 , I use the adversary method of Ambainis [ 27 ] to prove a
lower bound on the quantum query complexity of any @xmath variant.  This
bound, however, requires a parameter that I call the “nonparity
coefficient” to be large.  Intuitively, given a Boolean function @xmath
, the nonparity coefficient measures how far @xmath is from being the
parity of some subset of its input bits—not under the uniform
distribution over inputs (the standard assumption in Fourier analysis),
but under an adversarial distribution.  The crux of the argument is that
either the nonparity coefficient is zero (meaning the @xmath variant in
question is trivial), or else it is bounded below by a positive
constant.  This statement is proved in Section 28 , and seems like it
might be of independent interest.  Section 29 concludes with some open
problems.

#### 27 Preliminaries

In ordinary Fourier sampling, we are given oracle access to a Boolean
function @xmath , and are promised that there exists a secret string
@xmath such that @xmath for all @xmath .  The problem is to find @xmath
—or rather, since we need a problem with Boolean output, the problem is
to return @xmath , where @xmath is some known Boolean function.  We can
think of @xmath as the “hard-core bit” of @xmath , and can assume that
@xmath itself is efficiently computable, or else that we are given
access to an oracle for @xmath .

To obtain a height- @xmath recursive Fourier sampling tree, we simply
compose this problem.  That is, we are no longer given direct access to
@xmath , but instead are promised that @xmath , where @xmath is the
secret string for another Fourier sampling problem.  A query then takes
the form @xmath , and produces as output @xmath .  As before, we are
promised that there exists an @xmath such that @xmath for all @xmath ,
meaning that the @xmath strings must be chosen consistent with this
promise.  Again we must return @xmath .

Continuing, we can define height- @xmath recursive Fourier sampling, or
@xmath , recursively as follows.  We are given oracle access to a
function @xmath for all @xmath , and are promised that

1.  for each fixed @xmath , @xmath is an instance of @xmath on @xmath ,
    having answer bit @xmath ; and

2.  there exists a secret string @xmath such that @xmath for each @xmath
    .

Again the answer bit to be returned is @xmath .  Note that @xmath is
assumed to be the same everywhere in the tree—though using the
techniques in this chapter, it would be straightforward to generalize to
the case of different @xmath ’s.  As an example that will be used later,
we could take @xmath , where @xmath if @xmath and @xmath otherwise, and
@xmath denotes the Hamming weight of @xmath .  We do not want to take
@xmath to be the parity of @xmath , for if we did then @xmath could be
evaluated using a single query.  To see this, observe that if @xmath is
the all- @xmath ’s string, then @xmath is the parity of @xmath .

By an ‘input,’ I will mean a complete assignment for the @xmath oracle
(that is, @xmath for all @xmath ).  I will sometimes refer also to an ‘
@xmath tree,’ where each vertex at distance @xmath from the root has a
label @xmath .  If @xmath then the vertex is a leaf; otherwise it has
@xmath children, each with a label @xmath for some @xmath .  The
subtrees of the tree just correspond to the sub-instances of @xmath .

Bernstein and Vazirani [ 55 ] showed that @xmath , or @xmath with height
@xmath (all logarithms are base @xmath ), is solvable on a quantum
computer in time polynomial in @xmath .  I include a proof for
completeness.  Let @xmath be an oracle that, for each @xmath , encodes
an instance of @xmath whose answer is @xmath .  Then let @xmath be the
unary language @xmath .

###### Lemma 43

@xmath for any choice of @xmath

Proof. @xmath can be solved exactly in four queries, with no garbage
bits left over.  The algorithm is as follows: first prepare the state

  -- -------- --
     @xmath   
  -- -------- --

using one query to @xmath .  Then apply a phase flip conditioned on
@xmath , and uncompute @xmath using a second query, obtaining

  -- -------- --
     @xmath   
  -- -------- --

Then apply a Hadamard gate to each bit of the @xmath register.  It can
be checked that the resulting state is simply @xmath .  One can then
compute @xmath and uncompute @xmath using two more queries to @xmath ,
to obtain @xmath .  To solve @xmath , we simply apply the above
algorithm recursively at each level of the tree.  The total number of
queries used is @xmath .

One can further reduce the number of queries to @xmath by using the
“one-call kickback trick,” described by Cleve et al. [ 87 ] .  Here one
prepares the state

  -- -------- --
     @xmath   
  -- -------- --

and then exclusive- @xmath ’s @xmath into the second register.  This
induces the desired phase @xmath without the need to uncompute @xmath .
 However, one still needs to uncompute @xmath after computing @xmath .

A remark on notation: to avoid confusion with subscripts, I denote the
@xmath bit of string @xmath by @xmath .

#### 28 Quantum Lower Bound

In this section I prove a lower bound on the quantum query complexity of
@xmath .  Crucially, the bound should hold for any nontrivial one-bit
function of the secret strings, not just a specific function such as
@xmath defined in Section 27 .  Let @xmath be height- @xmath recursive
Fourier sampling in which the problem at each vertex is to return @xmath
.  The following notion turns out to be essential.

###### Definition 44

Given a Boolean function @xmath (partial or total), the nonparity
coefficient @xmath is the largest @xmath for which there exist
distributions @xmath over the @xmath -inputs of @xmath , and @xmath over
the @xmath -inputs, such that for all @xmath , all @xmath -inputs @xmath
, and all @xmath -inputs @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

Loosely speaking, the nonparity coefficient is high if there exist
distributions over @xmath -inputs and @xmath -inputs that make @xmath
far from being a parity function of a subset of input bits.  The
following proposition develops some intuition about @xmath .

###### Proposition 45

1.  @xmath for all nonconstant @xmath .

2.  @xmath if and only if @xmath can be written as the parity (or the
    NOT of the parity) of a subset @xmath of input bits.

Proof.

1.  Given any @xmath and @xmath , a uniform random @xmath will satisfy

      -- -------- --
         @xmath   
      -- -------- --

    (If @xmath then this probability will be @xmath ; otherwise it will
    be @xmath .)  So certainly there is a fixed choice of @xmath that
    works for random @xmath and @xmath .

2.  For the ‘if’ direction, take @xmath if and only if @xmath , and
    choose @xmath and @xmath arbitrarily.  This ensures that @xmath .
     For the ‘only if’ direction, if @xmath , we can choose @xmath to
    have support on all @xmath -inputs, and @xmath to have support on
    all @xmath -inputs.  Then there must be a @xmath such that @xmath is
    constant as we range over @xmath -inputs, and @xmath is constant as
    we range over @xmath -inputs.  Take @xmath if and only if @xmath .

If @xmath , then @xmath is easily solvable using a single classical
query.  Theorem 47 will show that for all @xmath (partial or total),

  -- -- --
        
  -- -- --

where @xmath is bounded-error quantum query complexity as defined in
Section 8 .  In other words, any @xmath problem with @xmath bounded away
from @xmath requires a number of queries exponential in the tree height
@xmath .

However, there is an essential further part of the argument, which
restricts the values of @xmath itself.  Suppose there existed a family
@xmath of ‘pseudoparity’ functions: that is, @xmath for all @xmath , yet
@xmath .  Then the best bound obtainable from Theorem 47 would be @xmath
, suggesting that @xmath might still be solvable in quantum polynomial
time.  On the other hand, it would be unclear a priori how to solve
@xmath classically with a logarithmic number of alternations.  Theorem
49 will rule out this scenario by showing that pseudoparity functions do
not exist: if @xmath then @xmath is a parity function, and hence @xmath
.

The theorem of Ambainis that we need is his “most general” lower bound
from [ 27 ] , which he introduced to show that the quantum query
complexity of inverting a permutation is @xmath , and which we used
already in Chapter I .  Let us restate the theorem in the present
context.

###### Theorem 46 (Ambainis)

Let @xmath and @xmath be sets of inputs to function @xmath .  Let @xmath
be a symmetric real-valued relation function, and for @xmath , @xmath ,
and index @xmath , let

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where the denominators are all nonzero.  Then @xmath where

  -- -------- --
     @xmath   
  -- -------- --

We are now ready to prove a lower bound for @xmath .

###### Theorem 47

For all @xmath (partial or total), @xmath .

Proof. Let @xmath be the set of all @xmath -inputs to @xmath , and let
@xmath be the set of all @xmath -inputs.  We will weight the inputs
using the distributions @xmath from the definition of the nonparity
coefficient @xmath .  For all @xmath , let @xmath be the product, over
all vertices @xmath in the @xmath tree for @xmath , of the probability
of the secret string @xmath at @xmath , if @xmath is drawn from @xmath
(where we condition on @xmath ’s output bit, @xmath ).  Next, say that
@xmath and @xmath differ minimally if, for all vertices @xmath of the
@xmath tree, the subtrees rooted at @xmath are identical in @xmath and
in @xmath whenever the answer bit @xmath at @xmath is the same in @xmath
and in @xmath .  If @xmath and @xmath differ minimally, then we will set
@xmath ; otherwise we will set @xmath .  Clearly @xmath for all @xmath .
 Furthermore, we claim that @xmath for all @xmath that differ minimally
and all @xmath such that @xmath .  For suppose @xmath is chosen with
probability proportional to @xmath , and @xmath is chosen with
probability proportional to @xmath .  Then @xmath equals the probability
that we would notice the switch from @xmath to @xmath by monitoring
@xmath , times the probability that we would notice the switch from
@xmath to @xmath .

Let @xmath be the @xmath vertex along the path in the @xmath tree from
the root to the leaf vertex @xmath , for all @xmath .  Also, let @xmath
be the label of the edge between @xmath and @xmath , and let @xmath and
@xmath be the secret strings at @xmath in @xmath and @xmath
respectively.  Then since @xmath and @xmath differ minimally, we must
have @xmath for all @xmath —for otherwise the subtrees rooted at @xmath
would be identical, which contradicts the assumption @xmath .  So we can
think of the process of choosing @xmath as first choosing a random
@xmath from @xmath so that @xmath , then choosing a random @xmath from
@xmath so that @xmath , and so on.  Choosing @xmath is analogous, except
that whenever we used @xmath in choosing @xmath we use @xmath , and vice
versa.  Since the @xmath secret strings @xmath to be updated are
independent of one another, it follows that

  -- -------- -------- --
     @xmath   @xmath   
                       
              @xmath   
  -- -------- -------- --

by the definition of @xmath .  Therefore

  -- -- --
        
  -- -- --

by Theorem 46 .

Before continuing further, let me show that there is a natural, explicit
choice of @xmath —the function @xmath from Section 27 —for which the
nonparity coefficient is almost @xmath .  Thus, for @xmath , the
algorithm of Lemma 43 is essentially optimal.

###### Proposition 48

@xmath .

Proof. Let @xmath .  Let @xmath be the uniform distribution over all
@xmath with @xmath (so @xmath ); likewise let @xmath be the uniform
distribution over @xmath with @xmath ( @xmath ).  We consider only the
case of @xmath drawn from @xmath ; the @xmath case is analogous.  We
will show that for any @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

(all congruences are @xmath ).  The theorem then follows, since by the
definition of the nonparity coefficient, given any @xmath the choices of
@xmath and @xmath are independent.

Assume without loss of generality that @xmath (if @xmath , then replace
@xmath by its complement).  We apply induction on @xmath .  If @xmath ,
then clearly

  -- -------- --
     @xmath   
  -- -------- --

For @xmath , let @xmath , where @xmath contains only the rightmost
@xmath of @xmath and @xmath contains all the other @xmath ’s.  Suppose
the proposition holds for @xmath .  Then

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where

  -- -------- --
     @xmath   
  -- -------- --

for some @xmath .  Furthermore, even conditioned on @xmath , the
expected number of @xmath ’s in @xmath outside of @xmath is @xmath and
they are uniformly distributed.  Therefore

  -- -------- --
     @xmath   
  -- -------- --

for some @xmath .  So

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Finally it must be shown that pseudoparity functions do not exist.  That
is, if @xmath is too close to a parity function for the bound of Theorem
47 to apply, then @xmath actually is a parity function, from which it
follows that @xmath admits an efficient classical algorithm.

###### Theorem 49

Suppose @xmath .  Then @xmath is a parity function (equivalently, @xmath
).

Proof. By linear programming duality, there exists a joint distribution
@xmath over @xmath , @xmath -inputs @xmath , and @xmath -inputs @xmath ,
such that for all @xmath and @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Furthermore @xmath , since otherwise we could violate the hypothesis by
taking @xmath or @xmath .  It follows that there exists a joint
distribution @xmath over @xmath and @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath , and

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath .  But this implies that @xmath is a bounded-error
threshold function of parity functions.  More precisely, there exist
probabilities @xmath , summing to @xmath , as well as @xmath such that
for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

We will consider @xmath , the variance of the above quantity @xmath if
@xmath is drawn uniformly at random from @xmath .  First, if @xmath for
any @xmath , then @xmath is a parity function and hence @xmath .  So we
can assume without loss of generality that @xmath for all @xmath .  Then
since @xmath is uniform, for each @xmath we know that @xmath and @xmath
are pairwise independent @xmath random variables, both with expectation
@xmath .  So

  -- -- --
        
  -- -- --

On the other hand, since @xmath is always less than @xmath or greater
than @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Combining,

  -- -------- --
     @xmath   
  -- -------- --

#### 29 Open Problems

An intriguing open problem is whether Theorem 47 can be proved using the
polynomial method of Beals et al. [ 45 ] , rather than the adversary
method of Ambainis [ 27 ] .  It is known that one can lower-bound
polynomial degree in terms of block sensitivity, or the maximum number
of disjoint changes to an input that change the output value.  The
trouble is that the @xmath function has block sensitivity @xmath —the
“sensitive blocks” of each input tend to have small intersection, but
are not disjoint.  For this reason, I implicitly used the quantum
certificate complexity of Chapter I rather than block sensitivity to
prove a lower bound.

I believe the constant of Theorem 49 can be improved.  The smallest
nonzero @xmath value I know of is attained when @xmath and @xmath :

###### Proposition 50

@xmath .

Proof. First, @xmath , since @xmath can choose @xmath to be @xmath ,
@xmath , or @xmath each with probability @xmath ; then for any @xmath
and the unique @xmath -input @xmath , we have @xmath with probability at
most @xmath .  Second, @xmath , since applying linear programming
duality, we can let the pair @xmath equal @xmath , @xmath , or @xmath
each with probability @xmath .  Then @xmath always, and for any @xmath
-input @xmath , we have @xmath with probability @xmath .

Finally, I conjecture that uncomputation is unavoidable not just for
@xmath but for many other recursive problems, such as game-tree
evaluation.  Formally, the conjecture is that the quantum query
complexity of evaluating a game tree increases exponentially with depth
as the number of leaves is held constant, even if there is at most one
winning move per vertex (so that the tree can be evaluated with zero
probability of error).

### Chapter \thechapter Limitations of Quantum Advice

How many classical bits can “really” be encoded into @xmath qubits?  Is
it @xmath , because of Holevo’s Theorem [ 145 ] ; @xmath , because of
dense quantum coding [ 78 ] and quantum teleportation [ 53 ] ;
exponentially many, because of quantum fingerprinting [ 75 ] ; or
infinitely many, because amplitudes are continuous?  The best general
answer to this question is probably mu , the Zen word that “unasks” a
question. ³² ³² 32 Another mu -worthy question is, “Where does the power
of quantum computing come from?  Superposition?  Interference?  The
large size of Hilbert space?”

To a computer scientist, however, it is natural to formalize the
question in terms of quantum one-way communication complexity [ 43 , 75
, 154 , 250 ] .  The setting is as follows: Alice has an @xmath -bit
string @xmath , Bob has an @xmath -bit string @xmath , and together they
wish to evaluate @xmath where @xmath is a Boolean function.  After
examining her input @xmath , Alice can send a single quantum message
@xmath to Bob, whereupon Bob, after examining his input @xmath , can
choose some basis in which to measure @xmath .  He must then output a
claimed value for @xmath .  We are interested in how long Alice’s
message needs to be, for Bob to succeed with high probability on any
@xmath pair.  Ideally the length will be much smaller than if Alice had
to send a classical message.

Communication complexity questions have been intensively studied in
theoretical computer science (see the book of Kushilevitz and Nisan [
160 ] for example).  In both the classical and quantum cases, though,
most attention has focused on two-way communication, meaning that Alice
and Bob get to send messages back and forth.  I believe that the study
of one-way quantum communication presents two main advantages.  First,
many open problems about two-way communication look gruesomely
difficult—for example, are the randomized and quantum communication
complexities of every total Boolean function polynomially related?  We
might gain insight into these problems by tackling their one-way
analogues first.  And second, because of its greater simplicity, the
one-way model more directly addresses our opening question: how much
“useful stuff” can be packed into a quantum state?  Thus, results on
one-way communication fall into the quantum information theory tradition
initiated by Holevo [ 145 ] and others, as much as the communication
complexity tradition initiated by Yao [ 247 ] .

Related to quantum one-way communication is the notion of quantum advice
.  As pointed out by Nielsen and Chuang [ 182 , p.203] , there is no
compelling physical reason to assume that the starting state of a
quantum computer is a computational basis state: ³³ ³³ 33 One might
object that the starting state is itself the outcome of some
computational process, which began no earlier than the Big Bang.
 However, (1) for all we know highly entangled states were created in
the Big Bang, and (2) @xmath billion years is a long time.

  [W]e know that many systems in Nature ‘prefer’ to sit in highly
  entangled states of many systems; might it be possible to exploit this
  preference to obtain extra computational power?  It might be that
  having access to certain states allows particular computations to be
  done much more easily than if we are constrained to start in the
  computational basis.

One way to interpret Nielsen and Chuang’s provocative question is as
follows.  Suppose we could request the best possible starting state for
a quantum computer, knowing the language to be decided and the input
length @xmath but not knowing the input itself. ³⁴ ³⁴ 34 If we knew the
input, we would simply request a starting state that contains the right
answer! Denote the class of languages that we could then decide by
@xmath ---meaning quantum polynomial time, given an
arbitrarily-entangled but polynomial-size quantum advice state. ³⁵ ³⁵ 35
@xmath might remind readers of a better-studied class called @xmath
(Quantum Merlin-Arthur).  But there are two key differences: first,
advice can be trusted while proofs cannot; second, proofs can be
tailored to a particular input while advice cannot. How powerful is this
class?  If @xmath contained (for example) the @xmath -complete problems,
then we would need to rethink our most basic assumptions about the power
of quantum computing.  We will see later that quantum advice is closely
related to quantum one-way communication, since we can think of an
advice state as a one-way message sent to an algorithm by a benevolent
“advisor.”

This chapter is about the limitations of quantum advice and one-way
communication.  It presents three contributions which are basically
independent of one another.

First, Section 31 shows that @xmath for any Boolean function @xmath ,
partial or total.  Here @xmath is deterministic one-way communication
complexity, @xmath is bounded-error one-way quantum communication
complexity, and @xmath is the length of Bob’s input.  Intuitively,
whenever the set of Bob’s possible inputs is not too large, Alice can
send him a short classical message that lets him learn the outcome of
any measurement he would have wanted to make on the quantum message
@xmath .  It is interesting that a slightly tighter bound for total
functions— @xmath —follows easily from a result of Klauck [ 154 ]
together with a lemma of Sauer [ 212 ] about VC-dimension.  However, the
proof of the latter bound is highly nonconstructive, and seems to fail
for partial @xmath .

Using my communication complexity result, Section 31.1 shows that @xmath
—in other words, @xmath with polynomial-size quantum advice can be
simulated in @xmath with polynomial-size classical advice. ³⁶ ³⁶ 36
Given a complexity class @xmath , the class @xmath consists of all
languages decidable by a @xmath machine, given a polynomial-size
classical advice string that depends only on the input length.  See
Chapter Limits on Efficient Computation in the Physical World for more
information about the complexity classes mentioned in this chapter. This
resolves a question of Harry Buhrman (personal communication), who asked
whether quantum advice can be simulated in any classical complexity
class with short classical advice.  A corollary of this containment is
that we cannot hope to show an unrelativized separation between quantum
and classical advice (that is, that @xmath ), without also showing that
@xmath does not have polynomial-size circuits.

What makes this result surprising is that, in the minds of many computer
scientists, a quantum state is basically an exponentially long vector.
 Indeed, this belief seems to fuel skepticism of quantum computing (see
Goldreich [ 128 ] for example).  But given an exponentially long advice
string, even a classical computer could decide any language whatsoever.
 So one might imagine naïvely that quantum advice would let us solve
problems that are not even recursively enumerable given classical advice
of a similar size!  The failure of this naïve intuition supports the
view that a quantum superposition over @xmath -bit strings is “more
similar” to a probability distribution over @xmath -bit strings than to
a @xmath -bit string.

The second contribution of the chapter, in Section 32 , is an oracle
relative to which @xmath is not contained in @xmath .  Underlying this
oracle separation is the first correct proof of a direct product theorem
for quantum search.  Given an @xmath -item database with @xmath marked
items, the direct product theorem says that if a quantum algorithm makes
@xmath queries, then the probability that the algorithm finds all @xmath
of the marked items decreases exponentially in @xmath .  Notice that
such a result does not follow from any existing quantum lower bound.
 Earlier Klauck [ 155 ] had claimed a weaker direct product theorem,
based on the hybrid method of Bennett et al. [ 51 ] , in a paper on
quantum time-space tradeoffs for sorting.  Unfortunately, Klauck’s proof
is incorrect.  The proof uses the polynomial method of Beals et al. [ 45
] , with the novel twist that we examine all higher derivatives of a
polynomial (not just the first derivative).  The proof has already been
improved by Klauck, Špalek, and de Wolf [ 156 ] , who were able to
recover and even extend Klauck’s original claims about quantum sorting.

The final contribution, in Section 33 , is a new trace distance method
for proving lower bounds on quantum one-way communication complexity.
 Previously there was only one basic lower bound technique: the
VC-dimension method of Klauck [ 154 ] , which relied on lower bounds for
quantum random access codes due to Ambainis et al. [ 32 ] and Nayak [
180 ] .  Using VC-dimension one can show, for example, that @xmath ,
where the disjointness function @xmath is defined by @xmath if and only
if @xmath for all @xmath .

For some problems, however, the VC-dimension method yields no nontrivial
quantum lower bound.  Seeking to make this point vividly, Ambainis posed
the following problem.  Alice is given two elements @xmath of a finite
field @xmath (where @xmath is prime); Bob is given another two elements
@xmath .  Bob’s goal is to output @xmath if @xmath and @xmath otherwise.
 For this problem, the VC-dimension method yields no randomized or
quantum lower bound better than constant.  On the other hand, the
well-known fingerprinting protocol for the equality function [ 192 ]
seems to fail for Ambainis’ problem, because of the interplay between
addition and multiplication.  So it is natural to conjecture that the
randomized and even quantum one-way complexities are @xmath —that is,
that no nontrivial protocol exists for this problem.

Ambainis posed a second problem in the same spirit.  Here Alice is given
@xmath , Bob is given @xmath , and both players know a subset @xmath .
 Bob’s goal is to decide whether @xmath where subtraction is modulo
@xmath .  The conjecture is that if @xmath is chosen uniformly at random
with @xmath about @xmath , then with high probability the randomized and
quantum one-way complexities are both @xmath .

Using the trace distance method, I am able to show optimal quantum lower
bounds for both of Ambainis’ problems.  Previously, no nontrivial lower
bounds were known even for randomized protocols.  The key idea is to
consider two probability distributions over Alice’s quantum message
@xmath .  The first distribution corresponds to @xmath chosen uniformly
at random; the second corresponds to @xmath chosen uniformly conditioned
on @xmath .  These distributions give rise to two mixed states @xmath
and @xmath , which Bob must be able to distinguish with non-negligible
bias assuming he can evaluate @xmath .  I then show an upper bound on
the trace distance @xmath , which implies that Bob cannot distinguish
the distributions.

Theorem 65 gives a very general condition under which the trace distance
method works; Corollaries 66 and 67 then show that the condition is
satisfied for Ambainis’ two problems.  Besides showing a significant
limitation of the VC-dimension method, I hope the new method is a
non-negligible step towards proving that @xmath for all total Boolean
functions @xmath , where @xmath is randomized one-way complexity.  I
conclude in Section 34 with some open problems.

#### 30 Preliminaries

Following standard conventions, I denote by @xmath the deterministic
one-way complexity of @xmath , or the minimum number of bits that Alice
must send if her message is a function of @xmath .  Also, @xmath , the
bounded-error randomized one-way complexity, is the minimum @xmath such
that for every @xmath , if Alice sends Bob a @xmath -bit message drawn
from some distribution @xmath , then Bob can output a bit @xmath such
that @xmath with probability at least @xmath .  (The subscript @xmath
means that the error is two-sided.)  The zero-error randomized
complexity @xmath is similar, except that Bob’s answer can never be
wrong: he must output @xmath with probability at least @xmath and
otherwise declare failure.

The bounded-error quantum one-way complexity @xmath is the minimum
@xmath such that, if Alice sends Bob a mixed state @xmath of @xmath
qubits, there exists a joint measurement of @xmath and @xmath enabling
Bob to output an @xmath such that @xmath with probability at least
@xmath .  The zero-error and exact complexities @xmath and @xmath are
defined analogously.  Requiring Alice’s message to be a pure state would
increase these complexities by at most a factor of @xmath , since by
Kraus’ Theorem, every @xmath -qubit mixed state can be realized as half
of a @xmath -qubit pure state.  (Winter [ 243 ] has shown that this
factor of @xmath is tight.)  See Klauck [ 154 ] for more detailed
definitions of quantum and classical one-way communication complexity
measures.

It is immediate that @xmath , that @xmath , and that @xmath .  Also, for
total @xmath , Duriš et al. [ 101 ] showed that @xmath , while Klauck [
154 ] showed that @xmath and that @xmath .  In other words, randomized
and quantum messages yield no improvement for total functions if one is
unwilling to tolerate a bounded probability of error.  This remains true
even if Alice and Bob share arbitrarily many EPR pairs [ 154 ] .  As is
often the case, the situation is dramatically different for partial
functions: there it is easy to see that @xmath can be constant even
though @xmath : let @xmath if @xmath and @xmath and @xmath if @xmath and
@xmath , promised that one of these is the case.

Moreover, Bar-Yossef, Jayram, and Kerenidis [ 43 ] have almost shown
that @xmath can be exponentially smaller than @xmath .  In particular,
they proved that separation for a relation , meaning a problem for which
Bob has many possible valid outputs.  For a partial function @xmath
based on their relation, they also showed that @xmath whereas @xmath ;
and they conjectured (but did not prove) that @xmath .

##### 30.1 Quantum Advice

Informally, @xmath is the class of languages decidable in polynomial
time on a quantum computer, given a polynomial-size quantum advice state
that depends only on the input length.  I now make the definition more
formal.

###### Definition 51

A language @xmath is in @xmath if there exists a polynomial-size quantum
circuit family @xmath , and a polynomial-size family of quantum states
@xmath , such that for all @xmath ,

1.   If @xmath then @xmath , where @xmath is the probability that the
    first qubit is measured to be @xmath , after @xmath is applied to
    the starting state @xmath .

2.   If @xmath then @xmath . ³⁷ ³⁷ 37 If the starting state is @xmath
    for some @xmath , then the acceptance probability is not required to
    lie in @xmath .  Therefore, what I call @xmath corresponds to what
    Nishimura and Yamakami [ 186 ] call @xmath .  Also, it does not
    matter whether the circuit family @xmath is uniform, since we are
    giving it advice anyway.

The central open question about @xmath is whether it equals @xmath , or
@xmath with polynomial-size classical advice.  We do have a candidate
for an oracle problem separating the two classes: the group membership
problem of Watrous [ 239 ] , which I will describe for completeness.
 Let @xmath be a black box group ³⁸ ³⁸ 38 In other words, we have a
quantum oracle available that given @xmath outputs @xmath (i.e.
exclusive-OR’s @xmath into an answer register), and that given @xmath
outputs @xmath . whose elements are uniquely labeled by @xmath -bit
strings, and let @xmath be a subgroup of @xmath .  Both @xmath and
@xmath depend only on the input length @xmath , so we can assume that a
nonuniform algorithm knows generating sets for both of them.  Given an
element @xmath as input, the problem is to decide whether @xmath .

If @xmath is “sufficiently nonabelian” and @xmath is exponentially
large, we do not know how to solve this problem in @xmath or even @xmath
.  On the other hand, we can solve it in @xmath as follows.  Let the
quantum advice state be an equal superposition over all elements of
@xmath :

  -- -------- --
     @xmath   
  -- -------- --

We can transform @xmath into

  -- -------- --
     @xmath   
  -- -------- --

by mapping @xmath to @xmath to @xmath for each @xmath . Our algorithm
will first prepare the state @xmath , then apply a Hadamard gate to the
first qubit, and finally measure the first qubit in the standard basis,
in order to distinguish the cases @xmath and @xmath with constant bias.
 The first case occurs whenever @xmath , and the second occurs whenever
@xmath .

Although the group membership problem provides intriguing evidence for
the power of quantum advice, we have no idea how to show that it is not
also solvable using classical advice.  Indeed, apart from a result of
Nishimura and Yamakami [ 186 ] that @xmath , essentially nothing was
known about the class @xmath before the work reported here.

##### 30.2 The Almost As Good As New Lemma

The following simple lemma, which was implicit in [ 32 ] , is used three
times in this chapter—in Theorems 56 , 57 , and 64 .  It says that, if
the outcome of measuring a quantum state @xmath could be predicted with
near-certainty given knowledge of @xmath , then measuring @xmath will
damage it only slightly.  Recall that the trace distance @xmath between
two mixed states @xmath and @xmath equals @xmath , where @xmath are the
eigenvalues of @xmath .

###### Lemma 52

Suppose a @xmath -outcome measurement of a mixed state @xmath yields
outcome @xmath with probability @xmath .  Then after the measurement, we
can recover a state @xmath such that @xmath .  This is true even if the
measurement is a POVM (that is, involves arbitrarily many ancilla
qubits).

Proof. Let @xmath be a purification of the entire system ( @xmath plus
ancilla).  We can represent any measurement as a unitary @xmath applied
to @xmath , followed by a @xmath -qubit measurement.  Let @xmath and
@xmath be the two possible pure states after the measurement; then
@xmath and @xmath for some @xmath such that @xmath and @xmath .  Writing
the measurement result as @xmath , it is easy to show that

  -- -------- --
     @xmath   
  -- -------- --

So applying @xmath to @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath be the restriction of @xmath to the original qubits of @xmath
.  Theorem 9.2 of Nielsen and Chuang [ 182 ] shows that tracing out a
subsystem never increases trace distance, so @xmath .

#### 31 Simulating Quantum Messages

Let @xmath be a Boolean function.  In this section I first combine
existing results to obtain the relation @xmath for total @xmath , and
then prove using a new method that @xmath for all @xmath (partial or
total).

Define the communication matrix @xmath to be a @xmath matrix with @xmath
in the @xmath row and @xmath column.  Then letting @xmath be the number
of distinct rows in @xmath , the following is immediate.

###### Proposition 53

For total @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Also, let the VC-dimension @xmath equal the maximum @xmath for which
there exists a @xmath submatrix @xmath of @xmath with @xmath .  Then
Klauck [ 154 ] observed the following, based on a lower bound for
quantum random access codes due to Nayak [ 180 ] .

###### Proposition 54 (Klauck)

@xmath for total @xmath .

Now let @xmath be the number of distinct columns in @xmath .  Then
Proposition 54 yields the following general lower bound:

###### Corollary 55

@xmath for total @xmath , where @xmath is the size of Bob’s input.

Proof. It follows from a lemma of Sauer [ 212 ] that

  -- -------- --
     @xmath   
  -- -------- --

Hence @xmath , so

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

In particular, @xmath and @xmath are polynomially related for total
@xmath , whenever Bob’s input is polynomially smaller than Alice’s, and
Alice’s input is not “padded.”  More formally, @xmath whenever @xmath
for some @xmath and @xmath (i.e. all rows of @xmath are distinct).  For
then @xmath by Proposition 53 , and @xmath by Corollary 55 .

I now give a new method for replacing quantum messages by classical ones
when Bob’s input is small.  Although the best bound I know how to obtain
with this method— @xmath —is slightly weaker than the @xmath of
Corollary 55 , our method works for partial Boolean functions as well as
total ones.  It also yields a (relatively) efficient procedure by which
Bob can reconstruct Alice’s quantum message, a fact I will exploit in
Section 31.1 to show @xmath .  By contrast, the method based on Sauer’s
Lemma seems to be nonconstructive.

###### Theorem 56

@xmath for all @xmath (partial or total).

Proof. Let @xmath be a partial Boolean function with @xmath , and for
all @xmath , let @xmath .  Suppose Alice can send Bob a quantum state
with @xmath qubits, that enables him to compute @xmath for any @xmath
with error probability at most @xmath .  Then she can also send him a
boosted state @xmath with @xmath qubits, such that for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the probability that some measurement @xmath yields a ‘
@xmath ’ outcome when applied to @xmath .  We can assume for simplicity
that @xmath is a pure state @xmath ; as discussed in Section 30 , this
increases the message length by at most a factor of @xmath .

Let @xmath be any subset of @xmath satisfying @xmath .  Then starting
with @xmath , Bob can measure @xmath for each @xmath in lexicographic
order, reusing the same message state again and again but uncomputing
whatever garbage he generates while measuring.  Let @xmath be the state
after the @xmath measurement; thus @xmath .  Since the probability that
Bob outputs the wrong value of @xmath on any given @xmath is at most
@xmath , Lemma 52 implies that

  -- -------- --
     @xmath   
  -- -------- --

Since trace distance satisfies the triangle inequality, this in turn
implies that

  -- -------- --
     @xmath   
  -- -------- --

Now imagine an “ideal scenario” in which @xmath for every @xmath ; that
is, the measurements do not damage @xmath at all.  Then the maximum bias
with which Bob could distinguish the actual from the ideal scenario is

  -- -------- --
     @xmath   
  -- -------- --

So by the union bound, Bob will output @xmath for every @xmath
simultaneously with probability at least

  -- -------- --
     @xmath   
  -- -------- --

for sufficiently large @xmath .

Now imagine that the communication channel is blocked, so Bob has to
guess what message Alice wants to send him.  He does this by using the
@xmath -qubit maximally mixed state @xmath in place of @xmath .  We can
write @xmath as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath are orthonormal vectors such that @xmath .  So if Bob uses
the same procedure as above except with @xmath instead of @xmath , then
for any @xmath with @xmath , he will output @xmath for every @xmath
simultaneously with probability at least @xmath .

The classical simulation of the quantum protocol is now as follows.
 Alice’s message to Bob consists of @xmath inputs @xmath , together with
@xmath . ³⁹ ³⁹ 39 Strictly speaking, Bob will be able to compute @xmath
for himself given @xmath ; he does not need Alice to tell him the @xmath
values. Thus the message length is @xmath .  Here are the semantics of
Alice’s message: “Bob, suppose you looped over all @xmath in
lexicographic order; and for each one, guessed that @xmath , where
@xmath is @xmath if @xmath and @xmath if @xmath .  Then @xmath is the
first @xmath for which you would guess the wrong value of @xmath .  In
general, let @xmath be the state obtained by starting from @xmath and
then measuring @xmath in that order, given that the outcomes of the
measurements are @xmath respectively.  (Note that @xmath is not changed
by measurements of every @xmath up to @xmath , only by measurements of
@xmath .)  If you looped over all @xmath in lexicographic order
beginning from @xmath , then @xmath is the first @xmath you would
encounter for which @xmath .”

Given the sequence of @xmath ’s as defined above, it is obvious that Bob
can compute @xmath for any @xmath .  First, if @xmath for some @xmath ,
then he simply outputs @xmath .  Otherwise, let @xmath be the largest
@xmath for which @xmath lexicographically.  Then Bob prepares a
classical description of the state @xmath —which he can do since he
knows @xmath and @xmath —and then outputs @xmath as his claimed value of
@xmath .  Notice that, although Alice uses her knowledge of @xmath to
prepare her message, Bob does not need to know @xmath in order to
interpret the message.  That is why the simulation works for partial as
well as total functions.

But why can we assume that the sequence of @xmath ’s stops at @xmath for
some @xmath ?  Suppose @xmath ; we will derive a contradiction.  Let
@xmath .  Then @xmath , so we know from previous reasoning that if Bob
starts with @xmath and then measures @xmath in that order, he will
observe @xmath simultaneously with probability at least @xmath .  But by
the definition of @xmath , the probability that @xmath yields the
correct outcome is at most @xmath , conditioned on @xmath having yielded
the correct outcomes.  Therefore @xmath are observed simultaneously with
probability at most @xmath , contradiction.

##### 31.1 Simulating Quantum Advice

I now apply the new simulation method to upper-bound the power of
quantum advice.

###### Theorem 57

@xmath .

Proof. For notational convenience, let @xmath if input @xmath is in
language @xmath , and @xmath otherwise.  Suppose @xmath is computed by a
@xmath machine using quantum advice of length @xmath .  We will give a
@xmath machine that computes @xmath using classical advice of length
@xmath .  Because of the close connection between advice and one-way
communication, the simulation method will be essentially identical to
that of Theorem 56 .

By using a boosted advice state on @xmath qubits, a polynomial-time
quantum algorithm @xmath can compute @xmath with error probability at
most @xmath .  Now the classical advice to the @xmath machine consists
of @xmath inputs @xmath , together with @xmath .  Let @xmath be the
maximally mixed state on @xmath qubits.  Also, let @xmath be the
probability that @xmath outputs ‘ @xmath ’ on input @xmath , given
@xmath as its advice state.  Then @xmath is the lexicographically first
input @xmath for which @xmath .  In general, let @xmath be the state
obtained by starting with @xmath as the advice and then running @xmath
on @xmath in that order (uncomputing garbage along the way), if we
postselect on @xmath correctly outputting @xmath .  Then @xmath is the
lexicographically first @xmath for which @xmath .

Given the classical advice, we can compute @xmath as follows: if @xmath
then output @xmath .  Otherwise let @xmath be the largest @xmath for
which @xmath lexicographically, and output @xmath .  The proof that this
algorithm works is the same as in Theorem 56 , and so is omitted for
brevity.  All that needs to be shown is that the algorithm can be
implemented in @xmath .

Adleman, DeMarrais, and Huang [ 16 ] (see also Fortnow and Rogers [ 116
] ) showed that @xmath , by using what physicists would call a “Feynman
sum-over-histories.”  Specifically, let @xmath be a polynomial-size
quantum circuit that starts in the all- @xmath state, and that consists
solely of Toffoli and Hadamard gates (Shi [ 217 ] has shown that this
gate set is universal).  Also, let @xmath be the amplitude of basis
state @xmath after all gates in @xmath have been applied.  We can write
@xmath as a sum of exponentially many contributions, @xmath , where each
@xmath is a rational real number computable in classical polynomial
time.  So by evaluating the sum

  -- -------- --
     @xmath   
  -- -------- --

putting positive and negative terms on “opposite sides of the ledger,” a
@xmath machine can check whether @xmath for any rational constant @xmath
.  It follows that a @xmath machine can also check whether

  -- -------- --
     @xmath   
  -- -------- --

(or equivalently, whether @xmath ) for any classical polynomial-time
predicates @xmath and @xmath .

Now suppose the circuit @xmath does the following, in the case @xmath .
 It first prepares the @xmath -qubit maximally mixed state @xmath (as
half of a @xmath -qubit pure state), and then runs @xmath on @xmath in
that order, using @xmath as its advice state.  The claimed values of
@xmath are written to output registers but not measured.  For @xmath ,
let the predicate @xmath hold if and only if basis state @xmath contains
the output sequence @xmath .  Then it is not hard to see that

  -- -------- --
     @xmath   
  -- -------- --

so @xmath and hence @xmath if and only if @xmath .  Since the case
@xmath is trivial, this shows that @xmath is computable in @xmath .

Let me make five remarks about Theorem 57 .  First, for the same reason
that Theorem 56 works for partial as well as total functions, one
actually obtains the stronger result that @xmath , where @xmath and
@xmath are the promise-problem versions of @xmath and @xmath
respectively.

Second, as pointed out to me by Lance Fortnow, a corollary of Theorem 57
is that we cannot hope to show an unrelativized separation between
@xmath and @xmath , without also showing that @xmath does not have
polynomial-size circuits.  For @xmath clearly implies that @xmath .  But
the latter then implies that @xmath , since assuming @xmath we could
also obtain polynomial-size circuits for a language @xmath by defining a
new language @xmath , consisting of all @xmath pairs such that the
@xmath machine would accept @xmath given advice string @xmath .  The
reason this works is that @xmath is a syntactically defined class.

Third, initially I showed that @xmath , by using a simulation in which
an @xmath machine keeps track of a subspace @xmath of the advice Hilbert
space to which the ‘true’ advice state must be close.  In that
simulation, the classical advice specifies inputs @xmath for which
@xmath is at least halved; the observation that @xmath must be at least
@xmath by the end then implies that @xmath , meaning that the advice is
of polynomial size.  The huge improvement from @xmath to @xmath came
solely from working with measurement outcomes and their probabilities
instead of with subspaces and their dimensions .  We can compute the
former using the same “Feynman sum-over-histories” that Adleman et al. [
16 ] used to show @xmath , but I could not see any way to compute the
latter without explicitly storing and diagonalizing exponentially large
matrices.

Fourth, assuming @xmath , Theorem 57 is almost the best result of its
kind that one could hope for, since the only classes known to lie
between @xmath and @xmath and not known to equal either are obscure ones
such as @xmath [ 116 ] .  Initially the theorem seemed to me to prove
something stronger, namely that @xmath .  Here @xmath is the class of
languages decidable by polynomial-size quantum circuits with
postselection —meaning the ability to measure a qubit that has a nonzero
probability of being @xmath , and then assume that the measurement
outcome will be @xmath .  Clearly @xmath lies somewhere between @xmath
and @xmath ; one can think of it as a quantum analogue of the classical
complexity class @xmath [ 142 ] .  It turns out, however, that @xmath
(see Chapter II ).

Fifth, it is clear that Adleman et al.’s @xmath result [ 16 ] can be
extended to show that @xmath .  Here @xmath is the quantum analogue of
@xmath —that is, quantum polynomial time but where the probability of a
correct answer need only be bounded above @xmath , rather than above
@xmath .  It has been asked whether Theorem 57 could similarly be
extended to show that @xmath .  The answer is no—for indeed, @xmath
contains every language whatsoever!  To see this, given any function
@xmath , let the quantum advice state be

  -- -------- --
     @xmath   
  -- -------- --

Then a @xmath algorithm to compute @xmath is as follows: given an input
@xmath , first measure @xmath in the standard basis.  If @xmath is
observed, output @xmath ; otherwise output a uniform random bit.

#### 32 A Direct Product Theorem for Quantum Search

Can quantum computers solve @xmath -complete problems in polynomial
time?  In the early days of quantum computing, Bennett et al. [ 51 ]
gave an oracle relative to which @xmath , providing what is still the
best evidence we have that the answer is no.  It is easy to extend
Bennett et al.’s result to give an oracle relative to which @xmath ;
that is, @xmath is hard even for nonuniform quantum algorithms.  But
when we try to show @xmath relative to an oracle, a new difficulty
arises: even if the oracle encodes @xmath exponentially hard search
problems for each input length @xmath , the quantum advice, being an
“exponentially large object” itself, might somehow encode information
about all @xmath problems.  We need to argue that even if so, only a
miniscule fraction of that information can be extracted by measuring the
advice.

How does one prove such a statement?  As it turns out, the task can be
reduced to proving a direct product theorem for quantum search.  This is
a theorem that in its weakest form says the following: given @xmath
items, @xmath of which are marked, if we lack enough time to find even
one marked item, then the probability of finding all @xmath items
decreases exponentially in @xmath .  For intuitively, suppose there were
a quantum advice state that let us efficiently find any one of @xmath
marked items.  Then by “guessing” the advice (i.e. replacing it by a
maximally mixed state), and then using the guessed advice multiple
times, we could efficiently find all @xmath of the items with a success
probability that our direct product theorem shows is impossible.  This
reduction is formalized in Theorem 64 .

But what about the direct product theorem itself?  It seems like it
should be trivial to prove—for surely there are no devious correlations
by which success in finding one marked item leads to success in finding
all the others!  So it is surprising that even a weak direct product
theorem eluded proof for years.  In 2001, Klauck [ 155 ] gave an
attempted proof using the hybrid method of Bennett et al. [ 51 ] .  His
motivation was to show a limitation of space-bounded quantum sorting
algorithms.  Unfortunately, Klauck’s proof is fallacious. ⁴⁰ ⁴⁰ 40
Specifically, the last sentence in the proof of Lemma 5 in [ 155 ]
(“Clearly this probability is at least @xmath ”) is not justified by
what precedes it.

In this section I give the first correct proof of a direct product
theorem, based on the polynomial method of Beals et al. [ 45 ] .
 Besides showing that @xmath relative to an oracle, my result can be
used to recover the conclusions in [ 155 ] about the hardness of quantum
sorting (see Klauck, Špalek, and de Wolf [ 156 ] for details).  I expect
the result to have other applications as well.

I will need the following lemma of Beals et al. [ 45 ] .

###### Lemma 58 (Beals et al.)

Suppose a quantum algorithm makes @xmath queries to an oracle string
@xmath , and accepts with probability @xmath .  Then there exists a real
polynomial @xmath , of degree at most @xmath , such that

  -- -------- --
     @xmath   
  -- -------- --

for all integers @xmath , where @xmath denotes the Hamming weight of
@xmath .

Lemma 58 implies that, to lower-bound the number of queries @xmath made
by a quantum algorithm, it suffices to lower-bound @xmath , where @xmath
is a real polynomial representing the algorithm’s expected acceptance
probability.  As an example, any quantum algorithm that computes the
@xmath function on @xmath bits, with success probability at least @xmath
, yields a polynomial @xmath such that @xmath and @xmath for all
integers @xmath .  To lower-bound the degree of such a polynomial, one
can use an inequality proved by A. A. Markov in 1890 ( [ 172 ] ; see
also [ 203 ] ):

###### Theorem 59 (A. A. Markov)

Given a real polynomial @xmath and constant @xmath , let @xmath and
@xmath .  Then

  -- -------- --
     @xmath   
  -- -------- --

Theorem 59 deals with the entire range @xmath , whereas in our setting
@xmath is constrained only at the integer points @xmath .  But as shown
in [ 104 , 184 , 204 ] , this is not a problem.  For by elementary
calculus, @xmath and @xmath imply that @xmath for some real @xmath , and
therefore @xmath .  Furthermore, let @xmath be a point in @xmath where
@xmath .  Then @xmath and @xmath imply that @xmath .  Thus

  -- -------- --
     @xmath   
  -- -------- --

This is the proof of Beals et al. [ 45 ] that quantum search requires
@xmath queries.

When proving a direct product theorem, one can no longer apply Theorem
59 so straightforwardly.  The reason is that the success probabilities
in question are extremely small, and therefore the maximum derivative
@xmath could also be extremely small.  Fortunately, though, one can
still prove a good lower bound on the degree of the relevant polynomial
@xmath .  The key is to look not just at the first derivative of @xmath
, but at higher derivatives.

To start, we need a lemma about the behavior of functions under repeated
differentiation.

###### Lemma 60

Let @xmath be an infinitely differentiable function such that for some
positive integer @xmath , we have @xmath for all @xmath and @xmath .
 Also, let @xmath , where @xmath is the @xmath derivative of @xmath
evaluated at @xmath (thus @xmath ).  Then @xmath for all @xmath .

Proof. We claim, by induction on @xmath , that there exist @xmath points
@xmath such that @xmath for all @xmath and @xmath .  If we define @xmath
, then the base case @xmath is immediate from the conditions of the
lemma.  Suppose the claim is true for @xmath ; then by elementary
calculus, for all @xmath there exists a point @xmath such that @xmath .
 Notice that @xmath .  So there is also a point @xmath such that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

With the help of Lemma 60 , one can sometimes lower-bound the degree of
a real polynomial even its first derivative is small throughout the
region of interest. To do so, I will use the following generalization of
A. A. Markov’s inequality (Theorem 59 ), which was proved by A. A.
Markov’s younger brother V. A. Markov in 1892 ( [ 173 ] ; see also [ 203
] ).

###### Theorem 61 (V. A. Markov)

Given a real polynomial @xmath of degree @xmath and positive real number
@xmath , let @xmath .  Then for all @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Here @xmath is the @xmath Chebyshev polynomial of the first kind.

As demonstrated below, combining Theorem 61 with Lemma 60 yields a lower
bound on @xmath .

###### Lemma 62

Let @xmath be a real polynomial such that

1.  @xmath at all integer points @xmath , and

2.   for some positive integer @xmath and real @xmath , we have @xmath
    and @xmath for all @xmath .

Then @xmath .

Proof. Let @xmath and @xmath be as in Theorem 61 .  Then for all @xmath
, Theorem 61 yields

  -- -------- --
     @xmath   
  -- -------- --

Rearranging,

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath (if @xmath then @xmath so the bound is trivial).

There are now two cases.  First suppose @xmath .  Then as discussed
previously, condition (i) implies that @xmath , and hence that

  -- -------- --
     @xmath   
  -- -------- --

by Theorem 59 .  Next suppose @xmath .  Then @xmath for all @xmath by
Lemma 60 .  So setting @xmath yields

  -- -------- --
     @xmath   
  -- -------- --

Either way we are done.

Strictly speaking, one does not need the full strength of Theorem 61 to
prove a lower bound on @xmath that suffices for an oracle separation
between @xmath and @xmath .  For one can show a
“rough-and-ready” version of V. A. Markov’s inequality by applying A. A.
Markov’s inequality (Theorem 59 ) repeatedly, to @xmath and so on.  This
yields

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath .  If @xmath is small, then this upper bound on @xmath
contradicts the lower bound of Lemma 60 .  However, the lower bound on
@xmath that one gets from A. A. Markov’s inequality is only @xmath , as
opposed to @xmath from Lemma 62 . ⁴¹ ⁴¹ 41 An earlier version of this
chapter claimed to prove @xmath , by applying Bernstein’s inequality [
56 ] rather than A. A. Markov’s to all derivatives @xmath .  I have
since discovered a flaw in that argument.  In any case, the
Bernstein lower bound is both unnecessary for an oracle separation, and
superseded by the later results of Klauck et al. [ 156 ] .

Shortly after seeing my proof of a weak direct product theorem, Klauck,
Špalek, and de Wolf [ 156 ] managed to improve the lower bound on @xmath
to the essentially tight @xmath .  In particular, their bound implies
that @xmath decreases exponentially in @xmath whenever @xmath .  They
obtained this improvement by factoring @xmath instead of differentiating
it as in Lemma 60 .

In any case, a direct product theorem follows trivially from what has
already been said.

###### Theorem 63 (Direct Product Theorem)

Suppose a quantum algorithm makes @xmath queries to an oracle string
@xmath .  Let @xmath be the minimum probability, over all @xmath with
Hamming weight @xmath , that the algorithm finds all @xmath of the ‘
@xmath ’ bits.  Then @xmath for some constant @xmath .

Proof. Have the algorithm accept if it finds @xmath or more ‘ @xmath
’ bits and reject otherwise.  Let @xmath be the expected probability of
acceptance if @xmath is drawn uniformly at random subject to @xmath .
 Then we know the following about @xmath :

1.  @xmath at all integer points @xmath , since @xmath is a probability.

2.  @xmath for all @xmath , since there are not @xmath marked items to
    be found.

3.  @xmath .

Furthermore, Lemma 58 implies that @xmath is a polynomial in @xmath
satisfying @xmath .  It follows from Lemma 62 that @xmath , or
rearranging, that @xmath .

The desired oracle separation can now be proven using standard
complexity theory tricks.

###### Theorem 64

There exists an oracle relative to which @xmath .

Proof. Given an oracle @xmath , define the language @xmath by @xmath if
and only if @xmath lexicographically and there exists an @xmath such
that @xmath and @xmath .  Clearly @xmath for all @xmath .  We argue that
for some @xmath , no @xmath machine @xmath with oracle access to @xmath
can decide @xmath .  Without loss of generality we assume @xmath is
fixed, so that only the advice states @xmath depend on @xmath .  We also
assume the advice is boosted, so that @xmath ’s error probability on any
input @xmath is @xmath .

Choose a set @xmath subject to @xmath ; then for all @xmath , set @xmath
if and only if @xmath .  We claim that by using @xmath , an algorithm
could find all @xmath elements of @xmath with high probability after
only @xmath queries to @xmath .  Here is how: first use binary search
(repeatedly halving the distance between @xmath and @xmath ) to find the
lexicographically first element of @xmath .  By Lemma 52 , the boosted
advice state @xmath is good for @xmath uses, so this takes only @xmath
queries.  Then use binary search to find the lexicographically second
element, and so on until all elements have been found.

Now replace @xmath by the maximally mixed state as in Theorem 56 . This
yields an algorithm that uses no advice, makes @xmath queries, and finds
all @xmath elements of @xmath with probability @xmath .  But taking
@xmath , @xmath , @xmath , and @xmath , such an algorithm would satisfy
@xmath , which violates the bound of Theorem 63 .

Indeed one can show that @xmath relative a random oracle with
probability @xmath . ⁴² ⁴² 42 First group the oracle bits into
polynomial-size blocks as Bennett and Gill [ 54 ] do, then use the
techniques of Chapter I to show that the acceptance probability is a
low-degree univariate polynomial in the number of all- @xmath blocks.
 The rest of the proof follows Theorem 64 .

#### 33 The Trace Distance Method

This section introduces a new method for proving lower bounds on quantum
one-way communication complexity.  Unlike in Section 31 , here I do not
try to simulate quantum protocols using classical ones.  Instead I prove
lower bounds for quantum protocols directly, by reasoning about the
trace distance between two possible distributions over Alice’s quantum
message (that is, between two mixed states).  The result is a method
that works even if Alice’s and Bob’s inputs are the same size.

I first state the method as a general theorem; then, in Section 33.1 , I
apply the theorem to prove lower bounds for two problems of Ambainis.
 Let @xmath denote the variation distance between probability
distributions @xmath and @xmath .

###### Theorem 65

Let @xmath be a total Boolean function.  For each @xmath , let @xmath be
a distribution over @xmath such that @xmath .  Let @xmath be a
distribution over @xmath , and let @xmath be the distribution over
@xmath formed by first choosing @xmath and then choosing @xmath samples
independently from @xmath .  Suppose that @xmath and that @xmath Then
@xmath .

Proof. Suppose that if Alice’s input is @xmath , then she sends Bob the
@xmath -qubit mixed state @xmath .  Suppose also that for every @xmath
and @xmath , Bob outputs @xmath with probability at least @xmath .  Then
by amplifying a constant number of times, Bob’s success probability can
be made @xmath for any constant @xmath .  So with @xmath qubits of
communication, Bob can distinguish the following two cases with constant
bias:

Case I. @xmath was drawn from @xmath and @xmath from @xmath .

Case II. @xmath was drawn from @xmath and @xmath from @xmath .

For in Case I, we assumed that @xmath with constant probability, whereas
in Case II, @xmath always.  An equivalent way to say this is that with
constant probability over @xmath , Bob can distinguish the mixed states
@xmath and @xmath with constant bias.  Therefore

  -- -------- --
     @xmath   
  -- -------- --

We need an upper bound on the trace distance @xmath that is more
amenable to analysis.  Let @xmath be the eigenvalues of @xmath .  Then

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where @xmath is the @xmath entry of @xmath .  Here the second line uses
the Cauchy-Schwarz inequality, and the third line uses the unitary
invariance of the Frobenius norm.

We claim that

  -- -------- --
     @xmath   
  -- -------- --

From this claim it follows that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Therefore the message length @xmath must be @xmath to ensure that @xmath
.

Let us now prove the claim.  We have

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

since @xmath .  For a given @xmath pair,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Now for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Hence

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

and we are done.

The difficulty in extending Theorem 65 to partial functions is that the
distribution @xmath might not make sense, since it might assign a
nonzero probability to some @xmath for which @xmath is undefined.

##### 33.1 Applications

In this subsection I apply Theorem 65 to prove lower bounds for two
problems of Ambainis.  To facilitate further research and to investigate
the scope of our method, I state the problems in a more general way than
Ambainis did.  Given a group @xmath , the coset problem @xmath is
defined as follows.  Alice is given a left coset @xmath of a subgroup in
@xmath , and Bob is given an element @xmath .  Bob must output @xmath if
@xmath and @xmath otherwise.  By restricting the group @xmath , we
obtain many interesting and natural problems.  For example, if @xmath is
prime then @xmath is just the equality problem, so the protocol of Rabin
and Yao [ 192 ] yields @xmath .

###### Theorem 66

@xmath .

Proof. The upper bound is obvious.  For the lower bound, it suffices to
consider a function @xmath defined as follows.  Alice is given @xmath
and Bob is given @xmath ; then

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath be the uniform distribution over @xmath , and let @xmath be
the uniform distribution over @xmath such that @xmath .  Thus @xmath is
the uniform distribution over @xmath ; note that

  -- -- --
        
  -- -- --

But what about the distribution @xmath , which is formed by first
drawing @xmath , and then drawing @xmath and @xmath independently from
@xmath ?  Given a pair @xmath , there are three cases regarding the
probability of its being drawn from @xmath :

1.  @xmath ( @xmath pairs).  In this case

      -- -- -------- --
                     
            @xmath   
      -- -- -------- --

2.  @xmath ( @xmath pairs).  In this case there exists a unique @xmath
    such that @xmath and @xmath , so

      -- -- -------- --
                     
            @xmath   
      -- -- -------- --

3.  @xmath but @xmath ( @xmath pairs).  In this case @xmath .

Putting it all together,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

So taking @xmath , we have @xmath by Theorem 65 .

I now consider Ambainis’ second problem.  Given a group @xmath and
nonempty set @xmath with @xmath , the subset problem @xmath is defined
as follows.  Alice is given @xmath and Bob is given @xmath ; then Bob
must output @xmath if @xmath and @xmath otherwise.

Let @xmath be the distribution over @xmath formed by drawing @xmath and
@xmath uniformly and independently from @xmath .  Then let @xmath ,
where @xmath is the uniform distribution over @xmath .

###### Proposition 67

For all @xmath such that @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Proof. Let @xmath be the uniform distribution over @xmath , and let
@xmath be the uniform distribution over @xmath such that @xmath .  Thus
@xmath is the uniform distribution over @xmath ; note that

  -- -- --
        
  -- -- --

We have

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Therefore @xmath .

Having lower-bounded @xmath in terms of @xmath , it remains only to
upper-bound the variation distance @xmath .  The following proposition
implies that for all constants @xmath , if @xmath is chosen uniformly at
random subject to @xmath , then @xmath with constant probability over
@xmath .

###### Theorem 68

For all groups @xmath and integers @xmath , if @xmath is chosen
uniformly at random subject to @xmath , then @xmath with @xmath
probability over @xmath .

Proof. We have

  -- -------- --
     @xmath   
  -- -------- --

by the Cauchy-Schwarz inequality.  We claim that

  -- -------- --
     @xmath   
  -- -------- --

for some constant @xmath .  From this it follows by Markov’s inequality
that

  -- -- --
        
  -- -- --

and hence

  -- -------- --
     @xmath   
  -- -------- --

with probability at least @xmath .

Let us now prove the claim.  We have

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath are drawn uniformly and independently from
@xmath .  So by linearity of expectation,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

First we analyze @xmath .  Let @xmath be the order of @xmath in @xmath .
 Of the @xmath possible ordered pairs @xmath , there are @xmath pairs
with the “pattern” @xmath (meaning that @xmath ), and @xmath pairs with
the pattern @xmath (meaning that @xmath ).  If @xmath (that is, @xmath
is the identity), then we have @xmath , so @xmath under the pattern
@xmath , and @xmath under the pattern @xmath .  On the other hand, if
@xmath , then @xmath under the pattern @xmath , and @xmath under the
pattern @xmath .  So

  -- -------- --
     @xmath   
  -- -------- --

Though unnecessarily cumbersome, the above analysis was a warmup for the
more complicated case of @xmath .  Table 10.1 lists the expressions for
@xmath , given @xmath and the pattern of @xmath .

Let @xmath be the number of @xmath such that @xmath , and let @xmath be
the number such that @xmath .  Then

  -- -------- -------- --
     @xmath            
              @xmath   
  -- -------- -------- --

using the fact that @xmath .

Putting it all together,

  -- -------- --
     @xmath   
  -- -------- --

and we are done.

From fingerprinting one also has the following upper bound.  Let @xmath
be the periodicity of @xmath , defined as the number of distinct sets
@xmath where @xmath .

###### Proposition 69

@xmath .

Proof. Assume for simplicity that @xmath ; otherwise we could reduce to
a subgroup @xmath with @xmath .  The protocol is as follows: Alice draws
a uniform random prime @xmath from the range @xmath ; she then sends Bob
the pair @xmath where @xmath is interpreted as an integer.  This takes
@xmath bits.  Bob outputs @xmath if and only if there exists a @xmath
such that @xmath and @xmath .  To see the protocol’s correctness,
observe that if @xmath , then there at most @xmath primes @xmath such
that @xmath , whereas the relevant range contains @xmath primes.
 Therefore, if @xmath , then by the union bound

  -- -- --
        
  -- -- --

#### 34 Open Problems

Are @xmath and @xmath polynomially related for every total Boolean
function @xmath ?  Also, can we exhibit any asymptotic separation
between these measures?  The best separation I know of is a factor of
@xmath : for the equality function we have @xmath , whereas Winter [ 243
] has shown that @xmath using a protocol involving mixed states. ⁴³ ⁴³
43 If we restrict ourselves to pure states, then @xmath qubits are
needed.  Based on that fact, a previous version of this chapter claimed
incorrectly that @xmath . This factor- @xmath savings is tight for
equality: a simple counting argument shows that @xmath ; and although
the usual randomized protocol for equality [ 192 ] uses @xmath bits,
there exist protocols based on error-correcting codes that use only
@xmath bits.  All of this holds for any constant error probability
@xmath .

Can we lower-bound @xmath for groups other than @xmath (such as @xmath ,
or nonabelian groups)?  Also, can we characterize @xmath for all sets
@xmath , closing the gap between the upper and lower bounds?

Is there an oracle relative to which @xmath ?

Can we give oracles relative to which @xmath and @xmath are not
contained in @xmath ?  Even more ambitiously, can we prove a direct
product theorem for quantum query complexity that applies to any partial
or total function (not just search)?

For all @xmath (partial or total), is @xmath whenever @xmath ?  In other
words, is the separation of Bar-Yossef et al. [ 43 ] the best possible?

Can the result @xmath for partial @xmath be improved to @xmath ?  I do
not even know how to rule out @xmath .

In the Simultaneous Messages (SM) model, there is no direct
communication between Alice and Bob; instead, Alice and Bob both send
messages to a third party called the referee , who then outputs the
function value.  The complexity measure is the sum of the two message
lengths.  Let @xmath and @xmath be the randomized and quantum
bounded-error SM complexities of @xmath respectively, and let @xmath be
the randomized SM complexity if Alice and Bob share an arbitrarily long
random string.  Building on work by Buhrman et al. [ 75 ] , Yao [ 250 ]
showed that @xmath whenever @xmath .  He then asked about the other
direction: for some @xmath , does @xmath whenever @xmath , and does
@xmath whenever @xmath ?  In an earlier version of this chapter, I
showed that @xmath , which means that a positive answer to Yao’s first
question would imply a positive answer to the second.  Later I learned
that Yao independently proved the same result [ 249 ] .  Here I ask a
related question: can @xmath ever be exponentially smaller than @xmath ?
 (Buhrman et al. [ 75 ] showed that @xmath can be exponentially smaller
than @xmath .)  Iordanis Kerenidis has pointed out to me that, based on
the hidden matching problem of Bar-Yossef et al. [ 43 ] discussed in
Section 30 , one can define a relation for which @xmath is exponentially
smaller than @xmath .  However, as in the case of @xmath versus @xmath ,
it remains to extend that result to functions.

### Chapter \thechapter Summary of Part I

From my unbiased perspective, quantum lower bounds are some of the
deepest results to have emerged from the study of quantum computing and
information.  These results tell us that many problems we thought were
intractable based on classical intuition, really are intractable
according to our best theory of the physical world.  On the other hand,
the reasons for intractability are much more subtle than in the
classical case.  In some sense, this has to be true—for otherwise the
reasons would apply even to those problems for which dramatic quantum
speedups exist.

We currently have two methods for proving lower bounds on quantum query
complexity: the polynomial method of Beals et al. [ 45 ] , and the
adversary method of Ambainis [ 27 ] .  The preceding chapters have
illustrated what, borrowing from Wigner [ 242 ] , we might call the
“unreasonable effectiveness” of these methods.  Both continue to work
far outside of their original design specs—whether by proving classical
lower bounds, lower bounds for exponentially small success probabilities
(as in the direct product theorem), or polynomial lower bounds for
quantities that have “no right” to be polynomials (as in the collision
and set comparison problems).  Yet the two methods also have
complementary limitations.  The adversary method is useless when the
relevant probability gaps are small, or when every @xmath -input differs
from every @xmath -input in a constant fraction of locations.  Likewise,
the polynomial method cannot be applied to problems that lack
permutation symmetry, at least using the techniques we currently know.
 Thus, perhaps the most important open problem in quantum lower bounds
is to develop a new method that overcomes the limitations of both the
polynomial and the adversary methods. ⁴⁴ ⁴⁴ 44 Along these lines,
Barnum, Saks, and Szegedy [ 44 ] have given what in some sense is a
provably optimal method, but their method (based on semidefinite
programming) seems too difficult to apply directly.

In keeping with the theme of this thesis, I end Part I by listing some
classical intuitions about computing, that a hypothetical being from
Conway’s Game of Life could safely carry into the quantum universe.

-   The collision problem is not that much easier than unordered search.
    For despite being extremely far from any one-to-one function, a
    random two-to-one function still looks one-to-one unless we do an
    expensive search for collisions.

-   Finding a local minimum of a function is not that much easier than
    finding a global minimum. This is because the paths leading to local
    minima could be exponentially long.

-   If we want to distinguish an input @xmath from the set of all @xmath
    such that @xmath , then there is nothing much better to do than to
    query nonadaptively according to the minimax strategy.

-   The difficulty of recursive Fourier sampling increases exponentially
    with the height of the tree.

-   Given @xmath unrelated instances of a problem, but only enough time
    to solve @xmath of them, the probability of succeeding on all @xmath
    instances decreases exponentially with @xmath .

-   @xmath -complete problems are probably hard, even with the help of
    polynomial-size advice.

## Part II Models and Reality

  LS: So you believe quantum mechanics?
  Me: Of course I do!
  LS: So a thousand years from now, people will still be doing quantum
  mechanics?
  Me: Well… um… I guess so…
  —Conversation between me and Lee Smolin

### Chapter \thechapter Skepticism of Quantum Computing

  “QC of the sort that factors long numbers seems firmly rooted in
  science fiction … The present attitude would be analogous to, say,
  Maxwell selling the Daemon of his famous thought experiment as a path
  to cheaper electricity from heat.”
  —Leonid Levin [ 165 ]

Quantum computing presents a dilemma: is it reasonable to study a type
of computer that has never been built, and might never be built in one’s
lifetime?  Some researchers strongly believe the answer is ‘no.’  Their
objections generally fall into four categories:

1.  There is a fundamental physical reason why large quantum computers
    can never be built.

2.  Even if (A) fails, large quantum computers will never be built in
    practice.

3.  Even if (A) and (B) fail, the speedup offered by quantum computers
    is of limited theoretical interest.

4.  Even if (A), (B), and (C) fail, the speedup is of limited practical
    value. ⁴⁵ ⁴⁵ 45 Because of the ‘even if’ clauses, the objections
    seem to me logically independent, so that there are @xmath possible
    positions regarding them (or @xmath if one is against quantum
    computing).  I ignore the possibility that no speedup exists, in
    other words that @xmath .  By ‘large quantum computer’ I mean any
    computer much faster than its best classical simulation, as a result
    of asymptotic complexity rather than the speed of elementary
    operations.  Such a computer need not be universal; it might be
    specialized for (say) factoring.

The objections can be classified along two axes, as in Table 12.1.

This chapter focuses on objection (A), that quantum computing is
impossible for a fundamental physical reason.  Among computer
scientists, this objection is most closely associated with Leonid Levin
[ 165 ] . ⁴⁶ ⁴⁶ 46 More recently, Oded Goldreich [ 128 ] has also put
forward an argument against quantum computing. Compared to Levin’s
arguments, Goldreich’s is easily understood: he believes that states
arising in Shor’s algorithm have exponential “non-degeneracy” and
therefore take exponential time to prepare, and that there is no burden
on those who hold this view to suggest a definition of non-degeneracy.
The following passage captures much of the flavor of his critique:

  The major problem [with quantum computing] is the requirement that
  basic quantum equations hold to multi-hundredth if not millionth
  decimal positions where the significant digits of the relevant quantum
  amplitudes reside.  We have never seen a physical law valid to over a
  dozen decimals.  Typically, every few new decimal places require major
  rethinking of most basic concepts.  Are quantum amplitudes still
  complex numbers to such accuracies or do they become quaternions,
  colored graphs, or sick-humored gremlins? [ 165 ]

Among other things, Levin argues that quantum computing is analogous to
the unit-cost arithmetic model, and should be rejected for essentially
the same reasons; that claims to the contrary rest on a confusion
between metric and topological approximation; that quantum
fault-tolerance theorems depend on extravagant assumptions; and that
even if a quantum computer failed, we could not measure its state to
prove a breakdown of quantum mechanics, and thus would be unlikely to
learn anything new.

A few responses to Levin’s arguments can be offered immediately.  First,
even classically, one can flip a coin a thousand times to produce
probabilities of order @xmath .  Should one dismiss such probabilities
as unphysical?  At the very least, it is not obvious that amplitudes
should behave differently than probabilities with respect to error—since
both evolve linearly, and neither is directly observable.

Second, if Levin believes that quantum mechanics will fail, but is
agnostic about what will replace it, then his argument can be turned
around.  How do we know that the successor to quantum mechanics will
limit us to @xmath , rather than letting us solve (say) @xmath -complete
problems?  This is more than a logical point.  Abrams and Lloyd [ 15 ]
argue that a wide class of nonlinear variants of the Schrödinger
equation would allow @xmath -complete and even @xmath -complete problems
to be solved in polynomial time.  And Penrose [ 189 ] , who proposed a
model for ‘objective collapse’ of the wavefunction, believes that his
proposal takes us outside the set of computable functions entirely!

Third, to falsify quantum mechanics, it would suffice to show that a
quantum computer evolved to some state far from the state that quantum
mechanics predicts.  Measuring the exact state is unnecessary.  Nobel
prizes have been awarded in the past ‘merely’ for falsifying a
previously held theory, rather than replacing it by a new one.  An
example is the physics Nobel awarded to Fitch [ 110 ] and Cronin [ 89 ]
in 1980 for discovering CP symmetry violation.

Perhaps the key to understanding Levin’s unease about quantum computing
lies in his remark that “we have never seen a physical law valid to over
a dozen decimals.”  Here he touches on a serious epistemological
question: How far should we extrapolate from today’s experiments to
where quantum mechanics has never been tested? I will try to address
this question by reviewing the evidence for quantum mechanics.  For my
purposes it will not suffice to declare the predictions of quantum
mechanics “verified to one part in a trillion,” because we have to
distinguish at least three different types of prediction: interference ,
entanglement , and Schrödinger cats .  Let us consider these in turn.

1.   Interference. If the different paths that an electron could take in
    its orbit around a nucleus did not interfere destructively,
    canceling each other out, then electrons would not have quantized
    energy levels.  So being accelerating electric charges, they would
    lose energy and spiral into their respective nuclei, and all matter
    would disintegrate.  That this has not happened—together with the
    results of (for example) single-photon double-slit experiments—is
    compelling evidence for the reality of quantum interference.

2.   Entanglement. One might accept that a single particle’s position is
    described by a wave in three-dimensional phase space, but deny that
    two particles are described by a wave in six -dimensional phase
    space.  However, the Bell inequality experiments of Aspect et al. [
    37 ] and successors have convinced all but a few physicists that
    quantum entanglement exists, can be maintained over large distances,
    and cannot be explained by local hidden-variable theories.

3.   Schrödinger Cats. Accepting two- and three-particle entanglement is
    not the same as accepting that whole molecules, cats, humans, and
    galaxies can be in coherent superposition states.  However, recently
    Arndt et al. [ 35 ] have performed the double-slit interference
    experiment using @xmath molecules (buckyballs) instead of photons;
    while Friedman et al. [ 119 ] have found evidence that a
    superconducting current, consisting of billions of electrons, can
    enter a coherent superposition of flowing clockwise around a coil
    and flowing counterclockwise (see Leggett [ 164 ] for a survey of
    such experiments).  Though short of cats, these experiments at least
    allow us to say the following: if we could build a general-purpose
    quantum computer with as many components as have already been placed
    into coherent superposition, then on certain problems, that computer
    would outperform any computer in the world today.

Having reviewed some of the evidence for quantum mechanics, we must now
ask what alternatives have been proposed that might also explain the
evidence.  The simplest alternatives are those in which quantum states
“spontaneously collapse” with some probability, as in the GRW
(Ghirardi-Rimini-Weber) theory [ 123 ] . ⁴⁷ ⁴⁷ 47 Penrose [ 189 ] has
proposed another such theory, but as mentioned earlier, his theory
suggests that the quantum computing model is too restrictive. The
drawbacks of the GRW theory include violations of energy conservation,
and parameters that must be fine-tuned to avoid conflicting with
experiments.  More relevant for us, though, is that the collapses
postulated by the theory are only in the position basis, so that quantum
information stored in internal degrees of freedom (such as spin) is
unaffected.  Furthermore, even if we extended the theory to collapse
those internal degrees, large quantum computers could still be built.
 For the theory predicts roughly one collapse per particle per @xmath
seconds, with a collapse affecting everything in a @xmath -meter
vicinity.  So even in such a vicinity, one could perform a computation
involving (say) @xmath particles for @xmath seconds.  Finally, as
pointed out to me by Rob Spekkens, standard quantum error-correction
techniques might be used to overcome even GRW-type decoherence.

A second class of alternatives includes those of ’t Hooft [ 227 ] and
Wolfram [ 246 ] , in which something like a deterministic cellular
automaton underlies quantum mechanics.  On the basis of his theory, ’t
Hooft predicts that “[i]t will never be possible to construct a ‘quantum
computer’ that can factor a large number faster, and within a smaller
region of space, than a classical machine would do, if the latter could
be built out of parts at least as large and as slow as the Planckian
dimensions” [ 227 ] .  Similarly, Wolfram states that “[i]ndeed within
the usual formalism [of quantum mechanics] one can construct quantum
computers that may be able to solve at least a few specific problems
exponentially faster than ordinary Turing machines.  But particularly
after my discoveries … I strongly suspect that even if this is formally
the case, it will still not turn out to be a true representation of
ultimate physical reality, but will instead just be found to reflect
various idealizations made in the models used so far” [ 246 , p.771] .

The obvious question then is how these theories account for Bell
inequality violations.  I confess to being unable to understand ’t
Hooft’s answer to this question, except that he believes that the usual
notions of causality and locality might no longer apply in quantum
gravity.  As for Wolfram’s theory, which involves “long-range
threads” to account for Bell inequality violations, I will show in
Section 35 below that it fails Wolfram’s own desiderata of causal and
relativistic invariance.

#### 35 Bell Inequalities and Long-Range Threads

  This section is excerpted from my review [ 1 ] of Stephen Wolfram’s A
  New Kind of Science [ 246 ] .

The most interesting chapter of A New Kind of Science is the ninth, on
‘Fundamental Physics.’  Here Wolfram confronts general relativity and
quantum mechanics, arguably the two most serious challenges to the
deterministic, cellular-automaton-based view of nature that he espouses.
 Wolfram conjectures that spacetime is discrete at the Planck scale, of
about @xmath centimeters or @xmath seconds.  This conjecture is not new,
and has received considerable attention recently in connection with the
holographic principle [ 65 ] from black hole thermodynamics, which
Wolfram does not discuss.  But are new ideas offered to substantiate the
conjecture?

For Wolfram, spacetime is a causal network , in which events are
vertices and edges specify the dependence relations between events.
 Pages 486–496 and 508–515 discuss in detail how to generate such a
network from a simple set of rules.  In particular, we could start with
a finite undirected ‘space graph’ @xmath .  We then posit a set of
update rules, each of which replaces a subgraph by another subgraph with
the same number of outgoing edges.  The new subgraph must preserve any
symmetries of the old one.  Then each event in the causal network
corresponds to an application of an update rule.  If updating event
@xmath becomes possible as a result of event @xmath , then we draw an
edge from @xmath to @xmath .

Properties of space are defined in terms of @xmath .  For example, if
the number of vertices in @xmath at distance at most @xmath from any
given vertex grows as @xmath , then space can be said to have dimension
@xmath .  (As for formalizing this definition, Wolfram says only that
there are “some subtleties.  For example, to find a definite volume
growth rate one does still need to take some kind of limit—and one needs
to avoid sampling too many or too few” vertices  (p. 1030).)  Similarly,
Wolfram argues that the curvature information needed for general
relativity, in particular the Ricci tensor, can be read from the
connectivity pattern of @xmath .  Interestingly, to make the model as
simple as possible, Wolfram does not associate a bit to each vertex of
@xmath , representing (say) the presence or absence of a particle.
 Instead particles are localized structures, or ‘tangles,’ in @xmath .

An immediate problem is that one might obtain many nonequivalent causal
networks, depending on the order in which update rules are applied to
@xmath .  Wolfram calls a set of rules that allows such nondeterministic
evolution a ‘multiway system.’  He recognizes, but rejects, a possible
connection to quantum mechanics:

  The notion of ‘many-figured time’ has been discussed since the 1950s
  in the context of the many-worlds interpretation of quantum mechanics.
   There are some similarities to the multiway systems that I consider
  here.  But an important difference is that while in the many-worlds
  approach, branchings are associated with possible observation or
  measurement events, what I suggest here is that they could be an
  intrinsic feature of even the very lowest-level rules for the universe
  (p. 1035-6).

It is unclear exactly what distinction is being drawn: is there any
physical event that is not associated with a possible observation or
measurement?  In any case, Wolfram opts instead for rule sets that are
‘causal invariant’: that is, that yield the same causal network
regardless of the order in which rules are applied.  As noted by
Wolfram, a sufficient (though not necessary) condition for causal
invariance is that no ‘replaceable’ subgraph overlaps itself or any
other replaceable subgraph.

Wolfram points out an immediate analogy to special relativity, wherein
observers do not in general agree on the order in which spacelike
separated events occur, yet agree on any final outcome of the events.
 He is vague, though, about how (say) the Lorentz transformations might
be derived in a causal network model:

  There are many subtleties here, and indeed to explain the details of
  what is going on will no doubt require quite a few new and rather
  abstract concepts. But the general picture that I believe will emerge
  is that when particles move faster they will appear to have more nodes
  associated with them (p. 529).

Wolfram is “certainly aware that many physicists will want to know more
details,” he says in the endnotes, about how a discrete model of the
sort he proposes can reproduce known features of physics.  But, although
he chose to omit technical formalism from the presentation, “[g]iven my
own personal background in theoretical physics it will come as no
surprise that I have often used such formalism in the process of working
out what I describe in these sections” (p. 1043).  The paradox is
obvious: if technical formalism would help convince physicists of his
ideas, then what could Wolfram lose by including it, say in the
endnotes?  If, on the other hand, such formalism is irrelevant, then why
does Wolfram even mention having used it?

Physicists’ hunger for details will likely grow further when they read
the section on ‘Quantum Phenomena’ (p. 537–545).  Here Wolfram maintains
that quantum mechanics is only an approximation to an underlying
classical (and most likely deterministic) theory.  Many physicists have
sought such a theory, from Einstein to (in modern times) ’t Hooft [ 227
] .  But a series of results, beginning in the 1960’s, has made it clear
that such a theory comes at a price.  I will argue that, although
Wolfram discusses these results, he has not understood what they
actually entail.

To begin, Wolfram is not advocating a hidden-variable approach such as
Bohmian mechanics, in which the state vector is supplemented by an
‘actual’ eigenstate of a particular observable.  Instead he thinks that,
at the lowest level, the state vector is not needed at all; it is merely
a useful construct for describing some (though presumably not all)
higher-level phenomena.  Indeterminacy arises because of one’s inability
to know the exact state of a system:

  [I]f one knew all of the underlying details of the network that makes
  up our universe, it should always be possible to work out the result
  of any measurement.  I strongly believe that the initial conditions
  for the universe were quite simple.  But like many of the processes we
  have seen in this book, the evolution of the universe no doubt
  intrinsically generates apparent randomness.  And the result is that
  most aspects of the network that represents the current state of our
  universe will seem essentially random (p. 543).

Similarly, Wolfram explains as follows why an electron has wave
properties: “…a network which represents our whole universe must also
include us as observers.  And this means that there is no way that we
can look at the network from the outside and see the electron as a
definite object” (p. 538).  An obvious question then is how Wolfram
accounts for the possibility of quantum computing, assuming @xmath .  He
gives an answer in the final chapter:

  Indeed within the usual formalism [of quantum mechanics] one can
  construct quantum computers that may be able to solve at least a few
  specific problems exponentially faster than ordinary Turing machines.
   But particularly after my discoveries in Chapter 9 [‘Fundamental
  Physics’], I strongly suspect that even if this is formally the case,
  it will still not turn out to be a true representation of ultimate
  physical reality, but will instead just be found to reflect various
  idealizations made in the models used so far (p. 771).

In the endnotes, though, where he explains quantum computing in more
detail, Wolfram seems to hedge about which idealizations he has in mind:

  It does appear that only modest precision is needed for the initial
  amplitudes.  And it seems that perturbations from the environment can
  be overcome using versions of error-correcting codes.  But it remains
  unclear just what might be needed actually to perform for example the
  final measurements required (p. 1148).

One might respond that, with or without quantum computing, Wolfram’s
proposals can be ruled out on the simpler ground that they disallow Bell
inequality violations.  However, Wolfram puts forward an imaginative
hypothesis to account for bipartite entanglement.  When two particles
(or ‘tangles’ in the graph @xmath ) collide, long-range ‘threads’ may
form between them, which remain in place even if the particles are later
separated:

  The picture that emerges is then of a background containing a very
  large number of connections that maintain an approximation to
  three-dimensional space, together with a few threads that in effect go
  outside of that space to make direct connections between particles (p.
  544).

The threads can produce Bell correlations, but are somehow too small
(i.e. contain too few edges) to transmit information in a way that
violates causality.

There are several objections one could raise against this thread
hypothesis.  What I will show is that, if one accepts two of Wolfram’s
own desiderata—determinism and causal invariance—then the hypothesis
fails.  First, though, let me remark that Wolfram says little about
what, to me, is a more natural possibility than the thread hypothesis.
 This is an explicitly quantum cellular automaton or causal network,
with a unitary transition rule.  The reason seems to be that he does not
want continuity anywhere in a model, not even in probabilities or
amplitudes.  In the notes, he describes an experiment with a quantum
cellular automaton as follows:

  One might hope to be able to get an ordinary cellular automaton with a
  limited set of possible values by choosing a suitable [phase rotation]
  @xmath [ @xmath and @xmath are given as examples in an illustration].
   But in fact in non-trivial cases most of the cells generated at each
  step end up having distinct values (p. 1060).

This observation is unsurprising, given the quantum computing results
mentioned in Chapter Limits on Efficient Computation in the Physical
World , to the effect that almost any nontrivial gate set is universal
(that is, can approximate any unitary matrix to any desired precision,
or any orthogonal matrix in case one is limited to reals).  Indeed, Shi
[ 217 ] has shown that a Toffoli gate, plus any gate that does not
preserve the computational basis, or a controlled- @xmath gate plus any
gate whose square does not preserve the computational basis, are both
universal gate sets.  In any case, Wolfram does not address the fact
that continuity in amplitudes seems more ‘benign’ than continuity in
measurable quantities: the former, unlike the latter, does not enable an
infinite amount of computation to be performed in a finite time.  Also,
as observed by Bernstein and Vazirani [ 55 ] , the linearity of quantum
mechanics implies that tiny errors in amplitudes will not be magnified
during a quantum computation.

I now proceed to the argument that Wolfram’s thread hypothesis is
inconsistent with causal invariance and relativity.  Let @xmath be a set
of graph updating rules, which might be probabilistic.  Then consider
the following four assertions (which, though not mathematically precise,
will be clarified by subsequent discussion).

1.  @xmath satisfies causal invariance.  That is, given any initial
    graph (and choice of randomness if @xmath is probabilistic), @xmath
    yields a unique causal network.

2.  @xmath satisfies the relativity postulate.  That is, assuming the
    causal network approximates a flat Minkowski spacetime at a large
    enough scale, there are no preferred inertial frames.

3.  @xmath permits Bell inequality violations.

4.  Any updating rule in @xmath is always considered to act on a fixed
    graph, not on a distribution or superposition over graphs.  This is
    true even if parts of the initial graph are chosen at random, and
    even if @xmath is probabilistic.

The goal is to show that, for any @xmath , at least one of these
assertions is false.  Current physical theory would suggest that (1)-(3)
are true and that (4) is false.  Wolfram, if I understand him correctly,
starts with (4) as a premise, and then introduces causal invariance to
satisfy (1) and (2), and long-range threads to satisfy (3).  Of course,
even to state the two-party Bell inequalities requires some notion of
randomness.  And on pages 299–326, Wolfram discusses three mechanisms
for introducing randomness into a system: randomness in initial
conditions, randomness from the environment (i.e. probabilistic updating
rules), and intrinsic randomness (i.e. deterministic rules that produce
pseudorandom output).  However, all of these mechanisms are compatible
with (4), and so my argument will show that they are inadequate assuming
(1)-(3).  The conclusion is that, in a model of the sort Wolfram
considers, randomness must play a more fundamental role than he allows.

In a standard Bell experiment, Alice and Bob are given input bits @xmath
and @xmath respectively, chosen uniformly and independently at random.
 Their goal is, without communicating, to output bits @xmath and @xmath
respectively such that @xmath .  Under any ‘local hidden variable’
theory, Alice and Bob can succeed with probability at most @xmath ; the
optimal strategy is for them to ignore their inputs and output (say)
@xmath and @xmath .  However, suppose Alice has a qubit @xmath and Bob a
@xmath , that are jointly in the Bell state @xmath .  Then there is a
protocol ⁴⁸ ⁴⁸ 48 If @xmath then Alice applies a @xmath phase rotation
to @xmath , and if @xmath then Bob applies a @xmath rotation to @xmath .
 Both parties then measure in the standard basis and output whatever
they observe. by which they can succeed with probability @xmath .

To model this situation, let @xmath and @xmath , corresponding to Alice
and Bob, be disjoint subgraphs of a graph @xmath .  Suppose that, at a
large scale, @xmath approximates a Euclidean space of some dimension;
and that any causal network obtained by applying updates to @xmath
approximates a Minkowski spacetime.  One can think of @xmath as
containing long-range threads from @xmath to @xmath , though the nature
of the threads will not affect the conclusions.  Encode Alice’s input
@xmath by (say) placing an edge between two specific vertices in @xmath
if and only if @xmath .  Encode @xmath similarly, and also supply Alice
and Bob with arbitrarily many correlated random bits.  Finally, let us
stipulate that at the end of the protocol, there is an edge between two
specific vertices in @xmath if and only if @xmath , and similarly for
@xmath .  A technicality is that we need to be able to identify which
vertices correspond to @xmath , @xmath , and so on, even as @xmath
evolves over time.  We could do this by stipulating that (say) “the
@xmath vertices are the ones that are roots of complete binary trees of
depth @xmath ,” and then choosing the rule set to guarantee that,
throughout the protocol, exactly two vertices have this property.

Call a variable ‘touched’ after an update has been applied to a subgraph
containing any of the variable’s vertices.  Also, let @xmath be an
assignment to all random variables: that is, @xmath , @xmath , the
correlated random bits, and the choice of randomness if @xmath is
probabilistic.  Then for all @xmath we need the following, based on what
observers in different inertial frames could perceive:

1.  There exists a sequence of updates under which @xmath is output
    before any of Bob’s variables are touched.

2.  There exists another sequence under which @xmath is output before
    any of Alice’s variables are touched.

Now it is easy to see that, if a Bell inequality violation occurs, then
causal invariance must be violated.  Given @xmath , let @xmath , @xmath
be the values of @xmath that are output under rule sequence (i), and let
@xmath , @xmath be the values output under sequence (ii).  Then there
must exist some @xmath for which either @xmath or @xmath —for if not,
then the entire protocol could be simulated under a local hidden
variable model.  It follows that the outcome of the protocol can depend
on the order in which updates are applied.

To obtain a Bell inequality violation, something like the following
seems to be needed.  We can encode ‘hidden variables’ into @xmath ,
representing the outcomes of the possible measurements Bob could make on
@xmath .  (We can imagine, if we like, that the update rules are such
that observing any one of these variables destroys all the others.
 Also, we make no assumption of contextuality.)  Then, after Alice
measures @xmath , using the long-range threads she updates Bob’s hidden
variables conditioned on her measurement outcome.  Similarly, Bob
updates Alice’s hidden variables conditioned on his outcome.  Since at
least one party must access its hidden variables for there to be Bell
inequality violations, causal invariance is still violated.  But a sort
of probabilistic causal invariance holds, in the sense that if we
marginalize out @xmath (the ‘Alice’ part of @xmath ), then the
distribution of values for each of Bob’s hidden variables is the same
before and after Alice’s update.  The lesson is that, if we want both
causal invariance and Bell inequality violations, then we need to
introduce probabilities at a fundamental level—not merely to represent
Alice and Bob’s subjective uncertainty about the state of @xmath , but
even to define whether a set of rules is or is not causal invariant.

Note that I made no assumption about how the random bits were
generated—i.e. whether they were ‘truly random’ or were the pseudorandom
output of some updating rule.  The conclusion is also unaffected if we
consider a ‘deterministic’ variant of Bell’s theorem due to Greenberger,
Horne, and Zeilinger [ 136 ] .  There three parties, Alice, Bob, and
Charlie, are given input bits @xmath , @xmath , and @xmath respectively,
satisfying the promise that @xmath .  The goal is to output bits @xmath
, @xmath , and @xmath such that @xmath .  Under a local hidden variable
model, there is no protocol that succeeds on all four possible inputs;
but if the parties share the GHZ state @xmath , then such a protocol
exists.  However, although the output is correct with certainty,
assuming causal invariance one cannot implement the protocol without
introducing randomness into the underlying rules, exactly as in the
two-party case.

After a version of the above argument was sent to Wolfram, Todd Rowland,
an employee of Wolfram, sent me email claiming that the argument fails
for the following reason.  I assumed that there exist two sequences of
updating events, one in which Alice’s measurement precedes Bob’s and one
in which Bob’s precedes Alice’s.  But I neglected the possibility that a
single update, call it @xmath , is applied to a subgraph that straddles
the long-range threads.  The event @xmath would encompass both Alice and
Bob’s measurements, so that neither would precede the other in any
sequence of updates.  We could thereby obtain a rule set @xmath
satisfying assertions (1), (3), and (4).

I argue that such an @xmath would nevertheless fail to satisfy (2).  For
in effect we start with a flat Minkowski spacetime, and then take two
distinct events that are simultaneous in a particular inertial frame,
and identify them as being the same event @xmath .  This can be
visualized as ‘pinching together’ two horizontally separated points on a
spacetime diagram.  (Actually a whole ‘V’ of points must be pinched
together, since otherwise entanglement could not have been created.)
 However, what happens in a different inertial frame?  It would seem
that @xmath , a single event, is perceived to occur at two separate
times.  That by itself might be thought acceptable, but it implies that
there exists a class of preferred inertial frames: those in which @xmath
is perceived to occur only once.  Of course, even in a flat spacetime,
one could designate as ‘preferred’ those frames in which Alice and Bob’s
measurements are perceived to be simultaneous.  A crucial distinction,
though, is that there one only obtains a class of preferred frames after
deciding which event at Alice’s location, and which at Bob’s location,
should count as the ‘measurement.’  Under Rowland’s hypothesis, by
contrast, once one decides what counts as the measurement at Alice’s
location, the decision at Bob’s location is made automatically, because
of the identification of events that would otherwise be far apart.

### Chapter \thechapter Complexity Theory of Quantum States

In my view, the central weakness in the arguments of quantum computing
skeptics is their failure to suggest any answer the following question:
Exactly what property separates the quantum states we are sure we can
create, from the states that suffice for Shor’s factoring algorithm?

I call such a property a “Sure/Shor separator.”  The purpose of this
chapter is to develop a mathematical theory of Sure/Shor separators, and
thereby illustrate what I think a scientific discussion about the
possibility of quantum computing might look like.  In particular, I will
introduce tree states , which informally are those states @xmath
expressible by a polynomial-size ‘tree’ of addition and tensor product
gates.  For example, @xmath and @xmath are both tree states.  Section 36
provides the philosophical motivation for thinking of tree states as a
possible Sure/Shor separator; then Section 37 formally defines tree
states and many related classes of quantum states.  Next, Section 38
investigates basic properties of tree states.  Among other results, it
shows that any tree state is representable by a tree of polynomial size
and logarithmic depth; and that most states do not even have large inner
product with any tree state.  Then Section 39 shows relationships among
tree size, circuit size, bounded-depth tree size, Vidal’s @xmath
complexity [ 236 ] , and several other measures.  It also relates
questions about quantum state classes to more traditional questions
about computational complexity classes.

But the main results of the chapter, proved in Section 40 , are lower
bounds on tree size for various natural families of quantum states.  In
particular, Section 40.1 analyzes “subgroup states,” which are uniform
superpositions @xmath over all elements of a subgroup @xmath .  The
importance of these states arises from their central role in stabilizer
codes, a type of quantum error-correcting code.  I first show that if
@xmath is chosen uniformly at random, then with high probability @xmath
cannot be represented by any tree of size @xmath .  This result has a
corollary of independent complexity-theoretic interest: the first
superpolynomial gap between the formula size and the multilinear formula
size of a function @xmath .  I then present two improvements of the
basic lower bound.  First, I show that a random subgroup state cannot
even be approximated well in trace distance by any tree of size @xmath .
 Second, I “derandomize” the lower bound, by using Reed-Solomon codes to
construct an explicit subgroup state with tree size @xmath .

Section 40.2 analyzes the states that arise in Shor’s factoring
algorithm—for example, a uniform superposition over all multiples of a
fixed positive integer @xmath , written in binary.  Originally, I had
hoped to show a superpolynomial tree size lower bound for these states
as well.  However, I am only able to show such a bound assuming a
number-theoretic conjecture.

The lower bounds use a sophisticated recent technique of Raz [ 195 , 196
] , which was introduced to show that the permanent and determinant of a
matrix require superpolynomial-size multilinear formulas.  Currently,
Raz’s technique is only able to show lower bounds of the form @xmath ,
but I conjecture that @xmath lower bounds hold in all of the cases
discussed above.

One might wonder how superpolynomial tree size relates to more physical
properties of a quantum state.  Section 40.3 addresses this question, by
pointing out how Raz’s lower bound technique is connected to a notion
that physicists call “persistence of entanglement” [ 71 , 100 ] .  On
the other hand, I also give examples showing that the connection is not
exact.

Section 41 studies a weakening of tree size called “manifestly
orthogonal tree size,” and shows that this measure can sometimes be
characterized exactly , enabling us to prove exponential lower bounds.
 The techniques in Section 41 might be of independent interest to
complexity theorists—one reason being that they do not obviously
“naturalize” in the sense of Razborov and Rudich [ 200 ] .

Section 42 addresses the following question.  If the state of a quantum
computer at every time step is a tree state, then can the computer be
simulated classically?  In other words, letting @xmath be the class of
languages accepted by such a machine, does @xmath ?  A positive answer
would make tree states more attractive as a Sure/Shor separator.  For
once we admit any states incompatible with the polynomial-time
Church-Turing thesis, it seems like we might as well go all the way, and
admit all states preparable by polynomial-size quantum circuits!
 Although I leave this question open, I do show that @xmath , where
@xmath is the third level of the polynomial hierarchy @xmath .  By
contrast, it is conjectured that @xmath , though admittedly not on
strong evidence.

Section 43 discusses the implications of these results for experimental
physics.  It advocates a dialectic between theory and experiment, in
which theorists would propose a class of quantum states that encompasses
everything seen so far, and then experimenters would try to prepare
states not in that class.  It also asks whether states with
superpolynomial tree size have already been observed in condensed-matter
systems; and more broadly, what sort of evidence is needed to establish
a state’s existence.  Other issues addressed in Section 43 include how
to deal with mixed states and particle position and momentum states, and
the experimental relevance of asymptotic bounds.  I conclude in Section
44 with some open problems.

#### 36 Sure/Shor Separators

Given the discussion in Chapter II , I believe that the challenge for
quantum computing skeptics is clear.  Ideally, come up with an
alternative to quantum mechanics—even an idealized toy theory—that can
account for all present-day experiments, yet would not allow large-scale
quantum computation.  Failing that, at least say what you take quantum
mechanics’ domain of validity to be .  One way to do this would be to
propose a set @xmath of quantum states that you believe corresponds to
possible physical states of affairs. ⁴⁹ ⁴⁹ 49 A skeptic might also
specify what happens if a state @xmath is acted on by a unitary @xmath
such that @xmath , but this will not be insisted upon. The set @xmath
must contain all “Sure states” (informally, the states that have already
been demonstrated in the lab), but no “Shor states” (again informally,
the states that can be shown to suffice for factoring, say, @xmath
-digit numbers).  If @xmath satisfies both of these constraints, then I
call @xmath a Sure/Shor separator (see Figure 13.1).

Of course, an alternative theory need not involve a sharp cutoff between
possible and impossible states.  So it is perfectly acceptable for a
skeptic to define a “complexity measure” @xmath for quantum states, and
then say something like the following: If @xmath is a state of @xmath
spins, and @xmath is at most, say, @xmath , then I predict that @xmath
can be prepared using only “polynomial effort.”  Also, once prepared,
@xmath will be governed by standard quantum mechanics to extremely high
precision.  All states created to date have had small values of @xmath .
 However, if @xmath grows as, say, @xmath , then I predict that @xmath
requires “exponential effort” to prepare, or else is not even
approximately governed by quantum mechanics, or else does not even make
sense in the context of an alternative theory.  The states that arise in
Shor’s factoring algorithm have exponential values of @xmath .  So as my
Sure/Shor separator, I propose the set of all infinite families of
states @xmath , where @xmath has @xmath qubits, such that @xmath for
some polynomial @xmath .

To understand the importance of Sure/Shor separators, it is helpful to
think through some examples.  A major theme of Levin’s arguments was
that exponentially small amplitudes are somehow unphysical.  However,
clearly we cannot reject all states with tiny amplitudes—for would
anyone dispute that the state @xmath is formed whenever @xmath photons
are each polarized at @xmath ?  Indeed, once we accept @xmath and @xmath
as Sure states, we are almost forced to accept @xmath as well—since we
can imagine, if we like, that @xmath and @xmath are prepared in two
separate laboratories. ⁵⁰ ⁵⁰ 50 It might be objected that in some
theories, such as Chern-Simons theory, there is no clear tensor product
decomposition.  However, the relevant question is whether @xmath is a
Sure state, given that @xmath and @xmath are both Sure states that are
well-described in tensor product Hilbert spaces. So considering a Shor
state such as

  -- -- --
        
  -- -- --

what property of this state could quantum computing skeptics latch onto
as being physically extravagant?  They might complain that @xmath
involves entanglement across hundreds or thousands of particles; but as
mentioned in Chapter II , there are other states with that same
property, namely the “Schrödinger cats” @xmath , that should be regarded
as Sure states.  Alternatively, the skeptics might object to the
combination of exponentially small amplitudes with entanglement across
hundreds of particles.  However, simply viewing a Schrödinger cat state
in the Hadamard basis produces an equal superposition over all strings
of even parity, which has both properties.  We seem to be on a slippery
slope leading to all of quantum mechanics!  Is there any defensible
place to draw a line?

The dilemma above is what led me to propose tree states as a possible
Sure/Shor separator.  The idea, which might seem more natural to
logicians than to physicists, is this.  Once we accept the linear
combination and tensor product rules of quantum mechanics—allowing
@xmath and @xmath into our set @xmath of possible states whenever @xmath
—one of our few remaining hopes for keeping @xmath a proper subset of
the set of all states is to impose some restriction on how those two
rules can be iteratively applied.  In particular, we could let @xmath be
the closure of @xmath under a polynomial number of linear combinations
and tensor products.  That is, @xmath is the set of all infinite
families of states @xmath with @xmath , such that @xmath can be
expressed as a “tree” involving at most @xmath addition, tensor product,
@xmath , and @xmath gates for some polynomial @xmath (see Figure 13.2).

To be clear, I am not advocating that “all states in Nature are tree
states” as a serious physical hypothesis.  Indeed, even if I believed
firmly in a breakdown of quantum mechanics, ⁵¹ ⁵¹ 51 which I don’t there
are other choices for the set @xmath that seem equally reasonable.  For
example, define orthogonal tree states similarly to tree states, except
that we can only form the linear combination @xmath if @xmath .  Rather
than choose among tree states, orthogonal tree states, and the other
candidate Sure/Shor separators that occurred to me, my approach will be
to prove everything I can about all of them.  If I devote more space to
tree states than to others, that is simply because tree states are the
subject of the most interesting results.  On the other hand, if one
shows (for example) that @xmath is not a tree state, then one has also
shown that @xmath is not an orthogonal tree state.  So many candidate
separators are related to each other; and indeed, their relationships
will be a major theme of the chapter.

In summary, to debate whether quantum computing is fundamentally
impossible, we need at least one proposal for how it could be
impossible.  Since even skeptics admit that quantum mechanics is valid
within some “regime,” a key challenge for any such proposal is to
separate the regime of acknowledged validity from the quantum computing
regime.  Though others will disagree, I do not see any choice but to
identify those two regimes with classes of quantum states .  For gates
and measurements that suffice for quantum computing have already been
demonstrated experimentally.  Thus, if we tried to identify the two
regimes with classes of gates or measurements, then we could equally
well talk about the class of states on which all @xmath - and @xmath
-qubit operations behave as expected.  A similar argument would apply if
we identified the two regimes with classes of quantum circuits—since any
“memory” that a quantum system retains of the previous gates in a
circuit, is part of the system’s state by definition.  So: states,
gates, measurements, circuits—what else is there?

I should stress that none of the above depends on the interpretation of
quantum mechanics.  In particular, it is irrelevant whether we regard
quantum states as “really out there” or as representing subjective
knowledge—since in either case, the question is whether there can exist
systems that we would describe by @xmath based on their observed
behavior.

Once we agree to seek a Sure/Shor separator, we quickly find that the
obvious ideas—based on precision in amplitudes, or entanglement across
of hundreds of particles—are nonstarters.  The only idea that seems
plausible is to limit the class of allowed quantum states to those with
some kind of succinct representation.  That still leaves numerous
possibilities; and for each one, it might be a difficult problem to
decide whether a given @xmath is succinctly representable or not.  Thus,
constructing a useful theory of Sure/Shor separators is a nontrivial
task.  This chapter represents a first attempt.

#### 37 Classifying Quantum States

In both quantum and classical complexity theory, the objects studied are
usually sets of languages or Boolean functions.  However, a generic
@xmath -qubit quantum state requires exponentially many classical bits
to describe, and this suggests looking at the complexity of quantum
states themselves .  That is, which states have polynomial-size
classical descriptions of various kinds?  This question has been studied
from several angles by Aharonov and Ta-Shma [ 23 ] ; Janzing, Wocjan,
and Beth [ 148 ] ; Vidal [ 236 ] ; and Green et al. [ 134 ] .  Here I
propose a general framework for the question.  For simplicity, I limit
myself to pure states @xmath with the fixed orthogonal basis @xmath .
 Also, by ‘states’ I mean infinite families of states @xmath .

Like complexity classes, pure quantum states can be organized into a
hierarchy (see Figure 13.3).  At the bottom are the classical basis
states, which have the form @xmath for some @xmath .  We can generalize
classical states in two directions: to the class @xmath of separable
states, which have the form @xmath ; and to the class @xmath , which
consists of all states @xmath that are superpositions of at most @xmath
classical states, where @xmath is a polynomial.  At the next level,
@xmath contains the states that can be written as a tensor product of
@xmath states, with qubits permuted arbitrarily.  Likewise, @xmath
@xmath contains the states that can be written as a linear combination
of a polynomial number of @xmath states.  We can continue indefinitely
to @xmath @xmath , @xmath , etc.  Containing the whole ‘tensor-sum
hierarchy’ @xmath @xmath is the class @xmath , of all states expressible
by a polynomial-size tree of additions and tensor products nested
arbitrarily.  Formally, @xmath consists of all states @xmath such that
@xmath for some polynomial @xmath , where the tree size @xmath is
defined as follows.

###### Definition 70

A quantum state tree over @xmath is a rooted tree where each leaf vertex
is labeled with @xmath for some @xmath , and each non-leaf vertex
(called a gate) is labeled with either @xmath or @xmath .  Each vertex
@xmath is also labeled with a set @xmath , such that

1.   If @xmath is a leaf then @xmath ,

2.   If @xmath is the root then @xmath ,

3.   If @xmath is a @xmath gate and @xmath is a child of @xmath , then
    @xmath ,

4.   If @xmath is a @xmath gate and @xmath are the children of @xmath ,
    then @xmath are pairwise disjoint and form a partition of @xmath .

Finally, if @xmath is a @xmath gate, then the outgoing edges of @xmath
are labeled with complex numbers.  For each @xmath , the subtree rooted
at @xmath represents a quantum state of the qubits in @xmath in the
obvious way.  We require this state to be normalized for each @xmath .
⁵² ⁵² 52 Requiring only the whole tree to represent a normalized state
clearly yields no further generality.

We say a tree is orthogonal if it satisfies the further condition that
if @xmath is a @xmath gate, then any two children @xmath of @xmath
represent @xmath with @xmath .  If the condition @xmath can be replaced
by the stronger condition that for all basis states @xmath , either
@xmath or @xmath , then we say the tree is manifestly orthogonal .
 Manifest orthogonality is an extremely unphysical definition; I
introduce it because it turns out to be interesting from a lower bounds
perspective.

For reasons of convenience, let us define the size @xmath of a tree
@xmath to be the number of leaf vertices.  Then given a state @xmath ,
the tree size @xmath is the minimum size of a tree that represents
@xmath .  The orthogonal tree size @xmath and manifestly orthogonal tree
size @xmath are defined similarly.  Then @xmath is the class of @xmath
such that @xmath for some polynomial @xmath , and @xmath is the class
such that @xmath for some @xmath .

It is easy to see that

  -- -------- --
     @xmath   
  -- -------- --

for every @xmath , and that the set of @xmath such that @xmath has
measure @xmath in @xmath .  Two other important properties of @xmath and
@xmath are as follows:

###### Proposition 71

1.  @xmath and @xmath are invariant under local ⁵³ ⁵³ 53 Several people
    told me that a reasonable complexity measure must be invariant under
    all basis changes. Alas, this would imply that all pure states have
    the same complexity! basis changes, up to a constant factor of
    @xmath .

2.   If @xmath is obtained from @xmath by applying a @xmath -qubit
    unitary, then @xmath and @xmath .

Proof.

1.  Simply replace each occurrence of @xmath in the original tree by a
    tree for @xmath , and each occurrence of @xmath by a tree for @xmath
    , as appropriate.

2.  Suppose without loss of generality that the gate is applied to the
    first @xmath qubits.  Let @xmath be a tree representing @xmath , and
    let @xmath be the restriction of @xmath obtained by setting the
    first @xmath qubits to @xmath .  Clearly @xmath .  Furthermore, we
    can express @xmath in the form @xmath , where each @xmath represents
    a @xmath -qubit state and hence is expressible by a tree of size
    @xmath .

One can also define the @xmath -approximate tree size @xmath to be the
minimum size of a tree representing a state @xmath such that @xmath ,
and define @xmath and @xmath similarly.

###### Definition 72

An arithmetic formula (over the ring @xmath and @xmath variables) is a
rooted binary tree where each leaf vertex is labeled with either a
complex number or a variable in @xmath , and each non-leaf vertex is
labeled with either @xmath or @xmath .  Such a tree represents a
polynomial @xmath in the obvious way.  We call a polynomial multilinear
if no variable appears raised to a higher power than @xmath , and an
arithmetic formula multilinear if the polynomials computed by each of
its subtrees are multilinear.

The size @xmath of a multilinear formula @xmath is the number of leaf
vertices.  Given a multilinear polynomial @xmath , the multilinear
formula size @xmath is the minimum size of a multilinear formula that
represents @xmath .  Then given a function @xmath , we define

  -- -- --
        
  -- -- --

(Actually @xmath turns out to be unique [ 184 ] .)  We can also define
the @xmath -approximate multilinear formula size of @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath .  (This metric is closely related to the inner product
@xmath , but is often more convenient to work with.)  Now given a state
@xmath in @xmath , let @xmath be the function from @xmath to @xmath
defined by @xmath .

###### Theorem 73

For all @xmath ,

1.  @xmath .

2.  @xmath .

3.  @xmath where @xmath .

4.  @xmath .

Proof.

1.  Given a tree representing @xmath , replace every unbounded fan-in
    gate by a collection of binary gates, every @xmath by @xmath , every
    @xmath vertex by @xmath , and every @xmath vertex by a formula for
    @xmath .  Push all multiplications by constants at the edges down to
    @xmath gates at the leaves.

2.  Given a multilinear formula @xmath for @xmath , let @xmath be the
    polynomial computed at vertex @xmath of @xmath , and let @xmath be
    the set of variables that appears in @xmath .  First, call @xmath
    syntactic if at every @xmath gate with children @xmath and @xmath ,
    @xmath .  A lemma of Raz [ 195 ] states that we can always make
    @xmath syntactic without increasing its size.

    Second, at every @xmath gate @xmath with children @xmath and @xmath
    , enlarge both @xmath and @xmath to @xmath , by multiplying @xmath
    by @xmath for every @xmath , and multiplying @xmath by @xmath for
    every @xmath .  Doing this does not invalidate any @xmath gate that
    is an ancestor of @xmath , since by the assumption that @xmath is
    syntactic, @xmath is never multiplied by any polynomial containing
    variables in @xmath .  Similarly, enlarge @xmath to @xmath where
    @xmath is the root of @xmath .

    Third, call @xmath max-linear if @xmath but @xmath where @xmath is
    the parent of @xmath .  If @xmath is max-linear and @xmath , then
    replace the tree rooted at @xmath by a tree computing @xmath .
     Also, replace all multiplications by constants higher in @xmath by
    multiplications at the edges.  (Because of the second step, there
    are no additions by constants higher in @xmath .)  Replacing every
    @xmath by @xmath then gives a tree representing @xmath , whose size
    is easily seen to be @xmath .

3.  Apply the reduction from part (i).  Let the resulting multilinear
    formula compute polynomial @xmath ; then

      -- -------- --
         @xmath   
      -- -------- --

4.  Apply the reduction from part (ii).  Let @xmath be the resulting
    amplitude vector; since this vector might not be normalized, divide
    each @xmath by @xmath to produce @xmath .  Then

      -- -------- -------- --
         @xmath   @xmath   
                  @xmath   
                  @xmath   
      -- -------- -------- --

Besides @xmath , @xmath , and @xmath , four other classes of quantum
states deserve mention:

@xmath , a circuit analog of @xmath , contains the states @xmath such
that for all @xmath , there exists a multilinear arithmetic circuit of
size @xmath over the complex numbers that outputs @xmath given @xmath as
input, for some polynomial @xmath .  (Multilinear circuits are the same
as multilinear trees, except that they allow unbounded fanout—that is,
polynomials computed at intermediate points can be reused arbitrarily
many times.)

@xmath contains the states @xmath such that for all @xmath , there
exists a classical circuit of size @xmath that outputs @xmath to @xmath
bits of precision given @xmath as input, for some polynomial @xmath .

@xmath contains the states that are ‘polynomially entangled’ in the
sense of Vidal [ 236 ] .  Given a partition of @xmath into @xmath and
@xmath , let @xmath be the minimum @xmath for which @xmath can be
written as @xmath , where @xmath and @xmath are states of qubits in
@xmath and @xmath respectively.   ( @xmath is known as the Schmidt rank
; see [ 182 ] for more information.)  Let @xmath .  Then @xmath if and
only if @xmath for some polynomial @xmath .

@xmath contains the states @xmath such that for all @xmath and @xmath ,
there exists a quantum circuit of size @xmath that maps the all- @xmath
state to a state some part of which has trace distance at most @xmath
from @xmath , for some polynomial @xmath .  Because of the
Solovay-Kitaev Theorem [ 153 , 182 ] , @xmath is invariant under the
choice of universal gate set.

#### 38 Basic Results

Before studying the tree size of specific quantum states, it would be
nice to know in general how tree size behaves as a complexity measure.
 In this section I prove three rather nice properties of tree size.

###### Theorem 74

For all @xmath , there exists a tree representing @xmath of size @xmath
and depth @xmath , as well as a manifestly orthogonal tree of size
@xmath and depth @xmath .

Proof. A classical theorem of Brent [ 70 ] says that given an arithmetic
formula @xmath , there exists an equivalent formula of depth @xmath and
size @xmath , where @xmath is a constant.  Bshouty, Cleve, and Eberly [
73 ] (see also Bonet and Buss [ 62 ] ) improved Brent’s theorem to show
that @xmath can be taken to be @xmath for any @xmath .  So it suffices
to show that, for ‘division-free’ formulas, these theorems preserve
multilinearity (and in the @xmath case, preserve manifest
orthogonality).

Brent’s theorem is proven by induction on @xmath .  Here is a sketch:
choose a subformula @xmath of @xmath size between @xmath and @xmath
(which one can show always exists).  Then identifying a subformula with
the polynomial computed at its root, @xmath can be written as @xmath for
some formulas @xmath and @xmath .  Furthermore, @xmath and @xmath are
both obtainable from @xmath by removing @xmath and then applying further
restrictions.  So @xmath and @xmath are both at most @xmath .  Let
@xmath be a formula equivalent to @xmath that evaluates @xmath , @xmath
, and @xmath separately, and then returns @xmath .  Then @xmath is
larger than @xmath by at most a constant factor, while by the induction
hypothesis, we can assume the formulas for @xmath , @xmath , and @xmath
have logarithmic depth.  Since the number of induction steps is @xmath ,
the total depth is logarithmic and the total blowup in formula size is
polynomial in @xmath .  Bshouty, Cleve, and Eberly’s improvement uses a
more careful decomposition of @xmath , but the basic idea is the same.

Now, if @xmath is syntactic multilinear, then clearly @xmath , @xmath ,
and @xmath are also syntactic multilinear.  Furthermore, @xmath cannot
share variables with @xmath , since otherwise a subformula of @xmath
containing @xmath would have been multiplied by a subformula containing
variables from @xmath .  Thus multilinearity is preserved.  To see that
manifest orthogonality is preserved, suppose we are evaluating @xmath
and @xmath ‘bottom up,’ and let @xmath and @xmath be the polynomials
computed at vertex @xmath of @xmath .  Let @xmath , let @xmath be the
parent of @xmath , let @xmath be the parent of @xmath , and so on until
@xmath .  It is clear that, for every @xmath , either @xmath or @xmath .
 Furthermore, suppose that property holds for @xmath ; then by induction
it holds for @xmath .  If @xmath is a @xmath gate, then this follows
from multilinearity (if @xmath and @xmath are manifestly orthogonal,
then @xmath and @xmath are also manifestly orthogonal).  If @xmath is a
@xmath gate, then letting @xmath be the set of @xmath such that @xmath ,
any polynomial @xmath added to @xmath or @xmath must have

  -- -------- --
     @xmath   
  -- -------- --

and manifest orthogonality follows.

###### Theorem 75

Any @xmath can be prepared by a quantum circuit of size polynomial in
@xmath .  Thus @xmath .

Proof. Let @xmath be the minimum size of a circuit needed to prepare
@xmath starting from @xmath .  The claim, by induction on @xmath , is
that @xmath for some polynomial @xmath .  The base case @xmath is clear.
 Let @xmath be an orthogonal state tree for @xmath , and assume without
loss of generality that every gate has fan-in @xmath (this increases
@xmath by at most a constant factor).  Let @xmath and @xmath be the
subtrees of @xmath , representing states @xmath and @xmath respectively;
note that @xmath .  First suppose @xmath is a @xmath gate; then clearly
@xmath .

Second, suppose @xmath is a @xmath gate, with @xmath and @xmath .  Let
@xmath be a quantum circuit that prepares @xmath , and @xmath be a
circuit that prepares @xmath .  Then we can prepare @xmath .  Observe
that @xmath is orthogonal to @xmath , since @xmath is orthogonal to
@xmath .  So applying a @xmath to the first register, conditioned on the
@xmath of the bits in the second register, yields @xmath , from which we
obtain @xmath by applying @xmath to the second register.  The size of
the circuit used is @xmath , with a possible constant-factor blowup
arising from the need to condition on the first register.  If we are
more careful, however, we can combine the ‘conditioning’ steps across
multiple levels of the recursion, producing a circuit of size @xmath .
 By symmetry, we can also reverse the roles of @xmath and @xmath to
obtain a circuit of size @xmath .  Therefore

  -- -------- --
     @xmath   
  -- -------- --

for some constant @xmath .  Solving this recurrence we find that @xmath
is polynomial in @xmath .

###### Theorem 76

If @xmath is chosen uniformly at random under the Haar measure, then
@xmath with probability @xmath .

Proof. To generate a uniform random state @xmath , we can choose @xmath
for each @xmath independently from a Gaussian distribution with mean
@xmath and variance @xmath , then let @xmath where @xmath .  Let

  -- -------- --
     @xmath   
  -- -------- --

and let @xmath be the set of @xmath for which @xmath .  The claim is
that @xmath .  First, @xmath , so by a standard Hoeffding-type bound,
@xmath is doubly-exponentially small in @xmath .  Second, assuming
@xmath , for each @xmath

  -- -------- --
     @xmath   
  -- -------- --

and the claim follows by a Chernoff bound.

For @xmath , let @xmath , where @xmath is @xmath if @xmath and @xmath
otherwise.  Then if @xmath , clearly

  -- -------- --
     @xmath   
  -- -------- --

where @xmath , and thus

  -- -------- --
     @xmath   
  -- -------- --

Therefore to show that @xmath with probability @xmath , we need only
show that for almost all Boolean functions @xmath , there is no
arithmetic formula @xmath of size @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

Here an arithmetic formula is real-valued, and can include addition,
subtraction, and multiplication gates of fan-in @xmath as well as
constants.  We do not need to assume multilinearity, and it is easy to
see that the assumption of bounded fan-in is without loss of generality.
 Let @xmath be the set of Boolean functions sign-represented by an
arithmetic formula @xmath of size @xmath , in the sense that @xmath for
all @xmath .  Then it suffices to show that @xmath , since the number of
functions sign-represented on an @xmath fraction of inputs is at most
@xmath .  (Here @xmath denotes the binary entropy function.)

Let @xmath be an arithmetic formula that takes as input the binary
string @xmath as well as constants @xmath .  Let @xmath denote @xmath
under a particular assignment @xmath to @xmath .  Then a result of
Gashkov [ 121 ] (see also Turán and Vatan [ 230 ] ), which follows from
Warren’s Theorem [ 237 ] in real algebraic geometry, shows that as we
range over all @xmath , @xmath sign-represents at most @xmath distinct
Boolean functions, where @xmath is the size of @xmath .  Furthermore,
excluding constants, the number of distinct arithmetic formulas of size
@xmath is at most @xmath .  When @xmath , this gives @xmath .  Therefore
@xmath ; by Theorem 73 , part (iii), this implies that @xmath .

A corollary of Theorem 76 is the following ‘nonamplification’ property:
there exist states that can be approximated to within, say, @xmath by
trees of polynomial size, but that require exponentially large trees to
approximate to within a smaller margin (say @xmath ).

###### Corollary 77

For all @xmath , there exists a state @xmath such that @xmath but @xmath
where @xmath .

Proof. It is clear from Theorem 76 that there exists a state @xmath such
that @xmath and @xmath .  Take @xmath .  Since @xmath , we have @xmath .
 On the other hand, suppose some @xmath with @xmath satisfies @xmath .
 Then

  -- -------- --
     @xmath   
  -- -------- --

Thus, letting @xmath , we have @xmath where @xmath .  By Theorem 73 ,
part (iv), this implies that @xmath .  But @xmath when @xmath ,
contradiction.

#### 39 Relations Among Quantum State Classes

This section presents some results about the quantum state hierarchy
introduced in Section 37 .  Theorem 78 shows simple inclusions and
separations, while Theorem 79 shows that separations higher in the
hierarchy would imply major complexity class separations (and vice
versa).

###### Theorem 78

1.  @xmath .

2.   All states in @xmath have tree size @xmath .

3.  @xmath but @xmath .

4.  @xmath .

5.  @xmath , @xmath , @xmath , @xmath , @xmath , and @xmath are all
    distinct.  Also, @xmath .

Proof.

1.  @xmath since any multilinear tree is also a multilinear circuit.
    @xmath since the circuit yields a polynomial-time algorithm for
    computing the amplitudes.  For @xmath , we use an idea of Vidal [
    236 ] : given @xmath , for all @xmath we can express @xmath as

      -- -------- --
         @xmath   
      -- -------- --

    where @xmath is polynomially bounded.  Furthermore, Vidal showed
    that each @xmath can be written as a linear combination of states of
    the form @xmath and @xmath —the point being that the set of @xmath
    states is the same, independently of @xmath .  This immediately
    yields a polynomial-size multilinear circuit for @xmath .

2.  Given @xmath , we can decompose @xmath as

      -- -------- --
         @xmath   
      -- -------- --

    Then @xmath and @xmath for all @xmath , so we can recursively
    decompose these states in the same manner.  It follows that @xmath ;
    solving this recurrence relation yields @xmath .

3.  @xmath follows since a sum of @xmath separable states has @xmath
    , while @xmath follows from the example of @xmath Bell pairs: @xmath
    .

4.  @xmath is obvious, while @xmath follows from the example of @xmath ,
    an equal superposition over all @xmath -bit strings of parity @xmath
    .  The following recursive formulas imply that @xmath :

      -- -------- -- --
         @xmath      
         @xmath      
      -- -------- -- --

    On the other hand, @xmath follows from @xmath together with the fact
    that @xmath has no nontrivial tensor product decomposition.

5.  @xmath and @xmath are obvious. @xmath (and hence @xmath ) follows
    from part (iii). @xmath (and hence @xmath ) follows from part (iv),
    together with the fact that @xmath has a @xmath formula based on the
    Fourier transform:

      -- -- --
            
      -- -- --

    @xmath follows from @xmath and @xmath .  Also, @xmath follows from
    @xmath , together with the fact that we can easily construct states
    in @xmath that have no nontrivial tensor product decomposition—for
    example,

      -- -------- --
         @xmath   
      -- -------- --

    @xmath follows from @xmath and @xmath .  Finally, @xmath follows
    from @xmath and @xmath .

###### Theorem 79

1.  @xmath implies @xmath .

2.  @xmath implies @xmath

3.  @xmath implies @xmath .

4.  @xmath implies @xmath .

Proof.

1.  First, @xmath implies @xmath , since given a @xmath machine @xmath ,
    the language consisting of all @xmath such that @xmath accepts on
    input @xmath and advice @xmath is clearly in @xmath .  So assume
    @xmath , and consider a state @xmath with @xmath .  By the result of
    Bernstein and Vazirani [ 55 ] that @xmath , for all @xmath there
    exists a quantum circuit of size polynomial in @xmath and @xmath
    that approximates @xmath , or the probability that the first qubit
    is measured to be @xmath , to @xmath bits of precision.  So by
    uncomputing garbage, we can prepare a state close to @xmath .
     Similarly, given a superposition over length- @xmath prefixes of
    @xmath , we can prepare a superposition over length- @xmath prefixes
    of @xmath by approximating the conditional measurement
    probabilities.  We thus obtain a state close to @xmath .  The last
    step is to approximate the phase of each @xmath , apply that phase,
    and uncompute to obtain a state close to @xmath .

2.  Given a @xmath instance @xmath , first use Valiant-Vazirani [ 231 ]
    to produce a formula @xmath that (with non-negligible probability)
    has one satisfying assignment if @xmath is satisfiable and zero
    otherwise.  Then let @xmath if @xmath is a satisfying assignment for
    @xmath and @xmath otherwise; clearly @xmath is in @xmath .  By the
    assumption @xmath , there exists a polynomial-size quantum circuit
    that approximates @xmath , and thereby finds the unique satisfying
    assignment for @xmath if it exists.

3.  As in part (i), @xmath implies @xmath .  The containment @xmath
    follows since we can approximate amplitudes to polynomially many
    bits of precision in @xmath .

4.  As is well known [ 55 ] , any quantum computation can be made
    ‘clean’ in the sense that it accepts if and only if a particular
    basis state (say @xmath ) is measured.  The implication follows
    easily.

#### 40 Lower Bounds

We want to show that certain quantum states of interest to us are not
represented by trees of polynomial size.  At first this seems like a
hopeless task.  Proving superpolynomial formula-size lower bounds for
‘explicit’ functions is a notoriously hard open problem, as it would
imply complexity class separations such as @xmath .

Here, though, we are only concerned with multilinear formulas.  Could
this make it easier to prove a lower bound?  The answer is not obvious,
but very recently, for reasons unrelated to quantum computing, Raz [ 195
, 196 ] showed the first superpolynomial lower bounds on multilinear
formula size.  In particular, he showed that multilinear formulas
computing the permanent or determinant of an @xmath matrix over any
field have size @xmath .

Raz’s technique is a beautiful combination of the Furst-Saxe-Sipser
method of random restrictions [ 120 ] , with matrix rank arguments as
used in communication complexity.  I now outline the method.  Given a
function @xmath , let @xmath be a partition of the input variables
@xmath into two collections @xmath and @xmath .  This yields a function
@xmath .  Then let @xmath be a @xmath matrix whose rows are labeled by
assignments @xmath , and whose columns are labeled by assignments @xmath
.  The @xmath entry of @xmath is @xmath .  Let @xmath be the rank of
@xmath over the complex numbers.  Finally, let @xmath be the uniform
distribution over all partitions @xmath .

The following, Corollary 3.6 in [ 196 ] , is one statement of Raz’s main
theorem; recall that @xmath is the minimum size of a multilinear formula
for @xmath .

###### Theorem 80 ([196])

Suppose that

  -- -------- --
     @xmath   
  -- -------- --

Then @xmath .

An immediate corollary yields lower bounds on approximate multilinear
formula size.  Given an @xmath matrix @xmath , let @xmath where @xmath .

###### Corollary 81

Suppose that

  -- -------- --
     @xmath   
  -- -------- --

Then @xmath .

Proof. Suppose @xmath .  Then for all @xmath such that @xmath , we would
have @xmath , and therefore

  -- -------- --
     @xmath   
  -- -------- --

by Theorem 80 .  But @xmath , and hence

  -- -------- --
     @xmath   
  -- -------- --

contradiction.

Another simple corollary gives lower bounds in terms of restrictions of
@xmath .  Let @xmath be the following distribution over restrictions
@xmath : choose @xmath variables of @xmath uniformly at random, and
rename them @xmath and @xmath .  Set each of the remaining @xmath
variables to @xmath or @xmath uniformly and independently at random.
 This yields a restricted function @xmath .  Let @xmath be a @xmath
matrix whose @xmath entry is @xmath .

###### Corollary 82

Suppose that

  -- -------- --
     @xmath   
  -- -------- --

where @xmath for some constant @xmath .  Then @xmath .

Proof. Under the hypothesis, clearly there exists a fixed restriction
@xmath of @xmath , which leaves @xmath variables unrestricted, such that

  -- -------- --
     @xmath   
  -- -------- --

Then by Theorem 80 ,

  -- -------- --
     @xmath   
  -- -------- --

The following sections apply Raz’s theorem to obtain @xmath tree
size lower bounds for two classes of quantum states: states arising in
quantum error-correction in Section 40.1 , and (assuming a
number-theoretic conjecture) states arising in Shor’s factoring
algorithm in Section 40.2 .

##### 40.1 Subgroup States

Let the elements of @xmath be labeled by @xmath -bit strings.  Given a
subgroup @xmath , we define the subgroup state @xmath as follows:

  -- -------- --
     @xmath   
  -- -------- --

Coset states arise as codewords in the class of quantum error-correcting
codes known as stabilizer codes [ 80 , 133 , 225 ] .  Our interest in
these states, however, arises from their large tree size rather than
their error-correcting properties.

Let @xmath be the following distribution over subgroups @xmath .  Choose
an @xmath matrix @xmath by setting each entry to @xmath or @xmath
uniformly and independently.  Then let @xmath .  By Theorem 73 , part
(i), it suffices to lower-bound the multilinear formula size of the
function @xmath , which is @xmath if @xmath and @xmath otherwise.

###### Theorem 83

If @xmath is drawn from @xmath , then @xmath (and hence @xmath ), with
probability @xmath over @xmath .

Proof. Let @xmath be a uniform random partition of the inputs @xmath of
@xmath into two sets @xmath and @xmath .  Let @xmath be the @xmath
matrix whose @xmath entry is @xmath ; then we need to show that @xmath
is large with high probability.  Let @xmath be the @xmath submatrix of
the @xmath matrix @xmath consisting of all rows that correspond to
@xmath for some @xmath , and similarly let @xmath be the @xmath
submatrix corresponding to @xmath .  Then it is easy to see that, so
long as @xmath and @xmath are both invertible, for all @xmath settings
of @xmath there exists a unique setting of @xmath for which @xmath .
 This then implies that @xmath is a permutation of the identity matrix,
and hence that @xmath .  Now, the probability that a random @xmath
matrix over @xmath is invertible is

  -- -------- --
     @xmath   
  -- -------- --

So the probability that @xmath and @xmath are both invertible is at
least @xmath .  By Markov’s inequality, it follows that for at least an
@xmath fraction of @xmath ’s, @xmath for at least an @xmath fraction of
@xmath ’s.  Theorem 80 then yields the desired result.

Aaronson and Gottesman [ 14 ] showed how to prepare any @xmath -qubit
subgroup state using a quantum circuit of size @xmath .  So a corollary
of Theorem 83 is that @xmath .  Since @xmath clearly has a
(non-multilinear) arithmetic formula of size @xmath , a second corollary
is the following.

###### Corollary 84

There exists a family of functions @xmath that has polynomial-size
arithmetic formulas, but no polynomial-size multilinear formulas.

The reason Corollary 84 does not follow from Raz’s results is that
polynomial-size formulas for the permanent and determinant are not
known; the smallest known formulas for the determinant have size @xmath
(see [ 79 ] ).

We have shown that not all subgroup states are tree states, but it is
still conceivable that all subgroup states are extremely well
approximated by tree states.  Let us now rule out the latter
possibility.  We first need a lemma about matrix rank, which follows
from the Hoffman-Wielandt inequality.

###### Lemma 85

Let @xmath be an @xmath complex matrix, and let @xmath be the @xmath
identity matrix.  Then @xmath .

Proof. The Hoffman-Wielandt inequality [ 144 ] (see also [ 33 ] ) states
that for any two @xmath matrices @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the @xmath singular value of @xmath (that is, @xmath ,
where @xmath are the eigenvalues of @xmath , and @xmath is the conjugate
transpose of @xmath ).  Clearly @xmath for all @xmath .  On the other
hand, @xmath has only @xmath nonzero singular values, so

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath be @xmath normalized to have @xmath .

###### Theorem 86

For all constants @xmath , if @xmath is drawn from @xmath , then @xmath
with probability @xmath over @xmath .

Proof. As in Theorem 83 , we look at the matrix @xmath induced by a
random partition @xmath .  We already know that for at least an @xmath
fraction of @xmath ’s, the @xmath and @xmath variables are in one-to-one
correspondence for at least an @xmath fraction of @xmath ’s.  In that
case @xmath , and therefore @xmath is a permutation of @xmath where
@xmath is the identity.  It follows from Lemma 85 that for all matrices
@xmath such that @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

and therefore @xmath .  Hence

  -- -------- --
     @xmath   
  -- -------- --

and the result follows from Corollary 81 .

A corollary of Theorem 86 and of Theorem 73 , part (iii), is that @xmath
with probability @xmath over @xmath , for all @xmath .

Finally, let me show how to derandomize the lower bound for subgroup
states, using ideas pointed out to me by Andrej Bogdanov.  In the proof
of Theorem 83 , all we needed about the matrix @xmath was that a random
@xmath submatrix has full rank with @xmath probability, where @xmath .
 If we switch from the field @xmath to @xmath for some @xmath , then it
is easy to construct explicit @xmath matrices with this same property.
 For example, let

  -- -------- --
     @xmath   
  -- -------- --

be the @xmath Vandermonde matrix, where @xmath are labels of elements in
@xmath . Any @xmath submatrix of @xmath has full rank, because the
Reed-Solomon (RS) code that @xmath represents is a perfect erasure code.
⁵⁴ ⁵⁴ 54 In other words, because a degree- @xmath polynomial is
determined by its values at any @xmath points. Hence, there exists an
explicit state of @xmath “qupits” with @xmath that has tree size @xmath
—namely the uniform superposition over all elements of the set @xmath ,
where @xmath is the transpose of @xmath .

To replace qupits by qubits, we concatenate the RS and Hadamard codes to
obtain a binary linear erasure code with parameters almost as good as
those of the original RS code.  More explicitly, interpret @xmath as the
field of polynomials over @xmath , modulo some irreducible of degree
@xmath .  Then let @xmath be the @xmath Boolean matrix that maps @xmath
to @xmath , where @xmath and @xmath are encoded by their @xmath vectors
of coefficients.  Let @xmath map a length- @xmath vector to its length-
@xmath Hadamard encoding.  Then @xmath is a @xmath Boolean matrix that
maps @xmath to the Hadamard encoding of @xmath .  We can now define an
@xmath “binary Vandermonde matrix” as follows:

  -- -- --
        
  -- -- --

For the remainder of the section, fix @xmath for some @xmath and @xmath
.

###### Lemma 87

A @xmath submatrix of @xmath chosen uniformly at random has rank @xmath
(that is, full rank) with probability at least @xmath , for @xmath a
sufficiently large constant.

Proof. The first claim is that @xmath for all nonzero vectors @xmath ,
where @xmath represents the number of ‘ @xmath ’ bits.  To see this,
observe that for all nonzero @xmath , the “codeword vector” @xmath must
have at least @xmath nonzero entries by the Fundamental Theorem of
Algebra, where here @xmath is interpreted as an element of @xmath .
 Furthermore, the Hadamard code maps any nonzero entry in @xmath to
@xmath nonzero bits in @xmath .

Now let @xmath be a uniformly random @xmath submatrix of @xmath .  By
the above claim, for any fixed nonzero vector @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

So by the union bound, @xmath is nonzero for all nonzero @xmath (and
hence @xmath is full rank) with probability at least

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath and @xmath , the above quantity is at least @xmath for
sufficiently large @xmath .

Given an @xmath Boolean vector @xmath , let @xmath if @xmath and @xmath
otherwise.  Then:

###### Theorem 88

@xmath .

Proof. Let @xmath and @xmath be two disjoint @xmath submatrices of
@xmath chosen uniformly at random.  Then by Lemma 87 together with the
union bound, @xmath and @xmath both have full rank with probability at
least @xmath .  Letting @xmath , it follows that

  -- -------- --
     @xmath   
  -- -------- --

by the same reasoning as in Theorem 83 .  Therefore @xmath by Corollary
82 .

Let @xmath be a uniform superposition over all @xmath such that @xmath ;
then a corollary of Theorem 88 is that @xmath .  Naturally, using the
ideas of Theorem 86 one can also show that @xmath for all @xmath .

##### 40.2 Shor States

Since the motivation for this work was to study possible Sure/Shor
separators, an obvious question is, do states arising in Shor’s
algorithm have superpolynomial tree size? Unfortunately, I am only able
to answer this question assuming a number-theoretic conjecture.  To
formalize the question, let

  -- -------- --
     @xmath   
  -- -------- --

be a Shor state.  It will be convenient to measure the second register,
so that the state of the first register has the form

  -- -------- --
     @xmath   
  -- -------- --

for some integers @xmath and @xmath .  Here @xmath is written out in
binary using @xmath bits.  Clearly a lower bound on @xmath would imply
an equivalent lower bound for the joint state of the two registers.

To avoid some technicalities, assume @xmath is prime (since the goal is
to prove a lower bound, this assumption is without loss of generality).
 Given an @xmath -bit string @xmath , let @xmath if @xmath and @xmath
otherwise.  Then @xmath by Theorem 73 , so from now on we will focus
attention on @xmath .

###### Proposition 89

1.   Let @xmath .  Then @xmath , meaning that we can set @xmath without
    loss of generality.

2.  @xmath .

Proof.

1.  Take the formula for @xmath , and restrict the most significant
    @xmath bits to sum to a number congruent to @xmath (this is always
    possible since @xmath is an isomorphism of @xmath ).

2.  For @xmath , write out the @xmath ’s for which @xmath explicitly.
     For @xmath , use the Fourier transform, similarly to Theorem 78 ,
    part (v):

      -- -------- --
         @xmath   
      -- -------- --

    This immediately yields a sum-of-products formula of size @xmath .

I now state the number-theoretic conjecture.

###### Conjecture 90

There exist constants @xmath and a prime @xmath for which the following
holds.  Let the set @xmath consist of @xmath elements of @xmath chosen
uniformly at random.  Let @xmath consist of all @xmath sums of subsets
of @xmath , and let @xmath .  Then

  -- -------- --
     @xmath   
  -- -------- --

###### Theorem 91

Conjecture 90 implies that @xmath and hence @xmath .

Proof. Let @xmath and @xmath .  Let @xmath be a restriction of @xmath
that renames @xmath variables @xmath , and sets each of the remaining
@xmath variables to @xmath or @xmath .  This leads to a new function,
@xmath , which is @xmath if @xmath and @xmath otherwise for some
constant @xmath .  Here we are defining @xmath and @xmath where @xmath
are the appropriate place values.  Now suppose @xmath and @xmath both
assume at least @xmath distinct values as we range over all @xmath .
 Then by the pigeonhole principle, for at least @xmath possible values
of @xmath , there exists a unique possible value of @xmath for which
@xmath and hence @xmath .  So @xmath , where @xmath is the @xmath matrix
whose @xmath entry is @xmath .  It follows that assuming Conjecture 90 ,

  -- -------- --
     @xmath   
  -- -------- --

Furthermore, @xmath for sufficiently large @xmath since @xmath .
 Therefore @xmath by Corollary 82 .

Using the ideas of Theorem 86 , one can show that under the same
conjecture, @xmath and @xmath for all @xmath —in other words, there
exist Shor states that cannot be approximated by polynomial-size trees.

Originally, I had stated Conjecture 90 without any restriction on how
the set @xmath is formed.  The resulting conjecture was far more general
than I needed, and indeed was falsified by Carl Pomerance (personal
communication).

##### 40.3 Tree Size and Persistence of Entanglement

In this section I pursue a deeper understanding of the tree size lower
bounds, by discussing a physical property of quantum states that is
related to error-correction as well as to superpolynomial tree size.
 Dür and Briegel [ 100 ] call a quantum state “persistently
entangled,” if (roughly speaking) it remains highly entangled even after
a limited amount of interaction with its environment.  As an
illustration, the Schrödinger cat state @xmath is in some sense highly
entangled, but it is not persistently entangled, since measuring a
single qubit in the standard basis destroys all entanglement.

By contrast, consider the “cluster states” defined by Briegel and
Raussendorf [ 71 ] .  These states have attracted a great deal of
attention because of their application to quantum computing via @xmath
-qubit measurements only [ 194 ] .  For our purposes, a two-dimensional
cluster state is an equal superposition over all settings of a @xmath
array of bits, with each basis state having a phase of @xmath , where
@xmath is the number of horizontally or vertically adjacent pairs of
bits that are both ‘ @xmath ’.  Dür and Briegel [ 100 ] showed that such
states are persistently entangled in a precise sense: one can distill
@xmath -partite entanglement from them even after each qubit has
interacted with a heat bath for an amount of time independent of @xmath
.

Persistence of entanglement seems related to how one shows tree size
lower bounds using Raz’s technique.  For to apply Corollary 82 , one
basically “measures” most of a state’s qubits, then partitions the
unmeasured qubits into two subsystems of equal size, and argues that
with high probability those two subsystems are still almost maximally
entangled.  The connection is not perfect, though.  For one thing,
setting most of the qubits to @xmath or @xmath uniformly at random is
not the same as measuring them.  For another, Theorem 80 yields @xmath
tree size lower bounds without the need to trace out a subset of qubits.
 It suffices for the original state to be almost maximally entangled, no
matter how one partitions it into two subsystems of equal size.

But what about @xmath -D cluster states—do they have tree size @xmath ?
 I strongly conjecture that the answer is ‘yes.’  However, proving this
conjecture will almost certainly require going beyond Theorem 80 .  One
will want to use random restrictions that respect the @xmath -D
neighborhood structure of cluster states—similar to the restrictions
used by Raz [ 195 ] to show that the permanent and determinant have
multilinear formula size @xmath .

I end this section by showing that there exist states that are
persistently entangled in the sense of Dür and Briegel [ 100 ] , but
that have polynomial tree size.  In particular, Dür and Briegel showed
that even one -dimensional cluster states are persistently entangled.
 On the other hand:

###### Proposition 92

Let

  -- -------- --
     @xmath   
  -- -------- --

Then @xmath .

Proof. Given bits @xmath , let @xmath be an equal superposition over all
@xmath -bit strings @xmath such that @xmath , @xmath , and @xmath .
 Then

  -- -------- -- --
     @xmath      
     @xmath      
  -- -------- -- --

Therefore @xmath , and solving this recurrence relation yields

  -- -------- --
     @xmath   
  -- -------- --

Finally observe that

  -- -- --
        
  -- -- --

#### 41 Manifestly Orthogonal Tree Size

This section studies the manifestly orthogonal tree size of coset
states: ⁵⁵ ⁵⁵ 55 All results apply equally well to the subgroup states
of Section 40.1 ; the greater generality of coset states is just for
convenience. states having the form

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a coset in @xmath .  In particular, I present a tight
characterization of @xmath , which enables me to prove exponential lower
bounds on it, in contrast to the @xmath lower bounds for ordinary tree
size.  This characterization also yields a separation between orthogonal
and manifestly orthogonal tree size; and an algorithm for computing
@xmath whose complexity is only singly exponential in @xmath .  My proof
technique is independent of Raz’s, and is highly tailored to take
advantage of manifest orthogonality.  However, even if this technique
finds no other application, it has two features that I hope will make it
of independent interest to complexity theorists.  First, it yields tight
lower bounds, and second, it does not obviously “naturalize” in the
sense of Razborov and Rudich [ 200 ] .  Rather, it takes advantage of
certain structural properties of coset states that do not seem to hold
for random states.

Given a state @xmath , recall that the manifestly orthogonal tree size
@xmath is the minimum size of a tree representing @xmath , in which all
additions are of two states @xmath with “disjoint supports”—that
is, either @xmath or @xmath for every basis state @xmath .  Here the
size @xmath of @xmath is the number of leaf vertices.  We can assume
without loss of generality that every @xmath or @xmath vertex has at
least one child, and that every child of a @xmath vertex is a @xmath
vertex and vice versa.  Also, given a set @xmath , let

  -- -------- --
     @xmath   
  -- -------- --

be a uniform superposition over the elements of @xmath , and let @xmath
.

Let @xmath be a subgroup in @xmath , for some @xmath and @xmath .  Let
@xmath , and let @xmath be a nontrivial partition of @xmath (one where
@xmath and @xmath are both nonempty).  Then clearly there exist distinct
cosets @xmath in the @xmath subsystem, and distinct cosets @xmath in the
@xmath subsystem, such that

  -- -------- --
     @xmath   
  -- -------- --

The @xmath ’s and @xmath ’s are unique up to ordering.  Furthermore, the
quantities @xmath , @xmath , @xmath , and @xmath remain unchanged as we
range over @xmath .  For this reason I suppress the dependence on @xmath
when mentioning them.

For various sets @xmath , the strategy will be to analyze @xmath , the
ratio of tree size to cardinality.  We can think of this ratio as the
“price per pound” of @xmath : the number of vertices that we have to pay
per basis state that we cover.  The following lemma says that, under
that cost measure, a coset is “as good a deal” as any of its subsets:

###### Lemma 93

For all cosets @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where the minimum is over nonempty @xmath .

Proof. By induction on @xmath .  The base case @xmath is obvious,
so assume the lemma true for @xmath .  Choose @xmath to minimize @xmath
.  Let @xmath be a manifestly orthogonal tree for @xmath of minimum
size, and let @xmath be the root of @xmath .  We can assume without loss
of generality that @xmath is a @xmath vertex, since otherwise @xmath has
some @xmath child representing a set @xmath such that @xmath .
 Therefore for some nontrivial partition @xmath of @xmath , and some
@xmath and @xmath , we have

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where the last equality holds because if @xmath , then @xmath was not a
minimal tree for @xmath .  Then

  -- -------- --
     @xmath   
  -- -------- --

where the minimum is over nonempty @xmath and @xmath such that @xmath .
 Now there must be an @xmath such that @xmath and @xmath , since
otherwise some @xmath would be assigned nonzero amplitude.  By the
induction hypothesis,

  -- -- --
        
  -- -- --

where the minima are over nonempty @xmath and @xmath respectively.
 Define @xmath and @xmath .  Then since setting @xmath and @xmath
maximizes the four quantities @xmath , @xmath , @xmath , and @xmath
simultaneously, this choice also maximizes @xmath and @xmath
simultaneously.  Therefore it maximizes their harmonic mean,

  -- -------- --
     @xmath   
  -- -------- --

We have proved that setting @xmath maximizes @xmath , or equivalently
minimizes @xmath .  The one remaining observation is that taking the
disjoint sum of @xmath over all @xmath leaves the ratio @xmath
unchanged.  So setting @xmath also minimizes @xmath , and we are done.

I can now give a recursive characterization of @xmath .

###### Theorem 94

If @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

where the minimum is over nontrivial partitions @xmath of @xmath .

Proof. The upper bound is obvious; let us prove the lower bound.  Let
@xmath be a manifestly orthogonal tree for @xmath of minimum size, and
let @xmath be the topmost @xmath vertices in @xmath .  Then there exists
a partition @xmath of @xmath such that the subtree rooted at @xmath
represents @xmath .  We have

  -- -- --
        
  -- -- --

Now let @xmath .  We will construct a partition @xmath of @xmath such
that @xmath for all @xmath , which will imply a new tree @xmath with
@xmath .  Choose @xmath such that @xmath , and suppose vertex @xmath of
@xmath expresses @xmath as @xmath for some nontrivial partition @xmath .
 Then

  -- -------- --
     @xmath   
  -- -------- --

where @xmath follows from the minimality of @xmath .  As in Lemma 93 ,
there must be an @xmath such that @xmath and @xmath .  But Lemma 93 then
implies that @xmath and that @xmath .  Combining these bounds with
@xmath and @xmath , we obtain by a harmonic mean inequality that

  -- -- --
        
  -- -- --

So setting @xmath for all @xmath yields a new tree @xmath no larger than
@xmath .  Hence by the minimality of @xmath ,

  -- -- --
        
  -- -- --

One can express Theorem 94 directly in terms of the matrix @xmath as
follows.  Let @xmath where @xmath (the vector @xmath is irrelevant, so
long as @xmath is solvable).  Then

  -- -------- -- -----
     @xmath      (*)
  -- -------- -- -----

where the minimum is over all nontrivial partitions @xmath of the
columns of @xmath .  As a base case, if @xmath has only one column, then
@xmath if @xmath and @xmath otherwise.  This immediately implies the
following.

###### Corollary 95

There exists a deterministic @xmath -time algorithm that computes @xmath
, given @xmath as input.

Proof. First compute @xmath for all @xmath matrices @xmath that are
formed by choosing a subset of the columns of @xmath .  This takes time
@xmath .  Then compute @xmath for all @xmath with one column, then for
all @xmath with two columns, and so on, applying the formula ( * ‣ 41
) recursively.  This takes time

  -- -------- --
     @xmath   
  -- -------- --

Another easy consequence of Theorem 94 is that the language @xmath is in
@xmath .  I do not know whether this language is @xmath -complete but
suspect it is.

As mentioned above, my characterization makes it possible to prove
exponential lower bounds on the manifestly orthogonal tree size of coset
states.

###### Theorem 96

Suppose the entries of @xmath are drawn uniformly and independently at
random, where @xmath .  Then @xmath @xmath with probability @xmath over
@xmath .

Proof. Let us upper-bound the probability that certain “bad
events” occur when @xmath is drawn.  The first bad event is that @xmath
contains an all-zero column.  This occurs with probability at most
@xmath .  The second bad event is that there exists a @xmath submatrix
of @xmath with @xmath that has rank at most @xmath .  This also occurs
with probability @xmath .  For we claim that, if @xmath is drawn
uniformly at random from @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

To see this, imagine choosing the columns of @xmath one by one.  For
@xmath to be at most @xmath , there must be at least @xmath columns that
are linearly dependent on the previous columns.  But each column is
dependent on the previous ones with probability at most @xmath .  The
claim then follows from the union bound.  So the probability that any
@xmath submatrix of @xmath has rank at most @xmath is at most

  -- -------- --
     @xmath   
  -- -------- --

Set @xmath and @xmath ; then the above is at most

  -- -------- --
     @xmath   
  -- -------- --

where we have used the fact that @xmath .

Assume that neither bad event occurs, and let @xmath be a partition of
the columns of @xmath that minimizes the expression ( * ‣ 41 ).  Let
@xmath if @xmath and @xmath otherwise, where @xmath and @xmath are the
numbers of columns in @xmath and @xmath respectively (so that @xmath ).
 Likewise, let @xmath be an optimal partition of the columns of @xmath ,
and let @xmath if @xmath and @xmath otherwise.  Continue in this way
until an @xmath is reached such that @xmath .  Then an immediate
consequence of ( * ‣ 41 ) is that @xmath where

  -- -------- --
     @xmath   
  -- -------- --

and @xmath .

Call @xmath a “balanced cut” if @xmath , and an “unbalanced
cut” otherwise.  If @xmath is a balanced cut, then @xmath and @xmath ,
so @xmath .  If @xmath is an unbalanced cut, then call @xmath a
“freebie” if @xmath .  There can be at most @xmath freebies, since for
each one, @xmath by the assumption that all columns of @xmath are
nonzero.  For the other unbalanced cuts, @xmath .

Assume @xmath for each balanced cut and @xmath for each unbalanced cut.
 Then if the goal is to minimize @xmath , clearly the best strategy is
to perform balanced cuts first, then unbalanced cuts until @xmath , at
which point we can use the @xmath freebies.  Let @xmath be the number of
balanced cuts; then

  -- -------- --
     @xmath   
  -- -------- --

This is minimized by taking @xmath , in which case @xmath .

A final application of my characterization is to separate orthogonal
from manifestly orthogonal tree size.

###### Corollary 97

There exist states with polynomially-bounded orthogonal tree size, but
manifestly orthogonal tree size @xmath .  Thus @xmath .

Proof. Set @xmath , and let @xmath where @xmath is drawn uniformly at
random from @xmath .  Then by Theorem 96 ,

  -- -------- --
     @xmath   
  -- -------- --

with probability @xmath over @xmath .  On the other hand, if we view
@xmath in the Fourier basis (that is, apply a Hadamard to every qubit),
then the resulting state has only @xmath basis states with nonzero
amplitude, and hence has orthogonal tree size at most @xmath .  So by
Proposition 71 , part (i), @xmath as well.

Indeed, the orthogonal tree states of Corollary 97 are superpositions
over polynomially many separable states, so it also follows that @xmath
.

#### 42 Computing With Tree States

Suppose a quantum computer is restricted to being in a tree state at all
times.  (We can imagine that if the tree size ever exceeds some
polynomial bound, the quantum computer explodes, destroying our
laboratory.)  Does the computer then have an efficient classical
simulation?  In other words, letting @xmath be the class of languages
accepted by such a machine, does @xmath ?  A positive answer would make
tree states more attractive as a Sure/Shor separator.  For once we admit
any states incompatible with the polynomial-time Church-Turing thesis,
it seems like we might as well go all the way, and admit all states
preparable by polynomial-size quantum circuits!  The @xmath versus
@xmath problem is closely related to the problem of finding an efficient
(classical) algorithm to learn multilinear formulas.  In light of Raz’s
lower bound, and of the connection between lower bounds and learning
noticed by Linial, Mansour, and Nisan [ 166 ] , the latter problem might
be less hopeless than it looks.  In this section I show a weaker result:
that @xmath is contained in @xmath , the third level of the polynomial
hierarchy.  Since @xmath is not known to lie in @xmath , this result
could be taken as weak evidence that @xmath .  (On the other hand, we do
not yet have oracle evidence even for @xmath , though not for lack of
trying [ 5 ] .)

###### Definition 98

@xmath is the class of languages accepted by a @xmath machine subject to
the constraint that at every time step @xmath , the machine’s state
@xmath is exponentially close to a tree state.  More formally, the
initial state is @xmath (for an input @xmath and polynomial bound @xmath
), and a uniform classical polynomial-time algorithm generates a
sequence of gates @xmath .  Each @xmath can be either be selected from
some finite universal basis of unitary gates (as will be shown in
Theorem 99 , part (i), the choice of gate set does not matter), or can
be a @xmath -qubit measurement.  When we perform a measurement, the
state evolves to one of two possible pure states, with the usual
probabilities, rather than to a mixed state.  We require that the final
gate @xmath is a measurement of the first qubit.  If at least one
intermediate state @xmath had @xmath , then the outcome of the final
measurement is chosen adversarially; otherwise it is given by the usual
Born probabilities.  The measurement must return @xmath with probability
at least @xmath if the input is in the language, and with probability at
most @xmath otherwise.

Some comments on the definition: I allow @xmath to deviate from a tree
state by an exponentially small amount, in order to make the model
independent of the choice of gate set.  I allow intermediate
measurements because otherwise it is unclear even how to simulate @xmath
. ⁵⁶ ⁵⁶ 56 If we try to simulate @xmath in the standard way, we might
produce complicated entanglement between the computation register and
the register containing the random bits, and no longer have a tree
state. The rule for measurements follows the “Copenhagen
interpretation,” in the sense that if a qubit is measured to be @xmath ,
then subsequent computation is not affected by what would have happened
were the qubit measured to be @xmath .  In particular, if measuring
@xmath would have led to states of tree size greater than @xmath , that
does not invalidate the results of the path where @xmath is measured.

The following theorem shows that @xmath has many of the properties one
would want it to have.

###### Theorem 99

1.   The definition of @xmath is invariant under the choice of gate set.

2.   The probabilities @xmath can be replaced by any @xmath with @xmath
    .

3.  @xmath .

Proof.

1.  The Solovay-Kitaev Theorem [ 153 , 182 ] shows that given a
    universal gate set, one can approximate any @xmath -qubit unitary to
    accuracy @xmath using @xmath qubits and a circuit of size @xmath .
     So let @xmath be a sequence of states, with @xmath produced from
    @xmath by applying a @xmath -qubit unitary @xmath (where @xmath
    ).  Then using a polynomial-size circuit, one can approximate each
    @xmath to accuracy @xmath , as in the definition of @xmath .
     Furthermore, since the approximation circuit for @xmath acts only
    on @xmath qubits, any intermediate state @xmath it produces
    satisfies @xmath by Proposition 71 .

2.  To amplify to a constant probability, run @xmath copies of the
    computation in tensor product, then output the majority answer.  By
    part (i), outputting the majority can increase the tree size by a
    factor of at most @xmath .  To amplify to @xmath , observe that the
    Boolean majority function on @xmath bits has a multilinear formula
    of size @xmath .  For let @xmath equal @xmath if @xmath and @xmath
    otherwise; then

      -- -- --
            
      -- -- --

    so @xmath , and solving this recurrence yields @xmath .
     Substituting @xmath into @xmath yields @xmath , meaning the tree
    size increases by at most a polynomial factor.

3.  To simulate @xmath , just perform a classical reversible
    computation, applying a Hadamard followed by a measurement to some
    qubit whenever we need a random bit.  Since the number of basis
    states with nonzero amplitude is at most @xmath , the simulation is
    clearly in @xmath .  The other containment is obvious.

###### Theorem 100

@xmath .

Proof. Since @xmath is closed under complement, it suffices to show that
@xmath .  Our proof will combine approximate counting with a predicate
to verify the correctness of a @xmath computation.  Let @xmath be a
uniformly-generated quantum circuit, and let @xmath be a sequence of
binary measurement outcomes.  We adopt the convention that after making
a measurement, the state vector is not rescaled to have norm @xmath .
 That way the probabilities across all ‘measurement branches’ continue
to sum to @xmath .  Let @xmath be the sequence of unnormalized pure
states under measurement outcome sequence @xmath and input @xmath ,
where @xmath .  Also, let @xmath express that @xmath for every @xmath .
 Then @xmath accepts if

  -- -- --
        
  -- -- --

while @xmath rejects if @xmath .  If we could compute each @xmath
efficiently (as well as @xmath ), we would then have a @xmath predicate
expressing that @xmath .  This follows since we can do approximate
counting via hashing in @xmath [ 131 ] , and thereby verify that an
exponentially large sum of nonnegative terms is at least @xmath , rather
than at most @xmath .  The one further fact we need is that in our
@xmath ( @xmath ) predicate, we can take the existential quantifier to
range over tuples of ‘candidate solutions’—that is, @xmath pairs
together with lower bounds @xmath on @xmath .

It remains only to show how we verify that @xmath holds and that @xmath
.  First, we extend the existential quantifier so that it guesses not
only @xmath and @xmath , but also a sequence of trees @xmath ,
representing @xmath respectively.  Second, using the last universal
quantifier to range over @xmath , we verify the following:

1.  @xmath is a fixed tree representing @xmath .

2.  @xmath equals its claimed value to @xmath bits of precision.

3.  Let @xmath be the gates applied by @xmath .  Then for all @xmath and
    @xmath , if @xmath is unitary then @xmath to @xmath bits of
    precision.  Here the right-hand side is a sum of @xmath terms (
    @xmath being the number of qubits acted on by @xmath ), each term
    efficiently computable given @xmath .  Similarly, if @xmath is a
    measurement of the @xmath qubit, then @xmath if the @xmath bit of
    @xmath equals @xmath , while @xmath otherwise.

In the proof of Theorem 100 , the only fact about tree states I needed
was that @xmath ; that is, there is a polynomial-time classical
algorithm that computes the amplitude @xmath of any basis state @xmath .
 So if we define @xmath - @xmath analogously to @xmath except that any
states in @xmath are allowed, then @xmath - @xmath as well.

#### 43 The Experimental Situation

The results of this chapter suggest an obvious challenge for
experimenters: prepare non-tree states in the lab .  For were this
challenge met, it would rule out one way in which quantum mechanics
could fail, just as the Bell inequality experiments of Aspect et al. [
37 ] did twenty years ago.  If they wished, quantum computing skeptics
could then propose a new candidate Sure/Shor separator, and
experimenters could try to rule out that one, and so on.  The result
would be to divide the question of whether quantum computing is possible
into a series of smaller questions about which states can be prepared.
 In my view, this would aid progress in two ways: by helping
experimenters set clear goals, and by forcing theorists to state clear
conjectures.

However, my experimental challenge raises some immediate questions.  In
particular, what would it mean to prepare a non-tree state?  How would
we know if we succeeded?  Also, have non-tree states already been
prepared (or observed)?  The purpose of this section is to set out my
thoughts about these questions.

First of all, when discussing experiments, it goes without saying that
we must convert asymptotic statements into statements about specific
values of @xmath .  The central tenet of computational complexity theory
is that this is possible.  Thus, instead of asking whether @xmath -qubit
states with tree size @xmath can be prepared, we ask whether @xmath
-qubit states with tree size at least (say) @xmath can be prepared.
 Even though the second question does not logically imply anything about
the first, the second is closer to what we ultimately care about anyway.
 Admittedly, knowing that @xmath tells us little about @xmath or @xmath
, especially since in Raz’s paper [ 195 ] , the constant in the exponent
@xmath is taken to be @xmath (though this can certainly be improved).
 Thus, proving tight lower bounds for small @xmath is one of the most
important problems left open by this chapter.  In Section 41 I show how
to solve this problem for the case of manifestly orthogonal tree size.

A second objection is that my formalism applies only to pure states, but
in reality all states are mixed.  However, there are several natural
ways to extend the formalism to mixed states.   Given a mixed state
@xmath , we could minimize tree size over all purifications of @xmath ,
or minimize the expected tree size @xmath , or maximum @xmath , over all
decompositions @xmath .

A third objection is a real quantum state might be a “soup” of
free-wandering fermions and bosons, with no localized subsystems
corresponding to qubits.  How can one determine the tree size of such a
state?  The answer is that one cannot.  Any complexity measure for
particle position and momentum states would have to be quite different
from the measures considered in this chapter.  On the other hand, the
states of interest for quantum computing usually do involve localized
qubits.  Indeed, even if quantum information is stored in particle
positions, one might force each particle into two sites (corresponding
to @xmath and @xmath ), neither of which can be occupied by any other
particle.  In that case it again becomes meaningful to discuss tree
size.

But how do we verify that a state with large tree size was prepared?  Of
course, if @xmath is preparable by a polynomial-size quantum circuit,
then assuming quantum mechanics is valid (and assuming our gates behave
as specified), we can always test whether a given state @xmath is close
to @xmath or not.  Let @xmath map @xmath to @xmath ; then it suffices to
test whether @xmath is close to @xmath .  However, in the experiments
under discussion, the validity of quantum mechanics is the very point in
question.  And once we allow Nature to behave in arbitrary ways, a
skeptic could explain any experimental result without having to invoke
states with large tree size.

The above fact has often been urged against me, but as it stands, it is
no different from the fact that one could explain any astronomical
observation without abandoning the Ptolemaic system.  The issue here is
not one of proof, but of accumulating observations that are consistent
with the hypothesis of large tree size, and inconsistent with
alternative hypotheses if we disallow special pleading.  So for example,
to test whether the subgroup state

  -- -------- --
     @xmath   
  -- -------- --

was prepared, we might use CNOT gates to map @xmath to @xmath for some
vector @xmath .  Based on our knowledge of @xmath , we could then
predict whether the qubit @xmath should be @xmath , @xmath , or an equal
mixture of @xmath and @xmath when measured.  Or we could apply Hadamard
gates to all @xmath qubits of @xmath , then perform the same test for
the subgroup dual to @xmath .  In saying that a system is in state
@xmath , it is not clear if we mean anything more than that it responds
to all such tests in expected ways.  Similar remarks apply to Shor
states and cluster states.

In my view, tests of the sort described above are certainly sufficient ,
so the interesting question is whether they are necessary , or whether
weaker and more indirect tests would also suffice.  This question rears
its head when we ask whether non-tree states have already been observed.
 For as pointed out to me by Anthony Leggett, there exist systems
studied in condensed-matter physics that are strong candidates for
having superpolynomial tree size.  An example is the magnetic salt LiHo
@xmath Y @xmath F @xmath studied by Ghosh et al. [ 124 ] , which, like
the cluster states of Briegel and Raussendorf [ 71 ] , basically
consists of a lattice of spins subject to pairwise nearest-neighbor
Hamiltonians.  The main differences are that the salt lattice is 3-D
instead of 2-D, is tetragonal instead of cubic, and is irregular in that
not every site is occupied by a spin.  Also, there are weak interactions
even between spins that are not nearest neighbors.  But none of these
differences seem likely to change a superpolynomial tree size into a
polynomial one.

For me, the main issues are (1) how precisely can we characterize ⁵⁷ ⁵⁷
57 By “characterize,” I mean give an explicit formula for the amplitudes
at a particular time @xmath , in some standard basis.  If a state is
characterized as the ground state of a Hamiltonian, then we first need
to solve for the amplitudes before we can prove tree size lower bounds
using Raz’s method. the quantum state of the magnetic salt, and (2) how
strong the evidence is that that is the state.  What Ghosh et al. [ 124
] did was to calculate bulk properties of the salt, such as its magnetic
susceptibility and specific heat, with and without taking into account
the quantum entanglement generated by the nearest-neighbor Hamiltonians.
 They found that including entanglement yielded a better fit to the
experimentally measured values.  However, this is clearly a far cry from
preparing a system in a state of one’s choosing by applying a known
pulse sequence, and then applying any of a vast catalog of tests to
verify that the state was prepared.  So it would be valuable to have
more direct evidence that states qualitatively like cluster states can
exist in Nature.

In summary, the ideas of this chapter underscore the importance of
current experimental work on large, persistently entangled quantum
states; but they also suggest a new motivation and perspective for this
work.  They suggest that we reexamine known condensed-matter systems
with a new goal in mind: understanding the complexity of their
associated quantum states.  They also suggest that 2-D cluster states
and random subgroup states are interesting in a way that 1-D spin chains
and Schrödinger cat states are not.  Yet when experimenters try to
prepare states of the former type, they often see it as merely a
stepping stone towards demonstrating error-correction or another quantum
computing benchmark.  Thus, Knill et al. [ 158 ] prepared ⁵⁸ ⁵⁸ 58
Admittedly, what they really prepared is the ‘pseudo-pure’ state @xmath
, where @xmath is the maximally mixed state and @xmath .  Braunstein et
al. [ 69 ] have shown that, if the number of qubits @xmath is less than
about @xmath , then such states cannot be entangled.  That is, there
exists a representation of @xmath as a mixture of pure states, each of
which is separable and therefore has tree size @xmath .  This is a
well-known limitation of the liquid NMR technology used by Knill et
al.  Thus, a key challenge is to replicate the successes of liquid NMR
using colder qubits. the @xmath -qubit state

  -- -- --
        
  -- -- --

for which @xmath from the decomposition

  -- -- --
        
  -- -- --

and for which I conjecture @xmath as well.  However, the sole motivation
of the experiment was to demonstrate a @xmath -qubit quantum
error-correcting code.  In my opinion, whether states with large tree
size can be prepared is a fundamental question in its own right.  Were
that question studied directly, perhaps we could address it for larger
numbers of qubits.

Let me end by stressing that, in the perspective I am advocating, there
is nothing sacrosanct about tree size as opposed to other complexity
measures.  This chapter concentrated on tree size because it is the
subject of our main results, and because it is better to be specific
than vague.  On the other hand, Sections 38 , 39 , and 41 contain
numerous results about orthogonal tree size, manifestly orthogonal tree
size, Vidal’s @xmath complexity, and other measures.  Readers
dissatisfied with all of these measures are urged to propose new ones,
perhaps motivated directly by experiments.  I see nothing wrong with
having multiple ways to quantify the complexity of quantum states, and
much wrong with having no ways.

#### 44 Conclusion and Open Problems

A crucial step in quantum computing was to separate the question of
whether quantum computers can be built from the question of what one
could do with them.  This separation allowed computer scientists to make
great advances on the latter question, despite knowing nothing about the
former.  I have argued, however, that the tools of computational
complexity theory are relevant to both questions.  The claim that
large-scale quantum computing is possible in principle is really a claim
that certain states can exist—that quantum mechanics will not break down
if we try to prepare those states.  Furthermore, what distinguishes
these states from states we have seen must be more than precision in
amplitudes, or the number of qubits maintained coherently.  The
distinguishing property should instead be some sort of complexity .
 That is, Sure states should have succinct representations of a type
that Shor states do not.

I have tried to show that, by adopting this viewpoint, we make the
debate about whether quantum computing is possible less ideological and
more scientific.  By studying particular examples of Sure/Shor
separators, quantum computing skeptics would strengthen their case—for
they would then have a plausible research program aimed at identifying
what, exactly, the barriers to quantum computation are.  I hope,
however, that the ‘complexity theory of quantum states’ initiated here
will be taken up by quantum computing proponents as well.  This theory
offers a new perspective on the transition from classical to quantum
computing, and a new connection between quantum computing and the
powerful circuit lower bound techniques of classical complexity theory.

I end with some open problems.

1.  Can Raz’s technique be improved to show exponential tree size lower
    bounds?

2.  Can we prove Conjecture 90 , implying an @xmath tree size lower
    bound for Shor states?

3.  Let @xmath be a uniform superposition over all @xmath -bit strings
    of Hamming weight @xmath .  It is easy to show by divide-and-conquer
    that @xmath .  Is this upper bound tight?  More generally, can we
    show a superpolynomial tree size lower bound for any state with
    permutation symmetry?

4.  Is @xmath ?  That is, are there tree states that are not orthogonal
    tree states?

5.  Is the tensor-sum hierarchy of Section 37 infinite?  That is, do we
    have @xmath @xmath @xmath for all @xmath ?

6.  Is @xmath ?  That is, can a quantum computer that is always in a
    tree state be simulated classically?  The key question seems to be
    whether the concept class of multilinear formulas is efficiently
    learnable.

7.  Is there a practical method to compute the tree size of, say, @xmath
    -qubit states?  Such a method would have great value in interpreting
    experimental results.

### Chapter \thechapter Quantum Search of Spatial Regions

  This chapter represents joint work with Andris Ambainis.

The goal of Grover’s quantum search algorithm [ 139 ] is to search an
‘unsorted database’ of size @xmath in a number of queries proportional
to @xmath .  Classically, of course, order @xmath queries are needed.
 It is sometimes asserted that, although the speedup of Grover’s
algorithm is only quadratic, this speedup is provable , in contrast to
the exponential speedup of Shor’s factoring algorithm [ 219 ] .  But is
that really true?  Grover’s algorithm is typically imagined as speeding
up combinatorial search—and we do not know whether every problem in
@xmath can be classically solved quadratically faster than the
“obvious” way, any more than we know whether factoring is in @xmath .

But could Grover’s algorithm speed up search of a physical region ?
 Here the basic problem, it seems to us, is the time needed for signals
to travel across the region.  For if we are interested in the
fundamental limits imposed by physics, then we should acknowledge that
the speed of light is finite, and that a bounded region of space can
store only a finite amount of information, according to the holographic
principle [ 65 ] .  We discuss the latter constraint in detail in
Section 47 ; for now, we say only that it suggests a model in which a
‘quantum robot’ occupies a superposition over finitely many locations,
and moving the robot from one location to an adjacent one takes unit
time.  In such a model, the time needed to search a region could depend
critically on its spatial layout.  For example, if the @xmath entries
are arranged on a line, then even to move the robot from one end to the
other takes @xmath steps.  But what if the entries are arranged on, say,
a @xmath -dimensional square grid (Figure 9 )?

#### 45 Summary of Results

This chapter gives the first systematic treatment of quantum search of
spatial regions, with ‘regions’ modeled as connected graphs.  Our main
result is positive: we show that a quantum robot can search a @xmath
-dimensional hypercube with @xmath vertices for a unique marked vertex
in time @xmath when @xmath , or @xmath when @xmath .  This matches (or
in the case of @xmath dimensions, nearly matches) the @xmath lower bound
for quantum search, and supports the view that Grover search of a
physical region presents no problem of principle. Our basic technique is
divide-and-conquer; indeed, once the idea is pointed out, an upper bound
of @xmath follows readily.  However, to obtain the tighter bounds is
more difficult; for that we use the amplitude-amplification framework of
Brassard et al. [ 67 ] .

Section 50 presents the main results; Section 50.4 shows further that,
when there are @xmath or more marked vertices, the search time becomes
@xmath when @xmath , or @xmath when @xmath .  Also, Section 51
generalizes our algorithm to arbitrary graphs that have ‘hypercube-like’
expansion properties.  Here the best bounds we can achieve are @xmath
when @xmath , or @xmath when @xmath (note that @xmath need not be an
integer).  Table 14.1 summarizes the results.

Section 52 shows, as an unexpected application of our search algorithm,
that the quantum communication complexity of the well-known disjointness
problem is @xmath .  This improves an @xmath upper bound of Høyer and de
Wolf [ 146 ] , and matches the @xmath lower bound of Razborov [ 199 ] .

The rest of the chapter is about the formal model that underlies our
results.  Section 47 sets the stage for this model, by exploring the
ultimate limits on information storage imposed by properties of space
and time.  This discussion serves only to motivate our results; thus, it
can be safely skipped by readers unconcerned with the physical universe.
 In Section 63 we define quantum query algorithms on graphs , a model
similar to quantum query algorithms as defined in Section 8 , but with
the added requirement that unitary operations be ‘local’ with respect to
some graph.  In Section 48.1 we address the difficult question, which
also arises in work on quantum random walks [ 19 ] and quantum cellular
automata [ 238 ] , of what ‘local’ means. Section 49 proves general
facts about our model, including an upper bound of @xmath for the time
needed to search any graph with diameter @xmath , and a proof (using the
hybrid argument of Bennett et al. [ 51 ] ) that this upper bound is
tight for certain graphs.  We conclude in Section 53 with some open
problems.

#### 46 Related Work

In a paper on ‘Space searches with a quantum robot,’ Benioff [ 50 ]
asked whether Grover’s algorithm can speed up search of a physical
region, as opposed to a combinatorial search space.  His answer was
discouraging: for a @xmath -D grid of size @xmath , Grover’s algorithm
is no faster than classical search.  The reason is that, during each of
the @xmath Grover iterations, the algorithm must use order @xmath steps
just to travel across the grid and return to its starting point for the
diffusion step.  On the other hand, Benioff noted, Grover’s algorithm
does yield some speedup for grids of dimension @xmath or higher, since
those grids have diameter less than @xmath .

Our results show that Benioff’s claim is mistaken: by using Grover’s
algorithm more carefully, one can search a @xmath -D grid for a single
marked vertex in @xmath time.  To us this illustrates why one should not
assume an algorithm is optimal on heuristic grounds.  Painful
experience—for example, the “obviously optimal” @xmath matrix
multiplication algorithm [ 226 ] —is what taught computer scientists to
see the proving of lower bounds as more than a formality.

Our setting is related to that of quantum random walks on graphs [ 19 ,
83 , 84 , 216 ] .  In an earlier version of this chapter, we asked
whether quantum walks might yield an alternative spatial search
algorithm, possibly even one that outperforms our divide-and-conquer
algorithm.  Motivated by this question, Childs and Goldstone [ 86 ]
managed to show that in the continuous-time setting, a quantum walk can
search a @xmath -dimensional hypercube for a single marked vertex in
time @xmath when @xmath , or @xmath when @xmath .  Our algorithm was
still faster in @xmath or fewer dimensions (see Table 14.2).
 Subsequently, however, Ambainis, Kempe, and Rivosh [ 31 ] gave an
algorithm based on a discrete-time quantum walk, which was as fast as
ours in @xmath or more dimensions, and faster in @xmath dimensions.  In
particular, when @xmath their algorithm used only @xmath time to find a
unique marked vertex.  Childs and Goldstone [ 85 ] then gave a
continuous-time quantum walk algorithm with the same performance, and
related this algorithm to properties of the Dirac equation.  It is still
open whether @xmath time is achievable in @xmath dimensions.

Currently, the main drawback of the quantum walk approach is that all
analyses have relied heavily on symmetries in the underlying graph.  If
even minor ‘defects’ are introduced, it is no longer known how to
upper-bound the running time.  By contrast, the analysis of our
divide-and-conquer algorithm is elementary, and does not depend on
eigenvalue bounds.  We can therefore show that the algorithm works for
any graphs with sufficiently good expansion properties.

Childs and Goldstone [ 86 ] argued that the quantum walk approach has
the advantage of requiring fewer auxiliary qubits than the
divide-and-conquer approach.  However, the need for many qubits was an
artifact of how we implemented the algorithm in a previous version of
the chapter.  The current version uses only one qubit.

#### 47 The Physics of Databases

Theoretical computer science generally deals with the limit as some
resource (such as time or memory) increases to infinity.  What is not
always appreciated is that, as the resource bound increases, physical
constraints may come into play that were negligible at
‘sub-asymptotic’ scales.  We believe theoretical computer scientists
ought to know something about such constraints, and to account for them
when possible.  For if the constraints are ignored on the ground that
they “never matter in practice,” then the obvious question arises: why
use asymptotic analysis in the first place, rather than restricting
attention to those instance sizes that occur in practice?

A constraint of particular interest for us is the holographic principle
[ 65 ] , which arose from black-hole thermodynamics.  The principle
states that the information content of any spatial region is
upper-bounded by its surface area (not volume), at a rate of one bit per
Planck area, or about @xmath bits per square meter.  Intuitively, if one
tried to build a spherical hard disk with mass density @xmath , one
could not keep expanding it forever.  For as soon as the radius reached
the Schwarzschild bound of @xmath (in Planck units, @xmath ), the hard
disk would collapse to form a black hole, and thus its contents would be
irretrievable.

Actually the situation is worse than that: even a planar hard disk of
constant mass density would collapse to form a black hole once its
radius became sufficiently large, @xmath .  (We assume here that the
hard disk is disc-shaped.  A linear or @xmath -D hard disk could expand
indefinitely without collapse.)  It is possible, though, that a hard
disk’s information content could asymptotically exceed its mass.  For
example, a black hole’s mass is proportional to the radius of its event
horizon, but the entropy is proportional to the square of the radius
(that is, to the surface area).  Admittedly, inherent difficulties with
storage and retrieval make a black hole horizon less than ideal as a
hard disk.  However, even a weakly-gravitating system could store
information at a rate asymptotically exceeding its mass-energy.  For
instance, Bousso [ 65 ] shows that an enclosed ball of radiation with
radius @xmath can store @xmath bits, even though its energy grows only
as @xmath .  Our results in Section 51.1 will imply that a quantum robot
could (in principle!) search such a ‘radiation disk’ for a marked item
in time @xmath .  This is some improvement over the trivial @xmath upper
bound for a @xmath -D hard disk, though it falls short of the desired
@xmath .

In general, if @xmath bits are scattered throughout a @xmath -D ball of
radius @xmath (where @xmath and the bits’ locations are known), we will
show in Theorem 130 that the time needed to search for a ‘ @xmath ’ bit
grows as @xmath (omitting logarithmic factors).  In particular, if
@xmath (saturating the holographic bound), then the time grows as @xmath
or @xmath .  To achieve a search time of @xmath , the bits would need to
be concentrated on a @xmath -D surface.

Because of the holographic principle, we see that it is not only quantum
mechanics that yields a @xmath lower bound on the number of steps needed
for unordered search.  If the items to be searched are laid out
spatially, then general relativity in @xmath dimensions independently
yields the same bound, @xmath , up to a constant factor. ⁵⁹ ⁵⁹ 59
Admittedly, the holographic principle is part of quantum gravity and not
general relativity per se .  All that matters for us, though, is that
the principle seems logically independent of quantum-mechanical
linearity, which is what produces the “other” @xmath bound.
Interestingly, in @xmath dimensions the relativity bound would be @xmath
, which for @xmath is weaker than the quantum mechanics bound.  Given
that our two fundamental theories yield the same lower bound, it is
natural to ask whether that bound is tight.  The answer seems to be that
it is not tight, since (i) the entropy on a black hole horizon is not
efficiently accessible ⁶⁰ ⁶⁰ 60 In the case of a black hole horizon,
waiting for the bits to be emitted as Hawking radiation—as recent
evidence suggests that they are [ 209 ] —takes time proportional to
@xmath , which is much too long. , and (ii) weakly-gravitating systems
are subject to the Bekenstein bound [ 48 ] , an even stronger entropy
constraint than the holographic bound.

Yet it is still of basic interest to know whether @xmath bits in a
radius- @xmath ball can be searched in time @xmath —that is, whether it
is possible to do anything better than either brute-force quantum search
(with the drawback pointed out by Benioff [ 50 ] ), or classical search.
 Our results show that it is possible.

From a physical point of view, several questions naturally arise: (1)
whether our complexity measure is realistic; (2) how to account for time
dilation; and (3) whether given the number of bits we are imagining,
cosmological bounds are also relevant.  Let us address these questions
in turn.

(1) One could argue that to maintain a ‘quantum database’ of size @xmath
requires @xmath computing elements ( [ 251 ] , though see also [ 206 ]
).  So why not just exploit those elements to search the database in
parallel ?  Then it becomes trivial to show that the search time is
limited only by the radius of the database, so the algorithms of this
chapter are unnecessary.  Our response is that, while there might be
@xmath ‘passive’ computing elements (capable of storing data), there
might be many fewer ‘active’ elements, which we consequently wish to
place in a superposition over locations.  This assumption seems
physically unobjectionable.  For a particle (and indeed any object)
really does have an indeterminate location, not merely an indeterminate
internal state (such as spin) at some location.  We leave as an open
problem, however, whether our assumption is valid for specific quantum
computer architectures such as ion traps.

(2) So long as we invoke general relativity, should we not also consider
the effects of time dilation?  Those effects are indeed pronounced near
a black hole horizon.  Again, though, for our upper bounds we will have
in mind systems far from the Schwarzschild limit, for which any time
dilation is by at most a constant factor independent of @xmath .

(3) How do cosmological considerations affect our analysis?  Bousso [ 64
] argues that, in a spacetime with positive cosmological constant @xmath
, the total number of bits accessible to any one experiment is at most
@xmath , or roughly @xmath given current experimental bounds [ 208 ] on
@xmath . ⁶¹ ⁶¹ 61 Also, Lloyd [ 170 ] argues that the total number of
bits accessible up till now is at most the square of the number of
Planck times elapsed so far, or about @xmath .  Lloyd’s bound, unlike
Bousso’s, does not depend on @xmath being positive. The numerical
coincidence between the two bounds reflects the experimental finding [
208 , 207 ] that we live in a transitional era, when both @xmath and
“dust” contribute significantly to the universe’s net energy balance (
@xmath , @xmath ).  In earlier times dust (and before that
radiation) dominated, and Lloyd’s bound was tighter.  In later times
@xmath will dominate, and Bousso’s bound will be tighter. Why we should
live in such a transitional era is unknown. Intuitively, even if the
universe is spatially infinite, most of it recedes too quickly from any
one observer to be harnessed as computer memory.

One response to this result is to assume an idealization in which @xmath
vanishes, although Planck’s constant @xmath does not vanish.  As
justification, one could argue that without the idealization @xmath ,
all asymptotic bounds in computer science are basically fictions.  But
perhaps a better response is to accept the @xmath bound, and then ask
how close one can come to saturating it in different scenarios.
 Classically, the maximum number of bits that can be searched is, in a
crude model ⁶² ⁶² 62 Specifically, neglecting gravity and other forces
that could counteract the effect of @xmath . , actually proportional to
@xmath rather than @xmath .  The reason is that if a region had much
more than @xmath bits, then after @xmath Planck times—that is, about
@xmath years, or roughly the current age of the universe—most of the
region would have receded beyond one’s cosmological horizon.  What our
results suggest is that, using a quantum robot, one could come closer to
saturating the cosmological bound—since, for example, a @xmath -D region
of size @xmath can be searched in time @xmath .  How anyone could
prepare (say) a database of size much greater than @xmath remains
unclear, but if such a database existed, it could be searched!

#### 48 The Model

As discussed in Part I , much of what is known about the power of
quantum computing comes from the black-box or query model—in which one
counts only the number of queries to an oracle, not the number of
computational steps.  We will take this model as the starting point for
a formal definition of quantum robots.  Doing so will focus attention on
our main concern: how much harder is it to evaluate a function when its
inputs are spatially separated?  As it turns out, all of our algorithms
will be efficient as measured by the number of gates and auxiliary
qubits needed to implement them.

For simplicity, we assume that a robot’s goal is to evaluate a Boolean
function @xmath , which could be partial or total.  A ‘region of space’
is a connected undirected graph @xmath with vertices @xmath .  Let
@xmath be an input to @xmath ; then each bit @xmath is available only at
vertex @xmath .  We assume the robot knows @xmath and the vertex labels
in advance, and so is ignorant only of the @xmath bits.  We thus
sidestep a major difficulty for quantum walks [ 19 ] , which is how to
ensure that a process on an unknown graph is unitary.

At any time, the robot’s state has the form

  -- -------- --
     @xmath   
  -- -------- --

Here @xmath is a vertex, representing the robot’s location; and @xmath
is a bit string (which can be arbitrarily long), representing the
robot’s internal configuration.  The state evolves via an alternating
sequence of @xmath algorithm steps and @xmath oracle steps:

  -- -------- --
     @xmath   
  -- -------- --

An oracle step @xmath maps each basis state @xmath to @xmath , where
@xmath is exclusive-OR’ed into the first bit of @xmath .  An algorithm
step @xmath can be any unitary matrix that (1) does not depend on @xmath
, and (2) acts ‘locally’ on @xmath .  How to make the second condition
precise is the subject of Section 48.1 .

The initial state of the algorithm is @xmath .  Let @xmath be the
amplitude of @xmath immediately after the @xmath oracle step; then the
algorithm succeeds with probability @xmath if

  -- -------- --
     @xmath   
  -- -------- --

for all inputs @xmath , where @xmath is a bit of @xmath representing the
output.

##### 48.1 Locality Criteria

Classically, it is easy to decide whether a stochastic matrix acts
locally with respect to a graph @xmath : it does if it moves probability
only along the edges of @xmath .  In the quantum case, however,
interference makes the question much more subtle.  In this section we
propose three criteria for whether a unitary matrix @xmath is local.
 Our algorithms can be implemented using the most restrictive of these
criteria, whereas our lower bounds apply to all three of them.

The first criterion we call Z-locality (for zero): @xmath is Z-local if,
given any pair of non-neighboring vertices @xmath in @xmath , @xmath
“sends no amplitude” from @xmath to @xmath ; that is, the corresponding
entries in @xmath are all @xmath .  The second criterion, C-locality
(for composability), says that this is not enough: not only must @xmath
send amplitude only between neighboring vertices, but it must be
composed of a product of commuting unitaries, each of which acts on a
single edge.  The third criterion is perhaps the most natural one to a
physicist: @xmath is H-local (for Hamiltonian) if it can be obtained by
applying a locally-acting, low-energy Hamiltonian for some fixed amount
of time.  More formally, let @xmath be the entry in the @xmath column
and @xmath row of @xmath .

###### Definition 101

@xmath is Z-local if @xmath whenever @xmath and @xmath is not an edge of
@xmath .

###### Definition 102

@xmath is C-local if the basis states can be partitioned into subsets
@xmath such that

1.  @xmath whenever @xmath and @xmath belong to distinct @xmath ’s, and

2.   for each @xmath , all basis states in @xmath are either from the
    same vertex or from two adjacent vertices.

###### Definition 103

@xmath is H-local if @xmath for some Hermitian @xmath with eigenvalues
of absolute value at most @xmath , such that @xmath whenever @xmath and
@xmath is not an edge in @xmath .

If a unitary matrix is C-local, then it is also Z-local and H-local.
 For the latter implication, note that any unitary @xmath can be written
as @xmath for some @xmath with eigenvalues of absolute value at most
@xmath .  So we can write the unitary @xmath acting on each @xmath as
@xmath ; then since the @xmath ’s commute,

  -- -------- --
     @xmath   
  -- -------- --

Beyond that, though, how are the locality criteria related?  Are they
approximately equivalent?  If not, then does a problem’s complexity in
our model ever depend on which criterion is chosen?  Let us emphasize
that these questions are not answered by, for example, the
Solovay-Kitaev theorem (see [ 182 ] ), that an @xmath unitary matrix can
be approximated using a number of gates polynomial in @xmath .  For
recall that the definition of C-locality requires the edgewise
operations to commute—indeed, without that requirement, one could
produce any unitary matrix at all.  So the relevant question, which we
leave open, is whether any Z-local or H-local unitary can be
approximated by a product of, say, @xmath C-local unitaries.  (A product
of @xmath such unitaries trivially suffices, but that is far too many.)
 Again, the algorithms in this chapter will use C-local unitaries,
whereas the lower bounds will apply even to Z-local and H-local
unitaries.

#### 49 General Bounds

Given a Boolean function @xmath , the quantum query complexity @xmath is
the minimum @xmath for which there exists a @xmath -query quantum
algorithm that evaluates @xmath with probability at least @xmath on all
inputs.  (We will always be interested in the two-sided, bounded-error
complexity, denoted @xmath elsewhere in this thesis.)  Similarly, given
a graph @xmath with @xmath vertices labeled @xmath , we let @xmath be
the minimum @xmath for which there exists a @xmath -query quantum robot
on @xmath that evaluates @xmath with probability @xmath .  Here the
algorithm steps must be C-local; we use @xmath and @xmath to denote the
corresponding measure with Z-local and H-local steps respectively.
 Clearly @xmath and @xmath ; we do not know whether all three measures
are asymptotically equivalent.

Let @xmath be the diameter of @xmath , and call @xmath nondegenerate if
it depends on all @xmath input bits.

###### Proposition 104

For all @xmath ,

1.  @xmath .

2.  @xmath .

3.  @xmath .

4.  @xmath if @xmath is nondegenerate.

Proof.

1.  Starting from the root, a spanning tree for @xmath can be traversed
    in @xmath steps (there is no need to return to the root).

2.  We can simulate a query in @xmath steps, by fanning out from the
    start vertex @xmath and then returning.  Applying a unitary at
    @xmath takes @xmath step.

3.  Obvious.

4.  There exists a vertex @xmath whose distance to @xmath is at least
    @xmath , and @xmath could depend on @xmath .

We now show that the model is robust.

###### Proposition 105

For nondegenerate @xmath , the following change @xmath by at most a
constant factor.

1.   Replacing the initial state @xmath by an arbitrary (known) @xmath .

2.   Requiring the final state to be localized at some vertex @xmath
    with probability at least @xmath , for a constant @xmath .

3.   Allowing multiple algorithm steps between each oracle step (and
    measuring the complexity by the number of algorithm steps).

Proof.

1.  We can transform @xmath to @xmath (and hence @xmath to @xmath ) in
    @xmath steps, by fanning out from @xmath along the edges of a
    minimum-height spanning tree.

2.  Assume without loss of generality that @xmath is accessed only once,
    to write the output.  Then after @xmath is accessed, uncompute (that
    is, run the algorithm backwards) to localize the final state at
    @xmath .  The state can then be localized at any @xmath in @xmath
    steps.  We can succeed with any constant probability by repeating
    this procedure a constant number of times.

3.  The oracle step @xmath is its own inverse, so we can implement a
    sequence @xmath of algorithm steps as follows (where @xmath is the
    identity):

      -- -------- --
         @xmath   
      -- -------- --

A function of particular interest is @xmath , which outputs @xmath if
and only if @xmath for some @xmath .  We first give a general upper
bound on @xmath in terms of the diameter of @xmath .  (Throughout the
chapter, we sometimes omit floor and ceiling signs if they clearly have
no effect on the asymptotics.)

###### Proposition 106

  -- -------- --
     @xmath   
  -- -------- --

Proof. Let @xmath be a minimum-height spanning tree for @xmath , rooted
at @xmath .  A depth-first search on @xmath uses @xmath steps.  Let
@xmath be the set of vertices visited by depth-first search in steps
@xmath to @xmath , @xmath be those visited in steps @xmath to @xmath ,
and so on.  Then

  -- -------- --
     @xmath   
  -- -------- --

Furthermore, for each @xmath there is a classical algorithm @xmath ,
using at most @xmath steps, that starts at @xmath , ends at @xmath , and
outputs ‘ @xmath ’ if and only if @xmath for some @xmath .  Then we
simply perform Grover search at @xmath over all @xmath ; since each
iteration takes @xmath steps and there are @xmath iterations, the number
of steps is @xmath .

The bound of Proposition 106 is tight:

###### Theorem 107

For all @xmath , there exists a graph @xmath with diameter @xmath such
that

  -- -------- --
     @xmath   
  -- -------- --

Indeed, @xmath and @xmath are also @xmath .

Proof. For simplicity, we first consider the C-local and Z-local cases,
and then discuss what changes in the H-local case.  Let @xmath be a
‘starfish’ with central vertex @xmath and @xmath legs @xmath , each of
length @xmath (see Figure 10 ).

We use the hybrid argument of Bennett et al. [ 51 ] .  Suppose we run
the algorithm on the all-zero input @xmath .  Then define the query
magnitude @xmath to be the probability of finding the robot in leg
@xmath immediately after the @xmath query:

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath be the total number of queries, and let @xmath for some
constant @xmath .  Clearly

  -- -------- --
     @xmath   
  -- -------- --

Hence there must exist a leg @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath be the tip vertex of @xmath , and let @xmath be the input
which is @xmath at @xmath and @xmath elsewhere.  Then let @xmath be a
hybrid input, which is @xmath during queries @xmath to @xmath , but
@xmath during queries @xmath to @xmath .  Also, let

  -- -------- --
     @xmath   
  -- -------- --

be the algorithm’s state after @xmath queries when run on @xmath , and
let

  -- -------- -------- --
     @xmath            
              @xmath   
  -- -------- -------- --

Then for all @xmath , we claim that @xmath .  For by unitarity, the
Euclidean distance between @xmath and @xmath can only increase as a
result of queries @xmath through @xmath .  But no amplitude from outside
@xmath can reach @xmath during that interval, since the distance is
@xmath and there are only @xmath time steps.  Therefore, switching from
@xmath to @xmath can only affect amplitude that is in @xmath immediately
after query @xmath :

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

It follows that

  -- -------- --
     @xmath   
  -- -------- --

Here the first inequality uses the triangle inequality, and the third
uses the Cauchy-Schwarz inequality.  Now assuming the algorithm is
correct we need @xmath , which implies that @xmath .

In the H-local case, it is no longer true that no amplitude from outside
@xmath can reach @xmath in @xmath time steps.  But if @xmath is a small
enough constant, then the amount of amplitude that can reach @xmath
decreases exponentially in @xmath .  To see this, assume without loss of
generality that all amplitude not in @xmath starts in the state @xmath ,
where @xmath is some superposition over auxiliary qubits.  Let @xmath be
the local Hamiltonian that acts between the @xmath and @xmath
queries, all of whose eigenvalues have absolute value at most @xmath .
 Since @xmath is Hermitian, we can decompose it as @xmath where @xmath
is unitary and @xmath is diagonal.  So by Taylor series expansion,

  -- -------- --
     @xmath   
  -- -------- --

Now let @xmath be the set of basis states @xmath such that the distance
from @xmath to @xmath is @xmath , for some @xmath .  Notice that for all
@xmath and @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

by the locality of @xmath .  Therefore

  -- -------- -------- --
     @xmath   @xmath   
                       
              @xmath   
              @xmath   
  -- -------- -------- --

Here the second line uses the triangle inequality, the third line uses
the fact that @xmath has maximum eigenvalue at most @xmath (and
therefore @xmath has maximum eigenvalue at most @xmath ), and the fourth
line uses the fact that @xmath .  Intuitively, the probability that
@xmath sends the robot a distance @xmath from @xmath is at most @xmath ,
which decreases exponentially in @xmath .  One can now use a
Chernoff-Hoeffding bound to upper-bound the probability that @xmath
local Hamiltonians, applied in succession, ever move the robot a
distance @xmath from @xmath .  It is clear that the resulting upper
bound is @xmath for small enough @xmath .  Therefore

  -- -------- --
     @xmath   
  -- -------- --

and the remainder of the proof goes through as before.

#### 50 Search on Grids

Let @xmath be a @xmath -dimensional grid graph of size @xmath .  That
is, each vertex is specified by @xmath coordinates @xmath , and is
connected to the at most @xmath vertices obtainable by adding or
subtracting @xmath from a single coordinate (boundary vertices have
fewer than @xmath neighbors).  We write simply @xmath when @xmath is
clear from context.  In this section we present our main positive
results: that @xmath for @xmath , and @xmath for @xmath .

Before proving these claims, let us develop some intuition by showing
weaker bounds, taking the case @xmath for illustration.  Clearly @xmath
: we simply partition @xmath into @xmath subsquares, each a copy of
@xmath .  In @xmath steps, the robot can travel from the start vertex to
any subsquare @xmath , search @xmath classically for a marked vertex,
and then return to the start vertex.  Thus, by searching all @xmath of
the @xmath ’s in superposition and applying Grover’s algorithm, the
robot can search the grid in time @xmath .

Once we know that, we might as well partition @xmath into @xmath
subsquares, each a copy of @xmath .  Searching any one of these
subsquares by the previous algorithm takes time @xmath , an amount of
time that also suffices to travel to the subsquare and back from the
start vertex.  So using Grover’s algorithm, the robot can search @xmath
in time @xmath .  We can continue recursively in this manner to make the
running time approach @xmath .  The trouble is that, with each
additional layer of recursion, the robot needs to repeat the search more
often to upper-bound the error probability.  Using this approach, the
best bounds we could obtain are roughly @xmath for @xmath , or @xmath
for @xmath .  In what follows, we use the amplitude amplification
approach of Brassard et al. [ 67 ] to improve these bounds, in the case
of a single marked vertex, to @xmath for @xmath (Section 50.2 ) and
@xmath for @xmath (Section 50.3 ).  Section 50.4 generalizes these
results to the case of multiple marked vertices.

Intuitively, the reason the case @xmath is special is that there, the
diameter of the grid is @xmath , which matches exactly the time needed
for Grover search.  For @xmath , by contrast, the robot can travel
across the grid in much less time than is needed to search it.

##### 50.1 Amplitude Amplification

We start by describing amplitude amplification [ 67 ] , a generalization
of Grover search.  Let @xmath be a quantum algorithm that, with
probability @xmath , outputs a correct answer together with a witness
that proves the answer correct. (For example, in the case of search, the
algorithm outputs a vertex label @xmath such that @xmath .)
 Amplification generates a new algorithm that calls @xmath order @xmath
times, and that produces both a correct answer and a witness with
probability @xmath .  In particular, assume @xmath starts in basis state
@xmath , and let @xmath be a positive integer.  Then the amplification
procedure works as follows:

1.  Set @xmath .

2.  For @xmath to @xmath set @xmath , where

    -   @xmath flips the phase of basis state @xmath if and only if
        @xmath contains a description of a correct witness, and

    -   @xmath flips the phase of basis state @xmath if and only if
        @xmath .

We can decompose @xmath as @xmath , where @xmath is a superposition over
basis states containing a correct witness and @xmath is a superposition
over all other basis states.  Brassard et al. [ 67 ] showed the
following:

###### Lemma 108 ([67])

@xmath .

If measuring @xmath gives a correct witness with probability @xmath ,
then @xmath and @xmath .  So taking @xmath yields @xmath .  For our
algorithms, though, the multiplicative constant under the big-O also
matters.  To upper-bound this constant, we prove the following lemma.

###### Lemma 109

Suppose a quantum algorithm @xmath outputs a correct answer and witness
with probability exactly @xmath .  Then by using @xmath calls to @xmath
or @xmath , where

  -- -------- --
     @xmath   
  -- -------- --

we can output a correct answer and witness with probability at least

  -- -------- --
     @xmath   
  -- -------- --

Proof. We perform @xmath steps of amplitude amplification, which
requires @xmath calls @xmath or @xmath .  By Lemma 108 , this yields the
final state

  -- -------- --
     @xmath   
  -- -------- --

where @xmath .  Therefore the success probability is

  -- -------- -------- --
     @xmath   @xmath   
                       
              @xmath   
  -- -------- -------- --

Here the first line uses the monotonicity of @xmath in the interval
@xmath , and the second line uses the fact that @xmath for all @xmath by
Taylor series expansion.

Note that there is no need to uncompute any garbage left by @xmath ,
beyond the uncomputation that happens “automatically” within the
amplification procedure.

##### 50.2 Dimension At Least 3

Our goal is the following:

###### Theorem 110

If @xmath , then @xmath .

In this section, we prove Theorem 110 for the special case of a unique
marked vertex; then, in Sections 50.4 and 50.5 , we will generalize to
multiple marked vertices.  Let @xmath be the problem of deciding whether
there are no marked vertices or exactly @xmath of them, given that one
of these is true.  Then:

###### Theorem 111

If @xmath , then @xmath .

Choose constants @xmath and @xmath such that @xmath (for example, @xmath
and @xmath will work).  Let @xmath be a large positive integer; then for
all positive integers @xmath , let @xmath .  Also let @xmath . Assume
for simplicity that @xmath for some @xmath ; in other words, that the
hypercube @xmath to be searched has sides of length @xmath .  Later we
will remove this assumption.

Consider the following recursive algorithm @xmath .  If @xmath , then
search @xmath classically, returning @xmath if a marked vertex is found
and @xmath otherwise.  Otherwise partition @xmath into @xmath subcubes,
each one a copy of @xmath .  Take the algorithm that consists of picking
a subcube @xmath uniformly at random, and then running @xmath
recursively on @xmath .  Amplify this algorithm @xmath times.

The intuition behind the exponents is that @xmath , so searching @xmath
should take about @xmath steps, which dominates the @xmath steps needed
to travel across the hypercube when @xmath .  Also, at level @xmath we
want to amplify a number of times that is less than @xmath by some
polynomial amount, since full amplification would be inefficient.  The
reason for the constraint @xmath will appear in the analysis.

We now provide a more explicit description of @xmath , which shows that
@xmath can be implemented using C-local unitaries and only a single bit
of workspace.  At any time, the quantum robot’s state will have the form
@xmath , where @xmath is a vertex of @xmath and @xmath is a single bit
that records whether or not a marked vertex has been found.  Given a
subcube @xmath , let @xmath be the “corner” vertex of @xmath ; that is,
the vertex that is minimal in all @xmath coordinates.  Then the initial
state when searching @xmath will be @xmath .  Beware, however, that
“initial state” in this context just means the state @xmath from Section
50.1 .  Because of the way amplitude amplification works, @xmath will
often be invoked on @xmath with other initial states, and even run in
reverse.

Below we give pseudocode for @xmath .  Our procedure calls the three
unitaries @xmath , @xmath , and @xmath from Section 50.1 as subroutines.
 For convenience, we write @xmath to denote the level of recursion that
is currently active.

###### Algorithm 112 (@xmath)

Searches a subcube @xmath of size @xmath for the marked vertex, and
amplifies the result to have larger probability.  Default initial state:
@xmath .

If @xmath then:

1.   Use classical C-local operations to visit all @xmath vertices of
    @xmath in any order.  At each @xmath , use a query transformation to
    map the state @xmath to @xmath .

2.   Return to @xmath .

If @xmath then:

1.   Let @xmath be the smallest integer such that @xmath .

2.   Call @xmath .

3.   For @xmath to @xmath , call @xmath , then @xmath , then @xmath ,
    then @xmath .

Suppose @xmath is run on the initial state @xmath , and let @xmath be
the minimal subcubes in @xmath —meaning those of size @xmath .  Then the
final state after @xmath terminates should be

  -- -------- --
     @xmath   
  -- -------- --

if @xmath does not contain the marked vertex.  Otherwise the final state
should have non-negligible overlap with @xmath , where @xmath is the
minimal subcube in @xmath that contains the marked vertex.  In
particular, if @xmath , then the final state should be @xmath if @xmath
contains the marked vertex, and @xmath otherwise.

The two phase-flip subroutines, @xmath and @xmath , are both trivial to
implement.  To apply @xmath , map each basis state @xmath to @xmath .
 To apply @xmath , map each basis state @xmath to @xmath if @xmath for
some subcube @xmath of size @xmath , and to @xmath otherwise.  Below we
give pseudocode for @xmath .

###### Algorithm 113 (@xmath)

Searches a subcube @xmath of size @xmath for the marked vertex.  Default
initial state: @xmath .

1.   Partition @xmath into @xmath smaller subcubes @xmath , each of size
    @xmath .

2.   For all @xmath , let @xmath be the set of corner vertices @xmath
    that differ from @xmath only in the first @xmath coordinates.  Thus
    @xmath , and in general @xmath .  For @xmath to @xmath , let @xmath
    be the state

      -- -------- --
         @xmath   
      -- -------- --

    Apply a sequence of transformations @xmath , @xmath , @xmath ,
    @xmath where @xmath is a unitary that maps @xmath to @xmath by
    applying C-local unitaries that move amplitude only along the @xmath
    coordinate.

3.   Call @xmath recursively, to search @xmath in superposition and
    amplify the results.

If @xmath is run on the initial state @xmath , then the final state
should be

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the correct final state when @xmath is run on subcube
@xmath with initial state @xmath .  A key point is that there is no need
for @xmath to call @xmath twice, once to compute and once to
uncompute—for the uncomputation is already built in to @xmath .  This is
what will enable us to prove an upper bound of @xmath instead of @xmath
.

We now analyze the running time of @xmath .

###### Lemma 114

@xmath uses @xmath steps.

Proof. Let @xmath and @xmath be the total numbers of steps used by
@xmath and @xmath respectively in searching @xmath .  Then we have
@xmath , and

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

for all @xmath .  For @xmath and @xmath can both be implemented in a
single step, while @xmath uses @xmath steps to move the robot across the
hypercube.  Combining,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Here the second line follows because @xmath , the fourth because the
@xmath terms increase doubly exponentially, so adding @xmath to each
will not affect the asymptotics; the seventh because @xmath , the eighth
because @xmath ; and the last because @xmath , hence @xmath .

Next we need to lower-bound the success probability.  Say that @xmath or
@xmath “succeeds” if a measurement in the standard basis yields the
result @xmath , where @xmath is the minimal subcube that contains the
marked vertex.  Of course, the marked vertex itself can then be found in
@xmath steps.

###### Lemma 115

Assuming there is a unique marked vertex, @xmath succeeds with
probability @xmath .

Proof. Let @xmath and @xmath be the success probabilities of @xmath and
@xmath respectively when searching @xmath .  Then clearly @xmath , and
@xmath for all @xmath .  So by Lemma 109 ,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Here the third line follows because @xmath and the function @xmath is
nondecreasing in the interval @xmath ; the fourth because @xmath ; the
sixth because @xmath ; and the last because @xmath and @xmath , the
@xmath ’s increase doubly exponentially, and @xmath is sufficiently
large.

Finally, take @xmath itself and amplify it to success probability @xmath
by running it @xmath times.  This yields an algorithm for searching
@xmath with overall running time @xmath , which implies that @xmath .

All that remains is to handle values of @xmath that do not equal @xmath
for any @xmath .  The solution is simple: first find the largest @xmath
such that @xmath .  Then set @xmath , and embed @xmath into the larger
hypercube @xmath .  Clearly @xmath .  Also notice that @xmath and that
@xmath .  Next partition @xmath into @xmath subcubes, each a copy of
@xmath .  The algorithm will now have one additional level of recursion,
which chooses a subcube of @xmath uniformly at random, runs @xmath on
that subcube, and then amplifies the resulting procedure @xmath times.
 The total time is now

  -- -------- --
     @xmath   
  -- -------- --

while the success probability is @xmath .  This completes Theorem 111 .

##### 50.3 Dimension 2

In the @xmath case, the best we can achieve is the following:

###### Theorem 116

@xmath .

Again, we start with the single marked vertex case and postpone the
general case to Sections 50.4 and 50.5 .

###### Theorem 117

@xmath .

For @xmath , we performed amplification on large (greater than @xmath )
probabilities only once, at the end.  For @xmath , on the other hand,
any algorithm that we construct with any nonzero success probability
will have running time @xmath , simply because that is the diameter of
the grid.  If we want to keep the running time @xmath , then we can only
perform @xmath amplification steps at the end.  Therefore we need to
keep the success probability relatively high throughout the
recursion, meaning that we suffer an increase in the running time, since
amplification to high probabilities is less efficient.

The procedures @xmath , @xmath , @xmath , and @xmath are identical to
those in Section 50.2 ; all that changes are the parameter settings.
 For all integers @xmath , we now let @xmath , for some odd integer
@xmath to be set later.  Thus, @xmath and @xmath search the square grid
@xmath of size @xmath .  Also, let @xmath ; then @xmath applies @xmath
steps of amplitude amplification to @xmath .

We now prove the counterparts of Lemmas 114 and 115 for the
two-dimensional case.

###### Lemma 118

@xmath uses @xmath steps.

Proof. Let @xmath and @xmath be the time used by @xmath and @xmath
respectively in searching @xmath .  Then @xmath , and for all @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Combining,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

###### Lemma 119

@xmath succeeds with probability @xmath .

Proof. Let @xmath and @xmath be the success probabilities of @xmath and
@xmath respectively when searching @xmath .  Then @xmath for all @xmath
.  So by Lemma 109 , and using the fact that @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

This is because @xmath iterations of the map @xmath are needed to drop
from (say) @xmath to @xmath , and @xmath is greater than @xmath .

We can amplify @xmath to success probability @xmath by repeating it
@xmath times.  This yields an algorithm for searching @xmath that uses
@xmath steps in total.  We can minimize this expression subject to
@xmath by taking @xmath to be constant and @xmath to be @xmath , which
yields @xmath .  If @xmath is not of the form @xmath , then we simply
find the smallest integer @xmath such that @xmath , and embed @xmath in
the larger grid @xmath .  Since @xmath is a constant, this increases the
running time by at most a constant factor.  We have now proved Theorem
117 .

##### 50.4 Multiple Marked Items

What about the case in which there are multiple @xmath ’s with @xmath ?
 If there are @xmath marked items (where @xmath need not be known in
advance), then Grover’s algorithm can find a marked item with high
probability in @xmath queries, as shown by Boyer et al. [ 66 ] .  In our
setting, however, this is too much to hope for—since even if there are
many marked vertices, they might all be in a faraway part of the
hypercube.  Then @xmath steps are needed, even if @xmath .  Indeed, we
can show a stronger lower bound.  Recall that @xmath is the problem of
deciding whether there are no marked vertices or exactly @xmath of them.

###### Theorem 120

For all constants @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Proof. For simplicity, we assume that both @xmath and @xmath are
integers.  (In the general case, we can just replace @xmath by @xmath
and @xmath by the largest number of the form @xmath which is less than
@xmath .  This only changes the lower bound by a lower order term.)

We use a hybrid argument almost identical to that of Theorem 107 .
 Divide @xmath into @xmath subcubes, each having @xmath vertices and
side length @xmath .  Let @xmath be a regularly-spaced set of @xmath of
these subcubes, so that any two subcubes in @xmath have distance at
least @xmath from one another.  Then choose a subcube @xmath uniformly
at random and mark all @xmath vertices in @xmath .  This enables us to
consider each @xmath itself as a single vertex (out of @xmath in total),
having distance at least @xmath to every other vertex.

More formally, given a subcube @xmath , let @xmath be the set of
vertices consisting of @xmath and the @xmath subcubes surrounding it.
 (Thus, @xmath is a subcube of side length @xmath .)  Then the query
magnitude of @xmath after the @xmath query is

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the all-zero input.  Let @xmath be the number of
queries, and let @xmath for some constant @xmath .  Then as in Theorem
107 , there must exist a subcube @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath be the input which is @xmath in @xmath and @xmath elsewhere;
then let @xmath be a hybrid input which is @xmath during queries @xmath
to @xmath , but @xmath during queries @xmath to @xmath .  Next let

  -- -------- --
     @xmath   
  -- -------- --

Then as in Theorem 107 , for all @xmath we have @xmath . For in the
@xmath queries from @xmath through @xmath , no amplitude originating
outside @xmath can travel a distance @xmath and thereby reach @xmath .
 Therefore switching from @xmath to @xmath can only affect amplitude
that is in @xmath immediately after query @xmath .  It follows that

  -- -------- --
     @xmath   
  -- -------- --

Hence @xmath for constant @xmath , since assuming the algorithm is
correct we need @xmath .

Notice that if @xmath , then the bound of Theorem 120 becomes @xmath
which is just the diameter of @xmath .  Also, if @xmath , then @xmath
and the bound is simply @xmath independent of @xmath .  The bound of
Theorem 120 can be achieved (up to a constant factor that depends on
@xmath ) for @xmath , and nearly achieved for @xmath .  We first
construct an algorithm for the case when @xmath is known.

###### Theorem 121

1.   For @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

2.   For @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

To prove Theorem 121 , we first divide @xmath into @xmath subcubes, each
of size @xmath (where @xmath will be fixed later).  Then in each
subcube, we choose one vertex uniformly at random.

###### Lemma 122

If @xmath , then the probability that exactly one marked vertex is
chosen is at least @xmath .

Proof. Let @xmath be a marked vertex.  The probability that @xmath is
chosen is @xmath .  Given that @xmath is chosen, the probability that
one of the other marked vertices, @xmath , is chosen is @xmath if @xmath
and @xmath belong to the same subcube, or @xmath if they belong to
different subcubes.  Therefore, the probability that @xmath alone is
chosen is at least

  -- -------- --
     @xmath   
  -- -------- --

Since the events “ @xmath alone is chosen” are mutually disjoint, we
conclude that the probability that exactly one marked vertex is chosen
is at least @xmath .

In particular, fix @xmath so that @xmath ; then Lemma 122 implies that
the probability of choosing exactly one marked vertex is at least @xmath
.  The algorithm is now as follows.  As in the lemma, subdivide @xmath
into @xmath subcubes and choose one location at random from each.  Then
run the algorithm for the unique-solution case (Theorem 111 or 117 ) on
the chosen locations only, as if they were vertices of @xmath .

The running time in the unique case was @xmath for @xmath or

  -- -------- --
     @xmath   
  -- -------- --

for @xmath .  However, each local unitary in the original algorithm now
becomes a unitary affecting two vertices @xmath and @xmath in
neighboring subcubes @xmath and @xmath .  When placed side by side,
@xmath and @xmath form a rectangular box of size @xmath .  Therefore the
distance between @xmath and @xmath is at most @xmath .  It follows that
each local unitary in the original algorithm takes @xmath time in the
new algorithm.  For @xmath , this results in an overall running time of

  -- -------- --
     @xmath   
  -- -------- --

For @xmath we obtain

  -- -------- --
     @xmath   
  -- -------- --

##### 50.5 Unknown Number of Marked Items

We now show how to deal with an unknown @xmath .  Let @xmath be the
problem of deciding whether there are no marked vertices or at least
@xmath of them, given that one of these is true.

###### Theorem 123

1.   For @xmath ,

      -- -- --
            
      -- -- --

2.   For @xmath ,

      -- -- --
            
      -- -- --

Proof. We use the straightforward ‘doubling’ approach of Boyer et al. [
66 ] :

1.  For @xmath to @xmath

    -   Run the algorithm of Theorem 121 with subcubes of size @xmath .

    -   If a marked vertex is found, then output @xmath and halt.

2.  Query a random vertex @xmath , and output @xmath if @xmath is a
    marked vertex and @xmath otherwise.

Let @xmath be the number of marked vertices. @xmath If @xmath , then
there exists a @xmath such that @xmath .  So Lemma 122 implies that the
@xmath iteration of step (1) finds a marked vertex with probability at
least @xmath .  On the other hand, if @xmath , then step (2) finds a
marked vertex with probability at least @xmath .  For @xmath , the time
used in step (1) is at most

  -- -------- --
     @xmath   
  -- -------- --

the sum in brackets being a decreasing geometric series.  For @xmath ,
the time is @xmath , since each iteration takes @xmath time and there
are at most @xmath iterations.  In neither case does step (2) affect the
bound, since @xmath implies that @xmath .

Taking @xmath gives algorithms for unconstrained @xmath with running
times @xmath for @xmath and @xmath for @xmath , thereby establishing
Theorems 110 and 116 .

#### 51 Search on Irregular Graphs

In Section 46 , we claimed that our divide-and-conquer approach has the
advantage of being robust : it works not only for highly symmetric
graphs such as hypercubes, but for any graphs having comparable
expansion properties.  Let us now substantiate this claim.

Say a family of connected graphs @xmath is @xmath -dimensional if there
exists a @xmath such that for all @xmath and @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the set of vertices having distance at most @xmath from
@xmath in @xmath .  Intuitively, @xmath is @xmath -dimensional (for
@xmath an integer) if its expansion properties are at least as good as
those of the hypercube @xmath . ⁶³ ⁶³ 63 In general, it makes sense to
consider non-integer @xmath as well. It is immediate that the diameter
of @xmath is at most @xmath .  Note, though, that @xmath might not be an
expander graph in the usual sense, since we have not required that every
sufficiently small set of vertices has many neighbors.

Our goal is to show the following.

###### Theorem 124

If @xmath is @xmath -dimensional, then

1.   For a constant @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

2.   For @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

In proving part (i), the intuition is simple: we want to decompose
@xmath recursively into subgraphs (called clusters ), which will serve
the same role as subcubes did in the hypercube case.  The procedure is
as follows.  For some constant @xmath , first choose @xmath vertices
uniformly at random to be designated as @xmath - pegs .  Then form
@xmath -clusters by assigning each vertex in @xmath to its closest
@xmath -peg, as in a Voronoi diagram.  (Ties are broken randomly.)  Let
@xmath be the peg of cluster @xmath .  Next, split up any @xmath
-cluster @xmath with more than @xmath vertices into @xmath
arbitrarily-chosen @xmath -clusters, each with size at most @xmath and
with @xmath as its @xmath -peg.  Observe that

  -- -- --
        
  -- -- --

where @xmath .  Therefore, the splitting-up step can at most double the
number of clusters.

In the next iteration, set @xmath , for some constant @xmath .  Choose
@xmath vertices uniformly at random as @xmath -pegs.  Then form @xmath
-clusters by assigning each @xmath -cluster @xmath to the @xmath
-peg that is closest to the @xmath -peg @xmath .  Given a @xmath
-cluster @xmath , let @xmath be the number of @xmath -clusters in @xmath
.  Then as before, split up any @xmath with @xmath into @xmath
arbitrarily-chosen @xmath -clusters, each with size at most @xmath and
with @xmath as its @xmath -peg.  Continue recursively in this manner,
setting @xmath and choosing @xmath vertices as @xmath -pegs for each
@xmath .  Stop at the maximum @xmath such that @xmath .  For technical
convenience, set @xmath , and consider each vertex @xmath to be the
@xmath -peg of the @xmath -cluster @xmath .

At the end we have a tree of clusters, which can be searched recursively
just as in the hypercube case.  In more detail, basis states now have
the form @xmath , where @xmath is a vertex, @xmath is an answer bit, and
@xmath is the (label of the) cluster currently being searched.
 (Unfortunately, because multiple @xmath -clusters can have the same
peg, a single auxiliary qubit no longer suffices.)  Also, let @xmath be
the number of @xmath -clusters in @xmath -cluster @xmath ; then @xmath
where @xmath .  If @xmath , then place @xmath “dummy” @xmath -clusters
in @xmath , each of which has @xmath -peg @xmath .

The algorithm @xmath from Section 50.2 now does the following, when
invoked on the initial state @xmath , where @xmath is an @xmath
-cluster.  If @xmath , then @xmath uses a query transformation to
prepare the state @xmath if @xmath is the marked vertex and @xmath
otherwise.  If @xmath and @xmath is not a dummy cluster, then @xmath
performs @xmath steps of amplitude amplification on @xmath , where
@xmath is the largest integer such that @xmath . ⁶⁴ ⁶⁴ 64 In the
hypercube case, we performed fewer amplifications in order to lower the
running time from @xmath to @xmath .  Here, though, the splitting-up
step produces a @xmath factor anyway. If @xmath is a dummy cluster, then
@xmath does nothing for an appropriate number of steps, and then returns
that no marked item was found.

We now describe the subroutine @xmath , for @xmath .  When invoked with
@xmath as its initial state, @xmath first prepares a uniform
superposition

  -- -------- --
     @xmath   
  -- -------- --

It then calls @xmath recursively, to search @xmath in superposition and
amplify the results.

For @xmath , define the radius of an @xmath -cluster @xmath to be the
maximum, over all @xmath -clusters @xmath in @xmath , of the distance
from @xmath to @xmath .  Also, call an @xmath -cluster good if it has
radius at most @xmath , where @xmath .

###### Lemma 125

With probability @xmath over the choice of clusters, all clusters are
good.

Proof. Let @xmath be the @xmath -peg of an @xmath -cluster.  Then @xmath
, where @xmath is the ball of radius @xmath about @xmath .  So the
probability that @xmath has distance greater than @xmath to the nearest
@xmath -peg is at most

  -- -------- --
     @xmath   
  -- -------- --

Furthermore, the total number of pegs is easily seen to be @xmath .  It
follows by the union bound that every @xmath -peg for every @xmath has
distance at most @xmath to the nearest @xmath -peg, with probability
@xmath over the choice of clusters.

We now analyze the running time and success probability of @xmath .

###### Lemma 126

@xmath uses @xmath steps, assuming that all clusters are good.

Proof. Let @xmath and @xmath be the time used by @xmath and @xmath
respectively in searching an @xmath -cluster.  Then we have

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

with the base case @xmath .  Combining,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where the last line holds because @xmath and therefore @xmath .

###### Lemma 127

@xmath succeeds with probability @xmath in searching a graph of size
@xmath , assuming there is a unique marked vertex.

Proof. For all @xmath , let @xmath be the @xmath -cluster that contains
the marked vertex, and let @xmath and @xmath be the success
probabilities of @xmath and @xmath respectively when searching @xmath .
 Then for all @xmath , we have @xmath , and therefore

  -- -------- -------- --
     @xmath            
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Here the third line holds because @xmath , and the last line because
@xmath .

Finally, we repeat @xmath itself @xmath times, to achieve success
probability @xmath using @xmath steps in total.  Again, if @xmath is not
equal to @xmath for any @xmath , then we simply find the largest @xmath
such that @xmath , and then add one more level of recursion that
searches a random @xmath -cluster and amplifies the result @xmath times.
 The resulting algorithm uses @xmath steps, thereby establishing part
(i) of Theorem 124 for the case of a unique marked vertex.  The
generalization to multiple marked vertices is straightforward.

###### Corollary 128

If @xmath is @xmath -dimensional for a constant @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

Proof. Assume without loss of generality that @xmath , since otherwise a
marked item is trivially found in @xmath steps.  As in Theorem 123 , we
give an algorithm @xmath consisting of @xmath iterations.  In iteration
@xmath , choose @xmath vertices @xmath uniformly at random.  Then run
the algorithm for the unique marked vertex case, but instead of taking
all vertices in @xmath as @xmath -pegs, take only @xmath .  On the other
hand, still choose the @xmath -pegs, @xmath -pegs, and so on uniformly
at random from among all vertices in @xmath .  For all @xmath , the
number of @xmath -pegs should be @xmath .  In general, in iteration
@xmath of @xmath , choose @xmath vertices @xmath uniformly at random,
and then run the algorithm for a unique marked vertex as if @xmath were
the only vertices in the graph.

It is easy to see that, assuming there are @xmath or more marked
vertices, with probability @xmath there exists an iteration @xmath such
that exactly one of @xmath is marked.  Hence @xmath succeeds with
probability @xmath .  It remains only to upper-bound @xmath ’s running
time.

In iteration @xmath , notice that Lemma 125 goes through if we use
@xmath instead of @xmath .  That is, with probability @xmath over the
choice of clusters, every @xmath -cluster has radius at most @xmath .
 So letting @xmath be the running time of @xmath on an @xmath -cluster,
the recurrence in Lemma 126 becomes

  -- -------- --
     @xmath   
  -- -------- --

which is

  -- -------- --
     @xmath   
  -- -------- --

if @xmath .  As usual, the case where there is no @xmath such that
@xmath is trivially handled by adding one more level of recursion.  If
we factor in the @xmath repetitions of @xmath needed to boost
the success probability to @xmath , then the total running time of
iteration @xmath is

  -- -------- --
     @xmath   
  -- -------- --

Therefore @xmath ’s running time is

  -- -------- --
     @xmath   
  -- -------- --

For the @xmath case, the best upper bound we can show is @xmath .  This
is obtained by simply modifying @xmath to have a deeper recursion tree.
 Instead of taking @xmath for some @xmath , we take @xmath , so that the
total number of levels is @xmath .  Lemma 125 goes through without
modification, while the recurrence for the running time becomes

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Also, since the success probability decreases by at most a constant
factor at each level, we have that @xmath , and hence @xmath
amplification steps suffice to boost the success probability to @xmath .
 Handling multiple marked items adds an additional factor of @xmath ,
which is absorbed into @xmath .  This completes Theorem 124 .

##### 51.1 Bits Scattered on a Graph

In Section 47 , we discussed several ways to pack a given amount of
entropy into a spatial region of given dimensions.  However, we said
nothing about how the entropy is distributed within the region.  It
might be uniform, or concentrated on the boundary, or distributed in
some other way.  So we need to answer the following: suppose that in
some graph, @xmath out of the @xmath vertices might be marked, and we
know which @xmath those are.  Then how much time is needed to determine
whether any of the @xmath is marked?  If the graph is the hypercube
@xmath for @xmath or is @xmath -dimensional for @xmath , then the
results of the previous sections imply that @xmath steps suffice.
 However, we wish to use fewer steps, taking advantage of the fact that
@xmath might be much smaller than @xmath .  Formally, suppose we are
given a graph @xmath with @xmath vertices, of which @xmath are
potentially marked.  Let @xmath be the problem of deciding whether
@xmath has no marked vertices or at least @xmath of them, given that one
of these is the case.

###### Proposition 129

For all integer constants @xmath , there exists a @xmath -dimensional
graph @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

Proof. Let @xmath be the @xmath -dimensional hypercube @xmath .  Create
@xmath subcubes of potentially marked vertices, each having @xmath
vertices and side length @xmath .  Space these subcubes out in @xmath so
that the distance between any pair of them is @xmath .  Then choose a
subcube @xmath uniformly at random and mark all @xmath vertices in
@xmath .  This enables us to consider each subcube as a single vertex,
having distance @xmath to every other vertex.  The lower bound now
follows by a hybrid argument essentially identical to that of Theorem
120 .

In particular, if @xmath then @xmath time is always needed, since the
potentially marked vertices might all be far from the start vertex.  The
lower bound of Proposition 129 can be achieved up to a polylogarithmic
factor.

###### Proposition 130

If @xmath is @xmath -dimensional for a constant @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

Proof. Assume without loss of generality that @xmath , since otherwise a
marked item is trivially found.  Use algorithm @xmath from Corollary 128
, with the following simple change.  In iteration @xmath , choose @xmath
potentially marked vertices @xmath uniformly at random, and then run the
algorithm for a unique marked vertex as if @xmath were the only vertices
in the graph.  That is, take @xmath as @xmath -pegs; then for all @xmath
, choose @xmath vertices of @xmath uniformly at random as @xmath -pegs.
 Lemma 125 goes through if we use @xmath instead of @xmath .  So
following Corollary 128 , the running time of iteration @xmath is now

  -- -------- --
     @xmath   
  -- -------- --

if @xmath .  Therefore the total running time is

  -- -- --
        
  -- -- --

Intuitively, Proposition 130 says that the worst case for search occurs
when the @xmath potential marked vertices are scattered evenly
throughout the graph.

#### 52 Application to Disjointness

In this section we show how our results can be used to strengthen a
seemingly unrelated result in quantum computing.  Suppose Alice has a
string @xmath , and Bob has a string @xmath .  In the disjointness
problem , Alice and Bob must decide with high probability whether there
exists an @xmath such that @xmath , using as few bits of communication
as possible.  Buhrman, Cleve, and Wigderson [ 76 ] observed that in the
quantum setting, Alice and Bob can solve this problem using only @xmath
qubits of communication.  This was subsequently improved by Høyer and de
Wolf [ 146 ] to @xmath , where @xmath is a constant and @xmath is the
iterated logarithm function.  Using the search algorithm of Theorem 110
, we can improve this to @xmath , which matches the celebrated @xmath
lower bound of Razborov [ 199 ] .

###### Theorem 131

The bounded-error quantum communication complexity of the disjointness
problem is @xmath .

Proof. The protocol is as follows.  Alice and Bob both store their
inputs in a @xmath -D cube @xmath (Figure 11 ); that is, they let @xmath
and @xmath , where @xmath and @xmath .

Throughout, they maintain a joint state of the form

  -- -------- -- -----
     @xmath      (1)
  -- -------- -- -----

where @xmath is used for communication between the players, and @xmath
and @xmath store the answers to queries.  Thus, whenever Alice is at
location @xmath of her cube, Bob is at location @xmath of his cube.  To
decide whether there exists a @xmath with @xmath , Alice simply runs our
search algorithm for an unknown number of marked items, but with two
changes.  First, after each query, Alice inverts her phase if and only
if @xmath ; this requires @xmath qubits of communication from Bob, to
send @xmath to Alice and then to erase it.  Second, before each movement
step, Alice tells Bob in which of the six possible directions she is
going to move.  That way, Bob can synchronize his location with Alice’s,
and thereby maintain the state in the form ( 1 ).  This requires @xmath
qubits of communication from Alice, to send the direction to Bob and
then to erase it.  Notice that no further communication is necessary,
since there are no auxiliary registers in our algorithm that need to be
communicated.  Since the algorithm uses @xmath steps, the number of
qubits communicated in the disjointness protocol is therefore also
@xmath .

#### 53 Open Problems

As discussed in Section 48.1 , a salient open problem raised by this
work is to prove relationships among Z-local, C-local, and H-local
unitary matrices.  In particular, can any Z-local or H-local unitary be
approximated by a product of a small number of C-local unitaries?  Also,
is it true that @xmath for all @xmath ?

A second problem is to obtain interesting lower bounds in our model.
 For example, let @xmath be a @xmath grid, and suppose @xmath if and
only if every row of @xmath contains a vertex @xmath with @xmath .
 Clearly @xmath , and we conjecture that this is optimal.  However, we
were unable to show any lower bound better than @xmath .

Finally, what is the complexity of finding a unique marked vertex on a
2-D square grid?  As mentioned in Section 46 , Ambainis, Kempe, and
Rivosh [ 31 ] showed that @xmath .  Can the remaining factor of @xmath
be removed?

### Chapter \thechapter Quantum Computing and Postselection

  “Gill, in his seminal paper on probabilistic complexity classes,
  defined the class @xmath and asked whether the class was closed under
  intersection.  In 1990, Fenner and Kurtz and later myself, decided to
  try a new approach to the question: Consider a class defined like
  @xmath but with additional restrictions, show that this class is
  closed under intersection and then show the class was really the same
  as @xmath .”
  —Lance Fortnow, My Computational Complexity Web Log [ 113 ]
  (the approach didn’t succeed, though as this chapter will show, all it
  was missing was quantum mechanics)

Postselection is the power of discarding all runs of a computation in
which a given event does not occur.  Clearly, such an ability would let
us solve @xmath -complete problems in polynomial time, since we could
guess a random solution, and then postselect on its being correct.  But
would postselection let us do more than @xmath ?  Using a classical
computer, the class of problems we could efficiently solve coincides
with a class called @xmath , which was defined by Han, Hemaspaandra, and
Thierauf [ 142 ] and which sits somewhere between @xmath and @xmath .

This chapter studies the power of postselection when combined with
quantum computing.  In Section 54 , I define a new complexity class
called @xmath , which is similar to @xmath except that we can measure a
qubit that has some nonzero probability of being @xmath , and assume the
outcome will be @xmath .  The main result is that @xmath equals the
classical complexity class @xmath .

My original motivation, which I explain in Section 55 , was to analyze
the computational power of “fantasy” versions of quantum mechanics, and
thereby gain insight into why quantum mechanics is the way it is.  For
example, I show in Section 55 that if we changed the measurement
probability rule from @xmath to @xmath for some @xmath , or allowed
linear but nonunitary gates, then we could simulate postselection, and
hence solve @xmath -complete problems in polynomial time.  I was also
motivated by a concept that I call anthropic computing : arranging
things so that you’re more likely to exist if a computer produces a
desired output than if it doesn’t.  As a simple example, under the
many-worlds interpretation of quantum mechanics, you might kill yourself
in all universes where a computer’s output is incorrect.  My result
implies that, using this “technique,” the class of problems that you
could efficiently solve is exactly @xmath .

However, it recently dawned on me that the @xmath result is also
interesting for purely classical reasons.  In particular, it yields an
almost-trivial, quantum computing based proof that @xmath is closed
under intersection.  This proof does not use rational approximations,
threshold polynomials, or any of the other techniques pioneered by
Beigel, Reingold, and Spielman [ 47 ] in their justly-celebrated
original proof.  Another immediate corollary of my new characterization
of @xmath is a result originally due to Fortnow and Reingold [ 115 ] :
that @xmath is closed under polynomial-time truth-table reductions.
 Indeed, I can show that @xmath is closed under @xmath truth-table
reductions, which is a new result as far as I know.  I conclude in
Section 56 with some open problems.

#### 54 The Class @xmath

I hereby define a complexity class:

###### Definition 132

@xmath (Postselected Bounded-Error Quantum Polynomial-Time) is the class
of languages @xmath for which there exists a uniform family of
polynomial-size quantum circuits such that for all inputs @xmath ,

1.   At the end of the computation, the first qubit has a nonzero
    probability of being measured to be @xmath .

2.   If @xmath , then conditioned on the first qubit being @xmath , the
    second qubit is @xmath with probability at least @xmath .

3.   If @xmath , then conditioned on the first qubit being @xmath , the
    second qubit is @xmath with probability at most @xmath .

We can think of @xmath as the “nondeterministic” version of @xmath .
 Admittedly, there are already three other contenders for that title:
@xmath , defined by Watrous [ 239 ] ; @xmath , defined by Aharonov and
Naveh [ 21 ] ; and @xmath , defined by Adleman, DeMarrais, and Huang [
16 ] (which turns out to equal @xmath [ 107 ] ).  As we will see, @xmath
contains all of these as subclasses.

It is immediate that @xmath .  For the latter inclusion, we can use the
same techniques used by Adleman, DeMarrais, and Huang [ 16 ] to show
that @xmath , but sum only over paths where the first qubit is @xmath at
the end.  This is made explicit in Theorem 57 of Chapter I .

How robust is @xmath ?  Just as Bernstein and Vazirani [ 55 ] showed
that intermediate measurements don’t increase the power of ordinary
quantum computers, so it’s easy to show that intermediate postselection
steps don’t increase the power of @xmath .  Whenever we want to
postselect on a qubit being @xmath , we simply CNOT that qubit into a
fresh ancilla qubit that is initialized to @xmath and that will never be
written to again.  Then, at the end, we compute the AND of all the
ancilla qubits, and swap the result into the first qubit.  It follows
that we can repeat a @xmath computation a polynomial number of times,
and thereby amplify the probability gap from @xmath to @xmath for any
polynomial @xmath .

A corollary of the above observations is that @xmath has strong closure
properties.

###### Proposition 133

@xmath is closed under union, intersection, and complement.  Indeed, it
is closed under @xmath truth table reductions, meaning that @xmath
, where @xmath is the class of problems solvable by a @xmath
machine that can make a polynomial number of nonadaptive classical
queries to a @xmath oracle.

Proof. Clearly @xmath is closed under complement.  To show closure under
intersection, let @xmath .  Then to decide whether @xmath , run
amplified computations (with error probability at most @xmath ) to
decide if @xmath and if @xmath , postselect on both computations
succeeding, and accept if and only if both accept.  It follows that
@xmath is closed under union as well.

In general, suppose a @xmath machine @xmath submits queries @xmath to
the @xmath oracle.  Then run amplified computations (with error
probability at most, say, @xmath ) to decide the answers to these
queries, and postselect on all @xmath of them succeeding.  By the union
bound, if @xmath had error probability @xmath with a perfect @xmath
oracle, then its new error probability is at most @xmath , which can
easily be reduced through amplification.

One might wonder why Proposition 133 doesn’t go through with adaptive
queries.  The reason is subtle: suppose we have two @xmath computations,
the second of which relies on the output of the first.  Then even if the
first computation is amplified a polynomial number of times, it still
has an exponentially small probability of error.  But since the second
computation uses postselection, any nonzero error probability could be
magnified arbitrarily, and is therefore too large.

I now prove the main result.

###### Theorem 134

@xmath .

Proof. We have already observed that @xmath .  For the other direction,
let @xmath be an efficiently computable Boolean function and let @xmath
.  Then we need to decide in @xmath whether @xmath or @xmath .  (As a
technicality, we can guarantee using padding that @xmath .)

The algorithm is as follows: first prepare the state @xmath .  Then
following Abrams and Lloyd [ 15 ] , apply Hadamard gates to all @xmath
qubits in the first register and postselect ⁶⁵ ⁶⁵ 65 Postselection is
actually overkill here, since the first register has at least @xmath
probability of being @xmath . on that register being @xmath , to obtain
@xmath where

  -- -------- --
     @xmath   
  -- -------- --

Next, for some positive real numbers @xmath to be specified later,
prepare @xmath where

  -- -- --
        
  -- -- --

is the result of applying a Hadamard gate to @xmath .  Then postselect
on the second qubit being @xmath . This yields the reduced state

  -- -------- --
     @xmath   
  -- -------- --

in the first qubit.

Suppose @xmath , so that @xmath and @xmath are both at least @xmath .
 Then we claim there exists an integer @xmath such that, if we set
@xmath , then @xmath is close to the state @xmath :

  -- -------- --
     @xmath   
  -- -------- --

For since @xmath lies between @xmath and @xmath , there must be an
integer @xmath such that @xmath and @xmath fall on opposite sides of
@xmath in the first quadrant (see Figure 12 ).  So the worst case is
that @xmath , which occurs when @xmath and @xmath .  On the other hand,
suppose @xmath , so that @xmath .  Then @xmath never lies in the first
or third quadrants, and therefore @xmath .

It follows that, by repeating the whole algorithm @xmath times (as in
Proposition 133 ), with @xmath invocations for each integer @xmath , we
can learn whether @xmath or @xmath with exponentially small probability
of error.

Combining Proposition 133 with Theorem 134 immediately yields that
@xmath is closed under intersection, as well as under @xmath truth-table
reductions.

#### 55 Fantasy Quantum Mechanics

  “It is striking that it has so far not been possible to find a
  logically consistent theory that is close to quantum mechanics, other
  than quantum mechanics itself.”
  —Steven Weinberg, Dreams of a Final Theory [ 241 ]

Is quantum mechanics an island in theoryspace?  By “theoryspace,” I mean
the space of logically conceivable physical theories, with two theories
close to each other if they differ in few respects.  An “island” in
theoryspace is then a natural and interesting theory, whose neighbors
are all somehow perverse or degenerate.  The Standard Model is not an
island, because we do not know of any compelling (non-anthropic) reason
why the masses and coupling constants should have the values they do.
 Likewise, general relativity is probably not an island, because of
alternatives such as the Brans-Dicke theory.

To many physicists, however, quantum mechanics does seem like an island:
change any one aspect, and the whole theory becomes inconsistent or
nonsensical.  There are many mathematical results supporting this
opinion: for example, Gleason’s Theorem [ 127 ] and other
“derivations” of the @xmath probability rule [ 93 , 252 ] ; arguments
for why amplitudes have to be complex numbers, rather than real numbers
or quaternions [ 81 , 143 ] ; and “absurd” consequences of allowing
nonlinear transformations between states [ 15 , 126 , 191 ] .  The point
of these results is to provide some sort of explanation for why quantum
mechanics has the properties it does.

In 1998, Abrams and Lloyd [ 15 ] suggested that computational complexity
could also be pressed into such an explanatory role.  In particular,
they showed that under almost any nonlinear variant of quantum
mechanics, one could build a “nonlinear quantum computer” able to solve
@xmath -complete and even @xmath -complete problems in polynomial time.
⁶⁶ ⁶⁶ 66 A caveat is that it remains an open problem whether this can be
done fault-tolerantly.  The answer might depend on the allowed types of
nonlinear gate.  On the other hand, if arbitrary @xmath -qubit nonlinear
gates can be implemented without error, then even @xmath -complete
problems can be solved in polynomial time.  This is tight, since
nonlinear quantum computers can also be simulated in @xmath .  I will
give more details in a forthcoming survey paper [ 12 ] . One
interpretation of their result is that we should look very hard for
nonlinearities in experiments!  But a different interpretation, the one
I prefer, is that their result provides independent evidence that
quantum mechanics is linear. ⁶⁷ ⁶⁷ 67 Note that I would not advocate
this interpretation if it was merely (say) graph isomorphism that was
efficiently solvable in nonlinear quantum mechanics, just as I do not
take Shor’s factoring algorithm as evidence for the falsehood of
ordinary quantum mechanics.  I will explain in [ 12 ] why I think this
distinction, between @xmath -complete problems and specific @xmath
-intermediate problems, is a justified one.

In this section I build on Theorem 134 to offer similar “evidence” that
quantum mechanics is unitary, and that the measurement rule is @xmath .

Let @xmath be the class of problems solvable by a uniform family of
polynomial-size, bounded-error quantum circuits, where the circuits can
consist of arbitrary @xmath - and @xmath -qubit invertible linear
transformations, rather than just unitary transformations.  Immediately
before a measurement, the amplitude @xmath of each basis state @xmath is
divided by @xmath to normalize it.

###### Proposition 135

@xmath .

Proof. The inclusion @xmath follows easily from Adleman, DeMarrais, and
Huang’s proof that @xmath [ 16 ] , which does not depend on unitarity.
 For the other direction, by Theorem 134 it suffices to show that @xmath
.  To postselect on a qubit being @xmath , we simply apply the @xmath
-qubit nonunitary operation

  -- -------- --
     @xmath   
  -- -------- --

for some sufficiently large polynomial @xmath .

Next, for any nonnegative real number @xmath , define @xmath similarly
to @xmath , except that when we measure, the probability of obtaining a
basis state @xmath equals @xmath rather than @xmath .  Thus @xmath .
 Assume that all gates are unitary and that there are no intermediate
measurements, just a single standard-basis measurement at the end.

###### Theorem 136

@xmath for all constants @xmath , with @xmath when @xmath .

Proof. To simulate @xmath in @xmath , run the algorithm of Theorem 134
, having initialized @xmath ancilla qubits to @xmath .  Suppose the
algorithm’s state at some point is @xmath , and we want to postselect on
the event @xmath , where @xmath is a subset of basis states.  Here is
how: if @xmath , then for some sufficiently large polynomial @xmath ,
apply Hadamard gates to @xmath fresh ancilla qubits conditioned on
@xmath .  The result is to increase the “probability mass” of each
@xmath from @xmath to

  -- -------- --
     @xmath   
  -- -------- --

while the probability mass of each @xmath remains unchanged.  Similarly,
if @xmath , then apply Hadamard gates to @xmath fresh ancilla qubits
conditioned on @xmath .  This decreases the probability mass of each
@xmath from @xmath to @xmath , while the probability mass of each @xmath
remains unchanged.  The final observation is that Theorem 134 still goes
through if @xmath .  For it suffices to distinguish the case @xmath from
@xmath with exponentially small probability of error, using polynomially
many copies of the state @xmath .  But we can do this for any @xmath
, since all @xmath rules behave well under tensor products (in the sense
that @xmath ).

The inclusion @xmath follows easily from the techniques used by
Bernstein and Vazirani [ 55 ] to show @xmath .  Let @xmath be the set of
accepting states; then simply compute @xmath and @xmath and see which is
greater.

To simulate @xmath in @xmath when @xmath , we generalize the technique
of Adleman, DeMarrais, and Huang [ 16 ] , which handled the case @xmath
.  As in Theorem 57 in Chapter I , assume that all gates are Hadamard or
Toffoli gates; then we can write each amplitude @xmath as a sum of
exponentially many contributions, @xmath , where each @xmath is a
rational real number computable in classical polynomial time.  Then
letting @xmath be the set of accepting states, it suffices to test
whether

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

is greater than @xmath , which we can do in @xmath .

#### 56 Open Problems

The new proof that @xmath is closed under intersection came as a total
surprise to me.  But on reflection, it goes a long way toward convincing
me of a thesis expressed in Chapter Limits on Efficient Computation in
the Physical World : that quantum computing offers a new perspective
from which to revisit the central questions of classical complexity
theory.  What other classical complexity classes can we characterize in
quantum terms, and what other questions can we answer by that means?

A first step might be to prove even stronger closure properties for
@xmath .  Recall from Proposition 133 that @xmath is closed under
polynomial-time truth-table reductions.  Presumably this can’t be
generalized to closure under Turing reductions, since if it could then
we would have @xmath , which is considered unlikely. ⁶⁸ ⁶⁸ 68 Indeed,
Beigel [ 46 ] gave an oracle relative to which @xmath . But can we show
closure under nonadaptive quantum reductions?  More formally, let @xmath
be the class of problems solvable by a @xmath machine that can make a
single quantum query, which consists of a list of polynomially many
questions for a @xmath oracle.  Then does @xmath equal @xmath ?  The
difficulty in showing this seems to be uncomputing garbage after the
@xmath oracle is simulated.

As for fantasy quantum mechanics, an interesting open question is
whether @xmath for all nonnegative real numbers @xmath .  An obvious
idea for simulating @xmath in @xmath would be to use a Taylor series
expansion for the probability masses @xmath .  Unfortunately, I have no
idea how to get fast enough convergence.

### Chapter \thechapter The Power of History

Quantum mechanics lets us calculate the probability that (say) an
electron will be found in an excited state if measured at a particular
time.  But it is silent about multiple-time or transition probabilities:
that is, what is the probability that the electron will be in an excited
state at time @xmath , given that it was in its ground state at an
earlier time @xmath ?  The usual response is that this question is
meaningless, unless of course the electron was measured (or otherwise
known with probability @xmath ) to be in its ground state at @xmath .  A
different response—pursued by Schrödinger [ 213 ] , Bohm [ 59 ] , Bell [
49 ] , Nelson [ 181 ] , Dieks [ 97 ] , and others—treats the question as
provisionally meaningful, and then investigates how one might answer it
mathematically.  Specific attempts at answers are called
“hidden-variable theories.”

The appeal of hidden-variable theories is that they provide one possible
solution to the measurement problem.  For they allow us to apply unitary
quantum mechanics to the entire universe (including ourselves), yet
still discuss the probability of a future observation conditioned on our
current observations.  Furthermore, they let us do so without making any
assumptions about decoherence or the nature of observers.  For example,
even if an observer were placed in coherent superposition, that observer
would still have a sequence of definite experiences, and the probability
of any such sequence could be calculated.

This chapter initiates the study of hidden variables from a quantum
computing perspective.  I restrict attention to the simplest possible
setting: that of discrete time, a finite-dimensional Hilbert space, and
a fixed orthogonal basis.  Within this setting, I reformulate known
hidden-variable theories due to Dieks [ 97 ] and Schrödinger [ 213 ] ,
and also introduce a new theory based on network flows.  However, a more
important contribution is the axiomatic approach that I use.  I propose
five axioms for hidden-variable theories, and then compare theories
against each other based on which of the axioms they satisfy.  A central
question in this approach is which subsets of axioms can be satisfied
simultaneously.

In a second part of the chapter, I make the connection to quantum
computing explicit, by studying the computational complexity of
simulating hidden-variable theories.  Below I describe the computational
results.

#### 57 The Complexity of Sampling Histories

It is often stressed that hidden-variable theories yield exactly the
same predictions as ordinary quantum mechanics.  On the other hand,
these theories describe a different picture of physical reality, with an
additional layer of dynamics beyond that of a state vector evolving
unitarily.  I address a question that, to my knowledge, had never been
raised before: what is the computational complexity of simulating that
additional dynamics? In other words, if we could examine a hidden
variable’s entire history, then could we solve problems in polynomial
time that are intractable even for quantum computers?

I present strong evidence that the answer is yes.  The Graph Isomorphism
problem asks whether two graphs @xmath and @xmath are isomorphic; while
given a basis for a lattice @xmath , the Approximate Shortest Vector
problem asks for a nonzero vector in @xmath within a @xmath factor of
the shortest one.  I show that both problems are efficiently solvable by
sampling a hidden variable’s history, provided the hidden-variable
theory satisfies a reasonable axiom.  By contrast, despite a decade of
effort, neither problem is known to lie in @xmath .  Thus, if we let
@xmath (Dynamical Quantum Polynomial-Time) be the class of problems
solvable in the new model, then this already provides circumstantial
evidence that @xmath is strictly contained in @xmath .

However, the evidence is stronger than this.  For I actually show that
@xmath contains the entire class Statistical Zero Knowledge, or @xmath .
 Furthermore, Chapter I showed that relative to an oracle, @xmath is not
contained in @xmath .  Combining the result that @xmath with the oracle
separation of Chapter I , one obtains that @xmath relative to an oracle
as well.

Besides solving @xmath problems, I also show that by sampling histories,
one could search an unordered database of @xmath items for a single
“marked item” using only @xmath database queries.  By comparison,
Grover’s quantum search algorithm [ 139 ] requires @xmath queries, while
classical algorithms require @xmath queries.  On the other hand, I also
show that the @xmath upper bound is the best possible—so even in the
histories model, one cannot search an @xmath -item database in @xmath
steps for some fixed power @xmath .  This implies that @xmath relative
to an oracle, which in turn suggests that @xmath is still not powerful
enough to solve @xmath -complete problems in polynomial time.

At this point I should address a concern that many readers will have.
 Once we extend quantum mechanics by positing the “unphysical” ability
to sample histories, isn’t it completely unsurprising if we can then
solve problems that were previously intractable?  I believe the answer
is no, for three reasons.

First, almost every change that makes the quantum computing model more
powerful, seems to make it so much more powerful that @xmath
-complete and even harder problems become solvable efficiently.  To give
some examples, @xmath -complete problems can be solved in polynomial
time using a nonlinear Schrödinger equation, as shown by Abrams and
Lloyd [ 15 ] ; using closed timelike curves, as shown by Brun [ 72 ] and
Bacon [ 40 ] (and conjectured by Deutsch [ 91 ] ); or using a
measurement rule of the form @xmath for any @xmath , as shown in Chapter
II .  It is also easy to see that we could solve @xmath
-complete problems if, given a quantum state @xmath , we could request a
classical description of @xmath , such as a list of amplitudes or a
preparation procedure. ⁶⁹ ⁶⁹ 69 For as Abrams and Lloyd [ 15 ] observed,
we can so arrange things that @xmath if an @xmath -complete instance of
interest to us has no solution, but @xmath for some tiny @xmath if it
has a solution. By contrast, the @xmath model is the first independently
motivated model I know of that seems more powerful than quantum
computing, but only slightly so. ⁷⁰ ⁷⁰ 70 One can define other, less
motivated, models with the same property by allowing “non-collapsing
measurements” of quantum states, but these models are very closely
related to @xmath .  Indeed, a key ingredient in the results of this
chapter will be to show that certain kinds of non-collapsing
measurements can be simulated using histories. Moreover, the striking
fact that unordered search takes about @xmath steps in the @xmath model,
as compared to @xmath steps classically and @xmath quantum-mechanically,
suggests that @xmath somehow “continues a sequence” that begins with
@xmath and @xmath .  It would be interesting to find a model in which
search takes @xmath or @xmath steps.

The second reason the results are surprising is that, given a hidden
variable, the distribution over its possible values at any single time
is governed by standard quantum mechanics, and is therefore efficiently
samplable on a quantum computer.  So if examining the variable’s history
confers any extra computational power, then it can only be because of
correlations between the variable’s values at different times.

The third reason is the criterion for success.  I am not saying merely
that one can solve Graph Isomorphism under some hidden-variable theory;
or even that, under any theory satisfying the indifference axiom, there
exists an algorithm to solve it; but rather that there exists a single
algorithm that solves Graph Isomorphism under any theory satisfying
indifference.  Thus, we must consider even theories that are
specifically designed to thwart such an algorithm.

But what is the motivation for these results?  The first motivation is
that, within the community of physicists who study hidden-variable
theories such as Bohmian mechanics, there is great interest in actually
calculating the hidden-variable trajectories for specific physical
systems [ 190 , 140 ] .  My results show that, when many interacting
particles are involved, this task might be fundamentally intractable,
even if a quantum computer were available.  The second motivation is
that, in classical computer science, studying “unrealistic” models of
computation has often led to new insights into realistic ones; and
likewise I expect that the @xmath model could lead to new results
about standard quantum computation.  Indeed, in a sense this has already
happened—for the collision lower bound of Chapter I grew out of work on
the @xmath versus @xmath question.

#### 58 Outline of Chapter

Sections 59 through 62.2 develop the axiomatic approach to hidden
variables; then Sections 63 through 66 study the computational
complexity of sampling hidden-variable histories.

Section 59 formally defines hidden-variable theories in my sense; then
Section 59.1 contrasts these theories with related ideas such as Bohmian
mechanics and modal interpretations.  Section 59.2 addresses the most
common objections to my approach: for example, that the implicit
dependence on a fixed basis is unacceptable.

In Section 60 , I introduce five possible axioms for hidden-variable
theories.  These are indifference to the identity operation; robustness
to small perturbations; commutativity with respect to
spacelike-separated unitaries; commutativity for the special case of
product states; and invariance under decomposition of mixed states into
pure states.  Ideally, a theory would satisfy all of these axioms.
 However, I show in Section 61 that no theory satisfies both
indifference and commutativity; no theory satisfies both indifference
and a stronger version of robustness; no theory satisfies indifference,
robustness, and decomposition invariance; and no theory satisfies a
stronger version of decomposition invariance.

In Section 62 I shift from negative to positive results.  Section 62.1
presents a hidden-variable theory called the flow theory or @xmath ,
which is based on the Max-Flow-Min-Cut theorem from combinatorial
optimization.  The idea is to define a network of “pipes” from basis
states at an initial time to basis states at a final time, and then
route as much probability mass as possible through these pipes.  The
capacity of each pipe depends on the corresponding entry of the unitary
acting from the initial to final time.  To find the probability of
transitioning from basis state @xmath to basis state @xmath , we then
determine how much of the flow originating at @xmath is routed along the
pipe to @xmath .  The main results are that @xmath is well-defined and
that it is robust to small perturbations.  Since @xmath trivially
satisfies the indifference axiom, this implies that the indifference and
robustness axioms can be satisfied simultaneously, which was not at all
obvious a priori .

Section 62.2 presents a second theory that I call the Schrödinger theory
or @xmath , since it is based on a pair of integral equations introduced
in a 1931 paper of Schrödinger [ 213 ] .  Schrödinger conjectured, but
was unable to prove, the existence and uniqueness of a solution to these
equations; the problem was not settled until the work of Nagasawa [ 179
] in the 1980’s.  In the discrete setting the problem is simpler, and I
give a self-contained proof of existence using a matrix scaling
technique due to Sinkhorn [ 221 ] .  The idea is as follows: we want to
convert a unitary matrix that maps one quantum state to another, into a
nonnegative matrix whose @xmath column sums to the initial probability
of basis state @xmath , and whose @xmath row sums to the final
probability of basis state @xmath .  To do so, we first replace each
entry of the unitary matrix by its absolute value, then normalize each
column to sum to the desired initial probability, then normalize each
row to sum to the desired final probability.  But then the columns are
no longer normalized correctly, so we normalize them again , then
normalize the rows again, and so on.  I show that this iterative process
converges, from which it follows that @xmath is well-defined.  I also
observe that @xmath satisfies the indifference and product commutativity
axioms, and violates the decomposition invariance axiom.  I conjecture
that @xmath satisfies the robustness axiom; proving that conjecture is
one of the main open problems of the chapter.

In Section 63 I shift attention to the complexity of sampling histories.
 I formally define @xmath as the class of problems solvable by a
classical polynomial-time algorithm with access to a “history
oracle.”  Given a sequence of quantum circuits as input, this oracle
returns a sample from a corresponding distribution over histories of a
hidden variable, according to some hidden-variable theory @xmath .  The
oracle can choose @xmath “adversarially,” subject to the constraint that
@xmath satisfies the indifference and robustness axioms.  Thus, a key
result from Section 63 that I rely on is that there exists a
hidden-variable theory satisfying indifference and robustness.

Section 63.1 establishes the most basic facts about @xmath : for
example, that @xmath , and that @xmath is independent of the choice of
gate set.  Then Section 64 presents the “juggle subroutine,” a crucial
ingredient in both of the main hidden-variable algorithms.  Given a
state of the form @xmath or @xmath , the goal of this subroutine is to
“juggle” a hidden variable between @xmath and @xmath , so that when we
inspect the hidden variable’s history, both @xmath and @xmath are
observed with high probability.  The difficulty is that this needs to
work under any indifferent hidden-variable theory.

Next, Section 65 combines the juggle subroutine with a technique of
Valiant and Vazirani [ 231 ] to prove that @xmath , from which it
follows in particular that Graph Isomorphism and Approximate Shortest
Vector are in @xmath .  Then Section 66 applies the juggle subroutine to
search an @xmath -item database in @xmath queries, and also proves that
this @xmath bound is optimal.

I conclude in Section 67 with some directions for further research.

#### 59 Hidden-Variable Theories

Suppose we have an @xmath unitary matrix @xmath , acting on a state

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a standard orthogonal basis.  Let

  -- -------- --
     @xmath   
  -- -------- --

Then can we construct a stochastic matrix @xmath , which maps the vector
of probabilities

  -- -------- --
     @xmath   
  -- -------- --

induced by measuring @xmath , to the vector

  -- -------- --
     @xmath   
  -- -------- --

induced by measuring @xmath ?  Trivially yes.  The following matrix maps
any vector of probabilities to @xmath , ignoring the input vector @xmath
entirely:

  -- -------- --
     @xmath   
  -- -------- --

Here @xmath stands for product theory .  The product theory corresponds
to a strange picture of physical reality, in which memories and records
are completely unreliable, there being no causal connection between
states of affairs at earlier and later times.

So we would like @xmath to depend on @xmath itself somehow, not just on
@xmath and @xmath .  Indeed, ideally @xmath would be a function only of
@xmath , and not of @xmath .  But this is impossible, as the following
example shows.  Let @xmath be a @xmath rotation, and let @xmath and
@xmath .  Then @xmath implies that

  -- -------- --
     @xmath   
  -- -------- --

whereas @xmath implies that

  -- -------- --
     @xmath   
  -- -------- --

On the other hand, it is easy to see that, if @xmath can depend on
@xmath as well as @xmath , then there are infinitely many choices for
the function @xmath .  Every choice reproduces the predictions of
quantum mechanics perfectly when restricted to single-time
probabilities.  So how can we possibly choose among them?  My approach
in Sections 60 and 62 will be to write down axioms that we would like
@xmath to satisfy, and then investigate which of the axioms can be
satisfied simultaneously.

Formally, a hidden-variable theory is a family of functions @xmath ,
where each @xmath maps an @xmath -dimensional mixed state @xmath and an
@xmath unitary matrix @xmath onto a singly stochastic matrix @xmath .  I
will often suppress the dependence on @xmath , @xmath , and @xmath , and
occasionally use subscripts such as @xmath or @xmath to indicate the
theory in question.  Also, if @xmath is a pure state I may write @xmath
instead of @xmath .

Let @xmath denote the entry in the @xmath column and @xmath row of
matrix @xmath .  Then @xmath is the probability that the hidden variable
takes value @xmath after @xmath is applied, conditioned on it taking
value @xmath before @xmath is applied.  At a minimum, any theory must
satisfy the following marginalization axiom: for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

This says that after @xmath is applied, the hidden variable takes value
@xmath with probability @xmath , which is the usual Born probability.

Often it will be convenient to refer, not to @xmath itself, but to the
matrix @xmath of joint probabilities whose @xmath entry is @xmath .  The
@xmath column of @xmath must sum to @xmath , and the @xmath row must sum
to @xmath .  Indeed, I will define the theories @xmath and @xmath by
first specifying the matrix @xmath , and then setting @xmath .  This
approach has the drawback that if @xmath , then the @xmath column of
@xmath is undefined.  To get around this, I adopt the convention that

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath is the @xmath maximally mixed state.
 Technically, the limits

  -- -------- --
     @xmath   
  -- -------- --

might not exist, but in the cases of interest it will be obvious that
they do.

##### 59.1 Comparison with Previous Work

Before going further, I should contrast my approach with previous
approaches to hidden variables, the most famous of which is Bohmian
mechanics [ 59 ] .  My main difficulty with Bohmian mechanics is that it
commits itself to a Hilbert space of particle positions and momenta.
 Furthermore, it is crucial that the positions and momenta be continuous
, in order for particles to evolve deterministically.  To see this, let
@xmath and @xmath be discrete positions, and suppose a particle is in
state @xmath at time @xmath , and state @xmath at a later time @xmath .
 Then a hidden variable representing the position would have entropy
@xmath at @xmath , since it is always @xmath then; but entropy @xmath at
@xmath , since it is @xmath or @xmath both with @xmath probability.
 Therefore the earlier value cannot determine the later one. ⁷¹ ⁷¹ 71
Put differently, Bohm’s conservation of probability result breaks down
because the “wavefunctions” at @xmath and @xmath are degenerate, with
all amplitude concentrated on finitely many points.  But in a discrete
Hilbert space, every wavefunction is degenerate in this sense! It
follows that Bohmian mechanics is incompatible with the belief that all
physical observables are discrete.  But in my view, there are strong
reasons to hold that belief, which include black hole entropy bounds;
the existence of a natural minimum length scale ( @xmath cm); results on
area quantization in quantum gravity [ 205 ] ; the fact that many
physical quantities once thought to be continuous have turned out to be
discrete; the infinities of quantum field theory; the implausibility of
analog “hypercomputers”; and conceptual problems raised by the
independence of the continuum hypothesis.

Of course there exist stochastic analogues of Bohmian mechanics, among
them Nelsonian mechanics [ 181 ] and Bohm and Hiley’s “stochastic
interpretation” [ 60 ] .  But it is not obvious why we should prefer
these to other stochastic hidden-variable theories.  From a
quantum-information perspective, it is much more natural to take an
abstract approach—one that allows arbitrary finite-dimensional Hilbert
spaces, and that does not rule out any transition rule a priori .

Stochastic hidden variables have also been considered in the context of
modal interpretations; see Dickson [ 96 ] , Bacciagaluppi and Dickson [
39 ] , and Dieks [ 97 ] for example.  However, the central assumptions
in that work are extremely different from mine.  In modal
interpretations, a pure state evolving unitarily poses no problems at
all: one simply rotates the hidden-variable basis along with the state,
so that the state always represents a “possessed property” of the system
in the current basis.  Difficulties arise only for mixed states; and
there, the goal is to track a whole set of possessed properties.  By
contrast, my approach is to fix an orthogonal basis, then track a single
hidden variable that is an element of that basis.  The issues raised by
pure states and mixed states are essentially the same.

Finally I should mention the consistent-histories interpretation of
Griffiths [ 137 ] and Gell-Mann and Hartle [ 122 ] .  This
interpretation assigns probabilities to various histories through a
quantum system, so long as the “interference” between those histories is
negligible.  Loosely speaking, then, the situations where consistent
histories make sense are precisely the ones where the question of
transition probabilities can be avoided.

##### 59.2 Objections

Hidden-variable theories, as I define them, are open to several
technical objections.  For example, I required transition probabilities
for only one orthogonal observable.  What about other observables?  The
problem is that, according to the Kochen-Specker theorem, we cannot
assign consistent values to all observables at any single time, let
alone give transition probabilities for those values.  This is an issue
in any setting, not just mine.  The solution I prefer is to postulate a
fixed orthogonal basis of “distinguishable experiences,” and to
interpret a measurement in any other basis as a unitary followed by a
measurement in the fixed basis.  As mentioned in Section 59.1 , modal
interpretations opt for a different solution, which involves sets of
bases that change over time with the state itself.

Another objection is that the probability of transitioning from basis
state @xmath at time @xmath to basis state @xmath at time @xmath might
depend on how finely we divide the time interval between @xmath and
@xmath .  In other words, for some state @xmath and unitaries @xmath ,
we might have

  -- -------- --
     @xmath   
  -- -------- --

(a similar point was made by Gillespie [ 125 ] ).  Indeed, this is true
for any hidden-variable theory other than the product theory @xmath .
 To see this, observe that for all unitaries @xmath and states @xmath ,
there exist unitaries @xmath such that @xmath and @xmath .  Then
applying @xmath destroys all information in the hidden variable (that
is, decreases its entropy to @xmath ); so if we then apply @xmath , then
the variable’s final value must be uncorrelated with the initial value.
 In other words, @xmath must equal @xmath .  It follows that to any
hidden-variable theory we must associate a time scale, or some other
rule for deciding when the transitions take place.

In response, it should be noted that exactly the same problem arises in
continuous -time stochastic hidden-variable theories.  For if a state
@xmath is governed by the Schrödinger equation @xmath , and a hidden
variable’s probability distribution @xmath is governed by the stochastic
equation @xmath , then there is still an arbitrary parameter @xmath on
which the dynamics depend.

Finally, it will be objected that I have ignored special relativity.  In
Section 60 I will define a commutativity axiom , which informally
requires that the stochastic matrix @xmath not depend on the temporal
order of spacelike separated events.  Unfortunately, we will see that
when entangled states are involved, commutativity is irreconcilable with
another axiom that seems even more basic.  The resulting nonlocality has
the same character as the nonlocality of Bohmian mechanics—that is, one
cannot use it to send superluminal signals in the usual sense, but it is
unsettling nonetheless.

#### 60 Axioms for Hidden-Variable Theories

I now state five axioms that we might like hidden-variable theories to
satisfy.

Indifference. The indifference axiom says that if @xmath is
block-diagonal, then @xmath should also be block-diagonal with the same
block structure or some refinement thereof. Formally, let a block be a
subset @xmath such that @xmath for all @xmath and @xmath .  Then for all
blocks @xmath , we should have @xmath for all @xmath and @xmath .  In
particular, indifference implies that given any state @xmath in a tensor
product space @xmath , and any unitary @xmath that acts only on @xmath
(that is, never maps a basis state @xmath to @xmath where @xmath ), the
stochastic matrix @xmath acts only on @xmath as well.

Robustness. A theory is robust if it is insensitive to small errors in a
state or unitary (which, in particular, implies continuity).  Suppose we
obtain @xmath and @xmath by perturbing @xmath and @xmath
respectively.  Then for all polynomials @xmath , there should exist a
polynomial @xmath such that for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath , whenever @xmath and @xmath .  Robustness has an important
advantage for quantum computing: if a hidden-variable theory is robust
then the set of gates used to define the unitaries @xmath is irrelevant,
since by the Solovay-Kitaev Theorem (see [ 153 , 182 ] ), any universal
quantum gate set can simulate any other to a precision @xmath with
@xmath overhead.

Commutativity. Let @xmath be a bipartite state, and let @xmath and
@xmath act only on subsystems @xmath and @xmath respectively.  Then
commutativity means that the order in which @xmath and @xmath are
applied is irrelevant:

  -- -------- --
     @xmath   
  -- -------- --

Product Commutativity. A theory is product commutative if it satisfies
commutativity for all separable pure states @xmath .

Decomposition Invariance. A theory is decomposition invariant if

  -- -------- --
     @xmath   
  -- -------- --

for every decomposition

  -- -------- --
     @xmath   
  -- -------- --

of @xmath into pure states.  Theorem 138 , part (ii) will show that the
analogous axiom for @xmath is unsatisfiable.

##### 60.1 Comparing Theories

To fix ideas, let us compare some hidden-variable theories with respect
to the above axioms.  We have already seen the product theory @xmath in
Section 59 .  It is easy to show that @xmath satisfies robustness,
commutativity, and decomposition invariance.  However, I consider @xmath
unsatisfactory because it violates indifference: even if a unitary
@xmath acts only on the first of two qubits, @xmath will readily produce
transitions involving the second qubit.

Recognizing this problem, Dieks [ 97 ] proposed an alternative theory
that amounts to the following. ⁷² ⁷² 72 Dieks (personal communication)
says he would no longer defend this theory. First partition the set of
basis states into minimal blocks @xmath between which @xmath never sends
amplitude.  Then apply the product theory separately to each block; that
is, if @xmath and @xmath belong to the same block @xmath then set

  -- -------- --
     @xmath   
  -- -------- --

and otherwise set @xmath .  The resulting Dieks theory , @xmath
, satisfies indifference by construction.  However, it does not satisfy
robustness (or even continuity), since the set of blocks can change if
we replace ‘ @xmath ’ entries in @xmath by arbitrarily small nonzero
entries.

In Section 62 I will introduce two other hidden-variable theories, the
flow theory @xmath and the Schrödinger theory @xmath .  Table 16.1 lists
which axioms the four theories satisfy.

If we could prove that @xmath satisfies robustness, then Table 1
together with the impossibility results of Section 61 would completely
characterize which of the axioms can be satisfied simultaneously.

#### 61 Impossibility Results

This section shows that certain sets of axioms cannot be satisfied by
any hidden-variable theory.  I first show that the failure of @xmath ,
@xmath , and @xmath to satisfy commutativity is inherent, and not a
fixable technical problem.

###### Theorem 137

No hidden-variable theory satisfies both indifference and commutativity.

Proof. Assume indifference holds, and let our initial state be @xmath .
 Suppose @xmath applies a @xmath rotation to the first qubit, and @xmath
applies a @xmath rotation to the second qubit.  Then

  -- -------- -------- --
     @xmath   @xmath   
     @xmath            
  -- -------- -------- --

Let @xmath be the value of the hidden variable after @xmath unitaries
have been applied.  Let @xmath be the event that @xmath initially, and
@xmath at the end.  If @xmath is applied before @xmath , then the unique
‘path’ from @xmath to @xmath consistent with indifference sets @xmath .
 So

  -- -------- --
     @xmath   
  -- -------- --

But if @xmath is applied before @xmath , then the probability that
@xmath and @xmath is at most @xmath , by the same reasoning.  Thus,
since @xmath must equal @xmath with probability @xmath , and since the
only possibilities for @xmath are @xmath and @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

We conclude that commutativity is violated.

Let me remark on the relationship between Theorem 137 and Bell’s
Theorem.  Any hidden-variable theory that is “local” in Bell’s sense
would immediately satisfy both indifference and commutativity.  However,
the converse is not obvious, since there might be nonlocal information
in the states @xmath or @xmath , which an indifferent commutative theory
could exploit but a local one could not.  Theorem 137 rules out this
possibility, and in that sense is a strengthening of Bell’s Theorem.

The next result places limits on decomposition invariance.

###### Theorem 138

1.   No theory satisfies indifference, robustness, and decomposition
    invariance.

2.   No theory has the property that

      -- -------- --
         @xmath   
      -- -------- --

    for every decomposition @xmath of @xmath .

Proof.

1.  Suppose the contrary.  Let

      -- -------- -------- --
         @xmath   @xmath   
         @xmath   @xmath   
      -- -------- -------- --

    Then for every @xmath not a multiple of @xmath , we must have

      -- -------- -------- --
         @xmath   @xmath   
         @xmath   @xmath   
      -- -------- -------- --

    So by decomposition invariance, letting @xmath denote the maximally
    mixed state,

      -- -- --
            
      -- -- --

    and therefore

      -- -------- --
         @xmath   
      -- -------- --

    By robustness, this holds for @xmath as well.   But this is a
    contradiction, since by indifference @xmath must be half the
    identity.

2.  Suppose the contrary; then

      -- -------- --
         @xmath   
      -- -------- --

    So considering transitions from @xmath to @xmath ,

      -- -- --
            
      -- -- --

    But

      -- -------- --
         @xmath   
      -- -------- --

    also.  Since @xmath , we have

      -- -------- -------- --
         @xmath   @xmath   
                  @xmath   
                  @xmath   
                  @xmath   
      -- -------- -------- --

    which is a contradiction.

Notice that all three conditions in Theorem 138 , part (i) were
essential—for @xmath satisfies robustness and decomposition invariance,
@xmath satisfies indifference and decomposition invariance, and @xmath
satisfies indifference and robustness.

The last impossibility result says that no hidden-variable theory
satisfies both indifference and “strong continuity,” in the sense that
for all @xmath there exists @xmath such that @xmath implies @xmath .  To
see this, let

  -- -------- -------- --
     @xmath   @xmath   
     @xmath            
     @xmath            
  -- -------- -------- --

Then by indifference,

  -- -------- --
     @xmath   
  -- -------- --

This is the reason why I defined robustness in terms of the joint
probabilities matrix @xmath rather than the stochastic matrix @xmath .
 On the other hand, note that by giving up indifference, one can satisfy
strong continuity, as is shown by @xmath .

#### 62 Specific Theories

This section presents two nontrivial examples of hidden-variable
theories: the flow theory in Section 62.1 , and the Schrödinger theory
in Section 62.2 .

##### 62.1 Flow Theory

The idea of the flow theory is to convert a unitary matrix into a
weighted directed graph, and then route probability mass through that
graph like oil through pipes.  Given a unitary @xmath , let

  -- -- --
        
  -- -- --

where for the time being

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

are pure states.  Then consider the network @xmath shown in Figure 13 .

We have a source vertex @xmath , a sink vertex @xmath , and @xmath input
and @xmath output vertices labeled by basis states @xmath .  Each edge
of the form @xmath has capacity @xmath , each edge @xmath has capacity
@xmath , and each edge @xmath has capacity @xmath .  A natural question
is how much probability mass can flow from @xmath to @xmath without
violating the capacity constraints.  Rather surprisingly, I will show
that one unit of mass (that is, all of it) can.  Interestingly, this
result would be false if edge @xmath had capacity @xmath (or even @xmath
) instead of @xmath .  I will also show that there exists a mapping from
networks to maximal flows in those networks, that is robust in the sense
that a small change in edge capacities produces only a small change in
the amount of flow through any edge.

The proofs of these theorems use classical results from the theory of
network flows (see [ 88 ] for an introduction).  In particular, let a
cut be a set of edges that separates @xmath from @xmath ; the value of a
cut is the sum of the capacities of its edges.  Then a fundamental
result called the Max-Flow-Min-Cut Theorem [ 111 ] says that the maximum
possible amount of flow from @xmath to @xmath equals the minimum value
of any cut.  Using that result I can show the following.

###### Theorem 139

One unit of flow can be routed from @xmath to @xmath in @xmath .

Proof. By the above, it suffices to show that any cut @xmath in @xmath
has value at least @xmath .  Let @xmath be the set of @xmath such that
@xmath , and let @xmath be the set of @xmath such that @xmath .  Then
@xmath must contain every edge @xmath such that @xmath and @xmath , and
we can assume without loss of generality that @xmath contains no other
edges.  So the value of @xmath is

  -- -------- --
     @xmath   
  -- -------- --

Therefore we need to prove the matrix inequality

  -- -------- --
     @xmath   
  -- -------- --

or

  -- -------- -- -----
     @xmath      (2)
  -- -------- -- -----

Let @xmath be fixed, and consider the maximum of the right-hand side of
equation ( 2 ) over all @xmath .  Since

  -- -------- --
     @xmath   
  -- -------- --

this maximum is equal to the largest eigenvalue @xmath of the positive
semidefinite matrix

  -- -------- --
     @xmath   
  -- -------- --

where for each @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath be the subspace of states spanned by @xmath , and let @xmath
be the subspace spanned by @xmath .  Also, let @xmath be the length of
the projection of @xmath onto @xmath , and let @xmath be the length of
the projection of @xmath onto @xmath .  Then since the @xmath ’s and
@xmath ’s form orthogonal bases for @xmath and @xmath respectively, we
have

  -- -------- -- --
     @xmath      
                 
  -- -------- -- --

So letting @xmath be the angle between @xmath and @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
                       
              @xmath   
              @xmath   
  -- -------- -------- --

which completes the theorem.

Observe that Theorem 139 still holds if @xmath acts on a mixed state
@xmath , since we can write @xmath as a convex combination of pure
states @xmath , construct a flow for each @xmath separately, and then
take a convex combination of the flows.

Using Theorem 139 , I now define the flow theory @xmath .  Let @xmath be
the set of maximal flows for @xmath —representable by @xmath arrays of
real numbers @xmath such that @xmath for all @xmath , and also

  -- -------- --
     @xmath   
  -- -------- --

Clearly @xmath is a convex polytope, which Theorem 139 asserts is
nonempty.  Form a maximal flow @xmath as follows: first let @xmath be
the maximum of @xmath over all @xmath .  Then let @xmath be the maximum
of @xmath over all @xmath such that @xmath .  Continue to loop through
all @xmath pairs in lexicographic order, setting each @xmath to its
maximum possible value consistent with the @xmath previous values.
 Finally, let @xmath for all @xmath .  As discussed in Section 59 ,
given @xmath we can easily obtain the stochastic matrix @xmath by
dividing the @xmath column by @xmath , or taking a limit in case @xmath
.

It is easy to check that @xmath so defined satisfies the indifference
axiom.  Showing that @xmath satisfies robustness is harder.  Our proof
is based on the Ford-Fulkerson algorithm [ 111 ] , a classic algorithm
for computing maximal flows that works by finding a sequence of
“augmenting paths,” each of which increases the flow from @xmath to
@xmath by some positive amount.

###### Theorem 140

@xmath satisfies robustness.

Proof. Let @xmath be an arbitrary flow network with source @xmath , sink
@xmath , and directed edges @xmath , where each @xmath has capacity
@xmath and leads from @xmath to @xmath .  It will be convenient to
introduce a fictitious edge @xmath from @xmath to @xmath with unlimited
capacity; then maximizing the flow through @xmath is equivalent to
maximizing the flow through @xmath .  Suppose we produce a new network
@xmath by increasing a single capacity @xmath by some @xmath .  Let
@xmath be the optimal flow for @xmath , obtained by first maximizing the
flow @xmath through @xmath , then maximizing the flow @xmath through
@xmath holding @xmath fixed, and so on up to @xmath .  Let @xmath be the
maximal flow for @xmath produced in the same way.  We claim that for all
@xmath ,

  -- -------- --
     @xmath   
  -- -------- --

To see that the theorem follows from this claim: first, if @xmath is
robust under adding @xmath to @xmath , then it must also be robust under
subtracting @xmath from @xmath .  Second, if we change @xmath to @xmath
such that @xmath and @xmath , then we can imagine the @xmath edge
capacities are changed one by one, so that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

(Here we have made no attempt to optimize the bound.)

We now prove the claim.  To do so we describe an iterative algorithm for
computing @xmath .  First maximize the flow @xmath through @xmath , by
using the Ford-Fulkerson algorithm to find a maximal flow from @xmath to
@xmath .  Let @xmath be the resulting flow, and let @xmath be the
residual network that corresponds to @xmath .  For each @xmath , that
is, @xmath has an edge @xmath of capacity @xmath , and an edge @xmath of
capacity @xmath .  Next maximize @xmath subject to @xmath by using the
Ford-Fulkerson algorithm to find “augmenting cycles” from @xmath to
@xmath and back to @xmath in @xmath .  Continue in this manner until
each of @xmath has been maximized subject to the previous @xmath ’s.
 Finally set @xmath .

Now, one way to compute @xmath is to start with @xmath , then repeatedly
“correct” it by applying the same iterative algorithm to maximize @xmath
, then @xmath , and so on.  Let @xmath ; then we need to show that
@xmath for all @xmath .  The proof is by induction on @xmath .  Clearly
@xmath , since increasing @xmath by @xmath can increase the value of the
minimum cut from @xmath to @xmath by at most @xmath .  Likewise, after
we maximize @xmath , the value of the minimum cut from @xmath to @xmath
can increase by at most @xmath .  For of the at most @xmath new units of
flow from @xmath to @xmath that increasing @xmath made available, @xmath
of them were “taken up” in maximizing @xmath , but the process of
maximizing @xmath could have again increased the minimum cut from @xmath
to @xmath by up to @xmath .  Continuing in this way,

  -- -------- --
     @xmath   
  -- -------- --

and so on up to @xmath .  This completes the proof.

That @xmath violates decomposition invariance now follows from Theorem
138 , part (i).  One can also show that @xmath violates product
commutativity, by considering the following example: let @xmath be a
@xmath -qubit initial state, and let @xmath and @xmath be @xmath
rotations applied to the first and second qubits respectively.  Then

  -- -------- --
     @xmath   
  -- -------- --

We omit a proof for brevity.

##### 62.2 Schrödinger Theory

The final hidden-variable theory, which I call the Schrödinger theory or
@xmath , is the most interesting one mathematically.  The idea—to make a
matrix into a stochastic matrix via row and column rescaling—is natural
enough that we came upon it independently, only later learning that it
originated in a 1931 paper of Schrödinger [ 213 ] .  The idea was
subsequently developed by Fortet [ 112 ] , Beurling [ 58 ] , Nagasawa [
179 ] , and others.  My goal is to give what (to my knowledge) is the
first self-contained, reasonably accessible presentation of the main
result in this area; and to interpret that result in what I think is the
correct way: as providing one example of a hidden-variable theory, whose
strengths and weaknesses should be directly compared to those of other
theories.

Most of the technical difficulties in [ 58 , 112 , 179 , 213 ] arise
because the stochastic process being constructed involves continuous
time and particle positions.  Here I eliminate those difficulties by
restricting attention to discrete time and finite-dimensional Hilbert
spaces.  I thereby obtain a generalized version ⁷³ ⁷³ 73 In @xmath
-scaling, we are given an invertible real matrix, and the goal is to
rescale all rows and columns to sum to @xmath .  The generalized version
is to rescale the rows and columns to given values (not necessarily
@xmath ). of a problem that computer scientists know as @xmath -scaling
of matrices [ 221 , 118 , 167 ] .

As in the case of the flow theory, given a unitary @xmath acting on a
state @xmath , the first step is to replace each entry of @xmath by its
absolute value, obtaining a nonnegative matrix @xmath defined by @xmath
.  We then wish to find nonnegative column multipliers @xmath and row
multipliers @xmath such that for all @xmath ,

  -- -------- -------- -- -----
     @xmath   @xmath      (3)
     @xmath   @xmath      (4)
  -- -------- -------- -- -----

If we like, we can interpret the @xmath ’s and @xmath ’s as dynamical
variables that reach equilibrium precisely when equations ( 3 ) and ( 4
) are satisfied.  Admittedly, it might be thought physically implausible
that such a complicated dynamical process should take place at every
instant of time.  On the other hand, it is hard to imagine a more
“benign” way to convert @xmath into a joint probabilities matrix, than
by simply rescaling its rows and columns.

I will show that multipliers satisfying ( 3 ) and ( 4 ) always exist.
 The intuition of a dynamical process reaching equilibrium turns out to
be key to the proof.  For all @xmath , let

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

In words, we obtain @xmath by normalizing each column @xmath of @xmath
to sum to @xmath ; likewise we obtain @xmath by normalizing each row
@xmath of @xmath to sum to @xmath .  The crucial fact is that the above
process always converges to some @xmath .  We can therefore take

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

for all @xmath .  Although I will not prove it here, it turns out that
this yields the unique solution to equations ( 3 ) and ( 4 ), up to a
global rescaling of the form @xmath for all @xmath and @xmath for all
@xmath [ 179 ] .

The convergence proof will reuse a result about network flows from
Section 62.1 , in order to define a nondecreasing “progress
measure” based on Kullback-Leibler distance.

###### Theorem 141

The limit @xmath exists.

Proof. A consequence of Theorem 139 is that for every @xmath , there
exists an @xmath array of nonnegative real numbers @xmath such that

1.  @xmath whenever @xmath ,

2.  @xmath for all @xmath , and

3.  @xmath for all @xmath .

Given any such array, define a progress measure

  -- -------- --
     @xmath   
  -- -------- --

where we adopt the convention @xmath .  We claim that @xmath for all
@xmath .  To see this, assume without loss of generality that we are on
an odd step @xmath , and let @xmath be the @xmath column sum before we
normalize it.  Then

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

As a result of the @xmath normalization step, we had @xmath .  Subject
to that constraint, the maximum of

  -- -------- --
     @xmath   
  -- -------- --

over the @xmath ’s occurs when @xmath for all @xmath —a simple calculus
fact that follows from the nonnegativity of Kullback-Leibler distance.
 This implies that @xmath .  Similarly, normalizing rows leads to @xmath
.

It follows that the limit @xmath exists.  For suppose not; then some
@xmath is bounded away from @xmath , so there exists an @xmath such that
@xmath for all even @xmath .  But this is a contradiction, since @xmath
and @xmath for all @xmath .

Besides showing that @xmath is well-defined, Theorem 141 also yields a
procedure to calculate @xmath (as well as the @xmath ’s and @xmath ’s).
 It can be shown that this procedure converges to within entrywise error
@xmath after a number steps polynomial in @xmath and @xmath .  Also,
once we have @xmath , the stochastic matrix @xmath is readily obtained
by normalizing each column of @xmath to sum to @xmath .  This completes
the definition of the Schrödinger theory @xmath .

It is immediate that @xmath satisfies indifference.  Also:

###### Proposition 142

@xmath satisfies product commutativity.

Proof. Given a state @xmath , let @xmath act only on @xmath and let
@xmath act only on @xmath .  Then we claim that

  -- -------- --
     @xmath   
  -- -------- --

The reason is simply that multiplying all amplitudes in @xmath and
@xmath by a constant factor @xmath , as we do for each basis state
@xmath of @xmath , has no effect on the scaling procedure that produces
@xmath .  Similarly

  -- -------- --
     @xmath   
  -- -------- --

It follows that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

On the other hand, numerical simulations readily show that @xmath
violates decomposition invariance, even when @xmath (I omit a concrete
example for brevity).

#### 63 The Computational Model

I now explain the histories model of computation, building up to the
complexity class @xmath .  From now on, the states @xmath that we
consider will always be pure states of @xmath qubits.  That is, @xmath
where

  -- -------- --
     @xmath   
  -- -------- --

The algorithms of this chapter will work under any hidden-variable
theory that satisfies the indifference axiom.  On the other hand, if we
take into account that even in theory (let alone in practice), a generic
unitary cannot be represented exactly with a finite universal gate set,
only approximated arbitrarily well, then we also need the robustness
axiom.  Thus, it is reassuring that there exists a hidden-variable
theory (namely @xmath ) that satisfies both indifference and robustness.

Let a quantum computer have the initial state @xmath , and suppose we
apply a sequence @xmath of unitary operations, each of which is
implemented by a polynomial-size quantum circuit.  Then a history of a
hidden variable through the computation is a sequence @xmath of basis
states, where @xmath is the variable’s value immediately after @xmath is
applied (thus @xmath ).  Given any hidden-variable theory @xmath , we
can obtain a probability distribution @xmath over histories by just
applying @xmath repeatedly, once for each @xmath , to obtain the
stochastic matrices

  -- -- --
        
  -- -- --

Note that @xmath is a Markov distribution; that is, each @xmath is
independent of the other @xmath ’s conditioned on @xmath and @xmath .
 Admittedly, @xmath could depend on the precise way in which the
combined circuit @xmath is “sliced” into component circuits @xmath .
 But as noted in Section 59.2 , such dependence on the granularity of
unitaries is unavoidable in any hidden-variable theory other than @xmath
.

Given a hidden-variable theory @xmath , let @xmath be an oracle that
takes as input a positive integer @xmath , and a sequence of quantum
circuits @xmath that act on @xmath qubits.  Here each @xmath is
specified by a sequence @xmath of gates chosen from some finite
universal gate set @xmath .  The oracle @xmath returns as output a
sample @xmath from the history distribution @xmath defined previously.
 Now let @xmath be a deterministic classical Turing machine that is
given oracle access to @xmath .  The machine @xmath receives an input
@xmath , makes a single oracle query to @xmath , then produces an output
based on the response.  We say a set of strings @xmath is in @xmath if
there exists an @xmath such that for all sufficiently large @xmath and
inputs @xmath , and all theories @xmath satisfying the indifference and
robustness axioms, @xmath correctly decides whether @xmath with
probability at least @xmath , in time polynomial in @xmath .

Let me make some remarks about the above definition.  There is no real
significance in the requirement that @xmath be deterministic and
classical, and that it be allowed only one query to @xmath .  I made
this choice only because it suffices for the upper bounds; it might be
interesting to consider the effects of other choices.  However, other
aspects of the definition are not arbitrary.  The order of quantifiers
matters; we want a single @xmath that works for any hidden-variable
theory satisfying indifference and robustness.  Also, we require @xmath
to succeed only for sufficiently large @xmath since by choosing a large
enough polynomial @xmath in the statement of the robustness axiom, an
adversary might easily make @xmath incorrect on a finite number of
instances.

##### 63.1 Basic Results

Having defined the complexity class @xmath , in this subsection I
establish its most basic properties.  First of all, it is immediate that
@xmath ; that is, sampling histories is at least as powerful as standard
quantum computation.  For @xmath , the first hidden-variable value
returned by @xmath , can be seen as simply the result of applying a
polynomial-size quantum circuit @xmath to the initial state @xmath and
then measuring in the standard basis.  A key further observation is the
following.

###### Theorem 143

Any universal gate set yields the same complexity class @xmath .  By
universal, we mean that any unitary matrix (real or complex) can be
approximated, without the need for ancilla qubits.

Proof. Let @xmath and @xmath be universal gate sets.  Also, let @xmath
be a sequence of @xmath -qubit unitaries, each specified by a
polynomial-size quantum circuit over @xmath .  We have @xmath where
@xmath is the input length.  We can also assume without loss of
generality that @xmath , since otherwise we simply insert @xmath dummy
qubits that are never acted on (by the indifference axiom, this will not
affect the results).  We want to approximate @xmath by another sequence
of @xmath -qubit unitaries, @xmath , where each @xmath is specified by a
quantum circuit over @xmath .  In particular, for all @xmath we want
@xmath .  By the Solovay-Kitaev Theorem [ 153 , 182 ] , we can achieve
this using @xmath gates from @xmath ; moreover, the circuit for @xmath
can be constructed in polynomial time given the circuit for @xmath .

Let @xmath and @xmath .  Notice that for all @xmath ,

  -- -- -------- --
        @xmath   
        @xmath   
  -- -- -------- --

since @xmath .  Here @xmath denotes the maximum entrywise difference
between two vectors in @xmath .  Also, given a theory @xmath , let
@xmath and @xmath be the joint probabilities matrices corresponding to
@xmath and @xmath respectively.  Then by the robustness axiom, there
exists a polynomial @xmath such that if @xmath and @xmath , then @xmath
.  For all such polynomials @xmath , we have @xmath and @xmath for
sufficiently large @xmath .  Therefore @xmath for all @xmath and
sufficiently large @xmath .

Now assume @xmath is sufficiently large, and consider the distributions
@xmath and @xmath over classical histories @xmath .  For all @xmath and
@xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

It follows by the union bound that the variation distance @xmath is at
most

  -- -------- --
     @xmath   
  -- -------- --

In other words, @xmath can be distinguished from @xmath with bias at
most @xmath , which is exponentially small.  So any classical
postprocessing algorithm that succeeds with high probability given
@xmath , also succeeds with high probability given @xmath .  This
completes the theorem.

Unfortunately, the best upper bound on @xmath I have been able to show
is @xmath ; that is, any problem in @xmath is solvable in deterministic
exponential time.  The proof is trivial: let @xmath be the flow theory
@xmath .  Then by using the Ford-Fulkerson algorithm, we can clearly
construct the requisite maximum flows in time polynomial in @xmath
(hence exponential in @xmath ), and thereby calculate the probability of
each possible history @xmath to suitable precision.

#### 64 The Juggle Subroutine

This section presents a crucial subroutine that will be used in both
main algorithms: the algorithm for simulating statistical zero knowledge
in Section 65 , and the algorithm for search in @xmath queries in
Section 66 .  Given an @xmath -qubit state @xmath , where @xmath and
@xmath are unknown basis states, the goal of the juggle subroutine is to
learn both @xmath and @xmath .  The name arises because the strategy
will be to “juggle” a hidden variable, so that if it starts out at
@xmath then with non-negligible probability it transitions to @xmath ,
and vice versa.  Inspecting the entire history of the hidden variable
will then reveal both @xmath and @xmath , as desired.

To produce this behavior, we will exploit a basic feature of quantum
mechanics: that observable information in one basis can become
unobservable phase information in a different basis.  We will apply a
sequence of unitaries that hide all information about @xmath and @xmath
in phases, thereby forcing the hidden variable to “forget” whether it
started at @xmath or @xmath .  We will then invert those unitaries to
return the state to @xmath , at which point the hidden variable, having
“forgotten” its initial value, must be unequal to that value with
probability @xmath .

I now give the subroutine.  Let @xmath be the initial state.  The first
unitary, @xmath , consists of Hadamard gates on @xmath qubits chosen
uniformly at random, and the identity operation on the remaining qubit,
@xmath .  Next @xmath consists of a Hadamard gate on qubit @xmath .
 Finally @xmath consists of Hadamard gates on all @xmath qubits.  Let
@xmath and @xmath .  Then since @xmath , we have @xmath with probability
at least @xmath .  Assuming that occurs, the state

  -- -------- --
     @xmath   
  -- -------- --

assigns nonzero amplitude to all @xmath basis states.  Then @xmath
assigns nonzero amplitude to @xmath basis states @xmath , namely those
for which @xmath .  Finally @xmath .

Let @xmath be the value of the hidden variable after @xmath is applied.
 Then assuming @xmath , I claim that @xmath is independent of @xmath .
 So in particular, if @xmath then @xmath with @xmath probability, and if
@xmath then @xmath with @xmath probability.  To see this, observe that
when @xmath is applied, there is no interference between basis states
@xmath such that @xmath , and those such that @xmath .  So by the
indifference axiom, the probability mass at @xmath must spread out
evenly among all @xmath basis states that agree with @xmath on the
@xmath bit, and similarly for the probability mass at @xmath .  Then
after @xmath is applied, @xmath can differ from @xmath only on the
@xmath bit, again by the indifference axiom.  So each basis state of
@xmath must receive an equal contribution from probability mass
originating at @xmath , and probability mass originating at @xmath .
 Therefore @xmath is independent of @xmath , from which it follows that
@xmath is independent of @xmath as well.

Unfortunately, the juggle subroutine only works with probability @xmath
—for it requires that @xmath , and even then, inspecting the history
@xmath only reveals both @xmath and @xmath with probability @xmath .
 Furthermore, the definition of @xmath does not allow more than one call
to the history oracle.  However, all we need to do is pack multiple
subroutine calls into a single oracle call.  That is, choose @xmath
similarly to @xmath (except with a different value of @xmath ), and set
@xmath and @xmath .  Do the same with @xmath , @xmath , and @xmath , and
so on.  Since @xmath all return the quantum state to @xmath , the effect
is that of multiple independent juggle attempts.  With @xmath attempts,
we can make the failure probability at most @xmath .

As a final remark, it is easy to see that the juggle subroutine works
equally well with states of the form @xmath .  This will prove useful in
Section 66 .

#### 65 Simulating @xmath

This section shows that @xmath .  Here @xmath , or Statistical Zero
Knowledge, was originally defined as the class of problems that possess
a certain kind of “zero-knowledge proof protocol”—that is, a protocol
between an omniscient prover and a verifier, by which the verifier
becomes convinced of the answer to a problem, yet without learning
anything else about the problem.  However, for present purposes this
cryptographic definition of @xmath is irrelevant.  For Sahai and Vadhan
[ 209 ] have given an alternate and much simpler characterization: a
problem is in @xmath if and only if it can be reduced to a problem
called Statistical Difference, which involves deciding whether two
probability distributions are close or far.

More formally, let @xmath and @xmath be functions that map @xmath -bit
strings to @xmath -bit strings for some polynomial @xmath , and that are
specified by classical polynomial-time algorithms.  Let @xmath and
@xmath be the probability distributions over @xmath and @xmath
respectively, if @xmath is chosen uniformly at random.  Then the problem
is to decide whether @xmath is less than @xmath or greater than @xmath ,
given that one of these is the case.  Here

  -- -------- --
     @xmath   
  -- -------- --

is the variation distance between @xmath and @xmath .

To illustrate, let us see why Graph Isomorphism is in @xmath .  Given
two graphs @xmath and @xmath , take @xmath to be the uniform
distribution over all permutations of @xmath , and @xmath to be uniform
over all permutations of @xmath .  This way, if @xmath and @xmath are
isomorphic, then @xmath and @xmath will be identical, so @xmath .  On
the other hand, if @xmath and @xmath are non-isomorphic, then @xmath and
@xmath will be perfectly distinguishable, so @xmath .  Since @xmath and
@xmath are clearly samplable by polynomial-time algorithms, it follows
that any instance of Graph Isomorphism can be expressed as an instance
of Statistical Difference.  For a proof that Approximate Shortest Vector
is in @xmath , the reader is referred to Goldreich and Goldwasser [ 129
] (see also Aharonov and Ta-Shma [ 23 ] ).

The proof will use the following “amplification lemma” from [ 209 ] : ⁷⁴
⁷⁴ 74 Note that in this lemma, the constants @xmath and @xmath are not
arbitrary; it is important for technical reasons that @xmath .

###### Lemma 144 (Sahai and Vadhan)

Given efficiently-samplable distributions @xmath and @xmath , we can
construct new efficiently-samplable distributions @xmath and @xmath ,
such that if @xmath then @xmath , while if @xmath then @xmath .

In particular, Lemma 144 means we can assume without loss of generality
that either @xmath or @xmath for some constant @xmath .

Having covered the necessary facts about @xmath , we can now proceed to
the main result.

###### Theorem 145

@xmath .

Proof. We show how to solve Statistical Difference by using a history
oracle.  For simplicity, we start with the special case where @xmath and
@xmath are both one-to-one functions.  In this case, the circuit
sequence @xmath given to the history oracle does the following: it first
prepares the state

  -- -------- --
     @xmath   
  -- -------- --

It then applies the juggle subroutine to the joint state of the @xmath
and @xmath registers, taking @xmath .  Notice that by the indifference
axiom, the hidden variable will never transition from one value of
@xmath to another—exactly as if we had measured the third register in
the standard basis.  All that matters is the reduced state @xmath of the
first two registers, which has the form @xmath for some @xmath if @xmath
, and @xmath for some @xmath if @xmath .  We have already seen that the
juggle subroutine can distinguish these two cases: when the
hidden-variable history is inspected, it will contain two values of the
@xmath register in the former case, and only one value in the latter
case.  Also, clearly the case @xmath is statistically indistinguishable
from @xmath with respect to the subroutine, and likewise @xmath is
indistinguishable from @xmath .

We now consider the general case, where @xmath and @xmath need not be
one-to-one.  Our strategy is to reduce to the one-to-one case, by using
a well-known hashing technique of Valiant and Vazirani [ 231 ] .  Let
@xmath be the uniform distribution over all affine functions mapping
@xmath to @xmath , where we identify those sets with the finite fields
@xmath and @xmath respectively.  What Valiant and Vazirani showed is
that, for all subsets @xmath such that @xmath , and all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

As a corollary, the expectation over @xmath of

  -- -- --
        
  -- -- --

is at least @xmath .  It follows that, if @xmath is drawn uniformly at
random from @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

This immediately suggests the following algorithm for the many-to-one
case.  Draw @xmath uniformly at random from @xmath ; then draw @xmath .
 Have @xmath prepare the state

  -- -------- --
     @xmath   
  -- -------- --

and then apply the juggle subroutine to the joint state of the @xmath
and @xmath registers, ignoring the @xmath and @xmath registers as
before.

Suppose @xmath .  Also, given @xmath and @xmath , let @xmath and @xmath
, and suppose @xmath .  Then

  -- -------- --
     @xmath   
  -- -------- --

since the events @xmath and @xmath are independent of each other
conditioned on @xmath .  Assuming both events occur, as before the
juggle subroutine will reveal both @xmath and @xmath with high
probability, where @xmath and @xmath are the unique elements of @xmath
and @xmath respectively.  By contrast, if @xmath then only one value of
the @xmath register will ever be observed.  Again, replacing @xmath by
@xmath , and @xmath by @xmath , can have only a negligible effect on the
history distribution.

Of course, the probability that the correct value of @xmath is chosen,
and that @xmath and @xmath both have a unique element, could be as low
as @xmath .  To deal with this, we simply increase the number of calls
to the juggle subroutine by an @xmath factor, drawing new values of
@xmath for each call.  We pack multiple subroutine calls into a single
oracle call as described in Section 64 , except that now we uncompute
the entire state (returning it to @xmath ) and then recompute it between
subroutine calls.  A final remark: since the algorithm that calls the
history oracle is deterministic, we “draw” new values of @xmath by
having @xmath prepare a uniform superposition over all possible values.
 The indifference axiom justifies this procedure, by guaranteeing that
within each call to the juggle subroutine, the hidden-variable values of
@xmath , @xmath , and @xmath remain constant.

Recall from Chapter I that there exists an oracle @xmath relative to
which @xmath .  By contrast, since Theorem 145 is easily seen to
relativize, we have @xmath for all oracles @xmath .  It follows that
there exists an oracle @xmath relative to which @xmath .

#### 66 Search in @xmath Queries

Given a Boolean function @xmath , the database search problem is simply
to find a string @xmath such that @xmath .  We can assume without loss
of generality that this “marked item” @xmath is unique. ⁷⁵ ⁷⁵ 75 For if
there are multiple marked items, then we can reduce to the unique marked
item case by using the Valiant-Vazirani hashing technique described in
Theorem 145 . We want to find it using as few queries to @xmath as
possible, where a query returns @xmath given @xmath .

Let @xmath .  Then classically, of course, @xmath queries are necessary
and sufficient.  By querying @xmath in superposition, Grover’s algorithm
[ 139 ] finds @xmath using @xmath queries, together with @xmath
auxiliary computation steps (here the @xmath hides a factor of the form
@xmath ).  Bennett et al. [ 51 ] showed that any quantum algorithm needs
@xmath queries.

In this section, I show how to find the marked item by sampling
histories, using only @xmath queries and @xmath computation steps.
 Formally, the model is as follows.  Each of the quantum circuits @xmath
that algorithm @xmath gives to the history oracle @xmath is now able to
query @xmath .  Suppose @xmath makes @xmath queries to @xmath ; then the
total number of queries made by @xmath is defined to be @xmath .  The
total number of computation steps is at least the number of steps
required to write down @xmath , but could be greater.

###### Theorem 146

In the @xmath model, we can search a database of @xmath items for a
unique marked item using @xmath queries and @xmath computation steps.

Proof. Assume without loss of generality that @xmath with @xmath , and
that each database item is labeled by an @xmath -bit string.  Let @xmath
be the label of the unique marked item.  Then the sequence of quantum
circuits @xmath does the following: it first runs @xmath iterations of
Grover’s algorithm, in order to produce the @xmath -qubit state @xmath ,
where

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

(one can check that this state is normalized).  Next @xmath applies
Hadamard gates to the first @xmath qubits.  This yields the state

  -- -------- --
     @xmath   
  -- -------- --

where @xmath consists of the first @xmath bits of @xmath , and @xmath
consists of the remaining @xmath bits.  Let @xmath be the set of @xmath
basis states of the form @xmath , and @xmath be the set of @xmath basis
states of the form @xmath .

Notice that @xmath .  So with the sole exception of @xmath (which
belongs to both @xmath and @xmath ), the “marked” basis states in @xmath
have the same amplitude as the “unmarked” basis states in @xmath .  This
is what we wanted.  Notice also that, if we manage to find any @xmath ,
then we can find @xmath itself using @xmath further classical queries:
simply test all possible strings that end in @xmath .  Thus, the goal of
our algorithm will be to cause the hidden variable to visit an element
of @xmath , so that inspecting the variable’s history reveals that
element.

As in Theorem 145 , the tools that we need are the juggle subroutine,
and a way of reducing many basis states to two.  Let @xmath be drawn
uniformly at random from @xmath .  Then @xmath appends a third register
to the state, and sets it equal to @xmath if the first two registers
have the form @xmath , or to @xmath if they have the form @xmath .
 Disregarding the basis state @xmath for convenience, the result is

  -- -------- --
     @xmath   
  -- -------- --

Next @xmath applies the juggle subroutine to the joint state of the
first two registers.  Suppose the hidden-variable value has the form
@xmath (that is, lies outside @xmath ).  Then with probability @xmath
over @xmath , the first @xmath bits of @xmath are equal to @xmath .
 Suppose this event occurs.  Then conditioned on the third register
being @xmath , the reduced state of the first two registers is

  -- -------- --
     @xmath   
  -- -------- --

where @xmath consists of the last @xmath bits of @xmath .  So it follows
from Section 64 that with probability @xmath , the juggle subroutine
will cause the hidden variable to transition from @xmath to @xmath , and
hence from @xmath to @xmath .

The algorithm calls the juggle subroutine @xmath times, drawing a new
value of @xmath and recomputing the third register after each call.
 Each call moves the hidden variable from @xmath to @xmath with
independent probability @xmath ; therefore with high probability some
call does so.  Note that this juggling phase does not involve any
database queries.  Also, as in Theorem 145 , “drawing” @xmath really
means preparing a uniform superposition over all possible @xmath .
 Finally, the probability that the hidden variable ever visits the basis
state @xmath is exponentially small (by the union bound), which
justifies our having disregarded it.

A curious feature of Theorem 146 is the tradeoff between queries and
computation steps.  Suppose we had run @xmath iterations of Grover’s
algorithm, or in other words made @xmath queries to @xmath .  Then
provided @xmath , the marked state @xmath would have occurred with
probability @xmath , meaning that @xmath calls to the juggle subroutine
would have been sufficient to find @xmath .  Of course, the choice of
@xmath that minimizes @xmath is @xmath .  On the other hand, had we been
willing to spend @xmath computation steps, we could have found @xmath
with only a single query! ⁷⁶ ⁷⁶ 76 One should not make too much of this
fact; one way to interpret it is simply that the “number of
queries” should be redefined as @xmath rather than @xmath . Thus, one
might wonder whether some other algorithm could push the number of
queries below @xmath , without simultaneously increasing the number of
computation steps.  The following theorem rules out that possibility.

###### Theorem 147

In the @xmath model, @xmath computation steps are needed to search an
@xmath -item database for a unique marked item.  As a consequence, there
exists an oracle relative to which @xmath ; that is, @xmath -complete
problems are not efficiently solvable by sampling histories.

Proof. Let @xmath and @xmath .  Given a sequence of quantum circuits
@xmath that query @xmath , and assuming that @xmath is the unique string
such that @xmath , let @xmath be the quantum state after @xmath is
applied but before @xmath is.  Then the “hybrid argument” of Bennett et
al. [ 51 ] implies that, by simply changing the location of the marked
item from @xmath to @xmath , we can ensure that

  -- -------- --
     @xmath   
  -- -------- --

where @xmath represents trace distance, and @xmath is the total number
of queries made to @xmath by @xmath .  Therefore @xmath provides an
upper bound on the probability of noticing the @xmath change by
monitoring @xmath , the value of the hidden variable after @xmath is
applied.  So by the union bound, the probability of noticing the change
by monitoring the entire history @xmath is at most of order

  -- -------- --
     @xmath   
  -- -------- --

This cannot be @xmath unless @xmath or @xmath , either of which implies
an @xmath lower bound on the total number of steps.

To obtain an oracle relative to which @xmath , we can now use a standard
and well-known “diagonalization method” due to Baker, Gill, and Solovay
[ 41 ] to construct an infinite sequence of exponentially hard search
problems, such that any @xmath machine fails on at least one of the
problems, whereas there exists an @xmath machine that succeeds on all of
them.  Details are omitted.

#### 67 Conclusions and Open Problems

The idea that certain observables in quantum mechanics might have
trajectories governed by dynamical laws has reappeared many times: in
Schrödinger’s 1931 stochastic approach [ 213 ] , Bohmian mechanics [ 59
] , modal interpretations [ 39 , 96 , 97 ] , and elsewhere.  Yet because
all of these proposals yield the same predictions for single-time
probabilities, if we are to decide between them it must be on the basis
of internal mathematical considerations.  One message of this chapter
has been that such considerations can actually get us quite far.

To focus attention on the core issues, I restricted attention to the
simplest possible setting: discrete time, a finite-dimensional Hilbert
space, and a single orthogonal basis.  Within this setting, I proposed
what seem like reasonable axioms that any hidden-variable theory should
satisfy: for example, indifference to the identity operation, robustness
to small perturbations, and independence of the temporal order of
spacelike-separated events.  I then showed that not all of these axioms
can be satisfied simultaneously.  But perhaps more surprisingly, I also
showed that certain subsets of axioms can be satisfied for quite
nontrivial reasons.  In showing that the indifference and robustness
axioms can be simultaneously satisfied, Section 62 revealed an
unexpected connection between unitary matrices and the classical theory
of network flows.

As mentioned previously, an important open problem is to show that the
Schrödinger theory satisfies robustness.  Currently, I can only show
that the matrix @xmath is robust to exponentially small perturbations,
not polynomially small ones.  The problem is that if any row or column
sum in the @xmath matrix is extremely small, then the @xmath -scaling
process will magnify tiny errors in the entries.  Intuitively, though,
this effect should be washed out by later scaling steps.

A second open problem is whether there exists a theory that satisfies
indifference, as well as commutativity for all separable mixed states
(not just separable pure states).  A third problem is to investigate
other notions of robustness—for example, robustness to small
multiplicative rather than additive errors.

On the complexity side, perhaps the most interesting problem left open
by this chapter is the computational complexity of simulating Bohmian
mechanics.  I strongly conjecture that this problem, like the
hidden-variable problems we have seen, is strictly harder than
simulating an ordinary quantum computer.  The trouble is that Bohmian
mechanics does not quite fit in the framework of this chapter: as
discussed in Section 59.2 , we cannot have deterministic hidden-variable
trajectories for discrete degrees of freedom such as qubits.  Even
worse, Bohmian mechanics violates the continuous analogue of the
indifference axiom.  On the other hand, this means that by trying to
implement (say) the juggle subroutine with Bohmian trajectories, one
might learn not only about Bohmian mechanics and its relation to quantum
computation, but also about how essential the indifference axiom really
is for our implementation.

Another key open problem is to show better upper bounds on @xmath .
 Recall that I was only able to show @xmath , by giving a classical
exponential-time algorithm to simulate the flow theory @xmath .  Can we
improve this to (say) @xmath ?  Clearly it would suffice to give a
@xmath algorithm that computes the transition probabilities for some
theory @xmath satisfying the indifference and robustness axioms.  On the
other hand, this might not be necessary —that is, there might be an
indirect simulation method that does not work by computing (or even
sampling from) the distribution over histories.  It would also be nice
to pin down the complexities of simulating specific hidden-variable
theories, such as @xmath and @xmath .

### Chapter \thechapter Summary of Part Ii

Recall our hypothetical visitor from Conway’s Game of Life, on a
complexity safari of the physical universe.  Based on the results in
Part II , the following are some intuitions about efficient computation
that I would advise our visitor to toss in the garbage.

-   We can be fairly confident that the class of functions efficiently
    computable in the physical world coincides with @xmath (or @xmath ,
    which is presumably equal).

-   Although there are models of efficient computation more powerful
    than @xmath , involving the manipulation of arbitrary real or
    complex numbers, these models will inevitably blow up small errors
    in the numbers nonlinearly, and must be therefore be unphysical.

-   A robot, moving at unit speed, would need order @xmath steps to
    search a spatial region of size @xmath for a marked item.

-   The ability to see one’s entire “history” in a single time step
    cannot yield any complexity-theoretic advantage, since one could
    always just record the history as one went along, at the cost of a
    polynomial increase in memory.

On the other hand, just as in Part I , we have seen that many of the
intuitions in our visitor’s suitcase are good to go.  For example:

-   If the items in a database have distance @xmath from one another,
    then the time needed to search the database is about @xmath times
    what it would be if the items had unit distance from one another.

-   It is possible to choose a probability distribution over histories,
    in such a way that state @xmath is never followed in a history by
    state @xmath if the corresponding transition probability is zero,
    and such that a small change to the transition matrices produces
    only a small change in the history distribution.

-   If, at the moment of your death, your entire life’s history flashed
    before you in an instant, then you could probably still not solve
    @xmath -complete problems in polynomial time.