##### Contents

-    1 Introduction and preliminaries
    -    1.1 Introduction
    -    1.2 Preliminaries
        -    1.2.1 Complexity
        -    1.2.2 Probabilistic algorithms
        -    1.2.3 Straight line programs
        -    1.2.4 Solving polynomial equations
        -    1.2.5 Orders of invertible matrices
        -    1.2.6 Random group elements
        -    1.2.7 Constructive recognition overview
        -    1.2.8 Constructive membership testing overview
        -    1.2.9 CGT methods
        -    1.2.10 Aschbacher classes
        -    1.2.11 Conjectures
-    2 Twisted exceptional groups
    -    2.1 Suzuki groups
        -    2.1.1 Definition and properties
        -    2.1.2 Alternative definition
        -    2.1.3 Tensor indecomposable representations
    -    2.2 Small Ree groups
        -    2.2.1 Definition and properties
        -    2.2.2 Alternative definition
        -    2.2.3 Tensor indecomposable representations
    -    2.3 Big Ree groups
        -    2.3.1 Definition and properties
        -    2.3.2 Tensor indecomposable representations
-    3 Constructive recognition and membership testing
    -    3.1 Suzuki groups
        -    3.1.1 Recognition
        -    3.1.2 Finding an element of a stabiliser
        -    3.1.3 Constructive membership testing
        -    3.1.4 Conjugates of the standard copy
        -    3.1.5 Tensor decomposition
        -    3.1.6 Constructive recognition
    -    3.2 Small Ree groups
        -    3.2.1 Recognition
        -    3.2.2 Finding an element of a stabiliser
        -    3.2.3 Constructive membership testing
        -    3.2.4 Conjugates of the standard copy
        -    3.2.5 Tensor decomposition
        -    3.2.6 Symmetric square decomposition
        -    3.2.7 Constructive recognition
    -    3.3 Big Ree groups
        -    3.3.1 Recognition
        -    3.3.2 Finding elements of even order
        -    3.3.3 Constructive membership testing
        -    3.3.4 Conjugates of the standard copy
        -    3.3.5 Constructive recognition
-    4 Sylow subgroups
    -    4.1 Suzuki groups
    -    4.2 Small Ree groups
    -    4.3 Big Ree groups
-    5 Maximal subgroups
    -    5.1 Suzuki groups
    -    5.2 Small Ree groups
    -    5.3 Big Ree groups
-    6 Implementation and performance
    -    6.1 Suzuki groups
    -    6.2 Small Ree groups
    -    6.3 Big Ree groups

###### List of Figures

-    6.1 Benchmark of Suzuki stabiliser computation
-    6.2 Benchmark of Suzuki conjugation
-    6.3 Benchmark of small Ree conjugation
-    6.4 Benchmark of Large Ree conjugation

## Acknowledgements

I would first of all like to thank my supervisor, Charles Leedham-Green,
for endless help and encouragement. I would also like to thank the
following people, since they have helped me to various extent during the
work presented in this thesis: John Bray, Peter Brooksbank, John Cannon,
Sergei Haller, Derek Holt, Alexander Hulpke, Bill Kantor, Ross Lawther,
Martin Liebeck, Klaus Lux, Frank Lübeck, Scott Murray, Eamonn O’Brien,
Geoffrey Robinson, Colva Roney-Dougal, Alexander Ryba, Ákos Seress,
Leonard Soicher, Mark Stather, Bill Unger, Maud de Visscher, Robert
Wilson.

## Notation

-   Algebraic closure of the field @xmath

-   conjugate of @xmath by @xmath , i.e. @xmath

-   commutator of @xmath and @xmath , i.e. @xmath

-   trace of the matrix @xmath

-   cyclic group of order @xmath , i.e. @xmath

-   finite field of size @xmath (or its additive group)

-   multiplicative group of @xmath

-   The number of field operations required by a random element oracle
    for @xmath .

-   The number of field operations required by a random element oracle
    for @xmath with @xmath constant

-   number of field operations required by a discrete logarithm oracle
    in @xmath

-   number of field operations required by an integer factorisation
    oracle (which factorises @xmath , for @xmath )

-   matrix algebra of @xmath matrices over ring @xmath

-   identity @xmath matrix

-   matrix with @xmath in position @xmath and @xmath elsewhere

-   order of a group element @xmath

-   Euler totient function

-   number of positive divisors of @xmath (where @xmath )

-   Frobenius automorphism in a field @xmath ( i.e. @xmath where @xmath
    )

-   Frattini subgroup of @xmath (intersection of all maximal subgroups
    of @xmath )

-   largest normal @xmath -subgroup of @xmath

-   stabiliser in @xmath of the point @xmath

-   derived subgroup (commutator subgroup) of @xmath

-   normaliser of @xmath in @xmath

-   centraliser of @xmath in @xmath

-   centre of @xmath

-   algebra of @xmath -endomorphisms of @xmath -module @xmath , where
    @xmath

-   automorphism group of @xmath -module @xmath (if @xmath then @xmath )

-   symmetric square of module @xmath over a field @xmath (where @xmath
    if @xmath )

-   exterior square of module @xmath

-   symmetric group on the set @xmath

-   symmetric group on @xmath points

-   dihedral group of order @xmath

-   projective space corresponding to vector space @xmath

-   @xmath -dimensional projective space over the field @xmath

-   index of @xmath in @xmath

-   extension of @xmath by @xmath (a group @xmath such that @xmath and
    @xmath )

-   split extension of @xmath by @xmath (as with @xmath but @xmath also
    has a subgroup @xmath such that @xmath and @xmath )

-   standard time complexity notation

-   Automorphisms defining @xmath or @xmath

-   group homomorphisms

## Chapter 1 Introduction and preliminaries

### 1.1. Introduction

This thesis contains algorithms for some computational problems
involving a few classes of the finite simple groups. The main focus is
on providing efficient algorithms for constructive recognition and
constructive membership testing, but we also consider the conjugacy
problem for Sylow and maximal subgroups.

The work is in the area of computational group theory (abbreviated CGT),
where one studies algorithmic aspects of groups, or solves group
theoretic problems using computers. A group can be represented as a data
structure in several ways, and perhaps the most important ones are
permutation groups , matrix groups and finitely presented groups .

The permutation group setting has been studied since the early 1970’s,
and the basic technique which underlies most algorithms is the
construction of a base and a strong generating set . If @xmath is a
permutation group of degree @xmath , this involves constructing a
descending chain of subgroups of @xmath , where each subgroup in the
chain has index at most @xmath in its predecessor. The permutation group
algorithm machinery is summarised in [ Ser03 ] .

For matrix groups, the classical method is also to construct a base and
strong generating set. However, in general a matrix group has no
faithful permutation representation whose degree is polynomial in the
size of the input. Hence the indices in the subgroup chain will be too
large and the permutation group algorithms will not be efficient. For
example, @xmath has no proper subgroup of index less than @xmath , and
@xmath is exponential in the size of the input, since a matrix has size
@xmath .

Historically there were two schools within CGT. One consists of people
with a more computational complexity background, whose primary goal was
to find theoretically good (polynomial time) algorithms. The
implementation and practical performance of the algorithms were less
important, since the computational complexity view, based on many real
examples, is that if the “polynomial barrier” is broken, then further
research will surely lead also to good practical algorithms. The other
school consists of people with a more group theoretic background, whose
primary goal was to solve computational problems in group theory
(historically often one-time problems with the sporadic groups), and
hence to develop algorithms that can be easily implemented and that run
fast on the current hardware and on the specific input in question. The
asymptotic complexity of the algorithms was less important, and perhaps
did not even make sense in the case of sporadic groups.

The distinction between these schools has become less noticeable during
the last @xmath years, but during that time there has also been much
work on algorithms for matrix groups, and there are two main approaches
that roughly correspond to these two schools. The first approach is the
“black box” approach, that considers the matrix groups as black box
groups (see [ Ser03 , pp. 17] ). This was initiated by [ Luk92 ] (but
goes back to [ BS84 ] ) and much of it is summarised in [ BB99 ] . The
other approach is the “geometric” approach, also known as The Matrix
Group Recognition Project (abbreviated MGRP), whose underlying idea is
to use a famous theorem of Aschbacher [ Asc84 ] which roughly says that
a matrix group either preserves some geometric structure, or is a simple
group.

Although the author has a background perhaps more in the computational
complexity school, the work in this thesis forms a part of MGRP. The
first specific goal in that approach is to obtain an efficient algorithm
that finds a composition series of a matrix group. There exists a
recursive algorithm [ LG01 ] for this, which relies on a number of other
algorithms that determine which kind of geometric structure is
preserved, and the base cases in the recursion are the classes of finite
simple groups.

Hence this algorithm reduces the problem of finding a composition series
to various problems concerning the composition factors of the matrix
group, which are simple groups. The work presented here is about
computing with some of these simple groups.

For each simple group, a number of problems arise. The simple group is
given as @xmath for some @xmath and we need to consider the following
problems:

1.  The problem of recognition or naming of @xmath , i.e. decide the
    name of @xmath , as in the classification of the finite simple
    groups.

2.  The constructive membership problem. Given @xmath , decide whether
    or not @xmath , and if so express @xmath as a straight line program
    in @xmath .

3.  The problem of constructive recognition . Construct an isomorphism
    @xmath from @xmath to a standard copy @xmath of @xmath such that
    @xmath can be computed efficiently for every @xmath . Such an
    isomorphism is called effective . Also of interest is to construct
    an effective inverse of @xmath , which essentially is constructive
    membership testing.

To find a composition series using [ LG01 ] , the problems involving the
composition factors that we need to solve are naming and constructive
membership. However, the effective isomorphisms of these composition
factors to standard copies can also be very useful. Given such
isomorphisms, many problems, sometimes including constructive
membership, can be reduced to the standard copies. Hence these
isomorphisms play a central role in computing with a matrix group once a
composition series has been constructed.

In general, the constructive recognition problem is computationally
harder than the constructive membership problem, which in turn is harder
than the naming problem. Most of our efforts will therefore go towards
solving constructive recognition. In fact, the naming problem is not
considered in every case here, since the algorithm of [ BKPS02 ] solves
this problem. However, that algorithm is a Monte Carlo algorithm, and in
some cases we can improve on that and provide Las Vegas algorithms (see
Section 1.2.2 ).

The algorithms presented here are not black box algorithms, but rely
heavily on the fact that the group elements are matrices, and use the
representation theory of the groups in question. However, the algorithms
will work with all possible representations of the groups, so a user of
the algorithms can consider them black box in that sense.

### 1.2. Preliminaries

We now give preliminary discussions and results that will be necessary
later on.

#### 1.2.1. Complexity

We shall be concerned with the time complexity of the algorithms
involved, where the basic operations are the field operations, and not
the bit operations. All simple arithmetic with matrices can be done
using @xmath field operations, and raising a matrix to the @xmath power
can be done using @xmath field operations using [ CLG97 ] . These
complexity bounds arise when using the naive matrix multiplication
algorithm, which uses @xmath field operations to multiply two @xmath
matrices. More efficient algorithms for matrix multiplication do exist.
Some are also fast in practice, like the famous algorithm of Strassen [
Str69 ] , which uses @xmath field operations. Currently, the most
efficient matrix multiplication algorithm is the Coppersmith-Winograd
algorithm of [ CW90 , CKSU05 ] , which uses @xmath field operations, but
it is not practical. The improvements made by these algorithms over the
naive matrix multiplication algorithm are not noticeable in practice for
the matrix dimensions that are currently within range in the MGRP.
Therefore we will only use the naive algorithm, which also simplifies
the complexity statements.

When we are given a group @xmath defined by a set @xmath of generators,
the size of the input is @xmath . A field element takes up @xmath space,
and a matrix has @xmath entries.

We shall often assume an oracle for the discrete logarithm problem in
@xmath (see [ vzGG03 , Section @xmath ] and [ Shp99 , Chapter @xmath ]
). In the general discrete logarithm problem, we consider a cyclic group
@xmath of order @xmath . The input is a generator @xmath of @xmath , and
some @xmath . The task is to find @xmath such that @xmath . In @xmath
the multiplicative group @xmath is cyclic, and the discrete logarithm
problem turns up. It is a famous and well-studied problem in theoretical
computer science and computational number theory, and it is unknown if
it is @xmath -complete or if it is in @xmath , although the latter would
be very surprising. Currently the most efficient algorithm has
sub-exponential complexity. There are also algorithms for special cases,
and an important case for us is when @xmath . Then we can use
Coppersmith’s algorithm of [ Cop84 , GM93 ] , which is much faster in
practice than the general algorithms. It is not polynomial time, but has
time complexity @xmath , where @xmath is a small constant. We shall
assume that the discrete logarithm oracle in @xmath uses @xmath field
operations.

Similarly we will sometimes assume an oracle for the integer
factorisation problem (see [ vzGG03 , Chapter @xmath ] ), whose status
in the complexity hierarchy is similar to the discrete logarithm
problem. More precisely, we shall assume we have an oracle that, given
@xmath and @xmath , factorises all the integers @xmath for @xmath ,
using @xmath field operations. By [ BB99 , Theorem @xmath ] , assuming
the Extended Riemann Hypothesis this is equivalent to the standard
integer factorisation problem. The reason for having this slightly
different factorisation oracle will become clear in Section 1.2.5 .

Except for these oracles, our algorithms will be polynomial time, so
from a computational complexity perspective our results will imply that
the problems we study can be reduced to the discrete logarithm problem
or the integer factorisation problem. This is in line with MGRP, whose
goal from a complexity point of view is to prove that computations with
matrix groups are not harder than (and hence equally hard as) these two
well-known problems.

#### 1.2.2. Probabilistic algorithms

The algorithms we consider are probabilistic of the types known as Monte
Carlo or Las Vegas algorithms. These types of algorithms are discussed
in [ Ser03 , Section 1.3] and [ HEO05 , Section 3.2.1] . In short, a
Monte Carlo algorithm for a language @xmath is a probabilistic algorithm
with an input parameter @xmath such that on input @xmath

-   if @xmath then the algorithm returns true with probability at least
    @xmath (otherwise it returns false ),

-   if @xmath then the algorithm returns false .

The parameter @xmath is therefore the maximum error probability. This
type of algorithm is also called one-sided Monte Carlo algorithm with no
false negatives . The languages with such algorithms form the complexity
class @xmath . In the same way one can define algorithms with no false
positives, and the corresponding languages form the class @xmath . The
class @xmath consists of the languages that have Las Vegas algorithms,
and these are the type of algorithms that we will be most concerned
with. A Las Vegas algorithm either returns failure , with probability at
most @xmath , or otherwise returns a correct result. Such an algorithm
is easily contructed given a Monte Carlo algorithm of each type. The
time complexity of a Las Vegas algorithm naturally depends on @xmath .

Las Vegas algorithms can be presented concisely as probabilistic
algorithms that either return a correct result, with probability bounded
below by @xmath for some polynomial @xmath in the size @xmath of the
input, or otherwise return failure . By enclosing such an algorithm in a
loop that iterates @xmath times, we obtain an algorithm that returns
failure with probability at most @xmath , and hence is a Las Vegas
algorithm in the above sense. Clearly if the enclosed algorithm is
polynomial time, the Las Vegas algorithm is polynomial time.

One can also enclose the algorithm in a loop that iterates until the
algorithm returns a correct result, thus obtaining a probabilistic time
complexity, and the expected number of iterations is then @xmath . This
is the way we present Las Vegas algorithms since it is the one that is
closest to how the algorithm is used in practice.

#### 1.2.3. Straight line programs

For constructive membership testing, we want to express an element of a
group @xmath as a word in @xmath . Actually, it should be a straight
line program , abbreviated to @xmath . If we express the elements as
words, the length of the words might be too large, requiring exponential
space complexity.

An @xmath is a data structure for words, which ensures that during
evaluation, subwords occurring multiple times are not computed more
often than during construction. Often we want to express an element as
an @xmath in order to obtain its homomorphic image in another group
@xmath where @xmath . The evaluation time for the @xmath is then bounded
by the time to construct it times the ratio of the time required for a
group operation in @xmath and in @xmath .

Formally, given a set of generators @xmath , an @xmath is a sequence
@xmath where each @xmath represents one of the following

-   an @xmath

-   a product @xmath , where @xmath

-   a power @xmath where @xmath and @xmath

-   a conjugate @xmath where @xmath

so @xmath is either a pointer into @xmath , a pair of pointers to
earlier elements of the sequence, or a pointer to an earlier element and
an integer.

To construct an @xmath for a word, one starts by listing pointers to the
generators of @xmath , and then builds up the word. To evaluate the
@xmath , go through the sequence and perform the specified operations.
Since we use pointers to the elements of @xmath , we can immediately
evaluate the @xmath on @xmath , by just changing the pointers so that
they point to elements of @xmath .

#### 1.2.4. Solving polynomial equations

One of the main themes in this work is that we reduce search problems in
computational group theory to the problem of solving polynomial
equations over finite fields. The method we use to find the solutions of
a system of polynomial equations is the classical resultant technique,
described in [ vzGG03 , Section @xmath ] . For completeness, we also
state the corresponding result for univariate polynomials.

###### Theorem 1.1.

Let @xmath have degree @xmath . There exists a Las Vegas algorithm that
finds all the roots of @xmath that lie in @xmath . The expected time
complexity is @xmath field operations.

###### Proof.

Immediate from [ vzGG03 , Corollary 14.16] . ∎

###### Theorem 1.2.

Let @xmath be such that the ideal @xmath is zero-dimensional. Let @xmath
. There exists a Las Vegas algorithm that finds the corresponding affine
variety @xmath . The expected time complexity is @xmath field
operations.

###### Proof.

Following [ vzGG03 , Section @xmath ] , we compute @xmath pairwise
resultants of the @xmath with respect to @xmath to obtain @xmath
univariate polynomials in @xmath . By [ vzGG03 , Theorem @xmath ] , the
expected time complexity will be @xmath field operations and the
resultants will be non-zero since the ideal is zero-dimensional.

We can find the set @xmath of roots of the first polynomial, then find
the roots @xmath of the second polynomial and simultaneously find @xmath
. By continuing in the same way we can find the set @xmath of common
roots of the resultants. Since their degrees will be @xmath , by Theorem
1.1 we can find @xmath using @xmath field operations. Clearly @xmath .

We then substitute each @xmath into the @xmath polynomials and obtain
univariate polynomials @xmath . These will have degrees @xmath and as
above we find the set @xmath of their common roots using @xmath field
operations. Clearly @xmath and hence we can find @xmath using @xmath
field operations. Thus the time complexity is as stated. ∎

The following result is a generalisation of the previous result, and we
omit the proof, since it is complicated and outside the scope of this
thesis.

###### Theorem 1.3.

Let @xmath be such that the ideal @xmath is zero-dimensional. Let @xmath
. There exists a Las Vegas algorithm that finds the corresponding affine
variety @xmath . The expected time complexity is @xmath field
operations.

As can be seen, if the number of equations, the number of variables and
the degree of the equations are constant, then the variety of a
zero-dimensional ideal can be found in polynomial time. That is the
situation we will be most concerned with. As an alternative to the
resultant technique, one can compute a Gröbner basis and then find the
variety. By [ LL91 ] , if the ideal is zero-dimensional the time
complexity is @xmath where @xmath is the maximum degree of the
polynomials and @xmath is the number of variables. Hence in the
situation above this will be polynomial time.

#### 1.2.5. Orders of invertible matrices

The order of a group element @xmath is the smallest @xmath such that
@xmath . We denote the order of @xmath by @xmath . For elements @xmath
of a matrix group @xmath , an algorithm for finding @xmath is presented
in [ CLG97 ] . In general, to obtain the precise order, this algorithm
requires a factorisation of @xmath for @xmath , otherwise it might
return a multiple of the correct order. Therefore it depends on the
presumably difficult problem of integer factorisation, see [ vzGG03 ,
Chapter @xmath ] .

However, in most of the cases we will consider, it will turn out that a
multiple of the correct order will be sufficient. For example, at
certain points in our algorithms we shall be concerned with finding
elements of order dividing @xmath . Hence if we use the above algorithm
to find the order of @xmath and it reports that @xmath , this is
sufficient for us to use @xmath , even though we might have @xmath
properly dividing @xmath . Hence integer factorisation is avoided in
this case.

In [ BB99 , Section @xmath ] the concept of pseudo-order is defined. A
pseudo-order of an element @xmath is a product of primes and
pseudo-primes. A pseudo-prime is a composite factor of @xmath for some
@xmath that cannot conveniently be factorised. The order of @xmath is a
factor of the pseudo-order in which each pseudo-prime is replaced by a
non-identity factor of that pseudo-prime. Hence a pseudo-prime is not a
multiple of a “known” prime, and any two pseudo-primes are relatively
prime.

The algorithm of [ CLG97 ] can also be used to obtain a pseudo-order,
and for this it has time complexity @xmath field operations. In fact,
the algorithm computes the order factorised into primes and
pseudo-primes. However, even if we have just the pseudo-order, we can
still determine if a given prime divides the order, without integer
factorisation.

###### Proposition 1.4.

Let @xmath . There exists a Las Vegas algorithm that, given @xmath , a
prime @xmath and @xmath , determines if @xmath and if so finds the power
of @xmath of order @xmath , using @xmath field operations.

###### Proof.

Use [ CLG97 ] to find a pseudo-order @xmath of @xmath . Assume that
@xmath where @xmath , and @xmath where @xmath , @xmath and @xmath .
Since @xmath is given, [ CLG97 ] will make sure that @xmath , so we
assume that this is the case. Moreover, from [ CLG97 ] we see @xmath .
If a prime @xmath divides @xmath we must have @xmath . So if @xmath we
must have @xmath and hence @xmath . Thus @xmath .

Hence @xmath , @xmath and @xmath . Therefore @xmath if and only if
@xmath . Then @xmath has order @xmath . ∎

#### 1.2.6. Random group elements

Our analysis assumes that we can construct uniformly (or nearly
uniformly) distributed random elements of a group @xmath . The algorithm
of [ Bab91 ] produces independent nearly uniformly distributed random
elements, but it is not a practical algorithm. It has a preprocessing
step with time complexity @xmath group operations, and each random
element is then found using @xmath group operations.

A more commonly used algorithm is the product replacement algorithm of [
CLGM @xmath 95 ] . It also consists of a preprocessing step, which is
polynomial time by [ Pak00 ] , and each random element is then found
using a constant number of group operations (usually @xmath ). This
algorithm is practical and included in and . Most of the theory about it
is summarised in [ Pak01 ] . For a discussion of both these algorithms,
see [ Ser03 , pp. 26-30] .

We shall assume that we have a random element oracle, which produces a
uniformly random element of @xmath using @xmath field operations, and
returns it as an @xmath in @xmath .

An important issue is the length of the @xmath s that are computed. The
length of the @xmath s must be polynomial, otherwise it would not be
polynomial time to evaluate them. We assume that @xmath s of random
elements have length @xmath where @xmath is the number of random
elements that have been selected so far during the execution of the
algorithm.

In [ LGM02 ] , a variant of the product replacement algorithm is
presented that finds random elements of the normal closure of a
subgroup. This will be used here to find random elements of the derived
subgroup of a group @xmath , using the fact that this is precisely the
normal closure of @xmath .

#### 1.2.7. Constructive recognition overview

If @xmath is an @xmath -module for some group @xmath and field @xmath ,
with action @xmath , and if @xmath is an automorphism of @xmath , denote
by @xmath the @xmath -module which has the same elements as @xmath and
where the action is given by @xmath for @xmath and @xmath , extended to
@xmath by linearity. We call @xmath a twisted version of @xmath , or
@xmath twisted by @xmath . If @xmath is a matrix group and the
automorphism @xmath is a field automorphism, we call it a Galois twist .

From [ HEO05 , Section @xmath ] we know that @xmath preserves a
classical (non-unitary) form if and only if @xmath is isomorphic to its
dual. We shall use this fact occasionally.

When we say that an algorithm is “given a group @xmath ”, then the
generating set @xmath is fixed and known to the algorithm. In other
words, the algorithm is given the generating set @xmath and will operate
in @xmath .

###### Definition 1.5.

Let @xmath be matrix groups. An isomorphism @xmath is effective if there
exists a polynomial time Las Vegas algorithm that computes @xmath for
any given @xmath .

Of course, an effective isomorphism might be deterministic, since @xmath
.

###### Definition 1.6.

The problem of constructive recognition is:

-   A matrix group @xmath with standard copy @xmath .

-   An effective isomorphism @xmath , such that @xmath is also
    effective.

Now consider an exceptional group with standard copy @xmath , where
@xmath has characteristic @xmath . The standard copies of the
exceptional groups under consideration will be defined in Chapter 2 .
Our algorithms should be able to constructively recognise any input
group @xmath that is isomorphic to @xmath .

The assumptions that we make on the input group @xmath are:

1.  @xmath acts absolutely irreducibly on @xmath ,

2.  @xmath is written over the minimal field modulo scalars,

3.  @xmath is known to be isomorphic to @xmath , and hence @xmath and
    @xmath are known.

A user of our algorithms can easily first apply the algorithms of [ HR94
] and [ GLGO06 ] , described in Sections 1.2.10 and 1.2.10 , to make the
group satisfy the first two assumptions. The last two assumptions remove
much of the need for input verification using non-explicit recognition.
They are motivated by the context in which our algorithms are supposed
to be used. The idea is that our algorithms will serve as a base case
for the algorithm of [ LG01 ] or a similar algorithm. In the base case
it will be known that the group under consideration is almost simple
modulo scalars. We can then assume that the algorithm can decide if it
is dealing with a group of Lie type. Then it can use the Monte Carlo
algorithm of [ OL05 ] to determine the defining characteristic of the
group, and next use the Monte Carlo algorithm of [ BKPS02 ] to determine
the name of the group, as well as the defining field size @xmath . This
standard machinery motivates our assumptions. Because the group has only
been identified by a Monte Carlo algorithm, there is a small non-zero
probability that our algorithms might be executed on the wrong group.
This has to be kept in mind when implementing the algorithms.

We do not assume that the input is tensor indecomposable, since the
tensor decomposition algorithm described in Section 1.2.10 is not
polynomial time.

A number of different cases arise:

1.  @xmath where @xmath has characteristic @xmath . This is called the
    cross characteristic case. Then [ LS74 ] and [ SZ93 ] tells us that
    @xmath for some polynomial @xmath . This means that @xmath is
    polynomial in the size of the input, which is not the case in
    general. In this case we can therefore use algorithms which normally
    are exponential time. In particular, by [ BB99 , Theorem 8.6] we can
    use the classical permutation group methods. Therefore we will only
    consider the case when we are given a group in defining
    characteristic , so that @xmath .

2.  @xmath where @xmath and @xmath . Let @xmath be the module of @xmath
    . If @xmath is isomorphic to a tensor product of two modules which
    both have dimension less than @xmath , then we say that @xmath is
    tensor decomposable . Otherwise @xmath is tensor indecomposable .

    Every possible @xmath is isomorphic to a tensor product of twisted
    versions of tensor indecomposable modules of @xmath (and hence of
    @xmath ). By the Steinberg tensor product theorem of [ Ste63 ] , in
    our cases the twists are Galois twists and the number of tensor
    indecomposable modules is independent of the field size, up to
    twists.

    If @xmath is tensor decomposable, we want to construct a tensor
    indecomposable representation @xmath of @xmath . In general, this is
    done using the tensor decomposition algorithm described in Section
    1.2.10 on @xmath , which also provides an effective isomorphism from
    @xmath to @xmath ( i.e. between their acting groups). But since the
    algorithm is not polynomial time, a special version of one its
    subroutines has to be provided for each exceptional group.

    If @xmath is tensor indecomposable, we want to construct a
    representation @xmath of @xmath of dimension @xmath , and we want do
    it in a way that also constructs an effective isomorphism. In
    principle this is always possible by computing tensor products of
    @xmath and chopping them with the MeatAxe, because a composition
    factor of dimension @xmath will always turn up. However, this is not
    always a practical algorithm, and the time complexity is not very
    good.

    Note that if the minimal field @xmath is a proper subfield of @xmath
    , then the tensor decomposition will not succeed. Since we assume
    that we know @xmath , we can embed @xmath canonically into an @xmath
    -module. In this case we shall therefore always assume that @xmath ,
    contrary to our second assumption above.

3.  @xmath , so that, by [ Ste63 ] , @xmath is conjugate to @xmath in
    @xmath . This is the most interesting case since there are no
    standard methods, and we shall devote much effort to this case for
    the exceptional groups that we consider. A central issue will be to
    find elements of order a multiple of @xmath . This is a serious
    obstacle since by [ IKS95 , GL01 ] , the proportion @xmath of these
    elements in @xmath satisfies

      -- -------- -- -------
         @xmath      (1.1)
      -- -------- -- -------

    Hence we cannot find elements of order a multiple of @xmath by
    random search in polynomial time, so there is no straightforward way
    to find them.

To be able to deal with these various cases, we need to know all the
absolutely irreducible tensor indecomposable representations of @xmath
in defining characteristic. We also need to know how they arise from the
natural representation , which is the representation of dimension @xmath
over @xmath . In our cases, this information is provided by [ Lüb01 ] .

#### 1.2.8. Constructive membership testing overview

The other computational problem that we shall consider is the following.

###### Definition 1.7.

The problem of constructive membership testing is:

-   A matrix group @xmath , an element @xmath .

-   If @xmath , then true and an @xmath for @xmath in @xmath , false
    otherwise.

In our cases, @xmath is always taken to be the general linear group. One
can take two slightly different approaches to the problem of expressing
an element as an @xmath in the given generators, depending on whether
one wants to find an effective isomorphism or find standard generators.

1.  The approach using an effective isomorphism.

    1.  Given @xmath with standard copy @xmath , first solve
        constructive recognition and obtain an effective isomorphism
        @xmath . Hence obtain a generating set @xmath of @xmath .

    2.  Given @xmath , express @xmath as an @xmath in @xmath , hence
        also expressing @xmath in @xmath .

2.  The approach using standard generators.

    1.  Given @xmath with standard copy @xmath , find @xmath as @xmath s
        in @xmath , such that the mapping @xmath is an isomorphism.

    2.  Given @xmath , express @xmath as an @xmath in @xmath , hence
        also expressing it in @xmath .

In the first case, the constructive membership testing takes place in
@xmath , which is probably faster than in @xmath , so in this case we
use the standard copy in computations. In the second case, the standard
copy is only used as a theoretical tool. As it stands, the first
approach is stronger, since it provides the effective isomorphism, and
the standard generators in @xmath can be obtained in the first approach,
if necessary. However, if the representation theory of @xmath is known,
so that we can construct a module isomorphic to the module of @xmath
from the module of @xmath , then the standard generators can be used,
together with the MeatAxe, to solve constructive recognition. Hence the
two approaches are not very different. One can also mix them in various
ways, for example in the first case by finding standard generators in
@xmath expressed in @xmath , and then only express each element in the
standard generators, which might be easier than to express the elements
directly in @xmath .

#### 1.2.9. CGT methods

Here we describe some algorithmic methods that we will use. Like many
methods in CGT they are not really algorithms ( i.e. they may not
terminate on all inputs), or if they are they have very bad (worst-case)
time complexity. Nevertheless, they can be useful for particular groups,
as in our cases.

##### The dihedral trick

This trick is a method for conjugating involutions ( i.e. elements of
order @xmath ) to each other in a black-box group, defined by a set of
generators. The nice feature is that if the involutions are given as
straight line programs in the generators, the conjugating element will
be found as a straight line program. The dihedral trick is based on the
following observation.

###### Proposition 1.8.

Let @xmath be a group and let @xmath be involutions such that @xmath for
some @xmath . Then @xmath conjugates @xmath to @xmath .

###### Proof.

Observe that

  -- -------- --
     @xmath   
  -- -------- --

since @xmath and @xmath are involutions. ∎

###### Theorem 1.9 (The dihedral trick).

Let @xmath . Assume that the probability of the product of two random
conjugate involutions in @xmath having odd order is at least @xmath .
There exists a Las Vegas algorithm that, given conjugate involutions
@xmath , finds @xmath such that @xmath . If @xmath are given as @xmath s
of lengths @xmath , then @xmath will be found as an @xmath of length
@xmath . The algorithm has expected time complexity @xmath field
operations.

###### Proof.

The algorithm proceeds as follows:

1.  Find random @xmath and let @xmath .

2.  Let @xmath . Use Proposition 1.4 to determine if @xmath has even
    order, and if so, return to the first step.

3.  Let @xmath and let @xmath .

By Proposition 1.8 , this is a Las Vegas algorithm. The probability that
@xmath has odd order is @xmath and hence the expected time complexity is
as stated. Note that if @xmath and @xmath are given as @xmath s in
@xmath , then we obtain @xmath as an @xmath in @xmath . ∎

##### Involution centralisers

In [ HLO @xmath 06 ] an algorithm is described that reduces the
constructive membership problem in a group @xmath to the same problem in
three involution centralisers in @xmath . The reduction algorithm is
known as the Ryba algorithm and can be a convenient method to solve the
constructive membership problem. However, there are obstacles:

1.  We have to solve the constructive membership problem in the
    involution centralisers of @xmath . In principle this can be done
    using the Ryba algorithm recursively, but such a blind descent might
    not be very satisfactory. For instance, it might not be easy to
    determine the time complexity of such a procedure. Another approach
    is to provide a special algorithm for the involution centraliser.
    This assumes that the structure of @xmath and its involution
    centralisers are known, which it will be in the cases we consider.

2.  We have to find involutions in @xmath . As described in Section
    1.2.7 , this is a serious obstacle if the defining field @xmath of
    @xmath has characteristic @xmath . In odd characteristic the
    situation is better, and in [ HLO @xmath 06 ] it is proved that the
    Ryba algorithm is polynomial time in this case. Another approach is
    to provide a special algorithm that finds involutions.

3.  We have to find generators @xmath for @xmath of a given involution
    @xmath . This is possible using the Bray algorithm of [ Bra00 ] . It
    works by computing random elements of @xmath until the whole
    centraliser is generated. This automatically gives the elements of
    @xmath as @xmath s in @xmath , which is a central feature needed by
    the Ryba algorithm.

    There are two issues involved when using this algorithm. First, the
    generators that are computed may not be uniformly random in @xmath ,
    so that we might have trouble generating the whole centraliser. In [
    HLO @xmath 06 ] it is shown that this is not a problem with the
    exceptional groups. Second, we need to provide an algorithm that
    determines if the whole centraliser has been generated. In the cases
    that we will consider, this will be possible. It should be noted
    that the Bray algorithm works for any black-box group and not just
    for matrix groups.

Given these obstacles, we will still use the Ryba algorithm for
constructive membership testing in some cases. We will also use the Bray
algorithm independently, since it is a powerful tool.

##### The Formula

Like the dihedral trick, this is a method for conjugating elements to
each other. For a group @xmath , denote by @xmath the Frattini subgroup
of @xmath , which is the intersection of the maximal subgroups of @xmath
.

###### Lemma 1.10 (The Formula).

Let @xmath , where @xmath is a @xmath -group and @xmath is odd. If
@xmath have order @xmath and @xmath , then @xmath where @xmath .

###### Proof.

The orders of @xmath are their orders in @xmath . Hence we can replace
@xmath with @xmath without affecting the rest of the assumptions. We can
therefore reduce to the case when @xmath , in other words when @xmath is
elementary abelian.

Then @xmath , @xmath , with @xmath and @xmath . We want to prove that
@xmath where @xmath or equivalently that @xmath .

Now @xmath . We can move all occurrences of @xmath to the right, so that

  -- -------- --
     @xmath   
  -- -------- --

from which we see that

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

and we want these to be equal. Since @xmath , we see that @xmath , and
because @xmath is elementary abelian, they are equal if and only if
their product is the identity. But clearly @xmath , where @xmath , and
finally @xmath . ∎

###### Corollary 1.11.

Let @xmath , where @xmath is a @xmath -group and @xmath is odd, and let
@xmath for some group @xmath . If @xmath has order @xmath then for
@xmath , such that @xmath , we have @xmath , where @xmath .

###### Proof.

Observe that both @xmath and @xmath have order @xmath and lie in @xmath
. Now apply Lemma 1.10 , conclude that @xmath , and the result follows.
∎

##### Recognition of @xmath

In [ CLGO06 ] , an algorithm for constructive recognition and
constructive membership testing of @xmath is presented. This algorithm
is in several aspects the original which our algorithms are modelled
after, and it is in itself an extension of [ CLG01 ] , which handles the
natural representation.

We will use [ CLGO06 ] since @xmath arise as subgroups of some of the
exceptional groups that we consider. Because of this, we state the main
results here. Let @xmath be the number of divisors of @xmath . From [
HW79 , pp. @xmath ] , we know that for every @xmath , if @xmath is
sufficiently large then @xmath .

Here, @xmath is viewed as a quotient of @xmath . Hence the elements are
cosets of matrices.

###### Theorem 1.12.

Assume an oracle for the discrete logarithm problem in @xmath . There
exists a Las Vegas algorithm that, given @xmath satisfying the
assumptions in Section 1.2.7 , with @xmath and @xmath , finds an
effective isomorphism @xmath and performs preprocessing for constructive
membership testing. The algorithm has expected time complexity

  -- -------- --
     @xmath   
  -- -------- --

field operations.

The inverse of @xmath is also effective. Each image of @xmath can be
computed using @xmath field operations, and each pre-image using @xmath
field operations. After the algorithm has run, constructive membership
testing of @xmath uses @xmath field operations, and the resulting @xmath
has length @xmath .

The existence of this constructive recognition algorithm has led to
several other constructive recognition algorithms for the classical
groups [ BK01 , Bro03a , Bro03b , BK06 ] , which are polynomial time
assuming an oracle for constructive recognition of @xmath .

In some situations we shall also need a fast non-constructive
recognition algorithm of @xmath . It will be used to test if a given
subgroup of @xmath is in fact the whole of @xmath , so it is enough to
have a Monte Carlo algorithm with no false positives. We need to use it
in any representation, so the correct context is a black-box group.

###### Theorem 1.13.

Let @xmath . Assume that @xmath is isomorphic to a subgroup of @xmath
and that @xmath is known. There exists a one-sided Monte Carlo algorithm
with no false positives that determines if @xmath . Given maximum error
probability @xmath , the time complexity is

  -- -------- --
     @xmath   
  -- -------- --

field operations, where

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

It is well known that @xmath is generated by two elements having order
dividing @xmath , unless one of them lie in @xmath for some @xmath . We
are thus led to an algorithm that performs at most @xmath steps. In each
step it finds two random elements @xmath and @xmath of @xmath and
computes their pseudo-orders. If @xmath has order dividing @xmath , it
then determines if @xmath lies in some @xmath by testing if @xmath . If
@xmath then the possible values of @xmath are @xmath where @xmath , so
there are @xmath subfields. If @xmath does not lie in any @xmath then
@xmath is remembered.

Now do similarly for @xmath . Then, if it has found the two elements, it
returns true . On the other hand, if it completes the @xmath steps
without finding these elements, it returns false .

The proportion in @xmath of elements of order @xmath is @xmath , so the
probability that @xmath but we fail to find the elements is at most
@xmath . We require @xmath and hence @xmath can be chosen as @xmath . ∎

#### 1.2.10. Aschbacher classes

The Aschbacher classification of [ Asc84 ] classifies matrix groups into
a number of classes, and a major part of the MGRP has been to develop
algorithms that determine constructively if a given matrix group belongs
to a certain class. Some of these algorithms are used here.

##### The MeatAxe

Let @xmath acting on a module @xmath . The algorithm known as the
MeatAxe determines if @xmath is irreducible. If not, it finds a proper
non-trivial submodule @xmath of @xmath , and a change of basis matrix
@xmath that exhibits the action of @xmath on @xmath and on @xmath . In
other words, the first @xmath rows of @xmath form a basis of @xmath ,
and @xmath is block lower triangular for every @xmath .

Applying the MeatAxe recursively, one finds a composition series of
@xmath , and a change of basis that exhibits the action of @xmath on the
composition factors of @xmath . Hence we also obtain effective
isomorphisms from @xmath to the groups acting on the composition factors
of @xmath .

The MeatAxe was originally developed by Parker in [ Par84 ] and later
extended and formalised into a Las Vegas algorithm by Holt and Rees in [
HR94 ] . They also explain how very similar algorithms can be used to
test if @xmath is absolutely irreducible, and if two modules are
isomorphic. These are also Las Vegas algorithms with the same time
complexity as the MeatAxe. The worst case of the MeatAxe is treated in [
IL00 ] , where it is proved that the expected time complexity is @xmath
field operations. Unless the module is reducible and all composition
factors of the module are isomorphic, the expected time complexity is
@xmath field operations.

The MeatAxe is also fast in practice and is implemented both in and .
This important feature is the reason that it is used rather than the
first known polynomial time algorithm for the same problem, which was
given in [ Rón90 ] (and is, at best, @xmath ).

Some related problems are the following:

-   Determine if @xmath acts absolutely irreducibly on @xmath .

-   Given two irreducible @xmath -modules @xmath of dimension @xmath ,
    determine if they are isomorphic, and if so find a change of basis
    matrix @xmath that conjugates @xmath to @xmath .

-   Given that @xmath acts absolutely irreducibly on @xmath , determine
    if @xmath preserves a classical form and if so find a matrix for the
    form.

-   Find a basis for @xmath as matrices of degree @xmath .

Algorithms for these problems are described in [ HEO05 , Section @xmath
] and they all have expected time complexity @xmath field operations. We
will refer to the algorithms for all these problems as “the MeatAxe”.

##### Writing matrix groups over subfields

If @xmath , then @xmath might be conjugate in @xmath to a subgroup of
@xmath where @xmath , so that @xmath is a proper power of @xmath . An
algorithm for deciding if this is case is given in [ GLGO06 ] . It is a
Las Vegas algorithm with expected time complexity @xmath field
operations. In case @xmath can be written over a subfield, then the
algorithm also returns a conjugating matrix @xmath that exhibits this
fact, i.e. so that @xmath can be immediately embedded in @xmath . The
algorithm can also write a group over a subfield modulo scalars.

##### Tensor decomposition

Now let @xmath acting on a module @xmath . The module might have the
structure of a tensor product @xmath , so that @xmath where @xmath and
@xmath .

The Las Vegas algorithm of [ LGO97a ] determines if @xmath has the
structure of a tensor product, and if so it also returns a change of
basis @xmath which exhibits the tensor decomposition. In other words,
@xmath is an explicit Kronecker product for each @xmath . The images of
@xmath in @xmath and @xmath can therefore immediately be extracted from
@xmath , and hence we obtain an effective embedding of @xmath into
@xmath .

By [ LGO97b ] , for tensor decomposition it is sufficient to find a flat
in a projective geometry corresponding to the decomposition. A flat is a
subspace of @xmath of the form @xmath or @xmath where @xmath and @xmath
are proper subspaces of @xmath and @xmath respectively. This flat
contains a point , which is a flat with @xmath or @xmath . If we can
provide a proposed flat to the algorithm of [ LGO97a ] , then it will
verify that it is a flat, and if so find a tensor decomposition, using
expected @xmath field operations.

However, in general there is no efficient algorithm for finding a flat
of @xmath . If we want a polynomial time algorithm for decomposing a
specific tensor product, we therefore have to provide an efficient
algorithm that finds a flat.

#### 1.2.11. Conjectures

Most of the results presented will depend on a few conjectures. This
might be considered awkward and somewhat non-mathematical, but it is a
result of how the work in this thesis was produced. In almost every case
with the algorithms that are presented, the implementation of the
algorithm did exist before the proof of correctness of the algorithm. In
fact, the algorithms have been developed using a rather empirical
method, an interplay between theory (mathematical thought) and practice
(programming). We consider this to be an essential feature of the work,
and it has proven to be an effective way to develop algorithms that are
good in both theory and practice.

However, it has lead to the fact that there are certain results that
have been left unproven, either because they have been too hard to prove
or have been from an area of mathematics outside the scope of this
thesis (usually both). But because of the way the algorithms have been
developed, there should be no doubt that every one of the conjectures
are true. The implementations of the algorithms have been tested on a
vast number of inputs, and therefore the conjectures have also been
tested equally many times. There has been no case of a conjecture
failing.

More detailed information about the implementations can be found in
Chapter 6 .

## Chapter 2 Twisted exceptional groups

Here we will present the necessary theory about the twisted groups under
consideration.

### 2.1. Suzuki groups

The family of exceptional groups now known as the Suzuki groups were
first found by Suzuki in [ Suz60 , Suz62 , Suz64 ] , and also described
in [ HB82 , Chapter @xmath ] which is the exposition that we follow.
They should not be confused with the Suzuki @xmath -groups or the
sporadic Suzuki group.

#### 2.1.1. Definition and properties

We begin by defining our standard copy of the Suzuki group. Following [
HB82 , Chapter @xmath ] , let @xmath for some @xmath and let @xmath be
the unique automorphism of @xmath such that @xmath for every @xmath ,
i.e. @xmath where @xmath . For @xmath and @xmath , define the following
matrices:

  -- -------- -------- -- -------
     @xmath   @xmath      (2.1)
     @xmath   @xmath      (2.2)
     @xmath   @xmath      (2.3)
  -- -------- -------- -- -------

By definition,

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

If we define

  -- -------- -------- -- -------
     @xmath   @xmath      (2.5)
     @xmath   @xmath      (2.6)
  -- -------- -------- -- -------

then @xmath with @xmath and @xmath so that @xmath is cyclic of order
@xmath . Moreover, we can write @xmath as

  -- -------- -- -------
     @xmath      (2.7)
  -- -------- -- -------

where @xmath . Hence @xmath .

The following result follows from [ HB82 , Chapter @xmath ] .

###### Theorem 2.1.

1.   The order of the Suzuki group is

      -- -------- -- -------
         @xmath      (2.8)
      -- -------- -- -------

    and @xmath .

2.  @xmath and hence the three factors in ( 2.8 ) are pairwise
    relatively prime.

3.   For all @xmath and @xmath :

      -- -------- -------- -- --------
         @xmath   @xmath      (2.9)
         @xmath   @xmath      (2.10)
         @xmath   @xmath      (2.11)
         @xmath   @xmath      (2.12)
      -- -------- -------- -- --------

4.   There exists @xmath on which @xmath acts faithfully and doubly
    transitively, such that no nontrivial element of @xmath fixes more
    than @xmath points. This set is

      -- -------- -- --------
         @xmath      (2.13)
      -- -------- -- --------

5.   The stabiliser of @xmath is @xmath and if @xmath then the
    stabiliser of @xmath is @xmath .

6.  @xmath .

7.  @xmath is a Frobenius group with Frobenius kernel @xmath .

8.  @xmath has cyclic Hall subgroups @xmath and @xmath of orders @xmath
    . These act fixed point freely on @xmath and irreducibly on @xmath .
    For each non-trivial @xmath , we have @xmath .

9.   The conjugates of @xmath , @xmath , @xmath and @xmath partition
    @xmath .

10.  The proportion of elements of order @xmath in @xmath is @xmath ,
    where @xmath is the Euler totient function.

###### Remark 2.2 (Standard generators of @xmath).

As standard generators for @xmath we will use

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a primitive element of @xmath , whose minimal polynomial
is the defining polynomial of @xmath . Other sets are possible: in [
Bra07 ] , the standard generators are

  -- -------- --
     @xmath   
  -- -------- --

and uses

  -- -------- --
     @xmath   
  -- -------- --

From [ HB82 , Chapter @xmath , Remark @xmath ] we also immediately
obtain the following result.

###### Theorem 2.3.

A maximal subgroup of @xmath is conjugate to one of the following
subgroups.

1.   The point stabiliser @xmath .

2.   The normaliser @xmath .

3.   The normalisers @xmath for @xmath . These satisfy @xmath where
    @xmath for every @xmath and @xmath .

4.  @xmath where @xmath is a proper power of @xmath .

###### Proposition 2.4.

Let @xmath .

1.   Distinct conjugates of @xmath , @xmath , @xmath or @xmath intersect
    trivially.

2.   The subgroups of @xmath of order @xmath are conjugate, and there
    are @xmath distinct conjugates.

3.   The cyclic subgroups of @xmath of order @xmath are conjugate, and
    there are @xmath distinct conjugates.

4.   The cyclic subgroups of @xmath of order @xmath are conjugate, and
    there are @xmath distinct conjugates.

###### Proof.

1.  By Theorem 2.1 , each conjugate of @xmath fixes exactly one point of
    @xmath . If an element @xmath lies in two distinct conjugates it
    must fix two distinct points and hence lie in a conjugate of @xmath
    . But, by the partitioning, the conjugates of @xmath and @xmath
    intersect trivially, so @xmath .

    If @xmath for some @xmath and @xmath , then @xmath fixes more than
    @xmath points of @xmath , so that @xmath . If @xmath for some @xmath
    and @xmath , then @xmath , so that @xmath .

2.  This is clear since these subgroups are Sylow @xmath -subgroups, and
    hence conjugate to @xmath . Each subgroup fixes a point of @xmath
    and hence there are @xmath distinct conjugates.

3.  Because of the partitioning, an element of order @xmath must lie in
    a conjugate of @xmath , which must be the cyclic subgroup that it
    generates. By Theorem 2.3 there are @xmath distinct conjugates.

4.  Analogous to the previous case.

∎

###### Proposition 2.5.

Let @xmath and let @xmath be the Euler totient function.

1.   The number of elements in @xmath that fix at least one point of
    @xmath is @xmath .

2.   The number of elements in @xmath of order @xmath is @xmath .

3.   The number of elements in @xmath of order @xmath is @xmath .

###### Proof.

1.  By Theorem 2.1 , if @xmath fixes exactly one point, then @xmath is
    in a conjugate of @xmath , and if @xmath fixes two points, then
    @xmath is in a conjugate of @xmath . Hence by Proposition 2.4 ,
    there are @xmath elements that fix exactly one point. Similarly,
    there are @xmath elements that fix exactly two points.

    Thus the number of elements that fix at least one point is

      -- -------- -- --------
         @xmath      (2.14)
      -- -------- -- --------

2.  By Proposition 2.4 , an element of order @xmath must lie in a
    conjugate of @xmath . Since distinct conjugates intersect trivially,
    the number of such elements is the number of generators of all
    cyclic subgroups of order @xmath .

3.  Analogous to the previous case.

∎

###### Proposition 2.6.

If @xmath is uniformly random, then

  -- -------- -------- -- --------
     @xmath   @xmath      (2.15)
     @xmath   @xmath      (2.16)
     @xmath   @xmath      (2.17)
  -- -------- -------- -- --------

and hence the expected number of random selections required to obtain an
element of order @xmath or @xmath is @xmath , and @xmath to obtain an
element that fixes a point.

###### Proof.

The first equality follows immediately from Theorem 2.1 and Proposition
2.5 . The inequalities follow from [ MSC96 , Section II.8] .

Clearly the number of selections required is geometrically distributed,
where the success probabilities for each selection are given by the
inequalities. Hence the expectations are as stated. ∎

###### Proposition 2.7.

Let @xmath .

1.   For every @xmath , distinct conjugates of @xmath intersect
    trivially.

2.   If @xmath is cyclic of order @xmath and @xmath then @xmath .

###### Proof.

1.  By Theorem 2.1 , we consider three cases. If @xmath lies in a
    conjugate @xmath of @xmath , then @xmath . If @xmath lies in a
    conjugate @xmath of @xmath , then @xmath and if @xmath lies in a
    conjugate @xmath of @xmath or @xmath , then @xmath . In each case
    the result follows from Proposition 2.4 .

2.  Since @xmath it is enough to show that @xmath . This follows
    immediately from Proposition 2.4 .

∎

###### Proposition 2.8.

Elements of odd order in @xmath that have the same trace are conjugate.

###### Proof.

From [ Suz62 , §17] , the number of conjugacy classes of non-identity
elements of odd order is @xmath , and all elements of even order have
trace @xmath . Observe that

  -- -------- -- --------
     @xmath      (2.18)
  -- -------- -- --------

Since @xmath can be any element of @xmath , so can @xmath , and this
also implies that @xmath has odd order when @xmath . Therefore there are
@xmath possible traces for non-identity elements of odd order, and
elements with different trace must be non-conjugate, so all conjugacy
classes must have different traces. ∎

###### Proposition 2.9.

The proportion of elements of order @xmath among the elements of trace
@xmath is @xmath .

###### Proof.

The elements of trace @xmath are those with orders @xmath , and apart
from the identity these are the elements that fix precisely one point of
@xmath . From the proof of Proposition 2.5 , there are @xmath elements
of trace @xmath .

The elements of order @xmath lie in a conjugate of @xmath , and there
are @xmath elements in each conjugate. Hence from Proposition 2.4 there
are @xmath elements of order @xmath . ∎

###### Proposition 2.10.

Let @xmath be uniformly random, where @xmath for some @xmath . Then

1.  -- -------- -- --------
         @xmath      (2.19)
      -- -------- -- --------

2.   If @xmath is fixed, then

      -- -------- -- --------
         @xmath      (2.20)
      -- -------- -- --------

###### Proof.

1.  By [ HB82 , Chapter @xmath , Lemma @xmath ] , @xmath is an ovoid, so
    it intersects any (projective) line in @xmath in at most @xmath
    points. The condition @xmath defines a projective plane @xmath . If
    @xmath then it contains a point @xmath , and there are @xmath lines
    in @xmath that passes through @xmath . Each one of these lines
    passes through at most one other point of @xmath , but each line
    contains @xmath points of @xmath , and hence at least @xmath of
    those points are not in @xmath . Moreover, each pair of lines has
    only the point @xmath in common.

    Now we have considered @xmath distinct points of @xmath , which are
    all points of @xmath , and we have proved that at most @xmath of
    those lie in @xmath .

2.  Clearly, @xmath if and only if @xmath . If @xmath , where @xmath and
    @xmath , then @xmath for some @xmath .

    Introducing indeterminates @xmath and @xmath in place @xmath and
    @xmath , it follows that the expression @xmath is a polynomial
    @xmath with @xmath and @xmath . For each @xmath , the number of
    roots of @xmath is therefore at most @xmath , so the number of roots
    of @xmath is at most @xmath .

∎

###### Proposition 2.11.

If @xmath are uniformly random, then

  -- -------- -- --------
     @xmath      (2.21)
  -- -------- -- --------

###### Proof.

Let @xmath . By Theorem 2.1 , @xmath and has order @xmath if and only if
@xmath . It therefore suffices to find the proportion of pairs @xmath
such that @xmath .

If @xmath then @xmath can be any element of @xmath , which contributes
@xmath pairs. If @xmath then @xmath , so we again obtain @xmath pairs.
Finally, if @xmath then @xmath , so we obtain @xmath pairs. Thus we
obtain @xmath pairs from a total of @xmath pairs, and the result
follows. ∎

###### Proposition 2.12.

Let @xmath . If @xmath are uniformly random, then

  -- -------- -- --------
     @xmath      (2.22)
  -- -------- -- --------

###### Proof.

By ( 2.8 ) and Theorem 2.3 , the maximal subgroup @xmath with smallest
index is @xmath . Then @xmath and since @xmath , there are @xmath
conjugates of @xmath .

  -- -- -- --------
           (2.23)
  -- -- -- --------

The probability that @xmath lies in any maximal not conjugate to @xmath
must be less than @xmath because the other maximals have larger indices.
There are @xmath number of conjugacy classes of maximal subgroups, and
hence the probability that @xmath lies in a maximal subgroup is @xmath .
∎

#### 2.1.2. Alternative definition

The way we have defined the Suzuki groups resembles the original
definition, but it is not clear that the groups are exceptional groups
of Lie type. This was first proved in [ Ono62 , Ono63 ] . A more common
way to define the groups are as the fixed points of a certain
automorphism of @xmath . This approach is followed in [ Wil05 , Chapter
@xmath ] , and it provides a more straightforward method to deal with
non-constructive recognition of @xmath .

Let @xmath denote the standard copy of the symplectic group, preserving
the following symplectic form:

  -- -------- -- --------
     @xmath      (2.24)
  -- -------- -- --------

From [ Wil05 , Chapter @xmath ] , we know that the elements of @xmath
are precisely the fixed points of an automorphism @xmath of @xmath .
Computing @xmath for some @xmath amounts to taking a submatrix of the
exterior square of @xmath and then replacing each matrix entry @xmath by
@xmath . Moreover, @xmath is defined on @xmath for @xmath . A more
detailed description of how to compute @xmath can be found in [ Wil05 ,
Chapter @xmath ] .

###### Lemma 2.13.

Let @xmath have natural module @xmath and assume that @xmath is
absolutely irreducible. Then @xmath for some @xmath if and only if
@xmath .

###### Proof.

Assume @xmath . Both @xmath and @xmath preserve the form ( 2.24 ), and
this form is unique up to a scalar multiple, since @xmath is absolutely
irreducible. Therefore @xmath for some @xmath . But if @xmath then
@xmath , so that @xmath . Moreover, @xmath , and hence we may assume
that @xmath . Let @xmath and observe that for each @xmath , @xmath . It
follows that

  -- -------- -- --------
     @xmath      (2.25)
  -- -------- -- --------

so @xmath .

Conversely, assume that @xmath . Then there is some @xmath such that for
each @xmath we have @xmath . As above, since both @xmath and @xmath
preserve the form ( 2.24 ), we may assume that @xmath .

Let @xmath be the algebraic closure of @xmath . The Steinberg-Lang
Theorem (see [ Ste77 ] ) asserts that there exists @xmath such that
@xmath . It follows that

  -- -------- -- --------
     @xmath      (2.26)
  -- -------- -- --------

so that @xmath . Thus @xmath is conjugate in @xmath to a subgroup @xmath
of @xmath , and it follows from [ CR06 , Theorem @xmath ] , that @xmath
is conjugate to @xmath in @xmath . ∎

#### 2.1.3. Tensor indecomposable representations

It follows from [ Lüb01 ] that over an algebraically closed field in
defining characteristic, up to Galois twists, there is only one
absolutely irreducible tensor indecomposable representation of @xmath :
the natural representation.

### 2.2. Small Ree groups

The small Ree groups were first described in [ Ree60 , Ree61c ] . Their
structure has been investigated in [ War63 , War66 , LN85 , Kle88 ] . A
short survey is also given in [ HB82 , Chapter @xmath ] . They should
not be confused with the Big Ree groups, which are described in Section
2.3 .

#### 2.2.1. Definition and properties

We now define our standard copy of the Ree groups. The generators that
we use are those described in [ KLM01 ] . Let @xmath for some @xmath and
let @xmath . For @xmath and @xmath , define the matrices

  -- -------- -- --------
     @xmath      (2.27)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (2.28)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (2.29)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (2.30)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (2.31)
  -- -------- -- --------

and define the Ree group as

  -- -------- -- --------
     @xmath      (2.32)
  -- -------- -- --------

Also, define the subgroups of upper triangular and diagonal matrices:

  -- -------- -------- -- --------
     @xmath   @xmath      (2.33)
     @xmath   @xmath      (2.34)
  -- -------- -------- -- --------

From [ LN85 ] we then know that each element of @xmath can be expressed
in a unique way as

  -- -------- -- --------
     @xmath      (2.35)
  -- -------- -- --------

so that @xmath , and it follows that @xmath . We also know that @xmath
is a Sylow @xmath -subgroup of @xmath , and direct calculations show
that

  -- -------- -------- -- --------
     @xmath               (2.36)
              @xmath      (2.37)
     @xmath               (2.38)
  -- -------- -------- -- --------

and

  -- -------- -------- -- --------
     @xmath   @xmath      (2.39)
  -- -------- -------- -- --------

###### Remark 2.14 (Standard generators of @xmath).

As standard generators for @xmath we will use

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a primitive element of @xmath , whose minimal polynomial
is the defining polynomial of @xmath .

The Ree groups preserve a symmetric bilinear form on @xmath ,
represented by the matrix

  -- -------- -- --------
     @xmath      (2.40)
  -- -------- -- --------

From [ War66 ] and [ HB82 , Chapter @xmath ] we immediately obtain

###### Proposition 2.15.

Let @xmath .

1.  @xmath where @xmath .

2.   Conjugates of @xmath intersect trivially.

3.   The centre @xmath .

4.   The derived group @xmath , and its elements have order @xmath .

5.   The elements in @xmath have order @xmath and their cubes form
    @xmath .

6.  @xmath and @xmath acts doubly transitively on the right cosets of
    @xmath , i.e. on a set of size @xmath .

7.  @xmath is a Frobenius group with Frobenius kernel @xmath .

8.   The proportion of elements of order @xmath in @xmath is @xmath ,
    where @xmath is the Euler totient function.

For our purposes, we want another set to act (equivalently) on.

###### Proposition 2.16.

There exists @xmath on which @xmath acts faithfully and doubly
transitively. This set is

  -- -------- -- --------
     @xmath      (2.41)
  -- -------- -- --------

Moreover, the stabiliser of @xmath is @xmath , the stabiliser of @xmath
is @xmath and the stabiliser of @xmath is @xmath .

###### Proof.

Notice that @xmath consists of the first rows of the elements of @xmath
. From Proposition 2.15 it follows that @xmath is the disjoint union of
@xmath and @xmath . Define a map between the @xmath -sets as @xmath .

If @xmath then @xmath and hence the stabiliser of @xmath is @xmath . If
@xmath then @xmath where @xmath . Hence @xmath since @xmath is the first
row of @xmath . It follows that the map defines an equivalence between
the @xmath -sets. ∎

###### Proposition 2.17.

Let @xmath .

1.   The stabiliser in @xmath of any two distinct points of @xmath is
    conjugate to @xmath , and the stabiliser of any triple of points has
    order @xmath .

2.   The number of elements in @xmath that fix exactly one point is
    @xmath .

3.   All involutions in @xmath are conjugate in @xmath .

4.   An involution fixes @xmath points.

###### Proof.

1.  Immediate from [ HB82 , Chapter @xmath , Theorem @xmath (d)] .

2.  A stabiliser of a point is conjugate to @xmath , and there are
    @xmath conjugates. The elements fixing exactly one point are the
    non-trivial elements of @xmath . Therefore the number of such
    elements is @xmath .

3.  Immediate from [ HB82 , Chapter @xmath , Theorem @xmath (e)] .

4.  Each involution is conjugate to

      -- -------- --
         @xmath   
      -- -------- --

    Evidently, @xmath fixes @xmath since @xmath and if @xmath with
    @xmath , then @xmath is fixed by @xmath if and only if @xmath . But
    then @xmath is uniquely determined by @xmath , so there are @xmath
    possible choices for @xmath . Thus the number of points fixed by
    @xmath is @xmath .

∎

###### Proposition 2.18.

Let @xmath with natural module @xmath and let @xmath be an involution.
Then @xmath where @xmath and @xmath . Moreover, @xmath is irreducible
and @xmath acts trivially on @xmath .

###### Proof.

By Proposition 2.17 , @xmath is conjugate to @xmath so it has two
eigenspaces @xmath and @xmath for @xmath and @xmath respectively.
Clearly @xmath and @xmath , and it is sufficient to show that these are
preserved by @xmath , so that they are in fact submodules of @xmath .

Let @xmath and @xmath . Then @xmath since @xmath centralises @xmath and
@xmath fixes @xmath , which shows that @xmath , so this subspace is
fixed by @xmath . Similarly, @xmath is also fixed.

Let @xmath be the bilinear form preserved by @xmath . Observe that if
@xmath , @xmath then @xmath and hence @xmath . If @xmath is degenerate
then also @xmath so that @xmath which is impossible since @xmath is
non-degenerate. Hence @xmath is non-degenerate and @xmath is isomorphic
to its dual.

Now if @xmath is reducible, it must split as a direct sum of two
submodules of dimension @xmath and @xmath . Since @xmath acts trivially
on @xmath , it is in fact a module for @xmath , but @xmath have no
irreducible modules of dimension @xmath . Therefore @xmath must be
irreducible. ∎

###### Lemma 2.19.

Let @xmath with @xmath odd and @xmath any finite field, and assume that
@xmath preserves a non-degenerate bilinear form and that @xmath . Then
@xmath has @xmath as an eigenvalue.

###### Proof.

Let @xmath be the natural module of @xmath . Let @xmath be a
non-degenerate bilinear form preserved by @xmath . Over @xmath , the
multiset of eigenvalues of @xmath is @xmath . Let @xmath be the
corresponding eigenspaces (some of them might be equal).

If @xmath and @xmath then

  -- -------- --
     @xmath   
  -- -------- --

so either @xmath or @xmath . However, for a given @xmath there must be
some @xmath such that @xmath , otherwise @xmath for every @xmath and
@xmath , which is impossible since @xmath is non-degenerate.

Hence the eigenvalues can be arranged into pairs of inverse values.
Since @xmath is odd, there must be a @xmath such that @xmath is left
over. The above argument then implies that @xmath , and finally @xmath .
∎

From [ LN85 ] and [ Kle88 ] we obtain

###### Proposition 2.20.

A maximal subgroup of @xmath is conjugate to one of the following
subgroups

-   @xmath , the point stabiliser .

-   @xmath , the centraliser of an involution @xmath .

-   @xmath , where @xmath is cyclic of order @xmath .

-   @xmath , where @xmath is cyclic of order @xmath .

-   @xmath , where @xmath is cyclic of order @xmath .

-   @xmath where @xmath is a proper power of @xmath .

Moreover, all maximal subgroups except the last are reducible.

###### Proof.

It is sufficient to prove the final statement.

Clearly the point stabiliser is reducible, and the involution
centraliser is reducible by Proposition 2.18 .

Let @xmath be a normaliser of a cyclic subgroup and let @xmath be a
generator of the cyclic subgroup that is normalised. Since @xmath , by
Lemma 2.19 , @xmath has an eigenspace @xmath for the eigenvalue @xmath ,
where @xmath . Given @xmath and @xmath , we see that @xmath so that
@xmath is fixed by @xmath . This implies that @xmath and thus @xmath is
a proper non-trivial @xmath -invariant subspace, so @xmath is reducible.
∎

###### Proposition 2.21.

Let @xmath .

1.   All cyclic subgroups of @xmath of order @xmath are conjugate to
    @xmath and hence each one is a stabiliser of two points of @xmath .

2.   All cyclic subgroups of order @xmath or @xmath are conjugate.

3.   If @xmath is a cyclic subgroup of order @xmath , then distinct
    conjugates of @xmath intersect trivially.

4.   If @xmath is a cyclic subgroup of order @xmath , @xmath and @xmath
    , then @xmath .

###### Proof.

1.  Let @xmath be cyclic of order @xmath and let @xmath be an odd prime
    such that @xmath . Then there exists @xmath such that @xmath . Since
    @xmath , the cycle structure of @xmath on @xmath must be a number of
    @xmath -cycles and @xmath fixed points @xmath and @xmath . Since
    @xmath is doubly transitive there exists @xmath such that @xmath and
    @xmath .

    Now either @xmath fixes @xmath and @xmath or interchanges them, so
    @xmath . Hence @xmath since that is the unique cyclic subgroup of
    order @xmath in @xmath .

2.  This follows immediately from [ LN85 , Lemma @xmath ] .

3.  Let @xmath be such a cyclic subgroup. If @xmath for some @xmath and
    @xmath , then @xmath . But @xmath , so that @xmath .

4.  Since @xmath , this is analogous to the previous case.

∎

###### Proposition 2.22.

Let @xmath and let @xmath be the Euler totient function.

1.   The centraliser of an involution @xmath is isomorphic to @xmath and
    hence has order @xmath .

2.   The number of involutions in @xmath is @xmath .

3.   The number of elements in @xmath of order @xmath is @xmath .

4.   The number of elements in @xmath of order @xmath is @xmath .

5.   The number of elements in @xmath of order @xmath is @xmath .

6.   The number of elements in @xmath of even order is @xmath .

7.   The number of elements in @xmath that fix at least one point is
    @xmath

###### Proof.

1.  Immediate from [ HB82 , Chapter @xmath ] .

2.  All involutions are conjugate, and the index in @xmath of the
    involution centraliser is

      -- -------- -- --------
         @xmath      (2.42)
      -- -------- -- --------

    where we have used the fact that @xmath .

3.  By Proposition 2.21 , each cyclic subgroup of order @xmath is a
    stabiliser of two points and is uniquely determined by the pair of
    points that it fixes. Hence the number of cyclic subgroups of order
    @xmath is

      -- -------- -- --------
         @xmath      (2.43)
      -- -------- -- --------

    By Proposition 2.17 , the intersection of two distinct subgroups has
    order @xmath , so the number of elements of order @xmath is the
    number of generators of all these subgroups.

4.  By Proposition 2.21 , the number of cyclic subgroups of order @xmath
    is @xmath . Since distinct conjugates intersect trivially, the
    number of elements of order @xmath is the number of generators of
    these subgroups.

5.  Analogous to the previous case.

6.  By [ LN85 , Lemma @xmath ] , every element of even order lies in a
    cyclic subgroup of order @xmath or @xmath . In each cyclic subgroup
    of order @xmath there is a unique involution and hence @xmath
    non-involutions of even order, and similarly @xmath in a cyclic
    subgroup of order @xmath . By Proposition 2.21 the total number of
    elements of even order is therefore

      -- -------- -- --------
         @xmath      (2.44)
      -- -------- -- --------

7.  The only non-trivial elements of @xmath that fix more than @xmath
    points are involutions. Hence in each cyclic subgroup of order
    @xmath there are @xmath elements that fix exactly @xmath points, so
    by Proposition 2.17 , the number of elements that fix at least one
    point is

      -- -------- -- --------
         @xmath      (2.45)
      -- -------- -- --------

∎

###### Lemma 2.23.

If @xmath is uniformly random, then

  -- -------- -------- -- --------
     @xmath   @xmath      (2.46)
     @xmath   @xmath      (2.47)
     @xmath   @xmath      (2.48)
     @xmath   @xmath      (2.49)
  -- -------- -------- -- --------

  -- -------- -- --------
     @xmath      (2.50)
  -- -------- -- --------

###### Proof.

In each case, the first equality follows from Proposition 2.22 and
Proposition 2.15 . In the first case, the inequality follows from [
MSC96 , Section II.8] , and in the other cases the inequalities are
clear since @xmath . ∎

###### Corollary 2.24.

In @xmath , the expected number of random selections required to obtain
an element of order @xmath , @xmath or @xmath is @xmath , and @xmath to
obtain an element that fixes a point, or an element of even order.

###### Proof.

Clearly the number of selections is geometrically distributed, where the
success probabilities for each selection are given by Lemma 2.23 . Hence
the expectations are as stated. ∎

###### Proposition 2.25.

Elements in @xmath of order prime to @xmath with the same trace are
conjugate.

###### Proof.

From [ War66 ] , the number of conjugacy classes of non-identity
elements of order prime to @xmath is @xmath . Observe that for @xmath ,
@xmath and @xmath is prime to @xmath if also @xmath .

Moreover, @xmath has order @xmath and trace @xmath so there are @xmath
possible traces for non-identity elements of order prime to @xmath , and
elements with different trace must be non-conjugate. Thus all conjugacy
classes must have different traces. ∎

###### Proposition 2.26.

If @xmath are uniformly random involutions, then @xmath for some
constant @xmath .

###### Proof.

Follows immediately from [ WP06 , Theorem @xmath ] . ∎

###### Proposition 2.27.

Let @xmath . If @xmath are uniformly random, then

  -- -------- -- --------
     @xmath      (2.51)
  -- -------- -- --------

###### Proof.

The maximal subgroup @xmath consisting of the upper triangular matrices
modulo scalars has index @xmath , and all subgroups isomorphic to @xmath
are conjugate. Since @xmath , there are @xmath conjugates of @xmath .

  -- -------- -- --------
     @xmath      (2.52)
  -- -------- -- --------

The other maximal subgroups have index strictly greater than @xmath , so
the probability that @xmath lies in any maximal not conjugate to @xmath
must be less than @xmath . The number of conjugacy classes of maximal
subgroups is @xmath , and hence the probability that @xmath lies in a
maximal subgroup is @xmath . ∎

###### Proposition 2.28.

Let @xmath be uniformly random, where @xmath for some @xmath . Then

1.  -- -------- -- --------
         @xmath      (2.53)
      -- -------- -- --------

2.   If @xmath is given, then

      -- -------- -- --------
         @xmath      (2.54)
      -- -------- -- --------

###### Proof.

If @xmath , where @xmath and @xmath , then @xmath for some @xmath .

1.  By introducing indeterminates @xmath , @xmath and @xmath in place of
    @xmath , @xmath and @xmath , it follows that @xmath is a polynomial
    @xmath with @xmath , @xmath and @xmath . For each @xmath , the
    number of roots of @xmath is therefore at most @xmath , so the
    number of roots of @xmath is at most @xmath .

2.  Similarly, by introducing indeterminates @xmath , @xmath and @xmath
    in place of @xmath , @xmath and @xmath , it follows that the
    expression @xmath is a polynomial @xmath with @xmath , @xmath and
    @xmath . For each @xmath , the number of roots of @xmath is
    therefore at most @xmath , so the number of roots of @xmath is at
    most @xmath .

∎

###### Proposition 2.29.

If @xmath are uniformly random and independent, then

  -- -------- -- --------
     @xmath      (2.55)
  -- -------- -- --------

###### Proof.

By Proposition 2.15 , @xmath and has order @xmath if and only if @xmath
. It is therefore sufficient to find the proportion of (unordered) pairs
@xmath such that @xmath .

If @xmath then @xmath can be any element of @xmath , which gives @xmath
pairs. If @xmath then @xmath , so we again obtain @xmath pairs. Finally,
if @xmath then @xmath so we obtain @xmath pairs. Thus we obtain @xmath
pairs from a total of @xmath pairs, and the result follows. ∎

#### 2.2.2. Alternative definition

The definition of @xmath that we have given is the one that best suits
most our purposes. However, to deal with non-constructive recognition,
we need to mention the more common definition of @xmath .

Following [ Wil05 , Chapter @xmath ] and [ Wil06 ] , the exceptional
group @xmath is constructed by considering the Cayley algebra @xmath
(the octonion algebra), which has dimension @xmath , and defining @xmath
to be the automorphism group of @xmath . Thus each element of @xmath
fixes the identity and preserves the algebra multiplication, and it
follows that it is isomorphic to a subgroup of @xmath .

Furthermore, when @xmath is an odd power of @xmath , the group @xmath
has a certain automorphism @xmath , sometimes called the exceptional
outer automorphism , whose set of fixed points form a group, and this is
defined to be the Ree group @xmath . A more detailed description of how
to compute @xmath can be found in [ Wil06 ] .

#### 2.2.3. Tensor indecomposable representations

It follows from [ Lüb01 ] that over an algebraically closed field in
defining characteristic, up to Galois twists, there are precisely two
absolutely irreducible tensor indecomposable representations of @xmath :
the natural representation and a representation of dimension @xmath .
Let @xmath be the natural module of @xmath , of dimension @xmath . The
symmetric square @xmath has dimension @xmath , and is a direct sum of
two submodules of dimensions @xmath and @xmath . The @xmath -dimensional
submodule arises because @xmath preserves a quadratic form.

### 2.3. Big Ree groups

The Big Ree groups were first described in [ Ree61a , Ree61b ] , and are
covered in [ Wil05 , Chapter @xmath ] . The maximal subgroups are given
in [ Mal91 ] , and representatives of the conjugacy classes are given in
[ Shi74 ] and [ Shi75 ] . An elementary construction, suitable for our
purposes, is described in [ Wil06 ] .

#### 2.3.1. Definition and properties

We take the definition of the standard copy of @xmath from [ Wil06 ] .

The exceptional group @xmath is constructed by considering the
exceptional Jordan algebra (the Albert algebra), which has dimension
@xmath , and defining @xmath to be its automorphism group. Thus each
element of @xmath fixes the identity and preserves the algebra
multiplication, and one can show that it is a subgroup of @xmath .

Furthermore, when @xmath is an odd power of @xmath , the group @xmath
has a certain automorphism @xmath whose set of fixed points form a
group, and this is defined to be the Big Ree group @xmath . A more
detailed description of how to compute @xmath can be found in [ Wil06 ]
.

Let @xmath and let @xmath . From [ Wil06 ] we immediately obtain:

###### Proposition 2.30.

Let @xmath and @xmath with @xmath . Then @xmath is conjugate in @xmath
to an element of the form

  -- -------- -- --------
     @xmath      (2.56)
  -- -------- -- --------

for some @xmath .

Let @xmath be the standard basis of @xmath . Following [ Wil06 ] , we
then define the following matrices as permutations on this basis:

  -- -------- -------- -- --------
     @xmath   @xmath      (2.57)
     @xmath               (2.58)
  -- -------- -------- -- --------

We also define linear transformations @xmath and @xmath , where @xmath
fixes @xmath for @xmath and otherwise acts as follows:

  -- -------- -------- -------- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath   @xmath   @xmath   @xmath      (2.59)
     @xmath   @xmath   @xmath   @xmath   @xmath   @xmath      (2.60)
     @xmath   @xmath   @xmath   @xmath   @xmath   @xmath      (2.61)
     @xmath   @xmath   @xmath   @xmath                        (2.62)
  -- -------- -------- -------- -------- -------- -------- -- --------

Furthermore, we define a block-diagonal matrix @xmath as follows:

  -- -------- -- --------
     @xmath      (2.63)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (2.64)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (2.65)
  -- -------- -- --------

and then @xmath has diagonal blocks @xmath .

Finally, @xmath fixes @xmath for @xmath and otherwise acts as follows:

  -- -------- -------- -------- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath   @xmath   @xmath   @xmath      (2.66)
     @xmath   @xmath   @xmath   @xmath   @xmath   @xmath      (2.67)
  -- -------- -------- -------- -------- -------- -------- -- --------

  -- -------- -------- -------- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath   @xmath   @xmath   @xmath      (2.68)
     @xmath   @xmath   @xmath   @xmath   @xmath   @xmath      (2.69)
     @xmath   @xmath   @xmath   @xmath   @xmath   @xmath      (2.70)
  -- -------- -------- -------- -------- -------- -------- -- --------

From [ Wil06 ] we then immediately obtain:

###### Theorem 2.31.

Let @xmath be a primitive element of @xmath . The elements @xmath ,
@xmath , @xmath , @xmath and @xmath lie in @xmath , and if @xmath , then
@xmath .

###### Proposition 2.32.

Let @xmath and @xmath with @xmath . Then @xmath is conjugate in @xmath
to @xmath , which we write as

  -- -------- -- --------
     @xmath      (2.71)
  -- -------- -- --------

where @xmath and @xmath .

###### Proof.

Using the notation of [ Shi75 , Page @xmath ] , we see that with respect
to a suitable basis, @xmath lies in @xmath and that @xmath will have the
form @xmath for some @xmath . Hence it will lie in one of the factors of
@xmath . ∎

###### Proposition 2.33.

If @xmath is uniformly random, then

  -- -------- -- --------
     @xmath      (2.72)
  -- -------- -- --------

and hence the expected number of random selections required to obtain an
element of order @xmath is @xmath .

###### Proof.

Using the notation of [ Shi75 , Page @xmath ] , we see that such
elements lie in a conjugate of @xmath . The proportion of the elements
in @xmath is therefore @xmath .

From [ Shi75 , Table @xmath ] we see that the elements in @xmath are of
types @xmath and @xmath and that the total number of such elements in
@xmath is

  -- -------- --
     @xmath   
  -- -------- --

The proportion in @xmath of the elements of the required order is
therefore at least

  -- -------- --
     @xmath   
  -- -------- --

and the expression in parentheses is minimised when @xmath . The result
now follows from [ MSC96 , Section II.8] . ∎

By definition we have the inclusions @xmath , so @xmath preserves a
quadratic form @xmath with associated bilinear form @xmath . It follows
from [ Wil06 ] that

  -- -------- -------- -- --------
     @xmath   @xmath      (2.73)
     @xmath   @xmath      (2.74)
  -- -------- -------- -- --------

###### Proposition 2.34.

In @xmath , there are two conjugacy classes of involutions. The rank of
an involution is the number of @xmath -blocks in the Jordan form.

  Name     Centraliser   Maximal parabolic   Rank     Representative
  -------- ------------- ------------------- -------- ----------------
  @xmath   @xmath        @xmath              @xmath   @xmath
  @xmath   @xmath        @xmath              @xmath   @xmath

Moreover, in the @xmath case the centre of the centraliser has order
@xmath . The cyclic group @xmath acts fixed-point freely on @xmath .

Let @xmath be involutions. If @xmath and @xmath are conjugate, then
@xmath is odd with probability @xmath . If @xmath and @xmath are not
conjugate, then @xmath is even.

###### Proof.

The structure of the centralisers can be found in [ Mal91 ] . The
statement about @xmath and @xmath follows immediately from [ WP06 ,
Theorem @xmath ] . ∎

###### Corollary 2.35 (The dihedral trick).

There exists a Las Vegas algorithm that, given @xmath such that @xmath
and given conjugate involutions @xmath , finds @xmath such that @xmath .
If @xmath are given as @xmath s of length @xmath , then @xmath will be
found as an @xmath of length @xmath . The algorithm has expected time
complexity @xmath field operations.

###### Proof.

Follows from Proposition 2.34 and Theorem 1.9 .

∎

###### Proposition 2.36.

Let @xmath be a primitive element of @xmath . A maximal parabolic of
type @xmath is conjugate to @xmath , which consists of lower
block-triangular matrices. A maximal parabolic of type @xmath is
conjugate to

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Follows immediately from [ Wil06 ] . ∎

###### Conjecture 2.37.

Let @xmath be an involution of class @xmath and let @xmath satisfy
@xmath and @xmath where @xmath .

1.   Let @xmath be uniformly random such that @xmath . Then @xmath with
    probability @xmath .

2.   If @xmath and @xmath is uniformly random such that @xmath , then
    with high probability @xmath and @xmath .

###### Conjecture 2.38.

Let @xmath be involutions of class @xmath such that @xmath . Then the
proportion of @xmath such that @xmath , @xmath and @xmath is high.

###### Conjecture 2.39.

Let @xmath be an involution. Let @xmath satisfy @xmath and @xmath where
@xmath . Let @xmath be the natural module of @xmath .

  Class of @xmath   Constituents and multiplicities of @xmath
  ----------------- --------------------------------------------
  @xmath            @xmath , @xmath , @xmath
  @xmath            @xmath , @xmath , @xmath , @xmath , @xmath

Where @xmath is the natural module for @xmath , @xmath is the natural
module for @xmath and @xmath .

In the @xmath case, @xmath has submodules of dimensions

  -- -------- --
     @xmath   
  -- -------- --

It has @xmath submodules of dimensions @xmath and @xmath , and the
others are unique.

In the @xmath case, @xmath has submodules of every dimension @xmath . It
has @xmath submodules of dimensions @xmath , @xmath submodules of
dimensions @xmath and @xmath and @xmath submodules of dimensions @xmath
and @xmath . All the others are unique.

###### Conjecture 2.40.

Let @xmath be an involution of class @xmath , let @xmath and let @xmath
be the corresponding maximal subgroup.

1.  @xmath is nilpotent of class @xmath and has exponent @xmath .

2.  @xmath has a subnormal series

      -- -------- --
         @xmath   
      -- -------- --

    where @xmath , @xmath .

3.   The series induces a filtration of @xmath , so @xmath are @xmath
    -modules, for @xmath .

4.  @xmath , @xmath , @xmath is elementary abelian and @xmath .

5.  @xmath has exponent- @xmath central series

      -- -------- --
         @xmath   
      -- -------- --

6.   Pre-images of non-identity elements of @xmath have order @xmath and
    their squares, which lie in @xmath and have non-trivial images in
    @xmath , are involutions of class @xmath .

7.   Pre-images of non-identity elements of @xmath have order @xmath and
    their squares, which lie in @xmath and have non-trivial images in
    @xmath , are involutions of class @xmath .

8.   As @xmath -modules, @xmath is not isomorphic to @xmath .

By [ Mal91 ] , @xmath has a maximal subgroup @xmath , so from Section
2.1 we know that @xmath contains elements of order @xmath .

###### Conjecture 2.41.

Let @xmath be such that @xmath and let @xmath be one of its direct
factors. Let @xmath be the module of @xmath and let @xmath be the
natural module of @xmath .

1.  @xmath .

2.  @xmath has composition factors with multiplicities: @xmath .

###### Conjecture 2.42.

Let @xmath be such that @xmath and let @xmath be the module of @xmath .

1.   The elements of @xmath of order @xmath have @xmath as an eigenvalue
    of multiplicity @xmath . The proportion of these elements in @xmath
    (taken over every @xmath ) is @xmath .

2.  @xmath where @xmath .

3.  @xmath is absolutely irreducible and @xmath . @xmath has shape
    @xmath . The @xmath are natural @xmath -modules.

4.  @xmath has endomorphism ring @xmath and automorphism group @xmath .

###### Proposition 2.43.

Let @xmath such that @xmath and let @xmath be the module of @xmath .
Then @xmath .

###### Proof.

The subgroup of @xmath that preserves the direct sum decomposition of
@xmath has shape @xmath (the @xmath -dimensional part is of plus type
since @xmath is a tensor product of natural Suzuki modules, and these
preserve orthogonal forms of plus type). This implies that

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath is absolutely irreducible, @xmath consists of scalars only,
but there are no scalars in @xmath . Hence it suffices to show that
@xmath .

With respect to a suitable basis, an endomorphism of @xmath has the form
@xmath . It preserves the bilinear form @xmath restricted to @xmath if
@xmath , and it preserves the quadratic form @xmath restricted to @xmath
if @xmath is a transvection. This implies that only @xmath values of
@xmath are possible. ∎

###### Proposition 2.44.

Let @xmath be a primitive element of @xmath . The subgroup

  -- -------- --
     @xmath   
  -- -------- --

is isomorphic to @xmath . It contains @xmath and @xmath , where @xmath
and @xmath . The subgroup @xmath is isomorphic to @xmath .

###### Proof.

Follows immediately from [ Wil06 ] . ∎

###### Conjecture 2.45.

Let @xmath be such that @xmath and @xmath is conjugate to some @xmath
with @xmath . The proportion of @xmath such that the elements in @xmath
with @xmath as an eigenvalue of multiplicity @xmath also have even order
and power up to an involution of class @xmath , is bounded below by a
constant @xmath .

###### Conjecture 2.46.

The proportion of elements in @xmath that have @xmath as an eigenvalue
of multiplicity @xmath is @xmath .

###### Conjecture 2.47.

Let @xmath be such that @xmath and @xmath is conjugate to some @xmath
with @xmath . Then @xmath and there exists an absolute constant @xmath
such that for every @xmath , the number of @xmath that have @xmath as an
eigenvalue of multiplicity at least @xmath is bounded above by @xmath .

###### Proposition 2.48.

Let @xmath with natural module @xmath and let @xmath be a maximal
subgroup. Then either @xmath is reducible or @xmath where @xmath is a
proper power of @xmath .

###### Proof.

Follows from [ Mal91 ] . ∎

#### 2.3.2. Tensor indecomposable representations

It follows from [ Lüb01 ] that over an algebraically closed field in
defining characteristic, up to Galois twists, there are precisely three
absolutely irreducible tensor indecomposable representations of @xmath :

1.  the natural representation @xmath of dimension @xmath ,

2.  a submodule of @xmath of @xmath of dimension @xmath ,

3.  a submodule of @xmath of dimension @xmath

## Chapter 3 Constructive recognition and membership testing

Here we will present the algorithms for constructive recognition and
constructive membership testing. The methods we use are specialised to
each family of exceptional groups, so we treat each family separately.
When the methods are similar between the families, we present a complete
account for each family, in order to make each section self-contained.
In the cases where we have a non-constructive recognition algorithm that
improves on [ BKPS02 ] , we will also present it here.

Recall the various cases of constructive recognition of matrix groups,
given in Section 1.2.7 . For each group, we will deal with some, but not
always all, of the cases that arise.

We first give an overview of the various methods. From Chapter 2 we know
that both the Suzuki groups and the small Ree groups act doubly
transitively on @xmath and @xmath projective points, respectively (the
fields of size @xmath have different characteristics), and the idea of
how to deal with these groups is to think of them as permutation groups.
In fact we proceed similarly as in [ CLGO06 ] for @xmath , which acts
doubly transitively on @xmath projective points. The essential problem
in all these cases is to find an efficient algorithm that finds an
element of the group that maps one projective point to another.

In @xmath the algorithm proceeds by finding a random element of order
@xmath and considering a random coset of the subgroup generated by this
element. Since the coset is exponentially large, we cannot process every
element, and the idea is instead to construct the required element by
solving equations. We therefore consider a matrix whose entries are
indeterminates. In this way we reduce the coset search problem to two
other problems from computational algebra: finding roots of quadratic
equations over a finite field, and the famous discrete logarithm problem
.

In the Suzuki groups, the number of points is @xmath instead of @xmath ,
and this requires us to consider double cosets of elements of order
@xmath , instead of cosets. The problem is again reduced to finding
roots of univariate polynomials, in this case of degree at most @xmath ,
as well as to the discrete logarithm problem.

The small Ree groups are dealt with slightly differently, since we can
easily find involutions by random search and then use the Bray algorithm
to find the centraliser of an involution. Then the module of the group
restricted to the centraliser decomposes, and this is used to find an
element that maps one projective point to another.

The Big Ree groups cannot be considered as permutation groups in the
same way, so the essential problem in this case is to find an involution
expressed as a product of the given generators. Again the idea is to
find a cyclic subgroup of order @xmath and search for elements of even
order in random cosets of this subgroup. The underlying observation is
that the elements of even order are characterised by having @xmath as an
eigenvalue with a certain multiplicity. Therefore we can again consider
matrices whose entries are indeterminates and construct the required
element of even order.

### 3.1. Suzuki groups

Here we will use the notation from Section 2.1 . We will refer to
Conjectures 3.4 , 3.18 , 3.19 , 3.21 and 3.24 simultaneously as the
Suzuki Conjectures . We now give an overview of the algorithm for
constructive recognition and constructive membership testing. It will be
formally proved as Theorem 3.26 .

1.  Given a group @xmath , satisfying the assumptions in Section 1.2.7 ,
    we know from Section 2.1.3 that the module of @xmath is isomorphic
    to a tensor product of twisted copies of the natural module of
    @xmath . Hence we first tensor decompose this module. This is
    described in Section 3.1.5 .

2.  The resulting groups in dimension @xmath are conjugates of the
    standard copy, so we find a conjugating element. This is described
    in Section 3.1.4 .

3.  Finally we are in @xmath . Now we can perform preprocessing for
    constructive membership testing and other problems we want to solve.
    This is described in Section 3.1.3 .

Given a discrete logarithm oracle, the whole process has time complexity
slightly worse than @xmath field operations, assuming that @xmath is
given by a bounded number of generators.

#### 3.1.1. Recognition

We now discuss how to non-constructively recognise @xmath . We are given
a group @xmath and we want to decide whether or not @xmath , the group
defined in ( 2.4 ).

To do this, it suffices to determine if @xmath and if @xmath does not
generate a proper subgroup, i.e. if @xmath is not contained in a maximal
subgroup. To determine if @xmath is in @xmath , first determine if
@xmath preserves the symplectic form of @xmath and then determine if
@xmath is a fixed point of the automorphism @xmath of @xmath , mentioned
in Section 2.1 .

The recognition algorithm relies on the following result.

###### Lemma 3.1.

Let @xmath , where @xmath , let @xmath and let @xmath be the natural
module of @xmath . Then @xmath if and only if the following hold:

1.  @xmath is an absolutely irreducible @xmath -module.

2.  @xmath cannot be written over a proper subfield.

3.  @xmath and for every @xmath there exists @xmath such that @xmath .

###### Proof.

By Theorem 2.3 , the maximal subgroups of @xmath that do satisfy the
first two conditions are @xmath , @xmath and @xmath . For each, the
derived group is contained in the normalised cyclic group, so all these
maximal subgroups are metabelian. If @xmath is contained in one of them
and @xmath is not abelian, then @xmath , but @xmath for every @xmath and
@xmath since the second derived group of @xmath is trivial. Hence the
last condition is not satisfied.

Conversely, assume that @xmath . Then clearly, the first two conditions
are satisfied, and @xmath . Assume that the last condition is false, so
for some @xmath we have that @xmath for every @xmath . This implies that
@xmath , and it follows from Proposition 2.7 that @xmath . Thus @xmath
for all @xmath , so @xmath , but @xmath is simple and we have a
contradiction. ∎

###### Theorem 3.2.

There exists a Las Vegas algorithm that, given @xmath , decides whether
or not @xmath . It has expected time complexity @xmath field operations.

###### Proof.

The algorithm proceeds as follows.

1.  Determine if every @xmath is in @xmath , and return false if not.

2.  Determine if @xmath is absolutely irreducible, and return false if
    not.

3.  Determine if @xmath can be written over a smaller field. If so,
    return false .

4.  Using the notation of Lemma 3.1 , try to find @xmath such that
    @xmath . Return false if it cannot be found.

5.  If such @xmath can be found, and if @xmath for some @xmath , then
    return true , else return false .

From the discussion at the beginning of this section, the first step is
easily done using @xmath field operations. The MeatAxe can be used to
determine if the natural module is absolutely irreducible; the algorithm
described in Section 1.2.10 can be used to determine if @xmath can be
written over a smaller field.

The rest of the algorithm is a straightforward application of the last
condition in Lemma 3.1 , except that it is sufficient to use the
condition for one non-trivial commutator @xmath . By Lemma 3.1 , if
@xmath then @xmath ; but if @xmath , then @xmath and we cannot have
@xmath .

It follows from Section 1.2.10 that the expected time complexity of the
algorithm is as stated. Since the MeatAxe is Las Vegas, this algorithm
is also Las Vegas. ∎

We are also interested in determining if a given group is a conjugate of
@xmath , without necessarily finding a conjugating element. We consider
the subgroups of @xmath and rule out all except those isomorphic to
@xmath . This relies on the fact that, up to Galois automorphisms,
@xmath has only one equivalence class of faithful representations in
@xmath , so if we can show that @xmath then @xmath is a conjugate of
@xmath .

###### Theorem 3.3.

There exists a Las Vegas algorithm that, given @xmath , decides whether
or not there exists @xmath such that @xmath . The algorithm has expected
time complexity @xmath field operations.

###### Proof.

Let @xmath . The algorithm proceeds as follows.

1.  Determine if @xmath is absolutely irreducible, and return false if
    not.

2.  Determine if @xmath preserves a non-degenerate symplectic form
    @xmath . If so we conclude that @xmath is a subgroup of a conjugate
    of @xmath , and if not then return false . Since @xmath is
    absolutely irreducible, the form is unique up to a scalar multiple.

3.  Conjugate @xmath so that it preserves the form @xmath . This amounts
    to finding a symplectic basis, i.e. finding an invertible matrix
    @xmath such that @xmath , which is easily done. Then @xmath
    preserves the form @xmath and thus @xmath so that we can apply
    @xmath .

4.  Determine if @xmath , where @xmath is the natural module for @xmath
    and @xmath is the automorphism from Lemma 2.13 . If so we conclude
    that @xmath is a subgroup of some conjugate of @xmath , and if not
    then return false .

5.  Determine if @xmath is a proper subgroup of @xmath , i.e. if it is
    contained in a maximal subgroup. This can be done using Lemma 3.1 .
    If so, then return false , else return true .

From the descriptions in Section 1.2.10 , the algorithms for finding a
preserved form and for module isomorphism testing are Las Vegas, with
the same expected time complexity as the MeatAxe. Hence we obtain a Las
Vegas algorithm, with the same expected time complexity as the algorithm
from Theorem 3.2 . ∎

#### 3.1.2. Finding an element of a stabiliser

In this section the matrix degree is constant, so we set @xmath . In
constructive membership testing for @xmath the essential problem is to
find an element of the stabiliser of a given point @xmath , expressed as
an @xmath in our given generators @xmath of @xmath . The idea is to map
@xmath to @xmath by a random @xmath , and then compute @xmath such that
@xmath , so that @xmath .

Thus the problem is to find an element that maps @xmath to @xmath , and
the idea is to search for it in double cosets of cyclic subgroups of
order @xmath . We first give an overview of the method.

Begin by selecting random @xmath such that @xmath has pseudo-order
@xmath , and consider the equation

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

in the two indeterminates @xmath . If we can solve this equation for
@xmath and @xmath , thus obtaining integers @xmath such that @xmath and
@xmath , then we have an element that maps @xmath to @xmath .

Since @xmath has order dividing @xmath , by Proposition 2.4 , @xmath is
conjugate to a matrix @xmath for some @xmath . This implies that we can
diagonalise @xmath and obtain a matrix @xmath such that @xmath . It
follows that if we define @xmath , @xmath and @xmath then ( 3.1 ) is
equivalent to

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

Now change indeterminates to @xmath and @xmath by letting @xmath and
@xmath , so that we obtain the following equation:

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

This determines four equations in @xmath and @xmath , and in Section
3.1.2 we will describe how to find solutions for them. A solution @xmath
determines @xmath , and hence also @xmath .

If @xmath then @xmath , so that there exist integers @xmath and @xmath
as above with @xmath and @xmath . These integers can be found by
computing discrete logarithms, since we also have @xmath and @xmath .
Hence we obtain a solution to ( 3.1 ) from the solution to ( 3.3 ). If
@xmath is a proper divisor of @xmath , then it might happen that @xmath
or @xmath , but by Proposition 2.6 we know that this is unlikely.

Thus the overall algorithm is as in Algorithm 3.1.2 . We prove that the
algorithm is correct in Section 3.1.2 .

##### Solving equation (3.3)

We will now show how to obtain the solutions of ( 3.3 ). It might happen
that there are no solutions, in which case the method described here
will detect this and return with failure.

By letting @xmath , @xmath and @xmath , we can write out ( 3.3 ) and
obtain

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

for some constant @xmath . Henceforth, we assume that @xmath for @xmath
, since this is the difficult case, and also very likely when @xmath is
large, as can be seen from Proposition 2.10 . A method similar to the
one described in this section will solve ( 3.3 ) when some @xmath and
Algorithm 3.1.2 does not assume that all @xmath .

For convenience, we denote the expressions in the parentheses at the
left hand sides of ( 3.4 ) as @xmath and @xmath respectively. Since
@xmath we obtain three equations

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

and in particular @xmath is a function of @xmath , since

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

If we eliminate @xmath and @xmath by using the first two equations into
the third in ( 3.5 ), we obtain

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

and by raising the first equation to the @xmath -th power and
substituting into the second, we obtain

  -- -------- -- -------
     @xmath      (3.8)
  -- -------- -- -------

Also, @xmath and if we proceed similarly, we obtain two more equations

  -- -------- -------- -- --------
     @xmath   @xmath      (3.9)
     @xmath   @xmath      (3.10)
  -- -------- -------- -- --------

Now ( 3.7 ), ( 3.8 ), ( 3.9 ) and ( 3.10 ) are equations in @xmath only,
and by multiplying them by suitable powers of @xmath , they can be
turned into polynomial equations such that @xmath only occurs to the
powers @xmath for @xmath and to lower powers that are independent of
@xmath . The suitable powers of @xmath are @xmath , @xmath , @xmath and
@xmath , respectively.

Thus we obtain the following four equations.

  -- -------- -- --------
     @xmath      (3.11)
  -- -------- -- --------

The @xmath and @xmath are polynomials in @xmath with degree independent
of @xmath , for @xmath and @xmath respectively, so ( 3.11 ) can be
considered a linear system in the variables @xmath for @xmath , with
coefficients @xmath and @xmath . Now the aim is to obtain a single
polynomial in @xmath of bounded degree. For this we need the following
conjecture.

###### Conjecture 3.4.

For every @xmath where @xmath , @xmath and @xmath , if we regard ( 3.11
) as simultaneous linear equations in the variables @xmath for @xmath ,
over the polynomial ring @xmath , then it has non-zero determinant.

In other words, the determinant of the coefficients @xmath is not the
zero polynomial.

###### Lemma 3.5.

Assume Conjecture 3.4 . Given @xmath and @xmath as in Conjecture 3.4 ,
there exists a univariate polynomial @xmath of degree at most @xmath ,
such that for every @xmath that is a solution for @xmath in ( 3.3 ), we
have @xmath .

###### Proof.

So far in this section we have shown that if we can solve ( 3.11 ) we
can also solve ( 3.3 ). From the four equations of ( 3.11 ) we can
eliminate @xmath . We can solve for @xmath from the fourth equation, and
substitute into the third, thus obtaining a rational expression with no
occurrence of @xmath . Continuing this way and substituting into the
other equations, we obtain an expression for @xmath in terms of the
@xmath and the @xmath only. This can be substituted into any of the
equations of ( 3.11 ), where @xmath for @xmath is obtained by powering
up the expression for @xmath . Thus we obtain a rational expression
@xmath of degree independent of @xmath . We now take @xmath to be the
numerator of @xmath .

In other words, we think of the @xmath as independent variables and of (
3.11 ) as a linear system in these variables, with coefficients in
@xmath . By Conjecture 3.4 we can solve this linear system.

Two possible problems can occur: @xmath is identically zero or some of
the denominators of the expressions for @xmath , @xmath turn out to be
@xmath . However, Conjecture 3.4 rules out these possibilities. By
Cramer’s rule, the expression for @xmath is a rational expression where
the numerator is a determinant, so it consists of sums of products of
@xmath and @xmath . Each product consists of three @xmath and one @xmath
. By considering the calculations leading up to ( 3.11 ), it is clear
that each of the products has degree at most @xmath . Therefore the
expression for @xmath and hence also @xmath has degree at most @xmath .

We have only done elementary algebra to obtain @xmath from ( 3.11 ), and
it is clear that ( 3.11 ) was obtained from ( 3.4 ) by elementary means
only. Hence all solutions @xmath to ( 3.4 ) must also satisfy @xmath ,
although there may not be any such solutions, and @xmath may also have
other zeros. ∎

###### Corollary 3.6.

Assume Conjecture 3.4 . There exists a Las Vegas algorithm that, given
@xmath and @xmath as in Conjecture 3.4 , finds all @xmath that are
solutions of ( 3.3 ). The algorithm has expected time complexity @xmath
field operations.

###### Proof.

Let @xmath be the polynomial constructed in Lemma 3.5 . To find all
solutions to ( 3.3 ), we find the zeros @xmath of @xmath , compute the
corresponding @xmath for each zero @xmath using ( 3.6 ), and check which
pairs @xmath satisfy ( 3.4 ). These pairs must be all solutions of ( 3.3
).

The only work needed is simple matrix arithmetic and finding the roots
of a polynomial of bounded degree over @xmath . Hence the expected time
complexity is @xmath field operations. The algorithm is Las Vegas, since
by Theorem 1.1 the algorithm for finding the roots of @xmath is Las
Vegas, with this expected time complexity. ∎

By following the procedure outlined in Lemma 3.5 , it is straightforward
to obtain an expression for @xmath , where the coefficients are
expressions in the entries of @xmath , @xmath and @xmath , but we will
not display it here, since it would take up too much space.

##### Correctness and complexity

There are two issues when considering the correctness of Algorithm 3.1.2
. Using the notation in the algorithm, we have to show that ( 3.3 ) has
a solution with high probability, and that the integers @xmath and
@xmath are positive with high probability.

The algorithm in Corollary 3.6 tries to find an element in the double
coset @xmath , where @xmath , and we will see that this succeeds with
high probability when @xmath , which is very likely.

If the element @xmath has order precisely @xmath , then from the
discussion at the beginning of Section 3.1.2 , we know that the integers
@xmath and @xmath will be positive. By Proposition 2.6 we know that it
is likely that @xmath has order precisely @xmath rather than just a
divisor of @xmath .

###### Lemma 3.7.

Assume Conjecture 3.4 . Let @xmath and let @xmath and @xmath be given,
such that @xmath . Let @xmath be uniformly random. If @xmath , then

  -- -- -- --------
           (3.12)
  -- -- -- --------

where @xmath is the polynomial constructed in Lemma 3.5 . If instead
@xmath then

  -- -- -- --------
           (3.13)
  -- -- -- --------

###### Proof.

If @xmath then by Lemma 2.7 , @xmath , and hence @xmath .

On the other hand, for every @xmath we have

  -- -------- -- --------
     @xmath      (3.14)
  -- -------- -- --------

since this is the equation we consider in Section 3.1.2 , and from Lemma
3.5 we know that all solutions must be roots of @xmath . Thus @xmath .
Since @xmath is uniformly random from @xmath , and @xmath , the result
follows.

If @xmath then @xmath , and @xmath if @xmath does not fix @xmath . By
Proposition 2.4 , the number of cyclic subgroups of order @xmath is
@xmath and @xmath such subgroups fix @xmath . Moreover, if @xmath fixes
@xmath then @xmath . Thus

  -- -- -- --------
           (3.15)
  -- -- -- --------

and the result follows. ∎

###### Theorem 3.8.

Assume Conjecture 3.4 and an oracle for the discrete logarithm problem
in @xmath . Algorithm 3.1.2 is a Las Vegas algorithm with expected time
complexity @xmath field operations. The length of the returned @xmath is
@xmath .

###### Proof.

We use the notation from the algorithm. Let @xmath , @xmath , @xmath and
@xmath . Corollary 3.6 implies that line 3.1.2 will succeed if @xmath .
If @xmath , then @xmath , and the previous condition is equivalent to
@xmath . Moreover, if @xmath then line 3.1.2 will always succeed.

Let @xmath be the probability that the return statement is reached. Then
@xmath satisfies the following inequality.

  -- -- -- --------
           (3.16)
  -- -- -- --------

Since @xmath is uniformly random, using Theorem 2.3 we obtain

  -- -------- -- --------
     @xmath      (3.17)
  -- -------- -- --------

From Proposition 2.6 and Lemma 3.7 we obtain

  -- -- -- --------
           (3.18)
  -- -- -- --------

By Proposition 2.6 , the expected number of iterations of the outer
repeat statement is @xmath . The expected number of random selections to
find @xmath is @xmath . By Theorem 1.1 , diagonalising a matrix uses
expected @xmath field operations, since it involves finding the
eigenvalues, i.e. finding the roots of a polynomial of constant degree
over @xmath . Clearly, the expected time complexity for finding @xmath
is @xmath field operations.

From Corollary 3.6 , it follows that line 3.1.2 uses @xmath field
operations. We conclude that Algorithm 3.1.2 uses expected @xmath field
operations.

Each call to Algorithm 3.1.2 uses independent random elements, so the
double cosets under consideration are uniformly random and independent.
Therefore the elements returned by Algorithm 3.1.2 must be uniformly
random. The returned @xmath is composed of @xmath s of @xmath and @xmath
, both of which have length @xmath because of the number of iterations
of the loops. ∎

###### Corollary 3.9.

Assume Conjecture 3.4 and an oracle for the discrete logarithm problem
in @xmath . There exists a Las Vegas algorithm that, given @xmath such
that @xmath and @xmath , finds a uniformly random @xmath , expressed as
an @xmath in @xmath . The algorithm has expected time complexity @xmath
field operations. The length of the returned @xmath is @xmath .

###### Proof.

We compute @xmath as follows.

1.  Find random @xmath . Let @xmath and repeat until @xmath .

2.  Use Algorithm 3.1.2 to find @xmath such that @xmath .

3.  Now @xmath .

We see from Algorithm 3.1.2 that the choice of @xmath does not depend on
@xmath . Hence @xmath is uniformly random, since @xmath is uniformly
random. Therefore this is a Las Vegas algorithm. The probability that
@xmath is @xmath , so the dominating term in the complexity is the call
to Algorithm 3.1.2 , with expected time complexity given by Theorem 3.8
.

The element @xmath will be expressed as an @xmath in @xmath , since
@xmath is random and elements from Algorithm 3.1.2 are expressed as
@xmath s. Clearly the length of the @xmath is the same as the length of
the @xmath s from Algorithm 3.1.2 . ∎

###### Remark 3.10.

In fact, the algorithm of Corollary 3.9 works in any conjugate of @xmath
, since in Algorithm 3.1.2 the diagonalisation always moves into the
standard copy.

#### 3.1.3. Constructive membership testing

We will now give an algorithm for constructive membership testing in
@xmath . Given a set of generators @xmath , such that @xmath , and given
@xmath , we want to express @xmath as an @xmath in @xmath . The matrix
degree is constant here, so we set @xmath . Membership testing is
straightforward, using the first steps from the algorithm in Theorem 3.2
, and will not be considered here.

##### Preprocessing

The algorithm for constructive membership testing has a preprocessing
step and a main step. The preprocessing step consists of finding
“standard generators” for @xmath and @xmath . In the case of @xmath the
standard generators are defined as matrices @xmath for some unspecified
@xmath , such that @xmath and @xmath form vector space bases of @xmath
over @xmath (so @xmath ).

###### Lemma 3.11.

There exist algorithms for the following row reductions.

1.   Given @xmath , find @xmath expressed in the standard generators,
    such that @xmath .

2.   Given @xmath , find @xmath expressed in the standard generators,
    such that @xmath .

3.   Given @xmath , find @xmath expressed in the standard generators,
    such that @xmath .

Analogous algorithms exist for @xmath . If the standard generators are
expressed as @xmath s of length @xmath , the elements returned will have
length @xmath . The time complexity of the algorithms is @xmath field
operations.

###### Proof.

The algorithms are as follows.

1.  1.  Solve a linear system of size @xmath to construct the linear
        combination @xmath with @xmath . Let @xmath and @xmath , so that
        @xmath for some @xmath .

    2.  Solve a linear system of size @xmath to construct the linear
        combination @xmath with @xmath . Let @xmath and @xmath , so that
        @xmath .

    3.  Now @xmath .

2.  Analogous to the previous case.

3.  1.  Normalise @xmath so that @xmath for some @xmath . Solve a linear
        system of size @xmath to construct the linear combination @xmath
        with @xmath . Let @xmath .

    2.  Solve a linear system of size @xmath to construct the linear
        combination @xmath with @xmath . Let @xmath

    3.  Now @xmath maps @xmath to @xmath .

Clearly the dominating term in the time complexity is the solving of the
linear systems, which requires @xmath field operations. The elements
returned are constructed using @xmath multiplications, hence the length
of the @xmath follows. ∎

###### Theorem 3.12.

Assume Conjecture 3.4 and an oracle for the discrete logarithm problem
in @xmath . The preprocessing step is a Las Vegas algorithm that finds
standard generators for @xmath and @xmath , as @xmath s in @xmath of
length @xmath . It has expected time complexity @xmath field operations.

###### Proof.

The preprocessing step proceeds as follows.

1.  Find random elements @xmath and @xmath using the algorithm from
    Corollary 3.9 . Repeat until @xmath can be diagonalised to @xmath ,
    where @xmath and @xmath does not lie in a proper subfield of @xmath
    . Do similarly for @xmath .

2.  Find random elements @xmath and @xmath using the algorithm from
    Corollary 3.9 . Let @xmath , @xmath . Repeat until @xmath .

3.  Let @xmath and @xmath . As standard generators for @xmath we now
    take

      -- -------- -- --------
         @xmath      (3.19)
      -- -------- -- --------

    and similarly we obtain @xmath for @xmath .

It follows from ( 2.9 ) and ( 2.12 ) that ( 3.19 ) provides the standard
generators for @xmath . These are expressed as @xmath s in @xmath ,
since this is true for the elements returned from the algorithm
described in Corollary 3.9 . Hence the algorithm is Las Vegas.

By Corollary 3.9 , the expected time to find @xmath and @xmath is @xmath
, and these are uniformly distributed independent random elements. The
elements of order dividing @xmath can be diagonalised as required. By
Theorem 2.1 , the proportion of elements of order @xmath in @xmath and
@xmath is @xmath . Hence the expected time for the first step is @xmath
field operations.

Similarly, by Proposition 2.11 the expected time for the second step is
@xmath field operations.

By the remark preceding the theorem, @xmath determines two sets of field
elements @xmath and @xmath . In this case each @xmath and @xmath , for
some fixed @xmath , where @xmath is as in the algorithm. Since @xmath
does not lie in a proper subfield, these sets form vector space bases of
@xmath over @xmath .

To determine if @xmath or @xmath diagonalise to some @xmath it is
sufficient to consider the eigenvalues on the diagonal, since both
@xmath and @xmath are triangular. To determine if @xmath lies in a
proper subfield, it is sufficient to determine if @xmath , for some
proper divisor @xmath of @xmath . Hence the dominating term in the
complexity is the first step. ∎

##### Main algorithm

Now we consider the algorithm that expresses @xmath as an @xmath in
@xmath . It is given formally as Algorithm 3.1.3 .

###### Theorem 3.13.

Algorithm 3.1.3 is a Las Vegas algorithm with expected time complexity
@xmath field operations. The length of the @xmath is @xmath .

###### Proof.

First observe that since @xmath is randomly chosen we obtain it as an
@xmath . On line 3.1.3 we check if @xmath fixes a point, and from
Proposition 2.6 we see that the probability that the test succeeds is at
least @xmath .

The elements found at lines 3.1.3 and 3.1.3 are constructed using Lemma
3.11 , so we can obtain them as @xmath s.

The element @xmath found at line 3.1.3 clearly has trace @xmath , and it
is constructed using Lemma 3.11 , so we obtain it as an @xmath . From
Lemma 2.8 we know that @xmath is conjugate to @xmath and therefore must
fix @xmath points of @xmath . Hence lines 3.1.3 and 3.1.3 make sense,
and the elements found are constructed using Lemma 3.11 and therefore we
obtain them as @xmath s.

The only elements in @xmath that are conjugate to @xmath are @xmath , so
clearly @xmath must be one of them.

Finally, the elements that make up @xmath were found as @xmath s, and it
is clear that if we evaluate @xmath we obtain @xmath . Hence the
algorithm is Las Vegas.

From Lemma 3.11 it follows that lines 3.1.3 , 3.1.3 , 3.1.3 , 3.1.3 and
3.1.3 use @xmath field operations.

Finding the fixed points of @xmath , and performing the check at line
3.1.3 only amounts to considering eigenspaces, which uses @xmath field
operations. Thus the expected time complexity of the algorithm is @xmath
field operations.

The @xmath s of the standard generators have length @xmath . Because of
the row operations, @xmath will have length @xmath .

∎

#### 3.1.4. Conjugates of the standard copy

Now we assume that we are given @xmath such that @xmath is a conjugate
of @xmath , and we turn to the problem of finding some @xmath such that
@xmath , thus obtaining an isomorphism to the standard copy. The matrix
degree is constant here, so we set @xmath

###### Lemma 3.14.

There exists a Las Vegas algorithm that, given @xmath such that @xmath
for some @xmath , finds a point @xmath . The algorithm has expected time
complexity

  -- -------- --
     @xmath   
  -- -------- --

field operations.

###### Proof.

Clearly @xmath is the set on which @xmath acts doubly transitively. For
a matrix @xmath we see that the eigenspaces corresponding to the
eigenvalues @xmath will be in @xmath . Moreover, every element of order
dividing @xmath , in every conjugate @xmath of @xmath , will have
eigenvalues of the form @xmath , @xmath for some @xmath , and the
eigenspaces corresponding to @xmath will lie in the set on which @xmath
acts doubly transitively.

Hence to find a point @xmath it is sufficient to find a random @xmath ,
of order dividing @xmath . We compute the pseudo-order using expected
@xmath field operations, and by Proposition 2.6 , the expected time to
find the element is @xmath field operations. We then find the
eigenspaces of @xmath .

Clearly this is a Las Vegas algorithm with the stated time complexity. ∎

###### Lemma 3.15.

There exists a Las Vegas algorithm that, given @xmath such that @xmath
where @xmath , finds a diagonal matrix @xmath such that @xmath , using
expected

  -- -------- --
     @xmath   
  -- -------- --

field operations.

###### Proof.

Let @xmath . Since @xmath , @xmath must preserve the symplectic form

  -- -------- -- --------
     @xmath      (3.20)
  -- -------- -- --------

where @xmath is given by ( 2.24 ). Using the MeatAxe, we can find this
form, which is determined up to a scalar multiple. Hence the diagonal
matrix @xmath , that we want to find, is also determined up to a scalar
multiple (and up to multiplication by a diagonal matrix in @xmath ).

Since @xmath must take @xmath to @xmath , we must have @xmath and @xmath
. Because @xmath is determined up to a scalar multiple, we can choose
@xmath and @xmath . Hence it only remains to determine @xmath and @xmath
.

To conjugate @xmath into @xmath , we must have @xmath for every @xmath ,
which is the set on which @xmath acts doubly transitively. By Lemma 3.14
, we can find @xmath , and the condition @xmath is given by ( 2.13 ) and
amounts to

  -- -------- -- --------
     @xmath      (3.21)
  -- -------- -- --------

which is a polynomial equation in the two variables @xmath and @xmath .

Notice that we can consider @xmath to be the variable, instead of @xmath
, since if @xmath , then @xmath . Similarly, we can let @xmath be the
variable instead of @xmath , since if @xmath then @xmath . Thus instead
of ( 3.21 ) we obtain a linear equation

  -- -------- -- --------
     @xmath      (3.22)
  -- -------- -- --------

in the variables @xmath . Thus the complete algorithm for finding @xmath
proceeds as follows.

1.  Find the form @xmath that is preserved by @xmath , using the
    MeatAxe.

2.  Find @xmath using Lemma 3.14 .

3.  Let @xmath and find @xmath using Lemma 3.14 until the following
    linear system in the variables @xmath and @xmath is non-singular.

      -- -------- -- --------
         @xmath      (3.23)
      -- -------- -- --------

    By Proposition 2.10 , the probability of finding such a @xmath is
    @xmath .

4.  Let @xmath be a solution to the linear system. The diagonal matrix
    @xmath now satisfies @xmath .

By Lemma 3.14 and Section 1.2.10 , this is a Las Vegas algorithm that
uses expected @xmath field operations. ∎

###### Lemma 3.16.

There exists a Las Vegas algorithm that, given subsets @xmath , @xmath
and @xmath of @xmath such that @xmath and @xmath , respectively, where
@xmath , @xmath for some @xmath and @xmath , finds @xmath such that
@xmath for some diagonal matrix @xmath . The algorithm has expected time
complexity @xmath field operations.

###### Proof.

Notice that the natural module @xmath of @xmath is uniserial with four
non-zero submodules, namely @xmath for @xmath . Hence the same is true
for @xmath and @xmath (but the submodules will be different) since they
lie in conjugates of @xmath .

Now the algorithm proceeds as follows.

1.  Let @xmath be the natural module for @xmath and @xmath . Find
    composition series @xmath and @xmath using the MeatAxe.

2.  Let @xmath , @xmath , @xmath and @xmath . For each @xmath , choose
    @xmath .

3.  Now let @xmath be the matrix such that @xmath has @xmath as row
    @xmath , for @xmath .

We now motivate the second step of the algorithm.

One can choose a basis that exhibits the series @xmath , in other words,
such that the matrices acting on the module are lower triangular with
respect to this basis. Similarly one can choose a basis that exhibits
the series @xmath .

On the other hand, since @xmath , there exists @xmath such that @xmath
and @xmath . If we let @xmath , then @xmath and @xmath consist of lower
and upper triangular matrices, respectively. Thus, the rows of @xmath
form a basis of @xmath that exhibits the series @xmath and the series
@xmath in reversed order.

With respect to this basis, it is clear that @xmath , @xmath and that
all the @xmath are distinct. Hence the basis chosen in the algorithm
exhibits the series @xmath , and it exhibits the series @xmath in
reverse order. Therefore the chosen @xmath satisfies that @xmath is
lower triangular and @xmath is upper triangular. The former implies that
@xmath is a lower triangular matrix, and the latter that it is an upper
triangular matrix, and hence it must diagonal.

Thus the matrix @xmath found in the algorithm satisfies @xmath for some
diagonal matrix @xmath . Since @xmath , the algorithm returns a correct
result, and it is Las Vegas because the MeatAxe is Las Vegas. Clearly
the expected time complexity is the same as the MeatAxe, so the
algorithm uses @xmath field operations. ∎

###### Theorem 3.17.

Assume Conjecture 3.4 . There exists a Las Vegas algorithm that, given a
conjugate @xmath of @xmath , finds @xmath such that @xmath . The
algorithm has expected time complexity @xmath field operations.

###### Proof.

Let @xmath . By Remark 3.10 , we can use Corollary 3.9 in @xmath , and
hence we can find generators for a stabiliser of a point in @xmath ,
using the algorithm described in Theorem 3.12 . In this case we do not
need the elements as @xmath s, so a discrete log oracle is not
necessary.

1.  Find points @xmath using Lemma 3.14 . Repeat until @xmath .

2.  Find generating sets @xmath and @xmath such that @xmath and @xmath
    using the first three steps of the algorithm from the proof of
    Theorem 3.12 .

3.  Find @xmath such that @xmath for some diagonal matrix @xmath , using
    Lemma 3.16 .

4.  Find a diagonal matrix @xmath using Lemma 3.15 .

5.  Now @xmath satisfies that @xmath .

Be Lemma 3.14 , 3.16 and 3.15 , and the proof of Theorem 3.12 , this is
a Las Vegas algorithm with expected time complexity as stated. ∎

#### 3.1.5. Tensor decomposition

Now assume that @xmath where @xmath , @xmath and @xmath for some @xmath
. Then @xmath , where @xmath is the Frobenius automorphism. Let @xmath
be the given module of @xmath of dimension @xmath and let @xmath be the
natural module of @xmath of dimension @xmath . From Section 1.2.7 and
Section 2.1.3 we know that

  -- -------- -- --------
     @xmath      (3.24)
  -- -------- -- --------

for some integers @xmath . In fact, we may assume that @xmath and
clearly @xmath . As described in Section 1.2.7 , we now want to tensor
decompose @xmath to obtain an effective isomorphism from @xmath to
@xmath .

##### The main algorithm

We now describe our main algorithm that finds a tensor decomposition of
@xmath when @xmath is large. It is sufficient to find a flat in @xmath .
For @xmath , let @xmath be the image of the representation corresponding
to @xmath , and let @xmath be an isomorphism. Our goal is then to find
@xmath effectively for some @xmath .

We begin with an overview of the method. Our approach for finding a flat
in @xmath is to consider eigenspaces of an element of @xmath of order
dividing @xmath . By Proposition 2.6 we know that such elements are easy
to find by random search.

Let @xmath where @xmath , and let @xmath . By Proposition 2.4 we know
that for @xmath , @xmath has four distinct eigenvalues @xmath and @xmath
for some @xmath . Also, the eigenspaces of @xmath have dimension @xmath
. Our method for finding a flat in @xmath is to construct a line as a
suitable sum of eigenspaces of @xmath .

Let @xmath be the multiset of eigenvalues of @xmath , so that @xmath and
every element of @xmath has the form

  -- -------- -- --------
     @xmath      (3.25)
  -- -------- -- --------

where each @xmath and each @xmath . A set @xmath that satisfies

-   @xmath

-   @xmath or @xmath for each @xmath

is a set of base values for @xmath . Clearly @xmath is easily calculated
from @xmath .

Moreover @xmath , and since @xmath we must have @xmath for @xmath . For
every @xmath we have @xmath and therefore @xmath .

First we try to find a set of base values for @xmath .

###### Conjecture 3.18.

Assume @xmath . Let @xmath and

  -- -------- -- --------
     @xmath      (3.26)
  -- -------- -- --------

Then the following hold:

-   @xmath contains a set of base values for @xmath .

-    If the twists do not have a subsequence @xmath , then @xmath and
    hence @xmath consists of the base values and their inverses.

If the twists have a subsequence of the form in the Conjecture, or more
generally a subsequence of length @xmath , then @xmath for every @xmath
. Hence we need more conditions to extract the base values.

###### Conjecture 3.19.

Let @xmath be as in Lemma 3.18 . Define @xmath to be those @xmath for
which there exists @xmath such that @xmath for every @xmath . Then
@xmath and hence @xmath consists of the base values and their inverses.

Let @xmath denote the sum of eigenspaces of @xmath corresponding to the
eigenvalues @xmath and @xmath , where each @xmath ranges over @xmath .

###### Lemma 3.20.

If @xmath for some @xmath , then @xmath is a line in @xmath .

###### Proof.

For each @xmath , let @xmath be an eigenvector of @xmath for the
eigenvalue @xmath , where @xmath . Observe that @xmath contains the
following subspace.

  -- -------- --
     @xmath   
  -- -------- --

Clearly, @xmath is of the form @xmath where @xmath , so @xmath is a line
in @xmath of dimension @xmath .

If @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

and hence @xmath . Therefore @xmath , so if @xmath then @xmath and thus
@xmath is a line in @xmath .

∎

The success probability of the algorithm for finding a flat relies on
the following Conjecture.

###### Conjecture 3.21.

Let @xmath with @xmath be fixed. If @xmath and @xmath , then for every
absolutely irreducible @xmath with @xmath and every @xmath with @xmath ,
we have @xmath for some @xmath .

The algorithm for finding a flat is shown as Algorithm 3.1.5 .

###### Theorem 3.22.

Assume Conjectures 3.18 , 3.19 and 3.21 . Algorithm 3.1.5 is a Las Vegas
algorithm. The algorithm has expected time complexity

  -- -------- --
     @xmath   
  -- -------- --

field operations.

###### Proof.

The expected number of iterations in the initial loop is @xmath . Hence
the expected time for the loop is @xmath field operations. If @xmath
then line 3.1.5 will succeed, and Conjecture 3.21 asserts that line
3.1.5 will succeed for some @xmath . If @xmath is a proper divisor of
@xmath then these lines might still succeed, and the probability that
@xmath is high. By Proposition 2.6 , the expected number of iterations
of the outer repeat statement is @xmath .

If line 3.1.5 is reached, then the algorithm returns a correct result
and hence it is Las Vegas.

To find the eigenvalues of @xmath , we calculate the characteristic
polynomial of @xmath using @xmath field operations, and find its roots
using Theorem 1.1 . By Conjectures 3.18 and 3.19 , the rest of the
algorithm uses @xmath field operations. Thus the theorem follows. ∎

##### Small field approach

When @xmath is small, the feasibility of Algorithm 3.1.5 is not
guaranteed. In that case the approach is to find standard generators of
@xmath using permutation group techniques, then enumerate all tensor
products of the form ( 3.24 ) and for each one we determine if it is
isomorphic to @xmath .

Since @xmath is polynomial in @xmath , this will turn out to be an
efficient algorithm which is given as Algorithm 3.1.5 . It finds a
permutation representation of @xmath , which is done using the following
result.

###### Lemma 3.23.

There exists a Las Vegas algorithm that, given @xmath such that @xmath
with @xmath and @xmath , finds an effective injective homomorphism
@xmath where @xmath . The algorithm has expected time complexity @xmath
field operations.

###### Proof.

By Theorem 2.1 , @xmath acts doubly transitively on a set of size @xmath
. Hence @xmath also acts doubly transitively on @xmath , where @xmath ,
and we can find the permutation representation of @xmath if we can find
a point @xmath . The set @xmath is a set of projective points of @xmath
, and the algorithm proceeds as follows.

1.  Choose random @xmath . Repeat until @xmath .

2.  Choose random @xmath and let @xmath . Repeat until @xmath and @xmath
    .

3.  Find a composition series for the module @xmath of @xmath and let
    @xmath be the submodule of dimension @xmath in the series.

4.  Find the orbit @xmath and compute the permutation group @xmath of
    @xmath on @xmath , together with an effective isomorphism @xmath .

By Proposition 2.4 , elements in @xmath of order dividing @xmath fix two
points of @xmath , and hence @xmath for some @xmath if and only if
@xmath and @xmath have a common fixed point. All composition factors of
@xmath have dimension @xmath , so a composition series of @xmath must
contain a submodule @xmath of dimension @xmath . This submodule is a
fixed point for @xmath and its orbit must have size @xmath , since
@xmath and @xmath . It follows that @xmath .

All elements of @xmath of even order lie in the derived group of a
stabiliser of some point, which is also a Sylow @xmath -subgroup of
@xmath , and the exponent of this subgroup is @xmath . Hence @xmath if
and only if @xmath lie in a stabiliser of some point, if and only if
@xmath and @xmath have a common fixed point.

To find the orbit @xmath we can compute a Schreier tree on the
generators @xmath , with @xmath as root, using @xmath field operations.
Then @xmath can be computed for any @xmath using @xmath field
operations, by computing the permutation on @xmath induced by @xmath .
Hence @xmath is effective, and its image @xmath is found by computing
the image of each element of @xmath . Therefore the algorithm is correct
and it is clearly Las Vegas.

We find @xmath using expected @xmath field operations and we find @xmath
using expected @xmath field operations. Then @xmath is found using the
MeatAxe, in expected @xmath field operations. Thus the result follows. ∎

###### Proposition 3.24.

Let @xmath such that @xmath . There exists a Las Vegas algorithm that
finds @xmath as @xmath s in @xmath such that the map

  -- -------- -- --------
     @xmath      (3.27)
  -- -------- -- --------

is an isomorphism. Its time complexity is @xmath . The length of the
returned @xmath s are @xmath .

###### Proof.

Follows from [ BB07 , Theorem 1] . ∎

###### Theorem 3.25.

Assume Conjecture 3.24 . Algorithm 3.1.5 is a Las Vegas algorithm with
expected time complexity

  -- -------- --
     @xmath   
  -- -------- --

field operations.

###### Proof.

The permutation representation @xmath can be found using Lemma 3.23 ,
and the elements @xmath are found using Conjecture 3.24 . Testing if
modules are isomorphic can be done using the MeatAxe.

If the algorithm returns an element @xmath then the change of basis
determined by @xmath exhibits @xmath as a tensor product, so the
algorithm is Las Vegas.

The lengths of the @xmath s of @xmath is @xmath , so we need @xmath
field operations to obtain @xmath . The set @xmath has size @xmath .
Module isomorphism testing uses @xmath field operations. Hence by
Conjecture 3.24 and Theorem 3.23 the time complexity of the algorithm is
as stated. ∎

#### 3.1.6. Constructive recognition

Finally, we can now state and prove our main theorem.

###### Theorem 3.26.

Assume the Suzuki Conjectures and an oracle for the discrete logarithm
problem in @xmath . There exists a Las Vegas algorithm that, given
@xmath satisfying the assumptions in Section 1.2.7 , with @xmath ,
@xmath and @xmath , finds an effective isomorphism @xmath and performs
preprocessing for constructive membership testing. The algorithm has
expected time complexity @xmath field operations.

The inverse of @xmath is also effective. Each image of @xmath can be
computed using @xmath field operations, and each pre-image using
expected

  -- -------- --
     @xmath   
  -- -------- --

field operations.

###### Proof.

Let @xmath be the module of @xmath . From Section 2.1.3 we know that
@xmath where @xmath . The algorithm proceeds as follows:

1.  If @xmath then use Theorem 3.17 to obtain @xmath such that @xmath ,
    and hence an effective isomorphism @xmath defined by @xmath .

2.  If @xmath , use Algorithm 3.1.5 to find a flat @xmath . Then use the
    tensor decomposition algorithm described in Section 1.2.10 with
    @xmath , to obtain @xmath such the change of basis determined by
    @xmath exhibits @xmath as a tensor product @xmath , with @xmath .
    Let @xmath and @xmath be the images of the corresponding
    representations.

3.  If instead @xmath then use Algorithm 3.1.5 to find @xmath .

4.  Define @xmath as @xmath and let @xmath . Then @xmath is conjugate to
    @xmath .

5.  Use Theorem 3.17 to get @xmath such that @xmath .

6.  An effective isomorphism @xmath is given by @xmath .

The map @xmath is straightforward to compute, since given @xmath it only
involves dividing @xmath into submatrices of degree @xmath , checking
that they are scalar multiples of each other and returning the @xmath
matrix consisting of these scalars. Since @xmath might not lie in @xmath
, but only in @xmath , the result of @xmath might not have determinant
@xmath . However, since every element of @xmath has a unique @xmath th
root, we can easily scale the matrix to have determinant @xmath . Hence
by Theorems 3.22 , 3.25 and 3.17 , the algorithm is Las Vegas and any
image of @xmath can be computed using @xmath field operations.

In the case where we use Algorithm 3.1.5 we have @xmath , hence @xmath
and @xmath . The expected time complexity to find @xmath in this case is
@xmath field operations.

By Theorem 3.22 , finding @xmath uses @xmath field operations. From
Section 1.2.10 , finding @xmath uses @xmath field operations when a flat
@xmath is given. By Theorem 3.17 , finding @xmath uses expected @xmath
field operations, given a random element oracle for @xmath that finds a
random element using @xmath field operations. In this case we can
construct random elements for @xmath using the random element oracle for
@xmath , and then we will find them in @xmath field operations.

Hence the expected time complexity is as stated. Finally, @xmath is
computed by first using Algorithm 3.1.3 to obtain an @xmath of @xmath
and then evaluating it on @xmath . The necessary precomputations in
Theorem 3.12 have already been made during the application of Theorem
3.17 , and hence it follows from Theorem 3.13 that the time complexity
for computing the pre-image of @xmath is as stated. ∎

### 3.2. Small Ree groups

Here we will use the notation from Section 2.2 . We will refer to
Conjectures 3.44 , 3.50 , 3.51 , 3.52 , 3.57 and 3.60 simultaneously as
the small Ree Conjectures . We now give an overview of the algorithm for
constructive recognition and constructive membership testing. It will be
formally proved as Theorem 3.62 .

1.  Given a group @xmath , satisfying the assumptions in Section 1.2.7 ,
    we know from Section 2.2.3 that the module of @xmath is isomorphic
    to a tensor product of twisted copies of either the natural module
    of @xmath or its @xmath -dimensional module. Hence we first tensor
    decompose this module. This is described in Section 3.2.5 .

2.  The resulting group has degree @xmath or @xmath . In the latter case
    we need to decompose it into degree @xmath . This is described in
    Section 3.2.6 .

3.  Now we have a group of degree @xmath , so it is a conjugate of the
    standard copy. We therefore find a conjugating element. This is
    described in Section 3.2.4 .

4.  Finally we are in @xmath . Now we can perform preprocessing for
    constructive membership testing and other problems we want to solve.
    This is described in Section 3.2.3 .

Given a discrete logarithm oracle, the whole process has time complexity
slightly worse than @xmath field operations, assuming that @xmath is
given by a bounded number of generators.

#### 3.2.1. Recognition

We now consider the question of non-constructive recognition of @xmath ,
so we want to find an algorithm that, given @xmath , decides whether or
not @xmath . We will only consider this problem for the standard copy,
i.e. we will only answer the question whether or not @xmath .

###### Theorem 3.27.

There exists a Las Vegas algorithm that, given @xmath , decides whether
or not @xmath . The algorithm has expected time complexity @xmath field
operations.

###### Proof.

Let @xmath , with natural module @xmath . The algorithm proceeds as
follows:

1.  Determine if @xmath and return false if not. All the following steps
    must succeed in order to conclude that a given @xmath also lies in
    @xmath .

    1.  Determine if @xmath , which is true if @xmath and if @xmath ,
        where @xmath is given by ( 2.40 ) and where @xmath denotes the
        transpose of @xmath .

    2.  Determine if @xmath , which is true if @xmath preserves the
        algebra multiplication @xmath of @xmath . The multiplication
        table can easily be precomputed using the fact that if @xmath
        then @xmath , where @xmath is a generator of @xmath (which has
        dimension @xmath ).

    3.  Determine if @xmath is a fixed point of the exceptional outer
        automorphism of @xmath , mentioned in Section 2.2.2 . Computing
        the automorphism amounts to extracting a submatrix of the
        exterior square of @xmath and then replacing each matrix entry
        @xmath by @xmath .

2.  If @xmath is not a proper subgroup of @xmath , or equivalently if
    @xmath is not contained in a maximal subgroup, return true .
    Otherwise return false . By Proposition 2.20 , it is sufficient to
    determine if @xmath cannot be written over a smaller field and if
    @xmath is irreducible. This can be done using the algorithms
    described in Sections 1.2.10 and 1.2.10 .

Since the matrix degree is constant, the complexity of the first step of
the algorithm is @xmath field operations. For the same reason, the
expected time of the algorithms in Sections 1.2.10 and 1.2.10 is @xmath
field operations. Hence our recognition algorithm has expected time as
stated, and it is Las Vegas since the MeatAxe is Las Vegas. ∎

#### 3.2.2. Finding an element of a stabiliser

Let @xmath . In this section the matrix degree is constant, so we set
@xmath . The algorithm for the constructive membership problem needs to
find independent random elements of @xmath for a given point @xmath .
This is straightforward if, for any pair of points @xmath , we can find
@xmath as an @xmath in @xmath such that @xmath .

The general idea is to find an involution @xmath by random search, and
then compute @xmath using the Bray algorithm described in Section 1.2.9
. The given module restricted to the centraliser splits up as in
Proposition 2.18 , and the points @xmath restrict to points in the
@xmath -dimensional submodule. If the restrictions satisfy certain
conditions, we can then find an element @xmath that maps these
restricted points to each other, and we obtain @xmath as an @xmath in
the generators of @xmath using Theorem 1.12 . It turns out that with
high probability, we can then multiply @xmath by an element that fixes
the restriction of @xmath so that @xmath also maps @xmath to @xmath . A
discrete logarithm oracle is needed in that step. Since the Bray
algorithm produces generators for the centraliser as @xmath s in @xmath
, we obtain @xmath as an @xmath in @xmath .

If any of the steps fail, we can try again with another involution
@xmath , so using this method we can map @xmath to @xmath for any pair
of points @xmath .

It should be noted that it is easy to find involutions using the method
described in Section 1.2.5 , since by Corollary 2.24 it is easy to find
elements of even order by random search.

##### The involution centraliser

To use the Bray algorithm we need to provide an algorithm that
determines if the whole centraliser has been generated. Since we know
what the structure of the centraliser should be, this poses no problem.
If we have the whole centraliser, the derived group should be @xmath ,
and by Proposition 2.27 , with high probability it is sufficient to
compute two random elements of the derived group. Random elements of the
derived group can be found as described in Section 1.2.6 .

We can therefore find the involution centraliser @xmath and @xmath .

###### Lemma 3.28.

There exists a Las Vegas algorithm that, given @xmath such that @xmath
for some involution @xmath , finds

-    the submodule @xmath described in Proposition 2.18 ,

-    an effective @xmath -module homomorphism @xmath ,

-    the induced map @xmath ,

-    the corresponding map @xmath from the @xmath -dimensional
    representation of @xmath to the @xmath -dimensional representation.

The maps can be computed using @xmath field operations. The algorithm
has expected time complexity @xmath field operations.

###### Proof.

This is a straightforward application of the MeatAxe, so the fact that
the algorithm is Las Vegas and has the stated time complexity follows
from Section 1.2.10 . The maps consist of a change of basis followed by
a projection to a subspace, and so the Lemma follows. ∎

###### Lemma 3.29.

Use the notation of Lemma 3.28 . There exists a Las Vegas algorithm
that, given @xmath for an involution @xmath , finds effective
isomorphisms @xmath , @xmath and @xmath .

The map @xmath is the symmetric square map of @xmath ; both @xmath and
@xmath are the identity on @xmath . The maps @xmath and @xmath can be
computed using @xmath field operations and @xmath can be computed using
@xmath field operations. The algorithm has expected time complexity
@xmath field operations.

###### Proof.

By Proposition 2.18 , the group @xmath is an irreducible @xmath
-dimensional copy of @xmath , so it must be a conjugate of the symmetric
square of the natural representation. By using a change of basis from
the algorithm in Theorem 1.12 , we may assume that it is the symmetric
square. Moreover, we can use Theorem 1.12 to constructively recognise
@xmath and obtain the map @xmath . We can also solve the constructive
membership problem in the standard copy, and by evaluating straight line
programs we obtain the maps @xmath and @xmath .

It follows from Theorem 1.12 that the expected time complexity is as
stated. ∎

##### Finding a mapping element

We now consider the algorithm for finding elements that map one point of
@xmath to another. The notation from Lemma 3.28 and 3.29 will be used.

If we let @xmath then we can identify @xmath with the space of quadratic
forms in @xmath and @xmath modulo scalars, so that @xmath . Then @xmath
acts projectively on @xmath and @xmath .

###### Proposition 3.30.

Use the notation from Lemma 3.28 and 3.29 .

1.   The number of points in @xmath that are contained in @xmath is
    @xmath .

2.   The map @xmath restricted to @xmath is not injective, and @xmath .

###### Proof.

1.  The map @xmath is the projection onto @xmath , so the kernel are
    those vectors that lie in @xmath . From the proof of Proposition
    2.18 , with respect to a suitable basis, @xmath is the @xmath
    -eigenspace of @xmath . Hence by Proposition 2.17 , @xmath .

2.  Since @xmath and @xmath , it is clear that the map is not injective.
    In the above basis, the map @xmath is defined by @xmath . Hence if
    @xmath then @xmath .

    If @xmath we do not get a point in @xmath and if @xmath and @xmath
    we obtain @xmath points. Now let @xmath and let @xmath such that
    @xmath . Then @xmath is a square in @xmath if and only if @xmath is
    a square, so @xmath is always a square. Hence, if @xmath , @xmath
    and @xmath , the image of @xmath is @xmath . This gives @xmath
    points.

∎

###### Proposition 3.31.

Under the action of @xmath , the set @xmath splits up into @xmath
orbits.

1.   The orbit containing @xmath , i.e. the non-degenerate quadratic
    forms that represent @xmath , which has size @xmath .

2.   The orbit containing @xmath , i.e. the non-degenerate quadratic
    forms that do not represent @xmath , which has size @xmath .

3.   The orbit containing @xmath (and @xmath ), i.e. the degenerate
    quadratic forms, which has size @xmath .

The pre-image in @xmath of @xmath is dihedral of order @xmath ,
generated by the matrices

  -- -------- -------- -- --------
     @xmath   @xmath      (3.28)
  -- -------- -------- -- --------

where @xmath is a primitive element of @xmath .

###### Proof.

Let @xmath be any element of @xmath , so that the symmetric square
@xmath . Notice that

  -- -------- -- --------
     @xmath      (3.29)
  -- -------- -- --------

Let @xmath , @xmath and @xmath be points in @xmath . Then @xmath , and
the equation @xmath implies that @xmath or @xmath . If @xmath then
@xmath or @xmath which is impossible since @xmath . Similarly, we cannot
have @xmath , and hence @xmath and @xmath are not in the same orbit.

Similarly, let @xmath , and then the equation @xmath implies that @xmath
or @xmath , which is impossible. Hence @xmath and @xmath are not in the
same orbit.

Finally, let @xmath , and the equation @xmath implies that @xmath .
Since @xmath is not a square in @xmath , we must have @xmath . But this
is impossible since @xmath . Hence @xmath and @xmath are not in the same
orbit.

To verify the orbit sizes, consider the stabilisers of the three points.
Clearly the equation @xmath implies that @xmath , so the stabiliser of
@xmath consists of the (projections of the) lower triangular matrices.
There are @xmath choices for @xmath and @xmath choices for @xmath , so
the stabiliser has size @xmath modulo scalars, and the index in @xmath
is therefore @xmath .

Similarly, the equation @xmath implies that @xmath . If @xmath we obtain
a matrix of order @xmath and if @xmath we obtain a diagonal matrix. It
follows that the stabiliser is dihedral of order @xmath , and that the
pre-image in @xmath is as in ( 3.28 ).

Finally, in a similar way we obtain that the stabiliser of @xmath has
order @xmath , and hence the three orbits make up the whole of @xmath .
∎

The algorithm that maps one point to another is given as Algorithm 3.2.2
.

##### Finding a stabilising element

The complete algorithm for finding a uniformly random element of @xmath
is then as follows, given a generating set @xmath for @xmath and @xmath
.

1.  Find an involution @xmath .

2.  Compute probable generators for @xmath using the Bray algorithm, and
    probable generators for @xmath by taking commutators of the
    generators of @xmath .

3.  Use the MeatAxe to verify that the module for @xmath splits up only
    as in Proposition 2.18 . Use Theorem 1.13 to verify that we have the
    whole of @xmath . Return to the previous step if not.

4.  Compute the maps @xmath and @xmath using Lemma 3.28 . Return to the
    first step if @xmath lies in the kernel of @xmath , if @xmath is
    degenerate, or if it does not represent @xmath .

5.  Compute the maps from Lemma 3.29 .

6.  Take random @xmath and let @xmath . Then @xmath , so @xmath is not
    in the kernel of @xmath , and @xmath is non-degenerate and
    represents @xmath . Repeat until @xmath .

7.  Use Algorithm 3.2.2 to find @xmath such that @xmath . Return to the
    previous step if it fails, and otherwise return @xmath .

##### Correctness and complexity

###### Lemma 3.32.

If @xmath is uniformly random, such that @xmath , then @xmath is
non-degenerate and represents @xmath with probability at least @xmath .

###### Proof.

Since @xmath is uniformly random and @xmath was chosen independently of
@xmath , it follows that @xmath is uniformly random from @xmath . From
the proof of Proposition 3.30 , with probability @xmath , @xmath where
@xmath is uniformly distributed in @xmath such that @xmath .

This represents @xmath if the discriminant @xmath is a non-zero square
in @xmath . This is not the case if @xmath , but since @xmath , this
implies @xmath . If @xmath then it is a square with probability @xmath ,
so

  -- -------- --
     @xmath   
  -- -------- --

and the Lemma follows. ∎

###### Lemma 3.33.

If @xmath are uniformly random, such that @xmath and @xmath represent
@xmath , then the probability that there exists an element @xmath , such
that the pre-image of @xmath in @xmath is upper triangular and @xmath ,
is at least @xmath .

###### Proof.

Let @xmath , @xmath and

  -- -------- -- --------
     @xmath      (3.30)
  -- -------- -- --------

where @xmath and @xmath are uniformly distributed in @xmath , @xmath and
@xmath .

We want to determine @xmath such that @xmath . Note that @xmath is the
pre-image in @xmath of an element in @xmath and therefore @xmath
determine the same element of @xmath . The map @xmath is the symmetric
square map, so

  -- -------- -- --------
     @xmath      (3.31)
  -- -------- -- --------

This leads to the following equations:

  -- -------- -------- -- --------
     @xmath   @xmath      (3.32)
     @xmath   @xmath      (3.33)
     @xmath   @xmath      (3.34)
  -- -------- -------- -- --------

for some @xmath . We can solve for @xmath in ( 3.32 ) and for @xmath in
( 3.33 ), so that ( 3.34 ) becomes

  -- -------- -- --------
     @xmath      (3.35)
  -- -------- -- --------

This quadratic equation has a solution if the discriminant @xmath . This
does not happen if @xmath or @xmath , which each happens with
probability @xmath . If the discriminant is non-zero then it is a square
with probability @xmath . Therefore, the probability that we can find
@xmath is

  -- -------- --
     @xmath   
  -- -------- --

This is @xmath and the Lemma follows. ∎

###### Theorem 3.34.

If Algorithm 3.2.2 returns an element @xmath , then @xmath . If @xmath
and @xmath are uniformly random, such that @xmath and @xmath represent
@xmath , then the probability that Algorithm 3.2.2 finds such an element
is at least @xmath .

###### Proof.

By Proposition 3.31 , the point @xmath is in the same orbit as @xmath ,
so the element @xmath at line 3.2.2 can easily be found by diagonalising
the form corresponding to @xmath . Then @xmath is of order @xmath .
Hence @xmath also has order @xmath , and @xmath .

By definition of @xmath , there exists @xmath such that @xmath , and if
we let @xmath then @xmath and @xmath . Hence @xmath , and therefore
@xmath .

By Proposition 3.31 , @xmath is dihedral of order @xmath , and @xmath
generates a subgroup of index @xmath . Therefore @xmath , which is the
success probability of line 3.2.2 .

It is straightforward to determine if @xmath exists, since @xmath is
diagonal. The success probability of line 3.2.2 is given by Lemma 3.33 .
Hence the success probability of the algorithm is as stated. ∎

###### Theorem 3.35.

Assume an oracle for the discrete logarithm problem in @xmath . The time
complexity of Algorithm 3.2.2 is @xmath field operations. The length of
the returned @xmath is @xmath .

###### Proof.

By Lemma 3.33 , line 3.2.2 involves solving a quadratic equation in
@xmath , and hence uses @xmath field operations. Evaluating the maps
@xmath and @xmath uses @xmath field operations, and it is clear that the
rest of the algorithm can be done using @xmath field operations.

By Theorem 1.12 , the length of the @xmath from the constructive
membership testing in @xmath is @xmath , which is therefore also the
length of the returned @xmath . ∎

###### Corollary 3.36.

Assume an oracle for the discrete logarithm problem in @xmath . There
exists a Las Vegas algorithm that, given @xmath such that @xmath and
@xmath , computes a random element of @xmath as an @xmath in @xmath .
The expected time complexity of the algorithm is @xmath field
operations. The length of the returned @xmath is @xmath .

###### Proof.

The algorithm is given in Section 3.2.2 .

An involution is found by finding a random element and then use
Proposition 1.4 . Hence by Corollary 2.24 , the expected time to find an
involution is @xmath field operations.

As described in Section 1.2.9 , the Bray algorithm will produce
uniformly random elements of the centraliser. Hence as described in
Section 1.2.6 , we can also obtain uniformly random elements of its
derived group. By Proposition 2.27 , two random elements will generate
@xmath with high probability. This implies that the expected time to
obtain probable generators for @xmath is @xmath field operations.

By Proposition 3.31 , the point @xmath is equal to @xmath with
probability @xmath and by Lemma 3.32 the point @xmath do not represent
zero with probability @xmath , so the expected time of the penultimate
step is @xmath field operations.

Since the points @xmath can be considered uniformly random and
independent in Algorithm 3.2.2 , the element returned by that algorithm
is uniformly random. Hence the element returned by the algorithm in
Section 3.2.2 is uniformly random.

The expected time complexity of the last step is given by Theorem 3.35
and 3.34 . It follows by the above and from Lemma 3.28 and 3.29 and
Corollary 2.23 that the expected time complexity of the algorithm in
Section 3.2.2 is as stated.

The algorithm is clearly Las Vegas, since it is straightforward to check
that the element we compute really fixes the point @xmath . ∎

###### Remark 3.37.

The elements returned by the algorithm in Corollary 3.36 are not
uniformly random from the whole of @xmath , but from @xmath . Hence, to
obtain generators for the whole stabiliser, it is necessary to execute
the algorithm at least twice, with different choices of the involution
@xmath .

###### Remark 3.38.

The algorithm in Corollary 3.36 works in any conjugate of @xmath , since
it does not assume that the matrices lie in the standard copy.

#### 3.2.3. Constructive membership testing

We now describe the constructive membership algorithm for our standard
copy @xmath . The matrix degree is constant here, so we set @xmath .
Given a set of generators @xmath , such that @xmath , and given an
element @xmath , we want to express @xmath as an @xmath in @xmath .
Membership testing is straightforward, using the first step from the
algorithm in Theorem 3.27 , and will not be considered here.

The general structure of the algorithm is the same as the algorithm for
the same problem in the Suzuki groups. It consists of a preprocessing
step and a main step.

##### Preprocessing

The preprocessing step consists of finding “standard generators” for
@xmath and @xmath . In the case of @xmath the standard generators are
defined as matrices

  -- -- -- --------
           (3.36)
  -- -- -- --------

for some unspecified @xmath , such that @xmath , @xmath , @xmath form
vector space bases of @xmath over @xmath (so @xmath ).

###### Lemma 3.39.

There exist algorithms for the following row reductions.

1.   Given @xmath , find @xmath expressed in the standard generators,
    such that @xmath .

2.   Given @xmath , find @xmath expressed in the standard generators,
    such that @xmath .

3.   Given @xmath , find @xmath expressed in the standard generators,
    such that @xmath .

Analogous algorithms exist for @xmath . If the standard generators are
expressed as @xmath s of length @xmath , the elements returned will have
length @xmath . The time complexity of the algorithms is @xmath field
operations.

###### Proof.

The algorithms are as follows.

1.  1.  Solve a linear system of size @xmath to construct the linear
        combination @xmath with @xmath . Let @xmath and @xmath , so that
        @xmath for some @xmath .

    2.  Solve a linear system of size @xmath to construct the linear
        combination @xmath with @xmath . Let @xmath and @xmath , so that
        @xmath for some @xmath .

    3.  Solve a linear system of size @xmath to construct the linear
        combination @xmath with @xmath . Let @xmath and @xmath , so that
        @xmath .

    4.  Now @xmath .

2.  Analogous to the previous case.

3.  1.  Normalise @xmath so that @xmath .

    2.  Let @xmath , @xmath and @xmath . Then @xmath maps @xmath to
        @xmath .

    3.  Use the algorithm above to find @xmath such that @xmath , and
        hence express @xmath in the standard generators.

Clearly the dominating term in the time complexity is the solving of the
linear systems, which requires @xmath field operations. The elements
returned are constructed using @xmath multiplications, hence the length
of the @xmath follows. ∎

###### Theorem 3.40.

Given an oracle for the discrete logarithm problem in @xmath , the
preprocessing step is a Las Vegas algorithm that finds standard
generators for @xmath and @xmath as @xmath s in @xmath of length @xmath
. It has expected time complexity @xmath field operations.

###### Proof.

The preprocessing step proceeds as follows.

1.  Find random elements @xmath and @xmath using the algorithm from
    Corollary 3.36 . Repeat until @xmath can be diagonalised to @xmath ,
    where @xmath and @xmath does not lie in a proper subfield of @xmath
    . Do similarly for @xmath .

2.  Find random elements @xmath and @xmath using the algorithm from
    Corollary 3.36 . Let @xmath , @xmath . Repeat until @xmath .

3.  Let @xmath and @xmath . As standard generators for @xmath we now
    take @xmath where

      -- -------- -- --------
         @xmath      (3.37)
      -- -------- -- --------

    and

      -- -------- -- --------
         @xmath      (3.38)
      -- -------- -- --------

    Similarly we obtain @xmath for @xmath .

It follows from ( 2.36 ) and ( 2.39 ) that ( 3.37 ) and ( 3.38 )
provides the standard generators for @xmath . These are expressed as
@xmath s in @xmath , since this is true for the elements returned from
the algorithm described in Corollary 3.36 . Hence the algorithm is Las
Vegas.

By Corollary 3.36 , the expected time to find @xmath and @xmath is
@xmath , and these are uniformly distributed independent random
elements. The elements of order dividing @xmath can be diagonalised as
required. By Proposition 2.15 , the proportion of elements of order
@xmath in @xmath and @xmath is @xmath . Hence the expected time for the
first step is @xmath field operations.

Similarly, by Proposition 2.29 the expected time for the second step is

  -- -------- --
     @xmath   
  -- -------- --

field operations.

By the remark preceding the Theorem, @xmath determines three sets of
field elements @xmath , @xmath and @xmath . By ( 2.39 ), in this case
each @xmath , @xmath and @xmath , for some fixed @xmath , where @xmath
is as in the algorithm. Since @xmath does not lie in a proper subfield,
these sets form vector space bases of @xmath over @xmath .

To determine if @xmath or @xmath diagonalise to some @xmath it is
sufficient to consider the eigenvalues on the diagonal, since both
@xmath and @xmath are triangular. To determine if @xmath lies in a
proper subfield, it is sufficient to determine if @xmath , for some
proper divisor @xmath of @xmath . Hence the dominating term in the
complexity is the first step. ∎

##### Main algorithm

Given @xmath we now show the procedure for expressing @xmath as an
@xmath . It is given as Algorithm 3.2.3 .

##### Correctness and complexity

###### Theorem 3.41.

Algorithm 3.2.3 is correct, and is a Las Vegas algorithm.

###### Proof.

First observe that since @xmath is randomly chosen, we obtain it as an
@xmath .

The elements found at lines 3.2.3 and 3.2.3 can be computed using Lemma
3.39 , so we can obtain them as @xmath s.

The element @xmath found at line 3.2.3 clearly has trace @xmath .
Because @xmath can be computed using Lemma 3.39 , we obtain it as an
@xmath . From Proposition 2.25 we know that @xmath is conjugate to
@xmath and therefore must fix two points of @xmath . Hence lines 3.2.3
and 3.2.3 make sense, and the elements found can again be computed using
Lemma 3.39 , so we obtain them as @xmath s.

Finally, the elements that make up @xmath have been found as @xmath s,
and it is clear that if we evaluate @xmath we obtain @xmath . Hence the
algorithm is Las Vegas and the Theorem follows. ∎

###### Theorem 3.42.

Algorithm 3.2.3 has expected time complexity @xmath field operations and
the length of the returned @xmath is @xmath .

###### Proof.

It follows immediately from Lemma 3.39 that the lines 3.2.3 , 3.2.3 ,
3.2.3 , 3.2.3 and 3.2.3 use @xmath field operations.

From Corollary 2.24 , the expected time to find @xmath is @xmath field
operations. Half of the elements of @xmath are squares, and @xmath is
uniformly random, hence the expected time of the outer repeat statement
is @xmath field operations.

Finding the fixed points of @xmath , and performing the check at line
3.2.3 only amounts to considering eigenvectors, which is @xmath field
operations. Thus the expected time complexity of the algorithm is @xmath
field operations.

From Theorem 3.40 each standard generator @xmath has length @xmath and
hence @xmath will have length @xmath . ∎

#### 3.2.4. Conjugates of the standard copy

Now assume that we are given a conjugate @xmath of @xmath , and we turn
to the problem of finding some @xmath such that @xmath , thus obtaining
an algorithm that finds effective isomorphisms from any conjugate of
@xmath to the standard copy. The matrix degree is constant here, so we
set @xmath .

###### Lemma 3.43.

There exists a Las Vegas algorithm that, given @xmath such that @xmath
for some @xmath , finds a point @xmath . The algorithm has expected time
complexity

  -- -------- --
     @xmath   
  -- -------- --

field operations.

###### Proof.

Clearly @xmath is the set on which @xmath acts doubly transitively. For
a matrix @xmath we see that the eigenspaces corresponding to the
eigenvalues @xmath will be in @xmath . Moreover, every element of order
dividing @xmath , in every conjugate @xmath of @xmath , will have
eigenvalues of the form @xmath , for some @xmath , and the eigenspaces
corresponding to @xmath will lie in the set on which @xmath acts doubly
transitively.

Hence to find a point @xmath it is sufficient to find a random @xmath of
order dividing @xmath . We compute the order using expected @xmath field
operations, and by Corollary 2.24 , the expected number of iterations to
find the element is @xmath field operations. We then find the
eigenspaces of @xmath .

Clearly this is a Las Vegas algorithm with the stated time complexity. ∎

###### Conjecture 3.44.

Let @xmath and @xmath for some @xmath . Then for all @xmath , the ideal
in @xmath generated by

  -- -------- -------- -- --------
              @xmath      (3.39)
              @xmath      (3.40)
     @xmath               (3.41)
  -- -------- -------- -- --------

as well as the corresponding @xmath polynomials from @xmath , is
zero-dimensional.

###### Lemma 3.45.

Assume Conjecture 3.44 . There exists a Las Vegas algorithm that, given
@xmath such that @xmath where @xmath , finds a diagonal matrix @xmath
such that @xmath . The expected time complexity is @xmath field
operations.

###### Proof.

Let @xmath . Since @xmath , @xmath must preserve the symmetric bilinear
form

  -- -------- -- --------
     @xmath      (3.42)
  -- -------- -- --------

where @xmath is given by ( 2.40 ). Using the MeatAxe, we can find this
form, which is determined up to a scalar multiple. In the case where
@xmath turns out to be a non-square in @xmath we can therefore multiply
@xmath with a non-square scalar matrix. The diagonal matrix @xmath that
we want to find is also determined up to a scalar multiple (and up to
multiplication by a diagonal matrix in @xmath ).

Since @xmath must take @xmath to @xmath , we must have @xmath , @xmath ,
@xmath and @xmath . Because @xmath is determined up to a scalar
multiple, we can choose @xmath , @xmath and @xmath . Furthermore, @xmath
and @xmath so it only remains to determine @xmath and @xmath .

To conjugate @xmath into @xmath , we must have @xmath for every @xmath ,
which is the set on which @xmath acts doubly transitively. By Lemma 3.43
, we can find @xmath , and the condition @xmath is given by ( 2.41 ) and
amounts to the polynomial equations given in Conjecture 3.44 , with
@xmath , @xmath , @xmath , @xmath , @xmath , @xmath , @xmath .

By finding another random point @xmath using Lemma 3.43 , we obtain
@xmath polynomials which we label @xmath , @xmath , @xmath , @xmath ,
@xmath and @xmath . Conjecture 3.44 asserts that the resulting ideal is
zero-dimensional.

Now it follows from Proposition 2.28 that with high probability we have
@xmath and @xmath , so we repeat until @xmath and @xmath satisfy this.
Then we can solve for @xmath from @xmath and @xmath from @xmath , as

  -- -------- -------- -- --------
     @xmath   @xmath      (3.43)
     @xmath   @xmath      (3.44)
  -- -------- -------- -- --------

and if we substitute these into the other equations we obtain @xmath
polynomials in @xmath and @xmath , which generate a zero-dimensional
ideal. The variety of this ideal can now be found using Theorem 1.2 .

Hence we can find @xmath and @xmath . The diagonal matrix

  -- -------- --
     @xmath   
  -- -------- --

now satisfies @xmath .

By Lemma 3.43 , Lemma 3.44 , Theorem 1.2 and Section 1.2.10 , this is a
Las Vegas algorithm with the stated time complexity. ∎

###### Lemma 3.46.

There exists a Las Vegas algorithm that, given subsets @xmath , @xmath
and @xmath of @xmath such that @xmath and @xmath , respectively, where
@xmath , @xmath for some @xmath and @xmath , finds @xmath such that
@xmath for some diagonal matrix @xmath . The algorithm has expected time
complexity @xmath field operations.

###### Proof.

Notice that the natural module @xmath of @xmath is uniserial with seven
non-zero submodules, namely

  -- -------- --
     @xmath   
  -- -------- --

for @xmath . Hence the same is true for @xmath and @xmath (but the
submodules will be different) since they lie in conjugates of @xmath .

Now the algorithm proceeds as follows.

1.  Let @xmath be the natural module for @xmath and @xmath . Find
    composition series @xmath and @xmath using the MeatAxe.

2.  Let @xmath , @xmath , @xmath , @xmath , @xmath , @xmath and @xmath .
    For each @xmath , choose @xmath .

3.  Now let @xmath be the matrix such that @xmath has @xmath as row
    @xmath , for @xmath .

The motivation for the second step is analogous to the proof of Theorem
3.16 .

Thus the matrix @xmath found in the algorithm satisfies that @xmath for
some diagonal matrix @xmath . Since @xmath , the algorithm returns a
correct result, and it is Las Vegas because the MeatAxe is Las Vegas.
Clearly it has the same time complexity as the MeatAxe. ∎

###### Theorem 3.47.

Assume Conjecture 3.44 and an oracle for the discrete logarithm problem
in @xmath . There exists a Las Vegas algorithm that, given a conjugate
@xmath of @xmath , finds @xmath such that @xmath . The algorithm has
expected time complexity @xmath field operations.

###### Proof.

Let @xmath . By Remark 3.38 , we can use Corollary 3.36 in @xmath , so
we can find generators for a stabiliser of a point in @xmath , using the
algorithm described in Theorem 3.40 .

1.  Find points @xmath using Lemma 3.43 . Repeat until @xmath .

2.  Find generating sets @xmath and @xmath such that @xmath and @xmath
    using the first two steps of the algorithm from the proof of Theorem
    3.40 .

3.  Find @xmath such that @xmath for some diagonal matrix @xmath , using
    Lemma 3.46 .

4.  Find a diagonal matrix @xmath using Lemma 3.45 .

5.  Now @xmath satisfies that @xmath .

Be Lemma 3.14 , 3.46 and 3.45 , and the proof of Theorem 3.40 , this is
a Las Vegas algorithm with time complexity as stated. ∎

#### 3.2.5. Tensor decomposition

Now assume that @xmath where @xmath , @xmath and @xmath for some @xmath
. Then @xmath , where @xmath is the Frobenius automorphism. Let @xmath
be the given module of @xmath and let @xmath be the natural module of
@xmath , so that @xmath and @xmath . From Section 1.2.7 and Section
2.2.3 we know that

  -- -------- -- --------
     @xmath      (3.45)
  -- -------- -- --------

for some integers @xmath , and where @xmath is either @xmath or the
absolutely irreducible @xmath -dimensional submodule @xmath of the
symmetric square @xmath . In fact, we may assume that @xmath . As
described in Section 1.2.7 , we now want to tensor decompose @xmath to
obtain an effective isomorphism from @xmath to @xmath or to @xmath . In
the latter case we also have to decompose @xmath into @xmath to obtain
an isomorphism between @xmath and @xmath . We consider this problem in
Section 3.2.6 .

###### Proposition 3.48.

Let @xmath such that @xmath , let @xmath be an involution and let @xmath
. Then @xmath as an @xmath -module, where @xmath . Moreover, @xmath is
absolutely irreducible, @xmath and @xmath , where @xmath .

###### Proof.

By Proposition 2.18 , @xmath and hence @xmath and @xmath where @xmath is
the natural module of @xmath and @xmath .

  -- -------- -- --------
     @xmath      (3.46)
  -- -------- -- --------

Now consider

  -- -------- -- --------
     @xmath      (3.47)
  -- -------- -- --------

The final summand has dimension @xmath and is absolutely irreducible
because it is of the form ( 3.45 ). Hence @xmath and @xmath as required.

Furthermore, a direct calculation shows that @xmath has shape @xmath
when restricted to @xmath , and over @xmath the @xmath must fuse with
the middle composition factor, otherwise the module would not be
self-dual ( @xmath is self-dual since @xmath preserves a bilinear form).

Similarly, @xmath has shape @xmath over @xmath . Over @xmath each @xmath
fuses with a corresponding @xmath and we obtain the structure of @xmath
. This is also proves that the @xmath -dimensional factor of @xmath is
not isomorphic to any of the factors of @xmath . ∎

###### Corollary 3.49.

Let @xmath such that @xmath , let @xmath be an involution and let @xmath
. Then @xmath .

###### Proof.

By Proposition 3.48 ,

  -- -------- --
     @xmath   
  -- -------- --

The middle summand has dimension @xmath , by Schur’s Lemma.

A homomorphism @xmath must map the @xmath -dimensional submodule to
itself or to @xmath . The composition factor of dimension @xmath at the
top can either be mapped to itself, or to the factor at the bottom.
Hence @xmath .

Similarly, @xmath since the top factor can either be mapped to itself,
or to the bottom factor. Thus the result follows. ∎

Given a module @xmath of the form ( 3.45 ), we now consider the problem
of finding a flat. For @xmath , let @xmath be the image of the
representation corresponding to @xmath , so @xmath or @xmath , and let
@xmath be an isomorphism. Our goal is then to find @xmath effectively
for some @xmath .

For @xmath denote @xmath . We need the following conjectures.

###### Conjecture 3.50.

Let @xmath have module @xmath of the form ( 3.45 ), with @xmath for some
@xmath .

Let @xmath have order @xmath and let @xmath be its multiset of
eigenvalues. If @xmath then there exists @xmath such that @xmath , and
the sum of the eigenspaces of @xmath corresponding to @xmath has
dimension @xmath .

###### Conjecture 3.51.

Let @xmath have module @xmath of the form ( 3.45 ) and @xmath . Let
@xmath be an involution.

If @xmath has tensor factors both of dimension @xmath and @xmath , then
@xmath has unique submodules @xmath and @xmath of dimensions @xmath and
@xmath , respectively, such that @xmath is a point of @xmath of
dimension @xmath .

###### Conjecture 3.52.

Let @xmath have module @xmath of the form ( 3.45 ) and @xmath for some
@xmath . Let @xmath be an involution and let @xmath .

If @xmath then @xmath , for some @xmath .

###### Theorem 3.53.

Assume Conjecture 3.50 . There exists a Las Vegas algorithm that, given
@xmath , where @xmath , @xmath , @xmath , @xmath and @xmath , with
module @xmath of the form ( 3.45 ), finds a point of @xmath . The
algorithm has expected time complexity

  -- -------- --
     @xmath   
  -- -------- --

field operations.

###### Proof.

Let @xmath . By Corollary 2.24 , we can easily find @xmath such that
@xmath . Our approach is to construct a point as a suitable sum of
eigenspaces of @xmath . We know that for @xmath , @xmath has @xmath
eigenvalues @xmath , @xmath , @xmath and @xmath for some @xmath . Let
@xmath be the multiset of eigenvalues of @xmath . Each eigenvalue has
the form

  -- -------- -- --------
     @xmath      (3.48)
  -- -------- -- --------

where each @xmath and each @xmath . We can easily compute @xmath .

Because each @xmath may be @xmath , for each @xmath we have @xmath . We
can determine which @xmath can be one of the @xmath , since if @xmath
for some @xmath , then @xmath .

Thus we can obtain a list, with length between @xmath and @xmath , of
subsets @xmath of @xmath . Now Conjecture 3.50 asserts that there is
some @xmath such that @xmath , and such that the sum of the eigenspaces
corresponding to @xmath has dimension @xmath , and by its construction
it must therefore be a point of @xmath . Since @xmath , the set @xmath
will be on our list, and we can easily find the point.

The algorithm is Las Vegas, since we can easily calculate the dimensions
of the subspaces. The expected number of random selections for finding
@xmath is @xmath , and we can find its order using expected @xmath field
operations. We find the characteristic polynomial using @xmath field
operations and then find the eigenvalues using expected @xmath field
operations. Finally we use the Algorithm from Section 1.2.10 to verify
that we have a point, using @xmath field operations. The rest of the
algorithm is linear algebra, and hence the expected time complexity is
as stated. ∎

###### Theorem 3.54.

Assume Conjecture 3.51 . There exists a Las Vegas algorithm that, given
@xmath , where @xmath , @xmath , @xmath and @xmath , with module @xmath
of the form ( 3.45 ), finds a point of @xmath . The algorithm has
expected time complexity

  -- -------- --
     @xmath   
  -- -------- --

field operations.

###### Proof.

Let @xmath . Similarly as in Corollary 3.36 , we find an involution
@xmath and probable generators for @xmath . We can then use Theorem 1.13
to verify that we have got the whole centraliser.

Using the MeatAxe, we find the composition factors of @xmath . For each
pair of factors of dimensions @xmath and @xmath we compute module
homomorphisms into @xmath and then find the sum of their images.

Conjecture 3.51 asserts that this will produce a point of @xmath . We
can easily calculate the dimensions of the submodules and use the tensor
decomposition algorithm to verify that we do obtain a point, so the
algorithm is Las Vegas.

The expected time complexity for finding @xmath is @xmath field
operations. From the proof of Corollary 3.36 we see that we can find
probable generators for @xmath and verify that we have the whole
centraliser using expected @xmath field operations, if we let @xmath in
Theorem 1.13 . The MeatAxe uses expected @xmath field operations in this
case, since the number of generators for the centraliser is constant.
Then we consider @xmath pairs of submodules, and for each one we use the
tensor decomposition algorithm to determine if we have a point, using
@xmath field operations. Hence the expected time complexity is as
stated. ∎

###### Theorem 3.55.

Assume Conjecture 3.52 and an oracle for the discrete logarithm problem.
There exists a Las Vegas algorithm that, given @xmath , where @xmath ,
@xmath , @xmath , @xmath and @xmath , with module @xmath of the form (
3.45 ), finds a point of @xmath . The algorithm has expected time
complexity

  -- -------- --
     @xmath   
  -- -------- --

field operations.

###### Proof.

Let @xmath . Similarly as in Corollary 3.36 , we find an involution
@xmath and probable generators for @xmath . We can then use Theorem 1.13
to verify that we have got the whole centraliser.

Using the MeatAxe, we find the composition factors of @xmath . Let
@xmath be the group corresponding to a non-trivial composition factor.
Using Theorem 1.12 we constructively recognise @xmath as @xmath and
obtain an effective isomorphism @xmath .

Now let @xmath be the image of the representation corresponding to
@xmath , so @xmath . Again we find an involution @xmath and probable
generators for @xmath . As above, we chop the module @xmath with the
MeatAxe, constructively recognise one of its non-trivial factors and
obtain an effective isomorphism @xmath .

Note that both @xmath and @xmath have effective inverses. Hence we can
obtain standard generators for @xmath and @xmath . For each @xmath , do
the following:

1.  Find @xmath using the standard generators.

2.  If @xmath , then find random @xmath such that @xmath . Use the
    algorithm in Section 1.2.10 to determine if @xmath is a point.

From Proposition 3.48 we know that @xmath has a submodule of dimension
@xmath . This implies that that @xmath has a submodule @xmath , for any
@xmath , and some @xmath (depending on @xmath ). Moreover, @xmath , and
by Corollary 3.49 , the latter has dimension @xmath .

But by Conjecture 3.52 , for some @xmath the former also has dimension
@xmath , and hence these vector spaces are equal. Therefore, for some
@xmath , the subspace @xmath found in the algorithm must be equal to
@xmath , and hence it is a point.

The expected time complexity for finding the involutions is

  -- -------- --
     @xmath   
  -- -------- --

field operations. From the proof of Corollary 3.36 we see that we can
find probable generators for @xmath and verify that we have the whole
centraliser using expected @xmath field operations, if we let @xmath in
Theorem 1.13 . The MeatAxe uses expected @xmath field operations in this
case since the number of generators are constant. In the loop, we
require @xmath field operations to verify that @xmath is a point. Hence
the expected time complexity follows from Theorem 1.12 . ∎

Conjectures 3.50 and 3.52 do not apply when @xmath , so in this case we
need another algorithm. Then @xmath so we are content with an algorithm
that has time complexity polynomial in @xmath . The approach is not to
use tensor decomposition, since in this case we have no efficient method
of finding a flat. Instead we find standard generators of @xmath using
permutation group techniques, then enumerate all tensor products of the
form ( 3.45 ), and for each one we determine if it is isomorphic to
@xmath .

###### Lemma 3.56.

There exists a Las Vegas algorithm that, given @xmath such that @xmath
with @xmath and @xmath , finds an effective injective homomorphism
@xmath where @xmath . The algorithm has expected time complexity @xmath
field operations.

###### Proof.

By Proposition 2.16 , @xmath acts doubly transitively on a set of size
@xmath . Hence @xmath also acts doubly transitively on @xmath , where
@xmath , and we can find the permutation representation of @xmath if we
can find a point @xmath . The set @xmath is a set of projective points
of @xmath , and the algorithm proceeds as follows.

1.  Choose random @xmath . Repeat until @xmath .

2.  Choose random @xmath and let @xmath . Repeat until @xmath and @xmath
    .

3.  Find a composition series for the module @xmath of @xmath and let
    @xmath be the submodule of dimension @xmath in the series.

4.  Find the orbit @xmath and compute the permutation group @xmath of
    @xmath on @xmath , together with an effective isomorphism @xmath .

By Proposition 2.21 , elements in @xmath of order dividing @xmath fix
two points of @xmath , and hence @xmath for some @xmath if and only if
@xmath and @xmath have a common fixed point. All composition factors of
@xmath have dimension @xmath , so a composition series of @xmath must
contain a submodule @xmath of dimension @xmath . This submodule is a
fixed point for @xmath , and its orbit must have size @xmath , since
@xmath and @xmath . It follows that @xmath .

All elements of @xmath of order a power of @xmath lie in the derived
group of a stabiliser of some point, which is also a Sylow @xmath
-subgroup of @xmath , and the exponent of this subgroup is @xmath .
Hence @xmath if and only if @xmath lie in a stabiliser of some point, if
and only if @xmath and @xmath have a common fixed point.

To find the orbit @xmath we can compute a Schreier tree on the
generators in @xmath with @xmath as root, using @xmath field operations.
Then @xmath can be computed for any @xmath using @xmath field
operations, by computing the permutation on @xmath induced by @xmath .
Hence @xmath is effective, and its image @xmath is found by computing
the image of each element of @xmath . Therefore the algorithm is correct
and it is clearly Las Vegas.

We find @xmath using expected @xmath field operations and we find @xmath
using expected @xmath field operations. Then @xmath is found using the
MeatAxe, in expected @xmath field operations. Thus the result follows. ∎

###### Conjecture 3.57.

Let @xmath such that @xmath . There exists a Las Vegas algorithm that
finds @xmath as @xmath s in @xmath such that the map

  -- -------- -------- -- --------
     @xmath   @xmath      (3.49)
     @xmath   @xmath      (3.50)
     @xmath   @xmath      (3.51)
  -- -------- -------- -- --------

is an isomorphism. Its time complexity is @xmath field operations. The
length of the returned @xmath s are @xmath .

###### Remark 3.58.

There exists an implementation of the above mentioned algorithm, and the
Conjecture is then it always produces a correct result and has the
stated complexity.

###### Theorem 3.59.

Assume Conjecture 3.57 . There exists a Las Vegas algorithm that, given
@xmath , where @xmath , @xmath and @xmath or @xmath and @xmath , with
module @xmath of the form ( 3.45 ), finds a tensor decomposition of
@xmath . The algorithm has time complexity @xmath field operations.

###### Proof.

Let @xmath . The algorithm proceeds as follows:

1.  Find permutation representation @xmath using Lemma 3.56 .

2.  Find standard generators @xmath using Conjecture 3.57 . Evaluate
    them on @xmath to obtain a generating set @xmath .

3.  Let @xmath and let @xmath be the module of @xmath . If @xmath then
    replace @xmath with @xmath .

4.  Construct each module of dimension @xmath of the form ( 3.45 ) using
    @xmath as base. For each one test if it is isomorphic to @xmath ,
    using the MeatAxe.

5.  Return the change of basis from the successful isomorphism test.

The returned change of basis exhibits @xmath as a tensor product, so by
Lemma 3.56 the algorithm is Las Vegas.

The lengths of the @xmath s of @xmath is @xmath , so we need @xmath
field operations to obtain @xmath . The number of modules of dimension
@xmath of the form ( 3.45 ) using @xmath as base is @xmath . Module
isomorphism testing uses @xmath field operations. Hence by Conjecture
3.57 and Lemma 3.56 the time complexity of the algorithm is as stated. ∎

#### 3.2.6. Symmetric square decomposition

The two basic irreducible modules of @xmath are the natural module
@xmath of dimension @xmath , and an irreducible submodule @xmath of the
symmetric square @xmath . The symmetric square itself is not
irreducible, since @xmath preserves a quadratic form, and @xmath
therefore has a submodule of dimension @xmath . The complement of this
has dimension @xmath and is the irreducible module @xmath .

###### Conjecture 3.60.

The exterior square of @xmath has a submodule isomorphic to a twisted
version of @xmath .

###### Theorem 3.61.

Assume Conjecture 3.60 . There exists a Las Vegas algorithm that, given
@xmath with module @xmath such that @xmath is isomorphic to a twisted
version of @xmath , finds an effective isomorphism from @xmath to @xmath
for some @xmath . The algorithm has expected time complexity @xmath
field operations.

###### Proof.

Using Conjecture 3.60 , this is just an application of the MeatAxe. We
construct the exterior square @xmath of @xmath , which has dimension
@xmath , and find a composition series of this module using the MeatAxe.
By the Conjecture, the natural module of dimension @xmath will be one of
the composition factors and the MeatAxe will provide an effective
isomorphism to this factor, in the form of a change of basis @xmath of
@xmath that exhibits the action on the composition factors.

This induces an isomorphism @xmath , where @xmath is conjugate to @xmath
. For @xmath , @xmath is computed by taking a submatrix of @xmath of
degree @xmath . Clearly @xmath can be computed using @xmath field
operations.

Since the MeatAxe is Las Vegas and has expected time complexity @xmath ,
the result follows. ∎

#### 3.2.7. Constructive recognition

Finally, we can now state and prove our main theorem.

###### Theorem 3.62.

Assume the small Ree Conjectures, and an oracle for the discrete
logarithm problem in @xmath . There exists a Las Vegas algorithm that,
given @xmath satisfying the assumptions in Section 1.2.7 , with @xmath ,
@xmath and @xmath , finds an effective isomorphism @xmath and performs
preprocessing for constructive membership testing. The algorithm has
expected time complexity @xmath field operations.

Each image of @xmath can be computed in @xmath field operations, and
each pre-image in expected @xmath field operations.

###### Proof.

Let @xmath be the module of @xmath . The algorithm proceeds as follows:

1.  If @xmath then use Theorem 3.47 to obtain @xmath such that @xmath ,
    and hence an effective isomorphism @xmath defined by @xmath .

2.  If @xmath then @xmath for some @xmath . If @xmath then use Theorem
    3.53 to find a flat @xmath . If @xmath but @xmath is not a proper
    power of @xmath then use Theorem 3.54 to find such an @xmath .
    Otherwise @xmath for some @xmath . If @xmath , then use Theorem 3.55
    to find a flat @xmath .

3.  Use the tensor decomposition algorithm described in Section 1.2.10
    with @xmath , to obtain @xmath such the change of basis determined
    by @xmath exhibits @xmath as a tensor product @xmath , with @xmath
    or @xmath . If @xmath or @xmath and @xmath then use Theorem 3.59 to
    find @xmath . Let @xmath and @xmath be the images of the
    corresponding representations.

4.  Define @xmath as @xmath and let @xmath . If @xmath then let @xmath
    be the effective isomorphism from Theorem 3.61 , otherwise let
    @xmath be the identity map.

5.  Let @xmath . Then @xmath is conjugate to @xmath . Use Theorem 3.47
    to obtain @xmath such that @xmath .

6.  An effective isomorphism @xmath is given by @xmath .

The map @xmath is straightforward to compute, since given @xmath it only
involves dividing @xmath into submatrices of degree @xmath or @xmath ,
checking that they are scalar multiples of each other and returning the
@xmath or @xmath matrix consisting of these scalars. Since @xmath might
not lie in @xmath , but only in @xmath , the result of @xmath might not
have determinant @xmath . However, since every element of @xmath has a
unique @xmath th root, we can easily scale the matrix to have
determinant @xmath . Hence by Theorem 3.53 , Theorem 3.54 , Theorem 3.55
, Section 1.2.10 , Theorem 3.61 and Theorem 3.47 , the algorithm is Las
Vegas, and @xmath can be computed using @xmath field operations.

In the case where we use Theorem 3.59 we have @xmath and hence @xmath .
We see that @xmath , and the time complexity of the algorithm to find
@xmath , in Theorem 3.59 , simplifies to @xmath .

In the other cases, finding @xmath uses @xmath field operations. From
Section 1.2.10 , finding @xmath uses @xmath field operations when a flat
@xmath is given.

From Theorem 3.61 finding @xmath uses @xmath field operations, and from
Theorem 3.47 , finding @xmath uses @xmath field operations. Hence the
expected time complexity is as stated. Finally, @xmath is computed by
first using Algorithm 3.2.3 to obtain an @xmath of @xmath and then
evaluating it on @xmath . The necessary precomputations in Theorem 3.40
have already been made during the application of Theorem 3.47 , and
hence it follows from Theorem 3.42 that the time complexity for
computing the pre-image of @xmath is as stated. ∎

### 3.3. Big Ree groups

Here we will use the notation from Section 2.3 . We will refer to
Conjectures 2.37 , 2.38 , 2.39 , 2.40 , 2.41 , 2.42 , 2.45 , 2.46 and
3.64 simultaneously as the Big Ree Conjectures .

With the Big Ree groups, we will only deal with the natural
representation, the last case in Section 1.2.7 . We will not attempt any
tensor decomposition, or decomposition of the tensor indecomposables
listed in Section 2.3.2 . This can be partly justified by the fact that
at the present time the only representation, other than the one of
dimension @xmath , that it is within practical limits in the MGRP, is
the one of dimension @xmath . We have some evidence that it is feasible
to decompose this representation into the natural representation using
the technique known as condensation , see [ HEO05 , Section @xmath ] and
[ LW98 ] .

The main constructive recognition theorem is Theorem 3.79 .

#### 3.3.1. Recognition

We now consider the question of non-constructive recognition of @xmath ,
so we want to find an algorithm that, given a set @xmath , decides
whether or not @xmath .

###### Theorem 3.63.

There exists a Las Vegas algorithm that, given @xmath , decides whether
or not @xmath . The algorithm has expected time complexity @xmath field
operations.

###### Proof.

Let @xmath , with natural module @xmath . The algorithm proceeds as
follows:

1.  Determine if @xmath and return false if not. All the following steps
    must succeed in order to conclude that a given @xmath also lies in
    @xmath .

    1.  Determine if @xmath , which is true if @xmath and if @xmath ,
        where @xmath is the matrix corresponding to the quadratic form
        @xmath and where @xmath denotes the transpose of @xmath .

    2.  Determine if @xmath , which is true if @xmath preserves the
        exceptional Jordan algebra multiplication. This is easy using
        the multiplication table given in [ Wil06 ] .

    3.  Determine if @xmath is a fixed point of the automorphism of
        @xmath which defines @xmath . By [ Wil06 ] , computing the
        automorphism amounts to taking a submatrix of the exterior
        square of @xmath and then replacing each matrix entry @xmath by
        @xmath .

2.  If @xmath is not a proper subgroup of @xmath , or equivalently if
    @xmath is not contained in a maximal subgroup, return true .
    Otherwise return false . By Proposition 2.48 , it is sufficient to
    determine if @xmath cannot be written over a smaller field and if
    @xmath is irreducible. This can be done using the Las Vegas
    algorithms from Sections 1.2.10 and 1.2.10 .

Since the matrix degree is constant, the complexity of the first step of
the algorithm is @xmath field operations. For the same reason, the
complexity of the algorithms from Sections 1.2.10 and 1.2.10 is @xmath
field operations. Hence the expected time complexity is as stated. ∎

#### 3.3.2. Finding elements of even order

In constructive recognition and membership testing of @xmath , the
essential problem is to find elements of even order, as @xmath s in the
given generators. Let @xmath . We begin with an overview of the method.
The matrix degree is constant here, so we set @xmath .

Choose random @xmath of order @xmath , by choosing a random element of
order @xmath and powering up. By Proposition 2.33 it is easy to find
such elements, and by Proposition 2.32 , we can diagonalise @xmath and
obtain @xmath such that @xmath for some @xmath .

Now choose random @xmath . Let @xmath and let @xmath be a diagonal
matrix of the same form as @xmath , where @xmath and @xmath are replaced
by indeterminates @xmath and @xmath , so @xmath is a matrix over the
function field @xmath .

For any @xmath , such that @xmath , the matrix @xmath . Hence by
Conjecture 2.45 if we can find @xmath such that @xmath and @xmath has
the eigenvalue @xmath with multiplicity @xmath , then with high
probability @xmath will have even order.

###### Proposition 3.64.

Assume Conjecture 2.47 . For every @xmath the @xmath lowest coefficients
@xmath of the characteristic polynomial of @xmath generate a
zero-dimensional ideal.

###### Proof.

Since @xmath does not normalise @xmath , by Conjecture 2.47 , there must
be a bounded number of solutions. Hence the system must be
zero-dimensional. ∎

Finally we can solve the discrete logarithm problem and find an integer
@xmath such that @xmath . Then @xmath has even order, and therefore also
@xmath has even order. Since @xmath and @xmath are random, we obtain an
element of even order as an @xmath in @xmath . The algorithm for finding
elements of even order is given formally as Algorithm 3.3.2 .

###### Lemma 3.65.

Assume Conjecture 2.47 . There exists a Las Vegas algorithm that, given
the matrices @xmath and @xmath , finds @xmath such that @xmath and
@xmath has @xmath as an eigenvalue of multiplicity at least @xmath . The
algorithm has expected time complexity @xmath field operations.

###### Proof.

If we can find the characteristic polynomial @xmath of @xmath , then the
condition we want to impose is that @xmath should be a root of
multiplicity @xmath , or equivalently that @xmath should divide @xmath .

Hence we obtain @xmath polynomial equations in @xmath and @xmath of
bounded degree. By Proposition 3.64 , we can use Theorem 1.2 to find the
possible values for @xmath and @xmath .

Thus it only remains to find @xmath , which has the form

  -- -------- -- --------
     @xmath      (3.52)
  -- -------- -- --------

where @xmath and @xmath . Recall that @xmath is diagonal of the same
form as @xmath . This implies that in an echelon form of @xmath , the
diagonal has the form @xmath for some diagonal matrix @xmath . We obtain
@xmath by multiplying these diagonal elements, and since the sum of the
positive powers of @xmath on the diagonal is @xmath , and the sum of the
positive powers of @xmath on the diagonal is @xmath , each @xmath has
the form

  -- -------- -- --------
     @xmath      (3.53)
  -- -------- -- --------

where each @xmath , @xmath and @xmath .

Because of these bounds on the exponents @xmath and @xmath , we can find
the coefficients @xmath , and hence the coefficients @xmath and @xmath ,
using interpolation. Each @xmath is uniquely determined by at most
@xmath values of @xmath and the corresponding value of @xmath .

Therefore, choose @xmath random pairs @xmath . For each pair, calculate
the characteristic polynomial of @xmath , thus obtaining the
corresponding values of the coefficients @xmath . Finally perform the
interpolation by solving @xmath linear systems with @xmath equations and
variables.

It is clear that the algorithm is Las Vegas, and the dominating term in
the time complexity is the root finding of univariate polynomials of
bounded degree over @xmath . ∎

###### Lemma 3.66.

Assume Conjectures 2.46 and 2.47 . Let @xmath be such that @xmath and
@xmath is conjugate to some @xmath with @xmath . The proportion of
@xmath , such that @xmath contains an element with @xmath as an
eigenvalue of multiplicity @xmath , is bounded below by a constant
@xmath .

###### Proof.

Given such a coset @xmath , from the proof of Lemma 3.65 we see that our
algorithm constructs a bounded number @xmath of candidates of elements
of the required type in the coset.

Let @xmath be the number of cosets containing an element of the required
type. By Conjecture 2.46 , the total number of elements of the required
type is @xmath . Hence @xmath is minimised if all the @xmath cosets
contain @xmath such elements, in which case @xmath . Thus @xmath and the
proportion of cosets is @xmath , which is bounded below by a constant
@xmath since @xmath . ∎

###### Theorem 3.67.

Assume Conjectures 2.47 , 2.46 and 2.45 , and an oracle for the discrete
logarithm problem in @xmath . Algorithm 3.3.2 is a Las Vegas algorithm
with expected time complexity @xmath field operations. The length of the
returned @xmath is @xmath .

###### Proof.

By Proposition 2.32 , @xmath is conjugate to some @xmath and by
Proposition 2.33 , we can find @xmath using expected @xmath field
operations. The test at line 3.3.2 is easy since @xmath either
centralises or inverts @xmath . Furthermore, by Lemma 3.66 , the test at
line 3.3.2 will succeed with high probability and by Conjecture 2.45 ,
the test at line 3.3.2 will succeed with high probability. The test at
line 3.3.2 can only fail if @xmath is a proper divisor of @xmath , which
happens with low probability.

Hence by Lemma 3.65 , the algorithm is Las Vegas and the time complexity
is as stated. Clearly, the length of the @xmath of the returned element
is the same as the length of the @xmath of @xmath . ∎

###### Remark 3.68.

If we are given @xmath , then a trivial modification of Algorithm 3.3.2
finds an element of @xmath , of even order, of the form @xmath for some
@xmath . If we also have an @xmath of @xmath in @xmath , then we will
obtain @xmath as @xmath , otherwise we will only obtain an @xmath for
@xmath .

###### Proposition 3.69.

Assume Conjecture 2.45 . With probability @xmath , the element returned
by Algorithm 3.3.2 powers up to an involution of class @xmath .

###### Proof.

Follows immediately from Conjecture 2.45 . ∎

#### 3.3.3. Constructive membership testing

The overall method we use for constructive membership testing in @xmath
is the Ryba algorithm described in Section 1.2.9 .

Since we know that there are only two conjugacy classes of involutions
in @xmath , and since we know the structure of their centralisers, we
can improve upon the basic Ryba algorithm. When solving constructive
membership testing in the centralisers, instead of applying the Ryba
algorithm recursively, we can do it in a more direct way using Theorem
1.12 and the algorithms for constructive membership testing in the
Suzuki group, described in Section 3.1 . Involutions of class @xmath can
be found using Algorithm 3.3.2 , and Conjecture 2.40 give us a method
for finding involutions of class @xmath using random search. The Ryba
algorithm needs to find involutions of both classes, since it needs to
find two involutions whose product has even order.

As a preprocessing step to Ryba, we can therefore find an involution of
each class and compute their centralisers. In each call to Ryba we then
conjugate the involutions that we find to one of these two involutions,
which removes the necessity of computing involution centralisers at each
call.

##### The involution centralisers

We use the Bray algorithm to find generating sets for the involution
centralisers. This algorithm is described in Section 1.2.9 .

The following results show how to precompute generators and how to solve
the constructive membership problem for a centraliser of an involution
of class @xmath , using our Suzuki group algorithms to constructively
recognise @xmath . Analogous results hold for the centraliser of an
involution of class @xmath , using Theorem 1.12 to constructively
recognise @xmath .

###### Lemma 3.70.

Assume Conjecture 2.39 , and use its notation. There exists a Las Vegas
algorithm that, given @xmath such that @xmath , @xmath for an involution
@xmath of class @xmath and @xmath , finds a composition series for the
natural module @xmath of @xmath such that the composition factors are
ordered as @xmath , and finds the corresponding filtration of @xmath .
The algorithm has time complexity @xmath field operations.

###### Proof.

By Proposition 2.39 the composition factors are as stated, and we just
have to order them correctly.

1.  Find a composition factor of @xmath of @xmath , such that @xmath ,
    and find @xmath . By Conjecture 2.39 , @xmath has a unique @xmath
    -dimensional submodule, so @xmath .

2.  Let @xmath where @xmath . Then @xmath .

3.  Let @xmath be the orthogonal complement under the bilinear form
    preserved by @xmath , so that @xmath .

4.  Find a composition factor of @xmath of @xmath , such that @xmath ,
    and find @xmath . By Conjecture 2.39 , @xmath has a unique @xmath
    -dimensional submodule, so @xmath for one of its @xmath composition
    factors of dimension @xmath .

5.  Let @xmath where @xmath . Then @xmath .

6.  Let @xmath be the orthogonal complement under the bilinear form
    preserved by @xmath , so that @xmath .

7.  Now we have four proper submodules @xmath in the composition series
    that we want to find, and we can obtain the other submodules by
    continuing in the same way inside @xmath .

The filtration is determined by the composition factors, and immediately
found. Clearly the time complexity is the same as the MeatAxe, which is
@xmath field operations. ∎

###### Lemma 3.71.

Assume the Suzuki Conjectures, Conjectures 2.37 , 2.38 , 2.39 and 2.40 ,
and an oracle for the discrete logarithm problem in @xmath . There
exists a Monte Carlo algorithm with no false positives that, given
@xmath such that @xmath , @xmath and @xmath , where @xmath is an
involution of class @xmath and @xmath :

-    decides whether or not @xmath ,

-    finds effective homomorphisms @xmath and @xmath ,

-    finds @xmath such that @xmath , @xmath and @xmath is contained in a
    maximal parabolic in @xmath ,

-    finds @xmath such that @xmath , and @xmath .

The algorithm has expected time complexity

  -- -------- --
     @xmath   
  -- -------- --

field operations.

###### Proof.

The algorithm proceeds as follows:

1.  Find a composition series of the natural module of @xmath , as in
    Lemma 3.70 . By projecting to the middle composition factor we
    obtain an effective surjective homomorphism @xmath where @xmath and
    @xmath . Also obtain an effective surjective homomorphism @xmath ,
    where @xmath is the first non-zero block in the filtration of @xmath
    ( i.e. @xmath is a vector space). By Conjecture 2.40 , @xmath .

2.  Use Theorem 3.26 to constructively recognise @xmath and obtain an
    effective injective homomorphism @xmath , and an effective
    isomorphism @xmath . Now @xmath .

3.  Find random @xmath such that @xmath . By Proposition 2.34 , the
    proportion of such elements is high. Repeat until @xmath is of class
    @xmath and @xmath , which by Conjecture 2.37 happens with high
    probability.

4.  Using the dihedral trick, find @xmath such that @xmath , @xmath and
    @xmath is reducible. By Conjecture 2.38 , these elements are easy to
    find.

5.  Let @xmath , so that @xmath commutes with @xmath . Now @xmath is
    contained in a maximal parabolic, and @xmath is a proper subgroup,
    since @xmath , but @xmath is reducible and hence a proper subgroup
    of @xmath .

6.  Diagonalise @xmath to obtain @xmath for some @xmath . Repeat the two
    previous steps (find another @xmath ) if @xmath or @xmath lie in a
    proper subfield of @xmath . The probability that this happens is
    low, since @xmath with high probability.

7.  Find random @xmath , and let @xmath for @xmath . Then @xmath are
    random elements of @xmath . Return false if @xmath are not linearly
    independent elements of @xmath , since then with high probability
    @xmath . Clearly, if the elements are linearly independent, then
    @xmath so the algorithm has no false positives.

8.  Find random @xmath such that @xmath . By Conjecture 2.37 , with high
    probability @xmath and @xmath . Repeat until this is true.

9.  Finally let

      -- -------- --
         @xmath   
      -- -------- --

Clearly, the dominating term in the running time is Theorem 3.26 and the
computation of @xmath , so the expected time complexity is as stated. ∎

###### Lemma 3.72.

Assume the Suzuki Conjectures, Conjectures 2.38 , 2.39 and 2.40 and an
oracle for the discrete logarithm problem in @xmath . There exists a Las
Vegas algorithm that, given @xmath and an involution @xmath of class
@xmath , as an @xmath in @xmath of length @xmath ,

-    finds @xmath such that @xmath ,

-    finds effective inverse isomorphisms @xmath and @xmath ,

-    finds @xmath such that @xmath , @xmath and @xmath is contained in a
    maximal parabolic in @xmath ,

-    finds @xmath such that @xmath , and @xmath .

The elements @xmath are found as @xmath s in @xmath of length @xmath .
The algorithm has expected time complexity @xmath field operations.

###### Proof.

The algorithm proceeds as follows:

1.  Use the Bray algorithm to find probable generators @xmath for @xmath
    .

2.  Use the MeatAxe to split up the module of @xmath and verify that it
    splits up as in Conjecture 2.39 . Use Theorem 3.3 to verify that the
    groups acting on the @xmath -dimensional submodules are Suzuki
    groups. Return to the first step if not. It then follows from
    Proposition 2.34 that @xmath .

3.  Use Lemma 3.71 to determine if @xmath . Return to the first step if
    not. Since the algorithm of Lemma 3.71 has no false positives, this
    is Las Vegas.

By Proposition 2.34 , @xmath elements is sufficient to generate @xmath
with high probability, so the expected time complexity is as stated, and
the elements of @xmath will be found as @xmath s of the same length as
@xmath .

From Lemma 3.71 we also obtain @xmath , @xmath , @xmath and @xmath as
needed. We see from its proof that @xmath and @xmath will be found as
@xmath s of the same length as @xmath . ∎

###### Lemma 3.73.

There exists a Las Vegas algorithm that, given

-   @xmath such that @xmath and @xmath where @xmath is an involution of
    class @xmath and @xmath are given as @xmath s in @xmath of length
    @xmath ,

-    an effective surjective homomorphism @xmath ,

-    an effective injective homomorphism @xmath ,

-   @xmath ,

decides whether or not @xmath and if so returns an @xmath of @xmath in
@xmath of length @xmath . The algorithm has expected time complexity
@xmath field operations.

###### Proof.

Note that @xmath consists of a change of basis followed by a projection
to a submatrix, and hence can be applied to any element of @xmath using
@xmath field operations.

1.  Use Algorithm 3.1.3 to express @xmath in the generators of @xmath ,
    or return false if @xmath . Hence we obtain an @xmath for @xmath in
    @xmath , of length @xmath .

2.  Now @xmath . Using the elements of @xmath , we can apply row
    reduction to @xmath , and hence obtain an @xmath for @xmath in
    @xmath of length @xmath . Return false if @xmath is not reduced to
    the identity matrix using @xmath .

3.  Since @xmath and @xmath are @xmath s in @xmath , in time @xmath we
    obtain an @xmath for @xmath in @xmath , of the specified length.

The expected time complexity then follows from Theorem 3.13 . ∎

We are now ready to state our modified Ryba algorithm, which assumes
that the precomputations given by the above results have been done.

###### Theorem 3.74.

Assume Conjectures 2.40 , 2.45 , 2.46 and 3.64 , and an oracle for the
discrete logarithm problem in @xmath . Algorithm 3.3.3 is a Las Vegas
algorithm with expected time complexity @xmath field operations. The
length of the returned @xmath is @xmath where @xmath is the length of
the @xmath s for @xmath in @xmath .

###### Proof.

By Theorem 3.67 , the length of @xmath is @xmath . By Remark 3.68 ,
@xmath and @xmath is an @xmath for @xmath .

Then @xmath is found using Proposition 1.4 , and by Proposition 3.69 it
is of class @xmath with high probability. By Proposition 2.34 , the
class can be determined by computing the Jordan form.

By Conjecture 2.40 , @xmath will have class @xmath with high
probability, and then @xmath has even order by Proposition 2.34 . Again
we use Proposition 1.4 to find @xmath .

Using the dihedral trick, we find @xmath . Note that @xmath will be
found as an @xmath of length @xmath , since we have an @xmath for @xmath
, and we can assume that the @xmath for @xmath has length @xmath . Now
@xmath is dihedral with central involution @xmath , so @xmath . Using
the @xmath version of Lemma 3.73 , we find @xmath using @xmath field
operations, and @xmath has length @xmath .

The next @xmath is again found using the dihedral trick, and comes as an
@xmath of the same length as @xmath . Then @xmath since @xmath is
central in @xmath . Hence we again use Lemma 3.73 (or its @xmath
version) to obtain @xmath , with the same length as @xmath (or with
@xmath replaced by @xmath ).

Finally, @xmath clearly centralises @xmath , and we now have an @xmath
for @xmath , so we obtain another @xmath as @xmath in @xmath , and use
Lemma 3.73 to obtain an @xmath for @xmath . Hence we obtain @xmath ,
which is an @xmath for @xmath , and finally an @xmath @xmath for @xmath
. Since @xmath has length @xmath , the length of @xmath is as specified.

The expected time complexity follows from Theorem 3.67 , Lemma 3.73 and
Proposition 1.4 . ∎

#### 3.3.4. Conjugates of the standard copy

We now consider the situation where we are given @xmath , such that
@xmath , so that @xmath is a conjugate of @xmath , and the problem is to
find @xmath , such that @xmath .

###### Lemma 3.75.

Assume Conjectures 2.39 , 2.40 and 2.41 . There exists a Las Vegas
algorithm that, given

-   @xmath such that @xmath ,

-   @xmath such that @xmath for some involution @xmath of class @xmath ,

-   @xmath such that @xmath , @xmath and @xmath is contained in a
    maximal parabolic in @xmath ,

finds @xmath such that @xmath and @xmath such that @xmath .

The expected time complexity is @xmath field operations. If @xmath and
@xmath are given as @xmath s in @xmath of length @xmath , then @xmath
and @xmath will be returned as @xmath s in @xmath , also of length
@xmath .

###### Proof.

By Conjecture 2.39 we have @xmath . Since @xmath , it follows from
Proposition 2.34 that @xmath lies in the cyclic group @xmath on top of
@xmath . Then @xmath acts fixed-point freely on @xmath and hence @xmath
and @xmath .

The algorithm proceeds as follows:

1.  Choose random @xmath and use Corollary 1.11 to find @xmath , such
    that @xmath centralises @xmath modulo @xmath .

2.  Use Corollary 1.11 to find @xmath , such that @xmath centralises
    @xmath modulo @xmath . By Conjecture 2.40 , @xmath , so @xmath .
    Similarly find @xmath .

3.  Now @xmath satisfies @xmath , so find probable generators @xmath for
    @xmath . Clearly, @xmath if and only if @xmath . Use the MeatAxe to
    split up the module for @xmath and verify that it has the structure
    given by Conjecture 2.41 . Return to the first step if not.

4.  From the @xmath -dimensional submodules, we obtain an image @xmath
    of @xmath in @xmath . Use Theorem 3.3 to determine if @xmath .
    Return to the first step if not.

By Proposition 2.12 , two random elements generate @xmath with high
probability, so the probability that @xmath is also high. Hence by
Theorem 3.3 , the expected time complexity is as stated. ∎

###### Lemma 3.76.

Assume the Suzuki Conjectures, the Big Ree Conjectures and an oracle for
the discrete logarithm problem in @xmath . There exists a Las Vegas
algorithm that, given @xmath such that @xmath , finds @xmath and @xmath
such that @xmath and @xmath . The elements of @xmath are expressed as
@xmath s in @xmath of length @xmath ; @xmath is expressed as an @xmath
in @xmath of length @xmath . The algorithm has expected time complexity
@xmath field operations.

###### Proof.

The idea is to first find one copy of @xmath by finding a centraliser of
an involution of class @xmath , which has structure @xmath , then use
the Formula to find the @xmath inside this. Next we find a maximal
parabolic inside this @xmath and use the dihedral trick and the Formula
to conjugate it back to our involution centraliser. The conjugating
element together with the @xmath will generate a copy of @xmath .

The algorithm proceeds as follows:

1.  Use Algorithm 3.3.2 to find an element of even order and then use
    Proposition 1.4 to find an involution @xmath . Repeat until @xmath
    is of class @xmath , which by Proposition 3.69 happens with high
    probability.

2.  Use Lemma 3.72 to find @xmath and @xmath such that @xmath , @xmath ,
    @xmath and @xmath is contained in a maximal parabolic in @xmath .

3.  Use Lemma 3.75 to find @xmath such that @xmath and @xmath commutes
    with @xmath .

4.  Use the MeatAxe to split up the module of @xmath . By Conjecture
    2.41 we obtain @xmath -dimensional submodules, and hence a
    homomorphism @xmath .

5.  Use Theorem 3.1 to find an effective isomorphism @xmath .

6.  Use the first steps of the proof of Theorem 3.12 to find @xmath , as
    @xmath s in the generators of @xmath , such that @xmath , @xmath and
    @xmath is contained in a maximal parabolic in @xmath . Evaluate the
    @xmath s on @xmath to obtain @xmath with similar properties.

7.  Using the dihedral trick, find @xmath such that @xmath and let
    @xmath . Since @xmath is a proper subgroup of @xmath , it follows
    that @xmath is a proper subgroup of @xmath , and hence it is
    contained in a maximal parabolic. Clearly @xmath is also contained
    in the same maximal parabolic. We know from the structure of @xmath
    that @xmath , and since @xmath , it follows that @xmath , so @xmath
    and @xmath both lie in a group of shape @xmath and hence are
    conjugate modulo @xmath . Now we want to conjugate @xmath to @xmath
    while fixing @xmath .

8.  Diagonalise @xmath and @xmath to obtain @xmath and @xmath . Use the
    discrete logarithm oracle to find an integer @xmath such that @xmath
    . If no such @xmath exists, then find another pair of @xmath , but
    this can only happen if @xmath is a proper divisor of @xmath .

9.  Notice that @xmath also can diagonalise to @xmath , so now

      -- -------- --
         @xmath   
      -- -------- --

    or @xmath . In the latter case, invert @xmath .

10. Use Lemma 1.10 to find @xmath such that

      -- -------- --
         @xmath   
      -- -------- --

    and then @xmath such that

      -- -------- --
         @xmath   
      -- -------- --

    By Conjecture 2.40 , @xmath , so @xmath .

11. Now @xmath since by Lemma 1.10 , both @xmath and @xmath centralise
    @xmath . Clearly, @xmath conjugates @xmath to @xmath . Hence @xmath
    conjugates @xmath to @xmath , and by construction @xmath commutes
    with both @xmath and @xmath . Since @xmath is contained in a maximal
    parabolic of another copy of @xmath , it follows that @xmath
    commutes with this copy. Thus @xmath .

By Theorem 3.67 , the length of the @xmath for @xmath is @xmath . Then
by Lemma 3.72 , the @xmath s of @xmath and @xmath will also have length
@xmath . By Lemma 3.75 , the @xmath s for @xmath will have length @xmath
. By Theorem 3.12 , the @xmath s for @xmath and @xmath will have length
@xmath , and hence @xmath and @xmath will also have this length.

The expected time complexity follows from Lemma 3.72 . ∎

###### Lemma 3.77.

Assume Conjecture 2.42 . There exists a Las Vegas algorithm that, given
@xmath such that @xmath , @xmath and @xmath , finds @xmath such that
@xmath . The algorithm has expected time complexity @xmath field
operations.

###### Proof.

Let @xmath be the module of @xmath . Observe that @xmath must centralise
@xmath , so @xmath . By Conjecture 2.42 , the endomorphism ring of
@xmath has dimension @xmath . The algorithm proceeds as follows:

1.  Use the MeatAxe to find @xmath such that @xmath .

2.  Let @xmath be indeterminates and let

      -- -------- --
         @xmath   
      -- -------- --

3.  Use the MeatAxe to find matrices @xmath corresponding to the
    quadratic forms preserved by @xmath and @xmath .

4.  A necessary condition on @xmath for it to conjugate @xmath to @xmath
    is the following equation:

      -- -------- -- --------
         @xmath      (3.54)
      -- -------- -- --------

    which determines @xmath quadratic equations in @xmath .

5.  Hence we obtain @xmath where each element of @xmath has degree
    @xmath and @xmath . Every @xmath has @xmath coefficients, so we
    obtain an additive group homomorphism @xmath .

6.  Now @xmath , so let @xmath be a basis of @xmath and let @xmath for
    @xmath .

7.  By Proposition 2.43 , the variety of the ideal @xmath has size
    @xmath . Find this variety using Theorem 1.3 .

8.  Let @xmath be the corresponding elements of @xmath . Clearly, one of
    them must also lie in @xmath and conjugate @xmath to @xmath , since
    our @xmath exists and satisfies the necessary conditions which led
    to @xmath and @xmath . Use Theorem 3.63 to determine which @xmath
    satisfies @xmath .

Clearly, this is a Las Vegas algorithm and the expected time complexity
follows from Theorem 1.3 . ∎

###### Theorem 3.78.

Assume the Suzuki Conjectures, the Big Ree Conjectures, and an oracle
for the discrete logarithm problem in @xmath . There exists a Las Vegas
algorithm that, given @xmath such that @xmath , finds @xmath such that
@xmath . The algorithm has expected time complexity @xmath field
operations.

###### Proof.

The algorithm proceeds as follows:

1.  Use Lemma 3.76 to find @xmath and @xmath such that @xmath and @xmath
    . Let @xmath so that @xmath .

2.  Similarly find @xmath such that @xmath .

3.  Use the MeatAxe to split up the modules of each @xmath . By
    Proposition 2.41 , we obtain @xmath -dimensional submodules, and
    hence surjective homomorphisms @xmath for @xmath .

4.  Use Theorem 3.1 to find effective isomorphisms @xmath for @xmath .
    If @xmath is the standard generating set for @xmath , we then obtain
    standard generating sets @xmath for @xmath by obtaining @xmath s for
    @xmath in the generators of @xmath and then evaluating these on
    @xmath .

5.  Let @xmath be the module for @xmath and let @xmath be the module for
    @xmath . Now @xmath , and all @xmath are equal, so we can use the
    MeatAxe to find a change of basis matrix @xmath between @xmath and
    @xmath .

6.  Then @xmath and hence @xmath . Use Lemma 3.77 to find @xmath such
    that @xmath . Hence @xmath .

Clearly, this is a Las Vegas algorithm and the expected time complexity
follows from Lemma 3.76 . ∎

#### 3.3.5. Constructive recognition

Finally, we can now state and prove our main theorem.

###### Theorem 3.79.

Assume the Suzuki Conjectures, the Big Ree Conjectues, and an oracle for
the discrete logarithm problem in @xmath . There exists a Las Vegas
algorithm that, given @xmath satisfying the assumptions in Section 1.2.7
, with @xmath , @xmath and @xmath , finds an effective isomorphism
@xmath . The algorithm has expected time complexity @xmath field
operations.

The inverse of @xmath is also effective. Each image and pre-image of
@xmath can be computed using @xmath field operations.

###### Proof.

Use Theorem 3.78 to obtain @xmath such that @xmath . An an effective
isomorphism @xmath is then defined by @xmath , which clearly can be
computed in @xmath field operations. The expected time complexity
follows from Theorem 3.78 . ∎

## Chapter 4 Sylow subgroups

We will now describe algorithms for finding and conjugating Sylow
subgroups of the exceptional groups under consideration. Hence we
consider the following problems:

1.  Given @xmath , such that @xmath for one of our exceptional groups
    @xmath , and given a prime number @xmath , find @xmath such that
    @xmath is a Sylow @xmath -subgroup of @xmath .

2.  Given @xmath , such that @xmath for some of our exceptional groups
    @xmath , and given a prime number @xmath and @xmath such that both
    @xmath and @xmath are Sylow @xmath -subgroups of @xmath , find
    @xmath such that @xmath .

The second problem is the difficult one, and often there are some primes
that are especially difficult. We will refer to these problems as the
“Sylow subgroup problems” for a certain prime @xmath . The first problem
is referred to as “Sylow generation” and the second as “Sylow
conjugation”.

### 4.1. Suzuki groups

We now consider the Sylow subgroup problems for the Suzuki groups. We
will use the notation from Section 2.1 , and we will make heavy use of
the fact that we can use Theorem 3.26 to constructively recognise the
Suzuki groups. Hence we assume that @xmath satisfies the assumptions in
Section 1.2.7 , so @xmath .

By Theorem 2.1 and Proposition 2.4 , @xmath and all three factors are
pairwise relatively prime. Hence we obtain three cases for a Sylow
@xmath -subgroup @xmath of @xmath .

1.  @xmath divides @xmath , so @xmath . Then @xmath is conjugate to
    @xmath and hence @xmath fixes a unique point @xmath of @xmath ,
    which is easily found using the MeatAxe.

2.  @xmath divides @xmath . Then @xmath is cyclic and conjugate to a
    subgroup of @xmath . Hence @xmath fixes two distinct points @xmath ,
    and these points are easily found using the MeatAxe.

3.  @xmath divides @xmath . Then @xmath is cyclic and conjugate to a
    subgroup of @xmath or @xmath , and @xmath has no fixed points. This
    is the difficult case.

###### Theorem 4.1.

Assume the Suzuki Conjectures and an oracle for the discrete logarithm
problem in @xmath . There exist Las Vegas algorithms that solve the
Sylow subgroup problems for @xmath in @xmath . Once constructive
recognition has been performed, the expected time complexity of the
Sylow generation is @xmath field operations, and @xmath field operations
for the Sylow conjugation.

###### Proof.

Let @xmath . Using the effective isomorphism, it is sufficient to solve
the problems in the standard copy.

The constructive recognition uses Theorem 3.12 to find sets @xmath and
@xmath of “standard generators” for @xmath and @xmath , respectively.

A generating set for a random Sylow @xmath -subgroup @xmath of @xmath
can therefore be computed by taking a random @xmath , and as generating
set for @xmath take @xmath . To obtain a Sylow @xmath -subgroup @xmath
of @xmath , note that we already have the @xmath generators @xmath and
@xmath as @xmath s of length @xmath , so we can evaluate them on @xmath
. Hence the expected time complexity is as stated.

Given two Sylow @xmath -subgroups of @xmath , we use the effective
isomorphism to map them to @xmath using @xmath field operations. We can
use the MeatAxe to find the points @xmath that are fixed by the
subgroups. Then use Lemma 3.11 with @xmath to find @xmath such that
@xmath and @xmath such that @xmath . Then @xmath conjugates one Sylow
subgroup to the other, and we already have this element as an @xmath of
length @xmath .

In the case where @xmath and @xmath , we know that @xmath maps @xmath to
@xmath . Then use Algorithm 3.1.3 to obtain an @xmath for @xmath of
length @xmath .

Hence we can evaluate it on @xmath and obtain @xmath that conjugates
@xmath to @xmath . Thus the expected time complexity follows from Lemma
3.11 and Theorem 3.13 . ∎

###### Theorem 4.2.

Assume the Suzuki Conjectures and an oracle for the discrete logarithm
problem in @xmath . There exist Las Vegas algorithms that solve the
Sylow subgroup problems for @xmath in @xmath . Once constructive
recognition has been performed, the expected time complexity of the
Sylow generation is @xmath field operations, and @xmath field operations
for the Sylow conjugation.

###### Proof.

Let @xmath . In this case the Sylow generation is easy, since we can
determine the highest power @xmath of @xmath such that @xmath , find a
random element of pseudo-order @xmath , use Proposition 1.4 to obtain an
element of order @xmath , then evaluate its @xmath on @xmath . By
Proposition 2.6 , the expected number of random selections, and hence
the length of the @xmath , is @xmath , and we need @xmath field
operations to find the order. Then we evaluate the @xmath on @xmath
using @xmath field operations, so the expected time complexity is as
stated.

For the Sylow conjugation, recall that the constructive recognition uses
Theorem 3.12 to find sets @xmath and @xmath of “standard generators” for
@xmath and @xmath , respectively.

Given two Sylow @xmath -subgroups of @xmath , we use the effective
isomorphism to map them to @xmath using @xmath field operations. Let
@xmath be the resulting subgroups. Using the MeatAxe, we can find @xmath
that are fixed by @xmath and @xmath that are fixed by @xmath . Order the
points so that @xmath and @xmath .

Use Lemma 3.11 with @xmath to find @xmath such that @xmath . Then use
@xmath to find @xmath such that @xmath . Similarly we find @xmath such
that @xmath and @xmath . Then @xmath conjugates one Sylow subgroup to
the other, and we already have this element as an @xmath of length
@xmath . Hence we can evaluate it on @xmath and obtain @xmath that
conjugates @xmath to @xmath . Thus the expected time complexity is as
stated.

∎

###### Lemma 4.3.

There exists a Las Vegas algorithm that, given @xmath with @xmath both
dividing @xmath , finds @xmath such that @xmath is conjugate to @xmath
in @xmath . The expected time complexity is @xmath field operations.

###### Proof.

The algorithm proceeds as follows:

1.  Find the minimal polynomial @xmath of @xmath . By Theorem 2.1 ,
    @xmath acts irreducibly on @xmath , so @xmath is irreducible. Let
    @xmath , so that @xmath is the splitting field of @xmath . Clearly
    @xmath and @xmath where @xmath is a root of @xmath . Moreover,
    @xmath defines an isomorphism @xmath , where the latter is the
    subfield of @xmath generated by @xmath .

2.  Find the minimal polynomial @xmath of @xmath . Then @xmath is also
    the splitting field of @xmath , and if @xmath is a root of @xmath ,
    then @xmath is expressed as a polynomial @xmath in @xmath , with
    coefficients in @xmath . Similarly, @xmath defines an isomorphism
    @xmath .

3.  Now @xmath defines an isomorphism @xmath as @xmath , because @xmath
    and @xmath have the same minimal polynomial. Hence if we let @xmath
    , then @xmath has the same eigenvalues as @xmath , so @xmath and
    @xmath are conjugate in @xmath . Then @xmath and @xmath , so by
    Proposition 2.8 , @xmath is also conjugate to @xmath in @xmath .
    Moreover, both @xmath and @xmath are subgroups of @xmath , but since
    @xmath , they must be the same. Thus @xmath .

By [ Gie95 ] , the minimal polynomial is found using @xmath field
operations. Hence by Theorem 1.1 , the expected time complexity is as
stated. ∎

The conjugation algorithm described in the following result is
essentially due to Mark Stather and Scott Murray.

###### Theorem 4.4.

Assume the Suzuki Conjectures and an oracle for the discrete logarithm
problem in @xmath . There exist Las Vegas algorithms that solve the
Sylow subgroup problems for @xmath in @xmath . Once constructive
recognition has been performed, the expected time complexity of the
Sylow generation is @xmath field operations, and @xmath field operations
for the Sylow conjugation.

###### Proof.

Let @xmath . The Sylow generation is analogous to the case in Theorem
4.2 , since by Proposition 2.6 , we can easily find elements of
pseudo-order @xmath .

Given two Sylow @xmath -subgroups of @xmath , we use the effective
isomorphism to map them to @xmath , using @xmath field operations. The
resulting generating sets must contain elements @xmath of order @xmath ,
since the Sylow subgroups are cyclic. Let @xmath be as in ( 2.24 ), so
that @xmath preserves the symplectic form @xmath , and let @xmath be as
in Section 2.1.2 : the automorphism of @xmath whose set of fixed points
is @xmath .

1.  Use Lemma 4.3 to replace @xmath . Henceforth assume that @xmath and
    @xmath are conjugate in @xmath .

2.  Find @xmath such that @xmath . This can be done by a similarity
    test, or computation of Jordan forms, using [ Ste97 ] . The next
    step is to find a matrix @xmath such that @xmath , and @xmath also
    conjugates @xmath to @xmath .

3.  Let @xmath (the automorphism group of the module of @xmath ). Since
    @xmath is irreducible, by Schur’s Lemma @xmath . Such an isomorphism
    @xmath , and its inverse, can be found using the MeatAxe.

4.  Now define an automorphism @xmath of @xmath as @xmath . Then @xmath
    has order @xmath and @xmath . Recall that @xmath has a unique
    automorphism of order @xmath ( @xmath ), which must be @xmath .

5.  Let @xmath and observe that @xmath . We want to find @xmath such
    that @xmath , which is equivalent to @xmath . Using @xmath , this is
    a norm equation in @xmath over @xmath . In other words, we consider
    @xmath , which is solved for example using [ HRD05 , Lemma 2.2] .

6.  Hence @xmath lies in @xmath , and @xmath fixes @xmath , so @xmath
    conjugates @xmath to @xmath . The next step is to find a matrix
    @xmath , such that @xmath , and such that @xmath also conjugates
    @xmath to @xmath . Hence we want @xmath and @xmath .

7.  Find @xmath of order @xmath , by taking the @xmath power of a
    primitive element. Then @xmath , which implies that @xmath , and
    hence @xmath . Similarly, every element of @xmath gives rise to
    matrices in @xmath . We therefore want to find an integer @xmath ,
    such that @xmath .

8.  Moreover, we want

      -- -------- -- -------
         @xmath      (4.1)
      -- -------- -- -------

    so if we let @xmath , we want to find an integer @xmath such that
    @xmath .

9.  Use the discrete log oracle to find @xmath such that @xmath . Since
    @xmath it follows that @xmath . Use the discrete log oracle to find
    @xmath such that @xmath . Our equation turns into @xmath , which we
    solve to find @xmath .

By [ HRD05 , Lemma 2.2] , this whole process uses expected @xmath field
operations. Finally we use the effective isomorphism to map the
conjugating element back to @xmath . Hence the time complexity is as
stated. ∎

### 4.2. Small Ree groups

We now consider the Sylow subgroup problems for the small Ree groups. We
will use the notation from Section 2.2 , and we will make heavy use of
the fact that we can use Theorem 3.62 to constructively recognise the
small Ree groups. Hence we assume that @xmath satisfies the assumptions
in Section 1.2.7 , so @xmath .

By Proposition 2.15 , we obtain @xmath cases for a Sylow @xmath
-subgroup @xmath of @xmath .

1.  @xmath , so that by [ HB82 , Chapter @xmath , Theorem @xmath ] ,
    @xmath is elementary abelian of order @xmath and @xmath .

2.  @xmath divides @xmath , so @xmath . Then @xmath is conjugate to
    @xmath and hence @xmath fixes a unique point @xmath of @xmath ,
    which is easily found using the MeatAxe.

3.  @xmath divides @xmath and @xmath . Then @xmath is cyclic and
    conjugate to a subgroup of @xmath . Hence @xmath fixes two distinct
    points @xmath , and these points are easily found using the MeatAxe.

4.  @xmath divides @xmath and @xmath . Then @xmath is cyclic and
    conjugate to a subgroup of @xmath , @xmath or @xmath from
    Proposition 2.20 . In this case, we have only solved the Sylow
    generation problem.

###### Theorem 4.5.

Assume the small Ree Conjectures and an oracle for the discrete
logarithm problem in @xmath . There exist Las Vegas algorithms that
solve the Sylow subgroup problems for @xmath in @xmath . Once
constructive recognition has been performed, the expected time
complexity of the Sylow generation is @xmath field operations, and
@xmath field operations for the Sylow conjugation.

###### Proof.

Let @xmath . The constructive recognition uses Theorem 3.40 to find sets
@xmath and @xmath of “standard generators” for @xmath and @xmath ,
respectively.

A generating set for a random Sylow @xmath -subgroup @xmath of @xmath
can therefore be computed by finding a random @xmath , and as generating
set for @xmath take @xmath . To obtain a Sylow subgroup @xmath of @xmath
, note that we already have the @xmath generators of @xmath and @xmath
as @xmath s of length @xmath , so we can evaluate them on @xmath . Hence
the expected time complexity is as stated.

Given two Sylow @xmath -subgroups of @xmath , we use the effective
isomorphism to map them to @xmath using @xmath field operations. We can
use the MeatAxe to find the points @xmath that are fixed by the
subgroups. Then use Lemma 3.39 with @xmath to find @xmath , such that
@xmath , and @xmath , such that @xmath . Then @xmath conjugates one
Sylow subgroup to the other, and we already have this element as an
@xmath of length @xmath .

In the case where @xmath and @xmath , we know that @xmath maps @xmath to
@xmath . Then use Algorithm 3.2.3 to obtain an @xmath for @xmath of
length @xmath .

Hence we can evaluate it on @xmath and obtain @xmath that conjugates
@xmath to @xmath . Thus the expected time complexity follows from Lemma
3.39 and Theorem 3.42 . ∎

###### Theorem 4.6.

Assume the small Ree Conjectures and an oracle for the discrete
logarithm problem in @xmath . There exist Las Vegas algorithms that
solve the Sylow subgroup problems for @xmath , @xmath , in @xmath . Once
constructive recognition has been performed, the expected time
complexity of the Sylow generation is @xmath field operations, and
@xmath field operations for the Sylow conjugation.

###### Proof.

Let @xmath . In this case the Sylow generation is easy, since we can
determine the highest power @xmath of @xmath such that @xmath , find a
random element of pseudo-order @xmath , use Proposition 1.4 to obtain an
element of order @xmath , then evaluate its @xmath on @xmath . By
Proposition 2.23 , the expected number of random selections, and hence
the length of the @xmath is @xmath , and we need @xmath field operations
to find the order. Then we evaluate the @xmath on @xmath using @xmath
field operations, so the expected time complexity is as stated.

For the Sylow conjugation, recall that the constructive recognition uses
Theorem 3.40 to find sets @xmath and @xmath of “standard generators” for
@xmath and @xmath , respectively.

Given two Sylow @xmath -subgroups of @xmath , we use the effective
isomorphism to map them to @xmath using @xmath field operations. Let
@xmath be the resulting subgroups. Using the MeatAxe, we can find @xmath
that are fixed by @xmath and @xmath that are fixed by @xmath . Order the
points so that @xmath and @xmath .

Use Lemma 3.39 with @xmath to find @xmath , such that @xmath . Then use
@xmath to find @xmath , such that @xmath . Similarly we find @xmath ,
such that @xmath and @xmath . Then @xmath conjugates one Sylow subgroup
to the other, and we already have this element as an @xmath of length
@xmath . Hence we can evaluate it on @xmath and obtain @xmath that
conjugates @xmath to @xmath . Thus the expected time complexity is as
stated. ∎

###### Theorem 4.7.

Assume the small Ree Conjectures and an oracle for the discrete
logarithm problem in @xmath . There exists a Las Vegas algorithm that
solves the Sylow generation problem for @xmath , @xmath , in @xmath .
Once constructive recognition has been performed, the expected time
complexity of the Sylow generation is @xmath field operations.

###### Proof.

Let @xmath . The Sylow generation is easy, since we can find an element
of pseudo-order @xmath or @xmath , use Proposition 1.4 to obtain an
element of order @xmath , then evaluate its @xmath on @xmath . By
Proposition 2.23 , the expected number of random selections, and hence
the length of the @xmath is @xmath , and we need @xmath field operations
to find the order. Then we evaluate the @xmath on @xmath using @xmath
field operations, so the expected time complexity is as stated. ∎

This result is due to Mark Stather and is the same as [ Sta06 , Lemma
@xmath ] .

###### Lemma 4.8.

Let @xmath be a group and let @xmath be such that @xmath with @xmath
odd. Let @xmath have order @xmath . Let @xmath and @xmath be Sylow
@xmath -subgroups of @xmath . Then @xmath for some @xmath if and only if
@xmath . Moreover if @xmath , then @xmath

###### Proof.

Let @xmath . Then @xmath is a subgroup of @xmath of index @xmath , that
contains @xmath , but does not contain @xmath or @xmath . Since @xmath
has index @xmath in both @xmath and @xmath it follows that @xmath . But
@xmath is a Sylow @xmath -subgroup of @xmath so,

  -- -- --
        
  -- -- --

The second statement is an application of Proposition 1.8 , modulo
@xmath . ∎

###### Theorem 4.9.

Assume the small Ree Conjectures and an oracle for the discrete
logarithm problem in @xmath . There exists a Las Vegas algorithm that
solves the Sylow generation problem for @xmath in @xmath . Once
constructive recognition has been performed, the expected time
complexity is @xmath field operations.

###### Proof.

Let @xmath . We want to find three commuting involutions in @xmath .
Using the first three steps of the algorithm in Section 3.2.2 , we find
an involution @xmath and @xmath . Using the notation of that algorithm,
we can let the second involution @xmath be @xmath where @xmath is the
second matrix in ( 3.28 ).

We then want to find the third involution in the centraliser of @xmath
in @xmath . In our case this centraliser has structure @xmath . Hence
its proportion of elements of even order is @xmath , and @xmath of its
elements are involutions other than @xmath . Using the Bray algorithm we
can therefore compute random elements of this centraliser until we find
such an involution @xmath .

Clearly @xmath will all commute. As in the proof of Corollary 3.36 , the
expected time to find @xmath , constructively recognise @xmath and find
@xmath is @xmath field operations. By the above, the expected time to
find @xmath is @xmath field operations. The involutions will be found as
@xmath s, where @xmath and @xmath have length @xmath , because the
generators of @xmath are @xmath s of length @xmath . By Lemma 3.29 and
Theorem 1.12 , @xmath has length @xmath . Thus we can evaluate them on
@xmath using @xmath field operations, and the expected time complexity
is as stated. ∎

###### Theorem 4.10.

Assume the small Ree Conjectures and an oracle for the discrete
logarithm problem in @xmath . There exists a Las Vegas algorithm that
solves the Sylow conjugation problem for @xmath in @xmath . Once
constructive recognition has been performed, the expected time
complexity is @xmath field operations.

###### Proof.

Let @xmath . Given two Sylow @xmath -subgroups of @xmath , we use the
effective isomorphism to map them to @xmath , using @xmath field
operations. The resulting generating sets are @xmath and @xmath , where
both the @xmath and @xmath are commuting involutions. We may assume that
@xmath so that @xmath .

The algorithm proceeds as follows:

1.  By Proposition 2.26 , we can use the dihedral trick in @xmath and
    hence find @xmath such that @xmath . We then want to conjugate
    @xmath to @xmath while fixing @xmath .

2.  Using the first steps of the algorithm in Section 3.2.2 , we find
    @xmath , and use Theorem 1.13 to determine when we have the whole of
    @xmath . Observe that @xmath for all @xmath .

3.  Choose random @xmath . If @xmath has odd order, then @xmath .
    Conversely, if @xmath then @xmath has odd order with probability
    @xmath . Similarly, if @xmath has odd order, then @xmath , and the
    probability is the same. Repeat until either @xmath or @xmath has
    been proved to lie in @xmath and replace @xmath with this element.
    Do the same procedure with @xmath .

4.  Now @xmath , and by [ WP06 , Theorem @xmath ] , the dihedral trick
    works in @xmath . Hence find @xmath such that @xmath .

5.  Let @xmath , where @xmath is odd. If @xmath then let @xmath and
    otherwise let @xmath . By Lemma 4.8 , @xmath .

Finally, we use the effective isomorphism to map @xmath back to @xmath .
As in the proof of Corollary 3.36 , @xmath is found using expected
@xmath field operations, if we let @xmath in Theorem 1.13 . The expected
time complexity of the effective isomorphism follows from Theorem 3.62 .
∎

### 4.3. Big Ree groups

We now consider the Sylow subgroup problems for the Big Ree groups. We
will use the notation from Section 2.3 , and we will make heavy use of
the fact that we can use Theorem 3.79 to constructively recognise the
Big Ree groups. However, we can only do this in the natural
representation, and hence we will only consider the Sylow subgroup
problems in the natural representation. Hence we assume that @xmath .

It follows from [ DS99 ] that if @xmath then @xmath is even, or divides
any of the numbers @xmath . Hence we obtain several cases for a Sylow
@xmath -subgroup @xmath of @xmath .

1.  @xmath . Then @xmath has order @xmath and lies in @xmath for some
    involution @xmath of class @xmath . It consists of @xmath extended
    by a Sylow @xmath -subgroup in a Suzuki group contained in the
    centraliser.

2.  @xmath divides @xmath . Then @xmath has structure @xmath . If @xmath
    , then @xmath is contained in @xmath and consists of Sylow @xmath
    -subgroups from each Suzuki factor.

3.  @xmath divides @xmath or @xmath . Then @xmath is cyclic of order
    @xmath , and hence these Sylow subgroups are trivial to find. We do
    not consider this case.

4.  @xmath divides @xmath . We do not consider this case.

###### Theorem 4.11.

Assume the Suzuki Conjectures, the Big Ree Conjectures and an oracle for
the discrete logarithm problem in @xmath . There exists a Las Vegas
algorithm thats solve the Sylow generation problem for @xmath in @xmath
. Once constructive recognition has been performed, the expected time
complexity is @xmath field operations.

###### Proof.

Let @xmath . We see from the proof of Theorem 3.78 that during the
constructive recognition, we find @xmath for some involution @xmath of
class @xmath . We also find @xmath such that @xmath and @xmath .
Moreover, @xmath is constructively recognised, and @xmath are expressed
as @xmath s in @xmath of length @xmath .

Hence we can apply Theorem 4.1 and obtain a Sylow @xmath -subgroup
@xmath of @xmath using @xmath field operations. Now @xmath is a Sylow
@xmath -subgroup of @xmath . ∎

###### Theorem 4.12.

Assume the Suzuki Conjectures, the Big Ree Conjectures and an oracle for
the discrete logarithm problem in @xmath . There exist Las Vegas
algorithms that solve the Sylow generation problems for @xmath or @xmath
in @xmath . Once constructive recognition has been performed, the
expected time complexity is @xmath field operations.

###### Proof.

Let @xmath . We see from the proof of Theorem 3.78 that during the
constructive recognition, we find @xmath , and they commute, so @xmath .
Moreover, @xmath and @xmath are constructively recognised, and @xmath
are expressed as @xmath s in @xmath of length @xmath .

Hence we can apply Theorem 4.2 or 4.4 and obtain Sylow @xmath -subgroups
of @xmath and @xmath , using @xmath field operations. From the proof of
the Theorems, we see that there will be a constant number of generators,
which will be expressed as @xmath s in @xmath of length @xmath . Hence
we can evaluate them on @xmath using @xmath field operations. ∎

## Chapter 5 Maximal subgroups

We will now describe algorithms for finding and conjugating maximal
subgroups of the exceptional groups under consideration. Hence we
consider the following problems:

1.  Given @xmath , such that @xmath for some of our exceptional groups
    @xmath , find representatives @xmath of the conjugacy classes of
    (some or all of) the maximal subgroups of @xmath .

2.  Given @xmath , such that @xmath for some of our exceptional groups
    @xmath , and given @xmath such that @xmath and @xmath are conjugate
    to a specified maximal subgroup of @xmath , find @xmath such that
    @xmath .

It will turn out that because of the results about Sylow subgroup
conjugation in Chapter 4 , the second problem will most often be easy.
The first problem is therefore the difficult one. We will refer to these
problems as the “maximal subgroup problems”. The first problem is
referred to as “maximal subgroup generation” and the second as “maximal
subgroup conjugation”.

### 5.1. Suzuki groups

We now consider the maximal subgroup problems for the Suzuki groups. We
will use the notation from Section 2.1 , and we will make heavy use of
the fact that we can use Theorem 3.26 to constructively recognise the
Suzuki groups. Hence we assume that @xmath satisfies the assumptions in
Section 1.2.7 , so @xmath .

The maximal subgroups are given by Theorem 2.3 .

###### Theorem 5.1.

Assume the Suzuki Conjectures, an oracle for the discrete logarithm
problem in @xmath and an oracle for the integer factorisation problem.
There exist Las Vegas algorithms that solve the maximal subgroup
conjugation in @xmath . Once constructive recognition has been
performed, the expected time complexity is @xmath field operations.

###### Proof.

Let @xmath . In each case, we first use the effective isomorphism to map
the subgroups to @xmath using @xmath field operations. Therefore we
henceforth assume that @xmath .

Observe that all maximal subgroups, except the Suzuki groups over
subfields, are the normalisers of corresponding cyclic subgroups or
Sylow @xmath -subgroups. The Sylow conjugation algorithms can conjugate
these cyclic subgroups around, not only the Sylow subgroups that they
contain. Moreover, the cyclic subgroups and Sylow @xmath -subgroups are
the derived groups of the corresponding maximal subgroups.

Hence we can obtain probable generators for the cyclic subgroups or
Sylow @xmath -subgroups using @xmath field operations, and we can verify
that we have the whole of these subgroups as follows:

1.  For @xmath , the generators of the derived group @xmath must contain
    an element of order @xmath .

2.  For @xmath , the generators of the derived group @xmath must contain
    an element of order @xmath .

3.  For @xmath , we need not obtain the whole derived group @xmath . It
    is enough that we obtain a subgroup of the derived group that fixes
    a unique point of @xmath , but this might require @xmath generators.

Note that in these cases we need the integer factorisation oracle to
find the precise order. When we have generators for the cyclic
subgroups, we use Theorem 4.1 , 4.2 and 4.4 to find conjugating elements
for the cyclic subgroups. These elements will also conjugate the maximal
subgroups, because they normalise the cyclic subgroups.

Finally, consider the case when @xmath and @xmath are isomorphic to a
Suzuki group over @xmath . In this case we first use the algorithm in
Section 1.2.10 , to obtain @xmath that conjugates @xmath and @xmath into
@xmath . Then we use Theorem 3.17 to find @xmath that conjugates the
Suzuki groups to each other. Hence @xmath conjugates @xmath to @xmath ,
and therefore it normalises @xmath . However, @xmath does not
necessarily lie in @xmath , but only in @xmath , since neither @xmath
nor @xmath are guaranteed to lie in @xmath . Therefore @xmath where
@xmath and @xmath . We can find @xmath by calculating the determinant
and taking its (unique) @xmath th root, so we can divide by the scalar
matrix, and we then end up with @xmath , which also conjugates @xmath to
@xmath .

Finally we use the effective isomorphism to map @xmath to @xmath . The
expected time complexity follows from Theorem 4.1 , 4.2 , 4.4 and 3.17 .
∎

###### Lemma 5.2.

If @xmath satisfy that @xmath , @xmath , @xmath and @xmath , then @xmath
and hence is a maximal subgroup of @xmath .

###### Proof.

Clearly, @xmath is an image in @xmath of the group @xmath . Since @xmath
is soluble and @xmath has the specified order, @xmath must be one of the
@xmath from Theorem 2.3 . ∎

###### Theorem 5.3.

Assume the Suzuki Conjectures, an oracle for the discrete logarithm
problem in @xmath and an oracle for the integer factorisation problem.
There exist Las Vegas algorithms that solve the maximal subgroup
generation in @xmath . Once constructive recognition has been performed,
the expected time complexity is @xmath field operations.

###### Proof.

Let @xmath . Using the effective isomorphism, it is sufficient to obtain
generators for the maximal subgroups in @xmath .

Let @xmath be a primitive element. Clearly @xmath , and @xmath . For
each @xmath such that @xmath , we have @xmath where @xmath . Hence
@xmath and @xmath .

The difficult case is therefore @xmath and @xmath . We want to use Lemma
5.2 , with @xmath and @xmath playing the roles of @xmath and @xmath .
Hence we proceed as follows:

1.  Choose random @xmath of order @xmath . Note that we need the integer
    factorisation oracle, since we need the precise order of @xmath .
    Let @xmath .

2.  Let @xmath be indeterminates and consider the equations @xmath and
    @xmath . If we can find solutions for @xmath , then by Proposition
    2.9 , @xmath with high probability, and Proposition 2.8 implies that
    @xmath .

3.  The second equation implies @xmath , and

      -- -------- -- -------
         @xmath      (5.1)
      -- -------- -- -------

    where the third equation is @xmath times the first added to the
    second.

4.  The quadratic equation has solutions @xmath and @xmath , which both
    give the value @xmath of @xmath . Hence repeat with another @xmath
    if @xmath , which happens with probability @xmath .

5.  Lemma 5.2 now implies that @xmath is @xmath or @xmath .

Finally, we see that we have @xmath generators, which we map back to
@xmath using the effective isomorphism. Hence the expected time
complexity is as stated. ∎

### 5.2. Small Ree groups

We now consider the maximal subgroup problems for the small Ree groups.
We will use the notation from Section 2.1 . We will make heavy use of
the fact that we can use Theorem 3.62 to constructively recognise the
small Ree groups. Hence we assume that @xmath satisfies the assumptions
in Section 1.2.7 , so @xmath .

The maximal subgroups are given by Proposition 2.20 .

###### Theorem 5.4.

Assume the small Ree Conjectures and an oracle for the discrete
logarithm problem in @xmath . There exist Las Vegas algorithms that
solve the maximal subgroup conjugation in @xmath for the point
stabiliser, the involution centraliser and Ree groups over subfields.
Once constructive recognition has been performed, the expected time
complexity is @xmath field operations.

###### Proof.

Let @xmath . In each case, we first use the effective isomorphism to map
the subgroups to @xmath using @xmath field operations. Therefore we
henceforth assume that @xmath .

Observe that the point stabiliser is the normaliser of a Sylow @xmath
-subgroup, which is the derived group of the point stabiliser. We can
therefore obtain probable generators for the Sylow @xmath -subgroup
using @xmath field operations. We only need enough generators so that
they generate a subgroup of the derived group that fixes a unique point
of @xmath , and this we can easily verify using the MeatAxe. When we
have generators for this subgroup, we use Theorem 4.5 to find a
conjugating element. This element will also conjugate the maximal
subgroup, because it normalises the Sylow subgroup.

For the involution centraliser, we choose random elements of @xmath .
Since @xmath for some involution @xmath , with probability @xmath we
will obtain an element of even order that powers up to @xmath . We can
check that we obtain @xmath since it is the unique involution that is
centralised by @xmath (and therefore by @xmath ). Hence we can find the
involutions @xmath and @xmath that are centralised by @xmath and @xmath
. By Proposition 2.26 we can use the dihedral trick to find @xmath that
conjugates @xmath to @xmath , using @xmath field operations. Since
@xmath and @xmath centralise these, it follows that @xmath .

Finally, consider the case when @xmath and @xmath are isomorphic to a
Ree group over @xmath . In this case we first use the algorithm in
Section 1.2.10 , to obtain @xmath that conjugates @xmath and @xmath into
@xmath . Then we use Theorem 3.47 to find @xmath that conjugates the
resulting Ree groups to each other. Hence @xmath conjugates @xmath to
@xmath , and hence normalises @xmath . However, it does not necessarily
lie in @xmath , but only in @xmath , since neither @xmath nor @xmath has
to lie in @xmath . Therefore it is of the form @xmath , where @xmath and
@xmath . We can find @xmath by calculating the determinant and taking
the (unique) @xmath th root, so we can divide by the scalar matrix, and
we then end up with @xmath , that also conjugates @xmath to @xmath .

Finally we use the effective isomorphism to map the conjugating element
back to @xmath . The expected time complexity follows from Theorem 4.5 ,
3.47 and 3.62 . ∎

###### Lemma 5.5.

If @xmath satisfy that @xmath , @xmath , @xmath and @xmath or @xmath ,
then @xmath or @xmath and hence is a maximal subgroup of @xmath .

###### Proof.

Clearly, @xmath is an image in @xmath of the group @xmath . Since @xmath
is soluble and @xmath has the specified order, @xmath must be one of the
@xmath from Proposition 2.20 . ∎

###### Lemma 5.6.

Let @xmath . For each @xmath , there exist @xmath such that @xmath ,
@xmath , @xmath and @xmath .

###### Proof.

Consider the case @xmath . There exists @xmath such that @xmath with
@xmath and @xmath . Observe that @xmath . If @xmath and @xmath then
@xmath , and @xmath for some @xmath , depending on @xmath . Clearly we
can choose @xmath such that @xmath , and hence @xmath .

The other case is analogous. ∎

###### Lemma 5.7.

Let @xmath and @xmath . For each @xmath , there exist @xmath such that
@xmath and @xmath or @xmath and @xmath .

###### Proof.

Let @xmath be as in Lemma 5.6 . It is sufficient to prove that there
exists @xmath such that @xmath is conjugate to @xmath or @xmath .

Since @xmath has order @xmath , it fixes a point @xmath . Also, @xmath
is doubly transitive so there exists @xmath such that @xmath . Then
@xmath for some @xmath . Observe that @xmath does not fix @xmath , since
otherwise @xmath .

Now @xmath for some point @xmath , and @xmath acts transitively on the
points other than @xmath , so there exists @xmath such that @xmath .
Then @xmath interchanges @xmath and @xmath , and so does @xmath . Hence
@xmath , so @xmath , for some @xmath .

Let @xmath such that @xmath , and let @xmath . There are two possible
values for @xmath , either @xmath or @xmath . In the former case @xmath
, and in the latter case @xmath . ∎

###### Conjecture 5.8.

Let @xmath for some @xmath and let @xmath . For every @xmath , the
ideals in @xmath generated by the following systems are
zero-dimensional:

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (5.3)
  -- -------- -- -------

###### Theorem 5.9.

Assume the small Ree Conjectures, Conjecture 5.8 , an oracle for the
discrete logarithm problem in @xmath and an oracle for the integer
factorisation problem. There exist Las Vegas algorithms that solve the
maximal subgroup generation in @xmath . Once constructive recognition
has been performed, the expected time complexity is @xmath field
operations.

###### Proof.

Let @xmath . Using the effective isomorphism, it is sufficient to obtain
generators for the maximal subgroups in @xmath .

Let @xmath be a primitive element. Clearly @xmath , and by following a
procedure similar to [ Wil06 ] , we see that @xmath . For each @xmath
such that @xmath , we have @xmath where @xmath . Hence @xmath and @xmath
.

The difficult cases are therefore the @xmath . In the light of Lemma 5.7
, we can proceed as follows:

1.  Choose random @xmath of order @xmath or @xmath , corresponding to
    the order of @xmath . By Corollary 2.24 this is done using expected
    @xmath field operations. Note that we need the integer factorisation
    oracle since we need the precise order in this case.

2.  Introduce indeterminates @xmath and consider the equations @xmath
    and @xmath , or similarly with @xmath instead of @xmath . We want to
    find solutions @xmath for @xmath as in the Lemma. By Proposition
    2.25 , the trace determines the order in these cases, which leads us
    to consider these equations.

3.  Elements of order @xmath have trace @xmath . Hence we obtain
    equations in @xmath :

      -- -------- -- -------
         @xmath      (5.4)
      -- -------- -- -------

    By letting @xmath , @xmath , @xmath , @xmath , this is precisely one
    of the systems in Conjecture 5.8 , and thus we can use Theorem 1.3
    to find all solutions using @xmath field operations.

4.  By Lemma 5.7 , there will be solutions @xmath . By Lemma 5.5 , the
    resulting @xmath generates @xmath together with @xmath or @xmath .

Finally, we see that we have @xmath generators, which we map back to
@xmath using the effective isomorphism. Hence the expected time
complexity is as stated. ∎

### 5.3. Big Ree groups

We now consider the maximal subgroup problems for the Big Ree groups. We
will use the notation from Section 2.3 , and we will make heavy use of
the fact that we can use Theorem 3.79 to constructively recognise the
Big Ree groups. However, we can only do this in the natural
representation, and hence we will only consider the maximal subgroup
problems in the natural representation.

The maximal subgroups are listed in [ Mal91 ] , but we will only
generate some of them.

###### Theorem 5.10.

Assume Conjectures 3.4 and 3.64 , and an oracle for the discrete
logarithm problem in @xmath . There exist Las Vegas algorithms that,
given @xmath finds @xmath such that @xmath and @xmath are the two
maximal parabolics, @xmath and @xmath . Once constructive recognition
has been performed, the expected time complexity is @xmath field
operations.

###### Proof.

Let @xmath , and let @xmath be the effective isomorphism. Generators for
the subgroups are given in Proposition 2.36 and Proposition 2.44 . Since
they all have constant size, and pre-images of @xmath can be computed in
@xmath field operations, we obtain @xmath in @xmath field operations. ∎

## Chapter 6 Implementation and performance

All the algorithms that have been described have been implemented in the
computer algebra system . As we remarked in Section 1.2.11 , the
implementation has been a major part of the work and has heavily
influenced the nature of the theoretical results. The algorithms have
been developed with the implementation in mind from the start, and hence
only algorithms that can be implemented and executed on current hardware
have been developed.

This chapter is concerned with the implementation, and we will provide
experimental evidence of the fact that the algorithms indeed are
efficient in practice. The evidence will be in the form of benchmark
results, tables and diagrams. This chapter is therefore not so much
about mathematics, but rather about software engineering or computer
science.

The implementations were developed during a time span of 2-3 years,
using versions 2.11-5 and above. The benchmark results have all been
produced using version 2.13-12, Intel64 flavour, statically linked.

The hardware used during the benchmark was a PC, with an Intel Xeon CPU,
clocked at @xmath GHz, and with @xmath GB of RAM. The operating system
was Debian GNU/Linux Sarge, with kernel version 2.6.8-12-em64t-p4-smp.

The implementations used the existing implementations of the algorithms
described in Chapter 1 . These include implementations of the following:

-   A discrete log algorithm, in particular Coppersmith’s algorithm. The
    implementation is described in [ Tho01 ] .

-   The product replacement algorithm.

-   The algorithm from Theorem 1.1 .

-   The Order algorithm, for calculating the order (or pseudo-order) of
    a matrix.

-   The black box naming algorithm from [ BKPS02 ] .

-   The algorithms from Theorems 1.12 and 1.13 .

-   The three algorithms from Section 1.2.10 .

We used MATLAB and [ R D05 ] to produce the figures. In every case, the
benchmark of an algorithm was performed by running the algorithm a
number of times for each field with size @xmath lying in some specified
range. We then recorded the time @xmath (in seconds) taken by the
algorithm. However, to be able to compare the benchmark results with our
stated time complexities, we want to display not the time in seconds,
but the number of field operations. Moreover, the input size is
polynomial in @xmath and not @xmath . Hence we first recorded the time
@xmath for @xmath multiplications in @xmath and display @xmath against
@xmath . Of course, @xmath is in principle enough, but we chose @xmath
to achieve a scaling of the graph.

In , Zech logarithms are used for the finite field arithmetic in @xmath
if @xmath (for some @xmath , at present @xmath ), and for larger fields
represents the field elements as polynomials over the largest subfield
that is smaller than @xmath , rather than over the prime field. The
reason is that the polynomials then have fewer terms, and hence the
field arithmetic is faster, than if the polynomials have coefficients in
the prime field. Since the subfield is small enough to use Zech
logarithms, arithmetic in the subfield is not much slower than in the
prime field.

Now consider a field of size @xmath . If @xmath is prime, there are no
subfields except the prime field, and the field arithmetic will be slow,
but if @xmath has a divisor only slightly smaller than @xmath , then the
field arithmetic will be fast. Since it might happen that @xmath is
prime but @xmath is divisible by @xmath , we will get jumps in our
benchmark figures, unless we turn off all these optimisations in .
Therefore, this is what we do, and hence any jumps are the result of
group theoretical properties, the discrete log and factorisation
oracles, and the probabilistic nature of the algorithms.

All the non-constructive recognition algorithms that we have presented,
in Sections 3.1.1 , 3.2.1 and 3.3.1 are extremely fast, and in practice
constant time for the field sizes under consideration. Hence we do not
display any benchmarks of them.

### 6.1. Suzuki groups

In the cases of the Suzuki groups, the field size is always @xmath for
some @xmath . Hence we display the time against @xmath .

In Figure 6.1 we show the benchmark of the first two steps of the
algorithm in Theorem 3.12 , where a stabiliser in @xmath of a point of
@xmath is computed. For each field size, we made @xmath runs of the
algorithms, using random generating sets and random points.

Notice that the time is very much dominated by the discrete logarithm
computations. The oscillations in the discrete log timings have number
theoretic reasons. When @xmath , the factorisation of @xmath contains no
prime with more than @xmath decimal digits, hence discrete log is very
fast. On the other hand, when @xmath , the factorisation of @xmath
contains a prime with @xmath decimal digits.

In Figure 6.2 we show the benchmark of the algorithm in Theorem 3.17 .
For each field size, we made @xmath runs of the algorithms, using random
generating sets of random conjugates of @xmath .

The time complexity stated in the theorem suggests that the graph should
be slightly worse than linear. Figure 6.2 clearly supports this. The
minor oscillations can have at least two reasons. The algorithm is
randomised, and the core of the algorithm is to find an element of order
@xmath by random search. The proportion of such elements is @xmath which
oscillates slightly when @xmath increases.

We do not include graphs of the tensor decomposition algorithms for the
Suzuki groups. The reason is that at the present time, they can only be
executed on a small number of inputs (certainly not more than @xmath and
@xmath ) before running out of memory. Hence there is not much of a
graph to display.

### 6.2. Small Ree groups

In the cases of the small Ree groups, the field size is always @xmath
for some @xmath . Hence we display the time against @xmath .

In Figure 6.3 we show the benchmark of the algorithm in Theorem 3.47 .
For each field size, we made @xmath runs of the algorithms, using random
generating sets of random conjugates of @xmath . As can be seen from the
proof of the Theorem, the algorithm involves many ingredients: discrete
logarithms, @xmath evaluations, @xmath recognition. To avoid making the
graph unreadable, we avoid displaying the timings for these various
steps, and only display the total time. The graph still has jumps, for
reasons similar as with the Suzuki groups.

We do not include graphs of the tensor decomposition algorithms for the
small Ree groups. The reason is that at the present time, they can only
be executed on a small number of inputs (certainly not more than @xmath
and @xmath ) before running out of memory. Hence there is not much of a
graph to display.

### 6.3. Big Ree groups

In the cases of the Big Ree groups, the field size is always @xmath for
some @xmath . Hence we display the time against @xmath .

In Figure 6.4 we show the benchmark of the algorithm in Theorem 3.79 .
This involves all the results presented for the Big Ree groups.

For each field size, we made @xmath runs of the algorithms, using random
generating sets of random conjugates of @xmath . As can be seen from the
proof of the Theorem, the algorithm involves many ingredients: discrete
logarithms, @xmath evaluations, @xmath recognition, @xmath recognition.
To avoid making the graph unreadable, we avoid displaying the timings
for these various steps, and only display the total time.