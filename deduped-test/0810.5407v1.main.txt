# Acknowledgements

I am indebted to many people and institutions who have helped me to
survive and even enjoy the four years it took to produce this thesis.

First of all I wish to offer my sincerest thanks to my supervisors, Dr.
Vladimir Pestov, who was a Reader in Mathematics at Victoria University
of Wellington when I started my PhD studies and is now a Professor of
Mathematics at the University of Ottawa, and Dr. Bill Jordan, Reader in
Biochemistry at Victoria University of Wellington, who have supported me
and guided me in all imaginable ways during the course of the study. Dr.
Mike Boland from the Fonterra Research Centre was principal in getting
my study off the ground by introducing me to the problem of short
peptide fragments.

My scholarship stipend was provided through a Bright Future Enterprise
Scholarship jointly funded by the The Foundation for Research, Science
and Technology and Fonterra Research Centre (formerly The New Zealand
Dairy Research Institute).

I have enjoyed a generous and consistent support from the Faculty of
Science, the School of Mathematical and Computing Sciences and the
School of Biological Sciences at the Victoria University of Wellington.
Not only have they contributed significant funds towards my travels to
conferences and to Canada to visit my supervisor as well as towards a
part of tuition fees, but have provided an excellent environment to work
in. I would particularly like to thank Dr. Peter Donelan, who was the
head of the School of Mathematical and Computing Sciences for most of
the time I was doing my thesis and who signed my progress reports
instead of my principal supervisor. I am grateful to Professor Estate
Khmaladze and Dr. Peter Andreae for being willing to listen to my
numerous questions in their respective areas. I also wish to acknowledge
the system programmers Mark Davis and Duncan McEwan for maintaining our
systems and being always available to answer my questions about C
programming, UNIX, networks etc. I wish to thank the Department of
Mathematics and Statistics of the University of Ottawa, which has
accepted me as a visitor on two occasions for four months in total.

I thank my colleagues Azat Arslanov and Todd Rangiwhetu who at times
shared office with me for encouraging me and proofreading some of my
manuscripts.

I would like to thank Professor Vitali Milman who, while being a visitor
in Wellington, offered a lot of encouragement and some very helpful
advice on how to approach mathematics. A very special thanks goes to Dr.
Markus Hegland for convincing me to learn the Python programming
language and ease my programming burden. Markus was also one of the
supervisors (the other being Vladimir Pestov) for my summer 1999 project
at the Australian National University that is presented as Appendix A .
Professor Paolo Ciaccia and Dr. Marco Patella have generously made the
source code for their M-tree publicly available on the web and have
agreed to send me a copy of the code for mvp-tree.

My mother Ljiljana has supported me throughout my studies and sacrificed
a lot to see me where I am now. No words can ever be sufficient to
express my gratitude.

###### Contents

-    Acknowledgements
-    1 Introduction
    -    1.1 Proteins
        -    1.1.1 Basic concepts
        -    1.1.2 Protein sequence alignment
        -    1.1.3 Short peptide fragments
    -    1.2 Indexing for Similarity Search
    -    1.3 Quasi-metrics
    -    1.4 Overview of the Chapters
-    2 Quasi-metric Spaces
    -    2.1 Basic Definitions
    -    2.2 Topologies and quasi-uniformities
    -    2.3 Quasi-normed Spaces
    -    2.4 Lipschitz Functions
        -    2.4.1 Examples
        -    2.4.2 Quasi-normed spaces of left-Lipschitz functions and
            best approximation
    -    2.5 Hausdorff quasi-metric
    -    2.6 Weighted quasi-metrics and partial metrics
        -    2.6.1 Weighted quasi-metrics
        -    2.6.2 Bundles over metric spaces
        -    2.6.3 Partial metrics
        -    2.6.4 Semilattices, semivaluations and semigroups
    -    2.7 Weighted Directed Graphs
    -    2.8 Universal Quasi-metric Spaces
        -    2.8.1 Universal countable rational quasi-metric space
        -    2.8.2 Universal bicomplete separable quasi-metric space
-    3 Sequences and Similarities
    -    3.1 Free semigroups and monoids
    -    3.2 Generalised Hamming Distance
    -    3.3 String Edit Distances
        -    3.3.1 W-S-B distance
        -    3.3.2 Alignments
        -    3.3.3 Dynamic programming algorithms
    -    3.4 Global Similarity
        -    3.4.1 Correspondence to distances
    -    3.5 Local Similarity
    -    3.6 Score Matrices
        -    3.6.1 DNA score matrices
        -    3.6.2 BLOSUM matrices
    -    3.7 Profiles
        -    3.7.1 Position specific score matrices
        -    3.7.2 Profiles as distributions
-    4 Quasi-metric Spaces with Measure
    -    4.1 Basic Measure Theory
    -    4.2 pq-spaces
    -    4.3 Concentration Functions
    -    4.4 Deviation Inequalities
    -    4.5 LÃ©vy Families
    -    4.6 High dimensional pq-spaces are very close to mm-spaces
    -    4.7 Product Spaces
        -    4.7.1 Hamming cube
        -    4.7.2 General setting
-    5 Indexing Schemes for Similarity Search
    -    5.1 Introduction
    -    5.2 Basic Concepts
        -    5.2.1 Workloads
        -    5.2.2 Similarity queries
        -    5.2.3 Indexing schemes
        -    5.2.4 Inner and outer workloads
    -    5.3 Metric trees
        -    5.3.1 Vector space indexing schemes
        -    5.3.2 General metric space indexing schemes
    -    5.4 Quasi-metric trees
    -    5.5 Valuation Workloads and Indexing Schemes
    -    5.6 New indexing schemes from old
        -    5.6.1 Disjoint sums
        -    5.6.2 Query partitions
        -    5.6.3 Inductive reduction
        -    5.6.4 Projective reduction
    -    5.7 Performance and Geometry
        -    5.7.1 Cost model for indexing schemes
        -    5.7.2 Workloads and pq-spaces
        -    5.7.3 The Curse of Dimensionality
        -    5.7.4 Dimensionality estimation
    -    5.8 Discussion and Open problems
        -    5.8.1 Workload reductions
        -    5.8.2 Certification functions
    -    5.9 Conclusion
-    6 Indexing Protein Fragment Datasets
    -    6.1 Protein Sequence Workloads
        -    6.1.1 Sequence datasets
        -    6.1.2 Unique fragments
        -    6.1.3 Random sequences
        -    6.1.4 Quasi-metric or metric?
        -    6.1.5 Neighbourhood of dataset
        -    6.1.6 Distance Exponent
        -    6.1.7 Self-similarities
    -    6.2 Tries, Suffix Trees and Suffix Arrays
    -    6.3 FSIndex
        -    6.3.1 Data structure and construction
        -    6.3.2 Search
        -    6.3.3 Implementation
        -    6.3.4 Extensions
    -    6.4 Experimental Results
        -    6.4.1 Datasets and indexes
        -    6.4.2 General performance
        -    6.4.3 Dependence on similarity measures
        -    6.4.4 Scalability
        -    6.4.5 Access overhead
        -    6.4.6 Comparisons with other access methods
    -    6.5 Discussion
        -    6.5.1 Power laws and dimensionality
        -    6.5.2 Effect of subindexing of bins
        -    6.5.3 Effect of similarity measures
        -    6.5.4 Scalability
        -    6.5.5 Comparison with other indexing schemes
-    7 Biological Applications
    -    7.1 Introduction
    -    7.2 Methods
        -    7.2.1 General overview
        -    7.2.2 PSSM construction
        -    7.2.3 Statistical significance of search results
        -    7.2.4 Implementation
        -    7.2.5 Experimental parameters
    -    7.3 Results
    -    7.4 Discussion
        -    7.4.1 Hits to close homologs
        -    7.4.2 Low complexity regions and repeats
        -    7.4.3 Issues with algorithm and implementation
    -    7.5 Conclusion
-    8 Conclusions
    -    8.1 Directions for Future Work
-    A Distance Exponent
    -    A.1 Basic Concepts
    -    A.2 Theoretical Examples
        -    A.2.1 The cube @xmath
        -    A.2.2 Multivariate normal distribution
    -    A.3 Estimation From Datasets
        -    A.3.1 Estimation from log-log plots
        -    A.3.2 Estimation by polynomial fitting
    -    A.4 General Observations

###### List of Figures

-    2.1 Left open balls form a base for a quasi-metric topology.
-    2.2 Set difference quasi-metric.
-    2.3 Set of points of best approximation.
-    2.4 Hausdorff distance between two sets.
-    2.5 Illustration of Remark 2.5.10 .
-    4.1 @xmath function.
-    4.2 Left concentration function @xmath .
-    4.3 @xmath can take as much mass as required.
-    4.4 Space where @xmath .
-    4.5 Spaces @xmath where @xmath as @xmath but @xmath does not.
-    5.1 Growth of GenBank DNA sequence database.
-    5.2 An indexing scheme @xmath on a workload @xmath .
-    5.3 An indexing tree for range queries of a linearly ordered
    dataset.
-    5.4 A metric tree indexing scheme.
-    5.5 The shapes of the @xmath , @xmath and @xmath unit balls.
-    5.6 An example of R-tree.
-    5.7 Structure of X-tree.
-    5.8 An example of SS-tree.
-    5.9 An example of a binary vp-tree.
-    5.10 An example of an mvp-tree.
-    5.11 An example of GNAT.
-    6.1 Percentage of unique SwissProt fragments of various lengths.
-    6.2 Ratios between sizes of metric and quasi-metric balls.
-    6.3 Distributions of distances from random fragments to the
    SwissProt fragment datasets.
-    6.4 Growth of metric balls in SwissProt fragment datasets.
-    6.5 Distributions of self-similarities of SwissProt fragment
    datasets.
-    6.6 A trie and a PATRICIA tree.
-    6.7 A suffix tree and a suffix array.
-    6.8 Structure of an FSIndex.
-    6.9 An example of @xmath (FSIndex implicit search tree).
-    6.10 BLOSUM62 quasi-metric.
-    6.11 Distribution of SPEQ09 bin sizes.
-    6.12 Performance of FSIndex for fragments of length 6.
-    6.13 Performance of FSIndex for fragments of length 9.
-    6.14 Performance of FSIndex for fragments of length 12.
-    6.15 Performance of FSIndex for fragments of length 9 (datasets of
    different sizes).
-    6.16 Average access overhead of searches using FSIndex.
-    6.17 Performance of FMTree based on M-tree on a dataset of
    fragments of length 10.
-    A.1 A parabolic through in @xmath
-    A.2 A typical distance distribution function and its approximation.
-    A.3 Approximations of distance exponent from the slope of log-log
    graph for a variety of datasets.
-    A.4 Approximations of distance exponent from the slope of log-log
    graph for multivariate Gaussian distributions (large interval).
-    A.5 Approximations of distance exponent from the slope of log-log
    graph for multivariate Gaussian distributions (small interval).
-    A.6 Approximation of distance exponent by fitting monomials.

###### List of Tables

-    1.1 The standard amino acids.
-    3.1 An example of a dynamic programming table for computation of
    W-S-B distance between two strings.
-    3.2 An example of a dynamic programming table for computation of
    Smith-Waterman local similarity between two strings.
-    3.3 Numbers of triples of amino acids failing the triangle
    inequality in the BLOSUM family of score matrices.
-    6.1 Variables and functions of FSIndex creation and search
    algorithms.
-    6.2 Priority queue operations.
-    6.3 Instances of FSIndex used in experimental evaluations.
-    6.4 Performance of the FSIndex with different similarity measures.
-    6.5 Comparison of performance of FSIndex, suffix array and
    mvpt-tree.
-    7.1 Significant hits to query fragments.

## Chapter 1 Introduction

The main focus of this thesis is on application of concepts of modern
mathematics not previously used in biological context to problems of
biological sequence similarity search as well as to the general theory
of indexability of databases for fast similarity search. The biological
applications are concentrated to investigations of short protein
fragments using a novel tool, called FSIndex, which allows very fast
retrieval of similarity based queries of datasets of short protein
fragments.

Clearly, this work stands at an intersection of several disciplines. The
approach is mostly mathematical and rigorous where possible but also
touches some aspects of the database theory and computational biology.
The main result, presented in Chapter 3 , shows that deep connections
exist between quasi-metrics (asymmetric distance functions), and
similarity measures on biological sequences. This motivates an effort to
generalise the concepts and techniques from asymptotic geometric
analysis and database indexing that apply to metric spaces to their
quasi-metric counterparts, and to apply the resulting structures to
biological questions.

The present chapter introduces the biological background associated with
proteins and their short fragments and outlines the remainder of the
thesis. It is assumed that general concepts related to biological
macromolecules are well known and only those particularly relevant will
be emphasised. Many important concepts will only be mentioned briefly
and their detailed explanation left for the subsequent chapters.

### 1.1 Proteins

#### 1.1.1 Basic concepts

Proteins are organic macromolecules consisting of amino acids joined by
peptide bonds , essential for functioning of a living cell. They are
involved in all major cellular processes, playing a variety of roles,
such as catalytic (enzymes), structural, signalling, transport etc.

Structurally, proteins are linear chains ( polypeptides ) composed of
the twenty standard amino acids which can be classified according to
their chemical properties (Table 1.1 ). A protein in the living cell is
produced through the processes of transcription and translation . Simply
stated, the information encoded by a gene on DNA is transcribed into a
mRNA molecule which is then translated into a protein on ribosomes by
putting an amino acid for every codon triplet of nucleotides on mRNA.
Constituent amino acids of a protein can be post-translationally
modified, for example by attaching a sugar or a phosphate group on their
side chains.

Four distinct aspects of protein structure are generally recognised. The
primary structure of a protein is the sequence of its constituent amino
acids. The secondary structure refers to the local sub-structures such
as @xmath -helix , @xmath -sheet or random coil . The tertiary structure
is the spatial arrangement of a single polypeptide chain while the
quaternary structure refers to the arrangements of multiple polypeptides
( protein subunits ) forming a protein complex . We refer to the
tertiary and quaternary structures as conformations .

Protein function in general is determined by the conformation but it is
strongly believed that secondary, tertiary and quaternary structure are
all determined by the amino acid sequence. So far, there has been no
solution to the folding problem , which is to determine the conformation
solely from the amino acid sequence by computational means. All
presently known structures have been determined either experimentally,
by using crystallographic or NMR (Nuclear Magnetic Resonance)
techniques, or by homology modelling from closely related sequences with
experimentally derived structures.

While the number of possible amino acid sequences is very large, known
proteins take a relatively small amount of conformations [ 142 , 95 ] .
There is an ongoing effort to determine all possible conformations
proteins can take, that is, to produce a map of the conformation space [
95 , 96 , 97 ] . Such a map would enable modelling of all the structures
which have not been experimentally determined using the existing
structures of the similar proteins.

A structural motif is a three-dimensional structural element or fold
consisting of consecutive secondary structures, for example, the @xmath
-barell motif. Structural motifs can but need not be associated with
biological function. A structural domain is a unit of structure having a
specific function which combines several motifs and which can fold
independently. A protein sequence motif is a amino-acid pattern
associated with a biological function. It may, but need not, be
associated with a structural motif.

#### 1.1.2 Protein sequence alignment

Sequence alignment is presently one of the cornerstones of computational
biology and bioinformatics [ 180 ] . As mentioned before, all elements
of protein structure and function ultimately depend on the sequence and
in addition, sequence data is most readily available, mostly originating
from the translations of the sequences of genes and transcripts obtained
through large scale sequencing projects [ 196 , 213 ] such as the
recently completed Human Genome Project [ 43 ] . Raw sequences produced
by the sequencing projects need to be annotated , that is, functional
descriptions attached to each sequence and/or its constituent parts [
179 ] . The most widely used (but not always adequate [ 166 , 69 ] )
technique for annotation is homology or similarity search where the
unannotated sequences are annotated according to their similarity to
previously annotated sequences [ 24 ] resulting in great savings of time
and effort required for experimental analysis of each sequence.

Much of the sequence data is easily accessible from public repositories
[ 62 ] , the best known being the database collection at the National
Center for Biotechnology Information (NCBI â http://www.ncbi.nlm.nih.gov
) in the
United States [ 209 ] . The NCBI repository contains among many others
the GenBank [ 15 ] DNA sequence database, a part of the international
collaboration involving its European ( EMBL ) [ 117 ] and Japanese
(DDBJ) [ 139 ] counterparts and the RefSeq [ 158 ] , the set of
reference gene, transcript and protein sequences for a variety of
organisms. The major source of protein related resources is the ExPASy
site [ 67 ] at the Swiss Institute of Bioinformatics (
http://www.expasy.org ), the home of SwissProt , a human curated
database of annotated protein sequences, and its companion TrEMBL , a
database of machine-annotated translated coding sequences from EMBL [ 23
] . SwissProt and TrEMBL together form the Uniprot [ 10 ] universal
protein resource. Uniprot has sequence composition similar to the NCBI
RefSeq protein dataset.

The principal technique for general pairwise biological sequence
comparison is known as alignment Â¹ Â¹ 1 The term âalignmentâ is used to
denote both the method of sequence comparison and a particular
transformation of one sequence into another. . We distinguish a global
alignment where the whole extent of both sequences is aligned and local
alignment where only substrings (contiguous subsequences) are aligned.
The foundations of the algorithms for sequence alignment have been
developed in the 1970s and early 1980s [ 146 , 171 , 203 , 178 ]
culminating with the famous Smith-Waterman [ 177 ] algorithm for local
sequence alignments.

Pairwise sequence alignment is based on transformations of one sequence
into other which is broken into transformations of substrings one
sequence into substrings of other. Ultimately two types of
transformations are used: substitutions where one residue (amino acid in
proteins) is substituted for another and indels or insertions and
deletions where a residue or a sequence fragment is inserted (in one
sequence) or deleted (in the other). Indels are often called gaps and
alignments without gaps are called ungapped . Each of the basic
transformations is assigned a numerical score or weight and the
transformation with the optimal score is reported as the âbestâ
alignment of the two sequences. All algorithms for computation of
pairwise alignments use the dynamic programming [ 13 ] technique.

Alignment scores can be distances in which case all scores are positive
and identity transformations (no changes) have the score @xmath .
Distances are often required to have additional properties such as to
satisfy the triangle inequality . Alternatively, transformation scores
may be given as similarities which are large and positive for matches
(identity transformations) and some (âcloseâ) mismatches while other
mismatches and gaps have a negative score. The choice of whether to use
similarities or distances is influenced by available computational
algorithms: similarities are preferred in sequence comparisons because
they are more suitable for local alignments while distances are often
used in phylogenetics [ 83 ] . Furthermore, similarity scores are, at
least in some cases, amenable for statistical and information-theoretic
interpretations [ 105 , 5 , 104 ] .

According to the âbasicâ alignment model, the transformation scores only
depend on the residues being substituted in the case of substitutions,
and lengths of the gaps in the case of indels. There is no dependence on
the position of the transformation within the two sequences being
compared nor on the previous or subsequent transformations. In this
model, substitution scores come from score matrices , the best known
being the PAM [ 45 ] and BLOSUM [ 88 ] families of amino acid matrices.
Both PAM and BLOSUM matrices were derived from multiple alignments
(alignments of more than two sequences) of related proteins.

The most widely used tool for sequence similarity search is BLAST (Basic
Local Alignment Search Tool) [ 6 ] developed at the NCBI. BLAST is a
based on heuristic search algorithm which uses dynamic programming on
only a relatively small part of the sequence database searched while
retrieving most of the hits or neighbours . The importance of BLAST
cannot be overestimated â its applications range from day-to-day use by
biologists to find sequences similar to the sequences of their interest
to high throughput automated annotation, sequence clustering and many
others. Finding efficient algorithms which would improve on BLAST in
accuracy and/or speed remains one of the areas of very active
development [ 108 , 70 , 131 , 99 ] .

While BLAST is quite fast and accurate, it cannot always retrieve all
biologically significant homologs due to limitations of the basic
alignment model. Improvements to the basic alignment model involve the
use of Position Specific Score Matrices or PSSMs, also known as profiles
[ 78 ] , which assign different substitution scores at different
positions. PSI-BLAST [ 6 ] uses PSSMs through an iterative technique
where the results of each search are used to compute a PSSM for a
subsequent iteration â the first search is performed using the basic
model. This method is known to retrieve more âdistantâ homologues which
would be missed using the basic model. More sophisticated sequence and
alignment models such as Hidden Markov Models (HMMs) [ 52 , 53 , 106 ,
85 ] can be used with even more accuracy if there is sufficient data for
their training. In most common cases, a substantial body of statistical
theory for interpretation of the results exists [ 52 , 54 ] .

#### 1.1.3 Short peptide fragments

While most of the works relating to protein sequence analysis
concentrate on either full sequences, or fragments of medium length (50
amino acids â e.g. [ 126 ] ), the main biological focus of this thesis
is on short peptide fragments of lengths 6 to 15.

While short peptide fragments can be interesting as being parts of
larger functional domains, they often have important physiological
function on their own. To mention one of many examples, a large variety
of peptides are generated in the gut lumen during normal digestion of
dietary proteins and absorbed through the gut mucosa. Smaller fragments,
that is dipeptides and tripeptides, are the primary source of dietary
nitrogen. Larger peptides, many of which have been shown to have
physiological activity may also be absorbed. These peptides may modulate
neural, endocrine, and immune function [ 221 , 110 ] . Short peptide
motifs may also have a role in disease. For example, it was discovered
that one of the proteins encoded by HIV-1 and Ebola viruses contains a
conserved short peptide motif which, due to its interaction with host
cell proteins involved in protein sorting, plays a significant role in
progress of the disease [ 132 ] .

The biological part of this thesis aims to develop tools for identifying
conserved fragment motifs among possibly otherwise unrelated protein
sequences. Such tools may produce the results that would enable
determination of the origin of fragments with no obvious function. The
investigation is not restricted solely to bioactive peptides but
considers all possible fragments (of given lengths) of full sequences
available from the databases.

The main paradigm can be expressed as follows:

  A sequence fragment that recurs in a non random and unexpected pattern
  indicates a possible structural motif that has a biological function.

The approach taken here mirrors that of full sequence analysis â the
principal technique used is similarity search using substitution
matrices and profiles. However, the sequence comparison model uses a
global ungapped similarity measure comparing the fragments of the same
length. This can be justified by computational advantages â it leads to
sequence comparisons of linear instead of quadratic complexity, and also
by the specific nature of the problem.

One issue which is not so problematical with longer sequences is that of
statistical significance. According to the model of Karlin and Altschul
[ 105 ] used (in a slightly modified form) in BLAST, short alignments
are not statistically significant at the levels routinely used for full
sequence analysis â there are too few possible alignments between two
short fragments . In other words, high scoring alignments of two short
fragments are not unlikely to occur by chance and hence the results of
searches cannot be immediately assumed to have a biological
significance. The current attempt towards overcoming this problem is
based on using the iterative approach to refine the sequence profile and
insistence on strong conservation among the search results.

Reliance on similarity search and the vast scale of existing sequence
databases puts a premium on fast query retrieval that cannot be obtained
using existing tools such as BLAST, which, at significance levels
necessary to retrieve sufficient numbers of hits, essentially reduces to
sequential scan of all fragments. Hence it is necessary to first develop
an index that would speed up the search and to do so it is necessary to
explore the geometry of the space of peptide fragments. This leads to
the other central concepts of the thesis: indexing schemes and
quasi-metrics .

### 1.2 Indexing for Similarity Search

Indexing a dataset means imposing a structure on it which facilitates
query retrieval. Most common uses of databases require indexing for
exact queries, where all records matching a given key are retrieved. On
the other hand, many kinds of databases such as multimedia, spatial and
indeed biological, need to support query retrieval by similarity â then
need to fetch not only the objects that match the query key exactly but
also those that are âcloseâ according to some similarity measure. Hence,
substantial amount of research is directed towards efficient algorithms
and data structures for indexing of datasets for similarity search [ 130
] .

It is not surprising that geometric as well as purely computational
aspects such as I/O costs are heavily represented in the existing works
on indexing for similarity search. Indeed, most publications concentrate
on the algorithms and data structures which can be applied to the
datasets which can be represented as vector or metric (distance) spaces
[ 36 , 93 ] . In many cases, the so-called Curse of Dimensionality [ 61
] is encountered: performance of indexing schemes deteriorates as the
dimension of datasets grow so that at some stage sequential scan
outperforms any indexing scheme [ 20 , 91 ] . This manifestation has
been linked by Pestov [ 154 ] to the phenomenon of concentration of
measure on high-dimensional structures , well known from the asymptotic
geometric analysis [ 138 , 121 ] .

In their influential paper [ 87 ] , Hellerstein, Koutsoupias and
Papadimitriou stressed the need for a general theory of indexability in
order to provide a unified approach to a great variety of schemes used
to index into datasets for similarity search and provided a simple model
of an indexing scheme . The aim of this thesis is to extend their model
so that it corresponds more closely to the existing indexing schemes for
similarity search and to apply the methods from the asymptotic geometric
analysis for performance prediction. Sharing the philosophy espoused in
[ 150 ] , that theoretical developments and massive amounts of
computational work must proceed in parallel, we apply some of the
theoretical concepts to concrete datasets of short peptide fragments. In
that way we both demonstrate important theoretical and practical
techniques and obtain an efficient indexing scheme which can be used to
answer biological questions.

### 1.3 Quasi-metrics

One of the fundamental concepts of modern mathematics is the notion of a
metric space : a set together with a distance function which separates
points (i.e. the distance between two points @xmath if and only if they
are identical), is symmetric and satisfies the triangle inequality . The
theory of metric spaces is very well developed and provides the
foundation of many branches of mathematics such as geometry, analysis
and topology as well as more applied areas. In many practical
applications, it is to a great advantage if the distance function is a
metric and this is often achived by symmetrising or otherwise
manipulating other distance functions.

A quasi-metric is a distance function which satisfies the triangle
inequality but is not symmetric. There are two versions of the
separation axiom: either it remains the same as in the case of metric,
that is, for a distance between two points to be @xmath they must be the
same, or, it is allowed that one distance between two different points
be @xmath but not both. In all cases the distance between two identical
points has to be @xmath . Hence, for any pair of points in a
quasi-metric space there are two distances which need not be the same.
Quasi-metrics were first introduced in 1930s [ 212 ] and are a subject
of intensive research in the context of topology and theoretical
computer science [ 118 ] .

While much of the results from the theory of metric spaces transfer
directly to the quasi-metric case, there are some concepts which are
unique to the quasi-metrics, the most important being the concept of
duality . Every quasi-metric has its conjugate quasi-metric which is
obtained by reversing the order of each pair of points before computing
the distance. Existence of two quasi-metrics, the original one and its
conjugate leads to other dual structures depending on which quasi-metric
is used: balls, neighbourhoods, contractive functions etc. We
distinguish them by calling the structures obtained using the original
quasi-metric the left structures while the structures obtained using the
conjugate quasi-metric are called the right structures. The join or
symmetrisation of the left and right structures produces a corresponding
metric structure.

Another important concept which has no metric counterpart is that of an
associated partial order. Every quasi-metric space can be associated
with a partial order and every partial order can be shown to arise from
a quasi-metric. Hence, quasi-metrics are not only generalised metrics,
but also generalised partial orders. This fact has been important for
the theoretical computer science applications and also has significance
in the context of sequence based biology.

While the topological properties of quasi-metric and related structures
have been extensively investigated [ 118 ] , much less is known about
the geometric aspects. We therefore aim to extend the concepts from the
asymptotic geometric analysis to quasi-metric spaces in order to have
results analogous to those involving metric spaces as well as to
investigate the phenomena specific to the asymmetric case. Such results
can then be applied to the theory of indexing for similarity search and
its applications to sequence based biology.

### 1.4 Overview of the Chapters

Chapter 2 introduces quasi-metric spaces and related concepts. The
emphasis is on the notions used in the subsequent chapters as well as on
examples. In the last section, we construct examples of universal
quasi-metric spaces of some classes. A universal quasi-metric space of a
given class contains a copy of every quasi-metric space of that class
and satisfies in addition the ultrahomogeneity property. This notion is
a generalisation of a well known concept of a universal metric space
first constructed by Urysohn [ 191 ] . While there are no direct
applications of universal quasi-metric spaces in this thesis, our
construction serves two purposes: it provides examples of quasi-metric
spaces not previously known and sets the foundations for possible
further research mirroring the investigations [ 193 , 198 , 156 ]
relating to the universal metric spaces and their groups of isometries.

Chapter 3 explores in detail the connections between biological sequence
similarities and quasi-metrics. The main result is the Theorem 3.5.5
which shows that local similarity measures on biological sequences can
be, under some assumptions frequently fullfilled in the real
applications, naturally converted into equivalent quasi-metrics. While
it was long known that global similarities can be converted to metrics
or quasi-metrics, it was believed [ 178 ] that no such conversion exists
for the local case, at least with respect to metrics.

Chapter 4 introduces the central mathematical object of this study: the
quasi-metric space with measure, or pq-space . This is a generalisation
of a metric space with measure or an mm-space which provides the
framework for study of the phenomenon of concentration of measure on
high dimensional structures. We extend these concepts to pq-spaces and
point out the similarities and differences to the metric case. In
particular we study the interplay between asymmetry and concentration â
the Theorem 4.6.2 indicates that âa high dimensional quasi-metric space
is close to being a metric spaceâ. The results from Chapter 4 as well as
an alternative formulation of the main results from Chapter 3 are
published in a paper to appear in Topology Proceedings [ 181 ] .

Chapter 5 , partially based on the joint preprint with Pestov [ 157 ] ,
is dedicated to applications of the mathematical concepts and results of
previous chapters to indexing for similarity search. We extend, among
others, the concepts of workload and indexing scheme first introduced by
Hellerstein, Koutsoupias and Papadimitriou [ 87 ] in order to make them
more suitable for analysis of similarity search and apply them to
numerous existing published examples. We only consider consistent
indexing schemes â those that are guaranteed to always retrieve all
query results. Most existing indexing schemes for similarity search can
only be applied to metric workloads and while quasi-metrics are
mentioned in the literature (e.g. in [ 39 ] ), no general quasi-metric
indexing scheme exists. We therefore introduced a concept of a
quasi-metric tree and dedicated a separate section to it. Chapter 5 also
contains a proposal for a general framework for analysis of indexing
schemes and an application of the concepts developed in Chapter 4 to the
analysis of performance of range queries.

Chapter 6 , building on a second joint preprint with Pestov [ 182 ] ,
examines some aspects of geometry of workloads over datasets of short
peptide fragments and introduces FSIndex, an indexing scheme for such
workloads. FSIndex is based on partitioning of amino acid alphabet and
combinatorial generation of neighbouring fragments. Experimental results
provide an illustration of many concepts from Chapter 5 and show that
FSIndex strongly outperformes some established indexing schemes while
not using significantly more space. It also has an advantage that a
single instance of FSIndex can be used for searches using multiple
similarity measures.

Chapter 7 introduces the prototype of the PFMFind method for identifying
potential short motifs within protein sequences that uses FSIndex to
query datasets of protein fragments. Preliminary experimental
evaluations, involving six selected protein sequences, show that PFMFind
is capable of finding highly conserved and functionally important
domains but needs improvemement with respect to fragments having unusual
amino acid compositions.

Appendix A presents previously unpublished results on estimation of
dimension of datasets that the thesis author obtained as a summer
student at the Australian National University in summer 1999/2000. It
takes the concept of distance exponent introduced by Traina et al. [ 188
] and provides it with more rigourous foundations. Several computational
techniques for computing distance exponent are proposed and tested on
artificially generated datasets. The best performing method is applied
in Chapter 6 to estimate the dimensions of two datasets of short peptide
fragments.

## Chapter 2 Quasi-metric Spaces

In this chapter we introduce the concept of a quasi-metric space with
related notions. A quasi-metric can be thought of as an ââasymmetric
metricââ; indeed by removing the symmetry axiom from the definition of
metric one obtains a quasi-metric. However, we shall adopt a more
general definition which has the advantage of naturally inducing a
partial order. Thus, a notion of a quasi-metric generalises both
distances and partial orders.

There is substantial amount of publications about topological and
uniform structures related to quasi-metric spaces â the major review by
KÃ¼nzi [ 118 ] contains 589 references. In contrast, there is a relative
scarcity of works on geometric and analytic aspects which is partially
being addressed by the recent papers on quasi-normed and biBanach spaces
[ 63 , 64 , 160 , 65 , 66 ] . While most known applications of
quasi-metrics come from theoretical computer science, the aim for this
thesis is to show that there is a fundamental connection to sequence
based biology.

Duality is a very important phenomenon often associated with asymmetric
structures. The topological aspects of duality are investigated in great
detail in the paper by Kopperman [ 113 ] . In the case of quasi-metrics,
duality is manifested by having two structures, which we call left and
right, associated with notions generalised from metric spaces. The
symmetrisation (or a âjoinâ) of these two structures corresponds to a
metric structure.

The present chapter consists mostly of the review of the literature and
basic concepts illustrated by examples. Our main new contribution is
contained in Section 2.8 , which introduces universal quasi-metric
spaces analogous to the Urysohn universal metric spaces first introduced
by Urysohn [ 191 ] .

### 2.1 Basic Definitions

###### Definition 2.1.1.

Let @xmath be a set. Consider a mapping @xmath and the following axioms
for all @xmath :

1.  @xmath .

2.  @xmath .

3.  @xmath .

4.  @xmath .

The axiom (ii) is known as the triangle inequality , the axiom (iii) is
called the separation axiom and the axiom (iv) is called the symmetry
axiom .

A function @xmath satisfying axioms (i),(ii) and (iii) is called a
Quasi-metric and if it also satisfies (iv) it is a metric . A pair
@xmath , where @xmath is a set and @xmath a (quasi-) metric, is called a
(quasi-) metric space .

For a quasi-metric @xmath , its conjugate (or dual ) quasi-metric @xmath
is defined for all @xmath by

  -- -------- --
     @xmath   
  -- -------- --

and its associated metric @xmath by

  -- -------- --
     @xmath   
  -- -------- --

The associated metric is is the smallest metric majorising @xmath .
@xmath

A quasi-metric @xmath is a metric if and only if it coincides with its
conjugate quasi-metric.

###### Remark 2.1.2.

A function satisfying axioms (i),(ii) above but not necessarily
satisfying the separation axiom (axiom (iii)) is called a
pseudo-quasi-metric and if it also satisfies the axiom (iv) it is called
a pseudo-metric . We use the generic term distance to denote any of the
pseudo-quasi-metrics.

If a distance is allowed to take values in @xmath (the extended
half-reals), it is called an extended distance depending on the other
axioms satisfied (e.g. extended pseudo-quasi-metric).

Another often used symmetrisation of a quasi-metric is the âsumâ metric
@xmath where for each @xmath

  -- -------- --
     @xmath   
  -- -------- --

We now summarise some standard notation.

###### Definition 2.1.3.

Let @xmath be a quasi-metric space, @xmath , @xmath and @xmath . Denote
by

 @xmath 

    @xmath , the diameter of set @xmath ;

 @xmath 

    @xmath , the left open ball of radius @xmath centred at @xmath ;

 @xmath 

    @xmath , the right open ball of radius @xmath centred at @xmath ;

 @xmath 

    @xmath , the associated metric open ball of radius @xmath centred at
    @xmath ;

 @xmath 

    @xmath , the left distance from @xmath to @xmath ;

 @xmath 

    @xmath , the right distance from @xmath to @xmath ;

 @xmath 

    @xmath , the associated metric distance from @xmath to @xmath ;

 @xmath 

    @xmath , the left @xmath -neighbourhood of @xmath ;

 @xmath 

    @xmath , the right @xmath -neighbourhood of @xmath ;

 @xmath 

    @xmath , the associated metric @xmath -neighbourhood of @xmath .

 @xmath 

    @xmath , the distance between @xmath and @xmath .

@xmath

The left balls , distances, and neighbourhoods coincide with the right
versions in the case of metric spaces.

###### Remark 2.1.4.

Our notation in some cases slightly differs from that adopted in the
literature. We use @xmath to denote the associated metric (and later the
norm associated to a quasi-norm) in order to avoid any confusion that
can arise from the more usual symbols @xmath or @xmath . Also note that
we denote the open balls by @xmath while we shall use @xmath to denote a
Borel @xmath -algebra of measurable sets and @xmath to denote the set of
blocks of an indexing scheme. The notation @xmath is our own â âuâ is
the second letter of the word âsumâ and âsâ was already used.

###### Remark 2.1.5.

We shall often (but not always) use @xmath to denote @xmath and @xmath
to denote @xmath .

The following result generalises the triangle inequality to the
distances from points to sets.

###### Lemma 2.1.6.

Let @xmath be a pseudo-quasi-metric space. Then for all @xmath and
@xmath ,

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

By the triangle inequality, for all @xmath , @xmath . Taking infimum
over all @xmath of both sides of the inequality produces the desired
result. â

###### Definition 2.1.7.

Let @xmath and @xmath be two quasi-metric spaces. A map @xmath is called
a ( quasi-metric ) isometry if @xmath is a bijection and for all @xmath
,

  -- -------- --
     @xmath   
  -- -------- --

@xmath

###### Lemma 2.1.8.

Let @xmath be an isometry between quasi-metric spaces @xmath and @xmath
. Then @xmath is also an isometry between metric spaces @xmath and
@xmath . â

### 2.2 Topologies and quasi-uniformities

Each quasi-metric @xmath naturally induces a topology @xmath whose base
consists of all open left balls @xmath , centred at any @xmath , of
radius @xmath . This is a base indeed. Take any @xmath and @xmath such
that @xmath . For any @xmath set @xmath and observe that @xmath .

Thus, a set @xmath is open if for each @xmath there is an @xmath such
that @xmath . The topology @xmath is defined in similar way: its base
consists of all open right balls @xmath of radius @xmath . Hence, one
can naturally associate a bitopological space @xmath to a quasi-metric
space @xmath . The relationships between quasi-metric and bitopological
spaces are well researched [ 118 ] .

###### Definition 2.2.1.

A topological space is quasi-metrisable if there exists a quasi-metric
@xmath such that @xmath . @xmath

###### Remark 2.2.2.

Note that for any quasi-metric space @xmath , @xmath and hence the base
of the metric topology @xmath consists exactly of intersections of left
and right open balls of the same radius, centred at any point.
Therefore, @xmath is the supremum of @xmath and @xmath :

  -- -------- --
     @xmath   
  -- -------- --

Not every topology is induced by a quasi-metric, however Kopperman [ 112
] showed that every topology on a space @xmath is generated by a
continuity function ; that is, an analogue of a quasi-metric which takes
values in a semigroup of a special kind called a value semigroup . The
question of which topologies are quasi-metrisable (i.e. can be induced
from a quasi-metric) has been long open. We mention the
characterisations by Kopperman [ 114 ] in terms of bitopological spaces
and by Vitolo [ 200 ] (see Corollary 2.5.12 ) in terms of hyperspaces of
metric spaces.

The topology @xmath induced by a quasi-metric @xmath clearly satisfies
the @xmath separation axiom. The induced topology is @xmath if and only
if @xmath also satisfies the property @xmath for all @xmath . Often in
the literature, the @xmath quasi-metric is called the
pseudo-quasi-metric while the name quasi-metric is reserved only for the
@xmath case [ 47 , 118 ] . The definition presented here is also widely
used [ 161 , 201 ] and comes mostly from computer science applications
where the association with partial orders justifies consideration of the
@xmath quasi-metrics. Partial orders also arise naturally in the context
of biological sequences which are the main objects of study of this
thesis.

###### Definition 2.2.3.

A partial order on a set @xmath is a binary relation @xmath which is
reflexive, antisymmetric and transitive, that is,

1.  for all @xmath , @xmath .

2.  for all @xmath , @xmath .

3.  for all @xmath , @xmath .

@xmath

###### Definition 2.2.4.

Let @xmath be a quasi-metric space. The associated partial order @xmath
is defined by

  -- -------- --
     @xmath   
  -- -------- --

@xmath

It is easy to see that @xmath is indeed a partial order and hence one
can associate a partial order to every quasi-metric. The converse is
also true.

###### Example 2.2.5 ([119]).

Let @xmath be a partially ordered set and for any @xmath , set @xmath if
@xmath and @xmath otherwise. It is clear that @xmath is a quasi-metric
and that @xmath coincides with @xmath . The topology @xmath induced by
@xmath is called the Alexandroff topology . The metric associated to
@xmath is the discrete, that is @xmath -valued, metric (c.f. the Example
2.2.8 below).

Quasi-metrics also generate the so-called quasi-uniformities which are
uniformities but for the lack of symmetry [ 57 ] . More formally, a
quasi-uniformity @xmath on a set @xmath is a non-empty collection of
subsets of @xmath , called entourages (of the diagonal) , satisfying

1.  Every subset of @xmath containing a set of @xmath belongs to @xmath
    ;

2.  Every finite intersection of sets of @xmath belongs to @xmath ;

3.  Every set in @xmath contains the diagonal (the set @xmath );

4.  If @xmath belongs to @xmath , then exists @xmath in @xmath such
    that, whenever @xmath , then @xmath .

Axioms 1 and 2 mean that @xmath is a filter . Any collection @xmath of
entourages satisfying 3, 4 and which is a prefilter (that is, for each
@xmath there is a @xmath with @xmath ) generates a quasi-uniformity
@xmath which is the smallest filter on @xmath containing @xmath . In
this case, @xmath is called a basis of @xmath .

###### Definition 2.2.6.

A pair of the form @xmath where @xmath is a set and @xmath is
quasi-uniformity on @xmath is called a quasi-uniform space. @xmath

Let @xmath and @xmath be quasi-uniform spaces. A function @xmath is
called quasi-uniformly continuous iff for each @xmath , @xmath . This
exactly mirrors the notion of uniformly continuous function between
uniform spaces.

Let @xmath be a quasi-metric space. Denote by @xmath the entourage of
radius @xmath . The quasi-metric quasi-uniformity @xmath on @xmath has
as a base the set all entourages of radius @xmath , that is, @xmath .
The dual (conjugate) quasi-uniformity @xmath is generated by the
entourages @xmath and the symmetrisation @xmath produces a uniformity.
It is easy to see that for any quasi-metric, the uniformity @xmath is
equivalent to the uniformity generated by the associated metric @xmath .

We now recall parts of the basic theory of completions of quasi-metric
spaces. All statements are particular cases of corresponding statements
for quasi-uniformities.

Recall that a sequence @xmath of points in a metric space @xmath is
Cauchy if for every @xmath there exists @xmath such that for all @xmath
, @xmath . A metric space @xmath is complete if every Cauchy sequence is
convergent in @xmath .

###### Definition 2.2.7.

A quasi-metric space @xmath is called bicomplete if the associated
metric space @xmath is complete. @xmath

The theory of bicomplete quasi-uniformities was developed in [ 44 ] and
[ 124 ] . It is well known that every quasi-metric space @xmath has a
unique (up to a quasi-metric isometry) bicompletion @xmath such that
@xmath is a bicomplete extension of @xmath in which @xmath is @xmath
-dense. The associated metrics @xmath and @xmath coincide so @xmath is
also @xmath -dense in @xmath . Furthermore, if @xmath is a @xmath -dense
subspace of a quasi-metric space @xmath and @xmath is a quasi-uniformly
continuous map where @xmath is a bicomplete quasi-metric space, then
there exists a (unique) quasi-uniformly continuous extension @xmath of
@xmath .

Apart from the above definition there are in existence more restricted
notions of completeness of quasi-metric and quasi-uniform spaces
developed by Doitchinov [ 49 , 51 , 50 ] , which we will not use in this
work.

We now present some well-known examples of quasi-metric spaces.

###### Example 2.2.8.

Let @xmath be any set and set @xmath by:

  -- -------- --
     @xmath   
  -- -------- --

It can be easily checked that @xmath is a metric and such metric is
called the discrete metric. The topology induced by @xmath is discrete:
every singleton is open.

Next we define the quasi-metrics on @xmath generating the so-called
upper and lower topology.

###### Definition 2.2.9.

The left quasi-metric @xmath is given by

  -- -------- --
     @xmath   
  -- -------- --

Similarly, define the right quasi-metric @xmath by

  -- -------- --
     @xmath   
  -- -------- --

@xmath

It is trivial to show that @xmath and @xmath are quasi-metrics which are
conjugate to each other. The associated metric @xmath is the canonical
absolute value metric on @xmath given by @xmath . The base for the left
topology @xmath consists of all sets of the form @xmath and the base for
the right topology @xmath of all sets of the form @xmath , where @xmath
. Hence @xmath and @xmath are @xmath but not @xmath separated. The
partial order associated with @xmath (in this case a linear order) is
the usual order on reals, while @xmath induces the reverse order.

For any topological space @xmath , a continuous function @xmath is often
called lower semicontinuous and a continuous function @xmath is upper
semi-continuous . In accordance with this terminology, @xmath is often
called the topology of lower semicontinuity on reals while @xmath is
called the topology of upper semicontinuity .

###### Remark 2.2.10.

It is worth noting that for any quasi-metric space @xmath , the
quasi-metric @xmath , taken as a function @xmath is lower semicontinuous
with respect to the product topology @xmath and upper semicontinuous
with respect to the product topology @xmath . Indeed, let @xmath and let
@xmath . One can show using the triangle inequality that

  -- -- --
        
  -- -- --

and

  -- -- --
        
  -- -- --

and hence @xmath is open in @xmath and @xmath is open in @xmath .
However, @xmath is not in general lower or upper semicontinuous with
respect to the product topologies @xmath or @xmath . For the counter
example, set @xmath and consider neighbourhoods of @xmath .

###### Example 2.2.11 ([119, 47]).

Another quasi-metric on @xmath is given by

  -- -------- --
     @xmath   
  -- -------- --

In this case @xmath induces a @xmath topology @xmath on @xmath whose
base consists of all left balls centred at @xmath of the form @xmath ,
where @xmath (for any @xmath , and @xmath , @xmath ). The topological
space @xmath is called the Sorgenfrey line , a well known object in
topology and a source of many counter-examples. The associated metric
@xmath is the discrete metric.

Any unbounded quasi-metric can be converted to a bounded quasi-metric
while preserving the topology in the following way.

###### Example 2.2.12.

Let @xmath be an extended quasi-metric space. Then @xmath defined by

  -- -------- --
     @xmath   
  -- -------- --

is a quasi-metric such that @xmath . The proof of quasi-metric axioms is
trivial and the fact that topologies coincide follows from the fact that
all open balls of radius not greater than @xmath coincide.

###### Definition 2.2.13.

Let @xmath be a topological space. Denote by

 @xmath 

    @xmath ,âthe set of all subsets of @xmath ;

 @xmath 

    @xmath ,âthe set of all non-empty subsets of @xmath ;

 @xmath 

    @xmath ,âthe set of all finite subsets of @xmath ;

 @xmath 

    @xmath ,âthe set of all compact subsets of @xmath ;

 @xmath 

    @xmath ,âthe set of all non-empty compact subsets of @xmath ;

 @xmath 

    @xmath ,âthe set of all closed subsets of @xmath ;

 @xmath 

    @xmath ,âthe set of all non-empty closed subsets of @xmath .

If the topology @xmath is generated by a quasi-metric @xmath we will
often replace @xmath in the above expressions by @xmath , for example
obtaining @xmath for the set of all compact subsets of @xmath .

The set @xmath (or restrictions as above) with some (topological)
structure is often called a hyperspace . @xmath

###### Example 2.2.14 ([47]).

Let @xmath be a set and let @xmath . Define @xmath by @xmath .

It is easy to see that @xmath . The triangle inequality can be verified
by noting that @xmath and hence @xmath is a quasi-metric with the
associated order corresponding to the set inclusion. The symmetrisation
@xmath produces the well-known symmetric difference metric.

###### Example 2.2.15.

More generally, let @xmath be a measure space and @xmath , the set of
equivalence classes of measurable subsets of finite measure, that is,
for any @xmath such that @xmath and @xmath , @xmath . Then, by the same
argument as above, the function @xmath where @xmath , is a @xmath
quasi-metric.

###### Example 2.2.16.

Let @xmath , @xmath be quasi-metric spaces and suppose @xmath , that is,
for each @xmath , @xmath , @xmath . Define @xmath by

  -- -------- --
     @xmath   
  -- -------- --

Then it is easy to show that @xmath is a quasi-metric space. We will
call the product spaces of this kind the @xmath -type quasi-metric
spaces . They will feature extensively later on.

###### Example 2.2.17.

Let @xmath be an @xmath -type product space as above. The Hamming metric
is a metric obtained by setting each @xmath above to be the discrete
metric. In other words,

  -- -------- --
     @xmath   
  -- -------- --

### 2.3 Quasi-normed Spaces

Important examples of quasi-metrics are induced by quasi-norms, the
asymmetric versions of norms. The research area of quasi-normed spaces
has seen a significant development in recent years both in theory [ 63 ,
64 , 160 , 65 , 66 ] and applications [ 161 , 164 ] . We survey here
some of the main definitions and examples.

Recall that a semigroup @xmath is a set @xmath with a binary operation
@xmath satisfying

1.  @xmath (closure),

2.  @xmath (associativity).

A monoid or a semigroup with identity is a semigroup @xmath containing a
unique element @xmath (also called a neutral element ) such that @xmath
, @xmath , and a group @xmath is a monoid where each element has an
inverse, that is, @xmath , @xmath : @xmath . A homomorphism from a
semigroup @xmath to a semigroup @xmath is map @xmath such that @xmath ,
@xmath . An isomorphism is a homomorphism which is a bijection such that
its inverse is also a homomorphism.

###### Definition 2.3.1.

A semilinear (or semivector ) space on @xmath is a triple @xmath such
that @xmath is an Abelian semigroup with neutral element @xmath and
@xmath is a function @xmath which satisfies for all @xmath and @xmath :

1.  @xmath ,

2.  @xmath ,

3.  @xmath , and

4.  @xmath .

Whenever an element @xmath admits an inverse it can be shown to be
unique and is denoted @xmath . If we replace in the above definition
@xmath with @xmath and âsemigroupâ with âgroupâ we obtain an ordinary
vector (or linear) space. @xmath

###### Definition 2.3.2 ([164]).

Let @xmath be a linear space over @xmath where @xmath is the neutral
element of @xmath . A quasi-norm on @xmath is a is a function @xmath
such that for all @xmath and @xmath :

1.  @xmath ,

2.  @xmath , and

3.  @xmath .

The pair @xmath is called a quasi-normed space . @xmath

It is easy to verify that the function @xmath defined on @xmath by
@xmath is a norm on @xmath .

The quasi-norm @xmath induces a quasi-metric @xmath in a natural way.

###### Lemma 2.3.3.

Let @xmath be a quasi-normed space. Then @xmath defined for all @xmath
by

  -- -------- --
     @xmath   
  -- -------- --

is a quasi-metric whose conjugate @xmath is given by @xmath .

###### Proof.

Let @xmath . We have @xmath . Also if @xmath it follows by the first
axiom that @xmath and hence @xmath , that is @xmath .

For the triangle inequality we have

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

The statement about the conjugate is obvious. â

###### Definition 2.3.4 ([164]).

A quasi-normed space @xmath where the induced quasi-metric @xmath is
bicomplete is called a biBanach space. @xmath

###### Example 2.3.5.

A quasi-norm on @xmath is given for all @xmath by @xmath . It is easy to
show that @xmath (Definition 2.2.9 ) is induced by the above quasi-norm.

###### Example 2.3.6 ([164]).

Let @xmath be a quasi-normed space. Define

  -- -- --
        
  -- -- --

The set @xmath can be made into a linear space using standard addition
and scalar multiplication of functions. Set the quasi norm for each
@xmath by

  -- -------- --
     @xmath   
  -- -------- --

Then, the space @xmath is a quasi-normed space and is a biBanach space
if @xmath is a biBanach space.

We conclude this section by considering quasi-normed semilinear spaces
and the dual complexity space.

###### Definition 2.3.7 ([164]).

A quasi-normed semilinear space is a pair @xmath such that @xmath is a
non-empty subset of a quasi-normed space @xmath with the properties that
@xmath is semilinear space on @xmath and @xmath is a restriction of the
quasi-norm @xmath to @xmath .

The space @xmath is called a biBanach semilinear space if @xmath is a
biBanach space and @xmath is closed in the Banach space @xmath . @xmath

The complexity space and its dual have been introduced and extensively
studied in the papers by Schellekens [ 169 ] and Romaguera and
Schellekens [ 162 , 164 ] respectively, in order to study the complexity
of programs. The example below presents the dual complexity space as an
example of a quasi-normed semilinear space.

###### Example 2.3.8 ([164]).

Let @xmath be a quasi-normed semilinear space where @xmath is a
non-empty subset of a quasi-normed space @xmath . Let

  -- -------- --
     @xmath   
  -- -------- --

It is apparent that @xmath is a semilinear space and that @xmath
(Example 2.3.6 ). Define for each @xmath

  -- -- --
        
  -- -- --

so that @xmath becomes a quasi-normed semilinear space. It associated
quasi-metric space @xmath is called the dual complexity space .

Section 2.4 will present a further example of a quasi-normed semilinear
space.

### 2.4 Lipschitz Functions

While the quasi-metric spaces have been extensively studied from a
topological point of view, the properties of the non-contracting maps
between them, also called 1-Lipschitz functions, have not received the
same attention. The only widely available reference solely on this topic
is the paper by Romaguera and Sanchis [ 161 ] . In this section we will
define left- and right- Lipschitz maps, present a few basic results and
examples, as well as survey some of the results by Romaguera and
Sanchis. Lipschitz maps will be extensively used in subsequent chapters
and new structures will be introduced where needed.

###### Definition 2.4.1.

Let @xmath and @xmath be quasi-metric spaces. A map @xmath is called
left @xmath -Lipschitz if there exists @xmath such that for all @xmath

  -- -------- --
     @xmath   
  -- -------- --

The constant @xmath is called a left Lipschitz constant . Similarly,
@xmath is right @xmath -Lipschitz if @xmath .

Maps that are both left and right @xmath -Lipschitz are called @xmath
-Lipschitz. @xmath

Left-Lipschitz functions are commonly called semi-Lipschitz [ 161 ] but
we use the above nomenclature in order to be consistent with the other
âone-sidedâ (left- or right-) structures we introduced. Indeed, it is
easy to note that every left @xmath -Lipschitz map @xmath is right
@xmath -Lipschitz as a mapping @xmath .

###### Lemma 2.4.2.

Let @xmath and @xmath be quasi-metric spaces and let @xmath be a left
1-Lipschitz map. Then @xmath is continuous with respect to the left
topologies on both spaces.

###### Proof.

Take any @xmath . We need to show that there is @xmath such that for any
@xmath and @xmath , @xmath . Pick @xmath . It follows that for any
@xmath ,

  -- -------- -------- -- ---
     @xmath   @xmath      
              @xmath      
              @xmath      â
  -- -------- -------- -- ---

#### 2.4.1 Examples

From now on we will concentrate on the maps from a quasi-metric space
@xmath to @xmath . Recall that the quasi-metric @xmath is given by
@xmath . The following is an obvious fact.

###### Lemma 2.4.3.

Let @xmath be a quasi-metric space and @xmath a left @xmath -Lipschitz
function. Then, @xmath where @xmath is a right @xmath -Lipschitz
function. â

Unless stated otherwise, we will consider @xmath as the canonical
quasi-metric on @xmath . The main examples of Lipschitz functions are,
as in the metric case, distance functions from points or sets, as well
as sums of such functions. For each example both a left- and a right-
1-Lipschitz function will be produced but the proofs will be presented
only for the left case since the right case would be follow by duality.

###### Lemma 2.4.4.

Let @xmath be a quasi-metric space and @xmath . Then the function @xmath
, where

  -- -------- --
     @xmath   
  -- -------- --

is left 1-Lipschitz and the function @xmath , where

  -- -------- --
     @xmath   
  -- -------- --

is right 1-Lipschitz.

###### Proof.

Let @xmath . Then @xmath by the triangle inequality. Similarly, @xmath .
â

###### Lemma 2.4.5.

Let @xmath be a quasi-metric space and @xmath . Then @xmath , where

  -- -------- --
     @xmath   
  -- -------- --

is left 1-Lipschitz and @xmath , where

  -- -------- --
     @xmath   
  -- -------- --

is right 1-Lipschitz.

###### Proof.

Let @xmath . Then

  -- -------- -------- -- -- ---
     @xmath   @xmath         
              @xmath         
              @xmath         
              @xmath         â
  -- -------- -------- -- -- ---

###### Lemma 2.4.6.

Let @xmath be a quasi-metric space, @xmath a finite collection of left
(right) 1-Lipschitz functions @xmath and @xmath a collection of
coefficients such that @xmath for all @xmath and @xmath . Then,

  -- -------- --
     @xmath   
  -- -------- --

is left (right) 1-Lipschitz.

###### Proof.

We prove the left case only.

  -- -------- -------- -- ---
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      â
  -- -------- -------- -- ---

In particular, for any collection @xmath of left 1-Lipschitz functions,
the normalised sum @xmath is also left 1-Lipschitz.

#### 2.4.2 Quasi-normed spaces of left-Lipschitz functions and best
approximation

Another example of a semilinear quasi-normed space was produced by
Romaguera and Sanchis [ 161 ] who constructed a quasi-normed semilinear
space of left Lipschitz functions.

Denote by @xmath the set of all left Lipschitz functions on a
quasi-metric space @xmath that vanish at some fixed point @xmath . We
can define for all @xmath and @xmath the sum @xmath and scalar multiple
@xmath in the usual way, producing a semilinear space @xmath on @xmath .

Also, the function @xmath defined by

  -- -------- --
     @xmath   
  -- -------- --

is a quasi-norm on @xmath and hence @xmath forms a quasi-normed
semilinear space.

###### Theorem 2.4.7 ([161]).

The function @xmath where

  -- -------- --
     @xmath   
  -- -------- --

is a bicomplete extended quasi-metric on @xmath . â

Recall that a set @xmath in a linear space @xmath is convex if and only
if for any collection @xmath and @xmath such that @xmath , we have
@xmath . This definition can be extended to semilinear spaces and hence,
by the Lemma 2.4.6 , the set of 1-Lipschitz functions vanishing at a
fixed point is a convex subset of @xmath .

##### Best approximation

From now on to the end of this section let @xmath be, as before, a
quasi-metric space and denote by @xmath the closure @xmath of the subset
@xmath in the topology @xmath . Let @xmath , @xmath and denote by @xmath
the set of points of best approximation to @xmath by elements of Y ,
that is:

  -- -------- --
     @xmath   
  -- -------- --

###### Theorem 2.4.8 ([161]).

Let @xmath and let @xmath . Then @xmath if and only if there exists
@xmath such that

1.  @xmath ,

2.  @xmath , and

3.  @xmath for all @xmath . â

Furthermore, define @xmath , and for each @xmath such that @xmath set

  -- -------- --
     @xmath   
  -- -------- --

###### Theorem 2.4.9 ([161]).

Let @xmath and let @xmath . Then @xmath if and only if @xmath for all
@xmath . â

### 2.5 Hausdorff quasi-metric

Asymmetric variants of the Hausdorff metric provide further examples of
quasi-metrics.

###### Definition 2.5.1.

Let @xmath be a metric space. A map @xmath defined by

  -- -------- --
     @xmath   
  -- -------- --

is called the Hausdorff metric . @xmath

###### Remark 2.5.2.

An equivalent, more geometric way would be to define

  -- -------- --
     @xmath   
  -- -------- --

In other words, @xmath is the infimal @xmath such that for every @xmath
, @xmath is contained in the @xmath -neighbourhood of @xmath and @xmath
is contained in the @xmath -neighbourhood of @xmath (Fig. 2.4 ).

At this stage we omit the proof that Hausdorff metric is indeed a metric
on @xmath since it follows from the properties of the Hausdorff
quasi-metric defined below.

###### Definition 2.5.3.

Let @xmath be a pseudo-quasi-metric space. Denote by @xmath , @xmath ,
and @xmath , the maps @xmath where for all @xmath ,

  -- -------- -------- -- --
     @xmath   @xmath      
     @xmath   @xmath      
     @xmath   @xmath      
  -- -------- -------- -- --

@xmath

###### Lemma 2.5.4.

Let @xmath be a pseudo-quasi-metric space. Then @xmath , @xmath , and
@xmath are extended pseudo-quasi-metrics.

###### Proof.

It is obvious that for any @xmath , @xmath as @xmath is a
pseudo-quasi-metric. To prove the triangle inequality let @xmath . Take
any @xmath . By the Lemma 2.1.6 , we have

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Hence, @xmath and by taking supremum over @xmath on both sides we get
@xmath as required.

The statement for @xmath follows by the same argument once we note that
@xmath . It is obvious that if both @xmath and @xmath satisfy the
triangle inequality then @xmath does as well. â

###### Lemma 2.5.5.

Let @xmath be a quasi-metric space with @xmath , the associated metric.
Then for any @xmath

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

###### Proof.

The result follows straight from the definition.

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Similarly, @xmath . â

###### Lemma 2.5.6.

Let @xmath be a quasi-metric space. Then @xmath restricted to @xmath is
an extended quasi-metric and restricted to @xmath is a quasi-metric.

###### Proof.

To show @xmath is an extended quasi-metric, only the separation axiom
needs to be proven as the rest follows by the Lemma 2.5.4 .

Suppose @xmath and @xmath . Let @xmath . By the Lemma 2.5.5 , we have
@xmath . Now, if @xmath , then for all @xmath there exists a @xmath such
that @xmath as @xmath is closed, implying @xmath since @xmath is a
metric. Hence, @xmath . Similarly, @xmath as @xmath . Therefore, @xmath
implies @xmath .

If @xmath , for any @xmath , the function @xmath is left 1-Lipschitz
(Lemma 2.4.5 ), hence continuous (Lemma 2.4.2 ) and bounded since @xmath
is compact. Hence @xmath and thus @xmath is a quasi-metric. â

We are therefore justified to state the following

###### Definition 2.5.7.

Let @xmath be a quasi-metric space. The map @xmath restricted to @xmath
is called a Hausdorff extended quasi-metric and restricted to @xmath is
called a Hausdorff quasi-metric . @xmath

###### Corollary 2.5.8.

Let @xmath be a quasi-metric space. The Hausdorff metric over @xmath
restricted to @xmath is the metric associated to the Hausdorff
quasi-metric over @xmath .

###### Proof.

Follows from the Lemmas 2.5.5 and 2.5.6 . â

A stronger statement for @xmath and @xmath is possible if the underlying
space is @xmath -separated.

###### Lemma 2.5.9.

Let @xmath be a @xmath quasi-metric space. Then @xmath and @xmath ,
restricted to @xmath , are extended quasi-metrics whose associated
orders correspond to set inclusion. They are quasi-metrics if they are
restricted to @xmath .

###### Proof.

As in Lemma 2.5.6 , we only need to prove separation â the rest follows
by the Lemma 2.5.4 . Take any @xmath and suppose @xmath . Then, for all
@xmath and for all @xmath , there is a @xmath such that @xmath . Since
@xmath is closed, there exists a @xmath such that @xmath and therefore
@xmath as @xmath satisfies the @xmath separation axiom. Thus @xmath and
it immediately follows that the associated order is set inclusion and
that @xmath .

If @xmath , for any @xmath , the function @xmath is left 1-Lipschitz
(Lemma 2.4.5 ), hence continuous (Lemma 2.4.2 ) and bounded since @xmath
is compact. Hence @xmath .

The statements for @xmath follow by duality. â

###### Remark 2.5.10.

The assumption that @xmath satisfies the @xmath separation axiom is
indeed necessary for separation. Consider the following example of a
general quasi-metric space where the @xmath no longer implies @xmath .

Let @xmath and define a quasi-metric @xmath by @xmath and @xmath . Let
@xmath and @xmath . It can be easily verified (Figure 2.5 ) that @xmath
is indeed a quasi-metric on @xmath and that @xmath but @xmath .

The construction above was observed by Berthiaume [ 18 ] in a more
general context of quasi-uniformities over hyperspaces of quasi-uniform
spaces. There exist alternative definitions of Hausdorff quasi-metric.
Vitolo [ 200 ] defines an (extended) Hausdorff quasi-metric @xmath over
the collection of all nonempty closed subsets of a metric space @xmath
by

  -- -------- --
     @xmath   
  -- -------- --

that is, in our notation, his quasi-metric corresponds to @xmath . We
now briefly survey his application of this quasi-metric to
quasi-metrisability of topological spaces.

###### Theorem 2.5.11 (Vitolo [200]).

Every (extended) quasi-metric space embeds into the quasi-metric space
of the form @xmath , where @xmath is a metric space. â

Let @xmath be a quasi-metric space. The proof involves construction of
the space @xmath with the metric @xmath where

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath , @xmath . The mapping @xmath where

  -- -------- --
     @xmath   
  -- -------- --

produces the required embedding.

###### Corollary 2.5.12 (Vitolo [200]).

A topological space is quasi-metrisable if and only if it admits a
topological embedding into a hyperspace. â

### 2.6 Weighted quasi-metrics and partial metrics

Our main example of a quasi-metric comes from biological sequence
analysis. It turns out that the similarity scores between biological
sequences can often be mapped to a more restricted class of
quasi-metrics, the weighted quasi-metrics [ 119 , 201 ] , or
equivalently, the partial metrics [ 133 ] . Chapter 3 presents the full
development of the biological application while the present section
surveys the mathematical theory that was originally developed in the
context of theoretical computer science.

#### 2.6.1 Weighted quasi-metrics

###### Definition 2.6.1 ([119, 201]).

Let @xmath be a quasi-metric space. The quasi-metric @xmath is called a
weightable quasi-metric if there exists a function @xmath , called the
weight function or simply the weight , satisfying for every @xmath

  -- -------- --
     @xmath   
  -- -------- --

In this case we call @xmath weightable by @xmath .

A quasi-metric @xmath is co-weightable if its conjugate quasi-metric
@xmath is weightable. The weight function @xmath by which @xmath is
weightable is called the co-weight of @xmath and @xmath is co-weightable
by @xmath .

A triple @xmath where @xmath is a quasi-metric space and @xmath a
function @xmath is called a weighted quasi-metric space if @xmath is
weightable by @xmath and a co-weighted quasi-metric space if @xmath is
co-weightable by @xmath .

In all the above, if the weight function @xmath takes values in @xmath
instead of @xmath , the prefix generalised is added to the definitions.
@xmath

Not every quasi-metric space is weightable [ 133 ] but each metric space
is obviously weightable, admitting constant weight functions. If @xmath
is a weighted quasi-metric space then so is @xmath where @xmath .

###### Definition 2.6.2 ([170]).

Let @xmath be a set. A function @xmath is fading if @xmath . A weighted
quasi metric space @xmath is of fading weight if its weight function is
fading. @xmath

###### Lemma 2.6.3 ([119], [170]).

The weight functions of a weightable quasi-metric space are strictly
decreasing (with respect to the associated partial order). These are
exactly the functions of the form @xmath , where @xmath and where @xmath
is the unique fading weight of the space.

###### Example 2.6.4.

The set-difference quasi-metric on finite sets (Example 2.2.14 ) is
co-weightable with a co-weight assigning to each set @xmath its
cardinality @xmath .

###### Example 2.6.5 ([119]).

Let @xmath and set @xmath , the restriction of @xmath to positive reals
(i.e. for any @xmath @xmath if @xmath and @xmath if @xmath ). Set @xmath
for all @xmath . It is easy to verify that @xmath is a weighted
quasi-metric space and that @xmath is its unique fading weight function.

Example 2.6.5 shows that a weightable quasi-metric space need not be
co-weightable â in that case its weight is unbounded. Further examples
are provided in [ 119 ] . It is easy to see that a generalised
weightable quasi-metric space is exactly a space which is weightable or
co-weightable. The following result can be used to distinguish between
weighted and co-weighted quasi-metric spaces.

###### Lemma 2.6.6 ([119], [201]).

Let @xmath be a generalised weighted quasi-metric space.

-    If @xmath for all @xmath , @xmath is a weighted quasi-metric space;

-    If @xmath for all @xmath , @xmath is a weighted quasi-metric space;

-    If @xmath is a generalised weighted quasi-metric space then @xmath
    is constant on @xmath . â

###### Lemma 2.6.7.

Let @xmath be a weighted quasi-metric space. Then @xmath is a
right-1-Lipschitz function.

###### Proof.

Let @xmath . Then @xmath . â

Hence it follows that a weight function @xmath for a weightable
quasi-metric space @xmath is continuous function @xmath with regard to
the quasi-metric @xmath (i.e. it is upper semicontinuous).

Partial topological characterisation of weighted quasi-metric spaces was
obtained by KÃ¼nzi and Vajner [ 119 ] . For example, they show that
Sorgenfrey line is not weightable. The full results of their
investigation are out of scope of this thesis and we only present a
theorem about weightability of Alexandroff topologies.

###### Theorem 2.6.8 ([119]).

Let @xmath be a partial order on a set @xmath and @xmath be the full
Alexandroff topology on @xmath .

Then @xmath admits a weightable quasi-metric if and only if there is a
function @xmath such that for each @xmath there exists @xmath such that
for any @xmath with @xmath , @xmath and @xmath we have @xmath . â

#### 2.6.2 Bundles over metric spaces

Vitolo [ 201 ] characterised weighted quasi-metric spaces as bundles
over a metric space.

###### Definition 2.6.9.

Let @xmath be a metric space. A bundle over @xmath [ 201 ] is the
weighted quasi-metric space @xmath where

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

@xmath

###### Theorem 2.6.10 ([201]).

Every weighted quasi-metric space embeds into the bundle over a metric
space. â

In fact, every weighted quasi-metric space can be constructed from a
metric space and a non-distance-increasing (1-Lipschitz) positive
real-valued function on it. If a generalised weighted quasi-metric space
is desired, such function can take values over the whole real line.

###### Theorem 2.6.11 ([201]).

Given a metric space @xmath and a 1-Lipschitz function @xmath , let
@xmath be the graph of @xmath . If @xmath is defined by

  -- -------- --
     @xmath   
  -- -------- --

then @xmath is a weighted quasi-metric space. Moreover, every weighted
quasi-metric space can be constructed in this way.

The quasi-metric space @xmath is @xmath -separated if and only if the
function @xmath above also satisfies

  -- -------- --
     @xmath   
  -- -------- --

###### Theorem 2.6.12 ([201]).

A quasi-metric space @xmath admits a generalised weight if and only if

  -- -------- --
     @xmath   
  -- -------- --

Furthermore, @xmath is weightable if and only if it admits a generalised
weight and for some (equivalently for each) @xmath , the set

  -- -------- --
     @xmath   
  -- -------- --

is bounded below.

The generalised weight function above is given by @xmath , @xmath . The
statement can be dualised to the co-weightable case and used to
distinguish weightable and co-weightable quasi-metric spaces.

#### 2.6.3 Partial metrics

Matthews [ 133 ] proposed the concept of a partial metric, a
generalisation of metrics which allows distances of points from
themselves to be non-zero. He then showed that partial metrics
correspond to weighted quasi-metrics. Partial metrics were further
developed with a view to the applications in theoretical computer
science [ 147 , 30 , 31 , 163 , 170 ] . The greatest relevance of
partial metrics in the context of this thesis is that similarity scores
between biological sequences very often correspond exactly to partial
metrics.

###### Definition 2.6.13 (Matthews [133]).

Let @xmath be a set. A map @xmath is called a partial metric if for any
@xmath :

1.  @xmath ;

2.  @xmath ;

3.  @xmath ;

4.  @xmath .

For a partial metric @xmath its associated partial order @xmath is
defined so that for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

@xmath

A partial metric @xmath induces a topology @xmath whose base are the
open balls of radius @xmath of the form @xmath ( [ 147 ] ).

###### Example 2.6.14 ([133]).

Let @xmath be any set and @xmath , the set of all infinite sequences of
elements of @xmath . The Baire metric is a distance @xmath on @xmath
defined for all @xmath by:

  -- -------- --
     @xmath   
  -- -------- --

Denote by @xmath the set of all finite and infinite sequences over
@xmath and for each finite sequence @xmath denote by @xmath its length
(we agree that for all @xmath , @xmath ). The map @xmath , where for all
@xmath

  -- -------- --
     @xmath   
  -- -------- --

is called the Baire partial metric . It follows that @xmath .

###### Theorem 2.6.15 ([133]).

Let @xmath be a set.

1.   For any partial metric @xmath on @xmath , the map @xmath where for
    all @xmath

      -- -------- --
         @xmath   
      -- -------- --

    is a generalised weighted quasi-metric with weight function @xmath
    such that @xmath and @xmath .

2.   For any (generalised) weighted quasi-metric @xmath over @xmath with
    weight function @xmath , the map @xmath where for all @xmath

      -- -------- --
         @xmath   
      -- -------- --

    is a partial metric such that @xmath and @xmath . â

#### 2.6.4 Semilattices, semivaluations and semigroups

In this subsection we review the results of Schellekens [ 170 ] and
Romaguera and Schellekens [ 165 ] about the weightable quasi-metrics on
semilattices and semigroups. These are, in the context of lattices, also
mentioned in [ 147 , 30 , 31 ] . Again, the motivation comes from
biological sequences, which are also instances of semigroups.

###### Definition 2.6.16.

Let @xmath be a partial order. Then @xmath is called a join semilattice
if for every @xmath there exists a supremum, denoted @xmath and a meet
semilattice if for every @xmath there exists an infimum, denoted @xmath
. A lattice is a partial order which is both a join and a meet
semilattice. @xmath

###### Definition 2.6.17.

If @xmath is a join semilattice then a function @xmath is a join
valuation iff for all @xmath

  -- -------- --
     @xmath   
  -- -------- --

and @xmath is a join co-valuation iff for all @xmath

  -- -------- --
     @xmath   
  -- -------- --

If @xmath is a meet semilattice then a function @xmath is a meet
valuation iff for all @xmath

  -- -------- --
     @xmath   
  -- -------- --

and @xmath is a meet co-valuation iff for all @xmath

  -- -------- --
     @xmath   
  -- -------- --

A function is a semivaluation if it is either a join valuation or a meet
valuation. A semivaluation space is a semilattice equipped with a
semivaluation. @xmath

###### Definition 2.6.18.

A quasi-metric space @xmath is called a join (meet) semilattice
quasi-metric space if its associated partial order is a join (meet)
semilattice. @xmath

Equivalently, a quasi-metric space @xmath is a join semilattice if for
all @xmath there exists a @xmath such that @xmath and @xmath and a meet
semilattice if for all @xmath there exists a @xmath such that @xmath and
@xmath .

###### Definition 2.6.19.

A join semilattice quasi-metric space @xmath is called invariant if for
all @xmath @xmath . Similarly, a meet semilattice quasi-metric space
@xmath is invariant if for all @xmath @xmath . @xmath

We are now able to state the main theorem of [ 170 ] , associating
invariant weighted quasi-metrics and monotone semivaluations on meet
semilattices. There is also a dual of this theorem for join semilattices
that is not presented here.

###### Theorem 2.6.20 ([170]).

For every meet semilattice @xmath there exists a bijection between
invariant co-weightable quasi-metrics @xmath on @xmath with @xmath and
fading strictly increasing meet valuations @xmath . The map @xmath is
defined by @xmath . The inverse is the function which to each weightable
space @xmath assigns its unique fading co-weight.

Similarly, one can show that for every meet semilattice @xmath there
exists a bijection between invariant weightable quasi-metrics @xmath on
@xmath with @xmath and fading strictly decreasing meet valuations @xmath
. The map @xmath is defined by @xmath . The inverse is the function
which to each weightable space @xmath assigns its unique fading weight.
â

The connection of the above result to the quasi-metric semigroups was
explored in [ 165 ] .

###### Definition 2.6.21.

A quasi-metric semigroup is a triple @xmath such that @xmath is a
quasi-metric space and @xmath is a semigroup such that @xmath is @xmath
-invariant , that is, for all @xmath

  -- -------- --
     @xmath   
  -- -------- --

@xmath

###### Definition 2.6.22.

We call the triple @xmath an ordered semigroup if @xmath is a partial
order and @xmath a semigroup and for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Furthermore, if @xmath is a meet semilattice, @xmath is called an
ordered meet semigroup or just meet semigroup. @xmath

It is obvious that a quasi-metric semigroup @xmath corresponds to an
ordered semigroup @xmath . Romaguera and Schellekens obtained the
following extension of the Theorem 2.6.20 .

###### Theorem 2.6.23 ([165]).

Let @xmath be a meet semigroup, @xmath an invariant weighted
quasi-metric with @xmath and @xmath the corresponding strictly
decreasing meet valuation @xmath as per Theorem 2.6.20 . Then @xmath is
a meet semigroup if and only if for all @xmath

  -- -------- --
     @xmath   
  -- -------- --

We now survey some of the examples from [ 165 ] and [ 170 ] . More
examples will be provided by the biological sequences.

###### Example 2.6.24.

Recall the Baire partial metric from Example 2.6.14 on the set @xmath ,
of all finite and infinite sequences of elements of an alphabet @xmath .
We also include @xmath , the empty sequence in @xmath . The
corresponding weighted quasi-metric given by @xmath is an invariant meet
semilattice quasi-metric. The corresponding partial order corresponds to
prefix ordering: @xmath if and only if @xmath is a prefix of @xmath .

###### Example 2.6.25 ([148, 165]).

Denote by @xmath the set of all closed intervals of @xmath and equip it
with a partial metric @xmath defined by

  -- -------- --
     @xmath   
  -- -------- --

The associated weighted quasi-metric space is a join semilattice with
the partial order being the reverse inclusion.

###### Example 2.6.26.

Consider the dual complexity space @xmath (Example 2.3.8 ) over the
quasi-normed semilinear space @xmath where @xmath (this is a restriction
of the quasi-norm on @xmath from Example 2.3.5 ), that is

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

Then @xmath is a weighted quasi-metric with the weight being the
quasi-norm on @xmath (i.e. @xmath ), inducing an invariant meet
semilattice. As it is also a semigroup with respect to the addition, it
is an example of a weightable invariant meet semigroup.

### 2.7 Weighted Directed Graphs

A further important class of examples of quasi-metrics is provided by
directed graphs.

###### Definition 2.7.1.

A directed graph , or digraph is a pair @xmath , where @xmath is a set
of vertices or nodes and @xmath a set of edges .

A weighted directed graph or weighted digraph is a triple @xmath where
@xmath is a directed graph and @xmath is a function associating a weight
assigned to each edge. @xmath

###### Definition 2.7.2.

Let @xmath be a directed graph and let @xmath . A (directed) path
connecting @xmath and @xmath is a finite sequence of vertices @xmath ,
such that @xmath , @xmath and for all @xmath , @xmath .

For each @xmath , denote by @xmath the set of all paths connecting
@xmath and @xmath and by @xmath the length of a path @xmath .

A (directed) cycle is a path connecting a point with itself.

A directed graph @xmath is connected if for every pair of vertices
@xmath and @xmath there exists a path connecting them. @xmath

###### Remark 2.7.3.

A one element sequence @xmath is also a path. Indeed, in that case the
condition that for all @xmath , @xmath , is trivially true. The length
of such path is obviously @xmath .

A connected weighted directed graph with positive weights on all edges
can be turned into a quasi-metric space by using the weight of the
shortest path between two vertices as a distance.

###### Definition 2.7.4.

Let @xmath be a connected weighted directed graph and let @xmath be a
path in @xmath . Define the weight of @xmath , denoted @xmath by

  -- -------- --
     @xmath   
  -- -------- --

If in addition the weight @xmath , of any edge @xmath , is non-negative,
we call the map @xmath , defined by

  -- -------- --
     @xmath   
  -- -------- --

the path distance on @xmath . @xmath

###### Lemma 2.7.5.

Let @xmath be a connected weighted directed graph with non-negative
weights such that for all @xmath and for all paths @xmath and @xmath
such that @xmath and @xmath ,

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

Then the path distance @xmath is a quasi-metric on @xmath .

###### Proof.

Let @xmath . The path @xmath has length @xmath (c.f. the Remark 2.7.3 )
and the set @xmath is empty. Since a sum over an empty set must be
@xmath , and @xmath is a non-negative function, we have @xmath . The
separation axiom follows directly from ( 2.1 ). For the triangle
inequality, it is sufficient to observe that for any three points @xmath
and any paths @xmath and @xmath , there exists a path @xmath , where
@xmath such that @xmath . â

###### Remark 2.7.6.

The condition ( 2.1 ) is equivalent to the property that no cycle of
positive length can have a zero weight.

We call the above metric on graphs a path quasi-metric . The above
construction is natural and well known (there is a full book devoted to
distances in graphs [ 28 ] ), especially in the form of path metric
which is the metric associated to the path quasi-metric of the above
Lemma. It naturally leads to consideration of geometric properties of
digraphs, as in [ 35 ] . The converse is also true: every quasi-metric
space can be turned into a weighted directed graph such that the
quasi-metric corresponds to a path metric.

###### Lemma 2.7.7.

Let @xmath be a quasi-metric space. Then there exists a weighted
directed graph @xmath with non-negative weights such that @xmath .

###### Proof.

Set @xmath and @xmath the set of all pairs @xmath where @xmath . For any
pair @xmath , set @xmath so that @xmath is a weighted directed graph. It
is now straightforward to observe that @xmath . â

We now review other published work connecting quasi-metrics and graphs.

Jawhari, Misane and Pouzet [ 101 ] consider graphs and ordered sets as a
kind of quasi-metric space where the values of the distance function
belong to an ordered semigroup equipped with an involution. In this
framework, the graph- or order- preserving maps are exactly the
âLipschitzâ maps. They generalise various results on retraction and
fixed point property for classical metric spaces to such spaces.

Deza and Panteleeva [ 47 ] introduce polyhedral cones and polytopes
associated with quasi-metrics on finite sets. A cone @xmath generated by
a set @xmath is the set @xmath . They compute generators and facets of
these polyhedra for small values of @xmath and study their graphs. This
paper generalises some ideas presented in the book by Deza and Laurent [
48 ] . Unfortunately, analogues of @xmath embedability and other
interesting issues developed in the book are not touched.

### 2.8 Universal Quasi-metric Spaces

Universal metric spaces were introduced by Pavel Urysohn (an alternative
spelling is Uryson) in the 1920âs â his paper [ 191 ] was published
posthumously in 1927. He showed that there exists a unique universal
countable rational metric space @xmath and that its completion is the
universal complete separable metric space @xmath , also called the
Urysohn space . The spaces @xmath and @xmath are not only universal in
the usual sense that they contain an isometric copy of every complete
separable or countable rational metric space respectively â they are
also ultrahomogeneous , that is, every isometry between finite subspaces
of @xmath or @xmath extends to a global isometry.

Urysohn spaces and their groups of isometries have recently received
considerable attention [ 192 , 193 , 197 , 198 , 156 , 107 , 194 , 199 ]
. We construct the universal countable rational quasi-metric space,
which we shall denote @xmath and the universal bicomplete separable
quasi-metric space @xmath using a construction similar to Urysohnâs and
note that the associated metric spaces are exactly the spaces @xmath and
@xmath respectively.

###### Definition 2.8.1.

A quasi-metric @xmath where the quasi-metric @xmath takes only rational
values is called a rational quasi-metric space . @xmath

###### Definition 2.8.2.

Let @xmath be a class of quasi-metric spaces. A quasi-metric space
@xmath of class @xmath is called universal or Urysohn if it satisfies
the following properties:

1.  For every quasi-metric space @xmath of class @xmath there exists an
    isometric embedding @xmath ; ( Universality )

2.  For every two isometric finite quasi-metric subspaces @xmath of
    @xmath , the isometry @xmath extends to a global isometry @xmath ; (
    Ultrahomogeneity )

@xmath

We make use of the following definition.

###### Definition 2.8.3.

Let @xmath be a (rational) quasi-metric space, @xmath a finite
quasi-metric subspace of @xmath and @xmath a (rational) quasi-metric
space such that @xmath , a one point quasi-metric extension of @xmath .
A (rational) quasi-metric space @xmath is called a @xmath -extension
(respectively @xmath -extension) of @xmath with respect to @xmath and
@xmath if there exists an isometric embedding @xmath and a point @xmath
such that the embedding @xmath extends to an isometric embedding @xmath
sending @xmath to @xmath .

A quasi-metric space which is a @xmath -extension ( @xmath -extension)
of @xmath with respect to all finite subsets of @xmath and their one
point extensions is called a universal @xmath -extension ( @xmath
-extension) of @xmath .

A quasi-metric space which is a @xmath -extension ( @xmath -extension)
of all of its finite subsets is called @xmath -universal ( @xmath
-universal) . @xmath

We now characterise the universal countable rational quasi-metric space
as a countable @xmath -universal quasi-metric space and the universal
bicomplete separable quasi-metric space as a bicomplete separable @xmath
-universal quasi-metric space and show they are unique up to an
isometry. Existence of these spaces is proven in Subsections 2.8.1 and
2.8.2 .

###### Lemma 2.8.4.

Let @xmath and @xmath be countable @xmath -universal quasi-metric spaces
and @xmath and @xmath finite quasi-metric subspaces of @xmath and @xmath
respectively. Then an isometry @xmath extends to a global isometry
@xmath .

###### Proof.

We prove the statement using the so-called shuttle or back-and-forth
argument. Let @xmath be an enumeration of @xmath and @xmath an
enumeration of @xmath . Let @xmath and @xmath . By our assumption, there
exists an isometry @xmath . Now for each @xmath ,

-   If @xmath , set @xmath . Clearly @xmath is finite and by the @xmath
    -universality of @xmath there exists @xmath such that the isometric
    embedding @xmath extends to an isometric embedding @xmath . Set
    @xmath .

    If @xmath , set @xmath and @xmath .

-   If @xmath , set @xmath . By the @xmath -universality of @xmath ,
    there exists @xmath such that the isometric embedding @xmath extends
    to an isometric embedding @xmath . Set @xmath .

    If @xmath , set @xmath and @xmath .

It is clear by the recursive construction that for each @xmath , @xmath
, @xmath , there exists an isometry @xmath and for all @xmath , @xmath
and @xmath . It is now sufficient to observe that @xmath and @xmath to
establish existence of a global isometry @xmath . â

###### Lemma 2.8.5.

Let @xmath be a @xmath - ( @xmath -) universal quasi-metric space,
@xmath a countable (rational) quasi-metric space and @xmath a finite
subspace of @xmath . Then an isometric embedding @xmath extends to an
isometric embedding @xmath .

###### Proof.

Let @xmath be an enumeration of @xmath and set @xmath and @xmath for all
@xmath . By the @xmath - (or @xmath -) universality of @xmath , @xmath
extends to an isometric embedding @xmath . Assume that for all @xmath ,
an isometric embedding @xmath extends to an isometric embedding @xmath .
Since @xmath is finite subset of @xmath and @xmath embeds isometrically
in @xmath by our assumption, it follows by the @xmath - (or @xmath -)
universality of @xmath that an isometric embedding @xmath extends to an
isometric embedding @xmath . Hence, by induction, for all @xmath , an
isometric embedding @xmath extends to an isometric embedding @xmath and
therefore there exists an isometric embedding @xmath . â

###### Proposition 2.8.6.

A countable @xmath -universal quasi-metric space is the universal
countable rational quasi-metric space. Such space is unique up to an
isometry.

###### Proof.

Universality follows by @xmath -universality and the Lemma 2.8.5 while
ultrahomogeneity is a consequence of the Lemma 2.8.4 . Suppose @xmath
and @xmath are two universal countable rational quasi-metric spaces.
Take any finite rational quasi-metric space @xmath . By universality,
@xmath embeds isometrically into @xmath and @xmath and by the Lemma
2.8.4 the isometry between images of @xmath in @xmath and @xmath extends
to a global isometry. Hence any two universal countable rational
quasi-metric spaces are isometric. â

###### Remark 2.8.7.

In fact, @xmath -universality is equivalent to the universality for a
countable rational quasi-metric space since obviously universality
implies @xmath -universality.

###### Proposition 2.8.8.

A bicomplete separable @xmath -universal quasi-metric space is the
universal bicomplete separable quasi-metric space. Such space is unique
up to an isometry.

###### Proof.

Let @xmath be a bicomplete separable @xmath -universal quasi-metric
space. Every bicomplete separable quasi-metric space @xmath contains a
countable dense subset @xmath which, by the Lemma 2.8.5 embeds into a
dense subspace of a @xmath -universal space. This embedding obviously
extends to all Cauchy (with respect to the associated metric) sequences
of points in @xmath whose limits are all in @xmath . Therefore, @xmath
satisfies universality. On the other hand, the Lemma 2.8.4 can be used
to extend the isometric embedding @xmath of any finite subset of a
countable dense subset @xmath of @xmath to the isometric embedding
@xmath which can then be extended to a global embedding since @xmath and
@xmath are bicomplete.

The Lemma 2.8.4 also implies uniqueness. Suppose @xmath and @xmath are
two universal bicomplete separable quasi-metric spaces. Any finite
rational quasi-metric space @xmath embeds isometrically into @xmath and
@xmath by universality and by the Lemma 2.8.4 the isometry between
images of @xmath in @xmath and @xmath extends to a global isometry
between countable dense subsets of @xmath and @xmath . Since @xmath and
@xmath are bicomplete, such isometry extends to an isometry @xmath . â

###### Remark 2.8.9.

The metric space associated to a universal quasi-metric space is also
universal since every isometry between quasi-metric spaces is an
isometry between their associated metric spaces (Lemma 2.1.8 ).
Therefore, @xmath and @xmath .

#### 2.8.1 Universal countable rational quasi-metric space

###### Lemma 2.8.10.

Let @xmath be a quasi-metric space and @xmath a finite quasi-metric
subspace of @xmath . Let @xmath , where @xmath , be a (rational)
quasi-metric space containing @xmath as a quasi-metric subspace plus an
extra point @xmath . Then, there exists a @xmath -extension of @xmath
with respect to @xmath and @xmath . If all @xmath and @xmath are
rational quasi-metric spaces, there exists a @xmath -extension of @xmath
with respect to @xmath and @xmath .

###### Proof.

Let @xmath and @xmath be as above and @xmath the weighted directed graph
from the Lemma 2.7.7 such that the path quasi-metric on @xmath coincides
with @xmath . Add another point to @xmath , that is, let @xmath be a
weighted directed graph such that @xmath , @xmath and

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

It is clear that @xmath is connected and hence the path quasi-metric
@xmath is well-defined (Lemma 2.7.5 ). Let @xmath and @xmath . To
complete the proof we verify that @xmath and @xmath . Let @xmath .
Denote by @xmath the set of all paths in @xmath linking @xmath and
@xmath .

Since @xmath embeds isometrically in @xmath , and @xmath embeds
isometrically in @xmath it is clear that @xmath . Let @xmath and suppose
that there exists a path @xmath such that @xmath . Then @xmath must pass
through @xmath implying that @xmath by the triangle inequality. As
@xmath is an extension of @xmath , we have @xmath , implying @xmath and
contradicting our premise. Therefore, @xmath .

Let @xmath . It is clear from the Equation 2.2 that @xmath and @xmath .
Suppose there exists a path @xmath such that @xmath . As there is no
edge @xmath in @xmath for any @xmath , such @xmath cannot pass through
any point in @xmath , nor can it pass through @xmath except as a last
point. On the other hand, for any @xmath , @xmath by the triangle
inequality. This contradicts our supposition and hence @xmath . In the
same way it can be shown that @xmath and therefore @xmath .

It is obvious that @xmath is a rational quasi-metric space if @xmath and
@xmath take values in rationals. â

Denote by @xmath the @xmath - (or @xmath -) extension of @xmath with
respect to @xmath and @xmath constructed in the Lemma 2.8.10 .

###### Lemma 2.8.11.

Let @xmath be a countable rational quasi-metric space. Then there exists
a countable @xmath -universal extension of @xmath .

###### Proof.

Let @xmath be the set of all pairs @xmath where @xmath is a finite
subspace of @xmath and @xmath is a rational quasi-metric space @xmath
containing @xmath as a quasi-metric subspace plus an extra point @xmath
. Since @xmath is countable and @xmath takes values in @xmath , @xmath
is countable. Let @xmath be an enumeration of @xmath . We now construct
the required space recursively.

Let @xmath and @xmath for all @xmath . We claim that for each @xmath ,
@xmath and @xmath is a @xmath extension of @xmath with respect to @xmath
. Indeed, @xmath and @xmath is a @xmath extension of @xmath with respect
to @xmath . Assuming for all @xmath that @xmath and denoting @xmath , it
follows that @xmath is a finite subset of @xmath and hence @xmath is
well-defined. By the Lemma 2.8.10 , @xmath and @xmath is a @xmath
extension of @xmath with respect to @xmath . Our claim therefore follows
by induction and the union @xmath is the required countable @xmath
-universal extension of @xmath . â

Denote by @xmath the @xmath -universal extension of a rational
quasi-metric space constructed in the Lemma 2.8.11 .

###### Corollary 2.8.12.

There exists a countable @xmath -universal quasi-metric space @xmath .

###### Proof.

We again employ recursion. Set @xmath , a one-point quasi-metric space,
@xmath for all @xmath and @xmath . We claim that for every finite
rational quasi-metric space @xmath of cardinality @xmath

1.  there exists an isometric embedding @xmath , and

2.  @xmath is a @xmath -universal extension of @xmath .

It is clear by the above construction that this is indeed the case for
the one-point quasi-metric space. Assume our claim holds for some @xmath
and let @xmath be a finite quasi-metric space of cardinality @xmath .
Let @xmath be a @xmath -point restriction of @xmath . By our claim (ii),
@xmath is a @xmath -universal extension of @xmath and hence contains an
isometric copy of @xmath . By the Lemma 2.8.11 , @xmath is a @xmath
-universal extension of @xmath and we have proven our claim by
induction. Each of sets @xmath is countable and therefore @xmath is a
countable @xmath -universal quasi-metric space. â

#### 2.8.2 Universal bicomplete separable quasi-metric space

To show that the bicompletion of the universal countable rational
quasi-metric space is the universal bicomplete separable quasi-metric
space we extend the argument of Gromov ( [ 79 ] , pp.80â81) for the
universal metric spaces.

###### Lemma 2.8.13.

Let @xmath be a quasi-metric space admitting an everywhere dense @xmath
-universal quasi-metric subspace @xmath . Then for each finite subset
@xmath , every @xmath and any one point quasi-metric extension @xmath of
@xmath , where @xmath , there exists @xmath such that for all @xmath

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Let @xmath , @xmath , @xmath and @xmath be as above and let @xmath and
@xmath . Since @xmath is everywhere dense in @xmath we can approximate
@xmath by the set @xmath such that for all @xmath , @xmath and @xmath .
Let @xmath be the weighted directed graph from the Lemma 2.7.7 such that
the path quasi-metric on @xmath coincides with @xmath . Construct a one
point extension @xmath such that @xmath and @xmath . Set @xmath and for
each @xmath , let @xmath be any rational such that

  -- -------- --
     @xmath   
  -- -------- --

and @xmath a rational such that

  -- -------- --
     @xmath   
  -- -------- --

By the Lemma 2.7.5 , @xmath forms a rational quasi-metric space which is
a one point extension of @xmath . By the @xmath -universality of @xmath
, there exists @xmath such that for each @xmath , @xmath and @xmath . It
remains to verify the required inequalities.

Clearly, for each @xmath , @xmath and hence

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

On the other hand, since @xmath is a path quasi-metric, there exists
@xmath such that @xmath (this includes the case @xmath ) and therefore

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Thus, for all @xmath , @xmath . The other inequality is verified in the
same way. â

###### Lemma 2.8.14.

Let @xmath be a bicomplete quasi-metric space admitting an everywhere
dense @xmath -universal quasi-metric subspace. Then @xmath is a @xmath
-universal quasi-metric space.

###### Proof.

Let @xmath be a as above, @xmath a finite subset of @xmath and @xmath a
one-point quasi-metric extension of @xmath . We must show that there
exists a point @xmath such that for each @xmath , @xmath and @xmath .

Assume without loss of generality that for all @xmath , @xmath , that
is, one of the distances @xmath and @xmath is bounded below by @xmath
while the other can be @xmath . We find by induction a sequence of
points @xmath such that for all @xmath and all @xmath

1.  @xmath ,

2.  @xmath ,

3.  @xmath for all @xmath , and

4.  @xmath .

Indeed, assume such elements @xmath exist for all @xmath . Let @xmath
and @xmath , a one point extension of @xmath . We claim there exists a
quasi-metric @xmath on @xmath satisfying

1.  @xmath ,

2.  @xmath ,

3.  @xmath , and

4.  @xmath .

It clear that the condition (a) defines a quasi-metric on @xmath . We
will show that the conditions (a), (b), (c) and (d) together also define
a quasi-metric @xmath on @xmath .

Denote by @xmath the triangle inequality @xmath for some points @xmath .
The inequalities @xmath , @xmath and @xmath where @xmath follow from our
assumption of @xmath being a quasi-metric space while the inequalities
@xmath , @xmath , @xmath ,
@xmath and @xmath where @xmath clearly follow by (i) and (ii). The
remaining two inequalities, @xmath and @xmath follow directly from (iv)
(we have @xmath and @xmath ).

Therefore, @xmath is a quasi-metric on @xmath agreeing with the induced
quasi-metric on @xmath on the intersection @xmath . Hence, there exists
a quasi-metric on the union @xmath satisfying the properties (a) â (d)
(this is easily shown by taking the distance between any two points not
in the intersection to be the shortest path through the intersection).

By the Lemma 2.8.13 , there exists a point @xmath such that for each
@xmath ,

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

and thus, by (a) and (b), it follows that for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

Furthermore, by (d),

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

implying @xmath . Finally, for all @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Similarly, @xmath .

We conclude by induction that there exists an infinite sequence @xmath
satisfying (i) â (iv). By (iii), this sequence is @xmath -Cauchy and
hence convergent since @xmath is bicomplete. It converges to the
required @xmath by (i) and (ii). â

###### Corollary 2.8.15.

There exists a @xmath -universal bicomplete separable quasi-metric space
@xmath .

###### Proof.

The required space @xmath , the bicompletion of the universal countable
rational quasi-metric space @xmath . â

## Chapter 3 Sequences and Similarities

Pairwise sequence comparison is undoubtedly one of the core areas of
bioinformatics. The most well known tool (actually a set of tools) is
NCBI BLAST (Basic Local Alignment Search Tool) [ 6 ] which, given a DNA
or protein sequence of interest, retrieves all similar sequences from a
sequence database. The similarity measure according to which sequences
are compared is based on extension of a similarity measure on the set of
nucleotides in the case of DNA, or the set of amino acids in the case of
proteins to DNA or protein sequences, using a procedure known as
alignment . Two types of (pairwise) alignments are usually
distinguished: global , between whole sequences and local , between
fragments of sequences. Similarity scores on nucleotides or amino acids,
as well as the penalties for âgapsâ introduced into sequences while
aligning them, usually have statistical interpretation.

The objective of this chapter is to establish the link between
similarity measures on biological sequences and quasi-metrics. While the
connections of global similarities to (quasi-) metrics have been known
for long [ 178 ] , the novel result is that local similarities can also
be converted to quasi-metrics while preserving the neighbourhood
structure. The assumptions required for such conversion are satisfied by
the similarity measures most widely used for searching DNA and protein
databases. We develop this result in the context of free semigroups,
which correspond to sets of strings from a finite alphabet and use the
string and semigroup terminology interchangeably. The use of semigroup
terminology may point to generalisations and extensions of our results
to other areas.

### 3.1 Free semigroups and monoids

Recall that the free monoid on a nonempty set @xmath , denoted @xmath ,
is the monoid whose elements, called words or strings , are all finite
sequences of zero or more elements from @xmath , with the binary
operation of concatenation. The unique sequence of zero letters (empty
string), which we shall denote @xmath , is the identity element. The
free semigroup on @xmath , denoted @xmath is the subset of @xmath
containing all elements except the identity.

The length of a word @xmath , denoted @xmath , is the number of
occurrences of members of @xmath in it. For @xmath , where @xmath ,
@xmath and we set @xmath .

For two words @xmath , @xmath is a factor or substring of @xmath if
@xmath for some @xmath ; @xmath is a prefix of @xmath if @xmath for some
@xmath ; @xmath is a suffix of @xmath if @xmath for some @xmath ; @xmath
is a subsequence or subword of @xmath if @xmath , where @xmath , @xmath
and @xmath . For any @xmath , we use @xmath to denote the set of all
factors of @xmath .

We call a semigroup (monoid) @xmath free if it is isomorphic to the free
semigroup (monoid) on some set @xmath . The unique set of elements of
@xmath mapping to @xmath under the isomorphism is called the set of free
generators .

As a convention, for any word @xmath , the notation @xmath , where
@xmath shall mean that @xmath while the notation @xmath shall imply that
@xmath . For all @xmath we shall use @xmath to denote the word @xmath
and set @xmath .

The motivating examples of free semigroups for this chapter are
biological sequences and structures related to them. It is quite natural
that those macromolecules which are linear polymers of a limited number
of small molecules and whose properties strongly depend on the sequence
of their constituent building blocks can be represented in this way. For
example, a DNA molecule can be represented as a word in the free
semigroup generated by the four-letter nucleotide alphabet @xmath while
an RNA molecule is a word in the free semigroup generated by the
alphabet @xmath . A protein can be thought of as a word in the free
semigroup generated by the amino acid alphabet (Table 1.1 ).

A further example from biological sequence analysis is provided by
profiles [ 78 , 218 ] . Let @xmath be a set and denote by @xmath the set
of all probability measures supported on @xmath . We shall call the
elements of the free monoid @xmath profiles over @xmath . Profiles arise
as models of sets of structurally related biological sequences where
@xmath is the DNA or protein alphabet.

### 3.2 Generalised Hamming Distance

A simplest way to extend a distance from generators to words of equal
length is to use what we call a generalised Hamming distance , a special
case of the @xmath -type sum mentioned in the Example 2.2.16 .

###### Definition 3.2.1.

Let @xmath be a set and let @xmath , the set of words in the free
semigroup generated by @xmath of length @xmath . Let @xmath be a
distance on @xmath . The generalised Hamming distance on @xmath is a
function @xmath where

  -- -------- --
     @xmath   
  -- -------- --

@xmath

As mentioned in the Example 2.2.17 , the Hamming distance is a special
case where @xmath is the discrete metric. If the distance on the set of
generators @xmath is a quasi-metric, the same holds for the generalised
Hamming distance on @xmath (Example 2.2.16 ). Obviously, similarity
measures on the generators can be extended in the same way.

The generalised Hamming distance has an advantage that it can be
computed in linear time. It can be interpreted as the total cost of
substitutions necessary to transform one word into another. It is worth
noting that it is permutation invariant â permuting both words with a
same permutation does not change their distance.

The main practical disadvantage of the generalised Hamming distance is
that it is restricted to the words of the same size and that it does not
consider any other type of transformation but substitution. Hence it is
only suitable for modelling the sets of words of the same length where
insertions or deletions of factors (i.e. single characters or segments)
are unlikely.

### 3.3 String Edit Distances

The term string edit distances shall be used to refer to all distances
between words defined as the smallest weight of a sequence of permitted
weighted transformations transforming one word into another. In a
stricter sense, the string edit distance denotes the smallest number of
permitted edit operations required to transform one string into another
where the permitted edit operations are substitutions of one character
for another, insertions of one character into the first string and
deletions of one character from the first string. It was first mentioned
in the paper by V. Levenstein [ 122 ] and is often referred to as the
Levenstein distance. In their 1976 paper [ 203 ] , Waterman, Smith and
Beyer introduced the most general form of the string edit distance and
proposed an algorithm to compute it in some important cases. Below, we
outline their construction of the so-called @xmath -(quasi-) metric
which we shall refer to as the W-S-B distance .

#### 3.3.1 W-S-B distance

###### Definition 3.3.1.

Let @xmath be a set and @xmath a free monoid over @xmath with the
identity element @xmath . Suppose @xmath is a finite set of
transformations defined on subsets @xmath such that the identity
transformation @xmath is in @xmath . Let @xmath be a function such that
@xmath . We call the pair @xmath a set of weighted edit operations on
@xmath . @xmath

###### Definition 3.3.2.

Let @xmath be a set and @xmath a (finite) set of weighted edit
operations on @xmath . Let @xmath , where @xmath and let @xmath . Fix
@xmath and suppose @xmath . Then @xmath is defined by

  -- -------- --
     @xmath   
  -- -------- --

If @xmath , then @xmath is defined by @xmath .

For any @xmath define

  -- -------- --
     @xmath   
  -- -------- --

where @xmath , that is, @xmath is the set of all finite sequences of
transformations from @xmath such that ordered composition of such
transformation maps @xmath into @xmath . The members of @xmath are
called edit scripts . Also, if @xmath , for any @xmath , define

  -- -------- --
     @xmath   
  -- -------- --

@xmath

###### Remark 3.3.3.

In theory, @xmath can be allowed to be an infinite set. In that case,
the minimum in the Definition 3.3.4 of the @xmath -distance below must
be replaced by infimum and many proofs become very awkward. So far there
have been no interesting examples involving infinite sets of
transformations.

###### Definition 3.3.4.

Let @xmath be a set and @xmath a (finite) set of weighted edit
operations on @xmath . For any @xmath , define the @xmath -distance
@xmath by

  -- -------- --
     @xmath   
  -- -------- --

if @xmath and @xmath if @xmath . @xmath

Hence, the @xmath -distance between two words is the smallest weight of
an edit script of operations in @xmath transforming (in the sense of
ordered composition) one word into another.

The relation @xmath is an equivalence relation and partitions @xmath
into equivalence classes @xmath where the value of @xmath between any
two members of @xmath is finite. We have the following simple fact:

###### Theorem 3.3.5 ([203]).

Let @xmath be a set and @xmath a set of weighted edit operations on
@xmath . For each equivalence class @xmath of @xmath , @xmath is a
quasi-metric. â

The @xmath -metric is defined on each @xmath as the associated metric
@xmath . Note that the requirement that @xmath for each @xmath such that
@xmath implies that @xmath is a @xmath -quasi-metric.

###### Remark 3.3.6.

It is easy to observe that the @xmath -quasi-metric is equivalent to the
path quasi-metric on the connected components of a weighted directed
multigraph (two vertices can be joined by more than one directed edge)
where the vertices are words in @xmath and two words @xmath and @xmath
are joined with an edge if there is a transformation @xmath such that
for some @xmath , @xmath . The weight of each edge is the weight of the
corresponding transformation and an edit script is a path in the
multigraph. Section 2.7 presents the development of path quasi-metric on
a weighted directed graph and the same technique can be trivially
extended to multigraphs.

We now present the terminology and notation for the most biologically
relevant sets of weighted edit operations.

###### Definition 3.3.7.

Let @xmath be a set and @xmath a free monoid over @xmath with the
identity element @xmath . Define the following transformations of
elements of @xmath :

-   @xmath , where @xmath , @xmath ,

-   @xmath , where @xmath , @xmath , and

-   @xmath , where @xmath and @xmath .

The transformations of the type @xmath are called substitutions or
mutations , of the type @xmath are called insertions and of the type
@xmath are called deletions . Insertions and deletions are collectively
called indels .

Define

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

@xmath

Note that @xmath and @xmath implicitly contain the identity
transformation @xmath for any @xmath .

###### Example 3.3.8.

For a set of letters @xmath , the Levenstein distance is realised as
@xmath where @xmath for all @xmath such that @xmath .

While providing an easily interpretable example, the Levenstein distance
is too simplistic for comparison of biological sequences and more
general distances must be used. From an evolutionary point of view, each
transformation should correspond to a mutational event and the resulting
distance to the âevolutionary distanceâ between two sequences. In
practice, not all transformations of biological sequences are equally
likely. For example, substitutions are generally more likely than
indels, while some substitutions may be more likely than others. This is
certainly the case in proteins where one observes for example, that
substitutions of I for V are more common than substitutions of I for K.
It was also argued [ 178 ] that indels are more likely to take place by
segments than character-by-character and hence that indels of arbitrary
segments should take weights smaller than the sum of the weights of
indels of single characters comprising each segment.

###### Example 3.3.9.

The Sellers (or @xmath -) distance, introduced by Sellers in 1974 [ 171
] , is a metric obtained by extension of a metric @xmath on the set
@xmath , the set of generators plus the identity element, to the free
monoid @xmath . The value of @xmath for @xmath represents the cost of
substitution of @xmath for @xmath in a word in @xmath while @xmath is
the cost of insertion or deletion of a character @xmath .

The @xmath -metric can be considered as a special case of the W-S-B
metric by using @xmath as the set of transformations. Suppose @xmath ,
@xmath and @xmath . Waterman, Smith and Beyer [ 203 ] showed that the
necessary and sufficient condition for the @xmath -metric induced by the
above weights to coincide with an @xmath -metric is that @xmath be a
metric on @xmath .

In fact, the construction of Sellers has long been known in the theory
of topological groups [ 153 ] . The @xmath -metric on @xmath is
equivalent to the Graev pseudo-metric [ 75 , 76 ] on the free group
@xmath (i.e. the free group generated by @xmath ), restricted to @xmath
. The Graev pseudo-metric, can be described as the maximal bi-invariant
pseudo-metric @xmath on @xmath such that @xmath .

###### Example 3.3.10.

Let @xmath be a set and for @xmath denote by @xmath the longest common
subsequence of @xmath and @xmath . Define

  -- -------- --
     @xmath   
  -- -------- --

It can be easily shown that @xmath is a metric on @xmath and that @xmath
where @xmath and @xmath for all @xmath (i.e. optimal sequences of edit
operations only involve indels). The LCS metric provides a special case
of string edit distance (more specifically of Sellers distance) which
has been extensively studied in computer science [ 8 ] .

###### Example 3.3.11.

Let @xmath be a set and suppose @xmath consists only of the
transformations of the type @xmath , where @xmath . Suppose @xmath where
@xmath is a function @xmath such that @xmath for all @xmath and @xmath
for all @xmath .. It is clear that @xmath if and only if @xmath and
therefore the partitions of the equivalence relation @xmath are the sets
@xmath for all @xmath plus the set @xmath . It is easy to verify that on
each @xmath , @xmath coincides with the generalised Hamming distance
@xmath if and only if @xmath satisfies the triangle inequality (i.e.
@xmath is a quasi-metric).

#### 3.3.2 Alignments

In biology, one is usually interested not only in the distance between
two words, but also in the edit script realising it. A standard way of
representing an edit script mapping one sequence into another is called
a (pairwise) alignment .

###### Definition 3.3.12.

Let @xmath be a set, @xmath and suppose @xmath is a set of weighted edit
operations on @xmath . A global alignment between @xmath and @xmath is a
finite sequence of pairs @xmath such that @xmath for all @xmath and

1.  @xmath ,

2.  @xmath ,

3.  @xmath for all @xmath ,Â and

4.  there exists @xmath such that @xmath .

The weight or score of the alignment @xmath is the sum @xmath where
@xmath and @xmath . @xmath

The axiom (iii) in the Definition 3.3.12 above ensures that a sequence
that is a global alignment is finite.

###### Definition 3.3.13.

A local alignment between @xmath is a global alignment between @xmath
and @xmath where @xmath is a factor of @xmath and @xmath a factor of
@xmath . @xmath

Alignments are usually displayed by first inserting chosen spaces (or
dashes), either into or at the ends of @xmath and @xmath , and then
placing the two resulting strings one above the other so that every
character or space in either string is opposite a unique character of a
unique space in the other string [ 83 ] .

It is obvious that every (global) alignment can be associated with an
edit script of the same weight. The converse is not true in general as
the Example 3.3.14 attests. Recall that @xmath consists of
substitutions, insertions and deletions (Definition 3.3.7 ) and that a
superscript on a transformation @xmath denotes the start of the fragment
being acted on by @xmath (Definition 3.3.2 ).

###### Example 3.3.14.

Let @xmath and consider @xmath ,the set of weighted edit operations on
@xmath where @xmath , @xmath and for each @xmath , @xmath .

Suppose @xmath and @xmath . Then, it is clear that @xmath and that
@xmath . However, the alignment of smallest weight, @xmath , has weight
@xmath . It is easy to see that all other possible alignments have an
even greater weight.

###### Definition 3.3.15.

Let @xmath . An edit script @xmath admits an alignment if there exists a
sequence @xmath where @xmath such that @xmath and @xmath . @xmath

The following Lemma provides a straightforward characterisation of the
above definition.

###### Lemma 3.3.16.

Let @xmath . An edit script @xmath , where @xmath , admits an alignment
if @xmath and

1.  @xmath if @xmath for some @xmath ,

2.  @xmath if @xmath for some @xmath ,

3.  @xmath if @xmath for some @xmath ,

and for all @xmath ,

1.  @xmath if @xmath for some @xmath ;

2.  @xmath if @xmath for some @xmath ;

3.  @xmath if @xmath for some @xmath ;

###### Proof.

For each @xmath set

  -- -------- --
     @xmath   
  -- -------- --

We claim that @xmath and @xmath . The first claim is proven by showing
by induction that for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Indeed, the conditions (i), (ii) and (iii) directly imply the base step
while the conditions (iv), (v) and (vi) imply the inductive step. Since
@xmath , it follows that @xmath .

Similarly, the second claim is proven by showing by induction that for
all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

The base step in this case follows from the definition of @xmath while
the inductive step follows easily from the conditions (iv), (v) and
(vi). â

The following simple result was first observed by Smith, Waterman and
Fitch [ 178 ] .

###### Lemma 3.3.17 ([178]).

Let @xmath be a set, @xmath and suppose @xmath is a global alignment
between @xmath and @xmath . Then

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

where @xmath , @xmath and @xmath . â

String edits and alignments are best illustrated by examples. For
simplicity we use the Levenstein distance.

###### Example 3.3.18.

Let @xmath be the English alphabet, let @xmath and @xmath . It is easy
to see that the Levenstein distance between @xmath and @xmath is @xmath
. Indeed, if we align @xmath and @xmath in the following way,

    COMPLEXI----TY
    ---FLEXIBILITY

we note that seven indels and one substitutions are necessary to convert
@xmath into @xmath and vice versa. One can also easily see that this is
the smallest number of transformations necessary (more formally, this
fact would be a simple corollary of the Theorem 3.3.27 to be stated and
proven later).

The string edit distances may, in some cases, be more suitable for
comparison of strings of the same length than the (generalised) Hamming
distance.

###### Example 3.3.19.

Consider the words @xmath and @xmath of length 6. The Hamming distance
between @xmath and @xmath is 6 while the Levenstein distance is 2.

#### 3.3.3 Dynamic programming algorithms

While the @xmath -metric (and quasi-metric) can be generated from any
sets of transformations of @xmath , the main motivation of Waterman,
Smith and Beyer in [ 203 ] was to extend the construction of Sellers [
171 ] so that indels of multiple characters with weights less than the
sum of the weights of indels of individual characters can be permitted.
The algorithm they proposed for computing such distances is based on
dynamic programming technique, introduced by Bellman [ 13 ] in the
general context and first applied to biological sequence comparison by
Needleman and Wunsch [ 146 ] using similarities and by Sellers [ 171 ]
using distances. Dynamic programming remains the foundation of all
pairwise biological sequence alignment algorithms and we here briefly
present it in relation to the W-S-B algorithm.

The three essential components of the dynamic programming approach are
recurrence relation , tabular computation and the traceback .

##### Recurrence Relations

We now outline the recurrence relations used for computation of the
W-S-B metric which takes into account indels of multiple characters.

###### Definition 3.3.20.

Let @xmath be a set. The set of weighted edit operations @xmath on
@xmath satisfies the condition M if for all @xmath and for each sequence
of edit operations @xmath there exists @xmath which admits an alignment
and @xmath . @xmath

The condition M was introduced in [ 203 ] in a slightly different but
essentially equivalent form. It implies that the W-S-B distance between
any two points is determined solely from edit scripts admitting an
alignment and leads to the following theorem. Recall that for all @xmath
and for any @xmath , @xmath denotes the word @xmath and that @xmath .

###### Theorem 3.3.21 ([203]).

Let @xmath be a set, @xmath and suppose @xmath is a set of weighted edit
operations on @xmath satisfying the condition M . Then, for all @xmath ,
@xmath such that @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is ignored if @xmath or @xmath are negative.

###### Proof.

Obviously @xmath . Fix @xmath and @xmath such that @xmath . Since @xmath
satisfies the condition M , there exists an edit script @xmath that
admits an alignment and @xmath . Since @xmath admits an alignment, it
follows that @xmath for some @xmath , @xmath and that @xmath (otherwise
the assumption @xmath would be violated). The proof is completed by
considering all possibilities for @xmath . â

###### Remark 3.3.22.

Under the conditions of the Theorem 3.3.21 it is clear that @xmath is
invariant (in the sense of the Definition 2.6.21 ) with respect to the
string concatenation, that is, for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Hence, the triple @xmath where @xmath is the string concatenation
operation is a quasi-metric semigroup (Definition 2.6.21 ).

###### Definition 3.3.23.

Let @xmath be a set. A map @xmath is called increasing if for any @xmath
and any @xmath , @xmath . @xmath

###### Definition 3.3.24.

Let @xmath be a set. The set of weighted edit operations @xmath on
@xmath satisfies the condition N if

1.  @xmath for all @xmath ,

2.  @xmath for all @xmath , and

3.  @xmath for all @xmath .

where @xmath is a quasi-metric on @xmath , @xmath are non-decreasing
positive functions @xmath , and @xmath are non-negative functions @xmath
such that for all @xmath , @xmath ( @xmath is right 1-Lipschitz) and
@xmath ( @xmath is left 1-Lipschitz). @xmath

We now show that the condition N implies the condition M .

###### Lemma 3.3.25.

Let @xmath be a set and @xmath a set of weighted edit operations on
@xmath satisfying the condition N . Suppose @xmath , @xmath and let
@xmath such that @xmath is well-defined. Denote @xmath and @xmath .
Then, there exists an edit script @xmath such that @xmath and @xmath .

###### Proof.

There are nine principal cases corresponding to all combinations of
transformation types in @xmath .

If @xmath for some @xmath (the transformation acting on the position
@xmath is substitution), it is easy to see that @xmath , whatever @xmath
might be. Similarly, if @xmath for some @xmath (the transformation
acting on the position @xmath is deletion), we have @xmath , where
@xmath , again whatever @xmath might be. This covers six cases.

Now consider the three cases where @xmath (the transformation acting on
the position @xmath is insertion). If @xmath , then, whatever @xmath
might be, @xmath , where @xmath and the statement is satisfied. Hence,
assume without loss of generality that @xmath .

If @xmath for some @xmath , we have a situation where @xmath and

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

for some @xmath and @xmath and where @xmath . Since the weight of @xmath
depends solely on composition and length of inserted fragments and not
on the order of generators within them, we can set @xmath where @xmath
and @xmath . Clearly, @xmath and hence @xmath .

If @xmath for some @xmath , we have a situation where @xmath and

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

for some @xmath and @xmath . In this case, we can set @xmath , where
@xmath . As @xmath is right 1-Lipschitz ( @xmath ), it follows that
@xmath . The identity transformation @xmath is there so that the form of
@xmath exactly satisfies the statement of the Lemma.

If @xmath for some @xmath , we have a situation where @xmath and

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

for some @xmath such that @xmath , and @xmath . Set @xmath so that
@xmath . Since @xmath and @xmath are non-negative functions and @xmath
is a non-decreasing function, we have @xmath . â

###### Lemma 3.3.26.

Let @xmath be a set and @xmath a set of weighted edit operations on
@xmath satisfying the condition N . Then, for any @xmath and any edit
script @xmath , there exists an edit script @xmath such that @xmath and
@xmath .

###### Proof.

Let @xmath and let @xmath . We construct the required edit script @xmath
by using the Lemma 3.3.25 recursively on pairs of transformations from
@xmath .

Set @xmath and find the largest @xmath such that @xmath is the smallest
superscript in @xmath . If @xmath , set @xmath and proceed to the next
step. Otherwise, produce a new edit script @xmath such that @xmath , by
replacing the pair of terms @xmath in @xmath by the pair @xmath where
@xmath . By the Lemma 3.3.25 , this is always possible.

After this step, @xmath will remain the smallest superscript in @xmath .
Apply the same procedure to @xmath to produce @xmath and so on. After at
most @xmath steps we get an edit script @xmath , with the same number of
terms as @xmath , such that @xmath is the smallest superscript.

To get from @xmath to @xmath , @xmath , repeat the above procedure to
the edit script @xmath to obtain the edit script @xmath and then set
@xmath . After @xmath such steps we get @xmath where @xmath . Since the
weight did not increase at any step, it follows that @xmath . â

###### Theorem 3.3.27.

Let @xmath be a set and @xmath a set of weighted edit operations on
@xmath satisfying the condition N . Then, for any @xmath and any edit
script @xmath there exists an edit script @xmath such that @xmath admits
an alignment and @xmath .

###### Proof.

Let @xmath and let @xmath . If @xmath already admits an alignment, there
is nothing to prove. Otherwise, due to the Lemma 3.3.26 , we can assume
without loss of generality that @xmath . Using a recursive process
starting from @xmath , we construct an edit script @xmath that satisfies
the requirements of the Lemma 3.3.16 and hence admits an alignment. We
will use the notation @xmath , where @xmath to denote the edit script at
each step of the recursion.

If @xmath , set @xmath , otherwise set @xmath . For each @xmath , let
@xmath denote the largest index such that one of the conditions (iv),
(v) or (vi) of the Lemma 3.3.16 is not satisfied (which one of the three
is violated depends on the type of @xmath ).

If @xmath for some @xmath , the condition (iv) of the Lemma 3.3.16
requires that @xmath . Since the condition (iv) is violated, it must
follow that either @xmath or @xmath . In the former case, set @xmath
where @xmath . Since the inserted transformation is the identity
transformation, the weight does not change.

In the former case there are three possibilities. If @xmath for some
@xmath , construct @xmath by replacing the terms @xmath in @xmath , of
total weight @xmath , with a single transformation @xmath , of weight
@xmath , and leaving the rest of @xmath unchanged. Clearly, since @xmath
satisfies the triangle inequality, @xmath . If @xmath for some @xmath ,
construct @xmath by replacing the terms @xmath in @xmath , of total
weight @xmath with a single transformation @xmath , of weight @xmath .
Again, @xmath because of the right Lipschitz assumption on @xmath . If
@xmath for some @xmath , construct @xmath by replacing the @xmath in
@xmath with @xmath without changing the weight.

If @xmath for some @xmath , the condition (v) of the Lemma 3.3.16
requires that @xmath . Since we assume it is violated, it follows that
@xmath . Set @xmath where @xmath . Since the inserted transformation is
the identity transformation, the weight does not change.

Finally, if @xmath for some @xmath , the condition (vi) of the Lemma
3.3.16 requires that @xmath . If @xmath , set, without changing the
weight, @xmath where @xmath .

If @xmath and @xmath for some @xmath , we have a situation where @xmath
and

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

for some @xmath and @xmath . Construct @xmath by replacing the terms
@xmath in @xmath with @xmath such that @xmath and @xmath . Clearly, this
case is analogous to ( 3.2 ) of the Lemma 3.3.25 and, since the weight
of a deletion also depends only on composition and length of deleted
fragments, @xmath will have the same weight as @xmath .

If @xmath and @xmath for some @xmath , we have a situation where @xmath
and

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

for some @xmath . Construct @xmath by replacing the terms @xmath in
@xmath by a single transformation @xmath . This case is analogous to (
3.3 ) of the Lemma 3.3.25 and hence, by the left 1-Lipschitz assumption
on @xmath , @xmath .

If @xmath and @xmath for some @xmath , we have a situation where @xmath
and

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

for some @xmath . Construct @xmath by replacing the terms @xmath in
@xmath by a single transformation @xmath . This case is analogous to (
3.4 ) of the Lemma 3.3.25 and, by a similar argument, @xmath will have
the same weight as @xmath .

Hence, in all cases where one of the conditions (iv), (v) or (vi) of the
Lemma 3.3.16 is violated, we construct a new edit script of no greater
weight where all transformations up to and including the previously
violating transformation now fully satisfy the conditions. Depending on
the particular type of violation, the number of transformations in the
new edit script either decreases by one, remains the same or increases
by one. The only way it can increase is by inserting an identity
transformation and clearly, there can be finitely many such insertions.
Thus, the recursion terminates after finitely many steps. It remains to
satisfy the conditions (i), (ii) and (iii) of the the Lemma 3.3.16
concerning the first edit operation. This can be achieved by inserting
as many of the identity transformations as necessary. â

###### Remark 3.3.28.

The Theorem 3.3.27 is also valid in the case where @xmath and @xmath ,
but in that case, in order to satisfy the Definition 3.3.1 of @xmath ,
@xmath and @xmath must be strictly positive.

The Theorem 3.3.27 is a generalisation of the Theorem 4 of [ 203 ] ,
which assumes @xmath , @xmath and @xmath , where @xmath and @xmath are
positive increasing functions. The functions @xmath and @xmath giving
the weights of indels are called gap penalties . The most widely used
gap penalties are linear , of the form @xmath and affine , of the form
@xmath , where @xmath is the length of a gap and @xmath are constants.
Both linear and affine gap penalties are examples of concave functions ,
satisfying @xmath . Gap penalties of the form @xmath have also been
proposed [ 14 ] .

The complexity of dynamic programming algorithms depends on the gap
penalty. In general, Waterman, Smith and Beyer [ 203 ] obtained the
@xmath average and worst case running time, where @xmath and @xmath . If
@xmath and @xmath are linear, this can be reduced to @xmath . The same
bounds hold for affine gap penalties using the algorithm of Gotoh [ 74 ]
.

##### Tabular computation

The Theorem 3.3.21 can be used directly to compute @xmath for any @xmath
. Let @xmath and @xmath and let @xmath be an @xmath matrix with rows and
columns indexed from @xmath . Suppose @xmath , @xmath and @xmath where
@xmath is a quasi-metric and @xmath are positive increasing functions.
Clearly, @xmath satisfies the condition N and hence, by the Theorem
3.3.27 , condition M .

Set @xmath , @xmath ,
@xmath and for all @xmath and @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

The form of the recurrence above is the same as in the Theorem 3.3.21
and hence @xmath . The tabular computation approach involves computation
of @xmath bottom-up: the values of @xmath for all @xmath and @xmath are
computed in an increasing row (or column) order. The Example 3.3.29
provides an illustration.

###### Example 3.3.29.

Let @xmath be the English alphabet, let @xmath and @xmath as in the
Example 3.3.18 . For all @xmath , set @xmath if @xmath and @xmath if
@xmath and let @xmath . The matrix (or table) @xmath used for
computation of the W-S-B distance @xmath is given in the Table 3.1 â
observe that @xmath .

##### Traceback

Computation using a dynamic programming table provides the value of
distance but often, especially in biological applications, an optimal
edit script (need not be unique) and the corresponding alignment need to
be retrieved. This is most easily achieved (at least conceptually) by
keeping one or more pointers at each entry @xmath of the dynamic
programming table @xmath apart from @xmath , pointing to the entries
@xmath such that @xmath is obtained by summing @xmath and the weight of
the corresponding transformation. An optimal edit script is obtained by
following any path of pointers from @xmath to @xmath and accumulating
the transformations corresponding to each pointer. This procedure is
known as traceback . It is clear that there exists a 1-1 correspondence
between alignments and paths between @xmath and @xmath .

###### Example 3.3.30.

The path shown in bold in the Table 3.1 corresponds to the following
alignment:

COMPLEX---ITY ---FLEXBILITY.

Note that there exists a second optimal path in this case â it
corresponds to the alignment in the Example 3.3.18 .

The correspondence between alignments and paths in the dynamic
programming table suggests an alternative definition of a distance. Let
@xmath and suppose @xmath is a non-negative function @xmath such that
@xmath and @xmath are positive functions. Define

  -- -------- --
     @xmath   
  -- -------- --

where, as in the Lemma 3.3.17 , @xmath ,
@xmath and @xmath . The condition N is the sufficient condition for
@xmath to be a quasi-metric.

### 3.4 Global Similarity

An alternative approach to sequence comparison is maximise similarities
instead of minimising distances. In this case a similarity measure on
@xmath and gap penalties are used to define the global similarity
between two sequences in @xmath . The computation is handled using the
Needleman-Wunsch dynamic programming algorithm [ 146 ] which is very
similar to the W-S-B algorithm for computation of distances. We define
global similarity using a dynamic programming matrix.

###### Definition 3.4.1.

Let @xmath be a set, @xmath , @xmath and @xmath . Let @xmath and let
@xmath and @xmath . The Needleman-Wunsch dynamic programming matrix,
denoted @xmath , is an @xmath matrix @xmath with rows and columns
indexed from @xmath such that @xmath , @xmath , @xmath and for all
@xmath and @xmath

  -- -------- --
     @xmath   
  -- -------- --

We define the global similarity between the sequences @xmath and @xmath
(given @xmath , @xmath , and @xmath ), denoted @xmath , to be the value
@xmath . @xmath

###### Remark 3.4.2.

In terms of alignments, we have

  -- -------- --
     @xmath   
  -- -------- --

where, as before, @xmath , @xmath and @xmath . The term global is used
because the alignments in question are global â in the next section we
will examine local similarities which involve local alignments.

###### Remark 3.4.3.

Traditionally the gap penalty is a positive function in the case of both
distances and similarities, being added in one case and subtracted in
the other. The running times of dynamic programming algorithms still
depend on the types of gap penalties, as discussed in the section about
distances.

It is also possible to interpret similarities by considering the sets of
weighted transformations similar to those used to define the W-S-B
distance. In this case, the set @xmath still consists of weighted
transformations of the elements of @xmath but the requirement that
@xmath is dropped. In particular, this means that each transformation of
the form @xmath , where @xmath , does not need to have weight @xmath and
that the weights of @xmath and @xmath may be different for different
@xmath . It may be desirable to impose as an additional condition that
@xmath for all @xmath . The definition of @xmath remains as before and
the similarity @xmath of two words @xmath and @xmath is defined to be

  -- -------- --
     @xmath   
  -- -------- --

For this definition to be equivalent to the one obtained from the
Needleman-Wunsch algorithm, it is necessary that a condition similar to
the condition M is fulfilled: there must be at least one optimal
sequence of transformations which corresponds to a sequence of
transformations considered by the Needleman-Wunsch algorithm. This is
not always the case in practice (see Section 3.6 below) and one then
needs to assume in addition that only those transformations acting on
each alignment position only once are allowed.

#### 3.4.1 Correspondence to distances

The following observation allows conversion of similarity scores to
quasi-metrics.

###### Lemma 3.4.4 ([181]).

Let @xmath be a set and @xmath a map such that

1.  @xmath ,

2.  @xmath ,

3.  @xmath ,

4.  @xmath .

Then @xmath where @xmath is a quasi-metric. Furthermore, if @xmath is
symmetric, that is, @xmath for all @xmath , @xmath is a co-weighted
quasi-metric space with the co-weight @xmath .

###### Proof.

Positivity of @xmath is equivalent to (ii), separation of points is
equivalent to (iii) while the triangle inequality is equivalent to (iv).
If @xmath then @xmath and since @xmath it follows that @xmath is a
co-weight. â

Obviously, if @xmath satisfies all the requirements of the Lemma 3.4.4
and is symmetric, then @xmath is a partial metric (Subsection 2.6.3 )
and the Lemma 3.4.4 is equivalent to the Theorem 2.6.15 .

###### Lemma 3.4.5.

Let @xmath be a set and @xmath . If @xmath is a map satisfying the
conditions (i) and (ii) of the Lemma 3.4.4 , @xmath and @xmath are
functions @xmath and @xmath , then for all @xmath and for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

We prove our claim by induction. Let @xmath denote a partial order on
@xmath where @xmath if @xmath or @xmath and @xmath (lexicographic
order). The relation @xmath is wellâfounded of order type @xmath (but of
course the induction is finite) and our claim is trivially true for
@xmath . Assume it is true for all @xmath .

If @xmath and @xmath , we have for some @xmath , @xmath since @xmath by
the induction hypothesis and @xmath is non-negative. In a similar way,
it follows that @xmath since @xmath is non-negative.

We now consider the case where @xmath and @xmath and show that @xmath .
If @xmath we have @xmath by the induction hypothesis and @xmath by the
condition (ii), and therefore @xmath . If @xmath for some @xmath , the
result follows since @xmath is a non-negative function and @xmath by the
induction hypothesis. If @xmath , the same result follows by the
induction hypothesis and non-negativity of @xmath . The inequality
@xmath follows by the same argument. â

###### Corollary 3.4.6.

Suppose @xmath is a function satisfying the conditions (i) and (ii) of
the Lemma 3.4.4 , @xmath and @xmath are functions @xmath and @xmath the
global similarity on @xmath with respect to @xmath and @xmath . Then,
for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Let @xmath . If @xmath , by definition @xmath , coinciding with a sum
over an empty set. For @xmath , the Lemma 3.4.5 directly implies the
required result. â

###### Theorem 3.4.7.

Suppose @xmath is a map satisfying the conditions of the Lemma 3.4.4 and
let @xmath be increasing functions @xmath . Then, the formula

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath is the global similarity (given @xmath and
@xmath ), defines a @xmath -quasi-metric @xmath on @xmath .

###### Proof.

Set @xmath . By the Lemma 3.4.4 , @xmath is co-weightable quasi-metric
with co-weight @xmath . The Lemma 2.6.7 implies that a co-weight
function is left 1-Lipschitz. Consider the set @xmath of edit operations
over @xmath where @xmath , @xmath and @xmath . Let @xmath . By our
assumptions, @xmath satisfies the condition N and hence, by the Theorem
3.3.27 , the condition M . By the Theorem 3.3.21 , we have @xmath ,
@xmath , @xmath , and for all @xmath , @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

We claim that for all @xmath , @xmath , @xmath , where @xmath .

It is clear that @xmath and that @xmath . By the Lemma 3.4.6 , @xmath
and hence @xmath . Let @xmath , @xmath and assume @xmath for all @xmath
such that @xmath and @xmath but excluding @xmath . Then,

  -- -------- -- --
     @xmath      
                 
     @xmath      
     @xmath      
  -- -------- -- --

and our claim follows by induction. In particular, @xmath as required. â

###### Example 3.4.8.

It is well known [ 83 ] that the longest common subsequence problem can
be approached using similarities rather than distances. Let @xmath be a
set and set for all @xmath , @xmath and @xmath if @xmath . Let @xmath
for all @xmath . It is easy to confirm that for @xmath , @xmath .

By the Theorem 3.4.7 , @xmath gives a co-weightable quasi-metric with
co-weight @xmath . The metric @xmath is the metric @xmath from the
Example 3.3.10 . The associated order @xmath is clearly the subsequence
order:

  -- -------- --
     @xmath   
  -- -------- --

and @xmath forms a meet semilattice where @xmath .

The partial order @xmath is an example of an invariant meet semilattice
(Definition 2.6.19 ) since

  -- -------- --
     @xmath   
  -- -------- --

By the Theorem 2.6.20 , the map @xmath is a meet valuation and @xmath .

### 3.5 Local Similarity

Presently, most biological sequence comparison is done using local
rather than global similarity measures. The principal reason is that
elements of biological function whose detection is desired are usually
restricted to discrete fragments of sequences and the strong similarity
of fragments of two sequences may not extend to similarity of full
sequences. For example, the structure of a protein consists of discrete
structural domains interspersed with random coils linking them and
variation is much higher in the parts not directly related to the
function. Thus, even relatively closely related protein sequences may
show little similarity outside the functionally important regions and
their global similarity may not be significant.

The similar phenomenon occurs in DNA sequences, where events other than
point mutations and insertions and deletions, such as inversions or
translocations, may occur between very closely related sequences.
Therefore, local similarity measures, and the associated local
alignments between two sequences are most appropriate for general
comparison of biological sequences. A dynamic programming algorithm for
computation of local similarities, of the same complexity as the
Needleman-Wunsch algorithm was proposed by Smith and Waterman in 1981 [
177 ] . While its cubic (quadratic if gap penalties are affine)
complexity renders it not very suitable for sequential searches of large
datasets, it remains the canonical yardstick with which the accuracy of
any heuristic algorithms is assessed. We therefore follow the precedent
of the previous section and define local similarity between two
sequences using a dynamic programming matrix.

###### Definition 3.5.1.

Let @xmath be a set, @xmath , @xmath and @xmath . Let @xmath and let
@xmath and @xmath . The Smith-Waterman dynamic programming matrix,
denoted @xmath , is an @xmath matrix @xmath with rows and columns
indexed from @xmath such that @xmath and for all @xmath and @xmath

  -- -------- --
     @xmath   
  -- -------- --

We define the local similarity between the sequences @xmath and @xmath
(given @xmath , @xmath , and @xmath ), denoted @xmath , to be the
largest entry of @xmath , that is, @xmath . @xmath

An optimal edit script and a corresponding alignment is retrieved from
@xmath by a slightly modified traceback procedure: the traceback starts
at @xmath such that @xmath is maximal and ends at an entry of @xmath
with a value of @xmath (Example 3.5.2 ). Clearly, no traceback is
possible if @xmath .

Two additional requirements are usually associated with the
Smith-Waterman algorithm: the expected value of @xmath must be negative
and at least for some @xmath , @xmath must be positive. The first
requirement obviously requires a probability measure on @xmath and
exists to ensure that the alignments retrieved are indeed local rather
than global or close to global. The second requirement ensures that
pairs of sequences with a positive local similarity score exist.

###### Example 3.5.2.

Consider the English words @xmath and
@xmath from the Example 3.5.2 . Suppose @xmath , @xmath if @xmath and
let @xmath . The matrix @xmath is given in the Table 3.2 . The local
similarity score is 12 â the corresponding alignment is the exact match
of the common substring LEXI .

The local similarity between two words as defined using the
Smith-Waterman algorithm can be realised as a global similarity between
some of their fragments (provided there exist two fragments with
positive global similarity). Recall that we use @xmath to denote the set
of all factors (or fragments) of @xmath .

###### Lemma 3.5.3.

Let @xmath be a set, @xmath , @xmath and @xmath . Suppose @xmath . Then
there exist @xmath and @xmath such that @xmath , where both global and
local similarities are taken with respect to @xmath and @xmath .

###### Proof.

Since @xmath , it follows that @xmath . We find @xmath by traceback. Let
@xmath . By definition of local similarity there exist @xmath such that
@xmath . We trace back the path of cells of the Smith-Waterman dynamic
programming matrix from @xmath to a zero entry by constructing a
sequence @xmath such that @xmath , @xmath and @xmath , @xmath in the
following way. For each @xmath , if @xmath stop. Otherwise, if @xmath ,
set @xmath ; if @xmath , set @xmath ; if @xmath , set @xmath . Such
sequence always exists since @xmath . Furthermore, since @xmath and
@xmath are non-negative, it follows that @xmath and @xmath . Let @xmath
, @xmath and @xmath . Comparing the definitions of global and local
similarities, it is easy to see that @xmath . â

###### Corollary 3.5.4.

Let @xmath be a set, @xmath , @xmath and @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Let @xmath and @xmath . It can be easily verified from the definitions
(for example by induction) that for all @xmath , @xmath and therefore
for all @xmath , @xmath . If @xmath , the Lemma 3.5.3 implies @xmath . â

We now present the main result of this chapter which gives the
conditions for conversion of local similarity scores on a free semigroup
to a quasi-metric. We first introduce a necessary technical condition.

###### Theorem 3.5.5.

Let @xmath be a set and @xmath a strictly positive function @xmath . Let
@xmath be a metric on @xmath and let @xmath be the canonical homomorphic
extension of @xmath to the free semigroup @xmath given by @xmath for all
@xmath and @xmath . Suppose that for all @xmath ,

  -- -------- -- -------
     @xmath      (3.8)
  -- -------- -- -------

and

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

then @xmath defined by

  -- -------- --
     @xmath   
  -- -------- --

is a co-weightable quasi-metric with co-weight @xmath .

###### Proof.

Let @xmath . Since @xmath for any @xmath and since ( 3.8 ) implies that
@xmath is 1-Lipschitz, it follows that @xmath . It is also clear that
@xmath . If @xmath , there exists @xmath and @xmath such that

  -- -------- -- --------
     @xmath      (3.10)
  -- -------- -- --------

Since @xmath , there exist @xmath such that @xmath and the Equation 3.10
becomes

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath , @xmath and @xmath ( @xmath is 1-Lipschitz), it must
follow that @xmath , @xmath and

  -- -------- -- --------
     @xmath      (3.11)
  -- -------- -- --------

From @xmath and @xmath we conclude that @xmath , @xmath and @xmath while
( 3.9 ) implies that @xmath . Hence, since the maximum in the definition
of @xmath is invariant under permutation of @xmath and @xmath , it
follows that @xmath implies @xmath and @xmath and hence that @xmath .

Now let @xmath and suppose @xmath and @xmath for some @xmath , @xmath
and @xmath . Write out @xmath , @xmath where @xmath , @xmath , @xmath
and @xmath .

If @xmath and @xmath overlap, that is, if @xmath or @xmath , let @xmath
denote the whole overlapping fragment (for example, if @xmath , @xmath
). If @xmath and @xmath do not overlap or either @xmath or @xmath is
identity, let @xmath . Since @xmath and @xmath , by the triangle
inequality on @xmath and by ( 3.9 ), we have

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Since @xmath denotes the full extent of overlap of @xmath and @xmath ,
it follows that

  -- -------- --
     @xmath   
  -- -------- --

and therefore

  -- -------- -- --
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

The fact that @xmath is co-weightable with co-weight @xmath follows
straight from the definition of @xmath . â

###### Remark 3.5.6.

In general, the property ( 3.8 ) means that @xmath can be interpreted as
a distance from an abstract point @xmath with respect to a metric on the
set @xmath . Flood, in his PhD thesis [ 58 ] and a followup paper [ 59 ]
, introduced the term norm pair to denote the pair @xmath satisfying the
property ( 3.8 ). However, in the context of the Theorem 3.5.5 , it is
clear that @xmath . Hence, the property ( 3.8 ) can be reformulated to
state: for all @xmath , @xmath is given by a canonical homomorphic
extension of a strictly positive function on the set of generators.

The following Lemma 3.5.7 is a folklore result, see e.g. Floodâs paper [
59 ] , but we present the proof for the sake of completeness and because
we could not find a reference that would be readily available for the
reader.

###### Lemma 3.5.7 ([59]).

Let @xmath be a metric space and @xmath a positive 1-Lipschitz function.
Then, the map @xmath defined by

  -- -------- --
     @xmath   
  -- -------- --

is a metric.

###### Proof.

Let @xmath . Clearly @xmath and @xmath . Since @xmath is positive,
@xmath and hence @xmath . For the triangle inequality we consider four
cases. If @xmath and @xmath , @xmath by the triangle inequality of
@xmath . If @xmath and @xmath we have @xmath . In the case where @xmath
and @xmath the result follows in the same way. Finally, if @xmath and
@xmath , we have @xmath since @xmath is positive. â

###### Corollary 3.5.8.

Let @xmath be a set. Suppose @xmath is an increasing functions @xmath ,
@xmath and @xmath is a map satisfying the conditions of the Lemma 3.4.4
and being symmetric, that is @xmath for all @xmath . Let @xmath be the
local similarity with respect to @xmath and @xmath . Then, a function
@xmath given by

  -- -------- --
     @xmath   
  -- -------- --

is a co-weightable quasi-metric with co-weight @xmath (equivalently,
@xmath is a partial metric).

###### Proof.

Let @xmath be the global similarity with respect to @xmath and @xmath .
Clearly, @xmath is symmetric since @xmath is symmetric and @xmath . Let
@xmath for @xmath and let @xmath (Corollary 3.4.6 ). By the Theorem
3.4.7 , @xmath is a co-weighted quasi-metric with a co-weight @xmath and
therefore @xmath is a metric and @xmath is 1-Lipschitz with respect to
@xmath . By the Lemma 3.5.7 , @xmath gives a metric.

It is easy to see that for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

and hence, by the Corollary 3.5.4 ,

  -- -------- --
     @xmath   
  -- -------- --

Furthermore, @xmath since @xmath for all @xmath .

The main statement then follows from the Theorem 3.5.5 and the remark of
@xmath being a partial metric follows from the Theorem 2.6.15 . â

###### Remark 3.5.9.

An alternative treatment of the same problem is given in the Topology
Proc. paper by the thesis author. There however, a different definition
of an alignment is given and the statement of the main theorem
explicitly uses the properties of score matrices and gap penalties.
Theorem 3.5.5 is a more general statement of the same fact.

It is clear from the proof of the Theorem 3.5.5 that the partial order
@xmath associated to the quasi-metric @xmath of Corollary 3.5.8 is a
substring (factor) order:

  -- -------- --
     @xmath   
  -- -------- --

The set @xmath with @xmath forms a meet semilattice. However, in
general, @xmath is not invariant with respect to the concatenation or
meet operation. For example, let @xmath and for all @xmath set

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath and suppose @xmath is a global similarity with respect to
@xmath and @xmath . If @xmath , @xmath and @xmath , it is easy to verify
that @xmath , @xmath , @xmath and @xmath , and hence @xmath is not
invariant with respect to @xmath . On the other hand if @xmath , @xmath
and @xmath , we have @xmath while @xmath and therefore @xmath is not
invariant with respect to string concatenation.

### 3.6 Score Matrices

The main result from the previous section indicates that, at least under
some circumstances, free semigroups with local similarity measures can
be considered as partial metric spaces, or equivalently, as co-weighted
quasi-metric spaces. A consequence of the Theorem 2.6.15 of particular
significance for biological applications is the fact that the
transformation into quasi-metric preserves neighbourhoods with respect
to similarity scores.

Let @xmath and define for some @xmath

  -- -------- --
     @xmath   
  -- -------- --

that is, @xmath is the set of all points in @xmath whose local
similarity with @xmath is not less than @xmath . Retrieving points
belonging to such neighbourhoods from datasets is the principal aim of
similarity search, explored in detail in Chapter 5 . Corollary 3.5.8
implies that there exists a co-weightable quasi-metric @xmath with
co-weight @xmath such that @xmath (i.e. the neighbourhood system
consisting of @xmath for all @xmath and @xmath form a base for a
quasi-metrisable topology). Therefore, one can expect that existing and
newly developed indexing techniques for similarity search in
(weightable) quasi-metric spaces (see Chapter 5 ) can be used to
significantly speed-up sequence similarity searches without significant
sacrifice in accuracy. Furthermore, the result makes it worthwhile to
repeat the exploration of global geometry of proteins performed by
Linial, Linial, Tishby and Yona [ 126 ] , this time in the context of
quasi-metrics.

The current section explores the similarity measures (commonly called
score matrices for obvious reasons) on DNA and protein alphabets which
satisfy the Lemma 3.4.4 and which hence, with affine gap penalties, lead
to local similarities corresponding to quasi-metrics. In particular, the
most popular members of the BLOSUM [ 88 ] family of matrices satisfy all
the requirements of the Lemma 3.4.4 , unlike the members of the PAM
family [ 45 ] , which do not and which are therefore omitted from the
discussion here.

#### 3.6.1 DNA score matrices

The DNA alphabet consists of only 4 letters (nucleotides) and the
frequently used similarity measures on it are very simple. The common
feature of all general DNA matrices used in practice is that they are
symmetric and that self-similarities of all nucleotides are equal. The
consequence of this fact is that the distance @xmath resulting from the
transformation @xmath is always a metric and the co-weightable
quasi-metric arising from local similarity on DNA sequences has
co-weight proportional to the length of a sequence.

For example, the score matrix used by BLAST (more precisely, the blastn
program for search of DNA database with DNA query sequence) is given by

  -- -------- --
     @xmath   
  -- -------- --

More complex score matrices, mostly distance-based and used in
phylogenetics also exist.

#### 3.6.2 BLOSUM matrices

As the protein alphabet consists of 20 amino acids of markedly different
chemical properties and structural roles, it is to be expected that
similarity measures on amino acids involved in protein sequence
comparison are more complex. The BLOSUM family of matrices was
constructed by Steven and Jorja Henikoff in 1992 [ 88 ] who also showed
that one member of the family, the BLOSUM62 matrix, gave the best search
performance amongst all score matrices used at the time. For that
reason, BLOSUM62 matrix is the default matrix used by NCBI BLAST for
searches of protein databases.

The BLOSUM similarity scores are explicitly constructed as log-odds
ratios. Let @xmath be a (finite) set and let @xmath be a probability
measure on @xmath . The value of @xmath is called the background
frequency of @xmath . Let @xmath be a probability measure on @xmath .
The value of @xmath is called the target frequency of a match between
@xmath and @xmath , that is the likelihood that @xmath is aligned with
@xmath in related sequences. For unrelated sequences, we expect that the
probability of @xmath being aligned with @xmath would be @xmath . The
similarity score @xmath is defined (up to a scaling factor) by

  -- -------- --
     @xmath   
  -- -------- --

Thus, @xmath is positive if the target frequencies are greater than
background frequencies, @xmath if they are equal and negative if
background frequencies are greater. In this model, the condition (iv) of
the Lemma 3.4.4 (the triangle inequality of the corresponding
quasi-metric) is equivalent to

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath and can be interpreted as stating that a direct
substitution of one letter to another on each site in the sequence is
always preferred to two or more substitutions achieving the same
transformation. It should be noted that according to Altschul [ 5 ] ,
who studied the statistics of scores of ungapped local alignments, any
similarity score matrix can be interpreted as log-odds ratios (i.e.
target frequencies can be derived from similarity scores given the
background frequencies).

The target frequencies used to obtain the BLOSUM scores were derived
from multiple alignments. A multiple alignment between @xmath sequences
can be defined in the similar way as a pairwise alignment between two
sequences according to the Definition 3.3.12 : it is only necessary to
replace the sequence of pairs with a sequence of @xmath -tuples and to
adjust the remainder of the definition accordingly. The (ungapped)
multiple alignments of related sequences (also called blocks) used to
construct the BLOSUM similarities were obtained from the BLOCKS database
of protein motifs of Henikoff and Henikoff [ 89 ] .

In order to reduce the contribution of too closely related members of
blocks to target frequencies, members of blocks sharing at least @xmath
identity were clustered together and considered as one sequence (for a
block member to belong to a cluster, it was sufficient for it to share
@xmath identity with one member of the cluster), resulting in a family
of matrices. Thus, the matrix BLOSUM62 corresponds to @xmath (for
BLOSUMN, no clustering was performed). After clustering, the target
frequencies were obtained by counting the number of each pair of amino
acids in each column in each block having more than one cluster and
normalising by the total number of pairs. The background frequencies
were obtained from the amino acid composition of the clustered blocks
and log-odds ratios taken. The resulting score matrices are necessarily
symmetric since the pair @xmath cannot be distinguished from @xmath in
the multiple alignment.

Most BLOSUM matrices, when restricted to the standard amino acid
alphabet satisfy the Lemma 3.4.4 (Table 3.3 ). In fact, the first three
conditions are always satisfied and only the triangle inequality
presents problems. Where it is not satisfied, it is either in very small
number of cases or for small values of @xmath which correspond to
alignments of distantly related proteins and where it is to be expected
that a transformation from one amino acid to another can arise from more
than one substitution. However, it should be stressed that BLOSUM50 and
BLOSUM62, which are the most widely used score matrices for database
searches, do satisfy the Lemma 3.4.4 .

This observation leads to a conclusion that the ânear-metricâ of Linial,
Linial, Tishby and Yona [ 126 ] derived from local similarities based on
BLOSUM62 matrix and affine gap penalties by the formula @xmath is in
fact a true metric and that the rare instances where the triangle
inequality was observed to fail were solely due to non-standard letters
such as B,Z and X which represent sets of amino acids (for example X
stands for any amino acid) and whose similarity scores were derived by
averaging over all represented letters.

### 3.7 Profiles

#### 3.7.1 Position specific score matrices

From a biological point of view, profiles are generalised sequences.
They were originally introduced by Gribskov, McLachlan, and Eisenberg [
78 ] in order to model the situations where similarity measures based on
score matrices do not retrieve all biologically relevant neighbours. As
mentioned in Chapter 1 , the function of a protein depends on its
structure which in turn depends on its amino acid sequence. The
structure space is smaller than the sequence space [ 142 , 95 ] and
hence similar structures can arise from quite distantly related (in the
evolutionary sense) sequences that do not share sufficiently high
similarity to be detected using score matrix based methods. However,
even significantly different structurally related sequences often
contain a few sites, usually associated with a particular biological
role, that are strongly conserved across species. Hence the idea of
using position specific scores to model protein families and find their
new members.

In the sense of Gribskov, McLachlan, and Eisenberg, the term profile can
be used interchangibly with a term Position Specific Score Matrix or
PSSM . A PSSM is an @xmath -by- @xmath matrix where @xmath is an
appropriate finite alphabet (most often the set of 20 standard amino
acids used in proteins â in fact we will always assume this is the case
and use âamino acidâ and âletterâ interchangeably). For any PSSM @xmath
, an entry @xmath where @xmath and @xmath gives the score of the letter
@xmath in position @xmath . Obviously, entries of a PSSM can come from
similarity score matrices, that is, from similarities on @xmath . Let
@xmath and let @xmath be a similarity score function (or matrix since
@xmath is assumed finite). Then, one can produce a PSSM by setting

  -- -------- --
     @xmath   
  -- -------- --

Of course, in this case, the PSSM is really not âposition specificâ: the
scores for the same amino acid at different positions are the same. To
summarise, PSSMs are generalisations of similarity score matrices.

The score of a sequence with respect to a PSSM is calculated very
similarly to the usual similarity scores. Let @xmath and let @xmath be
an @xmath -by- @xmath PSSM. If @xmath , one can write the score @xmath
as

  -- -------- --
     @xmath   
  -- -------- --

that is, as an @xmath -type sum. On the other hand, if @xmath and gapped
local scores are desired, a modified Smith-Waterman algorithm can be
used.

Let @xmath be positive gap penalty functions @xmath and let @xmath be an
@xmath -by- @xmath matrix indexed from @xmath . Set @xmath and for all
@xmath and @xmath

  -- -------- --
     @xmath   
  -- -------- --

The local similarity score of @xmath with respect to the PSSM @xmath ,
denoted @xmath is given by @xmath . Global similarities can be produced
using an appropriate modification of the Needleman-Wunsch algorithm.

#### 3.7.2 Profiles as distributions

While we have seen that profiles may come from similarity score
matrices, they are usually produced from collections of related
sequences, that is, (putative) members of a protein family. Given a
(finite) set of sequences Â¹ Â¹ 1 The index is in superscript rather than
subscript in order to distinguish a sequence entry in @xmath ( @xmath )
and a residue of @xmath at position @xmath ( @xmath ). @xmath , we first
produce a multiple alignment of all of them. For the sake of simplicity,
assume that the multiple alignment is ungapped, that is, only letters
are present Â² Â² 2 Profile hidden Markov models [ 53 ] further generalise
the profiles by modelling gaps as well as âmatchesâ. , and that all
sequences have the same length. Clearly, the relative frequencies of
letters at each position @xmath define a probability distribution @xmath
where @xmath is the probability of an amino acid @xmath occurring at the
position @xmath . Given a background amino acid distribution @xmath ,
where @xmath is the overall relative frequency of @xmath , we can define
a PSSM as a matrix of log odds ratios

  -- -------- -- --------
     @xmath      (3.12)
  -- -------- -- --------

exactly mirroring the definition of the BLOSUM matrices in Subsection
3.6.2 .

This leads an alternative definition of profiles, used for example by
Yona and Levitt [ 218 ] . From this point of view, a profile is a
sequence of probability distributions on @xmath , that is, a member of a
free semigroup generated by @xmath , the set of all probability
distributions over @xmath . The two definitions are in fact closely
related since, given a background distribution @xmath , every sequence
of distributions can be converted into a PSSM using the Equation ( 3.12
), while it is also clear [ 5 , 105 ] that scores at each position can
be, after scaling, converted to probabilities. Note that the scaling
factors need not be the same for each position and thus each scaling
factor can be treated as a âweightâ for the particular position. The
log-odds scores and the scaling factors have information-theoretic
interpretations [ 5 , 105 , 52 ] that we will not discuss here.

The definition of profiles as members of @xmath opens interesting
possibilities for introducing quasi-metrics for profile-profile
comparison. Suppose we have a quasi-metric and a positive function on
@xmath . Then, we can extend them to obtain a weighted quasi-metric on
@xmath using dynamic programming and the Theorem 3.5.5 . The similarity
scores and distances thus obtained would have a similar interpretation
to the scores obtained from score matrices. Yona and Levitt [ 218 ]
produced a profile-profile comparison tool by using the same principles,
that is, by extending a similarity score function on @xmath to @xmath
using dynamic programming. However, it is unclear from their
presentation if their score function can induce a quasi-metric.

## Chapter 4 Quasi-metric Spaces with Measure

The main object of this chapter study is the pq-space , the quasi-metric
space with Borel probability measure (or probability quasi-metric space)
which we introduce here for the first time. As most of the theory of the
measure concentration was developed within the framework of a metric
space with measure, we will throughout this chapter state the
definitions and results for the metric case first and then give the
corresponding statements for the quasi-metric case. The proofs will be
given only for the quasi-metric case (as they include the metric case)
and where they are not available elsewhere. For an extensive review of
the theory for the metric case the reader is referred to the excellent
monograph by Ledoux [ 121 ] , Chapter @xmath of the well-known Gromovâs
book [ 79 ] as well as the book by Milman and Schechtman [ 138 ] which
mainly concentrates on the normed spaces.

We aim to explore the phenomenon of concentration of measure in high
dimensional structures in the case where the underlying structure is a
quasi-metric space with measure. Many results and proofs can be
transferred almost verbatim from the metric case. However, we also
develop new results which have no metric analogues.

### 4.1 Basic Measure Theory

Let @xmath be a set. A collection @xmath , of subsets of @xmath , is
called a @xmath -algebra if it satisfies

1.  @xmath ,

2.  if @xmath then @xmath ,

3.  if @xmath with @xmath for all @xmath , then @xmath .

Let @xmath be a collection of subsets of @xmath . The @xmath -algebra
generated by @xmath , denoted @xmath , is the smallest @xmath -algebra
containing @xmath (one @xmath -algebra containing @xmath always exists:
the power set @xmath ).

A function @xmath such that @xmath is a measure on @xmath if it is
additive, that is if

  -- -------- --
     @xmath   
  -- -------- --

for all pairwise disjoint sets @xmath . A measure space is a triple
@xmath where @xmath is a set, @xmath is a @xmath -algebra and @xmath is
a measure. A probability space is a measure space with total measure
@xmath .

Let @xmath be a measure space. The measure @xmath is called @xmath
-finite if there exists a countable collection of sets @xmath such that
@xmath and @xmath for each @xmath .

The Borel @xmath -algebra on a topological space @xmath is the smallest
@xmath -algebra containing @xmath . The existence and uniqueness of the
Borel algebra is shown by noting that the intersection of all @xmath
-algebras containing @xmath is itself a @xmath -algebra, so this
intersection is the Borel algebra. The elements of the Borel @xmath
-algebra are called Borel sets while the measures on @xmath -algebras
are called Borel measures .

The Borel @xmath -algebra may alternatively and equivalently be defined
as the smallest @xmath -algebra which contains all the closed subsets of
@xmath . A subset of @xmath is a Borel set if and only if it can be
obtained from open (or closed) sets by using the set operations union,
intersection and complement in countable number, more exactly via
transfinite recursion in countable ordinals.

### 4.2 pq-spaces

###### Definition 4.2.1.

A topological space @xmath is called Polish if it is separable and
metrisable by means of a complete metric. @xmath

We recall the definition of a metric space with measure, as defined in [
81 ] .

###### Definition 4.2.2 ([81, 79, 80]).

An mm-space is a triple @xmath where @xmath is a Polish metric space and
@xmath a @xmath -finite Borel measure on @xmath .

An mm-space where @xmath is called a pm-space . @xmath

We shall mostly be concerned with mm-spaces equipped with finite
measures and will assume wherever possible that the measure has been
normalised so that they become pm-spaces.

In order to define an analogue for a quasi-metric space @xmath we
observe that it is not sufficient to use the Borel @xmath -algebra
generated by @xmath since we want to have the open and closed sets with
respect to both @xmath and @xmath measurable. Hence, we use the Borel
@xmath -algebra generated by @xmath . It is easy to see that this
structure is equivalent to the Borel @xmath -algebra generated by @xmath
, the topology of the associated metric, by observing that @xmath
(Remark 2.2.2 ).

In order to make our definition fully analogous to the the definition of
the mm-space, we additionally require that our quasi-metric be
bicomplete, that is, that its associated metric be complete.

###### Definition 4.2.3.

Let @xmath be a bicomplete separable quasi-metric space, and @xmath a
@xmath -finite measure over @xmath , a Borel @xmath -algebra of
measurable sets generated by @xmath where @xmath is the associated
metric to @xmath . We call the triple @xmath an mq-space . If in
addition @xmath we call such triple a pq-space .

Furthermore, we call the mq-space @xmath the conjugate or dual mq-space
to @xmath and the mm-space @xmath the associated mm-space to @xmath .
@xmath

Henceforth, we shall always use the symbol @xmath in the context of
mq-spaces to denote the underlying Borel @xmath -algebra.

###### Remark 4.2.4.

The fact that @xmath , the associated mm-space to @xmath , is an
mm-space indeed is a direct consequence of having the Borel @xmath
-algebra of measurable sets generated by @xmath .

In this work we shall only consider pq-spaces, that is, the quasi-metric
spaces with finite measure. The definition of an mq-space was introduced
in order to correspond to the definition of an mm-space as given by
Gromov [ 79 , 80 ] .

In order to illustrate one possible way of interaction between a
quasi-metric and measure we give another example of Lipschitz functions.

###### Lemma 4.2.5.

Let @xmath be a pq-space and @xmath . The function @xmath , where @xmath
, is left 1-Lipschitz, while @xmath , where @xmath , is right
1-Lipschitz.

###### Proof.

Since @xmath (Fig. 4.1 ), one has

  -- -------- --
     @xmath   
  -- -------- --

and it follows that @xmath and therefore @xmath . The second statement
follows in a similar manner. â

### 4.3 Concentration Functions

Recall the definition of the concentration function for an mm-space.

###### Definition 4.3.1.

Let @xmath be an mm-space and @xmath the Borel @xmath -algebra of @xmath
-measurable sets. The concentration function @xmath , also denoted
@xmath , is a function @xmath such that @xmath and for all @xmath

  -- -------- --
     @xmath   
  -- -------- --

@xmath

The concentration function measures the maximum size of a complement
(âcapâ) of a neighbourhood of a Borel set of a measure not less than
@xmath . In a sense to be made more precise later, a space is
âconcentratedâ if its concentration function is extremely small for
small @xmath .

As before with asymmetric structures, we introduce two concentration
functions on a pq-space, left and right.

###### Definition 4.3.2.

Let @xmath be a pq-space and @xmath the Borel @xmath -algebra of @xmath
-measurable sets. The left concentration function @xmath , also denoted
@xmath , is a map @xmath such that @xmath and for all @xmath

  -- -------- --
     @xmath   
  -- -------- --

Similarly, the right concentration function @xmath , also denoted @xmath
, is a map @xmath such that @xmath and for all @xmath

  -- -------- --
     @xmath   
  -- -------- --

@xmath

###### Remark 4.3.3.

For an mm-space @xmath , @xmath and @xmath are equal and they coincide
with the usual concentration function @xmath . It is also easy to
observe that for a pq-space @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

The concentration functions @xmath and @xmath respectively measure the
maximum size of the complement to any left and right neighbourhood of a
Borel set of a measure not less than @xmath (Fig. 4.2 ).

###### Lemma 4.3.4.

For any pq-space @xmath , the concentration functions @xmath and @xmath
are decreasing and converge to @xmath as @xmath . Furthermore, if @xmath
is finite, then for all @xmath , @xmath .

###### Proof.

We prove the statement for @xmath . It is obvious that @xmath is bounded
below by @xmath and decreasing since @xmath and hence @xmath for any
Borel set @xmath and @xmath . Thus the limit exists and is non-negative
and we now show that @xmath .

Take any @xmath . We need to show that there is some @xmath such that
for all @xmath and for any Borel set @xmath such that @xmath we have
@xmath (this is trivially true for @xmath ). Take any @xmath . We will
show that there exist @xmath such that for all @xmath , @xmath . Indeed,
taking the open balls @xmath , @xmath with respect to the associated
metric @xmath we have

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

by @xmath -additivity of measure. Thus there is some @xmath such that
for all @xmath , @xmath . Now take any Borel set @xmath of measure
greater than @xmath . @xmath must intersect @xmath (Figure 4.3 ) because
if it would not, we would have @xmath leading to a contradiction. It now
clear that for any @xmath we have @xmath . Indeed, let @xmath and @xmath
. Then by the triangle inequality

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Therefore, for any @xmath , @xmath as required. It is obvious that the
same proof would work for @xmath by substituting @xmath by @xmath above.

It is also clear that if @xmath , then for any @xmath and any @xmath ,
@xmath and hence @xmath . â

The following lemmas show some relations between the various alpha
functions.

###### Lemma 4.3.5.

For any pq-space @xmath , for each @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Let @xmath be such that @xmath and let @xmath . Using @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

and it follows that @xmath .

For the second inequality, use the fact that @xmath , and thus @xmath ,
implying

  -- -- --
        
  -- -- --

â

It is easy to see that the above inequalities from the Lemma 4.3.5 are
strict. Consider the following example.

###### Example 4.3.6.

Let @xmath where @xmath , @xmath , @xmath and @xmath . Set an additive
measure in the following way: @xmath and @xmath (Figure 4.4 ). It is
clear that @xmath is a pq-space and that

  -- -------- --
     @xmath   
  -- -------- --

On the other hand

  -- -------- --
     @xmath   
  -- -------- --

Hence for @xmath we have @xmath .

The phenomenon of concentration of measure on high-dimensional
structures refers to the observation that in many metric spaces with
measure which are, intuitively, âhigh dimensionalâ, the concentration
function decreases very sharply, that is, an @xmath -neighbourhood of
any not vanishingly small set, even for very small @xmath , covers (in
terms of the probability measure) nearly the whole space. Examples are
numerous and come from many diverse branches of mathematics [ 135 , 81 ,
4 , 138 , 79 , 155 , 185 ] . Here we take a âhigh dimensionalâ pq-space
to be a pq-space where both @xmath and @xmath decrease sharply.

### 4.4 Deviation Inequalities

###### Definition 4.4.1.

Let @xmath be a probability space and @xmath a measurable real-valued
function on @xmath . A value @xmath is a median or LÃ©vy mean of @xmath
for @xmath if

  -- -------- --
     @xmath   
  -- -------- --

@xmath

A median need not be unique but it always exists. The following lemmas
are generalisations of the results for mm-spaces.

###### Lemma 4.4.2.

Let @xmath be a pq-space, with left and right concentration functions
@xmath and @xmath respectively and @xmath a left 1-Lipschitz function on
@xmath with a median @xmath . Then for any @xmath

  -- -------- -------- --
     @xmath   @xmath   
     @xmath            
  -- -------- -------- --

Conversely, if for some non-negative functions @xmath and @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
     @xmath            
  -- -------- -------- --

for every left 1-Lipschitz function @xmath with median @xmath and every
@xmath , then @xmath and @xmath .

###### Proof.

Set @xmath . Take any @xmath such that @xmath . Then, for any @xmath ,
@xmath and hence @xmath , implying @xmath . Therefore, @xmath .

Now set @xmath . Take any @xmath such that @xmath . Then, for any @xmath
, @xmath and hence @xmath , implying @xmath . Thus, @xmath .

The converse is equivalent to finding for each Borel set @xmath such
that @xmath , left 1-Lipschitz functions @xmath and @xmath with medians
@xmath and @xmath respectively, such that @xmath and @xmath .

Let @xmath be such a set such that @xmath and set for each @xmath ,
@xmath and @xmath . It is easy to see that both @xmath and @xmath are
left 1-Lipschitz and that @xmath . If @xmath , we have @xmath and thus
@xmath . Similarly, if @xmath , we have @xmath implying @xmath and the
result follows. â

Hence, we can state the alternative definitions of @xmath and @xmath :

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

Similar results can be easily obtained for the right 1-Lipschitz
functions by remembering that if @xmath is a right 1-Lipschitz, @xmath
is left 1-Lipschitz (Lemma 2.4.3 ). It is also straightforward to
observe that the absolute value of deviation of a 1-Lipschitz function
from a median thus depends on both @xmath and @xmath .

###### Corollary 4.4.3.

For any pq-space @xmath , a left 1-Lipschitz function @xmath with a
median @xmath and @xmath

  -- -------- --
     @xmath   
  -- -------- --

This result reduces to the well-known inequality @xmath when @xmath is a
metric. Deviations between the values of a left 1-Lipschitz function at
any two points are also bound by both concentration functions.

###### Lemma 4.4.4.

Let @xmath be a pq-space and @xmath a left (or right) 1-Lipschitz
function. Then

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

â

### 4.5 LÃ©vy Families

###### Definition 4.5.1.

A sequence of pq-spaces @xmath is called left LÃ©vy family if the left
concentration functions @xmath converge to @xmath pointwise, that is

  -- -------- --
     @xmath   
  -- -------- --

Similarly, a sequence of pq-spaces @xmath is called right LÃ©vy family if
the right concentration functions @xmath converge to @xmath pointwise,
that is

  -- -------- --
     @xmath   
  -- -------- --

A sequence which is both left and right LÃ©vy family will be called a
LÃ©vy family. Furthermore, if for some constants @xmath one has @xmath ,
such sequence is called normal LÃ©vy family . @xmath

It is a straightforward corollary of Lemma 4.3.5 that a sequence of
pq-spaces @xmath is a LÃ©vy family if and only if the sequence of
associated mm-spaces @xmath is a LÃ©vy family.

To illustrate existence of sequences of pq-spaces which are right but
not left LÃ©vy families consider the following example.

###### Example 4.5.2.

Let @xmath with @xmath and @xmath . Set @xmath and @xmath where @xmath
.(Fig. 4.5 ).

It is clear that

  -- -------- --
     @xmath   
  -- -------- --

Hence, @xmath converges to @xmath pointwise while @xmath does not. In
this case @xmath .

Examples of LÃ©vy families of mm-spaces abound in many diverse areas of
mathematics. We only mention a few.

###### Example 4.5.3 (Maurey [135]).

The sequence @xmath where @xmath is the group of permutations of rank
@xmath , @xmath is the normalised Hamming distance given by

  -- -------- --
     @xmath   
  -- -------- --

and @xmath is the normalised counting measure where

  -- -------- --
     @xmath   
  -- -------- --

forms a normal LÃ©vy family with the concentration functions satisfying

  -- -------- --
     @xmath   
  -- -------- --

###### Example 4.5.4 (LÃ©vy [123]).

The family of spheres @xmath with the geodesic metric and the rotation
invariant measure forms a normal LÃ©vy family where

  -- -------- --
     @xmath   
  -- -------- --

###### Example 4.5.5 (Gromov and Milman [81]).

The special orthogonal group @xmath consists of all orthogonal @xmath
matrices having the determinant @xmath . The family of these groups with
the geodesic metric and the normalised Haar measure forms a normal LÃ©vy
family where

  -- -------- --
     @xmath   
  -- -------- --

The hamming cube, discussed in Subsection 4.7.1 provides another example
(Proposition 4.7.4 ).

### 4.6 High dimensional pq-spaces are very close to mm-spaces

Most of the above concepts and results are generalisations of mm-space
results. However, we now develop some results which are trivial in the
case of mm-spaces. The main result is that, if both left and right
concentration functions drop off sharply, the asymmetry at each pair of
point is also very small and the quasi-metric is very close to a metric.

###### Definition 4.6.1.

For a quasi-metric space @xmath , the asymmetry is a map @xmath defined
by @xmath . @xmath

Obviously, @xmath on a metric space. However, @xmath is also close to
@xmath for high dimensional spaces, that is, those pq-spaces for which
both @xmath and @xmath decrease sharply near zero.

###### Theorem 4.6.2.

Let @xmath be a pq-space. For any @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Fix @xmath and set for each @xmath , @xmath . It is clear that @xmath is
a sum of two left 1-Lipschitz maps and therefore left 2-Lipschitz.
Furthermore, zero is its median since there is a measure-preserving
bijection @xmath which maps the set @xmath onto the set @xmath . By the
Lemma 4.4.2 , @xmath . Now, using Fubiniâs theorem,

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

â

Thus, any pq-space where both @xmath and @xmath (equivalently, by the
Lemma 4.3.5 , @xmath ) sharply decrease are, apart from a set of very
small size, very close to an mm-space.

If we restrict ourselves to longer ranges, that is, bound the distances
@xmath from below, then more precise bounds for the difference @xmath
can be obtained.

###### Corollary 4.6.3.

Let @xmath be a pq-space and @xmath . Then, for any pair @xmath such
that @xmath , apart from a set of ( @xmath ) measure at most @xmath ,
the values @xmath and @xmath differ by a factor of less than @xmath .
More precisely,

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

By the previous theorem, for any @xmath , apart from a set of measure at
most @xmath , the values of @xmath and @xmath differ by less than @xmath
. The result now follows by rearrangement of the inequality @xmath .
Indeed, if @xmath , we have @xmath . If @xmath , then @xmath . â

### 4.7 Product Spaces

#### 4.7.1 Hamming cube

###### Definition 4.7.1.

Let @xmath and @xmath . The collection of all binary strings of length
@xmath , denoted @xmath is called the Hamming cube . @xmath

###### Definition 4.7.2.

The Hamming distance (metric) for any two strings @xmath and @xmath is
given by

  -- -------- --
     @xmath   
  -- -------- --

The normalised Hamming distance @xmath is given by

  -- -------- --
     @xmath   
  -- -------- --

@xmath

###### Definition 4.7.3.

The normalised counting measure @xmath , of any subset @xmath of a
Hamming cube @xmath is given by

  -- -------- --
     @xmath   
  -- -------- --

@xmath

It is easy to see that the above definitions indeed give a set with a
metric and a measure and that @xmath is a pm-space. One may wish to
consider @xmath as a product space with @xmath as an @xmath -type sum of
discrete metrics on @xmath and @xmath an @xmath -product of @xmath ,
where @xmath .

The following bounds to the concentration function on the Hamming cube
were stated in the book by Milman and Schechtman [ 138 ] (Section 6.2):

###### Proposition 4.7.4.

For any Hamming cube @xmath with the normalised Hamming distance @xmath
and the normalised counting measure @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

â

##### Law of Large Numbers

Hence a sequence @xmath is a normal LÃ©vy family. An easy consequence of
the Proposition 4.7.4 is the well-known Law of large numbers .

###### Proposition 4.7.5.

Let @xmath be an independent sequence of Bernoulli random variables (
@xmath ). Then for all @xmath

  -- -------- --
     @xmath   
  -- -------- --

Equivalently, if @xmath is the number of ones in the sequence @xmath
then

  -- -------- --
     @xmath   
  -- -------- --

â

##### Asymmetric Hamming Cube

We will now produce a pq-space based on the Hamming cube by replacing
@xmath by a quasi-metric. The simplest way is to define @xmath by @xmath
and @xmath and set @xmath . The triple @xmath forms a pq-space. It would
not add much to generality to replace @xmath by a product of copies of a
different probability measure on @xmath . One immediately observes that
@xmath is also a normal LÃ©vy family.

Take two strings @xmath and @xmath and let us consider the asymmetry
@xmath . It is easy to see that @xmath takes value between @xmath and
@xmath , being equal to the quantity

  -- -------- --
     @xmath   
  -- -------- --

Since our asymmetric Hamming cube is a product space, we can consider
for each @xmath the value @xmath as a random variable taking values of
@xmath , @xmath and @xmath with @xmath and @xmath so that @xmath . Now,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

This is obviously the same bound as would be obtain by application of
the Theorem 4.6.2 and the Proposition 4.7.4 .

#### 4.7.2 General setting

Product spaces assume great importance in the present investigation for
two reasons. Firstly, the theory of concentration there is quite
extensively developed, mostly due to the work of Michel Talagrand [ 183
, 184 ] . Many of his results are quite general, that is, not restricted
to the products of metric spaces, and can be applied directly to the
quasi-metric spaces. Secondly, the space of protein fragments, the main
biological example of this thesis, can be modelled as a product space,
although the measure on it is definitely not a product measure. However,
the bounds on the concentration function thus obtained can be used as a
worst case estimate which can be useful in indexing applications.

It should also be noted that the generality of the results means that
they can even be applied to the similarity scores that do not transform
into quasi-metrics (i.e. which do not satisfy the triangle inequality).

Talagrand [ 183 ] obtained the exponential bounds for product spaces
endowed with a non-negative âpenaltyâ function generalising the distance
between two points. Penalties form a much wider class of distances than
quasi-metrics but provide ready bounds for the concentration functions.

We will outline here just one of results from [ 183 ] and apply it to
obtain bounds for concentration functions in product quasi-metric spaces
with product measure.

Consider a probability space @xmath and the product @xmath where the
product probability @xmath will be denoted by @xmath . Consider a
function @xmath which will measure the distance between a set and a
point in @xmath . More specifically, given a function @xmath such that
@xmath for all @xmath , set

  -- -------- --
     @xmath   
  -- -------- --

###### Theorem 4.7.6 ([183]).

Assume that

  -- -------- --
     @xmath   
  -- -------- --

is finite and set

  -- -------- --
     @xmath   
  -- -------- --

Then

  -- -------- --
     @xmath   
  -- -------- --

â

If we take as @xmath above @xmath , a quasi-metric on @xmath , and endow
@xmath with the @xmath -type quasi-metric @xmath so that @xmath , @xmath
, we have @xmath and @xmath . Hence, the following corollary is
obtained.

###### Corollary 4.7.7.

Suppose @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

â

Note that the bound applies to @xmath and hence to both @xmath and
@xmath because the norms referred to above are symmetric.

An advantage of an inequality of this sort in applications to the
biological sequences is that @xmath can be easily calculated for a
finite alphabet @xmath . On the other hand, it is remarked in [ 183 ]
that the constants above are not sharp.

###### Example 4.7.8.

Consider the pq-space @xmath where @xmath is the amino acid alphabet,
@xmath is the @xmath -quasi-metric extended from the quasi-metric @xmath
on @xmath and @xmath is a probability measure on amino acids. Then, the
Corollary 4.7.7 provides explicit bounds for the concentration functions
on @xmath .

In particular, if @xmath is the quasi-metric obtained from the BLOSUM62
similarity scores and @xmath is obtained from the amino acid counts from
a large protein dataset (they differ very little if the dataset is
general enough; specifically take the counts from the NCBI nr dataset
described in detail in Subsection 6.1.1 ), we have @xmath and @xmath .

While the above would give an explicit formula for the bounds of the
concentration functions on the space of peptide fragments @xmath under
the assumption that the measure on @xmath is a product measure, one
would ultimately wish to estimate the âtrueâ concentration functions on
@xmath â this is something we do not yet know how to do. Indeed, were it
to be attempted directly from the definition, by choosing a subset and
computing the measure of its @xmath -neighbourhood one at a time, the
computational complexity would be exponential in the size of the set.

## Chapter 5 Indexing Schemes for Similarity Search

### 5.1 Introduction

It would not be exaggerated to state that database search is one of the
pillars of the modern information society. Datasets come in many forms,
from simple flat-files to relational databases. Classical databases are
structured around data points ( records ) with keys which may contain
numeric, textual or categorical data, allowing comparison and search
queries. The most fundamental type of search queries is exact match â
all datapoints matching a given key are retrieved. If the type of the
key is numeric, it is possible to perform range queries where the set of
points within a given range of the query key is retrieved. If the key is
a string, a partial match query can be asked: it retrieved those
datapoints whose keys match the query key in part (for example, by
sharing a common prefix). In all cases an additional structure such as
for example linear order is imposed on data keys to facilitate retrieval
of queries.

Sometimes it is possible to assume that datapoints belong to an @xmath
-dimensional vector space with the coordinates corresponding to their
features . In this case, exact matches are often not sufficient: unless
the underlying space is strictly limited in some way, the probability
that there will be a datapoint exactly matching a query is close to
@xmath . On the other hand, before proceeding with range queries, it is
necessary to define a similarity or proximity measure used to retrieve
queries, a function of two variables that on input of the query and some
other point returns their similarity (degree to which the points are
similar) or distance (in this case it is commonly called a dissimilarity
measure ). For @xmath -dimensional vector spaces the obvious choice of a
dissimilarity measure is an @xmath or Minkowski metric where @xmath or
its weighted modifications where each coordinate is assigned a weight.

The approach of retrieving points according to a similarity measure can
be applied to datasets which cannot be easily represented as vector
spaces, for example sets of words from a finite alphabet, colour images,
time series, audio and video streams etc. Such sets are often large,
complex (both in the structure of data and the underlying similarity
measure) and fast growing. One well known example is GenBank [ 15 ] ,
the database of all publicly available DNA sequences (Figure 5.1 ). In
this case, the size of queries is much smaller than database size and it
is imperative to attempt to avoid scanning the whole dataset in order to
retrieve a very small part of it.

Loosely speaking, indexing denotes introduction of a structure, called
indexing scheme , to a dataset. This structure supports an access method
for fast retrieval of queries by enabling elimination of those parts of
the dataset which can be certified not to contain any points of the
query. There are numerous examples of indexing schemes and access
methods, the best known being the B-Tree [ 42 ] from the classical
database theory. However, in order to design new and efficient indexing
schemes, a fully developed mathematical paradigm of indexability that
would incorporate the existing structures and possess a predictive power
is needed.

The master concept was introduced in the influential paper by
Hellerstein, Koutsoupias and Papadimitriou [ 87 ] : a workload , @xmath
, is a triple consisting of a search domain @xmath , a dataset @xmath ,
and a set of queries, @xmath . An indexing scheme according to [ 87 ] is
just a collection of blocks covering @xmath . While this concept is
fully adequate for many aspects of theory, we believe that analysis of
indexing schemes for similarity search, which is the aim of this
chapter, with its strong geometric flavour, requires a more structured
approach. Hence, a concept of an indexing scheme as a system of blocks
equipped with a tree-like search structure and decision functions at
each step is put forward. This concept is a result of analysis of
numerous concrete existing approaches to indexing. The notion of a
consistent indexing scheme, guaranteeing full retrieval of all queries,
is stressed.

The notion of a reduction of one workload to another, allowing creation
of new access methods from the existing ones is also suggested. The
final sections of the present chapter discuss how geometry of high
dimensions (asymptotic geometric analysis) may offer a constructive
insight into the performance of indexing schemes and, in particular, in
the nature of the curse of dimensionality.

Apart from [ 87 ] , this work was influenced by the excellent reviews of
similarity search in metric spaces by Chavez, Navarro, Baeza-Yates and
Marroquin [ 36 ] and by Hjaltason and Samet [ 93 ] . While [ 93 ] is
mostly concerned with detailed descriptions of each of the existing
methods, the main focus of the [ 36 ] paper is on classification of
indexing schemes and analysis of their performance, with particular
emphasis on the curse of dimensionality . Another good survey (in
Italian) is Licia Capraâs Masters thesis [ 33 ] . The conceptual
framework and techniques for explaining the curse of dimensionality
comes from the works of Pestov [ 154 , 152 ] and this chapter can be
thought of as an extension of the results presented therein. The paper
of Ciaccia and Patella [ 39 ] , while focusing only on one particular
scheme, gives an important insight into cost models for similarity
search.

It should be noted that while the fundamental building blocks -
similarity measures, data distributions, hierarchical tree index
structures, and so forth - are in plain view, the only way they can be
assembled together is by examining concrete datasets of importance and
taking one step at a time. Generally, this thesis shares the philosophy
espoused by Papadimitriou in [ 150 ] that theoretical developments and
massive amounts of computational work must proceed in parallel. Indeed,
it is our general impression that indexing schemes which are able to
take into account the underlying structure of a domain often perform
better than âgenericâ schemes.

As noted earlier, the main motivation comes from sequence-based biology,
where similarity search already occupies a very prominent place and
where high-speed access methods for biological sequence databases will
be vital both for developing large-scale data mining projects [ 73 ] and
for testing the nascent mathematical conceptual models [ 34 ] .

As seen in Chapter 3 , the similarity measures used for biological
sequence comparison often correspond to partial metrics or
quasi-metrics. For that reason, a particular emphasis is placed on
indexing schemes for quasi-metric workloads, which, while frequently
mentioned as generalisations of metric workloads (e.g. in [ 39 ] ), have
been so far been neglected as far the practical indexing schemes are
concerned. The main technical result of this Chapter, the Theorem 5.7.11
about the performance of range searches, is stated and proved in terms
of the quasi-metric workloads.

An indexing scheme for short peptide fragments called FSIndex
illustrates many of the concepts introduced in the present chapter, and
is the main subject of the next chapter.

### 5.2 Basic Concepts

#### 5.2.1 Workloads

###### Definition 5.2.1 ([87, 154, 157]).

A workload is a triple @xmath , where @xmath is a set called the domain,
@xmath is a finite subset of the domain ( dataset , or instance ), and
@xmath is the set of queries, that is, some specified subsets of @xmath
.

(Here, as in the Definition 2.2.13 , @xmath denotes the set of all
subsets of @xmath including @xmath , the empty set.)

Answering a query @xmath means listing all data points @xmath . @xmath

The concept of workload was introduced in [ 87 ] and the original
definition is slightly extended here by having the queries as subsets of
@xmath rather than @xmath . This is however an important distinction
because it is often not directly known what the dataset contains and we
may want to ask âquestionsâ (queries) independently of possible
âanswersâ (dataset points). For that reason empty queries are also
allowed â some processing is usually required in order to decide whether
a query is in fact empty. There are also technical reasons which are
discussed in Subsection 5.7.2 .

The domain @xmath can be a very large, even infinite set. It would be
tempting at this stage to turn the domain with the set of queries into a
topological space by requiring @xmath to satisfy the axioms of topology
but there is no practical use for that. In the later sections, when we
define similarity queries, the queries will become neighbourhoods of
points according to some similarity measure (say a metric) and would
thus form a base of a topology over @xmath . Even in that case, there is
no need to require that finite intersections or infinite unions of
families of queries are queries themselves. Indeed, since the dataset
@xmath is finite, the finite unions would be sufficient for any
practical purpose. The dataset itself with the topology induced from the
domain would be topologically discrete and zero dimensional and thus
trivial from the topological point of view.

Examples of workloads abound in database theory - we here focus on the
most abstract versions that will be important further on.

###### Example 5.2.2.

The trivial workload : @xmath is a one-element set, with a sole possible
non-empty query, @xmath .

###### Example 5.2.3.

Let @xmath be a dataset. The exact match queries for @xmath are
singletons, that is, sets @xmath , @xmath .

###### Example 5.2.4.

Let @xmath , @xmath = @xmath and @xmath be a dataset. Define the set of
queries by @xmath where @xmath . This is the most common type of a query
in classical database theory where @xmath is a table with a key @xmath
and a query @xmath retrieves all elements of @xmath whose key is equal
to @xmath .

Here is the first way to create new workloads: by combining them as
disjoint sums.

###### Example 5.2.5.

Let @xmath be a finite collection of workloads. Their disjoint sum is a
workload @xmath , whose domain is the disjoint union @xmath , the
dataset is the disjoint union @xmath , and the queries are of the form
@xmath , where @xmath , @xmath .

###### Example 5.2.6.

Let @xmath be a workload, and let @xmath . The restriction of @xmath to
@xmath is a workload @xmath with domain @xmath , dataset @xmath and the
set @xmath of queries of the form @xmath , @xmath .

The main objects of this chapter are similarity workloads where the
queries are generated by similarity (or proximity) measures .

#### 5.2.2 Similarity queries

In general, a similarity measure [ 41 , 40 , 93 ] on a set @xmath is a
function of two variables @xmath , often subject to additional
restrictions. In a strict sense, such as in bioinformatics [ 6 ] , the
term similarity measure (or similarity score , or just similarity ) is
used for a function @xmath such that the pairs of âcloseâ points take a
large and often positive value while the points which are âfarâ from
each other take a small (often negative) value.

Throughout this work we shall always consider dissimilarity [ 41 , 40 ]
or distance measures, the similarity measures (in a wider sense) which
measure how far apart two points are. We require that all the values are
positive and add an additional requirement that the pair of identical
points takes the value @xmath (this is different from Remark 2.1.2 where
we assume in addition that a distance satisfies the triangle
inequality). The justification is that most commonly used
(dis)similarity measures are metrics or at least quasi-metrics and that
it is almost always possible to convert a similarity measure in a strict
sense into a dissimilarity measure.

###### Definition 5.2.7.

A dissimilarity measure on a set @xmath is a function @xmath where for
all @xmath , @xmath . @xmath

The three types of queries based on a dissimilarity measure of most
interest [ 36 ] are: a range query , a nearest neighbour query and a
@xmath -nearest neighbours (or kNN ) query .

###### Definition 5.2.8.

Let @xmath be a set, @xmath a dissimilarity measure on @xmath , @xmath a
dataset and @xmath . The ( @xmath -) range similarity query centred at
@xmath , denoted @xmath , is defined by

  -- -------- --
     @xmath   
  -- -------- --

that is, @xmath consists of all @xmath that are within the distance
@xmath of @xmath . We will denote by @xmath the set @xmath , of all
possible range queries.

We call a workload @xmath a range (dis)similarity workload . @xmath

If @xmath is a quasi-metric, the range query @xmath corresponds exactly
to the left closed ball @xmath and if @xmath is a metric then @xmath ,
the closed ball of radius @xmath about @xmath .

###### Definition 5.2.9.

Let @xmath be a set, @xmath a dissimilarity measure on @xmath and @xmath
a dataset. The nearest neighbour query centred at @xmath , denoted
@xmath , is defined by

  -- -------- --
     @xmath   
  -- -------- --

that is, it consists of members of @xmath closest to @xmath .

Denote by @xmath the distance to a nearest neighbour of @xmath in @xmath
.

We call a workload @xmath a nearest neighbour (dis)similarity workload .
@xmath

###### Definition 5.2.10.

Let @xmath be a set, @xmath a dissimilarity measure on @xmath and @xmath
a dataset and let

  -- -------- --
     @xmath   
  -- -------- --

The @xmath -nearest neighbour query centred at @xmath , also called a
kNN query , denoted @xmath , is defined by

  -- -------- --
     @xmath   
  -- -------- --

In other words, @xmath is a set of @xmath elements of @xmath closest to
@xmath plus any other elements of @xmath at the same distance as the
@xmath -th nearest neighbour.

We call a workload @xmath a kNN (dis)similarity workload . @xmath

The nearest neighbour and the @xmath -nearest neighbours queries are
jointly called NN-queries [ 36 ] . Unlike range queries, they directly
depend on the dataset @xmath . Note that our definition of @xmath NN
queries differs from the one commonly used in the literature [ 36 , 93 ]
, where any set of @xmath elements of @xmath closest to @xmath is
sufficient to satisfy a @xmath NN query. We chose the above definition
for consistency â every algorithm is guaranteed to return the same
result and @xmath denotes a single set and not a family of sets.

Our definition also makes the connection between NN-queries and range
queries explicit: any NN-query can be expressed in terms of a range
query. For example, for a nearest neighbour query, we have @xmath . Of
course, in practical situations, @xmath is not known in advance.
Nevertheless, we shall mostly concentrate on range similarity queries
and workloads as the most fundamental of the three and easiest to
process.

###### Definition 5.2.11.

Let @xmath be a domain and @xmath and @xmath dissimilarity measures. If
@xmath we call @xmath and @xmath equivalent . @xmath

###### Example 5.2.12.

Let @xmath and @xmath be metric spaces. Recall that two metrics @xmath
and @xmath are equivalent if and only if there exist strictly positive
constants @xmath such that for all @xmath , @xmath . The metric and
dissimilarity measure notions of equivalency do not follow from each
other.

Take a set @xmath with the metrics @xmath and @xmath where @xmath and
@xmath . It is clear that @xmath and @xmath are equivalent as
dissimilarity measures since they generate the same sets of balls while
there is no strictly positive constant @xmath such that for all @xmath ,
@xmath and thus @xmath and @xmath are not equivalent as metrics.

On the other hand, let @xmath where @xmath and @xmath . It is easy to
see that @xmath and @xmath are equivalent metrics but not equivalent
dissimilarity measures since @xmath generates the balls of circular
shape (Euclidean balls) while @xmath generates elliptical balls.

If @xmath is obtained from @xmath by a metric transform , (i.e. @xmath
where @xmath is a concave monotone function with @xmath ), then @xmath
and @xmath are equivalent as similarity measures. One example of a
metric transform is @xmath for some @xmath , where @xmath is a multiple
of @xmath .

#### 5.2.3 Indexing schemes

###### Definition 5.2.13.

An access method for a workload @xmath is an algorithm that on an input
@xmath outputs all elements of @xmath . @xmath

Typical access methods come from indexing schemes.

###### Definition 5.2.14.

Let @xmath be a rooted finite tree. Denote by @xmath the set of leaf
nodes and by @xmath the set of inner nodes of @xmath . The notation
@xmath means that @xmath is a node of @xmath , and @xmath denotes the
set of all children of a @xmath . For any non root node @xmath , the
parent of @xmath is denoted @xmath . @xmath

###### Definition 5.2.15.

Let @xmath be a workload. An indexing scheme on @xmath is a triple
@xmath , where

-   @xmath is a rooted finite tree, with root node @xmath ,

-   @xmath is a collection of subsets @xmath ( blocks , or bins ), where
    @xmath , such that @xmath .

-   @xmath is a collection of set-valued decision functions, @xmath ,
    where each value @xmath is a subset of children of the node @xmath .

@xmath

{pseudocode}

[ovalbox] @xmath .RetrieveIndexedQuery I ,Q \COMMENT Indexing scheme
@xmath over @xmath
\COMMENT Query @xmath
A_0 \GETS {â}
R \GETS â
i \GETS 0
\WHILE A_iâ â \DO \BEGIN A_i+1 \GETS â
\FOREACH tâA_i \DO \BEGIN \IF tâL(T) \THEN A_i+1 \GETS A_i+1âªF_t(Q)
\ELSE \FOREACH xâB_t \DO \BEGIN \IF xâQ \THEN R \GETS Râª{x}
\END
\END
i \GETS i+1
\END
\RETURN R

Hence, an indexing scheme consists of a cover @xmath of @xmath by blocks
and a tree structure that determines the way in which a query is
processed: for each query we traverse those nodes that have been
selected at their parent nodes using the decision functions (Figure 5.2
). Each of the bins associated with selected leaf nodes is sequentially
scanned for elements of the dataset satisfying the query. The Algorithm
5.2.15 depicts a breadth-first traversal of the tree but any other
equivalent algorithm can be used. We will only consider consistent
indexing schemes : those for which the above procedure retrieves all
dataset elements belonging to any query, that is, no query points are
missed. This is more formally expressed by the following definition:

###### Definition 5.2.16.

An indexing scheme @xmath for a workload @xmath is consistent if for
every @xmath and for every @xmath there exists @xmath such that @xmath
and the path @xmath , where @xmath , @xmath and @xmath , satisfies
@xmath for all @xmath . @xmath

Clearly, for a consistent indexing scheme, any algorithm which, for any
query, starting from the root, visits all branches returned by the
decision functions at each node and scans all bins associated with the
leaf nodes visited for the members of the query, is an access method.
The Algorithm 5.2.15 provides one example.

Our definition of indexing scheme extends the definition of [ 87 ] which
considers only the set of blocks. The computational complexity of the
decision functions @xmath , as well as the amount of âbranchingâ
resulting from an application of Algorithm 5.2.15 , become major
efficiency factors in case of similarity-based search, which is why we
feel they should be brought into the picture.

Note that blocks may overlap in an indexing scheme, that is, a point
@xmath can belong to several blocks. There may even be different leaves
pointing to the same block. This observation is at the heart of the
concept of storage redundancy developed in [ 87 ] and [ 86 ] which will
be examined later.

We now present examples of indexing schemes related to some of the most
fundamental algorithms of computer science, reformulating them within
our proposed framework. We provide a very short description and a
reference to the appropriate section of the Volume 3 (Sorting and
Searching) of Knuthâs âThe Art of Computer Programmingâ (TAOCP) [ 111 ]
. It should be noted that while the discussion in TAOCP applies to exact
searches, the ideas in many cases apply to more general cases with very
few modifications.

###### Example 5.2.17.

A simple linear scan (TAOCP, Vol. 3, Section 6.1) of a dataset @xmath
corresponds to the indexing scheme where the tree @xmath has a root
@xmath and a single child @xmath , @xmath consists of a single block
@xmath , and the decision function @xmath always outputs the same value
@xmath .

###### Example 5.2.18.

Hashing (TAOCP, Vol. 3, Section 6.4) can be described in terms of the
following indexing scheme for exact searches. The tree @xmath has depth
one, with its leaves corresponding to bins, and the decision function
@xmath is a hashing function: on input of a query object @xmath it
outputs the bin in which the elements of @xmath matching @xmath are
stored. If there are collisions (i.e. different objects mapping to the
same bin), the retrieved bin needs to be further processed.

A related technique, which can be used in some cases, is to store the
results of commonly used queries and retrieve them at search time using
a hash function.

###### Example 5.2.19.

If the domain @xmath is linearly ordered and the set of queries consists
of intervals @xmath then an efficient indexing structure is constructed
using a generalisation of binary search trees (TAOCP, Vol. 3, Section
6.2). Each bin contains one element of the dataset and every node @xmath
is associated with an interval @xmath which, in the case of an inner
node, covers the intervals associated with the children of @xmath and in
the case a leaf node corresponds to the element of the dataset contained
in the bin @xmath (Figure 5.3 ). Each decision function @xmath on an
input @xmath outputs the set of all children nodes @xmath of @xmath such
that @xmath .

Generalisations of this idea form the core of indexing schemes for
similarity workloads (Sections 5.3 and 5.4 ).

#### 5.2.4 Inner and outer workloads

###### Definition 5.2.20.

A workload @xmath is called inner if @xmath and outer otherwise. @xmath

Typically, for outer workloads @xmath . The difference between inner and
outer workloads is particularly significant for similarity searches
because inner similarity workloads can be thought of as directed
weighted graphs where the dataset points are nodes and two nodes are
connected with an edge with a weight corresponding to their similarity.
In such case, it may be possible, depending on the characteristics of
the graph and the types of queries, to use graph traversal algorithms as
access methods.

In theory, every workload @xmath can be replaced with an inner workload
@xmath , where the new set of queries @xmath consists of sets @xmath ,
@xmath . However, in practical terms this reduction often makes little
sense because while the complexity of storing and processing the query
sets @xmath remains essentially the same, and in addition to requiring
the domain @xmath to be implicitly present, we lose a geometric clarity
of having the set @xmath present explicitly.

### 5.3 Metric trees

Most existing indexing schemes for similarity search apply to metric
similarity workloads, where a dissimilarity measure on the domain is a
metric and the queries are balls of a given radius. Some indexing
schemes apply only to a restricted class of metric spaces, such as
vector spaces, others apply to any metric space. In most cases we
encounter a hierarchical tree index structure where each node is
associated with a set covering a portion of the dataset and a
certification function which certifies if the query ball does not
intersect the covering set, in which case the node is not visited and
the whole branch is pruned (Figure 5.4 ). We show that for such indexing
scheme to be consistent, that is, that no members of the dataset
satisfying the query are missed, the certification functions need to be
1-Lipschitz. The following concept of a metric tree in its present
precise form is new, and is based on our analysis of numerous existing
approaches, which all turn out to be particular cases of our concept.

###### Definition 5.3.1.

Let @xmath be a range dissimilarity workload, where @xmath is a metric.
Let @xmath be a finite rooted tree with root @xmath and @xmath a
collection of subsets of @xmath such that

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

and for every inner node @xmath ,

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

Also, let @xmath be a collection of functions, called certification
functions , such that for each @xmath ,

-   @xmath is 1-Lipschitz, and

-   For all @xmath , @xmath .

We call the triple @xmath a metric tree for the workload @xmath . Let
@xmath and @xmath where

  -- -------- -- -------
     @xmath      (5.3)
  -- -------- -- -------

The indexing scheme @xmath is called a metric tree indexing scheme .
@xmath

The theoretical significance of the proposed concept is stressed by the
following result.

###### Theorem 5.3.2.

Let @xmath be a metric similarity workload and @xmath a metric tree.
Then the metric indexing scheme @xmath is a consistent indexing scheme
for @xmath .

###### Proof.

Let @xmath be a range query and let @xmath , that is, @xmath . By ( 5.1
), there exists a leaf node @xmath such that @xmath . Consider the path
@xmath where @xmath , @xmath and @xmath , from root to @xmath . By ( 5.2
), for each @xmath , we have @xmath and hence @xmath . It follows that
@xmath and since @xmath is a 1-Lipschitz function, we have

  -- -------- --
     @xmath   
  -- -------- --

Therefore, @xmath and hence @xmath is a consistent indexing scheme. â

Once the collection @xmath of blocks has been chosen, the certification
functions always exist.

###### Theorem 5.3.3.

Let @xmath be a range dissimilarity workload, where @xmath is a metric,
@xmath be a finite rooted tree with root @xmath and @xmath a collection
of subsets of @xmath satisfying ( 5.1 ) and ( 5.2 ). Then, for each
@xmath where @xmath , there exists a 1-Lipschitz function @xmath such
that @xmath for all @xmath .

###### Proof.

Put @xmath . By the Lemma 2.4.5 , @xmath is 1-Lipschitz and clearly
@xmath . â

However, the distances from sets are typically computationally very
expensive. The art of constructing a metric tree consists in choosing
computationally inexpensive certification functions that at the same
time donât result in an excessive branching.

We now briefly review some of most prominent examples of metric trees.
We concentrate on their overall structures in terms of the above general
model and pay less attention to the details of algorithms and
implementations, even though they significantly influence the
performance. For many more examples and detailed descriptions the reader
is directed to the original references as well as the excellent reviews
[ 36 ] and [ 93 ] . The concept of a general metric tree equipped with
1-Lipschitz certification functions was first formulated in the present
exact form in [ 154 ] .

#### 5.3.1 Vector space indexing schemes

We first examine indexing schemes for âclassical range searchesâ, that
is, for vector space workloads where the domain is @xmath and the set of
queries is given by the balls with respect to the @xmath metric, also
called rectangles . The rationale for this terminology is given by the
shape of unit balls with respect to the @xmath norm in @xmath â the
shapes of @xmath , @xmath and @xmath balls are shown in Figure 5.5 .
Note also that this is the most general setting since for any @xmath an
@xmath ball is contained in the @xmath ball with the same centre and
radius and hence an access method for a @xmath workload can be obtained
by what we call a projective reduction (Subsection 5.6.4 below) to the
@xmath workload. In practice, queries can be even more general,
consisting of rectangles with sides of different lengths but this does
not add anything to generality conceptually (if not in practical terms)
since such queries can be represented, for example, as unions of (unit)
balls.

###### Example 5.3.4.

The R-tree [ 84 ] is a dynamic structure for indexing points and
rectangles in vector spaces. Many variants showing performance
improvements exist, such as the @xmath -tree [ 172 ] and the @xmath
-tree [ 12 ] . The main feature of all variants is that bounding
rectangles are used to enclose data points (at leaf nodes) or bounding
rectangles of children nodes.

The R-trees are paged structures â nodes are stored in secondary memory
and retrieved as needed. Each non-root node of the tree @xmath has
between @xmath and @xmath children with all leaves containing data
points or rectangles appearing at the same level. The minimum bounding
rectangle @xmath is associated to each node @xmath (Figure 5.6 ). A node
@xmath is visited if the query rectangle intersects @xmath , that is,
certification functions are @xmath , where @xmath is the @xmath -metric.
The structure is fully dynamic â insertions and deletions can be
intermixed with queries.

The main factor in performance of R-trees is organisation of bounding
rectangles. The optimisations of the @xmath -tree, which was shown to
have the best performance of the above mentioned three variants, are
based on reduction of volume and lengths of the edges of bounding
rectangles at each node as well as on minimisation of overlap between
rectangles associated with different nodes.

###### Example 5.3.5.

The X-tree [ 17 ] is a modification of the R-tree suitable for indexing
high-dimensional vector space workloads. It is based on the observation
(see Subsection 5.7.3 ) that high overlap between bounding rectangles of
many children of R-tree nodes in high dimensions, leading to sequential
scan of all them, is unavoidable. Hence the nodes whose bounding
rectangles overlap to an excessively high degree are collapsed into
supernodes which are organised for linear scan (Figure 5.7 ). The X-tree
uses the same certification functions as the R-tree: the distances to
bounding rectangles. The authors report that X-tree outperforms the
@xmath -tree by as much as 8 times on high dimensional datasets.

###### Example 5.3.6.

Consider the vector space workloads where the metric is the Euclidean (
@xmath ) distance (more generally the weighted Euclidean distance where
@xmath is a vector of weights and @xmath . The SS-tree [ 210 ] is an
indexing scheme where bounding spheres instead of bounding rectangles
are used at each node (Figure 5.8 ). More precisely, the region @xmath
associated with each node @xmath is a ball centred at @xmath , the
centroid of all dataset points covered by @xmath , with the covering
radius @xmath . Hence, the certification functions are of the form
@xmath .

#### 5.3.2 General metric space indexing schemes

We now turn to the indexing schemes for general metric space workloads
where no structure in addition to metric is assumed, that is, all that
is available at creation time is the set of data points and a metric
@xmath .

###### Example 5.3.7.

The vp-tree [ 217 ] is an indexing scheme with a binary tree and
certification functions of the form @xmath , where @xmath is a vantage
point chosen for the non-leaf node @xmath , @xmath is the median value
for the function @xmath , and @xmath are two children of @xmath . Thus,
at each non-leaf node @xmath , a part of the dataset covered by @xmath
is partitioned into two equal halfs where @xmath and @xmath (Figure 5.9
).

The @xmath -ary versions, where the dataset is split in @xmath -equal
parts at each node, have also been proposed.

###### Example 5.3.8.

The mvp-tree [ 25 ] is a modification of the vp-tree which uses multiple
vantage points at each node. In the binary case, for any node @xmath ,
two vantage points, @xmath and @xmath are chosen and the part of the
dataset covered by @xmath is split in four parts.

Let @xmath be an inner node and @xmath and @xmath be the functions
@xmath where @xmath and @xmath . Let @xmath be the median value for
@xmath and @xmath , @xmath . Let @xmath be the median value for @xmath
and @xmath the median value for @xmath . The certification functions for
the children @xmath are

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

The maxima above are computed from left to right and the second value is
not computed if the first exceeds the search radius. The main difference
from the binary vp-tree is that two instead of three vantage points are
used to divide a covering region into four regions, resulting in fewer
distance computations.

###### Example 5.3.9.

The GNAT (Geometric Near-neighbour Access Tree) indexing scheme proposed
by Sergey Brin [ 27 ] , one of the founders of Google, is based on
splitting the domain @xmath at each node @xmath into @xmath regions
@xmath based on proximity to the split points @xmath , yielding an
@xmath -ary tree (Figure 5.11 ). The sets @xmath , called Dirichlet
domains , correspond to Voronoi cells in @xmath . For each pair of split
points @xmath , the values @xmath and @xmath are stored. The
certification functions are of the form

  -- -------- --
     @xmath   
  -- -------- --

###### Example 5.3.10.

Unlike the vp-tree and the GNAT but like the R-trees, the M-tree [ 41 ]
is a dynamic and paged structure. The tree is binary and at each node
@xmath a routing object @xmath is stored together with the covering
radius @xmath and the distances to the routing objects of the children.
The certification functions are of the form

  -- -------- --
     @xmath   
  -- -------- --

If the value @xmath exceeds @xmath the rest of @xmath need not be
computed. This avoids potentially expensive computation of @xmath . The
way the routing points are chosen and data points divided between them
is determined by the user by choosing one of many available split
policies . The best performing policy was found to be the generalised
hyperplane decomposition where each data object is assigned to the
routing object closest to it.

The QIC-M-tree is a modification of the M-tree where instead of one,
three distances on @xmath are used: the index distance , @xmath , to
construct the index, the comparison distance , @xmath , to be used in
certification functions, and the query distance , @xmath , according to
which the actual result must be computed. The structure of the
QIC-M-tree is the same as the structure of the M-tree except that the
value of a certification function @xmath is

  -- -------- --
     @xmath   
  -- -------- --

where @xmath in the routing point of node @xmath and @xmath is the
associated covering radius. As before, the evaluation is from left to
right and is stopped as soon as one of the expressions exceeds the query
radius. It is clear that for consistency of such indexing scheme it is
necessary and sufficient that the identity maps @xmath and @xmath be
1-Lipschitz (Ciaccia and Patella allow for the scaling factors in the
case this is not so). Any @xmath finer than @xmath and @xmath can be
used as a query distance.

Modifications of the M-tree allowing for processing of complex queries
have been proposed in [ 40 ] .

### 5.4 Quasi-metric trees

Although often mentioned as possible generalisations of metric workloads
(e.g. in [ 39 ] ), quasi-metric workloads have been so far neglected as
far the practical indexing schemes are concerned. As our biological
examples attest (Chapter 3 ), quasi-metrics in fact often appear as
similarity measures on datasets, even if they are not recognised as
such.

For a nearly symmetric quasi-metric @xmath on a set @xmath , where the
asymmetry @xmath is small compared to the expected scale of the search,
it may be possible to replace it by a suitable metric without
significant loss of performance by the way of what we call a projective
reduction of a workload (Subsection 5.6.4 ). We find a metric @xmath
such that @xmath for all @xmath where @xmath is the smallest positive
constant ensuring the above inequality ( @xmath is in fact the Lipschitz
constant of the map @xmath ) and index the metric space @xmath . The
QIC-M-tree [ 39 ] provides exactly the framework to do so. Obvious
choices for @xmath are @xmath or @xmath . In the next chapter we perform
the analysis of this approach for a set of peptide fragments.

However, if the quasi-metric in question is highly asymmetric,
significant loss of performance may result because the required
Lipschitz constant may be very large (or even non-existent if @xmath is
a @xmath quasi-metric) and the metric @xmath becomes a poor
approximation to @xmath . It is therefore desirable to develop a theory
of indexability for quasi-metric spaces.

We use left 1-Lipschitz functions as certification functions to
establish the direct analogs of the Definition 5.3.1 and the Theorem
5.3.2 (indeed, the advantage of our general model is that it allows the
incorporation of the quasi-metric case with very few differences).
Recall that a left 1-Lipschitz function @xmath from a quasi-metric space
@xmath satisfies @xmath for all @xmath (Definition 2.4.1 ).

###### Definition 5.4.1.

Let @xmath be a range dissimilarity workload, where @xmath is a
quasi-metric. Let @xmath be a finite rooted tree with root @xmath and
let @xmath be a collection of subsets of @xmath such that

  -- -------- -- -------
     @xmath      (5.4)
  -- -------- -- -------

and for every inner node @xmath ,

  -- -------- -- -------
     @xmath      (5.5)
  -- -------- -- -------

Also, let @xmath be a collection of certification functions such that
for each @xmath ,

-   @xmath is left 1-Lipschitz, and

-   For all @xmath , @xmath .

We call the triple @xmath a quasi-metric tree for the workload @xmath .
Let @xmath and @xmath where

  -- -------- -- -------
     @xmath      (5.6)
  -- -------- -- -------

The indexing scheme @xmath is called a quasi-metric tree indexing scheme
. @xmath

###### Theorem 5.4.2.

Let @xmath be a quasi-metric similarity workload and @xmath a
quasi-metric tree. Then the quasi-metric indexing scheme @xmath is a
consistent indexing scheme for @xmath .

###### Proof.

Let @xmath . By ( 5.4 ), there exists a leaf node @xmath such that
@xmath . Consider the path @xmath where @xmath , @xmath and @xmath ,
from root to @xmath . By ( 5.5 ), for each @xmath , we have @xmath and
hence @xmath . It follows that @xmath and since @xmath is a left
1-Lipschitz function, we have

  -- -------- --
     @xmath   
  -- -------- --

Therefore, @xmath and consistency follows. â

As with metric trees, certification functions satisfying the above
properties always exist â they are provided by the distances from points
to covering sets.

###### Theorem 5.4.3.

Let @xmath be a range dissimilarity workload, where @xmath is a
quasi-metric, @xmath be a finite rooted tree with root @xmath and @xmath
a collection of subsets of @xmath satisfying ( 5.4 ) and ( 5.5 ). Then,
for each @xmath where @xmath , there exists a left 1-Lipschitz function
@xmath such that @xmath for all @xmath . â

###### Proof.

Put @xmath . By the Lemma 2.4.5 , @xmath is left 1-Lipschitz and @xmath
. â

No general quasi-metric tree indexing scheme has been produced as yet â
our indexing scheme for protein fragments (Chapter 6 ) is an example of
a quasi-metric tree but is not general. While it is possible to
generalise existing indexing schemes to support quasi-metric queries,
the resulting structure is usually more complex. For example, while the
function @xmath is left 1-Lipschitz (Lemma 2.4.4 ), @xmath is right
1-Lipschitz but not necessarily left 1-Lipschitz and hence the
generalisation of the vp-tree (Example 5.3.7 ) certification functions
as they are, just by replacing the metric with a quasi-metric, is not
possible. If the distances from the same vantage point are desired to be
used at each node, both the left and the right distance need to be
computed and cutoff values chosen so that the whole dataset is covered
and (if possible â it may not be) that overlap is minimal. The same is
true for the GNAT (Example 5.3.9 ): certification functions need to be
adjusted to be left 1-Lipschitz and for this it is necessary to compute
both left and right distance to the split points. Hence, additional
computation may be necessary at each node, adversely affecting the
performance.

It appears that, out of all our examples of metric indexing schemes, the
M-tree (Example 5.3.10 ) is most suitable for adaptation for indexing
quasi-metric workloads. The structure of a balanced binary tree should
remain while the covering set at each node @xmath should be the right
closed ball @xmath of radius @xmath about the routing object @xmath .
The certification function @xmath should be set so that

  -- -------- --
     @xmath   
  -- -------- --

The distances @xmath from routing objects to their parents, as well as
the covering radii @xmath , can be, as is the case with M-tree, computed
and stored at creation time.

The above proposal for turning the M-tree into a quasi-metric tree is,
at present, only conceptual. Many challenges remain, for example in
designing a good split policy to be used in the creation algorithm. If
an attempt to develop a quasi-metric version of M-tree is made, it will
be necessary to test it on a variety of actual quasi-metric datasets.

### 5.5 Valuation Workloads and Indexing Schemes

Closely related to similarity workloads are what we call valuation
workloads .

###### Definition 5.5.1.

Let @xmath be a set, @xmath a dataset and @xmath a function @xmath . For
@xmath the ( @xmath -) range valuation query , denoted @xmath , is
defined by

  -- -------- --
     @xmath   
  -- -------- --

We denote by @xmath the set @xmath and call a workload @xmath a range
valuation workload . @xmath

###### Definition 5.5.2.

Let @xmath be a rooted tree. A function @xmath is increasing on @xmath
if for all @xmath , @xmath , @xmath . @xmath

###### Definition 5.5.3.

Let @xmath be a range valuation workload and suppose @xmath is a finite
rooted tree with root @xmath and @xmath a collection of subsets of
@xmath such that @xmath . Suppose @xmath is increasing on @xmath and for
all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath where @xmath . The indexing scheme @xmath is called a
valuation indexing scheme . @xmath

###### Theorem 5.5.4.

Every valuation indexing scheme is consistent.

###### Proof.

Let @xmath be a valuation indexing scheme over a range valuation
workload @xmath and @xmath . Suppose @xmath , that is @xmath for some
@xmath . Since @xmath is a cover of @xmath , there exists a leaf node
@xmath such that @xmath . Consider the path @xmath where @xmath , @xmath
and @xmath , from root to @xmath . Since @xmath is increasing on @xmath
, we have @xmath and therefore @xmath for each @xmath . â

Valuation workloads are perhaps not very interesting on their own but it
should be noted that every workload can be decomposed as a union of
valuation workloads having the same underlying domain and dataset
(Subsection 5.6.2 ). If a tree structure is present, the Theorem 5.5.4
ensures that a consistent indexing scheme can be constructed.

### 5.6 New indexing schemes from old

Here we formulate in an abstract setting some constructions commonly
used to generate new access methods from the existing ones. Our general
approach makes these constructions amenable to analysis by means of
theoretical computer science.

#### 5.6.1 Disjoint sums

Any collection of access methods for workloads @xmath leads to an access
method for the disjoint sum workload @xmath : to answer a query @xmath ,
it suffices to answer each query @xmath , @xmath , and then merge the
outputs.

In particular, if each @xmath is equipped with an indexing scheme,
@xmath , then a new indexing scheme for @xmath , denoted @xmath , is
constructed as follows: the tree @xmath contains all @xmath âs as
branches beginning at the root node, while the families of bins and of
decision functions for @xmath are unions of the respective collections
for all @xmath , @xmath .

This construction is often used coupled which an equivalence relation
which partitions the domain, instance and each of the queries into
smaller spaces, perhaps with a better structure which are then indexed
separately (âsubindexedâ). A good illustration is our indexing scheme
for weighted quasi-metric spaces.

###### Example 5.6.1.

Recall that a weighted quasi-metric (Section 2.6 ) over a domain @xmath
is a quasi-metric @xmath such that for some weight function @xmath and
for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

The following Proposition shows that any weighted quasi-metric
similarity workload @xmath can be indexed using the decomposition into a
disjoint union of metric spaces or fibres , one for each value that the
weight function @xmath takes.

###### Proposition 5.6.2.

Let @xmath be a weighted quasi-metric space and denote by @xmath the set
@xmath , and by @xmath the closed ball of radius @xmath centred at
@xmath with respect to the metric @xmath where for each @xmath , @xmath
. Then

1.  @xmath ,

2.  @xmath for all @xmath , and

3.  @xmath for all @xmath .

###### Proof.

The first two statements are obvious while the third claim follows
directly from

  -- -------- --
     @xmath   
  -- -------- --

â

Therefore, provided that @xmath takes few values on the dataset
(otherwise close fibres need to be merged), it is possible to index into
@xmath by indexing data points for each fibre using one of the existing
indexing schemes for metric spaces and then collecting the results. We
call this scheme a FMTree (Fibre Metric Tree). Some of our attempts to
use this scheme to index into datasets of short protein fragments are
described in the next chapter.

#### 5.6.2 Query partitions

A similar technique can be used where the set of queries over some
domain is partitioned and separate indexing scheme exists for each
partition.

Let @xmath be a domain, @xmath a dataset and @xmath , @xmath a pairwise
disjoint family of queries over @xmath . A collection of access methods
for the workloads @xmath leads to an access method for the workload
@xmath : to answer a query @xmath , find @xmath such that @xmath and
answer it using the access method for the workload @xmath .

As in the disjoint sum case, if each @xmath is equipped with a
consistent indexing scheme, @xmath , then a new consistent indexing
scheme for @xmath , denoted @xmath is constructed as follows: the tree
@xmath contains all @xmath âs as branches beginning at the root node,
while the families of bins and of decision functions for @xmath contain
the unions of the respective collections for all @xmath , @xmath . The
decision function at the root for each query @xmath returns the set
consisting of the branch @xmath . We call such indexing scheme a query
partitioning indexing scheme .

A query partitioning indexing scheme can be considered to be highly
redundant (see Subsection 5.7.1 for the precise definition of redundancy
of indexing schemes) since each major branch contains the bins covering
the whole dataset which, in many cases, may occupy considerable space.
However, in some cases it may be possible for such indexing scheme to
occupy the space much more efficiently. Our indexing scheme for protein
fragment workloads, called FSindex, is a good example of the query
partitioning approach with no redundancy â each data point is stored
only once.

#### 5.6.3 Inductive reduction

Let @xmath , @xmath be two workloads. An inductive reduction of @xmath
to @xmath is a pair of mappings @xmath , @xmath , such that

-   @xmath ,

-   for each @xmath , @xmath .

Notation: @xmath .

An access method for @xmath leads to an access method for @xmath , where
a query @xmath is answered as in the Algorithm 5.6.3 :

{pseudocode}

[ovalbox] @xmath .RetrieveQueryQ \COMMENT @xmath , @xmath
R_1 \GETS â
R_2 \GETS \CALL @xmath .RetrieveQueryi^â(Q)
\COMMENT @xmath
\FOREACH yâR_2 \DO \BEGIN \IF i(y)âQ \THEN R_1 \GETS R_1âª{i(y)} \END
\RETURN R_1

If @xmath is a consistent indexing scheme for @xmath , then a consistent
indexing scheme @xmath for @xmath is constructed by taking @xmath ,
@xmath , and @xmath (the upper index @xmath refers to the two
workloads). The bigger workload used for inductive reduction usually
carries a structure that supports an efficient access method.

###### Example 5.6.3.

Let @xmath be a finite graph of bounded degree, @xmath . Associate to it
a graph workload , @xmath , which is an inner workload with @xmath , the
set of vertices, and @xmath , the set of @xmath NN queries where @xmath
is the shortest path metric on @xmath .

A linear forest is a graph that is a disjoint union of paths. The linear
arboricity , @xmath , of a graph @xmath is the smallest number of linear
forests whose union is @xmath . This number is, in fact, fairly small:
it does not exceed @xmath , where @xmath is the degree of @xmath [ 82 ,
3 ] . The Linear Arboricity Conjecture [ 1 , 2 ] , which states that
@xmath , was found to hold for numerous cases [ 3 ] . Results for @xmath
-linear arboricity, the minimum number of forests whose connected
components are paths of length at most @xmath are also available [ 125 ]
. This concept leads to an indexing scheme for the graph workload @xmath
, as follows.

Let @xmath , @xmath be linear forests. Denote @xmath and let @xmath be a
surjective map preserving the adjacency relation. Every linear forest
can be ordered, and indexed into as in Ex. 5.2.19 . At the next step,
index into the disjoint sum @xmath as in Subsection 5.6.1 . Finally,
index into @xmath using the inductive reduction @xmath . This indexing
scheme outputs nearest neighbours of any vertex of @xmath in time @xmath
, requiring storage space @xmath , where @xmath is the number of
vertices in @xmath .

#### 5.6.4 Projective reduction

Let @xmath , @xmath be two workloads. A projective reduction of @xmath
to @xmath is a pair of mappings @xmath , @xmath , such that

-   @xmath ,

-   for each @xmath , @xmath .

Notation: @xmath .

An access method for @xmath leads to an access method for @xmath , where
a query @xmath is answered as follows:

{pseudocode}

[ovalbox] @xmath .RetrieveQueryQ \COMMENT @xmath , @xmath
R_1 \GETS â
R_2 \GETS \CALL @xmath .RetrieveQueryr^â (Q)
\COMMENT @xmath
\FOREACH yâR_2 \DO \BEGIN \FOREACH xâr^-1(y) \DO \BEGIN \IF xâQ \THEN
R_1 \GETS R_1âª{x} \END
\END
\RETURN R_1

Let @xmath be a consistent indexing scheme for @xmath . The projective
reduction @xmath canonically determines an indexing scheme @xmath as
follows: @xmath , @xmath , and @xmath .

###### Example 5.6.4.

The linear scan of a dataset is a projective reduction to the trivial
workload: @xmath .

If @xmath is a workload and @xmath is a domain, then every mapping
@xmath determines the direct image workload, @xmath , where @xmath is
the image of @xmath under @xmath and @xmath is the family of all queries
@xmath .

###### Example 5.6.5.

Let @xmath be a finite collection of blocks partitioning @xmath . Define
the discrete workload @xmath , and define the reduction by mapping each
@xmath to the corresponding block and defining each @xmath as the union
of all blocks that meet @xmath . The corresponding reduction forms a
basic building block of many indexing schemes [ 36 ] .

###### Example 5.6.6.

Let @xmath , @xmath be two metric range similarity workloads, that is,
their query sets are generated by metrics @xmath , @xmath . In order for
a mapping @xmath with the property @xmath to determine a projective
reduction @xmath , it is necessary and sufficient that @xmath be
1-Lipschitz: indeed, in this case every ball @xmath will be mapped
inside of the ball @xmath in @xmath .

###### Example 5.6.7.

More specifically, the following technique (described in detail in [ 36
] ) is often used to map metric spaces into @xmath in order to use
vector space indexing schemes such as the R-tree (Example 5.3.4 ).

Let @xmath be a metric space and choose @xmath 1-Lipschitz functions
@xmath . It is easy to see that the map @xmath is a 1-Lipschitz map
@xmath and thus induces a projective reduction to the vector space
workload. The most common way of choosing the required 1-Lipschitz
functions is to select @xmath pivots @xmath and set @xmath .

###### Example 5.6.8.

Pre-filtering is an often used instance of projective reduction. In the
context of metric similarity workloads, this normally denotes a
procedure whereby a metric @xmath is replaced with a coarser distance
@xmath which is computationally cheaper. While the distance @xmath need
not be a metric (in fact it need not even satisfy the triangle
inequality), it is necessary and sufficient that @xmath for all @xmath
for the identity map to induce a projective reduction. The QIC-M-Tree [
39 ] provides an example of this approach.

###### Example 5.6.9.

A frequently used tool for dimensionality reduction of datasets is the
famous JohnsonâLindenstrauss lemma [ 102 ] . Let @xmath be an Euclidean
space of high dimension, and let @xmath be a dataset with @xmath points.
If @xmath and @xmath is a randomly chosen orthogonal projection of
@xmath onto a Euclidean subspace of dimension @xmath , then with
overwhelming probability the mapping @xmath does not distort distances
within @xmath by more than the factor of @xmath . More results of the
same type, for embedding @xmath -point datasets into lower dimensional
linear (not necessarily Euclidean) spaces, were obtained in [ 127 ] .

Such techniques do not extend with the same distortion to the entire
domain @xmath , meaning that they can be only applied to construct
consistent indexing schemes for the inner workload @xmath , and not the
outer workload @xmath .

### 5.7 Performance and Geometry

In the preceding sections we were mostly concerned with the abstract
foundations of indexing and similarity search and therefore have mostly
ignored the issue of the performance. This is of course the key
question: the rationale for indexing is exactly that it is supposed to
speed up searches. Our definitions of similarity workload and indexing
scheme clearly point towards a geometric setting for answering the
questions about the performance. Here we attempt to examine some factors
concerning the performance of indexing schemes, albeit at a purely
conceptual level. This is indeed the only possible way without either a
concrete dataset, or very detailed assumptions about the workload.

Our main result is yet another way of describing the Curse of
Dimensionality which is a general observation that indexing schemes for
high dimensional spaces perform very badly â often an optimised
sequential scan performs better. The framework we use was first
introduced in [ 154 ] : a metric similarity workload is identified with
an mm-space where the measure reflects the distribution of query points.
We use the techniques from [ 154 ] to derive the lower bounds on the
number of blocks that must be processed in order to answer a range query
of radius @xmath .

#### 5.7.1 Cost model for indexing schemes

In estimating the performance of indexing schemes, as with other
algorithms and data structures in computer science, we are primarily
interested in two quantities: the space occupied by the indexing
structure and the time required to process the query. As always there is
a tradeoff between the two. For example, for an @xmath -point dataset,
sequential scan (Example 5.2.17 ) takes @xmath time with @xmath space
(the space necessary to store all data points) while, if the workload is
inner, hashing (Example 5.2.18 ) takes @xmath time with @xmath space.
Therefore, an investigation of performance of an indexing scheme has to
take into account both the space and the query time complexity as well
as the time required to build or update the structures.

The space complexity is of great importance in practice, especially with
large datasets â often we are constrained to take no more than @xmath
space. However, we shall concentrate mostly on the query time complexity
since the space complexity can be easily estimated directly. At this
stage we deliberately ignore the index creation complexity â we always
assume that an index is already constructed, that is, that all of @xmath
are defined.

The general goal of indexing is to produce access methods that have time
complexity sublinear in the size of the dataset. Often, the authors of
indexing schemes claim to achieve @xmath time (see for example a summary
of space and time complexities of existing metric indexing schemes in [
36 ] ), but this claim usually only holds for âsmallâ queries.
Nevertheless, in practice, even a constant reduction of the number of
data points to be scanned, say to @xmath , if not accompanied with a too
large overhead, is worthwhile pursuing.

##### General time complexity

In most general terms, the time required to process query @xmath using a
consistent indexing scheme @xmath on a workload @xmath is given by the

  -- -------- -- -------
     @xmath      (5.7)
  -- -------- -- -------

where @xmath is the total time required to process query @xmath , @xmath
is the time associated with traversing the nodes of @xmath , @xmath is
the total time spent evaluating decision functions at all visited inner
nodes of @xmath and @xmath is the total time spent scanning the sets
@xmath for each block @xmath associated with the leaf nodes visited.

The @xmath is mostly associated with the data structures required for
tree traversal. It includes the cost of retrieving the nodes from
secondary memory (I/O costs) if it is used as well as the cost of any
additional data structures used. For example, some algorithms for kNN
similarity search [ 93 ] , which are described in more detail in the
context of our indexing scheme for peptide fragments in Chapter 6 , make
use of priority queue for tree traversal. Under some circumstances, such
as the large number of nearest neighbours required, both the space and
the time costs of the priority queue are not negligible. On the other
hand, if the whole structure is stored in primary memory and no
expensive data structures are used, the @xmath can be very small
compared with the other two times and is often ignored [ 36 ] .

The equation 5.7 can be elaborated in the following way: let @xmath be
the set of nodes of @xmath visited in order to retrieve a query @xmath .
Denote by @xmath the set @xmath and by @xmath the set @xmath . Then we
have

  -- -------- -- -------
     @xmath      (5.8)
  -- -------- -- -------

where @xmath is the time required to check if @xmath and @xmath is the
time required to evaluate @xmath .

Most frequently, we are not interested in the performance for a single
query but in either the average or the worst case performance. However,
in order to measure the average search time it is necessary to have a
probability distribution on the set queries @xmath . We shall return to
this theme in Subsection 5.7.2 .

###### Example 5.7.1.

In [ 36 ] the general cost of a (range) query for a metric indexing
scheme is measured by the number of distances evaluated. In this case
the @xmath is the time taken to evaluate the distance from the query
centre @xmath to @xmath and it is assumed that each evaluation of a
certification function is based on one or more distance evaluations. The
I/O costs ( @xmath ) are ignored and it is assumed that other costs of
the indexing structure are an order of magnitude less than costs of
distance evaluations.

###### Example 5.7.2.

A more elaborate cost model, consistent with the Equations 5.7 and 5.8 ,
was proposed by Ciaccia and Patella [ 39 ] in the context of the
QIC-M-tree (Example 5.3.10 ). Since the QIC-M-tree is a paged structure,
the I/O costs are explicitly included. The @xmath depends only upon the
comparison distance @xmath (it is exactly the time to evaluate query
distances to all points retrieved from the leaf nodes) while the @xmath
depends on the index distance @xmath as well as @xmath . The authors
note that the performance does not depend directly on the query distance
@xmath which is approximated by @xmath and @xmath , give formulae for
the average costs in terms of the distributions of @xmath and @xmath and
develop ways to choose comparison distances so as to optimise
performance.

##### Redundancy and Access Overhead

In their 1997 paper [ 87 ] and its followup with additional coauthors
Miranker and Samoladas [ 86 ] , Hellerstein, Koutsoupias and
Papadimitriou proposed two measures of performance of indexing schemes:
redundancy and access overhead and showed that there is a tradeoff
between the two. We present the adaptations of their concepts to our
model.

###### Definition 5.7.3.

Let @xmath be a workload and @xmath an indexing scheme. The redundancy
@xmath of @xmath is the number of blocks that contain @xmath , that is,

  -- -------- --
     @xmath   
  -- -------- --

The average redundancy @xmath , of the indexing scheme @xmath , is the
average of @xmath over all data points:

  -- -------- --
     @xmath   
  -- -------- --

@xmath

###### Definition 5.7.4.

Let @xmath be a workload and @xmath an indexing scheme. For a query
@xmath denote, as before, by @xmath the set of leaf nodes visited to
answer @xmath . The access overhead @xmath of query @xmath is defined as

  -- -- --
        
  -- -- --

The (worst case) access overhead @xmath for indexing scheme @xmath is

  -- -------- --
     @xmath   
  -- -------- --

If furthermore all blocks @xmath contain @xmath data points, we define
the block access overhead @xmath of query @xmath by

  -- -------- --
     @xmath   
  -- -------- --

and of indexing scheme @xmath by @xmath .

If @xmath is a probability measure on @xmath , we define the average
access overhead @xmath for the indexing scheme @xmath by

  -- -------- --
     @xmath   
  -- -------- --

and the average block access overhead @xmath by

  -- -------- --
     @xmath   
  -- -------- --

@xmath

The access overhead @xmath measures the cost of answering the query
@xmath using the set of blocks @xmath (that is, the @xmath â the costs
associated with @xmath and @xmath are ignored) normalised by the ideal
cost and hence takes values in @xmath . The block access overhead
measures the same cost in terms of block accesses and corresponds to the
original definition of access overhead in [ 87 ] . Our new definition
was chosen in order not to depend on block size which in some indexing
schemes may vary considerably and to allow for empty queries which do
take time to process.

The main result of [ 86 ] is the Redundancy Theorem which in a workload
independent way gives a lower bound for the redundancy in terms of the
block size and access overhead.

###### Theorem 5.7.5 ([86]).

Let @xmath be a workload and @xmath an indexing scheme such that all
blocks contain @xmath datapoints and @xmath . Let @xmath be queries such
that for every @xmath :

1.  @xmath , and

2.  @xmath , âfor all @xmath and @xmath .

Then, the average redundancy is bounded by @xmath .

In most applications, due to space constraints, the redundancy of each
datapoint @xmath is set to @xmath , that is, there is only one block
containing @xmath . The Theorem 5.7.5 then gives the lower bound for the
block access overhead provided the queries do not pairwise intersect to
a too great extent. If a better block access overhead is desired while
block size stays the same, it is necessary to increase the (average)
redundancy.

#### 5.7.2 Workloads and pq-spaces

In order to estimate the average performance it is necessary to have a
probability distribution on the set of queries which is often not
available in any useful form. This is true in particular for similarity
workloads with range queries which depend both on the query centre
@xmath and the radius @xmath . Subsequently, we shall assume that the
radius is fixed and attempt to analyse the performance of indexing
schemes with only @xmath as a parameter.

Indeed, there are good reasons to consider performances of indexing
schemes for different search radii separately. We show in Subsection
5.7.3 that there are significant qualitative differences between
performances at different scales. Furthermore, this approach corresponds
with many real-life situations where the radius has a direct,
problem-specific interpretation and is chosen in advance. One example is
biological sequence search performed by BLAST [ 6 ] â in almost all
practical cases the users do not change the default threshold which
corresponds to the expected number of sequences to be retrieved
according to a null model. The threshold is translated into a cutoff
similarity score and thus into a quasi-metric radius (depending on the
query centre only).

Therefore, we shall assume that the domain @xmath is equipped with a
(Borel) probability measure @xmath reflecting the distribution of query
centres. If the dissimilarity measure @xmath is a metric (respectively
quasi-metric), it follows that the triple @xmath is a pm- (respectively
pq-) space. The measure @xmath can always be approximated from the
dataset itself: for any @xmath set @xmath . This would imply that the
distribution of the query centres coincides with the distribution of the
dataset and is the approach taken in [ 39 ] .

A complementary way of looking at the measure @xmath on @xmath is to
treat it as a sort of an âidealâ measure and the dataset as an @xmath
-point sample according to @xmath . One can consider a family of
datasets from @xmath distributed according to @xmath and attempt to
construct an indexing scheme which would answer queries of all datasets
efficiently. This was one of the reasons we defined the queries as
subsets of @xmath rather than @xmath .

One can go even further by having two measures on @xmath â one giving
the dataset distribution as above and another, possibly very different,
providing the distribution of the query centres. It has long been
observed in the context of relational databases [ 37 ] that that it is
necessary to consider non-uniform distributions of queries in order to
well estimate the query performance and there is no reason to suppose
that the same does not hold for similarity-based queries. However, the
introduction of a second measure would present non-trivial technical
challenges and we therefore leave it for subsequent work.

#### 5.7.3 The Curse of Dimensionality

It has long been known (c.f. for example [ 16 ] ) that exponential
complexity might be inherent in any algorithm for answering near
neighbour queries because a point in a high-dimensional space can have
many âcloseâ neighbours. In fact, this phenomenon is not only associated
with similarity searches but with other data analysis related areas such
as machine learning using neural networks [ 22 ] , clustering [ 92 ] ,
function or density estimation [ 61 ] , signal processing [ 202 ] and
many others. In all cases the procedures that perform well on two or
three dimensional sets fail to do in higher dimensions. We take the
paradigm of Pestov [ 154 ] that the curse of dimensionality is primarily
a manifestation of the concentration phenomenon. It allows us to use the
techniques developed in Chapter 4 to provide estimates of performance of
indexing schemes with as few assumptions as possible regarding the
nature of the dataset. We first outline the previous results for the
nearest neighbour queries and then proceed to our contribution for range
queries in quasi-metric workloads.

##### Nearest Neighbour Queries

In their 1999 paper, Beyer et al. [ 20 ] investigated the effect of
dimensionality to the nearest neighbour problem. Their main result
states that under certain conditions every nearest neighbour query (in a
metric space) is unstable : the distance from any point to its nearest
neighbour is very close to the distances to most other points. We
outline here the contribution of Pestov [ 154 ] who both relaxed the
assumptions of Beyer et al. and obtained stronger conclusions using the
techniques of the asymptotic geometric analysis, that is, the
concentration phenomenon.

###### Definition 5.7.6 ([20]).

Let @xmath be a workload where @xmath is a metric space and @xmath is
the set of nearest neighbour queries. A query @xmath is called @xmath
-unstable for an @xmath if

  -- -------- --
     @xmath   
  -- -------- --

@xmath

###### Definition 5.7.7.

Let @xmath be an pm-space and @xmath a finite subset. For an @xmath
denote by @xmath the maximal radius of an open ball in @xmath centred at
@xmath of measure not more that @xmath . For a @xmath we say that @xmath
is weakly @xmath -homogeneous in @xmath if all radii @xmath belong to an
interval of length less than @xmath . @xmath

###### Theorem 5.7.8 ([154]).

Let @xmath be an pm-space and @xmath a finite subset. Denote by @xmath a
median value of @xmath , the distance from a point in @xmath to its
nearest neighbour in @xmath . Let @xmath and assume that @xmath is
weakly @xmath -homogeneous in @xmath .

Then for all points @xmath , apart from a set of total measure at most
@xmath , the open ball of radius @xmath centred at @xmath contains at
least

  -- -------- --
     @xmath   
  -- -------- --

elements of @xmath .

Hence, provided that @xmath is weakly @xmath -homogeneous in @xmath
(which it is, as remarked in [ 154 ] , with probability not less than
@xmath if @xmath is sampled randomly with regard to @xmath ) and that
@xmath has concentration property, with very high probability every
nearest neighbour query is @xmath -unstable.

The point of all this is that in the case of query instability there is
little information to be gained by the nearest neighbour search â the
quality of results is such that they can not be well interpreted.
Hinnenburg et al. [ 91 ] proposed a solution to a generalised nearest
neighbour problem by dimensionality reduction and weighting of the
dimensions according to the query point. This amounts to a redefinition
of a metric to be used. In all cases, it is not hard to see that the
performance of any indexing scheme is poor if almost the whole dataset
is to be retrieved.

##### Range Queries

Turning to range queries in quasi-metric spaces we adopt the paradigm
outlined in Subsection 5.7.2 . The radius is fixed while the query
centres are distributed according to a measure @xmath on @xmath . We are
interested in the number of blocks that need to be processed in order to
answer the query @xmath which would give us an estimate on the @xmath
and the access overhead. Since metric and quasi-metric trees are built
hierarchically so that at each level and at each node we have a set
covering a portion of the dataset, the same result can be used to give
an estimate for the @xmath .

###### Lemma 5.7.9.

Let @xmath be a quasi-metric space, @xmath and @xmath . Then @xmath ,
where @xmath .

###### Proof.

Suppose @xmath . Then there exists @xmath such that @xmath . By the
Lemma 2.1.6 , @xmath . â

###### Lemma 5.7.10.

Let @xmath be a pq-space, @xmath a Borel subset of @xmath , @xmath and
@xmath . Then @xmath .

###### Proof.

Suppose that @xmath and @xmath . Let @xmath . Then @xmath and therefore
@xmath , leading to a contradiction. â

The following is proved using a similar technique to the Lemma 4.2 of [
154 ] . In addition to the worst case result similar to the one provided
in [ 154 ] , we also give a bound for the average case performance which
is arguably more important than the worst case.

###### Theorem 5.7.11.

Let @xmath be a pq-space, @xmath and @xmath a collection of subsets
@xmath such that @xmath and for all @xmath , @xmath . Denote by @xmath
the generalised inverse of @xmath at @xmath . Then, for any @xmath ,

1.   There exists @xmath such that @xmath meets at least

      -- -------- --
         @xmath   
      -- -------- --

    elements of @xmath .

2.   A left ball @xmath around @xmath meets on average (in @xmath ) at
    least

      -- -------- --
         @xmath   
      -- -------- --

    elements of @xmath .

###### Proof.

By assumption on each @xmath and by the choice of @xmath , @xmath .
Decompose @xmath into a collection of pairwise disjoint subfamilies
@xmath , @xmath in a such way that @xmath for each @xmath . Clearly,

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath . Then, by the Lemmas 5.7.9 and 5.7.10 ,

  -- -------- --
     @xmath   
  -- -------- --

and hence the probability that a random left ball of radius @xmath does
not intersect @xmath is less than @xmath . For any @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

The first claim follows by choosing @xmath such that @xmath so that
@xmath . To prove the second statement observe that the probability that
a random ball of radius @xmath meets at least @xmath elements is at
least @xmath . Hence, the average number of subsets of @xmath
intersecting a ball of radius @xmath is at least @xmath . â

Our result directly leads to the following Corollary stated in terms of
a range similarity workload (with fixed radius). Note that the open
balls are replaced by the closed balls in order to be consistent with
the definition of the range similarity workload.

###### Corollary 5.7.12.

Let @xmath and @xmath be a workload where @xmath (the left closed balls
are taken with respect to a quasi-metric @xmath on @xmath ). Suppose the
dataset @xmath and the query centres are distributed according to the
Borel probability measure @xmath on @xmath . Let @xmath be a finite set
of blocks such that @xmath and for any @xmath , @xmath . Then the number
of blocks accessed to retrieve the query @xmath is on average at least
@xmath and in the worst case at least @xmath or @xmath , whichever is
smaller. â

As observed in Chapter 4 , for many metric spaces we have @xmath where
@xmath is the dimension of the space. In this case it is easy to see
that any indexing scheme, unless its blocks have all very small measure,
will need to scan very many blocks in order to retrieve not only the
worst case but also a typical range query. Even if the access overhead
is not large, the sequential scan of the whole dataset might outperform
an indexing scheme due to the overhead associated with the tree
structure. The bounds from the Theorem 5.7.11 while certainly not tight,
give some indication on the number of blocks that can be expected to be
retrieved.

Note that the Theorem 5.7.11 holds only for @xmath â the value @xmath is
the scale at which we observe such phenomenon. Obviously, at the scales
smaller than @xmath the indexing scheme need not suffer in performance.
Observe that both @xmath and @xmath are involved but their role is not
the same. The left concentration function determines the scale at which
the concentration effect take place while the @xmath establishes the
number of bins accessed. For âbadâ performance it is necessary that the
@xmath decreases sharply near @xmath .

Since our metric and quasi-metric indexing schemes, as defined in
Sections 5.3 and 5.4 involve covering sets at each level of the tree, it
is straightforward to apply the Theorem 5.7.11 to derive the bounds for
the number of certification function evaluations at each level.

#### 5.7.4 Dimensionality estimation

Unlike our approach above, which uses only geometric assumptions and
where the performance is linked to the concentration functions, Pagel,
Korn and Faloutsos [ 149 ] seek to estimate the performance of nearest
neighbour query retrieval based on fractal (Hausdorff or correlation)
dimensions of the dataset. This line of investigation stems from the
observation that for real datasets embedded in vector spaces, features
are often correlated and hence the estimates based on independence
assumptions are too pessimistic. Hence the effort to find the ârealâ
dimensionality of the datasets.

Traina, Traina and Faloutsos [ 188 ] introduced the distance exponent
which gives the intrinsic dimension of any metric space by assuming that
(at least for small @xmath ), the size of a ball @xmath grows
proportionally to @xmath where @xmath is the dimension of the space.
They claimed that performance of metric trees could be well approximated
in terms of the distance exponent. As a part of his summer research
assistantship at the Australian National University in summer 1999/2000,
the thesis author performed some experiments to determine the ways of
estimating the distance exponent from the datasets. These previously
unpublished results are presented in the Appendix A .

In [ 36 ] another definition of the intrinsic dimensionality is given
(again in terms of the distance distribution) and bounds on the number
of distances to be evaluated by metric indexing schemes are derived.

### 5.8 Discussion and Open problems

So far we have provided a conceptual framework for similarity search and
hinted that the Curse of Dimensionality is related to the concentration
phenomenon. The Theorem 5.7.11 extends the previous results to the case
of range searches in quasi-metric spaces. We next outline possible
directions for further investigation.

#### 5.8.1 Workload reductions

Our definition of an indexing scheme (Definition 5.2.15 ) emphasises the
three structures which are found in all examples known to us: the set of
blocks that cover the dataset, the tree structure supporting an access
method and the decision functions. While this setting allows us to
directly identify the factors that influence the performance, access
methods for similarity queries could be investigated through workload
reductions as in Section 5.6 , without the explicit reference to
indexing schemes.

Consider a tree workload , @xmath where @xmath is a finite rooted
directed weighted tree, such that every edge is assigned a zero weight
in the direction towards the root and a positive weight in the opposite
direction. The @xmath is the set of range similarity queries induced by
the path quasi-metric (Section 2.7 ). There is an obvious access method
associated with such workload: traverse the tree starting from the query
point and retrieve all nodes closer than the cutoff value.

Observe that any metric or quasi-metric indexing scheme where the blocks
are pairwise disjoint can be represented as a projective reduction of
the original workload @xmath to a discrete workload mapping each point
to its block, followed by an inductive reduction to a tree workload. In
our notation,

  -- -------- --
     @xmath   
  -- -------- --

The requirement that the blocks are pairwise disjoint comes from @xmath
being a function â this is a limitation that may need to be overcome.

While this approach is perhaps too abstract and limited at this stage,
hiding the decision functions in the reduction maps, it opens new lines
of investigation. In particular, one can ask if all access methods
involve reductions to inner workloads and attempt to construct access
methods involving inductive reductions to non-tree workloads.

Another topic for investigation would be to construct a hierarchy of all
workloads (with measures on the sets of queries) according to their
indexability , a term introduced in [ 87 ] . For example, a workload
would be higher in the hierarchy if it is more difficult to index and
one could decide indexability of any particular workload in reference to
some canonical workloads. It is clear that the trivial workload should
be on the top of the hierarchy as the most difficult to index.

For mm-spaces, one can hope to be able to use Gromovâs relation @xmath
between mm-spaces ( [ 79 ] , Chapter @xmath , pp. 133â140): for two
mm-spaces @xmath and @xmath , @xmath (Lipschitz) dominates @xmath ,
denoted @xmath , if there exists a 1-Lipschitz map @xmath pushing
forward the measure @xmath to a measure @xmath on @xmath proportional to
@xmath . Obviously, a one point space @xmath (with any measure) is a
minimal mm-space and the more concentrated a space is, the more it is
dominated by other mm-spaces. This notion should be able to be
generalised to quasi-metric spaces with measure. Going even further, one
would wish to include the dataset in any resulting theory.

#### 5.8.2 Certification functions

As we noted before, the bounds from the Corollary 5.7.12 are not tight â
they usually indicate better than actual performance. Indeed, much
closer estimates can be obtained if the distributions of the values of
the certification functions are known, such as in [ 39 ] where they
correspond to the distance distributions. Ciaccia and Patella also
emphasise that their model attests that the performance depends only on
the distributions of the index and comparison distances (i.e. the
certification functions) and not on the query distance. This is not
contrary to our results â our bounds are for a best possible indexing
scheme and the performance in practice could be much worse.

Hence, there are reasons to believe that the main reason for the Curse
of Dimensionality is not the inherent high-dimensionality of datasets,
but a poor choice of certification functions. Efficient indexing schemes
require usage of dissipating functions, that is, 1-Lipschitz functions
whose spread of values is more broad, and which are still
computationally cheap. Such functions correspond to âtighterâ covering
sets with little overlap between them. This interplay between complexity
and dissipation is, we believe, at the very heart of the nature of
dimensionality curse, at least in relation to the @xmath . Requirements
for blocks to contain certain number of points have a large contribution
as well.

Generic metric indexing schemes use only distances (from points) to
construct their certification functions. While this ensures that they
can be applied to any metric space, it may also be significant
limitation if the distances are computationally expensive. More specific
knowledge of the geometry of the domain is clearly necessary to produce
computationally cheaper certification functions. The QIC-M-tree [ 39 ]
is a great step in this direction as it allows the user to specify three
distances to be used. It should be possible to go even further by
developing a structure which allows the user to specify classes of
certification functions and an algorithm which fits them to a dataset
and produces an indexing scheme. The insight gained by the approaches
attempting to reduce overlap between the covering sets associated with
the nodes of a metric tree, such as Slim-trees [ 189 ] , will no doubt
play a role.

### 5.9 Conclusion

Our proposed approach to indexing schemes used in similarity search
allows for a unifying look at them and facilitates the task of
transferring the existing expertise to more general similarity measures
than metrics. In particular, we have extended the concepts associated to
metric workloads to the quasi-metric workloads.

We hope that our concepts and constructions will meld with methods of
geometry of high dimensions and lead to further insights on performance
of indexing schemes. While we have not yet reached the stage where
asymptotic geometric analysis can give accurate predictions of
performance as there exists no algorithm for estimating concentration
functions from a dataset, at least it leads to some conceptual
understanding of their behaviour. We have deliberately ignored
non-consistent indexing schemes in our discourse â while they may show
much better performance, they do so at a price of losing some members of
the query.

In the next Chapter we shall further illustrate our concepts on the
concrete dataset of peptide fragments and point out some specific issues
affecting performance of indexing schemes.

## Chapter 6 Indexing Protein Fragment Datasets

While the previous chapters emphasised the theory, laying the
foundations and introducing the concepts, the present chapter and the
one following focus on applications to actual protein sequence datasets.
The present chapter has two principal aims: to illustrate the notions of
Chapter 5 on the sets of biological sequences and to introduce an
indexing scheme for datasets of short peptide fragments to be used for
biological investigations of Chapter 7 .

An additional reason for studying indexing schemes for short peptide
fragments is that it has been frequently pointed in the literature [ 32
, 143 , 99 , 100 , 103 , 29 , 144 , 70 ] that algorithms for indexing
short fragments could be used as subroutines of BLAST-like programs for
searches of full sequences. It is hoped that as a part of the future
work, the experience gained from indexing short fragment could be
applied to the challenge of indexing datasets of full DNA and protein
sequences.

### 6.1 Protein Sequence Workloads

Let @xmath denote the standard 20 amino acid alphabet. A full sequence
workload has the domain @xmath and the sets of queries consisting of
range or kNN queries based on the quasi-metric corresponding to the
local (Smith-Waterman) similarity scores based on BLOSUM matrices and
affine gap penalties. The dataset in this case is any actual set of
protein sequences.

A short fragment workload has the domain @xmath , the set of all amino
acid sequences of length @xmath which will mostly range from 6 to 12.
The set of queries consists of range or kNN queries based on an @xmath
-type quasi-metric extending a quasi-metric @xmath on @xmath (Section
3.2 ). The co-weightable quasi-metric @xmath is derived from a
similarity score matrix @xmath from the BLOSUM family using the formula
@xmath while the dataset is obtained from a full sequence dataset by
taking all fragments of length @xmath from all sequences.

Depending on the protein sequence dataset, there may exist cases where
two short fragments have the same sequence (Subsection 6.1.2 ). For the
purpose of this thesis, a kNN query is defined with respect to the
original fragment dataset (which is therefore a pseudo-quasi-metric
space), not to the quotient set where points with identical sequence are
merged into one point.

Most of the present chapter, as well as Chapter 7 , examines short
fragment workloads with some ideas transferable to full sequence
workloads. The remainder of the present section investigates some
geometric aspects of sets of short peptide fragments.

#### 6.1.1 Sequence datasets

Two protein sequence datasets were used for investigations of the
present chapter: NCBI nr (non-redundant) [ 208 ] and SwissProt [ 23 ] .

The NCBI nr dataset is a comprehensive general protein sequence
database, including entries from most other major protein sequence
databases (such as SwissProt) as well as the translated coding sequences
from GenBank entries (GenPept). Where multiple identical sequences
exist, they are consolidated into one entry. The nr dataset is the main
dataset searched by NCBI BLAST and the latest version can be downloaded
from ftp://ftp.ncbi.nlm.nih.gov/blast/db/ where other datasets searched
by NCBI BLAST can be found as well. Since the full nr dataset is very
large (the version from June 2004 contains 1,866,121 sequences
consisting of 619,474,291 amino acids) smaller samples rather than the
full dataset were used. It should be noted that many protein sequences
belonging to GenPept and hence nr were translated from coding segments
of GenBank sequences that were verified solely using computational
techniques, that is, without experimental validation. Thus, nr may
contain sequences which are not expressed in any organism.

The SwissProt dataset, maintained at the Swiss Institute of
Bioinformatics http://www.expasy.org/sprot/ , is âa curated protein
sequence database which strives to provide a high level of annotation
(such as the description of the function of a protein, its domains
structure, post-translational modifications, variants, etc.), a minimal
level of redundancy and high level of integration with other databasesâ.
Its entries contain, apart from the sequence information, extensive
functional annotation, literature citations and links to other
resources. Because of its moderate size, non-redundancy and high level
of sequence characterisations, SwissProt (Release 43.2 of April 2004,
containing 144,731 sequences consisting of 53,363,726 amino acid
residues) was used as the main dataset for the experiments of this
chapter.

#### 6.1.2 Unique fragments

SwissProt and nr are (almost â there are few duplicate sequences in
SwissProt) non-redundant. However, when short fragments are taken to
form the fragment database, it often occurs that multiple instances of
the same fragment exist (Figure 6.1 ). In other words, the underlying
measure on @xmath where @xmath is small is not the counting measure.

For similarity searches, this situation can be handled in two ways. If
many duplicate fragments are present (very short fragment lengths), a
preprocessing step is necessary to collect the identical fragments
together, introducing some space overhead but significantly saving
search time. If relatively few duplicates (longer fragment lengths) are
present, they can be treated as separate points introducing an
additional time cost for unnecessary distance evaluations but avoiding
space overhead for collecting identical fragments.

A further observation that can be made from the Figure 6.1 is that for
very short fragments, almost every possible sequence is represented in
the dataset â the workload is effectively inner, allowing the
possibility of using combinatorial algorithms for indexing. This is
definitely not true for longer fragments and full sequences where the
workload is outer. For example, the number of potential fragments of
length 10 is @xmath while there are only about 38.5 million (or
0.0004%)) unique fragments in SwissProt.

#### 6.1.3 Random sequences

Most experiments of this chapter, investigating geometry of datasets and
performance of indexing schemes, involve simulating a probability
measure on the set of all possible protein fragments using generated
random sequences. It is necessary to do so because the workloads (with
the exception of sets of fragments of very short lengths) are outer and
it is quite likely that a query sequence would be (slightly) different
from all sequences existing in a dataset. Generally, the âtrueâ
distribution of protein sequences or fragments is unknown and the
measure obtained by counting the points of an actual dataset is not
appropriate because the full natural variation of protein sequences
cannot be captured by any dataset, that is, one always expects to
discover novel sequences. Hence, it is necessary to use theoretical
models of sequence distributions and attempt to balance the practical
issues, such as the ability to quickly generate sufficiently many random
sequences, with accuracy.

The simplest way of generating random fragments of fixed length is to
assume the underlying measure is the product measure based on background
(overall) amino acid frequencies, that is, to generate each fragment by
an independent, identically distributed process where the probability
measure is given by the background frequencies. Such approach can be
extended to sequences of arbitrary length by modelling sequence length
according to some distribution (for example, discretised log-normal [
151 ] ) and once the length is chosen, proceeding as above.

A more general model, actually used to generate testing datasets for the
experiments of the current chapter, is based on Dirichlet mixtures [ 174
] . As in the previous case, the length of each sequence is taken from a
discretised log-normal distribution and the amino acids of a sequence
are generated by an independent, identically distributed process.
However, the probabilities for that distribution are selected from a
mixture of Dirichlet densities (for a description of Dirichlet
distributions and mixtures see Chapter 11 of the Durbin et.al. book [ 52
] ) instead from a single (background) distribution.

The code and the data for generating random sequences according to
Dirichlet mixtures were obtained from
http://www.cse.ucsc.edu/research/compbio/dirichlets/ . To obtain samples
of fragments of fixed length to be used in experiments, for each desired
length, 5000 non-overlapping fragments were sampled from full sequences
generated according to the above method. The same testing datasets were
used for all experiments ensuring that performances of different
indexing schemes can be directly compared.

#### 6.1.4 Quasi-metric or metric?

Chapter 3 has shown that most common distances on protein sequences are
quasi-metrics. However, since the theory and practice of indexability of
metric spaces is much better studied, it is worthwhile to investigate
the overhead of replacing a quasi-metric by a metric.

From the point of view of performance, the best measure of the average
overhead is the ratio between the sizes of the metric and the
quasi-metric ball containing at least @xmath nearest neighbours with
respect to the quasi-metric. If this ratio is close to 1, the metric and
the quasi-metric have similar geometry and the replacement of the
quasi-metric by a metric is feasible. The average sampled ratios for the
fragment datasets of lengths 6, 9 and 12, using the associated metric
(the smallest metric majorising the quasi-metric), are shown in the
Figure 6.2 .

It is clear that replacement of quasi-metric by a metric would be very
costly except for the nearest neighbour searches of very short fragments
(length 6) and that it is indeed necessary to develop the theory and
algorithms that would allow the use of the intrinsic quasi-metric. This
observation was one of the principal motivations behind the development
of the theory of quasi-metric trees in Chapter 5 .

#### 6.1.5 Neighbourhood of dataset

A further way of assessing the way a dataset is embedded into its domain
is by considering how far the closest point from the dataset is to any
point in the domain, or alternatively, the smallest @xmath such that the
dataset forms an @xmath -net inside the domain. Even more information is
revealed by the distribution of distances of points in the domain to the
dataset; for example, it can be determined if there is a sizable amount
of points significantly farther from the dataset than the rest. Note
that such distribution function clearly depends on the underlying
measure on the domain (query distribution).

While an overwhelming amount of computation would be necessary to obtain
the exact distribution, it is possible to approximate it by resorting to
simulation, that is, by generating points according to the assumed
measure and finding for each generated point the distance to its nearest
neighbour in the dataset. If an efficient indexing scheme is available,
such approach is computationally inexpensive. Figure 6.3 shows the
results for SwissProt fragment datasets of lengths 6, 9 and 12 using the
sample points generated according to Dirichlet mixtures (Subsection
6.1.3 ).

The estimated distribution for the fragments of length 6 supports the
observations from Subsection 6.1.2 that the workloads based on sets of
fragments of very short length are close to inner: almost 60 @xmath of
random points are in the dataset (the BLOSUM62 quasi-metric (Figure 6.10
) and hence its derived @xmath type distance on fragments is @xmath and
therefore the distance of @xmath implies identical fragments) and most
of the remainder are within one amino acid substitution from a dataset
point (Figure 6.10 shows the full BLOSUM62 quasi-metric). In fact, the
number of random points belonging to the dataset is much greater than
the proportion of the dataset in the domain from the Figure 6.1 (about
30 @xmath ), which is essentially based on the counting measure on the
domain. This (not surprisingly) indicates that the measure based on
Dirichlet mixtures indeed approximates the dataset better than the
counting measure. The distributions for the lengths 9 and 12 indicate
that a neighbour is very likely to be found in the biologically
significant ranges (20â35).

#### 6.1.6 Distance Exponent

Distance exponent (Appendix A ), measuring the rate of growth of balls
in a metric space can be used to estimate the dimensionality and hence
the complexity of workloads. The theory presently applies only to metric
spaces (although the rationale is equally valid for quasi-metric spaces)
and therefore the associated metric to the BLOSUM62 quasi-metric was
used. Since the estimate of the dimensionality of the full domain,
rather than just of the dataset was desired, the average size (in terms
of points of the dataset) of a ball of given radius centred at a random
point was computed and used to estimate the distance exponent. This
approach is justified by the Remark A.1.6 , provided the measure induced
by the dataset is a good approximation to the measure used to generate
the ball centres (i.e. the measure on the domain). The sizes of the
balls of small radii for datasets of length 6 and 9 are shown in Figure
6.4 (log-log scale).

It is apparent that the log-log graphs are not linear and therefore the
method based on fitting a polynomial (Subsection A.3.2 ) was used for
distance exponent estimation. The estimated distance exponent is 7.6 for
the fragments of length 6 and 10.6 for the fragments of length 9. Hence,
in this context, the datasets are approximately equivalent to the cubes
@xmath and @xmath respectively, with the @xmath metric (Subsection A.2.1
). An interesting problem is to determine if âgoodâ embeddings into
cubes @xmath exist and if so, to index them as vector spaces, say using
X-tree.

#### 6.1.7 Self-similarities

As mentioned previously, in Chapter 3 as well as in the current chapter,
protein sequence fragments with (some) BLOSUM similarity measures can be
treated as co-weighted quasi-metric spaces with the co-weight of each
point given by its self-similarity. Self-similarities are significant
because they are the sole source of asymmetry of the quasi-metric: we
have @xmath where @xmath denotes the asymmetry function introduced in
Section 4.6 . Therefore, the distribution of self-similarties determines
the âdistanceâ of the quasi-metric space from its associated metric
space. Furthermore, if self-similarities of dataset points take very few
values, as is the case with short fragment datasets, the co-weighted
quasi-metric space can be divided into metric fibres which can be
indexed separately using an indexing scheme for metric workloads (FMtree
â Example 5.6.1 ). Figure 6.5 shows the estimates of distributions of
self-similarities of SwissProt fragment datasets of length 7 and 12
based on approximately 1,000,000 samples.

It can be seen that both distributions are skewed to the right and that
the distribution for the length 12 is more spread out, that is, less
concentrated. However, if something is to be inferred about the measure
concentration and hence indexability from self-similarities, it is
necessary to take into account the scale. The median distance to the
nearest neighbour for the length 12 workload is about 23 (Figure 6.3 )
while it clearly cannot be greater than 10 in length 7 case (the data
for length 7 is not available in the Figure 6.3 but it can be inferred
from the data for lengths 6 and 9). Thus, if scaled in this way, the
distribution for the length 7 would be indeed less concentrated.

### 6.2 Tries, Suffix Trees and Suffix Arrays

Trie, suffix tree and suffix array data structures form the basis of
many of the established string search methods and provide an inspiration
for some features of the FSIndex access method described in Section 6.3
.

Let @xmath be a finite alphabet and @xmath be a collection of @xmath
-strings (i.e. @xmath ). A trie [ 60 ] is an ordered tree structure for
storing strings having one node for every common prefix of two strings.
The strings are stored in extra leaf nodes (Figure 6.6 ). A PATRICIA
tree (Practical Algorithm to Retrieve Information Coded in Alphanumeric
[ 140 ] ) is a compact representation of a trie where all nodes with one
child are merged with their parent. Tries and PATRICIA trees can be
easily used for string searches, that is, to find if a string @xmath
belongs to @xmath . Such searches take @xmath time where @xmath .

Now consider a single (long) string @xmath where @xmath . The suffix
tree [ 206 ] for @xmath is the PATRICIA tree of the suffixes of @xmath
and can be constructed in @xmath time [ 206 , 136 , 190 ] . Suffix
trees, in their original form as well as generalised to suffixes of more
than one string, can be used to solve a great variety of problems
involving matching substrings of long strings (Gusfield, in his book [
83 ] dedicates full five chapters exclusively to suffix trees and their
applications).

One disadvantage of suffix trees is that they often occupy too much
space â up to @xmath in many common cases [ 83 ] . The suffix array data
structure, first proposed by Manber and Myers [ 129 ] , is a compact
representation of the suffix tree for @xmath consisting of the array
@xmath , of integers in the range @xmath specifying the lexicographic
ordering of suffixes of @xmath (i.e. @xmath is the starting position of
the @xmath -th suffix of @xmath in lexicographic order), and the array
@xmath , where @xmath contains the longest common prefix of the
substrings starting at positions @xmath and @xmath (the first element of
@xmath is @xmath ). Efficient @xmath construction algorithms exist and
using binary search on array @xmath and the @xmath values, it is
possible to search for occurrence of a string @xmath in @xmath in @xmath
time, where @xmath [ 83 ] . Figure 6.7 shows an example of a suffix tree
and a suffix array.

PATRICIA trees (and hence suffix trees and arrays), being compact
representations of a set of strings, can be used to speed-up string
comparisons and searches [ 72 ] . Indeed it is very easy to construct a
quasi-metric tree for the short fragment similarity workload @xmath
(Section 6.1 ) with a quasi-metric @xmath . The tree is given by a trie
or a PATRICIA tree for @xmath and each block is a set containing a
single fragment associated with a leaf node. At each non-root node, a
certification function calculates the distance between a prefix given by
the path from the root to the node in question and a prefix of the query
fragment of the same length, say @xmath . In effect, a certification
function calculates the distance from the query to the âcylindrical setâ
of fragments where the letters at first @xmath positions are fixed while
varying arbitrarily at the remaining @xmath positions.

### 6.3 FSIndex

FSIndex is an access method for short peptide fragment workloads mainly
based on two procedures: combinatorial generation and amino acid
alphabet reduction.

For very short fragments (lengths 2-4), the number of all possible
fragment instances is very small (for length 3, @xmath ) and almost
every fragment instance generated exists in the dataset. Hence, it is
possible to enumerate all neighbours of a given point in a very
efficient and straightforward manner using digital trees or even
hashing. For larger lengths, the number of fragments in a dataset is
generally much smaller than the number of all possible fragments (Figure
6.1 ) and generation of neighbours is not feasible. If it were to be
attempted, most of the computation would be spent generating fragments
that do not exist in the dataset. Hence the idea of mapping peptide
fragment datasets to smaller, densely and, as much as possible,
uniformly packed spaces where the neighbours of a query point can be
efficiently generated using a combinatorial algorithm.

Partitions of amino acid alphabet provide the means to achieve the
above. Amino acids can be classified by chemical structure and function
into groups such as hydrophobic, polar, acidic, basic and aromatic
(Table 1.1 ). Such classification appears in every undergraduate text in
biochemistry and has been previously used in sequence pattern matching [
176 ] . In general, substitutions between the members of the same group
are more likely to be observed in closely related proteins than
substitutions between amino acids of markedly different properties. The
widely used similarity score matrices such as PAM [ 45 ] or BLOSUM [ 88
] are derived from target frequencies of substitutions and therefore
capture these relationships more precisely.

The required mapping is constructed as following. Given a set of
fragments of fixed fragment length @xmath , an alphabet partition @xmath
is chosen for each position @xmath , where @xmath . This induces the
mapping @xmath where @xmath . The members of @xmath are called bins and
the number of bins is denoted by @xmath . The partitions @xmath are
often equal for each @xmath . An important consequence of such mapping
is that distances to bins are easy to compute and can be used as
certification functions.

###### Remark 6.3.1.

Positions in each fragment are zero based, that is, numbered from @xmath
rather than from @xmath , because the reference implementation of
FSIndex is in the C programming language [ 109 ] where arrays are
indexed from @xmath .

#### 6.3.1 Data structure and construction

The FSIndex data structure consists of three arrays: @xmath , @xmath and
@xmath . The array @xmath contains pointers to each fragment in the
dataset and is sorted by bin. The array @xmath , of size @xmath is
indexed by the rank of each bin and contains the offset of the start of
each bin in @xmath (the @xmath -th entry gives the total number of
fragments while the last entry is used solely for index creation). The
bin ranking function @xmath is defined as follows. For each @xmath let
@xmath be a ranking function of @xmath and define @xmath by

  -- -------- -- -------
     @xmath      (6.1)
  -- -------- -- -------

In the case @xmath the empty product above is taken to be equal to
@xmath . Then,

  -- -------- -- -------
     @xmath      (6.2)
  -- -------- -- -------

In addition, each bin is sorted in lexicographic order and the value of
@xmath provides the length of the longest common prefix between @xmath
and @xmath . The value of @xmath is set to @xmath . Figure 6.8 depicts
an example of the full structure of an FSIndex.

###### Remark 6.3.2.

The arrays @xmath and @xmath are inspired by suffix arrays but the order
of offsets in @xmath is different because @xmath is first sorted by bin
and then each bin is sorted in lexicographic order. Sorting @xmath
within each bin and constructing and storing the @xmath array is not
strictly necessary and incurs a significant space and construction time
penalty. The benefit is improved search performance for large bins,
compensating for unbounded bin sizes. In effect, each bin is subindexed
using a compact version of a PATRICIA tree.

To construct the FSIndex data structure, any sorting algorithm can be
used to produce the @xmath array from which the @xmath and @xmath arrays
can be easily computed. Algorithm 6.3.3 outlines the reference
implementation.

The space requirement of FSIndex is @xmath . The exact space and time
complexity of the construction algorithm depends on the sorting
algorithm used for sorting the @xmath array. If the quicksort [ 94 ]
algorithm is used (the reference implementation), the space requirement
is @xmath and the running time is @xmath on average and @xmath in the
worst case. Using radix sort [ 173 ] , the average and worst case
running time can both be reduced to @xmath with @xmath (or @xmath )
additional space overhead. Another alternative is to use heapsort [ 211
] to sort the @xmath array with the time complexity @xmath but no
additional space overhead.

#### 6.3.2 Search

Search using FSIndex is based on traversal of implicit trees whose nodes
are associated with reduced fragments (bins).

###### Definition 6.3.3.

Let @xmath . For any @xmath and @xmath , denote by @xmath the sequence
@xmath .

Let @xmath . Denote by @xmath the tree having the root @xmath connected
to the subtrees @xmath for all @xmath and @xmath and by @xmath the tree
@xmath . @xmath

The trees @xmath are connected and unbalanced and can be shown to have
depth @xmath while the root has the degree @xmath . The tree topology is
clearly independent of the choice of @xmath . If @xmath , @xmath is
isomorphic to the multinomial tree of order @xmath . If @xmath , such
tree is called the binomial tree of order @xmath . An example is shown
in the Figure 6.9 .

The following Proposition is easily established.

###### Proposition 6.3.4.

Let @xmath , @xmath be finite sets and @xmath . Then there exists a
bijection between the nodes of @xmath and the set @xmath . â

Retrieval of a quasi-metric range query @xmath using the implicit tree
structure is conceptually straightforward. Given a query point @xmath
and the radius @xmath , map @xmath to its bin @xmath and traverse the
tree @xmath from the root. At each node @xmath , calculate the distance
@xmath and prune the subtree rooted at @xmath if @xmath . For every
visited node which is not pruned, calculate the distance to each
fragment in the associated bin and collect all the fragments whose
distance from @xmath is not greater than @xmath .

The indexing scheme providing the access method described above can be
described as a query partitioning indexing scheme (Subsection 5.6.2 )
where the workload @xmath is partitioned into a union of valuation
workloads @xmath for each @xmath , where @xmath . Each valuation
workload is associated with the valuation indexing scheme @xmath ,
defined as follows. The set of blocks is @xmath and the tree @xmath
consists of the tree @xmath where a leaf node corresponding to the same
reduced sequence is attached to each node. The function @xmath
increasing on @xmath is given by Â¹ Â¹ 1 This is a slight abuse of
notation because the tree @xmath now has two distinct copies of each
bin: one as an inner node and one as a leaf node attached to the inner
node. The context should be clear nevertheless.

  -- -------- --
     @xmath   
  -- -------- --

It is clear that @xmath is indeed a valuation indexing scheme. The
proposition 6.3.4 ensures that the number of leaf nodes is @xmath while
@xmath is increasing on @xmath because each child node is obtained by
replacing one letter from the parent with another, different letter, an
operation which increases the distance. Therefore, by the Theorem 5.5.4
, @xmath is a consistent indexing scheme and it follows that the query
partitioning indexing scheme over @xmath is also consistent.

Unlike most published metric indexing schemes mentioned in Chapter 5 ,
FSIndex does not have a balanced tree. Therefore, the expected average
and worst-case search time complexity is @xmath â the overhead is
proportional to @xmath , the number of inner nodes. So, based on these
considerations, FSIndex is not scalable for queries of a fixed radius.
However, the performance can be to a large extent controlled by the
choice of alphabet partitions and hence some scalability can be achieved
by using more partitions for larger datasets in order to reduce the
scanning time while incurring some additional overhead.

#### 6.3.3 Implementation

Descriptions of FSIndex algorithms in this section are based on the
reference implementation developed in the C programming language [ 109 ]
(some optimisations are omitted for clarity). Table 6.1 shows the
descriptions of all global variables and functions used.

##### Construction

The construction algorithm (Algorithm 6.3.3 ) is closely related to
counting sort [ 173 ] . It makes three passes over data fragments: to
count the number of fragments in each bin, to insert the fragments into
the @xmath array and to compute the @xmath array. It allocates the
memory for the arrays after counting.

The fragment dataset is in practice always obtained from a full sequence
dataset by iterating over all subfragments of length @xmath from each
sequence and it is often necessary to verify each fragment and reject
those that contain non-standard letters such as âXâ, âBâ or âZâ that do
not represent actual amino acids and violate the triangle inequality for
the score matrices. Therefore, the true number of data points is not
known before the first pass through the dataset.

{pseudocode}

[ovalbox]CreateFSIndexX, m, N, Ï , r bin \GETS \CALL AllocateMemoryN+2
bin[0] \GETS 0, bin[1] \GETS 0
\COMMENT Count bin sizes
n \GETS 0
\FOREACH sâX \DO \BEGIN i \GETS r( Ï (s))
bin[i+2] \GETS bin[i+2]+1
n \GETS n+1
\END
\FOR i \GETS 2 \TO N+2 \DO bin[i] \GETS bin[i]+bin[i-1]
\COMMENT Insert fragments into bins
frag \GETS \CALL AllocateMemoryn
\FOREACH sâX \DO \BEGIN i \GETS r( Ï (s))
frag[bin[i+1]] \GETS s
bin[i+1] \GETS bin[i+1]+1
\END
\COMMENT Calculate longest common prefixes
\FOR i \GETS 0 \TO N \DO \CALL QuickSortfrag[bin[i]:bin[i+1]]
lcp \GETS \CALL AllocateMemoryn
lcp[0] \GETS 0
\FOR j \GETS 1 \TO n-1 \DO \BEGIN k \GETS 0, s \GETS frag[j-1], t \GETS
frag[j]
\WHILE s_k = t_k \DO k \GETS k+1
lcp[j] \GETS k \END
\RETURN bin, frag, lcp

##### Search

Range search (Algorithm 6.3.3 ) makes a recursive, depth-first traversal
of the implicit tree implemented in the function CheckNode (Algorithm
6.3.3 ). The function ProcessBin (Algorithm 6.3.3 ) scans each bin
associated with an inner node not pruned using the @xmath array in order
to reduce the number of computations necessary to calculate distances to
each member of the bin. Â² Â² 2 Conceptually, Algorithm 6.3.3 is
equivalent to depth-first traversal of a compact form of a PATRICIA tree
for the set of fragments in the bin. The function InsertHit (omitted in
the case of range search) inserts the neighbour into the list of search
results.

The search algorithm computes and stores the values of @xmath ,
@xmath and @xmath for all @xmath and all @xmath before tree traversal so
that the CheckNode function uses a table lookup.

{pseudocode}

[ovalbox]RangeSearch Ï , d, Îµ \COMMENT Recursive tree traversal
\GLOBAL bin,frag,lcp, Î¾ _k, Ï ,r,HL,CD
Initialise list of hits @xmath
Initialise cumulative distances @xmath , @xmath
u \GETS r( Ï ( Ï ))
\CALL ProcessBinu
\CALL CheckNodeu,0,0
\RETURN HL

{pseudocode}

[ovalbox]CheckNodeu,D,i \COMMENT Recursive tree traversal
\GLOBAL d, Îµ , Î¾ _j, Ï _j
\FOR j \GETS m-1 \DOWNTO i \DO \BEGIN \IF D + min { d( Ï _j, Ï )Â â Ï
âÎ£_j â{ Ï _j( Ï _j)} } â¤ Îµ \THEN \BEGIN \FOREACH Ï âÎ£_j â{ Ï _j( Ï _j)}
\DO \BEGIN E \GETS D + d( Ï _j, Ï )
\IF Eâ¤ Îµ \THEN \BEGIN v \GETS u - Î¾ _k( Ï _j( Ï _j)) + Î¾ _j( Ï )
\CALL ProcessBinv
\CALL CheckNodev,E,j+1
\END \END \END \END

{pseudocode}

[ovalbox]ProcessBinu \COMMENT Sequentially scan all entries.
\GLOBAL d, Îµ ,HL,bin,frag,lcp,CD
n \GETS bin[u+1]-bin[u]
\IF nÂ¿0 \THEN \RETURN
\FOR i \GETS 0 \TO n-1 \DO \BEGIN s \GETS frag[u+i]
\FOR j \GETS lcp[u+i] \TO lcp[u+i+1]-1 \DO CD[j+1] \GETS CD[j] + d( Ï
_j,s_j)
\IF CD[lcp[u+i+1]]â¤ Îµ \THEN \BEGIN \FOR j \GETS lcp[u+i+1] \TO m-1 \DO
CD[j+1] \GETS CD[j] + d( Ï _j,s_j)
\IF CD[m]â¤ Îµ \THEN \CALL InsertHitHL,s,CD[m]
\END \END

The kNN search algorithms use branch-and-bound [ 41 , 93 ] traversal
involving initially setting the radius @xmath to a very large number (
@xmath ), inserting first @xmath data points encountered into the list
of hits and then setting @xmath to be the largest distance of a hit from
a query. From then on, if a point closer to the query than the farthest
hit is found, it is inserted in the list and the previous farthest hit
is removed. Eventually, the current search radius is reduced to the
exact radius necessary to retrieve @xmath nearest neighbours.

The branch-and-bound procedure is implemented using a priority queue
(heap) which returns the farthest data point in the list of hits (Table
6.2 outlines the operations on priority queue). Most of the code for
range search can be reused: it is only necessary to use a different
InsertHit function involving a priority queue (Algorithm 6.3.3 ) and to
initialise the priority queue in the main search function (Algorithm
6.3.3 ). Algorithm 6.3.3 uses the final list of results @xmath as an
auxiliary list to store those neighbours that have the same distance
from the query as the farthest point in the priority queue. It copies
the hits in the priority queue into @xmath after finishing the tree
traversal.

The performance of the branch-and-bound algorithm depends on the order
of nodes visited â it is to a great advantage if the nodes containing
data points closest to the query are visited first so that the bounding
radius becomes small early on. A frequently used solution [ 41 , 93 ] is
to traverse the tree breadth-first, keeping the nodes to be visited in a
second priority queue, where the priority of a node is given by the
upper bound of the distance of its covering set from the query.

The second priority queue is not used for the FSIndex based kNN search.
Since the implicit tree is heavily unbalanced, the branches with
smallest depth are visited first with a similar effect without the
overhead of the second priority queue. The visiting order of nodes is
ensured in the outer loop of the CheckNode function where the index
@xmath starts at @xmath , decreasing to @xmath (Algorithm 6.3.3 ). Since
the order does not affect the range search performance, the same code
can be used for range search.

{pseudocode}

[ovalbox]KNNSearch Ï , d, k \COMMENT Recursive tree traversal
\GLOBAL Îµ , bin,frag,lcp, Î¾ _j, Ï ,r,HL,CD
Initialise list of hits @xmath
Initialise cumulative distances @xmath , @xmath
Initialise priority queue @xmath
u \GETS r( Ï ( Ï ))
Îµ \GETS â
\CALL ProcessBinu
\CALL CheckNodeu,0,0
Insert all hits from @xmath to @xmath
\RETURN HL

{pseudocode}

[ovalbox]InsertHitHL,s,dist \COMMENT Hit insertion for kNN search.
\GLOBAL k, Îµ , PQ
\IF \CALL PQ.SizeÂ¡k \THEN \BEGIN \CALL PQ.Inserts,dist
\IF \CALL PQ.Size = k \THEN \BEGIN s1,dist1 \GETS \CALL PQ.Peek
Îµ \GETS dist1 \END \END \ELSEIF dist Â¡ Îµ \THEN \BEGIN s1,dist1 \GETS
\CALL PQ.Remove
\CALL PQ.Inserts,dist
s2,dist2 \GETS \CALL PQ.Peek
Îµ \GETS dist2
\IF dist1 = dist2 \THEN \CALL HL.Inserts,dist \ELSE \CALL HL.Clear \END
\ELSE \CALL HL.Inserts,dist

#### 6.3.4 Extensions

FSIndex as described so far provides an access method for workloads of
fragments of fixed length with quasi-metric similarity measures.
However, with minor modifications it can be extended to fragment
(suffix) datasets of arbitrary length and almost arbitrary similarity
measures.

##### Arbitrary fragment lengths

In most practical situations, fragment datasets are datasets of suffixes
of full sequences. The FSIndex structure as is can be used without
modifications for answering queries longer than @xmath , the original
length: each fragment of length @xmath is a prefix of a suffix of length
@xmath where @xmath . To search with a query of length @xmath , traverse
the search tree using the first @xmath positions and sequentially scan
all the bins retrieved, using all @xmath positions to calculate the
distance. If @xmath , the few fragments of length @xmath at the end of
each full sequence can be identified and ignored at the sequential scan
step.

Similarly, FSIndex can be used to answer queries centered on fragments
of length @xmath where @xmath . At the construction step, insert all
suffixes, including those of length less than @xmath into the index by
mapping each fragment @xmath such that @xmath , into the bin @xmath ,
where @xmath are chosen so that @xmath .

To answer a query centered on @xmath such that @xmath , traverse the
search tree up to the depth @xmath and sequentially scan all the bins
attached to subtrees rooted at the accepted nodes using first @xmath
positions to calculate the distance. The ranking function given by the
Equations 6.1 and 6.2 ensures that the bins that are the children of a
given node are adjacent in the @xmath array.

##### Arbitrary similarity measures

FSIndex does not directly depend on a quasi-metric: it is constructed
solely from alphabet partitions. While index performance strongly
depends on the way the distance agrees with partitions, the same index
can be used for any distance which is an @xmath -type sum. It is
possible to make even further generalisations.

Let @xmath and suppose @xmath are finite alphabets and @xmath are
arbitrary functions @xmath . Suppose @xmath is given by @xmath . Let
@xmath , @xmath and let @xmath denote the sequence @xmath . It is clear
that the function @xmath given by @xmath is increasing on the tree
@xmath and therefore the FSIndex can be used to answer queries for any
valuation workload or a union of valuation workloads. Important
biological cases include PSSM or profile based similarities which are
exactly @xmath -type sums of real-valued functions at each position as
well as any score matrix based similarity, whether or not the triangle
inequality on the alphabet is satisfied. Note that the above statement
applies only to consistency of the indexing scheme and not to the
computational efficiency of query retrieval.

### 6.4 Experimental Results

This section describes the experiments on actual fragment datasets
carried out to evaluate the performance of FSIndex. Three main classes
of tests were conducted investigating general performance, effects of
similarity measures and scalability. The final set of experiments
compares performance of FSIndex to performances of suffix arrays M-Tree
and mvp-tree.

Each experiment consisted of 5000 searches using randomly generated
queries (Subsection 6.1.3 ). The main measures of performance are the
number of bins and dataset fragments scanned in order to retrieve @xmath
nearest neighbours. The principal reason for expressing the results in
terms of the number of nearest neighbours retrieved rather than the
radius was that it allows comparison across different indexing schemes,
datasets and similarity measures. Furthermore, most existing protein
datasets are strongly non-homogeneous and the number of points scanned
in order to retrieve a range query for a fixed radius varies greatly
compared to the number of points scanned in order to retrieve a fixed
number of nearest neighbours. Nevertheless, most experiments involve
range search algorithms, because they are generally more efficient and
because in some cases no @xmath NN implementation was available.

Other performance criteria were total running time (only shown where all
experiments compared were performed on the same machine with similar
loads) and the percentage of residues (letters) scanned out the total
number of residues in all scanned fragments. The later statistic
measures the effect of sub-indexing each bin using the suffix-array-like
structure which involves âpartiallyâ scanning each fragment with a help
of the @xmath array. The final statistic is access overhead, discussed
in Section 5.7 .

The obvious reference algorithm, which was not run due to excessive
running times for large datasets, is sequential scan of all fragments in
a dataset. Most of the experiments were run on a Sun Fire[tm] 280R
server (733 Mhz CPU).

#### 6.4.1 Datasets and indexes

Experiments investigating general performance and effect of different
similarity measures used overlapping protein fragment datasets derived
from the SwissProt Release 43.2 of April 2004. Scalability experiments
used, in addition to SwissProt, the datasets nr018K , nr036K , nr072K ,
and nr288K , obtained by randomly sampling 18, 36, 72 and 288 thousands
of sequences respectively from the nr dataset (SwissProt fills the gap
because it contains about 150,000 sequences). The experiments comparing
FSindex to suffix arrays and mvp-tree used only the nr018K dataset.

Table 6.3 describes the instances of FSIndex used in the evaluations.
Two instances ( SPNA09 and SPNB09 ) were based on partitions that are
not equal at all positions while the remainder had the same partitions
at all positions.

The choice of amino acid alphabet partitions was mainly a result of
practical considerations based on the BLOSUM62 quasi-metric (Figure 6.10
). It was not possible to partition the alphabet in a way that all
distances within partitions are smaller than distances between and hence
the primary criterion was to have as high lower bound on distances from
any possible query point to any partition but its own. The additional
criterion was to balance to the greatest possible extent the sizes of
bins and to avoid having too many empty bins which would introduce large
overhead. Therefore, the number of partitions per residue was decreased
with fragment length by amalgamating âcloseâ partitions. Some amino
acids having very small overall frequencies, such as tryptophan (âWâ)
and cysteine (âCâ), were in some cased clustered together in order to
reduce the total number of partitions, even though their distances from
and to any other amino acid are very large.

The alphabet partitions from the Table 6.3 agree with the âbiochemical
intuitionâ (i.e. the classification from the Table 1.1 based on chemical
properties of amino acids). For example, the clusters outlined in the
Figure 6.10 used for fragments of length 9 approximately correspond to
polar uncharged, hydrophobic, basic, acidic, aromatic and âotherâ amino
acids. The partition used for the fragments of length 12 is obtained by
merging together acidic and basic as well as aromatic and âotherâ
clusters. An interesting fact is that in this case each of the the four
clusters has a relative frequency very close to @xmath .

Despite efforts to balance bin sizes, the distributions of bin sizes
were strongly skewed in favour of small sizes in all cases (Figure 6.11
shows one example) with many empty but also a few very large bins. Such
distributions appear to follow the DGX distribution, a generalisation of
Zipf-Mandelbrot law described by Bi, Faloutsos and Korn [ 21 ] .

#### 6.4.2 General performance

Figures 6.12 , 6.13 and 6.14 present selected statistics of search
experiments for fragment lengths 6,9 and 12 respectively, consisting in
each case of range queries retrieving 1, 10, 50, 100, 500 and 1000
nearest neighbours with respect to the BLOSUM62-based @xmath -type
quasi-metric. For each length, @xmath NN searches were performed prior
to range searches using the index that was expected to be the fastest in
order to determine the search ranges for each random query fragment.

#### 6.4.3 Dependence on similarity measures

While queries based on more than one similarity measure can be used on a
single FSIndex, it is to be expected that similarity measures different
from the one originally used to determine the partitions would have
worse performance. To investigate the difference in performance for
different BLOSUM matrices, range queries needed to retrieve 100 nearest
neighbours of testing fragments of length 9 were run using the index
SPEQ09 which was performing the best for the length 9 in the previous
experiment (Figure 6.13 ). In addition, searches were performed using
the PSSMs (Section 3.7 ) constructed for each test fragment from the
results of a BLOSUM62-based 100 NN search in order to gain an insight in
the actual search performance using the PSSM constructed from the
results of a previous search that could be used to plan the biological
experiments in Chapter 7 . Table 6.4 presents a summary of the results.

#### 6.4.4 Scalability

Figure 6.15 shows the results of a set of experiments involving
instances of FSIndex based on datasets of fragments of length 9 of
different sizes ( nr018K , nr036K , nr072K , SwissProt and nr288K ). All
indexes used the same alphabet partition (Table 6.3 ) and all queries
were based on the BLOSUM62 @xmath -type quasi-metric. Unlike the Figures
6.12 , 6.13 and 6.14 , Figure 6.15 does not contain the total running
time graph because the experiments were performed on different machines
but instead includes a plot showing the total number of residues scanned
against the database size. This graph indicates the dependence of the
performance of (an example of) FSIndex on dataset size, that is, its
scalability.

#### 6.4.5 Access overhead

Figure 6.16 summarises some of the results of Sections 6.4.2 and 6.4.4
by showing the average access overhead (Definition 5.7.4 ), that is, the
average ratio between the number of fragments scanned and the number of
true neighbours retrieved, for all combinations of indexes and fragment
lengths available. Range search algorithm and the BLOSUM62-based @xmath
-type quasi-metric were used in all cases.

#### 6.4.6 Comparisons with other access methods

The final set of experiments compares FSIndex with M-tree, mvp-tree and
suffix arrays. In general, other methods take significantly more space
and time compared with FSIndex and it was therefore necessary to
restrict the comparisons to small datasets and queries retrieving fewer
neighbours.

##### M-tree

Recall that M-tree is a paged metric access method that stores the
majority of the structure in secondary memory, usually on hard disk.
This is in contrast with the implementations of FSIndex, mvp-tree and
suffix arrays used here, which store the whole index structure in
primary memory. Hence, although M-tree occupies large amounts of space,
most of the costs are associated with the secondary memory, which is
much less expensive. On the other hand, I/O costs, not considered here,
can be quite large.

The experiments described below were performed earlier than the other
experiments presented in the present Chapter, using the resources from
the High Performance Computing Laboratory (HPCVL), a consortium of
several Canadian universities that the thesis author had the fortune to
access during his visits to University of Ottawa. M-tree was not tested
directly but as a part of the FMTree structure (Example 5.6.1 ) that
allows use of metric indexing schemes for retrieval of quasi-metric
queries.

The FMTree structure consisted of an array of M-trees with additional
data describing the score matrix and the distribution self-similarities.
FMTree was constructed by splitting the dataset into fibres and indexing
each fibre separately using an instance of M-tree that was created using
the BulkLoading algorithm of Ciaccia and Patella [ 38 ] . To perform a
range search, the FMTree range search algorithm queries all M-trees
associated with fibres as described in the Example 5.6.1 and collects
the hits to produce the answer to the query. The M-tree implementation
was obtained from its authorsâ site:
http://www-db.deis.unibo.it/Mtree/index.html .

The dataset in this experiment was the set of 1,753,832 unique fragments
fragments of length 10 obtained from a 5000 protein sequence random
sample taken from SwissProt (Release 41.21). An FMTree was generated for
BLOSUM62 @xmath -type quasi-metric at a cost of 34,142,940 distance
computations. Figure 6.17 shows the results based on 100 random queries
(unfortunately, mostly due to I/O costs, each search took over 1 minute
and it was necessary to use a smaller number of runs).

##### Suffix arrays and mvp-tree

Table 6.5 presents the results of comparisons between FSIndex ( @xmath
NN and range search algorithm), suffix array and mvp-tree over the
datasets of fragments of length 6 and 9 from nr018K . The similarity
measure used was the associated metric to the BLOSUM62 @xmath -type
quasi-metric because mvp-tree is a metric access method and the
performance of FSIndex does not much differ if a quasi-metric is
replaced by its associated metric. If the mvp-tree showed good
performance on metric workloads, the next step would be to split the
datasets into fibres to create an FMTree for quasi-metric searches.

Instances of suffix array were constructed using the routines published
at http://www.cs.dartmouth.edu/~doug/sarray/ . The search algorithm was
identical to the Algorithm 6.3.3 where the input is a single bin
containing all fragments in the dataset. In order to construct an
instance of mvp-tree, duplicate fragments in the datasets were collected
together and the sets of unique fragments provided to the mvp-tree
construction algorithm. The mvp-tree implementation, developed by the
original authors of mvp-tree [ 25 ] , was kindly provided by Marco
Patella and modified for use with protein fragments by the thesis
author. The maximum size of a leaf node was set to be 5.

### 6.5 Discussion

While the experiments presented in Section 6.4 covered very few datasets
and a small proportion of possible parameters for FSIndex creation, it
can still be observed that FSIndex performed well. Not only did it
perform much better than the other indexing schemes tested but it has
proven itself to be very usable in practice: it does not take too much
space (5 bytes per residue in the original sequence dataset plus a fixed
overhead of the @xmath array), considerably accelerates common
similarity queries and the same index can be used for multiple
similarity measures without significant loss of performance. The
remainder of the current section will examine some salient features of
the experimental results.

#### 6.5.1 Power laws and dimensionality

The most striking feature of the Figures 6.12 , 6.13 and 6.14 is the
apparent power-law dependence of the total running time, the number of
bins scanned and number of bins scanned on the number of actual
neighbours retrieved, manifesting as straight lines on the corresponding
graphs on log-log scale. For each index, the slopes of of the three
graphs (i.e. running time, bins scanned and fragments scanned) are very
close, implying that the same power law governs the dependence of all
three variables on the number of neighbours retrieved. The exponents are
0.81 for length 6, between 0.57 and 0.63 for length 9, and about 0.45
for length 12. While a rigorous theory, especially in the context of
quasi-metrics, is still missing, it is possible to offer an intuitive
explanation for this phenomenon.

Clearly, the graphs in question show the average growth of a ball in the
projection @xmath against the growth of a ball same radius in the
original space @xmath . Denote by @xmath the number of true neighbours
retrieved and by @xmath the corresponding number of fragments scanned.
The power relationship then can be written as @xmath . If we accept the
reasoning behind the distance exponent (not obvious from the data and
not justified except for very small radii â see Appendix A ), that is
that @xmath where @xmath is the âdimensionâ of the space, it follows
that @xmath . Using the same reasoning about the size of the ball in the
projection (but note that the distance in the projection need not
satisfy the triangle inequality), we conclude that the âdimensionâ of
the projection is @xmath , that is, the original dimension @xmath is
reduced by a factor @xmath . Assuming that the values of the distance
exponent do not depend on whether a quasi-metric or its associated
metric is used and taking the values of distance exponent estimated in
Subsection 6.1.6 , the âdimensionâ of the projected space is close to
6.5 for both length 6 and length 9.

#### 6.5.2 Effect of subindexing of bins

PATRICIA-like subindexing of bins was introduced in order to accelerate
scanning of bins containing many duplicate or highly similar fragments.
Figures 6.12 , 6.13 , 6.14 and 6.15 (Subfigure (e) in each case) show
that there are two main factors influencing the proportion of residues
scanned out of the total number of residues in the fragments belonging
to the bins needed to be scanned: the (average) size of bins and the
number of alphabet partitions at starting positions. Instances of
FSIndex having many partitions at first few positions perform well (
SPEQ06 , SPNA09 ), those that have few partitions with many letters per
partition, less so.

Clearly, if a bin has a single letter partition at its first position,
the distance at that position need be only retrieved once, at the start
of the scan, independently of the number of fragments the bin contains.
The effects for the second and subsequent positions are less prominent,
if only for the reason that using many partitions would result in many
bins being empty. The actual composition of the dataset is also
important, as Figure 6.15 (e) attests: although same partitions are used
and nr0288K is almost twice as large, SPEQ09 scans fewer characters. The
possible reason lies in the nature of SwissProt, which, as a human
curated database, is biased towards the well-researched sequences which
are more related among themselves while not necessarily being
representative of the set of all known proteins. On the other hand,
nr0288K is a random sample from the nr database which is exactly the
non-redundant set of all known proteins.

The actual proportion varies from 30% ( SPEQ06 , length 6) to over 85% (
nr018K , length 9). The percentage of characters scanned grows slowly
with increase of the number of neighbours retrieved â most probably this
is because the number of bins accessed also grows, requiring that at
least one full sequence is scanned.

To summarise, subindexing of bins does produce some savings, the exact
amount depending on the dataset and alphabet partitioning. However, and
this is further attested by poor performance of pure suffix array
compared to FSIndex (Table 6.5 ), the good performance of FSIndex is
mostly due to alphabet partitioning.

#### 6.5.3 Effect of similarity measures

Table 6.4 indicates very little difference in performance of the same
instance of FSIndex with respect to different similarity measures. This
should not be a surprise because the BLOSUM matrices are indeed very
similar, modelling the same phenomenon in slightly different ways but
generally retaining the same groupings of amino acids. The PSSM-based
searches also performed well, mainly because the PSSMs are usually
constructed out of sets of sequences that are strongly conserved at
least in one or two positions, and hence, in those positions, the
âdistancesâ to all other clusters are so large that many branches of the
implicit search tree can be pruned.

#### 6.5.4 Scalability

Figure 6.15 (b) indicates that FSIndex is scalable with respect to the
number of nearest neighbours retrieved â the number of residues needed
to be scanned grows sublinearly with dataset size (in fact, the exponent
is 0.25 to 0.3). The exponent for the growth of the number of scanned
points (graphs not shown in any figure) is about 0.4, indicating that
using PATRICIA-like structure improves scalability. The principal reason
for sublinear growth of the number of items needed to be scanned is
definitely that search radius decreases with dataset size (Figure 6.15
(a)). Unfortunately, the results in terms of search radius are not
available and it is not possible to examine the scalability with respect
to a fixed radius although theoretical considerations imply that the
growth would be linear. However, it may be that subindexing of bins
would bring an appreciable sublinear behaviour in this case as well.

#### 6.5.5 Comparison with other indexing schemes

Results of Subsection 6.4.6 indicate that FSIndex decisively outperforms
all other indexing schemes considered. M-tree performed the worst,
needing to scan 1.3 million fragments of length 10 in order to retrieve
the nearest neighbour. The performance of mvp-tree is not much better,
taking into account the dimensionality: it requires scanning about 1
million fragments of length 9 to retrieve the nearest neighbour. Suffix
array was generally performing better than mvp-tree, except for
retrieving the nearest neighbour of length 6.

In the case of suffix arrays, it is clear that large alphabet and
relatively small dataset (Figure 6.1 ) are responsible for relatively
poor performance. Also note that suffix trees (and hence suffix arrays)
generally are not good approximations of the geometry with respect to
@xmath -type distances â two fragments lacking a common prefix may have
a small distance. It should be noted that performance of suffix array
based scheme appears to improve with fragment length compared to
FSIndex.

The poor performance of M-tree and mvp-tree is somewhat surprising
because Mao, Xu, Singh and Miranker [ 131 ] have recently proposed using
exactly M-tree for fragment similarity searches. However, on closer
inspection, several differences appear. First, Mao, Xu, Singh and
Miranker use a different metric. More importantly, they use a
significantly improved M-tree creation algorithms. Finally, if their
results are compared with those from Figure 6.17 (this can be done at
least approximately because the same fragment length was used and the
size of the yeast proteome dataset used in [ 131 ] was very close to the
size of SwissProt sample used in our experiment), it appears that there
is no more than 10-fold improvement. While this is quite significant,
the total performance appears still worse than that of FSIndex. For more
detailed comparisons it would be necessary to obtain the code of the
improved M-tree from [ 131 ] and run a full suite of comparison
experiments.

## Chapter 7 Biological Applications

The present chapter introduces the prototype of the PFMFind method for
identifying potential short motifs within protein sequences. PFMFind
uses the FSindex access method to query datasets of protein fragments.

### 7.1 Introduction

Most of the widely used sequence-based techniques for protein motif
detection depend on regular expressions (deterministic patterns) [ 176 ,
26 ] , profiles (PSSMs) [ 78 , 6 ] or profile hidden Markov models [ 116
, 53 ] . As outlined in Chapter 3 , a PSSM is constructed by taking a
set of protein fragments, Â¹ Â¹ 1 Fragments are usually used rather than
full sequences because the motifs are associated with domains, which are
by their nature local. constructing a multiple alignment, estimating the
positional distributions of amino acids and producing positional
log-odds scores for each amino acid. A PSSM can then be used to search a
sequence dataset in order to identify new sequences fitting the profile
(that is, its underlying positional distribution). This procedure can be
performed iteratively, using sequences retrieved in one iteration to
construct a profile for the subsequent one. Profile hidden Markov models
generalise profiles by also modelling the distributions of gaps found in
the multiple alignments (see Chapter 5 of the book by Durbin et al. [ 52
] ).

The initial set of sequences consists of known examples of the motif in
question. It can be obtained from results of laboratory investigations,
from alignments of structures (for example using the SCOP database [ 7 ]
) or from results of sequence similarity searches. PSI-BLAST [ 6 ] uses
the latter approach: it searches a protein dataset using a score matrix
such as BLOSUM62 and uses the results to construct a multiple alignment
and produce a profile for the second iteration. Subsequent searches are
based on profiles constructed from the results retrieved in the
preceding iteration. Variations to this basic approach are possible,
mostly involving the choice of dataset and weights of sequences used for
profile construction [ 167 ] . The performance of any particular
technique is measured by its ability to retrieve relevant items from the
database (sensitivity) and to retrieve only such items (selectivity).

The focus of the present investigation is short protein fragments of
lengths 7â15 with the aim to develop new bioinformatic tools for
discovery of relationships between protein fragments that cannot be
necessarily found when considering longer fragments. Such relationships
need not imply a common ancestor but could have arisen from convergence.
The motifs discovered should correspond to a conserved function and
should give an insight into a possible origin of such a function.

Watt and Doyle [ 204 ] recently observed that BLAST is not suitable for
identifying shorter sequences with particular constraints and proposed a
pattern search tool to find DNA or protein fragments matching exactly a
given sequence or a pattern Â² Â² 2 A âpatternâ in the sense of Watt and
Doyle is a group of âtarget sequencesâ, which are essentially regular
expressions. I propose here an alternative technique, named PFMFind (PFM
stands for Protein Fragment Motif) that involves the use of full
similarity search with almost arbitrary scoring schemes and iterated
searches closely resembling PSI-BLAST. It differs from PSI-BLAST in that
it uses a global ungapped similarity measure over the fragments of fixed
length (referred to as an @xmath -type sum in the Chapter 3 ) allowing
use of FSindex as a subroutine. The similarity score being ungapped
could affect sensitivity but one should note that gapped alignments of
short fragments, at least of lengths not greater than 10, are often
statistically insignificant if the usual gap penalties are used (for
example, BLAST uses 11 as gap opening penalty, which is larger than the
cost of any single substitution â in fact two to three conservative
substitutions can be usually had for that cost, depending on the exact
score matrix). It is also possible to examine several fragment lengths
thus compensating for the similarity being global rather than local. Of
particular biological interest are cases where certain relationships can
be found at a particular fragment length and not the others indicating a
strongly conserved short motif that cannot be extended to a longer one.

The present chapter contains the description of the current PFMFind
algorithm together with six case studies based on SwissProt [ 23 ] query
sequences. The query sequences (SwissProt accessions in brackets) are:
prion protein 1 precursor (PrP) (P10279), @xmath -casein precursor
(P02666), @xmath -casein precursor (P02668), @xmath -lactoglobulin
precursor (P02754), cytochrome P450 11A1 mitochondrial precursor
(cholesterol side-chain cleavage enzyme) (P00189), and sensor-type
histidine kinase prrB (Q10560). The first five sequences are bovine (
Bos taurus ) while the histidine kinase is from Mycobacterium
tuberculosis .

The PrP protein is found in high quantity in the brain of humans and
animals infected with transmissible spongiform encephalopathies (TSEs).
These are degenerative neurological diseases such as kuru,
Creutzfeldt-Jakob disease (CJD), Gerstmann-Straussler syndrome (GSS),
scrapie, bovine spongiform encephalopathy (BSE) and transmissible mink
encephalopathy (TME) [ 219 , 220 , 159 , 207 ] that are caused by an
infectious agent designated prion. While many aspects of the role of PrP
in susceptibility to prions are known, its physiological role and the
pathological mechanisms of neurodegeneration in prion diseases are still
elusive [ 56 ] .

Caseins are major mammalian milk proteins involved in determination of
the surface properties of the casein micelle which contain calcium and
have major role in mammalian neonate nutrition [ 137 ] . Bovine milk
contains four different types of casein: @xmath -S1-, @xmath -S2-,
@xmath - and @xmath -. Caseins are expressed in mammary glands, secreted
with milk and following digestion may give rise to bioactive peptides [
137 ] .

@xmath -Lactoglobulin is another major component of milk. It is the
primary component of whey, binds retinol and unlike the caseins, has a
well-defined conformation [ 120 ] containing an eight-stranded
continuous @xmath -barrel and one major @xmath -helix.

Cytochromes P450s are a superfamily of heme-containing enzymes involved
in metabolism of drugs, foreign chemicals, arachidonic acid,
eicosanoids, and cholesterol, synthesis of bile-acid, steroids and
vitamin D3, retinoic acid hydroxylation and many still unidentified
cellular processes [ 145 ] . The cytochrome P450 A11 is a mitochondrial,
enzyme coded by the CYP11A1 gene and catalyses a cholesterol side
cleavage chain reaction [ 98 ] .

Histidine kinases phosphorylate their substrates on histidine residues
and have been well-characterised in bacteria, yeast and plants [ 215 ] ,
with a variety of functions including chemotaxis and quorum sensing in
bacteria and hormone-dependent developmental processes in eukaryotes.
They are also present in mammals [ 19 ] . Typically, histidine protein
kinases are transmembrane receptors with an amino-terminal extracellular
sensing domain and a carboxy-terminal cytosolic signaling domain and do
not show significant similarity to serine/threonine or tyrosine protein
kinases although they might be distantly related [ 115 ] .

The query sequences were chosen mainly according to the interests of the
author and his supervisors. For example, caseins have no known function
apart from nutrition while being strongly conserved in mammals, leading
to questions about their origins. Cytochromes P450 form a large and
well-researched superfamily with many examples in SwissProt and TrEMBL,
thus being particularly suitable for the PFMFind approach. Histidine
kinases are a subset of the class of protein kinases while being very
distantly related to the remainder of the class. PrPs are involved a
well-publicised set of neurological diseases and have a relatively
unusual structure of aromatic-glycine tandem repeats [ 68 ] .

### 7.2 Methods

#### 7.2.1 General overview

PFMFind takes a full sequence of interest and divides it into all
overlapping fragments of a given fixed length. For each fragment, it
uses FSindex-based range search to find the set of statistically
significant neighbours from a protein fragment dataset with respect to a
general similarity scoring matrix such as BLOSUM62. All fragments that
have fewer significant neighbours than a given threshold are excluded
from further iterations. For each fragment where the number of
significant results is sufficiently large, it constructs a PSSM from the
results and proceeds with the next iteration. The procedure is repeated
several times, each time using the results of one iteration, if their
number is over the threshold, to construct the profile for the next
search.

As in PSI-BLAST, the measure of statistical significance is E-value, the
expected number of fragments similar to a given query fragment under the
assumption that amino acids in a protein fragment are independently and
identically distributed. Subsection 7.2.3 below describes the derivation
and computation of the distribution of similarity scores with respect to
a given query fragment and similarity measure. The E-value threshold
decreases with iterations. This is because preliminary investigations
have shown that too few results of the initial, general score
matrix-based search, are significant under the model from Subsection
7.2.3 at a level usually set in bioinformatics applications of a similar
kind (for example, in PSI-BLAST, the inclusion threshold E-value is
0.005) while the hits having E-value up to 1.0 clearly belonged to the
same protein (in a different species) as the query protein. In the
iterations using profiles, more stringent significance levels have led
to expected results.

#### 7.2.2 PSSM construction

Since the fragment length is fixed, a collection of fragments directly
corresponds to an ungapped multiple alignment. Therefore, the first
nontrivial step is assigning a weight to each sequence in order to
compensate the possible bias of the set of hits caused by over- and
under- representation of a particular sequence. While each sequence is
assigned a new weight, the total weight of the fragment set remains the
original number of hits. The current version of PFMFind uses the
weighting scheme proposed by Henikoff and Henikoff [ 90 ] , which gives
smaller weight to well-represented sequences and is computationally
simple. The second step involves obtaining the âobservedâ (given the
weights) frequencies of amino acids at each position and combining them
with mixtures of Dirichlet priors in a way described by SjÃ¶lander and
others [ 174 ] (see also Chapter 5 of [ 52 ] ). The contribution of
Dirichlet priors decreases with sample size, preventing overfitting the
profile to a small sample while leaving the distribution derived from a
large set essentially unchanged. Finally, the procedure calculates
log-odds similarity scores to be used for searches. The scores are
multiplied by two (that is, scaled to half-bit units) and converted to
integers, enabling direct comparison with the BLOSUM62 scores which are
also in half-bit units.

#### 7.2.3 Statistical significance of search results

To evaluate the statistical significance of a particular similarity
score and therefore an alignment associated with it, we estimate how
probable that score is given a null, or background hypothesis. In this
case, we assume as a null hypothesis that fragments are generated by the
independent, identically distributed process where the probability of
each amino acid is given by its relative frequency in the dataset
(Subsection 6.1.3 discusses this and an alternative model of protein
sequences). Let @xmath be the fragment length. For each @xmath , let
@xmath be the score function at position @xmath . If the similarity
measure is given by a score matrix @xmath , we have @xmath where @xmath
is the query fragment and @xmath ,while in the case of a PSSM @xmath is
the score function at its @xmath -th position.

By our assumptions, it is clear that @xmath is a collection of
independent random variables and that the similarity score @xmath of a
fragment @xmath is given by the sum of the values @xmath for each @xmath
. Hence, the density of @xmath , denoted by @xmath is given by the
convolution of the densities @xmath of the random variables @xmath ,
that is

  -- -------- --
     @xmath   
  -- -------- --

where

  -- -------- --
     @xmath   
  -- -------- --

By the well-known Convolution Theorem, the Fourier transform of the
convolution of a collection of functions is a product of their Fourier
transforms. Since the functions in questions are discrete, the efficient
way of computing @xmath is to compute the discrete Fourier transforms of
@xmath for each @xmath , multiply them together and take the inverse
discrete Fourier transform of the product, all using the FFT (Fast
Fourier Transform) algorithm (the book by Smith [ 175 ] provides a good
reference about signals, convolutions and Fourier Transforms) and is
freely available on the web).

Once the density of similarity scores is obtained, it is straightforward
to compute the p-value of each score @xmath , that is the probability
that a random score @xmath is greater than @xmath . The number of
fragments in the dataset expected by chance to be equal to or exceed
@xmath , also known as E-value, is obtained by multiplying the p-value
by the size of the dataset. The relationships represented by the search
hits where the E-value of the similarity score is very low (usually
@xmath ) are considered unlikely to have arisen by chance and therefore
statistically significant. The significance cutoff can be computed prior
to search so that search by E-value reduces to range search.

#### 7.2.4 Implementation

PFMFind is implemented in the Python programming language [ 195 ] ,
accessing the FSindex library, which is written in the C programming
language [ 109 ] , through the SWIG [ 11 ] interface. The PFMFind code
uses the routines from the Python standard library [ 128 ] as well as
from the Biopython [ 186 ] , Numeric [ 9 ] and Transcendental [ 46 ]
packages.

Architecturally, PFMFind system consists of a master server, several
slave servers and at least one client, all communicating through TCP/IP
sockets. The master server handles computation of searches and
statistical significance by distributing the load to slave servers while
the client is responsible for storage of results and computation of
profiles. Â³ Â³ 3 It is planned to move the profile construction to the
server side as well leaving only the storage and interface to the
client. Python programs making use of PFMFind create an instance of a
client, connect to a master server and provide the parameters of desired
searches. A graphical user interface, called FragToolbox, was written
using the Tkinter module [ 77 ] from the Python standard library in
order to facilitate the analysis of the results by displaying them in a
human-usable format.

The above configuration is necessary in order to use large datasets
which cannot fit into memory of a single machine. It also opens the
possibility of parallelisation of most of computation, leaving only
storage and display to clients.

#### 7.2.5 Experimental parameters

##### Dataset

Preliminary investigations using SwissProt as the database have shown
that in most cases too few sequences are available in order to be able
to construct good profiles even if the initial E-value is relaxed. While
SwissProt is manually annotated and therefore provides most confidence
in functional annotation, it is also biased in favour of well-researched
sequences. I therefore decided to use the full Uniprot [ 10 ] dataset
consisting of SwissProt together with TrEMBL (translated EMBL DNA
sequence dataset). Since the size of Uniprot is large (Release 3.5 that
was used together with alternative splicing forms of some proteins had
556,628,177 amino acid residues in 1,737,387 sequences), it was
necessary to divide it into 12 SwissProt-sized parts and to run a
PFMFind slave server for each part on a different machine.

##### Search and profile construction parameters

The cutoff E-values were 1.0 for the first and second, 0.1 for the third
and fourth and 0.01 for all subsequent iterations. As preliminary
investigations indicated that at E-value thresholds of 1.0 or smaller
most BLOSUM matrices produce similar results, my choice was to use
BLOSUM62 in the first iteration. Profile construction algorithm used the
Dirichlet mixture recode3.20comp downloaded from the web site
http://www.cse.ucsc.edu/research/compbio/dirichlets/ of some of the
authors of [ 174 ] . They recommend the
recode3.20comp mixture as the best to be used with close homologs. After
several trials I set the number of hits necessary to proceed with the
next iteration to 30 as a compromise between the need to have as large
number of hits as possible in order to have a good profile and the
average number of neighbours given the required statistical
significance.

### 7.3 Results

The full PFMFind algorithm was run for the six test sequences. Fragment
lengths 8 to 15 were considered for all test proteins except PrP where
only fragments of length 8 were considered because of technical
limitations: too many hits were encountered and the available memory was
insufficient to store all but the length 8 results (there were usually
more than 100 hits for each overlapping fragment, sometimes over 1000
hits). The hits were almost exclusively exact matches to fragments of
the query sequence or other prion proteins, in the same or different
species. PrP is glycine rich and contains several repeats which
manifested as several hits to the same protein in a single fragment
search.

The running time for searches for all the examples was in the order of
one to two hours, using 12 Intel @xmath Pentium @xmath IV 2.8 GHz
machines running in parallel, with indices optimised for lengths 10 and
12. Running FSindex did not take more than half of that time, the
remainder being taken by calculation of statistical significance,
construction of profiles, communication between machines and I/O
operations.

Table 7.1 provides the summary of the results for all examples except
PrP. The âRegionâ column denotes the region of the original query
sequence where significant hits to database proteins were found and
usually refers to the maximal extent of such region for the longest
fragment length where hits were found. The âFeatureâ column contains the
annotations of the region in question taken from SwissProt and InterPro
[ 141 ] , a database of protein families, domains and functional sites
consisting of several member databases using a variety of motif-finding
techniques. The last column includes the description of the major
categories of proteins found in the hits. Some of the @xmath -casein
hits are not included because they were difficult to characterise (no
SwissProt entry present).

### 7.4 Discussion

Two kinds of hits can be observed in general: hits to the query protein
itself and its very close homologs and hits to low-complexity regions of
arbitrary proteins. There were also few hits to fragments of apparently
unrelated proteins which were not low-complexity.

#### 7.4.1 Hits to close homologs

Most commonly found hits, apart from the low-complexity fragments, were
to the instances of the same protein in a variety of species and to its
close homologs. The hits were concentrated in the regions where
sufficiently many strongly conserved examples existed. In histidine
kinases, the hits are found in the histidine kinase domain, more
specifically, according to InterPro, in the His Kinase A
(phosphoacceptor) subdomain (230â257) and the ATPase domain (373â398,
400â425). PFMFind identified DNA gyrase (a bacterial DNA repair enzyme)
as being associated with the (373â398) region, which is also confirmed
by InterPro. Hence, in the histidine kinase example, PFMFind retrieved
strongly conserved, functionally important regions, agreeing with the
established methods.

In the case of @xmath -casein, PFMFind identified a single region
corresponding to the signal peptide whose role is to target the protein
to a particular cellular compartment or, as in this case, to be
secreted. The hits were to signal sequences of other caseins and other
secreted proteins (amelogenin, having a role in biomineralisation of
teeth and vitellogenin, a major yolk protein). No hits were found in the
mature protein segment (mature protein is the precursor from which the
signal peptide and potentially other parts have been cleaved), mainly
because the initial hits were only to the other @xmath -casein instances
of which there were not sufficiently many to proceed to the next
iteration. Apart from these, there were also hits to low complexity and
transmembrane regions of clearly unrelated proteins.

In the case of @xmath -casein, the majority of hits were to other @xmath
-caseins, the remainder being to low complexity regions. The only
difference from the @xmath -casein case is that Uniprot apparently
contains more @xmath -casein sequences (that is, more than the minimum
number necessary to proceed to the next iteration) so that PFMFind
obtained the hits over most of the length of the protein. In the @xmath
-lactoglobulin, PFMFind found hits to @xmath -lactoglobulin itself and
its close relatives (glycodelin, a pregnancy associated protein and
other members of lipocalin family) as well as to some apparently
unrelated proteins such as bacterial RecA (DNA recombination enzyme) and
SbnH (polyamine biosynthesis). However, under closer scrutiny, it
appears that at least the SbnH fragment has been identified to belong to
the lipocalin domain (ProSite [ 55 ] reference PS00213) together with
@xmath -lactoglobulin and glycodelin. All regions in @xmath
-lactoglobulin corresponded to identified elements of secondary
structure.

Cytochromes P450 are well represented both in SwissProt and in TrEMBL,
providing sufficient amount of examples to produce good profiles. Unlike
with @xmath -casein, it appears that only truly conserved regions were
identified. Most hits were to the other cytochromes P450 (but not always
to all members of superfamily â sometimes only very closely related
cytochromes are retrieved) with the exception of the regions associated
with turns.

#### 7.4.2 Low complexity regions and repeats

Many of the significant hits retrieved by PFMFind were to low-complexity
fragments, for example consisting all of proline or glutamine or
histidine. Such fragments are much more common than would be expected
from their amino acid compositions, at least in eukaryotes [ 71 ] and
frequently present problems for similarity searches. It is important to
note that whenever low complexity regions are hit, the profile
âdivergesâ from the seed: the original sequence becomes no longer
significant (or at least not most significant) and the profile describes
a totally different target. This is mainly because of compositional bias
of the results where there are too many âundesirableâ hits which âtake
overâ the profile for a subsequent iteration. Even though the algorithm
uses Dirichlet mixtures to smooth the positional distributions, it can
be swamped by the large amounts of apparently genuine hits. The same
issue is evident where transmembrane domains, which are strongly
hydrophobic and not associated with any specific function, are hit (for
example, region 3â14 in @xmath -casein).

The problem with low-complexity segments has been recognised and several
tools that identify and filter out such regions exist [ 216 , 214 ] . In
BLAST, the default option is for all low-complexity segments to be
masked prior to search. However, some low-complexity regions may be
biologically significant â for example, some bioactive peptides could be
classified as low-complexity. A different way to avoid the effect of
compositional bias is to use Z-score statistic based on the distribution
of scores of the fragments having the same composition as a given hit
but different order of amino acids [ 205 ] . While this approach is
commonly taken where global alignments are used, it fails to give
sufficiently many sufficiently significant fragments of short lengths
(datasets are too large and @xmath is too small for small @xmath ).

Hence, it appears that selective filtering of low-complexity hits is
necessary. Highly compositionally biased fragments of query sequences
should be filtered prior to search. Other fragments should be filtered
at profile construction time, if computationally feasible. The aim
should be to retain as many of the results while ensuring that the
profile does not diverge. One of the reasons for appearance of
low-complexity fragments within the results is the relaxed significance
requirements for the first few iterations but one should take care in
that respect because genuine hits also have low significance at first.

The PrP searches have revealed a further weakness of the current PFMFind
algorithm and implementation. Most of the PrP hits were to the sequence
itself and its very close, almost identical homologs. While the numbers
of such sequences are not too large, the structure of the PrP itself,
containing many aromatic-glycine tandem repeats was responsible for very
large result sets: every PrP homolog appeared several times (in a
different region) as a hit for a single fragment. This made it
impossible to proceed because the current implementation of PFMFind
stores all results in main memory. The problem should be rectified by
better filtering/weighting of hits and storage of results on disk, to be
retrieved as needed.

#### 7.4.3 Issues with algorithm and implementation

A major issue that dominated all examples of PFMFind searches presented
here was the non-homogeneity of the database. Some proteins are
extremely well represented, containing instances from a variety of
species, some are very rare while others have multiple instances from
few species. Subsection 7.4.2 discussed the problems arising from
low-complexity fragments. However, @xmath -casein case has shown that
too many instances of the same protein can also present difficulties at
least due to overfitting. Weighting of hits prior to profile
construction is clearly a solution but it is necessary to use weighting
that could lower the total weight instead of just redistributing it. An
even better approach would be to use other information (structure,
function, domains) contained in the databases as well as sequence
information. However, the quality of annotations varies considerably and
this would present an implementation challenge because it would require
full access to annotated databases by the PFMFind algorithm.

PFMFind would also benefit from access to biological information because
of general low significance of short fragment hits under the current
statistical model. A Bayesian model, including the prior information
available as annotation, could be more appropriate, provided that
sufficient data is available. One must note however, that any increase
in complexity of profile construction algorithm would affect the running
time. Already, except in rare cases, similarity search does not take the
most of the running time of PFMFind. This can of course be attributed to
the good performance of FSindex.

### 7.5 Conclusion

The six examples have shown that PFMFind is able to identify the regions
in the query sequence that are strongly conserved and functionally
important in the closely related proteins as well as in some apparently
unrelated proteins. The results also indicated that some sort of
filtering of low-complexity hits and repeats is desirable. Several
improvements to the algorithms and implementation are necessary before
large-scale experiments can be conducted.

## Chapter 8 Conclusions

The motivation for this thesis comes from the biological objective of
developing the methods for discovering the origin and function of short
peptide fragments with conserved sequence. While most of the current
approaches to protein sequence analysis consider either full sequences
or longer domains, short fragments have significant biological
importance on their own. For example, there are several peptide
fragments in various milk proteins that are cleaved during digestion and
have possible physiological activity. Other peptides, from completely
unrelated organisms, may have the same activity. Hence, from a
biological point of view, it would be very useful to have the tools to
discover the relationships between short fragments that do not
necessarily extend to whole proteins.

As in the analysis of the longer sequences, the primary technique used
to relate the short fragments is similarity search: we find similar
fragments to a given query fragment and associate the function of the
search results of the known function to it. The existing methods such as
BLAST proved inadequate, primarily for reasons concerning computational
efficiency â they were too slow for the large number of searches that
were considered necessary. Hence the need to construct an efficient
index for similarity search in short peptide fragments that would speed
up the retrieval of queries.

Indexing a dataset in an efficient manner is only possible through a
good understanding of the geometric properties of the similarity measure
on it. While most existing indexing techniques assume that the
similarity measure is given by a metric, that is, a distance function,
this is not the case for biological sequences where the similarity
measures are generally given by similarity scores. The principal reasons
for using similarity scores in biology are that they have fewer
constraints and have information-theoretic and statistical
interpretations. For our work, as a similarity measure, we have chosen
the one given by the ungapped global alignment between fragments of
fixed length because we believe that gaps do not have major importance
in the context of short fragments.

One of the important results of the thesis is the discovery that many of
the widely used BLOSUM similarity score matrices, restricted to the
standard amino acid alphabet, can be converted into weightable
quasi-metrics (metrics without the symmetry axiom), which generate the
same range queries as the original similarity scores.

This in turn lead to the following questions:

1.  What is known about the quasi-metrics and what are the principal
    examples?

2.  Can the results from asymptotic geometric analysis be extended to
    quasi-metric spaces with measure and applied to the theory of
    indexing for similarity search?

3.  Can some insights from the theory of quasi-metrics be used to build
    an efficient indexing scheme for short peptide fragments that can be
    applied towards answering the original biological problem?

4.  Does the relationship between similarities and quasi-metrics on the
    alphabet extend to local (Smith-Waterman) alignments between full
    sequences?

Chapter 2 answers the first question above. Quasi-metrics generalise
both metrics and partial orders and are well known in topology and
theoretical computer science. The main motif that is encountered with
quasi-metrics is duality: the interplay between the quasi-metric, its
conjugate and their join, the associated metric. The novel contribution
of the Chapter 2 is the construction of the universal bicomplete
separable quasi-metric space @xmath . This space is an analog of the
well-known Urysohn metric space and is universal, ultrahomogeneous and
unique up to isometry. The main motivation for constructing such space
was to provide a previously unknown example of a quasi-metric space and
to lay foundations for future work. In particular, the universality
property means that all bicomplete separable quasi-metric spaces can be
studied as subspaces of @xmath .

The second question is considered in Chapters 4 and 5 . The main object
introduced there is @xmath -space: a quasi-metric space with probability
measure. The notion of concentration functions from asymptotic geometric
analysis can be defined for @xmath -spaces in a way that emphasises
duality â instead of one concentration function, we have two: left and
right. The main theoretical result of Chapter 4 is that a
âhigh-dimensionalâ quasi-metric space is very close to being a metric
space â in other words, that asymmetry is being lost with concentration.
In the context of the theory of similarity search, the thesis extends
the theoretical framework for indexing metric spaces to quasi-metric
spaces by introducing the concept of a quasi-metric tree. Furthermore,
the developments from Chapter 4 are used to give bounds for performance
of quasi-metric indexing schemes.

Chapters 6 and 7 give answer to the third question. FSIndex was
developed as an indexing scheme for fragments of fixed length based on
two principles: reduction of the amino acid alphabet based on
biochemical properties of amino acids and combinatorial generation of
neighbours in the space of reduced fragments. It uses distances to
reduced sequences as certification functions and thus combines the
insights from biochemistry and geometry, having significantly better
performance than existing indexing schemes (by 1-2 orders of magnitude).
In addition FSIndex can be also used for profile-based searches and as
such provides the main component of PFMFind â a system for retrieving
short conserved motifs from protein sequences. The preliminary
experimental results from Chapter 7 show that PFMFind is very good at
identifying conserved regions but has some problems with fragments of
low-complexity. FSIndex also offers useful insight into the nature of
indexing in general.

The fourth question leads to what we consider as another important
contribution of this thesis to bioinformatics and computational biology:
the discovery of the relationships between local similarities and
quasi-metrics in Chapter 3 , under the assumptions satisfied by the most
widely used similarity score functions. The most significant aspect of
this discovery is the triangle inequality property which could lead to
novel applications to clustering and of course to indexing for
similarity search.

### 8.1 Directions for Future Work

While the phenomenon of concentration of measure is well-researched for
many classical objects of mathematics, the contribution of the Chapter 4
of this thesis and the corresponding paper in Topology Proc. [ 181 ] is
only the beginning. Many non-trivial questions are opened by introducing
asymmetry, that is, by replacing a metric by a quasi-metric. For
example, it would be interesting to generalise Gromovâs [ 79 ] metric
between @xmath -spaces to @xmath -spaces and hence to obtain a framework
for discussing convergence to an arbitrary @xmath -space, where
concentration of measure is a particular case of convergence to a single
point. Similarly, one would want to find out if Vershikâs [ 197 ]
relationships between @xmath -spaces, measures on sets of infinite
matrices and Urysohn spaces, can be extended to @xmath -spaces. Finally,
the task of constructing a universal quasi-metric space that is not
bicomplete, as well as a universal quasi-metric space complete under
different notions of completeness remains open.

Turning to indexing schemes for similarity search, while other factors
play no doubt a significant role, the performance is principally
determined by geometry. The main task ahead is to further adapt the
concepts of abstract asymptotic geometric analysis to datasets, which
are discrete but growing objects and to develop computational tools and
techniques for predicting and improving performance. It is clear that
due to the Curse of Dimensionality, indexing âhigh-dimensionalâ datasets
gains nothing. However, it is a common perception that, in reality,
useful datasets are never intrinsically high-dimensional. It remains a
highly challenging geometric problem to formalise this perception, first
in geometric terms, and subsequently algorithmic.

Unfortunately, many indexing schemes perform badly for datasets that
cannot be said to be âhigh-dimensionalâ â recall the performance of
M-tree and mvp-tree for datasets of protein fragments â and therefore,
there is a lot of scope for improvements to existing algorithms and data
structures. Another general observation, made apparent from experiences
with FSIndex, is that additional knowledge of domain structure could be
of significant help in developing an indexing scheme.

FSIndex has shown its usability for searches of protein fragments.
Another possible application that ought to be examined is as a
subroutine of a full sequence search algorithm. The experiments using
the preliminary versions of PFMFind have shown its significant potential
for finding short conserved patterns in protein sequences. It remains
however, to make further improvements in order to eliminate problems
associated with low-complexity sequences.

The relationship between similarities and quasi-metrics also opens the
possibility of characterising the global geometry of DNA or protein
datasets directly, without resorting to projections or approximations.
As quasi-metrics capture many important properties of biological
sequences, it is an opinion of the thesis author that asymmetry should
be cherished rather than avoided by symmetrisations.

A general conclusion from this work is that methods based on asymmetric
distances and measures have a future in analysis of data, especially in
bioinformatics and computational biology, and those applications, in
turn, can provide directions for further mathematical research.

## Appendix A Distance Exponent

In this Appendix we outline some methods for estimating the
dimensionality of datasets based on the distance exponent of Traina,
Traina and Faloutsos [ 188 ] . A more rigorous definition of distance
exponent is introduced and the methods for estimating it are tested on
some artificial datasets of known dimensions.

### a.1 Basic Concepts

We give a brief introduction to the Hausdorff and Minkowski fractal
dimensions. All the definitions and results are from the book by Mattila
[ 134 ] and the reader should refer to it for more detailed treatment.

###### Definition A.1.1.

Let @xmath be a separable metric space. The @xmath -dimensional
Hausdorff measure , denoted @xmath is defined for any set @xmath by

  -- -------- --
     @xmath   
  -- -------- --

where

  -- -------- --
     @xmath   
  -- -------- --

@xmath

It can be shown that @xmath is a Borel regular measure. The measure
@xmath corresponds to the counting measure while @xmath has an
interpretation as a generalised length measure. In @xmath , @xmath .

###### Definition A.1.2.

The Hausdorff dimension of a set @xmath is

  -- -------- --
     @xmath   
  -- -------- --

@xmath

The Hausdorff dimension has some desirable properties for the dimension
namely:

-   @xmath for all @xmath ,

-   @xmath for @xmath , @xmath , and

-   @xmath .

Hence @xmath for all @xmath .

###### Definition A.1.3.

Let @xmath be a non-empty bounded subset of @xmath . For @xmath , let
@xmath be the smallest number of @xmath -balls needed to cover @xmath :

  -- -------- --
     @xmath   
  -- -------- --

The upper and lower Minkowski dimensions of @xmath are defined by

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

@xmath

It follows from the definitions that @xmath and these inequalities can
be strict. Equivalently,

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

The following theorem provides a motivation for considering the fractal
dimension to be the exponent of the growth of the measure of a ball, at
least in @xmath .

###### Theorem A.1.4 ([168]).

Let @xmath be a non-empty bounded subset of @xmath . Suppose there
exists a Borel measure @xmath on @xmath and positive numbers @xmath ,
@xmath , @xmath and @xmath such that @xmath and

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath and @xmath . Then @xmath , where @xmath is the Hausdorff
dimension and @xmath and @xmath are the lower and upper Minkowski
dimensions of @xmath . â

Traina, Traina and Faloutsos [ 188 ] observed that the distributions of
distances between points of many existing datasets follow a power law
for small distances and proposed a concept of distance exponent as an
estimate of the fractal dimension of datasets. By their definition, the
distance exponent is the slope of the linear part of the graph of the
distance distribution function on the log-log scale. However, a more
rigorous definition is necessary, because the power law is only an
approximation and it is difficult to ascertain the exact bounds of the
linear part. We define the distance exponent in the framework of
pm-spaces.

###### Definition A.1.5.

Let @xmath be a pm-space. Define @xmath , the cumulative distance
distribution function of @xmath by

  -- -------- --
     @xmath   
  -- -------- --

@xmath

###### Remark A.1.6.

Clearly, @xmath is the average measure of a closed ball of radius @xmath
. By Fubiniâs Theorem,

  -- -------- --
     @xmath   
  -- -------- --

###### Definition A.1.7.

Let @xmath be a pm-space and @xmath its cumulative distance distribution
function. The distance exponent , denoted @xmath , is defined by

  -- -------- --
     @xmath   
  -- -------- --

@xmath

Note that the distance exponent need not be defined and that it makes
sense only for the case where @xmath is an infinite set and @xmath a
continuous measure. Many existing workloads can be modelled in this way,
with a domain a large infinite space and the dataset a finite sample
according to some continuous measure (see the Section 5.7.2 ).

The exact relation between the distance exponent and fractal dimensions
in general remains an open question â indeed, our definition the
Minkowski dimension applies only for @xmath . If a set @xmath satisfies
the conditions of the Theorem A.1.4 , then clearly @xmath for @xmath and
hence the distance exponent corresponds to the Hausdorff and Minkowski
dimensions.

### a.2 Theoretical Examples

Although it is usually difficult to derive a general distribution
function of distances of points on a arbitrary manifold, it is sometimes
possible to use the symmetry of specific objects and metrics to obtain
the exact forms for their cumulative distance distribution functions.

Let @xmath be a pm-space where @xmath and @xmath is the density function
of the probability measure @xmath . Suppose the metric @xmath on @xmath
is induced by the norm @xmath on @xmath . Denote by @xmath the unit ball
with respect to @xmath (i.e. @xmath ). Let @xmath and @xmath be random
variables taking values in @xmath according to @xmath . Then the
cumulative distance distribution function of @xmath is given by

  -- -------- -- -------
     @xmath      (A.1)
  -- -------- -- -------

where @xmath is the density function of differences @xmath . The
integral above can be quite hard to evaluate in closed form but there
are cases where this poses no problem. Two of such cases are provided
for illustration.

#### a.2.1 The cube @xmath

Consider the pm-space @xmath where @xmath is the unit cube @xmath ,
@xmath is the @xmath metric (i.e. @xmath ) and @xmath is a uniform
measure on @xmath . The density function @xmath is given by

  -- -------- -- -------
     @xmath      (A.2)
  -- -------- -- -------

Observe that @xmath is a product of uniform distributions on @xmath ,
that is:

  -- -- -- -------
           (A.3)
  -- -- -- -------

Thus

  -- -------- --
     @xmath   
  -- -------- --

Now if @xmath then @xmath
Remember that the unit ball with respect to the @xmath norm is @xmath
and therefore

  -- -------- --
     @xmath   
  -- -------- --

It therefore follows that @xmath as expected.

#### a.2.2 Multivariate normal distribution

Now consider the pm-space @xmath where @xmath , @xmath is the @xmath
metric (i.e. @xmath ) and @xmath is a multivariate Gaussian measure
(normal distribution) on @xmath with mean @xmath and variance 1 in all
coordinate directions. The density function @xmath is given by

  -- -------- -- -------
     @xmath      (A.4)
  -- -------- -- -------

Again, @xmath defines a product distribution as in the Equation ( A.3 ),
where @xmath . Hence, we can use the fact that @xmath is an even
function and a well-known result that the sum of two normal random
variables is a normal random variable where the mean is the sum of means
and the variance is the sum of variances of these random variables, to
conclude that

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Let @xmath . Using the radial symmetry of @xmath and the spherical
coordinates,

  -- -------- --
     @xmath   
  -- -------- --

The above expression can be evaluated as power series. Let @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

The above recurrence relation can be solved for even and odd @xmath
separately. If @xmath is even,

  -- -------- --
     @xmath   
  -- -------- --

If @xmath is odd,

  -- -------- --
     @xmath   
  -- -------- --

Therefore,

  -- -------- -- -------
     @xmath      (A.5)
  -- -------- -- -------

and hence it is not difficult to verify that @xmath .

### a.3 Estimation From Datasets

Two algorithms were used to estimate the distance exponent from
artificially generating datasets corresponding to geometric objects of
known dimension. In each case an estimate @xmath of @xmath was obtained
by taking a random sample @xmath and calculating all distances between
the points in @xmath . Therefore,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the normalised counting measure on @xmath . All
computation was handled by the MATLAB package [ 187 ] . In all cases
(i.e. for all dimensions) the artificial datasets consisted of no more
than 20000 points while approximately 200000 distances were sampled to
obtain @xmath .

The main algorithms tested were based on calculation of the slope of the
@xmath vs @xmath graph (original definition of Traina, Traina and
Faloutsos [ 188 ] ) and the fitting of polynomial to @xmath , both for
small values of @xmath . A third method which was tried was based on
estimation of derivatives but was not successful for the objects of
dimensions greater than @xmath .

The following artificial datasets were used to test the estimation
algorithms:

-   Euclidean spaces @xmath with standard multivariate normal (Gaussian)
    distributions and @xmath metrics;

-   Cubes @xmath with uniform distributions and @xmath metrics;

-   Spheres @xmath with uniform distributions and @xmath and geodesic
    metrics;

-   Parabolic through in @xmath with @xmath metrics.

All objects were generated using the built-in MATLAB routines which
provide random vectors in @xmath according to the Gaussian or uniform
distribution. These routines were used directly to generate the
multivariate Gaussians and the cubes while additional transformations
needed to be applied for the remaining spheres and parabolic throughs.

Uniform distributions on the spheres were obtained by projecting
multivariate Gaussian vectors in @xmath onto the unit sphere @xmath . We
define a parabolic through @xmath to be a surface in @xmath which is a
Cartesian product of a parabola @xmath where @xmath , and a @xmath
dimensional cube (Figure A.1 ). In order to obtain the uniformly
distributed points on @xmath , it is sufficient to generate uniformly
distributed points on the parabola and the cube separately. Uniform
distribution on parabola was obtained by parameterising the parabola by
arc-length, sampling from the uniform distribution on @xmath and mapping
the sampled points to the parabola.

A typical example of the function @xmath and its sampling approximation
@xmath is shown in the Figure A.2 below.

#### a.3.1 Estimation from log-log plots

The definition of Traina, Traina and Faloutsos [ 188 ] involves
estimation of distance exponent from the slope of the âlinear partâ of
the log-log plot of the cumulative distance distribution function @xmath
. Our implementation produced a least-squares estimation of the slope of
@xmath vs @xmath on a given interval @xmath . The end-point of the
interval was the fifth percentile (i.e. the smallest value @xmath such
that @xmath ) while the starting point was chosen so as to avoid the
first few points corresponding to very small distances which were found
not to be good estimates of the true distance distribution function
@xmath (see the Figure A.2 ). The estimates of dimensions of some of the
above mentioned objects using this method are shown in the Figure A.3

It is clear that our algorithm systematically underestimated the
dimension of objects of âtrueâ (i.e. expected) dimension greater than
@xmath . The distance exponent estimates for multivariate Gaussians and
spheres did not differ to a significant extent while the dimension of
parabolic throughs was underestimated to a greater degree than in the
other two cases.

In order to find an explanation for our results we sampled the exact
values of @xmath for the multivariate Gaussian on @xmath (Equation ( A.5
)) and applied our algorithm to them. The results are shown in the
Figure A.4 .

It can be observed that the estimates of distance exponent obtained
using the true values of @xmath (which has no variance due to sampling)
are not significantly better than those obtained using the approximation
@xmath . We conclude that most of the observed error is due to bias:
@xmath (and therefore @xmath ) is not linear in the region used for
estimation of the distance exponent). A method based on weighted least
squares, giving more weight to smaller distances (or equivalently
reduction of the interval to include very few points, equally
distributed along the âlinear partâ) brought some improvement up to the
dimension @xmath at a price of instability due to variance (Figure A.5
).

#### a.3.2 Estimation by polynomial fitting

The second approach was based on the least squares approximation of
@xmath near zero by a polynomial @xmath . The estimation of distance
exponent @xmath was based on the assumption that there exists @xmath
such that for @xmath , @xmath , and hence that the polynomial @xmath
would have the best fit to @xmath among all other @xmath âs. The
polynomials were in computed as follows.

Let @xmath for @xmath where @xmath . Given a possible dimension @xmath ,
and the number of terms of the polynomial @xmath , we want to find
@xmath which such that the @xmath norm of the differences between @xmath
and the sampled function @xmath is minimal. Taking into account that
@xmath is a step function, we minimise

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where

  -- -------- --
     @xmath   
  -- -------- --

Differentiating with respect to each @xmath we get for each @xmath ,

  -- -------- -- -------
     @xmath      (A.6)
  -- -------- -- -------

Thus we have a system of linear equations @xmath where @xmath which can
be solved numerically. For our computations only the one term
polynomials were used and in that case the Equation A.6 is reduced to

  -- -------- -- -------
     @xmath      (A.7)
  -- -------- -- -------

Given the value of @xmath , the estimate of distance exponent was
obtained by computing the errors for different values of @xmath and
selecting the value of @xmath for which the @xmath produced the smallest
error. For our tests only the integral values of @xmath were tried since
it was known that the datasets had the integral dimensions. In general,
the optimal value of @xmath can be obtained by numerical optimisation.
For the computations, the @xmath data was divided into two equally sized
sets: the âtrainingâ set was used to compute the coefficient of the
polynomial and the âtestingâ set to compute the errors.

The problem of choosing @xmath (that is, the number of points) was
solved by considering a variety of endpoints and picking the maximal
value of estimated distance exponent among all of them. This approach
was based on the observation that the value of @xmath for which @xmath
fits @xmath the best has a maximum which is usually (for the low
dimensions) the true dimension. The estimated dimension drops for @xmath
close to zero because few points are used and a large variance component
is present and also because the first few points of @xmath usually
overestimate @xmath . On the other hand, if @xmath is large, the
behaviour of @xmath is no longer dominated by @xmath .

The above heuristic method gave surprisingly good results for our simple
objects (Figure A.6 ). The approximations using the above heuristic
method were much closer to the true dimension than those using the slope
of @xmath vs @xmath .

While it was hoped that the polynomials with more than one term could be
used, allowing us to use larger values of @xmath , the approximations
were not as accurate as those obtained by monomials and their
interpretation was more difficult.

### a.4 General Observations

It should be noted that estimation of the distance exponent appears to
be an ill-posed problem because it is essentially equivalent to
calculating derivatives of @xmath around zero (one can prove using
lâHÃ´pitalâs rule that if distance exponent is @xmath then the first
@xmath derivatives of @xmath at @xmath must be @xmath ). We met the
variance against the bias problem in both proposed methods. A large
interval in which @xmath is approximated by @xmath was necessary in
order to reduce the variance (since a small interval meant that fewer
values of @xmath were available) but it introduced the bias which
lowered the estimate of the dimension (since the behaviour of @xmath was
no longer dominated by @xmath . In addition, in higher dimensions, most
of distances at which the values of @xmath were available were
concentrated very close to the median. This was another manifestation of
the Curse of Dimensionality.

In our experiments, the polynomial fitting approach performed better in
the higher dimensions than the estimation from log-log plots. It should
be noted that all the datasets tested by Traina, Traina and Faloutsos [
188 ] had the dimension less than @xmath (in some cases only estimates
were available) so that the underestimation we observed was not as
pronounced as in higher dimensions. Our polynomial fitting algorithm can
be improved by using numerical optimisation to find the optimal values
of @xmath and @xmath .