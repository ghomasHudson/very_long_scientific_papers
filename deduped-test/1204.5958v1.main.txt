##### Acknowledgements. This thesis is based on a series of papers I
coauthored with a long list of friends, colleagues and mentors: Boris
Alexeev, Waheed U. Bajwa, Afonso S. Bandeira, Jameson Cahill, Robert
Calderbank, Matthew Fickus, Negar Kiyavash, Christopher J. Quinn, Janet
Tremain, and Percy Wong. Each member of this list taught me a thing or
two throughout the course of my thesis research, and I very much
appreciate it! My time at Princeton has been a lot of fun, thanks in
large part to the good friends I’ve made here. From eating sushi, to
playing board games, to solving fun math riddles, the experience has
been a blast, and I’ll always remember it. My wife has a gift for
filling my life with beauty and love, and last year, she gave me a
beautiful new life to love. Thank you, Tessia and Charlotte, for making
my life wonderful. Finally, I thank my parents for their unfailing love
and support, and I thank God for His role in all of these things. This
research was supported in part by the A.B. Krongard Fellowship. The
views expressed in this thesis are those of the author and do not
reflect the official policy or position of the United States Air Force,
Department of Defense, or the U.S. Government. \dedication To all those
who never dedicated a dissertation to themselves.
And to my daughter, Charlotte.

### 0.1 Overview

In several applications, data is traditionally collected in massive
quantities before employing a reasonable compression strategy. The
result is a storage bottleneck that can be prevented with a data
collection alternative known as compressed sensing . The philosophy
behind compressed sensing is that we might as well target the meaningful
data features up front instead of spending our storage budget on
less-telling measurements. As an example, natural images tend to have a
highly compressible wavelet decomposition because many of the wavelet
cofficients are typically quite small. In this case, one might consider
targeting large wavelet coefficients as desired image features; in fact,
removing the contribution of the smallest wavelet coefficients will have
little qualitative effect on the image [ 57 ] , and so using sparsity in
this way is intuitively reasonable.

Let @xmath be an unknown @xmath -dimensional vector with the property
that at most @xmath of its entries are nonzero, that is, @xmath is
@xmath -sparse . The goal of compressed sensing is to construct
relatively few non-adaptive linear measurements along with a stable and
efficient reconstruction algorithm that exploits this sparsity
structure. Expressing each measurement as a row of an @xmath matrix
@xmath , we have the following noisy system:

  -- -------- -- -----
     @xmath      (1)
  -- -------- -- -----

In the spirit of compressed sensing, we only want a few measurements:
@xmath . Also, in order for there to exist an inversion process for ( 1
), @xmath must map @xmath -sparse vectors injectively, or equivalently,
every subcollection of @xmath columns of @xmath must be linearly
independent. Unfortunately, the natural reconstruction method in this
general case, i.e., finding the sparsest approximation of @xmath from
the dictionary of columns of @xmath , is known to be @xmath -hard [ 108
] . Moreover, the independence requirement does not impose any sort of
dissimilarity between the columns of @xmath , meaning distinct identity
basis elements could lead to similar measurements, thereby bringing
instability in reconstruction.

To get around the @xmath -hardness of sparse approximation, we need more
structure in the matrix @xmath . Indeed, several efficient
reconstruction algorithms have been considered (e.g., Basis Pursuit [ 61
, 62 , 77 ] , Orthogonal Matching Pursuit [ 62 , 134 ] , and the Least
Absolute Shrinkage and Selection Operator [ 20 ] ), and their original
performance guarantees depend on the additional structure that the
columns of @xmath are nearly orthogonal to each other. Depending on the
algorithm, this structure in the sensing matrix enables successful
reconstruction when noise term @xmath in ( 1 ) is zero, adversarial, or
stochastic, but for any of the original guarantees to apply, the
sparsity level must be @xmath . To reconstruct signals with larger
sparsity levels, Candès and Tao [ 39 ] impose a much stronger
requirement on the sensing matrix: that every submatrix of @xmath
columns of @xmath be well-conditioned. To be explicit, we have the
following definition:

###### Definition 1.

The matrix @xmath has the @xmath -restricted isometry property (RIP) if

  -- -------- --
     @xmath   
  -- -------- --

for every @xmath -sparse vector @xmath . The smallest @xmath for which
@xmath is @xmath -RIP is the restricted isometry constant (RIC) @xmath .

In words, matrices which satisfy RIP act as a near-isometry on
sufficiently sparse vectors. Among other things, this structure imposes
near-orthogonality between the columns of @xmath , and so in light of
the previous results, it is not surprising that RIP sensing matrices
enable efficient reconstruction:

###### Theorem 2 (Theorem 1.3 in [34]).

Suppose an @xmath matrix @xmath has the @xmath -restricted isometry
property for some @xmath . Assuming @xmath , then for every @xmath
-sparse vector @xmath , the following reconstruction from ( 1 ):

  -- -------- --
     @xmath   
  -- -------- --

satisfies @xmath , where @xmath only depends on @xmath .

The exciting part about this guarantee is how the sparsity level @xmath
of recoverable signals scales with the number of measurements @xmath .
Certainly, we expect at least @xmath since RIP is a stronger matrix
requirement than near-orthogonality between columns. In analyzing the
sparsity level, random matrices have found the most success,
specifically matrices with independent Gaussian or Bernoulli entries [
17 ] , or matrices whose rows were randomly selected from the discrete
Fourier transform matrix [ 118 ] . With high probability, these random
constructions support sparsity levels @xmath on the order of @xmath for
some @xmath . Intuitively, this level of sparsity is near-optimal
because @xmath cannot exceed @xmath by the linear independence
condition. Thus, Theorem 2 is a substantial improvement over the
previous guarantees, and this has prompted further investigation of RIP
matrices. Unfortunately, it is difficult to check whether a particular
instance of a random matrix is @xmath -RIP, as this involves the
calculation of singular values for all @xmath submatrices of @xmath
columns of the matrix. For this reason, and for the sake of reliable
sensing standards, many have pursued deterministic RIP matrix
constructions; Tao discusses the significance of this open problem in [
132 ] .

Throughout this thesis, we consider the problem from a variety of
directions. In Chapter 1, we observe a technique which is commonly used
to analyze the restricted isometry of deterministic constructions: the
Gershgorin circle theorem. This technique fails to demonstrate RIP for
large sparsity levels; it is only capable of showing RIP for sparity
levels on the order of @xmath , as opposed to @xmath . This limitation
has become known as the “square-root bottleneck.” To illustrate that
this bottleneck is not merely an artifact of the Gershgorin analysis, we
consider a construction which is optimal in the Gershgorin sense, and we
establish that this construction is @xmath -RIP for every @xmath but is
not @xmath -RIP for any @xmath . The first inequality is proved by the
Gershgorin circle theorem, while the second uses the spark of the
matrix, that is, the number of nonzero entries in the sparsest vector in
its nullspace. While this disparity between @xmath and @xmath is
significant in many applications, such constructions are particularly
well-suited for the sparse signal processing application of digital
fingerprinting, and so we briefly investigate this application.

For the applications with larger sparsity levels, we note that spark
deficiency is incompatible with restricted isometry; indeed, any matrix
which is @xmath -RIP necessarily has spark strictly greater than @xmath
. As such, in Chapter 2, we consider @xmath full spark matrices, that
is, matrices whose spark is as large as possible: @xmath . We start by
finding various full spark constructions using Vandermonde matrices and
discrete Fourier transforms. These deterministic constructions are
particularly attractive as RIP candidates because they satisfy the
necessary condition of large spark, a property which is difficult to
verify in general. To solidify this notion of difficulty, we also show
that the problem of testing whether a matrix is full spark is hard for
@xmath under randomized polynomial-time reductions; this contrasts with
the similar problem of testing for RIP, which currently has unknown
computational complexity [ 93 ] . To demonstrate that full spark
matrices are useful in their own right, we use them to solve another
important problem in sparse signal processing: signal recovery without
phase.

To date, the only deterministic RIP construction that manages to go
beyond the square-root bottleneck is given by Bourgain et al. [ 29 ] .
In Chapter 3, we discuss the technique they use to demonstrate RIP. It
is important to stress the significance of their contribution: Before [
29 ] , it was unclear how deterministic analysis might break the
bottleneck, and as such, their result is a major theoretical
achievement. On the other hand, their improvement over the square-root
bottleneck is notably slight compared to what random matrices provide.
However, we show that their technique can actually be used to
demonstrate RIP for sparsity levels much larger than @xmath , meaning
one could very well demonstrate random-like performance given the proper
construction. Our result applies their technique to random matrices, and
it inadvertently serves as a simple alternative proof that certain
random matrices are RIP. We also introduce another technique, and we
show that it can demonstrate RIP for similarly large sparsity levels.
Later, we propose a specific class of full spark matrices as candidates
for being RIP. Using a correspondence between these matrices and the
Paley graphs, we observe certain combinatorial and number-theoretic
implications; this lends some probabilistic intuition for a new bound on
the clique number of Paley graphs of prime order.

After investigating deterministic RIP matrices in Chapters 1–3, we have
yet to find deterministic @xmath sensing matrices which provably allow
for the efficient reconstruction of signals with sparsity level @xmath
for some @xmath . To fill this gap, in Chapter 4, we consider an
alternative model for the sparsity in our signal, namely, that the
locations of the nonzero entries are drawn uniformly at random. With
this model, we show that a particularly simple algorithm called one-step
thresholding can reconstruct the signal with high probability provided
@xmath . In fact, this performance guarantee requires relatively modest
structure in the sensing matrix: that the columns are nearly orthogonal
to each other and well-distributed over the unit sphere. Indeed, this
structural requirement is much less stringent than RIP, and we provide a
catalog of random and deterministic sensing matrices which satisfy these
conditions. Later, we further analyze the two conditions separately,
finding new fundamental limits on near-orthogonality and illustrating
how to manipulate a given sensing matrix to achieve good distribution
over the sphere.

Throughout this thesis, we use ideas from frame theory , and so it is
fitting to take some time to review the basics:

### 0.2 A brief introduction to frame theory

A frame is a sequence @xmath in a Hilbert space @xmath with frame bounds
@xmath that satisfy

  -- -------- --
     @xmath   
  -- -------- --

Frames were introduced by Duffin and Schaeffer [ 64 ] in the context of
nonharmonic Fourier analysis, where @xmath and the frame elements @xmath
are sinusoids of irregularly spaced frequencies. However, the modern
application of frame theory to signal processing came decades later
after the landmark paper of Daubechies et al. [ 55 ] . This paper gave
the first nontrivial examples of tight frames , that is, frames with
equal frame bounds @xmath . The utility of tight frames lies partially
in their painless reconstruction formula:

  -- -------- --
     @xmath   
  -- -------- --

Note that orthonormal bases are tight frames with @xmath ; in this way,
frames form a natural and useful generalization. While this founding
research in frame theory concerned frames over infinite-dimensional
Hilbert spaces, many of today’s applications of frames require a
finite-dimensional treatment. In fact, finite frame theory has found
some important progress in the past decade [ 18 , 33 , 42 , 43 , 47 ,
129 ] , and the remainder of this section will discuss the basics of
this field.

In finite dimensions, say, @xmath , a frame is given by the columns of a
full-rank @xmath matrix @xmath with @xmath . Here, the extreme
eigenvalues of @xmath are the frame bounds, and a tight frame has equal
frame bounds; equivalently, a frame @xmath is tight if

-   the rows are equal-norm and orthogonal.

As established above, tight frames @xmath are useful because they give a
redundant linear encoding @xmath of a signal @xmath that permits
painless recovery: @xmath , where @xmath is the common squared-norm of
the rows. Constructing tight frames is rather simple: perform
Gram-Schmidt on the rows of any frame to orthogonalize with equal norms.
For the sake of democracy in the entries of the encoding @xmath , some
applications opt for a unit norm tight frame (UNTF) [ 45 ] , which has
the additional property that

-   the columns are unit-norm.

Constructing UNTFs has proven a bit more difficult, and there has been a
lot of research to characterize these [ 18 , 33 , 127 ] . As a special
example of a UNTF, take any rows from a discrete Fourier transform
matrix and normalize the resulting columns. In addition to unit-norm
tightness, it is often beneficial to have the columns of @xmath be
incoherent, and this occurs when @xmath is an equiangular tight frame
(ETF) , that is, a UNTF with the final property that

-   the sizes of the inner products between distinct columns are equal.

ETFs do not exist for all matrix dimensions [ 19 ] , and there are only
three general constructions to date [ 70 , 141 , 146 ] ; these invoke
block designs, strongly regular graphs, and difference sets,
respectively.

To mitigate any confusion, the reader should be aware that throughout
the literature, both UNTFs and ETFs are referred to as Welch-bound
equality sequences [ 120 ] . As one might expect, each achieves equality
in one of two important inequalities, and it is important to review
them. Consider @xmath matrices @xmath which have (ii), but not
necessarily (i) or (iii). As such, @xmath might not be a frame, but we
can still take the Hilbert-Schmidt norm of the Gram matrix of its
columns:

  -- -------- --
     @xmath   
  -- -------- --

This is oftentimes called the frame potential of @xmath [ 18 ] , and its
significance will become apparent shortly. Since the columns of @xmath
have unit norm, and since @xmath has at most @xmath nonzero eigenvalues,
we have

  -- -------- --
     @xmath   
  -- -------- --

where the inequality follows from the Cauchy-Schwarz inequality with the
all-ones vector. As such, equality is achieved if and only if the @xmath
largest eigenvalues of @xmath are equal; since these are also the
eigenvalues of @xmath , this implies that @xmath is a multiple identity,
and so @xmath satisfies (ii). Thus, the frame potential of @xmath
satisfies @xmath , with equality if and only if @xmath is a UNTF. Some
call this the Welch bound , and therefore say that UNTFs have
Welch-bound equality.

Another bound is also (more correctly) referred to as the Welch bound,
and its derivation uses the previous one. It concerns the worst-case
coherence of an @xmath matrix @xmath that satisfies (ii):

  -- -------- --
     @xmath   
  -- -------- --

Since the columns of @xmath have unit norm, we have

  -- -------- --
     @xmath   
  -- -------- --

Again, equality is achieved in the first inequality if and only if
@xmath satisfies (i). Also, equality is achieved in the second
inequality if and only if @xmath satisfies (iii). Rearranging gives the
following:

###### Theorem 3 (Welch bound [129, 143]).

Every @xmath matrix @xmath with unit-norm columns has worst-case
coherence

  -- -------- --
     @xmath   
  -- -------- --

with equality if and only if @xmath is an equiangular tight frame.

Equiangular lines have long been a subject of interest [ 97 ] , and
since equiangular tight frames have minimal coherence, they are
particularly useful in a number of applications. Recent work on ETFs was
spurred by results inspired by communication theory [ 26 , 84 , 129 ]
that show that the linear encoders provided by ETFs are optimally robust
against channel erasures. In the real setting, the existence of an ETF
of a given size is equivalent to the existence of a strongly regular
graph with certain corresponding parameters [ 84 , 122 ] . Such graphs
have a rich history and remain an active topic of research [ 31 ] ; the
specific ETFs which arise from particular graphs are detailed in [ 141 ]
. Some of this theory generalizes to the complex-variable setting in the
guise of complex Seidel matrices [ 25 , 27 , 65 ] . Many approaches to
constructing ETFs have focused on the special case in which every entry
of @xmath is a root of unity [ 88 , 115 , 128 , 130 , 146 ] . Other
approaches are given in [ 46 , 125 , 137 ] . In the complex setting,
much attention has focused on the maximal case of @xmath vectors in
@xmath [ 9 , 68 , 91 , 116 , 121 ] .

In the next chapter, we construct one of three known general families of
ETFs, and we evaluate their performance as RIP matrices. Having reviewed
the frame-theoretic background for this thesis, the interested reader is
encouraged to discover more about frame theory in [ 49 ] .

## Chapter 1 Steiner equiangular tight frames

In this chapter, we provide a new method for constructing equiangular
tight frames (ETFs), that is, matrices @xmath with orthogonal and
equal-norm rows, and unit-norm columns whose inner products are equal in
modulus. As discussed earlier, such frames have minimal worst-case
coherence, and are therefore quite useful in applications. However, up
to this point, they have proven notoriously difficult to construct. By
contrast, the construction of Steiner equianglar tight frames is
particularly simple: a tensor-like combination of a Steiner system and a
regular simplex. This simplicity permits us to resolve an open question
regarding ETFs and the restricted isometry property (RIP): we show that
the RIP performance of some ETFs is unfortunately no better than the
so-called “square-root bottleneck.”

In the next section, we provide some simple tests for demonstrating
whether a given matrix is RIP; not only will this clarify the notion of
the square-root bottleneck, it will show how ETFs are in some sense
optimal as deterministic RIP matrices, thereby motivating the
construction of ETFs. Later, we provide the main result of this chapter,
namely Theorem 7 , which shows how certain Steiner systems may be
combined with regular simplices to produce ETFs [ 69 , 70 ] . In the
third section, we discuss each of the known infinite families of such
Steiner systems, and compute the corresponding infinite families of ETFs
they generate. We further provide some necessary and asymptotically
sufficient conditions, namely Theorem 8 , to aid in the quest for
discovering other examples of such frames that lie outside of the known
infinite families. Finally, after demonstrating that Steiner ETFs fail
to break the square-root bottleneck, we consider their application to
the design of digital fingerprints to combat data piracy [ 103 , 104 ] .

### 1.1 Simple tests for restricted isometry

Before formally defining Steiner equiangular tight frames, we motivate
their construction by reviewing a couple common methods for determining
whether a matrix is RIP:

  ------------------------ -----------------------------------------------------------------
  Positive test for RIP:   Apply the Gershgorin circle theorem to the submatrices @xmath .
  Negative test for RIP:   Find a sparse vector in the nullspace of @xmath .
  ------------------------ -----------------------------------------------------------------

In what follows, we discuss each of these tests in more detail, and
later, we will use these tests to analyze Steiner ETFs as RIP matrices.

#### 1.1.1 Applying Gershgorin’s circle thoerem

Take an @xmath matrix @xmath , and recall Definition 1 . For a given
@xmath , we wish to find some @xmath for which @xmath is @xmath -RIP. To
this end, it is useful to consider the following expression for the
restricted isometry constant:

###### Lemma 4.

The smallest @xmath for which @xmath is @xmath -RIP is given by

  -- -------- -- -------
     @xmath      (1.1)
  -- -------- -- -------

where @xmath denotes the submatrix consisting of columns of @xmath
indexed by @xmath .

###### Proof.

We first note that @xmath being @xmath -RIP trivially implies that
@xmath is @xmath -RIP for every @xmath . It therefore suffices to show
that the expression for @xmath in ( 1.1 ) satisfies two criteria: (i)
@xmath is @xmath -RIP, and (ii) @xmath is not @xmath -RIP for any @xmath
. To this end, pick some @xmath -sparse vector @xmath . To prove (i), we
need to show that

  -- -------- -- -------
     @xmath      (1.2)
  -- -------- -- -------

Let @xmath be the size- @xmath support of @xmath , and let @xmath be the
corresponding subvector. Then rearranging ( 1.2 ) gives

  -- -------- -- -------
     @xmath      (1.3)
  -- -------- -- -------

Since the expression for @xmath in ( 1.1 ) maximizes ( 1.3 ) over all
supports @xmath and entry values @xmath , the inequality necessarily
holds; that is, @xmath is necessarily @xmath -RIP. Furthermore, equality
is achieved by the support @xmath which maximizes ( 1.1 ) and the
eigenvector @xmath corresponding to the largest eigenvalue of @xmath ;
this proves (ii). ∎

Note that we are not tasked with actually computing @xmath ; rather, we
recognize that @xmath is @xmath -RIP for every @xmath , and so we seek
an upper bound on @xmath . The following classical result offers a
particularly easy-to-calculate bound on eigenvalues:

###### Theorem 5 (Gershgorin circle theorem [73]).

For each eigenvalue @xmath of a @xmath matrix @xmath , there is an index
@xmath such that

  -- -------- --
     @xmath   
  -- -------- --

To use this theorem, take some @xmath with unit-norm columns. Note that
@xmath is the Gram matrix of the columns indexed by @xmath , and as
such, the diagonal entries are @xmath , and the off-diagonal entries are
inner products between distinct columns of @xmath . Let @xmath denote
the worst-case coherence of @xmath :

  -- -------- --
     @xmath   
  -- -------- --

Then the size of each off-diagonal entry of @xmath is @xmath ,
regardless of our choice for @xmath . Therefore, for every eigenvalue
@xmath of @xmath , the Gershgorin circle theorem gives

  -- -------- -- -------
     @xmath      (1.4)
  -- -------- -- -------

Since ( 1.4 ) holds for every eigenvalue @xmath of @xmath and every
choice of @xmath , we conclude from ( 1.1 ) that @xmath , i.e., @xmath
is @xmath -RIP. This process of using the Gershgorin circle theorem to
demonstrate RIP for deterministic constructions has become standard in
the community [ 8 , 60 , 70 ] .

Recall that random RIP constructions support sparsity levels @xmath on
the order of @xmath for some @xmath . To see how well the Gershgorin
circle theorem demonstrates RIP, we need to express @xmath in terms of
@xmath and @xmath . To this end, we consider the Welch bound (Theorem 3
):

  -- -------- --
     @xmath   
  -- -------- --

Since equiangular tight frames (ETFs) achieve equality in the Welch
bound (as demonstrated in Section 0.2 ), we can further analyze what it
means for an @xmath ETF @xmath to be @xmath -RIP. In particular, since
Theorem 2 requires that @xmath be @xmath -RIP for @xmath , it suffices
to have @xmath , since this implies

  -- -------- -- -------
     @xmath      (1.5)
  -- -------- -- -------

That is, ETFs form sensing matrices that support sparsity levels @xmath
on the order of @xmath . Most other deterministic constructions have
identical bounds on sparsity levels [ 8 , 60 , 70 ] . In fact, since
ETFs minimize coherence, they are necessarily optimal constructions in
terms of the Gershgorin demonstration of RIP, but the question remains
whether they are actually RIP for larger sparsity levels; the Gershgorin
demonstration fails to account for cancellations in the sub-Gram
matrices @xmath , and so this technique is too weak to indicate either
possibility.

#### 1.1.2 Spark considerations

Recall that, in order for an inversion process for ( 1 ) to exist,
@xmath must map @xmath -sparse vectors injectively, or equivalently,
every subcollection of @xmath columns of @xmath must be linearly
independent. This linear independence condition can be nicely expressed
in more general terms, as the following definition provides:

###### Definition 6.

The spark of a matrix @xmath is the size of the smallest linearly
dependent subset of columns, i.e.,

  -- -------- --
     @xmath   
  -- -------- --

This definition was introduced by Dohono and Elad [ 61 ] to help build a
theory of sparse representation that later gave birth to modern
compressed sensing. The concept of spark is also found in matroid
theory, where it goes by the name girth . The condition that every
subcollection of @xmath columns of @xmath is linearly independent is
equivalent to @xmath . Relating spark to RIP, suppose @xmath is @xmath
-RIP with @xmath . Then there exists a nonzero @xmath -sparse vector
@xmath such that @xmath , and so @xmath . The reason behind this stems
from our necessary linear independence condition: RIP implies linear
independence, and so small spark implies linear dependence, which in
turn implies not RIP.

As an example of using spark to test RIP, consider the @xmath matrix
@xmath that comes from concatenating the identity matrix @xmath with the
unitary discrete Fourier transform matrix @xmath . In this example,
columns from a common orthonormal basis are orthogonal, while columns
from different bases have an inner product of size @xmath . As such, the
Gershgorin analysis gives that @xmath is @xmath -RIP for all @xmath .
However, when @xmath is a perfect square, the Dirac comb @xmath of
@xmath Kronecker deltas is an eigenvector of @xmath , and so
concatenating @xmath with @xmath produces a @xmath -sparse vector in the
nullspace of @xmath . In other words, @xmath , and so @xmath is not
@xmath -RIP for any @xmath . After building Steiner equiangular tight
frames, we will see that they perform similarly as RIP matrices.

### 1.2 Constructing Steiner equiangular tight frames

Steiner systems and block designs have been studied for over a century;
the background facts presented here on these topics are taken from [ 1 ,
52 ] . In short, a @xmath - block design is a @xmath -element set @xmath
along with a collection @xmath of @xmath size- @xmath subsets of @xmath
, dubbed blocks , that have the property that any element of @xmath lies
in exactly @xmath blocks and that any @xmath -element subset of @xmath
is contained in exactly @xmath blocks. The corresponding incidence
matrix is a @xmath matrix @xmath that is one in a given entry if that
block contains the corresponding point, and is otherwise zero; in this
chapter, it is more convenient for us to work with the @xmath transpose
@xmath of this incidence matrix. Our particular construction of ETFs
involves a special class of block designs known as @xmath - Steiner
systems . These have the property that any @xmath -element subset of
@xmath is contained in exactly one block, that is, @xmath . With respect
to our purposes, the crucial facts are the following:

  The transpose @xmath of the @xmath -incidence matrix @xmath of a
  @xmath -Steiner system:

  1.  is of size @xmath ,

  2.  has @xmath ones in each row,

  3.  has @xmath ones in each column, and

  4.  has the property that any two of its columns have a inner product
      of one.

The first three facts follow immediately from solving for @xmath and
@xmath , using the well-known relations @xmath and @xmath . Meanwhile,
(iv) comes from the fact that @xmath : each column of @xmath corresponds
to an element of the set, and the inner product of any two columns
computes the number of blocks that contains the corresponding pair of
points. This in hand, we present the main result of this chapter; here,
the density of a matrix is the ratio of the number of nonzero entries of
that matrix to the total number of its entries:

###### Theorem 7.

Every @xmath -Steiner system generates an equiangular tight frame
consisting of @xmath vectors in @xmath -dimensional space with
redundancy @xmath and density @xmath .

Moreover, if there exists a real Hadamard matrix of size @xmath , then
such frames are real.

Specifically, a @xmath ETF matrix @xmath may be constructed as follows:

1.   Let @xmath be the @xmath transpose of the adjacency matrix of a
    @xmath -Steiner system.

2.   For each @xmath , let @xmath be any @xmath matrix that has
    orthogonal rows and unimodular entries, such as a possibly complex
    Hadamard matrix.

3.   For each @xmath , let @xmath be the @xmath matrix obtained from the
    @xmath th column of @xmath by replacing each of the one-valued
    entries with a distinct row of @xmath , and every zero-valued entry
    with a row of zeros.

4.   Concatenate and rescale the @xmath ’s to form @xmath .

It is important to note that a version of this ETF construction was
previously employed by Seidel in Theorem 12.1 of [ 122 ] to prove the
existence of certain strongly regular graphs. In the context of that
result, our contributions are as follows: (i) the realization that when
Seidel’s block design arises from a particular type of Steiner system,
the resulting strongly regular graph indeed corresponds to a real ETF;
(ii) noting that in this case, the graph theory may be completely
bypassed, as the idea itself directly produces the requisite frame
@xmath ; and (iii) having bypassed the graph theory, realizing that this
construction immediately generalizes to the complex-variable setting if
Seidel’s requisite Hadamard matrix is permitted to become complex. These
realizations permit us to exploit the vast literature on Steiner systems
[ 52 ] to construct several new infinite families of ETFs, in both the
real and complex settings. Moreover, these ETFs are extremely sparse in
their native space; sparse tight frames have recently become a subject
of interest in their own right [ 44 ] .

We refer to the ETFs produced by Theorem 7 as @xmath -Steiner ETFs . In
essence, the idea of the construction is that the nonzero rows of any
particular @xmath form a regular simplex in @xmath -dimensional space;
these vectors are automatically equiangular amongst themselves; by
requiring the entries of these simplices to be unimodular, and requiring
that distinct blocks have only one entry of mutual support, one can
further control the inner products of vectors arising from distinct
blocks. This idea is best understood by considering a simple example,
such as the ETF that arises from a @xmath -Steiner system whose
transposed incidence matrix is

  -- -------- --
     @xmath   
  -- -------- --

One can immediately verify that @xmath corresponds to a block design:
there is a set @xmath of @xmath elements, each corresponding to a column
of @xmath ; there is also a collection @xmath of @xmath subsets of
@xmath , each corresponding to a row of @xmath ; every row contains
@xmath elements; every column contains @xmath elements; any given pair
of elements is contained in exactly one row, that is, @xmath , a fact
which is equivalent to having the inner product of any two distinct
columns of @xmath being @xmath . To form an ETF, for each of the four
columns of @xmath we must choose a @xmath matrix @xmath with unimodular
entries and orthogonal rows; the size of @xmath is always one more than
the number @xmath of ones in a given column of @xmath . Though in
principle one may choose a different @xmath for each column, we choose
them all to be the same, namely the Hadamard matrix:

  -- -------- --
     @xmath   
  -- -------- --

To form the ETF, for each column of @xmath we replace each of its @xmath
-valued entries with a distinct row of @xmath . Again, though in
principle one may choose a different sequence of rows of @xmath for each
column, we simply decide to use the second, third and fourth rows, in
that order. The result is a real ETF of @xmath elements of dimension
@xmath :

  -- -------- -- -------
     @xmath      (1.6)
  -- -------- -- -------

One can immediately verify that the rows of @xmath are orthogonal and
have constant norm, implying @xmath is indeed a tight frame. One can
also easily see that the inner products of two columns from the same
block are @xmath , while the inner products of columns from distinct
blocks are @xmath . Theorem 7 states that this behavior holds in general
for any appropriate choice of @xmath and @xmath .

###### Proof of Theorem 7.

To verify @xmath is a tight frame, note that the inner product of any
two distinct rows of @xmath is zero, as they are the sum of the inner
products of the corresponding rows of the @xmath ’s over all @xmath ;
for any @xmath , these shorter inner products are necessarily zero, as
they either correspond to inner products of distinct rows of @xmath or
to inner products with zero vectors. Moreover, the rows of @xmath have
constant norm: as noted in (ii) above, each row of @xmath contains
@xmath ones; since each @xmath has unimodular entries, the squared-norm
of any row of @xmath is the squared-scaling factor @xmath times a sum of
@xmath ones, which, as is necessary for any unit norm tight frame,
equals the redundancy @xmath .

Having that @xmath is tight, we show @xmath is also equiangular. We
first note that the columns of @xmath have unit norm: the squared-norm
of any column of @xmath is @xmath times the squared-norm of a column of
one of the @xmath ’s; since the entries of @xmath are unimodular and
(iii) above gives that each column of @xmath contains @xmath ones, the
squared-norm of any column of @xmath is @xmath , as claimed. Moreover,
the inner products of any two distinct columns of @xmath has constant
modulus. Indeed, the fact (iv) that any two distinct columns of @xmath
have but a single entry of mutual support implies the same is true for
columns of @xmath that arise from distinct @xmath blocks, implying the
inner product of such columns is @xmath times the product of two
unimodular numbers. That is, the squared-magnitude of the inner products
of two columns that arise from distinct blocks is @xmath , as needed.
Meanwhile, the same holds true for columns that arise from the same
block @xmath . To see this, note that since @xmath is a scalar multiple
of a unitary matrix, its columns are orthogonal. Moreover, @xmath
contains all but one of the @xmath ’s rows, namely one for each of the
@xmath -valued entries of @xmath , à la (iii). Thus, the inner products
of the portions of @xmath that lie in @xmath are their entire inner
product of zero, less the contribution from the left-over entries.
Overall, the inner product of two columns of @xmath that arise from the
same @xmath block is @xmath times the negated product of one entry of
@xmath and the conjugate of another; since the entries of @xmath are
unimodular, we have that the squared-magnitude of such inner products is
@xmath , as needed.

Thus @xmath is an ETF. Moreover, as noted above, its redundancy is
@xmath . All that remains to verify is its density: as the entries of
each @xmath are all nonzero, the proportion of @xmath ’s nonzero entries
is the same as that of the incidence matrix @xmath , which is clearly
@xmath , having @xmath ones in each @xmath -dimensional row. Moreover,
substituting @xmath and @xmath into the quantity @xmath reveals it to be
@xmath , and so the density can be alternatively expressed as @xmath . ∎

In the next section, we apply Theorem 7 to produce several infinite
families of Steiner ETFs. Before doing so, however, we pause to remark
on the redundancy and sparsity of such frames. In particular, note that
since the parameters @xmath and @xmath of the requisite Steiner system
always satisfy @xmath , the redundancy @xmath of Steiner ETFs is always
between @xmath and @xmath ; the redundancy is therefore on the order of
@xmath , and is always strictly greater than @xmath . If a
low-redundancy ETF is desired, one can always take the Naimark
complement [ 43 ] of an ETF of @xmath elements in @xmath -dimensional
space to produce a new ETF of @xmath elements in @xmath -dimensional
space; though the complement process does not preserve sparsity, it
nevertheless transforms any Steiner ETF into a new ETF whose redundancy
is strictly less than @xmath . However, such a loss of sparsity should
not be taken lightly. Indeed, the low density of Steiner ETFs gives them
a large computational advantage over their non-sparse brethren.

To clarify, the most common operation in frame-theoretic applications is
the evaluation of the analysis operator @xmath on a given @xmath . For a
non-sparse @xmath , this act of computing @xmath requires @xmath
operations; for a frame @xmath of density @xmath , this cost is reduced
to @xmath . Indeed, using the explicit value of @xmath given in Theorem
7 as well as the aforementioned fact that the redundancy of such frames
necessarily satisfies @xmath , we see that the cost of evaluating @xmath
when @xmath is a Steiner ETF is on the order of @xmath operations, a
dramatic cost savings when @xmath is large. Further efficiency is gained
when @xmath is real, as its nonzero elements are but a fixed scaling
factor times the entries of a real Hadamard matrix, implying @xmath can
be evaluated using only additions and subtractions. The fact that every
entry of @xmath is either @xmath or @xmath further makes real Steiner
ETFs potentially useful for applications that require binary
measurements, such as design of experiments.

### 1.3 Examples of Steiner equiangular tight frames

In this section, we apply Theorem 7 to produce several infinite families
of Steiner ETFs. When designing frames for real-world applications,
three considerations reign supreme: size, redundancy and sparsity. As
noted above, every Steiner ETF is very sparse, a serious computational
advantage in high-dimensional signal processing. Moreover, some of these
infinite families, such as those arising from finite affine and
projective geometries, provide great flexibility in choosing the ETF’s
size and redundancy. Indeed, these constructions provide the first known
guarantee that for a given application, one is always able to find ETFs
whose frame elements lie in a space whose dimension matches, up to an
order of magnitude, that of one’s desired class of signals, while
simultaneously permitting one to have an almost arbitrary fixed level of
redundancy, a handy weapon in the fight against noise. To be clear,
recall that the redundancy of a Steiner ETF is always strictly greater
than @xmath . Moreover, general bounds on the maximal number of
equiangular lines [ 97 ] require that any real @xmath ETF satisfy @xmath
and any complex ETF satisfy @xmath ; thus, the redundancy of an ETF is
never truly arbitrary. Nevertheless, if one prescribes a given level of
redundancy in advance, the Steiner method can produce arbitrarily large
ETFs whose redundancy is approximately the prime power closest to the
desired level.

#### 1.3.1 Infinite families of Steiner equiangular tight frames

We now detail eight infinite families of ETFs, each generated by
applying Theorem 7 to one of the eight completely understood infinite
families of @xmath -Steiner systems. Table 1.1 summarizes the most
important features of each family, and Table 1.2 gives the first few
examples of each type, summarizing those that lie in 100 dimensions or
less.

##### All two-element blocks: @xmath-Steiner ETFs for any @xmath.

The first infinite family of Steiner systems is so simple that it is
usually not discussed in the design-theory literature. For any @xmath ,
let @xmath be a @xmath -element set, and let @xmath be the collection of
all @xmath -element subsets of @xmath . Clearly, we have @xmath blocks,
each of which contains @xmath elements; each point is contained in
@xmath blocks, and each pair of points is indeed contained in but a
single block, that is, @xmath .

By Theorem 7 , the ETFs arising from these @xmath -Steiner systems
consist of @xmath vectors in @xmath -dimensional space. Though these
frames can become arbitrarily large, they do not provide any freedom
with respect to redundancy: @xmath is essentially @xmath . These frames
have density @xmath . Moreover, these ETFs can be real-valued if there
exists a real Hadamard matrix of size @xmath . In particular, it
suffices to have @xmath to be a power of @xmath ; should the Hadamard
conjecture prove true, it would suffice to have @xmath divisible by
@xmath .

One example of such an ETF with @xmath was given in the previous
section. For a complex example, consider @xmath . The @xmath transposed
incidence matrix @xmath is @xmath , with each row corresponding to a
given @xmath -element subset of @xmath :

  -- -------- --
     @xmath   
  -- -------- --

To form the corresponding @xmath ETF @xmath , we need a @xmath
unimodular matrix with orthogonal rows, such as a DFT; letting @xmath ,
we can take

  -- -------- --
     @xmath   
  -- -------- --

To form @xmath , in each column of @xmath , we replace each @xmath
-valued entry with a distinct row of @xmath . Always choosing the second
and third rows yields an ETF of @xmath elements in @xmath :

  -- -------- --
     @xmath   
  -- -------- --

This is the only known instance of when the Steiner-based construction
of Theorem 7 produces a maximal ETF, that is, one that has @xmath .

##### Steiner triple systems: @xmath-Steiner ETFs for any @xmath.

Steiner triple systems , namely @xmath -Steiner systems, have been a
subject of interest for over a century, and are known to exist precisely
when @xmath [ 52 ] . Each of the @xmath blocks contains @xmath points,
while each point is contained in @xmath blocks. The corresponding ETFs
produced by Theorem 7 consist of @xmath vectors in @xmath -dimensional
space. The density of such frames is @xmath . As with ETFs stemming from
@xmath -element blocks, Steiner triple systems offer little freedom in
terms of redundancy: @xmath is always approximately @xmath . Such ETFs
can be real if there exists a real Hadamard matrix of size @xmath .

##### Four element blocks: @xmath-Steiner ETFs for any @xmath.

It is known that @xmath -Steiner systems exist precisely when @xmath [ 1
] . Continuing the trend of the previous two families, these ETFs can
vary in size but not in redundancy: they consist of @xmath vectors in
@xmath -dimensional space, having redundancy @xmath and density @xmath .
Interestingly, such frames can never be real: with the exception of the
trivial @xmath and @xmath cases, the dimensions of all real Hadamard
matrices are divisible by @xmath ; since @xmath , the requisite matrices
@xmath here are of size @xmath .

##### Five element blocks: @xmath-Steiner ETFs for any @xmath.

It is also known that @xmath -Steiner systems exist precisely when
@xmath [ 1 ] . The corresponding ETFs consist of @xmath vectors in
@xmath -dimensional space, having redundancy @xmath and density @xmath .
Such frames can be real whenever there exists a real Hadamard matrix of
size @xmath . In particular, letting @xmath , we see that there exists a
real Steiner ETF of @xmath vectors in @xmath -dimensional space, a fact
not obtained from any other known infinite family.

##### Affine geometries: @xmath-Steiner ETFs for any prime power @xmath,
@xmath.

At this point, the constructions depart from those previously
considered, allowing both @xmath and @xmath to vary. In particular,
using techniques from finite geometry, one can show that for any prime
power @xmath and any @xmath , there exists a @xmath -Steiner system with
@xmath and @xmath [ 52 ] . The corresponding ETFs consist of @xmath
vectors in @xmath -dimensional space. Like the preceding four classes of
Steiner ETFs, these frames can grow arbitrarily large: fixing any prime
power @xmath , one may manipulate @xmath to produce ETFs of varying
orders of magnitude. However, unlike the four preceding classes, these
affine Steiner ETFs also provide great flexibility in choosing
redundancy. That is, they provide the ability to pick @xmath and @xmath
somewhat independently. Indeed, the redundancy of such frames @xmath is
essentially @xmath , which may be an arbitrary prime power. Moreover, as
these frames grow large, they also become increasingly sparse: their
density is @xmath . Because of their high sparsity and flexibility with
regards to size and redundancy, these frames, along with their
projective geometry-based cousins detailed below, are perhaps the best
known candidates for use in ETF-based applications. Such ETFs can be
real if there exists a real Hadamard matrix of size @xmath , such as
whenever @xmath , or when @xmath and @xmath .

##### Projective geometries: @xmath-Steiner ETFs for any prime power
@xmath, @xmath.

With finite geometry, one can show that for any prime power @xmath and
any @xmath , there exists a @xmath -Steiner system with @xmath and
@xmath [ 52 ] . Qualitatively speaking, the ETFs that these projective
geometries generate share much in common with their affinely generated
cousins, possessing very high sparsity and great flexibility with
respect to size and redundancy. The technical details are as follows:
they consist of @xmath vectors in @xmath -dimensional space, with
density @xmath and redundancy @xmath . These frames can be real if there
exists a real Hadamard matrix of size @xmath ; note this restriction is
identical to the one for ETFs generated by affine geometries for the
same @xmath and @xmath , implying that real Steiner ETFs generated by
finite geometries always come in pairs, such as the @xmath and @xmath
ETFs generated when @xmath , @xmath , and the @xmath and @xmath ETFs
generated when @xmath , @xmath .

##### Unitals: @xmath-Steiner ETFs for any prime power @xmath.

For any prime power @xmath , one can show that there exists a @xmath
-Steiner system with @xmath and @xmath [ 52 ] . Though one may pick a
redundancy of one’s liking, such a choice confines one to ETFs of a
given size: they consist of @xmath vectors in @xmath -dimensional space,
having redundancy @xmath and density @xmath . These ETFs can never be
real: the requisite Hadamard matrices are of size @xmath which is never
divisible by @xmath since @xmath and @xmath are the only squares in
@xmath .

##### Denniston designs: @xmath-Steiner ETFs for any @xmath.

For any @xmath , one can show that there exists a @xmath -Steiner system
with @xmath and @xmath [ 52 ] . By manipulating @xmath and @xmath , one
can independently determine the order of magnitude of redundancy and
size: the corresponding ETFs consist of @xmath vectors in @xmath
-dimensional space, having redundancy @xmath and density @xmath . As
such, this family has some qualitative similarities to the familes of
ETFs produced by affine and projective geometries. However, unlike those
families, the ETFs produced by Denniston designs can never be real: the
requisite Hadamard matrices are of size @xmath , which is never
divisible by @xmath .

#### 1.3.2 Conditions for the existence of Steiner equiangular tight
frames

@xmath -Steiner systems have been actively studied for over a century,
with many celebrated results. Nevertheless, much about these systems is
still unknown. In this subsection, we discuss some known partial
characterizations of the Steiner systems which lie outside of the eight
families we have already discussed, as well as what these results tell
us about the existence of certain ETFs. To begin, recall that, for a
given @xmath and @xmath , if a @xmath -Steiner system exists, then the
number @xmath of blocks that contain a given point is necessarily @xmath
, while the total number of blocks @xmath is @xmath . As such, in order
for a @xmath -Steiner system to exist, it is necessary for @xmath to be
admissible , that is, to have the property that @xmath and @xmath are
integers.

However, this property is not sufficient for existence: it is known that
a @xmath -Steiner system does not exist [ 1 ] despite the fact that
@xmath and @xmath . In fact, letting @xmath be either @xmath , @xmath ,
@xmath , or @xmath results in an admissible pair with @xmath , despite
the fact that none of the corresponding Steiner systems exist; there are
twenty-nine additional values of @xmath which form an admissible pair
with @xmath and for which the existence of a corresponding Steiner
system remains an open problem [ 1 ] . Similar nastiness arises with
@xmath . The good news is that admissibility, though not sufficient for
existence, is, in fact, asymptotically sufficient: for any fixed @xmath
, there exists a corresponding admissible index @xmath for which for all
@xmath such that @xmath and @xmath are integers, a @xmath -Steiner
system indeed exists [ 1 ] . Moreover, explicit values of @xmath are
known for small @xmath : @xmath , @xmath , @xmath , @xmath . We now
detail the ramifications of these design-theoretic results on frame
theory:

###### Theorem 8.

If an @xmath Steiner equiangular tight frame exists, then letting @xmath
, the corresponding block design has parameters:

  -- -------- --
     @xmath   
  -- -------- --

In particular, if such a frame exists, then these expressions for @xmath
, @xmath and @xmath are necessarily integers.

Conversely, for any fixed @xmath , there exists an index @xmath for
which for all @xmath such that @xmath and @xmath are integers, there
exists a Steiner equiangular tight frame of @xmath vectors for a space
of dimension @xmath .

In particular, for any fixed @xmath , letting @xmath be either @xmath or
@xmath for increasingly large values of @xmath results in a sequence of
Steiner equiangular tight frames whose redundancy is asymptotically
@xmath ; these frames can be real if there exist real Hadamard matrices
of sizes @xmath or @xmath , respectively.

###### Proof.

To prove the necessary conditions on @xmath and @xmath , recall that
Steiner ETFs, namely those ETFs produced by Theorem 7 , have @xmath and
@xmath . Together, these two equations imply @xmath . Solving for @xmath
and substituting the resulting expression into @xmath yields the
quadratic equation @xmath . With some algebra, the only positive root of
this equation can be found to be @xmath , as claimed. Substituting this
expression for @xmath into @xmath yields @xmath . Having @xmath and
@xmath , the previously mentioned relations @xmath and @xmath imply
@xmath and @xmath , as claimed.

The second set of conclusions is the result of applying Theorem 7 to the
aforementioned @xmath -Steiner ETFs that are guaranteed to exist for all
sufficiently large @xmath , provided @xmath and @xmath are integers. The
final set of conclusions are then obtained by applying this fact in the
special cases where @xmath is either @xmath or @xmath . In particular,
if @xmath then @xmath and @xmath are integers, and the resulting ETF of
@xmath vectors has a redundancy of @xmath that tends to @xmath for large
@xmath ; such an ETF can be real if there exists a real Hadamard matrix
of size @xmath . Meanwhile, if @xmath then @xmath and @xmath are
integers, and the resulting ETF of @xmath vectors has a redundancy of
@xmath that tends to @xmath for large @xmath ; such an ETF can be real
if there exists a real Hadamard matrix of size @xmath . ∎

We conclude this section with a few thoughts on Theorems 7 and 8 .
First, we emphasize that the method of Theorem 7 is a method for
constructing some ETFs, and by no means constructs them all. Indeed, as
noted above, the redundancy of Steiner ETFs is always strictly greater
than @xmath ; while some of those ETFs with @xmath will be the Naimark
complements of Steiner ETFs, one must admit that the Steiner method
contributes little towards the understanding of those ETFs with @xmath ,
such as those arising from Paley graphs [ 141 ] . Moreover, Theorem 8
implies that not even every ETF with @xmath arises from a Steiner
system: though there exists an ETF of @xmath -elements in @xmath [ 141 ]
, the corresponding parameters of the design would be @xmath , @xmath
and @xmath , not all of which are integers.

That said, the method of Theorem 7 is truly significant: comparing Table
1.2 with a comprehensive list of all real ETFs of dimension @xmath or
less [ 141 ] , we see the Steiner method produces @xmath of the @xmath
ETFs that have redundancy greater than @xmath , namely @xmath , @xmath ,
@xmath and @xmath ETFs. Interestingly, an additional @xmath of these
@xmath ETFs can also be produced by the Steiner method, but only in
complex form, namely those of @xmath , @xmath , @xmath and @xmath
dimensions; it is unknown whether this is the result of a deficit in our
analysis or the true non-existence of real-valued Steiner-based
constructions of these sizes. The plot further thickens when one
realizes that an additional @xmath of these @xmath real ETFs satisfy the
necessary conditions of Theorem 8 , but that the corresponding @xmath
-Steiner systems are known to not exist: if a @xmath ETF was to arise as
a result of Theorem 7 , the corresponding Steiner system would have
@xmath and @xmath , while the @xmath ETF would have @xmath and @xmath ;
in fact, @xmath - and @xmath -Steiner systems cannot exist [ 1 ] . With
our limited knowledge of the rich literature on Steiner systems, we were
unable to resolve the existence of two remaining candidates: @xmath and
@xmath ETFs could potentially arise from @xmath - and @xmath -Steiner
systems, respectively, provided they exist.

### 1.4 Restricted isometry and digital fingerprinting

In the previous section, we used Theorem 7 to construct many examples of
Steiner ETFs. In this section, we investigate the feasibility of using
such frames for applications in sparse signal processing. Regarding
restricted isometry, one of the sad consequences of the Steiner
construction method in Theorem 7 is that we now know there is a large
class of ETFs for which the seemingly coarse estimate from the
Gershgorin analysis ( 1.4 ) is, in fact, accurate. In particular, recall
that Gershgorin guarantees that every @xmath ETF is @xmath -RIP whenever
@xmath . Furthermore, recall from Theorem 7 that every Steiner ETF is
built by carefully overlapping @xmath regular simplices, each consisting
of @xmath vectors in an @xmath -dimensional subspace of @xmath
-dimensional space. Thus, the corresponding subcollection of @xmath
vectors that lie in a given block are linearly dependent. Considering
the value of @xmath given in Theorem 8 , we see that Steiner ETFs @xmath
have

  -- -------- --
     @xmath   
  -- -------- --

where the last inequality uses the fact that Steiner ETFs have
redundancy @xmath . Therefore, Steiner ETFs are not @xmath -RIP for any
@xmath , that is, they fail to break the square-root bottleneck. This
begs the open question: Are there any ETFs which are as RIP as random
matrices, or does being optimal in the Gershgorin sense necessarily come
at the cost of being able to support large sparsity levels? In
Chapter 3, we address this problem directly and make some interesting
connections with graph theory and number theory, but we do not give a
conclusive answer.

Despite their provably suboptimal performance as RIP matrices, we will
see that Steiner ETFs are particularly well-suited for the application
of digital fingerprints. Digital media protection has become an
important issue in recent years, as illegal distribution of licensed
material has become increasingly prevalent. A number of methods have
been proposed to restrict illegal distribution of media and ensure only
licensed users are able to access it. One method involves cryptographic
techniques, which encrypt the media before distribution. By doing this,
only the users with appropriate licensed hardware or software have
access; satellite TV and DVDs are two such examples. Unfortunately,
cryptographic approaches are limited in that once the content is
decrypted (legally or illegally), it can potentially be copied and
distributed freely.

An alternate approach involves marking each copy of the media with a
unique signature. The signature could be a change in the bit sequence of
the digital file or some noise-like distortion of the media. The unique
signatures are called fingerprints , by analogy to the uniqueness of
human fingerprints. With this approach, a licensed user could illegally
distribute the file, only to be implicated by his fingerprint. The
potential for prosecution acts as a deterrent to unauthorized
distribution. However, fingerprinting systems are vulnerable when
multiple users form a collusion by combining their copies to create a
forged copy. This attack can reduce and distort the colluders’
individual fingerprints, making identification of any particular user
difficult. Some examples of potential attacks involve comparing the bit
sequences of different copies, averaging copies in the signal space, as
well as introducing noise, rotations, or cropping.

One of the principal approaches to designing fingerprints with
robustness to collusions uses what is called the distortion assumption .
In this regime, fingerprints are noise-like distortions to the media in
signal space. In order to preserve the overall quality of the media,
limits are placed on the magnitude of this distortion. The content owner
limits the power of the fingerprint he adds, and the collusion limits
the power of the noise they add in their attack. When applying the
distortion assumption, the literature typically assumes that the
collusion linearly averages their individual copies to forge the host
signal. Also, while results using the distortion assumption tend to
accommodate fewer users than those with other assumptions, this
assumption is distinguished by its natural embedding of fingerprints,
namely in the signal space.

Cox et al. introduced one of the first robust fingerprint designs under
the distortion assumption [ 54 ] ; the robustness was later analytically
proven in [ 92 ] . Different fingerprint designs have since been
studied, including orthogonal fingerprints [ 142 ] and simplex
fingerprints [ 94 ] . We propose ETFs as a fingerprint design under the
distortion assumption, and we analyze their performance against the
worst-case collusion [ 103 , 104 ] . Using analysis from Ergun et al. [
66 ] , we will show that ETFs perform particularly well as fingerprints;
as a matter of fact, Steiner ETF fingerprints perform comparably to
orthogonal and simplex fingerprints on average, while accommodating
several times as many users [ 104 ] . We start by formally presenting
the fingerprinting and collusion processes.

#### 1.4.1 Problem setup

A content owner has a host signal that he wishes to share, but he wants
to mark it with fingerprints before distributing it. We view this host
signal as a vector @xmath , and the marked versions of this vector will
be given to @xmath users. Specifically, the @xmath th user is given

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denotes the @xmath th fingerprint; we assume the
fingerprints have equal norm. We wish to design the fingerprints @xmath
to be robust to a linear averaging attack. In particular, let @xmath
denote a collection of users who together make a different copy of the
host signal. Then their linear averaging attack produces a forgery:

  -- -------- -- -------
     @xmath      (1.7)
  -- -------- -- -------

where @xmath is a noise vector introduced by the colluders. This attack
model is illustrated in Figure 1.1 .

Certainly, the ultimate goal of the content owner is to detect every
member of the forgery coalition. This can prove difficult in practice,
though, particularly when some individuals contribute little to the
forgery, with @xmath . However, in the real world, if at least one
colluder is caught, then other members could be identified through the
legal process. As such, we consider focused detection, where a test
statistic is computed for each user, and we perform a binary hypothesis
test to decide whether that particular user is guilty.

Our detection procedure is as follows: With the cooperation of the
content owner, the host signal can be subtracted from a forgery to
isolate the fingerprint combination:

  -- -------- -- -------
     @xmath      (1.8)
  -- -------- -- -------

To help the content owner discern who is guilty, we then use a
normalized correlation function as a test statistic for each user @xmath
:

  -- -------- --
     @xmath   
  -- -------- --

Having devised a test statistic, let @xmath denote the guilty hypothesis
( @xmath ) and @xmath denote the innocent hypothesis ( @xmath ). Then
picking some correlation threshold @xmath , we use the following
detector:

  -- -------- -- -------
     @xmath      (1.9)
  -- -------- -- -------

To determine the effectiveness of our fingerprint design and focused
detector, we will investigate the corresponding error probabilities, but
first, we build our intuition for fingerprint design using a certain
geometric figure of merit.

#### 1.4.2 A geometric figure of merit for fingerprint design

For each user @xmath , consider the distance between forgeries deriving
from two types of potential collusions: those of which @xmath is a
member, and those of which @xmath is not. Intuitively, if every
fingerprint combination involving @xmath is distant from every
combination not involving @xmath , then even with moderate noise, there
should be little ambiguity as to whether the @xmath th user was
involved. To make this precise, for each user @xmath , we define the
“guilty” and “not guilty” sets of noiseless fingerprint combinations:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

In words, @xmath is the set of size- @xmath fingerprint combinations of
equal weights which include @xmath , while @xmath is the set of
combinations which do not include @xmath . Note that in our setup ( 1.7
), the weights @xmath were arbitrary values which sum to @xmath . We
will show in Theorem 11 that the best attack from the collusion’s
perspective uses equal weights so that no single colluder is
particularly vulnerable. From this perspective, it makes sense to bound
the distance between these two sets:

  -- -------- -- --------
     @xmath      (1.10)
  -- -------- -- --------

Note that by taking @xmath to be the @xmath matrix whose columns are the
fingerprints @xmath , the fingerprint combination ( 1.8 ) can be
rewritten as @xmath , where the entries of @xmath are @xmath when @xmath
and zero otherwise. Thus, if the matrix of fingerprints @xmath is @xmath
-RIP with @xmath , then we can recover the @xmath -sparse vector @xmath
using Theorem 2 . However, the error in the estimate @xmath of @xmath
will be on the order of @xmath times the size of the noise @xmath [ 34 ]
. Due to the potential legal ramifications of false accusations, this
order of error is not tolerable. Note that the methods of compressed
sensing recover the entire vector @xmath , the support of which
identifies the entire collusion. By contrast, we will investigate RIP
matrices for fingerprint design, but to minimize false accusations, we
will use focused detection ( 1.9 ) to identify colluders.

We now investigate how well RIP matrices perform with respect to our
geometric figure of merit. Without loss of generality, we assume the
fingerprints are unit norm; since they have equal norm, the fingerprint
combination can be scaled by @xmath before the detection phase. With
this in mind, we have the following a lower bound on the distance ( 1.10
) between the “guilty” and “not guilty” sets corresponding to any user
@xmath :

###### Theorem 9.

Suppose fingerprints @xmath have restricted isometry constant @xmath .
Then

  -- -------- -- --------
     @xmath      (1.11)
  -- -------- -- --------

###### Proof.

Take @xmath such that @xmath and @xmath . Then the left-hand inequality
of the restricted isometry property gives

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (1.12)
  -- -------- -------- -- --------

For a fixed @xmath , we will find a lower bound for

  -- -- -- --------
           (1.13)
  -- -- -- --------

Since we can have @xmath , we know @xmath when ( 1.13 ) is minimized.
That said, @xmath must be as small as possible, i.e., @xmath . Thus,
when ( 1.13 ) is minimized, we have

  -- -- --
        
  -- -- --

i.e., @xmath must be as large as possible. Since @xmath , we have @xmath
. Therefore,

  -- -- -- --------
           (1.14)
  -- -- -- --------

Substituting ( 1.14 ) into ( 1.12 ) gives

  -- -------- --
     @xmath   
  -- -------- --

Since this bound holds for every @xmath , @xmath and @xmath with @xmath
, we have ( 1.11 ). ∎

Combining Theorem 9 with the Gershgorin estimate @xmath in terms of
worst-case coherence @xmath yields the following:

###### Corollary 10.

Suppose fingerprints @xmath are unit-norm with worst-case coherence
@xmath . Then

  -- -------- -- --------
     @xmath      (1.15)
  -- -------- -- --------

In words, Corrolary 10 says that less coherent fingerprints provide a
greater distance between the “guilty” and “not guilty” sets. It is
therefore fitting to consider minimizers of worst-case coherence, namely
equiangular tight frames. One type of ETF has already been proposed for
fingerprint design: the simplex [ 94 ] . The simplex is an ETF with
@xmath and @xmath . In fact, [ 94 ] gives a derivation for the exact
value of the distance ( 1.10 ) in this case:

  -- -------- -- --------
     @xmath      (1.16)
  -- -------- -- --------

The bound ( 1.15 ) is lower than ( 1.16 ) by a factor of @xmath , and
for practical cases in which @xmath , the two are particularly close.
Overall, ETF fingerprint design is a natural generalization of the
provably optimal simplex design of [ 94 ] .

Having applied the Gershgorin analysis to illustrate how ETF
fingerprints perform with respect to our geometric figure of merit, we
have yet to establish any fingerprint-specific consequences of Steiner
ETFs not being as RIP as random matrices. Certainly, whether @xmath
scales as @xmath or @xmath is an important distinction in the compressed
sensing community, but interestingly, in the context of fingerprints,
this difference offers no advantage. To be clear, Ergun et al. [ 66 ]
showed that for any fingerprinting system, there is a tradeoff between
the probabilities of successful detection and false positives imposed by
a linear-average-plus-noise attack from sufficiently large collusions.
Specifically, a collusion of size @xmath is sufficient to overcome the
fingerprints, as the detector will not be able to identify any attacker
without incurring a false-alarm probability that is too large to be
admissible in court. This constraint is more restrictive than the
coherence-based reconstruction guarantees which require @xmath , and so
from this perspective, random RIP constructions are no better for
fingerprint design than deterministic constructions.

#### 1.4.3 Error analysis

We now investigate the errors associated with using ETF fingerprints and
a focused correlation detector with linear-average-plus-noise attacks.
To do this, we assume that the noise @xmath included in the attack ( 1.7
) has independent Gaussian entries of mean zero and variance @xmath .
One type of error we can expect is the false-positive error, in which an
innocent user @xmath is found guilty ( @xmath ). This could have
significant ramifications in legal proceedings, so this error
probability @xmath should be kept extremely low. To ensure this type of
error is improbable, we consider the worst-case type I error probability
, which depends on the fingerprint design @xmath , the correlation
threshold @xmath , and the weights @xmath used by the colluders in their
linear average:

  -- -------- -- --------
     @xmath      (1.17)
  -- -------- -- --------

In words, the probability that an innocent user @xmath is found guilty
is no larger than @xmath , regardless of the coalition @xmath or how the
coalition members assign weights from @xmath . The other error type is
the false-negative error, in which a guilty user @xmath is found
innocent ( @xmath ). In this case, since the goal of our detection is to
catch at least one of the colluders, we define the worst-case type II
error probability as follows:

  -- -------- -- --------
     @xmath      (1.18)
  -- -------- -- --------

This way, regardless of who the colluders are or how they assign the
weights, at least one of the colluders will have a false-negative
probability less than @xmath , meaning even in the worst-case scenario,
we can correctly identify one of the colluders with probability @xmath .

###### Theorem 11.

Take fingerprints as the columns of an @xmath matrix @xmath , which,
when normalized by the fingerprints’ common norm @xmath , forms an
equiangular tight frame. If the noise @xmath included in the attack (
1.7 ) has independent Gaussian entries of mean zero and variance @xmath
, then the worst-case type I and type II error probabilities, ( 1.17 )
and ( 1.18 ), satisfy

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where @xmath and @xmath .

###### Proof.

To bound @xmath , assume a given user @xmath is innocent, i.e., @xmath .
Then the test statistic for our detector ( 1.9 ) is given by

  -- -- --
        
  -- -- --

By the symmetry of @xmath ’s Gaussian distribution, we know the
projection @xmath also has Gaussian distribution with mean zero and
variance @xmath , meaning our test statistic @xmath has Gaussian
distribution with mean @xmath and variance @xmath . Furthermore, since
the normalized fingerprints form an ETF with worst-case coherence @xmath
, we can use the triangle inequality to bound the mean of @xmath :

  -- -------- --
     @xmath   
  -- -------- --

We use this to bound the false-positive probability for user @xmath :

  -- -------- --
     @xmath   
  -- -------- --

Since this bound holds for all coalitions, weight assignments and
innocent users, this bound must also hold for @xmath .

Next, to bound @xmath , assume a given user @xmath is guilty, i.e.,
@xmath . In this case, the test statistic for our detector ( 1.9 ) is
given by

  -- -- --
        
  -- -- --

As before, @xmath has Gaussian distribution with variance @xmath , but
this time, the mean is

  -- -------- --
     @xmath   
  -- -------- --

As such, the false-negative probability for user @xmath is

  -- -------- --
     @xmath   
  -- -------- --

Applying the definition of @xmath therefore gives

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

From Theorem 11 , we can glean a few interesting insights about ETF
fingerprints. First, the upper bound on @xmath is independent of @xmath
, indicating that the coalition cannot pick weights in a way that frames
an innocent user. Additionally, the upper bound on @xmath is maximized
when the weights @xmath are equal, corresponding to our use of equal
weights in the geometric figure of merit. This confirms our intuition
that the coalition has the best chance of not being caught if no member
is particularly vulnerable.

## Chapter 2 Full spark frames

In the previous chapter, we reviewed how to use the Gershgorin circle
theorem to demonstrate the restricted isometry property (RIP), and how
identifying small spark disproves RIP. We then showed that Steiner
equiangular tight frames (ETFs) are optimal in the Gershgorin sense, but
have particularly small spark. Among other things, this illustrates that
the “square-root bottleneck” with deterministic RIP matrices is not
merely an artifact of the Gershgorin analysis. That said, as an
intermediate goal to constructing RIP matrices, we seek deterministic
matrices with large spark, understanding that RIP matrices necessarily
have this property. To this end, one is naturally led to consider full
spark matrices, that is, @xmath matrices @xmath with the largest spark
possible: @xmath . Equivalently, @xmath full spark matrices have the
property that every @xmath submatrix is invertible; as such, a full
spark matrix is necessarily full rank, and therefore a frame.

Interestingly, in sparse signal processing, the specific application of
full spark frames has already been studied for some time. In 1997,
Gorodnitsky and Rao [ 74 ] first considered full spark frames, referring
to them as matrices with the unique representation property . Since [ 74
] , the unique representation property has been explicitly used to find
a variety of performance guarantees for sparse signal processing [ 30 ,
105 , 144 ] . Tang and Nehorai [ 133 ] also obtain performance
guarantees using full spark frames, but they refer to them as
non-degenerate measurement matrices .

For another application of full spark frames, we consider the problem of
reconstructing a signal from distorted frame coefficients. Specifically,
we observe a scenario in which frame coefficients @xmath are transmitted
over a noisy or lossy channel before reconstructing the signal:

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

where @xmath represents the channel’s random and not-necessarily-linear
deformation process. Using an additive white Gaussian noise model, Goyal
[ 75 ] established that, of all unit norm frames, unit norm tight frames
minimize mean squared error in reconstruction. For the case of a lossy
channel, Holmes and Paulsen [ 84 ] established that, of all tight
frames, unit norm tight frames minimize worst-case error in
reconstruction after one erasure, and that equiangular tight frames
minimize this error after two erasures. We note that the reconstruction
process in ( 2.1 ), namely the application of @xmath , is inherently
blind to the effect of the deformation process of the channel. This
contrasts with Püschel and Kovačević’s more recent work [ 113 ] , which
describes an adaptive process for reconstruction after multitudes of
erasures. In this context, they reconstruct the signal after first
identifying which frame coefficients were not erased; with this
information, the signal can be estimated provided the corresponding
frame elements span. In this sense, full spark frames are maximally
robust to erasures , as coined in [ 113 ] . In particular, an @xmath
full spark frame is robust to @xmath erasures since any @xmath of the
frame coefficients will uniquely determine the original signal.

Yet another application of full spark frames is phaseless
reconstruction, which can be viewed in terms of a channel, as in ( 2.1
); in this case, @xmath is the entrywise absolute value function.
Phaseless reconstruction has a number of real-world applications
including speech processing [ 15 ] , X-ray crystallography [ 37 ] , and
quantum state estimation [ 116 ] . As such, there has been a lot of work
to reconstruct an @xmath -dimensional vector (up to an overall phase
factor) from the magnitudes of its frame coefficients, most of which
involves frames in operator space, which inherently require @xmath
measurements [ 14 , 116 ] . However, Balan et al. [ 15 ] show that if an
@xmath real frame @xmath is full spark with @xmath , then @xmath is
injective, meaning an inversion process is possible with only @xmath
measurements. This result prompted an ongoing search for efficient
phaseless reconstruction processes [ 13 , 37 ] , but no reconstruction
process can succeed without a good family of frames, such as full spark
frames.

Despite the fact that full spark frames have a multitude of
applications, to date, there has not been much progress in constructing
deterministic full spark frames, let alone full spark frames with
additional desirable properties. A noteworthy exception is Püschel and
Kovačević’s work [ 113 ] , in which real full spark tight frames are
constructed using polynomial transforms. In the present chapter, we
start by investigating Vandermonde frames, harmonic frames, and
modifications thereof [ 2 ] . While the use of certain Vandermonde and
harmonic frames as full spark frames is not new [ 30 , 36 , 72 ] , the
fruits of our investigation are new: For instance, we demonstrate that
certain classes of ETFs are full spark, and we characterize the @xmath
full spark harmonic frames for which @xmath is a prime power. Later, we
prove that verifying whether a matrix is full spark is hard for @xmath
under randomized polynomial-time reductions [ 2 ] . In other words,
assuming @xmath (a computational complexity assumption slightly stronger
than @xmath and nearly as widely believed), then there is no method by
which one can efficiently test whether matrices are full spark. As such,
the deterministic constructions we provide are significant in that they
guarantee a property which is otherwise difficult to check. We conclude
the chapter by introducing a new technique for efficient phaseless
recovery, which explicitly makes use of deterministic full spark frames
to design @xmath measurements.

### 2.1 Deterministic constructions of full spark frames

A square matrix is invertible if and only if its determinant is nonzero,
and in our quest for deterministic constructions of full spark frames,
this characterization will reign supreme. One class of matrices has a
particularly simple determinant formula: Vandermonde matrices.
Specifically, Vandermonde matrices have the following form:

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

and square Vandermonde matrices, i.e., with @xmath , have the following
determinant:

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

Consider ( 2.2 ) in the case where @xmath . Since every @xmath submatrix
of @xmath is also Vandermonde, we can modify the indices in ( 2.3 ) to
calculate the determinant of the submatrices. These determinants are
nonzero precisely when the bases @xmath are distinct, yielding the
following result:

###### Lemma 12.

A Vandermonde matrix is full spark if and only if its bases are
distinct.

To be clear, this result is not new. In fact, the full spark of
Vandermonde matrices was first exploited by Fuchs [ 72 ] for sparse
signal processing. Later, Bourguignon et al. [ 30 ] specifically used
the full spark of Vandermonde matrices whose bases are sampled from the
complex unit circle. Interestingly, when viewed in terms of frame
theory, Vandermonde matrices naturally point to the discrete Fourier
transform:

###### Theorem 13.

The only @xmath Vandermonde matrices that are equal norm and tight have
bases in the complex unit circle. Among these, the frames with the
smallest worst-case coherence have bases that are equally spaced in the
complex unit circle, provided @xmath .

###### Proof.

Suppose a Vandermonde matrix is equal norm and tight. Note that a zero
base will produce the zeroth identity basis element @xmath . Letting
@xmath denote the indices of the nonzero bases, the fact that the matrix
is full rank implies @xmath . Also, equal norm gives that the frame
element length

  -- -------- --
     @xmath   
  -- -------- --

is constant over @xmath . Since @xmath is strictly increasing over
@xmath , there exists @xmath such that @xmath for all @xmath . Next,
tightness gives that the rows have equal norm, implying that the first
two rows have equal norm, i.e., @xmath . Thus @xmath , and so the
nonzero bases are in the complex unit circle. Furthermore, since the
zeroth and first rows have equal norm by tightness, we have @xmath , and
so every base is in the complex unit circle.

Now consider the inner product between Vandermonde frame elements whose
bases @xmath come from the complex unit circle:

  -- -------- --
     @xmath   
  -- -------- --

We will show that the worst-case coherence comes from the two closest
bases. Consider the following function:

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

Figure 2.1 gives a plot of this function in the case where @xmath . We
will prove two things about this function:

-   @xmath for every @xmath ,

-   @xmath for every @xmath .

First, we claim that (i) and (ii) are sufficient to prove our result. To
establish this, we first show that the two closest bases @xmath and
@xmath satisfy @xmath . Without loss of generality, the @xmath ’s are
ordered in such a way that @xmath are nondecreasing. Define

  -- -------- --
     @xmath   
  -- -------- --

and let @xmath be the @xmath which minimizes @xmath . Since the minimum
is less than the average, we have

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

provided @xmath . Note that if we view @xmath as members of @xmath ,
then @xmath . Since @xmath is even, then (i) implies that @xmath is
larger than any other @xmath in which @xmath . Next, ( 2.5 ) and (ii)
together imply that @xmath is larger than any other @xmath in which
@xmath , provided @xmath . Combined, (i) and (ii) give that @xmath
achieves the worst-case coherence of @xmath . Additionally, (i) gives
that the worst-case coherence @xmath is minimized when @xmath is
maximized, i.e., when the @xmath ’s are equally spaced in the unit
interval.

To prove (i), note that the geometric sum formula gives

  -- -- -- -------
           (2.6)
  -- -- -- -------

where the final expression uses the identity @xmath . To show that
@xmath is decreasing over @xmath , note that the base of ( 2.6 ) is
positive on this interval, and performing the quotient rule to calculate
its derivative will produce a fraction whose denominator is nonnegative
and whose numerator is given by

  -- -------- -- -------
     @xmath      (2.7)
  -- -------- -- -------

This factor is zero at @xmath and has derivative:

  -- -------- --
     @xmath   
  -- -------- --

which is strictly negative for all @xmath . Hence, ( 2.7 ) is strictly
negative whenever @xmath , and so @xmath for every @xmath .

For (ii), note that for every @xmath , we can individually bound the
numerator and denominator of what the geometric sum formula gives:

  -- -------- --
     @xmath   
  -- -------- --

Consider the @xmath discrete Fourier transform (DFT) matrix, scaled to
have entries of unit modulus:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath . The first @xmath rows of the DFT form a Vandermonde
matrix of distinct bases @xmath ; as such, this matrix is full spark by
Lemma 12 . In fact, the previous result says that this is in some sense
an optimal Vandermonde frame, but this might not be the best way to pick
rows from a DFT. Indeed, several choices of DFT rows could produce full
spark frames, some with smaller coherence or other desirable properties,
and so the remainder of this section focuses on full spark DFT
submatrices. First, we note that not every DFT submatrix is full spark.
For example, consider the @xmath DFT:

  -- -------- --
     @xmath   
  -- -------- --

Certainly, the zeroth and second rows of this matrix are not full spark,
since the zeroth and second columns of this submatrix form the all-ones
matrix, which is not invertible. So what can be said about the set of
permissible row choices? The following result gives some necessary
conditions on this set:

###### Theorem 14.

Take an @xmath discrete Fourier transform matrix, and select the rows
indexed by @xmath to build the matrix @xmath . If @xmath is full spark,
then so is the matrix built from rows indexed by

-    any translation of @xmath ,

-    any @xmath with @xmath relatively prime to @xmath ,

-    the complement of @xmath in @xmath .

###### Proof.

For (i), we first define @xmath to be the @xmath diagonal matrix whose
diagonal entries are @xmath . Note that, since @xmath , translating the
row indices @xmath by @xmath corresponds to multiplying @xmath on the
right by @xmath . For some set @xmath of size @xmath , let @xmath denote
the @xmath submatrix of @xmath whose columns are indexed by @xmath , and
let @xmath denote the @xmath diagonal submatrix of @xmath whose diagonal
entries are indexed by @xmath . Then since @xmath is unitary, we have

  -- -------- --
     @xmath   
  -- -------- --

Thus, if @xmath is full spark, @xmath , and so @xmath is also full
spark. Using this fact inductively proves (i) for all translations of
@xmath .

For (ii), let @xmath denote the submatrix of rows indexed by @xmath .
Then for any @xmath of size @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath is relatively prime to @xmath , multiplication by @xmath
permutes the elements of @xmath , and so @xmath has exactly @xmath
distinct elements. Thus, if @xmath is full spark, then @xmath , and so
@xmath is also full spark.

For (iii), we let @xmath be the @xmath submatrix of rows indexed by
@xmath , so that

  -- -------- -- -------
     @xmath      (2.8)
  -- -------- -- -------

We will use contraposition to show that @xmath being full spark implies
that @xmath is also full spark. To this end, suppose @xmath is not full
spark. Then @xmath has a collection of @xmath linearly dependent columns
@xmath , and so there exists a nontrivial sequence @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

Considering @xmath , where @xmath is the @xmath th identity basis
element, we can use ( 2.8 ) to express this linear dependence in terms
of @xmath :

  -- -------- --
     @xmath   
  -- -------- --

Rearranging then gives

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

Here, we note that @xmath is nonzero since @xmath is nontrivial, and
that @xmath . Furthermore, whenever @xmath , we have from ( 2.9 ) that

  -- -------- --
     @xmath   
  -- -------- --

and so @xmath . Thus, the containment @xmath is proper, and so

  -- -------- --
     @xmath   
  -- -------- --

Since the @xmath submatrix @xmath is rank-deficient, it is not
invertible, and therefore @xmath is not full spark. ∎

We note that our proof of (iii) above uses techniques from Cahill et al.
[ 32 ] , and can be easily generalized to prove that the Naimark
complement of a full spark tight frame is also full spark. Theorem 14
tells us quite a bit about the set of permissible choices for DFT rows.
For example, not only can we pick the first @xmath rows of the DFT to
produce a full spark Vandermonde frame, but we can also pick any
consecutive @xmath rows, by Theorem 14 (i). We would like to completely
characterize the choices that produce full spark harmonic frames. The
following classical result does this in the case where @xmath is prime:

###### Theorem 15 (Chebotarëv, see [126]).

Let @xmath be prime. Then every square submatrix of the @xmath discrete
Fourier transform matrix is invertible.

As an immediate consequence of Chebotarëv’s theorem, every choice of
rows from the DFT produces a full spark harmonic frame, provided @xmath
is prime. This application of Chebotarëv’s theorem was first used by
Candès et al. [ 36 ] for sparse signal processing. Note that each of
these frames are equal-norm and tight by construction. Harmonic frames
can also be designed to have minimal coherence; Xia et al. [ 146 ]
produces harmonic equiangular tight frames by selecting row indices
which form a difference set in @xmath . Interestingly, most known
families of difference sets in @xmath require @xmath to be prime [ 87 ]
, and so the corresponding harmonic equiangular tight frames are
guaranteed to be full spark by Chebotarëv’s theorem. In the following,
we use Chebotarëv’s theorem to demonstrate full spark for a class of
frames which contains harmonic frames, namely, frames which arise from
concatenating harmonic frames with any number of identity basis
elements:

###### Theorem 16 (cf. [131, Theorem 1.1]).

Let @xmath be prime, and pick any @xmath rows of the @xmath discrete
Fourier transform matrix to form the harmonic frame @xmath . Next, pick
any @xmath , and take @xmath to be the @xmath diagonal matrix whose
first @xmath diagonal entries are @xmath , and whose remaining @xmath
entries are @xmath . Then concatenating @xmath with the first @xmath
identity basis elements produces an @xmath full spark unit norm tight
frame.

As an example, when @xmath and @xmath , we can pick @xmath rows of the
@xmath DFT which are indexed by @xmath . In this case, @xmath makes the
entries of the first DFT row have size @xmath and the entries of the
remaining rows have size @xmath . Concatenating with the first identity
basis element then produces an equiangular tight frame which is full
spark:

  -- -------- -- --------
     @xmath      (2.10)
  -- -------- -- --------

###### Proof of Theorem 16.

Let @xmath denote the resulting @xmath frame. We start by verifying that
@xmath is unit norm. Certainly, the identity basis elements have unit
norm. For the remaining frame elements, the modulus of each entry is
determined by @xmath , and so the norm squared of each frame element is

  -- -------- --
     @xmath   
  -- -------- --

To demonstrate that @xmath is tight, it suffices to show that @xmath .
The rows of @xmath are orthogonal since they are scaled rows of the DFT,
while the rows of the identity portion are orthogonal because they have
disjoint support. Thus, @xmath is diagonal. Moreover, the norm squared
of each of the first @xmath rows is @xmath , while the norm squared of
each of the remaining rows is @xmath , and so @xmath .

To show that @xmath is full spark, note that every @xmath submatrix of
@xmath is invertible since

  -- -------- --
     @xmath   
  -- -------- --

by Chebotarëv’s theorem. Also, in the case where @xmath , we note that
the @xmath submatrix of @xmath composed solely of identity basis
elements is trivially invertible. The only remaining case to check is
when identity basis elements and columns of @xmath appear in the same
@xmath submatrix @xmath . In this case, we may shuffle the rows of
@xmath to have the form

  -- -------- --
     @xmath   
  -- -------- --

Since shuffling rows has no impact on the size of the determinant, we
may further use a determinant identity on block matrices to get

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath is a multiple of a square submatrix of the @xmath DFT, we
are done by Chebotarëv’s theorem. ∎

As an example of Theorem 16 , pick @xmath to be a prime congruent to
@xmath , and select @xmath rows of the @xmath DFT according to the index
set @xmath . If we take @xmath , the process in Theorem 16 produces an
equiangular tight frame of redundancy @xmath , which we will verify in
the next chapter using quadratic Gauss sums; in the case where @xmath ,
this construction produces ( 2.10 ). Note that this corresponds to a
special case of a construction in Zauner’s thesis [ 150 ] , which was
later studied by Renes [ 115 ] and Strohmer [ 128 ] . Theorem 16 says
that this construction is full spark.

Maximally sparse frames have recently become a subject of active
research [ 44 , 70 ] . We note that when @xmath , Theorem 16 produces a
maximally sparse @xmath full spark frame, having a total of @xmath zero
entries. To see that this sparsity level is maximal, we note that if the
frame had any more zero entries, then at least one of the rows would
have @xmath zero entries, meaning the corresponding @xmath submatrix
would have a row of all zeros and hence a zero determinant. Similar
ideas were studied previously by Nakamura and Masson [ 107 ] .

Another interesting case is where @xmath , i.e., when the frame
constructed in Theorem 16 is a union of the unitary DFT and identity
bases. Unions of orthonormal bases have received considerable attention
in the context of sparse approximation [ 61 , 136 ] . In fact, when
@xmath is a perfect square, concatenating the DFT with an identity basis
forms the canonical example @xmath of a dictionary with small spark [ 61
] , and we used this example in the previous chapter. Recall the Dirac
comb of @xmath spikes is an eigenvector of the DFT, and so concatenating
this comb with the negative of its Fourier transform produces a @xmath
-sparse vector in the nullspace of @xmath . In stark contrast, when
@xmath is prime, Theorem 16 shows that @xmath is full spark.

The vast implications of Chebotarëv’s theorem leads one to wonder
whether the result admits any interesting generalization. In this
direction, Candès et al. [ 36 ] note that any such generalization must
somehow account for the nontrivial subgroups of @xmath which are not
present when @xmath is prime. Certainly, if one could characterize the
full spark submatrices of a general DFT, this would provide ample
freedom to optimize full spark frames for additional considerations.
While we do not have a characterization for the general case, we do have
one for the case where @xmath is a prime power. Before stating the
result, we require a definition:

###### Definition 17.

We say a subset @xmath is uniformly distributed over the divisors of
@xmath if, for every divisor @xmath of @xmath , the @xmath cosets of
@xmath partition @xmath into subsets, each of size @xmath or @xmath .

At first glance, this definition may seem rather unnatural, but we will
discover some important properties of uniformly distributed rows from
the DFT. As an example, we briefly consider uniform distribution in the
context of the restricted isometry property (RIP). Recall that a matrix
of random rows from a DFT and normalized columns is RIP with high
probability [ 118 ] . We will show that harmonic frames satisfy RIP only
if the selected row indices are nearly uniformly distributed over
sufficiently small divisors of @xmath .

To this end, recall that for any divisor @xmath of @xmath , the Fourier
transform of the @xmath -sparse normalized Dirac comb @xmath is the
@xmath -sparse normalized Dirac comb @xmath . Let @xmath be the @xmath
unitary DFT, and let @xmath be the harmonic frame which arises from
selecting rows of @xmath indexed by @xmath and then normalizing the
columns. In order for @xmath to be @xmath -RIP, @xmath must contain at
least one member of @xmath for every divisor @xmath of @xmath which is
@xmath , since otherwise

  -- -- --
        
  -- -- --

which violates the lower RIP bound at @xmath . In fact, the RIP bounds
indicate that

  -- -------- --
     @xmath   
  -- -------- --

cannot be more than @xmath away from @xmath . Similarly, taking @xmath
to be @xmath modulated by @xmath , i.e., @xmath for every @xmath , gives
that @xmath is also no more than @xmath away from @xmath . This
observation gives the following result:

###### Theorem 18.

Select rows indexed by @xmath from the @xmath discrete Fourier transform
matrix and then normalize the columns to produce the harmonic frame
@xmath . Then @xmath satisfies the @xmath -restricted isometry property
only if

  -- -------- --
     @xmath   
  -- -------- --

for every divisor @xmath of @xmath with @xmath and every @xmath .

Now that we have an intuition for uniform distribution in terms of
modulated Dirac combs and RIP, we take this condition to the extreme by
considering uniform distribution over all divisors. Doing so produces a
complete characterization of full spark harmonic frames when @xmath is a
prime power:

###### Theorem 19.

Let @xmath be a prime power, and select rows indexed by @xmath from the
@xmath discrete Fourier transform matrix to build the submatrix @xmath .
Then @xmath is full spark if and only if @xmath is uniformly distributed
over the divisors of @xmath .

Note that, perhaps surprisingly, an index set @xmath can be uniformly
distributed over @xmath but not over @xmath , and vice versa. For
example, @xmath is uniformly distributed over @xmath but not @xmath ,
while @xmath is uniformly distributed over @xmath but not @xmath .

Since the first @xmath rows of a DFT form a full spark Vandermonde
matrix, let’s check that this index set is uniformly distributed over
the divisors of @xmath . For each divisor @xmath of @xmath , we
partition the first @xmath indices into the @xmath cosets of @xmath .
Write @xmath with @xmath . The first @xmath of the @xmath indices are
distributed equally amongst all @xmath cosets, and then the remaining
@xmath indices are distributed equally amongst the first @xmath cosets.
Overall, the first @xmath cosets contain @xmath indices, while the
remaining @xmath cosets have @xmath indices; thus, the first @xmath
indices are indeed uniformly distributed over the divisors of @xmath .
Also, when @xmath is prime, every subset of @xmath is uniformly
distributed over the divisors of @xmath in a trivial sense. In fact,
Chebotarëv’s theorem follows immediately from Theorem 19 . In some ways,
portions of our proof of Theorem 19 mirror recurring ideas in the
existing proofs of Chebotarëv’s theorem [ 59 , 67 , 126 , 131 ] . For
the sake of completeness, we provide the full argument and save the
reader from having to parse portions of proofs from multiple references.
We start with the following lemmas, whose proofs are based on the proofs
of Lemmas 1.2 and 1.3 in [ 131 ] .

###### Lemma 20.

Let @xmath be a power of some prime @xmath , and let @xmath be a
polynomial with integer coefficients. Suppose there exists @xmath th
roots of unity @xmath such that @xmath . Then @xmath is a multiple of
@xmath .

###### Proof.

Denoting @xmath , then for every @xmath , we have @xmath for some @xmath
. Defining the polynomial @xmath , we have @xmath by assumption. Also,
@xmath is a polynomial with integer coefficients, and so it must be
divisible by the minimal polynomial of @xmath , namely, the cyclotomic
polynomial @xmath . Evaluating both polynomials at @xmath then gives
that @xmath divides @xmath . ∎

###### Lemma 21.

Let @xmath be a power of some prime @xmath , and pick @xmath such that

  -- -------- -- --------
     @xmath      (2.11)
  -- -------- -- --------

is not a multiple of @xmath . Then the rows indexed by @xmath in the
@xmath discrete Fourier transform form a full spark frame.

###### Proof.

We wish to show that @xmath for all @xmath -tuples of distinct @xmath th
roots of unity @xmath . Define the polynomial @xmath . Since columns
@xmath and @xmath of @xmath are identical whenever @xmath , we know that
@xmath vanishes in each of these instances, and so we can factor:

  -- -------- --
     @xmath   
  -- -------- --

for some polynomial @xmath with integer coefficients. By Lemma 20 , it
suffices to show that @xmath is not a multiple of @xmath , since this
implies @xmath is nonzero for all @xmath -tuples of distinct @xmath th
roots of unity @xmath .

To this end, we proceed by considering

  -- -------- -- --------
     @xmath      (2.12)
  -- -------- -- --------

To compute @xmath , we note that each application of @xmath produces
terms according to the product rule. For some terms, a linear factor of
the form @xmath or @xmath is replaced by @xmath or @xmath ,
respectively. For each the other terms, these linear factors are
untouched, while another factor, such as @xmath , is differentiated and
multiplied by @xmath . Note that there are a total of @xmath linear
factors, and only @xmath differentiation operators to apply. Thus, after
expanding every product rule, there will be two types of terms: terms in
which every differentiation operator was applied to a linear factor, and
terms which have at least one linear factor remaining untouched. When we
evaluate at @xmath , the terms with linear factors vanish, and so the
only terms which remain came from applying every differentiation
operator to a linear factor. Furthermore, each of these terms before the
evaluation is of the form @xmath , and so evaluation at @xmath produces
a sum of terms of the form @xmath ; to determine the value of @xmath ,
it remains to count these terms. The @xmath copies of @xmath can only be
applied to linear factors of the form @xmath , of which there are @xmath
, and so there are a total of @xmath ways to distribute these operators.
Similarly, there are @xmath ways to distribute the @xmath copies of
@xmath amongst the @xmath linear factors of the form @xmath . Continuing
in this manner produces an expression for @xmath :

  -- -------- -- --------
     @xmath      (2.13)
  -- -------- -- --------

For an alternate expression of @xmath , we substitute the definition of
@xmath into @xmath . Here, we exploit the multilinearity of the
determinant and the fact that @xmath to get

  -- -------- -- --------
     @xmath      (2.14)
  -- -------- -- --------

where the final equality uses the fact that @xmath is the transpose of a
Vandermonde matrix. Equating ( 2.13 ) to ( 2.14 ) reveals that ( 2.11 )
is an expression for @xmath . Thus, by assumption, @xmath is not a
multiple of @xmath , and so we are done. ∎

###### Proof of Theorem 19.

( @xmath ) We will use Lemma 21 to demonstrate that @xmath is full
spark. To apply this lemma, we need to establish that ( 2.11 ) is not a
multiple of @xmath , and to do this, we will show that there are as many
@xmath -divisors in the numerator of ( 2.11 ) as there are in the
denominator. We start by counting the @xmath -divisors of the
denominator:

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

For each pair of integers @xmath , there are @xmath factors in ( 2.15 )
of the form @xmath . By adding these, we count each factor @xmath as
many times as it can be expressed as a multiple of a power of @xmath ,
which equals the number of @xmath -divisors in @xmath . Thus, the number
of @xmath -divisors of ( 2.15 ) is

  -- -------- -- --------
     @xmath      (2.16)
  -- -------- -- --------

Next, we count the @xmath -divisors of the numerator of ( 2.11 ). To do
this, we use the fact that @xmath is uniformly distributed over the
divisors of @xmath . Since @xmath is a power of @xmath , the only
divisors of @xmath are smaller powers of @xmath . Also, the cosets of
@xmath partition @xmath into subsets @xmath . We note that @xmath is a
multiple of @xmath precisely when @xmath and @xmath belong to the same
subset @xmath for some @xmath . To count @xmath -divisors, we again
count each factor @xmath as many times as it can be expressed as a
multiple of a prime power:

  -- -------- -- --------
     @xmath      (2.17)
  -- -------- -- --------

Write @xmath with @xmath . Then @xmath . Since @xmath is uniformly
distributed over @xmath , there are @xmath subsets @xmath with @xmath
elements and @xmath subsets with @xmath elements. We use this to get

  -- -------- --
     @xmath   
  -- -------- --

Rearranging and substituting @xmath then gives

  -- -- --
        
  -- -- --

Thus, there are as many @xmath -divisors in the numerator ( 2.17 ) as
there are in the denominator ( 2.16 ), and so ( 2.11 ) is not divisible
by @xmath . Lemma 21 therefore gives that @xmath is full spark.

( @xmath ) We will prove that this direction holds regardless of whether
@xmath is a prime power. Suppose @xmath is not uniformly distributed
over the divisors of @xmath . Then there exists a divisor @xmath of
@xmath such that one of the cosets of @xmath intersects @xmath with
@xmath or @xmath indices. Notice that if a coset of @xmath intersects
@xmath with @xmath indices, then the complement @xmath intersects the
same coset with @xmath indices. By Theorem 14 (iii), @xmath produces a
full spark harmonic frame precisely when @xmath produces a full spark
harmonic frame, and so we may assume without loss of generality that
there exists a coset of @xmath which intersects @xmath with @xmath
indices.

To prove that the rows with indices in @xmath are not full spark, we
find column entries which produce a singular submatrix. Writing @xmath
with @xmath , let @xmath contain @xmath cosets of @xmath along with
@xmath elements from an additional coset. We claim that the DFT
submatrix with row entries @xmath and column entries @xmath is singular.
To see this, shuffle the rows and columns to form a matrix @xmath in
which the row entries are grouped into common cosets of @xmath and the
column entries are grouped into common cosets of @xmath . This breaks
@xmath into rank-1 submatrices: each pair of cosets @xmath and @xmath
produces a submatrix

  -- -------- --
     @xmath   
  -- -------- --

for some index sets @xmath and @xmath ; this is a rank-1 outer product.
Let @xmath be the largest intersection between @xmath and a coset of
@xmath . Then @xmath is the number of rows in the tallest of these
rank-1 submatrices. Define @xmath to be the @xmath matrix with entries
@xmath whenever @xmath and zero otherwise. Then

  -- -------- -- --------
     @xmath      (2.18)
  -- -------- -- --------

Since @xmath has @xmath rows of zero entries, we also have

  -- -------- -- --------
     @xmath      (2.19)
  -- -------- -- --------

Moreover, since we can decompose @xmath into a sum of @xmath zero-padded
rank-1 submatrices, we have @xmath . Combining this with ( 2.18 ) and (
2.19 ) then gives that @xmath , and so the DFT submatrix is not
invertible. ∎

Note that our proof of Theorem 19 establishes the necessity of having
row indices uniformly distributed over the divisors of @xmath in the
general case. This leaves some hope for completely characterizing full
spark harmonic frames. Naturally, one might suspect that the uniform
distribution condition is sufficient in general, but this suspicion
fails when @xmath . Indeed, the following DFT submatrix is singular
despite the row indices being uniformly distributed over the divisors of
@xmath :

  -- -------- --
     @xmath   
  -- -------- --

Just as we used Chebotarëv’s theorem to analyze the harmonic equiangular
tight frames from Xia et al. [ 146 ] , we can also use Theorem 19 to
determine whether harmonic equiangular tight frames with a prime power
number of frame elements are full spark. Unfortunately, none of the
infinite families in [ 146 ] have the number of frame elements in the
form of a prime power (other than primes). Luckily, there is at least
one instance in which the number of frame elements happens to be a prime
power: the harmonic frames that arise from Singer difference sets have
@xmath and @xmath for a prime power @xmath and an integer @xmath ; when
@xmath and @xmath , the number of frame elements @xmath is a prime
power. In this case, the row indices we select are

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

but these are not uniformly distributed over 11, and so the
corresponding harmonic frame is not full spark by Theorem 19 .

### 2.2 The computational complexity of verifying full spark

In the previous section, we constructed a large collection of
deterministic full spark frames. To see how special these constructions
are, we consider the following question: How much computation is
required to check whether any given frame is full spark? At the heart of
the matter is computational complexity theory, which provides a rigorous
playing field for expressing how hard certain problems are. In this
section, we consider the complexity of the following problem:

###### Problem 22 (Full Spark).

Given a matrix, is it full spark?

For the lay mathematician, Full Spark is “obviously” @xmath -hard
because the easiest way he can think to solve it for a given @xmath
matrix is by determining whether each of the @xmath submatrices is
invertible; computing @xmath determinants would do, but this would take
a lot of time, and so Full Spark must be @xmath -hard. However,
computing @xmath determinants may not necessarily be the fastest way to
test whether a matrix is full spark. For example, perhaps there is an
easy-to-calculate expression for the product of the determinants; after
all, this product is nonzero precisely when the matrix is full spark.
Recall that Theorem 19 gives a very straightforward litmus test for Full
Spark in the special case where the matrix is formed by rows of a DFT of
prime-power order—who’s to say that a version of this test does not
exist for the general case? If such a test exists, then it would suffice
to find it, but how might one disprove the existence of any such test?
Indeed, since we are concerned with the necessary amount of computation,
as opposed to a sufficient amount, the lay mathematician’s intuition is
a bit misguided.

To discern how much computation is necessary, the main feature of
interest is a problem’s complexity . We use complexity to compare
problems and determine whether one is harder than the other. As an
example of complexity, intuitively, doubling an integer is no harder
than adding integers, since one can use addition to multiply by @xmath ;
put another way, the complexity of doubling is somehow “encoded” in the
complexity of adding, and so it must be lesser (or equal). To make this
more precise, complexity theorists use what is called a polynomial-time
reduction , that is, a polynomial-time algorithm that solves problem
@xmath by exploiting an oracle which solves problem @xmath ; the
reduction indicates that solving problem @xmath is no harder than
solving problem @xmath (up to polynomial factors in time), and we say “
@xmath reduces to @xmath ,” or @xmath . Since we can use the
polynomial-time routine @xmath to produce @xmath , we conclude that
doubling an integer reduces to adding integers, as expected.

In complexity theory, problems are categorized into complexity classes
according to the amount of resources required to solve them. For
example, the complexity class @xmath contains all problems which can be
solved in polynomial time, while problems in @xmath may require as much
as exponential time. Problems in @xmath have the defining quality that
solutions can be verified in polynomial time given a certificate for the
answer. As an example, the graph isomorphism problem is in @xmath
because, given an isomorphism between graphs (a certificate), one can
verify that the isomorphism is legit in polynomial time. Clearly, @xmath
, since we can ignore the certificate and still solve the problem in
polynomial time. Finally, a problem @xmath is called @xmath - hard if
every problem @xmath in @xmath reduces to @xmath , and a problem is
called @xmath - complete if it is both @xmath -hard and in @xmath . In
plain speak, @xmath -hard problems are harder than every problem in
@xmath , while @xmath -complete problems are the hardest of problems in
@xmath .

At this point, it should be clear that @xmath -hard problems are not
merely problems that seem to require a lot of computation to solve.
Certainly, @xmath -hard problems have this quality, as an @xmath -hard
problem can be solved in polynomial time only if @xmath ; this is an
open problem, but it is widely believed that @xmath . However, there are
other problems which seem hard but are not known to be @xmath -hard
(e.g., the graph isomorphism problem). Rather, to determine whether a
problem is @xmath -hard, one must find a polynomial-time reduction that
compares the problem to all problems in @xmath . To this end, notice
that @xmath and @xmath together imply @xmath , and so to demonstrate
that a problem @xmath is @xmath -hard, it suffices to show that @xmath
for some @xmath -hard problem @xmath .

Unfortunately, it can sometimes be difficult to find a deterministic
reduction from one problem to another. One example is reducing the
satisfiability problem ( SAT ) to the unique satisfiability problem (
Unique SAT ). To be clear, SAT is an @xmath -hard problem [ 89 ] that
asks whether there exists an input for which a given Boolean function
returns “true,” while Unique SAT asks the same question with an
additional promise: that the given Boolean function is satisfiable only
if there is a unique input for which it returns “true.” Intuitively,
Unique SAT is easier than SAT because we might be able to exploit the
additional structure of uniquely satisfiable Boolean functions; thus, it
could be difficult to find a reduction from SAT to Unique SAT . Despite
this intuition, there is a randomized polynomial-time reduction from SAT
to Unique SAT [ 138 ] . Defined over all Boolean functions of @xmath
variables, the reduction maps functions that are not satisfiable to
other functions that are not satisfiable, and with probability @xmath ,
it maps satisfiable functions to uniquely satisfiable functions. After
applying this reduction to a given Boolean function, if a Unique SAT
oracle declares “uniquely satisfiable,” then we know for certain that
the original Boolean function was satisfiable. But the reduction will
only map a satisfiable problem to a uniquely satisfiable problem with
probability @xmath , so what good is this reduction? The answer lies in
something called amplification ; since the success probability is, at
worst, polynomially small in @xmath (i.e., @xmath ), we can repeat our
oracle-based randomized algorithm a polynomial number of times @xmath
and achieve an error probability @xmath which is exponentially small.

In this section, we give a randomized polynomial-time reduction from a
problem in matroid theory. Before stating the problem, we first briefly
review some definitions. To each bipartite graph with bipartition @xmath
, we associate a transversal matroid @xmath , where @xmath is the
collection of subsets of @xmath whose vertices form the ends of a
matching in the bipartite graph; subsets in @xmath are called . Next,
just as spark is the size of the smallest linearly dependent set, the
girth of a matroid is the size of the smallest subset of @xmath that is
not in @xmath . In fact, this analogy goes deeper: A matroid is
representable over a field @xmath if, for some @xmath , there exists a
mapping @xmath such that @xmath is linearly independent if and only if
@xmath ; as such, the girth of @xmath is the spark of @xmath . In our
reduction, we make use of the fact that every transversal matroid is
representable over @xmath [ 112 ] . We are now ready to state the
problem from which we will reduce Full Spark :

###### Problem 23.

Given a bipartite graph, what is the girth of its transversal matroid?

Before giving the reduction, we note that Problem 23 is @xmath -hard.
This is demonstrated in McCormick’s thesis [ 100 ] , which credits the
proof to Stockmeyer; since [ 100 ] is difficult to access, we refer the
reader to [ 2 ] . We now turn to the main result of this section; note
that our proof is specifically geared toward the case where the matrix
in question has integer entries—this is stronger than manipulating real
(complex) numbers exactly as well as with truncations and tolerances.

###### Theorem 24.

Full Spark is hard for @xmath under randomized polynomial-time
reductions.

###### Proof.

We will give a randomized polynomial-time reduction from Problem 23 to
Full Spark . As such, suppose we are given a bipartite graph @xmath , in
which every edge is between the disjoint sets @xmath and @xmath . Take
@xmath and @xmath . Using this graph, we randomly draw an @xmath matrix
@xmath using the following process: for each @xmath and @xmath , pick
the entry @xmath randomly from @xmath if @xmath in @xmath ; otherwise
set @xmath . In Proposition 3.11 of [ 99 ] , it is shown that the
columns of @xmath form a representation of the transversal matroid of
@xmath with probability @xmath . For the moment, we assume that @xmath
succeeds in representing the matroid.

Since the girth of the original matroid equals the spark of its
representation, for each @xmath , we test whether @xmath . To do this,
take @xmath to be some @xmath full spark frame. We will determine an
appropriate value for @xmath later, but for simplicity, we can take
@xmath to be the Vandermonde matrix formed from bases @xmath ; see Lemma
12 . We claim we can randomly select @xmath indices @xmath and test
whether @xmath is full spark to determine whether @xmath . Moreover,
after performing this test for each @xmath , the probability of
incorrectly determining @xmath is @xmath , provided @xmath is
sufficiently large.

We want to test whether @xmath is full spark and use the result as a
proxy for whether @xmath . For this to work, we need to have @xmath
precisely when @xmath for every @xmath of size @xmath . To this end, it
suffices to have the nullspace @xmath of @xmath intersect trivially with
the column space of @xmath for every @xmath . To be clear, it is always
the case that @xmath , and so @xmath implies @xmath . If we further
assume that @xmath , then the converse also holds. To see this, suppose
@xmath . Then by the rank-nullity theorem, there is a nontrivial @xmath
. Since @xmath , we must have @xmath , which in turn implies @xmath
since @xmath by assumption. Thus, @xmath by the rank-nullity theorem.

Now fix @xmath of size @xmath such that @xmath . We will show that the
vast majority of choices @xmath of size @xmath satisfy @xmath . To do
this, we consider the columns @xmath of @xmath one at a time, and we
make use of the fact that @xmath . In particular, since @xmath is full
spark, there are at most @xmath columns of @xmath in the orthogonal
complement of @xmath , and so there are at least @xmath choices of
@xmath for which @xmath does not contain @xmath , i.e.,

  -- -------- --
     @xmath   
  -- -------- --

Similarly, after selecting the first @xmath @xmath ’s, we have @xmath ,
where

  -- -- --
        
  -- -- --

Again, since @xmath is full spark, there are at most @xmath columns of
@xmath in the orthogonal complement of @xmath , and so the remaining
@xmath columns are candidates for @xmath that give

  -- -------- --
     @xmath   
  -- -------- --

Overall, if we randomly pick @xmath of size @xmath , then

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where the final step is by Bernoulli’s inequality. Taking a union bound
over all choices of @xmath and all values of @xmath then gives

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Thus, to make the probability of failure @xmath , it suffices to have
@xmath .

In summary, we succeed in representing the original matroid with
probability @xmath , and then we succeed in determining the spark of its
representation with probability @xmath . The probability of overall
success is therefore @xmath . Since our success probability is, at
worst, polynomially small, we can apply amplification to achieve an
exponentially small error probability. ∎

Our use of random linear projections in the above reduction to Full
Spark is similar in spirit to Valiant and Vazirani’s use of random hash
functions in their reduction to Unique SAT [ 138 ] . Since their
randomized reduction is the canonical example thereof, we find our
reduction to be particularly natural.

To conclude this section, we clarify that Theorem 24 is a statement
about the amount of computation necessary in the worst case . Indeed,
the hardness of Full Spark does not rule out the existence of smaller
classes of matrices for which full spark is easily determined. As an
example, Theorem 19 determines Full Spark in the special case where the
matrix is formed by rows of a DFT of prime-power order. This illustrates
the utility of applying additional structure to efficiently solve the
Full Spark problem, and indeed, such classes of matrices are rather
special for this reason.

### 2.3 Phaseless recovery with polarization

In the previous sections, we constructed deterministic full spark frames
and showed that checking for full spark in general is computationally
hard. In this section, we provide a new technique for phaseless recovery
which makes use of full spark frames in the measurement design. We are
particularly interested in using the fewest measurements necessary for
recovery, namely @xmath , where @xmath is the dimension of the signal [
15 ] .

Take a finite set @xmath , and suppose we take phaseless measurements of
@xmath with a frame @xmath with the task of recovering @xmath up to a
global phase factor. For notational convenience, we take @xmath to be
the equivalence relation of being identical up to a global phase factor,
and we say @xmath is a member of the equivalence class @xmath if @xmath
. Having @xmath for every @xmath , we claim it suffices to determine the
relative phase between all pairs of frame coefficients. If we had this
information, we could arbitrarily assign some nonzero frame coefficient
@xmath to have positive phase. If @xmath is also nonzero, then it has
well-defined relative phase

  -- -------- --
     @xmath   
  -- -------- --

which determines the frame coefficent by multiplication: @xmath .
Otherwise when @xmath , we naturally take @xmath , and for notational
convenience, we arbitrarily take @xmath . From here, @xmath can be
identified by applying the canonical dual frame @xmath of @xmath :

  -- -------- --
     @xmath   
  -- -------- --

To find the relative phase between frame coefficients, we turn to the
polarization identity:

  -- -------- --
     @xmath   
  -- -------- --

Thus, if in addition to @xmath , we measure with @xmath , we can use the
above calculation to determine @xmath and then normalize to get the
relative phase @xmath , provided both @xmath and @xmath are nonzero. To
summarize, if we measure with @xmath and @xmath for every pair @xmath ,
then we can recover @xmath . However, such a method uses @xmath
measurements, and since @xmath is a frame, we necessarily have @xmath
and thus a total of @xmath measurements.

In pursuit of @xmath measurements, take some simple graph @xmath , and
only take measurements with @xmath and @xmath . To recover @xmath , we
again arbitrarily assign some nonzero vertex measurement to have
positive phase, and then we propagate relative phase information along
the edges by multiplication to determine the phase of the other vertex
measurements relative to the original vertex measurement. However, if
@xmath is orthogonal to a given vertex vector, then that measurement is
zero, and so relative phase information cannot propagate through the
corresponding vertex; indeed, such orthogonality has the effect of
removing the vertex from the graph, and for some graphs, this will
prevent recovery. For example, if @xmath is a star, then @xmath could be
orthogonal to the vector corresponding to the internal vertex, whose
removal would render the remaining graph edgeless. That said, we should
select @xmath and @xmath so as to minimize the impact of orthogonality
with vertex vectors.

First, we can take @xmath to be full spark so that every subcollection
of @xmath frame elements spans. This implies that @xmath is orthogonal
to at most @xmath members of @xmath , thereby limiting the extent of
@xmath ’s damage to our graph. Additionally, @xmath being full spark
frees us from requiring the graph to be connected after the removal of
vertices; indeed, any remaining component of size @xmath or more will
correspond to a subframe of @xmath that necessarily has a dual frame to
reconstruct with. It remains to find a graph of @xmath vertices and
edges that maintains a size- @xmath component after the removal of any
@xmath vertices.

To this end, we consider a well-studied family of sparse graphs known as
expander graphs . We choose these graphs for their notably strong
connectivity properties. There is a combinatorial definition of expander
graphs, but we will focus on the spectral definition. Given a @xmath
-regular graph @xmath of @xmath vertices, consider the eigenvalues of
its adjacency matrix: @xmath . We say @xmath has expansion @xmath .
Furthermore, a family of @xmath -regular graphs @xmath is a spectral
expander family if there exists @xmath such that every @xmath has
expansion @xmath . Since @xmath is constant over an expander family, we
see that expanders with many vertices are particularly sparse. There are
many results which describe the connectivity of expanders, but the
following is particularly relevant to our application:

###### Lemma 25 ([78]).

Consider a @xmath -regular graph @xmath of @xmath vertices with spectral
expansion @xmath . For all @xmath , removing any @xmath edges from
@xmath results in a connected component of size @xmath .

For our application, removing @xmath vertices from a @xmath -regular
graph necessarily removes @xmath edges, and so this lemma directly
applies. Also,

  -- -------- --
     @xmath   
  -- -------- --

where the last inequality is a rearrangement of @xmath . Since we want
to guarantee that the removal of any @xmath vertices maintains a size-
@xmath component, we must therefore take @xmath . Overall, we use the
following criteria to pick our expander graph: Given the signal
dimension @xmath , use a @xmath -regular graph @xmath of @xmath vertices
with spectral expansion @xmath such that @xmath . Then by the previous
discussion, the total number of measurements is @xmath . We wish to find
choices of graphs which yield only @xmath measurements.

To minimize the redundancy @xmath , we see that for a fixed degree
@xmath , we would like minimal spectral expansion @xmath . Spectral
graph families known as Ramanujan graphs are asymptotically optimal in
this sense; taking @xmath to be the set of connected @xmath -regular
graphs with @xmath vertices, Alon and Boppana (see [ 4 ] ) showed that
for any fixed @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

while Ramanujan graphs are defined to have spectral expansion @xmath .
To date, Ramanujan graphs have only been constructed for certain values
of @xmath . One important construction was given by Lubotzky et al. [ 98
] , which produces a Ramanujan family whenever @xmath is prime. Among
these graphs, we get the smallest redundancy @xmath when @xmath and
@xmath :

  -- -------- --
     @xmath   
  -- -------- --

Thus, in such cases, we may perform phaseless recovery with only @xmath
measurements. However, the number of vertices in each Ramanujan graph
from [ 98 ] is of the form @xmath or @xmath , where @xmath is prime, and
so any bound on redundancy @xmath using graphs from [ 98 ] will only be
valid for particular values of @xmath .

In order to get @xmath in general, we use the fact that random graphs
are nearly Ramanujan with high probability. In particular, for every
@xmath and even @xmath , a random @xmath -regular graph has spectral
expansion @xmath with high probability as @xmath [ 71 ] . Thus, picking
@xmath and @xmath to satisfy @xmath , we may again take @xmath to get

  -- -------- --
     @xmath   
  -- -------- --

with high probability. Note that in this case, @xmath can be any
sufficiently large integer, and so the above bound is valid for all
sufficiently large @xmath , i.e., our procedure can perform phaseless
recovery with @xmath measurements in general.

Note that this section has only considered the case in which the
phaseless measurements were not corrupted by noise. For the noisy case,
Candès et al. [ 37 ] used semidefinite programming to stably reconstruct
from @xmath measurements. Our technique also appears to be stable, and
we expect positive results in this vein using synchronization-type
analysis [ 124 ] ; we leave this for future work.

## Chapter 3 Deterministic matrices with the restricted isometry
property

In Chapter 1, we observed how to use the Gershgorin circle theorem to
demonstrate that certain @xmath matrices have the restricted isometry
property (RIP) for sparsity levels @xmath . In this chapter, we consider
better demonstration techniques which promise to break this “square-root
bottleneck” [ 16 ] . To date, the only deterministic construction that
manages to go beyond the bottleneck is given by Bourgain et al. [ 29 ] ;
in the following section, we discuss what they call flat RIP , which is
the technique they use to demonstrate RIP. We will see that their
technique can be used to demonstrate RIP for sparsity levels much larger
than @xmath , meaning one could very well demonstrate random-like
performance given the proper construction. Later, we introduce an
alternate technique, which can also demonstrate RIP for large sparsity
levels.

After considering the efficacy of these techniques to demonstrate RIP,
it remains to find a deterministic construction that is amenable to
analysis. To this end, we discuss various properties of certain
equiangular tight frames (ETFs). Specifically, real ETFs can be
characterized in terms of their Gram matrices using strongly regular
graphs [ 141 ] . By applying our demonstration techniques to real ETFs,
we derive equivalent combinatorial statements in graph theory. By
focussing on the ETFs which correspond to Paley graphs of prime order,
we are able to make important statements about their clique numbers and
provide some intuition for an open problem in number theory. We conclude
by conjecturing that the Paley ETFs are RIP in a manner similar to
random matrices.

### 3.1 Flat restricted orthogonality

In [ 29 ] , Bourgain et al. provided a deterministic construction of
@xmath RIP matrices that support sparsity levels @xmath on the order of
@xmath for some small value of @xmath . To date, this is the only known
deterministic RIP construction that breaks the square-root bottleneck.
In this section, we analyze their technique for demonstrating RIP, but
first, we provide some historical context. We begin with a definition:

###### Definition 26.

The matrix @xmath has @xmath -restricted orthogonality (RO) if

  -- -------- --
     @xmath   
  -- -------- --

for every pair of @xmath -sparse vectors @xmath with disjoint support.
The smallest @xmath for which @xmath has @xmath -RO is the restricted
orthogonality constant (ROC) @xmath .

In the past, restricted orthogonality was studied to produce
reconstruction performance guarantees for both @xmath -minimization and
the Dantzig selector [ 38 , 40 ] . Intuitively, restricted orthogonality
is important to compressed sensing because any stable inversion process
for ( 1 ) would require @xmath to map vectors of disjoint support to
particularly dissimilar measurements. For the present chapter, we are
interested in upper bounds on RICs; in this spirit, the following result
illustrates some sort of equivalence between RICs and ROCs:

###### Lemma 27 (Lemma 1.2 in [38]).

@xmath .

To be fair, the above upper bound on @xmath does not immediately help in
estimating @xmath , as it requires one to estimate @xmath . Certainly,
we may iteratively apply this bound to get

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

Note that @xmath is particularly easy to calculate:

  -- -------- --
     @xmath   
  -- -------- --

which is zero when the columns of @xmath have unit norm. In pursuit of a
better upper bound on @xmath , we use techniques from [ 29 ] to remove
the log factor from ( 3.1 ):

###### Lemma 28.

@xmath .

###### Proof.

Given a matrix @xmath , we want to upper-bound the smallest @xmath for
which @xmath , or equivalently:

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

for every nonzero @xmath -sparse vector @xmath . We observe from ( 3.2 )
that we may take @xmath to have unit norm without loss of generality.
Letting @xmath denote a size- @xmath set that contains the support of
@xmath , and letting @xmath denote the corresponding entries of @xmath ,
the triangle inequality gives

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      
              @xmath      (3.3)
  -- -------- -------- -- -------

Since @xmath , the second term of ( 3.3 ) satisfies

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

and so it remains to bound the first term of ( 3.3 ). To this end, we
note that for each @xmath with @xmath , the term @xmath appears in

  -- -------- --
     @xmath   
  -- -------- --

as many times as there are size- @xmath subsets of @xmath which contain
@xmath but not @xmath , i.e., @xmath times. Thus, we use the triangle
inequality and the definition of restricted orthogonality to get

  -- -------- -------- --
     @xmath   @xmath   
                       
              @xmath   
  -- -------- -------- --

At this point, @xmath having unit norm implies @xmath , and so

  -- -------- --
     @xmath   
  -- -------- --

Applying both this and ( 3.4 ) to ( 3.3 ) gives the result. ∎

Having discussed the relationship between restricted isometry and
restricted orthogonality, we are now ready to introduce the property
used in [ 29 ] to demonstrate RIP:

###### Definition 29.

The matrix @xmath has @xmath -flat restricted orthogonality if

  -- -------- --
     @xmath   
  -- -------- --

for every disjoint pair of subsets @xmath with @xmath .

Note that @xmath has @xmath -flat restricted orthogonality (FRO) by
taking @xmath and @xmath in Definition 26 to be the characteristic
functions @xmath and @xmath , respectively. Also to be clear, flat
restricted orthogonality is called flat RIP in [ 29 ] ; we feel the name
change is appropriate considering the preceeding literature. Moreover,
the definition of flat RIP in [ 29 ] required @xmath to have unit-norm
columns, whereas we strengthen the corresponding results so as to make
no such requirement. Interestingly, FRO bears some resemblence to the
cut-norm of the Gram matrix @xmath , defined as the maximum value of
@xmath over all subsets @xmath ; the cut-norm has received some
attention recently for the hardness of its approximation [ 6 ] . The
following theorem illustrates the utility of flat restricted
orthogonality as an estimate of the RIC:

###### Theorem 30.

A matrix with @xmath -flat restricted orthogonality has a restricted
orthogonality constant @xmath which is @xmath , and we may take @xmath .

Indeed, when combined with Lemma 28 , this result gives an upper bound
on the RIC: @xmath . The noteworthy benefit of this upper bound is that
the problem of estimating singular values of submatrices is reduced to a
combinatorial problem of bounding the coherence of disjoint sums of
columns. Furthermore, this reduction comes at the price of a mere log
factor in the estimate. In [ 29 ] , Bourgain et al. managed to satisfy
this combinatorial coherence property using techniques from additive
combinatorics. While we will not discuss their construction, we find the
proof of Theorem 30 to be instructive; our proof is valid for all values
of @xmath (as opposed to sufficiently large @xmath in the original [ 29
] ), and it has near-optimal constants where appropriate. The proof can
be found in the Appendix.

To reiterate, Bourgain et al. [ 29 ] used flat restricted orthogonality
to build the only known deterministic construction of @xmath RIP
matrices that support sparsity levels @xmath on the order of @xmath for
some small value of @xmath . We are particularly interested in the
efficacy of FRO as a technique to demonstrate RIP in general. Certainly,
[ 29 ] shows that FRO can produce at least an @xmath improvement over
the Gershgorin technique discussed in the previous section, but it
remains to be seen whether FRO can do better.

In the remainder of this section, we will show that flat restricted
orthogonality is actually capable of demonstrating RIP with much higher
sparsity levels than indicated by [ 29 ] . Hopefully, this realization
will spur further research in deterministic constructions which satisfy
FRO. To evaluate FRO, we investigate how well it performs with random
matrices; in doing so, we give an alternative proof that certain random
matrices satisfy RIP with high probability:

###### Theorem 31.

Construct an @xmath matrix @xmath by drawing each of its entries
independently from a Gaussian distribution with mean zero and variance
@xmath , take @xmath to be the constant from Theorem 30 , and set @xmath
. Then @xmath has @xmath -flat restricted orthogonality and @xmath , and
therefore the @xmath -restricted isometry property, with high
probability provided @xmath .

In proving this result, we will make use of the following Bernstein
inequality:

###### Theorem 32 (see [23, 148]).

Let @xmath be independent random variables of mean zero with bounded
moments, and suppose there exists @xmath such that

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

for every @xmath . Then

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

provided @xmath .

###### Proof of Theorem 31.

Considering Lemma 28 , it suffices to show that @xmath has restricted
orthogonality and that @xmath is sufficiently small. First, to
demonstrate restricted orthogonality, it suffices to demonstrate FRO by
Theorem 30 , and so we will ensure that the following quantity is small:

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

Notice that @xmath and @xmath are mutually independent over all @xmath
since @xmath and @xmath are disjoint. Also, @xmath is Gaussian with mean
zero and variance @xmath , while @xmath similarly has mean zero and
variance @xmath . Viewed this way, ( 3.7 ) being small corresponds to
the sum of independent random variables @xmath having its probability
measure concentrated at zero. To this end, Theorem 32 is naturally
applicable, as the absolute central moments of a Gaussian random
variable @xmath with mean zero and variance @xmath are well known:

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath is a product of independent Gaussian random variables, this
gives

  -- -------- --
     @xmath   
  -- -------- --

Further since @xmath , we may define @xmath to get ( 3.5 ). Later, we
will take @xmath . Considering

  -- -------- --
     @xmath   
  -- -------- --

we therefore have ( 3.6 ), which in this case has the form

  -- -------- --
     @xmath   
  -- -------- --

where the probability is doubled due to the symmetric distribution of
@xmath . Since we need to account for all possible choices of @xmath and
@xmath , we will perform a union bound. The total number of choices is
given by

  -- -------- --
     @xmath   
  -- -------- --

and so the union bound gives

  -- -------- -- -------
     @xmath      (3.8)
  -- -------- -- -------

Thus, Gaussian matrices tend to have FRO, and hence restricted
orthogonality by Theorem 30 ; this is made more precise below.

Again by Lemma 28 , it remains to show that @xmath is sufficiently
small. To this end, we note that @xmath has chi-squared distribution
with @xmath degrees of freedom, and so we can use another (simpler)
concentration-of-measure result; see Lemma 1 of [ 95 ] :

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath . Specifically, we pick

  -- -------- --
     @xmath   
  -- -------- --

and we perform a union bound over the @xmath choices for @xmath :

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

To summarize, Lemma 28 , the union bound, Theorem 30 , and ( 3.8 ) and (
3.9 ) give

  -- -------- -------- --
     @xmath   @xmath   
                       
              @xmath   
              @xmath   
  -- -------- -------- --

and so @xmath gives that @xmath has @xmath -RIP with high probability. ∎

We note that a version of Theorem 31 also holds for matrices whose
entries are independent Bernoulli random variables taking values @xmath
with equal probability. In this case, one can again apply Theorem 32 by
comparing moments with those of the Gaussian distribution; also, a union
bound with @xmath will not be necessary since the columns have unit
norm, meaning @xmath .

### 3.2 Restricted isometry by the power method

In the previous section, we established the efficacy of flat restricted
orthogonality as a technique to demonstrate RIP. While flat restricted
orthogonality has proven useful in the past [ 29 ] , future
deterministic RIP constructions might not use this technique. Indeed, it
would be helpful to have other techniques available that demonstrate RIP
beyond the square-root bottleneck. In pursuit of such techniques, we
recall that the smallest @xmath for which @xmath is @xmath -RIP is given
in terms of operator norms in ( 1.1 ). In addition, we notice that for
any self-adjoint matrix @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denotes the spectrum of @xmath with multiplicities. Let
@xmath be the eigenvalue decomposition of @xmath . When @xmath is even,
we can express @xmath in terms of an easy-to-calculate trace:

  -- -------- --
     @xmath   
  -- -------- --

Combining these ideas with the fact that @xmath pointwise leads to the
following:

###### Theorem 33.

Given an @xmath matrix @xmath , define

  -- -------- --
     @xmath   
  -- -------- --

Then @xmath has the @xmath -restricted isometry property for every
@xmath . Moreover, the restricted isometry constant of @xmath is
approached by these estimates: @xmath .

Similar to flat restricted orthogonality, this power method has a
combinatorial aspect that prompts one to check every sub-Gram matrix of
size @xmath ; one could argue that the power method is slightly less
combinatorial, as flat restricted orthogonality is a statement about all
pairs of disjoint subsets of size @xmath . Regardless, the work of
Bourgain et al. [ 29 ] illustrates that combinatorial properties can be
useful, and there may exist constructions to which the power method
would be naturally applied. Moreover, we note that since @xmath
approaches @xmath , a sufficiently large choice of @xmath should deliver
better-than- @xmath improvement over the Gershgorin analysis. How large
should @xmath be? If we assume @xmath has unit-norm columns, taking
@xmath gives

  -- -------- -- --------
     @xmath      (3.10)
  -- -------- -- --------

where @xmath is the worst-case coherence of @xmath . Equality is
achieved above whenever @xmath is an ETF, in which case ( 3.10 ) along
with reasoning similar to ( 1.5 ) demonstrates that @xmath is RIP with
sparsity levels on the order of @xmath , as the Gershgorin analysis
established. It remains to be shown how @xmath compares. To make this
comparison, we apply the power method to random matrices:

###### Theorem 34.

Construct an @xmath matrix @xmath by drawing each of its entries
independently from a Gaussian distribution with mean zero and variance
@xmath , and take @xmath to be as defined in Theorem 33 . Then @xmath ,
and therefore @xmath has the @xmath -restricted isometry property, with
high probability provided @xmath .

While flat restricted orthogonality comes with a negligible penalty of
@xmath in the number of measurements, the power method has a penalty of
@xmath . As such, the case @xmath uses the order of @xmath measurements,
which matches our calculation in ( 3.10 ). Moreover, the power method
with @xmath can demonstrate RIP with @xmath measurements, i.e., @xmath ,
which is considerably better than an @xmath improvement over the
Gershgorin technique.

###### Proof of Theorem 34.

Take @xmath and pick @xmath . Then Theorem II.13 of [ 58 ] states

  -- -------- --
     @xmath   
  -- -------- --

Continuing, we use the fact that @xmath to get

  -- -- -------- -- --------
        @xmath      
        @xmath      
        @xmath      (3.11)
  -- -- -------- -- --------

where the last inequality follows from the fact that @xmath . Since
@xmath and @xmath are simultaneously diagonalizable, the spectrum of
@xmath is given by @xmath . Combining this with ( 3.11 ) then gives

  -- -------- --
     @xmath   
  -- -------- --

Considering @xmath , we continue:

  -- -------- --
     @xmath   
  -- -------- --

From here, we perform a union bound over all possible choices of @xmath
:

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (3.12)
  -- -------- -------- -- --------

Rearranging @xmath gives @xmath , and so

  -- -------- -- --------
     @xmath      (3.13)
  -- -------- -- --------

Combining ( 3.12 ) and ( 3.13 ) gives the result. ∎

### 3.3 Equiangular tight frames as RIP candidates

In Chapter 1, we observed that equiangular tight frames (ETFs) are
optimal RIP matrices under the Gershgorin analysis. In the present
section, we reexamine ETFs as prospective RIP matrices. Specifically, we
consider the possibility that certain classes of @xmath ETFs support
sparsity levels @xmath larger than the order of @xmath . Before
analyzing RIP, let’s first observe some important features of ETFs.
Recall that Section 0.2 characterized ETFs in terms of their rows and
columns. Interestingly, real ETFs have a natural alternative
characterization.

Let @xmath be a real @xmath ETF, and consider the corresponding Gram
matrix @xmath . Observing Section 0.2 , we have from (ii) that the
diagonal entries of @xmath are 1’s. Also, (iii) indicates that the
off-diagonal entries are equal in absolute value (to the Welch bound);
since @xmath has real entries, the phase of each off-diagonal entry of
@xmath is either positive or negative. Letting @xmath denote the
absolute value of the off-diagonal entries, we can decompose the Gram
matrix as @xmath , where @xmath is a matrix of zeros on the diagonal and
@xmath ’s on the off-diagonal. Here, @xmath is referred to as a Seidel
adjacency matrix , as @xmath encodes the adjacency rule of a simple
graph with @xmath whenever @xmath ; this correspondence originated in [
139 ] .

There is an important equivalence class amongst ETFs: given an ETF
@xmath , one can negate any of the columns to form another ETF @xmath .
Indeed, the ETF properties in Section 0.2 are easily verified to hold
for this new matrix. For obvious reasons, @xmath and @xmath are called
flipping equivalent . This equivalence plays a key role in the following
result, which characterizes real ETFs in terms of a particular class of
strongly regular graphs:

###### Definition 35.

We say a simple graph @xmath is strongly regular of the form @xmath if

-   @xmath has @xmath vertices,

-   every vertex has @xmath neighbors (i.e., @xmath is @xmath -regular
    ),

-   every two adjacent vertices have @xmath common neighbors, and

-   every two non-adjacent vertices have @xmath common neighbors.

###### Theorem 36 (Corollary 5.6 in [141]).

Every real @xmath equiangular tight frame with @xmath is flipping
equivalent to a frame whose Seidel adjacency matrix corresponds to the
join of a vertex with a strongly regular graph of the form

  -- -------- --
     @xmath   
  -- -------- --

Conversely, every such graph corresponds to flipping equivalence classes
of equiangular tight frames in the same manner.

The first chapter illustrated the main issue with the Gershgorin
analysis: it ignores important cancellations in the sub-Gram matrices.
We suspect that such cancellations would be more easily observed in a
real ETF, since Theorem 36 neatly represents the Gram matrix’s
off-diagonal oscillations in terms of adjacencies in a strongly regular
graph. The following result gives a taste of how useful this graph
representation can be:

###### Theorem 37.

Take a real equiangular tight frame @xmath with worst-case coherence
@xmath , and let @xmath denote the corresponding strongly regular graph
in Theorem 36 . Then the restricted isometry constant of @xmath is given
by @xmath for every @xmath , where @xmath denotes the size of the
largest clique in @xmath .

###### Proof.

The Gershgorin analysis ( 1.4 ) gives the bound @xmath , and so it
suffices to prove @xmath . Since @xmath , there exists a clique of size
@xmath in the join of @xmath with a vertex. Let @xmath denote the
vertices of this clique, and take @xmath to be the corresponding Seidel
adjacency submatrix. In this case, @xmath , where @xmath is the @xmath
matrix of all 1’s. Observing the decomposition @xmath , it follows from
( 1.1 ) that

  -- -------- --
     @xmath   
  -- -------- --

which concludes the proof. ∎

This result indicates that the Gershgoin analysis is tight for all real
ETFs, at least for sufficiently small values of @xmath . In particular,
in order for a real ETF to be RIP beyond the square-root bottleneck, its
graph must have a small clique number. As an example, note that the
first four columns of the Steiner ETF in ( 1.6 ) have negative inner
products with each other, and thus the corresponding subgraph is a
clique. In general, each block of an @xmath Steiner ETF, whose size is
guaranteed to be @xmath , is a lower-dimensional simplex and therefore
has this property; this is an alternative proof that the Gershgorin
analysis of Steiner ETFs is tight for @xmath .

#### 3.3.1 Equiangular tight frames with flat restricted orthogonality

To find ETFs that are RIP beyond the square-root bottleneck, we must
apply better techniques than Gershgorin. We first consider what it means
for an ETF to have @xmath -flat restricted orthogonality. Take a real
ETF @xmath with worst-case coherence @xmath , and note that the
corresponding Seidel adjacency matrix @xmath can be expressed in terms
of the usual @xmath -adjacency matrix @xmath of the same graph: @xmath
whenever @xmath . Therefore, for every disjoint @xmath with @xmath , we
want

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (3.14)
  -- -------- -------- -- --------

where @xmath denotes the number of edges between @xmath and @xmath in
the graph. This condition bears a striking resemblence to the following
well-known result in graph theory:

###### Lemma 38 (Expander mixing lemma [85]).

Given a @xmath -regular graph of @xmath vertices, the second largest
eigenvalue @xmath of its adjacency matrix satisfies

  -- -- --
        
  -- -- --

for every pair of vertex subsets @xmath .

In words, the expander mixing lemma says that the number of edges
between vertex subsets of a regular graph is roughly what you would
expect in a random regular graph. For this lemma to be applicable to (
3.14 ), we need the strongly regular graph of Theorem 36 to satisfy
@xmath . Using the formula for @xmath , it is not difficult to show that
@xmath provided @xmath and @xmath . Furthermore, the second largest
eigenvalue of the strongly regular graph will be @xmath , and so the
expander mixing lemma says the optimal @xmath is @xmath since @xmath .
This is a rather weak estimate for @xmath because the expander mixing
lemma does not account for the sizes of @xmath and @xmath being @xmath .
Put in this light, a real ETF that has flat restricted orthogonality
corresponds to a strongly regular graph that satisfies a particularly
strong version of the expander mixing lemma.

#### 3.3.2 Equiangular tight frames and the power method

Next, we try applying the power method to ETFs. Given a real ETF @xmath
, let @xmath denote the “hollow” Gram matrix. Also, take @xmath to be
the @xmath matrix built from the columns of @xmath that are indexed by
@xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath , where @xmath is the @xmath th identity basis element, we
continue:

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (3.15)
  -- -------- -------- -- --------

where the last step used the cyclic property of the trace. From here,
note that @xmath has a zero diagonal, meaning several of the terms in (
3.15 ) are zero, namely, those for which @xmath for some @xmath . To
simplify ( 3.15 ), take @xmath to be the set of @xmath -tuples
satisfying @xmath for every @xmath :

  -- -------- -- --------
     @xmath      (3.16)
  -- -------- -- --------

where @xmath is the wost-case coherence of @xmath , and @xmath is the
corresponding Seidel adjacency matrix. Note that the left-hand side is
necessarily nonnegative, while it is not immediate why the right-hand
side should be. This indicates that more simplification can be done, but
for the sake of clarity, we will perform this simplification in the
special case where @xmath ; the general case is very similar. When
@xmath , we are concerned with 4-tuples @xmath . Let’s partition these
4-tuples according to the value taken by @xmath and @xmath . Note, for a
fixed @xmath and @xmath , that @xmath can be any value other than @xmath
or @xmath , as can @xmath . This leads to the following simplification:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

The first term above is @xmath , while the other term is not as easy to
analyze, as we expect a certain degree of cancellation. Substituting
this simplification into ( 3.16 ) gives

  -- -- --
        
  -- -- --

If there were no cancellations in the second term, then it would equal
@xmath , thereby dominating the expression. However, if oscillations
occured as a @xmath Bernoulli random variable, we could expect this term
to be on the order of @xmath , matching the order of the first term. In
this hypothetical case, since @xmath , the parameter @xmath defined in
Theorem 33 scales as @xmath , and so @xmath ; this corresponds to the
behavior exhibited in Theorem 34 . To summarize, much like flat
restricted orthogonality, applying the power method to ETFs leads to
interesting combinatorial questions regarding subgraphs, even when
@xmath .

#### 3.3.3 The Paley equiangular tight frame as an RIP candidate

Pick some prime @xmath , and build an @xmath matrix @xmath by selecting
the @xmath rows of the @xmath discrete Fourier transform matrix which
are indexed by @xmath , the quadratic residues modulo @xmath (including
zero). To be clear, the entries of @xmath are scaled to have unit
modulus. Next, take @xmath to be an @xmath diagonal matrix whose zeroth
diagonal entry is @xmath , and whose remaining @xmath entries are @xmath
. Now build the matrix @xmath by concatenating @xmath with the zeroth
identity basis element; for example, when @xmath , we have a @xmath
matrix:

  -- -------- --
     @xmath   
  -- -------- --

We claim that in general, this process produces an @xmath equiangular
tight frame, which we call the Paley ETF [ 115 ] . Presuming for the
moment that this claim is true, we have the following result which lends
hope for the Paley ETF as an RIP matrix:

###### Lemma 39.

An @xmath Paley equiangular tight frame has restricted isometry constant
@xmath for all @xmath .

###### Proof.

First, we note that Theorem 16 used Chebotarëv’s theorem [ 126 ] to
prove that the spark of the @xmath Paley ETF @xmath is @xmath , that is,
every size- @xmath subcollection of columns of @xmath forms a spanning
set. Thus, for every @xmath of size @xmath , the smallest singular value
of @xmath is positive. It remains to show that the square of the largest
singular value is strictly less than 2. Let @xmath be a unit vector for
which @xmath . Then since the spark of @xmath is @xmath , the columns of
@xmath span, and so

  -- -------- --
     @xmath   
  -- -------- --

where the final step follows from (i) and (ii) of Section 0.2 , which
imply @xmath . ∎

Now that we have an interest in the Paley ETF @xmath , we wish to verify
that it is, in fact, an ETF. It suffices to show that the columns of
@xmath have unit norm, and that the inner products between distinct
columns equal the Welch bound in absolute value. Certainly, the zeroth
identity basis element is unit-norm, while the squared norm of each of
the other columns is given by @xmath . Also, the inner product between
the zeroth identity basis element and any other column equals the zeroth
entry of that column: @xmath . It remains to calculate the inner product
between distinct columns which are not identity basis elements. To this
end, note that since @xmath if and only if @xmath , the sequence @xmath
doubly covers @xmath , and so

  -- -- --
        
  -- -- --

This well-known expression is called a quadratic Gauss sum, and since
@xmath , its value is determined by the Legendre symbol in the following
way: @xmath for every @xmath with @xmath , where

  -- -------- --
     @xmath   
  -- -------- --

Having established that @xmath is an ETF, we notice that the inner
products between distinct columns of @xmath are real. This implies that
the columns of @xmath can be unitarily rotated to form a real ETF @xmath
; indeed, one may take @xmath to be the @xmath matrix formed by taking
the nonzero rows of @xmath in the Cholesky factorization @xmath . As
such, we consider the Paley ETF to be real. From here, Theorem 36
prompts us to find the corresponding strongly regular graph. First, we
can flip the identity basis element so that its inner products with the
other columns of @xmath are all negative. As such, the corresponding
vertex in the graph will be adjacent to each of the other vertices;
naturally, this will be the vertex to which the strongly regular graph
is joined. For the remaining vertices, @xmath precisely when @xmath ,
that is, when @xmath is not a quadratic residue. The corresponding
subgraph is therefore the complement of the Paley graph, namely, the
Paley graph [ 119 ] . In general, Paley graphs of order @xmath
necessarily have @xmath , and so this correspondence is particularly
natural.

One interesting thing about the Paley ETF’s restricted isometry is that
it lends insight into important properties of the Paley graph. The
following is the best known upper bound for the clique number of the
Paley graph of prime order (see Theorem 13.14 of [ 28 ] and discussion
thereafter), and we give a new proof of this bound using restricted
isometry:

###### Theorem 40.

Let @xmath denote the Paley graph of prime order @xmath . Then the size
of the largest clique is @xmath .

###### Proof.

We start by showing @xmath . Suppose otherwise: that there exists a
clique @xmath of size @xmath in the join of a vertex with @xmath . Then
the corresponding sub-Gram matrix of the Paley ETF has the form @xmath ,
where @xmath is the worst-case coherence and @xmath is the @xmath matrix
of 1’s. Since the largest eigenvalue of @xmath is @xmath , the smallest
eigenvalue of @xmath is @xmath , which is negative when @xmath ,
contradicting the fact that @xmath is positive semidefinite.

Since @xmath , we can apply Lemma 39 and Theorem 37 to get

  -- -------- -- --------
     @xmath      (3.17)
  -- -------- -- --------

and rearranging gives the result. ∎

It is common to apply probabilistic and heuristic reasoning to gain
intuition in number theory. For example, consecutive entries of the
Legendre symbol are known to mimic certain properties of a @xmath
Bernoulli random variable [ 110 ] . Moreover, Paley graphs enjoy a
certain quasi-random property that was studied in [ 50 ] . On the other
hand, Graham and Ringrose [ 76 ] showed that, while random graphs of
size @xmath have an expected clique number of @xmath , Paley graphs of
prime order deviate from this random behavior, having a clique number
@xmath infinitely often. The best known universal lower bound, @xmath ,
is given in [ 51 ] , which indicates that the random graph analysis is
at least tight in some sense. Regardless, this has a significant
difference from the upper bound @xmath in Theorem 40 , and it would be
nice if probabilistic arguments could be leveraged to improve this
bound, or at least provide some intuition.

Note that our proof ( 3.17 ) hinged on the fact that @xmath , courtesy
of Lemma 39 . Hence, any improvement to our estimate for @xmath would
directly lead to the best known upper bound on the Paley graph’s clique
number. To approach such an improvement, note that for large @xmath ,
the Fourier portion of the Paley ETF @xmath is not significatly
different from the normalized partial Fourier matrix @xmath ; indeed,
@xmath for every @xmath of size @xmath , and so the difference vanishes.
If we view the quadratic residues modulo @xmath (the row indices of
@xmath ) as random, then a random partial Fourier matrix serves as a
proxy for the Fourier portion of the Paley ETF. This in mind, we appeal
to the following:

###### Theorem 41 (Theorem 3.2 in [114]).

Draw rows from the @xmath discrete Fourier transform matrix uniformly at
random with replacement to construct an @xmath matrix, and then
normalize the columns to form @xmath . Then @xmath has restricted
isometry constant @xmath with probability @xmath provided @xmath , where
@xmath is a universal constant.

In our case, both @xmath and @xmath scale as @xmath , and so picking
@xmath to achieve equality above gives

  -- -------- --
     @xmath   
  -- -------- --

Continuing as in ( 3.17 ), denote @xmath and take @xmath to get

  -- -------- --
     @xmath   
  -- -------- --

and then rearranging gives @xmath with probability @xmath .
Interestingly, having @xmath with high probability (again, under the
model that quadratic residues are random) agrees with the results of
Graham and Ringrose [ 76 ] . This gives some intuition for what we can
expect the size of the Paley graph’s clique number to be, while at the
same time demonstrating the power of Paley ETFs as RIP candidates. We
conclude with the following, which can be reformulated in terms of both
flat restricted orthogonality and the power method:

###### Conjecture 42.

The Paley equiangular tight frame has the @xmath -restricted isometry
property with some @xmath whenever @xmath , for some universal constants
@xmath and @xmath .

### 3.4 Appendix

In this section, we prove Theorem 30 , which states that a matrix with
@xmath -flat restricted orthogonality has @xmath , that is, it has
restricted orthogonality. The proof below is adapted from the proof of
Lemma 3 in [ 29 ] . Our proof has the benefit of being valid for all
values of @xmath (as opposed to sufficiently large @xmath in the
original [ 29 ] ), and it has near-optimal constants where appropriate.
Moreover in this version, the columns of the matrix are not required to
have unit norm.

###### Proof of Theorem 30.

Given arbitrary disjoint subsets @xmath with @xmath , we will bound the
following quantity three times, each time with different constraints on
@xmath and @xmath :

  -- -------- -- --------
     @xmath      (3.18)
  -- -------- -- --------

To be clear, our third bound will have no constraints on @xmath and
@xmath , thereby demonstrating restricted orthogonality. Note that by
assumption, ( 3.18 ) is @xmath whenever the @xmath ’s and @xmath ’s are
in @xmath . We first show that this bound is preserved when we relax the
@xmath ’s and @xmath ’s to lie in the interval @xmath .

Pick a disjoint pair of subsets @xmath with @xmath . Starting with some
@xmath , note that flat restricted orthogonality gives that

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

for every disjoint @xmath with @xmath and @xmath . Thus, we may take any
@xmath to form a convex combination of these two expressions, and then
the triangle inequality gives

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
                          (3.19)
  -- -------- -------- -- --------

Since ( 3.19 ) holds for every disjoint @xmath with @xmath and @xmath ,
we can do the same thing with an additional index @xmath or @xmath , and
replace the corresponding unit coefficient with some @xmath or @xmath in
@xmath . Continuing in this way proves the claim that ( 3.18 ) is @xmath
whenever the @xmath ’s and @xmath ’s lie in the interval @xmath .

For the second bound, we assume the @xmath ’s and @xmath ’s are
nonnegative with unit norm: @xmath . To bound ( 3.18 ) in this case, we
partition @xmath and @xmath according to the size of the corresponding
coefficients:

  -- -------- --
     @xmath   
  -- -------- --

Note the unit-norm constraints ensure that @xmath and @xmath . The
triangle inequality thus gives

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (3.20)
  -- -------- -------- -- --------

By the definitions of @xmath and @xmath , the coefficients of @xmath and
@xmath in ( 3.20 ) all lie in @xmath . As such, we continue by applying
our first bound:

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (3.21)
  -- -------- -------- -- --------

We now observe from the definition of @xmath that

  -- -------- --
     @xmath   
  -- -------- --

Thus for any positive integer @xmath , the Cauchy-Schwarz inequality
gives

  -- -------- -------- -- --------
     @xmath   @xmath      
                          
              @xmath      (3.22)
  -- -------- -------- -- --------

and similarly for the @xmath ’s. For a fixed @xmath , we note that (
3.22 ) is minimized when @xmath , and so we pick @xmath to be the
smallest positive integer such that @xmath . With this, we continue (
3.21 ):

  -- -------- -------- -- --------
     @xmath   @xmath      
                          (3.23)
  -- -------- -------- -- --------

From here, we claim that @xmath . Considering the definition of @xmath ,
this is easily verified for @xmath by showing @xmath for @xmath . For
@xmath , one can use calculus to verify the second inequality of the
following:

  -- -------- --
     @xmath   
  -- -------- --

meaning @xmath . Substituting @xmath and @xmath into ( 3.23 ) then gives

  -- -------- --
     @xmath   
  -- -------- --

with @xmath , @xmath . As such, ( 3.18 ) is @xmath with @xmath in this
case.

We are now ready for the final bound on ( 3.18 ) in which we apply no
constraints on the @xmath ’s and @xmath ’s. To do this, we consider the
positive and negative real and imaginary parts of these coefficients:

  -- -------- --
     @xmath   
  -- -------- --

and similarly for the @xmath ’s. With this decomposition, we apply the
triangle inequality to get

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Finally, we normalize the coefficients by @xmath and @xmath so we can
apply our second bound:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where @xmath by the Cauchy-Schwarz inequality, and so we are done. ∎

## Chapter 4 Two fundamental parameters of frame coherence

Chapters 1–3 of this thesis were dedicated to a particularly popular
understanding of compressed sensing: that matrices which satisfy the
restricted isometry property (RIP) are very well-suited as sensing
matrices. However, as these chapters show, it is very difficult to
deterministically construct matrices which are provably RIP. It is
therefore desirable to find a worthy alternative to RIP which admits
deterministic sensing matrices. The present chapter is dedicated to one
such alternative, namely the strong coherence property , but before we
define this property, we first motivate it in the context of a support
recovery method known as one-step thresholding (OST) .

The main idea behind OST is that the noiseless measurement vector @xmath
will look similar to the active columns of @xmath , provided the
sparsity level is sufficiently small and the nonzero members of @xmath
are sufficiently large in some sense. Using this intuition, it makes
sense to find the support of @xmath by finding the large values of

  -- -------- --
     @xmath   
  -- -------- --

assuming the columns of @xmath have unit norm. Indeed, if the nonzero
entries of @xmath are larger than the contribution of the cross-column
interactions, then the above calculation serves as a reasonable test for
the support of @xmath . The magnitude of this contribution can be
assessed using two measures of coherence. Indeed, if the columns are
incoherent, then each term of this sum is small, and so it makes sense
to consider the worst-case coherence of @xmath :

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

However, this measure of coherence does not account for sign fluxuations
in the inner products, which should bring significant cancellations in
the sum. If we assume the support of @xmath is drawn randomly, then by a
concentration-of-measure argument, this sum will typically be close to
its expectation, and so its size will rarely exceed some multiple of
@xmath times the following maximum average:

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

For this reason, this notion of coherence, called average coherence ,
was recently introduced in [ 11 ] .

Intuitively, worst-case coherence is a measure of dissimilarity between
frame elements, whereas average coherence measures how well the frame
elements are distributed in the unit hypersphere. As we will see, both
worst-case and average coherence play an important role in various
portions of sparse signal processing, provided we describe the sparse
signal’s support with a probabilistic model. In fact, [ 11 ] used
worst-case and average coherence to produce probabilistic reconstruction
guarantees for OST, permitting sparsity levels on the order of @xmath
(akin to the RIP-based guarantees). In accordance with our motivation
above, these probabilistic guarantees require that worst-case and
average coherence together satisfy the following property:

###### Definition 43.

We say an @xmath unit norm frame @xmath satisfies the strong coherence
property if

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath are given by ( 4.1 ) and ( 4.2 ), respectively.

The reader should know that the constant @xmath is not particularly
essential to the above definition; it is used in [ 11 ] to simplify some
analysis and make certain performance guarantees explicit, but the
constant is by no means optimal. In the next section, we will use the
strong coherence property to continue the work of [ 11 ] . Where [ 11 ]
provided guarantees for noiseless reconstruction, we will produce
near-optimal guarantees for signal detection and reconstruction from
noisy measurements of sparse signals. These guarantees are related to
those in [ 35 , 62 , 135 , 136 ] , and we will also elaborate on this
relationship.

The results given in [ 11 ] and the following section, as well as the
applications discussed in [ 35 , 62 , 84 , 103 , 129 , 134 , 136 , 149 ]
demonstrate a pressing need for nearly tight frames with small
worst-case and average coherence, especially in sparse signal
processing. This chapter offers three additional contributions in this
regard [ 12 , 102 ] . In Section 4.2, we provide a sizable catalog of
frames that exhibit small spectral norm, worst-case coherence, and
average coherence. With all three frame parameters provably small, these
frames are guaranteed to perform well in relevant applications. Next,
performance in many applications is dictated by worst-case coherence. It
is therefore particularly important to understand which worst-case
coherence values are achievable. To this end, the Welch bound (Theorem 3
) is commonly used in the literature. However, the Welch bound is only
tight when the number of frame elements @xmath is less than the square
of the spatial dimension @xmath [ 129 ] . Another lower bound, given in
[ 106 , 146 ] , beats the Welch bound when there are more frame
elements, but it is known to be loose for real frames [ 53 ] . Given
this context, Section 4.3 gives a new lower bound on the worst-case
coherence of real frames. Our bound beats both the Welch bound and the
bound in [ 106 , 146 ] when the number of frame elements far exceeds the
spatial dimension. Finally, since average coherence is so new, there is
currently no intuition as to when (SCP-2) is satisfied. In Section 4.4,
we use ideas akin to the switching equivalence of graphs to transform a
frame that satisfies (SCP-1) into another frame with the same spectral
norm and worst-case coherence that additionally satisfies (SCP-2).

### 4.1 Implications of worst-case and average coherence

Frames with small spectral norm, worst-case coherence, and/or average
coherence have found use in recent years with applications involving
sparse signals. Donoho et al. used the worst-case coherence in [ 62 ] to
provide uniform bounds on the signal and support recovery performance of
combinatorial and convex optimization methods and greedy algorithms.
Later, Tropp [ 136 ] and Candès and Plan [ 35 ] used both the spectral
norm and worst-case coherence to provide tighter bounds on the signal
and support recovery performance of convex optimization methods for most
support sets under the additional assumption that the sparse signals
have independent nonzero entries with zero median. Recently, Bajwa et
al. [ 11 ] made use of the spectral norm and both coherence parameters
to report tighter bounds on the noisy model selection and noiseless
signal recovery performance of an incredibly fast greedy algorithm
called one-step thresholding (OST) for most support sets and arbitrary
nonzero entries. In this section, we discuss further implications of the
spectral norm and worst-case and average coherence of frames in
applications involving sparse signals.

#### 4.1.1 The weak restricted isometry property

A common task in signal processing applications is to test whether a
collection of measurements corresponds to mere noise [ 90 ] . For
applications involving sparse signals, one can test measurements @xmath
against the null hypothsis @xmath and alternative hypothesis @xmath ,
where the entries of the noise vector @xmath are independent, identical
zero-mean complex-Gaussian random variables and the signal @xmath is
@xmath -sparse. The performance of such signal detection problems is
directly proportional to the energy in @xmath [ 56 , 80 , 90 ] . In
particular, existing literature on the detection of sparse signals [ 56
, 80 ] leverages the fact that @xmath when @xmath satisfies the
restricted isometry property (RIP) of order @xmath . In contrast, we now
show that the strong coherence property also guarantees @xmath for most
@xmath -sparse vectors. We start with a definition:

###### Definition 44.

We say an @xmath frame @xmath satisfies the @xmath -weak restricted
isometry property (weak RIP) if for every @xmath -sparse vector @xmath ,
a random permutation @xmath of @xmath ’s entries satisfies

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

with probability exceeding @xmath .

At first glance, it may seem odd that we introduce a random permutation
when we might as well define weak RIP in terms of a @xmath -sparse
vector whose support is drawn randomly from all @xmath possible choices.
In fact, both versions would be equivalent in distribution, but we
stress that in the present definition, the values of the nonzero entries
of @xmath are not random; rather, the only randomness we have is in the
locations of the nonzero entries. We wish to distinguish our results
from those in [ 35 ] , which explicitly require randomness in the values
of the nonzero entries. We also note the distinction between RIP and
weak RIP—weak RIP requires that @xmath preserves the energy of most
sparse vectors. Moreover, the manner in which we quantify “most” is
important. For each sparse vector, @xmath preserves the energy of most
permutations of that vector, but for different sparse vectors, @xmath
might not preserve the energy of permutations with the same support.
That is, unlike RIP, weak RIP is not a statement about the singular
values of submatrices of @xmath . Certainly, matrices for which most
submatrices are well-conditioned, such as those discussed in [ 135 , 136
] , will satisfy weak RIP, but weak RIP does not require this. That
said, the following theorem shows, in part, the significance of the
strong coherence property.

###### Theorem 45.

Any @xmath unit norm frame @xmath with the strong coherence property
satisfies the @xmath -weak restricted isometry property provided @xmath
and @xmath .

###### Proof.

Let @xmath be as in Definition 44 . Note that ( 4.3 ) is equivalent to
@xmath . Defining @xmath , then the Cauchy-Schwarz inequality gives

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      (4.4)
  -- -------- -------- -- -------

where the last inequality uses the fact that @xmath in @xmath . We now
consider Lemma 3 of [ 11 ] , which states that for any @xmath and @xmath
, @xmath with probability exceeding @xmath provided @xmath . We claim
that ( 4.4 ) together with Lemma 3 of [ 11 ] guarantee @xmath with
probability exceeding @xmath . In order to establish this claim, we fix
@xmath and @xmath . It is then easy to see that (SCP-1) gives @xmath ,
and also that (SCP-2) and @xmath give @xmath . Therefore, since the
assumption that @xmath together with @xmath implies @xmath , we obtain
@xmath . The result now follows from the observation that @xmath implies
@xmath . ∎

This theorem shows that having small worst-case and average coherence is
enough to guarantee weak RIP. This contrasts with related results by
Tropp [ 135 , 136 ] that require @xmath to be nearly tight. In fact, the
proof of Theorem 45 does not even use the full power of the strong
coherence property; instead of (SCP-1), it suffices to have @xmath ,
part of what [ 11 ] calls the coherence property. Also, if @xmath has
worst-case coherence @xmath and average coherence @xmath , then even if
@xmath has large spectral norm, Theorem 45 states that @xmath preserves
the energy of most @xmath -sparse vectors with @xmath , i.e., the
sparsity regime which is linear in the number of measurements.

#### 4.1.2 Reconstruction of sparse signals from noisy measurements

Another common task in signal processing applications is to reconstruct
a @xmath -sparse signal @xmath from a small collection of linear
measurements @xmath . Recently, Tropp [ 136 ] used both the worst-case
coherence and spectral norm of frames to find bounds on the
reconstruction performance of basis pursuit (BP) [ 48 ] for most support
sets under the assumption that the nonzero entries of @xmath are
independent with zero median. In contrast, [ 11 ] used the spectral norm
and worst-case and average coherence of frames to find bounds on the
reconstruction performance of OST for most support sets and arbitrary
nonzero entries. However, both [ 11 ] and [ 136 ] limit themselves to
recovering @xmath in the absence of noise, corresponding to @xmath , a
rather ideal scenario.

Our goal in this section is to provide guarantees for the reconstruction
of sparse signals from noisy measurements @xmath , where the entries of
the noise vector @xmath are independent, identical complex-Gaussian
random variables with mean zero and variance @xmath . In particular, and
in contrast with [ 62 ] , our guarantees will hold for arbitrary unit
norm frames @xmath without requiring the signal’s sparsity level to
satisfy @xmath . The reconstruction algorithm that we analyze here is
the OST algorithm of [ 11 ] , which is described in Algorithm 1 . The
following theorem extends the analysis of [ 11 ] and shows that the OST
algorithm leads to near-optimal reconstruction error for certain
important classes of sparse signals.

Before proceeding further, we first define some notation. We use @xmath
to denote the signal-to-noise ratio associated with the signal
reconstruction problem. Also, we use

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath to denote the locations of all the entries of @xmath
that, roughly speaking, lie above the noise floor @xmath . Finally, we
use

  -- -------- --
     @xmath   
  -- -------- --

to denote the locations of entries that, roughly speaking, lie above the
self-interference floor @xmath .

Input: An @xmath unit norm frame @xmath , a vector @xmath , and a
threshold @xmath
Output: An estimate @xmath of the true sparse signal @xmath

@xmath \hfill {Initialize}

@xmath \hfill {Form signal proxy}

@xmath \hfill {Select indices via OST}

@xmath \hfill {Reconstruct signal via least-squares}

Algorithm 1 One-Step Thresholding (OST) for sparse signal reconstruction
[ 11 ]

###### Theorem 46 (Reconstruction of sparse signals).

Take an @xmath unit norm frame @xmath which satisfies the strong
coherence property, pick @xmath , and choose @xmath . Further, suppose
@xmath has support @xmath drawn uniformly at random from all possible
@xmath -subsets of @xmath . Then provided

  -- -------- -- -------
     @xmath      (4.5)
  -- -------- -- -------

Algorithm 1 produces @xmath such that @xmath and @xmath such that

  -- -------- -- -------
     @xmath      (4.6)
  -- -------- -- -------

with probability exceeding @xmath . Finally, defining @xmath , we
further have

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

in the same probability event. Here, @xmath , @xmath , and @xmath are
numerical constants.

###### Proof.

To begin, note that since @xmath , we have from ( 4.5 ) that @xmath . It
is then easy to conclude from Theorem 5 of [ 11 ] that @xmath satisfies
@xmath with probability exceeding @xmath . Therefore, conditioned on the
event @xmath , we can make use of the triangle inequality to write

  -- -------- -- -------
     @xmath      (4.8)
  -- -------- -- -------

Next, we may use ( 4.5 ) and the fact that @xmath satisfies the strong
coherence property to conclude from [ 135 ] (see, e.g., Proposition 3 of
[ 11 ] ) that @xmath with probability exceeding @xmath . Hence,
conditioning on @xmath and @xmath , we have that @xmath since @xmath is
a submatrix of a full column rank matrix @xmath . Therefore, given
@xmath and @xmath , we may write

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

and so substituting ( 4.9 ) into ( 4.8 ) and applying the triangle
inequality gives

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (4.10)
  -- -------- -------- -- --------

Since, given @xmath , we have that @xmath and @xmath are submatrices of
@xmath , and since the spectral norm of a matrix provides an upper bound
for the spectral norms of its submatrices, we have the following given
@xmath and @xmath : @xmath and @xmath . We can now substitute these
bounds into ( 4.10 ) and make use of the fact that @xmath to conclude
that

  -- -------- --
     @xmath   
  -- -------- --

given @xmath and @xmath . At this point, define the event @xmath and
note from Lemma 6 of [ 11 ] that @xmath . A union bound therefore gives
( 4.6 ) with probability exceeding @xmath . For ( 4.7 ), note that
@xmath implies @xmath , and so @xmath implies that @xmath . ∎

A few remarks are in order now for Theorem 46 . First, if @xmath
satisfies the strong coherence property and @xmath is nearly tight, then
OST handles sparsity that is almost linear in @xmath : @xmath from ( 4.5
). Second, we do not impose any control over the size of @xmath , but
rather we state the result in generality in terms of @xmath ; its size
is determined by the signal class @xmath belongs to, the worst-case
coherence of the frame @xmath we use to measure @xmath , and the
magnitude of the noise that perturbs @xmath . Third, the @xmath error
associated with the OST algorithm is the near-optimal (modulo the @xmath
factor) error of @xmath plus the best @xmath -term approximation error
caused by the inability of the OST algorithm to recover signal entries
that are smaller than @xmath . In particular, if the @xmath -sparse
signal @xmath , the worst-case coherence @xmath , and the noise @xmath
together satisfy @xmath , then the OST algorithm succeeds with a
near-optimal @xmath error of @xmath . To see why this error is
near-optimal, note that a @xmath -dimension vector of random entries
with mean zero and variance @xmath has expected squared norm @xmath ; in
our case, we pay an additional log factor to find the locations of the
@xmath nonzero entries among the entire @xmath -dimensional signal. It
is important to recognize that the optimality condition @xmath depends
on the signal class, the noise variance, and the worst-case coherence of
the frame; in particular, the condition is satisfied whenever @xmath ,
since

  -- -------- --
     @xmath   
  -- -------- --

The following lemma provides classes of sparse signals that satisfy
@xmath given sufficiently small noise variance and worst-case coherence,
and consequently the OST algorithm is near-optimal for the
reconstruction of such signal classes.

###### Lemma 47.

Take an @xmath unit norm frame @xmath with worst-case coherence @xmath
for some @xmath , and suppose that @xmath for some @xmath . Fix a
constant @xmath , and suppose the magnitudes of @xmath nonzero entries
of @xmath are some @xmath , while the magnitudes of the remaining @xmath
nonzero entries are not necessarily same, but are smaller than @xmath
and scale as @xmath . Then @xmath , provided @xmath .

###### Proof.

Let @xmath be the support of @xmath , and define @xmath . We wish to
show that @xmath , since this implies @xmath . In order to prove @xmath
, notice that

  -- -------- --
     @xmath   
  -- -------- --

and so combining this with the fact that @xmath gives

  -- -------- --
     @xmath   
  -- -------- --

Therefore, provided @xmath , we have that @xmath . ∎

In words, Lemma 47 implies that OST is near-optimal for those @xmath
-sparse signals whose entries above the noise floor have roughly the
same magnitude. This subsumes a very important class of signals that
appears in applications such as multi-label prediction [ 86 ] , in which
all the nonzero entries take values @xmath . Theorem 46 is the first
result in the sparse signal processing literature that does not require
RIP and still provides near-optimal reconstruction guarantees for such
signals from noisy measurements, while using either random or
deterministic frames, even when @xmath .

Note that our techniques can be extended to reconstruct noisy signals,
that is, we may consider measurements of the form @xmath , where @xmath
is also a noise vector of independent, identical zero-mean
complex-Gaussian random variables. In particular, if the frame @xmath is
tight, then our measurements will not color the noise, and so noise in
the signal may be viewed as noise in the measurements: @xmath ; if the
frame is not tight, then the noise will become correlated in the
measurements, and performance would be depend nontrivially on the
frame’s Gram matrix. Also, Theorem 46 can be generalized to
approximately sparse signals; the analysis follows similiar lines, but
is rather cumbersome, and it appears as though the end result is only
strong enough in the case of very nearly sparse signals. As such, we
omit this result.

### 4.2 Frame constructions

In this section, we consider a range of nearly tight frames with small
worst-case and average coherence. We investigate various ways of
selecting frames at random from different libraries, and we show that
for each of these frames, the spectral norm, worst-case coherence, and
average coherence are all small with high probability. Later, we will
consider deterministic constructions that use Gabor and chirp systems,
spherical designs, equiangular tight frames, and error-correcting codes.
For the reader’s convenience, all of these constructions are summarized
in Table 4.1 . Before we go any further, we consider the following
lemma, which gives three different sufficient conditions for a frame to
satisfy (SCP-2). These conditions will prove quite useful in this
section and throughout the chapter.

###### Lemma 48.

For any @xmath unit norm frame @xmath , each of the following conditions
implies @xmath :

1.  @xmath for every @xmath ,

2.  @xmath and @xmath ,

3.  @xmath and @xmath .

###### Proof.

For condition (i), we have

  -- -- --
        
  -- -- --

The Welch bound (Theorem 3 ) therefore gives @xmath . For condition
(ii), we have

  -- -------- --
     @xmath   
  -- -------- --

Considering the Welch bound, it suffices to show @xmath . Rearranging
gives

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

When @xmath , the left-hand side of ( 4.11 ) becomes @xmath , which is
trivially nonnegative. Otherwise, we have

  -- -------- --
     @xmath   
  -- -------- --

In this case, by the quadratic formula and the fact that the left-hand
side of ( 4.11 ) is concave up in @xmath , we have that ( 4.11 ) is
indeed satisfied. For condition (iii), we use the triangle and
Cauchy-Schwarz inequalities to get

  -- -------- --
     @xmath   
  -- -------- --

Considering the Welch bound, it suffices to show @xmath . Taking @xmath
and rearranging gives a polynomial: @xmath . By convexity and
monotonicity of the polynomial in @xmath , it can be shown that the
largest real root of this polynomial is always smaller than @xmath .
Also, considering it is concave up in @xmath , it suffices that @xmath ,
which we have since @xmath . ∎

#### 4.2.1 Normalized Gaussian frames

Construct a matrix with independent, Gaussian-distributed entries that
have zero mean and unit variance. By normalizing the columns, we get a
matrix called a normalized Gaussian frame . This is perhaps the most
widely studied type of frame in the signal processing and statistics
literature. To be clear, the term “normalized” is intended to
distinguish the results presented here from results reported in earlier
works, such as [ 11 , 17 , 38 , 140 ] , which only ensure that Gaussian
frame elements have unit norm in expectation. In other words, normalized
Gaussian frame elements are independently and uniformly distributed on
the unit hypersphere in @xmath . The following theorem characterizes the
spectral norm and the worst-case and average coherence of normalized
Gaussian frames.

###### Theorem 49 (Geometry of normalized Gaussian frames).

Build a real @xmath frame @xmath by drawing entries independently at
random from a Gaussian distribution of zero mean and unit variance.
Next, construct a normalized Gaussian frame @xmath by taking @xmath for
every @xmath . Provided @xmath , then the following simultaneously hold
with probability exceeding @xmath :

1.  @xmath ,

2.  @xmath ,

3.  @xmath .

###### Proof.

Theorem 49 (i) can be shown to hold with probability exceeding @xmath by
using a bound on the norm of a Gaussian random vector in Lemma 1 of [ 95
] and a bound on the magnitude of the inner product of two independent
Gaussian random vectors in Lemma 6 of [ 79 ] . Specifically, pick any
two distinct indices @xmath , and define probability events @xmath ,
@xmath , and @xmath for @xmath and @xmath . Then it follows from the
union bound that

  -- -------- --
     @xmath   
  -- -------- --

One can verify that @xmath because of Lemma 1 of [ 95 ] , and we further
have @xmath because of Lemma 6 of [ 79 ] and the fact that @xmath .
Thus, for any fixed @xmath and @xmath , @xmath with probability
exceeding @xmath . It therefore follows by taking a union bound over all
@xmath choices for @xmath and @xmath that Theorem 49 (i) holds with
probability exceeding @xmath .

Theorem 49 (ii) can be shown to hold with probability exceeding @xmath
by appealing to the preceding analysis and Hoeffding’s inequality for a
sum of independent, bounded random variables [ 83 ] . Specifically, fix
any index @xmath , and define random variables @xmath . Next, define the
probability event

  -- -------- --
     @xmath   
  -- -------- --

Using the analysis for the worst-case coherence of @xmath and taking a
union bound over the @xmath possible @xmath ’s gives @xmath .
Furthermore, taking @xmath , then elementary probability analysis gives

  -- -------- -- -- --------
     @xmath         
                    (4.12)
  -- -------- -- -- --------

where @xmath denotes the unit hypersphere in @xmath , @xmath denotes the
@xmath -dimensional Hausdorff measure on @xmath , and @xmath denotes the
probability density function for the random vector @xmath . The first
thing to note here is that the random variables @xmath are bounded and
jointly independent when conditioned on @xmath and @xmath . This
assertion mainly follows from Bayes’ rule and the fact that @xmath are
jointly independent when conditioned on @xmath . The second thing to
note is that @xmath for every @xmath . This comes from the fact that the
random vectors @xmath are independent and have a uniform distribution
over @xmath , which in turn guarantees that the random variables @xmath
have a symmetric distribution around zero when conditioned on @xmath and
@xmath . We can therefore make use of Hoeffding’s inequality [ 83 ] to
bound the probability expression inside the integral in ( 4.12 ) as

  -- -- -- --------
           (4.13)
  -- -- -- --------

which is bounded above by @xmath provided @xmath . We can now substitute
( 4.13 ) into ( 4.12 ) and take the union bound over the @xmath possible
choices for @xmath to conclude that Theorem 49 (ii) holds with
probability exceeding @xmath .

Lastly, Theorem 49 (iii) can be shown to hold with probability exceeding
@xmath by using a bound on the spectral norm of standard Gaussian random
matrices reported in [ 117 ] along with Lemma 1 of [ 95 ] .
Specifically, define an @xmath diagonal matrix @xmath , and note that
the entries of @xmath are independently and normally distributed with
zero mean and unit variance. We therefore have from (2.3) in [ 117 ]
that

  -- -------- -- --------
     @xmath      (4.14)
  -- -------- -- --------

In addition, we can appeal to the preceding analysis for the probability
bound on Theorem 49 (i) and conclude using Lemma 1 of [ 95 ] and a union
bound over the @xmath possible choices for @xmath that

  -- -------- -- --------
     @xmath      (4.15)
  -- -------- -- --------

Finally, since @xmath , we can take a union bound over ( 4.14 ) and (
4.15 ) to argue that Theorem 49 (iii) holds with probability exceeding
@xmath .

The complete result now follows by taking a union bound over the failure
probabilities for the conditions (i)-(iii) in Theorem 49 . ∎

###### Example 50.

To illustrate the bounds in Theorem 49 , we ran simulations in MATLAB.
Picking @xmath , we observed @xmath realizations of normalized Gaussian
frames for each @xmath . The distributions of @xmath , @xmath , and
@xmath were rather tight, so we only report the ranges of values
attained, along with the bounds given in Theorem 49 :

  -- -------- --
     @xmath   
  -- -------- --

These simulations seem to indicate that our bounds on @xmath and @xmath
reflect real-world behavior, at least within an order of magnitude,
whereas the bound on @xmath is rather loose.

#### 4.2.2 Random harmonic frames

Random harmonic frames, constructed by randomly selecting rows of a
discrete Fourier transform (DFT) matrix and normalizing the resulting
columns, have received considerable attention lately in the compressed
sensing literature [ 36 , 39 , 118 ] . However, there is no result in
the literature that gives the worst-case coherence of random harmonic
frames. To fill this gap, the following theorem gives the spectral norm
and the worst-case and average coherence of random harmonic frames.

###### Theorem 51 (Geometry of random harmonic frames).

Let @xmath be an @xmath non-normalized discrete Fourier transform
matrix, explicitly, @xmath for each @xmath . Next, let @xmath be a
collection of independent Bernoulli random variables with mean @xmath ,
and take @xmath . Finally, construct an @xmath harmonic frame @xmath by
collecting rows of @xmath which correspond to indices in @xmath and
normalizing the columns. Then @xmath is a unit norm tight frame: @xmath
. Also, provided @xmath , the following simultaneously hold with
probability exceeding @xmath :

1.  @xmath ,

2.  @xmath ,

3.  @xmath .

###### Proof.

The claim that @xmath is tight follows trivially from the fact that the
rows of @xmath are orthogonal and that the rows of @xmath correspond to
a subset of the rows of @xmath . Next, we define the probability events
@xmath and @xmath , and claim that @xmath . The proof of this claim
follows from a Bernstein-like large deviation inequality. Specifically,
note that @xmath with @xmath , and so we have from Theorems A.1.12
and A.1.13 of [ 7 ] and page 4 of [ 118 ] that for any @xmath ,

  -- -------- -- --------
     @xmath      (4.16)
  -- -------- -- --------

Taking @xmath , then a union bound gives @xmath provided @xmath .
Conditioning on @xmath , we have that Theorem 51 (i) holds trivially,
while Theorem 51 (ii) follows from Lemma 48 . Specifically, we have that
@xmath guarantees @xmath because of the conditioning on @xmath , which
in turn implies that @xmath satisfies either condition (i) or (ii) of
Lemma 48 , depending on whether @xmath . This therefore establishes that
Theorem 51 (i)-(ii) simultaneously hold with probability exceeding
@xmath .

The only remaining claim is that @xmath with high probability. To this
end, define @xmath , and pick any two distinct indices @xmath . Note
that

  -- -------- -- --------
     @xmath      (4.17)
  -- -------- -- --------

where the last equality follows from the fact that @xmath has orthogonal
columns. Next, we write @xmath for some @xmath . Then applying the union
bound to ( 4.17 ) and to the real and imaginary parts of @xmath gives

  -- -- -------- -- --------
        @xmath      
        @xmath      
        @xmath      (4.18)
  -- -- -------- -- --------

where the last term follows from ( 4.16 ) and the fact that @xmath .
Define random variables @xmath . Note that the @xmath ’s have zero mean
and are jointly independent. Also, the @xmath ’s are bounded by @xmath
almost surely since @xmath and @xmath . Moreover, the variance of each
@xmath is bounded: @xmath . Therefore, we may use the Bernstein
inequality for a sum of independent, bounded random variables [ 21 ] to
bound the probability that @xmath deviates from @xmath :

  -- -------- --
     @xmath   
  -- -------- --

Similarly, the probability that @xmath is also bounded above by @xmath .
Substituting these probability bounds into ( 4.18 ) gives @xmath with
probability at most @xmath provided @xmath . Finally, we take a union
bound over the @xmath possible choices for @xmath and @xmath to get that
Theorem 51 (iii) holds with probability exceeding @xmath .

The result now follows by taking a final union bound over @xmath and
@xmath . ∎

As stated earlier, random harmonic frames are not new to sparse signal
processing. Interestingly, for the application of compressed sensing, [
38 , 118 ] provides performance guarantees for both random harmonic and
Gaussian frames, but requires more rows in a random harmonic frame to
accommodate the same level of sparsity. This suggests that random
harmonic frames may be inferior to Gaussian frames as compressed sensing
matrices, but practice suggests otherwise [ 63 ] . In a sense, Theorem
51 helps to resolve this gap in understanding; there exist compressed
sensing algorithms whose performance is dictated by worst-case coherence
[ 11 , 62 , 134 , 136 ] , and Theorem 51 states that random harmonic
frames have near-optimal worst-case coherence, being on the order of the
Welch bound with an additional @xmath factor.

###### Example 52.

To illustrate the bounds in Theorem 51 , we ran simulations in MATLAB.
Picking @xmath , we observed @xmath realizations of random harmonic
frames for each @xmath . The distributions of @xmath , @xmath , and
@xmath were rather tight, so we only report the ranges of values
attained, along with the bounds given in Theorem 51 . Notice that
Theorem 51 gives a bound on @xmath in terms of both @xmath and @xmath .
To simplify matters, we show that @xmath , where the minimum and maximum
are taken over all realizations in the sample:

  -- -------- --
     @xmath   
  -- -------- --

The reader may have noticed how consistently the average coherence value
of @xmath was realized. This occurs precisely when the zeroth row of the
DFT is not selected, as the frame elements sum to zero in this case:

  -- -------- --
     @xmath   
  -- -------- --

These simulations seem to indicate that our bounds on @xmath , @xmath ,
and @xmath leave room for improvement. The only bound that lies within
an order of magnitude of real-world behavior is our bound on @xmath .

#### 4.2.3 Gabor and chirp frames

Gabor frames constitute an important class of frames, as they appear in
a variety of applications such as radar [ 82 ] , speech processing [ 145
] , and quantum information theory [ 121 ] . Given a nonzero seed
function @xmath , we produce all time- and frequency-shifted versions:
@xmath , @xmath . Viewing these shifted functions as vectors in @xmath
gives an @xmath Gabor frame. The following theorem characterizes the
spectral norm and the worst-case and average coherence of Gabor frames
generated from either a deterministic Alltop vector [ 3 ] or a random
Steinhaus vector.

###### Theorem 53 (Geometry of Gabor frames).

Take an Alltop function defined by @xmath , @xmath . Also, take a random
Steinhaus function defined by @xmath , @xmath , where the @xmath ’s are
independent random variables distributed uniformly on the unit interval.
Then the @xmath Gabor frames @xmath and @xmath generated by @xmath and
@xmath , respectively, are unit norm and tight, i.e., @xmath . Also,
both frames have average coherence @xmath . Furthermore, if @xmath is
prime, then @xmath , while if @xmath , then @xmath with probability
exceeding @xmath .

###### Proof.

The tightness claim follows from [ 96 ] , in which it was shown that
Gabor frames generated by nonzero seed vectors are tight. The bound on
average coherence is a consequence of Theorem 7 of [ 11 ] concerning
arbitrary Gabor frames. The claim concerning @xmath follows directly
from [ 129 ] , while the claim concerning @xmath is a simple consequence
of Theorem 5.1 of [ 111 ] . ∎

Instead of taking all translates and modulates of a seed function, [ 41
] constructs chirp frames by taking all powers and modulates of a chirp
function. Picking @xmath to be prime, we start with a chirp function
@xmath defined by @xmath , @xmath . The @xmath frame elements are then
defined entrywise by @xmath , @xmath . Certainly, chirp frames are, at
the very least, similar in spirit to Gabor frames. As a matter of fact,
the chirp frame is in some sense equivalent to the Gabor frame generated
by the Alltop function: it is easy to verify that @xmath , and when
@xmath , the map @xmath is a permutation over @xmath . Using terminology
from Definition 67 , we say the chirp frame is wiggling equivalent to a
unitary rotation of permuted Alltop Gabor frame elements. As such, by
Lemma 68 , the chirp frame has the same spectral norm and worst-case
coherence as the Alltop Gabor frame, but the average coherence may be
different. In this case, the average coherence still satisfies (SCP-2).
Indeed, adding the frame elements gives

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

and so @xmath . Therefore, applying Lemma 48 (i) gives the result:

###### Theorem 54 (Geometry of chirp frames).

Pick @xmath prime, and let @xmath be the @xmath frame of all powers and
modulates of the chirp function @xmath . Then @xmath is a unit norm
tight frame with @xmath , and has worst case coherence @xmath and
average coherence @xmath .

###### Example 55.

To illustrate the bounds in Theorems 53 and 54 , we consider the
examples of an Alltop Gabor frame and a chirp frame, each with @xmath .
In this case, the Gabor frame has @xmath , while the chirp frame has
@xmath . Note the Gabor and chirp frames have different average
coherences despite being equivalent in some sense. For the random
Steinhaus Gabor frame, we ran simulations in MATLAB and observed @xmath
realizations for each @xmath . The distributions of @xmath and @xmath
were rather tight, so we only report the ranges of values attained,
along with the bounds given in Theorem 53 :

  -- -------- --
     @xmath   
  -- -------- --

These simulations seem to indicate that bound on @xmath is conservative
by an order of magnitude.

#### 4.2.4 Spherical 2-designs

Lemma 48 (ii) leads one to consider frames of vectors that sum to zero.
In [ 84 ] , it is proved that real unit norm tight frames with this
property make up another well-studied class of vector packings:
spherical 2-designs. To be clear, a collection of unit-norm vectors
@xmath is called a spherical @xmath -design if, for every polynomial
@xmath of degree at most @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the unit hypersphere in @xmath and @xmath denotes the
@xmath -dimensional Hausdorff measure on @xmath . In words, vectors that
form a spherical @xmath -design serve as good representatives when
calculating the average value of a degree- @xmath polynomial over the
unit hypersphere. Today, such designs find application in quantum state
estimation [ 81 ] .

Since real unit norm tight frames always exist for @xmath , one might
suspect that spherical 2-designs are equally common, but this intuition
is faulty—the sum-to-zero condition introduces certain issues. For
example, there is no spherical 2-design when @xmath is odd and @xmath .
In [ 101 ] , spherical 2-designs are explicitly characterized by
construction. The following theorem gives a construction based on
harmonic frames:

###### Theorem 56 (Geometry of spherical 2-designs).

Pick @xmath even and @xmath . Take an @xmath harmonic frame @xmath by
collecting rows from a discrete Fourier transform matrix according to a
set of nonzero indices @xmath and normalizing the columns. Let @xmath
denote @xmath th largest index in @xmath , and define a real @xmath
frame @xmath by

  -- -------- --
     @xmath   
  -- -------- --

Then @xmath is unit norm and tight, i.e., @xmath , with worst-case
coherence @xmath and average coherence @xmath .

###### Proof.

It is easy to verify that @xmath is a unit norm tight frame using the
geometric sum formula. Also, since the frame elements sum to zero and
@xmath , the claim regarding average coherence follows from Lemma 48
(ii). It remains to prove @xmath . For each pair of indices @xmath , we
have

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

and so @xmath . This gives the result. ∎

###### Example 57.

To illustrate the bounds in Theorem 56 , we consider the spherical
2-design constructed from a @xmath harmonic equiangular tight frame [
146 ] . Specifically, we take a @xmath DFT matrix, choose nonzero row
indices

  -- -------- --
     @xmath   
  -- -------- --

and normalize the columns to get a harmonic frame @xmath whose
worst-case coherence achieves the Welch bound: @xmath . Following
Theorem 56 , we produce a spherical 2-design @xmath with @xmath and
@xmath .

#### 4.2.5 Steiner equiangular tight frames

We now consider the construction of Chapter 1: Steiner equiangular tight
frames (ETFs). Recall that these fail to break the square-root
bottleneck as deterministic RIP matrices. By contrast, Steiner ETFs are
particularly well-suited as sensing matrices for one-step thresholding.
To be clear, every Steiner ETF satisfies @xmath . Moreover, if in step
(iii) of Theorem 7 , we choose the distinct rows to be the @xmath rows
of the (complex) Hadamard matrix @xmath that are not all-ones, then the
sum of columns of each @xmath is zero, meaning the sum of columns of
@xmath is also zero. This was done in ( 1.6 ), and the columns sum to
zero, accordingly. Therefore, by Lemma 48 (ii), Steiner ETFs satisfy
(SCP-2). This gives the following theorem:

###### Theorem 58 (Geometry of Steiner equiangular tight frames).

Build an @xmath matrix @xmath according to Theorem 7 , and in step
(iii), choose rows from the (complex) Hadamard matrix @xmath that are
not all-ones. Then @xmath is an equiangular tight frame, meaning @xmath
and @xmath , and has average coherence @xmath .

###### Example 59.

To illustrate the bound in Theorem 58 , we note that the example given
in ( 1.6 ) has @xmath .

#### 4.2.6 Code-based frames

Many structures in coding theory are also useful in frame theory. In
this section, we build frames from a code that originally emerged with
Berlekamp in [ 22 ] , and found recent reincarnation with [ 147 ] . We
build a @xmath frame, indexing rows by elements of @xmath and indexing
columns by @xmath -tuples of elements from @xmath . For @xmath and
@xmath , the corresponding entry of the matrix @xmath is given by

  -- -------- -- --------
     @xmath      (4.19)
  -- -------- -- --------

where @xmath denotes the trace map, defined by @xmath . The following
theorem gives the spectral norm and the worst-case and average coherence
of this frame.

###### Theorem 60 (Geometry of code-based frames).

The @xmath frame defined by ( 4.19 ) is unit norm and tight, i.e.,
@xmath , with worst-case coherence @xmath and average coherence @xmath .

###### Proof.

For the tightness claim, we use the linearity of the trace map to write
the inner product of rows @xmath and @xmath :

  -- -- --
        
        
  -- -- --

This expression is @xmath when @xmath . Otherwise, note that @xmath
defines a homomorphism on @xmath . Since @xmath , the inverse images of
@xmath under this homomorphism must form two cosets of equal size, and
so @xmath , meaning distinct rows in @xmath are orthogonal. Thus, @xmath
is a unit norm tight frame.

For the worst-case coherence claim, we first note that the linearity of
the trace map gives

  -- -------- --
     @xmath   
  -- -------- --

i.e., every inner product between columns of @xmath is a sum over
another column. Thus, there exists @xmath such that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where the last equality is by the identity @xmath , whose proof is a
simple exercise of induction. From here, we perform a change of
variables: @xmath and @xmath . Notice that @xmath corresponds to @xmath
for some @xmath whenever @xmath has two solutions, that is, whenever
@xmath . Since @xmath corresponds to both @xmath and @xmath , we must
correct for under-counting:

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (4.20)
  -- -------- -------- -- --------

where the second equality is by repeated application of @xmath , and

  -- -------- --
     @xmath   
  -- -------- --

To bound @xmath , we will count the @xmath ’s that produce nonzero
summands in ( 4.20 ).

For each @xmath we have a homomorphism @xmath defined by @xmath . Pick
@xmath for which there exists a @xmath such that both @xmath and @xmath
. Then @xmath , and so the kernel of @xmath is the same size as the
coset @xmath , meaning the summand associated with @xmath in ( 4.20 ) is
zero. Hence, the nonzero summands in ( 4.20 ) require @xmath and @xmath
. This is certainly possible whenever @xmath . Exponentiation gives

  -- -------- --
     @xmath   
  -- -------- --

which has degree @xmath . Thus, @xmath has at most @xmath solutions, and
each such @xmath produces a summand in ( 4.20 ) of size @xmath . Next,
we consider the @xmath ’s for which @xmath , @xmath , and @xmath . In
this case, the hyperplanes defined by @xmath and @xmath are parallel,
and so @xmath . Here,

  -- -------- --
     @xmath   
  -- -------- --

which has degree @xmath . Thus, @xmath has at most @xmath solutions, and
each such @xmath produces a summand in ( 4.20 ) of size @xmath . We can
now continue the bound from ( 4.20 ): @xmath . From here, isolating
@xmath gives the claim.

Lastly, for average coherence, pick some @xmath . Then summing the
entries in the @xmath th row gives

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

That is, the frame elements sum to a multiple of an identity basis
element: @xmath . Since every entry in row @xmath is @xmath , we have
@xmath for every @xmath , and so by Lemma 48 (i), we are done. ∎

###### Example 61.

To illustrate the bounds in Theorem 60 , we consider the example where
@xmath and @xmath . This is a @xmath code-based frame @xmath with @xmath
and @xmath .

### 4.3 Fundamental limits on worst-case coherence

In many applications of frames, performance is dictated by worst-case
coherence [ 11 , 35 , 62 , 84 , 103 , 129 , 134 , 136 , 149 ] . It is
therefore particularly important to understand which worst-case
coherence values are achievable. To this end, the Welch bound is
commonly used in the literature. When worst-case coherence achieves the
Welch bound, the frame is equiangular and tight [ 129 ] . However,
equiangular tight frames cannot have more vectors than the square of the
spatial dimension [ 129 ] , meaning the Welch bound is not tight
whenever @xmath . When the number of vectors @xmath is exceedingly
large, the following theorem gives a better bound:

###### Theorem 62 ([5, 109]).

Every sufficiently large @xmath unit norm frame with @xmath and
worst-case coherence @xmath satisfies

  -- -------- -- --------
     @xmath      (4.21)
  -- -------- -- --------

for some constant @xmath .

For a fixed worst-case coherence @xmath , this bound indicates that the
number of vectors @xmath cannot exceed some exponential in the spatial
dimension @xmath , that is, @xmath for some @xmath . However, since the
constant @xmath is not established in this theorem, it is unclear which
base @xmath is appropriate for each @xmath . The following theorem is a
little more explicit in this regard:

###### Theorem 63 ([106, 146]).

Every @xmath unit norm frame has worst-case coherence @xmath .
Furthermore, taking @xmath , this lower bound goes to @xmath as @xmath .

For many applications, it does not make sense to use a complex frame,
but the bound in Theorem 63 is known to be loose for real frames [ 53 ]
. We therefore improve Theorems 62 and 63 for the case of real unit norm
frames:

###### Theorem 64.

Every real @xmath unit norm frame has worst-case coherence

  -- -------- -- --------
     @xmath      (4.22)
  -- -------- -- --------

Furthermore, taking @xmath , this lower bound goes to @xmath as @xmath .

Before proving this theorem, we first consider the special case where
the dimension is @xmath :

###### Lemma 65.

Given @xmath points on the unit sphere @xmath , the smallest angle
between points is @xmath .

###### Proof.

We first claim there exists a closed spherical cap in @xmath with area
@xmath that contains two of the @xmath points. Suppose otherwise, and
take @xmath to be the angular radius of a spherical cap with area @xmath
. That is, @xmath is the angle between the center of the cap and every
point on the boundary. Since the cap is closed, we must have that the
smallest angle @xmath between any two of our @xmath points satisfies
@xmath . Let @xmath denote the closed spherical cap centered at @xmath
of angular radius @xmath , and let @xmath denote our set of @xmath
points. Then we know for @xmath , the @xmath ’s are disjoint, @xmath ,
and @xmath , and so taking 2-dimensional Hausdorff measures on the
sphere gives

  -- -------- --
     @xmath   
  -- -------- --

a contradiction.

Since two of the points reside in a spherical cap of area @xmath , we
know @xmath is no more than twice the radius of this cap. We use
spherical coordinates to relate the cap’s area to the radius: @xmath .
Therefore, when @xmath , we have @xmath , and so @xmath gives the
result. ∎

###### Theorem 66.

Every real @xmath unit norm frame has worst-case coherence @xmath .

###### Proof.

Packing @xmath unit vectors in @xmath corresponds to packing @xmath
antipodal points in @xmath , and so Lemma 65 gives @xmath . Applying the
double angle formula to

  -- -------- --
     @xmath   
  -- -------- --

gives the result. ∎

Now that we understand the special case where @xmath , we tackle the
general case:

###### Proof of Theorem 64.

As in the proof of Theorem 66 , we relate packing @xmath unit vectors to
packing @xmath points in the hypersphere @xmath . The argument in the
proof of Lemma 65 generalizes so that two of the @xmath points must
reside in some closed hyperspherical cap of hypersurface area @xmath .
Therefore, the smallest angle @xmath between these points is no more
than twice the radius of this cap. Let @xmath denote a hyperspherical
cap of angular radius @xmath . Then we use hyperspherical coordinates to
get

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (4.23)
  -- -------- -------- -- --------

We wish to solve for @xmath , but analytically inverting @xmath is
difficult. Instead, we use @xmath for @xmath . Note that we do not lose
generality by forcing @xmath , since this is guaranteed with @xmath .
Continuing ( 4.23 ) gives

  -- -------- -- --------
     @xmath      (4.24)
  -- -------- -- --------

Using the formula for a hypersphere’s hypersurface area, we can express
the left-hand side of ( 4.24 ):

  -- -------- --
     @xmath   
  -- -------- --

Isolating @xmath above and using @xmath and @xmath gives ( 4.22 ). The
second part of the result comes from a simple application of Stirling’s
approximation. ∎

In [ 53 ] , numerical results are given for @xmath , and we compare
these results to Theorems 63 and 64 in Figure 4.1 . Considering this
figure, we note that the bound in Theorem 63 is inferior to the maximum
of the Welch bound and the bound in Theorem 64 , at least when @xmath .
This illustrates the degree to which Theorem 64 improves the bound in
Theorem 63 for real frames. In fact, since @xmath for all @xmath , the
bound for real frames in Theorem 64 is asymptotically better than the
bound for complex frames in Theorem 63 . Moreover, for @xmath , Theorem
64 says @xmath , and [ 19 ] proved this bound to be tight for every
@xmath . Lastly, Figure 4.1 illustrates that Theorem 66 improves the
bound in Theorem 64 for the case @xmath .

In many applications, large dictionaries are built to obtain sparse
reconstruction, but the known guarantees on sparse reconstruction place
certain requirements on worst-case coherence. Asymptotically, the bounds
in Theorems 63 and 64 indicate that certain exponentially large
dictionaries will not satisfy these requirements. For example, if @xmath
, then @xmath by Theorem 63 , and if the frame is real, we have @xmath
by Theorem 64 . Such a dictionary will only work for sparse
reconstruction if the sparsity level @xmath is sufficiently small;
deterministic guarantees require @xmath [ 62 , 134 ] , while
probabilistic guarantees require @xmath [ 11 , 135 ] , and so in this
example, the dictionary can, at best, only accommodate sparsity levels
that are smaller than 10. Unfortunately, in real-world applications, we
can expect the sparsity level to scale with the signal dimension. This
in mind, Theorems 63 and 64 tell us that dictionaries can only be used
for sparse reconstruction if @xmath for some sufficiently small @xmath .
To summarize, the Welch bound is known to be tight only if @xmath , and
Theorems 63 and 64 give bounds which are asympotically better than the
Welch bound whenever @xmath . When @xmath is between @xmath and @xmath ,
the best bound to date is the (loose) Welch bound, and so more work
needs to be done to bound worst-case coherence in this parameter region.

### 4.4 Reducing average coherence

In [ 11 ] , average coherence is used to derive a number of guarantees
on sparse signal processing. Since average coherence is so new to the
frame theory literature, this section will investigate how average
coherence relates to worst-case coherence and the spectral norm. We
start with a definition:

###### Definition 67 (Wiggling and flipping equivalent frames).

We say the frames @xmath and @xmath are wiggling equivalent if there
exists a diagonal matrix @xmath of unimodular entries such that @xmath .
Furthermore, they are flipping equivalent if @xmath is real, having only
@xmath ’s on the diagonal.

The terms “wiggling” and “flipping” are inspired by the fact that
individual frame elements of such equivalent frames are related by
simple unitary operations. Note that every frame with @xmath nonzero
frame elements belongs to a flipping equivalence class of size @xmath ,
while being wiggling equivalent to uncountably many frames. The
importance of this type of frame equivalence is, in part, due to the
following lemma, which characterizes the shared geometry of wiggling
equivalent frames:

###### Lemma 68 (Geometry of wiggling equivalent frames).

Wiggling equivalence preserves the norms of frame elements, the
worst-case coherence, and the spectral norm.

###### Proof.

Take two frames @xmath and @xmath such that @xmath . The first claim is
immediate. Next, the Gram matrices are related by @xmath . Since
corresponding off-diagonal entries are equal in modulus, we know the
worst-case coherences are equal. Finally, @xmath , and so we are done. ∎

Wiggling and flipping equivalence are not entirely new to frame theory.
For a real equiangular tight frame @xmath , the Gram matrix @xmath is
completely determined by the sign pattern of the off-diagonal entries,
which can in turn be interpreted as the Seidel adjacency matrix of a
graph @xmath . As such, flipping a frame element @xmath has the effect
of negating the corresponding row and column in the Gram matrix, which
further corresponds to switching the adjacency rule for that vertex
@xmath in the graph—vertices are adjacent to @xmath after switching
precisely when they were not adjacent before switching. Graphs are
called switching equivalent if there is a sequence of switching
operations that produces one graph from the other; this equivalence was
introduced in [ 139 ] and was later extensively studied by Seidel in [
122 , 123 ] . Since flipping equivalent real equiangular tight frames
correspond to switching equivalent graphs, the terms have become
interchangeable. For example, [ 24 ] uses switching (i.e., wiggling and
flipping) equivalence to make progress on an important problem in frame
theory called the Paulsen problem , which asks how close a nearly unit
norm, nearly tight frame must be to a unit norm tight frame.

Now that we understand wiggling and flipping equivalence, we are ready
for the main idea behind this section. Suppose we are given a unit norm
frame with acceptable spectral norm and worst-case coherence, but we
also want the average coherence to satisfy (SCP-2). Then by Lemma 68 ,
all of the wiggling equivalent frames will also have acceptable spectral
norm and worst-case coherence, and so it is reasonable to check these
frames for good average coherence. In fact, the following theorem
guarantees that at least one of the flipping equivalent frames will have
good average coherence, with only modest requirements on the original
frame’s redundancy.

###### Theorem 69 (Constructing frames with low average coherence).

Let @xmath be an @xmath unit norm frame with @xmath . Then there exists
a frame @xmath that is flipping equivalent to @xmath and satisfies
@xmath .

###### Proof.

Take @xmath to be a Rademacher sequence that independently takes values
@xmath , each with probability @xmath . We use this sequence to randomly
flip @xmath ; define @xmath . Note that if @xmath , we are done. Fix
some @xmath . Then

  -- -------- -- --------
     @xmath      (4.25)
  -- -------- -- --------

We can view @xmath as a sum of @xmath independent zero-mean complex
random variables that are bounded by @xmath . We can therefore use a
complex version of Hoeffding’s inequality [ 83 ] (see, e.g., Lemma 3.8
of [ 10 ] ) to bound the probability expression in ( 4.25 ) as @xmath .
From here, a union bound over all @xmath choices for @xmath gives @xmath
, and so @xmath implies @xmath , as desired. ∎

While Theorem 69 guarantees the existence of a flipping equivalent frame
with good average coherence, the result does not describe how to find
it. Certainly, one could check all @xmath frames in the flipping
equivalence class, but such a procedure is computationally slow. As an
alternative, we propose a linear-time flipping algorithm (Algorithm 2 ).
The following theorem guarantees that linear-time flipping will produce
a frame with good average coherence, but it requires the original
frame’s redundancy to be higher than what suffices in Theorem 69 .

Input: An @xmath unit norm frame @xmath
Output: An @xmath unit norm frame @xmath that is flipping equivalent to
@xmath

@xmath \hfill {Keep first frame element}

for @xmath to @xmath do

if @xmath then

@xmath \hfill {Keep frame element to make sum length shorter}

else

@xmath \hfill {Flip frame element to make sum length shorter}

end if

end for

Algorithm 2 Linear-time flipping

###### Theorem 70.

Suppose @xmath . Then Algorithm 2 outputs an @xmath frame @xmath that is
flipping equivalent to @xmath and satisfies @xmath .

###### Proof.

Considering Lemma 48 (iii), it suffices to have @xmath . We will use
induction to show @xmath for @xmath . Clearly, @xmath . Now assume
@xmath . Then by our choice for @xmath in Algorithm 2 , we know that
@xmath . Expanding both sides of this inequality gives

  -- -------- --
     @xmath   
  -- -------- --

and so @xmath . Therefore,

  -- -- --
        
  -- -- --

where the last inequality uses the inductive hypothesis. ∎

###### Example 71.

Apply linear-time flipping to reduce average coherence in the following
matrix:

  -- -------- --
     @xmath   
  -- -------- --

Here, @xmath , and linear-time flipping produces the flipping pattern
@xmath . Then @xmath has average coherence @xmath . This illustrates
that the condition @xmath in Theorem 70 is sufficient but not necessary.