## Chapter 2 The Problem of Language Acquisition

At its most abstract, language acquisition is simply a mapping from some
input, consisting of speech and perhaps other evidence from the learning
environment, to “knowledge of language”– a grammar that can be used in
the generation and interpretation of new utterances. An understanding of
language acquisition must therefore be founded on an understanding of
the nature of the input, the form and interpretation of grammars, and
the mapping itself. These can each be understood at different levels.
For example, Marr [ Marr82 ] distinguishes between the broad goals of a
computation, the particular representations and algorithms employed, and
their hardware implementation. Given our limited understanding of
language, computation and cognition, a complete theory of language
acquisition at all three levels is presently beyond reach. This thesis
seeks formulate a computational theory that can be implemented using
specific algorithms and representations and tested on real input, by
which we mean evidence undeniably available to children.

This chapter serves as an introduction to problems and theories of
language acquisition. It surveys the evidence available to learners and
the parameters ¹ ¹ 1 The word parameter here refers to any acquired
piece of knowledge that contributes to language variation. This
definition extends the notion of a parameter as a characteristic
constant (“the parameter that determines word ordering”) by also
referring to such learned entities as words and rules. that learners
must acquire from this evidence. Switching attention, the chapter
introduces several conditions on theories of acquisition, in particular
that theories be testable and make as few unjustified assumptions as
possible. This leads to a discussion of what assumptions can safely be
made about the nature of the input to the learning mechanism. The
chapter concludes by arguing two important points: first, that language
acquisition is best thought of as a problem in unsupervised learning,
where the goal is to identify structure in the input that is not evident
on its surface; and second, that the logical starting point for a
complete theory of language acquisition is a theory of the acquisition
of the phonological lexicon.

### 2.1 An Introduction to Language Acquisition

At its most abstract, language acquisition is the process of mapping
from environmental evidence– spoken utterances and perhaps other clues–
to a grammar that can be used to generate and interpret new utterances.
Therefore, language acquisition is best understood by understanding the
nature of the evidence, the form and interpretation of grammars, and the
mechanism that performs the mapping. Here each of these are briefly
reviewed to provide a general background for further discussion.

#### 2.1.1 The Parameters

Speakers express thoughts by causing rapid changes in air pressure. The
production of this speech signal does not happen in one step but through
a complex derivational process [ Levelt91 ] that involves many
intermediate representations, each generated in a manner that depends on
information the speaker has learned. For example, in saying “John caught
the weasels” an English speaker relies on his knowledge that in English

-   there is a proper name John and a noun weasel that refers to a kind
    of animal;

-   subjects are spoken before verbs, objects after verbs, and
    determiners before nouns;

-   tense is usually expressed through the main verb and plural nouns
    are marked with a suffix /s/;

-   proper names and ordinary noun phrases are not marked for case,
    contrasting with pronouns like he and him ;

-    the is unstressed and pronounced /.0pt.0pt/ but catch is stressed
    and (in a past tense sentence) pronounced /k.0ptt/;

-   the sound /l/ can serve as the head of a syllable in weasels /wizlz/
    even though it is not a vowel;

-   the voicing (vocal cord vibration) in the /s/ plural marker is
    determined by the voicing in the immediately preceding sound;

-   at the start of words stopped consonants like /k/ are pronounced
    with a little puff of air;

-   declarative sentences are generally produced with a flat or decaying
    pitch.

These facts are peculiar to English and English speakers; they have been
learned. Knowledge of language thus includes an acoustic inventory;
various motor skills; a lexicon that links phonological and syntactic
and semantic information; many phonological and morphological and
syntactic dictums; an understanding of conversational conventions; and
perhaps much more. These parameters collectively constitute the grammar
that is the desired output of the language acquisition process, though
of course their exact form is open to debate.

From the standpoint of acquisition, grammars have several notable
properties. One is that they contain a very large number of parameters,
many (like words) capable of seemingly infinite variety. This implies
that the space of grammars can not be practically enumerated. Parameters
also come in a great variety of forms; language seems to be built from
different modules that each require different types of information.
Despite this fact, parameters in different modules are highly
interdependent. For example, syntactic ordering rules have meaning only
when combined with part-of-speech tags found in the lexicon.
Furthermore, parameters interact with the generation and interpretation
mechanisms in such a way that there are many parameter settings that
could explain any piece of evidence. For example, weasels could be
pronounced /wizlz/ because a root /wizl/ combines with the plural marker
and the voicing rule, or it could simply be listed in the lexicon like
caught . Finally, very few of the parameters relate directly to the
speech signal; almost all affect or link different hidden
representations.

#### 2.1.2 The Evidence

Children acquire their grammars principally from exposure to spoken
utterances, though it is widely conjectured that they also leverage
extralinguistic information derived from non-auditory senses like sight,
and expectations derived from their own internal state. The difficulty
of language acquisition would seem to depend crucially on two things:
first, the amount of evidence available to the learner and second, the
transparency of the relationship between the input and the grammar that
produced it. Even at the rate of ten utterances per minute for ten hours
each day, by the age of five a child can have heard no more than 11
million utterances. At this point most children have attained nearly all
the fluency and linguistic expertise of adults. Though 11 million
utterances may seem like a lot, it is far less data than is commonly
used to train computer models of language [ Brown92 ] , and allows for
precious few examples for each of the tens of thousands of words that
must be learned. However, the paucity of data is not nearly so
troublesome for acquisition as the opaque relation between the grammar
and the input signal.

Much of the complexity of the relationship between the grammar of the
target language and the signal available to the learner is caused by
factors external to the language faculty. Grammars are not the only
source of variation in the speech signal: language is a channel for the
transmission of information, and changing this information can have the
same effect on the speech signal as changing the grammar would. Other
factors that confuse the relationship between the parameters and the
signal include background noise, starts and stops, coughs, other
disfluencies, ungrammatical structure and nonsense words. An utterance
may even reflect incoherent thought or be from a different language.
Presumably, therefore, a learner must be suspicious of all input and
entertain the possibility that it might not be useful evidence for the
target language at all.

Even if speech signals could be taken at face value, they obscure the
parameters of the generating grammar quite effectively: without
knowledge of the generating grammar the derivational history of an
utterance is nearly invisible. To take just one example, the
phonological representations that are the basis for speech production
are rooted in coarse articulatory gestures [ Halle83b ] like tongue
movements that have complex and sometimes subtle affects on the acoustic
signal [ Rabiner93 ] . For this reason, it is extremely difficult to
determine the motor commands that produced a signal. Even if they were
known, this would not uniquely determine the control sequence that
caused them, because in the process of speaking gestures are routinely
(but not necessarily predictably) omitted and otherwise corrupted in an
attempt to minimize muscular effort [ Klatt92 ] . Furthermore, unlike in
the English writing system, neither phonemes (primitive bundles of
articulatory gestures) nor words nor other units in speech are routinely
separated by delimiters. The pause that is often supposed to exist
between words is usually a perceptual illusion apparent only to
competent speakers: unknown languages sound rapid and continuous.
Certainly word-internal boundaries (such as between /wizl/ and /z/) are
almost never highlighted, and there is little evidence that pause
duration or other information can be used to reliably segment higher
structures like phrases. The fact that the speech signal does not
uniquely reflect phonological representations and does not contain
segmentation information means that the naive learner can not determine
the number or sounds of the words that produced it, and therefore that
no signal provides conclusive evidence for any lexical parameters. There
are many additional ways that information about the derivational process
is lost. For example, many phonological processes destroy or hide
information about underlying memorized forms [ Anderson88b ] . For these
reasons and many more, the raw speech signal offers few direct insights
into the parameter settings of the process that generated it.

Perhaps the best evidence that the speech signal provides relatively
little constraint on the derivational process (and hence the parameters
that control it) comes from the fact that even if the generating grammar
is known , there are many possible interpretations for any given
utterance. Indeed, one of the principal components of any automatic
speech recognition device is a highly restrictive language model that
attempts to filter possible word sequences on the basis of
language-specific usage patterns [ Rabiner93 ] .

This leaves open the possibility that parameter values can be easily
determined from extralinguistic input, such as the way a mother wiggles
her eyebrows, or (more plausibly) the manner in which she emphasizes
different parts of the speech signal. This possibility is explored
further in section 2.3 ; the conclusion there is that there is little
evidence such felicitous cues exist and even less that they are required
for learning. Of course, it is clear that word meanings are not derived
from the speech signal alone, but it is doubtful that the evidence
learners use to acquire meaning also serves to determine low-level
parameters.

#### 2.1.3 The Learning Process

Very little is understood about the processes children employ to learn
language: researchers that have studied child language acquisition have
concentrated their efforts on characterizing children’s knowledge at
various stages of life. Roughly speaking, phonological distinctions,
syllable structure and other information concerning sound patterns are
learned early [ Jusczyk93 , Jusczyk94 ] , followed later by words and
syntax [ Goodluck91 , Pinker94 ] . But such facts shed little insight
into the character of the process that maps evidence to grammar. For
this reason, theorists have traditionally argued for or against
hypothesized learning mechanisms on the basis of how they accord with
abstract properties of the language learning problem (such as the
seeming ease with which children acquire language). These properties are
defined by the nature of the input, the form of parameters and the
mechanisms that interpret them, and the purpose of language.

#### 2.1.4 Summary

The preceding sections have shown that language acquisition is
characterized by the following important facts:

-   there is relatively little evidence available to the learner, at
    least compared to the demands of existing computational models;

-   the learner chooses a grammar from among a high-dimensional
    parameter space, spanning many different types of parameters;

-   parameters are highly interdependent;

-   the relationship between parameters and observables is complicated
    and non-transparent;

-   the evidence available to the learner can be explained by many
    different parameter settings.

Thus, language acquisition has all the hallmarks of an extremely
difficult learning problem. But these facts do not entirely specify the
task of researchers who seek to build theories of language acquisition;
this is the topic of the next section.

### 2.2 Theories of Language Acquisition

The purpose of a theory of language acquisition is to explain how a
learner can map from utterances (and perhaps other evidence available in
the learning environment) to a grammar that can be used in the
generation and interpretation of new utterances. Theories can be
evaluated on any of a number of bases. An engineer may be interested in
theories that provide an explicit algorithm for learning useful
parameters from readily available input. A psychologist may be
interested in theories that predict acquisition in the same manner as
children, perhaps even going so far as to require a description at the
level of neural anatomy. Or an evolutionary biologist might wish for an
abstract characterization that makes plain what classes of language are
learnable by any mechanism. Naturally, the ideal situation would be to
understand language acquisition at all levels from neural implementation
to computational theory [ Marr82 ] . As a practical matter such an
understanding is beyond current reach.

To understand the goals of this thesis, it is necessary to define
“knowledge of language” more precisely. In one sense, languages are sets
of sentences, or alternatively mappings between sound and meaning; this
is the traditional view of the structuralist and descriptivist schools
(see, for example, Bloomfield [ Bloomfield33 ] and Lewis [ Lewis75 ] ).
Chomsky [ Chomsky86 ] uses the term E-language (externalized language)
to refer to this notion. Viewing languages this way, learning language
means to acquire knowledge sufficient to generate and interpret new
utterances in the same manner as the rest of the speech community. This
suggests that learning mechanisms should be judged by the generalization
performance of the grammars they produce; in fact, this criteria has
historically been the driving force behind theories of language
acquisition. However, knowledge of language is also a property of
individuals. Each speaker has internalized some particular knowledge in
some particular representation, and it is this knowledge that allows
them to generate and interpret languages. Chomsky uses the term
I-language (internalized language) to refer to “the element of the mind
of the person who knows the language”. To a scientist interested in
characterizing human cognitive processes, a theory of language learning
must also be judged on the basis of whether it produces the same
internal characterization of language that a child would attain in the
same circumstances.

This thesis seeks to formulate a theory of language acquisition that is
consistent with both views of language. In other words, the theory (as
represented by a learning mechanism) will be evaluated both on the basis
of whether it produces grammars consistent with the E-language the
learner is exposed to and on the basis of whether it produces grammars
that have qualitatively similar internal representations to the grammars
children would produce in the same circumstances.

The remainder of this section argues several points relating to the
formulation of theories of language acquisition. The first is that a
primary goal must be to produce theories that can be tested with only a
minimal number of additional assumptions. The second is that, at the
present time, it is relatively unimportant that learning theories
explain the detailed manner in which children acquire language. Finally,
it is argued that although the learning mechanism is at the heart of any
theory of acquisition, it must be justified in terms of general
principles. This is essentially a statement that any theory at the level
of algorithms and representations must be related back to a more
abstract description at the level of computational theory.

#### 2.2.1 Testability and Theories of Acquisition

In chapters 4 and 5 a theory of language acquisition is presented,
formulated principally at the level of representations and algorithms.
The justification for this level of abstraction is that at this level
theories are both sufficiently abstract to shed insight into the general
nature of the learning problem, and sufficiently concrete to be
testable. There are at least six reasons to concentrate effort on
theories that can be evaluated with few additional assumptions, and in
particular, tested on real data.

-   Any theory that can be tested on real data can be falsified or
    verified in a far more convincing way than a theory that is either
    phrased in vague terms, or that is removed from data by additional
    assumptions; it therefore has greater content.

-   Such theories, if verified, are existence proofs, demonstrating
    conclusively that certain parameters can be learned. In this way
    they can form the foundations of further research that is predicated
    on language learnability, justifying certain assumptions.

-   As an existence proof, a tested theory also proves that it is not
    necessary to make assumptions beyond those that are in the theory.
    As discussed further in section 2.3 , many have assumed (without
    conclusive evidence) that the input children receive is quite rich;
    such input permits quite simple learning methods. If it can be
    demonstrated that rich evidence is not necessary for learning, then
    theories that assume it are put under the additional onus of having
    to both demonstrate its existence and the fact that children rely on
    it.

-   In the course of applying algorithms and representations to real
    input, incorrect and implicit assumptions in abstract theories can
    be identified. For example, without testing on real data it may not
    be apparent that a particular grammatical representation, while
    sufficient to model real language, cannot be correct because under
    it no plausible learning algorithm can identify a consistent grammar
    from unstructured evidence (see section 2.2.4 ). In a similar vein,
    Ristad, Barton and Berwick [ Barton87b , Ristad94 ] have argued that
    many theories can be dismissed on the basis of their computational
    complexity. Such deficiencies usually become apparent immediately
    upon implementation.

-   In the course of applying algorithms and representations to real
    input, the most significant “problems” of language learning are
    identified. This is not necessarily the case with more abstract
    theories of language. For example, as discussed further in section
    2.3 , many abstract theories have concerned themselves with the
    issue of whether grammars can be uniquely identified on the basis of
    positive evidence. But with the sort of grammatical theories that
    are necessary to explain real data, it quickly becomes clear that
    the answer is no. This suggests (see section 2.4.2 ) that the more
    important issue in language learning is how to select the correct
    grammar from among the set that are consistent with the input.

-   Since any learning theory that can be tested on real data
    necessarily includes an explicit, computationally feasible learning
    algorithm, it simultaneously serves as a solution to engineering
    problems involving the acquisition of human language.

#### 2.2.2 Conditions on Theories of Acquisition

In requiring that they be testable, various conditions have been placed
on theories of acquisition. In particular, a theory must be feasible
(the learning mechanism embodied in it must make reasonable use of
computational resources and demand no more from the learning environment
than what is available), complete (the parameters, learning mechanism,
and form of the input must each be specified in sufficient detail to be
implemented and simulated) and independent (the theory must not rely on
the presence of other unattested or undemonstrated mechanisms to
preprocess evidence or otherwise aid the learning mechanism).

One condition not listed above is that a theory should predict learning
in the same manner as human beings [ MacWhinney78 ] . This is omitted
for several reasons. First, in any scientific endeavor some
simplifications must be made and relaxing the manner condition is
unlikely to alter the fundamental character of the learning problem.
Secondly, it is important to understand how language can be learned,
irrespective of mechanism. For example, it is a goal of the engineering
community to create computer programs that mimic the end-to-end
linguistic performance of humans, though there is no desire for a neural
implementation. Even within the realm of science, it is interesting to
ask what the range of possible learning mechanisms for language is.
Important questions include “how much of language can be learned from
sound alone?” and “to what extent is the nature of language determined
by the learning mechanism?”. Finally, there is sufficiently little
evidence for how children learn that it is not clear a manner condition
can be usefully and fairly applied.

These three conditions are quite restrictive; in particular, the
completeness and independence conditions leave little room for theories
that advance our understanding of acquisition without completely solving
the learning problem. It could argued that by instilling these
conditions, scientific progress will be stifled, because they cannot be
met at the present time. For example, researchers are almost totally
ignorant of the mechanisms that process extralinguistic information in
the learning environment and provide the child with the representations
of meaning that must eventually be associated with sound. Plainly some
artificial substitute for these mechanisms must be used to test any
current theory of acquisition. This is unavoidable, but it does not
alter the fact that a more desirable theory would dispense with the
artificial input (and all the assumptions associated with it) and work
directly from attested evidence. Regardless of whether the conditions
can be met, they must be active goals.

#### 2.2.3 Assumptions and Modularity in Theories of Acquisition

No existing theory of language acquisition meets the above conditions.
Many assume grammatical and noiseless input. Some assume the learner has
access to unlikely representations of sentence meaning (section 2.3 ) or
similarly untestified segmentations of the speech signal. Most are based
on linguistic theories that can account for only small subsets of real
utterances. Almost all restrict the learning problem to a small subset
of linguistic parameters, assuming input neatly preprocessed to
eliminate all other aspects of acquisition (see below). Some relax all
computational constraints on the learning mechanism (section 2.2.4 ).
And finally, many theories are so vague and incomplete as to be entirely
unimplementable. Of course, some of these violations are less detracting
than others: a vague theory may be contentless and a theory that assumes
too much of the input may be irrelevant, but a theory that adequately
explains the acquisition of a small part of language represents
considerable progress, if it makes plausible assumptions about the
remainder of the acquisition process. The remainder of this section
explores this issue in more detail.

Theories of language processing generally divide the language faculty
into various weakly interacting modules, such as acoustic processing,
phonetics, phonology, morphology, syntax and semantics. The acquisition
literature reflects this split: most (reasonably well specified)
theories of language acquisition restrict their scope to the parameters
of particular modules. As a scientific practice this is not without
risk, because the modules themselves may be merely artifacts of current
linguistic theory, and because the boundaries between the theorized
modules are unobservable and hence uncertain. There are two undesirable
but common consequences of this:

-   An acquisition theory for one part of language may make implausible
    demands of its evidence, such as requiring noiseless input, input in
    a linguistically implausible form, or input that cannot be computed
    without communication between modules. Examples include theories of
    morphological acquisition that expect segmented, noiseless phoneme
    sequences as input and theories of syntactic acquisition that assume
    side semantic information is tree structured in a manner very
    similar to that of syntax.

-   An acquisition theory for one part of language may unreasonably
    assume that the parameters of other parts can be learned
    independently. Examples include theories of the acquisition of
    phonological rules that presume the underlying forms of words are
    already known (even though the underlying forms of words are
    difficult to derive without knowledge of phonological rules), and
    theories of the acquisition of syntax that assume word
    parts-of-speech are known (even though the principal source of
    information about word parts-of-speech is syntax).

Many theories fall into these traps: figure 2.1 catalogs a selection of
computational theories of language acquisition and their input-output
behavior. Various assumptions are common: no ungrammatical input, no
input from languages other than the target languages, no homonymy, etc.
These assumptions violate what we know about the real environment
children learn in. Most theories also demand the extraordinary from
other parts of language: the existence of a remarkable preprocessor that
maps from acoustic signals to noiseless token sequences; access to a
similarly unerring module that extracts semantic structure from the
learning environment; a means of segmenting and uniquely identifying
words in the input; and so forth. These requirements are far beyond the
capabilities of any known mechanisms. Finally, all of these theories
assume that other modules can function without feedback and can be
learned independently.

It is of course not possible to construct a complete theory of language
or language acquisition in one step. But the safest starting points are
the ones that require the fewest assumptions, and hence the ones nearest
to attested evidence. This suggests that most effort should be devoted
to explaining how the most primitive parameters are learned; these might
include sound classes, constraints on syllable structure, and other
parameters close to the speech signal. Of course, if it can be
reasonably argued (or demonstrated) that some parameters are not
strictly necessary for the acquisition of others, then their study can
be reasonably deferred. For example, it is possible that the
phonological form of words can be learned even without an understanding
of syllable structure.

#### 2.2.4 Specification of The Learning Mechanism

As has been mentioned, there are three principal components to any
theory of acquisition: the evidence, the parameters, and the learning
mechanism. The evidence is essentially fixed by what is available to
children (though what this evidence is is not entirely understood). The
parameters are theory-internal, but are defined by the processes that
interpret and generate utterances, and these can be investigated
independently of acquisition. Therefore theories of acquisition have
relatively little freedom to select the range and form of the parameters
that must be learned. This would seem to imply that a theory of
acquisition boils down to a specification of a learning mechanism. But
if a theory emphasizes the role of the learning mechanism, then it is
under an increased obligation to justify its function in terms of
general principles. For this reason, it is unsatisfying to assume a
baroque mechanism.

To understand the importance of the learning mechanism, it is worth
introducing a simple one (discussed in more length in the following
section). Imagine an algorithm that enumerates grammars in some
predetermined order and stops at the first one that is consistent with
the evidence, under some simple definition of consistency. Given the
number of possible grammars and the possibility of noise in the input,
it is clear that this algorithm is merely a theoretical tool; it cannot
possibly be computationally feasible or reliable. These issues cannot be
lightly dismissed on the grounds that the algorithm is merely being
described at the level of a computational theory and abstracts from
various details necessary to handle real-life situations. Efficiency,
convergence, robustness and other properties of learning mechanisms all
indirectly bear on other parts of the learning framework. For example,
there is significant evidence that the known induction algorithms for
certain classes of grammars (such as stochastic context-free grammars [
Carroll92 , deMarcken95b , Pereira92 ] ) are systematically incapable of
learning linguistically relevant languages; this reflects back on the
appropriateness of the grammar class as a model of human language.
Hence, a complete theory of language acquisition, even at the abstract
level of computational theory, must be explicit about the details of the
learning mechanism.

Unfortunately, there are good reasons not to overly burden the learning
mechanism. Complex learning algorithms are notoriously difficult to
analyze and make categorical statements about. In most cases, the only
means of evaluating them is to simulate their execution. Thinking in
terms of general principles provides greater insight into the language
learning process as a whole. It is for similar reasons that optimization
researchers think in terms of an objective function, even though their
algorithms may only consider its derivative when searching. An example
serves to illustrate the problematic nature of complex learning
algorithms. Dresher and Kaye [ Dresher90 ] , arguing that brute-force
enumeration strategies are unsuitable models of human language
acquisition, propose a cue-based learning algorithm for the parameters
of a metrical stress system. In cue-based strategies, the learner is
aware of the relationship between various sentences and parameter
values. Thus, in Dresher and Kaye’s model evidence of a certain stress
pattern might trigger the resetting of a parameter from its default
value to a marked one. They describe cues appropriate for their simple
parameter system and argue that the cues are sufficient for learning.
Unfortunately, the cues are not so simple as to be easily derivable from
the parameter system, and thus must be a hardwired part of the learning
algorithm, selected presumably by evolution. Little can be said about
the nature of the cues without reference to the details of the parameter
system; for any change in the model of stress the feasibility of a
cue-based strategy must be re-justified. In contrast, Gibson and
Wexler’s [ Gibson94 ] simpler “TLA” parameter-setting algorithm is
easily analyzed [ Niyogi94 ] , though its success is similarly dependent
on the structure of the parameter system.

### 2.3 The Nature of the Input

In section 2.2.3 it was argued that theories of language acquisition
should be built up from the evidence that is available to the learner.
This forces us to examine in more detail the nature of the input. There
are two important questions. The first is whether the learner has access
to feedback and evidence for what utterances are not in the target
language. The second is the extent to which extralinguistic input serves
to directly transmit parameter values. These are both discussed here in
the context of one particular framework for theories of acquisition.

Chomsky writes [ Chomsky61 , Chomsky65 ] that any theory of language
must provide

-   (i) an enumeration of the class @xmath of possible sentences;

-   (ii) an enumeration of the class @xmath of possible structural
    descriptions;

-   (iii) an enumeration of the class @xmath of possible generative
    grammars;

-   (iv) specification of a function @xmath such that @xmath is the
    structural description assigned to sentence @xmath by grammar @xmath
    ;

-   (v) specification of a function @xmath such that @xmath is an
    integer associated with the grammar @xmath .

In this abstraction, a language is a set of sentences . ² ² 2 Here, the
word language is used in the E-language sense (see section 2.2 ). In
more recent work Chomsky has treated learning as a problem of learning
an I-language. Presumably these sentences represent some slight
abstraction of the acoustic stream, though Chomsky is not specific about
this. A grammar is a set of parameters for a process that generates
sentences; thus, a grammar @xmath defines a language @xmath , the set of
all sentences that can be generated under the parameter setting @xmath .
By structural description Chomsky is collectively referring to
information that reflects the derivation of a sentence under a grammar,
such as sentence meaning and syntactic structure. This “side
information” might be extractable by the learner from the learning
environment and used to disambiguate between grammars, by means of the
function @xmath . The function @xmath is a preference function over
grammars, reflecting some arbitrary criterion such as simplicity.

Chomsky imagines the following learning strategy: a teacher with target
grammar @xmath presents a set of sentences drawn from @xmath to a child,
along with their structural descriptions under @xmath ; the child
enumerates grammars in order of their image under @xmath , and selects
the first grammar consistent with the input. Thus, the child’s grammar
is a complex function of the input and the class of grammars available
to the child. Having learned a grammar, the child can use it to
determine whether a sentence is in her language, and if it is, assign it
a structural description.

In this framework Chomsky is implicitly assuming that learning takes
place from positive examples – sample sentences from the target
language. This is consistent with Brown and Hanlon’s [ Brown70 ]
assessment (see also Marcus [ Marcus93 ] ) that children receive no
negative evidence , a term that refers to both feedback from the teacher
to the learner and negative examples – sentences labeled as outside of
the target language. But this assumption introduces an apparent paradox,
since it can be shown in Chomsky’s framework that under reasonable
definitions of learnability, most classes of formal languages that are
similar to human languages are not learnable from positive examples
alone. Restricting attention to the input, one way out of this paradox
is to assume the learner has access to side information, such as
“meaning”, culled from the extralinguistic environment or derived
independently from the speech stream. This is consistent with what is
known, but from a scientific standpoint it is important to explore the
possibility that such side information plays a limited role in the
learning process.

#### 2.3.1 Positive and Negative Examples and Restricted Language
Classes

Gold [ Gold67 ] presents a framework for the study of the induction of
formal languages that is very similar to Chomsky’s, but allows for
negative examples. There it is assumed that examples (labeled positive
or negative ) are presented to the learner in a felicitous sequence,
such that all possible examples are eventually presented. After each
example the learner names a language. If there is a learning strategy
that guarantees that for any target language, the learner will
eventually name the target language and never again change its
hypothesis, then the class of languages the learner is choosing among is
identifiable in the limit . It is possible to place strong bounds on
what classes of languages are identifiable in the limit from positive
examples alone [ Angluin80 ] , even assuming a preference ordering on
languages like Chomsky’s @xmath function. Gold proved that many
linguistically relevant classes of languages, such as the regular and
context-free languages, are identifiable from both positive and negative
examples but not from positive examples alone.

It is not surprising that powerful classes of languages are not
identifiable from positive examples alone. Any learning algorithm that
guesses a language that is a superset of the target will never receive
correcting evidence; this is especially relevant when the possibility of
noise (input outside of the target language) is taken into account. But
more fundamentally, for powerful classes of languages there are simply
too many languages consistent with any set of data. Nevertheless, many
restricted classes of languages can be identified from positive data
alone. This is the case, for instance, if every language contains a
sentence that is unique to that language. Some have proposed that the
class of grammars that children consider is highly restricted, with
particular properties that render it identifiable (see Berwick [
Berwick85 ] and Wexler and Culicover [ Wexler80 ] for discussion). This
possibility has generally been raised in the context of syntax.
Regardless of whether it holds, other parts of language, such as the
lexicon, are not so limited. For this reason, it is difficult to
construct linguistically plausible classes of grammars that are
unambiguous with respect to natural input. As an example, most sentences
are logically decomposable into words, but there also exist idiomatic
phrases that must be memorized. Given the two possibilities, it seems
that a child could account for any sentence as either following from
parts or being a lengthy idiom. To rule out the second possibility while
still permitting rote-memorized passages is difficult, and leads to
baroque and unwieldy theories of language. Any natural class of grammars
must allow for both possibilities, and hence arbitrary ambiguity.

The fact that most powerful classes of formal languages are not
identifiable in the limit from positive examples still leaves a variety
of possible outs for human language acquisition. One is that the child
has access to a generous source of negative examples. Many have
contested Brown and Hanlon (see Sokolov and Snow [ Sokolov94 ] for
review), and suggested that in fact implicit and explicit negative
evidence does appear in the input children receive. Unfortunately,
evidence for significant amounts of feedback is tenuous (it is not clear
how much is present, or of what sort) and there is little evidence that
children rely on it; some cultures do not even direct speech at
pre-linguistic infants [ Lieven94 ] . For this reason, although it is
possible that children make use of negative evidence, it appears more
promising to look for alternative explanations of learnability.

#### 2.3.2 Side Information

Chomsky allows that the learner may have access to structural
descriptions as well as sentences. More generally, it is possible that
side information extracted from beyond the speech stream or derived
independently from the speech stream could be used to disambiguate
between grammars, if the side information reflects properties of the
derivation of input sentences under the target grammar. For example,
Gleitman [ Gleitman90 ] suggests that syntactic parse trees can be
reconstructed from prosodic information alone. Perhaps more plausibly,
the actions taking place around a child may suggest various possible
“meanings” for the sentences the child is hearing. This in turn could
provide the child with information about the words in the sentences it
is hearing, as well as the manner in which the words are composed.

Providing the learner with linguistically structured information like
syntactic trees or semantic formulae can trivialize the learning
process, by making the grammar explicit in the input. Some recent papers
argue that there are powerful classes of languages identifiable from
positive data alone [ Kanazawa94 , Sakakibara92 , Shinohara90 ] ; these
learnability proofs assume access to structural descriptions in the
input. Sakakibara [ Sakakibara92 ] , for example, has shown that a
significant subset of context-free languages (those generated by
reversible context-free grammars) are identifiable from positive data,
if the example sentences come structured into unlabeled derivation
trees. Similarly, various algorithms have been constructed that use
artificial semantic representations to aid the acquisition of syntax [
Siskind92 , Siskind94 ] and the phonological lexicon [ deMarcken94b ] .
Indeed, much work on syntactic acquisition has assumed that the thematic
roles of noun phrases are known to the learner [ Gibson94 ] . Finally,
it has been shown that children do not learn much, if anything, from
sound patterns in isolation [ Sachs72 , Snow76 ] ; some environmental
clues are probably necessary for learning.

Despite the fact that it provides an easy way around troublesome
learning problems, there are a number of arguments against relying on
side information to explain language learnability:

-   There is only shaky evidence as to what side information is
    available to children, and no conclusive evidence that children make
    use of it in learning (other than the uncontested fact that meaning
    is not learned from sound alone).

-   It may be that significant learning needs to have taken place before
    side information becomes useful. For example, it seems unlikely that
    children pair sounds to extralinguistic events before they are
    capable of at least rudimentary segmentation of the sound stream.

-   It is not clear how much extralinguistic information is necessary
    for learning language. Thus, there is a substantial risk that we
    will incorrectly attribute all that we do not understand to magic in
    extralinguistic processing mechanisms.

-   The use of side information as an aid to language learning falls out
    naturally in some learning frameworks, and need not receive a
    special role in the learning model. See section 4.4.3 for further
    discussion.

-   There are many engineering tasks that demand learning about language
    from speech or text alone, such as the automated construction of
    automatic speech recognition systems.

To summarize, it is possible and even likely that children use other
information for learning than just the teacher’s speech signal. Even in
the speech stream, it is quite possible that occasional clues like pause
duration, accent and stress are used by the child in addition to the
sentence-like properties of the signal. However, given that we do not
know the extent that children rely on such information, it is important
to make as few assumptions as possible and to determine lower bounds on
the amount of side information that is necessary for learning language.

### 2.4 Conclusions

This chapter has surveyed the problem of language acquisition,
describing the evidence available to the learner and the obligations of
the learning mechanism. In doing so, it has promoted certain conditions
on theories of acquisition, in particular testability. Two statements
that have been made need reemphasis, as they motivate the focus of the
remainder of this document. The first, from section 2.2.3 , is that a
theory of acquisition should be built up from the evidence available to
the learner, because this guards against unjustified (and quite possibly
incorrect) assumptions. The second, from section 2.3 , is that the only
evidence that is known to be available to the learning mechanism, at
least during early stages of acquisition, is the speech signal. As
discussed below, these two facts determine the most natural starting
point for a theory of acquisition (the phonological lexicon) and the
fundamental challenge to acquisition (the unsupervised nature of the
problem).

#### 2.4.1 The Phonological Lexicon

The acquisition of the phonological lexicon is a natural starting point
for a complete theory of acquisition. This is the problem of mapping
from continuous speech to a discrete lexicon of phonological
representations, perhaps for English including words like /.0pt.0pt/ (
the ) and /k.0ptt/ ( caught ) and morphemes like /.0pt.0pt/ ( -ing ). A
theory of this process must predict the acquisition of parameters that
enable a new speech signal to be segmented into a sequence of these
representations. There are several justifications for the primacy of
this task:

-   The lexicon is close to the speech signal, so it seems likely that
    theories of lexical acquisition would rely on fewer assumptions
    about the nature of the input, and data is readily available for
    testing purposes.

-   The phonological lexicon is a natural foundation for other
    acquisition processes. All the work summarized in figure 2.1 assumes
    the existence of a mechanism that can map from an acoustic signal to
    a sequence of morpheme identifiers, in particular identifiers that
    can be used as attachment points for syntactic and semantic
    information.

-   Given that the acquisition of syntax and semantics is likely to be
    dependent on at least a rudimentary understanding of lexical
    parameters, it seems probable that at least the early stages of
    phonological acquisition occur without reference to extralinguistic
    information, ³ ³ 3 Some have argued that the acquisition of the
    phonological lexicon is dependent on knowledge of stress and
    intonational patterns [ Cutler94 , Jusczyk93 , Jusczyk94 ] . and
    consequently fewer potentially incorrect assumptions have to be made
    about extralinguistic processing mechanisms.

-   Even if assumptions must be made about the nature of acoustic
    processing, the use of (unsegmented) written text as a substitute
    input does not alter many of the fundamental aspects of the learning
    problem.

-   Although humans’ phonological lexicons are not directly observable,
    the plausibility of a learned lexicon can be judged on the basis of
    its predictions about pause (or space) placement and whether there
    is a natural correspondence between parameters and what are
    considered roots and affixes in standard dictionaries. Thus,
    theories of lexical acquisition can be objectively evaluated.

-   The lexicon accounts for a large portion of the total variability in
    language. Therefore any viable theory of lexical acquisition is a
    significant contribution to a complete theory of language
    acquisition.

-   Very few theories have been proposed that attempt to explain the
    acquisition of the lexicon from speech-like input; it is a
    fundamental topic that remains mostly unexplored.

These facts motivate the emphasis of chapters 4 and 5 , which formulate
representations and algorithms for the induction of the phonological
lexicon.

#### 2.4.2 Underdetermined Parameters and Unsupervised Learning

The fact that language (in the E-language sense) is a mapping between
sound and meaning would seem to imply that the learning problem is
fundamentally one of choosing the grammar that best reproduces the
mapping of the target language. In such a case the actual parameter
values that are hypothesized by the learning mechanism are of little
concern; only their collective performance matters. Unfortunately, the
principal challenge to theories of acquisition is that the choice of
parameter values is extremely important, but underdetermined by the
evidence available to the learner.

There are two reasons why the choice of parameter values is a
fundamental issue. First, different speakers of the same language
generalize consistently, which is explained only if they have similar
parameter settings; this similarity is not predicted from the evidence
available to the learner, since this evidence varies and any finite
sample is consistent with many grammars. Second, many underdetermined
layers of representation separate sound and meaning, so at least the
early stages of learning must be performed on the basis of the speech
signal alone, which has been argued to contain few explicit clues about
the source grammar. These stages must therefore produce parameter values
that are consistent with the mapping even though they have no access to
it.

In arguing that the choice of parameters values is important, and that
language is learned from signals that provide few explicit clues about
the source grammar, we are concluding that language acquisition involves
unsupervised learning. The term unsupervised learning is generally
applied to problems where the goal is to identify structure that is not
evident on the surface of the input. In the case of language acquisition
this structure can be thought of as the parameters. Scientists
interested in formulating a theory of child language acquisition are
faced with a doubly-difficult task. Not only must they propose an
unsupervised learning mechanism that can acquire a grammar that accounts
for the evidence and generalizes to new sound-meaning pairs, but this
mechanism must also acquire the same I-language that a child would
attain in the same circumstances. The nature of this I-language can be
partially deduced by experiments performed on adult speakers’ generation
and interpretation mechanisms– this has been the primary goal of modern
linguistics.

The next chapter presents a particular framework for unsupervised
learning, and explains various conditions that must be met for learning
mechanisms based on the framework to acquire grammars that accord with
human performance.

## Chapter 3 Stochastic Grammars, Model Selection and Language
Acquisition

In the previous chapter it was shown that during language acquisition a
single grammar must be selected from a set of many that are consistent
with the input signal; the lack of any explicit evidence favoring one
over another is one of the fundamental reasons language acquisition is a
difficult problem. Here it is shown that if grammars are given
stochastic interpretations, those grammars under which the input is
typical can be favored over those under which it is unusual. This
evaluation metric favors linguistically plausible grammars, and can be
justified by the statistical estimation technique of Bayesian inference
. Although Bayesian inference has a number of advantages over competing
learning frameworks, there are various subtleties involved in its
application that largely determine whether it will produce the correct
target grammar. The most important of these are the manner in which
stochastic interpretations are tied to linguistic reality, and the
manner in which generalization takes place from a small amount of
evidence to a grammar that explains unseen data. Discussions of these
two topics form the bulk of this chapter.

In the Bayesian inference framework, the language learning problem can
be expressed as follows: through some process hidden to the learner a
target grammar @xmath is chosen from a class @xmath . Various utterances
@xmath are generated in a manner that depends on the target grammar, and
this evidence is presented to the learner, who must select a single
hypothesis grammar from among the possibilities, presumably the one that
was most likely to have generated the evidence. If the learner has
access to two fundamental pieces of information, the prior probability
distribution @xmath of the grammar @xmath being selected, and the
conditional probability distribution @xmath of the evidence @xmath being
generated given that the grammar @xmath was selected, then there is a
principled way for the learner to choose a hypothesis. Bayes’ formula, a
rewriting of the definition of conditional probability, is a
mathematically sound expression of the posterior probability of a
grammar @xmath given evidence @xmath :

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

The value @xmath can be interpreted as the proper degree of belief in a
grammar @xmath after observing evidence @xmath , given an initial belief
@xmath . If at the conclusion of the presentation of evidence the
learner hypothesizes the grammar in which she has the highest belief,
then the hypothesis grammar @xmath is determined by

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

Equation 3.2 includes most of the important components of a formal
theory of language acquisition. The hypothesis class @xmath is the class
of all grammars the learner is capable of representing. The sequence
@xmath is the data available to the learner. The maximization over
@xmath can be thought of as a search the learning mechanism performs for
the best grammar in @xmath given the input @xmath . @xmath is the
learner’s default preference for certain grammars over others. Finally,
@xmath captures the relation between grammars and evidence. In a
complete theory of language acquisition, each of these components must
be explicitly defined. For expository convenience we will generally
assume that utterances are produced relatively independently of one
another, so that the conditional probability @xmath can be expressed in
a factored form @xmath .

### 3.1 Stochastic Language Models

With respect to language acquisition, the principal advantage of the
Bayesian framework over those of Chomsky (section 2.3 ) and Gold
(section 2.3.1 ) is that it evaluates grammars with respect to a graded
judgment of the typicality of the evidence. A simple example illustrates
this. Suppose a learner choosing over the class of finite context-free
grammars is given input @xmath , @xmath , @xmath , @xmath . Consider two
grammars, both consistent with this evidence: @xmath and @xmath . Which
is the prefered one? The intuitive answer is the first, because it
explains better why the observed evidence conforms to the pattern @xmath
. This fact can be captured naturally in the Bayesian framework, if
grammars are given a probabilistic interpretation. In particular,
compare the following two stochastic context-free grammars (SCFGs [
Baker79 , Jelinek90 ] ), where the choice of nonterminal expansion is
governed by probabilities:

  Grammar 1            
  ----------- -------- --------
  @xmath      @xmath   (1)
  @xmath      @xmath   @xmath
              @xmath   @xmath

  Grammar 2            
  ----------- -------- --------
  @xmath      @xmath   @xmath
              @xmath   @xmath
              @xmath   @xmath

The probability of the sentence @xmath under Grammar 1 is @xmath . Under
Grammar 2 there are two possible derivations of the sentence, each with
probability @xmath , for a combined probability of @xmath : aba is
substantially more likely under Grammar 1. The particular evidence
@xmath , @xmath , @xmath , @xmath is of course unlikely under both
grammars, but it is much more probable under the first one: @xmath . So
long as the prior probabilities of the two grammars are comparable,
equation 3.1 gives us @xmath , exactly in line with the intuition that
the first grammar is to be prefered. In learning frameworks that do not
allow for such graded judgments of “grammaticality”, heuristics (such as
the Subset Principle [ Angluin80 , Berwick85 ] ) must be introduced to
favor Grammar 1 over Grammar 2.

Generative grammars with probabilistic interpretations (in other words,
grammars that implicitly or explicitly define @xmath ) are commonly
called stochastic language models . The discriminatory power of
stochastic language models comes at a steep price. Unless probabilities
are computed arbitrarily, grammars must include extra parameters (such
as the expansion probabilities in the above example) that define the
exact probability of each utterance; the estimation of these extra
parameters presumably complicates the learning problem. More
fundamentally, stochastic language models burden the grammar with the
task of specifying the probability of utterances, which is decidedly
counterintuitive given that the source of utterances lies outside of
language altogether: the sentence please remove this egret from my
esophagus is undoubtedly rare in English, but not because of linguistic
parameters; the frequency that it occurs is principally determined by
the circumstances of life. This issue is one of the reasons why many
researchers have denied the appropriateness of stochastic language
models. But the fact that the grammar is not the principal cause of
frequency variation does not mean that stochastic extensions to
traditional grammars cannot be valuable aids to learning. In particular,
because a stochastic grammar’s ability to assign high probability to
evidence can be tied to the quality of the (non-stochastic) fit of the
grammar to that evidence, statistical measures such as equation 3.1 can
discriminate between multiple consistent grammars without relying on
extralinguistic evidence like utterance meanings. This is important in
the early stages of learning when such information may not be available
to the learner (or the learner may not know enough to make use of the
information).

#### 3.1.1 Typicality and Linguistic Plausibility

Under equation 3.1 , a stochastic English grammar that faithfully
approximates the distribution of English sentences should be a better
model for English input than a French grammar under which the input is
highly atypical. In this way statistical properties of the grammar serve
as an alternative to extralinguistic evidence (that would be in conflict
with the French grammar for different reasons). For equation 3.1 to be a
successful evaluation metric, however, statistical properties of
language models must mirror psychological reality: were a French
stochastic grammar to predict English-like output with high probability
(maybe by predicting frequent, pernicious misspellings) then the wrong
grammar could be favored. Thus, the important question is: given
evidence @xmath produced from a (non-stochastic, teacher’s) grammar
@xmath , does the stochastic grammar that maximizes the likelihood of
@xmath have the same core (non-stochastic) structure as @xmath ? ¹ ¹ 1
Note that as more and more extralinguistic evidence that constrains
derivations becomes available to the learner the answer tends towards
yes, because regardless of its stochastic nature a grammar with the
wrong underlying structure will be inconsistent with the input. The
answer, discussed at length in de Marcken [ deMarcken95b ] , depends
crucially on the way that the stochastic properties of language models
are tied to linguistic structure.

A natural way to estimate stochastic parameters for a language model is
to find the parameters that maximize the likelihood of the observed
evidence; this puts each grammar in its best possible light with respect
to equation 3.1 . Empirical tests [ Carroll92 , deMarcken95b , Pereira92
] using various naive classes of stochastic grammars indicate that the
stochastic grammars that maximize the probability of linguistic evidence
do not in general have “linguistically plausible” structure. For
example, although Grammar 3 is a closer approximation of how sentences
are generated in English, both of the stochastic context-free grammars
below perfectly account for the distribution of evidence on the left:

+-----------------------+-----------------------+-----------------------+
| The Evidence          | Grammar 3             | Grammar 4             |
+=======================+=======================+=======================+
|   --------            |   ---- -----          |   ---- ------         |
| ------------ -------- | ------------ -------- | ------------ -------- |
|   Pron V              |   S    @xm            |   S    @xma           |
| erb            @xmath | ath Pron VP    @xmath | th Pron Verb   @xmath |
|   Pron V              |   VP   @xm            |        @xma           |
| erb Noun       @xmath | ath Verb       @xmath | th Pron NP     @xmath |
|   Pron V              |        @xm            |   NP   @xma           |
| erb Det Noun   @xmath | ath Verb NP    @xmath | th VP Noun     @xmath |
|   --------            |   NP   @xm            |   VP   @xma           |
| ------------ -------- | ath Noun       @xmath | th Verb        @xmath |
|                       |        @xm            |        @xma           |
|                       | ath Det Noun   @xmath | th Verb Det    @xmath |
|                       |   ---- -----          |   ---- ------         |
|                       | ------------ -------- | ------------ -------- |
+-----------------------+-----------------------+-----------------------+

These simple stochastic grammars, however, do not make significant use
of the mechanisms of language in their definition of the conditional
probability @xmath ; for example, they do not take advantage of the
agreement relations that commonly exist between pairs of elements in a
common phrase. In a more linguistically sophisticated class of
stochastic grammars, the agreement relation that exists between
determiners and nouns in English might be incorporated into Grammar 3.
This extra constraint would enable a better statistical fit between the
stochastic grammar and English evidence. For example, if the grammar
contains the following co-occurrence information on determiner-noun
agreement

  NP @xmath Det Noun               
  -------------------- ----------- -------------
  determiner type      noun type   probability
  definite             singular    @xmath
  indefinite           singular    @xmath
  definite             plural      @xmath
  indefinite           plural      @xmath

then it will assign higher probability to English evidence than one that
naively wastes probability on the indefinite-determiner-plural-noun
possibility. Since under Grammar 4 determiners and nouns are not in the
proper structural relation to be constrained by agreement, the extra
stochastic machinery would not aid that grammar. Of course, the
Grammar 4 could use this sort of agreement model to account for any
statistical dependency between the verb and the determiner, but given
the way English is produced, there is no reason to believe that a strong
dependency exists there. This is one example of how, as stochastic
models are tied to linguistic mechanisms, they increasingly favor
linguistically plausible grammars.

One could argue in this example that the stochastic agreement model is
merely playing the same role that a traditional, non-stochastic
mechanism would. However this is a misinterpretation. It is true that a
mechanism that merely ruled out the possibility of indefinite/plural
pairs would model English almost as effectively as the stochastic
agreement model (though noise and the occasional ungrammatical sentence
might pose a problem). But the real issue is whether agreement would be
learned at all without the stochastic interpretation. Since English
evidence is “grammatical” whether or not an English grammar incorporates
the agreement restriction, there is no obvious incentive to acquire this
information (determiner-noun agreement is not a necessary component of a
grammar). In contrast, in the Bayesian inference framework there is an
incentive to understand agreement, because it enables the learner to
better predict the input @xmath . In fact, the statistical nature of the
learning problem gives the learner an incentive to acquire as much
knowledge of the target language as possible, since a stochastic grammar
that incorporates such knowledge is more likely to assign a high
probability to @xmath . ² ² 2 This can be argued more formally by
assuming that the utterances the language learner receives are produced
independently, each in a manner that depends not only on the source
grammar but also on other hidden information such as the teacher’s
thoughts. Thus, as far as the learner is concerned, @xmath is produced
piecemeal by a stochastic process with approximate distribution @xmath .
(This is not to imply that the teacher necessarily uses a stochastic
grammar; here the uncertainty in @xmath is principally due to the
learner’s ignorance of the input to the language mechanism.) If the
learner’s stochastic language model is also factored over individual
utterances ( @xmath ), then it can easily be shown that as the number of
sample utterances grows, @xmath is maximized when the learner’s grammar
is chosen to minimize the Kullback-Leibler distance @xmath between the
distributions @xmath and @xmath , where the Kullback-Leibler distance is
defined by

@xmath

It is possible [ Berger96 , DellaPietra95 ] (and indeed effective) to
construct stochastic language models by defining @xmath to be the
least-committal (maximum-entropy) distribution consistent with known
properties of the target language distribution @xmath . Using this class
of models, as more properties of the target language are incorporated
into @xmath , the Kullback-Leibler distance between @xmath and @xmath
decreases. In this sense, the grammar with the greatest chance of being
selected by equation 3.2 , ignoring for now the prior term, is the one
that incorporates the most knowledge of the target language.

All of these arguments rely on stochastic language models being defined
in such a way that their statistical modeling power is greatest when the
linguistic structure of the learner’s grammar is naturally aligned with
the linguistic structure of the evidence. In the above example, for
instance, the reason the linguistically plausible grammar is favored is
because it brings the stochastic agreement model to bear on a regularity
(the determiner-noun co-occurrence pattern), whereas under the
linguistically implausible grammar this mechanism is wasted.
Fortunately, language is not entirely uniform, so stochastic models
tailored for certain phenomena (say, explaining morphological agreement)
are unlikely to function well when applied to other phenomena
(explaining phonetic assimilation). Thus, the more finely tuned
stochastic models are to their expected role, the more likely Bayesian
inference is to converge to desired grammars. Of course, if a regularity
exists in the data but no statistical mechanism is built into the class
of language models to account for it, then there is a great risk that
some other (inappropriate) mechanism will be coopted to explain it,
confusing the estimation of whatever linguistic parameters that
mechanism was meant to be used for. This is a very important practical
matter: language models that offer only a single mechanism to explain
statistical regularities (such as SCFGs) will necessarily end up using
that mechanism to account for all regularities. The greatest risk is
that regularities that are not due to language but to the surrounding
environment that influences language will end up being modeled by
linguistic parameters; this is the subject of the next section.

#### 3.1.2 Linguistic and Extralinguistic Sources of Regularity

In Bayesian inference, a stochastic grammar fares well if it assigns
high probability to evidence produced by the target grammar. This is
accomplished by specifying a distribution that reproduces the
regularities of the target language– properties that are generally true
of signals produced by the target grammar but not of all possible
signals. Regularity in the input arises from two sources. One is
language; examples of linguistic sources of regularity include words,
agreement, syllable structure, syntax, and in general any mechanism or
parameter that reduces the space of possible utterances in a language or
favors some over others. These are the regularities that the learner is
interested in modeling, since in doing so the learner will hopefully
acquire the correct linguistic parameters of the target language.
Unfortunately, there is another source of regularity in the evidence
available to the learner, and that is the “control signal” to language–
the outside world and all of the rest of the teacher’s brain. This both
complicates and simplifies the problem of language acquisition.

Patterns in the input that are caused by mechanisms external to
language, but which appear similar to those imposed by language, can
obviously distract and mislead the learner. For instance, all learners
will hear certain phrases repeated often– examples include
conversational cliches like beg your pardon , prayers, legal idioms, and
popular quotes- whose frequency will not fall out of their linguistic
basis. One possibility the learner must entertain is that each is merely
a single (long) word. As words, the statistical regularity of the sounds
within these phrases is explained, and thus there is a motivation in the
stochastic framework for placing all passages which occur with unusual
frequency in the lexicon, regardless of whether they are linguistically
interesting. These problems can be partially alleviated by introducing
extra parameters into language models that serve only to capture
extralinguistic regularity; this is a principal motivation for the class
of language models introduced in chapter 4 .

More problematic are cases where extralinguistic regularities cross
linguistic boundaries. Consider the potential consequences of evidence
that can be bisected into a set of sentences involving John and Mary,
and another set involving Alice and Bob. In the first case there might
be many sentences of the form John verb Mary and in the second of the
form Alice verb Bob . To a learner with no access to sentence meanings,
there might appear to be an agreement phenomena between the first and
last positions in the sentence (that could have been imposed by the
language faculty). Since languages do not generally exhibit agreement
between subject and object positions, the learner might be led to
suppose a different structure than subject-verb-object (perhaps treating
Bob and Mary as main verbs rather than direct objects). Fortunately,
given carefully constructed classes of stochastic grammars and
sufficient evidence such pernicious examples are rare. Furthermore, as
extralinguistic evidence becomes available it can be used to separate
regularities imposed by the language faculty from external regularities.

The John-Mary-Alice-Bob example above is unusual: because ideas are
generally mapped to language in a compositional fashion, regularities
due to extralinguistic causes often (indirectly) provide evidence about
linguistic structure. Take for example the phrases walked the mangy dog
, bought a new car and ate a red apple . Each is more likely to occur
than arbitrary verb-determiner-adjective-noun sequences, because each
reflects natural associations of actions and modifiers with objects. The
fact that all of these associations take the same form (adjectives
attached to the left of nouns and noun phrases attached to the right of
verbs) suggests that common syntactic mechanisms are being used to
capture semantic relations. Thus, even nonlinguistic regularities are
good indicators of underlying linguistic structure. This fact is one of
the primary reasons that unsupervised learning schemes can be successful
at elucidating linguistic structure.

Extralinguistic patterns have been the downfall of many computational
theories of language acquisition, that have modeled them at the expense
of linguistic ones (see for example Olivier [ Olivier68 ] and Cartwright
and Brent [ Cartwright94 ] ). In chapter 4 a representation for language
is presented that does not prevent extralinguistic patterns from making
their way into the grammar, but does ensure that they do not preclude
desired parameters.

### 3.2 Generalization, Model Selection and the Prior

It was argued informally that the grammar @xmath is a better hypothesis
than @xmath for the input @xmath , @xmath , @xmath , @xmath , because
under it the input is more typical. On this measure the grammar @xmath
is better yet. Nevertheless, our intuition is that this grammar is an
undesirable choice, because it merely encodes the observations and is
unlikely to generalize to other sentences from the target language. In
language acquisition, where only a very small sample of the target
language is available to the learner, generalization from available
evidence to a grammar that also explains other data is a key issue. This
is a problem of model selection : which of many models consistent with
the data is best? In Bayesian inference, this question is answered by
equation 3.2 , which depends on the prior probability distribution
@xmath . Thus, the prior can be used to manipulate generalization
performance. However, Wolpert and others [ Schaffer94 , Wolpert95 ] have
shown that unless assumptions are made about the learning problem, no
generalization strategy (and hence no prior) performs better than any
other. In this section various properties of grammars and the language
acquisition problem are used to motivate a prior that favors simple
grammars over complex ones, where simplicity is defined syntactically.

By evolutionary necessity different speakers, exposed to different small
samples of a single target language, must each with high probability
converge to a language very close to the target language. With suitable
formalization it can be shown that for this to be possible, the class of
hypothesis languages must be heavily constrained; for example, in the
PAC learning framework [ Valiant84 ] it can be shown that the
VC-dimension of the hypothesis class is bounded by the number of samples
available to the learner [ Ehrenfeucht89 ] , up to a factor that depends
on the allowable error rate. ³ ³ 3 The VC-dimension of a set of
functions is, roughly speaking, a measure of the effective coverage of
the set [ Vapnik71 ] . For a set of indicator functions @xmath it is
defined to be the size of the largest set of elements that can be
labeled in all possible ways by functions in @xmath . This definition
can be extended to measure the VC-dimension of functions with arbitrary
ranges, such as probability distributions like @xmath . This means that
the complexity ⁴ ⁴ 4 Here the word complexity is used with no special
technical connotations. of the class of grammars that can be entertained
by the learner is inherently constrained by the amount of data available
for parameter estimation. Perhaps surprisingly, given this result, there
does not seem to be an upper bound on the number or complexity of
individual languages– new words can always be added to an existing
language, for example. One escape from this apparent paradox is for the
learner to adjust the hypothesis class of grammars to reflect the amount
of evidence available for estimation.

#### 3.2.1 Structural Risk Minimization

In the Bayesian inference framework, where the language learner attempts
to optimize a stochastic language model @xmath , generalization
performance can be measured by the divergence of this conditional
distribution from the “true” teacher’s distribution over evidence,
@xmath ; this divergence is computed as an expected value over all
utterances, not just the sample the learner is exposed to. Conceptually,
generalization error arises from two sources. The first is the choice of
the hypothesis class and the fidelity of its members to the true
distribution @xmath . If the hypothesis class is too restrictive even
the best possible grammar in it may be a poor approximation to the true
distribution. The second is the possibility that the learner will choose
incorrectly from among the members of the hypothesis class; the higher
the ratio of the VC-dimension of the hypothesis class to the amount of
evidence, the more likely the learner is to select a grammar that
generalizes more poorly than is necessary [ Vapnik82 ] (given sufficient
evidence for a given VC-dimension, any function consistent with the
evidence will generalize well [ Kearns94 ] ).

Vapnik [ Vapnik82 ] advocates the structural risk minimization framework
in which the learner selects a hypothesis class (from among a structural
hierarchy of classes) with VC-dimension that minimizes the sum of these
two contributions to the generalization error. In the case of language,
given a small amount of evidence the learner might restrict attention to
a small class of grammars, none of which are likely to approximate the
true function well, and as more evidence becomes available expand the
search to include a greater number of grammars, some of which will be
better approximators. Niyogi [ Niyogi95 ] explores this idea in more
mathematical detail, also with respect to language acquisition; see also
literature on the bias-variance tradeoff [ Breiman84 , Geman92 ] .

At face value structural risk minimization seems to be irrelevant to the
language acquisition problem. After all, the learner does not get to
choose what the class of human grammars is; that is defined externally
to learning altogether. This contrasts with the function approximation
tasks that motivated Vapnik, where parameters play a secondary role to
the quality of the approximation. In language acquisition as we have
defined it, the conditional probability distribution @xmath is merely an
algorithmic tool. Approximating it is useful only insofar as the members
of the hypothesis class serve to identify human grammars, and this
precludes artificially simplifying stochastic grammars to conform to a
structural hierarchy. Fortunately, the nature of human language is such
that stochastic language models can be defined over partial parameter
sets, in such a way that a structural hierarchy of stochastic grammar
classes of increasing complexity can be defined, each identifying a
greater portion of the target grammar. For example, one might imagine
structuring grammars by the size of the lexicon. Asked to choose among
lexicons with only one word the learner might opt for the lexicon
containing the word the . Given access to more data, the learner might
select between lexicons containing ten words each. Although there is
obviously some risk that the constraint of modeling with an artificially
small parameter set will lead the learner astray (perhaps, forced to
choose the single “word” that best improves the model @xmath , selecting
howareyoutoday over the ), the expectation is that as the amount of
evidence is increased, and with it the modeling power of the grammars,
core parameters will remain constant and additional parameters will be
devoted to explaining ever less important phenomena.

#### 3.2.2 The Minimum Description Length Principle

To implement structural risk minimization on top of a class of grammars
two items must be defined: a structural hierarchy over the grammars and
a function that determines the appropriate class in the hierarchy for a
given amount of evidence. Unfortunately, this function is dependent on
the VC-dimension of each class, as well as the expected fit of each
class of grammars to the target language. Both of these quantities are
extremely difficult if not impossible to compute in practice. For this
reason, heuristic approximations must be used in place of structural
risk minimization. One effective heuristic is Rissanen’s minimum
description length (MDL) principle [ Rissanen78 , Rissanen89 ,
Rissanen91 ] , in which description length is used as a substitute for
informational complexity measures like the VC-dimension. The minimum
description length principle, as applied to stochastic grammars, says
that the best grammar @xmath minimizes the combined description length
of the grammar and the evidence. More formally,

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

where @xmath is the length of the shortest encoding of @xmath and @xmath
is the length of the shortest encoding of @xmath given knowledge of the
grammar @xmath . Using near-optimal coding schemes, Shannon’s source
coding theorem [ Shannon48 ] implies that @xmath can be made to closely
approach @xmath , and therefore equation 3.3 can be rewritten

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

a more intuitive formulation from the standpoint of stochastic grammars.
The duality between description lengths and probabilities is convenient.
It means, among other things, that any coding scheme for utterances can
be interpreted as a stochastic grammar, and vice versa (see section 4.3
for further discussion). It also means that if the prior probability
@xmath is defined by @xmath then equations 3.2 and 3.4 coincide. Thus,
MDL can be interpreted as a Bayesian prior that is biased against
grammars with high syntactic complexity. Rather than try to argue for
MDL from first principles, ⁵ ⁵ 5 See [ Kolmogorov65 , Li91 , Li93 ,
Rissanen78 , Rissanen89 , Rissanen91 ] for attempted justifications of
MDL and the closely related Kolmogorov complexity. Other relevant
arguments for simplicity as measured by description length include [
Berwick85 , Blumer87 , Chomsky51 , Halle61 , Solomonoff60 ] . we note
that it is merely a heuristic, but point out three important ways in
which it mimics the philosophy of the better-justified structural risk
minimization:

-   In very many cases the VC-dimension of a parameterized class of
    functions is linear or near-linear in the number of free parameters
    in the class [ Baum89 , Kearns94 , Vapnik82 ] . Given an efficient
    coding scheme, the length of a description of a set of (independent)
    parameters is linear in the number of parameters. Hence, in a
    structural hierarchy where classes consist of functions with the
    same number of free parameters, the description length of a grammar
    should be linearly related to the VC-dimension of the class it is
    in. By penalizing grammars with high description length @xmath , MDL
    therefore weighs against classes that have too high VC-dimension for
    good generalization performance.

-   With sufficient evidence, for a class of a given VC-dimension good
    generalization performance can be achieved by selecting the function
    that models the evidence best [ Vapnik71 ] ; for stochastic
    grammars, this is the one that maximizes @xmath . Hence, the @xmath
    term biases toward grammars that are likely to generalize well.

-   Assuming a nearly stationary class of stochastic grammars, to a
    first approximation the probability distribution @xmath can be
    factored over individual utterances: @xmath , which tends towards
    @xmath where @xmath is the (geometric) mean probability per
    utterance and @xmath is the number of utterances. Thus, the term
    @xmath grows linearly with the amount of evidence available to the
    learner. As it grows, so does the incentive to increase @xmath (by
    moving to a grammar from a broader class with better approximation
    properties). In this way the choice of the VC-dimension of the
    hypothesis class is made to depend on the amount of evidence
    available to the learner.

Although MDL has had successful applications in language inference, it
depends on a syntactic definition of complexity and therefore its
effectiveness is tied to the encoding scheme used for stochastic
grammars. Despite its motivations, it does not trade VC-dimension
against evidence in the theoretically optimal way, and in no way
guarantees that generalization performance is maximized: although
results vary by application [ Murphy95 ] , as is to be expected,
practical experience indicates (see [ Murphy94 , Ristad95 , Webb96 ] and
section 6.1.3 ) that MDL as commonly used tends to underestimate the
number of parameters necessary for optimum generalization. From a
Bayesian perspective this is not surprising: the @xmath prior very
heavily biases towards grammars that are improbably simple from the
linguistic perspective. Despite the fact that MDL is only a heuristic
approximation to more desirable model-selection schemes such as
structural risk minimization, it will be used in the learning schemes
presented in the remainder of this thesis, because description lengths
can be conveniently computed and manipulated.

### 3.3 Example

At this point it is worth looking at a very simple example of how the
minimum description length principle (as embodied in equation 3.3 ) can
be used for language acquisition. The example is chosen to illustrate
ideas that will be relevant in the following chapters. Let us suppose
the learner receives evidence in the form of a sequence of characters,
such as iateicecream . The grammars the learner entertains each consist
of a set of words, where each word is a sequence of characters. Thus,
one possible grammar is { i , ate , ice , cream }.

In the Bayesian inference framework, two distributions must be defined.
The first is a prior distribution over possible grammars, @xmath , and
the second is a conditional distribution over possible character
sequences @xmath . The MDL principle is more simply expressed in terms
of description length than probabilities, so for the moment let us
concentrate on coding schemes rather than distributions. Suppose that
every word in a grammar is assigned a prefix-free codeword. Then the
evidence @xmath is encoded by writing down a sequence of codewords. For
example, given the grammar

  Word       c    a     i     e     r     m     t      ice
  ---------- ---- ----- ----- ----- ----- ----- ------ ------
  Codeword   00   010   011   100   101   110   1110   1111

then the evidence iateicecream can be encoded in 30 bits as i @xmath a
@xmath t @xmath e @xmath ice @xmath c @xmath r @xmath e @xmath a @xmath
m :

  -- -------- --
     @xmath   
  -- -------- --

A coding scheme for grammars must also be specified. Suppose that all
grammars include the 26 letters of the alphabet, so they don’t need to
be explicitly encoded into grammars. The words in a grammar that are
more than one character long are encoded by writing out the codewords of
their component characters. The word ice in the above grammar, for
example, is encoded @xmath ( i @xmath c @xmath e ). There are many
details being glossed over here, such as how codewords are assigned to
words; for the time being it is more important to focus on fundamental
issues.

Given this model of language, let us compare three grammars for the
evidence themanonthemoon .

+-----------------------------------+-----------------------------------+
| (A)                               |   ---------- ---- ---             |
|                                   | - ----- ----- ----- ------ ------ |
|                                   |   Word       o                    |
|                                   |   n    t     h     e     m      a |
|                                   |   Codeword   00   0               |
|                                   | 1   100   101   110   1110   1111 |
|                                   |   ---------- ---- ---             |
|                                   | - ----- ----- ----- ------ ------ |
+-----------------------------------+-----------------------------------+
| (B)                               |   ---------- ---- ---- -----      |
|                                   | ----- ------ ------ ------ ------ |
|                                   |   Word       o    n    t          |
|                                   | he   m     t      h      e      a |
|                                   |   Codeword   00   01   100        |
|                                   |   101   1100   1101   1110   1111 |
|                                   |   ---------- ---- ---- -----      |
|                                   | ----- ------ ------ ------ ------ |
+-----------------------------------+-----------------------------------+
| (C)                               |   -----                           |
|                                   | ----- ---- ---- ----- ----- ----- |
|                                   | - ------ ------ ----------------- |
|                                   |   Wor                             |
|                                   | d       o    n    t     h     e   |
|                                   |     m      a      themanonthemoon |
|                                   |   Codeword   00   01   100        |
|                                   |   101   1100   1101   1110   1111 |
|                                   |   -----                           |
|                                   | ----- ---- ---- ----- ----- ----- |
|                                   | - ------ ------ ----------------- |
+-----------------------------------+-----------------------------------+

Each of these grammars defines a total description length for
themanonthemoon . For Grammar A, which has no words other than single
characters, this is simply the length of the best encoding of the
evidence. Grammars B and C must add to this the cost of representing
extra words in the grammar.

  ----- ---------- -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  (A)   Evidence   100 @xmath 101 @xmath 110 @xmath 1110 @xmath 1111 @xmath 01 @xmath 00 @xmath 01 @xmath 100 @xmath 101 @xmath 110 @xmath 1111 @xmath 00 @xmath 00 @xmath 01
                   ( t @xmath h @xmath e @xmath m @xmath a @xmath n @xmath o @xmath n @xmath t @xmath h @xmath e @xmath m @xmath o @xmath o @xmath n )
        Length     42 bits.
  (B)   Evidence   100 @xmath 101 @xmath 1110 @xmath 01 @xmath 00 @xmath 01 @xmath 100 @xmath 1101 @xmath 00 @xmath 00 @xmath 01 ( the @xmath m @xmath a @xmath n @xmath o @xmath n @xmath the @xmath m @xmath o @xmath o @xmath n )
        Grammar    1100 @xmath 1101 @xmath 1110 ( t @xmath h @xmath e )
        Length     40 bits.
  (C)   Evidence   1111 ( themanonthemoon )
        Grammar    100 @xmath 101 @xmath 1100 @xmath 1101 @xmath 1110 @xmath 01 @xmath 00 @xmath 01 @xmath 100 @xmath 101 @xmath 1100 @xmath 1101 @xmath 00 @xmath 00 @xmath 01
                   ( t @xmath h @xmath e @xmath m @xmath a @xmath n @xmath o @xmath n @xmath t @xmath h @xmath e @xmath m @xmath o @xmath o @xmath n )
        Length     48 bits.
  ----- ---------- -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

The minimum description length principle says that the best grammar is
the one that results in the shortest description length for the evidence
and the grammar. That is Grammar B, at 40 bits. Grammar C has a very
short description of the evidence, but at the expense of an extremely
long and overly specific grammar. Grammar A has too general a grammar
and fails to capture an important pattern in the evidence. Grammar B,
which moves the word the into the lexicon and thus saves bits every time
it is used (the codeword for the is considerably shorter than the
combined length of the codewords for t , h and e ), strikes a happy
medium. Thus, in this case the MDL principle favors the grammar with the
most linguistically appealing structure.

Notice that the coding scheme for utterances is equivalent to a
stochastic language model @xmath . In particular, to stochastically
generate an utterance @xmath under a grammar @xmath , first generate a
random sequence of bits by flipping a coin, and then use @xmath to
decode that sequence into an utterance @xmath . This is why it doesn’t
matter whether we think in terms of stochastic language models or in
terms of probability distributions.

### 3.4 The Search Procedure

In section 2.2.4 it was argued that the learning mechanism must be given
a principled foundation. In the Bayesian inference framework the
function of the learning mechanism is to find the grammar with the
maximum posterior probability; at a conceptual level, therefore, it is
entirely defined by the class of grammars, the prior probability
distribution, and the conditional probability distribution. In practice,
however, the class of grammars will be large, if not infinite,
precluding maximization via enumeration and necessitating heuristic
searches that take advantage of the qualities of specific grammar
classes.

### 3.5 Related Work

Bayesian inference and MDL each have rich histories, and have been
routinely applied to problems of language acquisition. Some of the
earliest work on the inductive inference of language was performed by
Solomonoff [ Solomonoff59 , Solomonoff60 ] , who would later play a
major role in defining the theory that motivates MDL [ Solomonoff64 ] .
In his language work the importance of penalizing complexity is already
emphasized. As far back as 1955 Chomsky wrote in The Logical Structure
of Linguistic Theory [ Chomsky55 ]

  In applying this theory to actual linguistic material, we must
  construct a grammar of the proper form… Among all grammars meeting
  this condition, we select the simplest. The measure of simplicity must
  be defined in such a way that we will be able to evaluate directly the
  simplicity of any proposed grammar… It is tempting, then, to consider
  the possibility of devising a notational system which converts
  considerations of simplicity into considerations of length.

Stochastic methods have also been applied from very early on. One of the
first demonstrations of Markov models [ Markov13 ] was an elucidation of
the dependencies between adjacent characters in the text of Pushkin’s
Eugene Onegin . Olivier [ Olivier68 ] uses stochastic models in an early
computational study of language acquisition. However, very few in the
natural language community have looked carefully at the necessary
relation between stochastic models and the problems they are applied to;
as a consequence most experiments in the unsupervised learning of
language have tended to result in parameter values that fare well on
statistical criteria, but not on linguistic ones.

### 3.6 Conclusions

This chapter has surveyed the issues surrounding the application of
Bayesian inference to the problem of unsupervised language acquisition.
This framework for statistical estimation evaluates grammars largely on
the basis of whether they explain the typicality of the evidence, and
hence can discriminate between grammars even in absence of binary
grammaticality judgments and without reference to information from
beyond the speech signal, such as sentence meanings, that may not always
be available to the learner. Various subtleties have been discussed at
length, in particular the need for certain relations to hold between the
structure of stochastic language models and the linguistic parameters
that are the desired output of the learning process. The difficult
problem of ensuring good generalization from a small amount of evidence
was used to promote a bias in the learning algorithm towards simple
grammars.

The main purpose of this chapter has been to provide an objective
function (namely, the posterior probability given a prior that is
defined in terms of description length) by which a learning algorithm
can evaluate a grammar. Neither the form of grammars nor the learning
algorithm has been specified; these are the topics of the next two
chapters. The choices there will determine whether the MDL-based
inference procedure is successful. In particular, they will determine
whether the entire learning process converges to linguistic parameters
that agree with what is known about human language and human
performance.

It is important to note that stochastic grammars and the
description-length prior are serving here as tools to aid the learning
algorithm. This chapter has not argued that language is best viewed as a
random process, or even that analogs of stochastic parameters are
present in the grammars used by adults for generation and
interpretation. However, the discussion is equally relevant to human
language acquisition as it is to engineering applications in which it is
necessary to estimate stochastic language models for use in
disambiguation and compression.

## Chapter 4 A Representation for Lexical Parameters

This chapter presents the principal innovation of this thesis, a
framework for the representation of linguistic knowledge. In it,
parameters like words are represented in the lexicon as a perturbation
of the composition of other lexical parameters. ¹ ¹ 1 In this thesis the
word lexicon refers to the store of memorized, irregular knowledge about
language. As a matter of convenience the word word will often be used to
refer to any lexical parameter, though a more proper term would be
listeme (defined by Di Sciullo and Williams [ DiSciullo87 ] as an item
that must be memorized). Listemes include morphemes, many syntactic
words, idioms, and perhaps syllables. Here even syntactic rules are
treated as part of the lexicon, if there is reason to believe that they
are memorized. Under these definitions the lexicon does not include
objects that can be derived using completely regular processes, even if
they are words in the traditional sense; see Spencer [ Spencer91 ] for
further discussion. This recursive decomposition of knowledge in the
lexicon is similar in spirit to the hierarchical phrase structures
commonly associated with sentence processing, distinguished by the fact
that at every level in the hierarchy perturbations introduce changes to
default compositional behaviors. As a theory at the computational level,
the framework abstracts from details of linguistic theory while
highlighting issues of memory organization that are central to language
acquisition. When used in conjunction with the inference framework
presented in chapter 3 , it neatly circumvents many of the potential
pitfalls of unsupervised learning raised there, such as the propensity
for the learner to model extralinguistic patterns in the signal. In this
way it is a theory of language acquisition as well as a theory of
lexical organization. The success of the theory is demonstrated through
learning algorithms and results presented in chapters 5 and 6 .

The chapter begins with an introduction to the representational
framework, culminating in a simple example in which parameters are
character sequences built by concatenating other character sequences.
This example is used as background to present various motivations for
the framework, principally from the standpoint of unsupervised learning
but also with respect to the nature of language. The issue of coding is
then explored in more depth. Finally, four instantiations of the
framework are defined in greater detail.

### 4.1 The Representational Framework

A central tenet of modern linguistic theory is that language makes
“infinite use of finite means” [ Chomsky65 , Humboldt1836 ] , or in
plainer terms, that language combines a finite set of lexical parameters
to produce an infinite variety of sentences. This chapter argues that
these lexical parameters, the primitive units of sentence processing,
are themselves built by composing parts , inside the lexicon. Thus, each
lexical parameter is constructed very much like a sentence, with idioms
built from words, words from morphemes, and so on. What distinguishes
the lexicon from the sentence processing mechanism is that the
composition occurs off-line, and more importantly, that parts combine to
produce a whole that is greater (or at least different) than the sum of
the parts. This idea is captured here by a framework for lexical
representation in which each parameter @xmath in the lexicon is
represented as the perturbation of a composition of other parameters
@xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Here the composition operator @xmath is taken to represent the same
process that combines words and other elements from the lexicon during
on-line processing. The intuition behind this representation is that
@xmath inherits the linguistic properties of its components @xmath . At
the same time the perturbations introduce changes that give @xmath a
unique identity: a word that acts exactly as the composition of its
parts could be removed from the lexicon and reconstructed on-line during
normal sentence processing. Conceptually, this framework is quite
similar to the class hierarchy of a modern programming language, where
classes can modify default behaviors that are inherited from
superclasses. The more of its properties a parameter inherits from its
components, the fewer need to be specified via perturbations.

Figure 4.1 presents several (very informal) examples that should help
convey the intended use of this abstract framework. In each case
parameters are constructed by composing several parameters and
perturbing the result. Perturbations include sound changes ( want to
becomes wanna ), changes to syntactic properties ( cat and motor are
nouns), changes to meaning (a blueberry is more than just a blue berry
and kick the bucket has nothing to do with kicking or buckets), and
changes to frequency. Frequency information is used to give a stochastic
interpretation to the lexicon during unsupervised learning of the sort
described in chapter 3 . Its use and importance will be discussed in
greater detail later. The parameters that are composed in these examples
range from phonemes and syllables to words and syntactic rules. The
definition of the composition operator dictates how parameters combine.
Ideally, the composition operator encodes most of a detailed theory of
language, explaining how phonemes and syllables come together in words
like cat and motor , how syntactic rules combine, and even how semantic
interpretations are constructed by composing words under standard
syntactic relations (as with blueberry ). Note that in most of these
examples relatively little information needs to be added via
perturbations. For example, although blueberry does mean something
different than blue berry , much of the meaning and all of the syntactic
