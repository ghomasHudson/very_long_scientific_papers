###### Contents

-    1 Introduction
    -    1.1 Disclaimer
    -    1.2 Relativistic heavy-ion collision experiments
    -    1.3 Relativistic hydrodynamic simulations
    -    1.4 Observables
-    2 Comparison between event-by-event and single-shot hydrodynamic
    simulations
    -    2.1 Definitions
        -    2.1.1 Ellipticity
        -    2.1.2 Higher order eccentricity coefficients
        -    2.1.3 Harmonic flow coefficients
        -    2.1.4 Initial-state models
        -    2.1.5 Averaging procedures for the initial profiles
    -    2.2 Eccentricities
        -    2.2.1 Centrality dependence of different ellipticities
        -    2.2.2 Ellipticities with different weight functions
        -    2.2.3 Higher order harmonics
        -    2.2.4 Eccentricity correlations
    -    2.3 Event-by-event hydrodynamics and flow fluctuations
        -    2.3.1 Correlations between participant plane, event plane,
            and reaction plane
        -    2.3.2 Centrality dependence of event and participant plane
            correlations
        -    2.3.3 Harmonic flows and their corresponding initial
            eccentricities: nonlinear hydrodynamic response
    -    2.4 Single-shot versus event-by-event hydrodynamics
        -    2.4.1 Transverse momentum spectra
        -    2.4.2 Elliptic and triangular flow
        -    2.4.3 Elliptic flow fluctuations
    -    2.5 Chapter summary
-    3 Using simultaneously measured elliptic and triangular flow to
    resolve initial condition ambiguities
    -    3.1 Introduction
    -    3.2 Justification for using single-shot simulations
    -    3.3 Setup for the simulations
    -    3.4 Transverse momentum spectra
    -    3.5 @xmath -integrated elliptic and triangular flow
    -    3.6 @xmath -differential elliptic and triangular flow
    -    3.7 Chapter summary
-    4 Hydrodynamic event-plane correlations in Pb+Pb collisions
    -    4.1 Introduction
    -    4.2 Methodology
    -    4.3 Results
-    5 Resonance decay contributions to higher-order anisotropic flow
    coefficients
    -    5.1 Chapter introduction
    -    5.2 Resonance ordering
    -    5.3 Results and discussion
    -    5.4 Chapter conclusions
-    6 Fluctuating flow angles and anisotropic flow measurements
    -    6.1 Chapter introduction
    -    6.2 Differential flows from the event-plane method and from
        two-particle correlations
    -    6.3 The effect of flow fluctuations on differential @xmath
        measures
    -    6.4 Non-factorization of flow-induced two-particle correlations
    -    6.5 Chapter summary
-    7 Sampling particles from the Cooper-Frye distribution
    -    7.1 iS and iSS overview
    -    7.2 Random variables and sampling methods
        -    7.2.1 Random variable, PDF and CDF
        -    7.2.2 Sampling 1d random variables according to the inverse
            CDF
        -    7.2.3 Direct sampling of 1d random variables according to
            their PDF: special accept-reject method
        -    7.2.4 Sampling a 1d random variable using envelope
            distribution: general accept-reject method
        -    7.2.5 Sampling 1d random variable using grouping
        -    7.2.6 Sampling efficiency
        -    7.2.7 Automatically generated stair function envelope
            distribution
        -    7.2.8 Generating genuine continuous samples
        -    7.2.9 Sampling multi-dimensional random variables
        -    7.2.10 Possible issues
    -    7.3 Sampling the particle momentum distribution
        -    7.3.1 Emission function and Cooper-Frye formula
        -    7.3.2 Spectra and flow calculations
    -    7.4 Sampling the emission function
        -    7.4.1 The purely numerical approach
        -    7.4.2 Semi-analytic approach
        -    7.4.3 Comparison between the approaches
        -    7.4.4 Other optimizations and implemented models
    -    7.5 Code verification
-    8 Miscellaneous topics
    -    8.1 Comparison between eccentricities defined with @xmath and
        @xmath weights
    -    8.2 Robust viscous hydrodynamics
    -    8.3 Time evolution of the event plane orientations
    -    8.4 Best estimator for flows using eccentricities
-    9 Summary
-    A Choice of parameters used in the simulations
-    B Table of parameters for cutting centralities
-    C Feed down contribution tables for @xmath , @xmath , @xmath ,
    @xmath , @xmath , and @xmath [ 100 ]
-    D Compiling, running, and tuning iSS
    -    D.1 Compiling
    -    D.2 Running
    -    D.3 Input and output files
    -    D.4 Parameter file
    -    D.5 Tables
-    E Introduction to iSS support classes
    -    E.1 Table class and table-function classes
    -    E.2 Classes related to random variables
    -    E.3 Parameter reader class
-    F The iEBE package
    -    F.1 How to use the package to perform multi-job calculations
    -    F.2 How to analyze generated data
    -    F.3 How to tune parameters

## Chapter 1 Introduction

### 1.1 Disclaimer

I would like to start with a quote from the book [ 1 ] :

“The aim of classical mechanics and theoretical physics is to provide
and develop a self-consistent mathematical structure which runs so
closely parallel to the development of physical phenomena that, starting
from a minimum number of hypotheses, it may be used to accurately
describe and even predict the results of all carefully controlled
experiments.”

I quote it here to as a reminder that the development of theoretical
physics is the process of improving the mathematical model behind the
material world; thus, however successful the current achievement in
theoretical physics becomes, whether it can be categorized as “the
truth” depends on how well it stands the test of time. In this thesis, I
use the words “truth”, “nature”, etc., to mean the current state-of-art
understanding of theoretical physics.

### 1.2 Relativistic heavy-ion collision experiments

“Heavy ions” are the atomic nuclei of atoms with mass number @xmath .
The relativistic heavy-ion collision program was initiated partially in
the hope to study a new type of matter, the quark-gluon plasma, which
otherwise can only be found shortly after the big bang or (possibly) in
the core of neutron starts. In this section a brief introduction is
given to make the thesis self-contained; for a more thorough motivation
and background of heavy-ion collision experiments, see for example [ 2 ]
, [ 3 ] , and [ 4 ] .

Relativistic heavy-ion collision experiments are mainly performed at the
Relativistic Heavy-Ion Collider (RHIC) at Brookhaven National Lab (BNL)
near New York and at the Large Hadron Collider (LHC) operated by
European Organization for Nuclear Research (CERN) near Geneva. In
relativistic heavy-ion collision experiments, one generates beams of
bare heavy nuclei (“heavy ions”) and accelerates them to close to the
speed of light: RHIC: @xmath ; LHC: @xmath .

The two beams moving around the accelerator ring anti-parallel to each
other are brought to collision in the so-called interaction regions.
Those interaction regions are instrumented with detectors. Both RHIC and
LHC have multiple detectors. Each detector is a complicated engineering
piece of art that is a product of, and operated and maintained by a
large collaboration of physicists and engineers (ranging from @xmath 500
to @xmath 3500 members each).

The collision systems that have been run at RHIC include proton-proton
(p+p), deuterium-gold (d+Au), copper-copper (Cu+Cu), gold-gold (Au+Au),
uranium-uranium (U+U), and most recently copper-gold (Cu+Au) collisions.
At the LHC, so far only proton-proton (p+p), lead-lead (Pb+Pb), and
proton-lead (p-Pb) have been studied. Among these nuclei, Cu, Au, Pb,
and U are heavy nuclei, and others are light ones. The light nuclei
collisions such as p+p and mixed-type collision like d+Au are often used
to “calibrate” the heavy-ion collisions. It is worth mentioning that Cu,
Au, and Pb nuclei are almost perfectly spherical while U nuclei have an
ellipsoidal shape in their ground states.

The detectors can be used to identify the species of particles, and
measure their energy and momentum. What we do not know about the
particles are their creation times and their emission locations — the
resolution of the measurement is much coarser than the actual scale of
the collision zone; each collision is, from the detector’s point of
view, point-like and instantaneous.

When the two heavy nuclei collide, their nucleons are shattered. Their
constituents, the quarks and gluons, are spilled out for a brief amount
of time and form a new type of matter: the quark gluon matter. This
matter can exist for only very short amount of time ( @xmath second)
before it “evaporates”: the quarks and gluons quickly recombine into
hadrons and fly into the detectors.

It is not only heavy-ion collision that can shatter nucleons into quarks
and gluons; proton-proton collisions collisions can also break the
proton cage to free the quarks and gluons at high enough energy: in all
such collisions the quark-gluon matter is created. However one peculiar
property of the quark-gluon matter created by heavy-ion collision is
that the created matter can actually equilibrate, meaning that the
quarks and gluons in the matter can almost reach local equilibrium,
which is another way to say that the particle number probability density
of finding quarks and gluons with given energy satisfy their
corresponding Boltzmann distributions. This thermalized medium is the
quark-gluon plasma (QGP).

The information on the out-going particles can be analyzed to form
observables, which will be compared with various theoretical results in
order to learn about the properties of the QGP.

Another concept that needs to be introduced is ‘‘centrality’’. A
collision where the two nuclei hit head-on is very different from
another collision where the two nuclei only graze each other; for this
reason, collisions are usually sub-divided into ‘‘centrality classes’’,
where each class contains collisions under similar conditions. The
conventional quantity that can be used theoretically to perform such a
division is the impact parameter, which is defined as the closest
distance between the tracks of the centers of the two nuclei. However,
as mentioned above, in heavy-ion collisions no detailed information
including impact parameter can be directly obtained, therefore in
measurements another quantity is used to define the ‘‘centrality’’ of
the collision. Let us explain the idea: it is plausible, at least
statistically, that the more central a collision is, the more particles
it will produce. Based on this observation, the number of produced
particles can be used to indicate how central a collision is.
Operationally, all collisions are ordered by the number of charged
hadrons they produce, and the rank of a collision is its ‘‘centrality’’.
For example, the top 5% of all the events in this ordered list form the
0-5% centrality class (or centrality bin). The smaller the centrality
class, the more central the collisions it contains ¹ ¹ 1 This procedure
does not work for p-p collisions, due to multiplicity fluctuations. It
only works for nuclear collisions where the monotonic increase of
multiplicity with the number of nucleons participating in the collision
overwhelms the fluctuations in individual nucleon-nucleon collisions. .

### 1.3 Relativistic hydrodynamic simulations

The evolution of the quark-gluon plasma is simulated using relativistic
hydrodynamics. There are several stages involved in this process. For an
illustration, a typical simulation is visualized in Fig. 1.1 .

The first stage is to generate the initial state for the quark-gluon
plasma, which is usually represented as an energy density profile. In
principle, there is a pre-equilibrium stage between the time of
collision and the starting time of the quark-gluon plasma, during which
the quark matter quickly evolves towards local equilibrium. As a result
of this pre-equilibrium evolution, the initial hydrodynamic energy
density profile is not the same profile as just after the collision.
Relatively little is known about this pre-equilibrium stage and its
study is new and still on-going. However, the pre-equilibrium stage is
believed to last only for a very brief of amount of time ( @xmath fm/c)
and to contribute at most minor changes to observables I studied.
Therefore, in all of my simulations, this stage is assumed to be absent,
and energy density profiles produced by models which strictly speaking
should only be applied at the beginning of the pre-equilibrium stage are
used as the final state of the pre-equilibrium stage — that is, as the
initial state of the hydrodynamic simulation. Because of the
pre-equilibrium stage, there could be collective particle motion already
before the system equilibrates; in my simulations such initial flow is
ignored.

The initial condition models used in my simulations are the Glauber
model [ 5 , 6 ] and the fKLN model [ 6 , 7 , 8 , 9 ] . Both models start
with generating the three dimensional nucleon position configurations
assuming independent draws from the Woods-Saxon density distributions
for the colliding nuclei. Next, the density distributions of these
nucleons are projected onto the plane perpendicular to the beam
direction (the transverse plane). Fig. 1.2 is an illustration for such a
projection. The big dashed circles indicate the boundaries of the two
colliding nuclei, and the smaller disks are the nucleons inside each
nucleus, identified by their color.

The Glauber model first simulates how the nucleons from one nucleus
overlap in the transverse plane with nucleons from the other nucleus.
Each overlap of a pair of nucleons gives one binary collision, producing
two wounded (participant) nucleons. If a wounded nucleon scatters again
with another nucleon from the other nucleus, it remains wounded, wounds
the other nucleon (if not already wounded), and contributes one to the
binary collision count. In Fig. 1.2 , the solid colored small disks are
the wounded nucleons. The Glauber model then assumes that the initially
produced entropy density at a given location in the transverse plane is
proportional to a linear combination of the wounded nucleon and the
binary collision densities (“two-component” model). Both the
proportionality factor and the mixing factor are fitted later by
comparing simulated observables to experimental data.

The KLN model uses the nuclear density functions from the nucleon
profiles of the two colliding nuclei to determine their local saturation
scale @xmath , which is then used to determine the unintegrated nuclear
gluon density distribution; the unintegrated gluon distributions from
the two colliding nuclei can then be used to generated the density of
the gluons produced in the collision [ 7 , 8 , 6 , 9 ] . This produced
gluon density is then assumed to be proportional to the initial entropy
density, and the proportionality factor, as well as a model parameter
@xmath used in determining @xmath from nucleon density functions, are
fitted to experimental data.

For both models, the initial entropy density profiles are translated
into energy density profiles using the s95p-PCE equation of state (EOS)
[ 11 , 12 ] . Our group uses the superMC code (modified from the rcBk
model [ 13 ] and the MC-KLN code [ 6 , 8 ] ² ² 2
http://www.aiu.ac.jp/~ynara/. ) to generate both types of initial
conditions. The actual choice of parameters varies between different
simulations and will be reported in the corresponding chapters and in
Table LABEL:tab:1 in the Appendix.

The second stage is to evolve the initial energy density profile
hydrodynamically by numerically solving the relativistic hydrodynamic
evolution differential equations. Our group solves the Israel-Stewart
second-order hydrodynamic equations [ 14 ] under the assumption of
longitudinal boost-invariance ³ ³ 3 The longitudinal boost-invariant
assumption assumes that the system is invariant under a boost in the
longitudinal (beam) direction. The experimentally measured rapidity
region where this assumption approximately holds is known as the
“plateau”; it covers roughly @xmath units of rapidity at RHIC energy and
@xmath units of rapidity at LHC energy, and these regions are the
validity regions of our simulations. In particular, our simulations can
be applied to the important “mid-rapidity” region (around zero rapidity)
where most experiments have the best sensitivities. , using the VISH2+1
code [ 15 ] . There are several tunable parameters that describe the
properties of the quark-gluon plasma, among which the most famous one is
the specific shear viscosity @xmath , the ratio of shear viscosity
@xmath to entropy density @xmath . These parameters are not well-known
theoretically and will be adjusted to experimental data. As we will see,
different initial conditions require different @xmath values to describe
the same data. This will be discussed later in this thesis. Fig. 1.3 is
an illustration of the evolution of the energy density profile from one
simulation.

The VISH2+1 code simulates the evolution of the energy density
distribution, and outputs information like flow velocity, energy
density, etc. along a constant temperature (isothermal) freeze-out
surface, whose functionality depends on the type of simulations:

1.  In a purely hydrodynamic simulation, both the quark-gluon plasma and
    the re-scattering of the emitted hadrons are simulated using
    hydrodynamics. In such an approach, the freeze-out surface is
    defined as the surface outside which hadrons cease to interact and
    reach the detectors by streaming freely.

2.  In a hybrid hydrodynamic simulation, only the quark-gluon plasma is
    simulated hydrodynamically, and the scattering of the hadrons, after
    they materialize from the quark-gluon plasma, is simulated using a
    hadron re-scattering simulator based on a transport approach. In
    such an approach, the freeze-out surface (or better “switching
    surface”) is the surface that separates the quark-gluon plasma phase
    from the hadronic phase.

In both types of simulation, the temperature of the freeze-out surface
is a tunable parameter, although for the hybrid simulations people
choose it according to the results from lattice QCD calculations [ 16 ,
17 ] .

The next stage of the simulation is to generate the momentum
distributions of particles from the freeze-out surface. The Cooper-Frye
formula (see Sec. 7.3 for details) is used to calculate the momentum
distribution of the emitted particles from the surface.

For purely hydrodynamic simulations, knowing these distributions as
continuous functions enables one to calculate many observables. However,
in real experiments, each collision only emits a limited number of
particles and calculations done using the continuous distribution
function do not allow one to study the fluctuations caused by finite
statistics. To study finite-statistics fluctuations, one can also
simulate a finite number of particles emitted from the freeze-out
surface by Monte-Carlo sampling the continuous Cooper-Frye distribution.

For hybrid simulations one must in any case simulate the emission of
finite numbers of particles, which is required by the hadron
re-scattering simulator.

There is one more subtlety: even for purely hydrodynamic simulations,
although the emitted particles are assumed to stop interacting, unstable
particles continue to decay into lighter ones before they reach the
detectors, and this process changes the momentum distributions of the
light particles. This process is called resonance decay and it needs to
be additionally computed. For hybrid simulations, the decay of unstable
particles is usually included in the hadronic re-scattering simulator
and it does not need to be computed separately.

Our group uses the iS code to calculate the continuous distributions of
emitted particles and their resonance decays, and the iSS code to
simulate the emission of a finite discrete number of particles. The
methodology used for sampling in the iSS code will be explained in
chapter Chap. 7 .

For purely hydrodynamic simulations, this is the end of the simulation
process. All of my publications are based on purely hydrodynamic
simulations, but since part of the work I have contributed is a package
for hybrid calculations, I will explain it briefly.

Once the emissions of hadrons has been successfully simulated, they can
then be passed to the hadronic re-scattering simulator, from which the
final-state momenta of the particles are produced, which can be analyzed
to generate simulated observables. Our group uses the UrQMD code [ 18 ]
to simulate the hadronic re-scattering. The particle information
generated from UrQMD is huge and to efficiently compute simulated
observables from it, a code binUtilities has been developed.

The stages involved in hydrodynamic simulations, and the corresponding
codes used by our group used are schematically summarized in Fig. 1.4 .

### 1.4 Observables

There are many interesting observables that can be studied using
hydrodynamic simulations. One type of observables that are particularly
important are the anisotropic flows @xmath , which are the harmonic
Fourier coefficients that describe the anisotropy of the particle
emission distribution in the transverse direction. Another set of
theoretically interesting quantities are the initial eccentricities
@xmath , which describe in the form of Fourier coefficients the
anisotropies of the initial density distributions. Fig. 1.5 is a
schematic illustration of the decomposition of one initial condition
into its first 4 harmonics deformations.

It is important to point out, that “anisotropy” is a vector, not a
scalar, because it carries the information for the orientation of the
deformation as well as its magnitude. Because of this, when people
report a single scalar as the “anisotropy coefficient”, they are
implicitly projecting the vector to a referencing direction and only
report its component along that direction. The choice of the referencing
direction is not unique, and we will be explicit on our choice of
references when used. For each given harmonic order, there is no
ambiguity about the direction of the deformation or the direction that
“maximizes the anisotropy”; these directions can be used to define
important planes. The plane spanned by the direction @xmath of the
anisotropic flow @xmath and the beam direction is the @xmath -th order
event plane (EP). The plane spanned by the direction @xmath of the
complex eccentricity vector @xmath (see eq. ( 2.17 ) for formal
definition) and the beam direction is the participant plane (PP). As a
quick illustration in Fig. 1.5 , the participant plane angles are marked
on the figures for each decomposed deformation. Fig. 1.2 has an
illustration of the 2nd-order participant plane angle. Precise formal
definitions will be given in each chapter when needed.

Another classification separates hydrodynamic simulations into
single-shot simulations and event-by-event simulations. The two
procedures and their differences are summarized in Fig. 1.6 .

Event-by-event simulation is de facto the standard simulation type for
heavy-ion collisions today, whose goal is to simulate each individual
collision, then statistically construct simulated observables using all
the simulated events in the same way as the experimentalists do with
measured events. In this type of simulations, the initial energy density
profile for each simulated collision is Monte-Carlo generated and
propagated through hydrodynamic simulations, and the results are
analyzed from a collection of particles summed over all simulated
events. However such simulations are extremely resource demanding, and
they have only been extensively applied in the last few years.

The single-shot simulation was dominantly used half a decade ago, when
due to technological limitations people tried to study results from
multiple collisions using only a single hydrodynamic simulation. The
underlying logic is the following: since event-by-event evolution of
many fluctuating initial conditions is expensive but most observables
are anyhow measured by summing over many events, we might as well
average over the fluctuations in the initial state and evolve only a
single, averaged and smooth initial profile, computing the observables
from the single final state.

This, unfortunately, ignores the nonlinearities in the hydrodynamic
evolution, which leads to characteristic differences between the initial
and final fluctuation distributions. I found in my work specific
limitations of the single-shot approach and thereby established the need
for event-by-event hydrodynamics unambiguously.

It is obvious that the applicability of the single-shot approach depends
on the type of observables, since by definition only observables that
are direct averages have a chance to be amenable to single-shot
hydrodynamics.

There are many ways to generate initial conditions for single-shot
calculations. The most common choice of averaging initial conditions is
to rotate the initial conditions to align their orientations @xmath
before averaging. This operation can be done for anisotropies of any
given harmonic order @xmath , and the resulting averaged profiles are
different for different @xmath . For example, aligning the second-order
harmonic deformation will result in an ellipse-shaped averaged profile
while aligning the third-order harmonics will result in a
triangular-shaped averaged profile. More details will be given
explicitly in related chapters.

The classification of hydrodynamic simulations into single-shot and
event-by-event ones is independent of their classification into pure
hydrodynamic and hybrid simulations: single-shot pure hydrodynamic
simulations dominated half a decade ago, event-by-event pure
hydrodynamic simulations are the basis of this thesis, but single-shot
hybrid simulations with a hybrid code (e.g. VISHNU [ 19 ] ) and
event-by-event hybrid simulations will be the mainstream in the future.

The event-by-event simulations and the data collection and management
processes are tedious to perform manually. To automate these processes,
we have collaborated with the QCD group at Duke University to develop
the iEBE package ⁴ ⁴ 4 https://bitbucket.org/qiu_24/iebe. which allows
one to easily set up and perform event-by-event simulations locally or
on a cluster; simulated observables can be calculated from the final
results using a simple one-line-command interface. The details of this
package will be explained in the Appendix Sec. F .

## Chapter 2 Comparison between event-by-event and single-shot
hydrodynamic simulations

This chapter focuses on the comparison of event-by-event hydrodynamic
simulations and the single-shot ones. The material is largely based on
previous work reported in [ 20 ] , but is supplemented by calculations
for nonzero shear viscosities. (The work in [ 20 ] was based on ideal
fluid dynamics.)

### 2.1 Definitions

In this section, we formally define the harmonic flow and eccentricity
coefficients and briefly describe the models used in computing the
initial entropy and energy density profiles.

#### 2.1.1 Ellipticity

The “ellipticity” @xmath of a given matter distribution in the
transverse @xmath plane is defined in terms of its @xmath -weighted
second azimuthal moment [ 21 , 22 ] ,

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

where @xmath , @xmath , and @xmath is the complex ellipticity. This
formula assumes that the origin is the center of the distribution @xmath
. In a Monte Carlo approach for generating the initial distribution
@xmath (see Sec. 2.1.4 ), this must be ensured by recentering each event
before using Eq. ( 2.1 ). By default, we characterize in Eq. ( 2.1 ) the
matter distribution by its energy density @xmath [ 23 ] . In Sec. 2.2.2
, we compare energy- and entropy-weighted ellipticities.

In Eq. ( 2.1 ), @xmath and @xmath are “reaction plane” (RP) coordinates:
the reaction plane is the @xmath plane, with @xmath pointing along the
beam and @xmath pointing along the direction of the impact parameter
@xmath between the colliding nuclei, while @xmath is perpendicular to
the reaction plane. Because of the minus sign on the right hand side of
Eq. ( 2.1 ), the angle @xmath on the left hand side of Eq. ( 2.1 )
points in the direction of the minor axis of the corresponding ellipse.
For an elliptically deformed Gaussian density distribution, this is the
direction of the largest density gradient and thus of the largest
hydrodynamic acceleration and also of the finally observed elliptic
flow. The direction of this minor axis together with the beam direction
@xmath defines the participant plane (PP). It is tilted relative to the
reaction plane by @xmath . The label “participant” is motivated by the
fact that the initial energy and entropy density distributions of the
collision fireball reflect (more or less directly, depending on the
model for secondary particle creation) the transverse distribution of
the nucleons participating in the particle production process. The
ellipticity @xmath in Eq. ( 2.1 ) is correspondingly called “participant
eccentricity” and also denoted as @xmath . ¹ ¹ 1 Traditionally @xmath is
defined in terms of the transverse density of wounded nucleons, but
since what matters for the subsequent hydrodynamic evolution is not the
distribution of wounded nucleons themselves but of the matter generated
by the wounded nucleons, we use the name @xmath for the ellipticity
characterizing the thermalized matter. It can be written as

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Here @xmath defines the “event average” over the matter distribution
@xmath in a single collision event [ 24 ] . Equivalently, the
participant eccentricity can be written as

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

where @xmath indicates the average over a rotated event with energy
density @xmath whose minor and major axes now align with @xmath and
@xmath .

The event-average @xmath is to be distinguished from the “ensemble
average” @xmath where @xmath is the total number of events and @xmath is
the event-average over the energy density @xmath in event number @xmath
. The average participant eccentricity is thus defined as

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

This differs from the mean eccentricity @xmath of the average
(recentered and rotated by @xmath ) energy density @xmath which can be
written in the following equivalent ways:

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

In contrast to ( 2.4 ), one here ensemble-averages over numerator and
denominator separately before forming the ratio.

In event-by-event simulations, the hydrodynamic forces generate in each
event an elliptic component @xmath of the anisotropic flow, which is
causally related to the specific initial ellipticity @xmath in that
event. In single-shot simulations, fluctuating initial conditions are
averaged into a single smooth initial distribution @xmath , which is
then evolved hydrodynamically and from which the mean elliptic flow
@xmath is extracted, corresponding to the mean eccentricity @xmath of
that averaged source distribution. Obviously, @xmath is a deterministic
consequence of @xmath and does not fluctuate at all; it can not be
measured experimentally. What could be measured experimentally [ 25 , 26
] is the average elliptic flow @xmath of a large ensemble of collision
events. This observable is conceptually more closely related to @xmath
than to @xmath ; for an exactly linear hydrodynamic response @xmath ,
one has @xmath [ 27 ] . We will explore the differences between @xmath
and @xmath and discuss consequences for the theoretically computed
@xmath as opposed to the measured @xmath in Secs. 2.2.1 and 2.4 .

In addition to these “participant eccentricities”, one can also define
“reaction plane eccentricities”. For a single event, the reaction plane
eccentricity @xmath is defined by

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

in terms of an event average over the (properly centered) energy density
@xmath . The so-called standard eccentricity is defined as the analogous
ratio of expectation values taken with a smooth average energy density
@xmath obtained by superimposing many events without rotating them from
the participant to the reaction plane:

  -- -------- -- -------
     @xmath      (2.7)
  -- -------- -- -------

In other words, the standard eccentricity is the mean reaction-plane
eccentricity . In contrast, the average reaction-plane eccentricity is
defined by

  -- -------- -- -------
     @xmath      (2.8)
  -- -------- -- -------

In real experiments, the actual measured quantities are @xmath , @xmath
, and @xmath (defined below) that, even if non-flow contributions could
be completely ignored, are affected by event-by-event @xmath
-fluctuations and thus differ from @xmath . @xmath can be reconstructed
from the experimental measurements with some additional assumptions [ 25
] , which on the surface look harmless but should be further tested, and
we will come back to this point in Chap. 6 . Motivated by the hypothesis
of linear hydrodynamic response, @xmath , these @xmath measures motivate
the definition of corresponding ellipticity measures [ 27 ] , the 2
@xmath and 4 @xmath order cumulants:

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

and

  -- -------- -- --------
     @xmath      (2.10)
  -- -------- -- --------

Note that the last expression involves the difference of two
positive-definite quantities, which itself does not need to be positive
definite. If fluctuations get large, the expression under the fourth
root can become negative, leaving @xmath undefined. We will see that
this can happen in the most central and the most peripheral centrality
bins.

It was shown in [ 28 ] that in the MC-Glauber model, the real and
imaginary parts of the complex ellipticity defined by Eq. ( 2.1 ), with
the wounded nucleon density as weight function on the right hand side,
both have approximately Gaussian fluctuations, with equal widths @xmath
. If this is the case, the magnitude @xmath of this ellipticity exhibits
fluctuations of Bessel-Gaussian type ² ² 2 This takes into account that
@xmath can never fluctuate to negative values. [ 29 ] , leading to the
identity [ 28 ]

  -- -------- -- --------
     @xmath      (2.11)
  -- -------- -- --------

For sufficiently large average ellipticities @xmath (i.e. sufficiently
large impact parameters), one may hope to be able to ignore the
restriction that @xmath can never fluctuate to negative values, and
correspondingly assume that @xmath exhibits Gaussian (instead of
Bessel-Gaussian) fluctuations. In this case one has [ 28 ]

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (2.12)
  -- -------- -------- -------- -- --------

from which it follows that @xmath is the arithmetic mean of @xmath and
@xmath :

  -- -------- -- --------
     @xmath      (2.13)
  -- -------- -- --------

We will use Eqs. ( 2.11 ) and ( 2.13 ) (which hold irrespective of the
fluctuation width @xmath ) in Sec. 2.2.1 , and their analogues for the
elliptic flow @xmath in Sec. 2.4.3 , to test the assumptions of
Bessel-Gaussian and Gaussian fluctuations of the event-by-event
ellipticity and elliptic-flow fluctuations using the Monte Carlo Glauber
(MC-Glauber) and Monte Carlo fKLN (MC-KLN) models.

If the hydrodynamic response were indeed linear, @xmath , and non-flow
effects could be ignored, the following identities would hold:

  -- -------- -- --------
     @xmath      (2.14)
  -- -------- -- --------

Assuming linear hydrodynamic response, one can compare the theoretically
computed @xmath with the experimentally measured @xmath or @xmath if one
normalizes the former by @xmath and the latter by @xmath or @xmath ,
respectively, calculated from the same initial-state model [ 30 , 31 ] .
In this context, the identity @xmath (which holds if the ellipticity
fluctuations are Gaussian) becomes particularly useful because it
suggests that the measured @xmath can be directly compared with a
single-shot hydrodynamic @xmath obtained from a smooth
reaction-plane-averaged initial density of ellipticity @xmath , without
any corrections for flow fluctuations. Even better, @xmath can be shown
to be completely free of two-particle non-flow contributions [ 27 , 28 ]
. These arguments have been used in [ 9 ] and provide a strong
motivation for us to test the underlying assumptions (Gaussian
ellipticity fluctuations and linear hydrodynamic elliptic flow response)
in the present work.

We close this subsection by recalling the expression for the participant
plane angle of a given event (see e.g. [ 24 ] )

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

and for its transverse area

  -- -------- -- --------
     @xmath      (2.16)
  -- -------- -- --------

Both expressions assume that the events are properly centered at the
origin.

#### 2.1.2 Higher order eccentricity coefficients

The definition ( 2.1 ) can be generalized to higher ( @xmath ) harmonic
eccentricity coefficients [ 21 , 22 ] :

  -- -------- -- --------
     @xmath      (2.17)
  -- -------- -- --------

where we call @xmath the @xmath -th order complex eccentricity.
Alternatively one can use @xmath instead of @xmath as radial weight on
the right hand side [ 10 ] :

  -- -------- -- --------
     @xmath      (2.18)
  -- -------- -- --------

Still another variant uses the entropy density @xmath instead of the
energy density @xmath as weight function:

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (2.19)
     @xmath   @xmath   @xmath      (2.20)
  -- -------- -------- -------- -- --------

We note that the @xmath -weighted eccentricity coefficients @xmath fall
off faster with increasing harmonic order @xmath than the @xmath
-weighted eccentricities @xmath (see section 8.1 ). Also, as in Eq. (
2.1 ), the minus sign in Eqs. ( 2.17 )-( 2.20 ) guarantees that, for a
Gaussian density distribution that has only @xmath -order eccentricity
@xmath , the angle @xmath points in the direction of the steepest
density gradient, and thus in the direction of the corresponding
hydrodynamically generated @xmath -order harmonic flow @xmath (see next
subsection). It can be written as @xmath and amounts to a rotation of
@xmath by @xmath . For example, if the profile is square-shaped, @xmath
points to the sides instead of its corners.

As stated before, we will use the energy density as the default weight
function; in cases of possible ambiguity, we will use the notations
@xmath , @xmath etc. to distinguish between energy- and
entropy-density-weighted eccentricity coefficients and angles.
Eccentricities @xmath without harmonic index @xmath denote ellipicities
(i.e. in the absence of @xmath , @xmath is implied).

The coefficients @xmath and angles @xmath define the eccentricies and
angles of the matter distribution in the participant plane. We note that
the participant plane angles @xmath associated with eccentricity
coefficients of different harmonic order @xmath do not, in general,
agree (see Sec. 2.3.1 ). We will not study higher harmonic
generalizations of the reaction-plane ellipticity ( 2.6 ).

#### 2.1.3 Harmonic flow coefficients

We characterize the final observed momentum distribution @xmath by
“harmonic flow coefficients” constructed in analogy to Eq. ( 2.17 ), but
without the extra minus sign:

  -- -------- -- --------
     @xmath      (2.21)
     @xmath      (2.22)
  -- -------- -- --------

where @xmath are the complex flows.

In boost-invariant hydrodynamics, they are rapidity-independent, so we
drop the argument @xmath and keep in mind that we should only compare
with midrapidity data at @xmath where the assumption of boost-invariant
longitudinal expansion is most justified. The spectra @xmath are
computed from the hydrodynamic output with the Cooper-Frye prescription
[ 32 ] along an isothermal kinetic decoupling surface whose temperature
is chosen in this chapter as @xmath MeV. Equation ( 2.21 ) defines the
@xmath -differential harmonic flow @xmath and flow angle @xmath ,
whereas Eq. ( 2.22 ) gives their @xmath -integrated values @xmath and
@xmath . The orientation of the @xmath th-order harmonics of the final
momentum distribution defines the @xmath th-order “event plane”. Again,
different harmonic flows are usually associated with
differently-oriented event planes. The first three harmonic flow
coefficients are the directed flow ( @xmath ), elliptic flow ( @xmath ),
and triangular flow ( @xmath ).

#### 2.1.4 Initial-state models

We use Monte Carlo versions [ 6 , 9 ] of the Glauber [ 5 ] and fKLN [ 8
] models to generate fluctuating initial conditions for the entropy
density in @xmath GeV Au+Au collisions. For the MC-Glauber model we
assume a two-component (soft+hard) model with a small hard fraction (
@xmath [ 6 ] ); we also use a Woods-Saxon profile for the distribution
of nucleon centers whose radius and surface-thickness parameters have
been corrected for the finite nucleon size [ 6 ] . The resulting entropy
density profile is normalized to the final charged-hadron multiplicity
density @xmath in central collisions; after this normalization, the
centrality dependence of the initial entropy production is fixed by the
model (MC-Glauber or MC-KLN). To convert the initial entropy density to
energy density, we use the equation of state (EOS) s95p-PCE which
matches Lattice QCD data at high temperatures to a chemically frozen
hadron resonance gas at low temperatures [ 11 , 12 ] , using @xmath MeV
as chemical freeze-out temperature.

In the following we compute harmonic eccentricity and flow coefficients
as functions of impact parameter @xmath and collision centrality. The
centrality classes are defined in terms of percentages of the total
inelastic cross section, calculated from the distribution of the number
of wounded nucleons @xmath in the optical Glauber model (i.e. without
accounting for fluctuations in @xmath at given impact parameter). Each
centrality class is thus characterized by a range of impact parameters
@xmath and an average value @xmath , together with a mean number of
wounded nucleons @xmath . They are listed in Table B.1 [ 6 ] .

#### 2.1.5 Averaging procedures for the initial profiles

In this section, we will compare results obtained from event-by-event
hydrodynamic simulations with traditional single-shot hydrodynamic
simulations, where one first averages over many fluctuating initial
profiles to obtain a smooth average profile, then evolves this smooth
profile hydrodynamically. The question addressed in this comparison is
to what extent the average harmonic-flow coefficients from
event-by-event hydrodynamics can (or cannot) be faithfully represented
by the harmonic-flow coefficients extracted (at much lower numerical
expense) from the hydrodynamic evolution of an “average event”.

Taking the initial density profiles from the Monte Carlo generator and
superimposing them directly without additional manipulations (such that
the impact parameter directions for each collision are aligned)
generates a “reaction-plane averaged” profile with ellipticity @xmath
(Eq. ( 2.7 )). After recentering each event to the origin of the @xmath
- @xmath -plane, we can compute event by event the reaction and
participant plane ellipticities (Eqs. ( 2.6 ) and ( 2.1.1 , 2.3 )) and
evaluate their ensemble averages ( 2.8 ) and ( 2.4 ), respectively. To
generate a smooth average profile with ellipticity @xmath (Eq. 2.5 ), we
rotate each recentered event by the angle @xmath ( @xmath ) if we want
to determine the eccentricity of the average energy (entropy) density.
For the calculation of entropy-weighted average eccentricities, we
perform any ensemble average first and convert the result to energy
density later; in this case all events are rotated by their @xmath
angles. For energy-weighted ensemble averages, we convert @xmath to
@xmath in each event first, rotate by @xmath , and perform the ensemble
average last. Other sequences or mixtures of these steps are technically
possible but physically not meaningful. Note that the processes of
computing the energy density from the entropy density via the EOS and of
averaging the event profiles do not commute: The energy density obtained
via the EOS from the ensemble-averaged entropy density profile is not
the same as the ensemble-averaged energy density where the EOS is used
in each event to convert @xmath to @xmath .

### 2.2 Eccentricities

#### 2.2.1 Centrality dependence of different ellipticities

Fig. 2.1 shows a comparison between the different ellipticities defined
in Sec. 2.1.1 as functions of the impact parameter @xmath in panels (a)
and (b) and as functions of collision centrality (as defined in Sec
2.1.4 ) in panels (c) and (d). For panels (a) and (b), we generated
10,000 initial profiles for each impact parameter (except for @xmath and
2 fm for which we generated 30,000 events each), and the smooth initial
conditions are averaged from them; for panels (c) and (d) we generated
and averaged over 10,000 profiles for each centrality bin. Within the
centrality bins, the impact parameters were sampled between @xmath and
@xmath with @xmath weight. Compared to panels (a) and (b), this leads to
additional ellipticity fluctuations related to the fluctuating impact
parameter, whereas in Fig. 2.1 a,b, only @xmath fluctuations at fixed
@xmath contribute.

As discussed in Sec. 2.1.1 , Eq. ( 2.10 ), @xmath can become negative
when fluctuations grow large. Whenever this happens, we replace @xmath
by @xmath and indicate this by an open star in Fig. 2.1 (connected by
dotted lines to other points in the graph). One sees that @xmath has a
tendency to turn negative in the most peripheral collisions. In very
central collisions, @xmath becomes very small, with central values that
can have either sign depending on whether we keep the impact parameter
fixed (Figs. 2.1 a,b) or average over events with different impact
parameters in a given centrality bin (see the @xmath centrality values
in Figs. 2.1 c,d). Statistical errors are large, however, and within
errors @xmath is compatible with zero for impact parameters @xmath fm,
i.e. in the most central ( @xmath centrality) collisions. We indicate
this by open-ended error bars for @xmath , pointing from its upper limit
all the way to zero.

Comparing panels (a,c) for the MC-Glauber model with panels (b,d) for
the MC-KLN model we see great similarities in shape, but systematic
differences in magnitude of the ellipticities. The ratio of the MC-KLN
and MC-Glauber ellipticities is shown in Fig. 2.2 . Except for the most
central and most peripheral collisions, the MC-KLN ellipticities exceed
the MC-Glauber ones by an approximately constant factor of around 1.2.
Please note the difference in the ratios for the reaction plane and
participant eccentricities at small @xmath . (The point for @xmath at
@xmath fm is obtained from a ratio of very small numbers and probably
not statistically robust – we had only 30,000 events to determine the
ensemble-averaged density profile.) For the @xmath ratio we dropped all
points where the statistical error for @xmath extended into the region
of negative values for either the MC-Glauber or MC-KLN model.

Figure 2.1 shows that, for central and mid-peripheral collisions, the
ensemble-averaged participant and reaction plane eccentricities @xmath
and @xmath agree very well with the mean eccentricities @xmath and
@xmath of the corresponding ensemble-averaged profiles. For strongly
peripheral collisions ( @xmath fm), however, the average of the ratio
(Eqs. ( 2.3 , 2.4 , 2.8 )) differs strongly from the ratio of averages
(Eqs. ( 2.5 , 2.7 )), indicating strong event-by-event fluctuations. We
note that in very peripheral collisions, the average event ellipticity
drops quickly with increasing @xmath while the ellipticity of the
average profile remains large; single-shot hydrodynamic calculations
based on a smooth average initial profile thus overestimate the
effective initial source ellipticity and produce more elliptic flow than
expected from event-by-event hydrodynamic evolution of individual
peripheral events. Still, as first emphasized in [ 31 ] , the calculated
@xmath from single-shot hydrodynamics decreases steeply at large
collision centralities [ 9 , 36 , 71 ] , due to the decreasing fireball
lifetime, which contrasts with the initially reported experimentally
observed behaviour [ 33 , 34 ] , but agrees qualitatively with a
reanalysis [ 35 ] where non-flow effects have been largely eliminated
and/or corrected for. We do point out that our Monte-Carlo simulations
do not include fluctuations in the amount of entropy generated per
nucleon-nucleon collision [ 10 ] which have important effects on the
ellipticities in very peripheral collisions.

Comparing the curves for @xmath , @xmath , and @xmath in Fig. 2.1 , we
see that (as is manifest in the Gaussian model analysis in Eq. ( 2.1.1
)) @xmath receives a positive and @xmath receives a negative
contribution from event-by-event ellipticity fluctuations. In Fig. 2.3
we check, as a function of impact parameter, the validity of the
identities ( 2.11 ) and

( 2.13 ), which follow from Bessel-Gaussian and Gaussian @xmath
distributions, respectively. We see that both hold with good accuracy in
the mid-centrality range ( @xmath fm for Eq. ( 2.11 ), @xmath fm for
Eq. ( 2.13 )) but break down in the most peripheral collisions. Both the
Gaussian and Bessel-Gaussian hypotheses work slightly better for the
MC-KLN than for the MC-Glauber model. Consistent with the analysis in
Ref. [ 28 ] , the Gaussian fluctuation hypothesis for @xmath breaks down
at small impact parameters, whereas (as theoretically expected [ 28 ] )
the Bessel-Gaussian hypothesis appears to continue to hold, although we
are unable to make this statement with statistical confidence. (For the
ratio @xmath , we again dropped all points for which the error band for
@xmath reaches into negative territory.)

The assumption of Gaussian fluctuations of the real and imaginary parts
of the complex ellipticity ( 2.1 ) is often used to argue that the
average reaction-plane ellipticity @xmath can serve as a proxy for
@xmath (see Eq. ( 2.11 )), and that therefore reaction-plane averaged
initial density profiles can be used in single-shot hydrodynamics (which
ignores event-by-event fluctuations) to simulate the experimentally
measured @xmath values. Fig. 2.1 and the bottom curves in Fig. 2.3 show
that @xmath values obtained from single-shot hydrodynamic simulations
with reaction-plane averaged initial conditions [ 9 , 36 ] should not be
trusted quantitatively for centralities @xmath .

To summarize this subsection, all the simplifying assumptions that allow
one to focus attention on the three quantities @xmath , @xmath , and
@xmath only (by substituting @xmath for @xmath and @xmath for @xmath or
@xmath ) hold well for central to mid-central collisions ( @xmath
centrality) but break down for peripheral collisions. For @xmath
centrality there exists no substitute for event-by-event hydrodynamics
if one aims for quantitative precision in the comparison with
experimental elliptic flow data, since the latter are strongly affected
by non-Gaussian event-by-event fluctuations at those centralities.

#### 2.2.2 Ellipticities with different weight functions

Figure 2.4 shows a comparison between the energy- and entropy-weighted
ellipticities of the initial profiles generated with the MC-KLN model on
an event-by-event basis. The scatter plot is based on 6000 events, 1000
each for @xmath and for the following finite-width centrality bins:
@xmath , @xmath , @xmath , @xmath , and @xmath . This is not a realistic
mix in the experimental sense, but permits us to explore the full range
from very small to very large event ellipticities. The blue dots in Fig.
2.4 represent bin averages, and the solid black line is a linear fit
through the origin. The fitted slope is 1.00, the scatter plot is seen
to be tightly clustered around this fitted line, and only at small
ellipticities @xmath the @xmath -weighted values are seen to be slightly
larger on average than their @xmath -weighted counterparts (see also
Fig. 2.5 a below).

#### 2.2.3 Higher order harmonics

In Figs. 2.5 a-d we compare the centrality dependences of the
ensemble-averaged second to fifth harmonic eccentricity coefficients
(energy- and entropy-weighted) from the MC-Glauber and MC-KLN models.
The contour plots give a visual impression of the degree of deformation
corresponding to the (larger) MC-KLN eccentricities, assuming (for
illustration) the absence of any other eccentricity coefficients than
the one shown in the particular panel.

First, one observes very little difference between the eccentricities of
the entropy and energy density profiles, except for very central
collisions ( @xmath fm for the MC-Glauber, @xmath fm for the MC-KLN
model) where the energy-weighted eccentricities lie systematically
somewhat above the entropy-weighted ones (for all orders @xmath studied
here). The difference between @xmath - and @xmath -weighted
eccentricities at small @xmath is bigger in the MC-Glauber than in the
MC-KLN model.

Next, one notes the significantly larger ellipticities and
quadrangularities of the MC-KLN distributions compared to those from the
MC-Glauber model for all but the most central collisions. These are
driven by geometry, i.e. by the almond-shaped deformation of the nuclear
overlap zone in non-central collisions, which in the KLN-model is more
eccentric than in the Glauber model. The third and fifth order
harmonics, which are entirely due to fluctuations (and whose associated
angles @xmath are therefore completely uncorrelated to the reaction
plane – see Ref. [ 10 ] and discussion below), show remarkably similar
eccentricity values in the two initialization models, except for the
most peripheral events. Comparing the viscous suppression of elliptic
and triangular flow thus allow to distinguish experimentally between the
MC-Glauber and MC-KLN models (see Chap. 3 ).

Third, in central collisions all four eccentricity coefficients are
roughly of the same size. In peripheral collisions, the
fluctuation-dominated eccentricity coefficients ( @xmath and @xmath )
are generically smaller than the geometry-dominated ones ( @xmath , but
also to some extent @xmath ). ³ ³ 3 We checked that the centrality
dependences of the ratios @xmath agree qualitatively, but not
quantitatively, with Fig. 3 in Ref. [ 37 ] . We suspect that the
differences, which are larger for the MC-Glauber than the MC-KLN model,
are due to somewhat different Woods-Saxon and (in the MC-Glauber case)
fluctuation-size parameters used in Ref. [ 37 ] . This is less obvious
when one defines the higher order eccentricities with @xmath instead of
@xmath weight [ 10 ] , which tends to increase the values of the higher
harmonics in peripheral collisions.

Even with “only” an @xmath weight, @xmath and @xmath are seen to become
large enough around @xmath fm that, if collective acceleration happens
predominantly in the directions of steepest descent of the density
profile, one has to expect cross-currents in the developing anisotropic
flow patterns. These can lead to destructive interference and a
correspondingly reduced efficiency of converting @xmath -order
eccentricities @xmath into @xmath -order harmonic flows @xmath [ 22 ] .
In realistic situations this issue is exacerbated by the simultaneous
presence of several large eccentricity components @xmath , which is
expected to lead to a strongly nondiagonal and probably nonlinear
response matrix relating @xmath to @xmath [ 10 ] . This will be
discussed in Sec. 2.3 , and more in Sec. 8.4 .

#### 2.2.4 Eccentricity correlations

It is reasonable to ask whether and how the different harmonic
eccentricity coefficients @xmath are correlated with each other. Figure
2.6 shows scatter plots of the correlations between @xmath and the
ellipticity @xmath , which, for large @xmath values, is dominated by
geometric overlap effects. We note that, according to the definition (
2.17 ), all eccentricity coefficients are positive definite, @xmath .
Keeping this in mind, Figs. 2.6 a,c show that @xmath and @xmath are
uncorrelated with the fireball ellipticity; the slight growth of @xmath
with increasing @xmath is related to the growth of the variances of
their distributions in more peripheral collisions.

In contrast, the quadrangularity @xmath shows a clear positive
correlation with the ellipticity, see Fig. 2.6 b. It is of geometrical
origin: it reflects the football or almond shape of the overlap zone in
non-central collisions which is a little sharper than a pure @xmath
deformation. This is corroborated by the behavior of angle @xmath shown
in Fig. 2.7 a below, which, on average, points @xmath relative to @xmath
(which again points in @xmath -direction). This means that the
quadrangular component of the initial fireball definition is oriented
like a diamond, with its corners on the @xmath and @xmath axes.
Superimposing it on a pure @xmath deformation leads to a somewhat
sharper shape of the density distribution.

### 2.3 Event-by-event hydrodynamics and flow fluctuations

In this section we analyze the results from event-by-event hydrodynamic
evolution of the fluctuating initial profiles studied in the previous
section. We focus on the anisotropic flow coefficients @xmath , their
relationship to the initial eccentricity coefficients @xmath , and the
correlation between the @xmath -order flow angles @xmath and the
corresponding @xmath -order participant-plane angles @xmath associated
with @xmath .

#### 2.3.1 Correlations between participant plane, event plane, and
reaction plane

One of the key characteristics of fluid dynamics is its ability to
transform initial geometric deformation into a deformation of the final
momentum distribution, via collective flow. This happens through
spatially anisotropic hydrodynamic forces (i.e. pressure gradients)
which cause anisotropic acceleration of the fluid. As a result,
correlations between participant and event planes are expected: The
angle @xmath points in the direction of the largest pressure gradient
associated with the @xmath harmonic component of the spatial deformation
of the initial density distribution, while @xmath points into the
direction where the @xmath harmonic component of the final collective
flow is largest. Without interference between harmonics of different
order, we would thus expect @xmath and @xmath to point, on average and
up to event-by-event fluctuations, in the same direction.

In Figs. 2.7 a,b we show the distribution of participant- and
event-plane angles associated with the @xmath -order eccentricities and
harmonic flows, relative to the @xmath - @xmath reaction plane. The
analysis uses the same 6000 events as before, and evolves them with
ideal fluid dynamics ( @xmath ). In panel (a) we see that @xmath are
completely uncorrelated with the reaction plane [ 10 ] , as expected
from the fact that the corresponding eccentricities are entirely
fluctuation-driven, without contribution from the collision geometry.
Panel (b) shows that the same holds true for @xmath , which is (at least
superficially) consistent with the expectation that @xmath is mostly or
entirely driven by @xmath , and @xmath by @xmath . We will revisit this
below. @xmath and @xmath are strongly correlated with the reaction plane
@xmath , at least for this mixed-centrality set of events. This is
expected since, for non-central collisions, @xmath is mostly controlled
by the almond-shaped overlap geometry, and @xmath is mostly a collective
flow response to this geometric deformation; event-by-event fluctuations
contribute to @xmath (and thus @xmath ), but in general do not dominate
them.

The behavior of @xmath in Fig. 2.7 a is interesting because it is on
average strongly “anti-correlated” with the reaction plane, in the sense
that it points (on average) at @xmath relative to the @xmath -axis. The
geometric reason for this has already been discussed above in subsection
2.2.4 . On the other hand, Fig. 2.7 b shows that the angle @xmath points
on average into the reaction plane. This correlation of @xmath with the
reaction plane is somewhat weaker than the anti-correlation of @xmath
with that plane seen in panel (a). Still, it suggests that quadrangular
flow @xmath does not, on average, develop predominantly in the direction
of the steepest pressure gradient associated with @xmath , but in the
direction of steepest @xmath -induced pressure gradient.

Figure 2.7 c, however, in which we analyze directly the correlation
between the event and participant plane angles, paints a more subtle
picture. It shows, surprisingly, a correlation peak at zero relative
angle between @xmath and @xmath , whereas the above discussion should
have led us to expect a correlation peak at @xmath . The resolution of
this paradox is presented in the next subsection: the relative
importance of geometric and fluctuation-induced contributions to @xmath
, @xmath , and their associated angles changes with collision
centrality, with geometry playing a relatively larger role in peripheral
collisions. One should therefore look at the angle correlations as a
function of collision centrality. One finds that the correlation
function peaks in Figs. 2.7 a,b for the @xmath -order angles relative to
the reaction plane are almost entirely due to geometric effects in
peripheral collisions, while in central collisions both @xmath and
@xmath are fluctuation-dominated and thus essentially uncorrelated with
the reaction plane. On the other hand, precisely because in central
collisions geometric effects such as geometrically driven elliptic flow
do not dominate the hydrodynamic response to the fluctuation-driven
higher-order eccentricities, @xmath and @xmath remain relatively
strongly correlated in near-central collisions. This is the reason for
the peak at @xmath for @xmath in Fig. 2.7 c. (A hint of the
“anti-correlation” at @xmath is still visible in Fig. 2.7 c, and it
would be stronger if we had not (for unrelated reasons) strongly
oversampled central collisions in our mixed-centrality sample.)

We close this discussion with the following additional observations
about Fig. 2.7 c: (i) The second-order participant and event planes are
much more strongly correlated with each other than either one of them is
with the reaction plane. This shows that even in very central
collisions, where the source ellipticity is mostly fluctuation-driven
and its angle therefore only weakly correlated with the reaction plane,
elliptic flow develops event-by-event in the direction of the short axis
of the ellipsoid. (ii) Even though the angles associated with @xmath and
@xmath are uncorrelated with the reaction plane (Figs. 2.7 a,b), they
are strongly correlated with each other. This indicates that @xmath is
mostly driven by @xmath , especially in the more central collisions,
with relatively little interference from other harmonics. (iii) The
@xmath -order event and participant plane angles show correlation peaks
both at @xmath and @xmath . As we will see in the following subsection,
the former results from central and the latter from peripheral
collisions. The peak at @xmath indicates significant cross-feeding
between modes with @xmath , and 5.

#### 2.3.2 Centrality dependence of event and participant plane
correlations

Figure 2.8 looks at the correlation between the @xmath -order EP and PP
angles at different collision centralities. This generalizes a similar
analysis for @xmath in Ref. [ 38 ] to higher harmonics. Plotted are the
distributions of the absolute value of the difference between the two
angles in the main graph and the root mean square of this distribution
(i.e. the width around zero of the correlation) in the inset, as a
function of collision centrality. Panel (a) shows that the second-order
participant and event planes are strongly correlated at all collision
centralities. This demonstrates that elliptic flow is generated almost
exclusively by the source ellipticity. The variance of the correlation
is @xmath rad in the mid-central range (15-40% centrality) and increases
in very central and very peripheral collisions due to growing
ellipticity fluctuations.

A similar correlation exists for the @xmath -order participant and event
planes, at all collision centralities, but with a larger variance of
order @xmath rad (depending on centrality). The relatively strong
correlation suggests that @xmath is the dominant driver for @xmath [ 21
] .

For the @xmath - and @xmath -order participant and event planes the
situation is complicated, as seen in panels (c) and (d). The planes are
correlated with each other (i.e. the distributions peak at zero
difference angle) in central collisions, become essentially uncorrelated
in mid-central collisions and anti-correlated (i.e. peaked at a
difference angle of @xmath , @xmath ) in peripheral collisions. The
anti-correlation in peripheral collisions indicates strong mode-mixing,
driven by the large ellipticity @xmath and strong elliptic flow @xmath
at large impact parameters, which generates @xmath and @xmath
contributions by coupling to lower harmonics, as described in the
previous subsection. For @xmath in particular, a strong @xmath component
in the collective flow velocity generates a @xmath of the final momentum
distribution, without any need for nonzero @xmath . At large impact
parameters, @xmath -induced quadrupolar flow from the initial elliptic
deformation of the overlap region thus dominates over any contribution
from initial quadrangular deformation. However, there are additional
mode-coupling effects arising from the nonlinear hydrodynamic evolution.
As we will see in Chap. 4 , the flow angles @xmath reflected in the
measured momentum distribution closely correlate with the angles of the
hydrodynamic flow anisotropies, so there is a nonlinear contribution to
the 4th harmonic of the collective-flow velocity. In near-central
collisions, on the other hand, where all @xmath stem mostly from shape
fluctuations, @xmath are dominantly driven by @xmath .

To study viscous effects we show Fig. 2.9 , which is similar to Fig. 2.8
, but using simulated data from both ideal and viscous ( @xmath )
hydrodynamic simulations, starting from identical initial conditions.
Each impact parameter group has 1000 simulated events.

It can be seen that nonzero viscosity manifests itself in two distinct
ways: (1) For flows that are not strongly affected by mode couplings,
for example @xmath at any @xmath , and @xmath at @xmath , the effect of
shear viscosity is to reduce the fluctuations in @xmath . Viscosity
suppresses the randomness in the flow profile caused by the existence of
hotspots in the initial density distribution, thus helping to build
@xmath up from @xmath and to align @xmath with @xmath . (2) Viscosity
also increases the strength of mode-coupling between different
harmonics. In those cases where @xmath receives large contributions from
@xmath , @xmath (e.g. @xmath at large @xmath , @xmath in general), the
fluctuations in the angle difference @xmath receive multiple
contributions, with each contributor possibly contributing positively or
negatively, resulting in hard-to-predict net effects.

#### 2.3.3 Harmonic flows and their corresponding initial
eccentricities: nonlinear hydrodynamic response

It is often assumed that the harmonic flows @xmath respond linearly to
the eccentricities @xmath , at least as long as the latter are small.
This assumption receives support from hydrodynamic simulations [ 22 ] as
long as one probes deformed initial profiles with only a single
non-vanishing harmonic eccentricity coefficient. In Fig. 2.10 , we
investigate the validity of this assumption with fluctuating MC-KLN
events which feature nonzero @xmath values for all @xmath .

Figure 2.10 a generally provides support for the assumption of a linear
dependence of the elliptic flow @xmath on initial ellipticity @xmath ,
with two important caveats:

-   At small and large ellipticities, @xmath deviates upward from a
    best-fit line through the origin, indicating additional contributors
    to the elliptic flow. Indeed, for zero ellipticity @xmath we find a
    nonzero average @xmath . These are events with typically large
    nonzero values for eccentricities of higher harmonic order, which
    generate elliptic flow through mode-mixing (e.g. between @xmath and
    @xmath ). We see that this happens at all centralities, even for
    @xmath , due to event-by-event fluctuations of the eccentricity
    coefficients.

-   The slope of the curve @xmath decreases in very peripheral
    collisions, indicating destructive interference via mode-mixing from
    other harmonics in the hydrodynamic evolution of the small and
    highly fluctuating fireballs created at large impact parameters.

The @xmath -dependence of triangular flow @xmath , shown in Fig. 2.10 b,
shows a qualitatively similar story, but the deviations from linear
response are stronger, with significant nonzero triangular flow in
events with zero initial triangularity, especially for larger impact
parameters.

For @xmath and @xmath , shown in Figs. 2.10 c and 2.10 d, mode-mixing
effects are very strong, and a linear response of @xmath to @xmath (
@xmath ) can no longer be claimed. This is quite different from the
results in [ 22 ] , where @xmath was studied for a source that had only
@xmath deformation: in this case @xmath was found to be approximately
linear for small @xmath , with a downward bend at larger @xmath values
due to negative interference from cross-currents for sources with large
quadrangularities. (This approximately linear dependence survived in the
@xmath -integrated @xmath , even though it was noticed in a related
study [ 39 ] that, for mid-central collisions, the differential
quadrangular flow @xmath at high @xmath appears to be mostly determined
by the elliptic deformation of the hydrodynamic flow profile generated
by @xmath .) Our study shows that it is unlikely that the anisotropic
flow resulting from highly inhomogeneous initial profiles with nonzero
eccentricity coefficients of all harmonic orders can be obtained by some
sort of linear superposition of flows generated from sources with only a
single nonzero harmonic eccentricity coefficient, as suggested in [ 40 ]
. (We will discuss this topic in more detail in Chap. 4 .) The
hydrodynamic response @xmath to a set of initial eccentricity
coefficients @xmath is not only nondiagonal, but also (via mode-mixing)
nonlinear, and there is no suitable single-shot substitute for
event-by-event hydrodynamic evolution of fluctuating initial conditions.

The effect of viscosity can be studied using Fig. 2.11 , which contains
curves corresponding to both ideal and viscous ( @xmath ) hydrodynamic
evolutions. In contrast to Fig. 2.10 , the events are grouped directly
by impact parameters. Again 1000 events are simulated for each impact
parameter.

First it is clearly seen that the existence of viscosity reduces the
response of @xmath from @xmath , and this effect increases with
increasing @xmath . For elliptic and triangular flow, viscosity
suppresses the conversion coefficient @xmath more strongly for larger
impact parameters, indicating larger viscous effects in more peripheral
collisions. For @xmath viscosity wipes out the monotonicity of its
dependence on @xmath in central and semi-central collisions, meaning
that it receives larger contributions from other sources than @xmath —
this is another way to state that viscosity increases nonlinear
mode-coupling effects.

We note that in both ideal and viscous cases, nonlinear mode-mixing
effects appear to be minimal for the elliptic and triangular flow (Figs.
2.10 a,b, Figs. 2.11 a,b). @xmath and @xmath remain therefore the best
candidates for an extraction of the fluid’s viscosity, by studying (with
quantitative precision) the fluid’s efficiency in converting initial
spatial deformations into final momentum anisotropies and anisotropic
flows. We will further elaborate on this theme in the next section.

### 2.4 Single-shot versus event-by-event hydrodynamics

We now discuss the effects of event-by-event initial-state fluctuations
on the finally observed pion and proton @xmath -spectra and anisotropic
flow, comparing traditional single-shot hydrodynamic evolution of an
appropriately constructed smooth average initial profile with
event-by-event evolution of fluctuating initial conditions (with an
ensemble average taken at the end). Since the calculation of resonance
decay feeddown corrections is computationally expensive but not expected
to cause qualitative changes, we here concentrate on directly emitted
(“thermal”) pions and protons. For the graphs shown in this section, we
generated for each impact parameter 1000 fluctuating events from the
MC-KLN model and propagated them either event-by-event or via a
single-shot hydrodynamic simulation run down to a decoupling temperature
of 140 MeV.

#### 2.4.1 Transverse momentum spectra

In [ 41 ] , Chatterjee et al. showed that thermal photon spectra from
exploding heavy-ion collision fireballs with fluctuating initial
conditions which were hydrodynamically evolved event-by-event are
significantly harder than those obtained from single-shot hydrodynamic
evolution of the corresponding ensemble-averaged smoother initial
profiles. The authors of [ 41 ] attributed this effect to the existence
of “hot spots” in the fluctuating initial conditions that radiate
photons at a higher-than-average temperature. Figure 2.12 shows that the
same hardening effect occurs in the pion and proton spectra even though
these strongly interacting hadrons are emitted only at freeze-out, with
the same decoupling temperature assumed in both types of evolution. ⁴ ⁴
4 A similar effect was also seen in [ 38 ] , whose authors further
pointed out that the strength of this “hardening effect” depends on the
fluctuation size parameter in the initial conditions (i.e. the area over
which the entropy produced in a nucleon-nucleon collision is
distributed). This proves that the effect is due to stronger radial flow
in the event-by-event evolved fluctuating fireballs, driven by the
stronger than average pressure gradients associated with the “hot spots”
(i.e. over-dense regions) in the initial profile. The importance of
initial-state fluctuation effects on the final @xmath -spectra becomes
stronger in peripheral collisions, where the initial fireballs are
smaller and “hot spots” have a relatively larger influence. If stronger
radial flow is the explanation of the fluctuation-driven hardening of
the pion and proton spectra observed in Fig. 2.12 , it is probably also
an important contributor to the hardening of the photon spectra noted in
Ref. [ 41 ] , at least for low @xmath (i.e. in the hydrodynamic regime).

Nonzero viscosity softens the hardening effect, as shown in Fig. 2.13 .
It is clearly seen that the difference in @xmath -spectra between the
two types of simulations is reduced in the viscous case. We attribute
this reduction to the fact that the nonzero viscosity tends to quickly
wash out density inhomogeneities from hot spots by dissipation, thereby
reducing the difference between fluctuating and smooth initial
conditions in the later stages of the expansion.

#### 2.4.2 Elliptic and triangular flow

In Figures 2.14 and 2.15 , we compare the eccentricity-scaled elliptic
and triangular flows, @xmath and @xmath , for pions and protons as a
function of impact parameter, from single-shot (dashed lines) and
event-by-event hydrodynamics (solid lines). These ratios represent the
efficiency of the fluid for converting initial spatial deformations into
final-state momentum anisotropies. This conversion efficiency is
affected (to be more precise, reduced) by shear viscosity, so these
ratios form the basis of many analyses that aim to extract this
transport coefficient from experimental heavy-ion data.

For event-by-event hydrodynamics, we show two curves, using either the
entropy- (blue open circles) or the energy-weighted (red solid circles)
average eccentricities to normalize the average final flow @xmath . For
the ellipticity (Fig. 2.14 ), this choice is seen to make a difference
only in rather central collisions ( @xmath fm), but for the
triangularity, the differences are significant out to average impact
parameters probed in minimum-bias samples, @xmath fm. As stated earlier,
we prefer the energy-weighted eccentricities (solid circles) as
deformation measures because energy density and pressure are closely
related through the EOS, and it is the pressure gradients (and their
anisotropies) that drive the collective flow (and its anisotropies).

For the single-shot hydrodynamic simulations, a question arises as to
how exactly one should construct the ensemble-averaged smooth initial
profile which is then evolved hydrodynamically. We have explored three
reasonable procedures (variations of which have been used in the
literature) and show them as dashed lines in Figs. 2.14 and 2.15 . For
the lines labeled by stars , we rotate the entropy density for each
fluctuating event by the corresponding entropy-weighted
participant-plane angle @xmath ( @xmath , see Eq. ( 2.19 )), ⁵ ⁵ 5 Note
that for computation of @xmath , we rotate the events by a different
angle before averaging than for @xmath , i.e. @xmath and @xmath are
obtained from two different single-shot hydrodynamic runs, starting from
different averaged initial-energy-density profiles. then average the
rotated entropy profiles, compute the eccentricity @xmath of the
resulting average entropy density profile and convert it to energy
density using the EOS for input into the hydrodynamic code. For the
lines labeled by crosses , we rotate the energy density for each
fluctuating event (obtained from the EOS) by the corresponding
energy-weighted participant-plane angle @xmath (see Eqs. ( 2.1 , 2.1.1
)), compute the averaged rotated energy density profile and its
eccentricity @xmath , and use it directly as hydrodynamic input. For the
dashed lines without symbols , finally, the averaged initial energy
density (and therefore the final @xmath ) are exactly the same as for
the lines with crosses, but the final @xmath is scaled by the
entropy-weighted (rather than energy-weighted) eccentricity of the
averaged initial profile, where the entropy density is obtained from the
smooth averaged energy density via the EOS.

The differences between the different dashed lines illustrate the
uncertainties associated with the choice of averaging procedure for the
initial state. Keeping in mind that a 20% reduction in @xmath
corresponds (very roughly) to an increase of @xmath by an additive term
@xmath [ 30 ] , one sees that these differences are not negligible if
one aims for quantitative precision in the extraction of the specific
shear viscosity. Comparing the three dashed lines, we see that it
doesn’t make much difference whether we use the @xmath -weighted or
@xmath -weighted participant-plane angles to rotate the events before
superimposing them (the dashed lines without symbols and with stars are
all very close to each other), but that in the more central collisions
we obtain significantly different values for the conversion efficiencies
@xmath if we normalize by @xmath - or @xmath -weighted mean
eccentricities. Even though they look similar in Fig. 2.5 a, at small
impact parameters @xmath and @xmath are larger than @xmath and @xmath ,
respectively, and this is the main reason why the red and blue lines in
Fig. 2.14 diverge at small @xmath , for both event-by-event (solid
lines) and single-shot hydrodynamics (dashed lines).

An apples-to-apples comparison between event-by-event and single-shot
hydrodynamics (and between theory and experimental data) therefore must
ensure that the same (or at least conceptually compatible)
eccentricities are used to normalize the anisotropic flow coefficients
that are to be compared. In Figs. 2.14 , 2.15 , we should therefore
compare blue solid with blue dashed or red solid with red dashed lines,
but not curves of different colors.

Even this is not good enough if one wants to accurately assess the
relative space-to-momentum anisotropy conversion efficiency in
single-shot and event-by-event hydrodynamics: in the single-shot hydro
curves, we use @xmath to normalize the final elliptic flow, whereas the
event-by-event hydro results were normalized with @xmath . While each of
these eccentricity measures makes perfect sense in its own context, they
differ at large impact parameters, @xmath being larger (see Figs. 2.1
a,b). To avoid this problem, we have added in Figs. 2.14 and 2.15 an
additional “mixed ratio” (dash-dotted purple line), which normalizes the
ensemble-averaged anisotropic flow @xmath ( @xmath ) from event-by-event
hydrodynamics (used in the ratio @xmath denoted by solid lines with
solid red circles) by the mean @xmath -weighted eccentricity @xmath from
single-shot hydrodynamics (used in the ratio @xmath denoted by dashed
lines with crosses). This dot-dashed purple line agrees almost perfectly
with the solid red line with circles over most of the impact parameter
range, except for peripheral collisions with @xmath fm where @xmath and
@xmath begin to diverge. The red dashed lines with crosses and purple
dash-dotted lines show the anisotropic flows from single-shot and
event-by-event hydrodynamics normalized by the same eccentricity measure
characterizing the fluctuating event sample. Their comparison allows an
unambiguous assessment of the different efficiencies of single-shot and
event-by-event hydrodynamics in converting initial eccentricities to
final momentum anisotropies. Their ratio is shown in Fig. 2.16 .

From Fig. 2.16 a, one concludes that, for ideal hydrodynamics,
event-by-event fluctuations on average reduce the efficiency of the
fluid in converting initial source ellipticity into elliptic flow. Over
most of the centrality range, this reduction is about 4% for pions and
about twice as large for protons and it is similar for MC-KLN and
MC-Glauber initial profiles. In very central collisions, the ratio of
conversion efficiencies for event-by-event vs. single-shot hydrodynamics
is closer to 1, but it degrades strongly in very peripheral collisions
where event-by-event evolution generates on average @xmath less elliptic
flow than single-shot hydrodynamics. The generic tendency of the
event-by-event hydrodynamic evolution of fluctuating initial profiles to
generate less elliptic flow than expected from hydrodynamic evolution of
the corresponding smooth average profile has been observed before [ 42 ,
43 ] ; our systematic study in Fig. 2.16 a quantifies this effect over
the full range of collision centralities.

The situation with triangular flow, shown in Fig. 2.16 b, is quite
different: event-by-event propagation of initial-state flutuations can
lead to an increase or decrease of the triangular flow compared to
single-shot hydrodynamics, depending on particle mass (pions or
protons), the nature of the fluctuations (MC-Glauber or MC-KLN), and
collision centrality. Contrary to elliptic flow, in peripheral
collisions event-by-event evolution leads to significantly larger
average triangular flow than single-shot hydrodynamics.

The analogous plots to Fig. 2.14 and Fig. 2.15 using viscous ( @xmath )
hydrodynamic simulations with MC-KLN initial conditions are shown in
Fig. 2.17 . Comparing the ideal and viscous results, we see that
viscosity greatly reduces the gap between event-by-event and single-shot
simulations, both for @xmath and protons, for both elliptic and
triangular flow. This is consistent with viscosity damping
inhomogeneities by dissipation, making the density distributions
smoother, at least at the late times.

To make the statement more quantitative, we show the corresponding ratio
plot in Fig. 2.18 . Comparing to Fig. 2.16 , it is seen from panel (a)
that, except for extremely peripheral collisions ( @xmath fm), the
@xmath gap between the elliptic flows calculated from the two types of
simulations is reduced to @xmath . For the triangular flow, the story is
somewhat different: although the curves from event-by-event and
single-shot simulations appear to be closer to each other in Fig. 2.17
b,d than in Fig. 2.15 b,d, the relative difference between the two types
of simulations is actually slightly larger, rising from @xmath to @xmath
.

If one aims for quantitative calculations with accuracy better than
@xmath or for a study of higher-order flows, event-by-event hydrodynamic
evolution is an essential and indispensable ingredient; however if one
has larger accuracy tolerance, one might try to extract @xmath from
@xmath and @xmath using single-shot simulations. Unfortunately, it turns
out that @xmath and @xmath are not enough to constrain @xmath very well
unless one has complete control over the initial fluctuation spectrum
which, as already shown in this chapter, is not true for the MC-KLN and
MC-Glauber models. This will be discussed in more detail in the
following chapters.

#### 2.4.3 Elliptic flow fluctuations

Similar to what is shown in Figs. 2.1 a,b for the initial source
ellipticities, Fig. 2.19 shows the elliptic flow measures @xmath ,
@xmath , and @xmath from event-by-event hydrodynamics, together with
@xmath from single-shot hydrodynamic evolution of the corresponding
averaged initial profile, for pions and protons, using MC-Glauber and
MC-KLN initializations, respectively. @xmath and @xmath are defined in
analogy to Eqs. ( 2.9 , 2.10 ) by

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (2.23)
     @xmath   @xmath   @xmath      (2.24)
  -- -------- -------- -------- -- --------

Here @xmath is calculated event by event via Eq. ( 2.22 ) from the
Cooper-Frye spectrum at freeze-out (with zero statistical uncertainties
since it is determined with mathematical precision by the event-by-event
hydrodynamic output).

As in Fig. 2.1 , open stars show the central values for @xmath whenever
@xmath turns negative, and open-ended error bars indicate that the error
band for @xmath ranges from positive to negative values. Similar to the
ellipticities shown in Fig. 2.1 , the latter happens at small impact
parameters, but for the MC-Glauber model the @xmath -range over which
this happens for @xmath (for both pions and protons) is somewhat larger
than for @xmath . Still, @xmath is compatible with zero over this entire
range, and we do not find statistically significant negative values for
@xmath at small impact parameters. At large @xmath fm, @xmath turns
negative for both pions and protons when we use MC-Glauber initial
conditions, whereas it remains positive for MC-KLN initial profiles.

By comparing @xmath (open circles in Fig. 2.19 ) with @xmath (solid
green circles), one sees that in mid-central to peripheral collisions
the @xmath -suppression from event-by-event hydrodynamic evolution is of
the same order as or (especially for protons) even larger than the
difference between @xmath and @xmath (solid blue vs. solid green
circles) that arises from event-by-event flow fluctuations. As a result,
@xmath from event-by-event hydrodynamics lies in peripheral collisions
even below @xmath from single-shot hydrodynamics, in spite of its
fluctuation-induced enhancement.

Corresponding results from viscous hydrodynamic simulations with @xmath
are shown in Fig. 2.20 . We see that viscosity suppresses the elliptic
flow, consistent with what we concluded in previous sections. We also
see that the event-by-event fluctuation of @xmath is also suppressed,
giving much smaller error bars compared to the ideal case as shown in
Fig. 2.19 . Again we attribute this effect to the fact that viscosity
dissipates hot-spots and thus to some extent reduces the event-by-event
fluctuations caused by the randomness of the initial nucleons positions.

Similar to Eqs. ( 2.11 )-( 2.13 ) we can test whether the @xmath
fluctuations from event to event have Gaussian or Bessel-Gaussian
distributions. This is done in Fig. 2.21 . The upper set of curves
(thick lines) tests the @xmath -analogue of relation ( 2.13 ), whereas
the lower set (thin lines) tests the validity of Eq. ( 2.11 ). (In the
lower set of curves, we dropped all @xmath -values for which the error
band for @xmath extends to negative values.) Just as we saw for the
initial ellipticities in Fig. 2.3 , both the Gaussian and
Bessel-Gaussian hypotheses for @xmath -fluctuations are seen to hold
quite well in mid-central ( @xmath fm) collisions. The Bessel-Gaussian
hypothesis breaks down in peripheral collisions ( @xmath fm). We expect
that the hypothesis holds (as expected [ 28 ] ) in central collisions
although it is not clearly seen from the plot which is generated using
only 1000 events per @xmath -value. The assumption of Gaussian @xmath
-fluctuations breaks down in central collisions ( @xmath fm), as
expected. For the MC-Glauber model, it also breaks down in very
peripheral collisions, whereas for MC-KLN initial conditions the final
elliptic flow exhibits a nice Gaussian distribution all the way to the
largest impact parameters.

We checked that viscosity leads only to small changes in the ratios
shown in Fig. 2.21 , and we therefore refrain from plotting them for the
viscous simulations.

Overall, a comparison of Figs. 2.21 and 2.3 (as well as of Figs. 2.19
and 2.1 ) shows that the statistical properties of @xmath fluctuations
are qualitatively similar but quantitatively different from those of the
initial ellipticity fluctuations. This is consistent with the fact that
the main driver for elliptic flow is the initial ellipticity, but that
eccentricity coefficients of higher harmonic orders affect the evolution
of @xmath weakly but measurably through nonlinear mode-coupling effects.

### 2.5 Chapter summary

We summarize a few key results from this chapter:

– The average and mean ellipticities @xmath and @xmath agree with
excellent accuracy over a wide range of impact parameters, but diverge
in very peripheral collisions ( @xmath centrality) where @xmath (both
for participant-plane and reaction-plane averaged profiles).

– The average energy and entropy density weighted eccentricities agree
with excellent accuracy over a wide range of impact parameters, except
for central collisions ( @xmath fm) where @xmath .

– Whether the fluctuating entropy density distributions for individual
events are first converted to energy density and then rotated by @xmath
and averaged, or first rotated by @xmath and averaged and then converted
to energy density has very little influence on the shape of the
resulting smooth average initial energy density profile for single-shot
hydrodynamics. We prefer the conversion to energy density as the first
step, since in event-by-event hydrodynamics the energy density gradients
of each event generate (through the EOS) the pressure gradients that
drive the evolution of collective flow.

– The shortcut of using reaction-plane averaging to generate a smooth
profile for single-shot hydrodynamics with ellipticity approximately
equal to @xmath of the ensemble, in the hope of generating with a single
hydrodynamic run an elliptic flow @xmath that can be directly compared
with @xmath measurements, works only in the @xmath centrality range. For
peripheral collisions, this method cannot be trusted.

– The assumption of Bessel-Gaussian fluctuations for initial source
ellipticity and final elliptic flow work well for @xmath fm but breaks
down in more peripheral collisions. For more peripheral collisions, the
hypothesis that @xmath and @xmath are Gaussian distributed works better
than the Bessel-Gaussian assumption, but it breaks down for @xmath fm.
For MC-Glauber initial conditions, directly emitted pions and protons
feature negative values of @xmath in very peripheral collisions. The
fluctuations of initial source ellipticities and final elliptic flow
values have qualitatively similar but quantitatively different
statistical properties.

– Except for rather central collisions, the eccentricities @xmath ,
@xmath and @xmath from the MC-KLN model are all significantly larger
than those from the MC-Glauber model. In contrast, @xmath is numerically
very similar for the two models over most of the impact parameter range.
The viscous suppression of triangular flow @xmath thus allows for a
determination of the QGP shear viscosity @xmath that is free from the
large model uncertainties that arise from the different MC-Glauber and
MC-KLN ellipticities when using @xmath for such an extraction; or
alternatively, an extraction of sheer viscosity using both @xmath and
@xmath simultaneously can be used to study the correctness of initial
condition models, see Chap. 3 .

– The second- and fourth-order eccentricities @xmath and @xmath are
strongly correlated by collision geometry, and @xmath receives strong
contributions even from a purely elliptical deformation of the final
flow velocity distribution. These complications make @xmath a poor
candidate for systematic studies of viscous effects on the evolution of
collective flow. Similar comments apply to @xmath since it couples via
mode-coupling to triangularity from fluctuations and to ellipticity from
collision geometry. This mixture of contributions from conceptually
different origins complicates a systematic analysis. In general, flow
coefficients @xmath of high harmonic order ( @xmath ) show poor
correlation with the eccentricity coefficients @xmath of the same
harmonic order, except for very central collisions where all
eccentricities are driven by fluctuations alone (and not by overlap
geometry).

– In spite of nonlinear mode-coupling effects, the basic response of
elliptic flow @xmath to ellipticity @xmath , and of triangular flow
@xmath to triangularity @xmath , is approximately linear. These two
observables thus remain prime candidates for systematic studies of
viscous effects on collective hydrodynamic flow.

– Event-by-event hydrodynamics generates harder @xmath -spectra for the
emitted hadrons than single-shot hydrodynamic evolution of the
corresponding averaged initial profile. This is due to additional radial
flow generated by large pressure gradients arising from “hot spots” in
the initial fluctuating density distribution. The hardening effect is
particularly strong in peripheral collisions which produce small
fireballs that fluctuate strongly; it is reduced by shear viscosity.

– Event-by-event hydrodynamic evolution of fluctuating initial
conditions leads to smaller average elliptic flow than obtained by
evolving the corresponding averaged initial condition in a single shot.
This suppression depends somewhat on collision centrality, and for ideal
fluids it is generically of order 4-5% for pions and 8-10% for protons.
The effect is sufficiently large to possibly lead to a significant
over-estimate of the fluid’s specific shear viscosity if one extracts it
from elliptic flow measurements by comparing with single-shot
hydrodynamic simulations. The discrepancy between event-by-event and
single-shot hydrodynamics decreases, however, in viscous fluid dynamics.
More related studies are reported in Chap. 3 .

## Chapter 3 Using simultaneously measured elliptic and triangular flow
to resolve initial condition ambiguities

This chapter focuses on a simultaneous comparison of both elliptic and
triangular flow from viscous fluid dynamics with measurements in Pb+Pb
collisions at the LHC. Using initial density distributions from the
MC-Glauber and MC-KLN models, we show that the data favor a small
specific shear viscosity @xmath for the quark-gluon plasma. Using this
viscosity value, the relative magnitude of the elliptic and triangular
flow is well described with MC-Glauber initial conditions while the
MC-KLN initial conditions require twice as large viscosity to reproduce
the elliptic flow and then under-predict triangular flow by about 30%.
We show that compatibility of the experimental data with larger values
for the specific shear viscosity of the QGP would require initial-state
models whose density distributions fluctuate more strongly, yielding
significantly larger triangular deformations on average than those
obtained from both the MC-Glauber and MC-KLN models. The material in
this chapter is based on [ 44 ] and [ 45 ] .

### 3.1 Introduction

Much attention has been given to the extraction of the specific shear
viscosity ( @xmath ) of the quark-gluon plasma from elliptic flow data
in relativistic heavy-ion collisions [ 46 , 47 , 48 , 49 , 50 , 30 , 34
, 51 , 52 , 53 , 9 , 43 , 54 , 36 , 12 ] .

A major road block in this effort is insufficient knowledge of the
initial shape of the thermalized fireball created in these collisions,
whose initial ellipticity is uncertain by about 20% [ 55 , 56 , 6 , 57 ,
20 ] (Fig. 2.5 ). As shown in Chap. 2 , this induces an @xmath
uncertainty in the value of @xmath extracted from elliptic flow [ 49 ,
30 ] . After the discovery of triangular flow in heavy ion collisions at
RHIC [ 21 , 58 , 59 ] and LHC energies [ 60 , 61 , 62 ] , followed by
the confirmation of its collective hydrodynamic nature [ 21 , 22 , 63 ,
10 , 64 , 65 , 66 ] and the realization that shear viscosity suppresses
higher-order harmonic flow coefficients more strongly than elliptic flow
[ 22 , 43 , 54 , 67 , 68 ] , it was suggested [ 37 , 58 , 60 , 69 , 45 ]
that a combined analysis of the elliptic and triangular flow
coefficients @xmath and @xmath could yield a more precise value for the
QGP shear viscosity and thereby reduce or eliminate the model
uncertainty in the initial deformation of the QGP fireball and its
event-by-event fluctuations. This chapter focuses on such an analysis,
using Pb+Pb collision data collected by the ALICE collaboration at the
LHC [ 70 ] .

### 3.2 Justification for using single-shot simulations

Event-by-event viscous hydrodynamic simulations with full inclusion of
unstable resonance decays are at present numerically too costly for
systematic flow studies over a range of viscosities, collision energies,
centralities, and collision systems. To study the possibility of
replacing event-by-event simulations by less costly single-shot ones, we
draw on our intuition based on previous hydrodynamic simulations for
Au+Au collisions at @xmath GeV (Chap. 2 ). The initial conditions under
comparison are from the MC-Glauber and the MC-KLN models (see Sec. 1.3
).

We have shown in Chap. 2 that, for ideal hydrodynamic simulations, the
eccentricity-scaled elliptic and triangular flows @xmath calculated from
single-shot and event-by-event hydrodynamics show @xmath differences
(Figs. 2.14 , 2.15 ), for both light (thermal pions) and heavy (protons)
particles; the differences for viscous hydrodynamic simulations with
@xmath are even smaller (Fig. 2.17 ). This suggests that for viscous
hydrodynamic simulations with sufficiently large viscosity @xmath ,
single-shot hydrodynamics can substitute well for event-by-event
evolution for the purpose of calculating @xmath and @xmath . For ideal
hydrodynamic simulations, there is a @xmath inaccuracy caused by this
substitution, however this is already much smaller than the @xmath
uncertainly in ellipticity induced by the ambiguity between the
MC-Glauber and MC-KLN models. Therefore a (relatively) quick study using
single-shot simulations with both elliptic and triangular flows becomes
a meaningful proposition for gaining new information beyond the previous
studies based on @xmath alone.

However there is a subtlety: the experimental flow data are mostly
determined with two-particle and four-particle correlations methods
which give @xmath and @xmath instead of @xmath . Following [ 27 ] we try
to normalize the flow by the correspondingly calculated eccentricity,
that is, @xmath by @xmath , @xmath by @xmath , @xmath by @xmath , and
the results are shown in Fig. 3.1 .

Fig. 3.1 (ab) shows that, for the elliptic flow, the process of
normalizing the flow by the correspondingly calculated eccentricity
gives very similar scaled flow in the ideal fluid case (Fig. 3.1 (a)),
and almost completely equal scaled flow in the viscous case (Fig. 3.1
(b)). There are @xmath differences between @xmath and @xmath or @xmath
when small @xmath is used (Fig. 3.1 (a)); with @xmath , this difference
has almost vanished (Fig. 3.1 (b)).

The scaled triangular flows are shown in Fig. 3.1 (cd). It is clear that
@xmath is very close to @xmath and @xmath , which are almost identical
in both the ideal (Fig. 3.1 (c)) and viscous (Fig. 3.1 (d)) cases. For
triangular flow, we do not show @xmath since the values for both
numerator and denominator are small and plagued by large statistical
errors.

We take the results of this study as justification for applying a
similar single-shot approach to the LHC data in the remaining of this
chapter.

### 3.3 Setup for the simulations

We use the (2+1)-dimensional viscous hydrodynamic simulation code
VISH2+1, with longitudinal boost-invariance, describing numerically the
transverse evolution of the heavy-ion collision fireball near
midrapidity. As in past work [ 49 , 6 , 50 , 57 , 20 , 30 ] , we use the
MC-Glauber and the MC-KLN models.

The MC-KLN calculations were done using a Monte-Carlo sample of initial
state profiles with identical properties as those used in [ 71 ] for the
calculation of transverse momentum spectra and elliptic flow in 2.76
@xmath TeV Pb-Pb collisions at the LHC. To compute the nuclear thickness
function @xmath , we use an inelastic nucleon-nucleon cross section
@xmath mb at LHC energies. For the @xmath dependence of the gluon
structure function in the MC-KLN model, we used the power @xmath [ 6 ] ;
the normalization factor for the initial entropy density was fixed by
hand to reproduce the measured charged hadron multiplicity density
@xmath for the 5% most central collisions [ 72 ] ; the measured
dependence of @xmath on collision centrality [ 73 ] is then
automatically reproduced reasonably well by the model [ 71 ] (see Fig.
3.2 (a)). MC-KLN runs were done with @xmath which, for this type of
initial conditions, was shown to yield a good overall description of the
measured transverse momentum spectra and elliptic flow in 200 @xmath GeV
Au-Au collisions at RHIC [ 71 ] and gave an impressively accurate
prediction for the unidentified and identified charged-hadron spectra
and elliptic flows in 2.76 @xmath TeV Pb-Pb collisions at the LHC [ 71 ,
74 ] .

For the MC-Glauber runs, we generated a new set of initial
configurations that differ from those used for 200 @xmath GeV Au-Au
collisions in [ 30 ] by the wounded-nucleon-to-binary-collision ratio.
Taking the initial entropy density

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

we determine @xmath and @xmath by a two-parameter fit to the ALICE data
[ 73 ] shown in Fig. 3.2 (a). Due to viscous entropy production during
the hydrodynamic evolution, which itself depends on collision
centrality, the fitted value for @xmath depends on the assumed shear
viscosity. For MC-Glauber initial conditions, we took @xmath since this
value was shown in [ 54 , 68 , 60 ] to provide a reasonable description
of the charged hadron @xmath and @xmath data measured by the ALICE
experiment; this results in @xmath for Pb-Pb collisions at the LHC. Both
the MC-Glauber and MC-KLN initial conditions are hydrodynamically
evolved with equation of state (EOS) s95p-PCE [ 12 ] , which matches
numerical results from lattice QCD at high temperatures to a hadron
resonance gas at low temperatures [ 11 ] and implements chemical
freeze-out at @xmath MeV. The hydrodynamic output is converted to final
hadron distributions along an isothermal decoupling surface of
temperature @xmath MeV, using the Cooper-Frye prescription.

In Chap. 2 [ 20 ] , we showed that, due to similar fluctuation
mechanisms, the MC-KLN and MC-Glauber models generate similar
third-order eccentricities @xmath , whereas the ellipticity @xmath ,
which is mostly controlled by collision geometry, is about 20% larger in
the MC-KLN model. Event-by-event ideal and viscous hydrodynamic
simulations with both realistically-fluctuating [ 20 ] (Recall Fig. 2.14
, 2.15 , 2.17 ) and doubly-deformed Gaussian initial conditions [ 45 ]
(with simultaneously nonzero @xmath and @xmath eccentricities) have
shown that the hydrodynamic conversion efficiencies for translating
initial spatial eccentricities into final flow anisotropies [ 10 , 40 ,
75 ] , although different for @xmath and @xmath , are very similar in
the MC-KLN and MC-Glauber models. The similarities in @xmath and
differences in @xmath between these models should thus straightforwardly
reflect themselves in analogous differences in @xmath and @xmath [ 69 ,
45 ] , allowing for an experimental distinction between the models.

For all the reasons discussed in Sec. 3.2 , we use the single-shot
approach. For a meaningful comparison with experiment, we include the
full cascade of resonance decays in the final state. We compare with
recent data from Pb-Pb collisions at the LHC, and we use the properly
normalized ratios @xmath for our comparison. The results are shown in
next few sections. Our approach here differs from that in [ 22 ] by
replacing the singly-deformed Gaussian parametrization of the initial
density used there by the ensemble average of realistically-fluctuating,
non-Gaussian initial profiles and from [ 43 , 54 , 68 ] by employing a
more realistic EOS that accounts for the important effects of chemical
non-equilibrium hadronic evolution on the elliptic flow @xmath [ 76 ] .
In [ 60 ] it was shown that, with the approach used in [ 22 ] , MC-KLN
initial conditions with @xmath cannot describe the @xmath -integrated
@xmath measured in 2.76 @xmath TeV Pb-Pb collisions, whereas the
MC-Glauber based event-by-event calculations (with @xmath ) of Schenke
et al. [ 54 ] appear to describe @xmath at selected centralities
reasonably well.

### 3.4 Transverse momentum spectra

Fig. 3.2 (b) shows the charged hadron @xmath -spectra for 2.76 @xmath
TeV Pb-Pb collisions at different centralities, for both MC-Glauber (
@xmath ) and MC-KLN ( @xmath ) initial conditions. For the most central
( @xmath ) collisions, the spectra from both models agree well with
published ALICE data. In more peripheral collisions, the MC-KLN spectra
are harder than those from MC-Glauber initial conditions. This is a
consequence of larger radial flow caused by larger transverse viscous
pressure gradients in the MC-KLN case, where the fluid is taken to have
2.5 times larger shear viscosity than for the MC-Glauber simulations in
order to obtain the same elliptic flow [ 49 , 30 ] . In peripheral
collisions, these viscous effects are stronger than in more central
collisions where the fireball is larger [ 77 ] . As shown in [ 38 , 20 ]
and Chap. 2 , event-by-event evolution of fluctuating initial conditions
generates, for small values of @xmath , flatter hadron spectra than
single-shot hydrodynamics, especially in peripheral collisions, due to
stronger radial flow driven by hot spots in the fluctuating initial
states. Proper event-by-event evolution of the latter is therefore
expected to reduce the difference between the MC-Glauber and MC-KLN
curves, as explained in Sec. 3.6 , since this effect is relatively
strong for @xmath (MC-Glauber) [ 20 ] but almost absent for @xmath
(MC-KLN).

### 3.5 @xmath-integrated elliptic and triangular flow

In Fig. 3.3 , we compare our @xmath -integrated @xmath and @xmath as
functions of centrality with ALICE @xmath , @xmath , @xmath , and @xmath
data, extracted from 2- and 4-particle correlations [ 60 ] (also see
Chap. 2 ). For both models, @xmath from the averaged smooth initial
conditions lie between the experimental @xmath and @xmath values. This
is consistent with the theoretical expectation [ 78 , 28 ] that @xmath (
@xmath ) is shifted up (down) relative to the average flow by
event-by-event flow fluctuations and was also found elsewhere [ 30 , 54
, 51 ] . Upon closer inspection, however, and recalling that ideal
single-shot hydrodynamics with smooth initial condition was shown in [
20 ] and in Chap. 2 to generate @xmath similar to @xmath from the
corresponding event-by-event evolution, it seems that the MC-KLN is
favored since it produces @xmath results closer to the @xmath data.
Unfortunately, a similar argument using @xmath can be held against the
MC-KLN model. To eliminate the interpretation difficulties associated
with a comparison of average flows from single-shot evolution of
averaged initial conditions with data affected irreducibly by naturally
existing event-by-event fluctuations, we proceed to a comparison of
eccentricity-scaled flow coefficients.

Assuming linear response of @xmath to their respective eccentricities
@xmath (which was found to hold in Chap. 2 and in [ 20 ] with reasonable
accuracy for @xmath and @xmath , but not for higher order anisotropic
flows) we follow [ 27 ] and scale the flow @xmath from single-shot
hydrodynamics by the eccentricity @xmath of the ensemble-averaged smooth
initial energy density, while scaling the experimental @xmath and @xmath
data by the corresponding fluctuating eccentricity measures @xmath and
@xmath , respectively, calculated from the corresponding models. As
shown in Sec. 3.2 , this procedure is justified for @xmath and @xmath ;
we do not perform comparisons using @xmath because of the lack of
statistical confidence in our theoretical calculations.

The eccentricity-scaled elliptic and triangular flow coefficients for
the MC-KLN and MC-Glauber models are shown in Figs. 3.4 (a,b) and Fig.
3.4 (c,d), respectively, and compared with the corresponding data from
ALICE [ 60 ] . The first thing to note is the impressively accurate
agreement between the experimentally measured @xmath and @xmath ,
showing that for elliptic flow the idea of scaling “each flow with its
own eccentricity” [ 27 ] works very well. Secondly, both @xmath and
@xmath measured by ALICE agree well with the viscous hydrodynamic
calculations for both the MC-Glauber and MC-KLN models, confirming that
for each model the correct value of @xmath has been used as far as
elliptic flow is concerned.

The bottom panels in Fig. 3.4 show the triangular flow @xmath . Clearly,
with the viscosities needed to reproduce @xmath , the MC-KLN model badly
disagrees with the experimental data. The measured triangular flow is
too big to accommodate a specific shear viscosity as large as 0.2.
Within the present approach, the only possibility to avoid this
conclusion is that somehow the MC-Glauber and MC-KLN models both
under-predict the initial third-order eccentricity @xmath by about 50%.
With MC-Glauber initial conditions and @xmath , on the other hand, the
ALICE data agree well with viscous hydrodynamics, even if the measured
centrality dependence of @xmath is slightly steeper than the calculated
one.

Summarizing Fig. 3.4 , the only possibility to have a large @xmath for
the QGP that is compatible with large ALICE @xmath data is to require an
initial-condition model that produces much larger triangularities than
the MC-Glauber and the MC-KLN models [ 79 ] .

### 3.6 @xmath-differential elliptic and triangular flow

As a cross-check, we compare our calculations also to the @xmath
-differential anisotropic flow data at one collision centrality ( @xmath
) [ 60 ] . The corresponding comparison between data and theory is shown
in Fig. 3.5 ; as in Fig. 3.4 , we compare the eccentricity-scaled flows,
plotting @xmath for the models and @xmath ( @xmath ) for the elliptic
(triangular) flow data. As seen in the upper panels, both initial-state
models describe the measured elliptic flow well up to @xmath GeV/ @xmath
; at larger @xmath , they over-predict @xmath for charged particles – a
problem noticed before [ 36 , 71 ] and possibly related to an imperfect
model description of the measured final chemical composition [ 74 ] .
The disagreement at larger @xmath is worse for MC-Glauber initial
conditions; this is likely related to our earlier observation in Fig.
3.2 (b) that our the MC-Glauber @xmath -spectra are steeper than the
MC-KLN ones in peripheral collisions – an artifact of our single-shot
approach and possibly remedied by a proper event-by-event hydrodynamical
simulation.

Fig. 3.5 (b) shows again the disagreement between theory and experiment
for triangular flow when we use MC-KLN initial conditions: the model
strongly under-predicts the data at all @xmath , i.e. it gives the wrong
slope for @xmath . With MC-Glauber initial conditions and
correspondingly lower shear viscosity @xmath (Fig. 3.5 (d)), the
measured @xmath is well described up to @xmath GeV/ @xmath but
over-predicted at larger @xmath . Again, the latter can be at least
partially attributed to the fact that MC-Glauber @xmath -spectrum from
our single-shot hydrodynamic approach is too steep at this collision
centrality, which can be corrected by performing the hydrodynamic
evolution properly event by event.

### 3.7 Chapter summary

Using a single-shot viscous hydrodynamic approach without any hadronic
after-burner but properly implementing hadronic chemical freeze-out at
@xmath MeV and including a full set of resonance decays, we have shown
that a combined analysis of the ALICE data for elliptic and triangular
flow from 2.76 @xmath TeV Pb-Pb collisions leads to a strong preference
for initial conditions from the Monte-Carlo Glauber model, combined with
a low value for the QGP shear viscosity @xmath , and disfavors the
considerably larger viscosities of @xmath that are required to reproduce
the measured elliptic flow when assuming the more eccentric Monte-Carlo
KLN initial profiles.

The analysis presented in this chapter was restricted to only these two
initial-state models, and only to elliptic and triangular flows. While
this analysis, published in [ 44 ] , was the first combined analysis of
two different flow harmonics, it was later superseded by analyses that
included several higher order harmonics [ 80 , 81 , 82 , 4 ] . From the
later work one must conclude that neither the MC-KLN model with @xmath
nor the MC-Glauber model with @xmath can simultaneously explain all flow
harmonics. An additional ingredient is missing from both of these models
in order to produce the correct initial density fluctuation spectrum: In
addition to the fluctuating nucleon positions, fluctuations of the quark
and gluon fields inside the nucleons must be accounted for. Models that
do this were recently developed in [ 79 , 83 , 80 ] .

## Chapter 4 Hydrodynamic event-plane correlations in Pb+Pb collisions

In this chapter, we show that correlations between the flow angles
associated with higher harmonics measured by the ATLAS collaboration
have hydrodynamic origin. The correlation strength is found to be
sensitive to both the initial conditions and the shear viscosity of the
expanding fireball medium. The material in this chapter is based on [ 84
] .

### 4.1 Introduction

In this chapter, we follow the conventions to define the eccentricity
coefficients @xmath with associated participant plane angles @xmath [ 21
, 10 , 40 ] (Chap. 1 ) as:

  -- -------- -- -------
     @xmath      (4.1)
     @xmath      
  -- -------- -- -------

where @xmath is the initial energy density distribution in the plane
transverse to the beam direction at the collision point @xmath . The
final momentum distributions of the emitted charged hadrons are
characterized, as in previous chapters, by their anisotropic flow
coefficients @xmath and their associated flow (event plane) angles
@xmath [ 10 , 85 , 20 ] (Chap. 1 ):

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

As pointed out in [ 86 , 87 ] , a complete understanding of the entire
spectrum of harmonic flow coefficients @xmath is expected to yield
strong constraints on the initial conditions and dynamical evolution of
heavy-ion collisions, in particular the transport coefficients of the
fireball medium. The authors of [ 40 , 88 , 89 , 90 , 91 , 92 ] added
that correlations between the event plane angles @xmath of different
harmonic order can yield valuable additional insights into the initial
conditions. Such correlations were measured with good precision by the
ATLAS Collaboration in Pb+Pb collisions at the LHC [ 93 ] . In this
chapter, we demonstrate that some of the measured final-state
event-plane correlations have a qualitatively different centrality
dependence from the corresponding initial-state participant-plane
correlations, and that this characteristic change between initial and
final state is correctly reproduced by hydrodynamic evolution. This
provides additional strong support for the validity of the hydrodynamic
paradigm in relativistic heavy-ion collisions. Furthermore, we show that
the measured event-plane correlations are not only sensitive to the
initial conditions, but also to the shear viscosity of the hydrodynamic
medium, thus providing an independent constraint for this key transport
coefficient.

### 4.2 Methodology

We evolve fluctuating initial energy density profiles for Pb+Pb
collisions at @xmath ATeV using event-by-event viscous hydrodynamics. To
explore the sensitivity to model uncertainties in the initial state, we
have evolved events from two sets of initial conditions obtained from
the Monte-Carlo Glauber and the Monte-Carlo KLN models [ 56 , 6 ] ¹ ¹ 1
As mentioned in Chap. 3 , it was realized later that neither the
MC-Glauber model nor the MC-KLN model can give a complete description to
all the flow data, but the transition to the emerging new
initial-condition models had not happened when this thesis was written;
the only initial-condition models that are available to us are the
MC-Glauber and the MC-KLN models. . We divided each set into centrality
classes according to the number @xmath of wounded nucleons (see tables
B.2 - B.7 ); for each centrality class, we evolved 11,000 events for
each of the two initial condition models. Model parameters were tuned to
reproduce the @xmath spectra and elliptic flows of unidentified charged
particles and identified hadrons, as reported in [ 44 , 71 ] (Chap. 3 ).
As explained in Chap. 3 , this results in a choice of specific shear
viscosity @xmath for MC-Glauber initial conditions and the larger value
@xmath for MC-KLN initial conditions. Both the QGP phase and the
hadronic phase are evolved hydrodynamically; particle momentum
distributions are calculated with the Cooper-Frye prescription, taking
into account strong decays of all hadron resonances with masses up to
@xmath GeV. ² ² 2 We found, however, that the event-plane correlations
discussed below are almost identical for all particle species, so
including resonance decays is not essential for this work. From the
resulting charged hadron distribution we calculate for each event the
flow angles @xmath according to

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

employing the same pseudorapidity range @xmath and lower @xmath cutoff
@xmath GeV as used in the experimental analysis [ 93 ] . ³ ³ 3 The ATLAS
results were obtained with two independent methods: (a) using a
calorimetric measurement of transverse energy @xmath over rapidity range
@xmath , and (b) using charged particle tracks with @xmath GeV and
@xmath . The data from method (a) have better precision but are fully
compatible with those from method (b), within error bars. Since we
cannot simulate the calorimetric response of ATLAS theoretically, we
compute the event-plane correlations according to method (b), but
compare them in the figures to the more precise data obtained from
method (a). ⁴ ⁴ 4 It was however realized only recently that the flow
analysis performed by the ATLAS collaboration suffers from inaccuracies
caused by the event-plane method, and an analysis with the
scalar-product method yields much better agreement between theory and
experimental data [ 94 ] . From these event plane angles we compute for
each event @xmath for the two-plane ( @xmath ) and three-plane ( @xmath
) correlations listed in Tables 1 and 2 of Ref. [ 93 ] and shown in the
figures below, and then average this quantity over all events in the
given centrality class. We compare these event-plane correlations with
the corresponding correlations between the initial-state participant
plane angles, @xmath , calculated from the initial energy density
profile of each propagated event according to Eq. ( 4.1 ) and then
averaged over events in a similar way.

### 4.3 Results

Figures 4.1 and 4.2 show the initial and final state two-plane
correlations, for the eight different combinations of angles and weight
factors explored by the ATLAS experiment [ 93 ] . Each correlation
function is plotted against collision centrality, with peripheral
collisions (small @xmath values) on the left and central collisions
(large @xmath ) on the right. Fig. 4.1 shows that several of these
correlations are quite sensitive to the model used to generate the
initial energy density profiles (MC-Glauber vs. MC-KLN). These model
differences in the initial state manifest themselves in corresponding
model differences between the final-state event-plane correlations shown
in Fig. 4.2 , but they are additionally modified by the different shear
viscosities @xmath (0.08 and 0.2, respectively) used to evolve the
initial conditions from the two models. This is most clearly seen in the
“3-6 correlation”, where the two models give almost identical
initial-state participant-plane correlations @xmath (second lower panel
from the left in Fig. 4.1 ) whereas the corresponding final-state
event-plane correlators @xmath exhibit significant model differences.
This demonstrates the sensitivity of these event-plane correlations to
the specific shear viscosity of the expanding fireball medium.

It is worth emphasizing that several of these two-plane correlators
exhibit dramatically different centrality dependences for the
initial-state participant-plane and the final-state event-plane angles
(see, for example, the upper left, two upper right and second lower left
panels in Figs. 4.1 and 4.2 ). The difference is largest in peripheral
collisions (small @xmath ). This effect is caused by a dynamical
rotation of the event-plane angles during the hydrodynamic evolution,
driven by large elliptic flow in non-central collisions which leads to
mode coupling between the angles @xmath and @xmath (where @xmath is an
integer and the largest coupling coefficient should correspond to @xmath
). ⁵ ⁵ 5 This is different from the mode coupling at freeze-out [ 95 ]
caused by an elliptic (quadrupole) deformation of the collective flow
velocity appearing in the exponent of the Boltzmann factor in the
Cooper-Frye expression for the final-particle momentum distribution that
couples @xmath with @xmath . In contrast, in the presence of strong
elliptic flow, the nonlinear hydrodynamic evolution before freeze-out
leads to mode coupling between the modes @xmath and @xmath ( @xmath
integer) for the entire complex flow vector on the left-hand side of
Eq. ( 4.2 ). We will come back to this point later in this chapter.

Figures 4.3 and 4.4 show a number of three-plane correlations studied by
the ATLAS experiment [ 93 ] , with the initial-state participant-plane
correlators plotted in Fig. 4.3 and the corresponding final-state
event-plane correlators in Fig. 4.4 , together with the experimental
data. Again, we observe characteristic sign changes between several of
the initial-state correlations and their corresponding final-state
correlators. Even if neither of the two initial-state models (MC-Glauber
and MC-KLN) reproduces the experimental data exactly, we find it
impressive that the hydrodynamic model reproduces all the qualitative
features of the centrality dependences of the 14 different measured
event-plane correlation functions correctly: where the data show strong
(weak) correlations, the same is true for the theoretical results, and
where the data show correlations that increase (decrease) from
peripheral to central collisions, the same holds for the theoretical
predictions, without any parameter tuning. This provides very strong
support for the hydrodynamic model description of the fireball evolution
from a new set of observables that is quite independent of all
previously studied observables ( @xmath -spectra, anisotropic flow
coefficients @xmath , and HBT radii).

We note that the nonlinear mode coupling first discovered in [ 20 ] (see
Chap. 2 ) and the event-plane rotations driven by this nonlinear effect,
are key to the qualitative agreement between theory and data in Figs.
4.2 and 4.4 . It was recently shown that the measured correlation can
also be reproduced with the AMPT model [ 94 ] , a microscopic model that
also features strong collective flow and an approach to approximate
local thermal equilibrium [ 96 ] . We doubt that a similar agreement can
be obtained with dynamical models that do not rely on a large degree of
local thermalization in the expanding fireball, or from an approach
based on linear [ 40 , 88 , 97 ] hydrodynamic response to the
initial-state density fluctuations. Inclusion of first-order nonlinear
terms in the hydrodynamic response [ 98 ] appears to yield event-plane
correlations with qualitatively similar features as shown here [ 99 ] ,
but quantitative success likely requires a numerical approach that fully
accounts for the intrinsic nonlinearity of viscous hydrodynamics.

A closer look at Figs. 4.1 and 4.3 shows that the MC-KLN model tends to
produce stronger correlations between the initial-state
participant-plane angles @xmath than the MC-Glauber model. We observe
that hydrodynamic evolution translates the stronger initial-state
participant-angle correlations into stronger final-state event-plane
correlations, even though the signs of some of the correlators featuring
the strongest correlation strengths flip between initial and final
state. This is especially true for the two-plane correlations shown in
Fig. 4.1 , while the three-plane correlators exhibit some exceptions to
this “rule” in the most peripheral collisions. The experimental data
appear to prefer the stronger angle correlations in the initial profiles
from the MC-KLN model, even though this model gives an
elliptic-to-triangular flow ratio @xmath that is much larger than
measured [ 44 ] , caused by a larger @xmath ratio than in the MC-Glauber
model [ 20 ] (see Chap. 2 ). These observations show that a combined
analysis of both the anisotropic flow coefficients @xmath and their
associated flow angles @xmath (and the correlations among them) promises
to yield powerful constraints on initial state models for the fireball
energy density profiles created in heavy-ion collisions.

Finally, to demonstrate that the mode-coupling effect responsible for
the observed event-plane angle correlations is dynamically generated
during the hydrodynamic evolution rather than at the freeze-out stage,
we show angle correlations similar to the ones shown above, but
calculated using the flow velocity. We define the anisotropies of the
transverse fluid velocity along the freeze-out surface as:

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

In Fig. 4.5 we show the correlations between the so-defined flow
velocity angles @xmath ⁶ ⁶ 6 We checked that almost identical results
are obtained if the power @xmath is replaced by @xmath in the integrand
of the numerator and the denominator in eq. ( 4.4 ). . It is seen that
the correlations shown in Fig. 4.5 between the anisotropy angles
calculated using the fluid velocity are impressively similar to those
shown in Figs. 4.1 and 4.2 . They still show differences which should be
attributed to either the “freeze-out mode-coupling” effect or a viscous
correction to the equilibrium hadron emission function, or to the
resolution effects discussed in [ 94 ] . Still the qualitative
similarity between the correlation patterns calculated from the fluid
velocity and from anisotropic flows from hydrodynamical simulations is
an interesting finding in the following sense: the fluid velocity, which
is a result of only the dynamical process, cares little about the
mechanism for particle emissions and thus is not affected by final-stage
effects (for example, resonance decay, or the choice of forms for the
viscous correction terms to the distribution function, etc. ). Therefore
the fact that the correlations calculated from the fluid velocity are
similar to those from the anisotropic flow data provides strong evidence
that the correlations among anisotropic flow angles are generated during
the dynamical evolution rather than at the freeze-out stage.

Even though more detailed studies will be necessary to fully explore the
event-plane correlations discussed in this chapter, the calculations
presented here suggest that very likely neither the MC-Glauber nor the
MC-KLN initial conditions will ultimately provide a quantitatively
satisfactory description of the experimental data from the ATLAS
Collaboration [ 93 ] . While this statement is backed up by the study in
Chap. 3 , however, a recent study shows that the quantitative
disagreement between theory results and experiments is very likely to be
an artifact of the event-plane method used in the experimental flow
analysis [ 94 ] . We have not performed the same analysis using our own
simulated results, but no matter what the outcome might be, the question
of how to turn the multitude of anisotropic flow observables (magnitudes
and angles) that are already measured and will be measured into a
focused search for the correct initial-state model is an interesting and
welcome new challenge for the theory community.

## Chapter 5 Resonance decay contributions to higher-order anisotropic
flow coefficients

The computation of resonance decays at the end of a hydrodynamic
calculation in order to obtain the complete spectra of stable hadrons is
numerically very costly and therefore often omitted. In this chapter, we
show that, in hydrodynamic simulations for relativistic heavy-ion
collisions, strong resonance decay calculations can be performed with
fewer species of particle resonances than usually implemented while
preserving good accuracy in single-particle spectra and flow
anisotropies. Such partial resonance calculations boost computational
efficiency by a factor of 10, which is essential for large scale
event-by-event simulations. The material in this chapter is based on [
100 ] .

### 5.1 Chapter introduction

In event-by-event hydrodynamic simulations, due to the limited number of
final-state particles in each event, observables can be measured with
good statistical precision only as ensemble averages, which requires the
simulation of a large number of events. Due to algorithmic progress over
the last few years, the hydrodynamic evolution part is no longer the
bottleneck in such event-by-event studies; at least for
(2+1)-dimensional simulations (which assume longitudinal
boost-invariance), the largest fraction of the computer time is spent
converting the hydrodynamic output into final particle distributions,
either on a “switching surface” between a macroscopic hydrodynamic
description of the QGP fluid and a microscopic kinetic evolution of the
dilute late hadronic-rescattering stage [ 55 , 101 ] or on a “kinetic
decoupling” surface marking the transition from a strongly coupled fluid
directly to a noninteracting gas of free-streaming hadrons. The high
numerical cost of this “hydro-to-particle conversion” process results
from the large number of unstable hadron resonances that need to be
included and whose post-freeze-out decays (mostly due to
strong-interaction processes, although for some comparisons with
experimental data that have not been corrected [ 102 , 103 ] for
weak-decay feed-down, weak and electromagnetic decays must also be
considered) modify the finally observed particle distributions.

The hydro-to-hadron conversion algorithm is based on the Cooper-Frye
formula [ 32 ] , which expresses the final hadron momentum distribution
as an integral of the local equilibrium (for ideal fluid dynamics) or
slightly off-equilibrium (in viscous fluids) distribution function for
the particle species in question over the conversion surface.
Contributions to the spectra of experimentally measured stable particles
from the strong decays of unstable resonances are then calculated from
the single-particle spectra for the resonances [ 104 ] . This requires
the calculation of the directly emitted (“thermal”) particle momentum
distributions for all @xmath hadron species with mass typically up to 2
GeV via Cooper-Frye integrals, followed by the evaluation of the
phase-space integrals [ 104 ] for all contributing decay channels. On a
typical personal computer with a single CPU core in year 2012 this
calculation takes about 2-3 hours, compared to 10-15 minutes for the
preceding hydrodynamic evolution.

The @xmath GeV cutoff in resonance mass is dictated by requiring
convergence of the relative particle yields of the measured hadronic
final state after all unstable resonances have been allowed to decay.
(The pion yields are especially sensitive to resonance feeddown.)
Experimental evidence points to chemical decoupling at a temperature of
@xmath MeV, i.e. close to the (pseudo)critical temperature for the
quark-hadron phase transition [ 105 ] ; at this temperature, only
resonances with masses above 2 GeV are sufficiently strongly
Boltzmann-suppressed that their decay contributions to stable particle
yields can be safely ignored.

Here we show that for an accurate determination of the pion and proton
anisotropic flow coefficients @xmath , a much smaller number of
resonances needs to be taken into account than for the hadron yields,
and that even the shape of the azimuthally averaged pion and proton
transverse momentum spectra can be reliably determined by accounting for
only a small subset of the @xmath resonance species mentioned above.
These are the observables needed for an extraction of the QGP shear
viscosity from heavy-ion collision experiments [ 30 ] . In this chapter,
we show that by rearranging the resonance decay table in the order of
decreasing importance for the calculation of @xmath -spectra and @xmath
coefficients instead of increasing mass, good convergence for these
observables can be achieved with a significantly reduced set of only
about 20-30 resonances. This speeds up the computation by a factor of 10
– a significant gain in efficiency for the iterative determination of
the QGP shear viscosity.

The analysis presented here uses final states generated with the same
@xmath -dimensional boost-invariant viscous hydrodynamic code VISH2+1
for 200 @xmath GeV Au+Au collisions at the Relativistic Heavy-Ion
Collider (RHIC) and for 2.76 @xmath TeV Pb+Pb collisions at the Large
Hadron Collider (LHC) at various collision centralities, with previously
determined [ 30 , 71 , 44 ] hydrodynamic input parameters. We find very
similar results at both collision energies and therefore show here only
plots for LHC collisions. Since the decay contributions from different
resonances to the mentioned observables depend only on their decay
channels and transverse momentum distributions, we expect little
sensitivity to the assumption of longitudinal boost-invariance implicit
in our approach and expect our reordered resonance decay tables to
perform equally well for both @xmath -d and @xmath -d hydrodynamic
simulations, and for a wide range of input parameters, such as QGP
viscosity, thermalization time, initial entropy and energy density, etc.

### 5.2 Resonance ordering

The momentum distributions of directly emitted (“thermal”) resonances of
species @xmath are computed from the Cooper-Frye formula [ 32 ] :

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

Here @xmath is the hydro-to-hadron conversion hypersurface, @xmath is
its surface normal vector, @xmath is the Bose or Fermi thermal
equilibrium distribution function, and @xmath accounts for viscous
corrections (driven by the viscous pressure tensor @xmath on the
conversion surface) of the local phase-space distribution along @xmath .
We assume the quadratic form [ 106 , 107 ] :

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

Resonance decays increase the total yields of the stable hadrons and
change their momentum distributions. For kinematic reasons, most of the
light decay daughters have low transverse momenta, thus modifying the
shape of light stable hadrons (pions, kaons) particle spectra mostly in
the region @xmath GeV [ 104 ] . We denote the total decay contribution
to the momentum distribution of stable hadron species @xmath by @xmath ,
and the total spectrum (obtained by adding this to the thermally emitted
spectrum @xmath ) by @xmath . (We here include only strong and
electromagnetic decays.) The @xmath -integrated total yield @xmath of
decay products of species @xmath is denoted by @xmath , with @xmath ,
where the sum is over resonances @xmath and @xmath is the effective
branching ratio (see Eq. ( 5.2 ) below) for the decay @xmath .

The contribution to @xmath from a particular resonance @xmath is not
only influenced by its mass (through the Boltzmann suppression factor
@xmath ), but also by its spin degeneracy factor @xmath and its
branching ratio @xmath into the decay channel that feeds stable particle
species @xmath . For each stable hadron species @xmath , it is therefore
a different set of resonances that makes the most important
contributions. Our goal is to order the resonances in decreasing order
of importance for each stable particle species @xmath . We here assume
that the conversion surface has constant temperature @xmath . The
different hadron resonances have @xmath -dependent nonequilibrium
fugacities @xmath that ensure constant stable particle ratios equal to
their chemical equilibrium values at @xmath and @xmath , independent of
the hydro-to-hadron conversion temperature @xmath . While the actual
fractions contributed by each resonance to the stable particle yields
depend on @xmath , the ordering of these fractions is largely @xmath
-independent.

We start from the resonance table in the AZHYDRO package, ¹ ¹ 1 AZHYDRO
is available at http://www.physics.ohio-state.edu/~froderma/ . which
includes @xmath species of hadrons (counting different isospin states
such as @xmath , @xmath , @xmath as separate species) with rest masses
up to @xmath GeV. After fixing the value of @xmath we look up the
non-equilibrium fugacity @xmath for each of these 319 species from the
EOS s95p-PCE tables constructed in Ref. [ 11 ] . For each stable
particle species @xmath , we then generate an ordered list of resonances
@xmath that can decay directly into @xmath . Note that, in this
ordering, we account not only for direct decay contributions, but also
for multi-step decay cascades, where @xmath first decays into an
unstable resonance @xmath which further decays (directly or through more
intermediate steps) into the stable species @xmath .

Table 5.1 shows the beginning of this contribution table for positively
charged pions, for a conversion temperature @xmath MeV. The “total
contribution” percentages @xmath in the third column are computed as

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (5.3)
     @xmath   @xmath   @xmath      (5.4)
  -- -------- -------- -------- -- -------

where the effective branching ratios @xmath in Eq. ( 5.3 ) account for
multi-step decay cascades as follows:

  -- -------- --
     @xmath   
  -- -------- --

The sum over @xmath in ( 5.4 ) takes care of quantum statistical
effects, with the upper (lower) sign for bosons (fermions). For all
hadrons except pions, accurate results can be obtained by keeping only
the first term @xmath , i.e. by ignoring quantum statistical effects.
Even for pions, a few @xmath -terms suffice for good precision (in our
calculations we truncate the series in ( 5.4 ) at @xmath ). The complete
ordered resonance decay contribution tables for @xmath , @xmath , @xmath
, @xmath , @xmath and @xmath are given in the Appendix (Sec. C ).
Horizontal lines in the tables indicate where the cumulative resonance
decay contributions @xmath exceed certain threshold percentages (as
indicated) of the total resonance decay contribution to species @xmath .

In the following section, we show the stable hadron @xmath -spectra and
their anisotropic flow coefficients as functions of these cumulative
decay contribution percentages @xmath in order to assess how many
resonances from these ordered decay tables should be included for an
accurate computation of these observables.

### 5.3 Results and discussion

Using the ordered tables described in Sec. 5.2 and truncating the sum
over resonance decay contributions at @xmath values corresponding to
various different cumulative resonance decay contribution thresholds
@xmath , we performed calculations for @xmath , @xmath , and @xmath . We
tested individual bumpy as well as (ellipticity-aligned and
ensemble-averaged) smooth initial conditions at both RHIC and LHC
energies for a variety of collision centralities. Since the results were
found all to be qualitatively similar, we show only a small selection,
focussing on pions and protons from one bumpy Pb-Pb event from the
@xmath centrality class and from the smooth averaged initial condition
corresponding to the @xmath centrality class, both at LHC energy (
@xmath GeV).

Figure 5.1 shows the pion @xmath -spectra for the bumpy central
collision in the upper panels and the smooth peripheral event in the
lower panels. The left panels show the usual semilogarithmic plots of
the absolutely normalized @xmath -distribution. As is well-known, the
directly emitted (“thermal”) pions constitute only about 50-60% of all
observed pions, the rest coming from resonance decays. The “thermal”
spectrum also has the wrong shape: resonance-decay pions predominantly
contribute to the low- @xmath part of the spectrum, making it steeper.
However, this shape difference between the truncated and full
resonance-decay spectrum disappears almost completely already when
including only the 9 strongest decay channels, accounting for just 60%
of the total pion yield from resonance decays. This is shown in the
right panels of Fig. 5.1 where we plot the ratio

  -- -------- -- -------
     @xmath      (5.6)
  -- -------- -- -------

for @xmath as a function of @xmath . ( @xmath is the contribution to
particle species @xmath from decays of particle species @xmath (see
Eq. ( 5.3 )), and @xmath is the index of the last resonance in the
ordered resonance decay table from Sec. 5.2 .) The numerator includes
only resonance decays up to @xmath , but we renormalize those decay
contributions by the cumulative decay contribution @xmath corresponding
to the same @xmath value. ( @xmath is easily calculated from Eqs. ( 5.3
, 5.4 ) and directly obtained by summing the entries in the third column
of the resonance decay table.) This renormalization corrects for the
missing yield from the truncation of the decay table. The remaining
effect (after missing yield renormalization) of the truncation on the
shape of the @xmath -spectrum is seen in panels (b) and (d) of Fig. 5.1
: Whereas without any resonance decays the ratio @xmath changes by
almost a factor 2 between @xmath and 2 GeV, this variation is reduced to
less than 5% already for @xmath for both bumpy and smooth initial
conditions in both central and peripheral collisions.

In Fig. 5.2 , we show in the same way the proton spectra. Again the
shape of the spectra can be accurately reproduced by taking into account
a small fraction of all decay contributions (note the expanded vertical
scale in Figs. 5.2 b,d): after renormalization to account for the
missing yield, just the 4 strongest of 75 decay channels (three charge
states of the @xmath resonance and one charge state of @xmath ),
corresponding to 60% of the total resonance decay yield for protons,
reproduce the full proton spectrum with @xmath error between @xmath and
2 GeV.

We conclude that, by accounting for the missing yield through
appropriate renormalization, the correctly normalized total pion and
proton spectra can be obtained, with shape errors @xmath , by including
only the strongest decay channels accounting for the leading 60% of the
total resonance decay yields. A quick look at the tables in the Appendix
shows that this will reduce the number of resonance decays (and thus
computer time) by at least a factor 10.

We now proceed to a discussion of the differential and @xmath
-integrated anisotropic flow coefficients @xmath defined by eq. ( 2.21 )
and eq. ( 2.22 ). In the formula, the spectrum @xmath includes all
contributions from the ordered resonance decay table for the considered
stable species up to a certain threshold @xmath , with the truncated
resonance decay contribution renormalized for the missing yield by a
factor @xmath as shown in the numerator of Eq. ( 5.6 ). In Figs. 5.3 and
5.4 , we specify the cumulative decay contribution percentage @xmath to
indicate the truncation level corresponding to each curve.

Figure 5.3 shows the differential elliptic and triangular flows for
pions and protons, for one single bumpy central (0-10% centrality)
event. We see that once again excellent agreement with the full
resonance decay calculation is already obtained when including only the
small subset of resonances that account for the top 60% of the resonance
decay yields. We checked that this result is generic, i.e. it does not
depend on the selected event (although the elliptic and triangular flows
do).

For the @xmath -integrated harmonic flow coefficients @xmath , we show
in Fig. 5.4 results for all harmonic orders from @xmath to 9, again for
pions and protons and for a bumpy central as well as a smooth peripheral
event. For the smooth averaged initial condition, the odd harmonics
vanish by symmetry. For fluctuating initial conditions, the @xmath
values shown here and their relative size depend on the randomly
selected event. All plots shown in this chapter are based on one and the
same bumpy central collision event.

For each harmonic order @xmath , Fig. 5.4 shows two sets of results. The
left set corresponds to results obtained by using the truncated
resonance decay spectra shown in Figs. 5.1 a,c and 5.2 a,c, without
missing yield renormalization. The right set uses the renormalized
truncated decay spectra as defined in the numerator of Eq. ( 5.6 ). One
observes a much faster convergence towards the full result in the right
sets than in the left sets. The reason is that, by renormalizing the
truncated resonance decay contributions for the missing yield, the
correct mixing ratio between direct thermal and indirect decay
contributions is ensured and the shape of the total @xmath -spectrum is
approximated much more accurately than without renormalization (see
Figs. 5.1 b,d and 5.2 b,d). Figure 5.4 demonstrates that, when using the
renormalized truncated decay spectra, accounting for just the top 60%
decay contributions (i.e. including only the 9 strongest decay channels
contributing to pions and the 4 strongest decay channels contributing to
the proton spectra) reproduces the full results for the harmonic flow
coefficients @xmath with excellent precision: the lines corresponding to
different @xmath values @xmath are almost indistinguishable.

Future precision extractions of the QGP viscosity may require highly
precise @xmath values. For such a purpose one can adjust @xmath to
include a larger fraction of all resonance decays if needed.

For a given precision, the required minimal @xmath truncation indices
and cumulative resonance decay fractions @xmath for kaons lie between
those for pions and protons. The @xmath for @xmath , K, @xmath are
almost identical at RHIC and LHC energies, i.e. only weakly sensitive to
radial flow.

### 5.4 Chapter conclusions

In this chapter, we showed that for a sufficiently accurate
determination of the differential anisotropic flow coefficients @xmath ,
only those resonances need to be included that generate the top 60% of
the largest decay contributions to the stable particle yields. For the
single particle spectra, correct normalization of the total yield
requires a renormalization of the truncated resonance decay yield as
given in the numerator of Eq. ( 5.6 ). With this renormalization, good
convergence of the slope of the pion spectra and of the @xmath
-integrated anisotropic flow coefficients @xmath requires inclusion of
only the 9 strongest contributing channels for pions and only the 4
strongest channels for protons, accounting in both cases for just 60% of
the total decay yield. This reduces the number of resonance decay
channels to be evaluated by a factor @xmath , without loss of precision,
leading to a similar reduction of the total computing time for the final
stable hadron distributions.

In hybrid model calculations [ 101 ] , the late hadronic stage is
described microscopically by a Boltzmann cascade that propagates a
reduced set of resonances until final kinetic decoupling. In this case,
the spectra of all unstable resonances that are explicitly included in
the Boltzmann cascade must be generated on the conversion surface. This
is still only a small subset of all resonances included in the resonance
decay tables. The optimal ordering of the resonance decay tables for the
purpose of generating input for the late-stage Boltzmann cascade and the
corresponding optimized truncation fractions @xmath are left as an open
question.

## Chapter 6 Fluctuating flow angles and anisotropic flow measurements

Event-by-event fluctuations in the initial density distributions of the
fireballs created in relativistic heavy-ion collisions lead to
event-by-event fluctuations of the final anisotropic flow angles, and
density inhomogeneities in the initial state cause these flow angles to
vary with the transverse momentum of the emitted particles. In this
chapter, we show that these effects lead to characteristically different
transverse momentum dependencies for anisotropic flow coefficients
extracted from different experimental methods. These differences can be
used to experimentally constrain flow angle fluctuations in the final
state of heavy-ion collisions which, in turn, are sensitive to the
initial-state density fluctuations and the shear viscosity of the
expanding fireball medium.

The material in this chapter is based on [ 108 ] ; although for
completeness, the whole material is included, only part of the content
was contributed by the author.

### 6.1 Chapter introduction

Due to quantum fluctuations of the positions of the nucleons inside the
colliding nuclei and of the positions of the colored quark and gluon
constituents inside each nucleon, the density of the fireball matter
created in collisions between ultra-relativistic heavy ions is highly
inhomogeneous in the transverse plane, even for collisions with
identical impact parameters. It has been shown in this thesis that the
initial density inhomogeneities lead to highly anisotropic pressure
gradients, causing an anisotropic collective expansion of the fireball,
whose harmonic flow coefficients @xmath and associated flow angles
@xmath (both defined below) fluctuate from collision to collision [ 24 ]
.

While @xmath fluctuations and the effect of their variance on different
methods for measuring @xmath have been studied extensively over the last
few years, flow angle fluctuations and correlations have only recently
found attention [ 20 , 40 , 109 , 98 , 92 , 93 , 84 , 110 , 111 ] .
Gardim et al. [ 111 ] pointed out that, since the fluctuating flow
angles @xmath depend on transverse momentum @xmath and rapidity @xmath ,
the usually assumed (and experimentally observed [ 112 , 113 , 114 , 115
] ) factorization of the azimuthal oscillation amplitudes of the
two-particle angular correlations into a product of single-particle flow
coefficients is slightly broken even if these correlations are entirely
due to collective flow. We show here that the @xmath -dependence and
fluctuating nature of the flow angles @xmath also affects the @xmath
-dependence of the experimentally measured differential flow
coefficients @xmath , and that it does so in different ways for
different experimental methods of determining @xmath .

For each collision event, the momentum distribution of finally emitted
particles can be characterized by a set of harmonic flow coefficients
@xmath and flow angles @xmath through the complex quantities (see also
Chap. 2 )

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (6.1)
                       @xmath   @xmath   
     @xmath   @xmath   @xmath            (6.2)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

Here @xmath is the azimuthal angle of the particle’s transverse momentum
@xmath and the curly brackets denote the average over particles from a
single collision. ¹ ¹ 1 The average can include all charged particles or
only particles of a specific identified species; we will not clutter our
notation to account for these different possibilities. Eq. ( 6.1 )
defines the flow coefficients and associated flow angles for the entire
event, whereas Eq. ( 6.2 ) is the analogous definition for the subset of
particles in the event with a given magnitude of the transverse momentum
@xmath . We suppress the dependence of both types of flow coefficients
on the rapidity @xmath . As mentioned before, @xmath are the integrated
anisotropic flows and @xmath are the differential flows. By definition,
both @xmath and @xmath are positive definite. Hydrodynamic simulations
show that in general the flow angles @xmath depend on @xmath and that,
as a function of @xmath , @xmath wanders around the “average angle”
@xmath that characterizes the integrated flow @xmath of the entire event
(see Fig. 6.1 below

and also Fig. 2 in Ref. [ 110 ] ). Some theoretical and experimental
definitions of @xmath have yielded values that turn negative over
certain @xmath ranges; we will see that this is due to defining the
flows of each event relative to a fixed azimuthal angle (for example,
relative to the direction of the impact parameter of the collision in
theoretical calculations, or relative to the integrated elliptic flow
angle @xmath in experiment), and that the same thing can happen for
higher-order harmonic flow coefficients when defining them relative to a
fixed (i.e. @xmath -independent) flow angle @xmath . The subject of this
chapter is to elucidate the origins of such differences between
different anisotropic flow measures and, in particular, the
manifestation of event-by-event fluctuations of the @xmath -dependent
flow coefficients @xmath and flow angles @xmath in different
experimental flow measures.

### 6.2 Differential flows from the event-plane method and from
two-particle correlations

The key experimental difficulty is that, due to the finite number of
particles emitted in each collision, the left hand sides of Eqs. ( 6.1 ,
6.2 ) cannot be determined accurately for a single event. The @xmath are
characterized by probability distributions that depend on the studied
class of events (system size, collision energy and centrality) from
which each collision takes a sample. Experimental flow measurements rely
on a number of different methods that amount to taking different moments
of that probability distribution by averaging over large numbers of
events. Understanding the nature of these moments and reconstructing
them from theoretical event-by-event dynamical simulations are essential
steps in a meaningful comparison between theory and experiment.

Our main interest lies in the event-by-event fluctuations in the initial
state of the collision fireball. These are primarily caused by the
finite number of nucleons (or effective collision centers) in the
colliding nuclei and unrelated to detector capabilities. In addition,
there are fluctuations related to the finite number of particles
produced (or detected) in the event which depend on collision energy and
(in part) on detector capabilities. They reflect the fact that in
practice the final state of the fireball evolution, which in principle
(with the appropriate dynamical evolution model) can be predicted from
the initial state with perfect precision, cannot be measured with
perfect precision, due to finite sampling statistics. In this chapter,
we are not interested in the fluctuations arising from finite sampling
statistics; instead we focus on the hydrodynamical consequences of
unavoidable event-by-event fluctuations in the initial state over which
we have no control since they are rooted in the internal structure of
the colliding nuclei, and which we therefore have to live with in any
case even after we correct the measurements for finite final-state
multiplicity effects.

The most extensively used experimental methods for measuring anisotropic
flows are the event-plane and two-particle correlation methods [ 85 ] .
We begin with a discussion of the latter. Two-particle azimuthal
correlations receive contributions from the anisotropic collective flow
as well as from non-flow correlations; the latter can be minimized by
appropriate experimental cuts and corrected for [ 85 , 25 ] . Again, we
are not interested in non-flow correlations and will here simply ignore
their existence, assuming that they have been corrected for in the
experimental analysis.

Two-particle correlation measures of anisotropic flow are based on
correlators of the type

  -- -------- -- -------
     @xmath      (6.3)
  -- -------- -- -------

where @xmath and @xmath are the azimuthal angles around the beam
direction of two particles with transverse momenta @xmath and @xmath ,
and @xmath denotes the average over @xmath events from a set of given
characteristics (e.g. of collisions in a certain centrality bin),

  -- -------- -- -------
     @xmath      (6.4)
  -- -------- -- -------

whereas @xmath is the average of the observable @xmath over all (or a
specified subset of all) particle pairs in the event @xmath :

  -- -------- -- -------
     @xmath      (6.5)
  -- -------- -- -------

Different chosen subsets for the event-wise average @xmath define
different correlation measures for the anisotropic flow coefficients, as
we will explain below. In this chapter, we will always correlate pairs
of particles of the same kind (e.g. protons with protons or charged
hadrons with charged hadrons, but not protons with charged hadrons),
unless specifically stated otherwise. We will also assume that they have
the same rapidity @xmath ; the generalization to particles with
different rapidities is straightforward, following the procedure
discussed below when we go from particles with the same to particles
with different @xmath .

The magnitudes @xmath of the anisotropic flow coefficients defined in
Eq. ( 6.2 ) fluctuate from event to event according to some probability
distribution @xmath . Let us denote the rms mean of this distribution by
@xmath , and similarly the rms mean for the integrated flow @xmath by
@xmath . These rms means can be obtained from two-particle correlators
of the type ( 6.3 ) as follows:

  -- -------- -- -------
     @xmath      
     @xmath      (6.6)
  -- -------- -- -------

Note that for the differential flow in the first line of equation ( 6.2
), both particles are taken from the same @xmath bin, and that the
event-wise pair averages @xmath factorize in each event due to our
assumptions (absence of non-flow two-particle correlations, independent
hydrodynamic emission of particles 1 and 2). Due to @xmath symmetry
under particle exchange, the exponential can be replaced by the cosine,
and we get

  -- -------- -- -------
                 
     @xmath      (6.7)
  -- -------- -- -------

where @xmath and the pair distribution has already been integrated over
the average angle @xmath .

Note that in Eqs. ( 6.2 ), the single-event averages are normalized by
the number of pairs in the event, before averaging over events. This is
important: since the pair multiplicity fluctuates from event to event
and within a multiplicity bin, and multiplicity anti-correlates with
impact parameter with which the magnitudes of some of the anisotropic
flow coefficients are geometrically correlated, this event-wise
normalization avoids biasing the measured flow coefficients towards
their values in events with larger than average multiplicity.

Our definition of the integrated flow @xmath agrees with the standard
definition for the “two-particle cumulant” flow @xmath [ 117 , 118 , 119
] , but the same is not true for the differential flow @xmath which
differs from @xmath . The experimental definition of @xmath is [ 117 ,
118 , 119 ]

  -- -------- -- -------
     @xmath      
     @xmath      (6.8)
  -- -------- -- -------

Here only the first of the two particles within an event is taken from
the desired @xmath bin and particle species; it is correlated with all
other particles detected in the event, with obvious statistical
advantages compared with @xmath which requires both particles to be of
the same kind and from the same @xmath bin. The normalization factor is
the total rms flow of all charged hadrons. The last expression shows
that @xmath reduces to @xmath if and only if the flow angle @xmath does
not depend on @xmath , the event-by-event fluctuations of @xmath affect
only its normalization but not the shape of its @xmath dependence, and
the @xmath fluctuations of the particle species of interest are
proportional to those of all hadrons. All of these assumptions are
violated in hydrodynamic simulations of bumpy expanding fireballs. The
difference between @xmath and @xmath , is thus sensitive to
event-by-event fluctuations of the @xmath -dependent difference @xmath
between the flow angle of particles with momentum @xmath and the average
event flow angle, ² ² 2 More precisely, @xmath depends on the difference
between @xmath of the particles of interest and the average flow angle
@xmath of all detected particles . We checked numerically that the
average hydrodynamic flow angles @xmath for identified pions and protons
agree with great precision with the average flow angles for all
particles in the event: computing the ensemble average of @xmath for all
harmonics @xmath and all collision centralities, we found deviations of
less than @xmath in all cases except for some of the high-order
harmonics with @xmath , whose calculation is plagued by numerical errors
at low @xmath GeV arising from the finite grid spacing of our square
numerical grid used in solving the hydrodynamic equations. in addition
to the (largely independent) fluctuations in the magnitudes of @xmath
and @xmath .

Another approach to isolating effects arising from the @xmath
-dependence of the flow angles is a comparison of the @xmath -dependent
rms flow @xmath with the so-called event-plane flow ³ ³ 3 One can
replace the cosine function in this definition by the exponential,
omitting taking the real part in the second line, since the flow-angle
fluctuations are symmetrically distributed such that the imaginary part
vanishes after taking the event average (this has been verified
numerically). @xmath ⁴ ⁴ 4 Note that we define the @xmath -order
event-plane flow relative to the @xmath -order flow plane @xmath , and
not relative to the elliptic flow plane @xmath as sometimes done.

  -- -------- -- -------
     @xmath      
                 
     @xmath      (6.9)
  -- -------- -- -------

The equality in the second line arises from Eq. ( 6.2 ). Here for each
event the “average flow angle” @xmath is first obtained by computing the
@xmath vector [ 120 ]

  -- -------- -- --------
     @xmath      (6.10)
  -- -------- -- --------

(where @xmath is the number of detected particles in the event) and
determining its phase. In principle, different choices for the weights
@xmath can be considered [ 85 ] , but for consistency with Eq. ( 6.2 )
one must choose @xmath . The “average angle” @xmath for the event
extracted from @xmath in general depends on the types of particles
included in the sum in Eq. ( 6.10 ). As noted in footnote 2, however,
the average flow angle for particles emitted from a hydrodynamic source
is (within numerical precision) the same for all particle species, and
the precision of extracting @xmath in experiments can thus be maximized
by including all detected particles in the @xmath vector ( 6.10 ). ⁵ ⁵ 5
Since in this paper we ignore finite particle statistical fluctuations
in the final state, we know @xmath with infinite precision for each
particle species, and we will simply use these particle-specific values
in our numerical results below.

The last line in Eq. ( 6.2 ) makes it clear that the differential
event-plane flows @xmath are sensitive to the event-by-event
fluctuations of the @xmath -dependent flow angles @xmath around the
“average flow” angle @xmath . Just like the finite-number statistical
fluctuations ⁶ ⁶ 6 Due to the finite number of particles detected in
each event, the accuracy of determining @xmath is limited by finite
number statistics, and an accurate experimental estimation of the
event-plane flow @xmath requires an “event-plane resolution correction”
[ 85 ] . As shown in [ 24 , 25 ] (see also the discussion in [ 111 ] ),
which moment of the underlying @xmath distribution is actually measured
by the total event-plane flow @xmath depends on this event-plane
resolution: for perfect resolution, @xmath approaches the average flow
@xmath , whereas in the case of poor resolution, it is closer to the rms
flow @xmath [ 24 , 25 ] . The mathematical analysis in [ 25 ] applies
only to the integrated flow which allows one to ignore the @xmath
-dependence of @xmath fluctuations as well as initial-state-related,
@xmath -dependent fluctuations of the flow angles that are not caused by
finite multiplicity in the final state. In view of the latter,
event-plane resolution effects on differential flow measurements and
their correction require a new analysis. of the flow angle reconstructed
from @xmath around the “true” flow angle of the event, these
fluctuations smear out the azimuthal oscillations of the transverse
momentum spectra and thus reduce the oscillation amplitudes @xmath . In
contrast to the former, they arise from fluctuations in the initial
state and thus cannot be eliminated by improving or accounting for the
resolution of the measurement of the final state. They carry valuable
physical information about the initial state and the dynamics of its
evolution into the final state.

We can remove the sensitivity of the measured quantity to the @xmath
-dependent fluctuations of the flow angle by first computing for each
event the magnitude @xmath of @xmath , before summing over events:

  -- -------- -- --------
     @xmath      
     @xmath      (6.11)
  -- -------- -- --------

Since the quantity inside the event average does not depend on the
average flow angle @xmath , this observable is not subject to an
event-plane resolution correction. However, due to finite multiplicity
in the final state, the right hand side will still in general be
positive and non-zero experimentally even if there is no underlying
anisotropic flow in the event. Again, how to properly account for such
finite sampling statistical effects requires additional analysis.

By comparing @xmath ( 6.2 ) with @xmath ( 6.2 , 6.2 ), @xmath ( 6.2 ),
and @xmath ( 6.2 ), we can experimentally assess and separate the
relative importance of event-by-event fluctuations in the magnitudes and
directions of the anisotropic flows as functions of @xmath .

Let us now proceed to two-particle correlations between particles of
different (but specified) momenta. Since in the first line of Eq. ( 6.2
), both particles are taken from the same bin in @xmath , the flow angle
@xmath drops out from the expression. This is not true for azimuthal
correlations between two particles with different @xmath [ 111 ] . In
this case, one finds [ 111 , 66 ]

  -- -------- -- --------
                 
     @xmath      
     @xmath      
     @xmath      (6.12)
  -- -------- -- --------

Due to parity symmetry, @xmath is real: while the quantity inside the
event average @xmath is in general complex for each individual event,
its imaginary part averages to zero when summed over many events.

To properly account for multiplicity fluctuations, in Eq. ( 6.2 ) the
averages @xmath within an event are once again normalized by the total
number of particles included in the average, similar to Eq. ( 6.2 ). For
this reason, @xmath defined in Eq. ( 6.2 ) is not identical with the
experimental quantity @xmath , which is obtained from a Fourier
decomposition with respect to the difference angle @xmath of the
two-particle distribution obtained by summing over many events, without
normalizing the contribution from each event by the corresponding event
multiplicity [ 112 , 113 , 114 , 66 , 115 ] . For a meaningful
comparison between theory and experiment, one should either normalize on
the experimental side the contribution from each event to the
two-particle distribution by the number of pairs in the event, or weight
the theoretical prediction for @xmath for each event @xmath with a
factor @xmath before summing over events. We prefer the first option,
since it avoids the geometric bias arising from the correlation between
collision geometry and particle multiplicity.

Equation ( 6.2 ) makes it obvious that the two-particle correlation
coefficient @xmath does not factorize into a product of single-particle
anisotropic flow coefficients [ 111 ] . There are two contributions to
this breaking of factorization: @xmath -dependent event-by-event
fluctuations of the magnitude of the flow coefficient @xmath , and
@xmath -dependence of the flow angles @xmath [ 111 ] (which also
fluctuate from event to event). It is possible to define a
non-factorizing correlator that is only affected by the fluctuations of
@xmath but is insensitive to the flow angles:

  -- -- -- --------
           (6.13)
  -- -- -- --------

It is obtained experimentally by first obtaining the magnitude of @xmath
for each event, normalizing it to the number of pairs used for its
computation, and then adding the results for many events. Its
sensitivity to finite-number statistical effects should be similar to
Eq. ( 6.2 ) and needs to be explored. By comparing the quantity @xmath
from Eq. ( 6.2 ) with @xmath from Eq. ( 6.13 ) one can assess the
importance of the @xmath -dependence and event-by-event fluctuations of
the flow angles @xmath (which affect the former but not the latter).

### 6.3 The effect of flow fluctuations on differential @xmath measures

In this section we compare the differential flows @xmath extracted from
the 22,000 viscous hydrodynamic simulations per centrality bin of 2.76
@xmath TeV Pb+Pb collisions at the LHC (11,000 each with MC-Glauber and
MC-KLN initial density profiles) that were generated [ 84 ] . We use the
Cooper-Frye prescription to compute from the hydrodynamic output on the
freeze-out surface the single-particle distributions @xmath as
continuous functions of @xmath and @xmath (i.e. we do not sample the
distribution to generate a finite number of particles per event, but
pretend that the spectrum is sampled infinitely finely – this avoids the
need to correct for effects arising from finite-number statistics, such
as imperfect event-plane resolution). All resonance decays are included
in the final stable-hadron spectra. The details of the hydrodynamic
simulations, initial conditions and freeze-out parameters are not
important for the qualitative study presented here, but they can be
found in Refs. [ 84 , 44 , 71 ] . Here we only note that MC-Glauber
(MC-KLN) initial conditions were hydrodynamically evolved with specific
shear viscosity @xmath (0.2).

We present results for pions and protons, representing light and heavy
particle species. Qualitatively, although not quantitatively, the same
generic features are observed with MC-KLN and MC-Glauber model initial
density profiles, and we show examples of both. Figures 6.2 and 6.3 show
elliptic and triangular flows in their left and right panels, for
central (Fig. 6.2 ) and peripheral (Fig. 6.3 ) Pb+Pb collisions. The
curves for @xmath and @xmath , which are not affected by flow angle
fluctuations (Eqs. ( 6.2 ) and ( 6.2 )), should be compared with those
for @xmath and @xmath , which are affected by the @xmath -dependence of
the flow angles @xmath and their event-by-event fluctuations (Eqs. ( 6.2
) and ( 6.2 )): for protons with transverse momenta below about 1 GeV,
flow angle fluctuations are seen to cause a significant suppression of
the latter (in some cases even leading to negative elliptic flow
values). ⁷ ⁷ 7 Note that the factor @xmath in Eqs. ( 6.2 ) and ( 6.2 )
is maximal if @xmath is always aligned with @xmath . The suppression of,
say, @xmath relative to @xmath does therefore not indicate a definite
momentum tilt of the emitting source at a given @xmath relative to the
average @xmath , but simply reflects a nonzero difference @xmath that
fluctuates from event to event, suppressing the value of @xmath for
either sign of the difference. For the much lighter pions, flow angle
fluctuation effects are almost invisible at low @xmath . For protons
they gradually disappear, too, as one goes from central (Fig. 6.2 ) to
peripheral (Fig. 6.3 ) collisions. ⁸ ⁸ 8 The curves shown in Figs. 6.2 ,
6.3 include the decay products from unstable hadronic resonances. We
have observed that for protons the flow angle fluctuation induced
difference at low @xmath between ( @xmath , @xmath ) on the one hand and
( @xmath , @xmath ) on the other hand doubles if only directly emitted
(“thermal”) particles are included in the analysis. Resonance decays
thus dilute the sensitivity of the proposed observables to flow angle
fluctuations by about 50%.

Event-by-event fluctuations of the magnitudes of @xmath are accessible
by comparing @xmath with @xmath . When plotting the ratios @xmath for
central ( @xmath centrality) collisions, where anisotropic flows are
caused exclusively by fluctuations, with negligible geometric bias from
a non-zero average deformation of the nuclear overlap region, we found
for both pions and protons a constant (i.e. @xmath -independent) value
of @xmath . This is expected [ 28 , 121 ] : If the flow angle @xmath is
randomly distributed relative to the reaction plane, the components of
@xmath along and perpendicular to the reaction plane are approximately
Gaussian distributed around zero, and the magnitude @xmath of the
complex flow coefficient is Bessel-Gaussian distributed with @xmath (see
Eqs. (4) and (5) in Ref. [ 28 ] ). A similar @xmath -independent ratio
is not observed at larger impact parameters: even for triangular flow,
which continues to be fluctuation-dominated also at non-zero impact
parameters, we observe deviations of the ratio @xmath from @xmath at
both low and high @xmath ; for elliptic flow these deviations are larger
and significant at all @xmath .

Interestingly, for central collisions we found approximately the same
constant value @xmath for the ratio @xmath (except near the @xmath
values where either the numerator or denominator passes through zero).
Looking at the definitions ( 6.2 ) and ( 6.2 ), this suggests an
approximate factorization of the @xmath -dependent flow angle
fluctuations (which enter through the factor @xmath that cancels between
numerator and denominator if it fluctuates independently) from the
fluctuations of the magnitude @xmath , as well as an approximate @xmath
-independence of the @xmath fluctuations.

To follow up on these observations and gain deeper insight into the
relative importance of flow angle fluctuations in different @xmath
ranges, let us look at Figs. 6.2 , 6.3 and note that the frequently
measured quantity @xmath behaves like the event-plane flow @xmath at low
@xmath and like the differential rms flow @xmath at intermediate @xmath
. This suggests that it is dominated by flow angle fluctuations at low
@xmath and by fluctuations of the magnitude of @xmath at higher @xmath .
In central collisions, the proton @xmath even turns negative at low
@xmath , whereas @xmath is by definition always positive. A related
observation is that the proton event-plane flow @xmath in Fig. 6.2
approximately agrees with @xmath at low @xmath (where flow angle
fluctuations seem to have strong effects) but with the mean flow @xmath
at higher @xmath (where flow angle fluctuation effects are weak). This
is reminiscent of the behavior of the @xmath -integrated event-plane
flow, which approaches the mean flow for good event-plane resolution.
Flow angle fluctuations appear to have similar effects on flow measures
as a decrease in flow angle resolution. The difference is that the
former is a physical effect due to initial-state fluctuations, whereas
the latter is a finite-sampling statistical effect in the final state
and affected by detector performance.

To make these qualitative observations quantitative, we plot in the
upper two panels of Fig. 6.4 the ratios @xmath and @xmath as functions
of @xmath , for both pions and protons. (We focus here on the results
from Fig. 6.2 for central collisions, where all anisotropic flows are
fluctuation-dominated.) In each case the numerator is sensitive to the
flow angle fluctuations while the denominator is not. However, numerator
and denominator are also differently affected by fluctuations in the
magnitudes of @xmath . Both ratios are seen to behave very similarly,
staying close to 1 at intermediate @xmath but dropping steeply at low
@xmath and more moderately at high @xmath . The steep drop at low
transverse momenta sets in at @xmath GeV for protons, but at much
smaller @xmath GeV for pions. We do not have a full understanding of
this mass dependence, beyond the qualitative observation that the
minimum of the variance of the flow angle fluctuations shown in Figs.
6.1 b,c is shifted to higher @xmath for protons compared to pions, and
that quite generally strong radial flow shifts all flow anisotropies to
higher @xmath values for heavier particles.

The lower two panels of Fig. 6.4 demonstrate that the behavior of the
ratios shown in the two upper panels is strongly dominated by flow angle
fluctuations. The dashed lines in Figs. 6.4 c,d show the flow angle
fluctuations @xmath in isolation. Their @xmath dependence alone is
almost sufficient to completely explain the shape of the curves in
panels (a) and (b). The solid lines in Figs. 6.4 c,d show that at
intermediate @xmath fluctuations in the magnitudes of the @xmath
-dependent flow @xmath and the @xmath -integrated @xmath tend to be
correlated with each other ( @xmath ) while they appear to fluctuate
more independently at low and high @xmath . At high @xmath this
decorrelation contributes to the suppression of the ratios shown in
panels (a,b). At low @xmath , the decorrelation of the @xmath -dependent
flow magnitude fluctuations @xmath from the @xmath -integrated flow
@xmath does not become effective until after the ratios have already
been suppressed by flow angle fluctuations, and its effect is therefore
subdominant.

In summary, we see for central collisions that at low @xmath the
differences between @xmath and @xmath , as well as between @xmath and
@xmath , are dominated by flow angle fluctuations, whereas at high
@xmath fluctuations of both the flow angles and flow magnitudes must be
considered to explain their differences. At intermediate @xmath , flow
angle fluctuations appear to be unimportant, @xmath fluctuates in sync
with the @xmath -integrated @xmath , and the differences between @xmath
and @xmath , as well as between @xmath and @xmath , vanish.

Figure 6.5 shows the same ratios as Fig. 6.4 for peripheral Pb+Pb
collisions, again using MC-Glauber initial conditions with @xmath . ⁹ ⁹
9 The main difference with results from MC-KLN initial conditions with
@xmath (not shown) is that the latter exhibit stronger suppression
effects from the flow fluctuation factor @xmath in the high- @xmath
region @xmath GeV (see also Fig. 6.3 ). Compared to central collisions
(shown in Fig. 6.4 ), the flow angle fluctuation effects at low @xmath
are much weaker and appear to be shifted to lower transverse momenta,
for both pions and protons. At high @xmath GeV, Figs. 6.5 c,d show that
effects from fluctuations of the flow angles (dashed lines) dominate
over those from fluctuations of the flow magnitudes (solid lines).

Finally, in Figure 6.6 we explore (for near-central collisions) how the
flow angle fluctuation effects, which push the event-plane flow @xmath
at low- @xmath below the value of the average flow @xmath , evolve as
the harmonic order @xmath increases. (For @xmath , we do not show
results below @xmath GeV for technical reasons explained in footnote 2.)
For pions, flow angle fluctuations are invisible in the shown @xmath
region for all flow harmonics; for protons, they are clearly visible for
all harmonic flows. The relative magnitude of their effect on the
difference @xmath at any fixed @xmath decreases as @xmath increases, but
the difference remains nonzero over a larger @xmath range for the higher
harmonics.

### 6.4 Non-factorization of flow-induced two-particle correlations

The breaking of factorization of flow-induced two-particle correlations
by flow fluctuations was first emphasized by Gardim et al. [ 111 ] .
Their study was based on simulations using ideal fluid dynamics, which
are here repeated with viscous fluid dynamics. A comparison of Figs. 6.7
, 6.8 below with the plots shown in Ref. [ 111 ] shows that viscous
effects reduce the amount by which event-by-event fluctuations break
factorization. We here explore the relative role played in this context
by fluctuations in the magnitudes and angles of the flows.

To this end we define the following two ratios, both symmetric in @xmath
and @xmath :

  -- -------- -- -------- -- --------
     @xmath      @xmath      
     @xmath      @xmath      (6.14)
     @xmath      @xmath      
  -- -------- -- -------- -- --------

  -- -- -- --------
           (6.15)
  -- -- -- --------

The ratio @xmath , first introduced and studied with ideal fluid
dynamics in [ 111 ] , is sensitive to fluctuations of both the
magnitudes @xmath and angles @xmath of the complex anisotropic flow
coefficients @xmath defined in Eq. ( 6.2 ). The second ratio @xmath , on
the other hand, differs from unity only on account of flow angle
fluctuations. By comparing the two ratios with each other and with
experimental data, we can isolate the role played by flow angle
fluctuations in the breaking of factorization of the event-averaged
two-particle cross section. In the absence of non-flow correlations both
ratios are always @xmath .

Figures 6.7 show these ratios for all charged hadrons as functions of
@xmath for fixed ranges of @xmath , indicated by different colors. ¹⁰ ¹⁰
10 The @xmath ranges are adjusted to the experimental data, and the
ratios were computed by first averaging the numerator and denominator
over the given @xmath range. Figs. 6.7 a,b,c focus on central, Figs. 6.7
d,e,f on peripheral collisions; in both cases, we used MC-Glauber
initial conditions and evolved them with VISH2+1 using @xmath for the
specific shear viscosity. In central collisions, the hydrodynamic
simulations appear to overpredict the factorization breaking effects,
while in peripheral collisions, theory and data agree somewhat better.
More precise experimental data would be desirable. The comparison of
@xmath (dashed lines) with @xmath shows that a significant fraction (
@xmath or more) of the effects that cause the breaking of factorization
arises from flow angle fluctuations. This seems to hold at all the
transverse momenta shown in the figures. A comparison of the top and
bottom rows of panels in Fig. 6.7 shows that factorization-breaking
effects are stronger for harmonics that are fluctuation dominated ( i.e.
all harmonics in central collisions and the odd harmonics (especially
@xmath ) in peripheral collisions) and appear to weaken for @xmath and
@xmath in peripheral collisions, where both the magnitudes @xmath and
the flow angles @xmath are mostly controlled by collision geometry.

To explore the effects of shear viscosity of the expanding fluid on the
breaking of factorization, we show in Figure 6.8 the same data as in
Fig. 6.7 , but compared with hydrodynamic calculations that use MC-KLN
initial conditions evolved with @xmath (a 2.5 times larger viscosity
than used in Fig. 6.7 ). Obviously, the MC-KLN model produces a
different initial fluctuation spectrum than the MC-Glauber model, so not
all of the differences between Figs. 6.7 and 6.8 can be attributed to
the larger viscosity. However, in conjunction with the ideal fluid
results reported in [ 111 ] , the comparison of these two figures
strengthens the conclusion that increased shear viscosity tends to
weaken the fluctuation effects that cause the event-averaged
two-particle cross section to no longer factorize.

### 6.5 Chapter summary

All experimental precision measures of anisotropic flow in relativistic
heavy-ion collisions are based on observables that average over many
collision events. It has been known for a while that both the magnitudes
@xmath and flow angles @xmath of the complex anisotropic flow
coefficients @xmath fluctuate from event to event, but only very
recently it became clear that not only the @xmath , but also their
associated angles @xmath depend on @xmath , and that the difference
@xmath between the @xmath -dependent and @xmath -averaged flow angles
also fluctuates from event to event. Here in this chapter we pointed out
that these flow angle fluctuations leave measurable traces in
experimental observables from which the ensemble-averaged @xmath
-dependent anisotropic flows are extracted. We have introduced several
new flow measures and shown how their comparison with each other and
with flow measures that are already in wide use allows to separately
assess the importance of event-by-event fluctuations of the magnitudes
and angles of @xmath on experimentally determined flow coefficients.

Viscous hydrodynamic simulations show that flow angle fluctuations
affect the @xmath -dependent flow coefficients of heavy hadrons (such as
protons) more visibly than those of light hadrons (pions). In
near-central collisions, where anisotropic flow is dominated by initial
density fluctuations rather than overlap geometry, the effects from flow
angle fluctuations appear to be strongest for particles with transverse
momenta @xmath . A precise measurement and comparison of @xmath (Eq. (
6.2 )), @xmath (Eq. ( 6.2 )), @xmath (Eqs. ( 6.2 , 6.2 )), and @xmath
(Eq. ( 6.2 )) for identified pions, kaons and protons with transverse
momenta @xmath GeV should be performed to confirm the hydrodynamically
predicted effects from flow angle fluctuations. The theoretical
interpretation of these measurements requires a reanalysis of
finite-sampling statistical effects on the @xmath -dependent
differential flows, stemming from the finite multiplicity of particles
of interest in a single event, which we did not consider here. The
proposed comparison holds the promise of yielding valuable experimental
information to help constrain the distribution of initial density
fluctuations in relativistic heavy ion collisions and may prove crucial
for a precision determination of the QGP shear viscosity.

We also showed that flow angle fluctuations are responsible for more
than half of the hydrodynamically predicted factorization breaking
effects studied in Ref. [ 111 ] and in Sec. 6.4 above, and that these
effects are directly sensitive to the shear viscosity of the expanding
fluid, decreasing with increasing viscosity. By combining the study of
various types of differential anisotropic flow measures with an
investigation of the flow-induced breaking of the factorization of
two-particle observables into products of single-particle observables,
one can hope to independently constrain the fluid’s transport
coefficients and the initial-state fluctuation spectrum.

## Chapter 7 Sampling particles from the Cooper-Frye distribution

This chapter explains the methodology of sampling particles from the
emission function calculated with the Cooper-Frye formula along the
freeze-out surface. I will focus on the correctness and efficiency of
the sampling algorithm. The material product of this methodology are the
iS and the iSS code packages, which are among the core packages used in
our group’s simulations. For completeness, both the required
mathematical background and some technical details are given in this
chapter.

The main structure of the chapter is outlined as follows: Sec. 7.2
provides the background on random variables and general sampling
methodologies. The calculation of particle emission is briefly explained
in Sec. 7.3 . The actual sampling algorithm for particle momenta and
space-time position is given in Sec. 7.4 . Finally, a number of code
checks are performed in Sec. 7.5 .

The material is based on a document for the iSS code that is not yet
published. The code was developed together with Chun Shen, who shares
equal credit for this effort.

### 7.1 iS and iSS overview

The name “iS” stands for “iSpectra”; iS is a fast Cooper-Frye particle
momentum distribution calculator along the conversion surface. Its
output is a continuous function, evaluated at discrete momenta provided
by the users, for the invariant momentum distributions of the desired
hadron species. The code “iSS”, whose name stands for “iSpectraSampler”,
goes one step further by generating individual particles, using the
calculated particle momentum distributions as the relative emission
probability. iSS is an “event generator” which generates a complete
collision event of emitted hadrons, similar to the events created in the
experiment. Both codes are written keeping the following factors in
mind:

-    Readability and extendability. The most important goal is to create
    a cleanly written framework that calculates particle momentum
    distributions and performs sampling, whose components and output can
    be used easily for further physics analyses and tests of new
    physical ideas. To achieve this, the entire program is divided into
    modules according to their functionalities, the structures and the
    algorithms are documented with comments, and long but informative
    names are chosen for variables and function names.

-    Efficiency. Both the iS and iSS codes are written aiming for
    intensive hybrid event-by-event calculations where every CPU cycle
    counts. To achieve the necessary degree of efficiency, much effort
    is put into optimizing the algorithms at different levels of the
    calculations.

-    Easy maintainability and re-usability. The framework is divided
    into different carefully chosen functionality modules, for better
    interoperability and to maximize re-usability.

Although the codes are meant to serve as a basic framework upon which
additional physical analysis modules can be added, some such modules are
already implemented, as will be explained in the following sections. An
external parameter file and several tables allow for easy adjustment of
parameters and choosing among multiple choices between available
modules.

### 7.2 Random variables and sampling methods

#### 7.2.1 Random variable, PDF and CDF

A random variable @xmath is a variable that takes different values
within a set @xmath every time it is sampled. If @xmath is a discrete
set, then @xmath can be characterized by specifying the probability of
it taking each element in @xmath ; if @xmath is a continuous set, then
@xmath is characterized by the probability density function (PDF) @xmath
, defined such that @xmath gives the probability of finding @xmath in a
subset @xmath .

The cumulative distribution function (CDF) @xmath is formally defined to
be the function that gives the probability of finding @xmath :

  -- -------- -- -------
     @xmath      (7.1)
  -- -------- -- -------

The “inverse CDF” is the inverse function of @xmath .

By definition @xmath is non-negative and it integrates to 1, and @xmath
is non-decreasing and @xmath , @xmath .

In this chapter, a variable is “ @xmath -dimensional” if the dimension
of the set @xmath is @xmath . The PDF and CDF definitions are also valid
for multi-dimensional random variables, where for the CDF the notion of
@xmath should be understood component by component.

#### 7.2.2 Sampling 1d random variables according to the inverse CDF

The inverse CDF sampling is best understood if we partition the range
@xmath of the CDF into @xmath intervals with equal lengths @xmath , and
their pre-image in the domain space are denoted as @xmath . By
definition of CDF, the probability that @xmath lies inside any @xmath is
the same, therefore the probability for @xmath to lie in any @xmath is
the same. By throwing another uniform random variable in @xmath , we can
sample an index @xmath which means that the sampled @xmath . By taking
@xmath , we see that the process becomes: uniformly randomly sample a
value @xmath , then find the sampled @xmath .

If the range of the CDF is not properly normalized to @xmath but @xmath
, the process given above can be extended trivially by shift and
scaling.

#### 7.2.3 Direct sampling of 1d random variables according to their
PDF: special accept-reject method

Another intuitive sampling method is to sample the PDF directly using
the accept-reject method. It starts with sampling a uniformly randomly
chosen value for @xmath , then use another continuous random variable
@xmath uniform in @xmath to decide whether to accept or reject the
sample: if the sampled trial variable value is greater than @xmath ,
then it is rejected; otherwise it is accepted. By doing so it is ensured
that the probability of keeping @xmath is proportional to @xmath , which
is what we want.

One immediate improvement on efficiency is that instead of comparing the
test variable to @xmath , we can also compare it to @xmath , where
@xmath is a factor such that @xmath for all @xmath . A larger @xmath
values means less rejections and larger efficiency, and max efficiency
is attained by choosing @xmath .

Like the inverse CDF method, if the PDF is not properly normalized, a
scaling in @xmath can be used to generalize the method.

Note that the accept-reject method for sampling PDF does not require the
PDF to be properly normalized; the only requirement is that the
probabilities used to make accept-reject selections need to be less than
@xmath .

#### 7.2.4 Sampling a 1d random variable using envelope distribution:
general accept-reject method

A random variable @xmath on @xmath with PDF @xmath is considered as an
envelope distribution to @xmath on @xmath with PDF @xmath if there
exists a constant @xmath so that @xmath , @xmath .

Given an envelope distribution, the random variable @xmath can be
sampled using a two-step accept-reject procedure. First samples of
@xmath according to the distribution @xmath are generated using any
appropriate method, then each sample @xmath is accepted as a sample of
@xmath with probability @xmath .

The reasoning is simply based on a re-expressing the probability of an
event as a product of the probability of another event and a conditional
probability: the probability of generating a sample @xmath is @xmath ,
and it equals the probability @xmath of first generating a sample @xmath
, multiplied by the conditional probability of getting @xmath knowing
@xmath , which is proportional to @xmath .

As a special case, when @xmath is a constant, sampling using the
envelope distribution reduces to the case of direct sampling of PDF
method explained in Sec. 7.2.3 .

The efficiency of using an envelope distribution function is analyzed in
Sec. 7.2.6 .

Often an envelope distribution can be constructed as a sum of step
functions, see Sec. 7.2.7 .

#### 7.2.5 Sampling 1d random variable using grouping

Another less intuitive method to sample @xmath is to use the grouping
method, which divides @xmath into subsets and first samples the subset
the next sample belongs to, then samples the element inside that subset.
To illustrate it consider a simple case where @xmath and @xmath .

The way to divide @xmath into groups is not unique but suppose @xmath is
divided into @xmath and @xmath , then the probability of finding @xmath
is @xmath and the probability of finding @xmath is @xmath .

The subsets are first sampled, which is equivalent to sampling a random
variable @xmath on the indices @xmath of the subsets with PDF @xmath .
Next elements in that subset @xmath are sampled according to the PDF
@xmath but restricted to @xmath .

In general, after @xmath is partitioned into subsets @xmath , @xmath ,
first subsets are sampled using the induced “group-index-variable”
@xmath on @xmath with PDF @xmath , then @xmath is sampled with
restricted PDF @xmath .

The grouping method can be used recursively, where @xmath are
sub-divided into finer sub-subsets.

How @xmath should be divided into subsets, and the efficiency of the
grouping method are explained in Sec. 7.2.6 .

#### 7.2.6 Sampling efficiency

So far, four different sampling methods have been explained:

1.  sampling using inverse CDF (Sec. 7.2.2 )

2.  direct PDF sampling (Sec. 7.2.3 )

3.  sampling with envelope distribution (Sec. 7.2.4 )

4.  sampling using grouping (Sec. 7.2.5 )

Among them, (1) and (2) are direct sampling methods while (3) and (4)
are composite in the sense that they involve sampling a second random
variable, using any desired direct sampling method. For sampling with
the envelope distribution method, the way to sample the envelope
distribution is unspecified; and for sampling using grouping, both the
way to sample groups and the way to sample elements within the group are
left open.

In this section, sampling efficiencies are compared for methods (1)-(4).
Here the “sampling efficiencies” are measured in terms of the number of
times @xmath is evaluated, while all other calculations are assumed to
pose no costs. I will abbreviate the number of evaluations of @xmath in
this section as “NOE”. When sampling a large number of samples, another
good measurement of efficiency is the average number of times @xmath
needs to be calculated in order to produce @xmath accepted sample, which
is the reciprocal of the “acceptance rate”.

In almost all cases, @xmath either has an analytic expression or can be
pre-tabulated. In cases where @xmath can be calculated analytically or
pre-tabulated, the time spent on evaluating @xmath is similar to that of
evaluating @xmath . In such cases, the sampling using inverse CDF method
is no doubt the most efficient one, since each time @xmath is evaluated,
it gives one sample with @xmath acceptance. However in many cases,
@xmath cannot be evaluated analytically or pre-tabulated ¹ ¹ 1 One
common situation is that @xmath depends on another continuous parameter
and @xmath cannot be calculated analytically, thus there are “infinitely
many” @xmath that need to be tabulated. , and its calculation requires
numerical integration of @xmath . It is for these cases that the
efficiencies of different methods matters, and the following comparisons
are all done under such conditions.

For demonstration, it is assumed that @xmath is discrete and @xmath is a
finite set with @xmath elements. Assume also that a total number of
@xmath samples is desired. In such a case, the evaluation of @xmath for
all @xmath requires NOE= @xmath .

Two extreme scenarios for @xmath are used: scenario (A) assumes that the
distribution function @xmath is uniform, and scenario (B) assumes that
the distribution function @xmath is delta-like: @xmath is @xmath for one
special element and @xmath for others. A realistic distribution can have
any shape in between, and the sampling efficiency of it is expected to
also lie in between; a flat distribution should have sampling efficiency
close to scenario (A) and a sharply peaked one close to scenario (B).

It can be shown that if an event has success probability @xmath , then
the average waiting time to have one success in a series of repeated
independent sampling is @xmath , that is, on average the @xmath -th
sample is the successful one.

Starting with scenario (A) where @xmath is uniform, the direct PDF
sampling method (2) in the most optimized case where @xmath (see Sec.
7.2.3 ) gives full acceptance: every sample of @xmath is accepted
because the acceptance probability is @xmath . Thus NOE is @xmath . The
sampling using inverse CDF method (1), as explained requires @xmath NOE
for constructing the inverse CDF plus @xmath NOE for actual sampling.

For scenario (B) where @xmath is delta-like, the direct PDF sampling
method (2) in the most optimized case requires @xmath NOE since each
sampling has @xmath success rate and there are @xmath desired samples.
The inverse CDF sampling method (1) requires again @xmath NOE for
constructing @xmath plus @xmath NOE for sampling.

The results are summarized in the Table 7.1 .

Therefore for a flat distribution, the direct PDF sampling method, when
used properly, is most efficient. For distributions with peaks, the
sampling using inverse CDF method is always superior than the direct PDF
sampling method, except for @xmath . In general for @xmath or @xmath ,
using inverse CDF sampling will practically almost always grant
excellent efficiency; only for @xmath should the direct PDF sampling
method be considered.

From our study of scenario (B), it is also clear that an accurate
estimate for the maximum of the PDF is crucial for the direct PDF
sampling method: an overestimate of the maximum by a factor of @xmath
would decrease the success rate by a factor of @xmath , thus doubling
the sampling time.

As a quick summary, for steep distributions, the sampling using the
inverse CDF method is superior; for flat and general unknown
distributions, to generate a small number of samples the direct PDF
sampling is faster, while for large samples, the inverse CDF method is
faster.

The envelope distribution function method (3) first samples according to
the envelope distribution, after which it uses the accept-reject method
to sample the target distribution (see Sec. 7.2.4 ). The envelope
distribution can be sampled by direct PDF sampling or inverse CDF
sampling. However if direct PDF sampling is used for the envelope
function, then sampling with the envelope distribution method has no
advantage over the direct PDF sampling method since the over-all
probability for accepting a sample is still the same as the one in the
direct PDF sampling. The sampling using the envelope method is only more
efficient when the envelope variable can be sampled using the inverse
CDF method and its CDF can be calculated analytically or pre-tabulated,
or at least its evaluation takes a much shorter time.

To see why using envelopes improves the sampling efficiency, consider a
simple example of scenario (B): @xmath and @xmath has PDF @xmath ,
@xmath , which is delta-like. The direct PDF sampling method requires on
average @xmath NOE to acquire one sample. Consider using an envelope
distribution @xmath with PDF @xmath , @xmath . By assumption, the
sampling of @xmath takes only @xmath NOE, which yields either @xmath or
@xmath . Next consider using an accept-reject method to sample @xmath ,
but since the possible sample spaces has been restricted to @xmath from
sampling @xmath , the sampling of @xmath using direct PDF sampling has a
much larger acceptance rate, and the average NOE is @xmath . The
over-all required NOE is @xmath , which is smaller than the @xmath NOE
required for direct PDF sampling.

The efficiency of sampling using the envelope method depends on the
choice of envelope, and the closer the shape of @xmath is to @xmath ,
the better the efficiency. In the extreme case that the envelope becomes
the original distribution, @xmath , the acceptance becomes @xmath and
the sampling using the envelope method degenerates into the sampling
using the inverse CDF method (assuming the envelope distribution is
sampled using the inverse CDF method for efficiency).

As a summary, the efficiency of sampling using the envelope method is
greater than the direct PDF sampling method but worse than the inverse
CDF sampling method. Only when the inverse CDF cannot be analytically
calculated or pre-tabulated and when the NOE spent on constructing CDF
is significant should the sampling using envelopes method be tried. The
envelope distribution should be so constructed that: (1) its shape
resembles the desired distribution ² ² 2 However there is one
constraint: the envelope function, when multiplied by a constant, should
be larger than the original distribution (see 7.2.4 ). ; (2) its CDF can
be pre-tabulated, calculated analytically, or computed numerically with
only low expenses.

Often an envelope function can be numerically constructed as a sum of
step functions, which only requires evaluation of @xmath at a few
locations, and a general-purpose example is given in Sec. 7.2.7 .

The last method to analyze is the grouping method (4). Similar to the
envelope method, its efficiency varies depending on the choice of
groups.

As a case study, consider the set @xmath is divided into @xmath subsets:
@xmath , @xmath , and each subset is further divided into @xmath
sub-subsets: @xmath , @xmath , and so on. Assume “on the ground level”
each sub-sub-…-subset contains only one element, that is, @xmath is an
element. Assume that at each level, the probability of @xmath being in
any subset is the same: @xmath . Assume for now that we use direct PDF
sampling method to sample groups at each level. The efficiency for
sampling using such a grouping is discussed in the next paragraph.

For sampling subsets at the 1st level @xmath , the success rate is
@xmath and the average NOE is @xmath . Next, knowing which 1st level
subset @xmath belongs to, to sample the 2nd level subsets @xmath
requires similarly an average NOE @xmath . Continuing this it is clear
that the total NOE required in the end is @xmath . Note that @xmath ’s
are subject to the constraint @xmath , and recall that the direct PDF
sampling method requires @xmath NOE, it is then clear that the sampling
using grouping method is much more efficient.

The best efficiency is attained when @xmath . In such extreme case, the
required NOE is naively estimated as @xmath but in fact it is just
@xmath . The reason is that @xmath is different from @xmath because even
a “failure” in the sampling can also tell us which subset @xmath belongs
to. For example, if @xmath does not belong to @xmath , then it must
belong to @xmath . Therefore both “success” and “failure” can be used to
inquire the subset @xmath belongs to and in the end only @xmath NOE is
required ³ ³ 3 Again, all other calculations, like throwing @xmath
random dices, are considered to have negligible time here. .

In fact, when all @xmath ’s are @xmath , the grouping sampling method
becomes the sampling using inverse CDF method. To see this, first notice
that we can re-order the set @xmath so that @xmath and @xmath ⁴ ⁴ 4 The
boundary of the interval is irrelevant only for continuous distribution
but it is ignored here for simplicity. etc., which in general gives
that:

  -- -------- -- -------
     @xmath      (7.2)
  -- -------- -- -------

Then it is clear that what the grouping method does is just a binary
search for the element @xmath with a given sequence @xmath . The
sequence becomes a real number in @xmath when its elements are
interpreted as digits in binary code: @xmath . In this way, the grouping
method can be interpreted as: For a uniformly chosen real number @xmath
, first express it in binary code, then perform a binary search for the
element @xmath that, according to eq. ( 7.2 ), satisfies @xmath .
Therefore the grouping sampling method is just a particular numerical
implementation (binary search for inverse) of the inverse CDF sampling
method.

We just showed that in this extreme case, the grouping sampling and
inverse CDF methods become identical. For general cases, such as when
not all @xmath ’s are @xmath ’s, or when some of the subsets are sampled
using inverse CDF methods, its efficiency lies between the direct PDF
sampling and the inverse CDF methods.

In reality, the grouping method is much slower than the sampling using
inverse CDF method, since even choosing a random number between @xmath
and @xmath for @xmath number of times costs a significant number of
computations. The grouping method, like the sampling with envelope
method, should only be tried when the inverse CDF cannot be analytically
calculated or pre-tabulated. If part of the sampling (usually the
sampling of the groups) uses the inverse CDF sampling method, then for a
grouping scheme, the larger the portion of the entire sampling that is
done with the inverse CDF sampling method, the more efficient the whole
process becomes.

As a summary, the efficiency of the grouping sampling method is greater
than that of the direct PDF sampling but less than that of the inverse
CDF sampling. It should only be attempted if the inverse CDF cannot be
calculated analytically or pre-tabulated, or is too time consuming when
constructed numerically. When each level of the group is sampled using
the direct PDF sampling method, it achieves better efficiency when more
levels of groups are used, with fewer groups at each level. Also, the
more groups are sampled using the inverse CDF method, the more efficient
the method.

#### 7.2.7 Automatically generated stair function envelope distribution

Many distributions are piecewise monotonic. Within one such monotonic
interval @xmath , an envelope function can be constructed using constant
function. Taking the proportional constant to be @xmath (see Sec. 7.2.4
), due to the monotonicity of the PDF, the envelope can be taken as a
constant function @xmath where @xmath . Given that the real axis is
divided into a series of intervals on each of which @xmath is monotonic,
a stair-shaped envelope function can be generated by using a constant
envelop function on each monotonicity interval. Such a construction only
involves evaluating @xmath at those boundary points that separate the
intervals, and the inverse CDF of a stair function is easily calculable;
therefore when the number of monotonicity intervals is small,
constructing and sampling with such a stair-shaped envelope function
becomes efficient.

For example, assuming one wants to sample a Gaussian distribution @xmath
on @xmath . When using direct PDF sampling, @xmath of the time the
suggested sample for @xmath lies in the interval @xmath and its
acceptance is below @xmath and it almost always is rejected. However, if
a two-stair envelope distribution @xmath with PDF @xmath , @xmath , and
@xmath , @xmath is used, then the acceptance rate for @xmath will be
greatly increased because sampling @xmath will only give a tiny chance
to suggest a sample above @xmath for @xmath . Furthermore, the
construction of such an envelope including its inverse CDF only requires
@xmath evaluations of @xmath , at @xmath and @xmath , which is
negligible compared to the NOE required in direct PDF sampling, which is
typically @xmath (inverse acceptance rate).

In many cases, the encountered distribution function has a unique
maximum located at its “mode” and is monotonically decreasing on both
sides when @xmath goes away from the mode. Examples include the Gaussian
distribution function, the negative binomial distribution (NBD), etc.
For such distribution functions, one way to automatically generate an
envelope is to start with its mode, and construct stair-shaped envelope
functions whose width for each step is, for example, its standard
deviation, and which reaches out for a few (e.g. 6) multiples of it.
Such a stair-shaped envelope function can be constructed without knowing
the analytic form of the inverse CDF and it only requires @xmath more
NOE. For rapidly dropping distributions like Gaussian and NBD, the time
constructing the envelope is negligible compared to the time spent on
typical direct PDF sampling.

For piecewise monotonic distributions like the Gaussian and NBD
distributions, if they do not have parameter dependence , their inverse
CDF should be calculated in advance and pre-tabulated to provide maximum
sampling efficiency; if however they do have parameter dependence and
tabulating their inverse CDF becomes impossible, the automatically
constructed stair-shaped envelope can greatly boost performance compared
to a brute-force PDF sampling method.

#### 7.2.8 Generating genuine continuous samples

For a variable @xmath defined on a continuous set @xmath , it can be
sampled using the direct PDF sampling method (or other composite
methods) with continuum PDF, which will produce samples that can take
any continuous values in @xmath ⁵ ⁵ 5 This is also true if the inverse
CDF has an analytical expression, which is really rare. . However, when
@xmath is sampled using numerically constructed discretized inverse CDF,
the generated samples are on a discrete set upon which the CDF is
constructed. When the discrete set is dense, such a discrete sampling
hardly even causes any practical problems. If the discrete set is not so
dense and genuine continuous samples are preferred, the continuum can
still be approximately restored by an additional random process: instead
of using a sample in the discrete set, a “perturbed” sample, which is
generated by assigning an additional random shift whose range is half of
the discretization spacing, can be used. Such a process generates
samples close to the continuous sampling method while maintaining the
efficiency of the inverse CDF sampling method (the cost for the
additional random process is by our assumption ignored, see Sec. 7.2.6
).

#### 7.2.9 Sampling multi-dimensional random variables

For multi-dimensional random variables, the direct PDF sampling method,
the sampling with envelope method, and the grouping sampling method can
be generalized naturally, but it is not clear how the most efficient
sampling method, the inverse CDF sampling method, can be generalized to
multi-dimensional case for a continuous set @xmath , while the solution
for a discrete @xmath is simple. Since almost all functions are
discretized on a computer, in this section the discussion is restricted
to the case that @xmath is discrete.

When @xmath is discrete ⁶ ⁶ 6 The following argument actually only
requires a weaker assumption that @xmath can be made into an ordered
set. , it can be “flattened” into a 1d set by re-ordering its elements
in any desired fashion. In this way, any random variable @xmath on
@xmath can be treated as a 1d random variable and all the sampling
methods and their sampling efficiency analyses explained in previous
sections directly apply.

For example, supposing @xmath resembles the @xmath area, after
discretizing along both directions the interval @xmath into @xmath
points, @xmath , @xmath . The set @xmath can be ordered, for example,
as:

  -- -------- -- -------
     @xmath      (7.3)
  -- -------- -- -------

Therefore any “2d” random variable @xmath on @xmath can be treated as a
1d random variable on the re-ordered set. Higher-dimensional random
variables can be “flattened” similarly.

As explained (Sec. 7.2.6 ), sampling using the inverse CDF is the most
efficient method: if it can be calculated analytically or pre-tabulated,
or if a large number of samples are desired so the time spent on
constructing the CDF numerically is negligible, the sampling using
inverse CDF method should always be used. Otherwise the sampling with
envelope or grouping methods should be tried before using the direct PDF
sampling method.

When applying the grouping sampling method to multi-dimensional
variables, it is very natural to divide @xmath into subsets according to
one of the dimensions. For example, the set @xmath in eq. ( 7.3 ) can be
divided into @xmath groups @xmath . By doing so, the sampling using
grouping method essentially samples @xmath by first sampling @xmath then
sampling @xmath .

For multi-dimensional random variables, there is yet one more practical
constraint: even if the CDF of the variable can be pre-calculated, it
cannot be tabulated due to memory shortage. In such situations, one
practical solution is to tabulate not the CDF for the multi-dimensional
variable, but only for the CDF corresponding to higher level groups
created in grouping sampling. To give an example, suppose that
discretization into @xmath points is too much so that the inverse CDF
cannot be stored for @xmath on @xmath given in eq. ( 7.3 ), but the
inverse CDF for the variable @xmath , which controls the sampling of
groups @xmath , requires only @xmath points as it has a much larger
chance to fit into the memory.

#### 7.2.10 Possible issues

##### Deviations due to limited number of samples

It may seem trivial that, with a limited number of samples, the
statistically calculated quantities, for example the mean and standard
deviation, have statistical errors compared to the exact values
calculated from the distribution function. However during numerical
calculations, some situations may still seem to give “counterintuitive”
results when using exotic distributions. For example, if @xmath takes
the value @xmath with @xmath probability and @xmath with @xmath
probability, it has mean @xmath but in almost any actual sampling
process the mean is @xmath . Therefore when sampling distributions with
fast-dropping tails, the mean calculated from samples is expect to
deviate from the exact value, especially for a highly skewed
distribution.

##### Discretizing continuous distribution

When continuous distributions are discretized, the weight on each
discretized point should be the integral of the PDF (probability) around
that point, instead of being the PDF itself (probability density) at
that point. The difference only matters if the discretization is on an
irregular lattice. For example, assuming that @xmath is uniform on
@xmath and one discretization is @xmath represented by the middle point
@xmath and @xmath represented by middle point @xmath , then the weight
at @xmath should be twice that at @xmath , in order to generate twice as
many samples in @xmath than @xmath . This requires using the integral of
the PDF on @xmath and @xmath as the sampling weights instead of the
values of the PDF at @xmath and @xmath as the sampling weights; that is,
weights are probabilities, not probability densities.

One scenario in which irregular lattice discretization is encountered is
during the sampling on a lattice specified by Gauss quadrature. Assuming
that a function is numerically costly to evaluate but it needs to be
integrated, then the numerical integral can be efficiently calculated by
summing its values on only a few lattice points given by Gauss
quadrature, during which process values of it on the lattice are stored
to avoid repeated calculation. If such a discretization of this function
is used later as a PDF in sampling, then samples should be generated not
using the values of the function on the lattice, but using the integral
of it on each interval, that is, with the specified Gauss weight.

### 7.3 Sampling the particle momentum distribution

The iS and iSS codes read the hydrodynamic variables along the
conversion surface generated from the hydrodynamical simulation, then
calculate the particle momentum distribution and observables, and also
sample particles accordingly. Both codes calculate the particle momentum
distribution using the Cooper-Frye formula for the emission function
which, together with the details for calculating related observables and
sampling, will be explained in this section.

#### 7.3.1 Emission function and Cooper-Frye formula

The emission function is the function that determines how many particles
on average will be emitted from a given location on the conversion
surface. In general, it depends on @xmath , where the @xmath -vector
@xmath has the following components: the proper time @xmath ; the
transverse location @xmath , and the space-time rapidity @xmath on the
conversion surface. The @xmath -vector @xmath has the following
components: the energy @xmath , the transverse momentum @xmath , and the
particle rapidity @xmath . All the out-going particles are on-shell, and
the 4-vector @xmath is constrained to be on the conversion surface, so
there are only @xmath free components on which the emission function
depends.

The conversion surface is conventionally denoted as @xmath ; it is a
3-dimensional set of points in 4-dimensional space-time. An
infinitesimal piece of @xmath is characterized by its surface normal
vector @xmath , whose norm gives the 3-volume of the element and whose
components give the direction.

The Cooper-Frye formula has been widely used to calculate the emission
function, whose differential form is the following:

  -- -------- -- -------
     @xmath      (7.4)
  -- -------- -- -------

Here @xmath is the spin degeneracy, and @xmath is the distribution
function taking the form ⁷ ⁷ 7 All through this chapter, the upper sign
is for fermions and lower sign is for bosons. :

  -- -------- -- -------
     @xmath      (7.5)
  -- -------- -- -------

@xmath , @xmath , @xmath , @xmath are the energy density, pressure,
temperature, and 4-flow velocity of the fluid, respectively; and @xmath
is the chemical potential of the particle; the function @xmath is a
function of only @xmath ; @xmath corresponds to the quadratic ansatz for
@xmath .

In numerical calculations, it is natural to separate the rapidity
dependence from the surface element as @xmath . It is also convenient to
use the variable @xmath and to write @xmath . With these choices, the
emission function eq. ( 7.4 ) assumes the form:

  -- -- -- -------
           (7.6)
  -- -- -- -------

The emission function drops rapidly with increasing @xmath or @xmath ,
and this influences the sampling efficiency, as will be explained in
Sec. 7.4 .

The differential emission function given by eq. ( 7.4 ) is not
necessarily always positive since, depending on the shape of @xmath ,
@xmath can become negative; for sampling with probability methods, we
need, however, a positive-definite emission probability density. This
issue and its possible treatments are discussed in Sec. 7.4 .

#### 7.3.2 Spectra and flow calculations

The azimuthally averaged spectrum is calculated from @xmath as:

  -- -------- -- -------
     @xmath      (7.7)
  -- -------- -- -------

and the differential anisotropic flows @xmath ’s are the Fourier
coefficients with respect to the azimuthal angles of the same
distribution:

  -- -------- -- -------
     @xmath      (7.8)
  -- -------- -- -------

Therefore the calculation of spectra and flow requires the calculation
of the distribution function @xmath , which from eq. ( 7.6 ) is
calculated as:

  -- -------- -- -------
     @xmath      (7.9)
  -- -------- -- -------

In numerical calculations, the distribution function @xmath is
represented as an array whose elements are calculated using eq. ( 7.9 ).
Usually to achieve high numerical accuracy, a dense conversion surface
discretization is used, and the numerical integrals in eq. ( 7.9 ),
which involve numerical sums over the conversion surface, become very
time consuming. Therefore a quadrature rule that requires a smaller size
of the @xmath array while not losing accuracy means less computing time
for the integral over the conversion surface, resulting in better
efficiency. Gauss quadrature, which requires much fewer sampling points,
is thus used in our codes for both the @xmath and @xmath integrals.

The integral over @xmath in eq. ( 7.9 ) can also be done using Gauss
quadrature. Under the boost-invariant assumption, the integrand is an
even function and the integral further reduces to one along only the
positive axis, which is then calculated using @xmath -point Gauss
quadrature.

Further optimization for performing the numerical integral in eq. ( 7.9
) involves adjusting the order of the integration loops, using local
variables, pre-tabulating mathematical functions, etc. The resulting
code iS is @xmath times faster compared to the previous used AZSpectra
code.

Note that a negative differential emission function is physically
allowed in all the integrals in this section, since only @xmath (i.e. a
momentum distribution integrated over @xmath ) and @xmath (i.e. the
spatial emission density integrated over momentum) have to be always
positive definite. However, for the sampling algorithms to work, the
integrands need to be replaced by their absolute values, or the negative
contributions must be set to zero by hand. While this gives physically
incorrect results, it is necessary for testing the sampling algorithm.
This is explained in detail in Sec. 7.4 .

### 7.4 Sampling the emission function

We now discuss how to sample the emission function to generate particles
whose space-time and momentum distributions are given by the
differential emission function ( 7.6 ). Required information are the
total emitted number of particles, their species, the emission location,
and their momenta. The space-time information of the particles comes
from the location on the conversion surface from where they are emitted.
Their momentum information is sampled according to the Cooper-Frye
formula (eq. ( 7.4 ) or ( 7.6 )).

In numerical calculations, the conversion surface is generated from the
hydrodynamic calculation, and it is discretized into surface cells which
are labeled by the “conversion surface cell index”, denoted as @xmath in
this section. The space-time information @xmath , @xmath , and @xmath on
the conversion surface cell are functions of this index. For a
boost-invariant system, the conversion surface is invariant under a
shift in the @xmath direction, and only the conversion surface at @xmath
is stored. Therefore only the @xmath and @xmath information are
expressed through the conversion surface cell index, and the @xmath
dependence is kept explicitly. Finally, for boost-invariant systems, the
@xmath -integrated emission function does not depend on particle
rapidity; the rapidity @xmath dependence of the particle can always be
sampled additionally in the end with a uniform distribution in a given
rapidity range, which together with @xmath determines @xmath . Therefore
the quantities that the differential emission function @xmath depends on
in practice are: the conversion surface cell index @xmath , the
transverse momentum @xmath and @xmath , and the relative space-time
rapidity @xmath .

Of these @xmath quantities, the conversion surface cell index is already
discretized, and the other @xmath can be discretized or kept as
continuous quantities. As explained in Sec. 7.2.8 , continuous samples
can still be approximately generated from samples on a discrete set, so
the issue of whether the generated samples assume continuous or discrete
values will be largely ignored in this section, and the main focus is on
the efficiency of the sampling algorithm.

#### 7.4.1 The purely numerical approach

The straightforward approach is where all required quantities are
calculated numerically.

For a given species of particle, the average total yield @xmath is
calculated by numerically integrating the differential emission function
over all its dependences. By storing all the partial sums encountered in
the numerical integration, the inverse CDF for the flattened variable
(see Sec. 7.2.9 ) can be generated at the same time. As explained in
Sec. 7.2.6 , the inverse CDF sampling method is the most efficient one,
and it should be used for the generation of particle samples.

However in practice, the inverse CDF for the full differential @xmath
can hardly fit into the current generation computer memory. In a typical
calculation, we have @xmath surface cells, @xmath discretized @xmath
points, and @xmath discretized @xmath points, giving an array of size
@xmath MB, already dangerously large. In the most central Pb-Pb
collisions at @xmath AGeV energy at the LHC, the number of conversion
surface cells exceeds @xmath , and the memory demand reaches @xmath GB.
My personal design philosophy is to leave flexible memory spaces to
allow for unexpected extreme cases, for possible future modifications,
etc. Furthermore, it is soon realized that input/output to files is more
costly than the sampling time difference between the inverse CDF
sampling method and the one that is currently used, as explained below.

From Sec. 7.2.6 , it is clear that, if the inverse CDF sampling method
cannot be applied, then the other two composite sampling methods should
be tried. For a multi-dimensional random variable @xmath , it is most
natural to use the grouping sampling method (see Sec. 7.2.9 ), with the
sampling of groups being efficiently handled by the inverse CDF sampling
method.

As explained, the variable @xmath effectively depends on a tuple of
@xmath quantities: the conversion surface cell index @xmath , the
transverse momentum magnitude @xmath , the transverse momentum angle
@xmath , and the relative rapidity @xmath . Their discretized sizes are
@xmath , @xmath , @xmath , and @xmath , respectively. The total sampling
space size is the product of these numbers, which will be divided into
groups. Recall that the sampling of groups is done using the efficient
inverse CDF sampling method, thus the larger the groups the more
efficient the process becomes (see Sec. 7.2.6 ). With the memory
limitations and the desire for code simplicity, the largest group can be
formed using the surface index and the relative rapidity, leaving out
the transverse momentum degrees of freedom. Such a grouping gives groups
of size @xmath which easily fit into memory, and their partially
integrated emission function

  -- -------- -- --------
     @xmath      (7.10)
  -- -------- -- --------

will be used in the inverse CDF sampling.

Each emitted particle is generated by first sampling the group it
belongs to using the inverse CDF method, which gives a conversion
surface cell index @xmath and a relative rapidity @xmath . Once having
them, the transverse momentum of this particle is then sampled using the
direct PDF sampling method, which is efficient since it is the
transverse momentum information @xmath for only one particle that needs
to be sampled, which falls into the category of “generating a small
number of samples”, see Sec. 7.2.6 .

Another small advantage is that direct sampling with the PDF generates
samples with continuous @xmath distributions, which reduces the
discretization error for quantities that depend sensitively on angular
distributions, for example the high-order anisotropic flows.

The efficiency of the direct PDF sampling relies strongly on the
estimate of the maximum (Sec. 7.2.6 ). These maxima for the @xmath
-weighted differential emission function can be approximated by their
maxima on the discrete lattice in momentum space, and they are stored
during the calculation of the total multiplicity.

Other grouping configurations have also been tried, including one with a
single inverse CDF function for the whole differential emission
function. Indeed the efficiency from inverse CDF sampling method is the
fastest, and it is @xmath faster than the one proposed above, when
clocked for pure sampling without writing to files. When writing to
files is turned on, the speed difference is no more than @xmath , and
@xmath of the time is spent on writing to files. In fact, significantly
more time would be spent on writing to files had there not been a
manually constructed “buffer” for file writing, which is explained in
Sec. 7.4.4 .

#### 7.4.2 Semi-analytic approach

For a boost-invariant system, the flow rapidity is equal to the
space-time rapidity, and it follows that the distribution function is a
function of only @xmath instead of both @xmath and @xmath . By changing
variables, the invariant integral measure @xmath can also be written as
@xmath . Therefore the emission function calculated in each cell of the
conversion surface:

  -- -------- -- --------
     @xmath      (7.11)
  -- -------- -- --------

is a scalar under boosts. The semi-analytic approach takes advantage of
this invariance and evaluates @xmath in the local fluid rest frame.

First it can be shown that the integral of @xmath is zero. To see this ⁸
⁸ 8 As an alternative argument, first write the integral measure as
@xmath , then it is clear that the integral involving @xmath is
proportional to:

@xmath

since for @xmath the integrand is odd in @xmath and thus integrates to
@xmath . Similarly @xmath has to be the same as @xmath to produce
non-zero results. Since in the local rest frame @xmath , the integral in
eq. ( 7.12 ) for @xmath can thus be written as:

@xmath

Next notice that by symmetry, @xmath , @xmath , and @xmath can be
replaced by @xmath and then it follows from the traceless condition of
@xmath that the integral over @xmath is zero. , write the integral as

  -- -------- -- --------
     @xmath      (7.12)
  -- -------- -- --------

The quantity in the parentheses is a contravariant rank-3 tensor that
depends only on @xmath , therefore it can only be a sum of tensors
composed from @xmath and @xmath . If the indices @xmath or @xmath is
carried by @xmath , then by transversality of @xmath and @xmath , the
corresponding contraction upon @xmath or @xmath gives zero; otherwise
the term contains @xmath which also leads to zero when contracted with
@xmath by the tracelessness of @xmath .

Thus the full distribution function @xmath can be replaced by the
equilibrium distribution in the local rest frame:

  -- -------- -- --------
     @xmath      (7.13)
  -- -------- -- --------

where the last equality used that @xmath in local rest frame. From the
symmetry of @xmath , it is clear that:

  -- -------- -------- -------- --------
     @xmath   @xmath            (7.14)
              @xmath   @xmath   
  -- -------- -------- -------- --------

And finally,

  -- -------- -- --------
     @xmath      (7.15)
  -- -------- -- --------

and the last integral can be pre-calculated and tabulated for rescaled
mass @xmath and rescaled chemical potential @xmath .

Furthermore in the local rest frame for any @xmath -vector @xmath , its
@xmath -th component can be written as @xmath where the last expression
can be evaluated in any frame; thus in the local rest frame, @xmath
becomes @xmath in the lab frame. Therefore eq. ( 7.11 ) becomes:

  -- -------- -- --------
     @xmath      (7.16)
  -- -------- -- --------

Using eq. ( 7.16 ), the emission function on each conversion surface
cell in eq. ( 7.11 ) can be calculated without preforming the numerical
integral, but by evaluating an algebraic expression with results read
from a pre-calculated table. This is the most important step in this
fast semi-analytic sampling approach.

As mentioned, the variable @xmath depends on a tuple of @xmath
quantities: conversion surface cell index @xmath , transverse momentum
magnitude @xmath , transverse momentum angle @xmath , and relative
rapidity @xmath . The semi-analytic sampling approach is a grouping
sampling method that divides the full @xmath sampling space into groups
labeled by @xmath . The probability for a sampled particle to belong to
each group is given by @xmath in eq. ( 7.16 ). The groups are sampled by
the inverse CDF method, which gives the conversion surface cell index
from which a particle is emitted; once a particle is known to be emitted
from a certain conversion surface cell, its @xmath dependence is sampled
using the direct PDF sampling method.

Recall from Sec. 7.2.6 that the efficiency of the direct PDF sampling
method depends intensively on the closeness of the estimated maximum of
the PDF to the real one. Compared to the purely numerical approach (Sec.
7.4.1 ), the maximum of the differential @xmath cannot be extracted
numerically since there is no more numerical integration thus no more
numerical scan of the integrand over the sampling space, and it has to
be estimated analytically.

In the following derivation, the maximum is closely related to the
function:

  -- -------- -- --------
     @xmath      (7.17)
  -- -------- -- --------

where @xmath . By setting its derivative to zero the extrema can be
found by solving:

  -- -------- -- --------
     @xmath      (7.18)
  -- -------- -- --------

This equation is transcendental and it cannot be solved algebraically;
however, the solution to the equations expressed in terms of @xmath and
@xmath can be pre-calculated and tabulated. For fermions (upper sign),
the solution always exists and it is expressed by the Lambert
W-function; for bosons (lower sign) the equation only has real solutions
when @xmath , and it yields two solutions; the one needed is the one
satisfying @xmath . In the following, the solution to this equation will
be denoted as @xmath when it exists.

The maximum of @xmath with constraint @xmath will be denoted as @xmath
and it depends on several conditions:

1.  For fermions (upper sign), @xmath has a single peak at @xmath and
    the constraint maximum is @xmath if @xmath and otherwise is @xmath .

2.  For bosons (lower sign) with @xmath , eq. ( 7.18 ) has no solution
    and the maximum is @xmath .

3.  For bosons (lower sign) with @xmath , @xmath has two extrema in
    @xmath with the larger one being the maximum and given by @xmath .
    If @xmath then the maximum is @xmath ; otherwise the maximum is the
    larger one between @xmath and @xmath .

In the following, an estimate is given of the maximum of the
differential emission function:

  -- -------- -- --------
     @xmath      (7.19)
  -- -------- -- --------

Write @xmath corresponding to @xmath and @xmath , respectively. Note
that @xmath and @xmath are scalars whose evaluations, including their
maximum, are frame independent. Their maxima are thus estimated in the
local rest frame, but expressed using quantities in the lab frame.

For a boost-invariant system, @xmath and the conversion surface cell
volume in the local rest frame can be estimated as:

  -- -------- -------- -------- --------
     @xmath   @xmath            (7.20)
                       @xmath   
  -- -------- -------- -------- --------

As explained earlier in this section, in the lab frame @xmath should be
replaced by @xmath . The quantity @xmath in the lab frame has the form
@xmath , where @xmath is the spacial projection operator, and it can be
explicitly calculated by an actual boost from @xmath in the lab frame
as:

  -- -------- -------- -------- --------
     @xmath   @xmath            (7.21)
                       @xmath   
  -- -------- -------- -------- --------

For the equilibrium contribution @xmath , it is clear from eq. ( 7.20 )
and eq. ( 7.19 ) that the remaining part is to calculate the maximum for
the function:

  -- -------- -- --------
     @xmath      (7.22)
  -- -------- -- --------

and the solution is @xmath .

Next I give an estimate to the maximum of the off-equilibrium part with
choice @xmath :

  -- -------- -- --------
     @xmath      (7.23)
  -- -------- -- --------

The conversion surface cell volume is again estimated as in eq. ( 7.20 )
and eq. ( 7.21 ). For a boost-invariant system @xmath ; in the local
rest frame, the rotational freedom in the transverse direction can be
used to set @xmath ; it then follows from the Hölder inequality and the
fact that @xmath that

  -- -------- -- -------- --------
     @xmath               (7.24)
                 @xmath   
  -- -------- -- -------- --------

The quantity @xmath is a scalar and thus can be evaluated using @xmath
from the lab frame. Using that @xmath and after combining all the power
of @xmath ’s and assuming that @xmath , the rest is straightforward:

  -- -------- -- --------
     @xmath      (7.25)
  -- -------- -- --------

where @xmath for fermions (upper sign) and @xmath for bosons (lower
sign).

To summarize, the maximum for the differential @xmath that integrates to
the one given in eq. ( 7.11 ) is estimated as:

  -- -------- -- --------
     @xmath      (7.26)
  -- -------- -- --------

where @xmath is given in eq. ( 7.21 ).

The validity of the assumption @xmath depends on the choice of the
conversion surface cell temperature, chemical potential, etc. It is
practically true almost all the time, although there are instances where
it is violated. If a more rigorous result is desired, the inequality
7.25 can be replaced by the following one:

  -- -------- -- --------
     @xmath      (7.27)
  -- -------- -- --------

where @xmath is a parameter one can tune to sharpen the inequality.

The efficiency of direct PDF sampling with maximum given by eq. ( 7.26 )
can be tested numerically by studying the ratio between the real maximum
encountered in actual sampling and @xmath . This ratio varies: it is
@xmath for pions, which take most of the sampling time due to their
large multiplicity, and it can be as low as @xmath for some heavy
particles.

There is yet another approximate treatment for the maximum: the observed
numerical maximum from the large number of samples is very close to the
real maximum; therefore when a huge number of samples are needed, the
observed numerical maximum from some portion of the samples can be used
to approximate the real distribution maximum for the rest of the
sampling process. Such a “dynamic maximum” mechanism has been
implemented and it can be enabled by switching parameters, as explained
in detail in Sec. D .

#### 7.4.3 Comparison between the approaches

Both methods use the grouping sampling method, with groups sampled by
the inverse CDF method and sampling within groups by the direct PDF
sampling method. The grouping between these two approaches are
different, and the main difference is how the PDF for the probability
distribution of the group random variables is constructed: the numerical
approach constructs it numerically and the semi-analytic approach
construct it semi-analytically. The total amount of time spent on
sampling can be split into two parts: the part for calculating the total
multiplicity and constructing the PDF for the group variable and its
inverse CDF, and the part for the actual sampling.

For the construction of the group variable PDF and its inverse CDF, the
numerical approach takes a much longer time, due to the numerical
summation over a huge array. As a comparison, the semi-analytic approach
takes almost no time at this stage; in fact most of the time it takes is
to read the pre-calculated tables from files.

For the actual sampling, the numerical approach takes a much shorter
time per particle sample compared to the semi-analytic approach, and the
reasons are explained in the following.

The first reason is that due to the different grouping scheme, the
semi-analytic approach needs to sample one more degree of freedom @xmath
compared to the numerical approach.

The second reason is that the numerically estimated maximum in the
numerical approach is closer to the real maximum compared to the
analytically estimated maximum in the semi-analytic approach. However,
it should be warned that the numerically estimated maximum in the
numerical approach is from a discrete @xmath lattice and it is not the
exact maximum on the continuum @xmath space, although the actual
difference is so tiny that it generates only @xmath portion of defects.

Because of the different characteristics of the two approaches, the
numerical approach is suitable for a large number of repeated sampling
while the semi-analytic approach is suitable for a small number of
repeated sampling. For clarity, I define @xmath -repeated sampling such
that with a given conversion surface, @xmath number of particles will be
sampled, where @xmath is the total number of particle calculated from
the Cooper-Frye formula; thus a large number of repeated sampling means
@xmath and a small number of repeated sampling means @xmath .

The sampling time is tested with a realistic conversion surface
containing @xmath conversion surface cells. The calculated thermal
@xmath yield is @xmath . The test is done using one CPU core with @xmath
GHz frequency.

First the two approaches are tested with @xmath repeated sampling. The
numerical approach takes @xmath s to calculate the group variable PDF
and its inverse CDF and the total multiplicity for @xmath ; a similar
amount of time is spent for each of the rest of the @xmath species of
the particles ⁹ ⁹ 9 Another “grouping optimization” is used here to
speed up the calculations so the actual time spent is @xmath , see Sec.
7.4.4 . . The sampling for @xmath takes @xmath s and much less for heavy
particles due to the smaller number yields. The total execution time for
all species of particles is @xmath s.

The semi-analytic approach takes @xmath s to calculate the group
variable PDF and its inverse CDF ¹⁰ ¹⁰ 10 A high precision table for the
integral in eq. ( 7.16 ) is used here; when a low-precision ( @xmath
times larger table spacing when tabulating) table is used it takes
@xmath s. for all @xmath species of particles. It then take @xmath s to
sample @xmath , and much less time for the heavy particle species due to
their rareness. The total execution time is @xmath s.

Next, they are tested for @xmath repeated samplings. As expected, the
running time for the numerical approach does not change much, since most
of its time is spent on calculating the group variable PDF and its
inverse CDF, and the run time for the semi-analytic approach becomes
@xmath s.

Finally they are tested for @xmath repeated samplings. The time spent on
constructing the group variable PDF and its inverse CDF does not change
and the numerical approach spent @xmath s on sampling @xmath , while the
semi-analytic approach spent @xmath s on sampling @xmath . ¹¹ ¹¹ 11 The
rest of the species are not tested, but we expect the sampling time
ratio between the two sampling methods to be roughly the same for other
species of particles.

The comparison is summarized in table 7.2 .

As a quick summary, for a small number of repeated samplings, the
semi-analytic approach is @xmath times faster and should be used; for a
large number of repeated samplings, the numerical approach is @xmath
times faster and should be used.

Another subtlety is that due to numerical inaccuracy, especially with
fluctuating initial conditions, the trace of @xmath is not exactly zero.
Therefore the @xmath for one piece of the conversion surface calculated
from the semi-analytic approach, which assumes the traceless of the
@xmath , is not exactly the same as that calculated from the numerical
approach. However, with our @xmath regulation treatment in the
hydrodynamical simulation (Sec. 8.2 ), the difference is small. It has
been checked that only for small @xmath can the difference become as
large as @xmath and only for extremely small @xmath can the difference
becomes as large as @xmath .

#### 7.4.4 Other optimizations and implemented models

##### Particle grouping

Within the Cooper-Frye formalism, two particles that are both fermions
or both bosons are considered the same if they have the same mass,
chemical potential, and spin degeneracy. Such particles can be grouped
and the emission function needs only to be calculated once for each
group. At least a particle and its anti-particle belong to the same
group, which cuts the total number of particles that need to be
calculated from @xmath to @xmath . Furthermore, if high accuracy is not
necessary, particles with mass and chemical potential difference within
a certain threshold can also be grouped to speed up the calculation.
These particles still need to be of the same type, meaning that they are
both fermions or both bosons, and have the same spin degeneracy.

An automatic particle grouping treatment is implemented. Such a
treatment first orders all particles according to their mass, then
during the loop over particle species for calculating the emission
function, if two successive particles with the same type have mass and
chemical potentials with the given threshold, the previous result from
the first particle will be reused.

##### Determining total number yield

The Cooper-Frye formula gives average particle yields, which usually is
not an integer; however the actually particle yield in each sampling has
to be an integer. The way to get the integer number yield from the
average value is model dependent. So far three models have been
implemented.

The first model separates the average particle yield into an integer and
a fractional part, and the factional decimal is used as the probability
to sample one more particle. For example, if the average pion yield is
@xmath , then during the actual sampling @xmath of the time @xmath pions
are generated instead of @xmath .

The second model also separates the average particle yield into an
integer and a factional part. However, instead of being used as the
probability of sampling one more particle, the fractional part is used
as the mean number of particle in a negative binomial distribution. The
actual yield of the particle then is the integer yield plus a number
sampled from the negative binomial distribution.

The third model uses the average particle yield (not only the fractional
part) as the mean for a negative binomial distribution to sample the
actual integer number yield.

These three models yield the same average yields of particles
theoretically, however when performing code checks, one should bare in
mind the issue mentioned in Sec. 7.2.10 .

How to switch between these models and to tune their associated
parameters are explained in Sec. D .

There are other models that can be used to determine the number yield.
For example, the total energy on the conversion surface can be used as
one such constraint. Such a sampling model is only partially
implemented, but can be completed relatively easily ¹² ¹² 12 The
advantage of using total energy to constrain the total number yield is
that only one numerical integral needs to be done on the conversion
surface instead of for all @xmath species of particles, and it is
computationally cheap. However, after the discovery of the
“semi-analytic” (Sec. 7.4.2 ) approach, the total number yield
calculation does not cost heavy computation time anymore and the
sampling using the total energy method became less appealing to us. .

It should be emphasized again that the purpose of the iSS code is to
create an easy-to-use framework in which additional physics models can
be implemented and tested relatively easily. For this reason, the
currently implemented models are more for demonstration purpose: the
justification and comparison between models are left for future work.

##### File-writing buffer

When using the numerical approach to sample a large amount of samples,
the efficiency of writing particle information into files becomes the
bottleneck of the sampling subroutine. It was once tested on a scenario
that the sampling itself takes only @xmath s, while without properly
handling, the process of writing to files takes @xmath s. The frequent
access to files causes a drop in efficiency, and an internal string
buffer is then created to temporarily store the samples, and the buffer
is only written to files when a large amount of data is accumulated.
This string buffer mechanism is tested to be faster than the default
buffer provided by the operating system, and its size can be tuned. With
optimized buffer size, the time for writing for the example mentioned
above is reduced to @xmath s.

### 7.5 Code verification

The sampling algorithm is tested by comparing quantities calculated
directly from the emission function with those calculated using actual
generated samples. The results are shown in this section, but the
subtlety regarding locally-negative emission functions needs to be
elaborated on first. For clarity, the quantities calculated directly
from the emission function in this section are referred to as
“theoretical results” and the quantities calculated from actual
generated samples are referred to as “results from samples”.

The emission function used in the sampling needs to be non-negative, but
its actual calculated value from the Cooper-Frye formula may not be
necessarily so, and when it is not, it physically describes particles
that flow from the hadron gas phase into the quark-gluon plasma phase.
These are rare and usually are ignored during calculation, although they
are required for ensuring energy conservation for the whole system. The
different treatments of the negative emission function therefore create
only tiny differences; but in a code checking for the sampling
procedure, such systematic uncertainties had better be excluded. In the
following, two different treatments for negative emission functions are
given, and it will be explained that a slightly unphysical treatment is
the one most suitable for a code check. (After the code has been
verified, it was only used in the physically correct mode.)

The simplest treatment is to set the differential emission function
given in eq. ( 7.6 ) to zero whenever it is negative, which is referred
to as the “enforced-positivity” treatment. Such a treatment ignores
those particles flying back into the quark-gluon plasma phase, and it is
considered to be slightly unphysical; but then the emission function is
a perfectly legitimate PDF from which particles can be sampled. For
comparison, the same treatment needs to be applied when calculating
theoretical results, which means that there then has to be a theta
function enforcing the positivity of the emission function in any
integrals involving the emission function. The conclusion is that an
accurate code check for the sampling algorithm is possible with such a
positivity requirement as long as the theoretical calculations are
modified accordingly.

Note that this treatment for the emission function is not commonly used
in previous calculations of theoretical results, where the negative
emission function is kept “as-it-is” to maintain energy conservation.
The integrated elliptic flow calculated from these two treatments has
been checked to give @xmath difference.

There are two approaches used for the sampling (see Sec. 7.4 ). The
enforced-positivity method is possible within the numerical approach,
where the differential emission function is numerically calculated and
thus open to modification. This treatment is not possible within the
semi-analytic approach, because the emission function is enclosed in
analytic integrals and it needs to stay unmodified in order for the
integrals to be solved analytically.

Another treatment is to keep the sign of the emission function “as long
as possible”, and only enforcing it to be positive when necessary. This
treatment will be referred as the “semi-positivity” treatment in the
following. For the current two sampling approaches, which both use the
grouping sampling method (Sec. 7.2.5 ), this treatment means that the
emission probabilities from group variables are calculated, either as
analytic integrals or numerical sums, allowing negative contributions.
It is only after the group variable for particle emission has been
sampled that during the next stage of the sampling (within groups) the
particles are sampled with the enforced-positivity emission function.

However, no theoretical calculations can accommodate the semi-positivity
requirement for the emission function used in the sampling. This is
because in the sampling procedure, the emission function is first used
unmodified in the integration and then enforced to be positive later
during the sampling of the integrand, and such a “timing” treatment has
no associated analytic modifications to the theoretical integrals.
Because of this, the theoretical integrals need to be solved at least in
the same order, such that the same sequence of treatments can be applied
for an honest comparison; for example, for elliptic flows where the
spatial integral is performed before the momentum integral, the
semi-positivity requirement cannot be implemented because the sampling
procedure solves the momentum integral first.

Note that for the first approach, the emission probabilities for group
variables are calculated by integrals of only the positive part of the
emission function, and they are different from those calculated from the
second approach, which has no such constrains. This means that the
different treatment not only alters the distributions of the particle
samples within each group, that is, the momentum distributions of
particles at given spacial locations, but it also alters the
distribution of group variables, which is the spacial distribution of
the particles.

For code comparison, the enforced-positivity treatment for negative
emission functions is used. As explained above, this means that only the
purely numerical sampling approach (Sec. 7.4.1 ) is feasible. The
comparison is performed using an averaged Monte-Carlo Glauber initial
condition corresponding to RHIC collisions in the @xmath centrality bin.
Samples are generated from @xmath repeated samplings.

The @xmath -spectra from theoretical results and samples for thermal
pions, kaons, and protons are compared in Fig. 7.1 .

The spectra calculated from samples agree with the theoretically
calculated spectra up to @xmath GeV, beyond which the signal becomes
noisy due to low statistics.

The differential elliptic flow from theoretical results and samples for
thermal pions, kaons, and protons are compared in Fig. 7.2 .

The differential elliptic flow calculated from samples agrees with the
theoretically calculated flows up to @xmath GeV, where the signal starts
to become noisy due to low statistics. For the integrated @xmath , the
sampled results agree with theoretical results to first @xmath
significant decimals for pions, @xmath for kaons, and @xmath for protons
due to the decrease in statistics; such highly accurate agreements give
confidence in the verification of the codes and the sampling algorithm.

The emission rate as a function of the proper time @xmath for thermal
pions, kaons, and protons are compared in Fig. 7.3 . The agreement
between the theoretical results and those from the samples are
excellent.

The distributions of the emission functions in terms of @xmath at @xmath
calculated theoretically and from samples are compared in Fig. 7.4 using
thermal pions, kaons, and protons. The results from samples agree with
those calculated theoretically. Note that the bins used to average the
samples need to be chosen according to those used in the theoretical
calculations (see Sec. 7.2.10 ), where they are separated by the
location of the points used by the Gauss quadrature (see also Sec. D ).

In Fig. 7.5 , the angular distribution of the emission function @xmath
calculated theoretically and from samples are compared for thermal
pions, kaon, and protons. The results from samples agree well with those
calculated theoretically. Here again the bins used to average samples
need to be chosen according to those used in the theoretical
calculations (see Sec. 7.2.10 ), which are separated by the location
points for Gauss quadrature (see also Sec. D ).

As explained earlier in this section, only the pure numerical sampling
approach is appropriate for a high-accuracy code check. For the
semi-analytic sampling approach, the semi-analytic results for the
emission probability from each conversion surface cells have been
checked to agree well with the numerical results, except when the
emission function is tiny. Possible issues stem from the non-positivity
of the differential emission function and from the non-tracelessness of
@xmath produced by the hydrodynamics code (see more details in Sec.
7.4.3 ).

Still, as a physical comparison rather than a code check, the results
from samples generated from the semi-analytic approach can be compared
to the theoretical ones. For one bumpy initial condition and @xmath
samples, the spectra and the differential flows are compared in Fig. 7.6
. Here the spectra and differential flows are calculated without any
modifications to the emission function, and samples are generated by
semi-analytic approach with naturally the semi-positivity modification
as explained. The agreement on spectra is good, but the agreement on
differential flow for kaons and protons are bad. This is expected since,
for bumpy initial conditions, the emission function can become negative
at random locations, which (by enforcing it to be positive) generates
numeric noise in the signal for the momentum anisotropy that influences
the elliptic flow more than the spectra, which rely only on the averaged
flow.

## Chapter 8 Miscellaneous topics

In this chapter, we discuss some topics addressing questions that
occurred during my research. All sections in this chapter are
independent from each other.

### 8.1 Comparison between eccentricities defined with @xmath and @xmath
weights

In this section we present a brief comparison between the @xmath
-weighted eccentricity coefficients @xmath (Eq. ( 2.17 )) and the @xmath
-weighted @xmath (Eq. ( 2.17 )), as well as their associated angles
@xmath and @xmath . Fig. 8.1 shows a scatter plot of @xmath vs. @xmath
for @xmath . One observes approximate proportionality ( @xmath , @xmath
, @xmath ) over most of the eccentricity range, with slopes that
increase with @xmath . So whereas Fig. 2.5 shows a decrease of @xmath
with increasing @xmath at large impact parameters, the same is not
necessarily true for the @xmath [ 10 ] . On the other hand, the linear
relations between @xmath vs. @xmath imply that the relations between
@xmath and @xmath will look qualitatively the same as those between
@xmath and @xmath in Fig. 2.10 , with appropriately rescaled horizontal
axes.

At the same time, the participant plane angles associated with @xmath
-weighted and @xmath -weighted eccentricities are tightly correlated, as
shown in Fig. 8.2 . For given @xmath , the angles @xmath and @xmath
fluctuate around each other, with a relative variance that increases
with @xmath , on account of the decreasing values of @xmath . From a
practical point of view, both definitions are equivalent, and choosing
between them is a matter of personal preference.

### 8.2 Robust viscous hydrodynamics

Viscous hydrodynamics is a theory perturbative around the ideal-fluid
theory, where the energy-momentum tensor is expanded as:

  -- -------- --
     @xmath   
  -- -------- --

Here @xmath and @xmath are the local energy density and pressure, @xmath
is the flow 4-velocity, @xmath is the energy-momentum tensor assuming
local equilibrium, and @xmath is the spatial projection operator in the
local rest frame, @xmath is the energy flow vector, with @xmath the
baryon density, @xmath the baryon flow vector, and @xmath the heat flow
vector. Choosing the Landau frame and working in the case @xmath and
@xmath , the only viscous effects come from the shear pressure tensor
@xmath .

There are several criteria that @xmath needs to satisfy.

1.  Since viscous hydrodynamics is a perturbative theory, it is only
    reasonable as long as the @xmath term is smaller than the @xmath
    term. A component-wise comparison between them is not appropriate
    since this would be frame-dependent; @xmath and @xmath can only be
    compared after they are contracted into scalars. We choose:

      -- -------- --
         @xmath   
      -- -------- --

    The positivity of the former can be checked easily in the local rest
    frame. Therefore from the perturbative nature of the theory we
    require:

      -- -------- -- -------
         @xmath      (8.1)
      -- -------- -- -------

2.  @xmath should be traceless:

      -- -------- --
         @xmath   
      -- -------- --

3.  @xmath should be perpendicular to @xmath :

      -- -------- --
         @xmath   
      -- -------- --

In actual calculations, there are limits to the numerical accuracy so we
choose a small number @xmath to be the “numerical zero” and conditions 2
and 3 become:

  -- -------- -- -------
     @xmath      (8.2)
  -- -------- -- -------

The vector @xmath should be component-wise zero (in any frame),
therefore all its components should be compared to the “numerical zero”,
multiplied by @xmath . Here we use the scalar @xmath as a measure for
the magnitude of the @xmath tensor that sets the scale (via the factor
@xmath ) for how close the numerical result is to zero.

In practice, to ensure that eq. ( 8.1 ) is satisfied, we choose a number
@xmath and require that ¹ ¹ 1 @xmath corresponds to the required “
@xmath ” condition in eq. ( 8.2 ); @xmath corresponds to no constraint
at all. :

  -- -------- -- -------
     @xmath      (8.3)
  -- -------- -- -------

This, together with eq. ( 8.3 ), then implies

  -- -------- -------- -------- -------
     @xmath   @xmath            (8.4)
              @xmath   @xmath   
  -- -------- -------- -------- -------

These requirements may be violated during the hydrodynamic evolution for
various reasons. For example, the @xmath could be initialized to be
unphysically large (we use the Navier-Stokes condition to initialize
@xmath ), so the relations would be violated at early time, or the
initial profile could be not smooth enough, so the numerical derivative
gives artificially large (and possibly wrong) numerical values that
violate these conditions near discontinuities of the profile, etc. In
principle, once the system is sufficiently close to thermal equilibrium,
a properly generated @xmath , evolved on a super-fine lattice, should
avoid such problems; in practice however, a super-fine lattice is
computationally infeasible and (what is worse) the Isreal-Stewart
equations are not causal on a super-fine lattice [ 122 ] . In practice,
fluctuating initial conditions and early hydrodynamic starting times
lead to patches in the numerical grid where one or several of the
conditions 8.3 are violated. For our simulations, we found, however,
that the region where these conditions are violated are mostly at the
early stage and/or in the dilute regions outside the freeze-out surface.
The violations in these regions do not have much influence on the
dynamical behavior of the QGP in the physical region inside the
freeze-out surface; however if they are left untreated, these violations
lead to accumulating numerical errors that cause the evolution code to
break down later. For these reasons, in the following we develop a
systematic treatment that suppresses large viscous terms. This
stabilizes the code at a low price to pay: we effectively change the
evolution equations in the fireball region where we have very large
viscous corrections to the ideal fluid @xmath .

We enforce a continuous systematic regulation on @xmath in each time
step on the whole lattice by replacing @xmath by @xmath :

  -- -------- -- -------
     @xmath      (8.5)
  -- -------- -- -------

where @xmath is the largest quantity at each lattice point among the
following:

  -- -------- --
     @xmath   
  -- -------- --

It is easy to check that @xmath satisfies eq. ( 8.3 ), and that it is
close to @xmath when no modifications are needed; that is, when the left
hand sides of the inequalities in eq. ( 8.3 ) are small compared to
their right hand sides, the regulated quantity is close to its original
value. Only those grid points where @xmath violates or is close to
violating the inequalities ( 8.3 ) will be strongly modified, in which
case all components of @xmath are suppressed by the same factor.

Because smoother flow velocity profiles give smaller @xmath , the
systematic suppression of @xmath can be understood as locally replacing
sharp jumps in the flow profile by flatter pieces; the regulation
process is therefore an implicit and automatic way of smoothing
profiles. This treatment allows us to perform hydrodynamic calculations
using very bumpy initial conditions, including those using disk-like
nucleons that have density discontinuities. Without this regularization
VISH2+1 breaks down for such initial conditions.

In our calculations, we take @xmath , which is reasonable because the
relative discretization error from solving transport equations is of the
order of a few powers of the lattice spacing and our lattice spacing is
chosen to be @xmath fm. During our simulations we found that @xmath is
best chosen to be a value between @xmath in order to enable the
simulation to finish, and to keep the momentum distribution generated
from the QGP almost unmodified. By choosing @xmath of order unity or
larger, we modify the effective evolution equations only in regions
where the conditions of validity of the Isreal-Stewart equations are
badly violated.

### 8.3 Time evolution of the event plane orientations

The orientations of the event planes are not fixed during the evolution
of the fireball: one of the reasons for their changing is the
mode-coupling effect, by which the orientation of an event plane of a
particular harmonic order receives contributions from geometric
deformations of the fireball of a different harmonic order. To
demonstrate such effects visually, we take 3 randomly selected events,
and divide their freeze-out surface into strips with time step @xmath
fm/c. Each strip of the freeze-out surface is then used to calculate the
anisotropy flows formed by particles emitted only from this part of the
freeze-out surface using the Cooper-Frye formula. The resulting
anisotropic flow angles from different pieces are then studied; their
evolution is interpreted as the time evolution of the anisotropic flow
angles during the lifetime of the fireball. Typical results are shown in
Fig. 8.3 .

Here the anisotropy angles are the @xmath th-order flow angles @xmath
(see eq. ( 2.22 )) calculated from the spectra emitted by the strips of
the freeze-out surface, measured relative to the participant plane
angles @xmath . To make the time evolution manifest, we interpret @xmath
as polar coordinates @xmath , i.e. we plot the complex quantities @xmath
as trajectories (as a function of @xmath ) in the complex plane. The
arrows point to the positive @xmath direction along the trajectories.

An anisotropy angle is said to be “in-plane” if @xmath is close to
@xmath and “out-of-plane” if @xmath is close to @xmath . The most
dramatic rotation happens if the angle changes from one sector to the
opposite sector that differs by @xmath .

It is clear that the 3rd- through 6th-order anisotropy angles in the 1st
event (Fig. 8.3 left), the 5th- and 6th- order anisotropy angles in the
2nd event (Fig. 8.3 middle), and the 3rd-order anisotropy angle in the
3rd event (Fig. 8.3 right) all have undergone dramatic rotations during
the evolution of the fireball. The 2nd- and 3rd-order anisotropy angles
are relatively stable during most of the early evolution, reflecting the
relative smallness of the mode-coupling effects; the angles only rotate
near the end of the evolution of the fireball where the remnant of the
fireball becomes dilute and irregular and the orientation of all
harmonics becomes somewhat random.

### 8.4 Best estimator for flows using eccentricities

The authors of [ 123 ] use regression methods to study the flow response
from the eccentricity predictors, where they use a quantity similar to
the coefficient of determination to find the “best estimator” and to
study the second-order mode-coupling effects. Their calculations are
based on simulations with NeXSPheRIO which uses ideal hydrodynamics,
with flow calculated from Monte-Carlo sampled particles. Here we extend
their work in three ways: 1) by including viscosity, 2) by using the
continuous particle emission formalism, 3) and by including higher-order
mixing terms. Including higher-order terms will always improve the fit,
but the degree of improvement depends on whether the expansion is made
with eccentricities defined using moments or cumulants [ 98 ] .

We used the same pure event-by-event hydrodynamic simulation data tuned
for Pb-Pb collision at 2.76 ATeV as reported in [ 84 ] (and Chap. 4 ),
and plotted the “Quality value” [ 123 ] for various settings as
functions of centrality in Fig. 8.4 . The “Quality value” is so defined
that it is between 0 and 1; the closer it is to 1, the better the
approximation. When approximating @xmath using only linear terms
(equation (2) in [ 123 ] )

  -- -------- -- -------
     @xmath      (8.6)
  -- -------- -- -------

where @xmath is the “error”, the “Quality value” is defined to be
(equation (3) in [ 123 ] ):

  -- -------- -- -------
     @xmath      (8.7)
  -- -------- -- -------

using the best-fit parameter @xmath from eq. ( 8.6 ). When approximating
@xmath including two terms (equation (5) in [ 123 ] )

  -- -------- -- -------
     @xmath      (8.8)
  -- -------- -- -------

the “Quality value” is defined to be (equation (6) in [ 123 ] ):

  -- -------- -- -------
     @xmath      (8.9)
  -- -------- -- -------

using the best-fit parameters @xmath and @xmath from eq. ( 8.8 ). More
terms can be included in the expansion and the “Quality value” is always
defined to be the ratio between the fit and the actual value.

The differences from [ 123 ] include that we use continuous Cooper-Frye
particle emission to calculate flows, and that we include viscosity,
although the viscosity was found to play little role, so only the
results using MC-KLN initial condition with @xmath are shown here.

The elliptic and triangular flows are known to be well approximated as
linear functions of the eccentricities of the same order, so we skip
them here and only report on the results for the quadrangular and
pentangular flows which receive large contributions from mode-coupling
effects. More specifically, various studies including [ 123 ] show that
the quartic flow @xmath mainly receives a contribution from @xmath and
the pentangular flow @xmath receives a contribution from @xmath , and it
is shown in [ 123 ] that by including these two quadratic terms in the
expansions the approximations are improved, but the approximations are
still not good enough, especially in very peripheral collisions.

Here the four curves labeled as @xmath are those “Quality values”
calculated using expansions that contain only two terms: for @xmath ,
they are @xmath and @xmath , and for @xmath , they are @xmath and @xmath
² ² 2 In fact, it is the complex product of the complex eccentricity
vectors that are used (see [ 123 ] ). ; for the cumulant expansion,
@xmath is replaced by @xmath [ 98 ] . It is seen that when including two
terms in the expansion, both the moment- and the cumulant-defined
eccentricities give very similar results in terms of the goodness of the
fit.

Next we allow more terms in the expansion to test the convergence. The
terms we included in the linear combinations are summarized in the
following. Here we use the complex eccentricity notation @xmath (see
Chap. 2 ), with @xmath replaced by @xmath in the case for cumulants, and
“*” for complex conjugate.

-   @xmath using 5 moment-defined eccentricities: @xmath , @xmath ,
    @xmath , @xmath , @xmath .

-   @xmath using 12 moment-defined eccentricities: @xmath , @xmath ,
    @xmath , @xmath , @xmath , @xmath , @xmath , @xmath , @xmath ,
    @xmath , @xmath , @xmath .

-   @xmath using 4 cumulant-defined eccentricities: @xmath , @xmath ,
    @xmath , @xmath .

-   @xmath using 9 cumulant-defined eccentricities: @xmath , @xmath ,
    @xmath , @xmath , @xmath , @xmath , @xmath , @xmath , @xmath .

The curves labeled by “moment/cumulant @xmath dim=…” show the resulting
“Quality values” by allowing more terms in the expansion listed above.
It is seen that the cumulant-defined eccentricities give faster
convergence (the “k values” for both expansion methods converge to 1 if
an infinite number of terms are included) than moment-defined
eccentricities, but neither is good enough to be used as a quantitative
replacement of the hydrodynamics for @xmath (or higher flows), even when
higher order couplings are taken into account. This suggests that, in
terms of a quantitative study for higher-order flows, the role of
hydrodynamics cannot be replaced by simply looking at the response of
the flow to various eccentricities.

## Chapter 9 Summary

In this thesis, I summarized my Ph.D. work on event-by-event
hydrodynamic simulations for relativistic heavy-ion collisions and
related fields.

In Chap. 2 , we compared the previously dominating single-shot
hydrodynamics with the current and future mainstream event-by-event
hydrodynamic simulations. The event-by-event simulations are more
realistic, but they are very time consuming; the single-shot simulation
is economical, but then the question arises as to whether it can be used
as a sufficiently precise replacement for the event-by-event
simulations. The answer to this question depends on the observable
studied. We showed that for the event average of the multiplicity and
elliptic and triangular flow, the time consuming event-by-event
hydrodynamic simulations can, to a good approximation, be replaced by
the single-shot ones, when using properly constructed, participant-plane
rotated and averaged initial conditions. For azimuthally averaged @xmath
spectra the single-shot simulation can still give results not very
different from those given by event-by-event simulations, especially if
the shear viscosity is not very small. For higher-order flows, such as
@xmath , the single-shot simulations are shown to be incapable of
reproducing those from event-by-event simulations. The main reason for
this failure are nonlinear mode-coupling effects that are very sensitive
to initial-state density fluctuations. In this chapter, I also showed
related quantities, such as the distributions of eccentricities and
anisotropic flows from event-by-event calculations. These can be used as
calibrations for future studies.

In Chap. 3 , we showed that the elliptic and triangular flows can be
studied using the much more economical single-shot hydrodynamic
simulations to a good ( @xmath ) approximation. We further showed that
the eccentricity-normalized elliptic and triangular flows are
“definition independent”: @xmath is very close to @xmath , @xmath , and
@xmath ; likewise, @xmath is very close to @xmath and @xmath . The
eccentricity normalized flows from single-shot hydrodynamic simulations
can thus be used to study the experimental flow data when normalized by
similarly constructed eccentricities. This is particularly important
when comparing experimental results (which are always affected by
event-by-event fluctuations) with results from single-shot hydrodynamics
(which does not account for fluctuations). The eccentricity normalized
flows can be used individually to extract the specific shear viscosity
for a given initial condition model or, when combined, can be used to
reduce the ambiguity introduced by different initial condition models
with varying initial fluctuation spectra. We followed this strategy and
showed that the elliptic and triangular flow data measured by the ALICE
collaboration at the LHC prefer a small specific shear viscosity close
to @xmath , when considering the MC-Glauber and MC-KLN models. In order
to allow for a much larger @xmath value, the initial condition model
must feature triangularity values @xmath larger than the ones provided
by the MC-Glauber and MC-KLN models. This was found to be the case in
the IP-Glasma model [ 79 ] .

Chap. 4 focused on correlations between event-plane angles. We showed
that the event-plane angle correlation measurements by the ATLAS
collaboration can be explained using results from the same set of
hydrodynamic simulations tuned for spectra and flow observables. The
same correlation patterns cannot be explained directly from the initial
conditions, implying that they are generated dynamically during the
evolution of the medium. Some of the strong correlation patterns, like
the one between harmonic orders @xmath and @xmath , and the one among
orders @xmath , @xmath , and @xmath , are direct evidence for strong
mode-coupling effects. The correlations serve as another set of
observables, complementing the spectra and anisotropic flow coefficients
@xmath , and the success of their hydrodynamic explanation adds to the
evidence that the medium created in heavy-ion collisions is fact
hydrodynamical. We also note that the strong mode-coupling effects
cannot be reproduced from a linear response treatment of hydrodynamics.

In Chap. 5 , we showed that including only @xmath out of @xmath
carefully chosen resonances can already yield spectra and flow results
within @xmath relative error. Such a treatment can be used to shorten
the currently lengthy ( @xmath 3 hours) resonance decay calculations by
a factor of ten — since these calculations are the current bottleneck
for event-by-event hydrodynamic simulations — and consequently lower the
cost of performing event-by-event simulations by the same factor. We
provided a complete table listing the contribution of resonances to
stable particles for all major stable particles.

Chap. 6 focused on various ways of calculating (theoretically) and
measuring (experimentally) anisotropic flows, and their comparisons. We
pointed out that the event-plane angle @xmath fluctuates from event to
event, and that the differential event-plane angle @xmath also
fluctuates from event to event. We showed that traces of such angular
fluctuations can be measured experimentally, and explained how such
measurements allow the study of the fluctuation of the orientation of
flows in addition to the fluctuation of the magnitude of flows. We
showed that the effects are larger for heavy particles, in near-central
collisions, and for small @xmath . In particular, we proposed a precise
measurement and comparison of @xmath , @xmath , @xmath , and @xmath for
identified pions, kaons and protons with transverse momenta @xmath GeV
to confirm the hydrodynamically predicted effects from flow angle
fluctuations. We also pointed out the flow angle fluctuations are
responsible for half of the factorization breaking effects, the other
half being caused by fluctuations in the magnitudes @xmath of the
anisotropic flows.

In Chap. 7 , we gave a thorough description of the general sampling
methodology, and showed that it can be applied in an efficient way to
sample particles whose emission is determined by the Cooper-Frye
formula, using the simplification of longitudinal boost invariance. We
explained several sampling algorithms and analyzed their strengths and
weaknesses, focusing heavily on sampling efficiencies. This chapter
serves as a study in sampling methodology as well as a technical
document that explains the actual sampling implementation used in the
iSpectraSampler program.

We also included discussions of a number of other miscellaneous topics
in Chap. 8 . We compared the @xmath - and @xmath -weighted
eccentricities. We described how we regulate unphysically large @xmath
values that can arise especially during the early stage or in the dilute
tail of the density distribution of the fireball hydrodynamic evolution.
In addition, we showed that the event plane angles determined using
particles emitted at different proper times are not necessarily the
same, and that their time evolution can be dramatic. Finally, we showed
quantitatively that including nonlinear combinations of eccentricities
can improve the prediction of @xmath and @xmath as the corresponding
responses.

## Appendix A Choice of parameters used in the simulations

This section contains tables for the choice of parameters used in our
simulations. Unless for academic study (Chap. 2 ), all our simulations
are tuned to explain experimental data, which requires a choice of
parameters according to the following tables. These tables should be
used for pure hydrodynamic simulations only, not for hybrid simulations
(see Chap. 1 ).

Here the “initial time” is the initial proper time @xmath when
hydrodynamics starts. The “decoupling temp.” is the decoupling
temperature under which hadrons cease to interact and fly into the
detector via free streaming. The “model parameter” for the MC-Glauber
model is the soft-hard ratio @xmath ( @xmath in eq. ( 3.1 ), @xmath in
eq. (20) in [ 6 ] ), and for the MC-KLN model it is the @xmath parameter
used when determining the saturation scale (Eq. (9) in [ 8 ] ). Finally
the ‘‘norm.” is the normalization factor that, when multiplied with the
profile directly generated from the initial condition models, gives the
initial entropy density profile that can be used in hydrodynamic
simulations. ¹ ¹ 1 For the MC-Glauber model, it is the @xmath parameter
in eq. ( 3.1 ) or @xmath in eq. (20) in [ 6 ] . For the MC-KLN model, we
take @xmath .

## Appendix B Table of parameters for cutting centralities

When using the optical Glauber model, the centrality class can be
determined and cut in terms of ranges in impact parameter @xmath ; Table
LABEL:tab:1 lists the choice of impact parameters used in cutting
centralities this way. A given centrality class includes events with
impact parameters ranging from @xmath to @xmath ; the event-averaged
impact parameter is @xmath and the averaged @xmath is @xmath . This
table was used in the study presented in Chap. 2 .

In later studies, we use centrality classes determined by @xmath ; such
centrality class cuts are contained in tables B.2 - B.8 . For a given
centrality bin, each table includes the @xmath and @xmath values which
define the bin, the average @xmath , the range in impact parameter
@xmath to @xmath , and the average @xmath .

## Appendix C Feed down contribution tables for @xmath, @xmath, @xmath,
@xmath, @xmath, and @xmath [100]

This section contains tables for the resonance decay contributions as
explained in Chap. 5 . For all the tables listed in this section, @xmath
MeV.

## Appendix D Compiling, running, and tuning iSS

### d.1 Compiling

The iS and iSS ¹ ¹ 1 They can be found from the iEBE package at
https://bitbucket.org/qiu_24/iebe. codes are easily compiled using the
provided Zmake.sh script files. These are not real makefiles, but are
actually short bash scripts that compile the code in one line. Makefiles
are convenient for compiling large scale programs whenever keeping the
intermediate output binary files saves compiling time. For small scale
programs, however, a cleaner direct compile is preferable. The script
also tries to compile with Intel compilers when available, to generate
faster executables.

### d.2 Running

After compilation, executables with the name “iS.e” or “iSS.e” are
generated. They can be executed either without supplying any command
line parameters, or with specific parameter sets using the
variable=value syntax. These programs read input data files from the
results directory, and write output data files to the same directory;
details for this are given in the following sections. The iSS program
reads parameters.dat for all the parameter assignments, and overwrites
them with any additional parameter assignments read from the command
line arguments. The structure of the parameter file and explanations for
parameters it contains are presented in the following sections.

The number of outputs can be modified by editing the AMOUNT_OF_OUTPUT
macro in the emissionfunction.cpp file.

### d.3 Input and output files

Currently ² ² 2 By December 14, 2021, iSS has version 2.3.0.2 and iS has
version 1.2.1.12. both programs read as input the freeze-out surface
information data files surface.dat and decdat2.dat , and the chemical
potential file decdat_mu.dat from the results directory, which are all
output files from the VISH2+1 hydrodynamical simulation program. For the
format of these files, refer to the corresponding documents.

Both programs also need equation of state (EOS) information and particle
data information, which are given by files in the EOS directory:
EOS_particletable.dat , pdg.dat , resoweak.dat . Another file
chosen_particles.dat in the same directory is a one-column list of
standard particle Monte-Carlo indices, and only particles whose Monte
Carlo index is included in this file will be processed by the programs;
others will be skipped. The Monte-Carlo indices can be looked up in the
pdg.dat file, for example, @xmath is indexed as @xmath .

When the program finishes, depending on the operations that were
performed, different sets of files will be generated in the results
folder.

If spectra and flow calculations are enabled (optional in iSS, mandatory
in iS), files with names like thermal_xxxxx_vn.dat and
thermal_xxxxx_integrated_vn.dat will be generated. Here xxxxx refers to
the standard Monte-Carlo particle index given by the particle data book
( pdg.dat ).

The files thermal_xxxxx_integrated_vn.dat record @xmath -integrated flow
results. These files contain @xmath rows of output, where @xmath is the
largest harmonic flow coefficient calculated, starting from @xmath .
Each row has @xmath columns: the first column is the order of the flow,
the second and the third columns contain the real and imaginary part of
the numerator on the right hand side of eq. ( 7.8 ) ³ ³ 3 With an
additional factor @xmath ; therefore the second quantity in the first
row is the total multiplicity. These two columns are mainly for
debugging purposes. , and the fourth, fifth, and sixth columns contain
the real and imaginary parts, and the magnitude of the complex flow
vector @xmath .

The files thermal_xxxxx_vn.dat store differential flow results. These
are block-shaped data files that store the @xmath matrix for given
@xmath and @xmath indices. The row index corresponds to the @xmath index
and the column index to the @xmath index. For each @xmath or @xmath
index, the actual @xmath or @xmath value can be looked up from the
corresponding tables in the tables directory, which by default are the
pT_gauss_table.dat or phi_gauss_table.dat files. The tables directory
will be explained below.

If the sampling of particles is enabled (optional with iSS, not possible
with iS), files with names like samples_xxxxx.dat ,
samples_control_xxxxx.dat , and samples_format.dat are generated, with
xxxxx the Monte-Carlo particle indices.

The files samples_control_xxxxx.dat contain a one-column listing of the
actual number of sampled particles from each of the given number of
repeated samplings. The sum of all its elements gives the total number
of sampled particles over the specified rapidity range whose average is
given by the @xmath -integrated Cooper-Frye formula.

The files samples_xxxxx.dat contain the information of the generated
samples, with each line corresponding to one particle and containing all
its spatial and momentum information. The meaning of elements in a
particular column varies from version to version, so they are recorded
in the samples_format.dat file to ease the reading process. The
samples_format.dat file is written with “equal-sign assignment” syntax,
which can be conveniently read back using the ParameterReader class (
E.3 ). For example, the line tau = 2 means that the @xmath information
of the particles are written in the second column.

The iSS program also supports writing to OSCAR format ⁴ ⁴ 4
http://karman.physics.purdue.edu/OSCAR-old/ , in which case the file
OSCAR_header.txt is copied to as the header of the generated OSCAR file.

### d.4 Parameter file

The Parameters.dat file is an “equal-sign assignment” file storing the
major tunable parameters used by the iSS program. Such a file can be
easily read using the ParameterReader class (§ E.3 ).

All the parameters are accompanied by detailed explanations. The most
important parameters are:

-    calculate_vn : When set to @xmath , spectra and flows will be
    calculated.

-    MC_sampling : When set to @xmath , no sampling will be done;
    setting it to @xmath generates samples using the numerical sampling
    approach (§ 7.4.1 ); setting it to @xmath generates samples using
    the semi-analytic approach (§ 7.4.2 ).

-    number_of_repeated_sampling : How many repeated sampling should be
    performed.

-    dN_dy_sampling_model : Used to switch between different models for
    generating the integer @xmath from the averaged value, see 7.4.4 .

-    use_dynamic_maximum : Whether to turn on the “dynamic maximum”
    mentioned in § 7.4.2 .

-    grouping_particles and grouping_tolerance : if grouping_particles
    is set to @xmath , particles with similar mass and chemical
    potentials will be grouped together for calculation, and the
    threshold for determining such similarity is controlled by the
    grouping_tolerance parameter, see § 7.4.4 .

Other parameters are explained by their comments in the file.

### d.5 Tables

Tables (§ E.1 ) are used extensively throughout the iS and iSS programs.
They are read from block-shaped data files located under the tables
directory using Table or TableFunction class.

In particular, all the numerical integrals are performed by evaluating
the sum:

  -- -------- -- -------
     @xmath      (D.1)
  -- -------- -- -------

with the location-weight pair @xmath specified by some table. For
example, it is set in the main.cpp file that the @xmath integral should
be done using the pT_gauss_table.dat file, which stores location and
weight information for integration using Gauss quadrature. Other
integration methods (e.g. Simpson’s method etc.), can be conveniently
substituted by simply switching the integration table file, without
changing the source program. Similarly the file phi_gauss_table.dat and
eta_gauss_table_20_full.dat are currently used to perform the @xmath and
@xmath integrals.

Files p_integral_table_0.02.dat and m_integral_table_0.02.dat are the
pre-calculated tables used to evaluate the integral in eq. ( 7.15 ). The
lambertw_function.dat and z_exp_m_z.dat files contain tables solving the
transcendental equations ( 7.18 ).

There are other files for tables used during the binning process of the
samples.

## Appendix E Introduction to iSS support classes

As mentioned in the introduction of Chap. 7 , common tasks are
encapsulated into classes for re-usability. In this section, such
classes are introduced briefly; details can be found in the codes.

### e.1 Table class and table-function classes

It is common to load a block of data of unknown size into memory to
analyze. The Table class is made for such a purpose. It can be
initialized from block-form data file or double array. It provides an
interface to read and write elements in the table. The size of the table
is dynamically allocated and it can change automatically once a
write-action exceeds the current data boundary. It can perform
interpolation between any two specified columns of data using
nearest-neighbor, linear, or cubic methods, and it can perform bilinear
interpolation using the whole block as a matrix as well. The Table class
is used extensively throughout the iSS program to read in replaceable
tables, like those used for integration or pre-tabulations.

The TableFunction class is oriented for 2-column tables that represent
functions. It can be initialized from a file or directly, and it
provides interpolation and inverse search interfaces. This class is
intended to be used as the underlying class for representing numerical
functions using tables.

### e.2 Classes related to random variables

The sampling of random variables are implemented in the base class
RandomVariable , which supports direct PDF sampling, sampling using the
inverse CDF, and sampling with envelope distribution methods.

The PDF function and the inverse CDF function for the random variable
being sampled and for the envelope random variables are implemented by
default using a table through TableFunction class, which can
conveniently return results using several methods of interpolation, and
which can be initialized from file, double array, or generated directly.
All these distribution functions are declared as virtual, meaning that,
when the class is inherited, these functions can be overloaded and
implemented in any desired way. For example, one implemented derived
class NBD overloads the PDF function by an analytic expression, since
the PDF depends on several parameters and it is more naturally expressed
analytically, rather than in tabulated form.

Once the function required for one sampling method is loaded, the
sampling of the random variable can be done by calling the corresponding
sampling member function. The class also has a member function that can
calculate the inverse CDF function from the PDF function, and a function
that can automatically generate a stair-shaped envelope function (see §
7.2.7 ) given the mode and standard deviation.

The RandomVariable class is meant to be the base class for the most
general 1d random variable sampling. One derived class based on it is
the NBD class which samples the negative binomial distribution (NBD)
using an automatically constructed stair-shaped envelope function (see §
7.2.7 and code). The PDF for the NBD is overloaded as an analytic
function; for efficiency, the automatically generated envelope function
is recalculated only when the parameters for NBD change.

There are several other simpler classes RandomVariable1DArray ,
RandomVariable2DArray , and RandomVariableNDArray that are designed for
sampling discrete PDFs given as double arrays, using the inverse CDF
sampling method. They are less general compared to the RandomVariable
class, but they are optimized for index sampling and they require less
memory.

### e.3 Parameter reader class

It is very common in scientific programming that results depend on many
parameters. It would be convenient if they could be stored and loaded
from files, which are then passed between functions via references
instead of by values. The ParameterReader class is created for such a
purpose. Parameters can be assigned using “equal-sign assignments”
syntax throughout the file, from command line arguments, or by calling
member functions directly. After initialization, the parameters are
stored and can be extracted easily. With such a class, parameters can be
passed between functions and classes using an instance of this class,
instead of being passed directly, since this would be tedious to
maintain, and prone to errors.

The most common usage of this class is to read parameters stored in a
file, which can be done through a member function or simply through the
constructor. In such cases, the file has to be written with “equal-sign
assignment” syntax, meaning that each line of the file can contain an
equal-sign assignment, a piece of comment beginning with the character
“#”, or neither, or both. For example, the line:

MC_sampling = 2 # 0/1/2: whether to perform Monte-Carlo sampling

creates a variable with string name “MC_sampling” and assigns the
initial value “2” to it, which can be conveniently extracted by the
getVal member function. The part of the line after the “#” symbol is
treated as a comment and is discarded during the reading process.

There is another member function that reads the command line arguments
for parameter assignments. In this case, each argument needs to be an
“equal-sign assignment”, without blanks or comments; for details, see
the header file.

Parameters read later will overwrite any parameters with the same name
read earlier; otherwise, new variables will be created. Parameters can
also be assigned directly using a member function. For other
functionalities and details, refer to the header file and comments
directly following the implementation of each function.

## Appendix F The iEBE package

The iEBE package is a convenient package for automating event-by-event
hybrid calculations. It divide calculations into “jobs”, where each job
consists of multiple “ebe-calculations”. Each “ebe-calculation” is a
complete hybrid calculation that in execution order performs: heavy-ion
event generation (superMC), hydrodynamics simulation (VISHNew), particle
emission sampling (iSS), hadron rescattering simulation (osc2u and
urqmd), flow calculation (binUtilities), and finally, collection and
storing of important results in databases (EbeCollector). Each “job”
runs the given number of “ebe-calculations” sequentially, and “jobs” are
run in parallel. The package has utility scripts that can combine the
generated SQLite database files from different jobs into one, which can
be analyzed later.

The main programs are contained in the subfolder “EBE-Node”, which is
used to perform one job, and which will be duplicated when multiple jobs
are run. The package needs two locations to perform multi-job
calculations: one folder is used to store duplications of “EBE-Node” and
intermediate results generated during the calculation (refer to as
“working folder” in the following), and another folder is used to store
final results (refer to as “result folder” in the following). By
default, the working folder is named “PlayGround” and the result folder
is named “RESULTS”, both in the root directory of the package.

### f.1 How to use the package to perform multi-job calculations

This section explains how to use the highest-level scripts provided by
the package to perform event-by-event hybrid calculations. Any one using
the package should read this section thoroughly, even those who are not
interested in modifying the package.

VERY IMPORTANT: Make sure you have Python 2.7+ (or Python 3) installed
before proceeding.

In the following, all paths are relative to the root directory of the
package.

Step 1 Generate jobs using the ./generateJobs.py script.

To generate jobs, use the generateJobs.py script in the root directory.
Most of the runnable scripts in this package provide the feature that if
you run them without additional arguments, they will print the usage
echo, for example:

$ ./generateJobs.py

And you should see the output:

Usage: generateJobs.py number_of_jobs number_of_events_per_job

[working_folder=“./PlayGround”] [results_folder=“./RESULTS”]

[walltime=“03:00:00” (per event)] [compress_results_folder=“yes”]

The echo says that the 1st argument for the script should specify the
number of jobs you want to generate; the 2nd argument specifies the
number of ebe-calculations for each job; the 3rd argument points to the
result folder; the 4th argument specifies the “wall time” (used in
torque system, explained later); the 5th argument points to the working
folder; and the 6th argument is for whether to compress final results.
Except for the first two, all other arguments have default values. The
simplest way to generate jobs is just to accept the default values. As
an example, to generate 2 jobs, each performing 5 ebe-calculations,
simply do the following:

$ ./generateJobs.py 2 5

This script will first check required libraries, compile any programs
not existing already, and generate the actual folders for jobs.

After you see the echo “Jobs generated.”, you should see the working
folder “PlayGround” and the result folder “RESULTS” in the root
directory.

Step 2 Submit jobs.

The way to submit jobs depends on the system. For a cluster that has the
“Torque” scheduling system (therefore the “qsub” command is available),
submit jobs using the submitJobs_qsub.py script; for a local
computation, use the submitJobs_local.py script. The difference is that
the local computation is only paralleled for the local CPUs and
calculation on the cluster, via the torque system, will be distributed
(trivailly) to multiple nodes. To submit a local calculation, simply
type (the script knows how to get the location of the working folder
automatically):

$ ./submitJobs_local.py

You should see some feedbacks listing the jobs that have been submitted.

Step 3 Checking progress.

Progress for all jobs can be checked by the progressReport.py script in
the root directory:

$ ./progressReport.py

It will list the current progress for all jobs.

Step 4 Combining databases.

Once all calculations are finished, the generated database files from
all events will be combined automatically, and a single file
“collected.db” will be generated in the results folder.

### f.2 How to analyze generated data

The “collected.db” generated from previous steps is the SQLite database
file that can be analyzed by any desired means. The recommended way is
to use the uhg.py script in the utilities folder. This script not only
reads the database, but also performs additional analyses, like
interpolation along @xmath , calculation of the mean, etc. It can either
be run from the command line to evaluate a single expression or
interactively from a shell. To evaluate a single expression, run the
uhg.py script in the utilities folder:

$ ./uhg.py database_filename “expression to evaluate”

A more convenient way to evaluate multiple expressions as well as
perform additional analyses is to run the uhg.py script interactively.
For example:

$ python -ic “from uhg import *”

The interactive mode will also print out a simple help showing
recognizable symbols that can be included in the expression.

Another way is to use the databaseQuery.py script to evaluate a single
piece of SQL query from the command line:

# ./databaseQuery.py database_filename “SQL_query”

A third way is to use unpackDatabase.py under /EBE-Node/EbeCollector/ to
dump the whole database into separated space-separated text files, each
for individual tables. For example, running the following under
/RESULTS/:

$ ../EBE-Node/EbeCollector/unpackDatabase.py ./collected.db .

will generate several “.dat” files, each containing data for the
corresponding type. Each file has a one-line header to indicate what
data each column records, and the rest are data separated by spaces.

For more details about the structure of the database and the uhg.py
script, see /EBE-Node/EbeCollector/EbeCollector_readme.txt.

### f.3 How to tune parameters

After you have familiarized yourself with how to perform multi-job
hybrid calculations, you are finally ready to tune parameters for the
simulations. The most commonly tuned parameters are in the
ParameterDict.py file in the directory, which should be the only file
used to direct the simulations. This file will be copied to the result
folder for record-keeping purposes when generating jobs.
