Children are exposed to speech and other environmental evidence, from which they learn language. How
do they do this? More specifically, how do children map from complex, physical signals to grammars
that enable them to generate and interpret new utterances from their language?
This thesis presents a computational theory of unsupervised language acquisition. By computational
we mean that the theory precisely defines procedures for learning language, procedures that have been
implemented and tested in the form of computer programs. By unsupervised we mean that the theory
explains how language learning can take place with no explicit help from a teacher, but only exposure
to ordinary spoken or written utterances. The theory requires very little of the learning environment.
For example, it predicts that much knowledge of language can be acquired even in situations where the
learner has no access to the meaning of utterances. In this way the theory is extremely conservative,
making few or no assumptions that are not obviously true of the situation children learn in.
The theory is based heavily on concepts borrowed from machine learning and statistical estimation.
In particular, learning takes place by fitting a stochastic, generative model of language to the evidence.
Thus, the goal of the learner is to acquire a grammar under which the evidence is “typical”, in a statistical
sense. Much of the thesis is devoted to explaining conditions that must hold for this learning strategy
to arrive at the desired form of grammar. The thesis introduces a variety of technical innovations,
among them a common representation for evidence and grammars that has many linguistically and
statistically desirable properties. In this representation, both utterances and parameters in the grammar
are represented by composing parameters. A second contribution is a learning strategy that separates
the “content” of linguistic parameters from their representation. Algorithms based on it suffer from few
of the search problems that have plagued other computational approaches to language acquisition.
The theory has been tested on problems of learning lexicons (vocabularies) and stochastic grammars from
unsegmented text and continuous speech signals, and mappings between sound and representations of
meaning. It performs extremely well on various objective criteria, acquiring knowledge that causes it to
assign almost exactly the same linguistic structure to utterances as humans do. This work has application
to data compression, language modeling, speech recognition, machine translation, information retrieval,
and other tasks that rely on either structural or stochastic descriptions of language.
