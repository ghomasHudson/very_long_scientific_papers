###### Contents

-    1 Introduction
-    2 Sparse @xmath Processes — The Simple Model
    -    2.1 Introduction
    -    2.2 Basic equations
    -    2.3 Averaging over atom positions
    -    2.4 The averaged signal and lineshape
        -    2.4.1 The Laplace and Fourier transforms
        -    2.4.2 Inverting the Laplace transform
        -    2.4.3 A numerical method using Eq. ( 2.25 )
        -    2.4.4 Comparison of the three methods
        -    2.4.5 Plots and discussion
    -    2.5 Averaging more complicated interaction potentials
        -    2.5.1 Interaction potential of aligned dipoles
        -    2.5.2 Effect of a different radial dependence on the
            initial rise
    -    2.6 Summary
-    3 Localization in the Simple Model
    -    3.1 Introduction
    -    3.2 The function @xmath
        -    3.2.1 Computation of the Laplace transform of @xmath
        -    3.2.2 Inverting the Laplace transform
        -    3.2.3 A check on the result for @xmath
        -    3.2.4 Plots
    -    3.3 The moments of @xmath
        -    3.3.1 Computation of @xmath
        -    3.3.2 Inverting the Laplace transform
        -    3.3.3 A more convenient form for @xmath
        -    3.3.4 A check on the result for @xmath
        -    3.3.5 Plots
    -    3.4 Summary
-    4 Sparse @xmath Processes with @xmath Interaction
    -    4.1 Introduction
    -    4.2 The Cayley tree approximation
        -    4.2.1 Justification of the Cayley tree approximation
        -    4.2.2 Cayley tree approximation with the @xmath process
            only
        -    4.2.3 Cayley tree approximation with @xmath and @xmath
            processes in the absence of @xmath
        -    4.2.4 Cayley tree approximation with @xmath and @xmath
            processes in the presence of @xmath
    -    4.3 The Hubbard approximation
        -    4.3.1 Computation of @xmath
        -    4.3.2 Computation of @xmath
        -    4.3.3 Computation of @xmath with the @xmath process only at
            resonance
    -    4.4 The coherent potential approximation
        -    4.4.1 Computation of @xmath
        -    4.4.2 Computation of @xmath
    -    4.5 Comparison of the Cayley, Hubbard, and CPA approximations
    -    4.6 Plots
        -    4.6.1 The averaged amplitude @xmath as a function of time
        -    4.6.2 The averaged amplitude @xmath as a function of
            frequency
    -    4.7 Summary
-    5 Analogy With Spin Glasses
    -    5.1 Introduction
    -    5.2 The rubidium system as a spin glass
        -    5.2.1 Spin glass Hamiltonian and equations of motion
        -    5.2.2 Reduction to the sparse limit
        -    5.2.3 Another spin glass Hamiltonian for the rubidium
            system
    -    5.3 Spin glass model for the cesium system
    -    5.4 Summary
-    6 The Effect of Spin
    -    6.1 Introduction
    -    6.2 Matrix elements for the @xmath and @xmath processes
        -    6.2.1 The dipole-dipole interaction
        -    6.2.2 Dipolar matrix elements
        -    6.2.3 A change of basis
        -    6.2.4 The @xmath and @xmath matrix elements
    -    6.3 Determining the potential
        -    6.3.1 States and probability amplitudes with spin included
        -    6.3.2 Equations of motion
        -    6.3.3 Dirac matrices
        -    6.3.4 Separability
        -    6.3.5 Averaging the @xmath process when @xmath for the
            @xmath atom
    -    6.4 Summary
-    7 Numerical Simulations
    -    7.1 Introduction
    -    7.2 Review of theoretical model
    -    7.3 Description of method
    -    7.4 The effect of many @xmath atoms
        -    7.4.1 The chemical argument
        -    7.4.2 Extrapolation to many @xmath atoms
    -    7.5 Results of simulations for the signal
    -    7.6 Results of simulations for the width
    -    7.7 Comparison with experimental data
    -    7.8 Summary
-    8 Final Thoughts

###### List of Figures

-    1.1 The @xmath , @xmath , @xmath , and @xmath energy levels of Rb
    as a function of electric field
-    1.2 Illustration of the @xmath and @xmath processes
-    1.3 Experimental signal as a function of time
-    1.4 Experimental lineshape as a function of detuning
-    2.1 Signal as a function of time
-    2.2 Resonance width as a function of time
-    2.3 Saturation lineshape
-    3.1 The quantity @xmath as a function of @xmath for several values
    of @xmath
-    3.2 The moments @xmath for @xmath , @xmath , @xmath , and @xmath
-    3.3 The moments @xmath for @xmath , @xmath , @xmath , and @xmath
-    3.4 The moments @xmath for @xmath , @xmath , @xmath , and @xmath
-    3.5 The localization lengths @xmath for @xmath , @xmath , @xmath ,
    and @xmath
-    3.6 The localization lengths @xmath for @xmath , @xmath , @xmath ,
    and @xmath
-    3.7 The localization lengths @xmath for @xmath , @xmath , @xmath ,
    and @xmath
-    4.1 Averaged signal as a function of time for the Hubbard
    approximation
-    4.2 Real part of @xmath for @xmath
-    4.3 Real part of @xmath for @xmath
-    4.4 Real part of @xmath for @xmath
-    4.5 Real part of @xmath for @xmath
-    4.6 Real part of @xmath
-    4.7 Imaginary part of @xmath
-    4.8 Real part of @xmath for @xmath — comparison with simulation
-    4.9 Real part of @xmath for @xmath — comparison with simulation
-    4.10 Real part of @xmath for @xmath — comparison with simulation
-    4.11 Real part of @xmath for @xmath — comparison with simulation
-    7.1 Comparison of simulation with exact results
-    7.2 Averaged signal as a function of time for @xmath
-    7.3 Averaged signal as a function of time for @xmath
-    7.4 Averaged signal as a function of time for @xmath
-    7.5 Averaged signal as a function of time for @xmath
-    7.6 Width as a function of time for @xmath
-    7.7 Width as a function of @xmath
-    7.8 Experimental signal as a function of time — larger density
-    7.9 Experimental signal as a function of time — smaller density
-    7.10 Experimental width as a function of time

###### List of Tables

-    6.1 Decomposition of @xmath and @xmath states
-    6.2 Matrix elements for the @xmath process ( @xmath )
-    6.3 Matrix elements for the @xmath process ( @xmath )
-    6.4 Matrix elements for the @xmath process
-    6.5 The @xmath matrix for the @xmath process @xmath
-    6.6 The @xmath matrix for the @xmath process @xmath
-    6.7 The @xmath matrix for the @xmath process
-    6.8 Projections of @xmath onto the Dirac matrices for the @xmath
    process @xmath
-    6.9 Projections of @xmath onto the Dirac matrices for the @xmath
    process @xmath
-    6.10 Projections of @xmath onto the Dirac matrices for the @xmath
    process

## Chapter 1 Introduction

Frozen gases are a new and in many ways ideal laboratory to test our
understanding of quantum theory in a complex system. With present
technology one can manipulate and detect electronic processes with an
extraordinary selectivity and precision. Furthermore, the translational
temperature of the gas can be lowered to the point where it can be
ignored when discussing electronic processes. Frozen Rydberg gases have
the added advantages that their states are well understood and many
processes occur on a microsecond time scale, easily allowing for
time-resolved spectroscopy.

Pioneering experiments on resonant processes in these gases have been
carried out by Anderson et al. [ 5 , 6 ] and Mourachko et al. [ 45 , 44
] . The present work was motivated by the desire to understand those
experiments and the subsequent work of Lowell et al. [ 40 ] highlighting
the dynamic aspects of these resonant, many particle systems. The
measurements of Anderson et al. and Lowell et al. are performed on
mixtures of cold (about @xmath @xmath K) @xmath atoms initially prepared
in the @xmath and @xmath states, henceforth to be called the @xmath and
@xmath states respectively. There are initially @xmath atoms in the
@xmath state and @xmath atoms in the @xmath state. A dipole matrix
element @xmath connects the state @xmath to the state @xmath , and
another dipole matrix element @xmath connects the state @xmath to the
state @xmath , where @xmath refers to the @xmath state and @xmath refers
to the @xmath state. Dipole-dipole interactions cause @xmath
interactions to occur, and then @xmath and @xmath flippings become
possible. ¹ ¹ 1 Although the @xmath process is present, it is smaller
than the @xmath process by a factor of @xmath in this case and can
therefore be neglected to a first approximation, except when @xmath .
The number of @xmath states is measured after a given time has elapsed,
up to about 5 @xmath s. As a function of time, the number of @xmath
states rises rapidly at first and then slowly saturates. The energy of a
@xmath pair can be swept through resonance with an @xmath pair, where
@xmath , via the Stark effect by the application of an electric field. ²
² 2 For a discussion of the Stark effect in hydrogenic atoms, see pages
228–241 of Ref. [ 9 ] or pages 164–171 of Ref. [ 19 ] . At any one time
the resonance lineshape can be obtained as a function of the detuning
@xmath , which is a known function of the applied electric field.

Figure 1.1 shows the @xmath , @xmath , @xmath , and @xmath energies as a
function of the applied electric field. This figure is a slightly
modified version of Fig. 3.1 on page 54 of Ref. [ 40 ] . Because the
@xmath state is in the @xmath state, there are actually two resonances.
One corresponds to the two @xmath states, and the other corresponds to
the two @xmath states. To a first approximation the two resonances can
be treated separately.

Figure 1.2 is a schematic of the @xmath and @xmath processes. In this
illustration the circles represent atoms in an @xmath state and the
ovals represent atoms in a @xmath state. In addition, the black shapes
represent primed states, while the grey shapes represent unprimed
states. The first panel shows a single atom in the @xmath state
surrounded by atoms in the @xmath state. In going from the first to the
second panel an @xmath pair has made the @xmath transition via an
interaction potential @xmath . In the third panel an @xmath pair has
made the @xmath transition via an interaction potential @xmath . In the
final panel the @xmath pair has made the @xmath transition, and the
system has returned to the initial state of the first panel.

Several features of the experimental data present a challenge to
theorists and are investigated in this thesis. The first feature we wish
to understand is the rapid rise followed by slow approach to saturation
of the signal. This behavior can be seen in Fig. 1.3 , which is a plot
of the experimental data taken from Fig. 3.8 on page 71 of Ref. [ 40 ]
with all the data points shown. Each individual interaction between
pairs of atoms leads to a coherent oscillatory behavior, but as we will
see in Chapter 2 , averaging the @xmath interaction over the random
positions of the atoms greatly smooths out the signal. The effective
incoherence brought about by the @xmath process (the “walking away”
discussed by Mourachko et al. [ 45 , 44 ] ) completes the smoothing out
of the on-resonance signal but has less effect on the off-resonance
signal.

We also want to understand the width of the lineshape in the detuning
@xmath as a function of time. Fig. 1.4 shows a plot of the experimental
data for the lineshape as a function of detuning for an interaction time
of @xmath @xmath s. These data are taken from Fig. 3.9 on page 74 of
Ref. [ 40 ] . We note the presence of two resonances, which as was
stated before correspond to the @xmath and @xmath states of the @xmath
atom. The behavior of the collisional resonance width as a function of
temperature for hot (i.e., roughly room temperature) Rydberg gases is
known to be

  -- -------- -- -------
     @xmath      (1.1)
  -- -------- -- -------

as can be found in Ref. [ 21 ] or on page 293 of Ref. [ 22 ] . This
result agrees quite well with experiments on such gases, ³ ³ 3 See, for
example, pages 309–310 of Ref. [ 22 ] . and is obtained by considering
only two-body collisions. Simply plugging the temperature @xmath @xmath
K into this result gives a width that is smaller than the room
temperature width by a factor of about @xmath . The experimental
results, however, show a reduction in the width of only a factor of ten.
Thus we can conclude that the resonance effects of the frozen gas are
not determined by two-body interactions alone.

It was surmised by Anderson et al. [ 6 ] that the linewidth must be of
the order of the average interaction energy. This interaction splits the
@xmath – @xmath degeneracy and the subsequent migration of the @xmath
state to other atoms broadens the energy of this “elementary excitation”
into a band, as appropriate to an amorphous solid. Cluster calculations
were performed to illustrate this band formation [ 40 ] , but averaging
over atom positions was done by order-of-magnitude arguments only.
Mourachko et al. [ 45 , 44 ] pointed out, in connection with their
experiments on a different system, that the “walking away” of the @xmath
excitation from its original location should be regarded as a diffusion
process, and dynamical equations for the resonance transition in the
presence of this diffusion were written down [ 3 , 44 ] .

The present work builds on these earlier insights, but goes considerably
further in the process of accurately averaging over atomic positions. It
turns out that this averaging (effectively, a phase averaging) itself
produces an @xmath dependence at large times, characteristic of a
diffusive process [ 18 ] . We discuss in detail in Chapter 2 a
particular case that cleanly shows the effects of randomness and phase
averaging. In this simple case, the averaging process can be carried out
exactly. For the practically important @xmath interaction potential, one
can proceed in a mathematically elegant way and obtain closed formulas
that can be evaluated by a computer package such as Mathematica , or in
some cases by hand. It appears to be a fortunate coincidence that the
@xmath potential accurately describes the dipole-dipole interactions
that are so prevalent in nature. Some aspects of this approach are
easily extended to more general interactions; however, the @xmath
dependence has special properties which, when coupled with the
assumption of a random distribution of atoms, lead to relatively simple
results.

Although this work was stimulated by experiments on Rydberg atoms, it is
generally applicable to resonances induced by dipole-dipole
interactions. These may be common in highly excited gases and in
molecular systems. Frozen, resonant gases may even exist in interstellar
clouds.

First, in Chapter 2 , we discuss a single @xmath atom interacting
resonantly with a surrounding gas of @xmath atoms, without any
possibility for the @xmath atom to interact with @xmath atoms through an
@xmath process. When the result is averaged over the positions of the
@xmath atoms, it corresponds to a “sparse” system of @xmath atoms,
i.e. a system where @xmath . We do not know of an existing experiment to
which this treatment applies, so this section can be viewed as a
theoretical prediction of the outcome of a possible experiment. Our
purpose here is also one of exposition, since this example allows us to
introduce in the simplest context some of the mathematical techniques
used throughout the rest of this thesis. Some plots of the results are
presented, and in the final sections of this chapter we consider the
effect of interaction potentials more complicated than just a simple
@xmath potential.

In Chapter 3 we continue with our discussion of the simple model of
Chapter 2 by examining the issue of localization. We study localization
in this system because we need to have some idea of the relative
importance of close pairs of atoms, as well as some idea of the
effective range of the @xmath interaction. Naively, it seems that the
close pairs are dominant and that the range of the @xmath potential is
infinite because @xmath diverges at @xmath and yet the integral @xmath
over all space diverges at both limits.

In Chapter 4 we come one step closer to the actual system used in the
experiments by allowing the @xmath process to take place. This renders
the problem analytically insoluble but we consider several
approximations, each of which allows us to make some progress. When
compared with the results of numerical simulations, we see that the
coherent potential approximation compares most favorably.

In Chapter 5 , we introduce an effective spin Hamiltonian which fully
models the Rb system of Anderson et al. [ 5 , 6 ] and Lowell et al. [ 40
] for all values of @xmath and @xmath , as well as for any strength of
the resonant process @xmath and of the mixing processes @xmath and
@xmath . This Hamiltonian corresponds to a system of two
interpenetrating spin glasses. We show its equivalence to the results of
Chapter 4 in the sparse limit @xmath . We then derive an entirely
equivalent but seemingly more cumbersome Hamiltonian to describe this
system. This second derivation embodies a more general technique, which
we demonstrate by constructing a spin Hamiltonian that describes the
cesium system studied by Mourachko et al. [ 45 , 44 ] .

In Chapter 6 we take into account fully the complication introduced by
the consideration of spin, and we determine exactly what the
spin-dependent interatomic potentials are for the @xmath and @xmath
processes in the experiment we are modeling [ 6 , 40 ] .

In Chapter 7 we discuss numerical simulations of the system we are
considering. We start by describing in detail how the simulations are
carried out, then we present the results, including comparisons of the
simulations with the experimental data.

The limitations of the present theory and some of the many possible
extensions are discussed in Chapter 8 .

## Chapter 2 Sparse @xmath Processes — The Simple Model

### 2.1 Introduction

In this chapter we discuss a single @xmath atom interacting resonantly
with a surrounding gas of @xmath atoms, without any possibility for the
@xmath atom to interact with @xmath atoms through an @xmath process. In
Section 2.2 we write down and solve the Schrödinger equation for a given
spatial configuration of the @xmath atoms. Then in Section 2.3 this
result is averaged over the positions of the @xmath atoms, so that it
corresponds to a sparse system of @xmath atoms where @xmath . In Section
2.4 we compute the averaged signal and the lineshape. Specifically, in
Sections 2.4.1 and 2.4.2 we start from the Laplace transform of the
signal and develop series expansions for the signal as a function of
time. We also discuss the use of the fast Fourier transform to obtain
the signal as a function of time. In Section 2.4.3 we develop a
numerical method to compute the averaged signal and lineshape that works
exceedingly well, and in Section 2.4.4 we discuss the good and bad
points of the three methods. Next, in Section 2.4.5 we present plots of
the averaged signal and lineshape. In Section 2.5 we consider the effect
of interaction potentials more complicated than just a simple @xmath
potential. Finally, we summarize the results of this chapter in Section
2.6 .

### 2.2 Basic equations

We consider one atom at the origin, initially in the state @xmath , in
interaction with a gas of @xmath randomly distributed @xmath atoms
through an @xmath process. Like Mourachko et al. [ 45 ] , we describe
the system by the equations

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (2.1a)
     @xmath   @xmath   @xmath      (2.1b)
  -- -------- -------- -------- -- --------

Here @xmath is the amplitude of the state in which the atom at the
origin is in state @xmath and all other atoms are in state @xmath ,
while @xmath (with @xmath running from @xmath to @xmath ) is the
amplitude of the state in which the atom at the origin is in state
@xmath and the atom at @xmath is in state @xmath , while all the others
remain in state @xmath . The quantity @xmath is the interaction
potential and @xmath is the detuning from resonance. For simplicity, we
will assume for now that @xmath is of the dipole-dipole form

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

and we will discuss later in Section 2.5 what happens when the
interaction potential has a different dependence on @xmath or contains
an angular dependence. Furthermore, the atoms are sufficiently cold that
during the time scale of interest they move only a very small fraction
of their separation, and therefore @xmath can be taken to be independent
of time. As the temperature increases one approaches the opposite limit,
where binary collisions control the resonant process [ 57 ] .

The set of differential equations ( 2.1a ) and ( 2.1b ) can easily be
solved with the initial condition @xmath and @xmath for all @xmath ,
yielding

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

with

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

Eq. ( 2.1b ) then gives

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

The experiment detects transitions to any one of the @xmath states (or,
equivalently, to any one of the @xmath states), and is thus a measure of

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (2.6)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

We see that the signal @xmath exhibits Rabi oscillations.

We apply this result to a system of @xmath atoms, initially in state
@xmath , randomly dispersed among a much larger number, @xmath , of
@xmath atoms. In this sparse limit, the experimental signal is
proportional to @xmath times the sample average of @xmath , and the
sample average is equivalent to an ensemble average over the atomic
positions @xmath that are hidden in @xmath (see Eq. ( 2.8 ) below). In
general, averaging is more easily done on the Laplace transform of the
signal, which in this case is

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (2.7)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

This Laplace transform can also be found as Eq. (5.9) on page 28 of Ref.
[ 51 ] .

### 2.3 Averaging over atom positions

To compute ensemble averages we use the following result, valid in the
limit @xmath , for a random distribution of the variables @xmath : ¹ ¹ 1
The procedure here is, in effect, that used in the derivation of the
second virial coefficient. See, for example, page 421 of Ref. [ 49 ] .

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (2.8)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

where @xmath is the volume of the gas in the trap and

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

In particular, for the dipolar interaction @xmath , we have

  -- -------- -- --------
     @xmath      (2.10)
  -- -------- -- --------

leading to

  -- -------- -- --------
     @xmath      (2.11)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (2.12)
  -- -------- -- --------

For the typical densities in the experiments of Anderson et al. [ 6 ]
and Lowell et al. [ 40 ] , @xmath is on the order of MHz. Since @xmath
has the units of an energy, it must be of order @xmath on dimensional
grounds. It is still remarkable, however, that Eq. ( 2.12 ) gives simply
and exactly the quantity @xmath that will enter in all the averaged
quantities in this section. ² ² 2 In this section we perform an average
over many configurations of randomly positioned atoms. The corresponding
dipolar sums for regular lattices are discussed in Refs. [ 13 ] and [ 20
] .

Using Eq. ( 2.11 ), we can evaluate the average of any function @xmath .
We simply need to determine the probability distribution of the variable
@xmath , which we will denote as @xmath . By definition, we have

  -- -------- -- --------
     @xmath      (2.13)
  -- -------- -- --------

We proceed by writing the delta function as a Fourier transform over the
dummy variable @xmath . This leads to

  -- -------- -- --------
     @xmath      (2.14)
  -- -------- -- --------

The average can now be performed using Eq. ( 2.11 ), and we have

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

The remaining integral requires a little finesse to evaluate. First, we
split the integral into two parts, so that

  -- -------- -- --------
     @xmath      (2.16)
  -- -------- -- --------

and

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.17)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Now we note that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.18)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Changing variables to @xmath , we see that @xmath and hence

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.19)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

This can be rewritten as

  -- -------- -- --------
     @xmath      (2.20)
  -- -------- -- --------

and the remaining integral is easily looked up in tables, such as Ref. [
25 ] . One finds that

  -- -------- -- --------
     @xmath      (2.21)
  -- -------- -- --------

and hence we have

  -- -------- -- --------
     @xmath      (2.22)
  -- -------- -- --------

When the derivative is evaluated we are left with

  -- -------- -- --------
     @xmath      (2.23)
  -- -------- -- --------

It is interesting to note that @xmath is not Gaussian, and the central
limit theorem does not apply, because @xmath does not have a root mean
square value. In fact, @xmath is not even normalizable.

Therefore, using Eq. ( 2.23 ) we can compute the average of any function
@xmath as

  -- -------- -- --------
     @xmath      (2.24)
  -- -------- -- --------

We can further define @xmath , and then by a simple change of variable
the average of @xmath can be written in the form

  -- -------- -- --------
     @xmath      (2.25)
  -- -------- -- --------

which agrees with the result derived by an alternative method in Ref. [
18 ] .

This last set of relations implies that we can average a function @xmath
over the positions of the interacting dipoles (atoms in our case) by
replacing @xmath with @xmath and integrating over the kernel @xmath .
This trick is not always useful as such, because it can lead to highly
oscillatory integrals. However, if the function @xmath has suitable
analytic properties, then the integration path of Eq. ( 2.25 ) can be
shifted in the complex @xmath plane in such a way that the oscillations
are damped out and the integral is rather easy to evaluate numerically.
An example of this procedure is discussed in detail in Section 2.4.3 .

### 2.4 The averaged signal and lineshape

The average over atomic positions can be obtained in closed form for the
Laplace transform of the signal, @xmath . Starting from Eq. ( 2.7 ) in
the form

  -- -------- -- --------
     @xmath      (2.26)
  -- -------- -- --------

and using Eq. ( 2.11 ), we obtain

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.27)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where @xmath . For the sake of completeness, and because it will be used
later on, we note that a similar result can be obtained for @xmath by
applying the same method to Eq. ( 2.5 ). One finds

  -- -- -- --------
           (2.28)
  -- -- -- --------

The problem is then to invert the Laplace transform of Eq. ( 2.27 ). As
is shown in Section 2.4.1 , it is possible in this case to convert the
Laplace transform to a Fourier transform, which can then be inverted
numerically by a fast Fourier transform algorithm. A more convenient
inversion method was presented in Ref. [ 18 ] and is described in more
detail in Section 2.4.2 . It leads to an expression for @xmath in terms
of generalized hypergeometric functions, which can be conveniently
handled by Mathematica .

A third method to obtain @xmath is to apply Eq. ( 2.25 ) directly to
@xmath as given by Eq. ( 2.6 ). The remaining integral is difficult to
perform numerically, as we already noted, but this difficulty can be
circumvented as is shown in Section 2.4.3 . The advantages and
limitations of the three numerical methods for evaluating @xmath are
compared in Section 2.4.4 .

Plots of @xmath are presented and discussed in Section 2.4.5 . Readers
who are not interested in mathematical methods can proceed directly to
Section 2.4.5 .

#### 2.4.1 The Laplace and Fourier transforms

We see from Eq. (7.1.6) on page 297 of Ref. [ 2 ] that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.29)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Now we use the fact that

  -- -------- -- --------
     @xmath      (2.30)
  -- -------- -- --------

and we find

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.31)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Therefore Eq. ( 2.27 ) gives

  -- -- -- --------
           (2.32)
  -- -- -- --------

This equation shows that @xmath can be analytically continued to complex
@xmath with @xmath , although it has a branch cut ending at essential
singularities located at @xmath and @xmath . In particular, the Fourier
transform is obtained by putting @xmath . We have obtained @xmath by
inverting this Fourier transform numerically using a fast Fourier
transform routine. However, the following method of inverting the
Laplace transform is much more convenient, for reasons that are
explained in Section 2.4.4 .

#### 2.4.2 Inverting the Laplace transform

According to Eq. (2.66) of Ref. [ 51 ] , the inverse Laplace transform
of

  -- -------- -- --------
     @xmath      (2.33)
  -- -------- -- --------

is

  -- -------- -- --------
     @xmath      (2.34)
  -- -------- -- --------

where the function @xmath is a generalized hypergeometric function, and
is defined on pages 750–751 of Ref. [ 62 ] to be

  -- -------- -- --------
     @xmath      (2.35)
  -- -------- -- --------

Hence we find that the inverse Laplace transform of @xmath is

  -- -------- -- --------
     @xmath      (2.36)
  -- -------- -- --------

Expanding @xmath using Eq. ( 2.35 ) yields

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.37)
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

which can also be obtained directly by expanding Eq. ( 2.27 ) in @xmath
and then taking the inverse Laplace transform term by term.

For some purposes, it is convenient to rearrange Eq. ( 2.37 ) in the
form of a series in powers of the detuning squared. To achieve this, we
use the second line of this equation and find

  -- -------- -- --------
     @xmath      (2.38)
  -- -------- -- --------

where

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.39)
                       @xmath   @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

Again using Eq. ( 2.30 ), we see that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.40)
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

Thus the expression for the signal becomes

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.41)
                                @xmath   
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

If we consider the signal at resonance, then the expression in Eq. (
2.41 ) simplifies considerably. We note that only the @xmath term
contributes when @xmath . Thus we have

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.42)
                                @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

We further note that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.43)
                                @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Thus the signal at resonance can be written as

  -- -------- -- --------
     @xmath      (2.44)
  -- -------- -- --------

which agrees perfectly with Eq. (19) of Ref. [ 18 ] .

#### 2.4.3 A numerical method using Eq. (2.25)

We have from Eq. ( 2.25 ) that the averaged signal can be written as

  -- -- -- --------
           (2.45)
  -- -- -- --------

Then, using Eq. ( 2.6 ), we have

  -- -------- -- --------
     @xmath      (2.46)
  -- -------- -- --------

We can rewrite this as

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.47)
                       @xmath   @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

If @xmath , then we have

  -- -------- -- --------
     @xmath      (2.48)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (2.49)
  -- -------- -- --------

We can change the path of integration from the positive real axis to a
path where we start from the origin and proceed along the negative
imaginary axis to the point @xmath , then proceed to the point @xmath
along the line @xmath . Then we have

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.50)
                                @xmath   
  -- -------- -------- -------- -------- --------

Clearly the integral over @xmath is purely real, and so

  -- -------- -- --------
     @xmath      (2.51)
  -- -------- -- --------

Mathematica is quite adept at quickly evaluating this integral for any
value of @xmath .

The case where @xmath is only slightly more complicated. We have from
Eq. ( 2.47 ) that

  -- -------- -- --------
     @xmath      (2.52)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (2.53)
  -- -------- -- --------

The path we used in the resonant case will still work well for small
@xmath , but will fail for @xmath . This is clear because the radicand
@xmath will change sign as @xmath goes from zero to @xmath . By analogy
with the resonant case and through a bit of trial and error, however,
one finds that for @xmath the path where we start from the origin and
proceed along the positive imaginary axis to the point @xmath , then
proceed to the point @xmath along the line @xmath works well. Using this
path we have

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.54)
                                @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

It is again the case that the @xmath integral gives zero, and we have
that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.55)
                                @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

Mathematica is again fairly adept at evaluating this expression
numerically for any values of @xmath and @xmath , although the
computation time increases with increasing @xmath . In practice, we have
used Eq. ( 2.51 ) for @xmath and Eq. ( 2.55 ) for @xmath .

Note that, using this method, it is possible to obtain a plot of the
resonance width versus time by solving numerically for a value of @xmath
such that

  -- -------- -- --------
     @xmath      (2.56)
  -- -------- -- --------

at many values of @xmath .

We can also use this method to obtain asymptotic expressions for the
signal @xmath . Consider the resonant case. We see from Eqs. ( 2.48 )
and ( 2.49 ) that we want to obtain an asymptotic expression for the
integral

  -- -------- -- --------
     @xmath      (2.57)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (2.58)
  -- -------- -- --------

We can simply apply the method of steepest descent to this integral, as
discussed on pages 82–90 of Ref. [ 43 ] . We see that

  -- -------- -- --------
     @xmath      (2.59)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (2.60)
  -- -------- -- --------

Thus we find that @xmath vanishes when @xmath . To ensure convergence,
we want to pick the root closest to the original integration path in the
lower half plane, and so we choose the root

  -- -------- -- --------
     @xmath      (2.61)
  -- -------- -- --------

We then have

  -- -------- -- --------
     @xmath      (2.62)
  -- -------- -- --------

Proceeding carefully we find that

  -- -------- -- --------
     @xmath      (2.63)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (2.64)
  -- -------- -- --------

Finally there is only a Gaussian integral to do, and one easily finds
that

  -- -------- -- --------
     @xmath      (2.65)
  -- -------- -- --------

for large @xmath .

The same procedure can be applied when @xmath , although this situation
is somewhat more complicated because in order to determine the position
of the saddle point one must first solve a cubic equation. This is best
done numerically, and one finds that the result is qualitatively the
same as Eq. ( 2.65 ).

#### 2.4.4 Comparison of the three methods

Each of the three methods we have discussed has its drawbacks.

In order for the fast Fourier transform method of Section 2.4.1 to be
practical, we must first subtract off certain parts of the Fourier
transform that are not well behaved. For instance, we must subtract off
the part of the Fourier transform that goes as @xmath at small @xmath .
Because we must sample points over a wide range of values of @xmath in
Fourier space in order to obtain an accurate picture of the signal in
@xmath space, however, we must also ensure that the Fourier transform
falls off suitably fast as @xmath increases. We actually subtract off
the quantity

  -- -------- -- --------
     @xmath      (2.66)
  -- -------- -- --------

so that the coefficient of @xmath at large @xmath also vanishes. The
parts that are subtracted off must have a known inverse transform, which
at the end is added to the remaining part that is transformed by a fast
Fourier transform algorithm. Finding a suitable subtraction is a rather
cumbersome process. In addition, because the fast Fourier transform
method is a numerical method (as opposed to an analytical method), we do
not obtain an explicit form for @xmath .

The series obtained using the method of Section 2.4.2 , on the other
hand, do give an explicit form for @xmath . Unfortunately, however, they
have the very serious limitation that they do not always converge for
larger values of @xmath unless @xmath is small.

The method discussed in Section 2.4.3 shares a drawback with the fast
Fourier transform method, namely that unless the integral can be done
analytically, an explicit form for @xmath cannot be obtained. This
method is, however, superior to the fast Fourier transform method
because it does not require the cumbersome subtractions.

As a general rule, the series method of Section 2.4.2 is best when it
converges, and the integral method of Section 2.4.3 is the best
otherwise.

#### 2.4.5 Plots and discussion

Plots of @xmath for several values of @xmath are shown in Fig. 2.1 .
Notice that the initial slope of the signal is independent of @xmath .
This feature translates into resonance widths that vary as @xmath for
small @xmath – the so-called transform broadening discussed by Thomson
et al. [ 60 , 59 ] . With @xmath on the order of MHz, this initial rise
occurs in a fraction of a @xmath s. We will see later that when the
@xmath process is included, the initial rise is independent of both the
detuning @xmath and the strength of the @xmath interaction. Another
point of interest is that the averaging is much more effective at
smoothing out the oscillations for @xmath than it is for @xmath .

We can construct a measure of the width from the first two terms in the
@xmath -expansion, Eq. ( 2.41 ). If

  -- -------- -- --------
     @xmath      (2.67)
  -- -------- -- --------

and we define

  -- -------- -- --------
     @xmath      (2.68)
  -- -------- -- --------

then @xmath would be the FWHM if the lineshape were Lorentzian. We note
that the functions @xmath and @xmath are easily obtained in closed form,
since they are essentially just the @xmath and @xmath terms in the sum
of Eq. ( 2.41 ), respectively. The quantity @xmath is plotted in Fig.
2.2 as the dashed curve. For small @xmath , we have explicitly @xmath .
While the @xmath behavior follows from the transform broadening
argument, or even simply from dimensional analysis, the coefficient
@xmath is a prediction of the detailed theory.

The solid curve in Fig. 2.2 is the exact half width, as determined using
Eq. ( 2.56 ) of Section 2.4.3 . We see that the width of Eq. ( 2.68 ) is
a good approximation to the exact result only at small times.

The lineshape at saturation ( @xmath ) can be obtained directly from the
first line of Eq. ( 2.47 ). For large times the cosine term averages to
zero, giving

  -- -------- -- --------
     @xmath      (2.69)
  -- -------- -- --------

which can also be extracted from the small @xmath behavior of Eq. ( 2.27
). This lineshape is plotted in Fig. 2.3 . The FWHM is approximately
@xmath . Also plotted is a Lorentzian lineshape with the same height and
FWHM. Notice that this lineshape is sharper than the Lorentzian for
small @xmath and falls off more slowly for large @xmath .

### 2.5 Averaging more complicated interaction potentials

In Section 2.3 we averaged the quantity @xmath , where @xmath and @xmath
. In this section we consider the problem of averaging a more
complicated interaction potential. Here we perform the averaging
procedure for a potential that varies as

  -- -------- -- --------
     @xmath      (2.70)
  -- -------- -- --------

where @xmath is a constant and @xmath , @xmath , and @xmath are the
spherical coordinates corresponding to @xmath . We have seen that we can
evaluate the average of the signal if we can compute the average of the
quantity @xmath . A careful look at Eq. ( 2.8 ) shows that the steps do
not make any assumptions on how @xmath depends on @xmath , so this
equation still holds and we have

  -- -------- -- --------
     @xmath      (2.71)
  -- -------- -- --------

Thus we are left with the task of evaluating the integral

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

We can define the variable @xmath , and then

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.73)
                       @xmath   @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

The remaining integrals over @xmath and the angular variables can now be
looked up in tables or done numerically.

It is very important to note that the introduction of an angular
dependence into the potential does not affect the averaging process
except to change the definition of the scaling parameter @xmath by a
constant factor. This means that we can, for simplicity, use a simple
potential that contains no angular dependence to get an answer, and then
take into account any angular dependence by simply changing the
definition of @xmath in the formulas.

#### 2.5.1 Interaction potential of aligned dipoles

The induced dipoles in the magneto-optical trap become aligned when the
applied electric field is sufficiently large that the Stark effect has
passed from the quadratic to the linear regime. The aligned dipole case
is the one considered in Refs. [ 32 ] and [ 54 ] , for example. It is
also the potential we use in performing the numerical simulations
discussed in Chapter 7 , for reasons that are explained fully in Section
7.2 . Specifically, we have

  -- -------- -- --------
     @xmath      (2.74)
  -- -------- -- --------

Thus we need to evaluate the integral

  -- -- -- -------- -------- --------
           @xmath            (2.75)
           @xmath   @xmath   
           @xmath   @xmath   
  -- -- -- -------- -------- --------

We see that the additional angular dependence changes @xmath by a factor
of @xmath .

#### 2.5.2 Effect of a different radial dependence on the initial rise

In this section, we want to determine the effect of a different radial
dependence in the interaction potential on the initial rise. We saw that
if the potential goes as @xmath , then the signal starts linearly with
@xmath , with a slope that is independent of the detuning @xmath . We
will see that if the interaction potential goes as @xmath , then the
initial rise will go as @xmath , with a coefficient that is again
independent of the detuning.

Consider again Eq. ( 2.26 ) in the form

  -- -------- -- --------
     @xmath      (2.76)
  -- -------- -- --------

We noted previously that any angular dependence in the interaction
potential will only change the scale factor @xmath , and will not change
the functional form of the signal. Therefore we can simply consider a
potential of the form

  -- -------- -- --------
     @xmath      (2.77)
  -- -------- -- --------

Of course, we can incorporate the effect of an angular contribution by
simply rescaling @xmath .

It follows, then, from Eqs. ( 2.71 ) and ( 2.73 ) that

  -- -------- -- --------
     @xmath      (2.78)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (2.79)
  -- -------- -- --------

If we define @xmath , then

  -- -------- -- --------
     @xmath      (2.80)
  -- -------- -- --------

We want the behavior at small times, which corresponds to large @xmath .
Therefore we can replace @xmath with @xmath and expand the second
exponential, finding

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.81)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

It immediately follows that

  -- -------- -- --------
     @xmath      (2.82)
  -- -------- -- --------

Thus we see that if the interaction potential goes as @xmath , then the
initial rise is independent of the detuning, @xmath , and is
proportional to @xmath . As a check, we note that if @xmath , then we
have

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.83)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.84)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

and so

  -- -------- -- --------
     @xmath      (2.85)
  -- -------- -- --------

which agrees perfectly with the @xmath , @xmath term of Eq. ( 2.37 ).

### 2.6 Summary

In this chapter we discussed the case of a single @xmath atom
interacting resonantly with a surrounding gas of @xmath atoms, without
the possibility of any @xmath – @xmath interaction. The results, when
averaged over the positions of the @xmath atoms, are applicable to a
sparse distribution of @xmath atoms among the @xmath atoms (or vice
versa, with the appropriate changes). This case is important to study
for several reasons. First, exact analytical results can be obtained.
This not only provides an excellent means for testing the validity of
the numerical simulations discussed in Chapter 7 , but it also provides
a simpler arena for the development of the techniques that are used
again and again in the more complicated models of the later chapters.
Two key examples of the latter are the averaging technique of Sections
2.3 and 2.5 , and the writing of sums as generalized hypergeometrics, as
was done in Section 2.4.2 .

The averaging is non-trivial because the interaction potential itself,
being proportional to @xmath , does not have an average value.
Nevertheless, an effective average potential @xmath given by Eq. ( 2.12
) appears naturally in the theory. The basic averaging equation is Eq. (
2.11 ), which enables the computation of averages of functions that
depend on the sum of the squares of the interaction potentials in terms
of @xmath . This is equivalent to the distribution of Eq. ( 2.23 ) for
the quantity @xmath , where @xmath is the dipole-dipole interaction
potential defined by Eq. ( 2.2 ). The initial rate of conversion from
@xmath to @xmath is given by Eq. ( 2.37 ) or Eq. ( 2.85 ) to be @xmath ,
and is independent of the detuning @xmath . As we will see later, this
initial rate remains the same even when the @xmath – @xmath interaction
is switched on.

## Chapter 3 Localization in the Simple Model

### 3.1 Introduction

In this chapter we continue with our discussion of the simple model of
Chapter 2 by examining the issue of localization. We want to determine
the effective range, or localization length, of the disturbance caused
by the presence of an @xmath atom in the frozen gas of @xmath atoms.
Since the @xmath interaction does not have a length scale, it follows
that the localization length must be of the form @xmath , where @xmath
is the density of @xmath atoms, @xmath is the time, @xmath is the
detuning from resonance, and @xmath is defined in Eq. ( 2.12 ), or more
generally in Eq. ( 2.73 ). The task, then, is to give a precise
definition of the localization length and to compute the function @xmath
.

First, in Section 3.2 we compute a distribution function @xmath , which
describes the contribution to the @xmath process due to @xmath atoms at
position @xmath when the @xmath atom is at the origin. We begin by
computing the Laplace transform of this function in Section 3.2.1 , then
we invert this Laplace transform in Section 3.2.2 . In Section 3.2.3 we
perform a check on the results of Section 3.2.1 and 3.2.2 by using them
to rederive the result for the signal that was found in Chapter 2 . In
Section 3.2.4 we present plots of the distribution @xmath .

In Section 3.3 we compute the moments of the distribution @xmath . We
again start by computing the Laplace transform of the moments in Section
3.3.1 , then we invert this Laplace transform in Section 3.3.2 . The
result of Section 3.3.2 is rearranged in Section 3.3.3 to obtain a more
convenient form, and in this section we also define localization lengths
in terms of the moments in the usual way. The end result for the moments
is checked in Section 3.3.4 by seeing that the zeroth moment agrees with
the result for the signal as a function of time that was found in
Chapter 2 . In Section 3.3.5 we present plots of the results.

The summary and conclusions are given in Section 3.4 . It is possible to
go to this section directly and refer back to final formulas and the
plots of Section 3.3.5 , skipping the intervening mathematics.

### 3.2 The function @xmath

In order to study localization in the simple model of Chapter 2 , we
consider the quantity

  -- -- -- -------
           (3.1)
  -- -- -- -------

As is illustrated explicitly below, the computation of the average is
very similar to the computation of the average signal, @xmath , of
Chapter 2 .

#### 3.2.1 Computation of the Laplace transform of @xmath

From Eq. ( 2.3 ) of this thesis or Eq. (3) of Ref. [ 18 ] we have that

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

where @xmath . Therefore we define

  -- -- -- -------
           (3.3)
  -- -- -- -------

Consider for now just the @xmath term in the sum. For convenience, we
will call this term @xmath . We have

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

where I have written @xmath as @xmath to be explicit. The integral over
@xmath is trivial, and we find

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

We observe now that we can write the denominator as an integral over an
exponential and use the same averaging technique as in Chapter 2 . The
fact that the sum over @xmath starts from @xmath does not affect the
validity of Eq. ( 2.11 ), as long as @xmath is large. Thus we have

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (3.6)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

It is clear that it did not matter that we chose the @xmath term from
the sum in Eq. ( 3.3 ). We would have gotten the result of Eq. ( 3.6 )
no matter which term we picked. It follows that all the terms are equal,
and so

  -- -- -- -------
           (3.7)
  -- -- -- -------

The remaining integral over @xmath is familiar from Chapter 2 and can be
looked up in tables (see, for example, Ref. [ 25 ] ) or done by
Mathematica . The result is

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (3.8)
                                @xmath   
                                @xmath   
  -- -------- -------- -------- -------- -------

#### 3.2.2 Inverting the Laplace transform

Now that we have @xmath , the next step is to invert the Laplace
transform. Because Eq. ( 3.8 ) is of a form that is similar to Eq. (
2.27 ), we can again use Eqs. ( 2.31 ), ( 2.33 ), and ( 2.34 ) to
achieve this.

Using Eq. ( 2.31 ), we have

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (3.9)
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
  -- -------- -------- -------- -------- -------

Next, using the fact (from Eq. (2.66) of Ref. [ 51 ] ) that the inverse
Laplace transform of Eq. ( 2.33 ) is Eq. ( 2.34 ), we see that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.10)
                                @xmath   
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

#### 3.2.3 A check on the result for @xmath

It is clear from Eq. ( 3.1 ) that

  -- -- -- --------
           (3.11)
  -- -- -- --------

and since taking the Laplace transform with respect to time and
integrating over the variable @xmath clearly commute, it must also be
the case that

  -- -------- -- --------
     @xmath      (3.12)
  -- -------- -- --------

For the sake of completeness, we now check that this is true. We will
also find this exercise useful in the next section. From Eq. ( 3.7 ) we
have

  -- -- -------- -------- -------- --------
        @xmath   @xmath            (3.13)
                 @xmath   @xmath   
                 @xmath   @xmath   
                          @xmath   
  -- -- -------- -------- -------- --------

We now recall from Eqs. ( 2.10 ) and ( 2.12 ), or more generally from
Section 2.5 of this thesis, that

  -- -------- -- --------
     @xmath      (3.14)
  -- -------- -- --------

It follows from this that

  -- -- -- --------
           (3.15)
  -- -- -- --------

and so

  -- -- -------- -------- -------- --------
        @xmath   @xmath            (3.16)
                 @xmath   @xmath   
  -- -- -------- -------- -------- --------

The integral over @xmath can again be found in tables or done by
Mathematica . The result is

  -- -- -------- -------- -------- --------
        @xmath   @xmath            (3.17)
                 @xmath   @xmath   
  -- -- -------- -------- -------- --------

where we have used Eq. ( 2.27 ).

A similar check for @xmath is carried out in Section 3.3.4 .

#### 3.2.4 Plots

In Fig. 3.1 we see plots of the dimensionless quantity

  -- -------- -- --------
     @xmath      (3.18)
  -- -------- -- --------

as a function of @xmath , for @xmath , @xmath , and @xmath . The
quantity @xmath is most easily computed by multiplying Eq. ( 3.8 ) by
@xmath and then taking the limit @xmath . We see that the peak of the
distribution shifts to the left with increasing @xmath .

### 3.3 The moments of @xmath

In order to compute the moments of @xmath , we first define

  -- -- -- --------
           (3.19)
  -- -- -- --------

so that @xmath is the @xmath th moment of @xmath . We will see that we
can then determine @xmath by applying the inverse Laplace transform
methods we have already refined.

#### 3.3.1 Computation of @xmath

Much of this computation is very similar to that done in Section 3.2.3 .
Using Eq. ( 3.7 ), we have

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.20)
                       @xmath   @xmath   
                       @xmath   @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

We now note that

  -- -------- -- --------
     @xmath      (3.21)
  -- -------- -- --------

Although the first term diverges, it will disappear when we take the
derivative over @xmath because it does not depend on that variable.

We now need to compute the remaining integral over @xmath . I will
compute it here for the case where @xmath . If there is an angular
dependence in the potential, then the computation is somewhat more
complicated because the extra factors of @xmath in the integrand will
bring down extra angular factors when we apply the methods of Section
2.5 . This is one case where we cannot simply rescale @xmath to obtain
the correct result when an angular dependence is introduced into the
potential. With this in mind, we have

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.22)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where @xmath . The remaining integral over @xmath can be found in tables
or done by Mathematica . The result is

  -- -------- -- --------
     @xmath      (3.23)
  -- -------- -- --------

for @xmath . This should suffice, since we will be primarily interested
in @xmath and @xmath .

Thus we have (for @xmath )

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.24)
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

where in the last step we have used the fact that @xmath and Eq. ( 2.12
). For @xmath , we have from Mathematica that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.25)
                                @xmath   
  -- -------- -------- -------- -------- --------

where as in Chapter 2 , @xmath . It follows that for @xmath , we have

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.26)
                                @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

#### 3.3.2 Inverting the Laplace transform

We know from Eq. ( 2.35 ) that

  -- -------- -- --------
     @xmath      (3.27)
  -- -------- -- --------

so

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.28)
                                @xmath   
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

where we have used the facts that @xmath and @xmath .

Now we note that

  -- -------- -- --------
     @xmath      (3.29)
  -- -------- -- --------

can be rewritten as

  -- -------- -- --------
     @xmath      
     @xmath      (3.30)
  -- -------- -- --------

and so Eq. ( 3.29 ) is equivalent to the single sum

  -- -------- -- --------
     @xmath      (3.31)
  -- -------- -- --------

Therefore we have

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.32)
                                @xmath   
  -- -------- -------- -------- -------- --------

Using Eqs. ( 2.33 ) and ( 2.34 ), we see that the inverse Laplace
transform of

  -- -------- -- --------
     @xmath      (3.33)
  -- -------- -- --------

is

  -- -------- -- --------
     @xmath      (3.34)
  -- -------- -- --------

Therefore we have that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.35)
                                @xmath   
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

#### 3.3.3 A more convenient form for @xmath

As we mentioned in Section 2.4.2 , it is sometimes useful to have a
result as a series in powers of the detuning squared. Now we exploit the
method used there to obtain such a form for @xmath . For convenience, we
define

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.36)
                                @xmath   
  -- -------- -------- -------- -------- --------

so that

  -- -------- -- --------
     @xmath      (3.37)
  -- -------- -- --------

We now recall from Eq. ( 2.35 ) that

  -- -------- -- --------
     @xmath      (3.38)
  -- -------- -- --------

and hence

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.39)
                                @xmath   
  -- -------- -------- -------- -------- --------

Recalling Eq. ( 2.30 ), we see that

  -- -------- -- --------
     @xmath      (3.40)
  -- -------- -- --------

and so

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.41)
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

Now we define

  -- -------- -- --------
     @xmath      (3.42)
  -- -------- -- --------

and similarly,

  -- -------- -- --------
     @xmath      (3.43)
  -- -------- -- --------

so that

  -- -------- -- --------
     @xmath      (3.44)
  -- -------- -- --------

We then have (again using Eq. ( 2.35 ))

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.45)
                       @xmath   @xmath   
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

Performing the analogous calculation for @xmath , we see that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.46)
                       @xmath   @xmath   
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

Using Eq. ( 3.44 ), we arrive at

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.47)
                                @xmath   
                                @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

Recalling Eq. ( 3.37 ), we obtain finally

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.48)
                                @xmath   
                                @xmath   
                                @xmath   
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
                                @xmath   
                                @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

It is important to recall that this result is valid for @xmath .

The result of Eq. ( 3.48 ) shows that the @xmath th moment @xmath is
equal to the quantity @xmath multiplied by a dimensionless function.
Since @xmath is proportional to @xmath , as can be seen from Eq. ( 2.12
), it follows that @xmath has the dimensions of length to the @xmath th
power as it should.

To extract a localization length @xmath from the moment @xmath , we must
first normalize the distribution @xmath of Eq. ( 3.10 ). The
localization length corresponding to the @xmath th moment is thus given
by

  -- -------- -- --------
     @xmath      (3.49)
  -- -------- -- --------

For each value of @xmath , this expression gives a somewhat different
measurement of localization. The quantity @xmath is proportional to
@xmath , which according to Eq. ( 2.12 ) is equal to @xmath , or @xmath
. Therefore we see that @xmath has units of length for all values of
@xmath , as one would expect.

#### 3.3.4 A check on the result for @xmath

We see from Eq. ( 3.11 ) that @xmath , and this provides one way to
check Eq. ( 3.48 ). From Eq. ( 3.48 ) we have

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.50)
                                @xmath   
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

which agrees perfectly with Eq. ( 2.41 ).

#### 3.3.5 Plots

Figs. 3.2 – 3.4 give plots of the dimensionless quantities

  -- -------- -- --------
     @xmath      (3.51)
  -- -------- -- --------

as a function of @xmath , for @xmath , @xmath , @xmath , and @xmath and
for @xmath , @xmath , and @xmath , respectively.

In Figs. 3.5 – 3.7 we see plots of the dimensionless quantities

  -- -------- -- --------
     @xmath      (3.52)
  -- -------- -- --------

as a function of @xmath , for @xmath , @xmath , and @xmath ,
respectively. We see that all the plots initially rise rapidly like
@xmath , and then oscillate around a value of order one that depends
rather weakly on the detuning @xmath .

### 3.4 Summary

We study localization in this system because we need to have some idea
of the relative importance of close pairs of atoms, as well as some idea
of the effective range of the @xmath interaction. Naively, it seems that
the close pairs are dominant and that the range of the @xmath potential
is infinite because @xmath diverges at @xmath and yet the integral
@xmath over all space diverges at both limits.

## Chapter 4 Sparse @xmath Processes with @xmath Interaction

### 4.1 Introduction

In this section we complicate the simple model of Chapter 2 by
introducing the @xmath process through the addition of an interaction
potential @xmath . The @xmath process can be viewed as an exciton
propagating through the medium [ 8 , 15 , 35 ] , as was discussed in
Ref. [ 17 ] .

In Section 4.2 we discuss the Cayley tree approximation, starting with
its justification in Section 4.2.1 . In Sections 4.2.2 – 4.2.4 we solve
for the Laplace transform of the probability amplitude in the case where
@xmath and @xmath are both zero, the case where @xmath is nonzero but
@xmath is still zero, and the case where both @xmath and @xmath are
nonzero, respectively.

We consider a Hubbard band approximation in Section 4.3 . In Section
4.3.1 we again solve for the Laplace transform of the probability
amplitude in the case where @xmath and @xmath are both zero and in
Section 4.3.2 we do the same for the case where @xmath is nonzero but
@xmath is still zero. However, we note in this case that the latter
solution is not consistent. In the case of the Hubbard approximation, we
are also able to compute the square of the probability amplitude in the
@xmath case, as is done in Section 4.3.3 .

In Section 4.4 we consider a coherent potential approximation (CPA). In
this approximation we are able to compute the probability amplitude in
all cases. We will see in Section 4.6 that the CPA results agree best
with the results of the numerical simulations discussed in Chapter 7 .

In Section 4.5 we compare the results of these three approximations. We
see that they are in some ways rather similar, but that the differences
are sufficiently large that it is easy to tell them apart, particularly
when the @xmath process is weak. This means that one approximation will
fit the exact results better than the others, and in the case we are
studying it is the CPA. Thus, in Section 4.6 we present plots of the CPA
results in comparison with the results of the numerical simulations
discussed in Chapter 7 .

Finally, in Section 4.7 we summarize these results and conclude.

### 4.2 The Cayley tree approximation

#### 4.2.1 Justification of the Cayley tree approximation

We start with the equations

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (4.1a)
     @xmath   @xmath   @xmath      (4.1b)
  -- -------- -------- -------- -- --------

These are the same as Eqs. ( 2.1a ) and ( 2.1b ), except that we now
allow for the @xmath process by including the @xmath term in the
equation for @xmath . For the purposes of compressing indices on sums
and products, it is convenient to define @xmath to be zero when @xmath .
With this definition, the previous equation becomes

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (4.2a)
     @xmath   @xmath   @xmath      (4.2b)
  -- -------- -------- -------- -- --------

Unfortunately the potential @xmath makes the set of equations insoluble
as they stand, so we cannot obtain an exact solution as we did in
Chapter 2 . However, the equations can be solved numerically, and one
can also make quite a lot of progress by making approximations and
proceeding analytically.

In developing approximations, it is useful to cast the problem in the
standard Green’s function language [ 16 ] , as discussed in Section 3 of
Ref. [ 11 ] . The Green’s function of Eq. ( 4.2b ) satisfies the
equation

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

and then

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

Actually, @xmath is the @xmath element of the Green’s function for the
entire set of @xmath equations.

We can derive a convenient form for the on-site Green’s function @xmath
. From Eq. ( 4.3 ) we have, for @xmath ,

  -- -------- -- -------
     @xmath      (4.5)
  -- -------- -- -------

and if we write separately the term containing @xmath , we obtain

  -- -------- -- -------
     @xmath      (4.6)
  -- -------- -- -------

for @xmath . We note here that Eqs. ( 4.5 ) and ( 4.6 ) are the analogs
of Eqs. ( 4.2a ) and ( 4.2b ), respectively.

Now we imagine removing the @xmath th atom. The new Green’s function for
the remaining @xmath atoms, which we will call @xmath , obeys the
equation

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

where @xmath and @xmath are never equal to @xmath . If we multiply both
sides of this equation by @xmath and sum over @xmath , we obtain

  -- -------- -- -------
     @xmath      (4.8)
  -- -------- -- -------

Comparing this equation with Eq. ( 4.6 ), we see that for @xmath

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

If we insert this result into Eq. ( 4.5 ), then we have finally that

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

and hence

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

It is important to keep in mind that these Green’s function equations
are exact.

The Cayley approximation [ 1 , 37 , 38 , 39 ] keeps only the diagonal
@xmath terms in the sum over @xmath and @xmath in Eqs. ( 4.4 ) and (
4.11 ). It further assumes that each @xmath is independently
distributed, and that there are sufficiently many sites that @xmath and
@xmath have the same distribution.

We choose to set @xmath and work in Laplace space instead of Fourier
space because this makes many of the integrals that follow more
obviously convergent. Up to a point, the same analysis can be carried
out with the Fourier analogs, but Laplace transforms are needed to solve
the Cayley equations exactly in Section 4.2.2 . We also work with the
inverses of these Green’s functions, setting @xmath and @xmath , so in
the Cayley approximation we have from Eqs. ( 4.7 ) and ( 4.8 ),
respectively

  -- -------- -------- -------- -- ---------
     @xmath   @xmath   @xmath      (4.12a)
     @xmath   @xmath   @xmath      (4.12b)
  -- -------- -------- -------- -- ---------

#### 4.2.2 Cayley tree approximation with the @xmath process only

First we consider just the @xmath process and solve Eq. ( 4.12b ), which
is to say we look at the @xmath process by itself. Note that @xmath is
always real and positive. The distribution of @xmath is given by

  -- -------- -- --------
     @xmath      (4.13)
  -- -------- -- --------

where for a uniform gas of volume @xmath ,

  -- -------- -- --------
     @xmath      (4.14)
  -- -------- -- --------

Writing the delta function as a Fourier transform, we see that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.15)
                       @xmath   @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

Using the fact that @xmath , we can write

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.16)
                       @xmath   @xmath   
                       @xmath   @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

where in the last step we have simply changed variables from @xmath to
@xmath . Using the averaging method detailed in Section 2.3 , we see
that ¹ ¹ 1 Of course, we must use the more elaborate averaging method of
Section 2.5 if the potential @xmath is more complicated.

  -- -------- -- --------
     @xmath      (4.17)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (4.18)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (4.19)
  -- -------- -- --------

if @xmath is of the simple @xmath form with no angular dependence, and
is the equivalent of @xmath in Eq. ( 2.73 ) otherwise. For @xmath and
@xmath real, we have that

  -- -------- -- --------
     @xmath      (4.20)
  -- -------- -- --------

where this integral is evaluated by the same procedure used to evaluate
the integral of Eq. ( 2.15 ). ² ² 2 In case the reader is curious, the
theta function appears here but is omitted in Eq. ( 2.23 ) because
@xmath is an intrinsically positive quantity. With this result, we have

  -- -------- -- --------
     @xmath      (4.21)
  -- -------- -- --------

and so

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.22)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

We also have

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.23)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

or

  -- -------- -- --------
     @xmath      (4.24)
  -- -------- -- --------

Thus we can solve numerically the transcendental relation for @xmath
given in Eq. ( 4.22 ) and plug it into Eq. ( 4.24 ) to find @xmath . It
is important to note that Eq. ( 4.22 ) defines @xmath as an analytic
function of @xmath in the half-plane where @xmath . Thus @xmath can be
analytically continued, in the variable @xmath , from the positive
imaginary axis to the entire upper half-plane.

#### 4.2.3 Cayley tree approximation with @xmath and @xmath processes in
the absence of @xmath

Now we allow the @xmath process to be present, but we still ignore the
detuning, @xmath . We start with the equations

  -- -------- -------- -------- -- ---------
     @xmath   @xmath   @xmath      (4.25a)
     @xmath   @xmath   @xmath      (4.25b)
  -- -------- -------- -------- -- ---------

Repeating the analysis we did before, we have

  -- -- -- --------
           (4.26)
  -- -- -- --------

which leads to the form

  -- -------- -- --------
     @xmath      (4.27)
  -- -------- -- --------

At this point it is clear from the work we did to obtain Eq. ( 4.21 )
from ( 4.16 ) that this equation leads to

  -- -------- -- --------
     @xmath      (4.28)
  -- -------- -- --------

where @xmath is still given by Eq. ( 4.22 ), and depends on @xmath .
Therefore we obtain the result

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.29)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

which again can be analytically continued.

#### 4.2.4 Cayley tree approximation with @xmath and @xmath processes in
the presence of @xmath

If we further consider the effect of a detuning, @xmath , then we have
the full set of Eqs. ( 4.12a ) and ( 4.12b ). The equation for @xmath is
unchanged from the previous case, but because the equation for @xmath is
now complex we have to consider a distribution in the two variables
@xmath and @xmath . We have

  -- -------- -- --------
     @xmath      (4.30)
  -- -------- -- --------

Clearly the delta function involving @xmath does not participate in the
averaging or in the integrations over @xmath . Therefore we can treat
the part involving @xmath just as we did before, and the @xmath part
will just be carried along. Hence we find

  -- -------- -- --------
     @xmath      (4.31)
  -- -------- -- --------

It follows, then, that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.32)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

### 4.3 The Hubbard approximation

It is also possible to make approximations other than the one we have
chosen. For example, instead of working with Eq. ( 4.12b ), one can
instead choose to work with

  -- -------- -- --------
     @xmath      (4.33)
  -- -------- -- --------

which gives explicitly

  -- -------- -- --------
     @xmath      (4.34)
  -- -------- -- --------

This approximation corresponds to a Hubbard band.

#### 4.3.1 Computation of @xmath

It is possible to average Eq. ( 4.34 ), although it requires a little
more manipulation than the averaging methods we have applied previously
because of the presence of the square root. We first note that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.35)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where

  -- -------- -- --------
     @xmath      (4.36)
  -- -------- -- --------

Interestingly enough, the Laplace transform in Eq. ( 4.35 ) has a known
inverse. We see from Eq. (2.1.35) of Ref. [ 51 ] that the inverse
Laplace transform of Eq. ( 4.35 ) is

  -- -------- -- --------
     @xmath      (4.37)
  -- -------- -- --------

We will use Eq. ( 4.37 ) in Section 4.3.3 to compute @xmath in the case
where only the @xmath process is present.

We now note that

  -- -------- -- --------
     @xmath      (4.38)
  -- -------- -- --------

so we can rewrite Eq. ( 4.35 ) as

  -- -- -- --------
           (4.39)
  -- -- -- --------

We introduce yet another integral by observing that

  -- -------- -- --------
     @xmath      (4.40)
  -- -------- -- --------

if @xmath , so

  -- -------- -- --------
     @xmath      (4.41)
  -- -------- -- --------

From our previous experience with averaging over @xmath , we know that

  -- -------- -- --------
     @xmath      (4.42)
  -- -------- -- --------

Here we write @xmath in place of @xmath because the averages of all the
@xmath are equal. The integral over @xmath is familiar to us, since it
is the type of integral that shows up repeatedly in this averaging
method. We have

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.43)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

The remaining integral over @xmath can be done by parts, with the result
that

  -- -------- -- --------
     @xmath      (4.44)
  -- -------- -- --------

We can also compute @xmath by first computing the distribution @xmath .
It will turn out that knowing @xmath will aid greatly in the computation
of @xmath for this approximation, as we will see in Section 4.3.2 .
Because in this case @xmath is explicitly given in terms of @xmath
through Eq. ( 4.34 ), we have

  -- -------- -- --------
     @xmath      (4.45)
  -- -------- -- --------

This average is easy to compute using the analog of Eq. ( 2.23 ) for the
potential @xmath . We have

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.46)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where we have used the fact that

  -- -------- -- --------
     @xmath      (4.47)
  -- -------- -- --------

The theta function is present in Eq. ( 4.46 ) because @xmath and @xmath
are positive definite quantities, and hence @xmath cannot be nonzero
unless @xmath .

Using the distribution of Eq. ( 4.46 ) we can compute

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.48)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

This integral is very difficult to compute analytically in its present
form, even by a computer package such as Mathematica . Therefore we
change variables to @xmath . Then @xmath , and because @xmath must be
positive we have @xmath . It follows that

  -- -------- -- --------
     @xmath      (4.49)
  -- -------- -- --------

This integral can now be done by Mathematica , and it gives exactly the
result of Eq. ( 4.44 ).

#### 4.3.2 Computation of @xmath

Going back to Eq. ( 4.12a ), we see that

  -- -------- -- --------
     @xmath      (4.50)
  -- -------- -- --------

Since this is the same equation we considered for the Cayley
approximation in Section 4.2.4 we see that the result for @xmath is very
similar to Eq. ( 4.31 ). Specifically, we have

  -- -------- -- --------
     @xmath      (4.51)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (4.52)
  -- -------- -- --------

It then follows from Eq. ( 4.32 ) that

  -- -- -- --------
           (4.53)
  -- -- -- --------

One might think that because @xmath is known explicitly in this case,
instead of involving transcendental quantities as Eq. ( 4.21 ) does, it
should be possible to compute the function @xmath explicitly as well.
Unfortunately the integral of Eq. ( 4.52 ) cannot be done analytically.
If we make the same change of variable that we did to obtain Eq. ( 4.49
), however, then Eq. ( 4.52 ) becomes

  -- -------- -- --------
     @xmath      (4.54)
  -- -------- -- --------

and this integral can be done numerically for a given value of @xmath .

Thus @xmath can be computed numerically, and the result substituted into
Eq. ( 4.53 ) to obtain @xmath .

Although we have shown how to compute @xmath for the Hubbard model using
Eqs. ( 4.12a ) and ( 4.34 ), we note that these equations are not
consistent. Consistency demands that @xmath reduces to @xmath when
@xmath and @xmath , and this is clearly not the case with Eqs. ( 4.53 )
and ( 4.33 ). In fact, one can see this inconsistency immediately from
Eqs. ( 4.12a ) and ( 4.34 ). If we set @xmath and @xmath in these
equations, then we have

  -- -------- -------- -------- -- ---------
     @xmath   @xmath   @xmath      (4.55a)
     @xmath   @xmath   @xmath      (4.55b)
  -- -------- -------- -------- -- ---------

We see that even in this limit, @xmath and @xmath are still treated
differently in this approximation. Therefore the Hubbard approximation
is inconsistent, and to be safe one should disregard the result for
@xmath and only consider the result for @xmath of Section 4.3.1 as
valid.

#### 4.3.3 Computation of @xmath with the @xmath process only at
resonance

We saw in Eq. ( 4.37 ) that at resonance

  -- -------- -- --------
     @xmath      (4.56)
  -- -------- -- --------

and so

  -- -------- -- --------
     @xmath      (4.57)
  -- -------- -- --------

According to Eq. (10.1.1) of Ref [ 51 ] , this function has the inverse
Laplace transform

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.58)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Now we again apply Eq. ( 4.38 ), with the result that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.59)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

At this point, we apply Eq. ( 4.40 ) to find that

  -- -- -- --------
           (4.60)
  -- -- -- --------

This expression can now be averaged. We find that

  -- -------- -- --------
     @xmath      (4.61)
  -- -------- -- --------

The integration over @xmath can be performed at this point, with the
result that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.62)
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

The @xmath integral can be done by parts. The result is

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.63)
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

The last two terms are standard integrals, and we have that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.64)
                                @xmath   
  -- -------- -------- -------- -------- --------

Now we consider the remaining integral

  -- -------- -- --------
     @xmath      (4.65)
  -- -------- -- --------

First we define @xmath , so that @xmath . Thus @xmath and similarly
@xmath . In addition, since @xmath , it follows that @xmath , and hence
both @xmath and @xmath are positive quantities. With this substitution
we have

  -- -------- -- --------
     @xmath      (4.66)
  -- -------- -- --------

Using Eq. ( 2.31 ), we see that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.67)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

The integral over @xmath is a standard one, and we find that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.68)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Substituting this back into Eq. ( 4.64 ), we see that

  -- -------- -- --------
     @xmath      (4.69)
  -- -------- -- --------

At this point we can invert this Laplace transform term by term, using
the fact that the inverse Laplace transform of @xmath is @xmath . We
finally arrive at

  -- -- -- --------
           (4.70)
  -- -- -- --------

The even terms in this sum reduce to

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.71)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where in the third step we have used Eq. ( 2.30 ) and in the final step
we have used Eq. ( 2.35 ). Similarly, the odd terms in this sum reduce
to

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.72)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

and so we have finally that

  -- -------- -- --------
     @xmath      (4.73)
  -- -------- -- --------

Figure 4.1 shows a plot of this result as a function of time. Looking
back to Fig. 1.3 , we see that this signal saturates too quickly to
provide a good fit to the experimental data. In fact, looking ahead to
Fig. 7.3 , we see that it reaches saturation too quickly to favorably
compare with the numerical simulation results for @xmath at resonance.

### 4.4 The coherent potential approximation

Instead of making the Hubbard substitution of Eq. ( 4.33 ), we can
replace @xmath by its average in Eq. ( 4.12b ), obtaining

  -- -------- -- --------
     @xmath      (4.74)
  -- -------- -- --------

This corresponds to a coherent potential approximation (CPA).

#### 4.4.1 Computation of @xmath

We have from Eq. ( 4.74 ) that

  -- -------- -- --------
     @xmath      (4.75)
  -- -------- -- --------

We again note that @xmath is independent of @xmath , so we drop the
subscript and we see that

  -- -------- -- --------
     @xmath      (4.76)
  -- -------- -- --------

The averaging procedure of Section 2.3 is by now second nature, and we
see that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.77)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

This is an implicit equation for @xmath , which can be rearranged to
give

  -- -------- -- --------
     @xmath      (4.78)
  -- -------- -- --------

Eq. ( 4.78 ) can then be solved numerically for @xmath .

#### 4.4.2 Computation of @xmath

Just as we saw when considering the Hubbard approximation, the
modification of Eq. ( 4.12b ) to Eq. ( 4.74 ) makes the latter equation
inconsistent with Eq. ( 4.12a ). It is possible to use Eq. ( 4.78 )
coupled with Eq. ( 4.12a ) to obtain an expression for @xmath , but it
does not reduce to Eq. ( 4.78 ) when we take the limit @xmath and @xmath
. In this case, however, we can modify Eq. ( 4.12a ) to

  -- -------- -- --------
     @xmath      (4.79)
  -- -------- -- --------

so that it and Eq. ( 4.74 ) are again consistent.

Proceeding as we did in going from Eq. ( 4.76 ) to 4.77 , we have

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.80)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

and hence

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.81)
                                @xmath   
  -- -------- -------- -------- -------- --------

### 4.5 Comparison of the Cayley, Hubbard, and CPA approximations

We have examined these approximations and found that the results that
agree best with the simulations are those obtained using the CPA
approximation. In fact, as we will see in Section 4.6 , the CPA results
agree strikingly well with the simulations. This is somewhat surprising,
since in the Cayley equations ( 4.12a ) and ( 4.12b ) the on-site
Green’s functions for the @xmath problem are allowed to vary from site
to site, whereas in Eqs. ( 4.33 ) and ( 4.74 ) they are not. However,
the Cayley tree approximation is exact in a one-dimensional system with
nearest neighbor interactions and becomes increasingly inaccurate as the
dimensionality of the problem and the range of the interaction increase.
The CPA, on the other hand, is a mean field approximation, and it
becomes accurate for higher dimensional problems and long-range
interactions.

We see in Fig. 4.2 plots of @xmath for @xmath and several values of
@xmath . The Cayley and CPA approximations coincide in this case. The
exact result for @xmath can actually be obtained analytically by the
methods for computing @xmath used in Ref. [ 18 ] and described in detail
in Chapter 2 . We reproduce here the result of Eq. ( 2.28 ), which is

  -- -- -- --------
           (4.82)
  -- -- -- --------

As one would expect, this agrees with Eq. ( 4.32 ) for @xmath and with
Eq. ( 4.81 ) for @xmath .

We note the “gap” that is present in each of the curves, and we also
note that the width of the gap increases with @xmath . This effect can
be seen analytically from Eq. ( 4.82 ). We start by converting from
Laplace to Fourier space by making the substitution @xmath , so that we
have

  -- -------- -- --------
     @xmath      (4.83)
  -- -------- -- --------

Using Eq. ( 2.31 ), we see that

  -- -------- -- --------
     @xmath      (4.84)
  -- -------- -- --------

which reduces to

  -- -------- -- --------
     @xmath      (4.85)
  -- -------- -- --------

if @xmath , and zero otherwise. This explains the gaps seen in the
@xmath and @xmath curves. For @xmath , Eq. ( 4.85 ) becomes

  -- -------- -- --------
     @xmath      (4.86)
  -- -------- -- --------

which produces the quasigap around @xmath seen in the resonance curve.

In Fig. 4.3 , which presents plots of @xmath for @xmath and several
values of @xmath , we begin to see differences between the CPA and
Cayley approximations. We see that the CPA results retain the gap
structure of the @xmath case somewhat, while the only vestiges of the
gap remaining in the Cayley results are the dimple in the @xmath curve
at @xmath and a barely perceptible bend in the @xmath curve at @xmath .
The off-resonant CPA results are also noticeably asymmetric around
@xmath .

In Fig. 4.4 we see plots for @xmath , and many of the trends of Fig. 4.3
continue. The off-resonant CPA results continue to retain some memory of
the gap and are still asymmetric around @xmath . The gap structure is no
longer evident in the on-resonance curve. For @xmath the result for the
Hubbard approximation is also shown.

We see that the CPA and Cayley results look the most similar in Fig. 4.5
, where @xmath . However, upon careful examination one notes that the
CPA results are still slightly asymmetric about @xmath . The peaks are
also slightly wider for the CPA results.

### 4.6 Plots

#### 4.6.1 The averaged amplitude @xmath as a function of time

Figs. 4.6 and 4.7 display the numerical simulations of @xmath and @xmath
, respectively, for @xmath , and @xmath . We recall that @xmath is the
probability amplitude that the @xmath has not occurred during the time
@xmath . The striking feature of these graphs is that, off-resonance,
oscillations with an approximate period @xmath persist for a long time,
not only when the exciton does not propagate @xmath , but even for
@xmath .

The graphs for @xmath can be obtained analytically by the methods for
computing @xmath used in Ref. [ 18 ] and described in detail in Chapter
2 . In fact, we have obtained @xmath by expanding Eq. ( 4.82 ) (or,
equivalently, Eq. ( 2.28 )) in powers of @xmath and inverting the
Laplace transform term by term. As in Chapter 2 , various formulas
involving special functions can also be obtained. The exact analytical
results in this case (not shown here) agree with the numerical
simulations about as well as in Fig. 7.1 . The initial dependence on
time is given by

  -- -------- -- --------
     @xmath      (4.87)
  -- -------- -- --------

Looking at the graphs, it seems that the initial decrease of @xmath
depends on @xmath , but this perception only indicates that the range of
validity of the linear approximation Eq. ( 4.87 ) is small. On the other
hand, the graphs of @xmath as a function of time in Figs. 7.1 – 7.5 look
linear and independent of @xmath over a wide range of @xmath . Note also
that @xmath decreases initially twice as fast as @xmath . Of course, it
must be true that the mod square of the average is smaller than or equal
to the average of the square, but the above result shows that @xmath is
not even a fair approximation to @xmath .

#### 4.6.2 The averaged amplitude @xmath as a function of frequency

Figs. 4.8 – 4.11 present graphs of @xmath that correspond to the graphs
of @xmath in Figs. 4.6 and 4.7 . For @xmath and @xmath , @xmath is the
exciton spectral density (times @xmath ); more generally, it is an
excitation spectral density for the resonant process @xmath . Each
figure gives a comparison of the numerical simulation with the CPA
approximation, which in particular for @xmath reduces correctly to the
real part of the exact result of Eq. ( 4.82 ), with @xmath . We compare
spectral densities, rather than time graphs, in part because obtaining
@xmath for the CPA approximation is laborious, and in part because the
comparison of spectral densities is instructive, as we now discuss.

In the notation of Section 7.3 , the spectral density is obtained
numerically from Eq. ( 7.7 ) as

  -- -------- -- --------
     @xmath      (4.88)
  -- -------- -- --------

in the limits @xmath and @xmath . In practice it is difficult to go
beyond @xmath , and one must choose @xmath to be fairly large to obtain
a smooth plot for the spectral density where the eigenvalues are sparse,
even after averaging over @xmath realizations. On the other hand, the
distribution of eigenvalues for a random dipolar gas is highly
structured near @xmath , and if @xmath is too large this structure will
be missed. Rather than spending much time to go to large @xmath and
refine the numerical technique, we set @xmath , plot the results for
@xmath and @xmath , and let the eye extrapolate to the correct limit. It
is remarkable that the CPA approximation agrees best with the smallest
value of @xmath , except for wiggles that are usually in regions of
sparse eigenvalues. This is exactly how the exact spectral density is
expected to compare with a calculation for finite @xmath . A finite
@xmath can also be viewed as a simulation of the decoherence introduced
by atomic motions, or of losses to other channels, such as the decay of
the excited atomic states due to blackbody radiation as described in
Chapter 5 of Ref. [ 22 ] . Thus, @xmath is almost equivalent to the
@xmath of the Lorentzian broadening model of Ref. [ 18 ] , the
difference being that in Ref. [ 18 ] @xmath is added to @xmath for the
Fourier transform of Eq. ( 2.1b ) only, while here @xmath is added to
every @xmath that appears. As @xmath increases, the highly structured
spectral densities of the ideal frozen gas become increasingly similar
to Lorentzian lines. Because of unavoidable losses and incoherences, the
experimental spectral densities will resemble the results of the
simulations for some finite @xmath .

In Fig. 4.8 , where results are presented for @xmath , the spectral
density is quite different from a Lorentzian. At @xmath there is a
quasi-gap around @xmath . As @xmath increases the gap extends from
@xmath to @xmath , as was described in Section 4.5 .

In Fig. 4.9 are presented results for @xmath . We see that the sharp
features of the @xmath spectrum are rounded off and the gap is beginning
to be filled, but is still recognizable.

Results for @xmath are presented in Fig. 4.10 . We note that the
spectrum at @xmath has the appearance of a single, asymmetric line. At
@xmath and @xmath a shoulder on the left of the line is the remnant of
the gap of Fig. 4.8 .

Fig. 4.11 contains results for @xmath . We see that there is a single
line for any value of @xmath , which becomes increasingly sharp as
@xmath increases. The lineshape is not Lorentzian, but even a small
additional broadening will make it nearly so, as indicated by the
calculation for @xmath . This sharp line leads to the damped sinusoidal
oscillations seen in the @xmath graphs of Figs. 4.6 and 4.7 . The
general phenomenon of line narrowing can be discussed using the simple
Lorentzian model for the exciton band, in which Eq. (1b) of Ref. [ 18 ]
is replaced by

  -- -------- -- --------
     @xmath      (4.89)
  -- -------- -- --------

Solving the equations in this limit, one finds a pole at

  -- -------- -- --------
     @xmath      (4.90)
  -- -------- -- --------

which gives a decreasing width as @xmath increases, since @xmath is
proportional to @xmath . One cannot simply replace @xmath by its
average, which does not even exist. Instead, one can Fourier transform
the average to find the time dependence

  -- -------- -- --------
     @xmath      (4.91)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (4.92)
  -- -------- -- --------

and @xmath has been defined in Eq. ( 2.12 ). A similar argument was
applied in Ref. [ 18 ] to find the time dependence of @xmath .

From the comparison with the simulations, it is apparent that a
shortcoming of the CPA approximation is that it gives a symmetric
spectral density at resonance @xmath , while the correct spectral
density is slightly asymmetric, except for @xmath . Correspondingly,
@xmath is not purely real at resonance, as can be seen in Fig. 4.7 . For
@xmath , the spectral density at @xmath is underestimated by the CPA
approximation.

Clearly, the asymmetry of the spectral density at @xmath comes from the
terms with @xmath in the denominator of Eq. ( 4.4 ), which are neglected
in the CPA approximation. It is surprising that these terms have such
small effect on @xmath at @xmath . At finite @xmath , the underestimate
of @xmath for @xmath is probably due to the neglect of these same terms.
We have already shown how to include these terms in the CPA formulation
[ 11 ] , but in practice, when the simple CPA is not adequate one can
just as well use the numerical simulations.

### 4.7 Summary

In this chapter we have extended the simple model of Chapter 2 by
allowing the @xmath process to take place. Although this complication no
longer allows a direct analytical solution of the problem, we consider
three approximations, each of which allows us to make some progress. We
compare these approximations with the results of numerical simulations,
which are carried out for a finite number of atoms by the methods
described later in Chapter 7 of this thesis.

The results of the Cayley tree approximation are not found to give the
best results for the problem we are considering here, but they are still
extremely worthwhile results. They extend the Cayley tree results of
Ref. [ 1 ] , which are used extensively, for example, in the study of
dipolar liquids [ 12 , 28 , 31 , 30 , 37 , 39 , 58 ] .

The results of the Hubbard approximation are somewhat disappointing,
since in this case we can obtain an explicit expression for the exciton
spectrum @xmath but we are not able to obtain a consistent result when
the effects of the @xmath process are included. However, in this
approximation we can obtain the average of @xmath as a function of time
when the @xmath process is absent, and perhaps in the future this will
lead to a more general technique that will allow this quantity to be
computed in the other approximations as well.

The CPA approximation is found to agree best with the results of the
numerical simulations, and we are able to carry our computations in this
approximation just as far as in the Cayley tree approximation. It seems
at first strange that the CPA approximation gives better results than
the Cayley, since in the Cayley equations ( 4.12a ) and ( 4.12b ) the
on-site Green’s functions for the @xmath problem are allowed to vary
from site to site, whereas in Eqs. ( 4.33 ) and ( 4.74 ) they are not.
However, the Cayley tree approximation is exact in a one-dimensional
system with nearest neighbor interactions and becomes increasingly
inaccurate as the dimensionality of the problem and the range of the
interaction increase. The CPA, on the other hand, is a mean field
approximation, and it becomes accurate for higher dimensional problems
and long-range interactions. It may also happen that, when the
off-diagonal terms of Eqs. ( 4.4 ) and ( 4.11 ) are included, the Cayley
is found to be the better approximation.

## Chapter 5 Analogy With Spin Glasses

### 5.1 Introduction

In this chapter, we examine a correspondence between the atomic system
we are seeking to model and spin glasses. We start in Section 5.2 by
considering the rubidium experiments of Anderson et al. [ 5 , 6 ] and
Lowell et al. [ 40 ] . In Section 5.2.1 we derive in a rather
straightforward manner a Hamiltonian that describes these experiments as
a system of two interpenetrating spin glasses. We also determine the
corresponding equations of motion. In Section 5.2.2 we illustrate how
the results of Section 5.2.1 reduce to those of Chapter 4 in the
appropriate limit. We derive a new, and seemingly more cumbersome
Hamiltonian in Section 5.2.3 that also describes the rubidium
experiments and is entirely equivalent to the Hamiltonian of Section
5.2.1 . The method used to generate the new Hamiltonian is more
generally applicable, however, as we demonstrate in Section 5.3 by using
the new method to construct a spin Hamiltonian that describes the cesium
experiments of Mourachko et al. [ 45 , 44 ] . Finally, we conclude in
Section 5.4 .

### 5.2 The rubidium system as a spin glass

#### 5.2.1 Spin glass Hamiltonian and equations of motion

We consider @xmath atoms at positions @xmath which are initially in
state @xmath , and @xmath atoms at positions @xmath which are initially
in state @xmath . We represent the @xmath and @xmath states at @xmath
with the down and up states of an effective spin @xmath , respectively,
and similarly represent the @xmath and @xmath states at @xmath with a
spin @xmath . The Hamiltonian that describes this system with the @xmath
and @xmath processes mediated by dipole-dipole interaction potentials
@xmath and @xmath , respectively, is then

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (5.1)
                                @xmath   
  -- -------- -------- -------- -------- -------

where

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

and

  -- -------- -- -------
     @xmath      (5.3)
  -- -------- -- -------

if @xmath and @xmath otherwise. ¹ ¹ 1 Of course, the @xmath potentials
of Eqs. ( 5.2 ) and ( 5.3 ) are just the simplest examples of
dipole-dipole interaction potentials. We are free to choose more
complicated potentials if we so desire. This is precisely the
Hamiltonian for two interpenetrating spin glasses that interact via
dipole-dipole potentials.

For the experiments under consideration @xmath , and the @xmath coupling
@xmath , which leads to “spin diffusion,” is larger than the @xmath
coupling @xmath that is responsible for the resonant energy transfer.

We can write down basic equations of motion such as

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (5.4)
     @xmath   @xmath   @xmath      (5.5)
  -- -------- -------- -------- -- -------

and

  -- -------- -- -------
     @xmath      (5.6)
  -- -------- -- -------

where @xmath is the detuning from resonance. ² ² 2 To obtain these
expressions we take the equations of motion that come about naturally
from the Hamiltonian in Eq. ( 5.1 ) and introduce a phase @xmath into
the definitions of @xmath and @xmath . From these we can obtain

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (5.7)
                                @xmath   
  -- -------- -------- -------- -------- -------

These differential equations are rather difficult to work with, but we
have already discussed one possible approximation scheme in Ref. [ 18 ]
.

#### 5.2.2 Reduction to the sparse limit

We now show explicitly how the general spin-variable formalism of
Section 5.2.1 relates to the coefficients @xmath and @xmath of Eqs. (
4.1a ) and ( 4.1b ) when there is only a single atom that can be in the
@xmath , @xmath pair of states (i.e., when only a single spin variable
of the type @xmath is present.) The key to the correspondence is that in
this case the initial state @xmath , which consists of all spins down,
evolves to

  -- -------- -- -------
     @xmath      (5.8)
  -- -------- -- -------

at time @xmath , where the spin variables now denote time-independent
Schrödinger operators. With the understanding that @xmath , we have:

  -- -------- -- -------
     @xmath      (5.9)
  -- -------- -- -------

  -- -------- -- --------
     @xmath      (5.10)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (5.11)
  -- -------- -- --------

It is then clear that the expectation value of Eq. ( 5.6 ) is equivalent
to

  -- -------- -- --------
     @xmath      (5.12)
  -- -------- -- --------

where @xmath is short for @xmath . This equation is itself a consequence
of Eq. ( 4.1a ). Similarly, the equations for @xmath and @xmath are
equivalent to the equations for @xmath and @xmath that follow from
Eqs. ( 4.1a ) and ( 4.1b ).

#### 5.2.3 Another spin glass Hamiltonian for the rubidium system

In the rubidium experiments of Anderson et al. [ 5 , 6 ] and Lowell et
al. [ 40 ] there are the four states @xmath , @xmath , @xmath , and
@xmath . Therefore we can define a column vector of length four for each
site, so that if an atom at site @xmath is in the @xmath , @xmath ,
@xmath , or @xmath state, then the column vector corresponding to that
site is @xmath , @xmath , @xmath , or @xmath , respectively. The
Hamiltonian for this system can then be written

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (5.13)
                                @xmath   
  -- -------- -------- -------- -------- --------

where @xmath , @xmath , and @xmath are the @xmath matrices

  -- -------- -- --------
     @xmath      (5.14)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (5.15)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (5.16)
  -- -------- -- --------

and the subscripts on these matrices indicate on which site they are to
be applied. The quantities @xmath and @xmath are defined to be zero if
@xmath , and are the interaction potentials

  -- -------- -- --------
     @xmath      (5.17)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (5.18)
  -- -------- -- --------

otherwise. In the interest of completeness, we note that we could also
include the @xmath interaction by adding a term

  -- -------- -- --------
     @xmath      (5.19)
  -- -------- -- --------

to the Hamiltonian of Eq. ( 5.13 ).

The Hamiltonian of Eq. ( 5.13 ) is entirely equivalent to the
Hamiltonian of Eq. ( 5.1 ). The fact that Eq. ( 5.1 ) contains one set
of spins corresponding to the primed sites and another set of spins
corresponding to the unprimed sites is seen to arise from the fact that
Eq. ( 5.13 ) can be viewed as containing two distinct types of
operators. One type operates only on the @xmath and @xmath states and
hence only on primed sites, while the other acts only on the @xmath and
@xmath states and hence only on the unprimed sites. Eq. ( 5.13 ) appears
more complicated than Eq. ( 5.1 ), but it actually embodies the general
technique for constructing spin Hamiltonians for these systems, as we
will see in Section 5.3 .

### 5.3 Spin glass model for the cesium system

In the cesium experiments of Mourachko et al. [ 45 , 44 ] , there are
three states that contribute to the experimental results. These are the
@xmath , @xmath , and the @xmath states, which we refer to as the @xmath
, @xmath , and @xmath states, respectively. The processes @xmath ,
@xmath , and @xmath can occur. The @xmath process can be tuned in and
out of resonance via the Stark shift by application of an electric
field, while the @xmath and @xmath processes are always resonant. In the
case of the cesium experiments, the @xmath and @xmath processes are of
comparable strength and both must be considered.

Just as we did in Section 5.2 , we define a column vector of length
three for each site. If an atom at site @xmath is in the @xmath , @xmath
, or @xmath state, then the column vector corresponding to that site is
@xmath , @xmath , or @xmath , respectively. The Hamiltonian for this
system is then

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (5.20)
                                @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

where @xmath , @xmath , and @xmath are the @xmath matrices

  -- -------- -- --------
     @xmath      (5.21)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (5.22)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (5.23)
  -- -------- -- --------

and the interaction potentials @xmath , @xmath , and @xmath are zero if
@xmath and otherwise are the potentials corresponding to the @xmath ,
@xmath , and @xmath processes, respectively.

The equations of motion can be determined in this case as well, simply
by computing commutators.

### 5.4 Summary

We have seen in this chapter how the frozen dipolar gases studied by
Anderson et al. [ 5 , 6 ] and Lowell et al. [ 40 ] , as well as
Mourachko et al. [ 45 , 44 ] , are completely equivalent to systems of
interpenetrating spin glasses. Casting the problem in this form has the
advantage that it becomes possible to at least write down equations that
go beyond the sparse model of Chapters 2 and 4 .

## Chapter 6 The Effect of Spin

### 6.1 Introduction

Throughout most of Chapter 2 , we used an interaction potential that
varied as @xmath , with no angular dependence. In Section 2.5 we
discussed how to deal with interaction potentials that have a more
complicated dependence on @xmath , depend on the angular variables, or
both. In this chapter we determine exactly what the spin-dependent
interatomic potentials are for the @xmath and @xmath processes in the
experiment we are modeling [ 5 , 6 , 40 ] . The interaction potential
depends directly on the @xmath values for the states involved, where
@xmath is the projection of the total angular momentum in the direction
of the applied electric field. For states with @xmath , @xmath is just
the projection of the outer electron’s spin. For states with @xmath ,
the spin-orbit coupling separates the states with @xmath from the states
with @xmath . Further, we assume that for @xmath , the applied electric
field splits the states with @xmath from those with @xmath . As is
illustrated in Fig. 1.1 , this is the case for the @xmath states of Rb
under the conditions of the experiments in which we are interested.
Then, for each of the states we consider, @xmath is fixed and @xmath can
only take on the values @xmath , so that it is effectively equivalent to
a spin of @xmath . When we refer to spin effects in this chapter we
mean, more precisely, the effects due to @xmath which are derived from
the spin-orbit coupling.

We start in Section 6.2 by examining the matrix elements of the
dipole-dipole interaction in more detail, and taking into account the
effect of the various atomic spin states. The techniques we use for
doing this are inspired by Ref. [ 46 ] , which we found to be extremely
helpful in that it pointed us in the right direction when we began our
study of spin effects in the frozen system. In Section 6.2.1 we recall
the form of the dipole-dipole interaction potential and in Section 6.2.2
we begin the calculation of the dipolar matrix elements with the effects
of the atomic states taken into consideration. This computation requires
a change of basis of the atomic spin states, as explained in Section
6.2.3 . In Section 6.2.4 the laborious procedure necessary to compute
the individual matrix elements is explained, and the results are
presented in tabular form.

Next, in Section 6.3 we attempt to determine the effective interaction
potential for the resonant and nonresonant processes. We start in
Section 6.3.1 by determining the states and probability amplitudes of
the system when the complications due to the presence of atomic spin are
taken into account. We proceed in Section 6.3.2 to discuss the
corresponding equations of motion. We find, as expected, that in general
the quantity @xmath is not conserved. Therefore the number of
interacting states for a system of @xmath atoms is @xmath , whereas it
is only @xmath when the effects of spin are neglected. However, when
states of @xmath are involved it is still possible to obtain exact
formulas as we did in Chapter 2 , provided that the quantity @xmath is
replaced by an effective interaction. We refer to this case as the
separable case, for reasons that will become apparent in Section 6.3.4 .
Section 6.3.3 concerns the properties of Dirac matrices, which are
instrumental in our discussion of separability and its applicability to
averaging in Section 6.3.4 . In Section 6.3.5 we determine the effective
interaction potential in the separable case. Finally, we summarize and
conclude in Section 6.4 .

### 6.2 Matrix elements for the @xmath and @xmath processes

#### 6.2.1 The dipole-dipole interaction

We consider two atoms, the first with its nucleus located at @xmath and
outer electron located at @xmath , and the second with nucleus located
at @xmath and outer electron located at @xmath . Next we define

  -- -------- -- -------
     @xmath      (6.1)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (6.2)
  -- -------- -- -------

and

  -- -------- -- -------
     @xmath      (6.3)
  -- -------- -- -------

The potential for the electrostatic interaction between the two atoms is
then, in atomic units,

  -- -------- -- -------
     @xmath      (6.4)
  -- -------- -- -------

The first two terms on the right hand side represent the interaction
between the two nuclei and the interaction between the two electrons,
respectively. The last two terms represent the interactions between a
nucleus and the electron that belongs to the other atom.

As is shown on page 283 of Ref. [ 56 ] and in more detail on page 260 of
Ref. [ 55 ] , expanding Eq. ( 6.4 ) and keeping terms only to leading
order in @xmath and @xmath gives

  -- -------- -- -------
     @xmath      (6.5)
  -- -------- -- -------

This is the well known result for the dipole-dipole potential between
two atoms.

It is important to note that, although Eq. ( 6.5 ) is the same potential
used in the derivation of the van der Waals interaction, the effects we
study are transient effects that take place after the atoms have been
allowed to interact only a very short time. Therefore they are quite
different from the steady-state van der Waals effect.

#### 6.2.2 Dipolar matrix elements

We will later be averaging over the positions of the atoms, so we only
want to determine the angular dependence of the matrix elements. Thus
the @xmath factor can be ignored until averaging, and we need only
consider matrix elements of the type

  -- -------- -- -------
     @xmath      (6.6)
  -- -------- -- -------

or

  -- -------- -- -------
     @xmath      (6.7)
  -- -------- -- -------

Here we note that when we write the states @xmath and @xmath we really
mean @xmath and @xmath , respectively. In Section 6.2.4 , where we deal
with products of a great many states and the tensor products are more
urgently called for, we will write the tensor product symbols out
explicitly. For now, however, since we are only dealing with products of
two states at a time, we omit them to save space and make the notation a
little simpler.

We now recall that, in the experiments we are modeling, @xmath , @xmath
, @xmath , and @xmath refer to the @xmath , @xmath , @xmath , and @xmath
states, respectively. We also recall that there are really two distinct
resonances in the @xmath interaction, corresponding to whether @xmath or
@xmath for the @xmath atom. The complication for the interaction
potential enters through the @xmath and @xmath states. Clearly the
matrix element in Eq. ( 6.7 ) will be different for the case where the
@xmath state on the left has @xmath and the one on the right has @xmath
than for the case where both @xmath states have @xmath , because the
@xmath state with @xmath will involve a different mixture of the @xmath
, @xmath , and @xmath states than the state where @xmath . The matrix
element in Eq. ( 6.6 ) will similarly depend on the @xmath values of the
@xmath and @xmath states, although there is some simplification because
we are able to consider the @xmath states with @xmath and those with
@xmath separately.

For simplicity, we define

  -- -------- -- -------
     @xmath      (6.8)
  -- -------- -- -------

and we note that

  -- -------- -- -------
     @xmath      (6.9)
  -- -------- -- -------

where @xmath , @xmath , and @xmath .

Consider for a moment the matrix element

  -- -------- -- --------
     @xmath      (6.10)
  -- -------- -- --------

where @xmath and @xmath can be @xmath , @xmath , or @xmath . We can
apply the usual parity argument to determine that @xmath is nonzero only
if @xmath , and in that case is equal to the dipole moment @xmath .
Similarly, @xmath is nonzero only if @xmath , and in that case is equal
to the dipole moment @xmath . Thus we have

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.11)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Analogously,

  -- -------- -------- -------- -------- --------
     @xmath   @xmath                     (6.12)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

In fact, the matrix elements we consider are a little more complicated
than this. The atoms have a total spin @xmath . The potential @xmath
changes @xmath , but does not at all affect @xmath and therefore cannot
change @xmath or @xmath . Therefore the matrix elements we really
consider are of the form

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.13)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where, for example, @xmath is short for @xmath . Analogously,

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.14)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Using Eqs. ( 6.13 ) and ( 6.14 ), it is trivial to see that

  -- -------- -- --------
     @xmath      (6.15)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (6.16)
  -- -------- -- --------

#### 6.2.3 A change of basis

We can easily write out the states @xmath , @xmath , @xmath , and @xmath
in terms of states of the form @xmath . For example, the state @xmath
denotes the pair of states @xmath and @xmath , while the state @xmath
denotes the pair of states @xmath and @xmath . Similarly, @xmath denotes
the pair of states @xmath and @xmath . Recalling that the @xmath states
with @xmath are treated separately from those with @xmath , we see that
@xmath denotes either the pair of states @xmath and @xmath , or the pair
of states @xmath and @xmath . In order to use Eqs. ( 6.15 ) and ( 6.16
), we have to write the @xmath and @xmath states as linear combinations
of states of the form @xmath . This is easily done by consulting a table
of Clebsch-Gordan coefficients, as can be found on pages 76–77 of Ref. [
14 ] . Using such a table, we determine the decompositions in Table 6.1
.

#### 6.2.4 The @xmath and @xmath matrix elements

With the decomposition of Table 6.1 and the rules of Eqs. ( 6.15 ) and (
6.16 ) in hand, it is now possible to compute the matrix elements we
need. I will first show how to compute one matrix element, as an
example, then present all the matrix elements in table form.

Consider the matrix element

  -- -- -- --------
           (6.17)
  -- -- -- --------

We have from Table 6.1 that

  -- -- -- --------
           (6.18)
  -- -- -- --------

and

  -- -------- -- --------
     @xmath      (6.19)
  -- -------- -- --------

Dropping the occurrences of “ @xmath ” that appear to save space, we see
that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.20)
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

Now we simply apply Eq. ( 6.15 ) to each matrix element corresponding to
a term in the sum of Eq. ( 6.20 ). For example, the first term in the
sum corresponds to the matrix element

  -- -------- -- --------
     @xmath      (6.21)
  -- -------- -- --------

which according to Eq. ( 6.15 ) is equal to

  -- -------- -- --------
     @xmath      (6.22)
  -- -------- -- --------

Computing the other terms similarly and summing them, we determine one
of the matrix elements for the @xmath process.

The process is straightforward, albeit laborious, and one obtains the
matrix elements presented in the Tables 6.2 – 6.4 .

### 6.3 Determining the potential

#### 6.3.1 States and probability amplitudes with spin included

In the initial state, the system is labeled by an index @xmath
representing the state of the @xmath atom and a set of @xmath indices
@xmath representing the states of the @xmath @xmath atoms. In the
absence of an applied magnetic field, the initial values of these
indices are random. The state corresponding to the configuration where
the primed atom is in the @xmath state with @xmath and the unprimed
atoms are all in the @xmath state with @xmath is

  -- -------- -- --------
     @xmath      (6.23)
  -- -------- -- --------

Of course, because the @xmath and @xmath states have total spin @xmath ,
all @xmath are required to have the value @xmath or @xmath . The
probability amplitude corresponding to this state is

  -- -- -- --------
           (6.24)
  -- -- -- --------

where @xmath is the state of the system at time @xmath .

Because the system is allowed to make @xmath transitions, we must also
consider states in which the primed atom is in the state @xmath and a
single unprimed atom is in the state @xmath . This state is labeled by
the indices @xmath and @xmath , together with the indices @xmath
corresponding to the @xmath atoms not involved in the @xmath transition.
The state corresponding to the configuration where the primed atom is in
the @xmath state with @xmath and all the unprimed atoms except for the
@xmath th are in the @xmath state while the @xmath th unprimed atom is
in the @xmath state with @xmath is

  -- -------- -- --------
     @xmath      (6.25)
  -- -------- -- --------

The quantities @xmath are restricted to the values @xmath because the
@xmath state has total spin @xmath . Because the @xmath state also has
total spin @xmath , @xmath is similarly restricted to the values @xmath
. The @xmath state has total spin @xmath , and the quantity @xmath is
restricted to the values @xmath or @xmath , depending on whether we are
considering the @xmath or @xmath resonance, respectively. Similarly to
Eq. ( 6.24 ), the probability amplitude corresponding to the state in
Eq. ( 6.25 ) is

  -- -- -- --------
           (6.26)
  -- -- -- --------

where @xmath is again the state of the system at time @xmath .

#### 6.3.2 Equations of motion

In the interest of brevity, we denote the subscript lists

  -- -------- -- --------
     @xmath      (6.27)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (6.28)
  -- -------- -- --------

by @xmath and @xmath , respectively. We label the @xmath th and @xmath
th components of @xmath with a different symbol than the rest because we
will sum over @xmath , and it will turn out that the @xmath term in the
sum corresponds to the @xmath atom at the origin and the @xmath atom at
position @xmath making the @xmath transition. Thus the use of a
different symbol serves as a reminder of which atoms are involved in the
transition.

The equations of motion for the @xmath and @xmath coefficients of Eqs. (
6.24 ) and Eqs. ( 6.26 ) in Fourier space are then

  -- -------- -------- -------- -- ---------
     @xmath   @xmath   @xmath      (6.29a)
     @xmath   @xmath   @xmath      (6.29b)
  -- -------- -------- -------- -- ---------

where @xmath is the initial @xmath value for the @xmath th atom and

  -- -------- -- --------
     @xmath      (6.30)
  -- -------- -- --------

is just the radial part of the interaction potential. Here we have
defined @xmath to be another set of indices subject to the same
constraints as @xmath . The matrix @xmath is a @xmath matrix that is
constructed from the @xmath matrix formed by Table 6.2 or 6.3 ,
depending on whether we are considering the case where the @xmath atom
has @xmath or @xmath . Explicitly, if the set of

  -- -------- -- --------
     @xmath      (6.31)
  -- -------- -- --------

are the elements belonging to the @xmath matrix that corresponds to the
value of @xmath that we want for the @xmath state, then

  -- -------- -- --------
     @xmath      (6.32)
  -- -------- -- --------

We see immediately from Eqs. ( 6.32 ) and ( 6.31 ) that

  -- -------- -- --------
     @xmath      (6.33)
  -- -------- -- --------

Multiplying both sides of Eq. ( 6.29b ) by @xmath and summing over
@xmath and @xmath , we see that

  -- -------- -- --------
     @xmath      (6.34)
  -- -------- -- --------

where

  -- -- -- --------
           (6.35)
  -- -- -- --------

Using Eq. ( 6.33 ) we see that, in terms of matrices, Eq. ( 6.35 ) is
equivalent to

  -- -------- -- --------
     @xmath      (6.36)
  -- -------- -- --------

Plugging the result of Eq. ( 6.34 ) into Eq. ( 6.29a ), we see that

  -- -------- -- --------
     @xmath      (6.37)
  -- -------- -- --------

For @xmath atoms, this represents @xmath equations.

The quantity @xmath actually depends on the two sets of indices @xmath
and @xmath , as is clear from the first term on the right hand side of
Eq. ( 6.37 ). Therefore we can consider @xmath to be a matrix, and we
can write

  -- -------- -- --------
     @xmath      (6.38)
  -- -------- -- --------

At this point we solve this equation formally and switch to Laplace
space by making the substitution @xmath . We find

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.39)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Because Eq. ( 6.29b ) is coupled to Eq. ( 6.29a ), @xmath also depends
on the set of indices @xmath . Thus @xmath can also be considered a
matrix, and we can rewrite Eqs. ( 6.29a ) and ( 6.29b ) in the matrix
form

  -- -------- -------- -------- -- ---------
     @xmath   @xmath   @xmath      (6.40a)
     @xmath   @xmath   @xmath      (6.40b)
  -- -------- -------- -------- -- ---------

We note that these equations correspond to Eqs. ( 2.1a ) and ( 2.1b ),
and so we hope to obtain an expression for @xmath analogous to Eq. ( 2.7
). We have to exercise a little caution, however, since Eqs. ( 2.1a )
and ( 2.1b ) are equations for scalar quantities while Eqs. ( 6.40a )
and ( 6.40b ) are matrix equations. First consider Eq. ( 6.39 ) in the
rational form we had before we introduced the integral over @xmath .
Because the matrix @xmath is Hermitian we can diagonalize it. If we let
@xmath be the eigenstate of @xmath with eigenvalue @xmath , then

  -- -------- -- --------
     @xmath      (6.41)
  -- -------- -- --------

The quantity @xmath is just a scalar, so we can certainly invert this
Laplace transform, take the mod square, and Laplace transform again just
as we did in Section 2.2 . In this way we find that

  -- -------- -- --------
     @xmath      (6.42)
  -- -------- -- --------

We could do this for any of the eigenvectors @xmath , and so it follows
that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.43)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

which looks very much like Eq. ( 2.7 ).

To obtain results that can be compared with the experiments, we also
want to average over initial spin configurations and sum over final spin
configurations. Thus we want to compute the quantity

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.44)
                                @xmath   
  -- -------- -------- -------- -------- --------

#### 6.3.3 Dirac matrices

Before we go any further with Eq. ( 6.44 ), we first examine some
properties of @xmath matrices. It will turn out that this will aid us
greatly in carrying out the average of Eq. ( 6.44 ).

We know from pages 211–213 of Ref. [ 7 ] that any @xmath matrix can be
written as a unique linear combination of the sixteen Dirac matrices.
These matrices are of the form @xmath , where @xmath and @xmath range
from zero to three. Here we define @xmath to be the @xmath unit matrix
if @xmath and the usual Pauli matrix otherwise. Thus, if we have any
@xmath matrix @xmath then we can write

  -- -------- -- --------
     @xmath      (6.45)
  -- -------- -- --------

for some coefficients @xmath . These coefficients are in fact easy to
determine, since

  -- -------- -- --------
     @xmath      (6.46)
  -- -------- -- --------

Eq. ( 6.46 ) follows easily from the well-known properties of the Pauli
matrices and the facts that

  -- -------- -- --------
     @xmath      (6.47)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (6.48)
  -- -------- -- --------

Let @xmath , @xmath , and @xmath be the @xmath matrices determined by
Tables 6.2 – 6.4 . Then define

  -- -------- -- --------
     @xmath      (6.49)
  -- -------- -- --------

and similarly for @xmath and @xmath . We will see in Section 6.3.4 that
these three matrices are very important when it comes to averaging Eq. (
6.44 ). Therefore we present the three @xmath matrices in Tables 6.5 –
6.7 .

We will also see in Section 6.3.4 that in order to obtain the average of
Eq. ( 6.44 ) we will need the decompositions of the @xmath matrices into
linear combinations of the Dirac matrices. We have seen that we can use
Eqs. ( 6.45 ) and ( 6.46 ) to determine these linear combinations, and
in this way we obtain Tables 6.8 – 6.10 .

#### 6.3.4 Separability

At this point we would like to write

  -- -------- -- --------
     @xmath      (6.50)
  -- -------- -- --------

so that we can perform our usual averaging procedure as described in
Section 2.3 . Unfortunately, Eq. ( 6.50 ) does not hold unless the
commutator of @xmath with @xmath is a @xmath -number. We can use Tables
6.8 – 6.10 to check if this is the case for our three potentials.

Because it is the simplest, we first consider the potential for the
@xmath process when @xmath . Using Table 6.8 we see that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.51)
                                @xmath   
  -- -------- -------- -------- -------- --------

for some coefficients @xmath , @xmath , @xmath , and @xmath that depend
on @xmath and @xmath . We also define

  -- -------- -- --------
     @xmath      (6.52)
  -- -------- -- --------

Similarly, we have

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.53)
                                @xmath   
  -- -------- -------- -------- -------- --------

Because @xmath is just the @xmath identity matrix, it follows that
@xmath is just the @xmath identity matrix. Thus @xmath will commute with
anything. Now consider the product

  -- -- -- --------
           (6.54)
  -- -- -- --------

Although the result does not depend on it, we assume for convenience
that @xmath . We also expand each of the terms in Eq. ( 6.54 ) and apply
Eq. ( 6.47 ). We find that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.55)
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

since @xmath is just the identity matrix. Note that if we had considered
instead the product

  -- -- -- --------
           (6.56)
  -- -- -- --------

then we would have obtained the same result. Therefore each term in
@xmath commutes with each of the terms in @xmath . It follows that
@xmath commutes with @xmath , and hence we can perform the decomposition
of Eq. ( 6.50 ) in this case.

Now consider the potential for the @xmath process when @xmath . Using
Table 6.9 we see that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.57)
                                @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

and similarly for @xmath . Examining the product

  -- -- -- --------
           (6.58)
  -- -- -- --------

we see that

  -- -------- -------- -- -------- --------
     @xmath   @xmath               (6.59)
                          @xmath   
  -- -------- -------- -- -------- --------

while if we had multiplied the factors in the opposite order we would
have obtained

  -- -------- -- --------
                 
     @xmath      (6.60)
  -- -------- -- --------

Since @xmath , while @xmath , we see that the commutator of @xmath and
@xmath is not a @xmath -number. Because the set of @xmath is not the
same as the set of @xmath , it follows that the commutator of @xmath and
@xmath cannot be a @xmath -number. Hence we cannot perform the
separation of Eq. ( 6.50 ) in this case.

Considering the potential for the @xmath process and consulting Table
6.9 , we see that the coefficients of @xmath and @xmath are both
nonzero. Therefore the argument of the previous paragraph will hold here
too, and again we cannot perform the separation of Eq. ( 6.50 ).

#### 6.3.5 Averaging the @xmath process when @xmath for the @xmath atom

In this case we can perform the separation of Eq. ( 6.50 ), and so we
see from Eq. ( 6.44 ) that the averaging problem reduces the computation
of the average of

  -- -------- -- --------
     @xmath      (6.61)
  -- -------- -- --------

Using Table 6.8 , we can write

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.62)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

since @xmath is just the @xmath identity matrix, which certainly
commutes with @xmath . Here @xmath depends on @xmath , and specifically

  -- -------- -- --------
     @xmath      (6.63)
  -- -------- -- --------

where @xmath is the same quantity as in Eq. ( 6.53 ).

If we define

  -- -------- -- --------
     @xmath      (6.64)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (6.65)
  -- -------- -- --------

then we can rewrite Eq. ( 6.62 ) as

  -- -- -- --------
           (6.66)
  -- -- -- --------

Now we note that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.67)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

so that

  -- -------- -- --------
     @xmath      (6.68)
  -- -------- -- --------

where to save space we define

  -- -------- -- --------
     @xmath      (6.69)
  -- -------- -- --------

A derivation very similar to that of Eq. ( 6.67 ) shows that

  -- -------- -- --------
     @xmath      (6.70)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (6.71)
  -- -------- -- --------

and by repeated application of these last two relations we finally
arrive at

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.72)
                                @xmath   
  -- -------- -------- -------- -------- --------

From page 166 of Ref. [ 53 ] we have the familiar result that

  -- -------- -- --------
     @xmath      (6.73)
  -- -------- -- --------

From this it is easy to show that

  -- -------- -- --------
     @xmath      (6.74)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (6.75)
  -- -------- -- --------

Thus we see that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.76)
                                @xmath   
  -- -------- -------- -------- -------- --------

and hence

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.77)
                                @xmath   
  -- -------- -------- -------- -------- --------

We now note that because @xmath if @xmath is equal to @xmath , @xmath ,
or @xmath , it follows that when we expand the product

  -- -------- -- --------
     @xmath      (6.78)
  -- -------- -- --------

and take the trace, only the term

  -- -------- -- --------
     @xmath      (6.79)
  -- -------- -- --------

survives. This together with Eqs. ( 6.61 ) and ( 6.77 ) implies that

  -- -------- -- --------
     @xmath      (6.80)
  -- -------- -- --------

We can now change variables so that

  -- -------- -- --------
     @xmath      (6.81)
  -- -------- -- --------

is equal to a product of @xmath copies of

  -- -------- -- --------
     @xmath      (6.82)
  -- -------- -- --------

This integral, in turn, is equal to the sum of the integrals

  -- -------- -- --------
     @xmath      (6.83)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (6.84)
  -- -------- -- --------

Once again using the coefficients of Table 6.8 , we find that

  -- -------- -- --------
     @xmath      (6.85)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (6.86)
  -- -------- -- --------

The integrals of Eqs. ( 6.83 ) and ( 6.84 ) are now of a familiar type,
and they can be done using the methods explained in Sections 2.3 and 2.5
. Unfortunately the angular integrals have to be done numerically, but
the result is that ¹ ¹ 1 The astute reader will be relieved to know that
the quantities @xmath and @xmath are both positive for all values of
@xmath and @xmath , and so there is no difficulty in applying the
methods of Sections 2.3 and 2.5 here.

  -- -------- -- --------
     @xmath      (6.87)
  -- -------- -- --------

It follows that

  -- -------- -- --------
     @xmath      (6.88)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (6.89)
  -- -------- -- --------

Recalling Eq. ( 2.12 ), we see that when we used the simple @xmath
interaction potential of Chapter 2 , we obtained

  -- -------- -- --------
     @xmath      (6.90)
  -- -------- -- --------

Hence we arrive at

  -- -------- -- --------
     @xmath      (6.91)
  -- -------- -- --------

Inserting our averaged result back into Eq. ( 6.44 ), we see that

  -- -- -- --------
           (6.92)
  -- -- -- --------

Comparing with Eq. ( 2.27 ), one immediately sees that Eq. ( 6.92 )
gives the same results as the signal computed in Chapter 2 but with the
quantity @xmath appearing in place of @xmath .

### 6.4 Summary

In this chapter we have considered the detailed effects of spin on the
system we are studying. We were only able to fully average the @xmath
process when @xmath for the @xmath atom, but we can see from Fig. 1.4
that the case where @xmath for the @xmath atom is not very different. In
the case that we could treat fully, we found that the effect of the spin
was to change the value of the parameter @xmath by a factor of @xmath
from the simple case where we neglect the effects of spin and consider
an angular independent @xmath interaction potential.

The consideration of the effects of spin becomes extremely important
when a magnetic field is applied to the system. It was reported by Renn
et al. [ 50 ] that the introduction of a magnetic field greatly
proliferates the number of nondegenerate states present in a room
temperature Rydberg gas. One can imagine applying a magnetic field that
is parallel to the detuning electric field and of sufficient strength to
lift the @xmath degeneracy. The form of the @xmath interaction would
then be given by the appropriate matrix element of Table 6.2 or 6.3 ,
and the form of the @xmath interaction would similarly be found in Table
6.4 . Thus we see that this is one possible scheme for selecting the
angular form of the interaction potential.

## Chapter 7 Numerical Simulations

### 7.1 Introduction

In this chapter we discuss numerical simulations of the system we are
considering. We start by describing in detail how the simulations are
carried out, then we present the results, including comparisons of the
simulations with the experimental data.

### 7.2 Review of theoretical model

Before discussing exactly how the simulations are carried out, we review
the basic elements of the theory presented at the beginning of Sections
2.2 and 4.2.1 of this thesis. We consider one atom at the origin,
initially in the state @xmath , in interaction with a gas of @xmath
atoms through a resonant @xmath dipole-dipole interaction @xmath . We
also allow for interaction among the unprimed atoms via an @xmath
process mediated by another dipole-dipole potential @xmath . This system
is described by Eqs. ( 4.1a ) and ( 4.1b ), which are

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (7.1a)
     @xmath   @xmath   @xmath      (7.1b)
  -- -------- -------- -------- -- --------

Here @xmath is the amplitude of the state in which the atom at the
origin is in state @xmath and all other atoms are in state @xmath ,
while @xmath is the amplitude of the state in which the atom at the
origin is in state @xmath and the atom at @xmath is in state @xmath ,
while all the others remain in state @xmath . The quantities @xmath and
@xmath are the interaction potentials and @xmath is the detuning from
resonance.

The simplest possibilities for the dipole-dipole interactions @xmath and
@xmath are

  -- -------- -- -------
     @xmath      (7.2)
  -- -------- -- -------

and

  -- -------- -- -------
     @xmath      (7.3)
  -- -------- -- -------

Here @xmath is the dipole matrix element connecting the @xmath and
@xmath states, and @xmath is the dipole matrix element connecting the
@xmath and @xmath states. We also define @xmath , so that the
restriction @xmath is automatic in sums of the type appearing in Eq. (
7.1b ). The atoms are assumed to be sufficiently cold that during the
time scale of interest they move only a very small fraction of their
separation, and therefore @xmath and @xmath can be taken to be
independent of time.

As was stated in Section 2.5.1 , the induced dipoles in the
magneto-optical trap become aligned if the applied electric field is
sufficiently large that the Stark effect has passed from the quadratic
to the linear regime. This is the case considered, for example, in Refs.
[ 32 ] and [ 54 ] . For the numerical simulation results of this thesis
we use the slightly more complicated dipole-dipole interaction potential
corresponding to this case, which is

  -- -------- -- -------
     @xmath      (7.4)
  -- -------- -- -------

where @xmath is the angle between @xmath and the applied electric field.
Similarly, @xmath is of the form

  -- -------- -- -------
     @xmath      (7.5)
  -- -------- -- -------

for @xmath , and @xmath . These formulas for @xmath and @xmath are also
valid, in some cases, in the quadratic Stark regime if a magnetic field
parallel to the electric field is used to align the total angular
momentum of the atoms, as we now discuss more fully.

We will show below that, using interactions such as Eqs. ( 7.2 ) and (
7.3 ), or Eqs. ( 7.4 ) and ( 7.5 ), we are able to reduce the
computation of any quantity to the diagonalization of an @xmath matrix.
On the other hand, we saw in Chapter 6 that in the quadratic Stark
regime the actual interaction potentials depend on the quantum numbers
@xmath of the atoms involved. A complete consideration of the spins
would require the diagonalization of a @xmath matrix in this case, which
is computationally prohibitive to say the least. The problem can be
reduced again to @xmath by applying a magnetic field that is parallel to
the electric field and of sufficient strength to lift the @xmath
degeneracy. The @xmath interaction is then given by the appropriate
matrix element of Table 6.2 or 6.3 , and the @xmath interaction is
similarly found in Table 6.4 . We see from these tables that for many
matrix elements the angular dependence is the same as in Eqs. ( 7.4 )
and ( 7.5 ), which can therefore be taken as representative. We have
also shown in Section 6.3.5 that, when a @xmath state with @xmath is
involved, each @xmath atom interacts separately with the @xmath atom
and, as the system evolves, the average number of @xmath states is the
same as for a simple spin-independent interaction, but with an effective
@xmath . The resulting effective interaction energy @xmath is given by
Eq. ( 6.91 ) to be @xmath , where @xmath is the effective interaction
energy for the interaction potential of Eq. ( 7.2 ) and is defined
explicitly in Eq. ( 2.12 ). For comparison, the effective interaction
energy for the interaction potential of Eq. ( 7.4 ) is given by Eq. (
2.75 ) to be @xmath . Thus we see that the exact angular form of the
interaction potential for @xmath does not greatly affect the results.
The numerical simulations reported in this chapter show that the same is
true for the angular form of @xmath .

### 7.3 Description of method

If we consider a column vector @xmath , then clearly we can construct a
matrix @xmath such that the set of @xmath coupled differential equations
of Eqs. ( 7.1a ) and ( 7.1b ) can be written in the form @xmath . If we
let @xmath and @xmath be the eigenvalues and eigenvectors (represented
as column vectors) of the real Hermitian matrix @xmath , respectively,
and if we further define @xmath to be the column vector of length @xmath
with the @xmath th entry set equal to one and all others set equal to
zero, then we can write @xmath and @xmath , where @xmath . The system
starts off in the state described by @xmath , so it follows that

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (7.6)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

The quantity @xmath is just the @xmath component of this expression, so

  -- -------- -- -------
     @xmath      (7.7)
  -- -------- -- -------

Thus we see that, in this form, solving for @xmath at a given time
becomes just a matter of diagonalizing the matrix @xmath .

To carry out the numerical simulations, we first generate a set of
random positions for the unprimed atoms. For the results presented in
this Sections 7.5 and 7.6 , these random positions are uniformly
distributed so that the results can be more easily compared with the
analytical results of previous chapters. However, when proceeding
numerically it is just as easy to compute random positions for a gas
blob with any given density profile. For the experiments we are modeling
[ 5 , 6 , 40 ] , the density profile is a Gaussian with an aspect ratio
of approximately @xmath [ 23 ] , and so this is what we use to generate
the results of Section 7.7 . Once the atomic positions are generated,
they are used to compute the interaction potentials @xmath and @xmath ,
and these are in turn used to compute the matrix @xmath . We then
diagonalize @xmath and construct the quantity @xmath at a set of values
of @xmath using Eq. ( 7.7 ). We do this for the same set of times for
many random distributions of the unprimed atoms and thereby obtain an
average of the quantities @xmath and @xmath at a given set of times over
many configurations of the system. The signal measured by the
experiments is then proportional to the average of @xmath . We can
further obtain plots of the linewidth as a function of time by
generating signal versus time curves for many values of the detuning,
@xmath , and then determining for each value of time which of these
signals is equal to one half the value of the resonance signal. One can
also directly generate Fourier transforms from Eq. ( 7.7 ), as we did in
Section 4.6.2 .

By comparing the results of the simulations to the analytical results of
Chapter 2 we find that an average over @xmath realizations, each
consisting of a single @xmath atom and @xmath @xmath atoms, is
sufficient, as can be seen in Fig. 7.1 . These are the parameters we
have used for all the simulation plots presented in this thesis,
although we have run the simulations for as many as @xmath @xmath atoms.

The simulations are written in Fortran 90 , and the computation process
is greatly accelerated by the use of a Beowulf cluster of thirteen Linux
machines. Using this cluster, the computation can be carried out in
parallel, as we now describe. A master program is started on one of the
machines. The master then starts up a slave program on each machine in
the cluster. The master initializes the slaves by sending them seeds for
their random number generators. ¹ ¹ 1 It is vital that the master
control the random number generator seeds, since each slave program must
be seeded differently so that each generates a different set of
realizations for the atomic positions. The slaves then generate random
distributions for the atoms, compute whatever quantity is required, and
send the result back to the master. The master collects and averages the
results returned by the slaves and saves the result to a data file. The
master also takes care of any interaction with the user. The passing of
messages and data between the master and slave processes is handled
through the software program Parallel Virtual Machine ( PVM ) [ 24 ] .

We use the (parallel) random number generation routines gasdev and ran1
described in Refs. [ 47 ] and [ 48 ] to generate the random
distributions of atoms. The routine gasdev generates Gaussian deviates
with unit standard deviation, while ran1 generates uniform deviates
between zero and one. The computation of eigenvalues and eigenvectors
for the matrix @xmath corresponding to each realization is performed
using the ssyevx routine of the LAPACK package, which is described in
Ref. [ 4 ] .

### 7.4 The effect of many @xmath atoms

#### 7.4.1 The chemical argument

Considering the @xmath process as a chemical reaction, we have the rate
equation

  -- -------- -- -------
     @xmath      (7.8)
  -- -------- -- -------

where @xmath is a constant with the units of inverse time, which is
equivalent to energy since we take @xmath equal to unity. There are
similar rate equations for @xmath , @xmath , and @xmath , but it is
sufficient to consider only one of these because of the following
constraints.

We know that @xmath , since if there is a @xmath or @xmath atom present
it can only have been produced as a result of a pair of atoms in the
@xmath and @xmath states making the @xmath transition. We also know that
@xmath and @xmath , since primed and unprimed atoms can change from
@xmath to @xmath states and vice versa, but can never be created or
destroyed. Therefore we have

  -- -------- -------- -------- -------- -------
     @xmath   @xmath                     (7.9)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

With the initial condition @xmath at @xmath , this gives

  -- -------- -- --------
     @xmath      (7.10)
  -- -------- -- --------

and at small times

  -- -------- -- --------
     @xmath      (7.11)
  -- -------- -- --------

#### 7.4.2 Extrapolation to many @xmath atoms

In our case, the reaction rate is not constant and the approach to
equilibrium is not accurately described by Eq. ( 7.10 ). However, the
result we have from Chapter 2 in the limit @xmath starts off at small
times as

  -- -------- -- --------
     @xmath      (7.12)
  -- -------- -- --------

where @xmath is a numerical constant that does not concern us here. This
expression is fine for all values of @xmath and @xmath , and in
particular satisfies the requirement that @xmath . It is of the same
form as Eq. ( 7.11 ) and shows that at small times we have

  -- -------- -- --------
     @xmath      (7.13)
  -- -------- -- --------

Furthermore, we can write

  -- -------- -- --------
     @xmath      (7.14)
  -- -------- -- --------

where the dots indicate that the function @xmath depends on other
dimensionless ratios that characterize the system but do not involve
@xmath . The prefactor @xmath and the characteristic energy @xmath are
suggested by the chemical argument of Section 7.4.1 , since these
factors are explicitly seen in Eq. ( 7.10 ) if we keep in mind Eq. (
7.13 ). The full function @xmath is computable only in the dilute limits
@xmath or @xmath , but we know in general that it starts off linearly in
@xmath .

For large values of @xmath we have, again for @xmath ,

  -- -------- -- --------
     @xmath      (7.15)
  -- -------- -- --------

where @xmath is some function that can be computed numerically using the
methods of Section 7.3 . This result cannot be correct for all values of
@xmath . In fact, for @xmath we can compute @xmath and we find

  -- -------- -- --------
     @xmath      (7.16)
  -- -------- -- --------

Since @xmath is equal to @xmath , we see that this (correct) result is
quite different from what one would obtain using Eq. ( 7.15 ), which is
incorrect in this limit. Interpolating between the limits given by
Eqs. ( 7.15 ) and ( 7.16 ), we postulate that for large @xmath

  -- -------- -- --------
     @xmath      (7.17)
  -- -------- -- --------

We can join this result smoothly to the small @xmath result of Eq. (
7.12 ) by postulating a general result of the form

  -- -------- -- --------
     @xmath      (7.18)
  -- -------- -- --------

where the function @xmath is proportional to @xmath for small @xmath and
approaches @xmath for large @xmath . We have also used the same scaling
for the quantities @xmath and @xmath , so that @xmath .

With this approach we can use the function @xmath , which can be
computed numerically in the dilute limit, as a reasonable guess for the
functional dependence on the suitable dimensionless variables for any
value of @xmath .

The combination

  -- -------- -- --------
     @xmath      (7.19)
  -- -------- -- --------

is just an interpolation between the correct results in the limits of
small @xmath and small @xmath . It is reasonable, however, since the
quantities

  -- -------- -- --------
     @xmath      (7.20)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (7.21)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (7.22)
  -- -------- -- --------

give the effective @xmath for the @xmath interaction for a fraction
@xmath of the atoms, the effective @xmath for the @xmath interaction for
a fraction @xmath of the atoms, and the @xmath interaction for the
@xmath process for all the atoms. The quantity of Eq. ( 7.19 ) is
obtained by taking the ratio

  -- -------- -- --------
     @xmath      (7.23)
  -- -------- -- --------

### 7.5 Results of simulations for the signal

For the results of this section, we well as those of Section 7.6 , we
use the interaction potentials of Eqs. ( 7.4 ) and ( 7.5 ). We also use
a uniform distribution to generate the random atomic positions, so that
the results can be more easily compared with the analytical results of
previous chapters.

Figs. 7.1 – 7.5 display the numerical simulations of @xmath ,
corresponding to the experimental signal @xmath , as a function of
@xmath for @xmath , @xmath , @xmath , @xmath , and @xmath , and enough
values of the detuning @xmath to give an idea of the resonance width in
each case. The FWHM at large @xmath can be directly deduced from the
intercepts of the graphs with the right edge of the figure, as is
discussed more fully below.

In addition, Fig. 7.1 shows the exact results for @xmath that were
discussed in Chapter 2 . Comparison with the numerical simulations,
which were carried out for @xmath , shows that the infinite system is
already well simulated, but that noticeable wiggles remain after
averaging over @xmath realizations. The reason for this persistence of
fluctuations is that most realizations do not look at all like the
average. In fact, the variance with realization is comparable to the
average. In particular, for small @xmath the average is given by @xmath
while each realization is quadratic in @xmath , as already discussed in
Ref. [ 18 ] . It can be seen from the graphs that the linear rise law,
@xmath , is independent both of @xmath and of @xmath and holds for a
substantial range of @xmath .

### 7.6 Results of simulations for the width

The time-dependent resonance lineshape is obtained from the ordinates of
the curves in Figs. 7.1 – 7.5 at a given @xmath . In general the
lineshape is not Lorentzian, but we never find a split line with a
minimum at resonance as reported in Ref. [ 45 ] . Thus it is fair to
characterize the line by its FWHM. This cannot be directly compared with
an experimental width, which is usually the FWHM of a Lorentzian fitted
to a noisy experimental line, but it is indicative of trends. For a more
accurate comparison with experiment, one should of course convolute the
calculated lineshape with the broadening due to other effects.

Fig. 7.6 shows one half of the FWHM, @xmath , as a function of @xmath
for @xmath . One sees the typical decrease of @xmath from a universal
@xmath behavior at small @xmath towards an asymptotic value, which is
almost reached at @xmath . The @xmath law, which results simply from the
uncertainty principle [ 6 , 60 ] , has already been discussed in Ref. [
18 ] where on the basis of a small @xmath expansion it was estimated
that @xmath for small @xmath . This @xmath behavior was observed
experimentally by Renn et al. [ 60 ] in a different system, and was
later observed by Anderson et al. [ 6 , 5 ] in the Rb system.

The near-asymptotic value of the FWHW at @xmath is shown in Fig. 7.7 as
a function of @xmath . Overall, the graph is roughly linear except at
small @xmath , although a careful examination shows that there are
really two linear regimes with somewhat different slopes that meet at
@xmath . There is an unexpected minimum around @xmath , which is not a
numerical artifact, but can be seen directly from Figs. 7.1 – 7.5 by
comparing the plot for @xmath with those for @xmath and @xmath . What
happens is that the resonance line always grows taller and wider as
@xmath increases, but for small @xmath its height (the signal at @xmath
) grows faster than its width.

### 7.7 Comparison with experimental data

For the results in this section, we use the interaction potentials of
Eqs. ( 7.4 ) and ( 7.5 ). Because it corresponds more closely to the
experimental situation, we also use a Gaussian distribution with aspect
ratio @xmath to generate the random atomic positions.

Figs. 7.8 and 7.9 show the experimental signal as a function of time,
together with a signal computed using the numerical simulations and fit
to the data. The experimental data for both graphs are the data of
Fig. 3.8 on page 71 of Ref. [ 40 ] , but we show here all the data
points.

The fits are made by first determining the vertical coordinates where
the experimental signal starts and stops. The difference between the two
gives the vertical scale of the experimental signal, which is necessary
because as one can see the experimental signal does not simply start at
zero and rise to some value less than one. For example, in Fig. 7.8 we
assumed that the experimental signal started at @xmath and ended at
@xmath so that the vertical scale was @xmath . The initial slope is then
determined by fitting the first several points to a straight line. For
Fig. 7.8 we obtained an initial slope of @xmath @xmath . We then have

  -- -------- -- --------
     @xmath      (7.24)
  -- -------- -- --------

which gives a value for the parameter @xmath and hence the density
@xmath . This fixes the horizontal scale, and so all that remains is to
adjust the parameter @xmath until we find a good fit.

Fig. 7.10 shows the experimental width as a function of time. The data
of Anderson et al. are taken from Fig. 4-18 on page 149 of Ref. [ 5 ] ,
while the data of Lowell et al. are taken from Fig. 3.10 on page 75 of
Ref. [ 40 ] . The discrepancy between the two sets of data has not yet
been resolved by the experimentalists. Although the result of the
simulations fit Anderson’s data quite well for the parameters @xmath and
@xmath @xmath , it is important to note that these data are for rather
small times, and are within or very close to the region where the
universal @xmath behavior discussed in Section 7.6 is present. Thus for
a wide range of values of the parameter @xmath there will exist
densities @xmath that produce a good fit to the Anderson data.

### 7.8 Summary

We have seen in this chapter how to develop numerical simulations to
solve Eqs. ( 7.1a ) and ( 7.1b ) for any quantity we want and average
the result over many realizations of the system. Although Eqs. ( 7.1a )
and ( 7.1b ) apply to the sparse case where we begin initially with a
single @xmath atom in a sea of @xmath atoms, we have also discussed in
this chapter how to extend these results to the nonsparse case. The
simulation results agree very well with the exact analytical results of
Chapter 2 , as well as with the approximate analytical results of
Chapter 4 . The results of the simulations can also be compared with the
experimental data of Anderson et al. [ 5 , 6 ] and Lowell et al. [ 40 ]
, and we find that the agreement is again quite good.

## Chapter 8 Final Thoughts

Although we have focused chiefly on resonant dipole-dipole interactions
among frozen Rydberg atoms in this thesis, there are many connections of
this work with related experiments, as well as more general applications
of the techniques we have developed.

In this thesis we have always considered the atoms to be frozen in
place. It is true that the atoms are sufficiently cold that they move
only a small fraction of their average interatomic spacing during the
course of the experiment, but close pairs of atoms can be separated by
distances considerably smaller than this, and hence the effects of
interatomic motion can be considered. Certainly this would be a
worthwhile endeavor, since one would ideally like to eventually have a
theory that reduces to the frozen results of this thesis at low
temperatures and, as the temperature increases, eventually makes the
transition to the high-temperature regime where the results are
dominated by binary atomic collisions. Some ideas for the inclusion of
finite temperature effects can be gleaned from the extensive literature
on dipolar liquids [ 12 , 28 , 31 , 30 , 37 , 39 , 58 ] .

Dipole-dipole interactions among frozen Rydberg atoms may one day be
useful for quantum computing, as methods for producing entanglement in
such systems have already been studied theoretically [ 10 , 32 , 41 ] as
well as experimentally [ 42 ] . This method for quantum computing has
advantages over others that have been proposed. Entanglement of photon
pairs, for example, has the disadvantage that the production of such
pairs is not very efficient.

Experiments have been done recently that illustrate the creation of
ultracold plasmas using Rydberg atoms [ 34 , 36 ] , and even the
spontaneous evolution of Rydberg atoms into an ultracold plasma [ 52 ] .
Unlike in a high-temperature plasma where the kinetic energy of the
particles always dominates, an ultracold plasma can be strongly coupled
because the Coulomb interaction between neighboring particles can exceed
their kinetic energy. Therefore such plasmas are of theoretical interest
because they exhibit qualitatively different features than their
high-temperature counterparts.

Although we have averaged over random distributions of atoms in this
work, it is also possible to construct periodic optical potentials [ 26
, 29 , 33 , 61 ] and even quasiperiodic optical lattices [ 27 ] . In the
periodic case it is necessary to, instead of averaging, consider instead
the appropriate dipolar sums for regular lattices, which are discussed
in Refs. [ 13 ] and [ 20 ] .
