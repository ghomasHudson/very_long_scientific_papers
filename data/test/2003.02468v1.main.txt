# Abstract

The purpose of this thesis is to develop new theories on
high-dimensional structured signal recovery under a rather weak
assumption on the measurements that only a finite number of moments
exists. High-dimensional recovery has been one of the emerging topics in
the last decade partly due to the celebrated work of Candes, Romberg and
Tao (e.g. [ candes2006stable , candes2004robust ] ). The original
analysis there (and the works thereafter) necessitates a strong
concentration argument (namely, the restricted isometry property), which
only holds for a rather restricted class of measurements with
light-tailed distributions. It had long been conjectured that
high-dimensional recovery is possible even if restricted isometry type
conditions do not hold, but the general theory was beyond the grasp
until very recently, when the works [ mendelson2014learning ,
koltchinskii2015bounding ] propose a new “small-ball method”. In these
two papers, the authors initiated a new analysis framework for general
empirical risk minimization (ERM) problems with respect to the square
loss, which is “robust” and can potentially allow heavy-tailed loss
functions. The materials in this thesis are partly inspired by [
mendelson2014learning ] , but are of a different mindset: rather than
directly analyzing the existing ERMs for signal recovery for which it is
difficult to avoid strong moment assumptions, we show that, in many
circumstances, by carefully re-designing the ERMs to start with, one can
still achieve the minimax optimal statistical rate of signal recovery
with very high probability under much weaker assumptions than existing
works.

## Chapter 1 Introduction and a Heavy-tailed Framework

The main focus of this thesis is to study robust recovery and estimation
in the presence of heavy-tailed design or noises. In the analysis of
regression models and matrix estimation procedures, it is common to
assume that the data satisfy an certain model along with a set of
assumptions such as i.i.d. observations from a Gaussian distribution.
However, the data in practical world often violate such assumptions due
to noise and outliers. One of the viable ways to model noisy data and
outliers is to assume that the observations are generated by a
heavy-tailed distribution ¹ ¹ 1 Throughout the thesis, a distribution is
“heavy-tailed” if and only if finite number of moments exists. .
Therefore, the practical significance of this research is to relax the
strong assumptions ubiquitous in previous high-dimensional recovery and
estimation works, thereby reducing the gap between mathematical theories
and the real world problems.

### 1.1 Background

#### 1.1.1 From least square to supremum of an empirical process

Our main focus is the high-dimensional empirical risk minimization
(ERM). We start by considering the classical least squares ERM, which is
easy to understand and serves as a foundation for all subsequent
development of this thesis. Let @xmath be a measurable subset of @xmath
, let @xmath be a random vector, and let @xmath be a target response
variable. One would like to find some vector @xmath so that @xmath and
@xmath are as close as possible. A classical way of measuring the
distance is to consider the square loss function @xmath and one hopes to
select this @xmath so as to minimize the expected loss:

  -- -- --
        
  -- -- --

The term @xmath is irrelevant in terms of mimization. However, it should
be noted that in most cases, the expectations @xmath and @xmath are not
known . Instead, we only have access to the i.i.d. samples @xmath of
@xmath . Thus, we instead aim to find @xmath minimizing the empirical
loss :

  -- -------- -- -------
     @xmath      (1.1)
  -- -------- -- -------

It should also be note that there are two aspects of this problem. One
aspect is the estimation problem which aims to find some @xmath so that
@xmath is as small as possible. The other aspect is the prediction
problem, namely, given an estimator @xmath , we would like to know how
it performs on future data compared to @xmath , i.e.

  -- -- --
        
  -- -- --

This is also known as the “generalization error” of @xmath . Throughout
the thesis, we mainly focus on the estimation problem.

The classical way one analyzes the performance of ( 1.1 ) is as follows
( [ bartlett2005local ] ): since @xmath minimizes ( 1.1 ), it must
satisfy:

  -- -------- --
     @xmath   
  -- -------- --

Rearranging the terms gives:

  -- -------- --
     @xmath   
  -- -------- --

Thus, it follows that:

  -- -- -- -------
           (1.2)
  -- -- -- -------

The right hand side corresponds to the classical “bias-variance
decomposition”. When @xmath , the last term (which is the bias) is 0 and
we only have the variance term. It should be kept in mind though that in
general this bias term can be non-zero and increasing the bias in some
sense can actually help us control the variance, which will be discussed
in more details later.

If one believes that the matrix @xmath is invertible in the range of
@xmath , i.e.

  -- -------- -- -------
     @xmath      (1.3)
  -- -------- -- -------

for some absolute constant ² ² 2 Throughout the thesis, an absolute
constant is a constant that is independent of parameters of the problem.
@xmath and

  -- -- -- -------
           (1.4)
  -- -- -- -------

for some constant @xmath . Then, ( 1.2 ) implies

  -- -------- --
     @xmath   
  -- -------- --

However, there are only limited scenarios where ( 1.3 ) holds. It is
wrong, for example, when @xmath and @xmath spans @xmath . Furthermore,
the validity of ( 1.4 ), which essentially requires @xmath to be
uniformly concentrated around @xmath , is also questionable.

On the other hand, it is obvious that @xmath has to satisfy some
invertibility conditions in order to estimate @xmath . For example, when
@xmath lies in the null space of @xmath , asking for a bound on @xmath
is meaningless. Over the years, people have been trying to identify
minimal conditions so that objectives like ( 1.3 ) and ( 1.4 ) holds
true probabilistically, and our goal is to further expand the scope of
this line of research.

#### 1.1.2 Supremum of an empirical process: binary functions

It turns out that proving inequalities ( 1.3 ) and ( 1.4 ) belongs to a
more general class of problems, namely, bounding the supremum of an
empirical process. Historically, such kind of problems originates from
the well-known Glivenko-Cantelli theorem.

###### Theorem 1.1.1 (Glivenko-Cantelli).

Suppose @xmath is a sequence of independent and identically distributed
(i.i.d.) random variables on the probability space @xmath with a
cumulative distribution function (CDF) @xmath . Define the empirical CDF
as @xmath , where @xmath is the indicator function which is 1 if @xmath
and 0 otherwise. Then,

  -- -------- --
     @xmath   
  -- -------- --

with probability 1.

The class of random variables @xmath is historically called an empirical
process. Of course, one can show that the supremum is measurable (i.e.
@xmath is a random variable on the space @xmath , see [
durrett2019probability ] ), on which we will not discuss here. We
further refer readers to Chapter 1 of [ wellner2013weak ] for a
synthetic treatment of the measurability issue of the supremum. In the
absence of supremum (i.e. for a fixed @xmath ), this is just law of
large numbers. However, with the supremum, it is not immediately clear
why the convergence is still true. More generally, for any class of
(measurable) sets @xmath , one can ask if the following supremum always
converges to zero:

  -- -- --
        
  -- -- --

which turns out to be wrong, as is illustrated in the following simple
example:

###### Remark 1.1.1 (A non-Glivenko-Cantelli class).

Consider the following class of indicator functions: ³ ³ 3 This example
is from Peter Bartlett’s lecture notes:
https://www.stat.berkeley.edu/~bartlett/courses/2013spring-stat210b/notes/8notes.pdf
@xmath , where @xmath denotes the cardinality of the set @xmath . Then,
it can be easily seen that for any random variable @xmath with a
continuous distribution function @xmath , @xmath . However, we have
@xmath . Thus, the supremum does not converge to 0.

This example indicates that there has to be some measure of complexity
which indicates that the class of function @xmath is “too large” for the
supremum to converge, whereas @xmath is small. This type of complexity,
which appears very often in machine learning theory, is call Rademacher
complexity .

###### Definition 1.1.1.

Consider a set of samples @xmath and a function class @xmath containing
@xmath . The empirical Rademacher complexity of the function class
@xmath given @xmath is defined as

  -- -- --
        
  -- -- --

where @xmath being i.i.d. Rademacher random variables (taking @xmath and
@xmath with equal probability) and independent of @xmath .

We have the following general theorem from [ bartlett2002rademacher ] :

###### Theorem 1.1.2 (Theorem 5 of [bartlett2002rademacher]).

Let @xmath be a probability distribution on the product space @xmath ,
where @xmath is a set ⁴ ⁴ 4 In general this set does not have to be in
@xmath . We state this way mainly because we only care about finite
dimensional spaces in this thesis. . Let @xmath be a class of functions
containing @xmath . Let @xmath be i.i.d. samples drawn according to
@xmath , then, with probability at least @xmath , for every function
@xmath ,

  -- -- --
        
  -- -- --

Intuitively, @xmath measures the correlations of @xmath with random
noise, and if @xmath can fit noise very well, then, its complexity is
high. To use this theorem, one should be able to compute or upper bound
@xmath . One way is to apply the following theorem.

###### Theorem 1.1.3 (Theorem 6 of [bartlett2002rademacher]).

Fix any sequence of samples @xmath . For a function class @xmath
containing @xmath , define the restriction of @xmath to the samples as
follows:

  -- -------- -- -------
     @xmath      (1.5)
  -- -------- -- -------

Then,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is an absolute constant and @xmath denotes the cardinality
of the set @xmath .

This theorem can be proved by using the fact that @xmath is a
sub-Gaussian random variable, together with a union bound. Using this
lemma, one can easily prove the Glivenko-Cantelli theorem. To be more
specific, we let @xmath . One can show that @xmath , and thus, it
follows from Theorem 1.1.2 with probability at least @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

By Borel-Cantelli Lemma, we finish the proof. Thus, not only do we prove
the Glivenko-Cantelli theorem, we also get the explicit rate of
convergence @xmath , which is otherwise difficult to obtain from
“classical” proof (for example, in [ durrett2019probability ] ).
However, as we shall see, this @xmath is in fact not needed.

It turns out for a class of binary functions @xmath , Rademacher
complexity can be upper bounded by the well known complexity measure,
namely, the Vapnik-Chervonenkis(VC) dimension.

###### Definition 1.1.2 (VC dimension of sets).

Consider a class of sets @xmath in @xmath . For a sequence of samples
@xmath , we say @xmath shatters @xmath if

  -- -------- --
     @xmath   
  -- -------- --

The VC dimension of the class @xmath , denoted as @xmath , is defined as

  -- -------- --
     @xmath   
  -- -------- --

We also have the definition of VC dimension for a class of binary
functions @xmath :

###### Definition 1.1.3 (VC dimension for classification functions).

Consider a function class @xmath containing @xmath . The VC dimension of
the class @xmath , denoted as @xmath , is defined as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is defined in ( 1.5 ).

We have the following theorem:

###### Theorem 1.1.4 (Theorem 7 of [bartlett2002rademacher]).

Fix any sequence of samples @xmath . For a function class @xmath
containing @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is an absolute constant.

The proof of this theorem is highly non-trivial as it is a delicate
combination of Dudley’s entropy bound together with Haussler’s
inequality (see Chapter 2.6-2.7 of [ wellner2013weak ] ). One can see
immediately though by using this theorem instead, we can remove the log
factor in the earlier proof of Glivenko-Cantelli theorem.

#### 1.1.3 Supremum of an empirical process: General cases

In this section, we review some key results which bound supremum of a
classes of function with range in @xmath instead of @xmath . During the
last 80’s and 90’s, there has been tremendous progress in empirical
process theory, mostly associated with the name of Michel Talagrand, who
has made significant contributions on various aspects of concentration
of empirical processes including (but not limited to): Talagrand’s
isoperimetric inequality [ talagrand1995concentration ] , Talagrand’s
concentration inequality [ massart2000constants ] , contraction
principle [ ledoux2013probability ] and generic chaining [
talagrand2014upper ] . Several of his results will be in use throughout
this thesis.

We will take this opportunity trying to explain why Talagrand’s generic
chaining is of central importance in modern empirical process theory and
how it leads to a tight bound for the supremum of an empirical process.
To understand this, we start with the follow basic definition of
covering and packing numbers:

###### Definition 1.1.4 (Covering and packing numbers).

Consider a compact metric space cosisting of a set @xmath and a metric
@xmath ,

-    An @xmath -covering of @xmath under the metric @xmath is a
    collection of @xmath such that for all @xmath , there exists some
    @xmath with @xmath . The @xmath -covering number @xmath is the
    cardinality of the minimal @xmath -covering.

-    An @xmath -packing of @xmath under the metric @xmath is a
    collection of @xmath such that for all @xmath , @xmath . The @xmath
    -packing number @xmath is the cardinality of the maximal @xmath
    -packing.

It can be shown that covering and packing are (up to constant) the same
[ wellner2013weak ] :

  -- -------- --
     @xmath   
  -- -------- --

The covering number can also be expressed in terms of general sets as
opposed to metrics.

###### Definition 1.1.5 (Covering net for general sets).

Let @xmath be two sets in @xmath , the covering number @xmath is the
minimum number of translates of @xmath in order to cover @xmath .

It is obvious that when @xmath , @xmath is the unit ball under the
metric @xmath , then, @xmath .

The log of the covering number is also commonly referred to as the
entropy number. A classical way of estimating the covering number in
@xmath is the volume argument: Let @xmath be a subset of @xmath , then,
it is not difficult to see that (Proposition 4.2 of [
vershynin2010lectures ] ):

  -- -------- -- -------
     @xmath      (1.6)
  -- -------- -- -------

where @xmath is the Euclidean @xmath volume of the set @xmath . In
particular, this implies for @xmath being the unit ball under the metric
@xmath in @xmath ,

  -- -- --
        
  -- -- --

However, in general, the volume argument can be suboptimal and sometimes
difficult to compute. A somewhat easier way to bound the covering number
is through Sudakov inequality. We need the following definition:

###### Definition 1.1.6 (Gaussian mean width).

Let @xmath be a set in @xmath and let @xmath . The Gaussian mean width
of the set @xmath is @xmath .

The quantity @xmath is crucial in learning theory. Intuitively, it
measures the average width of a set. One can easily check when @xmath
being a unit ball in the @xmath dimensional subset of @xmath , @xmath ,
and when @xmath is the cross-polytope, i.e. @xmath , @xmath for some
absolute constant @xmath .

The following is the well-known Sudakov inequality:

###### Theorem 1.1.5 (Theorem 2.2 of [vershynin2010lectures]).

Let @xmath be a unit ball in @xmath . For every symmetric convex set
@xmath , we have @xmath , where @xmath is an absolute constant.

One might wonder if it is possible to reverse the Sudakov inequality and
derive an upper bound on @xmath , i.e. the supremum of a Gaussian
process, in terms of covering numbers. This turns out to be a highly
non-trivial task. The technique bounding the supremum via covering nets
is commonly referred to as chaining . Intuitively, chaining is a method
of taking fine-grained union bounds on sets of infinite cardinality
through progressively finer covering nets. We start by defining the
sub-Gaussian process :

###### Definition 1.1.7.

A zero mean stochastic process @xmath with respect to a metric @xmath in
@xmath is called sub-Gaussian, if for every @xmath , and any @xmath ,

  -- -- --
        
  -- -- --

For sub-Gaussian processes, we have the following key result due to R.
Dudley. The technique proving this theorem is commonly referred to as
Dudley’s chaining:

###### Theorem 1.1.6 (Dudley’s entropy integral (Corollary 2.2.8 of
[wellner2013weak])).

Consider a zero mean sub-Gaussian stochastic process @xmath with respect
to a metric @xmath in @xmath . Then,

  -- -- --
        
  -- -- --

One might wonder how tight this bound is. The following (not so trivial)
example indicates that this bound is far from being tight.

###### Remark 1.1.2 (A difficult set for Dudley’s entropy integral).

This example can be found as an exercise in Chapter 2.2 of [
talagrand2014upper ] . Consider the Gaussian mean width @xmath of the
probability simplex:

  -- -------- -- -------
     @xmath      (1.7)
  -- -------- -- -------

where @xmath is entrywise. It is easy to check that @xmath for some
absolute constant @xmath . Now, compute the Dudley’s entropy integral
with @xmath being the @xmath -norm. One can show that (somewhat
surprisingly)

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is some absolute constant. Thus, Dudley’s integral is off
by a factor of @xmath .

One way to prove the previous remark is to rewrite the Dudley integral
in another form. We consider a sequence of subsets @xmath with the
condition that @xmath where

  -- -------- --
     @xmath   
  -- -------- --

For any @xmath , define @xmath . Note right away we have @xmath , @xmath
and the function @xmath is related to the fact that in some sense this
is the inverse of the function @xmath that governs the size of the tails
of a Gaussian random variables. Define the entropy number @xmath as

  -- -------- --
     @xmath   
  -- -------- --

where the infimum is taken over all possible admissible sequences.

###### Lemma 1.1.1 (Lemma 2.2.11 of [talagrand2014upper]).

Under the aforementioned conditions, there exists an absolute constant
@xmath such that

  -- -------- --
     @xmath   
  -- -------- --

Then, one could lower bound the entropy integral by the left hand side
and lower bound the sum by a properly constructed subset of the
probability simplex ( 1.7 ) (e.g. one can take subsets @xmath of @xmath
consisting of sequences @xmath for which @xmath .)

Note that combining Lemma 1.1.1 with Theorem 1.1.6 one readily get

  -- -- --
        
  -- -- --

The key contribution of Talagrand is to realize that, surprisingly, if
we exchange @xmath with the sum, then, this bound is tight! To make this
rigorous, we need the following definition of admissible sequence:

###### Definition 1.1.8 (Admissible sequence).

Given a metric space @xmath . We say a sequence of subsets @xmath of
@xmath is increasing if @xmath . A sequence of subsets @xmath is
admissible if it is increasing and satisfy the condition that @xmath
where

  -- -------- --
     @xmath   
  -- -------- --

###### Definition 1.1.9 (Talagrand functionals).

Given a constant @xmath and a metric space @xmath . The Talagrand @xmath
functional is defined as

  -- -------- --
     @xmath   
  -- -------- --

where the infimum is taken over all possible admissible sequences @xmath
.

We are now ready to state the main theorem due to Talagrand:

###### Theorem 1.1.7 (Talagrand majorizing measure theorem).

Consider a centered Gaussian process @xmath index by the set @xmath and
the metric @xmath defined by

  -- -- --
        
  -- -- --

There exists some absolute constant @xmath such that

  -- -- --
        
  -- -- --

Throughout the thesis, the @xmath -norm of a random variable @xmath is
defined as @xmath .

###### Definition 1.1.10.

A random variable @xmath is @xmath sub-Gaussian if @xmath . The
corresponding sub-Gaussian norm ( @xmath -norm) is defined as @xmath .

###### Definition 1.1.11 (Subgaussian random vector).

A random vector @xmath is @xmath sub-Gaussian if the collection random
variables @xmath are @xmath sub-Gaussian. The corresponding sub-Gaussian
norm of the vector @xmath is then given by

  -- -- --
        
  -- -- --

For sub-Gaussian processes, we have

###### Theorem 1.1.8 (Theorem 2.2.18 of [talagrand2014upper]).

Consider a centered sub-Gaussian process @xmath index by the set @xmath
and the metric @xmath defined by

  -- -- --
        
  -- -- --

We have

  -- -- --
        
  -- -- --

and

  -- -------- --
     @xmath   
  -- -------- --

Throughout the thesis, we seldom encounter any exact computation and our
bounds are always in terms of unspecified absolute constants.
Furthermore, the constants (for example, @xmath and @xmath ) can be
different per occurrence.

#### 1.1.4 Other key inequalities

Let @xmath be a semi-metric space, and let @xmath be independent
stochastic processes indexed by @xmath such that @xmath for all @xmath
and @xmath . We are interested in bounding the supremum of the empirical
process

  -- -- -- -------
           (1.8)
  -- -- -- -------

The following well-known symmetrization inequality reduces the problem
to bounds on a (conditionally) Rademacher process @xmath , where @xmath
are i.i.d. Rademacher random variables (meaning that they take values
@xmath with probability @xmath each), independent of @xmath ’s.

###### Lemma 1.1.2 (Symmetrization inequalities).

  -- -------- --
     @xmath   
  -- -------- --

and for any @xmath , we have

  -- -- --
        
  -- -- --

See Lemmas 6.3 and 6.5 in [ ledoux2013probability ] for proofs.

###### Lemma 1.1.3 (Bernstein’s inequality [wellner2013weak]).

Let @xmath be a sequence of independent centered random variables.
Assume that there exist positive constants @xmath and @xmath such that
for all integers @xmath

  -- -- --
        
  -- -- --

then

  -- -- --
        
  -- -- --

In particular, if @xmath are all sub-exponential random variables, then
@xmath and @xmath can be chosen as @xmath and @xmath .

###### Lemma 1.1.4 (Contraction principle [ledoux2013probability]).

Let @xmath be a sequence of samples in @xmath and let @xmath be a class
of functions containing @xmath . Let @xmath be a sequence of @xmath
-Lipschitz functions for some @xmath , then, we have

  -- -- --
        
  -- -- --

###### Lemma 1.1.5 (Contraaction principle [ledoux2013probability]).

Let @xmath be a sequence of samples in @xmath , let @xmath be a class of
functions containing @xmath , and let @xmath be a sequence of real
numbers (possibly depends on the samples) such that @xmath . We have for
any @xmath ,

  -- -- --
        
  -- -- --

###### Lemma 1.1.6 (Paley-Zygmund inequality [paley1930some]).

Suppose @xmath is a random variable with finite variance and @xmath ,
then,

  -- -- --
        
  -- -- --

Finally, the following lemma is crucial in the analysis of heavy-tailed
processes which is sometimes referred to as the Montgomery-Smith
inequality:

###### Lemma 1.1.7 ([montgomery1990distribution]).

Let @xmath be a sequence of scalars. Define the following quantity:

  -- -- --
        
  -- -- --

Then, we have

  -- -- -- -------
           (1.9)
  -- -- -- -------

Furthermore, there exists a universal constant @xmath such that

  -- -- --
        
  -- -- --

where @xmath is the non-increasing rearrangement of @xmath and @xmath is
a sequence of i.i.d. Rademancher random variables independent of @xmath
.

#### 1.1.5 Gordon’s theorem and bounds on the estimation error

Let’s go back to the least squares ERM discussed at the beginning and
see how to perform a rigorous analysis on the estimation error. We start
with ( 1.2 ) and assume the bias is 0. Further assume that @xmath are
i.i.d. Gaussian vectors from @xmath , and the noise @xmath for some
absolute constant @xmath . Recall the following Gordon’s “escape through
the mesh” theorem:

###### Theorem 1.1.9 (Gordon’s theorem (Corollary 1.2 of
[gordon1988milman])).

Let @xmath be a closed subset of unit sphere, and let matrix @xmath be a
@xmath entry-wise i.i.d. random matrix drawn from a standard Gaussian
distribution @xmath . Then, for any @xmath ,

  -- -- --
        
  -- -- --

where @xmath .

Note that we have @xmath . Let @xmath and @xmath is the sphere centered
at the origin with radius @xmath , i.e. @xmath . Furthermore, define the
descent cone of a set @xmath at some point @xmath as

  -- -------- --
     @xmath   
  -- -------- --

Note that for any vector @xmath , @xmath . Thus, we consider the
following infimum:

  -- -- --
        
  -- -- --

Using Gordon’s theorem, we readily have

  -- -- --
        
  -- -- --

with probability at least @xmath . Suppose @xmath , then, the above
quantity is no less than @xmath and it follows with probability at least
@xmath ,

  -- -------- -- --------
     @xmath      (1.10)
  -- -------- -- --------

On the other hand, for the right hand side of ( 1.2 ), we would like to
upper bound

  -- -- --
        
  -- -- --

By symmetrization inequality (Lemma 1.1.2 ), it is enough to consider

  -- -------- --
     @xmath   
  -- -------- --

where @xmath ’s are i.i.d Rademacher random variable. Since @xmath , by
contraction principle (Lemma 1.1.5 ), it is enough to consider

  -- -------- --
     @xmath   
  -- -------- --

Using Theorem 1.1.8 , we readily get with probability at least @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is some absolute constant. Thus, with probability at least
@xmath , where @xmath is some absolute constant,

  -- -- --
        
  -- -- --

Overall, combining this inequality with ( 1.10 ), we conclude with the
following theorem, which can also be found, for example, in [
rudelson2008sparse ] :

###### Theorem 1.1.10.

Suppose @xmath are i.i.d. Gaussian vectors from @xmath , and the noise
@xmath for some absolute constant @xmath . For any @xmath , if @xmath ,
then with probability at least @xmath , the solution to minimizing ( 1.1
) satisfies

  -- -------- --
     @xmath   
  -- -------- --

Note that such a quantity measures the “true” complexity of estimating
@xmath in the sense that the Gaussian mean width of a set can be much
smaller than the ambient dimension of that set. For example, one can
apply this theorem to sparse recovery problems and easily obtain a
minimax optimal rate. More specifically, the work [
chandrasekaran2012convex ] shows that when taking @xmath , i.e. the ball
of @xmath with radius @xmath , and @xmath is @xmath -sparse, we have
@xmath is on the order of @xmath . Thus, instead of having number of
samples @xmath scales with the dimension @xmath , we only need the
sample to scale with the sparsity level @xmath in order to get an
accurate estimation, which is in fact minimax optimal.

#### 1.1.6 Theorem 1.1.10 is restrictive

Despite the simplicity of proving Theorem ( 1.1.10 ), it is fairly
restrictive due to Gaussian measurements and bounded noise assumptions.
One might wonder if these two assumptions are really necessary. The
short answer is that they cannot be much relaxed if we would like to
more or less keep the same idea of analysis. The reason is that proving
Gordon’s theorem for general measurements is difficult. It is known that
one can significantly relax the Gaussian assumption for special sets
(For example, unit ball in @xmath [ mendelson2012generic ] ). For
general sets, it is recently established in [ liaw2017simple ] that one
can recover Theorem ( 1.1.10 ) using sub-Gaussian measurements, but with
inexplicit constants. For measurements that have heavier tails than
Gaussian, such a result is not known and likely untrue.

However, a closer look at the proof indicates that only a lower bound of
@xmath is needed whereas Gordon’s theorem provides a double sided bound.
As a simple example, we look at bounds like

  -- -- --
        
  -- -- --

as oppose to

  -- -- --
        
  -- -- --

Obviously, there are huge differences between these two inequalities.
Intuitively, large values on @xmath might ruin the second inequality, it
only helps with the first inequality. An example demonstrating this fact
is as follows:

###### Remark 1.1.3 (Differences between upper and lower bounds
[mendelson2014learning]).

Fix an integer @xmath and consider a sequence of i.i.d. random variables
@xmath such that each @xmath takes @xmath with probability @xmath and
takes @xmath with probability @xmath . We have

  -- -- --
        
  -- -- --

With probability at least @xmath , there exists some @xmath such that
@xmath , which implies @xmath . Thus, we have

  -- -- --
        
  -- -- --

On the other hand, if we consider the lower bound only, then, using
Chernoff’s inequality, we obtain

  -- -- --
        
  -- -- --

where @xmath is an absolute constant.

An immediate consequence of these observations is that the standard
method of analysis for the estimation problem, which is based on a
two-sided concentration argument that holds with exponential
probability, can never work in heavy-tailed situations. Thus, one must
find a different argument altogether if one wishes to deal with learning
problems that include classes of heavy-tailed functions or with a
heavy-tailed target.

### 1.2 Small-ball Method

#### 1.2.1 A general theorem

A key contribution in [ mendelson2014learning , koltchinskii2015bounding
] is a completely new method bounding the lower tail on the infimum of
the quadratic form @xmath without concentration. As is mentioned in [
mendelson2014learning ] , the term “without concentration” should be
understood in the sense of “when the concentration is false”, as oppose
to “concentration methods are not needed and will not take any part in
the analysis of ERM”. To state the main theorem, we need the following
definition, so called “small-ball condition”.

###### Definition 1.2.1.

A random vector @xmath is said to satisfy the small-ball condition over
a set @xmath if for any @xmath , there exist positive constants @xmath
and @xmath so that

  -- -- --
        
  -- -- --

To see how weak the small-ball condition is, we consider a random vector
@xmath satisfying @xmath and the @xmath equivalence condition, i.e.
@xmath , @xmath , where @xmath is an absolute constant. By Paley-Zygmund
inequality, for any @xmath ,

  -- -- --
        
  -- -- --

Thus, small-ball condition does allow heavy-tailed random vectors. The
key theorem by Mendelson is as follows:

###### Lemma 1.2.1 ([mendelson2014learning]).

Let @xmath and define the empirical mean width

  -- -- --
        
  -- -- --

Suppose @xmath , then, it follows

  -- -- --
        
  -- -- --

with probability at least @xmath for any @xmath .

#### 1.2.2 Application to least squares ERM

Lemma 1.2.1 is very powerful and applicable to analysis of many
different loss functions. Here, we will show how it helps in the
estimation error analysis of minimizing ( 1.1 ). We assume that the
measurement @xmath satisfies @xmath and the @xmath equivalence
condition, i.e. @xmath , @xmath , where @xmath is an absolute constant.
Again, we consider the following infimum:

  -- -- --
        
  -- -- --

By Paley-Zygmund inequality, we have

  -- -- --
        
  -- -- --

Applying Lemma 1.2.1 , we readily have

  -- -- --
        
  -- -- --

with probability at least @xmath , where

  -- -- -- --------
           (1.11)
  -- -- -- --------

is the empirical mean width. Suppose

  -- -- --
        
  -- -- --

then, it follows with probability at least @xmath ,

  -- -------- -- --------
     @xmath      (1.12)
  -- -------- -- --------

On the other hand, define @xmath , and define another empirical width:

  -- -- -- --------
           (1.13)
  -- -- -- --------

from which we have

  -- -- --
        
  -- -- --

Overall, we obtain the following theorem:

###### Theorem 1.2.1.

Suppose @xmath are @xmath equivalence condition, i.e. @xmath , @xmath ,
where @xmath is an absolute constant. For any @xmath , if

  -- -- --
        
  -- -- --

then with probability at least @xmath , the solution to minimizing ( 1.1
) satisfies

  -- -------- --
     @xmath   
  -- -------- --

Bounds of this flavor via the small-ball method can be found, for
example, in [ tropp2015convex ] . To apply this theorem to specific
problems, we need to compute the two quantities ( 1.11 ) and ( 1.13 ).
One might wonder if anything can be said regarding the general
properties of these two empirical quantities. It turns out when both
@xmath and @xmath are sub-Gaussian, we recover Theorem 1.1.10 up to
constant via the following theorem:

###### Theorem 1.2.2 (Lemma 3.2 of [goldstein2018non]).

Suppose @xmath is an isotropic sub-Gaussian random vector and @xmath is
a sub-Gaussian random variable. Suppose @xmath , then, with probability
at least @xmath ,

  -- -- --
        
  -- -- --

where @xmath is an absolute constant.

This theorem gives a bound on @xmath . For the term @xmath , one can
simply invoke Theorem 1.1.8 ,

  -- -- --
        
  -- -- --

where @xmath is an absolute constant. Overall, we obtain the following
corollary of Theorem 1.2.1 .

###### Corollary 1.2.1.

Suppose @xmath is an isotropic sub-Gaussian random vector, @xmath is a
sub-Gaussian random variable, and

  -- -- --
        
  -- -- --

then, for any @xmath , with probability at least @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath are absolute constants.

However, in general, when @xmath and @xmath exhibit heavier tails than
Gaussian, it is highly non-trivial to bound ( 1.11 ) and ( 1.13 ) in
terms of Gaussian mean width. It is an active research area and we will
introduce several methods later to bound them.

### 1.3 Organization of the Thesis

The rest of the thesis is organized as follows. In Chapter 2, we
introduce a new adaptively thresholded ERM for generalized linear model
with a new analysis framework, which refines the results from an earlier
draft [ wei2018structured ] . Special attention is devoted to recovering
an approximately sparse vector in @xmath -ball as well as bounded sparse
vectors with the minimax statistical rates under a rather weak
assumption that the design vector has more than @xmath moments. This
result significantly improves the previously known results which require
@xmath moments ( @xmath being the dimension of the vector). In Chapter
3, we show that if one knows the design vectors are sampled from a
specific class of distributions, then, a somewhat simpler analysis with
even weaker assumptions is possible [ goldstein2016structured ] [
goldstein2019non ] . In particular, we show that when the design vectors
are elliptical symmetric with more than 2 moments, then, one can
recovery a structured signal (up to constant scaling) with minimax rate
from measurements with unknown nonlinear transformations. Finally, in
Chapter 4, we look at a problem with a somewhat different flavor,
namely, the robust covariance matrix estimation. We show that a
Huber-type estimator achieves the minimax optimal statistical rate with
more than 4 moments on the samples [ wei2017estimation ] [
minsker2020robust ] .

## Chapter 2 Optimal Statistical Rate in Generalized Linear Models under
Weak Moment Assumptions

In this Chapter, we consider the scenario of high-dimensional estimation
in generalized linear models (GLMs). While high-dimensional recovery
problems have been studied extensively under the sub-Gaussian
assumption, much less is known in the case of heavy-tailed measurements,
such as those with moments of only constant order. In this paper, we
propose and analyze new thresholding methods recovering high-dimensional
structured vectors from nonlinear measurements under very weak
assumptions on the underlying distributions. In particular, we show
that, by solving a convex program, the proposed method achieves the
minimax statistical rate of estimation in @xmath -ball with only @xmath
moments on the design vectors. Our results improve upon the best known
analysis on the convex methods for ordinary linear models, i.e. LASSO
type estimators, which require @xmath moments to achieve the minimax
optimal statistical rate.

### 2.1 Introduction

We study a general model where the response @xmath is linked to the
covariate @xmath via a generalized linear model through a canonical link
function. More specifically, we assume @xmath satisfies the following
distribution

  -- -- -- -------
           (2.1)
  -- -- -- -------

where @xmath is a known scalar parameter and @xmath is a known mapping.
The vector @xmath is unknown to be estimated and @xmath is the link
function. Using the standard properties of an exponential family ( [
brown1986fundamentals ] ), we know that the function @xmath is twice
differentiable and @xmath is strictly positive on the realline. In
particular, this implies the function @xmath is a strictly convex
function. ¹ ¹ 1 This should be distinguished from the more restricted
class of strongly convex functions for which there is a positive lower
bound @xmath such that @xmath . On the other hand, for a strictly convex
function, there is no such a uniform lower bound. Some examples of GLMs
are as follows:

-   The ordinary linear model, i.e. @xmath with @xmath , corresponds to
    the condition distribution of @xmath being a Gaussian distribution
    with mean @xmath and variance @xmath . More specifically, we have
    @xmath and @xmath .

-   The logistic regression model corresponds to @xmath being a
    Bernoulli random variable (taking values in @xmath ). More
    specifically, we have @xmath and @xmath . In particular, we have

      -- -- --
            
      -- -- --

-   The poisson regression model corresponds to @xmath being a Poisson
    distribution taking values in @xmath and @xmath and @xmath .

The goal is to estimate the true parameter @xmath from a sequence of
@xmath samples @xmath . When assuming @xmath possesses certain structure
which tends to make the corresponding norm function @xmath small, one
proposes to estimate @xmath via the following maximum likelihood (ML)
with regularization:

  -- -- -- -------
           (2.2)
  -- -- -- -------

In particular, if @xmath is an approximately sparse vector, then, the
usual choice for @xmath is @xmath .

Note that in general, there is a sharp contrast between ordinary linear
model and the GLMs from an analysis perspective. For linear model, the
analysis in the previous chapter demonstrates that an important step of
controlling the error is to argue that the smallest eigenvalue of the
covariance matrix @xmath is away from zero in certain restricted area.
However, the same argument does not work here since the quadratic
component in least squares ERM is now replaced by @xmath , where @xmath
is only approximately quadratic on compact sets and it is not always
possible to bound @xmath by a quadratic form.

The difference is even more significant if we further assume that the
covariance matrix of @xmath is known, i.e. we know @xmath and it is
positive definite. Consider again the ordinary linear problem. Since we
know the covariance, instead of ( 1.1 ), we consider using the following
ERM problem:

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

We then show this objective is much easier to analyze. To start, we have

  -- -------- --
     @xmath   
  -- -------- --

Rearranging terms gives

  -- -- --
        
  -- -- --

where the expectation is taken given the @xmath samples @xmath and we
use the fact that @xmath . Since the covariance matrix is positive
definite, we have

  -- -- --
        
  -- -- --

As a consequence, we refrain from bounding the smallest eigenvalue of
the empirical covariance matrix completely and small-ball method is
never needed. This method was first proposed in the seminal work [
koltchinskii2011nuclear ] which deals with a low-rank matrix regression.
However, this very method cannot be extended to analyzing objectives
with general convex functions such as ( 2.2 ).

Of course knowing the covariance matrix and solving problems like ( 2.3
) can be unrealistic depending on the application. For example, in a
typical image classification problem [ deng2009imagenet ] , we are given
a series of image samples and several class hypotheses. We would like to
known which class they belong to. In such a scenario, it is unclear how
one is able to obtain the population covariance of the samples and the
notion of “population covariance” might not even be well-defined.

#### 2.1.1 Related works

The ordinary linear model with @xmath being an @xmath -sparse vector and
@xmath corresponds to the classical compressed sensing problem. Over the
past two decades, compressed sensing has been thoroughly studied under
the assumption that the measurement vectors are isotropic subgaussian
and the noise is also subgaussian, e.g. [ tibshirani1996regression ,
candes2006stable , candes2008restricted , bickel2009simultaneous ,
hastie2015statistical ] . It is shown that when each row of the the
measurement matrix @xmath is sub-Gaussian, @xmath , then, the restricted
isometric property (RIP) holds over all @xmath -sparse vectors @xmath ,
i.e. there exists a fixed constant @xmath , @xmath . Then, one can show
that by solving the LASSO: @xmath , one can achieve the following
optimal error rate: @xmath Estimation of sparse vectors in generalized
linear model via ( 2.2 ) with a similar statistical rate is also proved
in the work [ negahban2012unified ] .

As is mentioned in the previous chapter, the sub-Gaussian assumption is
restrictive, but RIP does not necessarily hold with the optimal sample
rate @xmath when the tail of @xmath decays slower than sub-Gaussian. The
crux lies in the fact that RIP simultaneously requires upper bounds on
the quadratic form, which is not needed in the proof of performance in
sparse recovery. Extending the small-ball method originally proposed in
[ koltchinskii2015bounding ] , the work [ lecue2017sparse ] shows that
by assuming the condition that @xmath has sub-Gaussian property up to
only @xmath moments, i.e. @xmath where @xmath is an absolute constant,
one can achieve the same aforementioned sample and error rates with high
probability by solving the LASSO. Furthermore, the work [
lecue2017regularization ] shows that the same @xmath moments assumption
also leads to minimax optimal estimation of an approximate sparse signal
in the @xmath -ball instead of exact sparse signals. Outlier robust
methods for sparse recovery based on the median-of-mean (MOM) estimators
is also proposed and analyzed in several works (e.g. [ lecue2017robust ,
lugosi2016risk ] ) but they generally require solving a highly
non-convex program with @xmath type moment assumptions on the
measurement vectors in order to get the optimal rate.

Our goal in this chapter is to further relax @xmath moment assumption
for optimal @xmath -ball recovery to just a constant moment requirement,
which we termed “weak moment assumption”, and at the same time allow
GLMs instead of just ordinary linear model. Recently, the works [
Fan-robust-estimation-2017 ] and [ Fan-huber-regression-2017 ] propose a
new class of thresholded estimators for sparse recovery, based on the
earlier work [ catoni2012challenging ] on adaptive shrinkage for
heavy-tailed mean estimation. While their methods are quite effective
when dealing with the heavy-tailed noise, the sample rate is suboptimal
when it comes to heavy-tailed measurement vectors.

### 2.2 Main Results

#### 2.2.1 Optimal Estimation in @xmath-ball

Throughout the chapter, we adopt the following assumption on the
measurements:

###### Assumption 2.2.1.

The samples @xmath are i.i.d. copies of @xmath with @xmath , satisfying
the model ( 2.1 ). For some absolute constants @xmath , there exist
corresponding constants @xmath such that

1. Bounded kutosis: @xmath .

2. Bounded moments: @xmath and @xmath @xmath .

3. Non-degeneracy: @xmath .

Our result in this section concerns with the estimation in @xmath -ball:

###### Assumption 2.2.2.

The true parameter @xmath .

Note that the set @xmath includes all bounded vectors that tend to be
small in the @xmath -norm ball (but not necessarily exactly sparse). The
benchmark we will compare to is the following minimax lower bound on
estimation within @xmath via Gaussian measurements:

###### Theorem 2.2.1 (Theorem 1 of [raskutti2011minimax]).

Consider the ordinary linear model, i.e. @xmath with @xmath and @xmath .
Suppose Assumption 2.2.2 holds and @xmath for some absolute constant
@xmath , then,

  -- -- --
        
  -- -- --

for some absolute constant @xmath .

Note that an underlying assumption in this theorem (which is not
explicit in [ raskutti2011minimax ] ) is that the the number of of
measurements @xmath for some absolute constant @xmath . ² ² 2 It is easy
to see when @xmath , @xmath and the minimax lower bound in this region
should be @xmath , which is achieved by the least squares regression.
Our goal would be to design an estimator achieving this rate for GLMs (
2.1 ) under Assumption 2.2.1 and 2.2.2 . Our robust estimator involves
generating the adapted truncated measurements @xmath from the samples
@xmath and solving the following problem:

  -- -- -- -------
           (2.4)
  -- -- -- -------

where @xmath is a trade-off parameter to be determined later and @xmath
for the @xmath -ball recovery problem. We take @xmath such that

  -- -- -- -------
           (2.5)
  -- -- -- -------

where @xmath .

Next, we will describe conditions on the link function @xmath in ( 2.1
), which trivially holds for the ordinary linear models.

###### Assumption 2.2.3.

There exists some constant @xmath such that the Hessian of the cumulant
function is uniformly bounded, i.e. @xmath .

The following is our main result.

###### Theorem 2.2.2.

Suppose Assumptions 2.2.1 , 2.2.2 , 2.2.3 hold. Let

  -- -------- --
     @xmath   
  -- -------- --

Suppose @xmath , @xmath . Then, with probability at least

  -- -- --
        
  -- -- --

for some absolute constant @xmath , we have

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

for any @xmath , where @xmath and @xmath are constants depending
polynomially on the parameters @xmath .

###### Remark 2.2.1.

Theorem 2.2.2 shows that our proposed method can attain the minimax
statistical rate when @xmath , and it does so without knowing how large
@xmath is. This result also (up to constants) matches previous bounds on
@xmath -ball estimation which in general require stronger moment
assumptions. For example, Theorem 4.2 of [ lecue2017regularization ]
shows when the model is linear and @xmath , one can attain the minimax
rate with @xmath moments on the measurement vector @xmath .

#### 2.2.2 Optimal Estimation of Bounded Sparse Vectors

In this section, we show a result regarding optimal estimation of sparse
vectors in a bounded range in the presence of heavy-tailed measurements.
More specifically, we consider the following set of vectors:

###### Assumption 2.2.4.

The true parameter @xmath , where @xmath denotes the set of @xmath
-sparse vectors and @xmath is the unit @xmath -norm ball.

The benchmark we compare to is the following lower bound:

###### Theorem 2.2.3.

Consider the ordinary linear model, i.e. @xmath with @xmath and @xmath .
Suppose @xmath , @xmath , and @xmath for some absolute constant @xmath ,
then,

  -- -- --
        
  -- -- --

for some absolute constant @xmath .

This lower bound is somewhat different from known lower bounds (e.g. [
raskutti2011minimax ] ) in the sense that it considers a restricted
candidate set of sparse vectors in a bounded set @xmath instead of all
sparse vectors. Nevertheless, Theorem 2.2.3 shows that imposing such a
restriction does not make the problem easier. To show why it is true, we
need the following definition:

###### Definition 2.2.1 (Local packing number).

Given a set @xmath , the local packing number @xmath is the packing
number of @xmath with balls of radius @xmath .

Theorem 2.2.3 is a corollary of the following theorem:

###### Theorem 2.2.4 (Theorem 4.2 of [plan2016high]).

Assume that @xmath where @xmath is a star-shaped subset of @xmath .
Assume that @xmath with @xmath and @xmath . Let

  -- -- --
        
  -- -- --

Then, there exists an absolute constant @xmath such that any estimator
@xmath which depends only on the observations @xmath and @xmath
satisfies

  -- -- --
        
  -- -- --

Now, using this theorem, it is enough to compute @xmath in our problem
with @xmath and @xmath , for which one can show the following:

###### Lemma 2.2.1.

When @xmath and @xmath , @xmath where @xmath is an absolute constant.

###### Proof of Lemma 2.2.1.

The proof of this lemma follows from ideas in Section 4.3 of [
plan2016high ] . To compute @xmath for @xmath , it is enough to consider
@xmath packing of @xmath . Consider a set @xmath , which contains
vectors of @xmath cardinality, where each nonzero entry is equal to
@xmath . Thus, @xmath . We will show that there exists a subset @xmath
such that @xmath , @xmath . Consider picking vectors @xmath uniformly at
random and compute the probability of the event @xmath . When the event
happens, it requires @xmath and @xmath to have at least @xmath matching
non-zero coordinates. Assume without loss of generality that @xmath is
an integer, this event happens with probability

  -- -- --
        
  -- -- --

Using Stirling’s approximation and @xmath , we have @xmath , where
@xmath is an absolute constant. This implies the claim when choose
@xmath to have @xmath uniformly chosen vectors from @xmath , which
satisfies @xmath , @xmath with a constant probability. ∎

Thus, by Lemma 2.2.1 , it follows that

  -- -- --
        
  -- -- --

When @xmath , the claim in Theorem 2.2.3 follows.

Our main result in this section is the following theorem:

###### Theorem 2.2.5.

Suppose Assumption 2.2.1 , 2.2.3 , 2.2.4 hold. Let @xmath , where @xmath
, @xmath and @xmath . Suppose @xmath , @xmath . Then, with probability
at least

  -- -- --
        
  -- -- --

for some absolute constant @xmath , we have

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

for any @xmath , where @xmath and @xmath are constants depending
polynomially on the parameters @xmath .

### 2.3 Proof of Theorems: A Heavy-tailed Framework

In this section, we provide a general analysis on ERM of the form ( 2.4
) which can also be applied to problems beyond @xmath -regularization,
and show that to control the estimation error, it is enough to control
local complexities around the true vector @xmath . Our procedure here is
an extension of the small-ball method proposed in the works [
lecue2017regularization , lecue2017sparse , mendelson2014learning ] ,
and the difference lies in the treatment of a general function @xmath as
well as the bias caused by the thresholding.

For the rest of the paper, the notations @xmath , @xmath denote the ball
of radius @xmath centered at @xmath for @xmath -norm, 2-norm
respectively, and @xmath , @xmath denote the sphere of radius @xmath
centered at @xmath for @xmath -norm, 2-norm respectively. We omit @xmath
if they are centered at the origin.

We start with the usual optimality analysis of the ERM. Since @xmath is
the solution to ( 2.4 ), we have

  -- -- --
        
  -- -- --

Simple algebraic manipulations give

  -- -- -- -------
           (2.6)
  -- -- -- -------

To simplify the notations, for any @xmath , define

  -- -------- -- --
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

In addition, for any Borel measurable function @xmath , @xmath . Let

  -- -- -- -------
           (2.7)
  -- -- -- -------

Having defined these notations, the criterion ( 2.6 ) simply implies
@xmath . Our goal is then to show that for any @xmath such that @xmath ,
where @xmath is a certain bounding radius, then,

  -- -- --
        
  -- -- --

The intuition why one would expect this to happen is as follows. Suppose
@xmath is not a smooth function near @xmath and the set of
sub-differentials of the norm function @xmath near @xmath (which we
denote as @xmath ) is “large”, then, the set of descent directions i.e.
@xmath would be relatively small. ³ ³ 3 The descent cone and the cone of
sub-differentials are dual to each other. This implies

-   For @xmath not in the descent directions, @xmath , and for an
    appropriate choice of @xmath , the possibly negative linear terms
    @xmath would be dominated by @xmath .

-   For the set of @xmath in the descent directions, we would expect the
    term @xmath to dominate the linear terms @xmath . Using the strictly
    convex property, for a sufficiently small set of descent directions
    intersecting with a bounded region, @xmath would be a
    non-degenerated quadratic form (i.e. @xmath for some constant @xmath
    ), which dominates the linear terms @xmath and @xmath for all @xmath
    sufficiently away from @xmath within this bounded region. We then
    extend this result to any vector sufficiently away from @xmath via
    convexity of @xmath .

To this point, we invoke an idea from [ lecue2017sparse ] and consider
the intersection of an @xmath -ball @xmath and a @xmath -ball @xmath ,
with a properly chosen @xmath , and we aim to show that if @xmath is
outside of @xmath with appropriate choices of @xmath and @xmath , then,
@xmath . As is shown in Fig. 2.1 , having this intersection essentially
divides the space outside of @xmath into two types of regions: 1. The
region containing the set of descent directions @xmath , where the term
@xmath is expected to take effect. 2. The region where @xmath , and the
term @xmath is expected to take effect.

Let @xmath and @xmath be three positive constants. For chosen @xmath and
@xmath , we define three critical radiuses:

  -- -------- -- --
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

We then set

  -- -- --
        
  -- -- --

Define the set of sub-differentials of the norm function @xmath near
@xmath (i.e. within @xmath -radius of @xmath ) as

  -- -- -- -------
           (2.8)
  -- -- -- -------

Then, the set @xmath being “large” is characterized by the following
quantity:

  -- -- --
        
  -- -- --

It characterizes the minimum amount of increase of the norm function
@xmath from @xmath on the boundary of region II in Fig. 2.1 , and the
set of sub-differentials @xmath being “large” means for any @xmath ,
there exists a vector in @xmath which is close to the sub-differential
of @xmath . Our goal is to show that when @xmath and @xmath is
comparable to @xmath , then, one has @xmath , as is shown in the
following theorem.

###### Theorem 2.3.1.

Suppose there exists @xmath and @xmath for some constant @xmath , such
that @xmath , @xmath and @xmath . Then, for any @xmath , @xmath with
probability at least @xmath .

Furthermore, if @xmath for some absolute constant @xmath , then, for
@xmath such that @xmath and @xmath . Then, with probability at least
@xmath ,

  -- -- --
        
  -- -- --

###### Remark 2.3.1.

This theorem shows that the desired estimation error follows readily
from tight bounds on @xmath and @xmath . Furthermore, in the second
scenario when @xmath for @xmath , the set @xmath contains the origin, in
which case @xmath must contain the unit ball in the dual norm and @xmath
.

To prove this theorem we need the following simple preliminary lemma:

###### Lemma 2.3.1.

For any @xmath , @xmath .

###### Proof of Lemma 2.3.1.

First of all, by convexity of the function @xmath ,

  -- -- --
        
  -- -- --

Rearranging the terms gives

  -- -- --
        
  -- -- --

Substituting this relation into the definition of @xmath gives

  -- -------- -------- --
     @xmath            
                       
              @xmath   
  -- -------- -------- --

finishing the proof. ∎

###### Proof of Theorem 2.3.1.

First of all, we have for any @xmath

  -- -- --
        
  -- -- --

We now prove the first part of the lemma, which is divided into the
following three steps.

1.  Consider first that @xmath and @xmath . By Lemma 2.3.1 and then the
    definition of @xmath , we have

      -- -------- --
         @xmath   
      -- -------- --

    with probability at least @xmath , and

      -- -- --
            
      -- -- --

    with probability at least @xmath . Also,

      -- -- --
            
      -- -- --

    Thus,

      -- -- -- -------
               (2.9)
      -- -- -- -------

    For @xmath , we have

      -- -------- -- --------
         @xmath      (2.10)
      -- -------- -- --------

    By the assumption that @xmath , we know that @xmath with probability
    at least @xmath .

2.  Consider the case @xmath and @xmath , then, for any specific @xmath
    satisfying the aforementioned conditions,

      -- -------- -- --
         @xmath      
         @xmath      
         @xmath      
      -- -------- -- --

    Let @xmath be the vector containing a sub-dfferential @xmath such
    that @xmath . Note that this is possible because by the assumption
    that @xmath , we have there exists @xmath with a sub-dfferential
    @xmath such that @xmath Thus, for the same choice of @xmath and
    @xmath , @xmath implies

      -- -- -- --------
               (2.11)
      -- -- -- --------

    This implies

      -- -------- -- --
         @xmath      
         @xmath      
         @xmath      
         @xmath      
         @xmath      
      -- -------- -- --

    where the second inequality follows from @xmath , the third
    inequality follows from the definition of sub-differential, the
    fourth inequality follows from Holder’s inequality @xmath and the
    final inequality follows from the preceding argument ( 2.11 ). Now,
    we use the assumption that @xmath and @xmath to conclude that @xmath
    .

3.  The case @xmath and @xmath . If @xmath , then, let @xmath . We have
    by Lemma 2.3.1 and then ( 2.9 ), ( 2.10 ) in step 1,

      -- -- --
            
      -- -- --

    On the other hand, if @xmath , then, let @xmath and we have

      -- -- --
            
      -- -- --

    by step 2.

This finishes the proof of the first part.

For the second part of the claim, one first considers the case @xmath
and @xmath . Using the fact that @xmath is a minimizer of @xmath , we
get @xmath . By ( 2.9 ) in step 1 of the proof,

  -- -------- --
     @xmath   
  -- -------- --

This implies

  -- -------- --
     @xmath   
  -- -------- --

For the case @xmath and @xmath , one can invoke step 2 of the above
proof. Instead of using the assumption @xmath . We consider the
following argument: Since @xmath , the set @xmath must contain the
origin. Thus, one can take @xmath in ( 2.8 ) to be 0 and by Hahn-Banach
theorem, the set @xmath must contain the unit ball of the dual norm,
i.e. for any @xmath , there exists a vector @xmath such that @xmath and
@xmath . As a consequence, @xmath and we have for any @xmath , there
exists a @xmath , such that @xmath . The rest of step 2 and step 3 carry
through. Overall, we finish the proof. ∎

### 2.4 Proof of Theorem 2.2.2: Computing Local Complexities

#### 2.4.1 Bounding @xmath: Preliminary estimates

In this section, we bound the local complexity @xmath . We let @xmath to
be the @xmath -norm. Note first that

  -- -------- -- -- --------
     @xmath         
                    (2.12)
  -- -------- -- -- --------

where @xmath . Define the constants @xmath and @xmath , where @xmath are
defined in Assumption 2.2.1 . Let @xmath be a constant less than @xmath
(to be defined later) and define @xmath to be the set of vectors with
@xmath cardinality.

Our goal is to show that the intersection of the following three sets,
when taking infimum over @xmath and @xmath is sufficiently large:

  -- -- --
        
  -- -- --

where @xmath is an absolute constant.

###### Lemma 2.4.1.

Let @xmath and @xmath for some absolute constant @xmath . With
probability at least @xmath ,

  -- -- --
        
  -- -- --

###### Proof of Lemma 2.4.1.

Let @xmath and define @xmath . First, by finite difference inequality,
we have with probability @xmath

  -- -- --
        
  -- -- --

Thus, it is enough to bound the expected supremum. We have

  -- -------- -- --
                 
     @xmath      
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

where the first inequality follows from @xmath , the second inequality
follows from symmetrization inequality and the last inequality follows
from Talagrand contraction principle. Now, we bound the two terms
respectively.

-   Bounding (I): First, by Bernstein’s ineuqality,

      -- -- --
            
      -- -- --

    Taking a union bound over @xmath ,

      -- -- --
            
      -- -- --

    Since @xmath and @xmath , this implies

      -- -- --
            
      -- -- --

    Thus,

      -- -- --
            
      -- -- --

    and

      -- -- --
            
      -- -- --

-   Bounding (II):

      -- -- --
            
      -- -- --

    where the last inequality follows from:

      -- -- --
            
      -- -- --

    and the following derivation:

      -- -- --
            
      -- -- --

    and

      -- -- --
            
      -- -- --

    where the first inequality follows from the definition that @xmath ,
    the second inequality follows from Holder’s inequality and the third
    inequality follows from Markov inequality.

Overall, we obtain with probability @xmath ,

  -- -- --
        
  -- -- --

Since @xmath and @xmath , it follows for @xmath large enough, we have

  -- -- --
        
  -- -- --

finishing the proof. ∎

###### Lemma 2.4.2.

Let @xmath and @xmath . With probability at least @xmath ,

  -- -- --
        
  -- -- --

###### Proof of Lemma 2.4.2.

First of all, note that

  -- -- --
        
  -- -- --

By Markov inequality,

  -- -- --
        
  -- -- --

By bounded difference inequality,

  -- -- --
        
  -- -- --

Thus, it follows when @xmath , the desired inequality holds. ∎

#### 2.4.2 Weak small-ball estimates for small @xmath

In this section, we consider lower bounding the cardinality of the set
@xmath when @xmath . This holds when @xmath which is @xmath .

We start with the following small-ball estimate via Paley-Zygmund
inequality:

###### Lemma 2.4.3.

Under Assumption 2.2.1 , let @xmath and @xmath , then, we have

  -- -- --
        
  -- -- --

###### Proof.

By Paley-Zygmund inequality, we know for any nonnegative real valued
random variable @xmath ,

  -- -- --
        
  -- -- --

for any @xmath . Now, fix any @xmath , we take @xmath , @xmath , and
obtain

  -- -- --
        
  -- -- --

Recall from Assumption 2.2.1 , @xmath , thus, @xmath for any @xmath ,
and it follows,

  -- -- -- --
           
           
           
  -- -- -- --

where the last inequality follows from Assumption 2.2.1 . Taking @xmath
and @xmath finishes the proof. ∎

We see from Lemma 2.4.3 that indeed such a small-ball condition is
easily satisfied merely under a bounded moment assumption. The following
lemma is the key to our analysis in this step. It says a somewhat “weak”
small-ball condition is preserved under adaptive truncation.

###### Lemma 2.4.4.

Let @xmath be a positive integer such that @xmath . Let @xmath be the
set of all vectors in @xmath with @xmath cardinality of the support set.
Suppose Assumption 2.2.1 holds and @xmath , then, for any @xmath ,

  -- -- --
        
  -- -- --

###### Proof.

First, note that for any vector @xmath ,

  -- -- --
        
  -- -- --

Thus, it follows

  -- -------- -- -- --------
                    
     @xmath         
     @xmath         (2.13)
  -- -------- -- -- --------

where the last inequality follows from the fact that for any two
measurable set @xmath in a probability space @xmath , @xmath . By Lemma
2.4.3 , @xmath . It remains to bound @xmath from above. To this point,
let @xmath be the orthogonal projection of a vector @xmath onto the
non-zero coordinates of @xmath . Then, by Holder’s inequality, we have

  -- -------- -- --
                 
     @xmath      
     @xmath      
  -- -------- -- --

where the last inequality follows from the definition of @xmath in ( 2.5
) that if every entry of @xmath is bounded by @xmath , then @xmath .
Furthermore,

  -- -- --
        
  -- -- --

where the second from the last inequality follows from Markov inequality
and the last inequality follows from the definition of @xmath and the
assumption that @xmath . Since @xmath by assumption, we have @xmath and
the proof is finished. ∎

Using the previous lemma one can show the following via a book-keeping
VC dimension argument.

###### Lemma 2.4.5.

Consider any integer @xmath such that @xmath . Suppose @xmath , then,
with probability at least @xmath ,

  -- -- --
        
  -- -- --

where @xmath are absolute constants.

###### Proof of Lemma 2.4.5.

First of all, by Lemma 2.4.4 , for any @xmath and @xmath , we have

  -- -- --
        
  -- -- --

Let @xmath , and define the following process parametrized by @xmath :

  -- -- --
        
  -- -- --

and we aim to bound the following supremum

  -- -- --
        
  -- -- --

Define the following class of indicator functions:

  -- -- --
        
  -- -- --

By the standard symmetrization argument and then Dudley’s entropy
estimate (see, for example, [ van1996weak ] for details of VC theory),
we have

  -- -- -- --------
           (2.14)
  -- -- -- --------

where @xmath is a constant and @xmath is the @xmath -covering number of
@xmath under the norm @xmath .

Consider, without loss of generality, a particular subspace @xmath of
@xmath consisting of all vectors whose first @xmath coordinates are
non-zero. Note that for any fixed number @xmath , the VC dimension of
the set of halfspaces @xmath is @xmath . Thus, by classical VC theorem,
for any distinctive @xmath points in @xmath , the number distinctive
projections from @xmath to these @xmath points is @xmath . Furthermore,
any set in @xmath is the intersection of two sets in @xmath , thus, the
number of distinctive projections from @xmath to those @xmath points is
at most

  -- -------- --
     @xmath   
  -- -------- --

This implies @xmath for some absolute constant @xmath .

Thus, the following class of indicator functions

  -- -- --
        
  -- -- --

has VC dimension @xmath . By Haussler’s inequality, we have the @xmath
covering number of @xmath can be bounded as

  -- -- --
        
  -- -- --

where @xmath is an absolute constant. Furthermore, @xmath is the union
of @xmath different subspaces @xmath . Thus, the @xmath covering number
of @xmath can be bounded as

  -- -- -------- --
        @xmath   
                 
  -- -- -------- --

Substituting this bound into ( 2.14 ) gives

  -- -- --
        
  -- -- --

for some absolute constant @xmath . By bounded difference inequality, we
have

  -- -- --
        
  -- -- --

with probability at least @xmath for some constant @xmath any @xmath ,
which implies

  -- -- --
        
  -- -- --

with probability at least @xmath . This implies the claim of the lemma.
∎

Combining Lemma 2.4.5 with Lemma 2.4.1 and 2.4.2 we obtain the following
lemma:

###### Lemma 2.4.6.

Let @xmath , @xmath , where @xmath is an absolute constant and @xmath ,
where @xmath is the constant defined in Lemma 2.4.5 . Let @xmath . then,
with probability at least @xmath for some absolute constant @xmath ,
there exists a set of indices @xmath such that @xmath and for any @xmath
, @xmath ,

  -- -- --
        
  -- -- --

###### Proof of Lemma 2.4.6.

First of all, by Lemma 2.4.5 , @xmath and @xmath , we have with
probability at least @xmath ,

  -- -- --
        
  -- -- --

On the other hand, by Lemma 2.4.1 and 2.4.2 , we have

  -- -- --
        
  -- -- --

and

  -- -- --
        
  -- -- --

with probability at least @xmath . Combining the above three bounds, we
have there exists a set of indices @xmath of cardinality at least @xmath
such that the claim in the lemma holds. ∎

The following theorem bounds @xmath :

###### Theorem 2.4.1.

Let @xmath , @xmath , @xmath and @xmath , where @xmath is the constant
defined in Lemma 2.4.5 . Let @xmath , @xmath , and @xmath , then,

  -- -- --
        
  -- -- --

with @xmath , where @xmath are absolute constants.

To prove Theorem 2.4.1 , we need the the following useful lower bound on
the random quadratic form, which comes from [ lecue2017sparse ] . Lower
bounds of this sort via Maurey’s empirical method originate from [
oliveira2013lower ] .

###### Lemma 2.4.7 (Lemma 2.7 of [lecue2017sparse]).

Let @xmath . Let @xmath be a positive integer such that @xmath . Assume
for any @xmath , @xmath for some absolute constant @xmath . If @xmath is
a non-zero vector and @xmath , then,

  -- -- --
        
  -- -- --

where @xmath is the standard basis in @xmath .

Denote @xmath in Lemma 2.4.6 to be @xmath and let @xmath . We then
deduce a lower bound for In view of the previous lemma, we also need an
upper bound for @xmath :

###### Lemma 2.4.8.

For any @xmath chosen by the thresholding parameter @xmath , we have
with probability at least @xmath ,

  -- -- --
        
  -- -- --

where @xmath is an absolute constant.

###### Proof of Lemma 2.4.8.

By Bernstein’s inequality, we have for any @xmath ,

  -- -- --
        
  -- -- --

where

  -- -- --
        
  -- -- --

@xmath , @xmath , and @xmath . Thus, it follows for any @xmath ,

  -- -- --
        
  -- -- --

with probability at least @xmath . Take a union bound over @xmath and
let @xmath give

  -- -------- --
     @xmath   
  -- -------- --

with probability at least @xmath , for some absolute constant @xmath .
This finishes the proof. ∎

###### Proof of Theorem 2.4.1.

First of all, by ( 2.12 ) and Lemma 2.4.6 , we have with probability at
least @xmath ,

  -- -- --
        
  -- -- --

Since @xmath , we have

  -- -- --
        
  -- -- --

By Lemma 2.4.7 and 2.4.8 , we have

  -- -- --
        
  -- -- --

Note that @xmath , @xmath , and @xmath . The infimum of @xmath such that
the right hand side is greater than @xmath can be obtained by letting
the right hand side equal to @xmath and solve for @xmath , which gives

  -- -- --
        
  -- -- --

for some absolute constant @xmath . It then follows from the definition
of @xmath that @xmath must be bounded above by this value. ∎

#### 2.4.3 Applying Mendelson’s small-ball method for large @xmath

In this section, we consider lower bounding the cardinality of the set
@xmath when @xmath . In this case, suppose Assumption 2.2.1 holds, by
Lemma 2.4.4 , we have for any @xmath ,

  -- -- -- --------
           (2.15)
  -- -- -- --------

We have the following lemma:

###### Lemma 2.4.9.

Let @xmath , @xmath for some absolute constant @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

and @xmath , then, with probability at least @xmath for some absolute
constant @xmath , there exists a set of indices @xmath such that @xmath
and for any @xmath , @xmath ,

  -- -- --
        
  -- -- --

###### Proof of Lemma 2.5.1.

The proof of this lemma almost follows from that of Lemma 1.2.1 from [
mendelson2014learning ] , the only difference is that we need to take
care of indices @xmath such that @xmath which are Lemma 2.4.1 and 2.4.2
. We consider the quantity

  -- -- --
        
  -- -- --

By the same argument as that of Theorem 5.4 in [ mendelson2014learning ]
(using ( 2.15 )), one obtains with probability at least @xmath ,

  -- -- --
        
  -- -- --

where for any @xmath ,

  -- -- --
        
  -- -- --

Similar to bounding term (I) is Lemma 2.4.1 , one obtains

  -- -------- -------- --
     @xmath            
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where the last inequality follows from the fact that @xmath . When

  -- -------- --
     @xmath   
  -- -------- --

we have

  -- -- --
        
  -- -- --

with probability @xmath . Combining this result with Lemma 2.4.1 and
2.4.2 finishes the proof. ∎

###### Theorem 2.4.2.

Let @xmath , @xmath , @xmath for some absolute constant @xmath , @xmath
, and @xmath . Suppose @xmath , then,

  -- -------- --
     @xmath   
  -- -------- --

with @xmath , where @xmath is absolute constant.

###### Proof of Theorem 2.4.2.

First, note that when @xmath and @xmath satisfying the condition
asserted in the theorem, then,

  -- -------- --
     @xmath   
  -- -------- --

For any @xmath , let @xmath and with probability at least @xmath ,

  -- -- --
        
  -- -- --

where the first inequality follows from Lemma 2.5.1 by taking the
corresponding @xmath , the second from the last inequality follows from

  -- -- --
        
  -- -- --

and the last inequality follows from Lemma 2.5.1 again. ∎

#### 2.4.4 Bounding @xmath via Montgomery-Smith inequality

The main objective is the following bound on @xmath :

###### Lemma 2.4.10.

Suppose @xmath and Assumption 2.2.1 , 2.2.3 hold. For any @xmath , we
have with probability at least

  -- -- --
        
  -- -- --

where @xmath are absolute constants,

  -- -- --
        
  -- -- --

where @xmath depends polynomially on @xmath and @xmath .

###### Proof of Lemma 2.4.10.

First of all, by symmetrization inequality, it is enough to bound

  -- -------- -- --
                 
     @xmath      
  -- -------- -- --

We define @xmath and note that

  -- -------- -- --------
     @xmath      (2.16)
  -- -------- -- --------

Now for each @xmath ,

  -- -- --
        
  -- -- --

Thus, it follows

  -- -- -- --------
           (2.17)
  -- -- -- --------

Then, we need to bound the three terms on the right hand side of ( 2.28
) separately.

1. Bounding the terms @xmath :

Let @xmath . A crucial first step analyzing such a Rademacher sum (see,
for example, [ mendelson2016upper , goldstein2016structured ] ) is to
apply Montgomery-Smith inequality from, i.e. Lemma 1.1.7 , conditioned
on @xmath , which results in

  -- -- --
        
  -- -- --

with probability at least @xmath , where @xmath is any chosen integer
within @xmath and @xmath , @xmath are non-increasing rearrangements of
@xmath , @xmath . We define the former sum to be 0 when @xmath .

By Holder’s inequality, we have

  -- -- --
        
  -- -- --

for some positive constants @xmath such that @xmath . Take a union bound
for all @xmath , gives with probsability at least @xmath ,

  -- -- -- --------
           (2.18)
  -- -- -- --------

where @xmath is to be chosen.

Now we bound the four terms in ( 2.18 ) respectively.

###### Lemma 2.4.11.

Let @xmath for some absolute constant @xmath , and suppose @xmath ,
then, we have

  -- -- --
        
  -- -- --

with probability at least @xmath for any @xmath and some absolute
constant @xmath .

###### Proof of Lemma 2.4.11.

First of all, using Binomial estimates, we have for any @xmath , and any
positive constant @xmath ,

  -- -------- -- --
                 
     @xmath      
     @xmath      
  -- -------- -- --

where we define @xmath and @xmath is a chosen positive constant. Then,
we choose @xmath , which implies

  -- -- --
        
  -- -- --

Thus, it follows,

  -- -- -- --------
           (2.19)
  -- -- -- --------

with probability at least

  -- -- --
        
  -- -- --

Note that for @xmath and @xmath chosen to be @xmath , the above sum is a
geometrically decreasing sequence, specifically, it is easy to verify
that @xmath . Thus, it follows the above probability is at least

  -- -- --
        
  -- -- --

for some absolute constant @xmath . Now, we bound the term @xmath . We
choose @xmath . Then, under the condition that @xmath , @xmath , and
@xmath . Furthermore, we have by Assumption 2.2.3 ,

  -- -- --
        
  -- -- --

Note that

  -- -- --
        
  -- -- --

where the first inequality follows from Minkowski’s inequality. Now, for
each @xmath , we have

  -- -- --
        
  -- -- --

where the second from the last inequality follows from Holder’s
inequality and the last inequality follows from Markov inequality. Thus,
we obtain,

  -- -- --
        
  -- -- --

for some constant @xmath and @xmath . Overall, substituting the above
bound into ( 2.19 ), we have with probability at least @xmath , where
@xmath ,

  -- -- --
        
  -- -- --

for some constant @xmath . ∎

###### Lemma 2.4.12.

Let @xmath for some absolute constant @xmath , and suppose @xmath ,
then, we have

  -- -- --
        
  -- -- --

with probability at least @xmath for any @xmath and some constant @xmath
.

###### Proof of Lemma 2.4.12.

First, for any set of @xmath random variables @xmath we have by
Bernstein’s inequality,

  -- -- --
        
  -- -- --

for some constant @xmath , where @xmath , @xmath and @xmath . Take a
union bound over all @xmath different combinations from @xmath , we
obtain,

  -- -- --
        
  -- -- --

Taking a union bound over all @xmath , we get

  -- -- --
        
  -- -- --

Substituting the definition of @xmath , we get

  -- -- --
        
  -- -- --

Setting @xmath and rearranging the terms gives the claim. ∎

###### Lemma 2.4.13.

Let @xmath for some absolute constant @xmath , and suppose @xmath ,
then, we have with probability at least @xmath , for some absolute
constant @xmath ,

  -- -- --
        
  -- -- --

for @xmath , any @xmath , and some absolute constant @xmath .

###### Proof of Lemma 2.4.13.

Let @xmath , then, @xmath . Using Binomial estimates, we have for any
@xmath , and any @xmath ,

  -- -- --
        
  -- -- --

where the second inequality follows from Markov inequality. We choose
@xmath and get

  -- -- --
        
  -- -- --

Thus, it follows

  -- -- --
        
  -- -- --

for some absolute constant @xmath , where the second from the last
inequality follows from the fact that for any @xmath , the summand is a
geometrically decreasing sequence since @xmath . Plugging in @xmath and
using the fact that @xmath give

  -- -- --
        
  -- -- --

Thus, it follows with probability at least @xmath , we have

  -- -- -- --------
           (2.20)
  -- -- -- --------

Since @xmath , it follows

  -- -- --
        
  -- -- --

Thus, with probability at least @xmath ,

  -- -- -- --------
           (2.21)
  -- -- -- --------

for some constant @xmath . It remains to bound @xmath . By Assumption
2.2.3 ,

  -- -- --
        
  -- -- --

Note that

  -- -- --
        
  -- -- --

where the first inequality follows from Minkowski’s inequality. Now, for
each @xmath , we have

  -- -- --
        
  -- -- --

where the second from the last inequality follows from Holder’s
inequality and the last inequality follows from Markov inequality. Thus,
we obtain,

  -- -- --
        
  -- -- --

for some constant @xmath and @xmath . Combining this bound with ( 2.21 )
finishes the proof. ∎

###### Lemma 2.4.14.

Let @xmath for some absolute constant @xmath , and suppose @xmath ,
then, we have with probability at least @xmath , for some absolute
constant @xmath .

  -- -- --
        
  -- -- --

for some constant absolute constant @xmath and @xmath .

###### Proof.

First, following the same procedure as that of Lemma 2.4.13 up to ( 2.20
), with @xmath , we have with probability at least @xmath ,

  -- -- --
        
  -- -- --

Note that @xmath by the assumption and @xmath , thus, @xmath and we have
with probability at least @xmath ,

  -- -- --
        
  -- -- --

Finally, taking a union bound over all @xmath finishes the proof. ∎

Finally, substituting Lemma 2.4.11 , 2.4.12 , 2.4.13 , 2.4.14 into (
2.18 ) with @xmath gives with probability at least @xmath ,

  -- -- -- --------
           (2.22)
  -- -- -- --------

2. Bounding the terms @xmath :

The proving techniques in this part are essentially the same as those of
the last part but with a slight change of exponents when applying
Holder’s inequality adapting to the moment condition of the term @xmath
. For simplicity of notations, let

  -- -- --
        
  -- -- --

Similar as before, one can employ the inequality from [
montgomery1990distribution ] , conditioned on @xmath , which results in

  -- -- --
        
  -- -- --

with probability at least @xmath , where @xmath is any chosen integer
within @xmath and @xmath , @xmath are non-increasing rearrangements of
@xmath , @xmath . We define the former sum to be 0 when @xmath . By
Holder’s inequality, we have

  -- -- --
        
  -- -- --

for some positive exponents @xmath such that @xmath . Take a union bound
for all @xmath , gives with probsability at least @xmath ,

  -- -- -- --------
           (2.23)
  -- -- -- --------

Again, our goal is to bound the four terms in ( 2.23 ) separately.

###### Lemma 2.4.15.

Let @xmath for some absolute constant @xmath , and suppose @xmath ,
then, we have

  -- -- --
        
  -- -- --

with probability at least @xmath for any @xmath and some absolute
constant @xmath , where @xmath with @xmath is defined in Assumption
2.2.1 .

###### Proof of Lemma 2.4.15.

First of all, by Markov inequality,

  -- -- --
        
  -- -- --

Choosing @xmath gives

  -- -- --
        
  -- -- --

Thus, it follows

  -- -- --
        
  -- -- --

with probability at least

  -- -- --
        
  -- -- --

Since for any @xmath and @xmath , the above summand is a geometrically
decreasing sequence. Specifically, it is easy to show that @xmath .
Thus, it follows the probability is at least

  -- -- --
        
  -- -- --

for some absolute constant @xmath . ∎

###### Lemma 2.4.16.

Let @xmath for some absolute constant @xmath , then, we have

  -- -- --
        
  -- -- --

with probability at least @xmath for any @xmath and some constant @xmath
.

###### Proof of Lemma 2.4.16.

First, for any set of @xmath random variables @xmath we have by
Bernstein’s inequality,

  -- -- --
        
  -- -- --

for some constant @xmath , where @xmath , @xmath and @xmath . Take a
union bound over all @xmath different combinations from @xmath , we
obtain,

  -- -- --
        
  -- -- --

Taking a union bound over all @xmath , we get

  -- -- --
        
  -- -- --

Substituting the definition of @xmath , we get

  -- -- --
        
  -- -- --

Setting @xmath and rearranging the terms gives the claim. ∎

###### Lemma 2.4.17.

Let @xmath for some absolute constant @xmath , then, we have with
probability at least @xmath , for some absolute constant @xmath ,

  -- -- --
        
  -- -- --

for @xmath , any @xmath , and some absolute constant @xmath .

###### Proof.

Following from the same proof as that of Lemma 2.4.13 up to ( 2.20 )
with @xmath , we have with probability at least @xmath ,

  -- -- -- --------
           (2.24)
  -- -- -- --------

Since @xmath by assumption, it follows,

  -- -- --
        
  -- -- --

which implies the claim. ∎

Also, by Lemma 2.4.14 , we have with probability at least @xmath , for
some absolute constant @xmath ,

  -- -- -- --------
           (2.25)
  -- -- -- --------

for some constant absolute constant @xmath and @xmath .

Overall, substituting Lemma 2.4.15 , 2.4.16 , 2.4.17 , and ( 2.25 ) into
( 2.23 ) with @xmath gives with probability at least

  -- -- --
        
  -- -- --

the following holds

  -- -- -- --------
           (2.26)
  -- -- -- --------

Overall, substituting the bounds ( 2.22 ) and ( 2.26 ) into ( 2.28 )
gives

  -- -- --
        
  -- -- --

with probability at least

  -- -- --
        
  -- -- --

This implies the claim when combining ( 2.16 ) and the fact that @xmath
. ∎

The following lemma gives a bound on @xmath in terms of @xmath .

###### Lemma 2.4.18.

Suppose @xmath , @xmath and Assumption 2.2.1 , 2.2.3 hold. For any
@xmath , we have

  -- -- --
        
  -- -- --

for any @xmath ,  where @xmath depends polynomially on @xmath , @xmath ,
@xmath and @xmath , when taking

  -- -- --
        
  -- -- --

where @xmath are absolute constants.

###### Proof of Lemma 2.4.18.

Since @xmath , the infimum of @xmath such that the right hand side of
Lemma 2.4.10 is less than @xmath can be achieved by setting the right
hand side equal to @xmath , which gives,

  -- -- --
        
  -- -- --

which implies the claim. ∎

#### 2.4.5 Bounding the radius @xmath

###### Lemma 2.4.19.

Suppose @xmath and @xmath , then,

  -- -------- --
     @xmath   
  -- -------- --

###### Proof of Lemma 2.4.19.

First of all,

  -- -- --
        
  -- -- --

For each @xmath , we have

  -- -- --
        
  -- -- --

where we use the fact that the conditional expectation

  -- -- --
        
  -- -- --

Note that for any @xmath , by Cauchy-Schwarz inequality,

  -- -------- -- --
                 
     @xmath      
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

where the second inequality follows from Assumption 2.2.3 , and the
third inequality follows from Minkowski’s inequality. Now, for each
@xmath , we have

  -- -- --
        
  -- -- --

where

  -- -- --
        
  -- -- --

Thus,

  -- -- --
        
  -- -- --

and we have

  -- -- --
        
  -- -- --

where we use the assumption that @xmath . Overall, we get

  -- -- --
        
  -- -- --

Since @xmath , let

  -- -------- --
     @xmath   
  -- -------- --

which results in

  -- -------- --
     @xmath   
  -- -------- --

and @xmath must be bounded above by this value. ∎

#### 2.4.6 Putting everything together

###### Proof of Theorem 2.2.2.

We choose @xmath , @xmath and @xmath . Then, @xmath . By Theorem 2.4.1
and 2.4.2 ,

  -- -------- --
     @xmath   
  -- -------- --

with @xmath , when @xmath . By Lemma 2.4.18 , we have

  -- -- --
        
  -- -- --

with

  -- -- --
        
  -- -- --

when @xmath . Finally, by Lemma 2.4.19 ,

  -- -------- --
     @xmath   
  -- -------- --

when @xmath . Thus, when @xmath for some absolute constant @xmath ,

  -- -- --
        
  -- -- --

Now, we choose @xmath for some @xmath and

  -- -- --
        
  -- -- --

By Theorem 2.3.1 , we have the estimator satisfies

  -- -- --
        
  -- -- --

and we finish the proof. ∎

### 2.5 Proof of Theorem 2.2.5: Computing Local Complexities

In this section, we prove Theorem 2.2.5 in a similar manner as that of
Theorem 2.2.2 . Building upon previous intermediate results, the proof
will be relatively simpler.

#### 2.5.1 Bounding radius @xmath

###### Lemma 2.5.1.

Let @xmath , @xmath where @xmath is the absolute constant defined in
Lemma 2.4.5 , then, with probability at least @xmath for some absolute
constant @xmath , there exists a set of indices @xmath such that @xmath
and for any @xmath , @xmath ,

  -- -- --
        
  -- -- --

###### Proof of Lemma 2.5.1.

First of all, by Lemma 2.4.5 , @xmath , we have with probability at
least @xmath ,

  -- -- --
        
  -- -- --

On the other hand, by Lemma 2.4.1 and 2.4.2 , we have

  -- -- --
        
  -- -- --

and

  -- -- --
        
  -- -- --

with probability at least @xmath . Combining the above three bounds, we
have there exists a set of indices @xmath of cardinality at least @xmath
such that the claim in the lemma holds. ∎

###### Theorem 2.5.1.

Suppose @xmath for some absolute constant @xmath , and @xmath and @xmath
, then,

  -- -------- --
     @xmath   
  -- -------- --

when taking @xmath in the definition of @xmath for @xmath .

###### Proof of Theorem 2.5.1.

First of all, recall that @xmath . By Lemma 2.5.1 , and the assumption
@xmath for some large enough absolute constant @xmath , we have

  -- -- --
        
  -- -- --

with probability at least @xmath . Thus, it follows from Lemma 2.4.7 and
2.4.8 that

  -- -- --
        
  -- -- --

where @xmath is an absolute constant. By assumption that @xmath for some
@xmath large enough, then,

  -- -- --
        
  -- -- --

Using the assumption that @xmath , we obtain

  -- -- --
        
  -- -- --

The infimum of @xmath such that the right hand side is greater than
@xmath can be obtained by letting the right hand side equal to @xmath
and solve for @xmath , which gives @xmath . It then follows from the
definition of @xmath that @xmath must be bounded above by this value. ∎

#### 2.5.2 Bounding @xmath and @xmath

The main objective is the following bound on @xmath :

###### Lemma 2.5.2.

Suppose @xmath for some absolute constant @xmath and Assumption 2.2.1 ,
2.2.3 and 2.2.4 hold. For any @xmath , we have with probability at least

  -- -- --
        
  -- -- --

where @xmath are absolute constants,

  -- -- --
        
  -- -- --

for any @xmath ,  where @xmath depends polynomially on @xmath and @xmath
.

###### Proof of Lemma 2.5.2.

First of all, by symmetrization inequality, it is enough to bound

  -- -- --
        
  -- -- --

We define @xmath . Let @xmath be any group of coordinates in @xmath with
@xmath largest coordinates of @xmath for @xmath . Then, it follows

  -- -- -- --------
           (2.27)
  -- -- -- --------

for any @xmath , where @xmath denotes the non-increasing ordering of
@xmath . Now for each @xmath , let @xmath ,

  -- -- --
        
  -- -- --

Thus, it follows

  -- -- -- --------
           (2.28)
  -- -- -- --------

By the same analysis as that of Lemma 2.4.10 , we obtain

  -- -- --
        
  -- -- --

with probability at least

  -- -- --
        
  -- -- --

This implies the claim when combining with ( 2.27 ). ∎

###### Lemma 2.5.3.

Suppose @xmath for some absolute constant @xmath , Assumption 2.2.1 ,
2.2.3 and 2.2.4 hold and @xmath . Then, we have

  -- -- --
        
  -- -- --

when taking

  -- -- --
        
  -- -- --

for some absolute constant @xmath and any @xmath , where @xmath depends
polynomially on @xmath and @xmath .

###### Proof of Lemma 2.5.3.

Since @xmath , let @xmath in Lemma 2.5.2 and the infimum of the @xmath
such that the right hand side of Lemma 2.5.2 is less than @xmath can be
achieved by setting the right hand side equal to @xmath , which gives,

  -- -- --
        
  -- -- --

Solving the above quadratic equation gives

  -- -- --
        
  -- -- --

Thus, the defined @xmath must be bounded above by this value and the
lemma is proved. ∎

###### Lemma 2.5.4.

Suppose @xmath and @xmath , then,

  -- -------- --
     @xmath   
  -- -------- --

The proof is the same as that of Lemma 2.4.19 .

#### 2.5.3 Putting everything together

###### Lemma 2.5.5.

Suppose @xmath , where @xmath an @xmath -sparse vector and @xmath ,
then, @xmath .

###### Proof of Lemma 2.5.5.

Let @xmath be the set of nonzero coordinates of @xmath , then, for any
vector @xmath , we have @xmath and since @xmath , by definition of
@xmath in ( 2.8 ), there exists a sub-differential @xmath such that
@xmath and @xmath . Thus, it follows,

  -- -- --
        
  -- -- --

where the second from the last inequality follows from @xmath that
@xmath and the last inequality follows from @xmath . The above bound is
greater than @xmath when @xmath . ∎

Finally, we are ready to prove the main theorem .

###### Proof of Theorem 2.2.5.

We set @xmath , @xmath and @xmath . By Theorem 2.5.1 , Lemma 2.5.2 and
2.5.4 , we have

  -- -- --
        
  -- -- --

By Lemma 2.5.5 , the condition @xmath is satisfied for any @xmath . Take
equality in the above bound and choose @xmath to be

  -- -------- --
     @xmath   
  -- -------- --

where @xmath depends polynomially on @xmath and @xmath is satisfied.
This implies

  -- -------- --
     @xmath   
  -- -------- --

Taking

  -- -------- --
     @xmath   
  -- -------- --

finishes the proof. ∎

## Chapter 3 Structured Recovery from Non-linear and Heavy-tailed
Measurements

In this chapter, we show that when the design vectors are selected from
a specific class of distributions, then, one can simultaneously relax
the moment condition as well as treat more general structured problems.
We study high-dimensional signal recovery from non-linear measurements
with design vectors having elliptically symmetric distribution. Special
attention is devoted to the situation when the unknown signal belongs to
a set of low statistical complexity, while both the measurements and the
design vectors are heavy-tailed. We propose and analyze a new estimator
that adapts to the structure of the problem, while being robust both to
the possible model misspecification characterized by arbitrary
non-linearity of the measurements as well as to data corruption modeled
by the heavy-tailed distributions. Moreover, this estimator has low
computational complexity. Theoretically, our results are expressed in
the form of exponential concentration inequalities relying on an
improved generic chaining method. Numerically, we conduct simulation
experiments demonstrating that our estimator outperforms existing
alternatives when data is heavy-tailed.

### 3.1 Introduction

In many practical settings, exact measurements from linear models or
GLMs ( 2.1 ) are not available. Instead, the data one observes are often
subject to unknown distortions such as quantization and hard
thresholding. Furthermore, one might not even know the exact model ( 2.1
). Is it possible to perform faithful parameter estimation in these
imperfect scenarios? This chapter treats this problem with a more
general setup than that of ( 2.1 ). Instead of adopting a specific
model, we assume the link function is unknown. More specifically, let
@xmath be a random couple satisfying the semi-parametric single index
model

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

where @xmath is a measurement vector with marginal distribution @xmath ,
@xmath is a noise variable that is assumed to be independent of @xmath ,
@xmath is a fixed but otherwise unknown signal (“index vector”), and
@xmath is an unknown link function; here and in what follows, @xmath
denotes the Euclidean dot product. We impose no explicit conditions on
@xmath , and in particular it is not assumed that @xmath is convex, or
even continuous. Our goal is to estimate the signal @xmath from a
sequence of samples @xmath which are copies of @xmath . As @xmath for
any @xmath , the best one can hope for is to recover @xmath up to a
scaling factor. Hence, without loss of generality, we will assume that
@xmath satisfies @xmath , where @xmath is the covariance matrix of
@xmath . Instead of being sparse or approximately sparse, in this
chapter, we will assume that @xmath is an element of a closed set @xmath
of small statistical complexity that is characterized by its Gaussian
mean width.

Due to the ambiguity of @xmath , such a task can easily fail regardless
of the algorithms [ ai2014one ] . As an example, consider the model
@xmath . Consider two sparse vectors: @xmath , @xmath , and i.i.d.
Bernoulli design vectors @xmath , where each entry takes +1 and -1 with
equal probabilities. It is obvious that for @xmath and @xmath , the
responses @xmath are identical and the model cannot distinguish between
@xmath and @xmath . Thus, one has to pose extra assumptions on the
design vector itself so that the problem is well-defined.

Generally, the task of estimating the index vector requires
approximating the link function @xmath [ hardle1993optimal ] or its
derivative, assuming that it exists (the so-called Average Derivative
Method), see [ stoker1986consistent , hristache2001direct ] . However,
when the measurement vector @xmath is Gaussian, a somewhat surprising
result states that one can estimate @xmath directly, avoiding
preliminary link function estimation step completely. More specifically,
[ Brillinger1983A-generalized-l00 ] proved that @xmath , where @xmath .
Later, [ li1989regression ] extended this result to the more general
case of elliptically symmetric distributions, which includes the
Gaussian as a special case; see Lemma 3.5.5 .

Our work was partly inspired by the work of Y. Plan, R. Vershynin and E.
Yudovina [ plan2014high , plan2016generalized ] , who presented the
non-asymptotic study for the case of Gaussian measurements in the
context of high-dimensional structured estimation; also, see [
genzel2016high , ai2014one , thrampoulidis2015lasso , yi2015optimal ]
for further details. On a high level, these works show that when @xmath
’s are Gaussian, nonlinearity can be treated as an additional noise
term. To give an example, [ plan2016generalized ] and [ plan2014high ]
demonstrate that under the same model as ( 3.1 ), when @xmath , @xmath ,
and @xmath is sub-Gaussian for @xmath , solving the constrained problem

  -- -- --
        
  -- -- --

with @xmath and @xmath , recovers @xmath up to a scaling factor @xmath
with high probability: namely, for all @xmath ,

  -- -- -- -------
           (3.2)
  -- -- -- -------

where, with formal definitions to follow in Section 3.2 , @xmath is the
unit sphere in @xmath , @xmath is the descent cone of @xmath at point
@xmath and @xmath is the Gaussian mean width of a subset @xmath . A
different approach to estimation of the index vector in model ( 3.1 )
with similar recovery guarantees has been developed in [ yi2015optimal ]
. However, the key assumption adopted in all these works that the
vectors @xmath follow Gaussian distributions preclude situations where
the measurements are heavy tailed, and hence might be overly restrictive
for some practical applications; for example, noise and outliers
observed in high-dimensional image recovery often exhibit heavy-tailed
behavior, see [ face-recognition ] . The works [ yang2017stein ] and [
yang2017learning ] later consider using Stein’s identity to perform
nonlinear recovery under the assumption that the distribution of the
sensing vector is known, both the distribution function and the
nonlinear transform must satisfy certain smoothness assumptions,

As we mentioned above, [ li1989regression ] have shown that direct
consistent estimation of @xmath is possible when @xmath belongs to a
family of elliptically symmetric distributions. Our main contribution is
the non-asymptotic analysis for this scenario, with a particular focus
on the case when @xmath and @xmath possesses special structure, such as
sparsity. Moreover, we make very mild assumptions on the tails of the
response variable @xmath : for example, when the link function satisfies
@xmath , it is only assumed that @xmath possesses @xmath moments, for
some @xmath . [ plan2016generalized ] present analysis for the Gaussian
case and ask “Can the same kind of accuracy be expected for random
non-Gaussian matrices?” In this chapter, we give a positive answer to
their question. To achieve our goal, we propose a Lasso-type estimator
that admits tight probabilistic guarantees in spirit of ( 3.2 ) despite
weak tail assumptions (see Theorem 3.3.1 below for details).

### 3.2 Definitions and Background Material.

This section introduces main notation and the key facts related to
elliptically symmetric distributions, convex geometry and empirical
processes. The results of this section will be used repeatedly
throughout the chapter.

For the unified treatment of vectors and matrices, it will be convenient
to treat a vector @xmath as a @xmath matrix. Let @xmath be such that
@xmath . Given @xmath , the Euclidean dot product is then defined as
@xmath , where @xmath stands for the trace of a matrix and @xmath
denotes the transpose of @xmath .

The @xmath -norm of @xmath is defined as @xmath . The nuclear norm of a
matrix @xmath is @xmath , where @xmath stand for the singular values of
@xmath , and the operator norm is defined as @xmath .

#### 3.2.1 Elliptically symmetric distributions.

A centered random vector @xmath has elliptically symmetric
(alternatively, elliptically contoured or just elliptical) distribution
with parameters @xmath and @xmath , denoted @xmath , if

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

where @xmath denotes equality in distribution, @xmath is a scalar random
variable with cumulative distribution function @xmath , @xmath is a
fixed @xmath matrix such that @xmath , and @xmath is uniformly
distributed over the unit sphere @xmath and independent of @xmath . Note
that distribution @xmath is well defined, as if @xmath , then there
exists a unitary matrix @xmath such that @xmath , and @xmath . Along
these same lines, we note that representation ( 3.3 ) is not unique, as
one may replace the pair @xmath with @xmath for any constant @xmath and
any orthogonal matrix @xmath . To avoid such ambiguity, in the following
we allow @xmath to be any matrix satisfying @xmath , and noting that the
covariance matrix of @xmath is a multiple of the identity, we further
impose the condition that the covariance matrix of @xmath is equal to
@xmath , i.e. @xmath .

Alternatively, the mean-zero elliptically symmetric distribution can be
defined uniquely via its characteristic function

  -- -- --
        
  -- -- --

where @xmath is called the characteristic generator of @xmath . For
further details information about elliptically distribution, see [
elliptical-paper ] for details.

An important special case of the family @xmath of elliptical
distributions is the Gaussian distribution @xmath , where @xmath with
@xmath , and the characteristic generator is @xmath .

The following elliptical symmetry property, generalizing the well known
fact for the conditional distribution of the multivariate Gaussian,
plays an important role in our subsequent analysis, see [
elliptical-paper ] :

###### Proposition 3.2.1.

Let @xmath , where are of dimension @xmath and @xmath respectively, with
@xmath . Let @xmath be partitioning accordingly as

  -- -- --
        
  -- -- --

Then, whenever @xmath has full rank, the conditional distribution of
@xmath given @xmath is elliptical @xmath , where

  -- -------- --
     @xmath   
  -- -------- --

and @xmath is the cumulative distribution function of @xmath given
@xmath .

Note that @xmath is always nonnegative, hence @xmath is well defined,
since by ( 3.3 ) we have

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the matrix consisting of the last @xmath rows of @xmath
in ( 3.3 ), and where the inequality holds due to the fact that @xmath
is a projection matrix. The following corollary is easily deduced from
the theorem above:

###### Corollary 3.2.1.

If @xmath with @xmath of full rank, then for any two fixed vectors
@xmath with @xmath ,

  -- -- --
        
  -- -- --

###### Proof.

Let @xmath be an orthonormal basis in @xmath such that @xmath . Let
@xmath and consider the linear transformation

  -- -------- --
     @xmath   
  -- -------- --

Then, by ( 3.3 ), @xmath , which is centered elliptical with full rank
covariance matrix @xmath . Applications of Theorem 3.2.1 with @xmath and
@xmath yields

  -- -------- -------- --
                       
     @xmath            
     @xmath   @xmath   
  -- -------- -------- --

where in the second to last equality we have used the fact that the
conditional distribution of @xmath given @xmath is elliptical with mean
zero. ∎

#### 3.2.2 Geometry.

###### Definition 3.2.1 (Restricted set).

Given @xmath , the @xmath -restricted set of the norm @xmath at @xmath
is defined as

  -- -- -- -------
           (3.4)
  -- -- -- -------

###### Definition 3.2.2 (Restricted compatibility).

The restricted compatibility constant of a set @xmath with respect to
the norm @xmath is given by

  -- -------- --
     @xmath   
  -- -------- --

###### Remark 3.2.1.

The restricted set from the definition 3.2.1 is not necessarily convex.
However, if the norm @xmath is decomposable (see definition 3.7.1 ),
then the restricted set is contained in a convex cone, and the
corresponding restricted compatibility constant is easier to estimate.
Decomposable norms have been introduced by [ general-m-estimator ] and
later appeared in a number of works, e.g. [ m-estimator-2 ] and
references therein. For reader’s convenience, we provide a
self-contained discussion in Appendix 3.7 .

### 3.3 Main Results

In this section, we define a version of Lasso estimator that is
well-suited for heavy-tailed measurements, and state its performance
guarantees.

We will assume that @xmath are i.i.d. copies of an isotropic vector
@xmath with spherically symmetric distribution @xmath . If @xmath for
some positive definite matrix @xmath , then by definition @xmath , and
@xmath , where @xmath . Hence, if we set @xmath , then all results that
we establish for isotropic measurements hold with @xmath replaced by
@xmath ; remark after Theorem 3.3.1 includes more details.

#### 3.3.1 Description of the proposed estimator.

We first introduce an estimator under the scenario that @xmath , for
some known closed set @xmath . Define the loss function @xmath as

  -- -- -- -------
           (3.5)
  -- -- -- -------

which is the unbiased estimator of

  -- -- --
        
  -- -- --

where the last equality follows since @xmath is isotropic. Clearly,
minimizing @xmath over any set @xmath is equivalent to minimizing the
quadratic loss @xmath . If distribution @xmath has heavy tails, the
sample average @xmath might not concentrate sufficiently well around its
mean, hence we replace it by a more “robust” version obtained via
truncation. Let @xmath , @xmath be such that @xmath (so that @xmath ,
and set

  -- -------- -- -------
     @xmath      (3.6)
     @xmath      
  -- -------- -- -------

so that @xmath and @xmath is uniformly distributed on the sphere of
radius @xmath , implying that its covariance matrix is @xmath , the
identity matrix. Next, define the truncated random variables

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

where @xmath for some @xmath that is chosen based on the integrability
properties of @xmath , see ( 3.16 ). Finally, set

  -- -- -- -------
           (3.8)
  -- -- -- -------

and define the estimator @xmath as the solution to the constrained
optimization problem:

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

We will also denote

  -- -- -- --------
           (3.10)
  -- -- -- --------

For the scenarios where structure on the unknown @xmath is induced by a
norm @xmath (e.g., if @xmath is sparse, then @xmath could be the @xmath
norm), we will also consider the estimator @xmath defined via

  -- -------- -- --------
     @xmath      (3.11)
  -- -------- -- --------

where @xmath is a regularization parameter to be specified, and @xmath
is defined in ( 3.8 ).

Let us note that truncation approach has previously been successfully
implemented by [ truncation-paper ] to handle heavy-tailed noise in the
context of matrix recovery with sub-Gaussian design. In the present
chapter, we show that truncation-based approach is also useful in the
situations where the measurements are heavy-tailed.

###### Remark 3.3.1.

Note that our estimator ( 3.11 ) is in general much easier to implement
than some other popular alternatives, such as the usual Lasso estimator
[ tibshirani1996regression ] . For example, when the signal @xmath is
sparse, our estimator takes the form

  -- -- --
        
  -- -- --

which yields a closed form solution in the form of “soft-thresholding”.
Specifically, let @xmath , then, the @xmath -th entry of @xmath takes
the form:

  -- -- -- --------
           (3.12)
  -- -- -- --------

We should note however that such simplification comes at the cost of
knowing the distribution of measurement vector @xmath . Despite being of
low computational complexity, our estimator can still exploit the
structure of the problem, while being robust both to the possible model
misspecification as well as to data corruption modeled by the
heavy-tailed distributions. We demonstrate this in the following
sections.

###### Remark 3.3.2 (Non-isotropic measurements).

When @xmath for some @xmath , then estimator ( 3.9 ) has to be replaced
by

  -- -- -- --------
           (3.13)
  -- -- -- --------

which is equivalent to

  -- -- --
        
  -- -- --

is a sense that @xmath . Hence, results obtained for isotropic
measurements easily extend to the more general case. Similarly,
estimator ( 3.11 ) should be replaced by

  -- -- -- --------
           (3.14)
  -- -- -- --------

which is equivalent to

  -- -- --
        
  -- -- --

meaning that @xmath .

#### 3.3.2 Estimator performance guarantees.

In this section, we present the probabilistic guarantees for the
performance of the estimators @xmath and @xmath defined by ( 3.9 ) and (
3.11 ) respectively.

Everywhere below, @xmath denote numerical constants; when these
constants depend on parameters of the problem, we specify this
dependency by writing @xmath . Let

  -- -- -- --------
           (3.15)
  -- -- -- --------

and assume that @xmath and @xmath .

###### Theorem 3.3.1.

Suppose that @xmath . Moreover, suppose that for some @xmath

  -- -------- -- --------
     @xmath      (3.16)
  -- -------- -- --------

Then there exist constants @xmath such that @xmath satisfies

  -- -- --
        
  -- -- --

for any @xmath and @xmath .

###### Remark 3.3.3.

1.   Unknown link function @xmath enters the bound only through the
    constant @xmath defined in ( 3.15 ).

2.   Aside from independence, conditions on the noise @xmath are
    implicit and follow from assumptions on @xmath . In the special case
    when the error is additive, that is, when @xmath , the moment
    condition ( 3.16 ) becomes @xmath , for which it is sufficient to
    assume that @xmath and @xmath .

3.   Theorem 3.3.1 is mainly useful when @xmath lies on the boundary of
    the set @xmath . Otherwise, if @xmath belongs to the relative
    interior of @xmath , the descent cone @xmath is the affine hull of
    @xmath (which will often be the whole space @xmath ). Thus, in such
    cases the Gaussian mean width @xmath can be on the order of @xmath ,
    which is prohibitively large when @xmath . We refer the reader to [
    plan2016generalized , plan2014high ] for a discussion of related
    result and possible ways to tighten them.

Next, we present performance guarantees for the unconstrained estimator
( 3.11 ).

###### Theorem 3.3.2.

Assume that the norm @xmath dominates the 2-norm, i.e. @xmath . Let
@xmath , and suppose that for some @xmath

  -- -------- --
     @xmath   
  -- -------- --

Then there exist constants @xmath such that for all @xmath

  -- -- --
        
  -- -- --

for any @xmath and @xmath , where @xmath is the unit ball of @xmath
norm, and @xmath and @xmath are given in Definitions 3.2.1 and 3.2.2
respectively.

###### Remark 3.3.4 (Non-isotropic measurements).

It follows from remark 3.3.2 and ( 3.13 ) that, whenever @xmath ,
inequality of Theorem 3.3.1 has the form

  -- -- --
        
  -- -- --

which can be further combined with the bound

  -- -- --
        
  -- -- --

that follows from remark 1.7 in [ plan2016generalized ] . Similarly, the
inequality of Theorem 3.3.2 holds with

  -- -------- --
     @xmath   
  -- -------- --

the unit ball of @xmath norm, in place of @xmath . Namely, for all
@xmath ,

  -- -- --
        
  -- -- --

Note that @xmath . Moreover, we show in Appendix 3.7 that for a class of
decomposable norms (which includes @xmath and nuclear norm), the upper
bounds for @xmath and @xmath differ by the factor of @xmath .

#### 3.3.3 Examples.

We discuss two popular scenarios: estimation of the sparse vector and
estimation of the low-rank matrix.

Estimation of the sparse signal. Assume that there exists @xmath of
cardinality @xmath such that @xmath for @xmath . Let @xmath , with
@xmath defined in ( 3.15 ). In this case, it is well-known that @xmath ,
see proposition 3.10 in [ chandrasekaran2012convex ] , hence Theorem
3.3.1 implies that, with high probability,

  -- -- -- --------
           (3.17)
  -- -- -- --------

as long as @xmath .

We compare this bound to result of Theorem 3.3.2 for constrained
estimator. Let @xmath be the @xmath norm. It is well-know that @xmath ,
where @xmath . Moreover, we show in Appendix 3.7 that @xmath . Hence,
for @xmath , Theorem 3.3.2 implies that

  -- -- --
        
  -- -- --

with high probability whenever @xmath . This bound is only marginally
weaker than ( 3.17 ) due to the logarithmic factor, however, definition
of @xmath does not require the knowledge of @xmath , as we have already
mentioned before.

Estimation of a low-rank matrix. Assume that @xmath with @xmath , and
@xmath has rank @xmath . Let @xmath . Then the Gaussian mean width of
the intersection of a descent cone with a unit ball is bounded as @xmath
, see proposition 3.11 in [ chandrasekaran2012convex ] , hence Theorem
3.3.1 yields that, with high probability,

  -- -- --
        
  -- -- --

as long as the number of observations satisfies @xmath .

Finally, we derive the corresponding bound from Theorem 3.3.2 . The
Gaussian mean width of the unit ball in the nuclear norm is bounded by
@xmath , see proposition 10.3 in [ vershynin2015estimation ] . It
follows from results in Appendix 3.7 that @xmath . Theorem 3.3.2 now
implies that with high probability

  -- -- --
        
  -- -- --

which matches the bound of Theorem 3.3.1 .

### 3.4 Numerical Experiments

In this section, we demonstrate the performance of proposed robust
estimator ( 3.11 ) for one-bit compressed sensing model. The model takes
the following form:

  -- -- -- --------
           (3.18)
  -- -- -- --------

where @xmath is the additive noise and the parameter @xmath is assumed
to be @xmath -sparse. This model is highly non-linear because one can
only observe the sign of each measurement.

The 1-bit compressed sensing model was previously discussed extensively
in a number of works [ plan2014high , ai2014one , plan2016generalized ]
. It was shown that when the measurement vectors are either Gaussian or
sub-Gaussian, the Lasso estimator recovers the support of @xmath with
high probability. Here, we show that under the heavy-tailed elliptically
distributed measurements, our estimator numerically outperforms the
standard Lasso estimator

  -- -------- --
     @xmath   
  -- -------- --

while taking the form of a simple soft-thresholding as explained in (
3.12 ).

In the first numerical experiment, data are simulated in the following
way: @xmath are i.i.d. with spherically symmetric distribution @xmath .
The random vectors @xmath are i.i.d. with uniform distribution over the
sphere of radius @xmath , and the random variables @xmath are also
i.i.d., independent of @xmath and such that

  -- -------- -- --------
     @xmath      (3.19)
  -- -------- -- --------

where @xmath and @xmath , @xmath are i.i.d. with Pareto distribution,
meaning that their probability density function is given by

  -- -------- --
     @xmath   
  -- -------- --

@xmath , and @xmath . The true signal @xmath has sparsity level @xmath ,
with index of each non-zero coordinate chosen uniformly at random, and
the magnitude having uniform distribution on @xmath .

Since we can only recover the original signal @xmath up to scaling,
define the relative error for any estimator @xmath with respect to
@xmath as follows:

  -- -- -- --------
           (3.20)
  -- -- -- --------

In each of the following two scenarios, we run the experiment 200 times
for both the Lasso estimator and the estimator defined in ( 3.11 ) with
@xmath being the @xmath norm. We set the truncation level as @xmath ,
and the values of @xmath and regularization parameter @xmath are
obtained via the standard 2-fold cross validation for the relative error
( 3.20 ). We then plot the histogram of obtained results over 200 runs
of the experiment.

In the first scenario, we set the additive error @xmath in the 1-bit
model ( 3.18 ) and plot the histogram in Fig. 3.2 . We can see from the
plot that the robust estimator ( 3.11 ) noticeably outperforms the Lasso
estimator.

In the second scenario, we set the additive error @xmath to be i.i.d.
heavy tailed noise with signal-to-noise ratio (SNR) ¹ ¹ 1 The
signal-to-noise ratio (dB) is defined as @xmath . In our case, since
@xmath can be positive or negative with equal probability, @xmath , and
thus, @xmath . equal to 10dB, so that the noise has the distribution

  -- -------- --
     @xmath   
  -- -------- --

and @xmath are i.i.d. random variables with Pareto distribution, see (
3.19 ). The results are plotted in Fig. 3.2 . The histogram shows that,
while performance of the Lasso estimator becomes worse, results of
robust estimator ( 3.11 ) are relatively stable.

In the second simulation study, the simulation framework similar to the
second scenario above, the only difference being the increased sample
size @xmath . The results are plotted in Fig. 3.5 - 3.5 with sample
sizes @xmath and 512, respectively.

### 3.5 Proofs.

This section is devoted to the proofs of Theorems 3.3.1 and 3.3.2 .

#### 3.5.1 Preliminaries.

We recall several useful facts from probability theory that we rely on
in the subsequent analysis.

The following well-known bound shows that the uniform distribution on a
high-dimensional sphere enjoys strong concentration properties.

###### Lemma 3.5.1 (Lemma 2.2 of [convex-geometry-Ball]).

Let @xmath have the uniform distribution on @xmath . Then for any @xmath
and any fixed @xmath ,

  -- -- --
        
  -- -- --

Next, we state several useful results from the theory of empirical
processes.

###### Definition 3.5.1 (@xmath-norm).

For @xmath , the @xmath -norm of a random variable @xmath is given by

  -- -- --
        
  -- -- --

Specifically, the cases @xmath and @xmath are known as the
sub-exponential and sub-Gaussian norms respectively. We will say that
@xmath is sub-exponential if @xmath , and @xmath is sub-Gaussian if
@xmath .

###### Remark 3.5.1.

It is easy to check that @xmath -norm is indeed a norm.

###### Remark 3.5.2.

A useful property, equivalent to the previous definition of a
sub-Gaussian random variable @xmath , is that there exists a positive
constant @xmath such that

  -- -- --
        
  -- -- --

For the proof, see Lemma 5.5 in [ introduction-to-random-matrix ] .

###### Definition 3.5.2 (sub-Gaussian random vector).

A random vector @xmath is called sub-Gaussian if there exists @xmath
such that @xmath for any @xmath . The corresponding sub-Gaussian norm is
then

  -- -------- --
     @xmath   
  -- -------- --

Next, we recall the notion of the generic chaining complexity. Let
@xmath be a metric space. We say a collection @xmath of subsets of
@xmath is increasing when @xmath for all @xmath .

###### Definition 3.5.3 (Admissible sequence).

An increasing sequence of subsets @xmath of @xmath is admissible if
@xmath , where @xmath and @xmath .

For each @xmath , define the map @xmath as @xmath . Note that, since
each @xmath is a finite set, the minimum is always achieved. When the
minimum is achieved for multiple elements in @xmath , we break the ties
arbitrarily. The generic chaining complexity @xmath is defined as

  -- -------- -- --------
     @xmath      (3.21)
  -- -------- -- --------

where the infimum is over all admissible sequences. The following
theorem tells us that @xmath -functional controls the “size” of a
Gaussian process.

###### Lemma 3.5.2 (Theorem 2.4.1 of [Talagrand-book-2]).

Let @xmath be a centered Gaussian process indexed by the set @xmath ,
and let

  -- -- --
        
  -- -- --

Then, there exists a universal constant @xmath such that

  -- -- --
        
  -- -- --

Let @xmath be a semi-metric space, and let @xmath be independent
stochastic processes indexed by @xmath such that @xmath for all @xmath
and @xmath . We are interested in bounding the supremum of the empirical
process

  -- -- -- --------
           (3.22)
  -- -- -- --------

The following well-known symmetrization inequality reduces the problem
to bounds on a (conditionally) Rademacher process @xmath , where @xmath
are i.i.d. Rademacher random variables (meaning that they take values
@xmath with probability @xmath each), independent of @xmath ’s.

###### Lemma 3.5.3 (Symmetrization inequalities).

  -- -------- --
     @xmath   
  -- -------- --

and for any @xmath , we have

  -- -- --
        
  -- -- --

###### Proof.

See Lemmas 6.3 and 6.5 in [ Talagrand-book ] ∎

Finally, we recall Bernstein’s concentration inequality.

###### Lemma 3.5.4 (Bernstein’s inequality).

Let @xmath be a sequence of independent centered random variables.
Assume that there exist positive constants @xmath and @xmath such that
for all integers @xmath

  -- -- --
        
  -- -- --

then

  -- -- --
        
  -- -- --

In particular, if @xmath are all sub-exponential random variables, then
@xmath and @xmath can be chosen as @xmath and @xmath .

#### 3.5.2 Roadmap of the proof of Theorem 3.3.1.

We outline the main steps in the proof of Theorem 3.3.1 , and postpone
some technical details to sections 3.5.4 and 3.5.5 .

As it will be shown below in Lemma 3.5.5 , @xmath for @xmath and @xmath
, hence

  -- -------- -------- -- --------
     @xmath               
              @xmath      
                          (3.23)
  -- -------- -------- -- --------

where @xmath stands for the conditional expectation given @xmath , and
where we used the equality @xmath in the last step. Since @xmath
minimizes @xmath , @xmath , and

  -- -------- -- --
     @xmath      
                 
  -- -------- -- --

Note that @xmath ; dividing both sides of the inequality by @xmath , we
obtain

  -- -- -- --------
           (3.24)
  -- -- -- --------

To get the desired bound, it remains to estimate two terms above. The
bound for the first term is implied by Lemma 3.5.8 : setting @xmath ,
and observing that the diameter @xmath , we get that with probability
@xmath ,

  -- -- --
        
  -- -- --

To estimate the second term, we apply Lemma 3.5.7 :

  -- -- --
        
  -- -- --

Result of Theorem 3.3.1 now follows from the combination of these
bounds. ∎

#### 3.5.3 Roadmap of the proof of Theorem 3.3.2.

Once again, we will present the main steps while skipping the technical
parts. Lemma 3.5.5 implies that @xmath for @xmath and

  -- -------- --
     @xmath   
  -- -------- --

Thus, arguing as in ( 3.23 ),

  -- -------- -------- --
     @xmath   @xmath   
                       
  -- -------- -------- --

Since @xmath is a solution of problem ( 3.11 ), it follows that

  -- -- --
        
  -- -- --

which further implies that

  -- -------- -- -- --------
     @xmath         
                    
     @xmath         
                    (3.25)
  -- -------- -- -- --------

Letting @xmath be the dual norm of @xmath (meaning that @xmath ), the
first term in ( 3.25 ) can be estimated as

  -- -- -- --------
           (3.26)
  -- -- -- --------

Since

  -- -- --
        
  -- -- --

lemma 3.5.8 applies with @xmath . Together with an observation that
@xmath (due to the assumption @xmath ), this yiels

  -- -- --
        
  -- -- --

for any @xmath and some constants @xmath . For the second term in ( 3.25
), we use Lemma 3.5.7 to obtain

  -- -- --
        
  -- -- --

for some constant @xmath , where we have again applied the inequality
@xmath . Combining the above two estimates gives that with probability
at least @xmath ,

  -- -- -- --------
           (3.27)
  -- -- -- --------

for some constant @xmath and any @xmath . Since @xmath by assumption,
and the right hand side of ( 3.27 ) is nonnegative, it follows that

  -- -------- --
     @xmath   
  -- -------- --

This inequality implies that @xmath . Finally, from ( 3.27 ) and the
triangle inequality,

  -- -------- --
     @xmath   
  -- -------- --

Dividing both sides by @xmath gives

  -- -- --
        
  -- -- --

This finishes the proof of Theorem 3.3.2 .

#### 3.5.4 Bias of the truncated mean.

The following lemma is motivated by and is similar to Theorem 2.1 in [
li1989regression ] .

###### Lemma 3.5.5.

Let @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

and for any @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Since @xmath , we have that for any @xmath

  -- -------- -------- --
              @xmath   
     @xmath            
     @xmath            
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where the third equality follows from the fact that the noise @xmath is
independent of the measurement vector @xmath , the second to last
equality from the properties of elliptically symmetric distributions
(Corollary 3.2.1 ), and the last equality from the definition of @xmath
. Thus,

  -- -------- -- --
     @xmath      
  -- -------- -- --

which is minimized at @xmath . Furthermore, @xmath , hence

  -- -------- --
     @xmath   
  -- -------- --

finishing the proof. ∎

Next, we estimate the “bias term” @xmath in inequality ( 3.24 ). In
order to do so, we need the following preliminary result.

###### Lemma 3.5.6.

If @xmath , then the unit random vector @xmath is uniformly distributed
over the unit sphere @xmath . Furthermore, @xmath is a sub-Gaussian
random vector with sub-Gaussian norm @xmath independent of the dimension
@xmath .

###### Proof.

First, we use decomposition ( 3.3 ) for elliptical distribution together
with our assumption that @xmath is the identity matrix, to write @xmath
, which implies that

  -- -------- --
     @xmath   
  -- -------- --

with the final distributional equality holding as @xmath , and hence its
uniform distribution, is invariant with respect to reflections across
any hyperplane through the origin.

To prove the second claim, it is enough to show that @xmath with
constant @xmath independent of @xmath . By the first claim and Lemma
3.5.1 , we have

  -- -- --
        
  -- -- --

Choosing @xmath gives

  -- -- --
        
  -- -- --

By an equivalent definition of sub-Gaussian random variables (Lemma 5.5
of [ introduction-to-random-matrix ] ), this inequality implies that
@xmath , hence finishing the proof. ∎

With the previous lemma in hand, we now establish the following result.

###### Lemma 3.5.7.

Under the assumptions of Theorem 3.3.1 , there exists a constant @xmath
such that

  -- -- --
        
  -- -- --

for all @xmath .

###### Proof.

By ( 3.6 ), we have that @xmath , thus the claim is equivalent to

  -- -- --
        
  -- -- --

Since @xmath , we have @xmath , and it follows that

  -- -------- -- --
                 
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

where the second to last inequality uses Cauchy-Schwarz, and the last
inequality follows from Hölder’s inequality.

For the first term, by Lemma 3.5.6 , @xmath is sub-Gaussian with @xmath
independent of @xmath . Thus, by the definition of the @xmath norm and
the fact that @xmath ,

  -- -- --
        
  -- -- --

Recall that @xmath . Then, the second term is bounded by @xmath . For
the final term, since @xmath , Markov’s inequality implies that

  -- -- --
        
  -- -- --

Combining these inequalities yields

  -- -- --
        
  -- -- --

completing the proof. ∎

#### 3.5.5 Concentration via generic chaining.

In the following sections, we will use @xmath to denote constants that
are either absolute, or depend on underlying parameters @xmath and
@xmath (in the latter case, we specify such dependence). To make
notation less cumbersome, constants denoted by the same letter ( @xmath
, etc.) might be different in various parts of the proof.

The goal of this subsection is to prove the following inequality:

###### Lemma 3.5.8.

Suppose @xmath and @xmath are as defined according to ( 3.6 ) and ( 3.7
) respectively. Then, for any bounded subset @xmath ,

  -- -- --
        
  -- -- --

for any @xmath , a positive constant @xmath and an absolute constant
@xmath . Here

  -- -------- -- --------
     @xmath      (3.28)
  -- -------- -- --------

The main technique we apply is the generic chaining method developed by
M. Talagrand [ Talagrand-book-2 ] for bounding the supremum of
stochastic processes. Recently, [ Mendelson-1 ] and [
tail-bound-chaining ] advanced the technique to obtain a sharp bound for
supremum of processes index by squares of functions. More recently, [
Mendelson-2 ] proved a concentration result for the supremum of
multiplier processes under weak moment assumptions. In the current work,
we show that exponential-type concentration inequalities for multiplier
processes, such as the one in Lemma 3.5.8 , are achievable by applying
truncation under a bounded @xmath -moment assumption.

Define

  -- -------- -- --
     @xmath      
     @xmath      
  -- -------- -- --

where @xmath is a bounded set in @xmath and @xmath is a sequence i.i.d.
Rademacher random variables taking values @xmath with probability @xmath
each, and independent of @xmath . Result of Lemma 3.5.8 easily follows
from the following concentration inequality:

###### Lemma 3.5.9.

For any @xmath ,

  -- -- -- --------
           (3.29)
  -- -- -- --------

where @xmath is another constant possibly different from that of Lemma
3.5.8 , and @xmath is an absolute constant.

To deduce the inequality of Lemma 3.5.8 , we first apply the
symmetrization inequality (Lemma 3.5.3 ), followed by Lemma 3.6.1 with
@xmath . It implies that

  -- -- --
        
  -- -- --

Application of the second bound of the symmetrization lemma with @xmath
and ( 3.29 ) completes the proof of Lemma 3.5.8 .

It remains to justify ( 3.29 ). We start by picking an arbitrary point
@xmath such that there exists an admissible sequence @xmath satisfying

  -- -------- -- --------
     @xmath      (3.30)
  -- -------- -- --------

where we recall that @xmath is the closest point map from @xmath to
@xmath and the factor 2 is introduced so as to deal with the case where
the infimum in the definition ( 3.21 ) of @xmath is not achieved. Then,
write @xmath as the telescoping sum:

  -- -- --
        
  -- -- --

We claim that the telescoping sum converges with probability 1 for any
@xmath . Indeed, note that for each fixed set of realizations of @xmath
and @xmath , each summand is bounded as

  -- -------- --
     @xmath   
  -- -------- --

Furthermore, since @xmath is a compact subset of @xmath , its Gaussian
mean width is finite. Thus, by lemma 3.5.2 , @xmath . This inequality
further implies that the sum on the left hand side of ( 3.30 ) converges
with probability 1.

Next, with @xmath being fixed, we split the index set @xmath into the
following three subsets:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

By the assumptions in Theorem 3.3.1 and the bound @xmath , we have that
@xmath , implying that @xmath , and hence these three index sets are
well defined. Depending on @xmath , some of them might be empty, but
this only simplifies our argument by making the partial sum over such an
index set equal 0.

The following argument yields a bound for @xmath , assuming all three
index sets are nonempty. Specifically, we show that

  -- -- -- --------
           (3.31)
  -- -- -- --------

for @xmath and @xmath , respectively.

##### The case @xmath.

###### Proof of inequality (3.31) for the index set @xmath.

Recall that @xmath .

For each @xmath we apply Bernstein’s inequality (Lemma 3.5.4 ) to
estimate each summand

  -- -- --
        
  -- -- --

For any integer @xmath , we have the following chains of inequalities:

  -- -------- -- --
                 
     @xmath      
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

where the second inequality follows from the truncation bound, the third
from Hölder’s inequality, and the last from the assumption that @xmath
and the following bound: by Lemma 3.5.6 , @xmath is sub-Gaussian, hence
for any @xmath

  -- -- --
        
  -- -- --

We also note that @xmath does not depend on @xmath by Lemma 3.5.6 .
Next, by Stirling’s approximation, @xmath , thus there exist constants
@xmath and @xmath such that

  -- -- --
        
  -- -- --

Bernstein’s inequality (Lemma 3.5.4 ), with @xmath , @xmath with @xmath
now implies

  -- -- --
        
  -- -- --

for any @xmath . Taking @xmath , noting that as @xmath by assumption, we
have @xmath , and since @xmath , @xmath . In turn, this implies

  -- -------- --
     @xmath   
  -- -------- --

where the last inequality follows from the fact that @xmath is dominated
by @xmath for all @xmath . This inequality implies that there exists a
positive constant @xmath such that for any @xmath

  -- -- -- --------
           (3.32)
  -- -- -- --------

where for all @xmath and @xmath we let

  -- -- --
        
  -- -- --

Notice that for each @xmath the number of pairs @xmath appearing in the
sum in ( 3.31 ) can be bounded by @xmath . Thus, by a union bound and (
3.32 ),

  -- -- --
        
  -- -- --

and hence,

  -- -------- -------- --
              @xmath   
     @xmath            
  -- -------- -------- --

for some absolute constant @xmath , where in the last inequality we use
the fact @xmath to get a geometrically decreasing sequence. Thus, on the
complement of the event @xmath , we have that with probability at least
@xmath ,

  -- -------- -------- --
                       
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

for @xmath , where the last inequality follows from triangle inequality
@xmath and ( 3.30 ). This proves the inequality ( 3.31 ) for @xmath . ∎

##### The case @xmath.

This is the most technically involved case of the three. For any fixed
@xmath and @xmath , we let @xmath and @xmath . Then @xmath and

  -- -------- -- --------
     @xmath      (3.33)
  -- -------- -- --------

For every fixed @xmath and fixed @xmath , we bound the summation using
the following inequality

  -- -- --
        
  -- -- --

where @xmath is the non-increasing rearrangement of @xmath and @xmath is
a sequence of i.i.d. Rademancher random variables independent of @xmath
.

###### Remark 3.5.3.

This bound was first stated and proved in [ Rademancher-sums ] with a
sequence of fixed constants @xmath . The current form can be obtained
using independence property and conditioning on @xmath . Furthermore, [
Rademancher-sums ] tells us that the optimal choice of @xmath is at
@xmath Applications of this inequality to generic chaining-type
arguments were previously introduced by [ Mendelson-2 ] .

Letting @xmath be the set of indices of the variables corresponding to
the @xmath largest coordinates of @xmath and of @xmath , we have @xmath
and with probability at least @xmath

  -- -- -- -- --------
              
              
              
              (3.34)
  -- -- -- -- --------

where the second to last inequality is a consequence of Hölder’s
inequality. We take @xmath . The key is to pick an appropriate cut point
@xmath for each @xmath . Here, we choose @xmath , which makes @xmath and
also guarantees that @xmath ; see Lemma 4.19 . Under this choice, we
have the following lemma:

###### Lemma 3.5.10.

Let @xmath , @xmath and @xmath be the nonincreasing rearrangement of
@xmath . Then there exists an absolute constant @xmath such that for all
@xmath ,

  -- -- --
        
  -- -- --

###### Proof.

By Lemma 3.5.6 , we know that @xmath are i.i.d. sub-Gaussian random
variables. Thus, by Lemma 3.6.2 , @xmath is sub-exponential with norm

  -- -------- -- --------
     @xmath      (3.35)
  -- -------- -- --------

It then follows from Bernstein’s inequality (Lemma 3.5.4 ) that for any
fixed set @xmath with @xmath ,

  -- -- --
        
  -- -- --

We choose @xmath . Since @xmath , the factor @xmath dominates the right
hand side. Noting that @xmath , we obtain

  -- -- --
        
  -- -- --

where @xmath ; note that the upper bound for @xmath is independent of
@xmath by Lemma 3.5.1 . Thus,

  -- -------- -------- --
                       
     @xmath            
     @xmath            
     @xmath   @xmath   
     @xmath            
  -- -------- -------- --

where the last step follows from @xmath , an inequality proved in
Appendix 3.6 . ∎

###### Lemma 3.5.11.

Let @xmath , @xmath and @xmath be the non-increasing rearrangement of
@xmath . Then

  -- -- --
        
  -- -- --

for any @xmath and some constant @xmath .

###### Proof.

To avoid possible confusion, we use @xmath to index the nonincreasing
rearrangement and @xmath for the original sequence. We start by noting
that @xmath are i.i.d. sub-Gaussian random variables with @xmath . By an
equivalent definition of sub-Gaussian random variables (Lemma 5.5. of [
introduction-to-random-matrix ] ), we have for any fixed @xmath ,

  -- -- -- --------
           (3.36)
  -- -- -- --------

for any @xmath and an absolute constant @xmath .

To establish the claim of the lemma, we bound each @xmath separately for
@xmath and then combine individual bounds. Instead of using a fixed
value of @xmath in ( 3.36 ), our choice of @xmath will depend on the
index @xmath . Specifically, for each @xmath , we choose @xmath with

  -- -- -- --------
           (3.37)
  -- -- -- --------

The reason for this choice will be clear as we proceed.

First, for a fixed nonincreasing rearrangement index @xmath , by ( 3.36
) and the fact that

  -- -- --
        
  -- -- --

we have

  -- -------- --
              
     @xmath   
  -- -------- --

To simplify notation, let @xmath (note that it depends only on @xmath ).
It then follows that

  -- -------- -- --
                 
     @xmath      
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

By a union bound, we have

  -- -------- -- --
                 
     @xmath      
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

where the second to last inequality follows since by the definition (
3.37 ) of @xmath , @xmath , the function @xmath is monotonically
decreasing with respect to @xmath (recall that @xmath ), and thus is
dominated by @xmath . The final inequality follows from Lemma 4.18 as
well as the fact that @xmath . Furthermore, by Lemma 4.19 in the
Appendix 3.6 and ( 3.37 ) implying @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

Overall, we have the following bound:

  -- -- --
        
  -- -- --

Thus, with probability at least @xmath ,

  -- -- --
        
  -- -- --

hence with the same probability

  -- -------- -------- --
                       
     @xmath            
     @xmath   @xmath   
  -- -------- -------- --

and the desired result follows. ∎

###### Lemma 3.5.12.

The following inequalities hold for any @xmath :

  -- -- --
        
        
  -- -- --

for some positive constants @xmath .

###### Proof.

Recall that @xmath , @xmath , and @xmath . Thus, @xmath , and for any
integer @xmath , we have

  -- -- --
        
  -- -- --

Thus, for any @xmath ,

  -- -- --
        
  -- -- --

By Bernstein’s inequality (Lemma 3.5.4 ), with probability at least
@xmath ,

  -- -- -------- --
                 
        @xmath   
  -- -- -------- --

which implies the first claim. To establish the second claim, note that
for any @xmath ,

  -- -------- -------- --
                       
     @xmath            
     @xmath   @xmath   
  -- -------- -------- --

where we used the fact that @xmath to obtain the third inequality.
Bernstein’s inequality implies that with probability at least @xmath ,

  -- -- --
        
  -- -- --

which yields the second part of the claim. ∎

###### Proof of inequality (3.31) for the index set @xmath.

Combining Lemmas 3.5.10 and 3.5.11 with the inequality ( 3.34 ), and
setting @xmath , we get that with probability at least @xmath , for all
@xmath ,

  -- -------- -------- --
     @xmath   @xmath   
                       
  -- -------- -------- --

for some constant @xmath ; note that the factor @xmath appears due to
equality ( 3.33 ). Next, we apply a chaining argument similar to the one
used in Section 3.5.5 , we obtain that with probability at least @xmath
,

  -- -- -- --------
           (3.38)
  -- -- -- --------

for a positive constant @xmath and an absolute constant @xmath . In
order to handle the remaining terms involving @xmath in ( 3.38 ), we
apply Lemma 3.5.12 , which gives

  -- -- --
        
  -- -- --

with probability at least @xmath , where @xmath and @xmath are positive
constants and @xmath . This completes the second part of the chaining
argument. ∎

##### The case @xmath.

###### Proof of inequality (3.31) for the index set @xmath.

Direct application of Cauchy-Schwartz on ( 3.33 ) yields, for all @xmath
,

  -- -- --
        
  -- -- --

where @xmath are sub-Gaussian random variables. Thus, by Lemma 3.6.2 ,
@xmath are sub-exponential with norm bounded as in ( 3.35 ). Using
Bernstein’s inequality again, we deduce that

  -- -- --
        
  -- -- --

Let @xmath . Using the fact that @xmath as well as @xmath , we see that
the term @xmath dominates the right hand side and

  -- -- --
        
  -- -- --

for some absolute constant @xmath . Thus, repeating a chaining argument
of section 3.5.5 (namely, the argument following ( 3.32 )), we obtain

  -- -- --
        
  -- -- --

with probability at least @xmath for some absolute constants @xmath .
Combining this inequality with the first claim of Lemma 3.5.12 gives

  -- -- --
        
  -- -- --

with probability at least @xmath for absolute constants @xmath and any
@xmath . This finishes the bound for the third (and final) segment of
the “chain”. ∎

##### Finishing the proof of Lemma 3.5.8

###### Proof.

So far, we have shown that

  -- -------- -------- -- --------
                          
     @xmath               
     @xmath   @xmath      (3.39)
  -- -------- -------- -- --------

with probability at least @xmath for some positive constants @xmath and
@xmath , and any @xmath . To finish the proof, it remains to bound
@xmath . With @xmath defined in ( 3.28 ), and since @xmath is an
arbitrary point in @xmath , we trivially have @xmath . Applying
Bernstein’s inequality in a way similar to Section 3.5.5 yields

  -- -- --
        
  -- -- --

for some constants @xmath and any @xmath . Choosing @xmath gives

  -- -- --
        
  -- -- --

for a constant @xmath and any @xmath . Combining this bound with ( 3.39
) shows that with probability at least @xmath ,

  -- -- --
        
  -- -- --

for @xmath , an absolute constant @xmath and all @xmath ; note that the
last inequality follows from Lemma 3.5.2 . We have established ( 3.29 ),
thus completing the proof. ∎

### 3.6 Technical Results.

###### Lemma 3.6.1.

For any nonnegative random variable @xmath , if @xmath for some
constants @xmath and all @xmath , then,

  -- -- --
        
  -- -- --

###### Proof.

Using a well known identity for the expectation of non-negative random
variables,

  -- -------- -- --
                 
     @xmath      
     @xmath      
  -- -------- -- --

∎

###### Lemma 3.6.2.

If @xmath and @xmath are sub-Gaussian random variables, then the product
@xmath is a subexponential random variable, and

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

See [ wellner1 ] .

∎

###### Lemma 3.6.3.

Let @xmath and @xmath , then, @xmath

###### Proof.

If @xmath , then, @xmath , which implies @xmath . Thus,

  -- -------- -- --
                 
     @xmath      
     @xmath      
  -- -------- -- --

where the second from last inequality follows from @xmath , and the last
inequality follows from @xmath , thus, @xmath .

On the other hand, if @xmath , then, since @xmath , @xmath finishing the
proof. ∎

###### Lemma 3.6.4.

With @xmath and @xmath , the integer @xmath satisfies @xmath , and

  -- -- --
        
  -- -- --

###### Proof.

Since @xmath , it follows that @xmath , and thus @xmath . It is then
enough to show that

  -- -- --
        
  -- -- --

Raising both sides to the power of @xmath , equivalently

  -- -- --
        
  -- -- --

Consider the function @xmath . Note that as @xmath , to prove the
inequality above it suffices to show that the @xmath is upper bounded by
the left hand side. Taking the derivative of @xmath yields

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath , the only critical point at which the global maximum
occurs is given by @xmath . As @xmath is exactly equal to the left hand
side the proof is complete. ∎

### 3.7 Decomposable Norms and Restricted Compatibility.

In this section, we recall some facts about decomposable norms that have
been introduced in [ general-m-estimator ] .

###### Definition 3.7.1.

Suppose that @xmath are two subspace of @xmath , and let @xmath be the
orthogonal complement of @xmath . Norm @xmath is said to be decomposable
with respect to @xmath if for any @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath stand for the orthogonal projectors onto @xmath
and @xmath respectively.

It is well known that many frequently used norms, including the @xmath
norm of a vector and the nuclear norm of a matrix, are decomposable with
respect to the appropriately chosen pair of subspaces. For instance, the
@xmath norm is decomposable with respect to the pair of subspaces @xmath
, where

  -- -- -- --------
           (3.40)
  -- -- -- --------

consists of sparse vectors with non-zero coordinates indexed by a set
@xmath .

Let @xmath be two linear subspaces. Then we define the subspace @xmath
via

  -- -- --
        
  -- -- --

where @xmath and @xmath are the linear subspaces spanned by the rows and
columns of @xmath respectively, and

  -- -- -- --------
           (3.41)
  -- -- -- --------

Then the nuclear norm @xmath is decomposable with respect to @xmath (see
[ general-m-estimator ] for details).

Assume that the norm @xmath is decomposable with respect to @xmath , and
let @xmath . It is clear that for any @xmath

  -- -------- -- --------
     @xmath      (3.42)
  -- -------- -- --------

Since @xmath , decomposability and the triangle inequality imply that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Substituting this bound into ( 3.42 ) gives

  -- -------- --
     @xmath   
  -- -------- --

which implies that for any @xmath

  -- -------- --
     @xmath   
  -- -------- --

It is easy to see that the set of all @xmath satisfying the inequality
above is a convex cone, which we will denote by @xmath . Since @xmath ,

  -- -- --
        
  -- -- --

by definition of the restricted compatibility constant. This inequality
is useful due to the fact that it is often easier to estimate @xmath .

Finally, we make a remark that is useful when dealing with non-isotropic
measurements. Let @xmath be a @xmath matrix, and consider the norm
corresponding to the convex set @xmath , so that @xmath . It is easy to
see that @xmath , hence

  -- -- -------- --
        @xmath   
                 
  -- -- -------- --

Example 1: @xmath norm. Let @xmath be as in ( 3.40 ) with @xmath . If
@xmath belongs to the corresponding cone @xmath , then clearly @xmath ,
where @xmath . Hence

  -- -------- --
     @xmath   
  -- -------- --

and @xmath

Example 2: nuclear norm. Let @xmath be as in ( 3.41 ). Note that for any
@xmath , @xmath , where @xmath and @xmath are the orthogonal projectors
onto subspaces @xmath and @xmath respectively. Then for any @xmath , we
have that

  -- -------- -- --------
     @xmath      (3.43)
  -- -------- -- --------

Note that

  -- -------- --
     @xmath   
  -- -------- --

hence @xmath , which yields together with ( 3.43 ) that

  -- -- --
        
  -- -- --

and @xmath

## Chapter 4 Estimation of the Covariance Structure of Heavy-tailed
Distributions

In this chapter, we propose and analyze a new estimator of the
covariance matrix that admits strong theoretical guarantees under weak
assumptions on the underlying distribution, such as existence of moments
of only low order. While estimation of covariance matrices corresponding
to sub-Gaussian distributions is well-understood, much less in known in
the case of heavy-tailed data. As K. Balasubramanian and M. Yuan write [
balasubramanian2016discussion ] , “data from real-world experiments
oftentimes tend to be corrupted with outliers and/or exhibit heavy
tails. In such cases, it is not clear that those covariance matrix
estimators .. remain optimal” and “..what are the other possible
strategies to deal with heavy tailed distributions warrant further
studies.” We make a step towards answering this question and prove tight
deviation inequalities for the proposed estimator that depend only on
the parameters controlling the “intrinsic dimension” associated to the
covariance matrix (as opposed to the dimension of the ambient space); in
particular, our results are applicable in the case of high-dimensional
observations.

### 4.1 Introduction

Estimation of the covariance matrix is one of the fundamental problems
in data analysis: many important statistical tools, such as Principal
Component Analysis(PCA) [ hotelling1933analysis ] and regression
analysis, involve covariance estimation as a crucial step. For instance,
PCA has immediate applications to nonlinear dimension reduction and
manifold learning techniques [ allard2012multi ] , genetics [
novembre2008genes ] , computational biology [ alter2000singular ] ,
among many others.

However, assumptions underlying the theoretical analysis of most
existing estimators, such as various modifications of the sample
covariance matrix, are often restrictive and do not hold for real-world
scenarios. Usually, such estimators rely on heuristic (and often
bias-producing) data preprocessing, such as outlier removal. To
eliminate such preprocessing step from the equation, one has to develop
a class of new statistical estimators that admit strong performance
guarantees, such as exponentially tight concentration around the unknown
parameter of interest, under weak assumptions on the underlying
distribution, such as existence of moments of only low order. In
particular, such heavy-tailed distributions serve as a viable model for
data corrupted with outliers – an almost inevitable scenario for
applications.

We make a step towards solving this problem: using tools from the random
matrix theory, we will develop a class of robust estimators that are
numerically tractable and are supported by strong theoretical evidence
under much weaker conditions than currently available analogues. The
term “robustness” refers to the fact that our estimators admit provably
good performance even when the underlying distribution is heavy-tailed.

#### 4.1.1 Notation

Given @xmath , let @xmath be transpose of @xmath . If @xmath is
symmetric, we will write @xmath and @xmath for the largest and smallest
eigenvalues of @xmath . Next, we will introduce the matrix norms used in
the chapter. Everywhere below, @xmath stands for the operator norm
@xmath . If @xmath , we denote by @xmath the trace of @xmath . For
@xmath , the nuclear norm @xmath is defined as @xmath , where @xmath is
a nonnegative definite matrix such that @xmath . The Frobenius (or
Hilbert-Schmidt) norm is @xmath , and the associated inner product is
@xmath . For @xmath , @xmath stands for the usual Euclidean norm of
@xmath . Let @xmath , @xmath be two self-adjoint matrices. We will write
@xmath iff @xmath is nonnegative (or positive) definite. For @xmath , we
set @xmath and @xmath . We will also use the standard Big-O and little-o
notation when necessary.

Finally, we give a definition of a matrix function. Let @xmath be a
real-valued function defined on an interval @xmath , and let @xmath be a
symmetric matrix with the eigenvalue decomposition @xmath such that
@xmath . We define @xmath as @xmath , where

  -- -- --
        
  -- -- --

Few comments about organization of the material in the rest of the
chapter: section 4.1.2 provides an overview of the related work. Section
4.2 contains the mains results of the chapter. The proofs are outlined
in section 4.4 ; longer technical arguments can be found in the
supplementary material.

#### 4.1.2 Problem formulation and overview of the existing work

Let @xmath be a random vector with mean @xmath , covariance matrix
@xmath , and assume @xmath . Let @xmath be i.i.d. copies of @xmath . Our
goal is to estimate the covariance matrix @xmath from @xmath . This
problem and its variations have previously received significant
attention by the research community: excellent expository chapters by [
cai2016 ] and [ fan2016overview ] discuss the topic in detail. However,
strong guarantees for the best known estimators hold (with few
exceptions mentioned below) under the restrictive assumption that @xmath
is either bounded with probability 1 or has sub-Gaussian distribution,
meaning that there exists @xmath such that for any @xmath of unit
Euclidean norm,

  -- -- --
        
  -- -- --

In the discussion accompanying the chapter by [ cai2016 ] , [
balasubramanian2016discussion ] write that “data from real-world
experiments oftentimes tend to be corrupted with outliers and/or exhibit
heavy tails. In such cases, it is not clear that those covariance matrix
estimators described in this article remain optimal” and “..what are the
other possible strategies to deal with heavy tailed distributions
warrant further studies.” This motivates our main goal: develop new
estimators of the covariance matrix that (i) are computationally
tractable and perform well when applied to heavy-tailed data and (ii)
admit strong theoretical guarantees (such as exponentially tight
concentration around the unknown covariance matrix) under weak
assumptions on the underlying distribution. Note that, unlike the
majority of existing literature, we do not impose any further conditions
on the moments of @xmath , or on the “shape” of its distribution, such
as elliptical symmetry.

Robust estimators of covariance and scatter have been studied
extensively during the past few decades. However, majority of rigorous
theoretical results were obtained for the class of elliptically
symmetric distributions which is a natural generalization of the
Gaussian distribution; we mention just a small subsample among the
thousands of published works. Notable examples include the Minimum
Covariance Determinant estimator and the Minimum Volume Ellipsoid
estimator which are discussed in [ hubert2008high ] , as well Tyler’s [
tyler1987distribution ] M-estimator of scatter. Works by [
fan2016overview , wegkamp2016adaptive , han2016eca ] exploit the
connection between Kendall’s tau and Pearson’s correlation coefficient [
fang1990symmetric ] in the context of elliptical distributions to obtain
robust estimators of correlation matrices. Interesting results for
shrinkage-type estimators have been obtained by [ ledoit2004well ,
ledoit2012nonlinear ] . In a recent work, [ chen2015robust ] study
Huber’s @xmath -contamination model which assumes that the data is
generated from the distribution of the form @xmath , where @xmath is an
arbitrary distribution of “outliers” and @xmath is an elliptical
distribution of “inliers”, and propose novel estimator based on the
notion of “matrix depth” which is related to Tukey’s depth function [
tukey1975mathematics ] ; a related class of problems has been studies by
[ diakonikolas2016robust ] . The main difference of the approach
investigated in this chapter is the ability to handle a much wider class
of distributions that are not elliptically symmetric and only satisfy
weak moment assumptions. Recent papers by [ catoni2016pac ] , [
giulini2015pac ] , [ fan2016eigenvector , fan2017estimation ,
fan2017robust ] and [ minsker2016sub ] are closest in spirit to this
direction. For instance, [ catoni2016pac ] constructs a robust estimator
of the Gram matrix of a random vector @xmath (as well as its covariance
matrix) via estimating the quadratic form @xmath uniformly over all
@xmath . However, the bounds are obtained under conditions more
stringent than those required by our framework, and resulting estimators
are difficult to evaluate in applications even for data of moderate
dimension. [ fan2016eigenvector ] obtain bounds in norms other than the
operator norm which the focus of the present chapter. [ minsker2016sub ]
and [ fan2016robust ] use adaptive truncation arguments to construct
robust estimators of the covariance matrix. However, their results are
only applicable to the situation when the data is centered (that is,
@xmath ). In the robust estimation framework, rigorous extension of the
arguments to the case of non-centered high-dimensional observations is
non-trivial and requires new tools, especially if one wants to avoid
statistically inefficient procedures such as sample splitting. We
formulate and prove such extensions in this chapter.

### 4.2 Main Results

Definition of our estimator has its roots in the technique proposed by [
catoni2012challenging ] . Let

  -- -- -- -------
           (4.1)
  -- -- -- -------

be the usual truncation function. As before, let @xmath be i.i.d. copies
of @xmath , and assume that @xmath is a suitable estimator of the mean
@xmath from these samples, to be specified later. We define @xmath as

  -- -- -- -------
           (4.2)
  -- -- -- -------

where @xmath is small (the exact value will be given later). It easily
follows from the definition of the matrix function that

  -- -- --
        
  -- -- --

hence it is easily computable. Note that @xmath in the neighborhood of
@xmath ; it implies that whenever all random variables @xmath are
“small” (say, bounded above by @xmath ) and @xmath is the sample mean,
@xmath is close to the usual sample covariance estimator. On the other
hand, @xmath “truncates” @xmath on level @xmath , thus limiting the
effect of outliers. Our results (formally stated below, see Theorem
4.2.1 ) imply that for an appropriate choice of @xmath ,

  -- -- --
        
  -- -- --

with probability @xmath for some positive constant @xmath , where

  -- -- --
        
  -- -- --

is the ”matrix variance”.

#### 4.2.1 Robust mean estimation

There are several ways to construct a suitable estimator of the mean
@xmath . We present the one obtained via the “median-of-means” approach.
Let @xmath . Recall that the geometric median of @xmath is defined as

  -- -- --
        
  -- -- --

Let @xmath be the confidence parameter, and set @xmath ; we will assume
that @xmath . Divide the sample @xmath into @xmath disjoint groups
@xmath of size @xmath each, and define

  -- -------- -------- -- -------
     @xmath   @xmath      
     @xmath   @xmath      (4.3)
  -- -------- -------- -- -------

It then follows from Corollary 4.1 in [ minsker2013geometric ] that

  -- -- -- -------
           (4.4)
  -- -- -- -------

#### 4.2.2 Robust covariance estimation

Let @xmath be the estimator defined in ( 4.2 ) with @xmath being the
“median-of-means” estimator ( 4.2.1 ). Then @xmath admits the following
performance guarantees:

###### Lemma 4.2.1.

Assume that @xmath , and set @xmath . Moreover, let @xmath , and suppose
that @xmath , where @xmath is an absolute constant. Then

  -- -- -- -------
           (4.5)
  -- -- -- -------

with probability at least @xmath .

###### Remark 4.2.1.

The quantity @xmath is a measure of “intrinsic dimension” akin to the
“effective rank” @xmath ; see Lemma 4.2.3 below for more details.
Moreover, note that the claim of Lemma 4.2.1 holds for any @xmath ,
rather than just for @xmath ; this “degree of freedom” allows
construction of adaptive estimators, as it is shown below.

The statement above suggests that one has to know the value of (or a
tight upper bound on) the “matrix variance” @xmath in order to obtain a
good estimator @xmath . More often than not, such information is
unavailable. To make the estimator completely data-dependent, we will
use Lepski’s method [ lepskii1992asymptotically ] . To this end, assume
that @xmath are “crude” preliminary bounds such that

  -- -------- --
     @xmath   
  -- -------- --

Usually, @xmath and @xmath do not need to be precise, and can
potentially differ from @xmath by several orders of magnitude. Set

  -- -- --
        
  -- -- --

Note that the cardinality of @xmath satisfies @xmath . For each @xmath ,
define @xmath . Define

  -- -- --
        
  -- -- --

Finally, set

  -- -- -- -------
           (4.6)
  -- -- -- -------

and @xmath . Note that the estimator @xmath depends only on @xmath , as
well as @xmath . Our main result is the following statement regarding
the performance of the data-dependent estimator @xmath :

###### Theorem 4.2.1.

Suppose @xmath , then, the following inequality holds with probability
at least @xmath :

  -- -- --
        
  -- -- --

An immediate corollary of Theorem 4.2.1 is the quantitative result for
the performance of PCA based on the estimator @xmath . Let @xmath be the
orthogonal projector on a subspace corresponding to the @xmath largest
positive eigenvalues @xmath of @xmath (here, we assume for simplicity
that all the eigenvalues are distinct), and @xmath – the orthogonal
projector of the same rank as @xmath corresponding to the @xmath largest
eigenvalues of @xmath . The following bound follows from the Davis-Kahan
perturbation theorem [ davis1970rotation ] , more specifically, its
version due to [ [ ] Theorem 3 ]Zwald2006On-the-Converge00.

###### Corollary 4.2.1.

Let @xmath , and assume that @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

with probability @xmath .

It is worth comparing the bound of Lemma 4.2.1 and Theorem 4.2.1 above
to results of the paper by [ fan2016robust ] , which constructs a
covariance estimator @xmath under the assumption that the random vector
@xmath is centered, and @xmath . More specifically, @xmath satisfies the
inequality

  -- -- -- -------
           (4.7)
  -- -- -- -------

where @xmath is an absolute constant. The main difference between ( 4.7
) and the bounds of Lemma 4.2.1 and Theorem 4.2.1 is that the latter are
expressed in terms of @xmath , while the former is in terms of @xmath .
The following lemma demonstrates that our bounds are at least as good:

###### Lemma 4.2.2.

Suppose that @xmath and @xmath . Then @xmath .

It follows from the above lemma that @xmath . Hence, By Theorem 4.2.1 ,
the error rate of estimator @xmath is bounded above by @xmath if @xmath
. It has been shown (for example, see [ lounici2014high ] ) that the
minimax lower bound of covariance estimation is of order @xmath . Hence,
the bounds of [ fan2016robust ] as well as our results imply correct
order of the error. That being said, the “intrinsic dimension” @xmath
reflects the structure of the covariance matrix and can potentially be
much smaller than @xmath , as it is shown in the next section.

#### 4.2.3 Bounds in terms of intrinsic dimension

In this section, we show that under a slightly stronger assumption on
the fourth moment of the random vector @xmath , the bound @xmath is
suboptimal, while our estimator can achieve a much better rate in terms
of the “intrinsic dimension” associated to the covariance matrix. This
makes our estimator useful in applications involving high-dimensional
covariance estimation, such as PCA. Assume the following uniform bound
on the kurtosis of linear forms @xmath :

  -- -- -- -------
           (4.8)
  -- -- -- -------

The intrinsic dimension of the covariance matrix @xmath can be measured
by the effective rank defined as

  -- -------- --
     @xmath   
  -- -------- --

Note that we always have @xmath , and it some situations @xmath , for
instance if the covariance matrix is “approximately low-rank”, meaning
that it has many small eigenvalues. The constant @xmath is closely
related to the effective rank as is shown in the following lemma (the
proof of which is included in the supplementary material):

###### Lemma 4.2.3.

Suppose that ( 4.8 ) holds. Then,

  -- -------- --
     @xmath   
  -- -------- --

As a result, we have @xmath . The following corollary immediately
follows from Theorem 4.2.1 and Lemma 4.2.3 :

###### Corollary 4.2.2.

Suppose that @xmath for an absolute constant @xmath and that ( 4.8 )
holds. Then

  -- -- --
        
  -- -- --

with probability at least @xmath .

### 4.3 Applications: Low-rank Covariance Estimation

In many data sets encountered in modern applications (for instance, gene
expression profiles [ saal2007poor ] ), dimension of the observations,
hence the corresponding covariance matrix, is larger than the available
sample size. However, it is often possible, and natural, to assume that
the unknown matrix possesses special structure, such as low rank, thus
reducing the “effective dimension” of the problem. The goal of this
section is to present an estimator of the covariance matrix that is
“adaptive” to the possible low-rank structure; such estimators are
well-known and have been previously studied for the bounded and
sub-Gaussian observations [ lounici2014high ] . We extend these results
to the case of heavy-tailed observations; in particular, we show that
the estimator obtained via soft-thresholding applied to the eigenvalues
of @xmath admits optimal guarantees in the Frobenius (as well as
operator) norm.

Let @xmath be the estimator defined in the previous section, see
equation ( 4.6 ), and set

  -- -- -- -------
           (4.9)
  -- -- -- -------

where @xmath controls the amount of penalty. It is well-known (e.g., see
the proof of Theorem 1 in [ lounici2014high ] ) that @xmath can be
written explicitly as

  -- -- --
        
  -- -- --

where @xmath and @xmath are the eigenvalues and corresponding
eigenvectors of @xmath . We are ready to state the main result of this
section.

###### Theorem 4.3.1.

For any @xmath

  -- -- -- --------
           (4.10)
  -- -- -- --------

with probability @xmath .

In particular, if @xmath and @xmath , we obtain that

  -- -- --
        
  -- -- --

with probability @xmath .

### 4.4 Proofs

#### 4.4.1 Proof of Lemma 4.2.1

The result is a simple corollary of the following statement.

###### Lemma 4.4.1.

Set @xmath , where @xmath and @xmath . Let @xmath . Then, with
probability at least @xmath ,

  -- -- --
        
  -- -- --

where @xmath is an absolute constant.

Now, by Corollary 4.5.1 in the supplement, it follows that @xmath .
Thus, assuming that the sample size satisfies @xmath , then, @xmath ,
and by some algebraic manipulations we have that

  -- -- -- --------
           (4.11)
  -- -- -- --------

For completeness, a detailed computation is given in the supplement.
This finishes the proof.

#### 4.4.2 Proof of Lemma 4.4.1

Let @xmath be the error bound of the robust mean estimator @xmath
defined in ( 4.2.1 ). Let @xmath , @xmath , @xmath , and

  -- -- --
        
  -- -- --

for any @xmath . We begin by noting that the error can be bounded by the
supremum of an empirical process indexed by @xmath , i.e.

  -- -- -- --------
           (4.12)
  -- -- -- --------

with probability at least @xmath . We first estimate the second term
@xmath . For any @xmath ,

  -- -- --
        
  -- -- --

with probability at least @xmath . It follows from Corollary 4.5.1 in
the supplement that with the same probability

  -- -- -- --------
           (4.13)
  -- -- -- --------

Our main task is then to bound the first term in ( 4.12 ). To this end,
we rewrite it as a double supremum of an empirical process:

  -- -- --
        
  -- -- --

It remains to estimate the supremum above.

###### Lemma 4.4.2.

Set @xmath , where @xmath and @xmath . Let @xmath . Then, with
probability at least @xmath ,

  -- -- --
        
  -- -- --

where @xmath is an absolute constant.

Note that @xmath by defnition, thus, @xmath . Combining the above lemma
with ( 4.12 ) and ( 4.13 ) finishes the proof.

#### 4.4.3 Proof of Theorem 4.2.1

Define @xmath , and note that @xmath . We will demonstrate that @xmath
with high probability. Observe that

  -- -- -- --
           
           
           
  -- -- -- --

where we applied ( 4.5 ) to estimate each of the probabilities in the
sum under the assumption that the number of samples @xmath and @xmath .
It is now easy to see that the event

  -- -- --
        
  -- -- --

of probability @xmath is contained in @xmath . Hence, on @xmath

  -- -- -------- --
        @xmath   
        @xmath   
  -- -- -------- --

and the claim follows.

#### 4.4.4 Proof of Theorem 4.3.1

The proof is based on the following lemma:

###### Lemma 4.4.3.

Inequality ( 4.10 ) holds on the event @xmath .

To verify this statement, it is enough to repeat the steps of the proof
of Theorem 1 in [ lounici2014high ] , replacing each occurrence of the
sample covariance matrix by its “robust analogue” @xmath .

It then follows from Theorem 4.2.1 that @xmath whenever @xmath .

### 4.5 Proof of Additional Technical Lemmas

#### 4.5.1 Preliminaries

###### Lemma 4.5.1.

Consider any function @xmath and @xmath . Suppose the following holds

  -- -- -- --------
           (4.14)
  -- -- -- --------

then, we have for any matrix @xmath ,

  -- -- --
        
  -- -- --

###### Proof.

Note that for any @xmath , @xmath , then, the claim follows immediately
from the definition of the matrix function. ∎

The above lemma is useful in our context mainly due to the following
lemma,

###### Lemma 4.5.2.

The truncation function @xmath satisfies the assumption ( 4.14 ) in
Lemma 4.5.1 .

###### Proof.

Denote @xmath , @xmath and @xmath . Note first that

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

and the subgradient

  -- -------- --
     @xmath   
  -- -------- --

Next, we take the derivative of @xmath and compare it to the derivative
of @xmath .

  -- -------- --
     @xmath   
  -- -------- --

Note that @xmath , @xmath , @xmath and @xmath . Thus, we have @xmath .
Similarly, we can take the derivative of @xmath and compare it to @xmath
, which results in @xmath , @xmath , @xmath and @xmath . This implies
@xmath and the Lemma is proved. ∎

The following lemma demonstrates the importance of matrix logarithm
function in matrix analysis, whose proof can be found in [
bhatia2013matrix ] and [ tropp2015introduction ] ,

###### Lemma 4.5.3.

(a) The matrix logarithm is operator monotone, that is, if @xmath are
two matrices in @xmath , then, @xmath .

(b) Given a fixed matrix @xmath , the function

  -- -------- --
     @xmath   
  -- -------- --

is concave on the cone of positive semi-definite matrices.

The following lemma is a generalization of Chebyshev’s association
inequality. See Theorem 2.15 of [ boucheron2013concentration ] for
proof.

###### Lemma 4.5.4 (FKG inequality).

Suppose @xmath are two functions non-decreasing on each coordinate. Let
@xmath be a random vector taking values in @xmath , then,

  -- -- --
        
  -- -- --

The following corollary follows immediately from the FKG inequality.

###### Corollary 4.5.1.

Let @xmath , then, we have @xmath .

###### Proof.

Consider any unit vector @xmath . It is enough to show @xmath . We
change the coordinate by considering an orthonormal basis @xmath with
@xmath . Let @xmath , @xmath , then we obtain,

  -- -- --
        
  -- -- --

where the last inequality follows from FKG inequality by taking @xmath
and @xmath . ∎

#### 4.5.2 Additional computation in the proof of Lemma 4.2.1

In order to show ( 4.11 ), it is enough to show that

  -- -- --
        
  -- -- --

Note that @xmath , and assuming that the sample size satisfies @xmath ,
we have @xmath . We then bound each of the 6 terms on the left side.

  -- -------- -------- --
                       
     @xmath            
     @xmath   @xmath   
     @xmath   @xmath   
                       
  -- -------- -------- --

Note that we have the following

  -- -- --
        
  -- -- --

thus, the rest three terms can be bounded as follows,

  -- -------- -------- --
              @xmath   
     @xmath   @xmath   
                       
  -- -------- -------- --

Overall, we have ( 4.11 ) holds.

#### 4.5.3 Proof of Lemma 4.4.2

First of all, by definition of @xmath , we have

  -- -- --
        
  -- -- --

Expanding the squares on the right hand side gives

  -- -- -- --
           
           
           
  -- -- -- --

We will then bound these three terms separately. Note that given @xmath
, the term (III) can be readily bounded as follows using the fact that
@xmath ,

  -- -- -- --------
           (4.15)
  -- -- -- --------

where the second from the last inequality follows from Corollary 4.5.1
and the last inequality follows from @xmath .

The rest two terms are bounded through the following lemma whose proof
is delayed to the next section:

###### Lemma 4.5.5.

Given @xmath , with probability at least @xmath , we have the following
two bounds hold,

  -- -- --
        
  -- -- --

  -- -- --
        
  -- -- --

Note that since @xmath , we have @xmath . Combining the above lemma with
( 4.15 ) finishes the proof of Lemma 4.4.2 .

#### 4.5.4 Proof of Lemma 4.5.5

Before proving the Lemma, we introduce the following abbreviations:

  -- -- --
        
        
  -- -- --

Our analysis relies on the following simply yet important fact which
gives deterministic upper and lower bound of @xmath around 1. Its proof
is delayed to the next section.

###### Lemma 4.5.6.

For any @xmath such that @xmath , the following holds:

  -- -------- --
     @xmath   
  -- -------- --

The following Lemma gives a general concentration bound for heavy tailed
random matrices under a mapping @xmath .

###### Lemma 4.5.7.

Let @xmath be a sequence of i.i.d. random matrices in @xmath with zero
mean and finite second moment @xmath . Let @xmath be any function
satisfying the assumption ( 4.14 ) of Lemma 4.5.1 . Then, for any @xmath
,

  -- -- --
        
  -- -- --

Specifically, if the assumption ( 4.14 ) holds for @xmath , then we
obtain the subgaussian tail @xmath .

The intuition behind this lemma is that the @xmath tends to “robustify”
a random variable by implicitly trading the bias for a tight
concentration. A scalar version of such lemma with a similar idea is
first introduced in the seminal work [ catoni2012challenging ] . The
proof of the current matrix version is similar to Lemma 3.1 and Theorem
3.1 of [ minsker2016sub ] by modifying only the constants. We omitted
the details here for brevity. Note that this lemma is useful in our
context by choosing @xmath . Next, we prove two parts of Lemma 4.5.5
separately.

###### Proof of (I) in Lemma 4.5.5.

Using the abbreviation introduced at the beginning of this section, we
have

  -- -- --
        
  -- -- --

We further split it into two terms as follows:

  -- -- -- --------
           (4.16)
  -- -- -- --------

The two terms in ( 4.16 ) are bounded as follows:

1.  For the second term in ( 4.16 ), note that we can write it back into
    the matrix form as

      -- -- --
            
      -- -- --

    Note that the matrix @xmath is a rank one matrix with the eigenvalue
    equal to @xmath , so it follows from the definition of matrix
    function,

      -- -- --
            
      -- -- --

    Now, applying Lemma 4.5.2 setting @xmath together with Lemma 4.5.7
    gives

      -- -- --
            
      -- -- --

    Setting @xmath (which results in @xmath ) gives

      -- -- -- --------
               (4.17)
      -- -- -- --------

    with probability at least @xmath .

2.  For the first term in ( 4.16 ), by the fact that @xmath and Lemma
    4.5.6 ,

      -- -- --
            
            
            
            
      -- -- --

    with probability at least @xmath , where the last inequality follows
    from the same argument leading to ( 4.17 ). Note that @xmath .

Overall, we get

  -- -- --
        
  -- -- --

with probability at least @xmath . Now we substitute @xmath and @xmath
into the above bound gives

  -- -- --
        
  -- -- --

Using Corollary 4.5.1 , we have

  -- -------- -- --------
     @xmath      (4.18)
  -- -------- -- --------

and also,

  -- -------- -- --------
     @xmath      (4.19)
  -- -------- -- --------

Substitute these two bounds into the bound of (I) gives the final bound
for (I) stated in Lemma 4.5.5 with probability at least @xmath . ∎

###### Proof of (II) in Lemma 4.5.5.

First of all, using the definition of @xmath and @xmath , we can rewrite
(II) as follows:

  -- -------- -- --
     @xmath      
     @xmath      
  -- -------- -- --

Similar to the analysis of (I), we further split the above term into two
terms and get

  -- -- -- --------
           (4.20)
  -- -- -- --------

For the first term, by Cauchy-Schwarz inequality and then Lemma 4.5.6 ,
we get

  -- -------- -- --
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

Note that @xmath , then, it follows,

  -- -- --
        
  -- -- --

Thus, by the same analysis leading to ( 4.17 ), we get

  -- -- -- --------
           (4.21)
  -- -- -- --------

with probability at least @xmath . For the second term (V), notice that
@xmath , thus we have

  -- -- -- --------
           (4.22)
  -- -- -- --------

For the second term, which measures the bias, we have by the fact @xmath
,

  -- -- --
        
  -- -- --

Now by Cauchy-Schwarz inequality and then Markov inequality, we obtain,

  -- -- --
        
  -- -- --

where the last two inequalities both follow from Lemma 4.5.1 . This
gives the second term in ( 4.22 ) is given by @xmath .

For the first term in ( 4.22 ), note that for any vector @xmath ,

  -- -- --
        
  -- -- --

and furthermore, the matrix @xmath has two same eigenvalues equal to
@xmath , which follows from

  -- -- --
        
  -- -- --

Thus, if we take

  -- -- --
        
  -- -- --

Then, the first term of ( 4.22 ) is equal to @xmath . For this @xmath ,
we have

  -- -- --
        
  -- -- --

By matrix Bernstein’s inequality ( [ tropp1 ] ), we obtain the bound

  -- -- --
        
  -- -- --

where @xmath is a fixed positive constant. Taking @xmath gives

  -- -- --
        
  -- -- --

where @xmath and the last inequality follows from the assumption that
@xmath . Overall, term (V) is bounded as follows

  -- -- --
        
  -- -- --

with probability at least @xmath . Note that @xmath , then, combining
with ( 4.21 ), the term (II) is bounded as

  -- -- --
        
  -- -- --

with probability at least @xmath . Substituting @xmath and @xmath gives

  -- -- --
        
  -- -- --

Using the bounds ( 4.18 ) and ( 4.19 ) with some algebraic
manipulations, we have the second bound in Lemma 4.5.5 holds with
probability at least @xmath . ∎

#### 4.5.5 Proof of Lemma 4.5.6

We divide our analysis into the following four cases:

1.  If @xmath and @xmath , then, we have @xmath .

2.  If @xmath and @xmath . Since @xmath , it follows @xmath , and we
    have

      -- -------- -------- --
         @xmath   @xmath   
         @xmath            
                  @xmath   
      -- -------- -------- --

    where the last inequality follows from the fact @xmath .

3.  If @xmath and @xmath . Since @xmath , it follows @xmath , and we
    have

      -- -------- -------- --
         @xmath   @xmath   
         @xmath            
      -- -------- -------- --

4.  If @xmath and @xmath . Then, we have

      -- -------- -------- --
         @xmath   @xmath   
                           
         @xmath            
                  @xmath   
      -- -------- -------- --

Overall, we proved the lemma.

#### 4.5.6 Proof of Lemma 4.2.2

By definition,

  -- -- --
        
  -- -- --

where @xmath denotes the @xmath -th entry of the random vector @xmath .
Also, for any fixed vector @xmath , we have

  -- -- --
        
        
  -- -- --

Taking the supremum from both sides of the above inequality and use the
previous bound on @xmath , we get

  -- -- --
        
  -- -- --

Summing over @xmath gives

  -- -- --
        
  -- -- --

#### 4.5.7 Proof of Lemma 4.2.3

First of all, let @xmath , then, we have @xmath . The lower bound of
@xmath follows directly from Corollary 4.5.1 . It remains to show the
upper bound. Note that by Cauchy-Schwarz inequality,

  -- -------- -- --
                 
     @xmath      
  -- -------- -- --

We then bound the two terms separately. For any vector @xmath , let
@xmath be the @xmath -th entry. Note that for any @xmath such that
@xmath , we have

  -- -- --
        
  -- -- --

where the first inequality uses the fact that the kurtosis is bounded.

Also, we have

  -- -------- -- --
                 
     @xmath      
     @xmath      
  -- -------- -- --

Combining the above two bounds gives

  -- -------- --
     @xmath   
  -- -------- --

which implies the result.