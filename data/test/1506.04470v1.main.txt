##### Contents

-    1 Introduction
-    2 Review of Nonequilibrium Equalities
    -    2.1 History
        -    2.1.1 Discovery of fluctuation theorem
        -    2.1.2 Jarzynski equality
        -    2.1.3 Crooks fluctuation theorem
        -    2.1.4 Further equalities
    -    2.2 Unified formulation based on reference probabilities
        -    2.2.1 Langevin system
        -    2.2.2 Hamiltonian system
-    3 Review of Information Thermodynamics
    -    3.1 Maxwell’s demon
        -    3.1.1 Original Maxwell’s demon
        -    3.1.2 Szilard engine
        -    3.1.3 Brillouin’s argument
        -    3.1.4 Landauer’s principle
    -    3.2 Second law of information thermodynamics
        -    3.2.1 Classical information quantities
        -    3.2.2 Second law under feedback control
        -    3.2.3 Second laws of memories
        -    3.2.4 Reconciliation of the demon with the conventional
            second law
    -    3.3 Nonequilibrium equalities under measurements and feedback
        control
        -    3.3.1 Information-thermodynamic nonequilibrium equalities
        -    3.3.2 Derivation of information-thermodynamic
            nonequilibrium equalities
    -    3.4 Experiments
-    4 Nonequilibrium Equalities in Absolutely Irreversible Processes
    -    4.1 Inapplicability of conventional integral nonequilibrium
        equalities and absolute irreversibility
        -    4.1.1 Inapplicability of the Jarzynski equality
        -    4.1.2 Definition of absolute irreversibility
    -    4.2 Nonequilibrium equalities in absolutely irreversible
        processes
        -    4.2.1 General formulation
        -    4.2.2 Physical implications
    -    4.3 Examples of absolutely irreversible processes
        -    4.3.1 Free expansion
        -    4.3.2 Process starting from a local equilibrium
        -    4.3.3 System with a trap
    -    4.4 Comparison with a conventional method
-    5 Information-Thermodynamic Nonequilibrium Equalities in Absolutely
    Irreversible Processes
    -    5.1 Inforamtion-thermodynamic equalities
    -    5.2 Unavailable information and associated equalities
    -    5.3 Examples of absolutely irreversible processes
        -    5.3.1 Measurement and trivial feedback control
        -    5.3.2 Two-particle Szilard engine
        -    5.3.3 Multi-particle Szilard engine
-    6 Gibbs’ Paradox Viewed from Absolute Irreversibility
    -    6.1 History and Resolutions
        -    6.1.1 Original Gibbs’ paradox
        -    6.1.2 Gibbs’ paradox is not a paradox
        -    6.1.3 Quantum resolution
        -    6.1.4 Pauli’s resolution based on the extensivity
    -    6.2 Resolution from absolute irreversibility
        -    6.2.1 Requirement and Results
        -    6.2.2 Difference of the two processes
        -    6.2.3 Derivation of the factor @xmath
-    7 Conclusions and Future Prospects
    -    7.1 Conclusions
    -    7.2 Future prospects
-    A From the Langevin Dynamics to Other Formulations
    -    A.1 Path-integral formula
    -    A.2 Fokker-Planck equation
-    B Measure Theory and Lebesgue’s Decomposition
    -    B.1 Preliminary subjects
    -    B.2 Classification of measures
    -    B.3 Radon-Nikodým theorem
    -    B.4 Lebesgue’s decomposition theorem

## Chapter 1 Introduction

In the mid-1990s, a significant breakthrough was achieved in the field
of nonequilibrium statistical physics. Evans, Cohen and Morris
numerically found a new symmetry of the probability distribution
function of the entropy production rate in a steady-state shear-driven
flow [ 1 ] . This symmetry, later formulated in the form of fluctuation
theorems, was proven in chaotic systems by Gallavotti and Cohen [ 2 ] ,
and later in various types of systems [ 3 , 4 , 5 , 6 ] . In this way,
the fluctuation theorems present a ubiquitous and universal structure
residing in nonequilibrium systems. What is important about the
fluctuation theorems is that they apply to systems far from equilibrium.
Moreover, they can be regarded as a generalized formulation of the
linear response theory to rather general nonequilibrium situations [ 7 ]
. In this respect, the fluctuation theorems have attracted considerable
attention.

In the course of the development of the fluctuation theorems, an
important relation was found by Jarzynski [ 8 , 9 ] . The Jarzynski
equality or the integral nonequilibrium equality relates the equilibrium
free-energy difference between two configurations to an ensemble
property of the work performed on the system during a rather general
nonequilibrium process that starts from one of the two configurations.
Thus, the Jarzynski equality enables us to estimate the free-energy
difference by finite-time measurements in a nonequilibrium process
outside of the linear response regime. Moreover, the Jarzynski equality
concisely reproduces the second law of thermodynamics and the
fluctuation-dissipation relation in the weak fluctuation limit. In this
way, the integral nonequilibrium equality is a fundamental relation with
experimental applications.

Another significant relation is the Crooks fluctuation theorem or the
detailed nonequilibrium equality [ 10 , 11 ] . The Crooks fluctuation
theorem compares the realization probability of a trajectory in phase
space under a given dynamics with that of the time-reversed trajectory
under the time-reversed dynamics. The ratio of these two probabilities
is exactly quantified by the exponentiated entropy production.
Therefore, irreversibility of a path under time reversal is
quantitatively characterized by the entropy production of the path
itself. Additionally, the Crooks fluctuation theorem succinctly
reproduces the Jarzynski equality.

Recently, the subject of nonequilibrium equalities marks a new
development in the field of feedback control. The theory of feedback
control dates back to Maxwell [ 12 ] . In his textbook of
thermodynamics, Maxwell pointed out that the second law can be violated
if we have access to microscopic degrees of freedom of the system and
illustrated the idea in his renowned gedankenexperiment later christened
by Lord Kelvin as Maxwell’s demon. Maxwell’s demon is able to reduce the
entropy of an isolated many-particle gas, by measuring the velocity of
the particles and manipulating them based on the information of the
measurement outcomes without expenditure of work. Maxwell’s demon
triggered subsequent discussions on the relations between thermodynamics
and information processing [ 13 , 14 , 15 ] . Although Maxwell’s demon
has been a purely imaginary object for about one and a half century,
thanks to technological advances, the realization of Maxwell’s demon in
real experiments is now within our hands [ 16 , 17 ] . Hence, the theory
of feedback control has attracted considerable interest in these days,
and quantitative relations between thermodynamic quantities and
information were established [ 18 , 19 ] , forming a new field of
information thermodynamics. One of the quantitative relation is known as
the second law of information thermodynamics, which states that the
entropy reduction in the feedback process is restricted by the amount of
mutual information obtained in the measurement process. As the Jarzynski
equality is a generalization of the second law of thermodynamics, the
second law of information thermodynamics is generalized in the form of
the integral nonequilibrium equality [ 20 ] . Thus, nonequilibrium
equalities are an actively developing field in the context of
information thermodynamics.

Despite of the wide applicability and interest of nonequilibrium
equalities, they are known to be inapplicable to free expansion [ 21 ,
22 ] . Moreover, information-thermodynamic nonequilibrium equalities
cannot apply to situations that involve such high-accuracy measurements
as error-free measurements [ 23 ] . This inapplicability roots from
divergence of the exponentiated entropy production, and can be
circumvented at the level of the detailed nonequilibrium equalities [ 24
] . However, to obtain the corresponding integral nonequilibrium
equalities, situation-specific modifications are needed, and moreover
the form of the obtained equalities is rather unusual. There has been no
unified strategy to derive these exceptional integral nonequilibrium
equalities in the situations to which the conventional integral
nonequilibrium equalities cannot apply.

In this thesis, we propose a new concept of absolute irreversibility
that constitutes those irreversible situations to which the conventional
integral nonequilibrium equalities cannot apply. In physical terms,
absolute irreversibility refers to those irreversible situations in
which paths in the time-reversed dynamics do not have the corresponding
paths in the original forward dynamics, which makes stark contrast to
ordinary irreversible situations, in which every time-reversed path has
the corresponding original path. Therefore, in the context of the
detailed nonequilibrium equalities, irreversibility is so strong that
the entropy production diverges, whereas, in ordinary irreversible
situations, irreversibility is quantitatively characterized by a finite
entropy production. In mathematical terms, absolute irreversibility is
characterized as the singular part of the probability measure in the
time-reversed dynamics with respect to the probability measure in the
original dynamics. Therefore, based on Lebesgue’s decomposition theorem
in measure theory [ 25 , 26 ] , the absolutely irreversible part is
uniquely separated from the ordinary irreversible part. As a result, we
obtain nonequilibrium integral equalities that are applicable to
absolutely irreversible situations [ 27 ] . Furthermore, our
nonequilibrium equalities give tighter restrictions on the entropy
production than the conventional second-law like inequalities [ 27 , 28
] .

As an illustrative application of our integral nonequilibrium equalities
and the notion of absolute irreversibility, we consider the problem of
gas mixing, which is what Gibbs’ paradox deals with [ 29 ] . Gibbs’
paradox is qualitatively resolved once we recognize the equivocal nature
of the thermodynamic entropy [ 30 , 31 , 32 ] . Although the standard
quantitative resolution of Gibbs’ paradox in many textbooks is based on
quantum statistical mechanics, this resolution is indeed irrelevant to
Gibbs’ paradox [ 31 , 32 ] . Pauli gave a correct quantitative analysis
of Gibbs’ paradox based on the extensivity of the thermodynamic entropy
[ 33 , 32 ] . However, this resolution holds only in the thermodynamic
limit, and ignores any sub-leading effects. Based on our nonequilibrium
equalities in the presence of absolute irreversibility, we give a
quantitative resolution of Gibbs’ paradox that is applicable even to a
classical mesoscopic regime, where sub-leading effects play an important
role.

This thesis is organized as follows. In Chap. 2, we briefly review
history of nonequilibrium equalities and a unified approach to derive
them. In Chap. 3, we review a part of historical discussions on
Maxwell’s demon and some fundamental relations under measurements and
feedback control including the second law of information thermodynamics.
Then, we review and derive information-thermodynamic nonequilibrium
equalities. In Chaps. 4-6, we describe the main results of this study.
In Chap. 4, we introduce a concept of absolute irreversibility in an
example of free expansion, to which the conventional integral
nonequilibrium equalities do not apply, and define absolute
irreversibility in mathematical terms. Then, in situations without
measurements and feedback control, we derive nonequilibrium equalities
in the presence of absolute irreversibility based on Lebesgue’s
decomposition theorem, and verify them in several illustrative examples.
In Chap. 5, we generalize the results in Chap. 4 to obtain
information-thermodynamic nonequilibrium equalities in the presence of
absolute irreversibility and verify them analytically in a few simple
examples. In Chap. 6, we briefly review discussions and conventional
resolutions on Gibbs’ paradox and quantitatively resolve Gibbs’ paradox
based on our nonequilibrium equalities with absolute irreversibility. In
Chap. 7, we summarize this thesis and discuss some future prospects.

## Chapter 2 Review of Nonequilibrium Equalities

In this chapter, we review nonequilibrium equalities in classical
statistical mechanics. Nonequilibrium equalities are exact equalities
applicable to quite general nonequilibrium systems, and are
generalizations of the second law of thermodynamics and well-known
relations in linear response theory. Moreover, nonequilibrium equalities
give one solution of Loschmidt’s paradox.

In the former half of this chapter, we review various nonequilibrium
equalities in the chronological order. In the latter half, we review a
unified method to derive the nonequilibrium equalities introduced in the
former half.

### 2.1 History

First of all, we briefly review history of nonequilibrium equalities.

#### 2.1.1 Discovery of fluctuation theorem

The field of nonequilibrium equalities was initiated by Evans, Cohen,
and Morris in 1993 [ 1 ] . In a steady state of thermostatted
shear-driven flow, they numerically discovered a novel symmetry in the
probability distribution of the entropy production rate, which is
nowadays called the steady-state fluctuation theorem. The theorem reads

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

where @xmath is the time interval and @xmath is the probability
distribution function for the time-averaged entropy production rate
@xmath in units of @xmath , where @xmath is the Boltzmann constant and
@xmath is the absolute temperature of the system (see Fig. 2.1 ). They
justified this symmetry by assuming a certain statistical ensemble of
the nonequilibrium steady state. Subsequently, in the same system, a
similar symmetry is disclosed in a transient situation from the
equilibrium state to the steady state [ 34 ] . It is known as the
transient fluctuation theorem and written as

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

This relation was proved under the same assumption as the steady-state
fluctuation theorem ( 2.1 ), which suggests that the fluctuation theorem
is not a property particular to steady states, but applicable to wider
classes of nonequilibrium situations. Equations ( 2.1 ) and ( 2.2 )
demonstrate that the probability of a positive entropy production is
exponentially greater than that of the reversed sign. Moreover, these
fluctuation theorems reproduce the Green-Kubo relation [ 35 , 36 ] and
Onsager’s reciprocity relation [ 37 , 38 ] in the limit of weak external
fields [ 7 ] . Therefore, the fluctuation theorems can be regarded as
extensions of the well-established relations in the linear-response
regime to a more general nonequilibrium regime.

These fluctuation theorems ( 2.1 ) and ( 2.2 ) resolve Loschmidt’s
paradox in the following sense. Loschmidt’s paradox originates from his
criticism to the @xmath -theorem proposed by Boltzmann, which
demonstrates that the Shannon entropy of the probability distribution
function of phase space increases with time in a system obeying the
Boltzmann equation, although this equation is symmetric under time
reversal. It was claimed that the @xmath -theorem is a derivation of the
second law of thermodynamics, and the irreversible macroscopic law can
be derived from the reversible microscopic dynamics. However, Loschmidt
criticized this argument by the observation that it should be impossible
to deduce irreversible properties from the time-reversal symmetric
dynamics. If we have a process with a positive entropy production and
reverse the velocity of all the particle in the system at once, we can
generate a process with a negative entropy production since the dynamics
is time-reversal symmetric. Therefore, the entropy of the system should
not always decrease, which is a defect of the @xmath -theorem.
Fluctuation theorems ( 2.1 ) and ( 2.2 ) demonstrate that, as Loschmidt
pointed out, paths with a negative entropy production have nonzero
probability. However, an important implication of the fluctuation
theorems is that the probability of a negative entropy production is
exponentially suppressed in large systems or in the long-time limit
(namely when @xmath ). Therefore, paths violating the second law cannot
be observed in macroscopic systems in practice. In this way, the
fluctuation theorems reconcile Loschmidt’s paradox with the second law
originating from the reversible dynamics.

Soon after the discovery of the fluctuation theorems, the steady-state
fluctuation theorem ( 2.1 ) was proved under the chaotic hypothesis,
which is an extension of the ergodic hypothesis, in dissipative
reversible systems [ 2 , 39 ] . Later, a proof free from the chaotic
hypothesis was proposed in Langevin systems since the Langevin dynamics
is ergodic in the sense that the system relaxes to the thermal
equilibrium distribution in the long-time limit [ 3 ] , and then
generalized to general Markov processes [ 4 ] . Moreover, both the
steady-state fluctuation theorem ( 2.1 ) and the transient fluctuation
theorem ( 2.2 ) were shown in general thermostatted systems in a unified
manner [ 5 ] . It was pointed out that this proof of the fluctuation
theorems is applicable even to Hamiltonian systems, although it had been
believed that the thermostatting mechanism is needed for the fluctuation
theorems [ 6 ] . Thus, the fluctuation theorems are known to apply to
wide classes of nonequilibrium systems.

The transient fluctuation theorem ( 2.2 ) was experimentally
demonstrated in Ref. [ 40 ] . They prepared a colloidal particle in an
optical trap at rest, and then translated the trap relative to the
surrounding water. In this transient situation, they obtained the
probability distribution of entropy production, and observed
trajectories of the particle violating the second law at the level of
individual paths (see Fig. 2.2 ). The amount of this violation was
confirmed to be consistent with Eq. ( 2.2 ). Later, the steady-state
fluctuation theorem ( 2.1 ) was also verified in the same setup [ 41 ] .

#### 2.1.2 Jarzynski equality

In 1997, Jarzynski discovered a remarkable exact nonequilibrium equality
in a Hamiltonian system [ 8 ] . Let @xmath denote the Hamiltonian of the
system, where @xmath is an external parameter that we control to
manipulate the system and @xmath represents internal degrees of freedom
of the system. The system is initially in equilibrium with the inverse
temperature @xmath , and we subject the system to a nonequilibrium
process by our manipulation of @xmath from @xmath to @xmath . Let @xmath
denote the equilibrium free energy of the system under a given external
parameter @xmath , i.e.,

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

The Jarzynski equality relates the free-energy difference @xmath to the
probability distribution of work @xmath performed during the
nonequilibrium process as

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

where the angular brackets mean the statistical average under the
initial equilibrium state and the given nonequilibrium protocol. Soon
after the discovery, the same equality is proved in stochastic systems
based on the master equation formalism [ 9 ] . It is noteworthy that we
assume nothing about how fast we change the external parameter @xmath ,
and therefore the Jarzynski equality ( 2.4 ) remains valid under a rapid
change of the parameter, which means that the Jarzynski equality applies
to processes beyond the linear response regime.

Moreover, the Jarzynski equality ( 2.4 ) is an extension of conventional
thermodynamic relations to the case of a rather general nonequilibrium
regime [ 8 ] . First, it leads to a second-law-like inequality in
isothermal processes. Using Jensen’s inequality, we obtain

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

Combining this inequality with the Jarzynski equality ( 2.4 ), we
conclude

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

which is the second law of thermodynamics in isothermal processes. The
equality condition is that @xmath has a single definite value, i.e.,
@xmath does not fluctuate. We can regard @xmath as the total entropy
production of the system. Let @xmath and @xmath denote the
internal-energy difference and dissipated heat from the system to the
heat bath, respectively. Then, the first law of thermodynamics is @xmath
. We rewrite Eq. ( 2.6 ) as

  -- -------- -- -------
     @xmath      (2.7)
  -- -------- -- -------

where @xmath is the temperature of the heat bath (and therefore the
initial temperature of the system). The first term represents the
entropy production of the system @xmath because @xmath , and the second
term is the entropy production of the heat bath. Therefore, Eq. ( 2.6 )
means that the entropy production of the total system must be positive.

Secondly, the Jarzynski equality ( 2.4 ) reproduces the
fluctuation-dissipation relation in the linear response theory [ 8 ] .
Let us denote @xmath , and assume that the dissipated work @xmath is
much smaller than the thermal energy @xmath , namely, @xmath . Expanding
@xmath up to the second order in @xmath , we obtain

  -- -------- -- -------
     @xmath      (2.8)
  -- -------- -- -------

Substituting the Jarzynski equality ( 2.4 ), we obtain

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

The left-hand side is dissipation of the total system, and the
right-hand side represents fluctuations of the work during the process.
This is one form of the fluctuation-dissipation relation.

Not only does the Jarzynski equality reproduce the second law of
thermodynamics, but also it gives a stringent restriction on the
probability of events violating the second law [ 42 ] . Let @xmath be a
positive number and let us calculate the probability that the entropy
production is smaller than @xmath :

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.10)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where we use the Jarzynski equality ( 2.4 ) to obtain the last line.
Therefore, the probability of negative entropy production is
exponentially suppressed. While a negative entropy production ( @xmath )
may occasionally occur, a greatly negative entropy production ( @xmath )
is effectively prohibited by the Jarzynski equality.

In addition to the above-described properties, the Jarzynski equality (
2.4 ) enables us to determine the free-energy difference from our
observation of how the system evolves under a nonequilibrium process
because

  -- -------- -- --------
     @xmath      (2.11)
  -- -------- -- --------

although the free-energy difference is an equilibrium property of the
system [ 9 , 43 ] . A naive method to determine the free-energy
difference in experiments or numerical simulations is to conduct
reversible measurements of work and use the fact @xmath . However, this
method is not realistic in general because an extremely long time is
needed to realize even approximately reversible processes. A more
sophisticated method is to use the linear response relation ( 2.9 ) to
determine the free energy difference through the work distribution
obtained in the measurements. This method may still be time-consuming
because the manipulation must be slow enough for the system to remain in
the linear response regime. Equation ( 2.11 ) enables us to reduce the
time of experiments or simulations when we calculate the free-energy
difference, because Eq. ( 2.11 ) is valid even for rapid nonequilibrium
processes. ¹ ¹ 1 The exact equality ( 2.11 ) requires more samples for
convergence than the approximate equality ( 2.9 ) does [ 43 ] .
Therefore, the total time required for convergence, which is the time of
the process multiplied by the number of samples, can be longer when we
use Eq. ( 2.11 ) than we use ( 2.9 ).

Hummer and Szabo found an experimentally useful variant of the Jarzynski
equality [ 44 ] . The Hummer-Szabo equality can be utilized to
rigorously reconstruct the free-energy landscape of a molecule from
repeated measurements based, for example, on an atomic force microscope
or an optical tweezer. Shortly thereafter, in a setup with an optical
tweezer shown in Fig. 2.3 (a), the free-energy profile of a single
molecule of RNA was reconstructed by mechanically stretching the RNA in
an irreversible manner [ 45 ] (see Fig. 2.3 (b)).

This experiment demonstrated that the Jarzynski equality is useful in
practice to determine free-energy differences of systems. In a similar
manner, the free-energy profile of a protein was estimated by stretching
the protein by an atomic force microscope [ 46 ] .

#### 2.1.3 Crooks fluctuation theorem

In 1998, Crooks offered a new proof of the Jarzynski equality in
stochastic systems [ 47 ] . What is remarkable about this proof is the
method comparing an original process with the time-reversed process to
derive nonequilibrium relations. One year later, this idea led him to
propose a novel relation now known as the Crooks fluctuation theorem [
10 ] , which reads

  -- -------- -- --------
     @xmath      (2.12)
  -- -------- -- --------

where @xmath is the probability distribution function of entropy
production @xmath , and @xmath is that in the time-reversed process.
Later, this theorem was generalized to Hamiltonian systems with multiple
heat baths by Jarzynski [ 48 ] .

The Crooks fluctuation theorem ( 2.12 ) can be regarded as a generalized
version of the steady-state fluctuation theorem ( 2.1 ) in systems
symmetric with respect to reversal of the perturbation that drives the
steady flow; in this case, we have @xmath , which reduces Eq. ( 2.12 )
to Eq. ( 2.1 ), because there is no difference between the original
process and the time-reversed one. What is more, the Crooks fluctuation
theorem easily reproduces the Jarzynski equality as follows:

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.13)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where we have used the Crooks fluctuation theorem ( 2.12 ) to obtain the
second line, and the normalization of probability to obtain the last
line. Moreover, the Crooks fluctuation theorem implies that

  -- -------- -- --------
     @xmath      (2.14)
  -- -------- -- --------

Soon after, Crooks found a significant generalization of his own theorem
[ 11 ] . He related irreversibility of an individual path @xmath in
phase space to its own entropy production @xmath , ² ² 2 We use box
brackets to indicate that @xmath is a functional of @xmath instead of a
function. that is,

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

where @xmath represents the time-reversed path of @xmath ; @xmath is the
path-probability functional under a given dynamics, and @xmath is that
under the time-reversed dynamics. To reproduce Eq. ( 2.12 ) from Eq. (
2.15 ), we note

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.16)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where @xmath denotes the natural measure on the set of all paths, and we
use Eq. ( 2.15 ) to obtain the second line, and we assume entropy
production is odd under time reversal, namely @xmath , to obtain the
third line. A more general form of the nonequilibrium integral equality
can be derived based on Eq. ( 2.15 ). Let @xmath be an arbitrary
functional. Then, we obtain

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.17)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where @xmath denotes the average over the time-reversed probability
@xmath . When we set @xmath to unity, Eq. ( 2.17 ) reduces to Eq. ( 2.13
). The idea to consider the path-probability of an individual path is a
crucial element to treat nonequilibrium equalities in a unified manner
as described in Sec. 2.2.

The Crooks fluctuation theorem ( 2.12 ) was experimentally verified [ 49
] in a similar setup in Ref. [ 45 ] , which was used to verify the
Jarzynski equality. The work extracted when an RNA is unfolded and
refolded was measured, and the work distribution of the unfolding
process and that of the refolding process, which is the time reversed
process of unfolding, were obtained (see Fig. 2.4 (a)). It was verified
that the two distributions are consistent with the Crooks fluctuation
theorem ( 2.12 ) (see Fig. 2.4 (b)). Moreover, the free-energy
difference of a folded RNA and an unfolded one was obtained using Eq. (
2.14 ), and the obtained value is consistent with a numerically obtained
one.

#### 2.1.4 Further equalities

In this section, we will briefly review further nonequilibrium
equalities. To this end, let us introduce some kinds of entropy
production. Mathematical definitions of these kinds of entropy
production are presented in Sec. 2.2.

The total entropy production @xmath is the sum of the Shannon entropy
production of the system @xmath and the entropy production of the heat
bath @xmath , that is,

  -- -------- -- --------
     @xmath      (2.18)
  -- -------- -- --------

The entropy production of the bath is related to the heat @xmath
dissipated from the system to the bath

  -- -------- -- --------
     @xmath      (2.19)
  -- -------- -- --------

where @xmath is the absolute temperature of the bath. In a steady-state
situation, the heat @xmath is split into two parts as

  -- -------- -- --------
     @xmath      (2.20)
  -- -------- -- --------

where @xmath is called the housekeeping heat, which is the inevitable
heat dissipation to maintain the corresponding nonequilibrium steady
state, and @xmath is called the excess heat, which arises due to a
non-adiabatic change of the external control parameter. Along with these
definitions of heat, we define two kinds of entropy production as

  -- -------- -- --------
     @xmath      (2.21)
  -- -------- -- --------

##### Hatano-Sasa relation

The Hatano-Sasa relation is a generalization of the Jarzynski equality [
50 ] . The Jarzynski equality relates the free-energy difference of two
equilibrium states to the nonequilibrium average starting from one of
the equilibrium states. In a similar manner, the Hatano-Sasa relation
relates the difference of nonequilibrium potentials @xmath of two
nonequilibrium steady states, which is a generalization of @xmath in
equilibrium situations, to an average starting from one of the steady
state. The Hatano-Sasa relation reads

  -- -------- -- --------
     @xmath      (2.22)
  -- -------- -- --------

From Jensen’s inequality, we obtain

  -- -------- -- --------
     @xmath      (2.23)
  -- -------- -- --------

This inequality can be interpreted as a nonequilibrium version of the
Clausius inequality ( @xmath ). In fact, the inequality ( 2.23 ) can be
rewritten as

  -- -------- -- --------
     @xmath      (2.24)
  -- -------- -- --------

The equality can be achieved in quasi-static transitions between the two
nonequilibrium steady states, which is also analogous to the Clausius
inequality, whose equality is also achieved in quasi-static transitions.

##### Seifert relation

The Seifert relation applies to an arbitrary nonequilibrium process
starting from an arbitrary initial state [ 51 ] . The integral Seifert
relation is given by

  -- -------- -- --------
     @xmath      (2.25)
  -- -------- -- --------

The corresponding inequality is

  -- -------- -- --------
     @xmath      (2.26)
  -- -------- -- --------

which can be considered as a second law in a nonequilibrium process. The
detailed version, which is analogous to the Crooks fluctuation theorem,
is given by

  -- -------- -- --------
     @xmath      (2.27)
  -- -------- -- --------

It is noteworthy that these relations hold for an arbitrary time
interval. Equation ( 2.27 ) can be regarded as a refinement of the
steady state fluctuation theorem ( 2.1 ). The steady state fluctuation
theorem ( 2.1 ) holds only in the long-time limit. This is because
@xmath in Eq. ( 2.1 ) is in fact the entropy production rate of the bath
and does not include that of the system. Therefore, in Eq. ( 2.1 ), the
time interval should be long enough to ignore the entropy production of
the system compared with that of the bath.

##### Relation for housekeeping entropy production

In Ref. [ 52 ] , a relation for housekeeping entropy production is also
obtained as

  -- -------- -- --------
     @xmath      (2.28)
  -- -------- -- --------

which leads to

  -- -------- -- --------
     @xmath      (2.29)
  -- -------- -- --------

### 2.2 Unified formulation based on reference probabilities

In this section, we review a unified strategy to derive the
nonequilibrium equalities introduced in the previous section.

In Ref. [ 11 ] , Crooks revealed that a unified approach to derive
nonequilibrium equalities is to compare the nonequilibrium process with
the time-reversed process and obtain a detailed fluctuation theorem,
namely, the Crooks fluctuation theorem given by

  -- -------- -- --------
     @xmath      (2.30)
  -- -------- -- --------

where quantities with a superscript @xmath are the ones in the
time-reversed process. Later, Hatano and Sasa derived the nonequilibrium
equality ( 2.22 ) in steady states by comparing the original dynamics
with its “time-reversed dual” dynamics in a sense to be specified later.
In their derivation, they essentially used a detailed fluctuation
theorem given by

  -- -------- -- --------
     @xmath      (2.31)
  -- -------- -- --------

where quantities accompanied by @xmath are, in this equality, the ones
in the time-reversed dual process. Their work indicates that a further
unification is possible, namely, a detailed fluctuation theorem will be
obtained when we compare the original dynamics with a properly chosen
reference dynamics. With this speculation, let us generalize the
detailed fluctuation theorems ( 2.30 ) and ( 2.31 ) to

  -- -------- -- --------
     @xmath      (2.32)
  -- -------- -- --------

where @xmath represents a reference probability of a reference path, and
@xmath is a formal entropy production. In the case of the Crooks
fluctuation theorem ( 2.12 ), the reference is the time reversal, and
@xmath reduces to @xmath . In the case considered by Hatano and Sasa, in
a similar way, the reference is the time-reversed dual, and @xmath
reduces to @xmath . Once we obtain the detailed fluctuation theorem (
2.32 ), we succinctly derive an integral nonequilibrium equality given
by

  -- -------- -- --------
     @xmath      (2.33)
  -- -------- -- --------

because

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.34)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where we use Eq. ( 2.32 ) to obtain the second line, and we use the
normalization condition for the reference probability to obtain the last
line. In the same way, we obtain

  -- -------- -- --------
     @xmath      (2.35)
  -- -------- -- --------

where @xmath is an arbitrary functional, and @xmath represents the
average over the reference probability @xmath .

In the rest of this section, we validate Eq. ( 2.32 ) in specific
systems. To be precise, we show that appropriate choices of the
reference probability make the formal entropy production @xmath reduce
to physically meaningful entropy productions. The results are summarized
in Table 2.1 .

#### 2.2.1 Langevin system

First of all, we consider a one-dimensional overdamped Langevin system.
One reason why we deal with the Langevin system as a paradigm is that
steady states can be simply achieved by applying an external driving
force. Moreover, the Langevin system is thermodynamically sound in that
it relaxes to the thermal equilibrium state, i.e., the Gibbs state after
a sufficiently long time without the external driving force.

This part is mainly based on a review article by Seifert, i.e., Ref. [
53 ] .

##### Basic properties and definition of heat

Let @xmath denote the position of a particle at time @xmath in a thermal
environment with temperature @xmath . We consider a nonequilibrium
process from time @xmath to @xmath controlled by an external parameter
@xmath . The overdamped Langevin equation is given by

  -- -------- -- --------
     @xmath      (2.36)
  -- -------- -- --------

where @xmath is the mobility, and @xmath is a systematic force applied
to the particle with the position @xmath when the external control
parameter is @xmath , and @xmath represents a random force. We assume
that @xmath is a white Gaussian noise satisfying

  -- -------- -- --------
     @xmath      (2.37)
  -- -------- -- --------

where @xmath is the diffusion constant. The systematic force @xmath
consists of two parts, that is,

  -- -------- -- --------
     @xmath      (2.38)
  -- -------- -- --------

The first part is due to the conservative potential @xmath and @xmath is
the external driving force. Under this Langevin dynamics, the
probability to generate an entire trajectory @xmath starting from @xmath
under a given entire protocol @xmath is calculated as

  -- -------- -- --------
     @xmath      (2.39)
  -- -------- -- --------

where the action @xmath of the trajectory @xmath is

  -- -------- -- --------
     @xmath      (2.40)
  -- -------- -- --------

(see Appendix A for a derivation). Another strategy to describe the
system is to trace the probability @xmath to find the particle at @xmath
at time @xmath . We can show @xmath to obey the Fokker-Planck equation
given by

  -- -------- -- --------
     @xmath      (2.41)
  -- -------- -- --------

where the probability current @xmath is defined as

  -- -------- -- --------
     @xmath      (2.42)
  -- -------- -- --------

(see Appendix A for a derivation). With this probability, the entropy of
the system is defined as the stochastic Shannon entropy of the system,
i.e.,

  -- -------- -- --------
     @xmath      (2.43)
  -- -------- -- --------

The mean local velocity is defined as

  -- -------- -- --------
     @xmath      (2.44)
  -- -------- -- --------

When the external driving force is not applied, i.e., @xmath , the
system relaxes to the thermal equilibrium state given by

  -- -------- -- --------
     @xmath      (2.45)
  -- -------- -- --------

where @xmath is the inverse temperature and free energy @xmath is
defined as

  -- -------- -- --------
     @xmath      (2.46)
  -- -------- -- --------

because the kinetic energy contributes nothing due to the assumption of
overdamping.

Next, we consider how we should define heat in this Langevin system [ 54
] . The dissipated heat is the energy flow from the system to the bath.
This energy transfer is done by the viscous friction force @xmath and
the thermal noise @xmath , where @xmath is the friction coefficient.
Therefore, the “work” done by these force should be identified with the
heat flowing into the system. Thus we define the dissipated heat as

  -- -------- -- --------
     @xmath      (2.47)
  -- -------- -- --------

Using the Langevin equation ( 2.36 ), we obtain

  -- -------- -- --------
     @xmath      (2.48)
  -- -------- -- --------

Now we can define the entropy production of the heat bath as

  -- -------- -- --------
     @xmath      (2.49)
  -- -------- -- --------

In this way, the concepts of the heat and entropy are generalized to the
level of an individual stochastic trajectory.

##### Steady-state properties and definitions of thermodynamic
quantities

When the external driving force @xmath is applied at a fixed @xmath ,
the system relaxes to a nonequilibrium steady state @xmath . In the
analogy of the equilibrium state ( 2.45 ), let us define a
nonequilibrium potential @xmath by

  -- -------- -- --------
     @xmath      (2.50)
  -- -------- -- --------

The steady current is defined as

  -- -------- -- --------
     @xmath      (2.51)
  -- -------- -- --------

and the mean velocity in the steady state is given by

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.52)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Using this expression, we rewrite Eq. ( 2.48 ) as

  -- -------- -- --------
     @xmath      (2.53)
  -- -------- -- --------

where the Einstein relation @xmath is assumed. The first term is the
inevitable dissipation proportional to the steady mean velocity @xmath
and is therefore identified as the housekeeping heat, namely,

  -- -------- -- --------
     @xmath      (2.54)
  -- -------- -- --------

and the second term is the excess contribution after the subtraction of
the housekeeping part from the total heat, and defined as

  -- -------- -- --------
     @xmath      (2.55)
  -- -------- -- --------

Therefore, following Ref. [ 50 ] , we define two kinds of entropy
production as

  -- -------- -- --------
     @xmath      (2.56)
  -- -------- -- --------

and

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.57)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

because @xmath . Note that the ensemble average of the excess entropy
production vanishes in steady states because @xmath is independent of
time, which justifies that the excess entropy production is indeed an
excess part due to non-adiabatic changes of the control parameter @xmath
.

##### Time reversal and @xmath

We consider a process starting from an initial probability distribution
@xmath under a protocol @xmath , and the time-reversed process starting
from an initial probability distribution @xmath under the time-reversed
protocol @xmath defined by @xmath . Here, we do not assume any relations
between @xmath and @xmath . Now we compare the realization probability
of the original path @xmath and that of the time-reversed path @xmath
defined by @xmath . The original probability is

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.58)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

and the time-reversed probability is

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.59)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Therefore, the formal entropy production defined in Eq. ( 2.32 ) reduces
to

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.60)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

The first term is called the bulk term, and the second term is called
the boundary term, because the second one arises from the boundary
conditions @xmath and @xmath . Using Eq. ( 2.40 ), we obtain

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.61)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where we change the integration variable from @xmath to @xmath to obtain
the last line. Therefore, the bulk term is

  -- -------- -- --------
     @xmath      (2.62)
  -- -------- -- --------

where we use the definition ( 2.48 ) to obtain the last equality. By
Eq. ( 2.49 ), we obtain

  -- -------- -- --------
     @xmath      (2.63)
  -- -------- -- --------

Here, we assume that the initial state is in equilibrium

  -- -------- -- --------
     @xmath      (2.64)
  -- -------- -- --------

Although the initial state of the reversed dynamics can be set to an
arbitrary probability distribution, we set it also to the canonical
ensemble as

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.65)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

In this case, we obtain

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.66)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

which means that we obtain the Crooks fluctuation theorem ( 2.12 ).
Therefore, we also obtain the Jarzynski equality ( 2.4 ) succinctly.

Next, we assume nothing about @xmath , and set the initial probability
of the reversed dynamics to the final probability of the original
dynamics, namely,

  -- -------- -- --------
     @xmath      (2.67)
  -- -------- -- --------

In this case, the boundary term reduces to the Shannon entropy
production of the system as

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.68)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Therefore, we obtain

  -- -------- -- --------
     @xmath      (2.69)
  -- -------- -- --------

and the detailed Seifert relation ( 2.27 ), which automatically derives
the integral Seifert relation ( 2.25 ).

Finally, we assume nothing about @xmath , and set the initial
probability of the reversed dynamics to @xmath . Then, we obtain

  -- -------- -- --------
     @xmath      (2.70)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (2.71)
  -- -------- -- --------

is the dissipation functional defined in Ref. [ 5 ] and used in the
transient fluctuation theorem ( 2.2 ).

##### Dual (steady-flow reversal) and @xmath

Next, we consider the steady-flow-reversed dynamics. To this aim, we
define the dual of the external force @xmath as

  -- -------- -- --------
     @xmath      (2.72)
  -- -------- -- --------

where we use @xmath to represent the steady velocity under force @xmath
. Let us demonstrate that the steady state under force @xmath is also
the steady state under force @xmath . Comparing Eq. ( 2.52 ), we obtain

  -- -------- -- --------
     @xmath      (2.73)
  -- -------- -- --------

or

  -- -------- -- --------
     @xmath      (2.74)
  -- -------- -- --------

Since @xmath is the steady solution of the Fokker-Planck equation ( 2.41
), we have

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.75)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Using Eq. ( 2.74 ), we obtain

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.76)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

which means @xmath is also the steady state solution under force @xmath
, that is,

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (2.77)
     @xmath   @xmath   @xmath      (2.78)
  -- -------- -------- -------- -- --------

Comparing Eqs. ( 2.51 ) and ( 2.74 ), we have

  -- -------- -- --------
     @xmath      (2.79)
  -- -------- -- --------

Therefore, the dual dynamics has the same steady state as the original
dynamics and the negative of the steady current in the original
dynamics.

Now, we consider a process starting from an initial probability
distribution @xmath and the dual process starting from an initial
probability distribution @xmath . The original probability is

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.80)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

and the dual probability is

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.81)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Therefore, the formal entropy production reduces to

  -- -------- -- --------
     @xmath      (2.82)
  -- -------- -- --------

The bulk term can be calculated as

  -- -------- -- --------
     @xmath      
     @xmath      
     @xmath      (2.83)
  -- -------- -- --------

Using Eqs. ( 2.72 ) and ( 2.73 ), we obtain

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.84)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where we use @xmath to obtain the second line and the fact that the
divergence of the current vanishes in the steady state to obtain the
last line. By the definition ( 2.56 ), we obtain

  -- -------- -- --------
     @xmath      (2.85)
  -- -------- -- --------

When we set @xmath to @xmath , the formal entropy production reduces to

  -- -------- -- --------
     @xmath      (2.86)
  -- -------- -- --------

and therefore we obtain the integral fluctuation theorem for the
housekeeping entropy production ( 2.28 ).

##### Time-reversed dual and @xmath

We consider a process starting from an initial probability distribution
@xmath under a protocol @xmath , and compare it with the dual process
starting from an initial probability distribution @xmath under the
time-reversed protocol @xmath . The original probability is

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.87)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

and the time-reversed dual probability is

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.88)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Therefore, we obtain

  -- -------- -- --------
     @xmath      (2.89)
  -- -------- -- --------

The bulk term is calculated as

  -- -------- -- --------
     @xmath      
     @xmath      
     @xmath      (2.90)
  -- -------- -- --------

Using Eqs. ( 2.72 ) and ( 2.73 ), we obtain

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.91)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

By the definition ( 2.57 ), we obtain

  -- -------- -- --------
     @xmath      (2.92)
  -- -------- -- --------

We assume that the initial state is the nonequilibrium steady state
given by

  -- -------- -- --------
     @xmath      (2.93)
  -- -------- -- --------

and set the initial state of the time-reversed dual dynamics to the
nonequilibrium steady state as

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.94)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Then, we obtain

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (2.95)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Therefore, we reproduce the Hatano-Sasa relation ( 2.22 ) in a simple
manner.

#### 2.2.2 Hamiltonian system

Next, we consider a Hamiltonian system consisting of a system and a heat
bath with inverse temperature @xmath . We consider only time reversal as
the reference dynamics because it is difficult to define steady flow in
a Hamiltonian system in general.

This part is partly based on Ref. [ 55 ] .

##### Setup

Let @xmath denote the position in the phase space of the total system.
We separate the degrees of freedom @xmath into two parts as @xmath ,
where @xmath denotes the degrees of freedom of the system, and @xmath is
the degrees of freedom of the bath. We assume that the Hamiltonian of
the total system can be decomposed into

  -- -------- -- --------
     @xmath      (2.96)
  -- -------- -- --------

where @xmath is an external control parameter. We also assume that the
Hamiltonian is invariant under time reversal. We vary @xmath from time
@xmath to @xmath , and the system is subject to a nonequilibrium
process. The free energy of the bath @xmath is defined by

  -- -------- -- --------
     @xmath      (2.97)
  -- -------- -- --------

and is invariant during the process. Moreover, we define an effective
Hamiltonian of the system @xmath by tracing out the degrees of freedom
of the bath as

  -- -------- -- --------
     @xmath      (2.98)
  -- -------- -- --------

The free energy of the total system @xmath defined by

  -- -------- -- --------
     @xmath      (2.99)
  -- -------- -- --------

and the free energy @xmath based on @xmath defined by

  -- -------- -- ---------
     @xmath      (2.100)
  -- -------- -- ---------

are related by

  -- -------- -- ---------
     @xmath      (2.101)
  -- -------- -- ---------

##### Time reversal

We compare the original process @xmath starting from an initial
probability distribution of the total system @xmath with the
time-reversed process @xmath starting from an initial probability
distribution of the total system @xmath , where @xmath and the
superscript @xmath represents the sign reversal of momenta. We note that
the probability to realize a path @xmath is the same as the probability
to have @xmath in the initial state because the Hamiltonian system is
deterministic as a whole. Therefore, we obtain

  -- -------- -------- -------- -------- ---------
     @xmath   @xmath   @xmath            (2.102)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- ---------

Now, we assume that the initial state is the equilibrium state of the
total Hamiltonian

  -- -------- -- ---------
     @xmath      (2.103)
  -- -------- -- ---------

Moreover, we set the initial state of the reversed process to the
equilibrium state of the total Hamiltonian

  -- -------- -- ---------
     @xmath      (2.104)
  -- -------- -- ---------

Thus, we obtain

  -- -------- -- ---------
     @xmath      (2.105)
  -- -------- -- ---------

Since the total system is isolated, the difference of the total
Hamiltonian is due to the work done by the external controller, that is,

  -- -------- -- ---------
     @xmath      (2.106)
  -- -------- -- ---------

Therefore, noting that @xmath is independent of @xmath , we obtain

  -- -------- -- ---------
     @xmath      (2.107)
  -- -------- -- ---------

which gives the Crooks fluctuation theorem ( 2.12 ) and the Jarzynski
equality ( 2.4 ).

Next, we assume that the initial state of the system is not correlated
with the initial state of the bath and that the initial state of the
bath is the canonical ensemble, namely,

  -- -------- -- ---------
     @xmath      (2.108)
  -- -------- -- ---------

In addition, we set the initial state of the time-reversed dynamics to
the product state of the final probability distribution of the system in
the original process and the canonical ensemble of the bath

  -- -------- -------- -------- -------- ---------
     @xmath   @xmath   @xmath            (2.109)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- ---------

Therefore, we obtain

  -- -------- -- ---------
     @xmath      (2.110)
  -- -------- -- ---------

We define an unaveraged Shannon entropy of the system by @xmath , and we
have

  -- -------- -- ---------
     @xmath      (2.111)
  -- -------- -- ---------

The energy change of the bath @xmath can be regarded as the heat @xmath
dissipated from the system to the bath. Moreover, the heat @xmath is
related to the entropy production of the bath @xmath as @xmath . Thus,
we obtain

  -- -------- -------- -------- -------- ---------
     @xmath   @xmath   @xmath            (2.112)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- ---------

where @xmath is the total entropy production. Therefore, we
automatically reproduce the detailed Seifert relation ( 2.27 ) and the
integral Seifert relation ( 2.25 ).
The results obtained in this section are summarized in Table 2.2 .

## Chapter 3 Review of Information Thermodynamics

In this chapter, we review thermodynamics with measurements and feedback
control. Historically, Maxwell pointed out that thermodynamics,
specifically the second law of thermodynamics, should break down when an
intelligent being, known as Maxwell’s demon, controls the system by
utilizing information obtained by measurements. Since then, numerous
researches have been done on the foundation of the second law of
thermodynamics [ 56 ] , and thermodynamics of information processing is
established [ 57 ] . Since the nonequilibrium equalities reviewed in the
previous chapter are generalizations of the second law of
thermodynamics, the second-law-like inequality of information processing
can be extended to nonequilibrium equalities.

First, we trace historical discussions on Maxwell’s demon. Then, we
formulate the second law of information thermodynamics from a modern
point of view. Next, nonequilibrium equalities of information
thermodynamics are reviewed. Finally, we review experimental
demonstrations of Maxwell’s demon.

### 3.1 Maxwell’s demon

In this section, we review historical discussions on Maxwell’s demon.

#### 3.1.1 Original Maxwell’s demon

Maxwell’s demon was proposed in his book titled “Theory of Heat”
published in 1871 [ 12 ] . In the second last section of the book,
Maxwell discussed the “limitation of the second law of thermodynamics”
in an example with an intelligent being, which was later christened
Maxwell’s demon by Lord Kelvin.

Let us consider a vessel filled with gas molecules. The vessel is
divided into two parts, and the division has a small window, through
which a molecule passes from one side to the other when the window is
open. When the window opens and the temperature of one side is different
from that of the other side, the second law of thermodynamics states
that the temperature becomes uniform (see Fig. 3.1 (a)). Maxwell’s demon
achieves the reverse process of this phenomenon (see Fig. 3.1 (b)). At
an initial time, the temperature is uniform throughout the vessel. The
demon observes molecules in the vessel. Some molecules are faster than
the mean velocity and others are slower because of thermal fluctuations.
The demon opens and closes the window to allow only the
faster-than-average molecules to pass from the left side to the right
side, and only the slower-than-average molecules to pass from the right
to the left. After some time, the demon succeeds in raising the
temperature of the right side and lowering the temperature of the left
without the expenditure of work. This means that the entropy is reduced
in an isolated system, which apparently contradicts the second law of
thermodynamics. In summary, Maxwell demonstrated that the control of the
system based on the outcomes of the measurement can reduce the entropy
of the system beyond the restriction from the second law of
thermodynamics.

#### 3.1.2 Szilard engine

In 1929, a simplest model of Maxwell’s demon, now known as the Szilard
engine, was proposed [ 13 ] . Although the Szilard engine is apparently
different from the original Maxwell’s gedankenexperiment, it captures
the essential features of the demon of utilizing measurement and
feedback control to reduce the entropy of a system. Moreover, the
Szilard engine enables us to quantitatively analyze the role of the
information.

We elaborate on the protocol of the Szilard engine (see Fig. 3.2 ). An
ideal classical gas molecule is confined in a box with volume @xmath ,
and the box is surrounded by an isothermal environment with temperature
@xmath . We insert a division in the middle of the box and separate the
box into two parts with the same volume @xmath . Then, we measure the
position of the particle to determine whether the particle is in the
left or right part. We assume this measurement is error-free. When we
find the particle is in the left, we isothermally shift the division to
the right end. On the other hand, when we find the particle is in the
right, we isothermally shift the division to the left end. In both
cases, we can extract a positive work of @xmath (see the discussion
below for the derivation of this result) from the particle in these
processes. We remove the division and the system returns to its initial
state. Therefore, we can repeatedly extract work from this isothermal
cycle.

Szilard pointed out that the correlation made by the measurement is the
resource for the entropy reduction and work extraction. The measurement
process creates a correlation between the position of the particle
@xmath and the measurement outcome @xmath . Let us set the origin of
@xmath at the middle. When @xmath , we obtain @xmath , where @xmath (
@xmath ) means the right (left) . After the process of feedback control,
namely isothermal shifting, this correlation vanishes because the
particle can now be present in the entire system, so @xmath can be
positive or negative regardless of the value of @xmath . Therefore, in
the feedback process, we extract a positive work at the cost of
eliminating the correlation between @xmath and @xmath .

Let us quantitatively analyze this protocol from a modern point of view.
By the position measurement, we obtain the Shannon information

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

In the process of isothermal expansion, we extract work @xmath . Because
we assume that the gas is ideal, the equation of state reads @xmath .
Therefore, the work is calculated as

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

Therefore, we conjecture that the information obtained by the
measurement can be quantitatively converted to the work during the
feedback process.

As explained above, Szilard revealed that we can utilize the correlation
established by the measurement to reduce the entropy of the system. He
also pointed out that the entropy reduction achieved in the feedback
process must be compensated by a positive entropy production during the
process to establish the correlation to be consistent with the second
law of thermodynamics of the entire process. However, it remained
unexplained why the measurement process should be accompanied by a
positive entropy production.

#### 3.1.3 Brillouin’s argument

An answer to the question was presented by Brillouin in 1951 [ 14 ] . He
argued, in the original setup of Maxwell’s demon, that the demon creates
more entropy when it observes molecules than the entropy reduction due
to his feedback control. Therefore, the observation process compensates
the entropy reduction of the gas, and as a result the entire process is
consistent with the second law of thermodynamics.

Brillouin assumed that, when the demon observes molecules, the demon
needs to shed a probe light to molecules. However, the demon and the
system are surrounded by an environment at temperature @xmath with the
blackbody radiation. Therefore, the energy of the probe photon should be
sufficiently larger than the thermal energy @xmath to distinguish the
probe from background noises. Thus, the frequency of the probe photon
@xmath satisfies

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

where @xmath is the Planck constant. The demon observes a molecule by
absorbing a photon scattered by the molecule. Therefore, the entropy
production of the demon by the single observation is given by

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

Let @xmath represent the temperature of the left (right) side satisfying

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

where @xmath is the temperature difference satisfying @xmath . The demon
transfers a fast molecule in the left box with kinetic energy @xmath to
the right box, and does a slow molecule in the right box with kinetic
energy @xmath to the left box, where @xmath , and @xmath and @xmath are
of the order of one. As a result, heat

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

is transferred from the left box to the right box, and the entropy
reduction is bounded as

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

because @xmath and @xmath . Therefore, comparing this equation with
Eq. ( 3.4 ), we conclude that the entropy production of the demon due to
the measurement is far beyond the entropy reduction achieved by the
feedback control. Thus, the second law remains valid for the entire
system. A similar discussion can be also done for the Szilard engine.

In this way, Brillouin argued that the entropy production for the
measurement process exceeds the entropy reduction during the feedback
process, and therefore the total entropy production of the system and
the demon is positive. However, his analysis depends on a specific model
of the measurement using a photon as the probe, and the idea that the
work gain by the feedback control is compensated by the work cost of the
measurement is not always true.

#### 3.1.4 Landauer’s principle

Landauer argued that the energy cost is needed not for measurement
processes to obtain information but for erasure processes of the
obtained information from the memory [ 15 ] . He considered a 1-bit
memory consisting of a particle in a bistable potential as shown in Fig.
3.3 . We label the particle in the left well as the zero state, and the
particle in the right well as the one state. In the erasure process, we
restore the particle to the standard state, namely the zero state.
Before the erasure, we do not know whether the particle is in the left
or right well. Therefore, the process is a two-to-one mapping, and
cannot be realized by a deterministic frictionless protocol. Thus, a
protocol must involve a process with friction to erase the information.
In this way, dissipation is inevitable to erase the information stored
in the memory.

The erasure process in which the system is brought to the zero state is
logically irreversible because we cannot recover the state before the
process from the state after the process. Landauer argued that logical
irreversibility implies physical irreversibility, which is accompanied
by dissipation. Therefore, logical irreversible operations such as
erasure cause heat dissipation.

In a 1-bit symmetric memory, where the zero and one states have the same
entropy, the erasure from the randomly-distributed state to the standard
state means the entropy reduction by @xmath . To compensate this entropy
reduction, heat dissipation must occur. Therefore, to erase the
information stored in a 1-bit memory, we have inevitable heat
dissipation of @xmath . This is a famous rule known as Landauer’s
principle.

In summary, Landauer argued that the cost for erasure of the stored
information compensates the gain in feedback processes. However, his
argument is crucially dependent on the structure of the symmetric
memory, and does not apply to general cases. In fact, erasure in an
asymmetric memory provides a counter-example of Landauer’s principle [
57 ] .

### 3.2 Second law of information thermodynamics

In this section, we review the second law of information thermodynamics
from a modern point of view. We restrict our attention to classical
cases, because it is sufficient for the aim of this thesis. First of
all, we shortly introduce classical information quantities because they
are crucial ingredients of information thermodynamics. Then, the second
law of a system under feedback control is discussed. After that, the
second law of a memory, which is a thermodynamic model of the demon, is
presented. Finally, we demonstrate that the conventional second law is
recovered for the entire system.

#### 3.2.1 Classical information quantities

In this section, we introduce three classical information quantities:
the Shannon entropy, the Kullback-Leibler divergence, and the mutual
information based on Refs. [ 57 , 58 ] .

##### Shannon entropy

First of all, we introduce the Shannon entropy. Let @xmath denote a
probability variable and @xmath denote the sample space, namely @xmath .
When @xmath is a discrete set, we define the Shannon entropy of a
probability distribution @xmath as ¹ ¹ 1 In the context of quantum
information theory, the symbol @xmath usually denotes the von Neumann
entropy instead of the Shannon entropy. However, in this thesis, we
denote the Shannon entropy by @xmath because it is the convention in the
field of classical statistical mechanics. There would be no confusion
because we do not use the von Neumann entropy in this thesis.

  -- -------- -- -------
     @xmath      (3.8)
  -- -------- -- -------

On the other hand, when @xmath is continuous, we would naively define
the Shannon entropy of a probability distribution density @xmath by

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

However, the second term is divergent in the limit of @xmath .
Therefore, we define the Shannon entropy as

  -- -------- -- --------
     @xmath      (3.10)
  -- -------- -- --------

Although @xmath is invariant under a transformation of variable, @xmath
alone is not invariant. Therefore, the continuous Shannon entropy as
defined in Eq. ( 3.10 ) is not invariant under transformation of the
coordinates.

The unaveraged Shannon entropy

  -- -------- -- --------
     @xmath      (3.11)
  -- -------- -- --------

is an indicator of how rare an event @xmath is. In fact, @xmath
increases as @xmath decreases. The Shannon entropy is the ensemble
average of this rarity. The reason why we use the logarithm is to
guarantee the additivity of the Shannon entropy when we have independent
events. Let us assume @xmath and @xmath . Then, when @xmath , we have

  -- -------- -- --------
     @xmath      (3.12)
  -- -------- -- --------

Here, we demonstrate that the Shannon entropy is invariant under a
Hamiltonian dynamics. In this case, @xmath is phase space, and the
dynamics is deterministic. Let @xmath denote the initial (final)
position in @xmath and @xmath denote the initial (final) probability
distribution. Since the probability is conserved, we have

  -- -------- -- --------
     @xmath      (3.13)
  -- -------- -- --------

In addition, Liouville’s theorem states that

  -- -------- -- --------
     @xmath      (3.14)
  -- -------- -- --------

which, together with Eq. ( 3.13 ), leads to

  -- -------- -- --------
     @xmath      (3.15)
  -- -------- -- --------

Therefore, the initial Shannon entropy

  -- -------- -- --------
     @xmath      (3.16)
  -- -------- -- --------

has the same value as the final Shannon entropy

  -- -------- -- --------
     @xmath      (3.17)
  -- -------- -- --------

namely,

  -- -------- -- --------
     @xmath      (3.18)
  -- -------- -- --------

Thus, the continuous Shannon ( 3.10 ) entropy is invariant in time under
the Hamiltonian dynamics.

##### Kullback-Leibler divergence

Next, we introduce the Kullback-Leibler divergence or the relative
entropy. This quantity is defined as a relative logarithmic distance
between two probability distributions @xmath and @xmath on the same
sample space @xmath . When @xmath is discrete, we define the
Kullback-Leibler divergence as

  -- -------- -- --------
     @xmath      (3.19)
  -- -------- -- --------

On the other hand, when @xmath is continuous, we define the
Kullback-Leibler divergence as

  -- -------- -- --------
     @xmath      (3.20)
  -- -------- -- --------

We note that the continuous Kullback-Leibler divergence is invariant
under a transformation of the coordinates.

Using an inequality

  -- -------- -- --------
     @xmath      (3.21)
  -- -------- -- --------

we obtain

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.22)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where the equality is achieved if and only if @xmath ( @xmath -almost
everywhere). Therefore, the Kullback-Leibler divergence is a kind of
distance to measure how different two probabilities are.

##### Mutual information

We consider the mutual information between two sample spaces @xmath and
@xmath . Let @xmath denote a joint probability distribution of @xmath .
The marginal probability distributions are defined as @xmath and @xmath
. We define the mutual information as

  -- -------- -- --------
     @xmath      (3.23)
  -- -------- -- --------

which represents the overlap of uncertainty of @xmath and @xmath (see
Fig. 3.4 ). If the systems are independent of each other, i.e., @xmath ,
we obtain @xmath due to the additivity of the Shannon entropy. Moreover,
we represent the mutual information in terms of the Kullback-Leibler
divergence as

  -- -------- -- --------
     @xmath      (3.24)
  -- -------- -- --------

Therefore, the mutual information quantifies how different @xmath is
from the non-correlated probability distribution @xmath , that is, how
correlated @xmath and @xmath are. Since the Kullback-Leibler divergence
is not negative, we obtain

  -- -------- -- --------
     @xmath      (3.25)
  -- -------- -- --------

The explicit form of the mutual information is

  -- -------- -- --------
     @xmath      (3.26)
  -- -------- -- --------

Therefore, we define the unaveraged mutual information as

  -- -------- -- --------
     @xmath      (3.27)
  -- -------- -- --------

or

  -- -------- -- --------
     @xmath      (3.28)
  -- -------- -- --------

where @xmath is the probability distribution function of @xmath
conditioned by @xmath . Therefore, the unaveraged mutual information
quantifies the decrease of the unaveraged Shannon entropy of the system
@xmath due to the fact that we know the system @xmath is in a state
@xmath .

#### 3.2.2 Second law under feedback control

In this section, we formulate the second law of a system under feedback
control. The mutual information plays a crucial role in quantifying the
gain of feedback control.

##### Setup

We consider feedback control on a Hamiltonian system with phase space
@xmath (see Fig. 3.5 ). At initial time @xmath , the system is at
position @xmath sampled from an initial probability distribution @xmath
. From time @xmath to @xmath , the system evolves under a Hamiltonian
@xmath , and ends up with a point @xmath with a probability distribution
@xmath . At time @xmath , we measure quantities of the system (e.g.
positions, velocities and the number of particles in a given region) and
obtain a measurement outcome @xmath . Let @xmath denote the sample space
of the outcome. The detail of the measurement is modeled by conditional
probabilities

  -- -------- -- --------
     @xmath      (3.29)
  -- -------- -- --------

which is the probability to obtain the outcome @xmath under the
condition that the system is at the position @xmath at time @xmath . Now
that we have the outcome @xmath , we know more detailed information on
the system than we did before the measurement. In fact, the probability
distribution of @xmath under the condition that we have obtained @xmath
is calculated by the Bayes theorem as

  -- -------- -- --------
     @xmath      (3.30)
  -- -------- -- --------

where the probability distribution of the outcome @xmath is defined by

  -- -------- -- --------
     @xmath      (3.31)
  -- -------- -- --------

From time @xmath to @xmath , the system evolves under a Hamiltonian
@xmath . Here, we conduct feedback control of the system, that is,
adjust the Hamiltonian in accordance with @xmath via the control
parameter @xmath . Therefore, the protocol after the measurement is
conditioned by @xmath . Let @xmath denote the final position and @xmath
be the final probability distribution conditioned by @xmath . The
unconditioned final probability distribution is calculated as

  -- -------- -- --------
     @xmath      (3.32)
  -- -------- -- --------

##### Shannon entropy production

We evaluate the Shannon entropy production in the above-described setup.
Since Hamiltonian dynamics does not change the Shannon entropy (see
Eq. ( 3.18 )), we obtain

  -- -------- -- --------
     @xmath      (3.33)
  -- -------- -- --------

By the same reason, we also obtain

  -- -------- -- --------
     @xmath      (3.34)
  -- -------- -- --------

To evaluate entropy production of the system, let us compare the
averaged final entropy

  -- -------- -- --------
     @xmath      (3.35)
  -- -------- -- --------

with the initial entropy

  -- -------- -- --------
     @xmath      (3.36)
  -- -------- -- --------

By Eq. ( 3.34 ), the final entropy is calculated as

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.37)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

On the other hand, by Eq. ( 3.33 ), the initial entropy can be
transformed as

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.38)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Using Eq. ( 3.28 ), we obtain

  -- -------- -- --------
     @xmath      (3.39)
  -- -------- -- --------

Therefore, the Shannon entropy production of the system is the negative
of the mutual information obtained by the measurement.

##### Second law under feedback control

We separate the degrees of freedom @xmath into that of the system @xmath
and that of the heat bath @xmath as @xmath . Let @xmath and @xmath
denote phase spaces of the system and the bath, respectively. We assume
that the total Hamiltonian reads

  -- -------- -- --------
     @xmath      (3.40)
  -- -------- -- --------

where the last term on the right-hand side is the interaction
Hamiltonian and is assumed to vanish at the initial and final times.

First, we assume that the initial state is the product state of an
initial probability distribution of the system and the canonical
ensemble of the heat bath as

  -- -------- -- --------
     @xmath      (3.41)
  -- -------- -- --------

where we define

  -- -------- -- --------
     @xmath      (3.42)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (3.43)
  -- -------- -- --------

In this case, since @xmath and @xmath are not correlated, the initial
total Shannon entropy is calculated as

  -- -------- -- --------
     @xmath      (3.44)
  -- -------- -- --------

where the initial energy of the bath is defined as

  -- -------- -- --------
     @xmath      (3.45)
  -- -------- -- --------

Using the final probability distribution of the total system @xmath , we
define the marginal final probability as

  -- -------- -- --------
     @xmath      (3.46)
  -- -------- -- --------

We calculate the relative entropy between the final state of the total
system and the product state of the final state of the system and the
canonical state of the heat bath as

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.47)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Since the relative entropy is positive, we obtain

  -- -------- -- --------
     @xmath      (3.48)
  -- -------- -- --------

Therefore, comparing Eq. ( 3.39 ), we obtain

  -- -------- -- --------
     @xmath      (3.49)
  -- -------- -- --------

where we identify the dissipated heat with the energy difference of the
bath as

  -- -------- -- --------
     @xmath      (3.50)
  -- -------- -- --------

Moreover, since the measurement outcome @xmath should depend only on the
degrees of freedom of the system @xmath , we have

  -- -------- -- --------
     @xmath      (3.51)
  -- -------- -- --------

and therefore

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.52)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Thus, we obtain

  -- -------- -- --------
     @xmath      (3.53)
  -- -------- -- --------

The left-hand side of Eq. ( 3.53 ) means the sum of the Shannon entropy
production of the system and the entropy production of the heat bath.
Therefore, Eq. ( 3.53 ) demonstrates that the total entropy production
can be reduced by the feedback control by the amount of the mutual
information @xmath .

To proceed further, we assume that the initial state of the system is in
equilibrium with the inverse temperature @xmath given by

  -- -------- -- --------
     @xmath      (3.54)
  -- -------- -- --------

where we define the canonical ensemble as

  -- -------- -- --------
     @xmath      (3.55)
  -- -------- -- --------

and the free energy as

  -- -------- -- --------
     @xmath      (3.56)
  -- -------- -- --------

The initial Shannon entropy is

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.57)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where we define the initial energy of the system as

  -- -------- -- --------
     @xmath      (3.58)
  -- -------- -- --------

On the other hand, since the relative entropy is positive, we obtain

  -- -------- -- --------
     @xmath      (3.59)
  -- -------- -- --------

This relation can be rewritten as

  -- -------- -- --------
     @xmath      (3.60)
  -- -------- -- --------

Averaging this over @xmath with the probability @xmath , we obtain

  -- -------- -- --------
     @xmath      (3.61)
  -- -------- -- --------

where we define the final internal energy as

  -- -------- -- --------
     @xmath      (3.62)
  -- -------- -- --------

and the final free energy as

  -- -------- -- --------
     @xmath      (3.63)
  -- -------- -- --------

Therefore, Eq. ( 3.53 ) reduces to

  -- -------- -- --------
     @xmath      (3.64)
  -- -------- -- --------

Identifying the energy change of the total system @xmath with the work
@xmath performed on the system, we obtain

  -- -------- -- --------
     @xmath      (3.65)
  -- -------- -- --------

We can rewrite this equation as

  -- -------- -- --------
     @xmath      (3.66)
  -- -------- -- --------

which means that we can extract more work from the system than the
conventional second law of thermodynamics by the amount of the mutual
information @xmath obtained by the measurement. This form of the second
law under feedback control was first formulated in Ref. [ 18 ] in a
quantum system.

In summary, we formulate the second law under feedback control, and
reveal that the gain of feedback control is precisely quantified by the
mutual information obtained by the measurement. This is a rigorous
formulation of Szilard’s idea that the correlation made by the
measurement can be utilized as a resource for the entropy reduction.

#### 3.2.3 Second laws of memories

In this section, we formulate second laws of memories during a
measurement process and during an erasure process. The second laws of
memories were discussed in quantum systems in Ref. [ 19 ] . Here, we
consider a classical version of this study.

##### Measurement process

First, we consider a measurement process. The phase space of the total
system is denoted by @xmath and the system is subject to a Hamiltonian
dynamics. We assume that the total system consists of three parts: the
system, the memory, and a heat bath with phase spaces @xmath , @xmath ,
and @xmath , respectively. Phase-space positions in @xmath , @xmath ,
@xmath , and @xmath are denoted by @xmath , @xmath , @xmath , and @xmath
, respectively. Moreover, we assume that the sample space of the memory
@xmath is the disjoint union of @xmath . An outcome @xmath is stored
when @xmath . Therefore, @xmath can be regarded as a coarse-grained
sample space of @xmath . We assume that the total Hamiltonian is
decomposed into

  -- -------- -- --------
     @xmath      (3.67)
  -- -------- -- --------

where the last term on the right-hand side is the interaction term,
which is assumed to vanish at the initial and final times.

Since the total system is a Hamiltonian system, the Shannon entropy of
the total system is conserved:

  -- -------- -- --------
     @xmath      (3.68)
  -- -------- -- --------

We assume that the initial state is a product state as

  -- -------- -- --------
     @xmath      (3.69)
  -- -------- -- --------

and then we obtain

  -- -------- -- --------
     @xmath      (3.70)
  -- -------- -- --------

Because of the positivity of the relative entropy, we obtain

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (3.71)
  -- -------- -------- -------- -- --------

Substituting Eqs. ( 3.68 ) and ( 3.70 ), we obtain

  -- -------- -- --------
     @xmath      (3.72)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (3.73)
  -- -------- -- --------

Using the definition of the mutual information, we obtain

  -- -------- -- --------
     @xmath      (3.74)
  -- -------- -- --------

Since we perform feedback control based on @xmath , we should evaluate
the entropy production by @xmath instead of @xmath . The difference
between these two values is calculated as

  -- -------- -------- -- -------- --------
     @xmath   @xmath               (3.75)
                          @xmath   
  -- -------- -------- -- -------- --------

where the joint probability of @xmath and @xmath is defined as

  -- -------- -- --------
     @xmath      (3.76)
  -- -------- -- --------

Therefore, we obtain

  -- -------- -- --------
     @xmath      (3.77)
  -- -------- -- --------

Since the right-hand side is in the form of the relative entropy, we
derive

  -- -------- -- --------
     @xmath      (3.78)
  -- -------- -- --------

We note that this result is natural since @xmath is the coarse-grained
sample space of @xmath . Thus, Eq. ( 3.74 ) reduces to

  -- -------- -- --------
     @xmath      (3.79)
  -- -------- -- --------

We assume that, before the measurement process, the system is in
equilibrium and the memory is prepared to be a fixed standard state
@xmath , namely, a local equilibrium state in @xmath given by

  -- -------- -- --------
     @xmath      (3.80)
  -- -------- -- --------

where we define

  -- -------- -- --------
     @xmath      (3.81)
  -- -------- -- --------

and @xmath is the characteristic function of a region @xmath ; the
conditional free energy is defined by

  -- -------- -- --------
     @xmath      (3.82)
  -- -------- -- --------

The initial entropy of the memory is

  -- -------- -- --------
     @xmath      (3.83)
  -- -------- -- --------

Using the positivity of the relative entropy, we obtain

  -- -------- -- --------
     @xmath      (3.84)
  -- -------- -- --------

where we define a reference probability by

  -- -------- -- --------
     @xmath      (3.85)
  -- -------- -- --------

Therefore, we obtain

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (3.86)
  -- -------- -------- -------- -- --------

where we use the fact that @xmath outside @xmath , and the final free
energy is defined with

  -- -------- -- --------
     @xmath      (3.87)
  -- -------- -- --------

Therefore, the entropy production of the memory is bounded as

  -- -------- -- --------
     @xmath      (3.88)
  -- -------- -- --------

On the other hand, we can derive

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.89)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Using Eq. ( 3.74 ), we obtain

  -- -------- -- --------
     @xmath      (3.90)
  -- -------- -- --------

Since the energy change of the total system @xmath should be identified
as work @xmath , we conclude

  -- -------- -- --------
     @xmath      (3.91)
  -- -------- -- --------

This equality presents the minimum work needed to perform the
measurement, which may be interpreted as a rigorous formulation of
Brillouin’s argument.

##### Erasure process

Next, we consider an erasure process of the stored information. We
assume that the total system of this process consists of the memory
@xmath and the heat bath @xmath , and the total system is subject to the
Hamiltonian dynamics. We assume that the Hamiltonian is written as

  -- -------- -- --------
     @xmath      (3.92)
  -- -------- -- --------

where @xmath is the interaction term, which is assumed to vanish at the
initial and final times.

Since the total system is under Hamiltonian dynamics, we obtain

  -- -------- -- --------
     @xmath      (3.93)
  -- -------- -- --------

We assume that the initial state is a product state as

  -- -------- -- --------
     @xmath      (3.94)
  -- -------- -- --------

and we obtain

  -- -------- -- --------
     @xmath      (3.95)
  -- -------- -- --------

By the same procedure to derive Eq. ( 3.48 ), we obtain

  -- -------- -- --------
     @xmath      (3.96)
  -- -------- -- --------

Therefore, Eq. ( 3.93 ) reduces to

  -- -------- -- --------
     @xmath      (3.97)
  -- -------- -- --------

where the dissipated heat is defined as

  -- -------- -- --------
     @xmath      (3.98)
  -- -------- -- --------

We assume that the memory initially stores a classical probability
distribution @xmath and it is in the local equilibrium state of @xmath
under the condition that the stored information is @xmath , i.e.,

  -- -- -- --------
           (3.99)
  -- -- -- --------

Therefore, the initial entropy of the memory is calculated as

  -- -------- -------- -------- -------- ---------
     @xmath   @xmath                     (3.100)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- ---------

On the other hand, by the positivity of the relative entropy, we obtain

  -- -------- -- ---------
     @xmath      (3.101)
  -- -------- -- ---------

which reduces Eq. ( 3.97 ) to

  -- -------- -- ---------
     @xmath      (3.102)
  -- -------- -- ---------

We identify @xmath as work @xmath , since it is the energy increase of
the total system. Therefore, we conclude

  -- -------- -- ---------
     @xmath      (3.103)
  -- -------- -- ---------

which reveals the minimum work needed to erase the information stored in
the memory.

When @xmath for arbitrary @xmath , we have no free-energy difference and
Eq. ( 3.102 ) reduces to

  -- -------- -- ---------
     @xmath      (3.104)
  -- -------- -- ---------

In other words, in symmetric memories, Eq. ( 3.102 ) reduces to
Landauer’s principle, which states that the minimum cost for erasure is
the Shannon entropy of the stored information.

#### 3.2.4 Reconciliation of the demon with the conventional second law

In this section, we summarize the results obtained in the previous
sections, and demonstrate that the second law is recovered in the total
process [ 19 ] .

The second law under feedback control is

  -- -------- -------- -------- -- ---------
     @xmath   @xmath   @xmath      (3.105)
  -- -------- -------- -------- -- ---------

The minus of the right-hand side quantifies the energy gain of feedback
control, or the extractable work beyond the conventional second law
thanks to Maxwell’s demon. In the measurement process, we have obtained

  -- -------- -------- -------- -- ---------
     @xmath   @xmath   @xmath      (3.106)
  -- -------- -------- -------- -- ---------

whose right-hand side represents the additional work cost needed for the
demon to perform the measurement. In the erasure process, we have
derived

  -- -------- -------- -------- -- ---------
     @xmath   @xmath   @xmath      (3.107)
  -- -------- -------- -------- -- ---------

The right-hand side is the additional work cost for the erasure of the
information stored by the demon. Therefore, the sum of the costs for
both the measurement and erasure is given by

  -- -------- -------- -------- -- ---------
     @xmath   @xmath   @xmath      (3.108)
  -- -------- -------- -------- -- ---------

where we define @xmath and other quantities in a similar way. We see
that the work gain by the demon is precisely compensated by the work
cost that the demon pays for the measurement and erasure. In fact, in
total, the information quantities on the right-hand sides of Eqs. (
3.105 ) and ( 3.108 ) are cancelled out, and we obtain

  -- -------- -------- -------- -- ---------
     @xmath   @xmath   @xmath      (3.109)
  -- -------- -------- -------- -- ---------

where we define @xmath and other quantities in a similar way. The
conventional second-law-like inequality ( 3.109 ) is recovered in the
total process consisting of the measurement, feedback control, and
erasure processes. In particular, in an isothermal cycle, Eq. ( 3.109 )
reduces to @xmath which is nothing but Kelvin’s principle for the
isothermal composite system consisting of the system and the memory.

As reviewed in Sec. 2.1, Brillouin argued that the entropy reduction by
the demon is compensated by the cost for the measurement. On the other
hand, Landauer’s principle says that the work cost for the erasure of
the stored information exceeds the work gain of the feedback control.
Although these views are true for some specific systems, they are not
generally true. What compensates the work gain by the demon in general
is not the individual work cost for the measurement or erasure but the
joint work cost for the measurement and erasure processes. The
information-thermodynamic inequalities summarized above reveal that the
reconciliation of Maxwell’s demon with the second law is achieved in a
rigorous manner.

In terms of the Shannon entropy, we have obtained

  -- -------- -------- -------- -- ---------
     @xmath   @xmath   @xmath      (3.110)
     @xmath   @xmath   @xmath      (3.111)
     @xmath   @xmath   @xmath      (3.112)
     @xmath   @xmath   @xmath      (3.113)
     @xmath   @xmath   @xmath      (3.114)
  -- -------- -------- -------- -- ---------

The inequalities ( 3.111 ) and ( 3.112 ) are simpler than Eqs. ( 3.106 )
and ( 3.107 ) in that they do not have the Shannon entropy term @xmath
explicitly. We note that the inequalities on the Shannon entropy ( 3.110
), ( 3.111 ), ( 3.112 ), ( 3.113 ), and ( 3.114 ) are stronger than the
inequalities on the work ( 3.105 ), ( 3.106 ), ( 3.107 ), ( 3.108 ), and
( 3.109 ), since the latter inequalities can be derived from the former
inequalities based on positivity of the relative entropy.

### 3.3 Nonequilibrium equalities under measurements and feedback
control

The second-law-like inequality under measurements and feedback control
derived in the previous section can be generalized to nonequilibrium
equalities as the conventional second law is generalized to the
nonequilibrium equalities reviewed in Sec 2.1.

In this section, we review information-thermodynamic nonequilibrium
equalities. Then, we derive these equalities in a unified manner similar
to that of Sec. 2.2.

#### 3.3.1 Information-thermodynamic nonequilibrium equalities

In 2010, Sagawa and Ueda generalized the Jarzynski equality in a Markov
stochastic system under feedback control based on a single measurement
and obtained the Sagawa-Ueda equality [ 20 ]

  -- -------- -- ---------
     @xmath      (3.115)
  -- -------- -- ---------

where @xmath is the unaveraged mutual information obtained by the
measurement. Using Jensen’s inequality, we succinctly reproduce the
second law of information thermodynamics as

  -- -------- -- ---------
     @xmath      (3.116)
  -- -------- -- ---------

Later, the Sagawa-Ueda equality is generalized to Markov systems with
multiple measurements [ 24 ] , and to non-Markov systems with multiple
measurements [ 59 ] .

The Sagawa-Ueda equality has variants as the Jarzynski equality has the
variants reviewed in Sec. 2.1. The Hatano-Sasa relation is generalized
to systems under feedback control as [ 60 ]

  -- -------- -- ---------
     @xmath      (3.117)
  -- -------- -- ---------

The associate inequality is given by

  -- -------- -- ---------
     @xmath      (3.118)
  -- -------- -- ---------

Moreover, in Ref. [ 60 ] , the Seifert relation is generalized to

  -- -------- -- ---------
     @xmath      (3.119)
  -- -------- -- ---------

which leads to a second-law-like inequality

  -- -------- -- ---------
     @xmath      (3.120)
  -- -------- -- ---------

Reference [ 61 ] derived

  -- -------- -- ---------
     @xmath      (3.121)
  -- -------- -- ---------

and

  -- -------- -- ---------
     @xmath      (3.122)
  -- -------- -- ---------

Equations ( 3.115 ), ( 3.117 ), ( 3.119 ), and ( 3.121 ) are summarized
in terms of the formal entropy production @xmath as

  -- -------- -- ---------
     @xmath      (3.123)
  -- -------- -- ---------

This equality is a general nonequilibrium equalities under measurements
and feedback control. The second law of information thermodynamics is
reproduced as

  -- -------- -- ---------
     @xmath      (3.124)
  -- -------- -- ---------

#### 3.3.2 Derivation of information-thermodynamic nonequilibrium
equalities

In this section, we derive the nonequilibrium equalities under
measurements and feedback control.

##### Setup

We consider a nonequilibrium process in a classical non-Markov
stochastic system with feedback control from time @xmath to @xmath . To
formulate the dynamics of the system, we discretize the time interval
into @xmath parts and define @xmath . Let @xmath and @xmath denote phase
spaces of the system and the memory, respectively. Moreover, we denote
the phase-space positions in @xmath and @xmath at time @xmath by @xmath
and @xmath , respectively. An external parameter to control the system
is denoted by @xmath , and the value of @xmath at time @xmath is @xmath
. Initially, the system and memory are at @xmath sampled from an initial
joint probability distribution @xmath . Then, the system evolves from
time @xmath to @xmath and a measurement is performed at time @xmath to
obtain a measurement outcome @xmath . After that, the system is subject
to feedback control driven by an external parameter @xmath from time
@xmath to @xmath . In this way, we repeat measurements and feedback
control. We define @xmath and @xmath , and therefore @xmath and @xmath
represents the entire trajectory of the system and memory, respectively.
Since @xmath is adjusted based on the measurement outcomes before time
@xmath , @xmath is a function of @xmath . Therefore, the non-Markov
dynamics of the system is determined by transition probabilities

  -- -------- -- ---------
     @xmath      (3.125)
  -- -------- -- ---------

On the other hand, since the measurement outcome depends on the
trajectory of the system before the measurement, the dynamics of the
memory is determined by

  -- -------- -- ---------
     @xmath      (3.126)
  -- -------- -- ---------

Therefore, the joint probability to realize @xmath and @xmath is
calculated as

  -- -------- -------- -------- -------- ---------
     @xmath   @xmath   @xmath            (3.127)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- ---------

where we define the transition probabilities as

  -- -------- -------- -------- -- ---------
     @xmath   @xmath   @xmath      (3.128)
     @xmath   @xmath   @xmath      (3.129)
  -- -------- -------- -------- -- ---------

Dividing Eq. ( 3.127 ) by @xmath , we calculate the conditional
probability as

  -- -------- -- ---------
     @xmath      (3.130)
  -- -------- -- ---------

Therefore, the conditional probability is different from the transition
probability under measurements and feedback control.

Following Ref. [ 59 ] , we define the information obtained by the
measurement at time @xmath as the mutual information between @xmath and
@xmath under the condition that we have obtained @xmath , that is,

  -- -------- -------- -------- -------- ---------
     @xmath   @xmath   @xmath            (3.131)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- ---------

The sum of the mutual information is calculated as

  -- -------- -------- -------- -------- ---------
     @xmath   @xmath   @xmath            (3.132)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- ---------

which is the total information obtained by all the measurements. Using
the total mutual information, we can transform Eq. ( 3.130 ) as

  -- -------- -- ---------
     @xmath      (3.133)
  -- -------- -- ---------

where @xmath is the mutual information at the initial time defined by

  -- -------- -- ---------
     @xmath      (3.134)
  -- -------- -- ---------

##### Derivation of information-thermodynamic nonequilibrium equalities

The formal entropy production @xmath is defined as the ratio of the
reference probability to the original probability under a fixed protocol
as in Eq. ( 2.32 ). Therefore, we obtain

  -- -------- -- ---------
     @xmath      (3.135)
  -- -------- -- ---------

We define the joint probability of the reference process as

  -- -------- -- ---------
     @xmath      (3.136)
  -- -------- -- ---------

which means that we sample the reference process conditioned by @xmath
with the same probability @xmath as the original process. From Eqs. (
3.127 ) and ( 3.136 ), the ratio of the reference joint probability to
the original joint probability is calculated as

  -- -------- -------- -------- -------- ---------
     @xmath   @xmath   @xmath            (3.137)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- ---------

Multiplying both sides by @xmath and integrating over @xmath and @xmath
, we obtain

  -- -------- -- ---------
     @xmath      (3.138)
  -- -------- -- ---------

because the reference probability is normalized to unity. In ordinary
feedback protocols, the system is assumed not to be correlated with the
memory at the initial time. Therefore, Eq. ( 3.138 ) reduces to

  -- -------- -- ---------
     @xmath      (3.139)
  -- -------- -- ---------

The appropriate choices of the reference probability explained in Sec.
2.2 reduce Eq. ( 3.139 ) to Eqs. ( 3.115 ), ( 3.117 ), ( 3.119 ), and (
3.121 ).

### 3.4 Experiments

In this section, we briefly review experimental demonstrations of
Maxwell’s demon.

The first experimental realization of Maxwell’s demon was done by Toyabe
et al. [ 16 ] . They demonstrated that the free energy of a Brownian
particle can be increased by feedback control based on measurements of
the position of the particle. A couple of polystyrene beads are
suspended in a water with one of them anchored to a glass plate. The
other bead can move on a ring, and is subjected to a washboard potential
created by four electrodes (see Fig. 3.6 (a)). At a certain instant in
time, the position of the particle is measured. After a delay time
@xmath , if the particle climbs up the potential due to thermal
agitation, the potential is switched to the other potential to prevent
the particle from descending; otherwise the potential is left unchanged.
This protocol of feedback control is repeated. As a result, the particle
is able to gain free energy larger than the work done on it in this
feedback-controlled process (see Fig. 3.6 (b)), namely, it was
demonstrated that information obtained by the measurements can be used
as a resource for free energy.

The information-thermodynamic nonequilibrium equality ( 3.115 ) was
experimentally verified in Ref. [ 17 ] . A feedback-controlled two-state
system similar to the Szilard engine was implemented in a
single-electron box (SEB) illustrated in Fig. 3.7 (a). The gate voltage
@xmath in Fig. 3.7 (a) is adjusted so that the minimum charging energy
is achieved when the average number @xmath of the electrons that have
tunneled from the left island to the right one is @xmath . The
temperature of the system is low enough that the SEB is in either the
@xmath or @xmath state. Therefore, the state with @xmath is realized
with the probability of @xmath , so is the state with @xmath . The state
of the SEB is monitored by a single-electron transistor (SET) (see Fig.
3.7 (a)), and the feedback control is conducted by changing @xmath based
on the value of @xmath to extract work from the SEB. The average in
Eq. ( 3.115 ) was experimentally confirmed to be unity within
experimental errors as shown in Fig. 3.7 (b).

## Chapter 4 Nonequilibrium Equalities in Absolutely Irreversible
Processes

As discussed in Chap. 2, nonequilibrium equalities apply to rather
general nonequilibrium situations. However, it is known that integral
fluctuation theorems are inapplicable to some situations. We propose a
new concept of absolute irreversibility as a novel class of
irreversibility that encompasses the entire range of those situations to
which conventional integral nonequilibrium equalities cannot apply. In
mathematical terms, the absolute irreversibility is defined as the
singular part of the reference probability measure, and is uniquely
separated from the ordinary irreversible part by Lebesgue’s
decomposition theorem [ 25 , 26 ] . We derive nonequilibrium equalities
that are applicable to absolutely irreversible processes based on
measure theory. Inequalities derived from our nonequilibrium equalities
give a positive-definite lower bound of the entropy production when a
process involves absolute irreversibility.

First of all, we consider free expansion to illustrate that absolute
irreversibility causes inapplicability of conventional nonequilibrium
integral equalities, and define absolute irreversibility in terms of
measure theory. Then, we derive nonequilibrium equalities in absolutely
irreversible processes based on Lebesgue’s decomposition theorem. Next,
we verify our nonequilibrium equalities in several examples. Finally, we
compare our method with a conventional method and discuss merits of
ours.

In this chapter, we restrict our attention to systems without
measurements and feedback control. This chapter is mainly based on Ref.
[ 27 ] .

### 4.1 Inapplicability of conventional integral nonequilibrium
equalities and absolute irreversibility

In this section, we introduce absolute irreversibility in an example of
free expansion, to which the Jarzynski equality cannot apply. Then, we
mathematically define absolute irreversibility in terms of measure
theory.

#### 4.1.1 Inapplicability of the Jarzynski equality

The Jarzynski equality is known to be inapplicable to free expansion [
21 , 22 ] . Here, we illustrate this fact. Suppose that an ideal
single-particle gas at temperature @xmath is prepared in the left side
of a box with a partition as illustrated in Fig. 4.1 (a). Then, we
remove the partition and let the gas expand to the entire box. In this
process, work is not extracted ( @xmath ), whereas the free energy
decreases ( @xmath ). Therefore, the dissipated work is always positive:

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

Thus, we have

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

which means that the Jarzynski equality ( 2.4 ) is not satisfied in this
process [ 21 ] . In physical terms, this is because the Jarzynski
equality assumes that the initial state is in a global equilibrium and
this assumption is not satisfied in the present case. Recall that, in
free expansion, the initial state is not a global equilibrium state, but
only a local equilibrium state [ 22 ] . Therefore, the Jarzynski
equality cannot apply to free expansion. Then, it is natural to ask why
a local equilibrium state cannot be assumed as an initial condition of
the conventional integral nonequilibrium equality.

The mathematical reason is that we have paths with a divergent entropy
production when we start from a local equilibrium state. This statement
is illustrated in free expansion as follows. We consider a set of
virtual paths @xmath starting from the right box. By assumption, the
probability of these paths in the forward process vanishes: @xmath . On
the other hand, the probability of the corresponding backward paths is
nonvanishing: @xmath . Therefore, we have

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

For these paths, the entropy production is negatively divergent in the
context of the Crooks fluctuation theorem ( 2.12 ) because

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

and

  -- -------- -- -------
     @xmath      (4.5)
  -- -------- -- -------

Due to the paths with a divergent entropy production, the conventional
integral nonequilibrium equality breaks down:

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (4.6)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

Therefore, we conclude that the negatively divergent entropy production
of the paths starting from the region in which the initial probability
vanishes is what makes the conventional integral nonequilibrium equality
inapplicable to the process starting from a local equilibrium state.

This situation [Eq. ( 4.3 )] makes a stark contrast to ordinary
irreversible processes, where every backward path has the corresponding
forward path with a nonvanishing probability:

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

or

  -- -------- -- -------
     @xmath      (4.8)
  -- -------- -- -------

Therefore, in the ordinary irreversible case, the exponentiated entropy
production remains finite

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

and the thermodynamic irreversibility is quantitatively characterized by
the entropy production. Although these paths are thermodynamically
irreversible, they are stochastically reversible in a sense of Eq. ( 4.7
), namely, every backward path has the nonvanishing forward counterpart.
In contrast, if the condition ( 4.3 ) holds, there exist backward paths
that have no counterparts in the forward process, which means that these
paths are not even stochastically reversible. Therefore, we shall call
these paths absolutely irreversible paths, and call the processes with
absolutely irreversible paths absolutely irreversible processes.

Mathematically, probability theory is based on measure theory, and the
ratio @xmath is interpreted as the transformation function of the two
probability measures. Therefore, we need measure theory to formulate
absolute irreversibility. We thus give a mathematical definition of
absolute irreversibility.

#### 4.1.2 Definition of absolute irreversibility

Let @xmath denote the probability measure of the original process. This
is a generalization of the description in terms of the probability
density which is written as @xmath . Let the probability measure of the
reference process be denoted by @xmath which is a generalization of
@xmath . According to Lebesgue’s decomposition theorem [ 25 , 26 ] , the
reference probability measure can be uniquely decomposed into two parts
as

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

where @xmath and @xmath are absolutely continuous and singular with
respect to @xmath , respectively (see Fig. 4.2 ). The absolute
continuity of @xmath guarantees that the probability ratio is
well-defined due to the Radon-Nikodým theorem [ 25 , 26 ] as

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

which is an integrable function with respect to @xmath . In physical
terms, it is this ratio that gives the entropy production. Therefore, in
measure theory, the Crooks fluctuation theorem reads

  -- -------- -- --------
     @xmath      (4.12)
  -- -------- -- --------

On the other hand, the probability defined by @xmath takes a nonzero
value in the region where the probability defined by @xmath vanishes.
Therefore, the ratio of @xmath to @xmath is divergent, and we cannot
define a finite entropy production through this ratio. Thus, in physical
terms, @xmath corresponds to the absolutely irreversible part. We
therefore identify @xmath as the ordinary irreversible part and @xmath
as the absolutely irreversible part. If @xmath does not vanish, the
conventional integral nonequilibrium equality breaks down [ 21 , 22 , 62
, 63 ] . See Appendix B for the mathematical definitions of absolute
continuity and singularity, and a mathematical statement of Lebesgue’s
decomposition theorem.

When @xmath can be written in terms of a probability density, a stronger
version of Lebesgue’s decomposition theorem holds. In this case, @xmath
is decomposed into three parts as

  -- -------- -- --------
     @xmath      (4.13)
  -- -------- -- --------

where @xmath and @xmath are absolutely continuous and singular
continusous with respect to @xmath , respectively, and @xmath is the
discrete part of @xmath (see Fig. 4.3 ). The singular continuous part
@xmath corresponds to the region where the probability ratio is
divergent because the denominator, which is the forward probability,
vanishes. This term represents the effect of free expansion. The
discrete part @xmath has @xmath -function-like localization, and the
probability ratio is divergent because the numerator, which is the
reference probability, diverges. This part arises when particles can
localize and do not undergo thermal diffusion; such a situation occurs
when there are trapping centers of particles. In this way, the absolute
irreversibility is classified into two categories. The correspondence
between the classification of irreversibility and that of probability
measure is summarized in Table 4.1 .

### 4.2 Nonequilibrium equalities in absolutely irreversible processes

In this section, we derive nonequilibrium equalities applicable to
absolutely irreversible processes based on Lebesgue’s decomposition
theorem ( 4.10 ) and ( 4.13 ).

First, we derive general nonequilibrium equalities in absolutely
irreversible processes. Then, we show that they reduce to several
individual nonequilibrium equalities with their specific meanings of the
entropy production by proper choices of the reference probability
distribution as described in Sec. 2.2.

#### 4.2.1 General formulation

First, we derive nonequilibrium equalities based on Lebesgue’s
decomposition theorem ( 4.10 ). Let @xmath denote an arbitrary
functional of a path @xmath , and let @xmath , @xmath and @xmath (I=AC,
S) denote the average over @xmath , @xmath and @xmath , respectively.
From Lebesgue’s decomposition theorem ( 4.10 ), we obtain

  -- -------- -- --------
     @xmath      (4.14)
  -- -------- -- --------

On the other hand, using the Radon-Nikodým derivative ( B.8 ), we
evaluate the average over the absolutely continuous part in terms of the
entropy production as

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.15)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where we use the Crooks fluctuation theorem ( 4.12 ) to obtain the third
line. Therefore, by Eqs. ( 4.14 ) and ( 4.15 ), we obtain

  -- -------- -- --------
     @xmath      (4.16)
  -- -------- -- --------

which can be regarded a generalization of the master fluctuation theorem
( 2.35 ). If we set @xmath to unity, we obtain

  -- -------- -- --------
     @xmath      (4.17)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (4.18)
  -- -------- -- --------

is the probability of the singular part. We note that @xmath is uniquely
defined because of the uniqueness of Lebesgue’s decomposition ( 4.10 ).
As explained in the previous section and summarized in Table 4.1 , the
absolute irreversibility has two classes. Therefore, @xmath is
calculated as the sum of two contributions of these classes of absolute
irreversibility. One is the probability of those reference paths whose
corresponding original paths have vanishing probability. The other
contribution is the probability of localized reference paths. To the
best of our knowledge, the localized contribution had not been
considered before our research [ 27 ] . However, this part renders the
conventional nonequilibrium equalities inapplicable. Namely, only when
@xmath , we reproduce @xmath

Using Jensen’s inequality @xmath , we obtain

  -- -------- -- --------
     @xmath      (4.19)
  -- -------- -- --------

Therefore, the second-law-like inequality @xmath is valid even with
absolute irreversibility, because the right-hand side of Eq. ( 4.19 ) is
nonnegative. Moreover, if there exists absolute irreversibility @xmath ,
then Eq. ( 4.19 ) imposes a stronger restriction on the entropy
production than the conventional inequality because the right-hand side
will then be strictly positive. Thus, the average of the entropy
production must be strictly positive in absolutely irreversible
processes.

When the stronger version of Lebesgue’s decomposition holds, by
following the same procedure as before, we obtain

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (4.20)
     @xmath   @xmath   @xmath      (4.21)
  -- -------- -------- -------- -- --------

where @xmath (i=sc, d) represents the average over @xmath , and @xmath
and @xmath are the probabilities of the singular continuous part and the
discrete part, respectively. We can calculate @xmath as the sum of the
probabilities of those reference paths whose corresponding counterparts
in the original process vanish. On the other hand, @xmath is calculated
as the sum of localized reference paths. From Eq. ( 4.21 ), we obtain an
inequality

  -- -------- -- --------
     @xmath      (4.22)
  -- -------- -- --------

which leads to a fundamental lower bound on the entropy production in
absolutely irreversible processes.

#### 4.2.2 Physical implications

We apply the general results in the previous section to processes
starting from a restricted region.

Here, we consider time reversal as the reference dynamics. In accordance
with Table 2.1 , in a Langevin or Hamiltonian system, Eq. ( 4.12 )
reduces to

  -- -- -- --------
           (4.23)
  -- -- -- --------

where we rewrite the boundary term in terms of measure theory and @xmath
represents the configuration coordinates in the case of an overdamped
Langevin system and the phase space coordinates of the system and heat
bath in the case of a Hamiltonian system. Here, @xmath , @xmath and
@xmath represent the initial probability measure of the original
process, the initial probability measure of the time-reversed process
and the Lebesgue measure on the phase space @xmath , respectively, and
@xmath is the absolutely continuous part of @xmath with respect to
@xmath and @xmath is the absolutely continuous part of @xmath with
respect to @xmath .

##### Generalization of the Jarzynski equality

Now, let us assume that the initial state of the original process is a
local equilibrium state which is restricted to a region @xmath as

  -- -------- -- --------
     @xmath      (4.24)
  -- -------- -- --------

where @xmath is the characteristic function defined by

  -- -------- -- --------
     @xmath      (4.25)
  -- -------- -- --------

and the free energy is defined by

  -- -------- -- --------
     @xmath      (4.26)
  -- -------- -- --------

In this case, the absolutely continuous part of @xmath with respect to
@xmath is

  -- -------- -- --------
     @xmath      (4.27)
  -- -------- -- --------

Therefore, we obtain

  -- -------- -- --------
     @xmath      (4.28)
  -- -------- -- --------

On the other hand, we set the initial probability measure of the
time-reversed process to a local equilibrium distribution in a region
@xmath as

  -- -------- -- --------
     @xmath      (4.29)
  -- -------- -- --------

which is already absolutely continuous with respect to @xmath , namely,
@xmath . Therefore, we obtain

  -- -------- -- --------
     @xmath      (4.30)
  -- -------- -- --------

Then, substituting Eqs. ( 4.28 ) and ( 4.30 ) into Eq. ( 4.23 ), we
obtain

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.31)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where we use the first law of thermodynamics to obtain the last
equality. Thus, Eq. ( 4.17 ) reduces to

  -- -------- -- --------
     @xmath      (4.32)
  -- -------- -- --------

In particular, if we set @xmath to @xmath , we obtain

  -- -------- -- --------
     @xmath      (4.33)
  -- -------- -- --------

which is a generalization of the Jarzynski equality ( 2.4 ).

##### Generalization of the Seifert relation

Here, we assume that the initial probability distribution of the
original process is absolutely continuous with respect to the Lebesgue
measure, and therefore we have

  -- -------- -- --------
     @xmath      (4.34)
  -- -------- -- --------

Let @xmath denote the support of @xmath . Then, the absolutely
continuous part of @xmath with respect to @xmath is

  -- -------- -- --------
     @xmath      (4.35)
  -- -------- -- --------

Therefore, the Radon-Nikodym derivative is

  -- -------- -- --------
     @xmath      (4.36)
  -- -------- -- --------

We set the initial probability distribution of the reference process to
be equal to the final probability distribution of the original process:
@xmath . Then, the Radon-Nikodym derivative

  -- -------- -- --------
     @xmath      (4.37)
  -- -------- -- --------

represents the (unnormalized) final probability distribution of the
original process. Therefore, Eq. ( 4.23 ) reduces to

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.38)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Thus, Eq. ( 4.17 ) reduces to

  -- -------- -- --------
     @xmath      (4.39)
  -- -------- -- --------

where we use @xmath because @xmath is the support of @xmath . This is a
generalization of the Seifert relation ( 2.25 ).
Up to here, we set the reference dynamics to the time-reversed dynamics,
and have shown that our nonequilibrium equality ( 4.17 ) with absolute
irreversibility derives generalized integral nonequilibrium equalities (
4.33 ) and ( 4.39 ) involving the entropy production with the heat bath.
We note that our nonequilibrium equality ( 4.17 ) also derives
generalized integral fluctuation theorems for the housekeeping and
excess entropy production in a similar manner under the proper choices
of the reference dynamics as summarized in Table 4.2 .

### 4.3 Examples of absolutely irreversible processes

In this section, we verify our nonequilibrium equality in three
examples: free expansion, an overdamped Langevin process starting from a
local equilibrium, and an overdamped Langevin system with a trapping
center.

#### 4.3.1 Free expansion

First of all, we discuss the case of free expansion (see Fig. 4.1 (a)).
Initially, an ideal single-particle gas is confined in the left box with
temperature @xmath . The entire box is assumed to be divided in the
volume ratio @xmath by a partition. Since the initial state can be
described by the probability density, the stronger version of Lebesgue’s
decomposition holds. Thus, Eq. ( 4.33 ) reduces to

  -- -------- -- --------
     @xmath      (4.40)
  -- -------- -- --------

We remove the partition, and the gas expands to the entire box. In this
process, work is not extracted: @xmath , whereas the free energy of the
gas decreases due to the expansion by @xmath . Therefore, the left-hand
side of Eq. ( 4.40 ) is calculated to be

  -- -------- -- --------
     @xmath      (4.41)
  -- -------- -- --------

We consider the time-reversed process of this free expansion to
calculate the absolute irreversible probabilities (see Fig. 4.1 (b)). In
the time-reversed process, the system is in a global equilibrium at the
initial time. Then, the partition is inserted at the same position as
the original process. The gas particle is in either the left or the
right box. The paths ending in the right box have no corresponding
forward paths in the original process. Therefore, these paths are
singular continuous paths. The probability of these paths is
proportional to the volume fraction of the right box, and therefore the
singular continuous probability is

  -- -------- -- --------
     @xmath      (4.42)
  -- -------- -- --------

On the other hand, since there is no discrete path, which is a single
path with a finite positive probability, we have

  -- -------- -- --------
     @xmath      (4.43)
  -- -------- -- --------

Therefore, we obtain

  -- -------- -- --------
     @xmath      (4.44)
  -- -------- -- --------

Thus, our nonequilibrium equality ( 4.40 ) is verified for the case of
free expansion of an ideal single-gas particle.

#### 4.3.2 Process starting from a local equilibrium

Next, we consider an overdamped Langevin process starting from a local
equilibrium state. The Langevin particle is confined in a
one-dimensional ring. The potential consists of @xmath identical
harmonic potential wells with the same stiffness (spring constant)
@xmath as illustrated in Fig. 4.4 (a). Initially, the system is prepared
in a local equilibrium state with temperature @xmath in a given well,
and therefore the initial probability vanishes elsewhere. We subject the
system to a nonequilibrium process during a time interval @xmath . We
decrease the stiffness of the potentials from @xmath to @xmath at a
constant rate between @xmath and @xmath , and then increase it from
@xmath to @xmath at a constant rate between @xmath and @xmath . Since
the initial state can be written in terms of the probability density,
the stronger version of Lebesgue’s decomposition holds. Therfore, Eq. (
4.33 ) reduces to

  -- -------- -- --------
     @xmath      (4.45)
  -- -------- -- --------

Now, we consider the time-reversed process to calculate the singular
probabilities. Initially, the system is in a global equilibrium at
temperature @xmath with stiffness @xmath . The stiffness is decreased
from @xmath to @xmath between @xmath and @xmath , and then increased
from @xmath to @xmath between @xmath and @xmath . Because a backward
path terminates in a certain well with probability @xmath due to the
symmetry of the potential and the initial state of the time-reversed
process, the probability that the backward path does not have the
corresponding forward path is

  -- -------- -- --------
     @xmath      (4.46)
  -- -------- -- --------

Moreover, since we have no discrete path, we obtain

  -- -------- -- --------
     @xmath      (4.47)
  -- -------- -- --------

Therefore, Eq. ( 4.45 ) reduces to

  -- -------- -- --------
     @xmath      (4.48)
  -- -------- -- --------

If we assume that @xmath is sufficiently large, @xmath in this process.
Thus, we obtain

  -- -------- -- --------
     @xmath      (4.49)
  -- -------- -- --------

The corresponding inequality reads

  -- -------- -- --------
     @xmath      (4.50)
  -- -------- -- --------

We obtain the probability distributions of @xmath at different @xmath by
numerical simulations as shown in Fig. 4.4 (b). Based on this
probability distribution, the value of @xmath is obtained and confirmed
to be @xmath (see Fig. 4.4 (c)). We also verify that the average
dissipation @xmath is larger than the minimum dissipation given by Eq. (
4.50 ), namely, the fundamental lower bound due to the absolute
irreversibility as demonstrated in Fig. 4.4 (d). We note that this
process may be regarded as an information erasure of an @xmath -digit
memory.

#### 4.3.3 System with a trap

Finally, we consider an overdamped Langevin system with a trap. The
system is one-dimensional, and the Langevin particle is confined in a
single harmonic potential with stiffness @xmath . We assume that there
is a trapping point in the system and the distance between the center of
the harmonic potential and the trapping point is denoted by @xmath as
illustrated in Fig. 4.5 (a). If the particle reaches the trapping point,
it is trapped with unit probability. Initially, the system is prepared
in an equilibrium of the harmonic potential. We subject the system to a
nonequilibrium process by changing stiffness @xmath . The stiffness is
decreased from @xmath to @xmath at a constant rate between @xmath and
@xmath , and then increased from @xmath to @xmath at a constant rate
between @xmath to @xmath . Since the initial probability can be written
by the probability density, Eq. ( 4.39 ) reduces to

  -- -------- -- --------
     @xmath      (4.51)
  -- -------- -- --------

To evaluate the singular probabilities, we consider the time-reversed
process. The initial state of the time-reversed process is set to the
final state of the original process. Let @xmath and @xmath denote the
trapping probabilities of the final state of the original and
time-reversed processes, respectively. Because the particle trapped in
the original process will remain trapped in the entire time-reversed
process, the probability of this single path is @xmath . Therefore, the
discrete probability is

  -- -------- -- --------
     @xmath      (4.52)
  -- -------- -- --------

Moreover, since the paths that fall into the trap in the time-reversed
process have no corresponding counterparts in the original process, they
are singular continuous. Therefore, the singular continuous probability
is

  -- -------- -- --------
     @xmath      (4.53)
  -- -------- -- --------

Thus, Eq. ( 4.51 ) reduces to

  -- -------- -- --------
     @xmath      (4.54)
  -- -------- -- --------

which leads to an inequality

  -- -------- -- --------
     @xmath      (4.55)
  -- -------- -- --------

This inequality is automatically satisfied because the left-hand side is
positively divergent due to the presence of those paths that fall into
the trap in the original path with a positively divergent entropy
production.

Figure 4.5 (b) shows the probability density of the total entropy
production. Based on this probability distribution, the exponentiated
average of the total entropy production is calculated and confirmed to
be consistent with Eq. ( 4.54 ) as demonstrated in Fig. 4.5 (c).

### 4.4 Comparison with a conventional method

In this section, we review a conventional method [ 64 ] to compare with
our method based on absolute irreversibility.

As an illustration, we rederive our nonequilibrium equality in free
expansion, namely, Eq. ( 4.40 ) by using the conventional method. We
consider a virtual process starting from a global equilibrium state with
temperature @xmath as shown in Fig. 4.6 . Namely, the particle is in the
left box with probability @xmath and in the right box with probability
@xmath . Therefore, the probability of paths starting from the right box
does not vanish, and absolutely irreversible paths do not exist. Thus,
the conventional nonequilibrium ( 2.17 ) applies as

  -- -------- -- --------
     @xmath      (4.56)
  -- -------- -- --------

where @xmath is an arbitrary functional, and @xmath denotes the
statistical average of the virtual process, and the caret ^ means that
the accompanying quantity is the one in the virtual process. In this
simple case, we can easily obtain relations between the physical
quantities in the virtual process and the corresponding quantities in
the original process as

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (4.57)
     @xmath   @xmath   @xmath      (4.58)
     @xmath   @xmath   @xmath      (4.59)
     @xmath   @xmath   @xmath      (4.60)
  -- -------- -------- -------- -- --------

To derive a nonequilibrium equality in the original process, we set
@xmath to the characteristic functional whose value is equal to one only
if the path starts from the left box and zero otherwise. Then,
considering the normalization of probabilities properly, we can express
the average in the original process in terms of that in the virtual
process as

  -- -------- -- --------
     @xmath      (4.61)
  -- -------- -- --------

In the time-reversed process, the probability of paths ending in the
right box is @xmath , and therefore we obtain

  -- -------- -- --------
     @xmath      (4.62)
  -- -------- -- --------

Therefore, Eq. ( 4.56 ) reduces to

  -- -------- -- --------
     @xmath      (4.63)
  -- -------- -- --------

which agrees with Eq. ( 4.40 ) derived from the nonequilibrium
equalities with absolute irreversibility. Following a similar procedure,
we can also rederive Eqs. ( 4.45 ) and ( 4.51 ) in principle.

Let us examine the meaning of this conventional procedure. First, we
extend the initial probability distribution to the global canonical
distribution of the system to circumvent the problem of the vanishing
probability. Then, we derive the nonequilibrium equality ( 4.56 ) in
this artificial process. Next, we express physical quantities of the
artificial process by those of the original process as in Eqs. ( 4.57 )
and ( 4.60 ). Finally, we erase some paths irrelevant to the original
dynamics by setting @xmath to the characteristic function, and relate
the average of the virtual process to that of the original process as in
Eq. ( 4.61 ).

In this way, the conventional method must introduce the artificial
process to derive the nonequilibrium equality to avoid the problem
arising from absolute irreversibility. In contrast, our method directly
deals with the original process. Moreover, the reason why the right-hand
sides of Eqs. ( 4.40 ), ( 4.45 ), and ( 4.51 ) deviate from one is
clearer in our method, that is, the probability of absolutely
irreversible paths should be subtracted from one. Incidentally, we note
that although Eq. ( 4.51 ) can in hindsight be derived by the
conventional method, we naturally find this absolutely irreversible
example by virtue of the stronger version of Lebesgue’s decomposition (
4.13 ).

## Chapter 5 Information-Thermodynamic Nonequilibrium Equalities in
Absolutely Irreversible Processes

In this chapter, we generalize the nonequilibrium equalities in the
presence of absolute irreversibility obtained in Chap. 4 to situations
under measurements and feedback control. This generalization is of vital
importance when error-free measurements are preformed, because they
project the probability distribution of the measured state onto a
localized region in the phase space. The subsequent time evolution of
this confined post-measurement state, in general, involves expansion
into an initially unoccupied region, which provides yet another example
of absolute irreversibility. The generalized nonequilibrium equalities
under measurements and feedback control enable us to identify the
unavailable information, which results from the inevitable loss of
information that we cannot utilize under a fixed feedback protocol. The
unavailable information provides a fundamental limit of performance of
the feedback protocol.

First, we derive information-thermodynamic nonequilibrium equalities in
the presence of absolute irreversibility. Next, we introduce a notion of
unavailable information and derive a different type of nonequilibrium
equalities that involve the unavailable information. Finally, we
demonstrate our nonequilibrium equalities in several examples.

This chapter is partly based on Refs. [ 27 , 28 ] .

### 5.1 Inforamtion-thermodynamic equalities

In this section, we derive information-thermodynamic nonequilibrium
equalities in the presence of absolute irreversibility. This section is
mainly based on Ref. [ 27 ] .

Under feedback control, it is important to consider the effect of the
singular part of the reference probability measure discussed in Sec.
4.1.2 because high-precision measurements such as error-free
measurements localize the probability distribution. Since the feedback
control starts from this post-measurement state confined in a narrow
region, the subsequent time evolution involves expansion into an
initially unoccupied region unless the feedback protocol is fine-tuned,
and therefore the process exhibits absolute irreversibility. In
experiments, since we have access to only a few parameters, the
fine-tuning is a difficult task in general. Hence, absolute
irreversibility has experimental relevance in a system under
measurements and feedback control.

We consider a nonequilibrium process in a classical system with
measurements and feedback control from time @xmath to @xmath as in Sec.
3.3.2. Let @xmath and @xmath denote phase-space points at time @xmath of
the system and the measurement outcomes, respectively. The external
control parameter @xmath is determined based on the measurement outcome
before @xmath , namely, @xmath . Let @xmath , @xmath and @xmath denote
the entire paths of the system, the measurement outcomes and the control
parameter, respectively. Moreover, let @xmath denote the conditional
probability measure of @xmath under given measurement outcomes @xmath ,
and let @xmath denote the reference probability measure of @xmath under
a given feedback protocol @xmath . We apply Lebesgue’s decomposition
theorem to @xmath with respect to @xmath , and obtain

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

where the first and second terms on the right-hand side give the
absolutely continuous and singular parts of the reference probability
measure, respectively. Lebesgue’s decomposition theorem ensures that
this decomposition is unique. Let @xmath denote an arbitrary path
functional. It follows from Eq. ( 5.1 ) that

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

where @xmath denotes the average over @xmath . Moreover, the average
over the absolutely continuous part can be transformed by the
Radon-Nikodým derivative because

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (5.3)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

where @xmath denotes the average over @xmath , and we define

  -- -------- -- -------
     @xmath      (5.4)
  -- -------- -- -------

Therefore, we obtain

  -- -------- -- -------
     @xmath      (5.5)
  -- -------- -- -------

If we set @xmath to unity, we obtain

  -- -------- -- -------
     @xmath      (5.6)
  -- -------- -- -------

where

  -- -------- -- -------
     @xmath      (5.7)
  -- -------- -- -------

is the probability of the singular part of the reference probability
measure conditioned by @xmath .

Here, we consider the physical meaning of @xmath defined in Eq. ( 5.4 )
¹ ¹ 1 Here, we assume that @xmath is absolutely continuous with respect
to @xmath and that @xmath is absolutely continuous with respect to
@xmath . Otherwise, we cannot define the entropy production and mutual
information individually, because their values are divergent and
mathematically ill-defined. However, the sum of these quantities remains
finite due to the absolute continuity of @xmath with respect to @xmath .
. The Crooks fluctuation theorem ( 4.12 ) applies to the ratio of the
transition probabilities under a fixed protocol:

  -- -------- -- -------
     @xmath      (5.8)
  -- -------- -- -------

where @xmath is the transition probability under the fixed protocol
@xmath . Therefore, @xmath is different from @xmath because the
conditional probability is different form the transition probability as
discussed in Sec. 3.2.2. The difference is represented by the
Radon-Nikodym derivative as

  -- -------- -- -------
     @xmath      (5.9)
  -- -------- -- -------

Comparing this with Eq. ( 3.133 ), we find that the Radon-Nikodym
derivative in this equation may be written as @xmath , where @xmath is
the total mutual information obtained by the measurements and @xmath is
the initial correlation between the system and the measurement outcomes.
Therefore, we have

  -- -------- -- --------
     @xmath      (5.10)
  -- -------- -- --------

and Eqs. ( 5.5 ) and ( 5.6 ) reduce to

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (5.11)
     @xmath   @xmath   @xmath      (5.12)
  -- -------- -------- -------- -- --------

respectively. Averaging these over measurement outcomes @xmath , we
obtain

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (5.13)
     @xmath   @xmath   @xmath      (5.14)
  -- -------- -------- -------- -- --------

where @xmath is the average over @xmath of the @xmath -conditioned
singular probability @xmath . Equation ( 5.14 ) derives a
second-law-like inequality as

  -- -------- -- --------
     @xmath      (5.15)
  -- -------- -- --------

Therefore, the lower bound of the entropy production is determined not
only by the mutual information but also by the term arising from the
absolute irreversibility. If the feedback protocol is so poorly designed
that @xmath , Eq. ( 5.15 ) implies that the entropy production is
positive and the feedback protocol does not work. In the case of
vanishing initial correlations, we obtain

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (5.16)
     @xmath   @xmath   @xmath      (5.17)
     @xmath   @xmath   @xmath      (5.18)
  -- -------- -------- -------- -- --------

Moreover, if the conditioned initial states @xmath satisfy the
assumption of the stronger version of Lebesgue’s decomposition ( 4.13 ),
we obtain

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (5.19)
     @xmath   @xmath   @xmath      (5.20)
     @xmath   @xmath   @xmath      (5.21)
  -- -------- -------- -------- -- --------

where the subscripts “sc” and “d” represent the singular continuous and
discrete parts, respectively.

In the same manner as the case without feedback control, proper choices
of the reference probability lead to nonequilibrium equalities with
specific meanings of the entropy production as listed in Table 4.2 , and
we obtain the corresponding equalities

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (5.22)
     @xmath   @xmath   @xmath      (5.23)
     @xmath   @xmath   @xmath      (5.24)
     @xmath   @xmath   @xmath      (5.25)
     @xmath   @xmath   @xmath      (5.26)
  -- -------- -------- -------- -- --------

under the proper assumptions described in Sec. 2.2.

### 5.2 Unavailable information and associated equalities

In this section, we define a concept of unavailable information based on
the information-thermodynamic nonequilibrium equalities obtained in the
previous section, and derive new nonequilibrium equalities that involve
the unavailable information. Inequalities derived from the new
equalities give an achievable lower bound of the entropy production in
the case of error-free measurements.

First of all, we point out that the equality of Eq. ( 5.18 ) cannot be
achieved in general even in the quasi-static limit. The equality
condition of Jensen’s inequality @xmath is that the quantity @xmath does
not fluctuate. On the other hand, in the quasi-static limit, @xmath is
expected to have a single definite value. Therefore, in situations
without feedback control, the equality in the inequality

  -- -------- -- --------
     @xmath      (5.27)
  -- -------- -- --------

is achieved. In contrast, the equality in the inequality under feedback
control

  -- -------- -- --------
     @xmath      (5.28)
  -- -------- -- --------

cannot be achieved in general even in the quasi-static limit due to
fluctuations of @xmath , that is, @xmath depends on the measurement
outcomes unless all outcomes are obtained with the same probability.
Hence, Eq. ( 5.28 ) gives only a loose lower bound of the entropy
production, although it is tighter than the conventional inequality (
@xmath ) [ 18 ] .

To find an achievable lower bound of the entropy production, we start
from Eq. ( 5.12 ). If we assume that we have no initial correlations,
Eq. ( 5.12 ) reduces to

  -- -------- -- --------
     @xmath      (5.29)
  -- -------- -- --------

From Jensen’s inequality, we obtain

  -- -------- -- --------
     @xmath      (5.30)
  -- -------- -- --------

What is noteworthy on this inequality is that the equality is achievable
under error-free measurements in the quasi-static limit, because @xmath
reduces to the unaveraged Shannon entropy of @xmath , which has a single
definite value under fixed measurement outcomes @xmath . Therefore, the
right-hand side of Eq. ( 5.30 ) gives an achievable lower bound of the
entropy production for a fixed @xmath . Hence, the last term in Eq. (
5.30 ) represents the inevitable dissipation due to the incompleteness
of the feedback protocol @xmath . In other words, the feedback protocol
@xmath cannot fully utilize the mutual information obtained by the
measurements. Thus, we define unavailable information in the protocol
@xmath as

  -- -------- -- --------
     @xmath      (5.31)
  -- -------- -- --------

We note that, under error-free measurements and in the quasi-static
limit. we have

  -- -------- -- --------
     @xmath      (5.32)
  -- -------- -- --------

Using Eq. ( 5.31 ), we rewrite Eq. ( 5.29 ) as

  -- -------- -- --------
     @xmath      (5.33)
  -- -------- -- --------

Averaging this equality over @xmath , we obtain a new nonequilibrium
equality:

  -- -------- -- --------
     @xmath      (5.34)
  -- -------- -- --------

This equality leads to

  -- -------- -- --------
     @xmath      (5.35)
  -- -------- -- --------

We note that this inequality is stronger than Eq. ( 5.28 ) due to the
convexity of the logarithmic function @xmath , that is,

  -- -------- -- --------
     @xmath      (5.36)
  -- -------- -- --------

Furthermore, Eq. ( 5.35 ) gives an achievable lower bound of the entropy
production under error-free measurements and in the quasi-static limit
because of Eq. ( 5.32 ). Therefore, we can quantitatively characterize
the incompleteness of the feedback protocol by calculating the
unavailable information.

Here, we assume that the initial state is an equilibrium state, and set
the reference probability to the time-reversed one starting from an
equilibrium state. Then, Eqs. ( 5.34 ) and ( 5.35 ) reduce to
Jarzynski-type equations as

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (5.37)
     @xmath   @xmath   @xmath      (5.38)
  -- -------- -------- -------- -- --------

respectively. Equation ( 5.38 ) gives an achievable upper bound of
extractable work in the feedback process. The bound is reduced due to
the unavailable information compared with the conventional result [ 18 ]
.

The other choices of the reference probability summarized in Table 2.1
also give nonequilibrium equalities with their individual meanings.

### 5.3 Examples of absolutely irreversible processes

In this section, we verify the information-thermodynamic nonequilibrium
equalities derived in the previous sections. Here, we only consider
relations with dissipated work, namely,

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (5.39)
     @xmath   @xmath   @xmath      (5.40)
     @xmath   @xmath   @xmath      (5.41)
     @xmath   @xmath   @xmath      (5.42)
  -- -------- -------- -------- -- --------

First, we consider a measurement and subsequent trivial feedback
control. Second, we discuss the two-particle Szilard engine. Finally,
the multi-particle Szilard engine is considered.

#### 5.3.1 Measurement and trivial feedback control

We consider a measurement and the subsequent feedback control.
Initially, an ideal single-particle gas is in a global equilibrium state
of the box as illustrated in Fig. 5.1 (a). At a certain time, we perform
an instantaneous error-free measurement to determine the position @xmath
of the particle. An outcome @xmath is obtained when the particle is in
the left side, that is, the length from the left-end wall is shorter
than the length of the whole box multiplied by @xmath ( @xmath ). On the
other hand, an outcome @xmath is obtained when the particle is in the
right side. In both cases, we conduct trivial feedback control, i.e., we
leave the system as it is. Therefore, the gas expands to the entire box.

In this process, work is not extracted: @xmath , and the free energy
does not change: @xmath . Let @xmath and @xmath denote the probabilities
to obtain outcomes @xmath and @xmath , respectively. If the measurement
outcome is @xmath , we obtain mutual information @xmath . If @xmath , we
obtain mutual information @xmath .

Next, we consider the time-reversed process to find the singular
probabilities. In the time-reversed process, we do nothing for both
@xmath and @xmath , because we do nothing in the forward process except
for the measurement. The time-reversed process corresponding to the
outcome @xmath is illustrated in the left half of Fig. 5.1 (b). The
particle is in the left side with probability @xmath and in the right
side with probability @xmath . The case of the particle being found in
the right side has no corresponding forward event under @xmath , and
therefore it is a singular event. Therefore, we obtain

  -- -------- -- --------
     @xmath      (5.43)
  -- -------- -- --------

On the other hand, in the time-reversed process corresponding to the
outcome @xmath illustrated in the right half of Fig. 5.1 (b), we obtain

  -- -------- -- --------
     @xmath      (5.44)
  -- -------- -- --------

Therefore, the unavailable information is obtained as

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (5.45)
     @xmath   @xmath   @xmath      (5.46)
  -- -------- -------- -------- -- --------

We note that the unavailable information coincides with the mutual
information in this case, because all the information is lost since we
do nothing as feedback control. The values of the physical quantities
are summarized in Table 5.1 .

The left-hand side of Eq. ( 5.39 ) is calculated as

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (5.47)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

On the other hand, the averaged singular probability is

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (5.48)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Therefore, by a simple calculation, we can verify Eq. ( 5.39 ). Next,
let us verify Eq. ( 5.40 ). The average of the dissipated work is

  -- -------- -- --------
     @xmath      (5.49)
  -- -------- -- --------

The right-hand side of Eq. ( 5.40 ) is

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (5.50)
  -- -------- -------- -------- -- --------

By analytic calculation, we can show that the right-hand side of Eq. (
5.50 ) is nonpositive, that is,

  -- -------- -- --------
     @xmath      (5.51)
  -- -------- -- --------

and it is zero if and only if @xmath . Thus, Eq. ( 5.40 ) is verified.
Moreover, the necessary and sufficient condition for the equality is
@xmath , which is consistent with the observation made in Sec. 5.2,
namely, the equality in the inequality ( 5.28 ) is achieved only when
all outcomes are obtained with the equal probability.

Now, we verify Eqs. ( 5.41 ) and ( 5.42 ) in the presence of the
unavailable information. The left-hand side of Eq. ( 5.41 ) is
calculated as

  -- -------- -- --------
     @xmath      (5.52)
  -- -------- -- --------

which verifies Eq. ( 5.41 ). Moreover, because @xmath in this case, the
right-hand side of Eq. ( 5.42 ) is

  -- -------- -- --------
     @xmath      (5.53)
  -- -------- -- --------

Therefore, we obtain

  -- -------- -- --------
     @xmath      (5.54)
  -- -------- -- --------

This equality means that the equality in the inequality ( 5.42 ) is
achieved regardless of the value of @xmath , which is what we expect
because Eq. ( 5.42 ) gives an achievable bound.

#### 5.3.2 Two-particle Szilard engine

Next, we consider the two-particle Szilard engine. The reason why we do
not consider the single-particle Szilard engine as in Refs. [ 20 , 24 ]
is that the singular part does not arise in the single-particle Szilard
engine since we can fully utilize the information obtained by the
measurements. Therefore, we should consider a Szilard engine with two or
more particles to observe effects of the absolute irreversibility. We
begin by the two-particle Szilard engine.

Initially, two identical ideal-gas particles are confined in a box as
illustrated in Fig. 5.2 . We insert a partition in the middle of the
box. Then, we perform an error-free measurement to find the number
@xmath of the particles in the right side. If @xmath , we isothermally
and quasi-statically shift the wall to the right end to extract work,
and then remove the partition. If @xmath , we shift the wall to the
opposite direction to obtain the same amount of work, and then remove
the partition. If @xmath , we just remove the wall because we cannot
extract work by shifting the partition. For all @xmath , the system
returns to the initial state after these protocols. The values of
extracted work are summarized in Table 5.2 .

We consider the time-reversed process to obtain the singular
probabilities (see Fig. 5.2 ). Initially, the two particles are in a
global equilibrium regardless of @xmath . When @xmath , we insert the
partition in the right end of the box. Then, we isothermally and
quasi-statically shift the partition to the middle of the box and remove
the partition. Because this path has the counterpart in the forward
process, we have no singularity when @xmath :

  -- -------- -- --------
     @xmath      (5.55)
  -- -------- -- --------

Similarly, we have no singular part when @xmath :

  -- -------- -- --------
     @xmath      (5.56)
  -- -------- -- --------

Now, let us consider the time-reversed process when @xmath . In this
case, we insert the partition in the middle of the box and then remove
it. The probability of the two particles being found in the left box has
nonvanishing probability, so does the probability of both particles
being found in the right box. However, we have no corresponding forward
paths in the protocol of @xmath . Therefore, these time-reversed paths
are singular, and the singular probability is

  -- -------- -- --------
     @xmath      (5.57)
  -- -------- -- --------

Thus, the unavailable information is

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (5.58)
     @xmath   @xmath   @xmath      (5.59)
     @xmath   @xmath   @xmath      (5.60)
  -- -------- -------- -------- -- --------

Now, we are ready to verify our nonequilibrium equalities. The left-hand
side of Eq. ( 5.39 ) is calculated as

  -- -------- -- --------
     @xmath      (5.61)
  -- -------- -- --------

On the other hand, the average of the singular part is

  -- -------- -- --------
     @xmath      (5.62)
  -- -------- -- --------

Therefore, Eq. ( 5.39 ) is verified. The left-hand side of Eq. ( 5.40 )
is

  -- -------- -- --------
     @xmath      (5.63)
  -- -------- -- --------

On the other hand, the right-hand side of Eq. ( 5.40 ) is

  -- -------- -- --------
     @xmath      (5.64)
  -- -------- -- --------

Hence, we obtain

  -- -------- -- --------
     @xmath      (5.65)
  -- -------- -- --------

and Eq. ( 5.40 ) is verified. We note that the equality in the
inequality ( 5.40 ) is not achieved because the probability of finding a
particular value of @xmath varies for each @xmath .

The left-hand side of Eq. ( 5.41 ) is calculated as

  -- -------- -- --------
     @xmath      (5.66)
  -- -------- -- --------

which verifies Eq. ( 5.41 ). The right-hand side of Eq. ( 5.42 ) is

  -- -------- -- --------
     @xmath      (5.67)
  -- -------- -- --------

Thus, we obtain

  -- -------- -- --------
     @xmath      (5.68)
  -- -------- -- --------

verifying Eq. ( 5.42 ). Because Eq. ( 5.42 ) gives an achievable bound
of work in the quasi-static limit, the equality in Eq. ( 5.42 ) is
achieved.

#### 5.3.3 Multi-particle Szilard engine

We generalize the two-particle Szilard engine to the multi-particle
case. Initially, an ideal @xmath -particle gas is in a global
equilibrium of the entire box. We insert a partition in the middle of
the box. Then, we measure the number @xmath of the particles in the
right box. Then, we isothermally and quasi-statically shift the
partition to the position that divides the entire box according to the
ratio @xmath . Finally, we remove the partition. The probability of the
outcome @xmath being found is

  -- -------- -- --------
     @xmath      (5.69)
  -- -------- -- --------

where @xmath represents the number of combination that we choose @xmath
objects from @xmath objects. Therefore, the information obtained by the
measurement is

  -- -------- -- --------
     @xmath      (5.70)
  -- -------- -- --------

The work during the process can be calculated as

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (5.71)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Since the system returns to the initial state, we have

  -- -------- -- --------
     @xmath      (5.72)
  -- -------- -- --------

We calculate the singular probabilities by considering the time-reversed
process, and obtain

  -- -- -- --------
           (5.73)
  -- -- -- --------

and

  -- -------- -- --------
     @xmath      (5.74)
  -- -------- -- --------

The left-hand side of Eq. ( 5.39 ) is

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (5.75)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

On the other hand, the averaged singular probability is

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (5.76)
              @xmath   @xmath      (5.77)
              @xmath   @xmath      (5.78)
  -- -------- -------- -------- -- --------

Therefore, Eq. ( 5.39 ) is verified.

By a simple calculation, we obtain

  -- -------- -- --------
     @xmath      (5.79)
  -- -------- -- --------

Hence, Eqs. ( 5.41 ) and ( 5.42 ) are automatically satisfied. Moreover,
the equality in Eq. ( 5.42 ) is achieved, because the measurement is
error-free and the process is quasi-static.

##### Large-@xmath limit

We consider the large- @xmath limit of the @xmath -particle Szilard
engine. The fluctuation of the particle number @xmath in the right box
after the partition is inserted is of the order of @xmath . Therefore,
the dominant contributions to the average of physical quantities come
from events with

  -- -------- -- --------
     @xmath      (5.80)
  -- -------- -- --------

We consider approximate formulae in this range of @xmath .

The logarithm of the probability @xmath is calculated as

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (5.81)
  -- -------- -------- -------- -- --------

By using the Stirling formula

  -- -------- -- --------
     @xmath      (5.82)
  -- -------- -- --------

we obtain

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (5.83)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Therefore, we obtain

  -- -------- -- --------
     @xmath      (5.84)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (5.85)
  -- -------- -- --------

In a similar manner, the unavailable information is calculated as

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (5.86)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

The average of the mutual information is evaluated as

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (5.87)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Therefore, the conventional second law of information thermodynamics

  -- -------- -- --------
     @xmath      (5.88)
  -- -------- -- --------

reveals that the effect of feedback control is sub-extensive and of the
order of @xmath .

On the other hand, the average of the unavailable information is
obtained as

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (5.89)
  -- -------- -------- -------- -- --------

Therefore, the amount of the available information is

  -- -------- -- --------
     @xmath      (5.90)
  -- -------- -- --------

Thus, our inequality

  -- -------- -- --------
     @xmath      (5.91)
  -- -------- -- --------

limits the effect of feedback control to the order of unity, which is a
qualitatively new restriction compared with that of the conventional
second law of information thermodynamics. The same behavior was derived
by another method [ 65 ] based on the Kawai-Parrondo-Van den Broeck
equality [ 66 , 67 ] .

## Chapter 6 Gibbs’ Paradox Viewed from Absolute Irreversibility

In this chapter, we apply our nonequilibrium equalities in the presence
of absolute irreversibility to the problem of gas mixing. This problem
is what Gibbs’ paradox concerns [ 29 ] . Gibbs’ paradox is qualitatively
resolved once we realize that there are many entropies [ 30 , 31 , 32 ]
. The most prevalent quantitative resolution based on quantum mechanics
is in fact irrelevant to Gibbs’ paradox [ 31 , 32 ] . Another well
recognized resolution is based on the extensivity of the thermodynamic
entropy [ 33 , 32 ] . However, this resolution is valid only in the
thermodynamic limit, and cannot deal with any sub-leading effects. We
propose a new quantitative resolution of Gibbs’ paradox based on our
nonequilibrium equalities.

First of all, we review the original Gibbs’ paradox and later
qualitative discussions. Then, we present two widespread quantitative
resolutions. Finally, we resolve Gibbs’ paradox from the viewpoint of
absolute irreversibility.

### 6.1 History and Resolutions

In this section, we review a brief history of Gibbs’ paradox and discuss
its resolutions. We first review the original discussion and
interpretation given by Gibbs. Next, we review works by later
researchers that clarify Gibbs’ interpretation and argue that Gibbs’
paradox is qualitatively resolved even in classical theory. Then, we
discuss the quantum resolution of Gibbs’ paradox, which is the standard
resolution of Gibbs’ paradox. Finally, we examine yet another resolution
given by Pauli.

#### 6.1.1 Original Gibbs’ paradox

First of all, we review Gibbs’ original discussion which appeared in his
writing titled “On the Equilibrium of Heterogeneous Substances” [ 29 ] .
We consider mixing of two different gases (see Fig. 6.1 (a)). The gases
are confined in a box partitioned into equal halves. Initially, an ideal
@xmath -particle gas of one kind is confined in the left side with
temperature @xmath , and an ideal @xmath -particle gas of the other kind
is in the right side with the same temperature. Then, we remove the
partition and the two gases expand to the entire box. Let @xmath denote
the thermodynamic entropy of an ideal gas with temperature @xmath ,
volume @xmath and the number of particles @xmath . The entropy
production of this mixing process can be calculated by the Clausius
definition of the thermodynamic entropy. The entropy production of one
gas is given by

  -- -------- -- -------
     @xmath      (6.1)
  -- -------- -- -------

where @xmath denotes the heat transferred from the system to a heat
bath, and the integral is conducted along an arbitrary virtual
quasi-static process that connects the state @xmath to @xmath . We set
the virtual process to the quasi-static isothermal process. Due to the
first law of thermodynamics and the fact that the internal energy of an
ideal gas does not change in an isothermal process, we have

  -- -------- -- -------
     @xmath      (6.2)
  -- -------- -- -------

where @xmath is the pressure of the gas. Therefore, we obtain

  -- -------- -- -------
     @xmath      (6.3)
  -- -------- -- -------

Using the equation of state @xmath , we obtain

  -- -------- -- -------
     @xmath      (6.4)
  -- -------- -- -------

Due to the additivity of the thermodynamic entropy, the entropy
production of two different gases is the sum of their individual entropy
productions. Therefore, the entropy production of the mixing of two
different gases is given by

  -- -------- -- -------
     @xmath      (6.5)
  -- -------- -- -------

After Gibbs derived Eq. ( 6.5 ), he argued [ 29 ]

  “It is noticeable that the value of this expression does not depend
  upon the kinds of gas which are concerned, if the quantities are such
  as has been supposed, except that the gases which are mixed must be of
  different kinds. If we should bring into contact two masses of the
  same kind of gas, they would also mix, but there would be no increase
  of entropy.”

In fact, when we consider mixing of two identical gases (see Fig. 6.1
(b)), the initial entropy of the system is given by

  -- -------- -- -------
     @xmath      (6.6)
  -- -------- -- -------

due to the additivity of the thermodynamic entropy. On the other hand,
the final entropy is

  -- -------- -- -------
     @xmath      (6.7)
  -- -------- -- -------

which is equal to the initial entropy ( 6.6 ) due to the extensivity of
the thermodynamic entropy:

  -- -------- -- -------
     @xmath      (6.8)
  -- -------- -- -------

where @xmath is an arbitrary positive real number. Therefore, the
entropy production of the mixing of two identical gases is

  -- -------- -- -------
     @xmath      (6.9)
  -- -------- -- -------

which is different from the entropy production of the mixing of two
different gases ( 6.5 ), although Eq. ( 6.5 ) is “independent of the
degrees of similarity or dissimilarity between them.” In particular, if
we consider

  “the case of two gases which should be absolutely identical in all the
  properties (sensible and molecular) which come into play while they
  exist as gases whether pure or mixed with each other, but which should
  differ in respect to the attractions between their atoms and the atoms
  of some other substances,”

the thermodynamic entropy increases in the mixing process of two
different gases, although

  “the process of mixture, dynamically considered, might be absolutely
  identical in its minutest details (even with respect to the precise
  path of each atom) with processes which might take place without any
  increase of entropy,”

i.e., with processes of the mixing of identical gases. This paradoxical
consequence of the gas mixing is referred to as Gibbs’ paradox.

To explain this fact, Gibbs stressed,

  “if we ask what changes in external bodies are necessary to bring the
  system to its original state, we do not mean a state which shall be
  undistinguishable from the previous one in its sensible properties. It
  is to states of systems thus incompletely defined that the problems of
  thermodynamic relate.”

In other words, the thermodynamic entropy is not a property of a
microstate, but a property of a set of microstates indistinguishable
from each other, i.e., a property of a macrostate. In this way, “entropy
stands strongly contrasted with energy.” Therefore,

  “the mixture of gas-masses of the same kind stands on a different
  footing from the mixture of gas-masses of different kinds,”

This explanation given by Gibbs was clarified by later researchers as we
see in the next section.

#### 6.1.2 Gibbs’ paradox is not a paradox

In this section, we review discussions by three researchers, and argue
that Gibbs’ paradox is not a paradox even within classical statistical
mechanics.

##### Grad [30]

Grad discussed Gibbs’ paradox in the introduction of his article titled
“The Many Faces of Entropy” [ 30 ] . In the introduction, he emphasized,

  “A given object of study cannot always be assigned a unique value, its
  ‘entropy.’ It may have many different entropies, each one worthwhile.”

and argued,

  “much of the confusion in the subject is traceable to the ostensibly
  unifying belief (possibly theological in origin!) that there is only
  one entropy.”

In fact, Gibbs’ paradox is resolved once we renounce this belief. Grad
continued,

  “Whether or not diffusion occurs when a barrier is removed depends not
  on a difference in physical properties of the two substances but on a
  decision that we are or are not interested in such a difference (which
  is what governs the choice of an entropy function). There is no
  paradox to any observer. When he is aware of a difference in
  properties, he observes diffusion together with an increase in
  entropy. When he is unaware of any difference, he observes no
  diffusion and no increase in the entropy which he is using. If two
  observers disagree, they must be interested in different phenomena,
  and there is no conflict.”

To clarify his idea, Grad raised different situations that describe
states of an @xmath -particle gas. In the first situation, we label
particles in the gas by @xmath . In the second situation, we count the
particle number of the gas in a certain region of space. These two
situations are different and give rise to two different non-comparable
entropies. In the first situation, when we remove the partition and mix
the gas, the entropy @xmath increases by @xmath . Then, when we reinsert
the partition, the @xmath decreases by @xmath and returns to its
original value. This is because we have complete information whether
each particle is in the left or right side since we label all the
particles. In the second situation, when we remove the partition, the
entropy @xmath increases by @xmath , if we distinguish the particles
originally in the two different sides when we count the number of
particles. On the other hand, @xmath does not increase, if we are not
interested in any difference of the particles. In both cases, when we
reinsert the partition, the entropy does not change. In this way, the
entropies in these descriptions differ from each other. Although @xmath
is completely impractical from the viewpoint of thermodynamics, it does
exist and give one description of the system.

In summary, the notion of entropy depends on our interest or description
of a system. If entropies differ from each other, it just implies a
difference of our interest, and there is no conflict. As to Gibbs’
paradox, the difference of the entropy production reflects our situation
of whether or not we are interested in a difference of the particles.

##### van Kampen [31]

In his essay, van Kampen discussed Gibbs’ paradox. The aim of his essay
is to refute such statements as

  “It is not possible to understand classically why we must divide
  @xmath to obtain the correct counting of states,” and “Classical
  statistics thus leads to a contradiction with experience even in the
  range in which quantum effects in the proper sense can be completely
  neglected.”

First, he calculated the entropy @xmath of an ideal @xmath -particle gas
with pressure @xmath and temperature @xmath , and obtained

  -- -------- -- --------
     @xmath      (6.10)
  -- -------- -- --------

Then, he emphasized,

  “the second law defines only entropy differences between states that
  can be connected by reversible change.”

Therefore, the constant @xmath is only required to be independent of
@xmath and @xmath . Thus, at this point, there is no way to compare
entropy with different @xmath , unless we introduce a new reversible
process that varies @xmath . We consider a process in which we attach
two boxes containing identical gases with the same @xmath , and open a
channel between them. Then, the constant @xmath should be proportional
to @xmath and we obtain

  -- -------- -- --------
     @xmath      (6.11)
  -- -------- -- --------

where @xmath does not depend on @xmath . If two boxes contain two
different gases A, B, the process in which the channel is opened is no
longer reversible. Instead, we need to consider a process with
semi-permeable walls. We combine two boxes using the semi-permeable
walls in a reversible way, and then isothermally and quasi-statistically
expand the box so that the pressure returns to its original value.
Together with the convention of the additivity of the thermodynamic
entropy, we obtain

  -- -------- -- --------
     @xmath      (6.12)
  -- -------- -- --------

Even when we set A @xmath B, Eq. ( 6.12 ) does not reduce to Eq. ( 6.11
). This difference is what Gibbs’ paradox concerns. Then, van Kampen
argued,

  “The origin of the difference is that two different process had to be
  chosen for extending the definition of entropy. They are mutually
  exclusive: the first one cannot be used for two different gases and
  the second one does not apply to a single one.”

Next, he considered the case in which A and B are so similar that an
experimenter cannot distinguish them operationally, namely, he does not
have the semi-permeable walls needed in the second process. Then, he
will conclude

  -- -------- -- --------
     @xmath      (6.13)
  -- -------- -- --------

This seems to be contradictory at a first glance, but

  “The point is, that this is perfectly justified and that he will not
  be led to any wrong results. If you tell him that ‘actually’ the
  entropy increased when he opened the channel he will answer that this
  is a useless statement since he cannot utilize the entropy increase
  for running a machine. The entropy increase is no more physical to him
  than the one that could be manufactured by taking a single gas and
  mentally tagging the molecules A or B. […] The expression for the
  entropy (which he constructs by one or the other processes mentioned
  above) depends on whether or not he is able and willing to distinguish
  between the molecules A and B. This is a paradox only for those who
  attach more physical reality to the entropy than is implied by its
  definition.”

Van Kampen concluded

  “The question is not whether they are identical in the eye of God, but
  merely in the eye of the beholder.”

##### Jaynes [32]

Jaynes argued that the writing of Gibbs [ 29 ] contains a correct
analysis of Gibbs’ paradox. However, this analysis has been lost due to
ambiguity in the writing and the fact that Gibbs did not include it in
his later renowned textbook [ 68 ] . Jaynes presented a “half direct
quotation” of Gibbs’ explanation [ 32 ] .

First of all, Jaynes stressed that we have to be circumspect about what
we mean by the words “state” and “reversible.” He wrote

  “But by the word ‘original state’ we do not mean that every molecule
  has been returned to its original position, but only a state which is
  indistinguishable from the original one in the macroscopic properties
  that we are observing.”

Therefore, in the mixing of two different gases, the particles
originally in the left (right) side must return to the left (right) when
we say that the state returns to the original state. In contrast, in the
mixing of two identical gases, we do not mean that the particles
originally in the left (right) side should return to the left (right)
when we say that the mixing is reversible and accompanied by no entropy
production. We say that the mixing is reversible because all macroscopic
properties (e.g. the chemical composition, the number of particles)
return to their original value after we reinsert the partition. Thus,

  “Trying to interpret the phenomenon as a discontinuous change in the
  physical nature of the gases (i.e. in the behavior of their
  microstates) when they become exactly the same, misses the point. […]
  We might put it thus: when the gases become exactly the same, the
  discontinuity is in what you and I mean by the words ‘restore’ and
  ‘reversible’.”

To clarify this point, Jaynes continued his discussion. He noted that a
thermodynamic state is defined by specifying a small number of
macroscopic quantities @xmath . The entropy is defined as a property of
a macrostate specified by these quantities: @xmath . As indicated by the
discussion of the gas mixing, the entropy is not a property of the
microstate, whereas other thermodynamic variables as the total mass and
the total energy are physically real properties of a microstate. In
fact, the thermodynamic entropy is a property of a macrostate @xmath ,
namely, a set of microstates compatible with @xmath . Thus, it is
possible to assign different entropies @xmath to the same microstate, if
we choose different sets of macroscopic variables and embed the
microstate in two different macrostates @xmath . This implies that we
have to specify macroscopic variables that we can measure and control in
advance to define the thermodynamic entropy. Then, the thermodynamic
entropy obeys the second law of thermodynamics as long as all
experimental manipulations are within the set of macroscopic variables
that we have chosen beforehand. Because this choice connotes whether we
regard the gases as different or identical, the behavior of the entropy
under the gas mixing hinges upon this choice.

##### Summary

As we have seen, the thermodynamic entropy is not an intrinsic property
of a microstate, and the definition of the thermodynamic entropy
involves arbitrariness. Whether the gases are identical or different is
predetermined within our thermodynamic framework that we have chosen
beforehand. Thus, it is meaningless to discuss the discontinuity in the
thermodynamic entropy when we consider the infinitely similar gases. In
this sense, Gibbs’ paradox is not a paradox.
Now that we qualitatively understand that Gibbs’ paradox is not a
paradox even within classical thermodynamics, the remaining question is
how to derive the factorial in the statistical definition of the
thermodynamic entropy, namely, the factor @xmath in the definition of
the entropy

  -- -------- -- --------
     @xmath      (6.14)
  -- -------- -- --------

where @xmath denotes the number of microstates, or in the definition of
the partition function

  -- -------- -- --------
     @xmath      (6.15)
  -- -------- -- --------

where @xmath is the Hamiltonian of the system. In the following two
sections, we review two well-known quantitative resolutions of Gibbs’
paradox.

#### 6.1.3 Quantum resolution

The standard resolution of Gibbs’ paradox is based on quantum mechanics.
In quantum theory, the interchange of identical particles does not lead
to another state, and identical particles are indistinguishable in
principle. This indistinguishability naturally leads to the factor
@xmath . Many textbooks (for example, see Refs. [ 69 , 70 , 71 , 72 , 73
] ) resolve Gibbs’ paradox in this way.

Following Ref. [ 69 ] , we consider an @xmath -particle quantum system
with a Hamiltonian

  -- -------- -- --------
     @xmath      (6.16)
  -- -------- -- --------

where @xmath is the mass of the particles. Let @xmath and @xmath denote
the @xmath -th eigenstate and eigenenergy, respectively. Then, the
(unnormalized) density matrix of the canonical ensemble is

  -- -------- -- --------
     @xmath      (6.17)
  -- -------- -- --------

The configurational representation reads

  -- -------- -- --------
     @xmath      (6.18)
  -- -------- -- --------

where @xmath . When the particles are bosons, this density matrix must
be symmetrized as

  -- -------- -- --------
     @xmath      (6.19)
  -- -------- -- --------

where @xmath is the symmetry group of degree @xmath . The partition
function is given by

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.20)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

The integrand in this equation involves factors

  -- -------- -- --------
     @xmath      (6.21)
  -- -------- -- --------

In the classical (i.e., high-temperature) limit, due to this exponential
decay, the dominant contribution in Eq. ( 6.20 ) is the term in which
@xmath is the identity permutation. Therefore, Eq. ( 6.20 ) is
approximated as

  -- -------- -- --------
     @xmath      (6.22)
  -- -------- -- --------

In the case of fermions, a similar discussion leads to the same
conclusion. In this way, the factor @xmath can naturally be derived from
quantum mechanics and this factor leads to the extensive thermodynamic
entropy.

Although this resolution is the standard resolution of Gibbs’ paradox,
it involves two crucial problems. First, this resolution cannot apply to
mesoscopic particles. Let us consider colloidal particles in liquid.
Because colloidal particles have vast internal degrees of freedom, we
cannot expect that these particles have the same internal states.
Therefore, the wave function of this system should not be symmetrized.
Hence, we cannot derive the factor @xmath , and thermodynamic quantities
(e.g. entropy) fail to be extensive. This implies that the quantum
resolution is inapplicable to a mesoscopic regime. The second point is
more fundamental and to be explained at the end of the next section,
because this point is closely linked to the topic described in the next
section.

#### 6.1.4 Pauli’s resolution based on the extensivity

As we see in the review of van Kampen’s work [ 31 ] , the Clausius
definition of thermodynamic entropy reveals nothing about the dependence
of the entropy on the particle number. Pauli recognized this fact and
gave a resolution within classical theory [ 33 , 32 ] . Pauli’s analysis
is based on the extensivity of the thermodynamic entropy.

Let @xmath be a phenomenological thermodynamic entropy of an ideal
@xmath -particle gas with temperature @xmath and volume @xmath defined
by the Clausius definition ( 6.1 ). Then, from the definition, we obtain

  -- -------- -- --------
     @xmath      (6.23)
  -- -------- -- --------

where @xmath is not an arbitrary constant, but an arbitrary function of
@xmath . This is because the Clausius definition does not involve the
@xmath -dependence of the thermodynamic entropy. To determine @xmath ,
we require the extensivity of the entropy as an additional condition.
The extensivity means

  -- -------- -- --------
     @xmath      (6.24)
  -- -------- -- --------

where @xmath is an arbitrary positive real number. Substituting Eq. (
6.23 ) into Eq. ( 6.24 ), we obtain

  -- -------- -- --------
     @xmath      (6.25)
  -- -------- -- --------

Differentiating with respect to @xmath and setting @xmath , we obtain

  -- -------- -- --------
     @xmath      (6.26)
  -- -------- -- --------

This equation can be rewritten as

  -- -------- -- --------
     @xmath      (6.27)
  -- -------- -- --------

Therefore, we obtain

  -- -------- -- --------
     @xmath      (6.28)
  -- -------- -- --------

where the second term is the factor @xmath due to the Stirling formula
in the large- @xmath limit, i.e., in the thermodynamic limit. Thus, the
thermodynamic entropy is

  -- -------- -- --------
     @xmath      (6.29)
  -- -------- -- --------

By this form, we see that the entropy is extensive and @xmath is
essentially the chemical potential. In this way, the requirement of the
extensivity leads to the factor @xmath in the phenomenological entropy
defined by the Clausius in the thermodynamic limit.

The same argument applies to the entropy defined by classical
statistical mechanics. The reason why we identify the entropy defined by
classical statistical mechanics as the thermodynamic entropy is that
these two entropies have the same response to any variation of such
macroscopic variables as the temperature and volume. In mathematical
terms, these two entropies have the identical differential form.
Therefore, what we can conclude about relations between the
thermodynamic entropy @xmath and the entropy in classical statistical
mechanics @xmath is

  -- -------- -- --------
     @xmath      (6.30)
  -- -------- -- --------

For an @xmath -particle ideal gas, we have

  -- -------- -- --------
     @xmath      (6.31)
  -- -------- -- --------

where @xmath is a constant of the dimension of action. Then, the
requirement of the extensivity ( 6.24 ) leads to

  -- -------- -- --------
     @xmath      (6.32)
  -- -------- -- --------

The extensivity reproduces the second term on the right-hand side, i.e.,
the factor @xmath again.

Quantum statistical mechanics is in the the same position as classical
statistical mechanics. We have

  -- -------- -- --------
     @xmath      (6.33)
  -- -------- -- --------

In the classical limit, since we have

  -- -------- -- --------
     @xmath      (6.34)
  -- -------- -- --------

the requirement of the extensivity leads to

  -- -------- -- --------
     @xmath      (6.35)
  -- -------- -- --------

where the second term on the right-hand side vanishes in the
thermodynamic limit. Hence, @xmath has only a trivial dependence on
@xmath as

  -- -------- -- --------
     @xmath      (6.36)
  -- -------- -- --------

which corresponds to the contribution from the chemical potential. In
this manner, the procedure to determine an arbitrary function of @xmath
is needed for quantum statistical mechanics as well, although the result
is simpler than that of classical statistical mechanics. Therefore, [ 31
]

  “the Gibbs paradox is no different in quantum mechanics, it is only
  less manifest.”

As we have seen, the quantum resolution is irrelevant to Gibbs’ paradox.
The resolution based on the extensivity is a better and logical
resolution, and applicable to the phenomenological entropy, the entropy
in classical statistical mechanics and the entropy in quantum
statistical mechanics. However, this resolution still suffers a problem.
The resolution is only applicable to systems in the thermodynamic limit
in which we are entitled to require the extensivity of the entropy.
Namely, it ignores deviations from the extensivity, which are essential
to deal with mesoscopic physics and surface effects.

### 6.2 Resolution from absolute irreversibility

In this section, we resolve Gibbs’ paradox based on the nonequilibrium
equalities with absolute irreversibility. We explain why the entropy
productions of the two mixing processes are different from each other,
and then derive the factor @xmath . Our resolution is valid even for
non-extensive entropy, for which the resolution given by Pauli breaks
down.

#### 6.2.1 Requirement and Results

In the resolution by Pauli, we require the extensivity of the
thermodynamic entropy. Instead of this requirement, we require the
additivity of the thermodynamic entropy. In mesoscopic systems, the
extensivity breaks down, whereas the additivity remains valid as long as
the interaction is short-range. The additivity plays a crucial role when
we compare the thermodynamic entropy with different @xmath .

Under the requirement of the additivity, in the context of our
nonequilibrium equality, we show that the entropy production of the
mixing of two identical @xmath -particle gases is

  -- -------- -- --------
     @xmath      (6.37)
  -- -------- -- --------

and that the entropy production of the mixing of two different @xmath
-particle gases is

  -- -------- -- --------
     @xmath      (6.38)
  -- -------- -- --------

in the thermodynamic limit. This difference of the two processes will be
explained in terms of absolute irreversibility. Moreover, we will derive
the factor @xmath , that is, we show that the arbitrary function @xmath
in classical statistical mechanics should take the following form:

  -- -------- -- --------
     @xmath      (6.39)
  -- -------- -- --------

This result is valid for a finite @xmath without the thermodynamic
limit.

#### 6.2.2 Difference of the two processes

We consider a difference between the mixing of two identical gases and
that of two different gases in terms of absolute irreversibility.

First, we consider the reverse process of the mixing of two identical
gases illustrated in Fig. 6.2 (a) to evaluate the absolute
irreversibility. Initially, an ideal @xmath -particle gas is in the
equilibrium of the entire box. Then, we insert the partition in the
middle. Let @xmath denote the number of the particles found in the left
side after the insertion. The event of @xmath is the only event that has
the corresponding event in the original process (see also Fig. 6.1 (b)).
Therefore, the events of @xmath are singular because they have no
counterparts. Hence, the singular probability is calculated as

  -- -------- -- --------
     @xmath      (6.40)
  -- -------- -- --------

Secondly, we consider the reverse process of the mixing of two different
gases illustrated in Fig. 6.2 (b). Initially, two ideal @xmath -particle
gases of different kinds are at thermal equilibrium in the entire box.
Then, we insert the partition in the middle. To recover the original
state, the particle number in the left side after the insertion must be
@xmath . Moreover, the chemical composition must return to the original
state. Namely, the particles from the left (right) side in the original
process must return to the left (right) side. This fact makes sharp
contrast to the case of two identical gases, in which the particles from
the left (right) may go to the right (left) sides as long as the
particle number returns to @xmath . The only non-singular event is the
one in which all the particles of one kind return to the left side and
the rest particles return to the right side. All the other events
indicated by blue arrows are singular. Thus, the singular probability is

  -- -------- -- --------
     @xmath      (6.41)
  -- -------- -- --------

In this way, the difference of the intuitive physical descriptions in
the reversed processes is quantitatively characterized by the difference
of the probabilities of the absolutely irreversible paths.

Next, we connect these singular probabilities to the thermodynamic
entropy production. The Jarzynski-type nonequilibrium equality ( 4.33 )
in the presence of absolute irreversibility reads

  -- -------- -- --------
     @xmath      (6.42)
  -- -------- -- --------

Since work is zero ( @xmath ) in the mixing process, we obtain

  -- -------- -- --------
     @xmath      (6.43)
  -- -------- -- --------

Thus, in terms of the thermodynamic entropy, we obtain

  -- -------- -- --------
     @xmath      (6.44)
  -- -------- -- --------

Substituting Eq. ( 6.40 ) into Eq. ( 6.44 ), we obtain

  -- -------- -- --------
     @xmath      (6.45)
  -- -------- -- --------

When @xmath is sufficiently large, the combination in Eq. ( 6.45 ) is
approximated as

  -- -------- -- --------
     @xmath      (6.46)
  -- -------- -- --------

Therefore, Eq. ( 6.45 ) reduces to

  -- -------- -- --------
     @xmath      (6.47)
  -- -------- -- --------

Since this value is sub-extensive, we have

  -- -------- -- --------
     @xmath      (6.48)
  -- -------- -- --------

in the thermodynamic limit. This is consistent with the fact that the
removal of the partition becomes reversible in the large- @xmath limit
due to the law of large numbers, namely, the reinsertion of the
partition results in the state of @xmath with almost unit probability.
In the case of two different gases, from Eqs. ( 6.41 ) and ( 6.44 ), we
obtain

  -- -------- -- --------
     @xmath      (6.49)
  -- -------- -- --------

The difference of these entropy productions of the two processes
originates from the difference of the degree of absolute
irreversibility, namely, the difference in the behaviors under the
reinsertion of the partition.

#### 6.2.3 Derivation of the factor @xmath

To determine the arbitrary function @xmath in Eq. ( 6.30 ), we consider
a mixing process of an @xmath -particle gas and an @xmath -particle gas
of the same kind illustrated in Fig. 6.3 (a). To evaluate the
probability of absolute irreversibility, we consider the reverse process
(see Fig. 6.3 (b)). We insert the partition in the middle in an @xmath
-particle gas. The number of particles in the left side, @xmath , after
the reinsertion must be @xmath to recover the original state. This is
the only non-singular event. Therefore, the singular probability is

  -- -------- -- --------
     @xmath      (6.50)
  -- -------- -- --------

Thus, from Eq. ( 6.44 ), the thermodynamic entropy production is

  -- -------- -- --------
     @xmath      (6.51)
  -- -------- -- --------

On the other hand, from Eq. ( 6.31 ) and the additivity of the
thermodynamic entropy, we obtain

  -- -------- -- --------
     @xmath      (6.52)
  -- -------- -- --------

Therefore, we obtain

  -- -------- -- --------
     @xmath      (6.53)
  -- -------- -- --------

When set @xmath , we have

  -- -------- -- --------
     @xmath      (6.54)
  -- -------- -- --------

Let us define

  -- -------- -- --------
     @xmath      (6.55)
  -- -------- -- --------

and then Eq. ( 6.54 ) reduces to

  -- -------- -- --------
     @xmath      (6.56)
  -- -------- -- --------

This equation can be rewritten as

  -- -------- -- --------
     @xmath      (6.57)
  -- -------- -- --------

Therefore, we obtain

  -- -------- -- --------
     @xmath      (6.58)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (6.59)
  -- -------- -- --------

Hence, we conclude

  -- -------- -- --------
     @xmath      (6.60)
  -- -------- -- --------

Thus, the desired factor @xmath is reproduced.
In summary, based on our nonequilibrium equality that is applicable in
the presence of absolute irreversibility, we have shown that the
difference of the entropy productions in the two mixing processes
originates from the difference of the degree of absolute
irreversibility. Furthermore, we have reproduced the factor @xmath in
the relation between the thermodynamic entropy and the classical
statistical mechanical entropy. Our new resolution automatically takes
account of the sub-leading term and mesoscopic effects in the
thermodynamic entropy, which was ignored in the resolution based on the
extensivity of the thermodynamic entropy.

## Chapter 7 Conclusions and Future Prospects

### 7.1 Conclusions

In this thesis, we have investigated the situations to which the
conventional integral nonequilibrium equalities cannot apply, and
proposed a new concept of absolute irreversibility to describe these
situations in a unified manner. In absolutely irreversible processes,
some of time-reversed paths have no counterpart in the original forward
process, and the entropy production diverges in the context of the
detailed fluctuation theorems. In mathematical terms, absolute
irreversibility is defined as the singular part of the time-reversed
probability measure with respect to the forward probability measure.
Lebesgue’s decomposition enables us to separate the absolutely
irreversible part from the ordinarily irreversible part. As a
consequence, we have obtained the integral nonequilibrium equalities in
the presence of absolute irreversibility. The obtained equalities
involve two physical quantities related to irreversibility: the entropy
production representing ordinary irreversibility and the singular
probability describing absolute irreversibility. The corresponding
inequalities give tighter fundamental restrictions on the entropy
production in nonequilibrium processes than the conventional second-law
like inequalities. Our nonequilibrium equalities have been verified in
free expansion and in numerical simulations of the two Langevin systems.

Moreover, we have generalized our nonequilibrium equalities in
absolutely irreversible processes to the situations in which the system
is subject to measurement-based feedback control. We have transformed
the obtained nonequilibrium equalities and introduced a concept of
unavailable information, which characterizes the inevitable inefficiency
of feedback protocols. As a result, we have derived inequalities that
give an achievable lower bound of the entropy production. We have
verified our information-thermodynamic absolutely irreversible
nonequilibrium equalities in the process with a measurement and trivial
feedback control and in the two- and multi-particle Szilard engines.

We have applied the notion of absolute irreversibility to the gas-mixing
problem of Gibbs’ paradox. The difference between the entropy production
of the mixing process of two different gases and that of two identical
gases originates from the difference of absolute irreversibility, i.e.,
different behaviors under the reinsertion of the partition. Moreover, we
have reproduced the factorial in the particle-number dependence of the
thermodynamic entropy. Our quantitative resolution of Gibbs’ paradox
applies to a classical mesoscopic regime, where Pauli’s resolution based
on the extensivity of the thermodynamic entropy breaks down.

### 7.2 Future prospects

As future prospects, I enumerate several outstanding issues.

First of all, I intend to generalize our absolutely irreversible
integral nonequilibrium equalities to the quantum regime. A part of this
extension has already been done, and we have shown that absolute
irreversibility is essential under inefficient feedback control and
projective measurements [ 74 ] . However, nonequilibrium equalities
under the condition that the initial state has such quantum correlations
as entanglement are elusive. Absolute irreversibility may play an
important role in these quantum nonequilibrium situations.

Next, I intend to apply our formulation based on measure theory to the
chaotic systems. Although the nonequilibrium equalities were first
proven in chaotic systems, most of recent researches of nonequilibrium
equalities are restricted to such simple systems as the Langevin
systems. I expect our formulation is compatible with the chaotic system
that has a singular continuous probability measure with respect to the
Lebesgue measure. This issue may be related to a topic of thermalization
because the relaxed state after a nonequilibrium process starting from
the canonical distribution sometimes exhibits singular behaviors.

Finally, I contemplate applying our nonequilibrium equality to
finite-time thermodynamics. Finite-time thermodynamics is a field of
thermodynamics that puts emphasis on power of thermodynamic engines and
seeks to their optimal efficiency. Therefore, thermodynamic engines
under consideration are subject to finite-time nonequilibrium processes.
Recently, in the context of nonequilibrium equalities, the efficiency of
finite-time engines has been studied [ 75 ] . I expect that our
nonequilibrium equalities with absolute irreversibility give new
restrictions on the efficiency.

## Appendix A From the Langevin Dynamics to Other Formulations

In this Appendix, we derive the path integral formula ( 2.39 ) and the
Fokker-Planck equation ( 2.41 ) from the overdamped Langevin equation

  -- -------- -- -------
     @xmath      (A.1)
  -- -------- -- -------

where @xmath is a white Gaussian noise satisfying

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (A.2)
     @xmath   @xmath   @xmath      (A.3)
  -- -------- -------- -------- -- -------

### a.1 Path-integral formula

To calculate the path probability, we first discretize the time interval
@xmath into @xmath sections with the same length @xmath . We define
@xmath , @xmath , and @xmath . Then, the discretized Langevin equation
reads ¹ ¹ 1 We use the Stratonovich convention.

  -- -------- -- -------
     @xmath      (A.4)
  -- -------- -- -------

where

  -- -------- -- -------
     @xmath      (A.5)
  -- -------- -- -------

for @xmath . The discretized noise @xmath obeys the Gaussian
distribution with the average

  -- -------- -- -------
     @xmath      (A.6)
  -- -------- -- -------

and the variance

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (A.7)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

Therefore, the probability distribution of @xmath is given by

  -- -------- -- -------
     @xmath      (A.8)
  -- -------- -- -------

Thus, the joint probability distribution of all @xmath is calculated as

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

In the large- @xmath limit, we obtain

  -- -------- -- --------
     @xmath      (A.10)
  -- -------- -- --------

where @xmath is the normalization constant.

The path probability @xmath is defined in terms of the probability
distribution @xmath as

  -- -------- -- --------
     @xmath      (A.11)
  -- -------- -- --------

To obtain the path probability, we calculate the Jacobian as

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (A.12)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Thus, in the continuous limit ( @xmath ), we obtain

  -- -------- -- --------
     @xmath      (A.13)
  -- -------- -- --------

and therefore

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

which is nothing but the path integral formula for the overdamped
Langevin equation ( 2.39 ).

### a.2 Fokker-Planck equation

First of all, we evaluate the time evolution of an arbitrary function
@xmath as

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (A.15)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where

  -- -------- -- --------
     @xmath      (A.16)
  -- -------- -- --------

We take the statistical average of Eq. ( A.15 ). Substituting Eq. ( A.16
) into the average of the first term on the right-hand side of Eq. (
A.15 ), we obtain

  -- -------- -- --------
     @xmath      (A.17)
  -- -------- -- --------

Since the stochastic quantity @xmath at time @xmath is independent of
the noise @xmath for @xmath , we obtain

  -- -------- -- --------
     @xmath      (A.18)
  -- -------- -- --------

Therefore, Eq. ( A.17 ) reduces to

  -- -------- -- --------
     @xmath      (A.19)
  -- -------- -- --------

In a similar way, the average of the second term on the right-hand side
of Eq. ( A.15 ) reduces to

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (A.20)
                                @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Therefore, the average of Eq. ( A.15 ) is

  -- -------- -- --------
     @xmath      (A.21)
  -- -------- -- --------

To derive the Fokker-Planck equation, we note that the probability
@xmath to find the Langevin particle at position @xmath at time @xmath
is given by

  -- -------- -- --------
     @xmath      (A.22)
  -- -------- -- --------

If we set @xmath , Eq. ( A.21 ) reduces to

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (A.23)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Thus, we obtain the following Fokker-Planck equation

  -- -------- -- --------
     @xmath      (A.24)
  -- -------- -- --------

## Appendix B Measure Theory and Lebesgue’s Decomposition

In this Appendix, we briefly review measure theory and Lebesgue’s
decomposition theorem. This Appendix is mainly based on [ 26 ] .

### b.1 Preliminary subjects

###### Definition 1 (@xmath-algebra)

A family @xmath of subsets of @xmath is said to be a @xmath -algebra if
the following three conditions are met:

    (i) @xmath and @xmath belong to @xmath ;

    (ii) If @xmath belongs to @xmath , then @xmath belongs to @xmath ;

    (iii) If @xmath is a sequence of sets in @xmath , then @xmath
    belongs to @xmath .

###### Definition 2 (measurable space)

An ordered pair @xmath consisting of a set @xmath and a @xmath -algebra
@xmath of subsets of @xmath is called a measurable space .

###### Definition 3 (measure)

A measure is an extended real-valued function @xmath defined on a @xmath
-algebra @xmath of subsets of @xmath satisfying the following
conditions:

    (i) @xmath ;

    (ii) @xmath for all @xmath ;

    (iii) @xmath is countably additive , i.e., if @xmath is any disjoint
    sequence of sets in @xmath , then

      -- -------- -- -------
         @xmath      (B.1)
      -- -------- -- -------

###### Definition 4 (measure space)

A measure space is an ordered triad @xmath consisting of a set @xmath ,
a @xmath -algebra @xmath of subsets of @xmath and a measure @xmath
defined on @xmath .

###### Definition 5 ((@xmath-)finite measure)

Let @xmath be a measure space. If @xmath does not take on an infinite
value, we say that @xmath is finite . If there exists a sequence @xmath
of sets in @xmath with @xmath and @xmath for all @xmath , then we say
@xmath is @xmath -finite .

###### Definition 6 (mesurable function)

An @xmath -valued function @xmath with domain @xmath is said to be
@xmath -measurable if for any real number @xmath the set

  -- -------- -- -------
     @xmath      (B.2)
  -- -------- -- -------

belongs to @xmath .

###### Definition 7 (almost everywhere)

Let @xmath be a measure on @xmath . A certain proposition is said to
hold @xmath -almost everywhere on @xmath if there exists a subset @xmath
with @xmath such that the proposition holds on @xmath .

###### Theorem 1

Suppose that @xmath is a nonnegative @xmath -measurable function. Then,
@xmath @xmath -almost everywhere on @xmath iff

  -- -------- -- -------
     @xmath      (B.3)
  -- -------- -- -------

### b.2 Classification of measures

###### Definition 8 (absolutely continuous)

A measure @xmath on @xmath is said to be absolutely continuous with
respect to a measure @xmath on @xmath if @xmath and @xmath imply @xmath
. In this case, we write @xmath .

###### Lemma 1

Let @xmath and @xmath be finite measures on @xmath . Then @xmath iff for
every @xmath there exists a @xmath such that @xmath and @xmath imply
that @xmath .

###### Proof.

If this condition is satisfied and @xmath , then @xmath for all @xmath ,
which implies @xmath .

Conversely, suppose that there exist some @xmath and @xmath with @xmath
and @xmath . Let @xmath , so that @xmath and @xmath . Since @xmath is a
decreasing sequence of measurable sets and @xmath are finite measures,
we have

  -- -------- -- -------
     @xmath      (B.4)
  -- -------- -- -------

Therefore, @xmath is not absolutely continuous with respect to @xmath .
∎

Intuitively, @xmath means that a set that has a small @xmath -measure
also has a small @xmath -measure.

###### Definition 9 (singular)

Two measures @xmath and @xmath on @xmath are said to be mutually
singular if there are sets @xmath and @xmath that satisfy @xmath ,
@xmath and @xmath . In this case, we write @xmath .

Despite this symmetric definition, we also say that @xmath is singular
with respect to @xmath .

###### Lemma 2

Let @xmath be a measure such that @xmath and @xmath , then @xmath .

###### Proof.

Since @xmath , there exist sets @xmath and @xmath such that

  -- -------- -- -------
     @xmath      (B.5)
  -- -------- -- -------

Since @xmath and @xmath , @xmath Then, due to the additivity of @xmath ,
we have

  -- -------- -- -------
     @xmath      (B.6)
  -- -------- -- -------

It follows that for all @xmath

  -- -------- -- -------
     @xmath      (B.7)
  -- -------- -- -------

which implies @xmath . ∎

###### Definition 10 (discontinuous point)

For @xmath , if @xmath , then @xmath is said to be a discontinuous point
of @xmath .

###### Definition 11 (discrete measure)

Let @xmath denote the set of all the discontinuous points of @xmath .
Then, @xmath is said to be discrete measure if @xmath .

###### Definition 12 (continuous measure)

A measure @xmath is said to be a continuous measure if @xmath has no
discontinuous points.

### b.3 Radon-Nikodým theorem

###### Theorem 2 (Radon-Nikodým theorem)

Let @xmath and @xmath be @xmath -finite measures defined on @xmath and
suppose that @xmath is absolutely continuous with respect to @xmath .
Then, there exists a nonnegative @xmath -measurable function @xmath such
that

  -- -------- -- -------
     @xmath      (B.8)
  -- -------- -- -------

Moreover, the function @xmath is uniquely determined @xmath -almost
everywhere.

The function @xmath is referred to as the Radon-Nikodým derivative, and
formally written as

  -- -------- -- -------
     @xmath      (B.9)
  -- -------- -- -------

Moreover, @xmath is the transformation function from @xmath to @xmath .
In fact, for an arbitrary @xmath -measurable function @xmath , we have

  -- -------- -- --------
     @xmath      (B.10)
  -- -------- -- --------

### b.4 Lebesgue’s decomposition theorem

###### Theorem 3 (Lebesgue’s decomposition theorem 1)

Let @xmath and @xmath be @xmath -finite measures defined on a @xmath
-algebra @xmath . Then there exist measures @xmath and @xmath such that
@xmath , @xmath and @xmath . Moreover, the measures @xmath and @xmath
are unique.

###### Proof.

Let @xmath . Then, @xmath and @xmath are absolutely continuous with
respect to @xmath . Therefore, we can apply the Radon-Nikodým theorem to
obtain

  -- -------- -- --------
     @xmath      (B.11)
  -- -------- -- --------

for all @xmath , where @xmath , @xmath are nonnegative @xmath
-measurable functions. Let @xmath and @xmath so that @xmath and @xmath .

Define @xmath and @xmath for @xmath by

  -- -------- -- --------
     @xmath      (B.12)
  -- -------- -- --------

Since @xmath is additive, @xmath . To see @xmath , we note that if
@xmath , then

  -- -------- -- --------
     @xmath      (B.13)
  -- -------- -- --------

In accordance with Theorem 1 , @xmath for @xmath -almost all @xmath ,
which means @xmath . Since @xmath , @xmath , and then @xmath . Thus,
@xmath is absolutely continuous with respect to @xmath . On the other
hand, since @xmath , @xmath is singular with respect to @xmath .

The uniqueness of this decomposition can be established by Lemma 2 . ∎

###### Lemma 3

Let @xmath be a measure defined on a @xmath -algebra @xmath . Then there
exist measures @xmath and @xmath such that @xmath , where @xmath is
continuous and @xmath is discrete. Moreover, the measures @xmath and
@xmath are unique.

###### Proof.

Let @xmath be the set of all the discontinuous points of @xmath . Define
@xmath and @xmath for @xmath by

  -- -------- -- --------
     @xmath      (B.14)
  -- -------- -- --------

Then, we can show that @xmath is continuous and @xmath is discrete.

Suppose

  -- -------- -- --------
     @xmath      (B.15)
  -- -------- -- --------

where @xmath is continuous and @xmath is discrete. If @xmath , there
exists a single point @xmath such that @xmath . On the other hand, due
to the continuity, @xmath . These relations lead to

  -- -------- -- --------
     @xmath      (B.16)
  -- -------- -- --------

which contradicts Eq. ( B.15 ). Hence, @xmath and therefore the
uniqueness of the decomposition is established. ∎

To introduce a stronger version of Lebesgue’s decomposition, we prove
the following lemma.

###### Lemma 4

Let @xmath and @xmath be measures on a @xmath -algebra. If @xmath is
continuous and @xmath is discrete, then @xmath is singular with respect
to @xmath .

###### Proof.

Let @xmath denote the set of all the discontinuous points of @xmath .
Then, @xmath . On the other hand, since @xmath is continuous and @xmath
is countable, @xmath . Thus, @xmath and @xmath are mutually singular. ∎

###### Theorem 4 (Lebesgue’s decomposition theorem 2)

Let @xmath and @xmath be @xmath -finite measures defined on a @xmath
-algebra @xmath and suppose @xmath is continuous. Then there exist
measures @xmath , @xmath and @xmath such that @xmath , where @xmath ;
@xmath and @xmath is continuous; @xmath is discrete. Moreover, the
measures @xmath , @xmath and @xmath are unique.

###### Proof.

We can apply Theorem 3 to uniquely decompose

  -- -------- -- --------
     @xmath      (B.17)
  -- -------- -- --------

where @xmath is absolutely continuous with respect to @xmath , and
@xmath is singular with respect to @xmath . In accordance with Lemma 3 ,
we can uniquely decompose @xmath into two parts:

  -- -------- -- --------
     @xmath      (B.18)
  -- -------- -- --------

where @xmath is continuous and @xmath is discrete. Thus, we obtain the
following decomposition:

  -- -------- -- --------
     @xmath      (B.19)
  -- -------- -- --------

The uniqueness of this decomposition follows from Lemma 4 . ∎

The continuity of @xmath is needed to establish the uniqueness of the
decomposition. If @xmath has a discontinuous point @xmath , then the
measure @xmath that satisfies @xmath is absolutely continuous with
respect to @xmath and discrete at the same time.

## Acknowledgement

The studies in this thesis was done when the author was a master-course
student in Masahito Ueda group in the University of Tokyo. This thesis
would not be completed without help of a lot of collaborators and
colleagues.

First of all, I would like to express my best gratitude to my
supervisor, Prof. Masahito Ueda. He made me cognizant of the topic in
this thesis and gave me a lot of insightful comments throughout
discussions. He also read the manuscript with great attention and gave
me enormous suggestions. I would also thank him for the best research
environment that he arranged for me.

I would also like to thank my collaborator, Ken Funo, for fruitful
discussions on our studies and for teaching me quantum aspects of
nonequilibrium equalities.

I am thankful for my collaborator, Yuto Ashida, for his constructive
ideas from his deep comprehension of our field.

I am grateful to Prof. Takahiro Sagawa for his critical comments and
beneficial discussions on our work.

I appreciate critical comments and constructive suggestions by Prof.
Shin-ichi Sasa.

I acknowledge comments from anonymous referees of our article [ 27 ] ,
which I find very useful to clarify physical meanings of our work.

I would like to express my deep sense of gratitude to the members in
Masahito Ueda group, especially to Yui Kuramochi and Tomohiro Shitara
for fruitful discussions on mathematical aspects of our work, and to
Tatsuhiko N. Ikeda for suggestive comments.

I am also indebted to the members in Masaki Sano group in the University
of Tokyo, especially to Kyogo Kawaguchi for lecturing me on theoretical
aspects of mesoscopic physics and to Yohei Nakayama, Yuta Hirayama and
Daiki Nishiguchi for teaching me experimental techniques of this field.

Finally, I acknowledge financial support from the Japan Society for the
Promotion of Science (JSPS) through the Program for Leading Graduate
Schools (MERIT).