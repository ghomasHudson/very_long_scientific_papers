Accurate linguistic annotation is a core requirement of natural language
processing systems. The demand for accuracy in the face of rapid
prototyping constraints and numerous target languages has led to the
employment of machine learning methods for developing linguistic
annotation systems.

The popularity of applying machine learning methods to computational
linguistics problems has given rise to a large supply of trainable
natural language processing systems. Most problems of interest have an
array of off-the-shelf products or downloadable code implementing
solutions using various techniques. In situations where these solutions
are developed independently, it is observed that their errors tend to be
independently distributed. In this thesis we discuss approaches for
capitalizing on this situation in a sample problem domain, Penn
Treebank-style parsing.

The machine learning community provides us with techniques for combining
outputs of classifiers, but parser output is more structured and
interdependent than classifications. To overcome this, two novel
strategies for combining parsers are used: learning to control a switch
between parsers and constructing a hybrid parse from multiple parsers’
outputs. In this thesis we give supervised and unsupervised techniques
for each of these strategies as well as performance and robustness
results from evaluation of the techniques.

One shortcoming of combining off-the-shelf parsers is that the parsers
are not developed with the intention to perform well on complementary
data or to compensate for each others’ weaknesses. The individual
parsers are globally optimized. We present two techniques for producing
an ensemble of parsers in such a way that their outputs can be
constructively combined. All of the ensemble members will be created
using the same underlying parser induction algorithm, and the method for
producing complementary parsers is only loosely coupled to that
algorithm.

\qtreecenterfalse \doctorphilosophy \dissertation \copyrightnotice
\degreeyear

1999 \degreemonth August

\advisor

Eric Brill \readers Steven Salzberg
David Yarowsky

{dedication}

Dedicated to
good parents,
Kathleen and Daniel

{acknowledgement}

No one flourishes in isolation and I am another example to support the
claim. There are many people I should thank, and tracing back all of the
paths of inspiration, motivation, and support that facilitate a thesis
is impossible. Here is an overview of the major supporting members of
the cast:

My readers, Steven Salzberg and David Yarowsky, have given me very
useful suggestions and comments about my thesis research. They have each
contributed immensely to my education.

My friends at Hopkins have given me plenty of arguments and feedback
concerning this thesis and other things, and I especially thank them for
tolerating my discussions concerning ground beef superhighways and
antiseptic qualities of Diet Coke. I was always in good company at
Hopkins.

My advisor, Eric Brill, provided me with both the liberty to pursue
research that I found interesting, and the constraints I required to
complete my thesis research. He made me feel valuable to the Hopkins NLP
lab and the larger research community without treating me as just
another information worker. I could not have asked for a better advisor.

My parents gifted me with a love of learning by teaching me through play
and letting me disassemble most household items. All along, my entire
family prepared me to pursue research.

Katya, my beloved wife, has developed a capacity for patience that I
would have never believed possible. Her love and support have eased many
desperate moments in my graduate career. Without her encouragement, my
days would have been dim and my nights dismal.
