even though the algorithms are based on the idea that sentences and
words are decomposed, sentences and words are stored as flat character
sequences. The best representation for a given word or sentence can be
found by parsing it. Because of this, the representation for a word like
watermelon can go from wa @xmath term @xmath el @xmath on to water
@xmath melon in a single step that would stymie other algorithms.

Our algorithms are tested on problems of learning words and word
meanings from both unsegmented (spaceless) text and continuous speech.
The grammars they produce are evaluated in terms of their linguistic and
statistical properties. For example, after training on a large corpus of
unsegmented text, one algorithm produces hierarchical segmentations of
the input such as:

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

These segmentations are compared against word boundaries; the results
indicate that the algorithm produces structure that agrees extremely
well with humans’ grammars. On Chinese text, for example, 97% of word
boundaries are matched and fewer than 1.3% are violated. On statistical
grounds the algorithms also fare very well: used as compression
algorithms they equal or better almost all other methods. Chapter 6
presents these and other results, including dictionaries learned both
from text and from speech. In the most ambitious test of any theory of
language acquisition, we run on 68,000 utterances of dictated Wall
Street Journal articles– complex, continuous speech produced by many
different speakers of both sexes. Among the entries in the resulting
9,600 word dictionary are

  ------------------------- ---------------- ------------------------ ---------------
  /pr.0ptz.0ptt.0ptn/       president        /gouldm.0ptnsæks/        Goldman-Sachs
  /kmpšutr/                 computer         /gavrm.0ptn/             government
  /m.0ptn.0ptstreiš.0ptn/   administration   /s ̵.0ptmp.0pt.0pt.0pt/   something
  /bouskgi/                 (Ivan) Boesky    /læzdǰ.0ptr/             last year
  /hauwævr/                 however          /.0ptn.0ptd.0ptš.0ptn/   in addition
  ------------------------- ---------------- ------------------------ ---------------

Results like these demonstrate the power of our theory, both as an
abstract description of how children might learn, and as a foundation
for the machine acquisition of linguistic knowledge.
