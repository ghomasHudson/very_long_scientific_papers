# Chapter 1 Introduction

### 1.1 How to Read This Thesis

This Thesis presents a number of recent developments in the formalism of
tensor network Ansätze and algorithms, in which the author played a
leading role. Tensor network Ansätze (or simply “tensor networks”) are
mathematical tools which may be used to efficiently represent a portion
of the Hilbert space of a quantum mechanical system, frequently either
the ground state or the low-energy subspace with respect to a specified
Hamiltonian, and tensor network algorithms are algorithms for the
efficient construction and manipulation of these tensor networks. They
are frequently used to calculate properties such as the low-energy
spectra of lattice Hamiltonians, and the evolution of states as a
function of time (see Secs. 1.3 and 1.4 for introductory citations).
However, for a given Hamiltonian the ability of a tensor network
algorithm to accurately represent the low energy subspace depends upon a
number of factors. Existing tensor network algorithms tend to favour
Hamiltonians which may be written as local operators over a small number
of adjacent sites on a 1D or 2D lattice, with other factors which
determine whether a particular system may be efficiently analysed
including the statistics of the system (e.g. bosonic, fermionic), and
whether the structure of the tensor network reflects the structure of
entanglement in the states being studied (Evenbly and Vidal, in
preparation ) .

The developments presented in the following chapters greatly extend the
range of systems to which tensor network techniques may be applied,
including formalisms for the study of infinite systems (Chapter 2 ),
fermions (Chapter 3 ), and anyons (Chapter 4 ). They also expand the
capabilities of existing tensor network Ansätze when analysing symmetric
Hamiltonians (Chapters 3 and 5 ), and yield substantial improvements in
computational performance (Chapters 2 — 5 ).

Although continuous tensor network algorithms do exist (Verstraete and
Cirac, 2010 ) , in this Thesis I will consider only tensor networks for
lattice models. With the exception of Chapter 2 , however, most of the
results presented in this Thesis are completely general and may in
principle be applied to any tensor network Ansatz or algorithm. However,
when providing examples and demonstrations I will favour tensor networks
of the Multi-scale Entanglement Renormalisation Ansatz (MERA) type.
\nomenclature MERA Multi-scale Entanglement Renormalisation
Ansatz/Ansätze: A class of tensor network Ansätze designed for the
representation of states close to the fixed points of a renormalisation
group flow, including both lightly-entangled states (close to the ground
state) and heavily-entangled states (close to a critical point). MERA in
1D and 2D may be encountered in the present literature. I therefore
begin with a brief review of tensor network Ansätze in general and the
1D MERA in particular.

As described in the Statements of Contributions, much of the work
presented in this Thesis has previously been published in international
peer-reviewed journals. Rather than re-invent the wheel, this material
has largely been reproduced verbatim in the Thesis. As a result the
Thesis has a modular structure, where individual Chapters of the Thesis
are essentially self-contained. Inevitably this approach comes at the
expense of some repetition of background material, with individual
Chapters, and sometimes Sections within those Chapters, often having
their own introduction drawing the reader’s attention to the relevant
parts of this material. These are supplemented by material in the
present Chapter, which provides context for the Thesis as a whole.
Supplementary text at the beginnings or ends of the Chapters serves to
bring out connections between the different research topics, and to
place the individual research areas into a larger context.

A Note on the Use of Personal Pronouns in this Thesis

In this Thesis, use has been made of both the singular personal pronoun
(“I”), and the plural (“we”), with the latter being used in different
contexts to indicate either the author and the reader, or the author and
his collaborators. Choice of personal pronoun should not therefore be
treated as an indicator of whether work was performed independently or
in collaboration. This information may be found in the Statements of
Contributions in the preface to this text.

### 1.2 A General Introduction to Tensor Networks

The idea that the state of a lattice model may be represented by a
network of tensors may be motivated as follows: First, consider an
@xmath -site lattice @xmath . If each site of this lattice is described
by a @xmath -dimensional Hilbert space @xmath , then the Hilbert space
of the @xmath -site lattice is @xmath . We may write a general state
@xmath on lattice @xmath as

  -- -------- -- -------
     @xmath      (1.1)
  -- -------- -- -------

where @xmath constitute an orthonormal basis of @xmath , and @xmath .
For a fixed basis of @xmath , a state of the lattice may be entirely
specified by giving the tensor @xmath . It is convenient to introduce a
graphical notation whereby a tensor with @xmath indices may be
represented by a blob with @xmath legs, so that @xmath is represented
graphically by

  -- -------- -- -------
     @xmath      (1.2)
  -- -------- -- -------

Optionally we may choose to give each leg a vertical orientation and
identify upgoing legs with upper indices and downgoing legs with lower
indices, but this practice is seldom followed for tensor networks which
do not exploit internal symmetries as the metric, and hence conversion
between upgoing and downgoing legs, is trivial. This notation may be
understood as a simplified form of the Penrose graphical calculus
(Penrose, 1971a ) . The distinction between upgoing and downgoing legs
will become more important in the scheme presented in Chapters 4 – 5 for
systems with non-Abelian symmetries and for anyons, and a different but
related graphical notation will be introduced in Chapter 4 for the
description of tensor networks with non-Abelian symmetries.

The product of multiple tensors may similarly be represented by multiple
blobs, with open legs corresponding to free indices and shared legs
corresponding to summed indices. For example, matrix multiplication may
be represented as

  -- -------- -- -------
     @xmath      (1.3)
  -- -------- -- -------

Unless otherwise specified, in this Thesis I will employ the Einstein
summation convention, so @xmath is summed over in the left-hand side of
the above expression. If we now evaluate this sum over @xmath , we can
write @xmath ,

  -- -- -- -------
           (1.4)
  -- -- -- -------

The idea behind tensor network Ansätze is that for any tensor @xmath
describing a state on a lattice, we may write down an equivalent
collection of tensors linked by summed indices, such that when all these
sums are evaluated, we recover the original tensor @xmath . Indeed,
Eq. ( 1.4 ) may be thought of as a simple example where @xmath and
@xmath form a tensor network which evaluates to give @xmath . In general
a tensor network diagram will contain multiple separate tensors, and the
process of evaluating this diagram to obtain a single tensor (or, where
there are no free indices, a number) is known as tensor network
contraction . Similarly, taking two tensors within such a network and
replacing them by a single, equivalent tensor (such that on contracting
the entire network, the same resulting tensor is obtained) is termed
contracting these two tensors. For example, @xmath and @xmath in Eq. (
1.4 ) might in fact constitute part of a larger tensor network, such as
the simple one shown below, and Eq. ( 1.4 ) then describes the
contraction of @xmath with @xmath such that

  -- -- -- -------
           (1.5)
  -- -- -- -------

In general, any tensor network may be completely contracted by a series
of pairwise contractions of its component tensors.

But why use tensor networks at all? The answer is simple: Efficiency. If
a tensor network is to be capable of representing any state in the
Hilbert space @xmath , then it must contain at least as many free
parameters as @xmath , i.e. @xmath , and in general will be even less
convenient for computation than the form of Eq. ( 1.1 ). However, there
exist particular choices of tensor network having less than @xmath
parameters which are nevertheless capable of providing an accurate
description of an interesting subregion of this Hilbert space, for
example the low energy subspace with respect to a particular
Hamiltonian. Using these tensor networks, we may therefore numerically
study the properties of a physical system governed by this Hamiltonian,
typically at a fraction of the computational cost we would have incurred
if we had chosen to retain the full description afforded by @xmath in
Eq. ( 1.1 ). This Thesis assumes a basic familiarity with the use of
tensor network states and their associated algorithms, although for the
reader desiring further material, a brief recapitulation of the MERA is
provided in Sec. 1.3 , and a number of references for further reading
are listed in Secs. 1.3 and 1.4 .

### 1.3 Multi-scale Entanglement Renormalisation Ansätze

Introduced in Vidal ( 2007a ) and Vidal ( 2008 ) , the family of tensor
network Ansätze known as MERA are motivated by the idea of implementing
a real-space renormalisation group transformation on the lattice. They
represent a state @xmath using a layered structure, where each layer
@xmath may be considered as map between an initial @xmath -site lattice
@xmath and a coarse-grained @xmath -site lattice @xmath , for @xmath .
In addition to the tensors which perform this coarse-graining, each
layer of a MERA also incorporates a number of unitary tensors which act
on the lattice to remove entanglement before coarse-graining takes
place.

As an example, consider the 3:1 1D MERA (Fig. 1.1 ). The open bonds at
the bottom of Fig. 1.1 (i) correspond to indices @xmath , the physical
sites of the lattice. The open index at the top of the diagram enables
the MERA to represent multiple sites within the Hilbert space, where the
value of this index enumerates the represented states. This index ranges
from 1 to @xmath . For a fixed value of this index @xmath , the network
represents a single state @xmath and may be contracted to the tensor
@xmath which specifies the coefficients of this state as per Eq. ( 1.1
). A MERA having @xmath consequently represents only a single state
@xmath . Finally, we limit the dimension of each of the summed indices
to at most @xmath , where @xmath is a tunable parameter determining the
number of free parameters in the Ansatz.

The diagrammatic counterpart of Hermitian conjugation is implemented by
vertically reflecting a tensor and complex conjugating all of its
entries. Thus if we denote a disentangler by @xmath and its Hermitian
conjugate by @xmath , then their diagrammatic representations are

  -- -------- -- -------
     @xmath      (1.6)
  -- -------- -- -------

respectively. Similarly, the MERA for a bra, @xmath , is constructed by
vertically reflecting the ket, @xmath , of Fig. 1.1 (i), and complex
conjugating the coefficients of all the tensors in the network.

For discussion of MERA algorithms to approximate the ground state of a
system, see Dawson et al. ( 2008 ) , Rizzi et al. ( 2008 ) , and Evenbly
and Vidal ( 2009a ) . A pedagogical introduction to the MERA,
predominantly in one dimension, may be found in Vidal ( 2010 ) ,
presenting interpretations of the MERA formalism both in terms of the
real-space renormalisation group transformation (coarse-graining)
described in this section, and also as a quantum circuit. Applications
of the MERA to 2D systems may be found in e.g. Evenbly and Vidal ( 2009a
) , Evenbly and Vidal ( 2009b ) , Evenbly and Vidal ( 2010a ) , Evenbly
and Vidal ( 2010b ) , Evenbly and Vidal ( 2010c ) , Cincio et al. ( 2008
) , Aguado and Vidal ( 2008 ) , and König et al. ( 2009 ) . Note that
Vidal ( 2010 ) also includes material on the scale-invariant MERA, which
is the subject of Chapter 2 of this Thesis.

### 1.4 Other Tensor Networks

There also exist a number of other tensor network Ansätze and
algorithms. This section lists a selection of introductory references
and example papers for a few of the more popular.

#### 1.4.1 Matrix Product States \nomenclature

MPS Matrix Product State: A 1D tensor network Ansatz capable of
efficiently representing lightly entangled states. Correlators under
this Ansatz decay exponentially as a function of @xmath . One of the
most common tensor network Ansätze in use today is the Matrix Product
State (MPS), which is the Ansatz underlying the Density Matrix
Renormalisation Group (DMRG) technique developed by White ( 1992 ) for
computation of ground states. \nomenclature DMRG Density Matrix
Renormalisation Group: A real-space renormalisation group technique
developed by White ( 1992 ) , which may be interpreted in terms of the
Matrix Product State tensor network Ansatz. Time evolution may be
simulated using the Time Evolving Block Decimation (TEBD) algorithm of
Vidal ( 2004 ) . \nomenclature TEBD Time Evolving Block Decimation
algorithm: An algorithm for simulating the time evolution of Matrix
Product States. For further reading on DMRG and MPS, see White and Noack
( 1992 ); White ( 1992 ); Noack and White ( 1993 ); White ( 1993 );
White and Feiguin ( 2004 ); Schollwöck ( 2005a ); Perez-Garcia et al. (
2007 ); Schollwöck ( 2011 ) , and for TEBD of infinite chains, see Vidal
( 2007a ) .

#### 1.4.2 Tree Tensor Networks \nomenclature

TTN Tree Tensor Network: A heirarchical tensor network Ansatz which is
capable of efficiently representing lightly entangled states, typically
in one or two dimensions. The Tree Tensor Network (TTN) may be used to
represent states on lattices of arbitrary dimension. It has a
heirarchical structure, but is not well suited to the representation of
large critical systems due to the need for large bond dimensions
(indices with large ranges) towards the top of the tree. Structurally, a
TTN may be thought of as a MERA without disentanglers (Fig. 1.2 ). For
further reading, see Shi et al. ( 2006 ) , and also Fannes et al. (
1992b ); Otsuka ( 1996 ); Niggemann et al. ( 1997 ); Friedman ( 1997 );
Lepetit et al. ( 2000 ); Martín-Delgado et al. ( 2002 ); Nagaj et al. (
2008 ); Tagliacozzo et al. ( 2009 ) .

#### 1.4.3 Projected Entangled Pair States \nomenclature

PEPS Projected Entangled Pair State: The 2D generalisation of the Matrix
Product State Ansatz. The Projected Entangled Pair State (PEPS) Ansatz
is a generalisation of the Matrix Product State Ansatz to two dimensions
(Verstraete and Cirac, 2004 ) . See also Sierra and Martín-Delgado (
1998 ); Maeshima et al. ( 2001 ); Nishio et al. ( 2004 ); Murg et al. (
2007 ); Jordan et al. ( 2008 ); Murg et al. ( 2009 ) .

## Chapter 2 Scale-Invariant MERA

### 2.1 Introduction

In this Chapter of the Thesis, we see how MERA-type tensor network
Ansätze may be used to study the properties of infinite scale-invariant
systems. Right from the start (Vidal, 2007a ) , the MERA has been
constructed to implement a real-space renormalisation group (RSRG)
\nomenclature RSRG Real-Space Renormalisation Group: A real-space
renormalisation group transformation on the lattice is a mapping from an
infinite lattice @xmath to an infinite lattice @xmath where @xmath sites
of local dimension @xmath on lattice @xmath are mapped into @xmath sites
of local dimension @xmath on @xmath , for @xmath and @xmath such that
@xmath . Otherwise known as a coarse-graining transformation.
transformation. It is by exploiting this property of the MERA that an
Ansatz for scale-invariant systems may be constructed, as described by
Giovannetti et al. ( 2008 ) and Pfeifer et al. ( 2009 ) . Further
publications studying and applying the scale-invariant MERA include
those by Evenbly and Vidal ( 2009b ) , Montangero et al. ( 2009 ) , and
Giovannetti et al. ( 2009 ) . Evenbly et al. ( 2010 ) address the
application of the scale-invariant MERA to half-infinite and bounded 1D
chains, and most recently, Pfeifer et al. ( 2010 ) applies the
scale-invariant MERA to a quantum critical system of anyons (see also
Chapter 4 of this Thesis).

The material presented in this Chapter is based upon research first
published as Pfeifer, Evenbly, and Vidal ( 2009 ) . The numerical
results of Sec. 2.3 , including Table 2.1 and Fig. 2.11 , are reproduced
or adapted from this reference and are © (2009) by the American Physical
Society.

#### 2.1.1 Real-Space Renormalisation Group Transformations

Real-space renormalisation group transformations have a long history in
condensed matter physics, dating back to Kadanoff’s spin-blocking
technique (Kadanoff, 1966 ) and Wilson’s solution of the Kondo problem
(Wilson, 1975 ) . However, such techniques really came of age with the
development of the DMRG algorithm by White ( 1992 , 1993 ) . The
defining feature of such techniques is that there exists some procedure
whereby a theory on an initial lattice @xmath may be subject to some
numerical coarse-graining process to yield an effective description on a
new lattice @xmath , where each lattice site @xmath on @xmath
corresponds to some region on @xmath , say sites @xmath , and

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

This concept was originally proposed by Kadanoff ( 1966 ) in a classical
context, with the idea of replacing a group of spins with a single
effective spin chosen to be representative of the group. The first
successful quantum mechanical application of an RSRG approach was the
treatment of the Kondo problem by Wilson ( 1975 ) , in which a
coarse-graining procedure was chosen so that the retained portion of the
Hilbert space corresponded locally to the low-energy eigenstates of
individual terms of the Hamiltonian, e.g. @xmath in a Hamiltonian of the
form

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

where @xmath and @xmath +1 are sites of a 1D lattice. However, the
approach did not appear to generalise well to other problems.

The development of DMRG by White in 1992 provided the next crucial
insight—that the ground state wavefunction which minimises @xmath does
not necessarily also minimise the expectation value of each local term
@xmath , and the retained portion of the Hilbert space must therefore be
chosen in a way which takes into account the total Hamiltonian. DMRG is
one algorithm which satisfies this requirement.

More generally, any tensor network which admits a description as a
procedure mapping between a series of increasingly coarse-grained
infinite lattices may be understood as defining an RSRG transformation,
and one can define a cost function [e.g. @xmath ] whose minimum
corresponds to the desired state or subspace, and attempt to numerically
optimise the defined RSRG transformation so as to extremise this cost
function. For example, applying this philosophy to construct a quantum
mechanical version of the spin-blocking technique, one obtains the Tree
Tensor Network formalism of Shi et al. ( 2006 ) (see Sec. 1.4.2 ,
above).

When an RSRG transformation is applied to a Hamiltonian @xmath on a
lattice @xmath , it yields a Hamiltonian @xmath on the coarse-grained
lattice @xmath . Repeated application of the RSRG transformation
therefore causes the set of Hamiltonians @xmath to describe a trajectory
in the space of Hamiltonians, termed a Renormalisation Group (RG) flow.
\nomenclature RG Abbreviation for “Renormalisation Group”. If the
Hamiltonians @xmath and @xmath satisfy @xmath , then the Hamiltonian
remains unchanged under repeated application of the RSRG transformation,
and we term @xmath a fixed point of the RG flow.

#### 2.1.2 Lattice Models Exhibiting Scale Invariance

In order to exhibit scale invariance, a lattice model must be free of
any characteristic length scales. Consequently, when a Hamiltonian is a
fixed point of the RG flow defined by an RSRG transformation @xmath ,
then the correlation length @xmath for all operators in a system must be
either zero or infinite.

Recall now that our tensor network Ansatz must not only describe the
RSRG transformation @xmath , but also provide an accurate description of
the low-energy subspace of the Hilbert space of the system. Let us
consider the different possible types of scale-invariant lattice model
we may encounter:

1.  @xmath , ground state is a product state: Unentangled. Product
    states may trivially be described by any tensor network.

2.  @xmath , ground state is a topologically ordered state: Entangled.
    Experience indicates that such states may be efficiently described
    by a MERA with finite bond dimension (Aguado and Vidal, 2008 ; König
    et al., 2009 ) .

3.  @xmath , quantum critical system: Highly entangled. In the ground
    state, entanglement of a contiguous region @xmath of length @xmath
    with the rest of the lattice (as measured by the von Neumann entropy
    @xmath ) scales as @xmath in 1D, and @xmath or @xmath in 2D,
    depending on the model under consideration.

Obviously, we may simulate a product state using any tensor network we
like, including the MERA. Prior experience shows that for topologically
ordered systems, the MERA may once again be a good choice. Finally, what
about quantum critical systems? We require a tensor network capable of
encoding a bipartite entanglement entropy which in 1D should scale as
@xmath when evaluated for a contiguous region @xmath having length
@xmath , and in 2D should scale at least as @xmath , and preferably as
@xmath , for a region @xmath having dimension @xmath . To understand why
the MERA is once again the natural choice, it is necessary to briefly
examine how entanglement entropy scales for any tensor network.

Consider a state @xmath represented by a single tensor @xmath as in
Eq. ( 1.1 ). Suppose we wish to investigate the entanglement between two
regions of the lattice, sites @xmath and @xmath . If we perform a
Schmidt decomposition of state @xmath , then we write

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

where @xmath is a list of strictly positive coefficients. In terms of
linear algebra, this is equivalent to performing a singular value
decomposition on a matrix @xmath , where indices @xmath and @xmath
enumerate states on indices @xmath and @xmath respectively. The
dimension of index @xmath may range from 1 (for a product state) to the
lesser of @xmath and @xmath (for a highly entangled state). Assuming
that the coefficients @xmath are sorted in decreasing order of
magnitude, then the more entangled the state, the higher the value of
@xmath before @xmath .

Now, consider as an example a 1D state represented by an MPS, where the
range of the indices in the network has been limited to @xmath . If we
perform a bipartition of such a state, as shown in the Fig. 2.1 (i),
then the range of @xmath in Eq. ( 2.3 ) will be limited to at most
@xmath , and this provides an upper bound on the amount of entanglement
which such a state may represent.

In general, a crude quantification of the maximum entanglement a tensor
network may encode between a region @xmath and the rest of the lattice
is therefore given by taking the product of the dimensions of the bonds
which one must cut to separate the tensor network into two regions, one
contacting the physical lattice only within region @xmath , and the
other contacting the lattice only outside of @xmath [e.g. Fig. 2.1
(ii)]. (Taking the logarithm of one over this value yields an upper
bound on the von Neumann or entanglement entropy, @xmath .) In general,
multiple such cuts exist, and the maximum amount of entanglement which
may be encoded is determined by the cut giving the smallest value.

By studying the scaling of @xmath with the size of region @xmath , we
observe that for an MPS, the maximum amount of entanglement which may be
encoded is independent of the size of region @xmath , @xmath For a TTN,
the amount of entanglement which may be encoded exhibits a more
complicated dependence on the size and position of region @xmath .
However, for an @xmath -into-1 tree it is possible for any @xmath to
choose a region @xmath of linear size @xmath , having the same entropy
as an appropriately chosen region of length @xmath . The overall
performance of the Ansatz is limited by this worst-case scenario, and
consequently a TTN of constant @xmath also exhibits an entropy scaling
@xmath In contrast, the 1D MERA exhibits a scaling @xmath , making it
well-suited to the study of quantum critical systems. In 2D the
situation is less ideal, with the 2D MERA exhibiting a scaling @xmath ,
and thus only being suited to the study of critical systems which do not
display a logarithmic correction to the entanglement entropy. However,
recent development of a branching MERA algorithm (Evenbly and Vidal, in
preparation ) with entropy scaling as @xmath or better suggests that
once again, some form of the MERA may prove to be a good choice of
Ansatz for all critical systems in 2D.

This is not to say that the MPS and TTN cannot be used to calculate
properties of quantum critical systems. They can, and in many situations
may yield excellent numerical approximations to ground state energies
and short-range correlators. However, due to their limited capacity for
encoding entanglement within the structure of the tensor network,
attempts to construct a RSRG transformation will fail at sufficiently
large length scales (and we shall see in Sec. 2.2.2 that many
interesting properties of quantum critical systems may be computed in
the large-length-scale, or infra-red, limit). Consider as an example the
application of an @xmath -into-1 TTN to a highly entangled lattice model
on a lattice @xmath . A single layer of the TTN coarse-grains @xmath
into an effective lattice @xmath , and each site in @xmath now
corresponds directly to @xmath sites in @xmath . The entanglement
between a single site of @xmath and the rest of the lattice will thus be
the same as between those @xmath sites on @xmath and the rest of the
lattice. If the entanglement entropy of the model exhibits any
dependence on @xmath , then repeated coarse-graining of the lattice will
cause this entanglement to continually increase. If the dimension of the
indices of the TTN is bounded by some value @xmath , then after some
number of coarse-graining steps, the approximation made in imposing this
limit on index dimension will lead to a failure of the TTN to accurately
reproduce the properties of the ground state over large length scales.
Alternatively, the index dimension would have to increase with each
layer of the coarse-graining procedure, eventually becoming infinite.
First, this is computationally unfeasible, and second, we anyway desire
that the structure of our tensor network should reflect the
scale-invariant nature of the ground state.

In contrast, the MERA may be thought of as a TTN supplemented by
additional tensors known as disentanglers (for illustration of this in
1D, compare Figs. 1.1 and 1.2 ). When the MERA is interpreted as an RSRG
transformation, then the disentanglers in each level act to remove
short-range entanglement from the ground state. We anticipated that in
1D, for a well-optimised MERA representation of the low-energy subspace
of a quantum critical system, they would do so to a sufficient extent
that the entanglement entropy of a region @xmath of length @xmath on
lattice @xmath would be the same as the entanglement entropy of a region
@xmath of length @xmath on lattice @xmath , for any coarse-grained
lattice @xmath , even when the entanglement entropy on an individual
lattice scales as @xmath . This proposition was based on the observation
that the 1D MERA is constructed to be capable of encoding an
entanglement entropy which scales as @xmath , and its predictions have
been borne out by subsequent experience.

#### 2.1.3 Interesting Properties of 1D Quantum Critical Systems

In studying quantum critical systems, we are particularly interested in
their behaviours in the infra-red limit. When we take the continuum
limit of a 1D quantum critical system, we obtain a Conformal Field
Theory (CFT) in 1+1D (Cardy, 1996 ; Di Francesco et al., 1997 ) which
describes the infra-red behaviour of the system in the vicinity of the
associated phase transition, and a useful question to ask is whether we
can extract from our Ansatz sufficient data to identify and fully
characterise the CFT. \nomenclature CFT Conformal Field Theory: A field
theory invariant under translation, rotation, rescaling, and special
conformal boosts. Taking the continuum limit of a 1D quantum critical
system yields an associated 1+1D CFT which describes the infra-red
behaviour of the associated quantum phase transition.

In the operator formalism, a 1+1D CFT may be described in terms of an
infinite number of operator-valued fields @xmath . It is conventional to
first define the theory on the cylinder, with @xmath and @xmath , before
mapping to the complex plane via the reparameterisation

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

with the fields of the theory now being denoted @xmath . Under the
action of a conformal mapping @xmath , the correlators of these fields
transform as

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

where each field @xmath is associated with a holomorphic and an
antiholomorphic conformal dimension , @xmath and @xmath respectively.
These in turn may be combined to give the scaling dimension of the
field, @xmath , and the conformal spin , @xmath .

For any 1+1D CFT, these fields @xmath , which we will term scaling
fields , may be organised into conformal families , each consisting of
an infinite number of fields. Within each conformal family, the field
with the smallest scaling dimension is termed the primary field , with
all others being termed descendants . We may associate with each
operator field @xmath a state @xmath generated by acting with @xmath on
the origin of the vacuum state (which corresponds to @xmath ),

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

If we define the operators

  -- -------- -- -------
     @xmath      (2.7)
  -- -------- -- -------

where @xmath is the energy-momentum tensor, and the contour integration
is performed over any contour which encircles the origin, then these
operators @xmath and @xmath form representations of the Virasoro
algebra. They obey the commutation relations

  -- -------- -------- -- --------
     @xmath   @xmath      (2.8)
              @xmath      (2.9)
              @xmath      (2.10)
  -- -------- -------- -- --------

where parameter @xmath is a constant known as the central charge of the
CFT, and repeated application of @xmath and @xmath to the state @xmath
associated with the primary field of any conformal family will generate
all other states associated with members of that family. There may be a
finite or an infinite number of conformal families, but of greatest
interest to us will be the CFTs known as minimal models , for which the
number of conformal families is finite.

The identity operator is always the primary field for one of the
conformal families, and we may always choose our operator fields to
satisfy

  -- -------- -------- -- --------
     @xmath   @xmath      (2.11)
     @xmath   @xmath      (2.12)
  -- -------- -------- -- --------

where @xmath corresponds to a particular choice of normalisation, and
@xmath if @xmath and 0 otherwise. We must also specify the coefficients
@xmath of the three-point function,

  -- -------- -------- -- --------
     @xmath               (2.13)
     @xmath   @xmath      (2.14)
  -- -------- -------- -- --------

Whereas @xmath was a normalisation factor which we were free to choose
as we liked, the values of @xmath form part of the description of the
CFT under consideration.

Expression ( 2.13 ) implies an algebra known as the Operator Product
Expansion (OPE),

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

which may be inserted into higher-order correlation functions with
higher-order terms vanishing in the limit that @xmath is much smaller
than any other separation in the correlator. \nomenclature OPE Operator
Product Expansion: An operator identity in conformal field theory, valid
within (and frequently used in the evaluation of) higher-order
correlators. See Sec. 2.1.3 of this Thesis, Cardy ( 1996 ) , Di
Francesco et al. ( 1997 ) , and Cardy ( 2006 ) .

To fully describe a 1+1D CFT in the operator formalism, it suffices to
specify

1.  The primary fields, @xmath .

2.  Their scaling dimensions @xmath and conformal spins @xmath .

3.  The central charge @xmath of the Virasoro algebra.

4.  The coefficients @xmath of the operator algebra for the primary
    fields.

We will see that it is possible to extract all of these data from the
Scale-Invariant MERA, with the exception of the conformal spin. However,
the data which can be obtained are nevertheless frequently sufficient to
uniquely identify the CFT describing the infra-red limit of a particular
quantum critical system.

### 2.2 Scale-Invariant MERA Algorithm

In Sec. 2.2.1 , I describe the algorithm for constructing an infinite,
scale-invariant MERA. This approach may be applied to either the 1D or
the 2D MERA, but in this Thesis I will primarily address the study of 1D
quantum critical systems, whose infra-red limits correspond to the
interesting and highly-studied 1+1D CFTs. Material on the extraction of
conformal data in Sec. 2.2.2 is addressed primarily to these systems,
and to computation of the parameters described in Sec. 2.1.3 .

An example application of the Scale-Invariant MERA algorithm to infinite
2D lattice models may be found in Evenbly and Vidal ( 2009b ) .

#### 2.2.1 Construction of MERA for the Low Energy Subspace

##### 2.2.1.1 Overview

For a finite system, a MERA normally consists of a finite number of
layers of tensors, each layer consisting of a row of disentanglers and a
row of isometries (Fig. 1.1 ). Each layer performs a coarse-graining
procedure, mapping from a lattice @xmath to a coarser lattice @xmath .
This process incorporates a truncation of the Hilbert space, such that
after all layers of the MERA have been applied, the dimension of the
Hilbert space on the maximally coarse-grained lattice is sufficiently
small to exactly diagonalise. Numerical optimisation of the MERA (Dawson
et al., 2008 ; Rizzi et al., 2008 ; Evenbly and Vidal, 2009a ) is
performed to ensure that the Hilbert space of the final coarse-grained
lattice exhibits maximal overlap with the interesting region of the
Hilbert space of the original lattice, typically the low-energy subspace
of a system.

For an infinite system, this procedure obviously requires some
modification. No matter how many times we apply a coarse-graining
transformation to an infinite lattice @xmath , the result is always an
infinite lattice, and the system never becomes small enough to exactly
diagonalise. However, the scale-invariant property of quantum critical
systems comes to our rescue. To see how this works, let us assume that
we have a Hamiltonian @xmath which is constructed on lattice @xmath and
lies exactly at the fixed point of an RG flow.

We know that if we were able to construct a MERA with an infinite number
of layers which represented the low-energy subspace of this Hamiltonian,
then because @xmath is a fixed point of the RG flow, application of a
layer of the MERA to perform a coarse-graining from @xmath to @xmath
would map @xmath into an identical operator @xmath on the coarse-grained
lattice. An object which maps operators into operators is termed a
superoperator , and we may therefore define the scaling superoperator
@xmath as the superoperator implemented by this layer of the MERA, which
maps operators from lattice @xmath to @xmath for our scale-invariant
system. Of course, because @xmath and @xmath are identical, the layer of
MERA constructed on @xmath will be identical to that constructed on
@xmath , and we may equally well define @xmath with reference to any
layer of this infinite MERA. Because the Hamiltonians are similarly
identical, we will drop the lattice index, and simply write @xmath for
the fixed-point Hamiltonian on any lattice @xmath .

Because the Hamiltonian is identical on all lattices @xmath , the
reduced density matrix @xmath which minimises the energy @xmath is
similarly also identical on every layer of coarse-graining. However, in
a MERA we may always calculate the reduced density matrix on a lattice
@xmath from the reduced density matrix on lattice @xmath (Evenbly and
Vidal, 2009a ) . Let us denote by @xmath the superoperator which is the
dual of @xmath , and maps operators on @xmath into operators on @xmath .
Because @xmath is identical on every layer, it must be an eigenoperator
of @xmath , and because @xmath on every layer, it must have eigenvalue
1. Provided there exists only one eigenoperator of @xmath which has
eigenvalue 1, this then suffices to uniquely define the fixed-point
reduced density matrix @xmath . When @xmath has only one eigenoperator
with eigenvalue 1, knowledge of the scaling superoperator @xmath and its
dual are sufficient to compute the reduced density matrix, and these
superoperators in turn may be constructed from any layer of this
infinite, scale-invariant MERA.

Finally, because all layers of this MERA are identical, we need only
describe the disentanglers and isometries of one layer in order to
describe the state of the entire system. Assuming also translation
invariance, we need only one disentangler and one isometry in order to
describe the entirety of this infinite MERA, or compute the reduced
density matrix on any lattice @xmath . What will be presented in this
Section is therefore an algorithm for determining exactly these tensors:
The disentangler and isometry of the scale-invariant MERA.

##### 2.2.1.2 A Less Idealised Situation

In the above discussion, it was assumed that the Hamiltonian of the
system was precisely the fixed point Hamiltonian of the RSRG transform;
that is, @xmath . For this to be true, @xmath must correspond to a
scaling field, or sum of scaling fields, of the associated CFT, all with
identical scaling dimension @xmath .

In practice, the Hamiltonian of the quantum critical system may not be
exactly the fixed point Hamiltonian, but may also include additional
scaling fields, provided these fields have scaling dimension @xmath . On
repeated coarse-graining these fields are suppressed relative to the
Hamiltonian. While these fields will in theory never vanish completely,
and on repeated coarse-graining @xmath will only approach the fixed
point of the RG flow asymptotically,

  -- -------- -- --------
     @xmath      (2.16)
  -- -------- -- --------

we will assume that they decay sufficiently rapidly that their existence
may be neglected after some finite number @xmath of applications of the
coarse-graining process. We therefore construct our Ansatz to consist of
@xmath layers of ordinary MERA, acting on lattices @xmath to @xmath ,
after which the difference between @xmath on lattice @xmath and @xmath
is negligible, and all subsequent layers of the MERA will be essentially
identical. We therefore surmount the layers @xmath of the MERA by one
further layer @xmath , which acts on lattice @xmath , and is assumed to
be repeated an infinite number of times (as layers @xmath and above,
acting on lattices @xmath to @xmath ). It is this layer @xmath of the
MERA which is then used in the construction of the scaling
superoperator.

We will call an operator a scaling operator if it is an eigenoperator of
@xmath , and in Secs. 2.2.2 – 2.3 we will endeavour to identify these
operators with the scaling fields @xmath of the CFT associated with our
quantum critical lattice model. As a note of terminology, scaling fields
which decay more rapidly than the fixed point Hamiltonian under the
action of the RSRG transformation are termed irrelevant , as are the
associated fields of CFT. Those which decay at the same rate are termed
marginal , and those which decay less rapidly are termed relevant . We
will adopt the same terminology for scaling operators. The Hamiltonian
of a quantum critical system will only ever contain marginal and
possibly irrelevant terms.

##### 2.2.1.3 The Algorithm Itself

I now present explicitly a practical algorithm for optimisation of the
Scale-Invariant MERA for a local quantum critical Hamiltonian on a
lattice, which may contain irrelevant terms. This algorithm will be
described in general language applicable to both 1D and 2D systems,
though accompanying illustrations will refer specifically to the 3:1
MERA in 1D.

It is assumed that the Hamiltonian under consideration is
nearest-neighbour, next-to-nearest neighbour, etc., as appropriate to
the MERA being employed (for example, the 3:1 MERA on the 1D lattice is
constructed for the study of nearest-neighbour Hamiltonians). This may
always be achieved by means of some initial coarse-graining onto an
effective lattice of higher site dimension if required. As an example,
we will subsequently consider the critical Ising model on a 1D lattice,
which is a nearest-neighbour Hamiltonian and thus is directly suitable
for analysis using the 3:1 MERA.

The MERA is initially constructed to consist of some small number of
free layers @xmath , where @xmath is typically around 1 or 2, over which
the local dimension of the lattice increases from @xmath on @xmath , to
@xmath on @xmath . These are surmounted by the scale-invariant layer,
which maps from lattice @xmath to lattice @xmath , both of dimension
@xmath . Initial choices of tensor for the disentanglers and isometries
are comparatively unimportant, and may be chosen randomly within the
constraints of Fig. 1.1 (ii), or assigned systematically to some known
initial configuration. Optimisation then proceeds as follows:

1.  Construct the fixed-point reduced density matrix, @xmath on lattice
    @xmath , by diagonalising the dual of the scaling superoperator
    @xmath (see Fig. 2.2 for an example).

2.  “Descend” @xmath to obtain @xmath in the usual manner (Evenbly and
    Vidal, 2009a ) .

3.  Proceeding row by row from layer 1 to layer @xmath of the MERA, for
    each layer @xmath :

    1.  Update the disentanglers in the usual manner (Evenbly and Vidal,
        2009a ) .

    2.  Update the isometries in the usual manner.

    3.  “Lift” the Hamiltonian from @xmath to @xmath in the usual
        manner.

4.  The Hamiltonian @xmath now closely resembles the fixed point
    Hamiltonian @xmath . Optionally, we may now “lift” @xmath a few more
    times, using the tensors of the scale-invariant layer @xmath of the
    MERA, to obtain a tensor @xmath which is even more close to @xmath .
    Using @xmath in lieu of @xmath may yield more accurate computation
    of critical exponents, but at the cost of slower convergence of the
    Scale-Invariant MERA.

5.  Using @xmath (or @xmath , if preferred) and the reduced density
    matrix @xmath from one layer further up the infinite MERA (which is
    taken to be the same as @xmath ), update the disentanglers of layer
    @xmath of the MERA (the scale-invariant layer).

6.  Compute numerical properties (e.g. ground state energy, scaling
    dimensions; see Sec. 2.2.2 ).

7.  Repeat all steps until the chosen cost function is satisfactorily
    converged.

Following initial convergence of the MERA, the quality of the numerical
results (e.g. ground state energy, scaling dimensions, etc.) may be
increased by adding more free layers below the scale-invariant layer. To
do so, copy the tensors of the scale-invariant layer (denoted @xmath )
to obtain a layer @xmath . Layer @xmath is now the scale-invariant
layer, and the above optimisation procedure is now repeated with layers
@xmath optimised in the usual manner for a standard MERA, and layer
@xmath being used to construct the fixed-point reduced density matrix
and the scaling superoperator. This process may be repeated until
insertion of additional layers no longer causes a significant change in
the computed properties of the MERA.

The above algorithm serves as an illustrative example as to how a
scale-invariant MERA may be converged. In practice, a significant time
saving may be made by modifying the computation of @xmath . Rather than
computing the dominant eigenoperator of @xmath exactly on every
iteration, we instead assume that after updating the scale-invariant
MERA layer, @xmath from the previous iteration has a non-trivial overlap
with the dominant eigenoperator of the new @xmath . We therefore take
@xmath from the previous iteration, and apply the dual of the scaling
superoperator once (i.e. we descend this operator using the
scale-invariant layer @xmath ). We then take the resulting operator to
be the new @xmath . In the limit that the MERA converges (assuming, as
always, that this limit exists—an assumption borne out well in
practice), @xmath remains constant from iteration to iteration, and thus
is repeatedly applied to @xmath , which will thus gradually converge to
the dominant eigenoperator of @xmath as required. In practice, this
process leads to a co-ordinated convergence of @xmath and @xmath , and
requires less time than exactly computing @xmath on every iteration.

#### 2.2.2 Extraction of Conformal Data

As described in Sec. 2.1.3 , we may associate the infra-red
(large-scale) behaviour of a 1D quantum critical theory with a 1+1D CFT.
To extract the conformal data describing this CFT, we must first
identify the objects in the quantum critical theory which correspond to
the scaling fields of the CFT. These are objects which remain invariant
under the action of an RSRG transformation, and consequently may be
identified with operators which are eigenoperators of the scaling
superoperator. Note that in this Section, we are interested in
calculating properties in the large-scale regime of the quantum system,
and consequently all disentanglers, isometries, reduced density
matrices, etc. are drawn from the scale-invariant layer of the MERA,
which is assumed to be repeated an infinite number of times and
therefore describes the behaviour of the quantum critical system on all
larger length scales.

For the 3:1 MERA, the causal cone has a width of two sites, and
consequently we may construct a two-site scaling superoperator @xmath
[Fig. 2.3 (i)] whose eigenoperators are two-site scaling operators [Fig.
2.3 (ii)]. Note that the scaling superoperator is the average of three
diagrams. This is because a two-site operator on the coarse-grained
lattice receives contributions from operators on three distinct pairs of
sites on the fine-grained lattice, and this must be taken into account
in the construction of the scaling superoperator. The two-site reduced
density matrix is similarly an eigenoperator of @xmath , the dual of
@xmath , with eigenvalue 1, as shown in Fig. 2.2 . However, we also note
that on privileged sites of the 3:1 MERA, it is also possible to
consider one-site scaling operators which remain invariant under the
action of the RSRG transformation. These operators are eigenoperators of
the one-site scaling superoperator, @xmath , as shown in Fig. 2.4 .

Note that due to the contraints on the disentanglers and isometries
[Fig. 1.1 (ii)], both scaling superoperators are necessarily unital ,
@xmath , so that the identity operator is always a scaling operator with
eigenvalue @xmath , and contractive , @xmath (Bratteli and Robinson,
1979 ) .

As the sites of the MERA are spacelike-separated, the one- and two-site
scaling operators will satisfy isotemporal versions of the correlators (
2.11 )–( 2.13 ),

  -- -------- -------- -- --------
     @xmath   @xmath      (2.17)
     @xmath   @xmath      (2.18)
     @xmath   @xmath      (2.19)
  -- -------- -------- -- --------

where @xmath is a purely spatial co-ordinate, and

  -- -------- -- --------
     @xmath      (2.20)
  -- -------- -- --------

For convenience, we shall now choose to work specifically with the
one-site scaling operators. We may normalise these scaling operators by
imposing Eq. ( 2.18 ) with @xmath . When two scaling operators are
placed on consecutive lattice sites, the correlator ( 2.18 ) reduces to

  -- -------- -- --------
     @xmath      (2.21)
  -- -------- -- --------

where the expectation value is computed with respect to the two-site
reduced density matrix on the scale-invariant portion of the MERA, which
we will denote @xmath . For the 3:1 MERA, this is the same as the
fixed-point reduced density matrix @xmath calculated during
optimisation. The local scaling operators @xmath must therefore be
(ortho)normalised to satisfy

  -- -------- -- --------
     @xmath      (2.22)
  -- -------- -- --------

as represented graphically in Fig. 2.5 .

We may now compute correlators for pairs of one-site local scaling
operators, provided these operators are located at a separation such
that each application of the MERA maps a one-site local scaling operator
into a one-site local scaling operator (e.g. Fig. 2.6 ). We find that
under these conditions, the correlators scale as

  -- -------- -- --------
     @xmath      (2.23)
  -- -------- -- --------

Using the identity

  -- -------- -- --------
     @xmath      (2.24)
  -- -------- -- --------

(try taking the log of both sides), we see that the scaling dimensions
of the primary fields may be computed according to

  -- -------- -- --------
     @xmath      (2.25)
  -- -------- -- --------

(More generally, for an @xmath -into-1 MERA, the same argument yields
@xmath .)

We may also use correlators to calculate the OPE coefficients @xmath .
Although we do not have direct access to a translation-invariant
scale-invariant reduced density matrix @xmath in direct analogy to the
two-site reduced density matrix @xmath , we may nevertheless easily
compute three-point correlators @xmath for certain privileged locations
on the lattice. Assuming translation invariance of @xmath and
symmetrising across the two diagrams given in Fig. 2.7 suffices to give
us the values of @xmath :

  -- -------- -------- -- --------
     @xmath   @xmath      (2.26)
  -- -------- -------- -- --------

If sufficient computational resources are available, a more rigorous
evaluation of @xmath may be achieved by computing @xmath according to
Fig. 2.8 , and then determining @xmath using @xmath according to Fig.
2.9 .

We may also compute the central charge, which is obtained from the von
Neumann entropies associated with the one- and two-site reduced density
matrices of the scale-invariant layer of the MERA:

  -- -------- -------- -- --------
     @xmath   @xmath      (2.27)
     @xmath   @xmath      (2.28)
  -- -------- -------- -- --------

(The one-site scale-invariant reduced density matrix @xmath may be
obtained by symmetrising over the two ways of tracing out one site of
@xmath —see Fig. 2.10 .)

How well may these calculations be expected to work in practice? It is
important to recognise that for any CFT, there are always an infinite
number of local scaling operators. However, for any MERA with finite
bond dimension @xmath , there are only ever a finite number of
eigenoperators of the scaling superoperator. We anticipated, and this is
borne out in practice, that by constructing a MERA which represents well
the low-energy subspace of the quantum critical theory, we would obtain
to a high level of accuracy the conformal parameters associated with the
scaling fields of lowest scaling dimension, but that this accuracy would
decrease on going to larger scaling dimensions, with an inevitable
truncation at some finite scaling dimension @xmath . The space of states
@xmath associated with the local scaling operators of the MERA is
therefore a finite-dimensional vector space, on which exists at best
only an approximate, truncated representation of the Virasoro algebra
describing the associated CFT (see Eqs. 2.8 – 2.10 of Sec. 2.1.3 ),
becoming exact in the limit @xmath . Nevertheless, we find that even for
relatively modest @xmath , it is frequently possible to construct a MERA
which yields reasonable accuracies for the conformal data.

### 2.3 Results

This Section presents results demonstrating the capabilities of the
Scale-Invariant MERA. Two systems were studied: The Ising model, and the
three-state Potts model, which are known to be associated with CFT
minimal models @xmath and @xmath respectively. Using a @xmath MERA,
scaling dimensions of the primary fields were obtained to within a
relative error of 0.01% for the Ising model, and 2.5% for the
three-state Potts model respectively (Table 2.1 ), with appropriate
multiplicities for all primary fields and also the
lower-scaling-dimension secondary fields (Fig. 2.11 ).

The computed central charges of @xmath and @xmath closely reflected the
exact values of @xmath and @xmath respectively. OPEs were also computed
for the Ising model and compared with the exact figures of

  -- -- -- --------
           (2.29)
  -- -- -- --------

and permutations of the indices thereon, with errors in all values being
bounded by @xmath (Pfeifer et al., 2009 ) .

These data are easily sufficient to identify the CFTs associated with
these lattice models to a high degree of accuracy, and confirm the
hypothesis that the Scale-Invariant MERA is an effective Ansatz for the
description of quantum critical systems in one dimension. An example of
the application of the scale-invariant MERA algorithm to infinite 2D
lattice models may be found in Evenbly and Vidal ( 2009b ) .

### 2.4 Adding a Boundary

The author of this Thesis was also briefly involved in the development
of the scale-invariant MERA with a boundary described in Evenbly et al.
( 2010 ) . The concept of a boundary scale-invariant MERA was proposed
by G. Vidal, with attempts at implementation by R. N. C. Pfeifer and
G. Evenbly, and theoretical support from V. Picó, S. Iblisdir,
L. Tagliacozzo, and I. McCulloch. The author’s implementation attempted
to optimise both the bulk and the boundary of the MERA simultaneously,
and was not overly successful. It was subsequently laid aside in favour
of the implementation by G. Evenbly described in the above reference.
The interested reader is directed to this paper for further information.

## Chapter 3 Abelian Symmetries of Spin Systems

In Chapter 2 , I explained how it is possible to exploit the scale
invariance of a quantum critical system on a lattice to construct an
efficient Ansatz for the description of the low energy subspace, and how
this could be used to extract the conformal data describing the
behaviour of this system in the infra-red (large length-scale) limit.
Although other numerical techniques and different Ansätze have
previously been employed to study such systems, the approach developed
in Chapter 2 was unique in the way in which it reflects the underlying
scaling symmetry of the system. This resulted in an Ansatz which
naturally reproduced the polynomially decaying correlators of a quantum
critical system, and provided easy access to the data of the associated
conformal field theory, as well as providing a numerical description of
an infinite physical system which is remarkably compact.

But scale invariance is not the only symmetry exhibited by quantum
lattice models. Frequently a Hamiltonian will exhibit additional
internal global symmetries which may be described by a group. For
example, the Hamiltonian of the Ising model,

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

is invariant under a rotation of @xmath radians about the @xmath axis,

  -- -------- -------- -- -------
     @xmath   @xmath      (3.2)
     @xmath   @xmath      (3.3)
     @xmath   @xmath      (3.4)
     @xmath               (3.5)
  -- -------- -------- -- -------

giving it a Z @xmath symmetry. The @xmath model,

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

similarly exhibits a U(1) symmetry corresponding to invariance under
rotation by any angle about the @xmath axis, and the Heisenberg model,

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

has a U(1) symmetry about the @xmath axis for @xmath (known as the
Heisenberg @xmath model), and an SU(2) symmetry if @xmath and @xmath
(the Heisenberg @xmath model). The natural question to ask was whether
global symmetries such as these can also be exploited, either to
facilitate the study of a particular symmetry sector of the model, or
for computational advantage. (As an example of the former, in the
Heisenberg @xmath model, a U(1) subgroup of the SU(2) symmetry may be
identified with particle number, and it may be desireable to study a
lattice with a particular number of particles present. This may be
achieved approximately by using a chemical potential, but the resulting
model is still subject to fluctuations in particle number. It would be
useful to be able to fix exactly either the particle number density or
the total particle number on the lattice, and we shall see that this can
indeed be done in Sec. 3.2 .)

In this Chapter I will therefore discuss the exploitation of global
symmetries of the Hamiltonian of a quantum lattice model, with
particular attention to Abelian symmetries. This Chapter is divided into
three parts:

1.  A concise summary of how these internal symmetries manifest in
    tensor network Ansätze (Sec. 3.1 ).

2.  An example, being a self-contained presentation of the exploitation
    of U(1) symmetry in the MERA (Sec. 3.2 ).

3.  A discussion of the practicalities of how these symmetries may be
    efficiently implemented for an Abelian symmetry group, and how this
    is modified in the presence of fermionic exchange statistics (Sec.
    3.5 ).

The focus of Sec. 3.1 will be predominantly on development of the
formalisms and techniques respectively whereby the global internal
symmetries of a Hamiltonian may be exploited. Section 3.2 puts this
material into practice, presenting in considerable detail how the
formalisms of the preceding Sections are applied in the construction of
a U(1)-invariant MERA, culminating in demonstrations of both the ability
to select out any symmetry sectors of a system which may be desired, and
a substantial (approximately eight- to tenfold) decrease in
computational cost when compared with the standard MERA, confirming that
it is indeed both feasible and useful to exploit these symmetries in the
MERA. Section 3.5 provides an additional level of implementation detail
not present in Sec. 3.2 , and discusses the extension of the approach
presented here to systems of fermions, which have non-trivial exchange
statistics in addition to a @xmath parity symmetry.

Section 3.1 of this Chapter has previously been published as Singh,
Pfeifer, and Vidal , Physical Review A , 82 , 050301, 2010, © (2010) by
the American Physical Society.

Sections 3.2 – 3.4 of this Chapter have previously been published as
Singh, Pfeifer, and Vidal , Physical Review B , 83 , 115125, 2011,
© (2011) by the American Physical Society.

### 3.1 Tensor Network Decompositions in the Presence of a Global
Symmetry

Tensor network decompositions offer an efficient description of certain
many-body states of a lattice system and are the basis of a wealth of
numerical simulation algorithms. In this Section I discuss how to
incorporate a global symmetry, given by a compact, completely reducible
group @xmath , into tensor network decompositions and algorithms. This
is achieved by considering tensors that are invariant under the action
of the group @xmath . Each symmetric tensor decomposes into two types of
tensors: degeneracy tensors , containing all the degrees of freedom, and
structural tensors , which depend only on the symmetry group. In
numerical calculations, the use of symmetric tensors ensures the
preservation of the symmetry, allows selection of a specific symmetry
sector, and significantly reduces computational costs. On the other
hand, the resulting tensor network may also be interpreted as a
superposition of exponentially many spin networks. Spin networks are
used extensively in loop quantum gravity, where they represent states of
quantum geometry. This work highlights their importance also in the
context of tensor network algorithms, thus setting the stage for
cross-fertilization between these two areas of research.

#### 3.1.1 Introduction

Locality and symmetry are pivotal concepts in the formulation of
physical theories. In a quantum many-body system, locality implies that
the dynamics are governed by a Hamiltonian @xmath that decomposes as the
sum of terms involving only a small number of particles, and whose
strength decays with the distance between the particles. In turn, a
symmetry of the Hamiltonian @xmath allows us to organize the kinematic
space of the theory according to the irreducible representations of the
symmetry group.

Both symmetry and locality can be exploited to obtain a more compact
description of many-body states and to reduce computational costs in
numerical simulations. In the case of symmetries, this has long been
understood. Space symmetries, such as invariance under translations or
rotations, as well as internal symmetries, such as particle number
conservation or spin isotropy, divide the Hilbert space of the theory
into sectors labeled by quantum numbers or charges. The Hamiltonian
@xmath is by definition block-diagonal in these sectors. If, for
instance, the ground state is known to have zero momentum, it can be
obtained by just diagonalizing the (comparatively small) zero-momentum
block of @xmath .

In recent times, the far-reaching implications of locality for our
ability to describe many-body systems have also started to unfold. The
local character of the Hamiltonian @xmath limits the amount of
entanglement that low-energy states may have, and in a lattice system,
restrictions on entanglement can be exploited to succinctly describe
these states with a tensor network decomposition. Examples of tensor
network decompositions include Matrix Product States (Östlund and
Rommer, 1995 ; Fannes et al., 1992b ) , Projected Entangled-Pair States
(Verstraete and Cirac, 2004 ; Sierra and Martín-Delgado, 1998 ) , and
the MERA (Vidal, 2007a , 2008 ) . Importantly, in a lattice made of
@xmath sites, where the Hilbert space dimension grows exponentially with
@xmath , tensor network decompositions often offer an efficient
description (with costs that scale roughly as @xmath ). This allows for
scalable simulations of quantum lattice systems, even in cases that are
beyond the reach of standard Monte Carlo sampling techniques. As an
example, the MERA has been recently used to investigate ground states of
frustrated antiferromagnets (Evenbly and Vidal, 2010a ) .

In this Section we investigate how to incorporate a global symmetry into
a tensor network, so as to be able to simultaneously exploit both the
locality and the symmetries of physical Hamiltonians to describe
many-body states. Specifically, in order to represent a symmetric state
that has a limited amount of entanglement, we use a tensor network made
of symmetric tensors. This leads to an approximate , efficient
decomposition that preserves the symmetry exactly . Moreover, a more
compressed description is obtained by breaking each symmetric tensor
into several degeneracy tensors (containing all the degrees of freedom
of the original tensor) and structural tensors (completely fixed by the
symmetry). This decomposition leads to a substantial reduction in
computational costs and reveals a connection between tensor network
algorithms and the formalism of spin networks (Penrose, 1971b ) used in
loop quantum gravity (Rovelli and Smolin, 1995 ; Rovelli, 2008 ) .

In the case of an MPS, global symmetries have already been studied by
many authors in the context of both one-dimensional quantum systems and
two-dimensional classical systems (see e.g. Östlund and Rommer 1995 ;
Fannes et al. 1992b ; White 1992 ; Sierra and Nishino 1997 ; McCulloch
and Gulácsi 2002 ; McCulloch 2007 ; Singh et al. 2010b ; Pérez-García
et al. 2008 ; Sanz et al. 2009 ). An MPS is a trivalent tensor network
(i.e., each tensor has at most three indices) and symmetries are
comparatively easy to characterize. The present analysis applies to the
more challenging case of a generic tensor network decomposition (where
tensors typically have more than three indices).

#### 3.1.2 Symmetric Decomposition of a Tensor Network

We consider a lattice @xmath made of @xmath sites, where each site is
described by a complex vector space @xmath of finite dimension @xmath .
A pure state @xmath of the lattice can be expanded as

  -- -------- -- -------
     @xmath      (3.8)
  -- -------- -- -------

where @xmath denotes a basis of @xmath for site @xmath . For our
purposes, a tensor network decomposition for @xmath consists of a set of
tensors @xmath and a network pattern or graph characterized by a set of
vertices and a set of directed edges. Each tensor @xmath sits at a
vertex @xmath of the graph, and is connected with neighboring tensors by
bond indices according to the edges of the graph. The graph also
contains @xmath open edges, corresponding to the @xmath physical indices
@xmath . The @xmath coefficients @xmath are expressed as [Fig. 3.1 (i)]

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

namely as the tensor product of the tensors @xmath on all the vertices
@xmath , where the tensor trace @xmath contracts all bond indices, so
that only the physical indices @xmath remain on the r.h.s. of Eq. ( 3.9
).

We also introduce a compact, completely reducible group @xmath . This
includes finite groups as well as Lie groups such as O( @xmath ), SO(
@xmath ), U( @xmath ), and SU( @xmath ). Let @xmath be a unitary matrix
representation of @xmath on the space @xmath of one site, so that for
each @xmath , @xmath denotes a unitary matrix and @xmath . Here we are
interested in states @xmath that are invariant under transformations of
the form @xmath , ¹ ¹ 1 A set of states @xmath that transform
covariantly , @xmath , where @xmath is a unitary representation of
@xmath , can be represented by an invariant pure state @xmath of lattice
@xmath and one additional site on which the group acts with @xmath . The
same is true for an invariant mixed state @xmath , with @xmath .

  -- -------- -- --------
     @xmath      (3.10)
  -- -------- -- --------

The space @xmath of one site decomposes as the direct sum of irreducible
representations (irreps) of @xmath ,

  -- -- -- --------
           (3.11)
  -- -- -- --------

where @xmath denotes the irrep labeled with charge @xmath and @xmath is
the number of times @xmath appears in @xmath . We denote by @xmath the
charge corresponding to the trivial irrep, so that @xmath and @xmath .
In Eq. ( 3.11 ) we have also rewritten the same decomposition in terms
of a @xmath -dimensional degeneracy space @xmath . We choose a local
basis @xmath in @xmath , where @xmath labels states within the
degeneracy space @xmath (i.e. @xmath ) and @xmath labels states within
irrep @xmath . In this basis, @xmath reads

  -- -------- -- --------
     @xmath      (3.12)
  -- -------- -- --------

Recall that an operator @xmath that commutes with the group, @xmath for
all @xmath , decomposes as (Cornwell, 1997 )

  -- -------- -- --------
     @xmath      (3.13)
  -- -------- -- --------

(Schur’s lemma).

Our goal is to characterize a tensor network made of symmetric tensors,
namely, tensors that are invariant under the simultaneous action of
@xmath on all their indices. A symmetric tensor @xmath with for example,
two outgoing indices @xmath and @xmath and one incoming index @xmath ,
fulfills [Fig. 3.1 (ii)]

  -- -------- -- --------
     @xmath      (3.14)
  -- -------- -- --------

where @xmath , @xmath , and @xmath denote unitary matrix representations
of @xmath . Clearly, this choice guarantees that Eq. ( 3.10 ) is
satisfied [Fig. 3.1 (iii)]. Standard group representation theory results
(Cornwell, 1997 ) imply that each symmetric tensor can be further
decomposed in such a way that the degrees of freedom that are not fixed
by the symmetry can be isolated (Fig. 3.2 ). Next we discuss the cases
of tensors with a small number of indices. Recall that an index @xmath
of a tensor is associated with a vector space that decomposes as in
Eq. ( 3.11 ); therefore we can write @xmath , @xmath , @xmath , and so
on.

One leg.— A tensor @xmath with only one index @xmath is invariant only
if @xmath acts on it trivially, so the only relevant irrep is @xmath ,
and index @xmath labels states within the degeneracy space @xmath .

Two legs.— Schur’s lemma (Cornwell, 1997 ) establishes that a symmetric
tensor @xmath with one outgoing index @xmath and one incoming index
@xmath decomposes as [cf. Eq. ( 3.13 )]

  -- -------- -- --------
     @xmath      (3.15)
  -- -------- -- --------

Thus, for fixed values of the charges @xmath and @xmath , @xmath breaks
into a degeneracy tensor @xmath (where only @xmath is relevant) and
another tensor @xmath . @xmath contains all the degrees of freedom of
@xmath that are not fixed by the symmetry, whereas @xmath is completely
determined by @xmath . Another combination of outgoing and incoming
indices, for example two incoming indices, leads to a different form for
tensor @xmath .

Three legs.— The tensor product of two irreps with charges @xmath and
@xmath can be decomposed as the direct sum of irreps,

  -- -------- -- --------
     @xmath      (3.16)
  -- -------- -- --------

where @xmath denotes the number of copies of @xmath that appear in the
tensor product. For notational simplicity, from now on we assume that
@xmath is multiplicity-free, ² ² 2 In non-multiplicity-free groups, such
as SU(3), where @xmath might be larger than 1, the coupled basis @xmath
and tensor @xmath must include an extra index @xmath . See, for example,
Chapter 4 of this Thesis, and Pfeifer et al. ( 2010 ) . that is, @xmath
, and denote by @xmath the change of basis between the product basis
@xmath and the coupled basis @xmath . The Wigner-Eckart theorem states
that a symmetric tensor @xmath with, for example, two outgoing indices
@xmath and one incoming index @xmath , then decomposes as

  -- -------- -- --------
     @xmath      (3.17)
  -- -------- -- --------

As before, for fixed values of the charges @xmath , @xmath , and @xmath
, @xmath factorizes into degeneracy tensors @xmath with all the degrees
of freedom and structural tensors @xmath (the Clebsch-Gordan
coefficients) completely determined by the group @xmath . An analogous
decomposition with different @xmath holds for other combinations of
incoming and outgoing indices.

Four legs.— The tensor product of three irreps @xmath may contain
several copies of an irrep @xmath . Let @xmath be the charge that
results from fusing @xmath and @xmath , @xmath . We can use the values
of @xmath for which @xmath (i.e., such that @xmath and @xmath fuse to
@xmath , and @xmath and @xmath fuse to @xmath ) to label the different
copies of @xmath that appear in @xmath . Let @xmath denote the change of
basis between the product basis @xmath and the coupled basis @xmath
obtained by fusing to the intermediate basis @xmath . Then a symmetric
tensor @xmath with three outgoing indices @xmath , @xmath , and @xmath
and one incoming index @xmath decomposes as ³ ³ 3 Note that Eq. ( 3.18 )
differs from Eq. (11) of Singh et al. ( 2010a ) . This is due to an
error in the published paper, where this equation was mistakenly given
as

@xmath (11)

with explicit degeneracy indices @xmath and @xmath associated with the
intermediate charge index @xmath . For a given value of @xmath , @xmath
is non-zero for precisely one set of values @xmath , and similarly for
@xmath , @xmath , and @xmath . Consequently, to obtain the most
efficient representation of @xmath we would always evaluate the sum over
@xmath and @xmath in the above expression, reducing it to Eq. ( 3.18 ).
Similar corrections have been made to Eqs. ( 3.19 ) and ( 3.22 ).

  -- -------- -- --------
     @xmath      (3.18)
  -- -------- -- --------

where the sum is over all relevant values of the intermediate charge
@xmath . Alternatively, @xmath can be decomposed as

  -- -------- -- --------
     @xmath      (3.19)
  -- -------- -- --------

where @xmath denotes the change of basis to another coupled basis @xmath
of @xmath obtained by fusing first @xmath and @xmath into @xmath , and
then @xmath and @xmath into @xmath , involving a different intermediate
charge index @xmath . The two coupled bases are related by a unitary
transformation given by the 6-index tensor @xmath [related to the 6-
@xmath symbols for e.g. @xmath ; see Eq. ( 5.7 ) of Chapter 5 ] such
that

  -- -------- -- --------
     @xmath      (3.20)
  -- -------- -- --------

Since Eqs. ( 3.18 ) and ( 3.19 ) represent the same tensor @xmath , the
degeneracy tensors @xmath and @xmath are related by

  -- -------- -- --------
     @xmath      (3.21)
  -- -------- -- --------

More generally, a symmetric tensor @xmath with @xmath indices @xmath ,
where @xmath , decomposes as

  -- -------- -- --------
     @xmath      (3.22)
  -- -------- -- --------

where the sum is over the intermediate charges @xmath , @xmath . The
degeneracy tensors @xmath contain all the degrees of freedom of @xmath ,
whereas the structural tensors @xmath are completely determined by the
symmetry. Here @xmath are intermediate charges that decorate the inner
branches of a trivalent tree used to label a basis in the space of
intertwining operators between the tensor products of incoming and
outgoing irreps. A different choice of tree will produce different sets
of tensors @xmath and @xmath , related to @xmath and @xmath by F-moves.
⁴ ⁴ 4 When @xmath is an Abelian group, such as U(1), the tensor product
@xmath of two irreps only gives rise to one irrep @xmath , so that no
intermediate charges @xmath need to be specified in Eq. ( 3.22 ),
simplifying significantly the decomposition of symmetric tensors.

We can now investigate how the tensor network decomposes if we write
each of its tensors @xmath in the @xmath form of Eq. ( 3.22 ) (see Fig.
3.3 ). For any fixed value of all the charges, the whole tensor network
factorizes into two terms. The first one is a network of degeneracy
tensors. The second one is a directed graph with edges labeled by irreps
of @xmath and vertices labeled by intertwining operators. This is
nothing other than a spin network (Penrose, 1971b ) , a well-known
object in mathematical physics and, especially, in loop quantum gravity
(Rovelli and Smolin, 1995 ; Rovelli, 2008 ) , where it is used to
describe states of quantum geometry. Accordingly, a symmetric tensor
network for the state @xmath of a lattice @xmath of @xmath sites can be
regarded as a linear superposition of spin networks with @xmath open
edges. The number of spin networks in the linear superposition grows
exponentially with the size of the tensor network. The expansion
coefficients are given by the degeneracy tensors.

#### 3.1.3 Applications of Symmetric Tensor Networks

Computationally, the present characterization of a symmetric tensor
network is of interest for several reasons. First, it allows us to
describe a state @xmath with specific quantum numbers, which are
preserved exactly during approximate numerical simulations. Let us
consider as an example the group U(1), with charge @xmath corresponding
to particle number ( @xmath ), and the group SU(2), with charge @xmath
corresponding to the spin ( @xmath ). The symmetric tensor network can
be used to describe a state with, for example, zero particles ( @xmath )
and zero spin ( @xmath ), respectively—or, more generally, covariant
states with any value of @xmath and @xmath . ⁵ ⁵ 5 See footnote 1 ,
above.

Second, the ( @xmath )-decomposition ( 3.22 ) concentrates all the
degrees of freedom of a symmetric tensor @xmath in the degeneracy
tensors @xmath , producing a more compact description. For instance, for
the U(1) and SU(2) groups, an approximation of the ground state of the
antiferromagnetic Heisenberg spin- @xmath chain with a MERA of bond
dimension @xmath requires five and thirty-five times less parameters
than with nonsymmetric tensors, respectively (Singh et al., 2011 ; Singh
and Vidal, in preparation ) .

In addition, the ( @xmath )-decomposition ( 3.22 ) lowers the cost of
simulations significantly. Consider the multiplication of two tensors
(Fig. 3.4 ) which is central to most tensor network algorithms. Cost
reductions come from two fronts:

1.  Block-sparse matrices.— The most costly step in multiplying two
    tensors @xmath and @xmath consists of multiplying two matrices
    @xmath and @xmath obtained from @xmath and @xmath . These matrices
    are of the form of Eq. ( 3.13 ), and therefore their multiplication
    can be done blockwise:

      -- -------- -- --------
         @xmath      (3.23)
      -- -------- -- --------

2.  Pre-computation.— Given a @xmath -decomposition of tensor @xmath ,
    another ( @xmath )-decomposition (as required, e.g., to obtain the
    matrices @xmath and @xmath above) involves a linear map @xmath :

      -- -------- -- --------
         @xmath      (3.24)
      -- -------- -- --------

    This map @xmath , of which Eq. ( 3.21 ) is an example, is completely
    determined by the symmetry. In those tensor network algorithms that
    proceed by repeating a sequence of manipulations, map @xmath can be
    computed once and stored in memory for repeated usage.

More detailed explanations of algorithmic details, as well as practical
examples of the gains obtained using invariant tensors, are presented in
Singh et al. ( 2011 ) (also Sec. 3.2 of this Thesis) and Singh and Vidal
( in preparation ) for the groups U(1) and SU(2), respectively. Evenbly
and Vidal ( 2010a ) exploited the U(1) symmetry in a 2D MERA calculation
that involved tensors with up to twelve indices.

Finally, the connection between symmetric tensor networks and spin
networks allows us to import into the context of tensor network
algorithms techniques developed to evaluate spin networks in loop
quantum gravity. Such techniques can be used, for example, to compute
the linear map @xmath of Eq. ( 3.24 ). Conversely, tensor network
algorithms may also prove useful in loop quantum gravity, since they
allow (for example) the efficient manipulation of superpositions of an
exponentially large number of spin networks.

### 3.2 Example: U(1)-Symmetric MERA

Tensor network decompositions offer an efficient description of certain
many-body states of a lattice system and are the basis of a wealth of
numerical simulation algorithms. In Sec. 3.1 I discussed how to
incorporate a global internal symmetry, given by a compact, completely
reducible group @xmath , into tensor network decompositions and
algorithms. Here I specialize to the case of Abelian groups and, for
concreteness, to a U(1) symmetry, associated, e.g., with particle number
conservation. I will consider tensor networks made of tensors that are
invariant (or covariant) under the symmetry, and explain how to
decompose and manipulate such tensors in order to exploit their
symmetry. In numerical calculations, the use of U(1)-symmetric tensors
allows selection of a specific number of particles, ensures the exact
preservation of particle number, and significantly reduces computational
costs. I illustrate all these points in the context of the multi-scale
entanglement renormalization Ansatz.

#### 3.2.1 Introduction

Tensor networks are becoming increasingly popular as a tool to represent
wave-functions of quantum many-body systems. Their success is based on
the ability to efficiently describe the ground state of a broad class of
local Hamiltonians on the lattice. Tensor network states are used both
as a variational Ansatz to numerically approximate ground states and as
a theoretical framework to characterize and classify quantum phases of
matter.

Examples of tensor network states for one dimensional systems include
the Matrix Product State or MPS ⁶ ⁶ 6 Fannes et al. ( 1992a ); Östlund
and Rommer ( 1995 ); Perez-Garcia et al. ( 2007 ) , which results
naturally from both Wilson’s numerical renormalization group (Wilson,
1975 ) and White’s Density Matrix Renormalization Group (DMRG), ⁷ ⁷ 7
White ( 1992 , 1993 ); Schollwöck ( 2005a , 2011 ); McCulloch ( 2008 )
and is also used as a basis for simulation of time evolution, e.g. with
the time evolving block decimation (TEBD) algorithm (Vidal, 2003 , 2004
, 2007b ) and variations thereof, often collectively referred to as
time-dependent DMRG; ⁸ ⁸ 8 Vidal ( 2003 , 2004 ); Daley et al. ( 2004 );
White and Feiguin ( 2004 ); Schollwöck ( 2005b ); Vidal ( 2007b ) the
Tree Tensor Network (TTN) (Shi et al., 2006 ) , which follows from
coarse-graining schemes where the spins are blocked hierarchically; and
the Multi-scale Entanglement Renormalization Ansatz (MERA), ⁹ ⁹ 9 Vidal
( 2007a , 2008 ); Evenbly and Vidal ( 2009a ); Giovannetti et al. ( 2008
); Pfeifer et al. ( 2009 ); Vidal ( 2010 ) which results from a
renormalization group procedure known as entanglement renormalization
(Vidal, 2007a , 2010 ) . For two dimensional lattices there are
generalizations of these three tensor network states, namely projected
entangled pair states (PEPS), ¹⁰ ¹⁰ 10 Verstraete and Cirac ( 2004 );
Sierra and Martín-Delgado ( 1998 ); Nishino and Okunishi ( 1998 );
Nishio et al. ( 2004 ); Murg et al. ( 2007 ); Jordan et al. ( 2008 ); Gu
et al. ( 2008 ); Jiang et al. ( 2008 ); Xie et al. ( 2009 ); Murg et al.
( 2009 ) 2D TTN (Tagliacozzo et al., 2009 ; Murg et al., 2010 ) , and 2D
MERA, ¹¹ ¹¹ 11 Evenbly and Vidal ( 2010b , c ); Aguado and Vidal ( 2008
); Cincio et al. ( 2008 ); Evenbly and Vidal ( 2009b ); König et al. (
2009 ); Evenbly and Vidal ( 2010a ) respectively. As variational
Ansätze, PEPS and 2D MERA are particularly interesting since they can be
used to address large two-dimensional lattices, including systems of
frustrated spins (Murg et al., 2009 ; Evenbly and Vidal, 2010a ) and
interacting fermions, ¹² ¹² 12 Corboz et al. ( 2010a ); Kraus et al. (
2010 ); Pineda et al. ( 2010 ); Corboz and Vidal ( 2009 ); Barthel
et al. ( 2009 ); Shi et al. ( 2009 ); Li et al. ( 2010 ); Corboz et al.
( 2010b ); Pižorn and Verstraete ( 2010 ); Gu et al. ( 2010 ) where
Monte Carlo techniques fail due to the sign problem.

A many-body Hamiltonian @xmath may be invariant under transformations
that form a group of symmetries (Cornwell, 1997 ) . The symmetry group
divides the Hilbert space of the theory into symmetry sectors labeled by
quantum numbers or conserved charges. On a lattice one can distinguish
between space symmetries, which correspond to some permutation of the
sites of the lattice, and internal symmetries, which act on the vector
space of each site. An example of space symmetry is invariance under
translations by some unit cell, which leads to conservation of momentum.
An example of internal symmetry is SU(2) invariance, e.g. spin isotropy
in a quantum spin model. An internal symmetry can in turn be global , if
it transforms the space of each of the lattice sites according to the
same transformation (e.g. a spin-independent rotation); or local , if
each lattice site is transformed according to a different transformation
(e.g. a spin-dependent rotation), as it is for gauge symmetric models. A
global internal SU(2) symmetry gives rise to conservation of total spin.
By targetting a specific symmetry sector during a calculation,
computational costs can often be significantly reduced while explicitly
preserving the symmetry. It is therefore not surprising that symmetries
play an important role in numerical approaches.

In Sec. 3.1 I described a formalism for incorporating global internal
symmetries into a generic tensor network algorithm. Both Abelian and
non-Abelian symmetries were considered. The purpose of Sec. 3.2 is to
address, at a pedagogical level, the implementation of Abelian
symmetries into tensor networks. We will also discuss several more
practical aspects of the exploitation of Abelian symmetries not covered
in Sec. 3.1 . For concreteness this Section concentrates on U(1)
symmetry, but extending these results to any Abelian group is
straightforward. A similar analysis of non-Abelian groups will be
considered in Singh and Vidal ( in preparation ) , as well as being
discussed briefly in Chapter 5 of this Thesis.

In tensor network approaches, the exploitation of global internal
symmetries has a long history, especially in the context of MPS. Both
Abelian and non-Abelian symmetries have been thoroughly incorporated
into DMRG code and have been exploited to obtain computational gains. ¹³
¹³ 13 Östlund and Rommer ( 1995 ); White ( 1992 ); Schollwöck ( 2005b );
Ramasesha et al. ( 1996 ); Sierra and Nishino ( 1997 ); Tatsuaki ( 2000
); McCulloch and Gulácsi ( 2002 ); Bergkvist et al. ( 2006 ); Pittel and
Sandulescu ( 2006 ); McCulloch ( 2007 ); Pérez-García et al. ( 2008 );
Sanz et al. ( 2009 ) Symmetries have also been used in more recent
proposals to simulate time evolution with MPS. ¹⁴ ¹⁴ 14 Vidal ( 2004 );
Daley et al. ( 2004 ); White and Feiguin ( 2004 ); Schollwöck ( 2005b );
Vidal ( 2007b ); Daley et al. ( 2005 ); Danshita et al. ( 2007 ); Muth
et al. ( 2010 ); Mishmash et al. ( 2009 ); Singh et al. ( 2010b ); Cai
et al. ( 2010 )

When considering symmetries, it is important to notice that an MPS is a
trivalent tensor network. That is, in an MPS each tensor has at most
three indices. The Clebsch–Gordan coefficients, or coupling
coefficients, of a symmetry group are also trivalent (Cornwell, 1997 ) ,
and this makes incorporating the symmetry into an MPS by considering
symmetric tensors particularly simple. In contrast, tensor network
states with a more elaborate network of tensors, such as MERA or PEPS,
consist of tensors having a larger number of indices. In this case a
more general formalism is required in order to exploit the symmetry. As
explained in Sec. 3.1 , a generic symmetric tensor can be decomposed
into a degeneracy part, which contains all degrees of freedom not
determined by symmetry, and a structural part, which is completely
determined by symmetry and can be further decomposed as a trivalent
network of Clebsch–Gordan coefficients.

The use of symmetric tensors in more complex tensor networks has also
been discussed in Pérez-García et al. ( 2010 ) and Zhao et al. ( 2010 )
. In particular, Pérez-García et al. ( 2010 ) has shown that under
convenient conditions (injectivity), a PEPS that represents a symmetric
state can be represented with symmetric tensors, generalizing similar
results for MPS obtained in Pérez-García et al. ( 2008 ) . Notice that
these studies are not concerned with how to decompose symmetric tensors
so as to computationally protect or exploit the symmetry. On the other
hand, exploitation of U(1) symmetry for computational gain in the
context of PEPS was reported in Zhao et al. ( 2010 ) , although no
implementation details were provided. Finally, several aspects of local
internal symmetries in tensor network algorithms have been addressed in
Schuch et al. ( 2010 ) , Swingle and Wen ( 2010 ) , Chen et al. ( 2010b
) , and Tagliacozzo and Vidal ( 2010 ) .

The discussion of the U(1)-symmetric MERA is organized into Sections as
follows:

Section 3.2.2 contains a review of the tensor network formalism and
introduces the nomenclature and diagrammatical representation of tensors
used in the rest of the Chapter. It also describes a set @xmath of
primitives for manipulating tensor networks, consisting of manipulations
that involve a single tensor (permutation, fusion and splitting of the
indices of a tensor) and matrix operations (multiplication and
factorization).

Section 3.2.3 reviews basic notions of representation theory of the
Abelian group U(1). The action of the group is analysed first on a
single vector space, where U(1)-symmetric states and U(1)-invariant
operators are decomposed in a compact, canonical manner. This canonical
form allows us to identify the degrees of freedom which are not
constrained by the symmetry. The action of the group is then also
analysed on the tensor product of two vector spaces and, finally, on the
tensor product of a finite number of vector spaces.

Section 3.2.4 explains how to incorporate the U(1) symmetry into a
generic tensor network algorithm, by considering U(1)-invariant tensors
in a canonical form, and by adapting the set @xmath of primitives for
manipulating tensor networks. These include the multiplication of two
U(1)-invariant matrices in their canonical form, which is at the core of
the computational savings obtained by exploiting the symmetry in tensor
network algorithms.

Section 3.2.5 illustrates the practical exploitation of the U(1)
symmetry in a tensor network algorithm by presenting MERA calculations
of the ground state and low energy states of two quantum spin chain
models.

The canonical form offers a more compact description of U(1)-invariant
tensors, and leads to faster matrix multiplications and factorizations.
However, there is also an additional cost associated with maintaining an
invariant tensor in its canonical form while reshaping (fusing and/or
splitting) its indices. In some situations, this cost may offset the
benefits of using the canonical form. In Sec. 3.4 we discuss a scheme to
lower this additional cost in tensor network algorithms that are based
on iterating a repeated sequence of transformations. This is achieved by
identifying, in the manipulation of a tensor, operations which only
depend on the symmetry. Such operations can be precomputed once at the
beginning of a simulation. Their result, stored in memory, can be
re-used at each iteration of the simulation. Section 3.4 describes two
such specific precomputation schemes.

#### 3.2.2 Review: Tensor Network Formalism

In this Section we review background material concerning the formalism
of tensor networks, without reference to symmetry. We introduce basic
definitions and concepts, as well as the nomenclature and graphical
representation for tensors, tensor networks, and their manipulations,
that will be used in Sec. 3.2 .

##### 3.2.2.1 Tensors

A tensor @xmath is a multidimensional array of complex numbers @xmath .
The rank of tensor @xmath is the number @xmath of indices. For instance,
a rank-0 tensor ( @xmath ) is a complex number. Similarly, rank-1 (
@xmath ) and rank-2 ( @xmath ) tensors represent vectors and matrices,
respectively. The size of an index @xmath , denoted @xmath , is the
number of values that the index takes, @xmath . The size of a tensor
@xmath , denoted @xmath , is the number of complex numbers it contains,
namely @xmath . In this discussion of the U(1)-symmetric MERA, we will
use the hat ( @xmath ) to indicate that an object is a tensor. Vectors
are included in this convention, writing their components as, e.g.,
@xmath , although for simplicity we will omit the hat when a vector is
written in bra or ket form, e.g. @xmath .

It is convenient to use a graphical representation of tensors, as
introduced in Fig. 3.5 , where a tensor @xmath is depicted as a circle
(more generally some shape, e.g. a square) and each of its indices is
represented by a line emerging from it.

In order to specify which index corresponds to which emerging line, we
follow the prescription that the lines corresponding to indices @xmath
emerge in counterclockwise order. Unless stated otherwise, the first
index will correspond to the line emerging at nine o’clock (or the first
line encountered while proceeding counterclockwise from nine o’clock).

Two elementary ways in which a tensor @xmath can be transformed are by
permuting and reshaping its indices. A permutation of indices
corresponds to creating a new tensor @xmath from @xmath by simply
changing the order in which the indices appear, e.g.

  -- -------- -- --------
     @xmath      (3.25)
  -- -------- -- --------

On the other hand, a tensor @xmath can be reshaped into a new tensor
@xmath by “fusing” and/or “splitting” some of its indices. For instance,
in

  -- -------- -- --------
     @xmath      (3.26)
  -- -------- -- --------

tensor @xmath is obtained from tensor @xmath by fusing indices @xmath
and @xmath together into a single index @xmath of size @xmath that runs
over all pairs of values of @xmath and @xmath , i.e. @xmath , whereas in

  -- -------- -- --------
     @xmath      (3.27)
  -- -------- -- --------

tensor @xmath is recovered from @xmath by splitting index @xmath of
@xmath back into indices @xmath and @xmath . The permutation and
reshaping of the indices of a tensor have a straightforward graphical
representation; see Fig. 3.6 .

##### 3.2.2.2 Multiplication of Two Tensors

Given two matrices @xmath and @xmath with components @xmath and @xmath ,
we can multiply them together to obtain a new matrix @xmath , @xmath ,
with components

  -- -------- -- --------
     @xmath      (3.28)
  -- -------- -- --------

by summing over or contracting index @xmath . The multiplication of
matrices @xmath and @xmath is represented graphically by connecting
together the emerging lines of @xmath and @xmath corresponding to the
contracted index, as shown in Fig. 3.7 (i).

Matrix multiplication can be generalized to tensors. For instance, given
tensors @xmath and @xmath with components @xmath and @xmath , we can
define a tensor @xmath with components @xmath given by

  -- -------- -- --------
     @xmath      (3.29)
  -- -------- -- --------

Again the multiplication of two tensors can be graphically represented
by connecting together the lines corresponding to indices that are being
contracted [indices @xmath and @xmath in Eq. ( 3.29 )]; see Fig. 3.7
(ii).

The multiplication of two tensors can be broken down into a sequence of
elementary steps by transforming the tensors into matrices, multiplying
the matrices together, and then transforming the resulting matrix back
into a tensor. These steps are now described for the contraction given
in Eq. ( 3.29 ). They are illustrated in Fig. 3.8 .

1.   Permute the indices of tensor @xmath in such a way that the indices
    to be contracted, @xmath and @xmath , appear in the last positions
    and in a given order, e.g. @xmath ; similarly, permute the indices
    of @xmath so that the indices to be contracted, again @xmath and
    @xmath , appear in the first positions and in the same order @xmath
    :

      -- -------- -------- -- --------
         @xmath   @xmath      
         @xmath   @xmath      (3.30)
      -- -------- -------- -- --------

2.   Reshape tensor @xmath into a matrix @xmath by fusing into a single
    index @xmath all the indices that are not going to be contracted,
    @xmath , and into a single index @xmath all indices to be
    contracted, @xmath . Similarly, reshape tensor @xmath into a matrix
    @xmath with indices @xmath and @xmath ,

      -- -------- -------- -- --------
         @xmath   @xmath      
         @xmath   @xmath      (3.31)
      -- -------- -------- -- --------

3.   Multiply matrices @xmath and @xmath to obtain a matrix @xmath ,
    with components

      -- -------- -- --------
         @xmath      (3.32)
      -- -------- -- --------

4.   Reshape matrix @xmath into a tensor @xmath by splitting indices
    @xmath and @xmath ,

      -- -------- -- --------
         @xmath      (3.33)
      -- -------- -- --------

5.   Permute the indices of @xmath into the order in which they appear
    in @xmath ,

      -- -------- -- --------
         @xmath      (3.34)
      -- -------- -- --------

Note that breaking down a multiplication of two tensors into elementary
steps is not necessary—one can simply implement the contraction of Eq. (
3.29 ) as a single process. However, it is often more convenient to
compose the above elementary steps since, for instance, in this way one
can use existing linear algebra libraries for matrix multiplication. In
addition, it can be seen that the leading computational cost in
multiplying two large tensors is not changed when decomposing the
contraction in the above steps. In Sec. 3.2.4.9 this subject will be
discussed in more detail for U(1)-invariant tensors.

##### 3.2.2.3 Factorization of a Tensor

A matrix @xmath can be factorized into the product of two (or more)
matrices in one of several canonical forms. For instance, the singular
value decomposition

  -- -------- -- --------
     @xmath      (3.35)
  -- -------- -- --------

factorizes @xmath into the product of two unitary matrices @xmath and
@xmath , and a diagonal matrix @xmath with non-negative diagonal
elements @xmath known as the singular values of @xmath ; see Fig. 3.9
(i).

On the other hand, the eigenvalue or spectral decomposition of a square
matrix @xmath is of the form

  -- -------- -- --------
     @xmath      (3.36)
  -- -------- -- --------

where @xmath is an invertible matrix whose columns encode the
eigenvectors @xmath of @xmath ,

  -- -------- -- --------
     @xmath      (3.37)
  -- -------- -- --------

@xmath is the inverse of @xmath , and @xmath is a diagonal matrix, with
the eigenvalues @xmath on its diagonal. Other useful factorizations
include the LU decomposition, the QR decomposition, etc. We refer to any
such decomposition generically as a matrix factorization .

A tensor @xmath with more than two indices can be converted into a
matrix in several ways, by specifying how to join its indices into two
subsets. After specifying how tensor @xmath is to be regarded as a
matrix, we can factorize @xmath according to any of the above matrix
factorizations, as illustrated in Fig. 3.9 (ii) for a singular value
decomposition. This requires first permuting and reshaping the indices
of @xmath to form a matrix, then decomposing the latter, and finally
restoring the open indices of the resulting matrices into their original
form by undoing the reshapes and permutations.

##### 3.2.2.4 Tensor Networks and Their Manipulation

A tensor network @xmath is a set of tensors whose indices are connected
according to a network pattern, e.g. Fig. 3.10 .

Given a tensor network @xmath , a single tensor @xmath can be obtained
by contracting all the indices that connect the tensors in @xmath [Fig.
3.10 (ii)]. Here, the indices of tensor @xmath correspond to the open
indices of the tensor network @xmath . We then say that the network
@xmath is a tensor network decomposition of @xmath . One way to obtain
@xmath from @xmath is through a sequence of contractions involving two
tensors at a time [Fig. 3.10 (iii)].

From a tensor network decomposition @xmath for a tensor @xmath , another
tensor network decomposition for the same tensor @xmath can be obtained
in many ways. One possibility is to replace two tensors in @xmath with
the tensor resulting from contracting them together, as is done in each
step of Fig. 3.10 (iii). Another way is to replace a tensor in @xmath
with a decomposition of that tensor (e.g. with a singular value
decomposition). In this Chapter, we will be concerned with manipulations
of a tensor network that, as in the case of multiplying two tensors or
decomposing a tensor, can be broken down into a sequence of operations
from the following list:

1.  Permutation of the indices of a tensor, Eq. ( 3.25 ).

2.  Reshape of the indices of a tensor, Eqs. ( 3.26 )–( 3.27 ).

3.  Multiplication of two matrices, Eq. ( 3.28 ).

4.  Decomposition of a matrix [e.g. singular value decomposition ( 3.35
    ) or spectral decomposition ( 3.36 )].

These operations constitute a set @xmath of primitive operations for
tensor network manipulations (or, at least, for the type of
manipulations we will be concerned with).

In Sec. 3.2.4 we will discuss how this set @xmath of primitive
operations can be generalized to tensors that are symmetric under the
action of the group U(1).

##### 3.2.2.5 Tensor Network States for Quantum Many-Body Systems

As mentioned in Sec. 3.2.1 , tensor networks are used as a means to
represent the wave-function of certain quantum many-body systems on a
lattice. Let us consider a lattice @xmath made of @xmath sites, each
described by a complex vector space @xmath of dimension @xmath . A
generic pure state @xmath of @xmath can always be expanded as

  -- -------- -- --------
     @xmath      (3.38)
  -- -------- -- --------

where @xmath labels a basis @xmath of @xmath for site @xmath . Tensor
@xmath , with components @xmath , contains @xmath complex coefficients.
This is a number that grows exponentially with the size @xmath of the
lattice. Thus, the representation of a generic pure state @xmath is
inefficient . However, it turns out that an efficient representation of
certain pure states can be obtained by expressing tensor @xmath in terms
of a tensor network.

Figure 3.11 shows several popular tensor network decompositions used to
approximately describe the ground states of local Hamiltonians @xmath of
lattice models in one or two spatial dimensions.

The open indices of each of these tensor networks correspond to the
indices @xmath of tensor @xmath . Notice that all the tensor networks of
Fig. 3.11 contain @xmath tensors. If @xmath is the rank of the tensors
in one of these tensor networks, and @xmath is the size of their
indices, then the tensor network depends on @xmath complex coefficients.
For a fixed value of @xmath this number grows linearly in @xmath , and
not exponentially. It therefore does indeed offer an efficient
description of the pure state @xmath that it represents. Of course only
a subset of pure states can be decomposed in this way. Such states,
often referred to as tensor network states, are used as variational
Ansätze, with the @xmath complex coefficients as the variational
parameters.

Given a tensor network state, a variety of algorithms ¹⁵ ¹⁵ 15 see e.g.
Wilson ( 1975 ); White ( 1992 , 1993 ); Schollwöck ( 2005a , 2011 );
McCulloch ( 2008 ); Vidal ( 2003 , 2004 , 2007b ); Daley et al. ( 2004
); White and Feiguin ( 2004 ); Schollwöck ( 2005b ); Shi et al. ( 2006
); Vidal ( 2007a , 2008 ); Evenbly and Vidal ( 2009a ); Giovannetti
et al. ( 2008 ); Pfeifer et al. ( 2009 ); Vidal ( 2010 ); Verstraete and
Cirac ( 2004 ); Sierra and Martín-Delgado ( 1998 ); Nishino and Okunishi
( 1998 ); Nishio et al. ( 2004 ); Murg et al. ( 2007 ); Jordan et al. (
2008 ); Gu et al. ( 2008 ); Jiang et al. ( 2008 ); Xie et al. ( 2009 );
Murg et al. ( 2009 ); Tagliacozzo et al. ( 2009 ); Murg et al. ( 2010 );
Evenbly and Vidal ( 2010b , c ); Aguado and Vidal ( 2008 ); Cincio
et al. ( 2008 ); Evenbly and Vidal ( 2009b ); König et al. ( 2009 );
Evenbly and Vidal ( 2010a ); Corboz et al. ( 2010a ); Kraus et al. (
2010 ); Pineda et al. ( 2010 ); Corboz and Vidal ( 2009 ); Barthel
et al. ( 2009 ); Shi et al. ( 2009 ); Li et al. ( 2010 ); Corboz et al.
( 2010b ); Gu et al. ( 2010 ) . are used for tasks such as:
(i) computation of the expectation value @xmath of a local observable
@xmath , (ii) optimization of the variational parameters so as to
minimize the expectation value of the energy @xmath , or
(iii) simulation of time evolution, e.g. @xmath . These tasks are
accomplished by manipulating tensor networks.

On most occasions, all required manipulations can be reduced to a
sequence of primitive operations in the set @xmath introduced in Sec.
3.2.2.4 . Thus, in order to adapt tensor network algorithms such as
those listed in footnote 15 , above, to the presence of a symmetry, we
only need to modify the set @xmath of primitive tensor network
operations. This will be done in Sec. 3.2.4 .

##### 3.2.2.6 Tensors as Linear Maps

A tensor can be used to define a linear map between vector spaces in the
following way. First, notice that an index @xmath can be used to label a
basis @xmath of a complex vector space @xmath of dimension @xmath . On
the other hand, given a tensor @xmath of rank @xmath , we can attach a
direction “in” or “out” to each index @xmath . This direction divides
the indices of @xmath into the subset @xmath of incoming indices and the
subset @xmath of outgoing indices. We can then build input and output
vector spaces given by the tensor product of the spaces of incoming and
outgoing indices,

  -- -------- -- --------
     @xmath      (3.39)
  -- -------- -- --------

and use tensor @xmath to define a linear map between @xmath and @xmath .
For instance, if a rank-3 tensor @xmath has one incoming index @xmath
and two outgoing indices @xmath , then it defines a linear map @xmath
given by

  -- -------- -- --------
     @xmath      (3.40)
  -- -------- -- --------

Graphically, we denote the direction of an index by means of an arrow;
see Fig. 3.12 (i).

By decorating the lines of a tensor network @xmath with arrows [Fig.
3.12 (ii)], this can be regarded as a composition of linear maps—namely,
one linear map for each tensor in @xmath . While arrows might be of
limited relevance in the absence of a symmetry, they will play an
important role when we consider symmetric tensors since they specify how
the group acts on each index of a given tensor.

#### 3.2.3 Review: Representation Theory of the Group U(1)

In this Section we review basic background material concerning the
representation theory of the group U(1). We first consider the action of
U(1) on a vector space @xmath , which decomposes into the direct sum of
(possibly degenerate) irreducible representations. We then consider
vectors of @xmath that are symmetric (invariant or covariant) under the
action of U(1), as well as linear operators that are U(1)-invariant.
Then we consider the action of U(1) on the tensor product of two vector
spaces, and its generalization to the tensor product of an arbitrary
number of vector spaces.

##### 3.2.3.1 Decomposition Into Direct Sum of Irreducible
Representations

Let @xmath be a finite-dimensional space and let @xmath label a set of
linear transformations @xmath ,

  -- -------- -- --------
     @xmath      (3.41)
  -- -------- -- --------

that are a unitary representation of the group U(1). That is,

  -- -------- -------- -- --------
     @xmath   @xmath      (3.42)
     @xmath   @xmath      (3.43)
  -- -------- -------- -- --------

Then @xmath decomposes as the direct sum of (possibly degenerate)
one-dimensional irreducible representations (or irreps ) of U(1),

  -- -------- -- --------
     @xmath      (3.44)
  -- -------- -- --------

where @xmath is a subspace of dimension @xmath , made of @xmath copies
of an irrep of U(1) with charge @xmath . We say that irrep @xmath is
@xmath -fold degenerate and that @xmath is the degeneracy space. For
concreteness, in this Section we identify the integer charge @xmath as
labelling the number of particles (another frequent identification is
with the @xmath component of the spin, in which case semi-integer
numbers may be considered). The representation of group U(1) is
generated by the particle number operator @xmath ,

  -- -------- -- --------
     @xmath      (3.45)
  -- -------- -- --------

where @xmath is a projector onto the subspace @xmath of particle number
@xmath , and the vectors @xmath ,

  -- -------- -- --------
     @xmath      (3.46)
  -- -------- -- --------

are an orthonormal basis of @xmath . In terms of @xmath , the
transformations @xmath read

  -- -------- -- --------
     @xmath      (3.47)
  -- -------- -- --------

It then follows from Eq. ( 3.46 ) that

  -- -------- -- --------
     @xmath      (3.48)
  -- -------- -- --------

The dual basis @xmath is transformed by the dual representation of U(1),
with elements @xmath , as

  -- -------- -- --------
     @xmath      (3.49)
  -- -------- -- --------

Example 1: Consider a two-dimensional space @xmath that decomposes as
@xmath , where the irreps @xmath and @xmath are non-degenerate (i.e.
@xmath ). Then the orthogonal vectors @xmath form a basis of @xmath . In
column vector notation,

  -- -------- -- --------
     @xmath      (3.50)
  -- -------- -- --------

the particle number operator @xmath and transformation @xmath read

  -- -------- -- --------
     @xmath      (3.51)
  -- -------- -- --------

Example 2: Consider a four-dimensional space @xmath that decomposes as
@xmath , where @xmath and @xmath , so that now irrep @xmath is twofold
degenerate. Let @xmath form a basis of @xmath . In column vector
notation,

  -- -------- -- --------
     @xmath      (3.52)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (3.53)
  -- -------- -- --------

the particle number operator @xmath and transformation @xmath read

  -- -------- -- --------
     @xmath      (3.54)
  -- -------- -- --------

##### 3.2.3.2 Symmetric States and Operators

In this work we are interested in states and operators that have a
simple transformation rule under the action of U(1). A pure state @xmath
is symmetric if it transforms as

  -- -------- -- --------
     @xmath      (3.55)
  -- -------- -- --------

The case @xmath corresponds to an invariant state, @xmath , which
transforms trivially under U(1), whereas for @xmath the state is
covariant , with @xmath being multiplied by a non-trivial phase @xmath .
Notice that a symmetric state @xmath is an eigenstate of @xmath : that
is, it has a well-defined particle number @xmath . @xmath can thus be
expanded in terms of a basis of the relevant subspace @xmath ,

  -- -------- -- --------
     @xmath      (3.56)
  -- -------- -- --------

where we have introduced a charge label @xmath on the state coefficients
of @xmath so that we can explicitly associate each coefficient @xmath
with its corresponding basis vector @xmath .

A linear operator @xmath is invariant if it commutes with the generator
@xmath ,

  -- -------- -- --------
     @xmath      (3.57)
  -- -------- -- --------

or equivalently if it commutes with the action of the group,

  -- -------- -- --------
     @xmath      (3.58)
  -- -------- -- --------

It follows that @xmath decomposes as (Schur’s lemma)

  -- -------- -- --------
     @xmath      (3.59)
  -- -------- -- --------

where @xmath is a @xmath matrix that acts on the subspace @xmath in
Eq. ( 3.44 ).

Notice that the operator @xmath in Eq. ( 3.59 ) transforms vectors with
a well-defined particle number @xmath into vectors with the same
particle number. That is, U(1)-invariant operators conserve particle
number .

Example 1 revisited: In Example 1 above, symmetric vectors must be
proportional to either @xmath or @xmath . An invariant operator @xmath
is of the form

  -- -------- -- --------
     @xmath      (3.60)
  -- -------- -- --------

Example 2 revisited: In Example 2 above, a symmetric vector @xmath must
be of the form

  -- -------- -- --------
     @xmath      (3.61)
  -- -------- -- --------

where @xmath . An invariant operator @xmath is of the form

  -- -------- -- --------
     @xmath      (3.62)
  -- -------- -- --------

where @xmath corresponds to the @xmath central block and @xmath .

The above examples illustrate that the symmetry imposes constraints on
vectors and operators. By using an eigenbasis @xmath of the particle
number operator @xmath , these constraints imply the presence of the
zeros in Eqs. ( 3.60 )–( 3.62 ). Thus, a reduced number of complex
coefficients is required in order to describe U(1)-symmetric vectors and
operators. As we will discuss in Sec. 3.2.4 , performing manipulations
on symmetric tensors can also result in a significant reduction in
computational costs.

##### 3.2.3.3 Tensor Product of Two Representations

Let @xmath and @xmath be two spaces that carry representations of U(1),
as generated by particle number operators @xmath and @xmath , and let

  -- -------- -- --------
     @xmath      (3.63)
  -- -------- -- --------

be their decompositions as a direct sum of (possibly degenerate) irreps.
Let us also consider the action of U(1) on the tensor product @xmath as
generated by the total particle number operator

  -- -------- -- --------
     @xmath      (3.64)
  -- -------- -- --------

that is, implemented by unitary transformations

  -- -------- -- --------
     @xmath      (3.65)
  -- -------- -- --------

The space @xmath also decomposes as the direct sum of (possibly
degenerate) irreps,

  -- -------- -- --------
     @xmath      (3.66)
  -- -------- -- --------

Here the subspace @xmath , with total particle number @xmath ,
corresponds to the direct sum of all products of subspaces @xmath and
@xmath such that @xmath ,

  -- -------- -- --------
     @xmath      (3.67)
  -- -------- -- --------

For each subspace @xmath in Eq. ( 3.66 ) we introduce a coupled basis
@xmath ,

  -- -------- -- --------
     @xmath      (3.68)
  -- -------- -- --------

where each vector @xmath corresponds to the tensor product @xmath of a
unique pair of vectors @xmath and @xmath , with @xmath . Let table
@xmath , with components

  -- -------- -- --------
     @xmath      (3.69)
  -- -------- -- --------

encode this one-to-one correspondence. Notice that each component of
@xmath is either a 0 or a 1. Then

  -- -------- -- --------
     @xmath      (3.70)
  -- -------- -- --------

For later reference (Sec. 3.4 ), we notice that @xmath can be decomposed
into two pieces. The first piece expresses a basis @xmath of @xmath in
terms of the basis @xmath of @xmath and the basis @xmath of @xmath .
This assignment occurs as in the absence of the symmetry, where one
creates a composed index @xmath by running, for example, fast over index
@xmath and slowly over index @xmath as in Eq. ( 3.26 ). Note that this
procedure does not always lead to the set @xmath being ordered such that
states corresponding to the same total particle number @xmath are
adjacent to each other within the set. This ordering is achieved by the
second piece: a permutation of basis elements that reorganizes them
according to their total particle number @xmath , so that they are
identified in an one-to-one correspondence with the coupled states
@xmath .

Finally, the product basis can be expressed in terms of the coupled
basis

  -- -------- -- --------
     @xmath      (3.71)
  -- -------- -- --------

with

  -- -------- -- --------
     @xmath      (3.72)
  -- -------- -- --------

Example 3: Consider the case where both @xmath and @xmath correspond to
the space of Example 1, that is @xmath and @xmath , where @xmath ,
@xmath , @xmath , and @xmath all have dimension 1. Then @xmath
corresponds to the space in Example 2, namely

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (3.73)
  -- -------- -------- -- --------

where

  -- -------- -------- -- --------
     @xmath   @xmath      (3.74)
     @xmath   @xmath      (3.75)
     @xmath   @xmath      (3.76)
  -- -------- -------- -- --------

The coupled basis @xmath reads

  -- -------- -- --------
     @xmath      
     @xmath      
     @xmath      
     @xmath      (3.77)
  -- -------- -- --------

where we emphasize that the degeneracy index @xmath takes two possible
values for @xmath , i.e. @xmath , since there are two states @xmath with
@xmath . The components @xmath of the tensor @xmath that encodes this
change of basis are all zero except for

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

##### 3.2.3.4 Lattice Models With U(1) Symmetry

The action of U(1) on the threefold tensor product

  -- -------- -- --------
     @xmath      (3.78)
  -- -------- -- --------

as generated by the total particle number operator

  -- -------- -- --------
     @xmath      (3.79)
  -- -------- -- --------

induces a decomposition

  -- -------- -- --------
     @xmath      (3.80)
  -- -------- -- --------

in terms of irreps @xmath which we can now relate to @xmath , @xmath and
@xmath . For example, we can consider first the product @xmath and then
the product @xmath , using a different table @xmath at each step to
relate the coupled basis to the product basis as discussed in the
previous Section. Similarly we could consider the action of U(1) on four
tensor products, and so on.

In particular we will be interested in a lattice @xmath made of @xmath
sites with vector space @xmath , where for simplicity we will assume
that each site @xmath is described by the same finite-dimensional vector
space @xmath (see Sec. 3.2.2.5 ). Given a particle number operator
@xmath defined on each site, we can consider the action of U(1)
generated by the total particle number operator

  -- -------- -- --------
     @xmath      (3.81)
  -- -------- -- --------

which corresponds to unitary transformations

  -- -------- -- --------
     @xmath      (3.82)
  -- -------- -- --------

The tensor product space @xmath decomposes as

  -- -------- -- --------
     @xmath      (3.83)
  -- -------- -- --------

and we denote by @xmath the particle number basis in @xmath .

We say that a lattice model is U(1)-symmetric if its Hamiltonian @xmath
commutes with the action of the group. That is,

  -- -------- -- --------
     @xmath      (3.84)
  -- -------- -- --------

or equivalently

  -- -------- -- --------
     @xmath      (3.85)
  -- -------- -- --------

One example of a U(1)-symmetric model is the hard core Bose–Hubbard
model, with Hamiltonian

  -- -------- -- --------
     @xmath      (3.86)
  -- -------- -- --------

where we consider periodic boundary conditions (by identifying sites
@xmath and @xmath ), and @xmath and @xmath are hard-core bosonic
creation and annihilation operators, respectively. In terms of the basis
introduced in Example 1 these operators are defined as

  -- -------- --
     @xmath   
  -- -------- --

To see that @xmath commutes with the action of the group, we first
observe that for two sites

  -- -------- -- --------
     @xmath      (3.87)
  -- -------- -- --------

from which it readily follows that @xmath .

Notice that the chemical potential term @xmath also commutes with the
rest of the Hamiltonian. The ground state @xmath of @xmath in a
particular subspace @xmath or particle number sector can be turned into
the absolute ground state by tuning the chemical potential @xmath . This
fact can be used to find the ground state @xmath of any particle number
sector through an algorithm which can only minimize the expectation
value of @xmath . However, we will later see that the use of symmetric
tensors in the context of tensor network states will allow us to
directly minimize the expectation value of @xmath in a given particle
number sector by restricting the search to states

  -- -------- -- --------
     @xmath      (3.88)
  -- -------- -- --------

with the desired particle number @xmath .

Finally, by making the identifications

  -- -------- --
     @xmath   
  -- -------- --

where @xmath are the Pauli matrices

  -- -------- -- --------
     @xmath      (3.89)
  -- -------- -- --------

one can map @xmath to the spin- @xmath @xmath quantum spin chain

  -- -------- -- --------
     @xmath      (3.90)
  -- -------- -- --------

where we have ignored terms proportional to @xmath and set @xmath . In
particular, for @xmath we obtain the quantum @xmath spin chain

  -- -------- -- --------
     @xmath      (3.91)
  -- -------- -- --------

and for @xmath , the quantum Heisenberg spin chain

  -- -------- -- --------
     @xmath      (3.92)
  -- -------- -- --------

In Sec. 3.2.5 , the quantum spin models ( 3.91 ) and ( 3.92 ) will be
used to benchmark the performance increase resulting from the use of
symmetries in tensor networks algorithms.

#### 3.2.4 Tensor Networks With U(1) Symmetry

In this Section we will consider U(1)-symmetric tensors and tensor
networks. I will explain how to decompose U(1)-symmetric tensors in a
compact, canonical form that exploits their symmetry, and then discuss
how to adapt the set @xmath of primitives for tensor network
manipulations in order to work in this form. We will also analyse how
working in the canonical form affects computational costs.

##### 3.2.4.1 U(1)-Symmetric Tensors

Let @xmath be a rank- @xmath tensor with components @xmath . As in Sec.
3.2.2.6 , we regard tensor @xmath as a linear map between the vector
spaces @xmath and @xmath ( 3.39 ). This implies that each index is
either an incoming or outgoing index. On each space @xmath , associated
with index @xmath , we introduce a particle number operator @xmath that
generates a unitary representation of U(1) given by matrices @xmath ,
@xmath . In the following, we use @xmath to denote the complex conjugate
of @xmath .

Let us consider the action of U(1) on the space

  -- -------- -- --------
     @xmath      (3.93)
  -- -------- -- --------

given by

  -- -------- -- --------
     @xmath      (3.94)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (3.95)
  -- -------- -- --------

That is, @xmath acts differently depending on whether index @xmath of
tensor @xmath is an incoming or outgoing index. We then say that tensor
@xmath , with components @xmath , is U(1)-invariant if it is invariant
under the transformation of Eq. ( 3.94 ),

  -- -------- -- --------
     @xmath      (3.96)
  -- -------- -- --------

for all @xmath . This is depicted in Fig. 3.13 .

Example 4: A U(1)-invariant vector @xmath —that is, a vector with @xmath
and components @xmath in the subspace @xmath which corresponds to
vanishing particle number @xmath [cf. Eq. ( 3.56 )]—fulfills

  -- -------- -- --------
     @xmath      (3.97)
  -- -------- -- --------

in accordance with Eq. ( 3.55 ), as shown in Fig. 3.13 (i).

Example 5: A U(1)-invariant matrix @xmath ( 3.59 ) fulfills

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (3.98)
              @xmath   @xmath      (3.99)
  -- -------- -------- -------- -- --------

in accordance with Eq. ( 3.58 ) [see Fig. 3.13 (ii)].

Example 6: Tensor @xmath in Eq. ( 3.40 ), with components @xmath where
@xmath and @xmath are outgoing indices and @xmath is an incoming index,
is U(1)-invariant iff

  -- -------- -------- -------- -- ---------
     @xmath   @xmath   @xmath      (3.100)
              @xmath   @xmath      (3.101)
  -- -------- -------- -------- -- ---------

for all @xmath [see Fig. 3.13 (iii)].

Further, we say that a tensor @xmath with components @xmath is
U(1)-covariant if under the transformation of Eq. ( 3.94 ) it acquires a
non-trivial phase @xmath ,

  -- -------- -- ---------
     @xmath      (3.102)
  -- -------- -- ---------

for all @xmath .

Example 7: A U(1)-covariant vector @xmath —that is, one which satisfies
@xmath for some @xmath , and has nonzero components @xmath only in the
relevant subspace @xmath [cf. Eq. ( 3.56 )]—fulfills

  -- -------- -- ---------
     @xmath      (3.103)
  -- -------- -- ---------

in accordance with Eq. ( 3.55 ). (See also Fig. 3.14 .)

Notice that we can describe the rank- @xmath covariant tensor @xmath
above by a rank- @xmath invariant tensor @xmath with components

  -- -------- -- ---------
     @xmath      (3.104)
  -- -------- -- ---------

This is built from @xmath by adding an extra incoming index @xmath ,
where index @xmath has fixed particle number @xmath and no degeneracy
(i.e., @xmath is associated to a trivial space @xmath ). We refer to
both invariant and covariant tensors as symmetric tensors. By using the
above construction, in this work we will represent all U(1)-symmetric
tensors by means of U(1)-invariant tensors. In particular, we represent
the non-trivial components @xmath of the covariant vector @xmath in
Eqs. ( 3.55 )–( 3.56 ) as an invariant matrix @xmath of size @xmath with
components @xmath . Consequently, from now on we will mostly consider
only invariant tensors.

##### 3.2.4.2 Canonical Form For U(1)-Invariant Tensors

Let us now write a tensor @xmath in a particle number basis on each
factor space in Eq. ( 3.93 ). That is, each index @xmath , @xmath ,
@xmath , @xmath is decomposed into a particle number index @xmath and a
degeneracy index @xmath , @xmath , @xmath , @xmath , @xmath , and

  -- -------- -- ---------
     @xmath      (3.105)
  -- -------- -- ---------

Here, for each set of particle numbers @xmath we regard @xmath as a
tensor with components @xmath . Let @xmath and @xmath denote the sum of
particle numbers corresponding to incoming and outgoing indices,

  -- -------- -- ---------
     @xmath      (3.106)
  -- -------- -- ---------

The condition for a non-vanishing tensor of the form @xmath to be
invariant under U(1), Eq. ( 3.94 ), is simply that the sum of incoming
particle numbers equals the sum of outgoing particle numbers. Therefore,
a U(1)-invariant tensor @xmath satisfies

  -- -------- -- ---------
     @xmath      (3.107)
  -- -------- -- ---------

[We use the direct sum symbol @xmath to denote that the different
tensors @xmath are supported on orthonormal subspaces of the tensor
product space of Eq. ( 3.93 ).] In components, the above expression
reads

  -- -------- -- ---------
     @xmath      (3.108)
  -- -------- -- ---------

Here, @xmath implements particle number conservation: if @xmath , then
all components of @xmath must vanish. This generalizes the block
structure of U(1)-invariant matrices in Eq. ( 3.59 ) (where @xmath is
denoted @xmath ) to tensors of arbitrary rank @xmath . The canonical
decomposition in Eq. ( 3.107 ) is important, in that it allows us to
identify the degrees of freedom of tensor @xmath that are not determined
by the symmetry. Expressing tensor @xmath in terms of the tensors @xmath
with @xmath ensures that we store @xmath in the most compact way
possible.

Notice that the canonical form of Eq. ( 3.107 ) is a particular case of
the canonical form presented in Eq. ( 3.18 ) of Sec. 3.1.2 for more
general (possibly non-Abelian) symmetry groups. There, a symmetric
tensor was decomposed into degeneracy tensors [analogous to tensors
@xmath in Eq. ( 3.107 )] and structural tensors [generalizing the term
@xmath in Eq. ( 3.107 )] which can in general be expanded as a trivalent
network of Clebsch–Gordan (or coupling) coefficients of the symmetry
group. In the case of non-Abelian groups, where some irreps have
dimension larger than 1, the structural tensors are highly non-trivial.
However, for the group U(1) discussed in this Section (as for any other
Abelian group) all irreps are one dimensional and the structural tensors
are always reduced to a simple expression such as @xmath in Eq. ( 3.107
). (Nevertheless, in Sec. 3.4 we will resort to a more elaborate
decomposition of the structural tensors in order to better exploit the
presence of symmetry in those tensor network algorithms based on
iterating a fixed sequence of manipulations.)

##### 3.2.4.3 U(1)-Symmetric Tensor Networks

In Sec. 3.2.2.6 we saw that a tensor network @xmath where each line has
a direction (represented with an arrow) can be interpreted as a
collection of linear maps composed into a single linear map @xmath of
which @xmath is a tensor network decomposition. By introducing a
particle number operator on the vector space associated to each line of
@xmath , we can define a unitary representation of U(1) on each index of
each tensor in @xmath . Then we say that @xmath is a U(1)-invariant
tensor network if all its tensors are U(1)-invariant. Notice that, by
construction, if @xmath is a U(1)-invariant tensor network, then the
resulting linear map @xmath is also U(1)-invariant. This is illustrated
in Fig. 3.15 .

More generally, we can consider a U(1)-symmetric tensor network, made of
tensors that are U(1)-symmetric (that is, either invariant or
covariant). Recall, however, that any covariant tensor can be
represented as an invariant tensor by adding an extra index ( 3.104 ).
Therefore without loss of generality we can restrict our attention to
invariant tensor networks.

##### 3.2.4.4 Tensor Network States and Algorithms With U(1) Symmetry

As discussed in Sec. 3.2.2.5 , a tensor network @xmath can be used to
describe certain pure states @xmath of a lattice @xmath . If @xmath is a
U(1)-symmetric tensor network then it will describe a pure state @xmath
that has a well-defined total particle number @xmath . That is, a
U(1)-symmetric pure state

  -- -------- -- ---------
     @xmath      (3.109)
  -- -------- -- ---------

In this way we can obtain a more refined version of popular tensor
network states such as MPS, TTN, MERA, PEPS, etc. As a variational
Ansatz, a symmetric tensor network state is more constrained than a
regular tensor network state, and consequently it can represent less
states @xmath . However, it also depends on fewer parameters. This
implies a more economical description, as well as the possibility of
reducing computational costs during its manipulation.

The rest of this Section is devoted to explaining how one can achieve a
reduction in computational costs. This is based on storing and
manipulating U(1)-invariant tensors expressed in the canonical form of
Eqs. ( 3.107 )–( 3.108 ). We next explain how to adapt the set @xmath of
four primitive operations for the tensor network manipulations discussed
in Sect 3.2.2.4 , namely permutation and reshaping of indices, matrix
multiplication, and factorization.

##### 3.2.4.5 Permutation of Indices

Given a U(1)-invariant tensor @xmath expressed in the canonical form of
Eqs. ( 3.107 )–( 3.108 ), permuting two of its indices is
straightforward. It is achieved by swapping the position of the two
particle numbers of @xmath involved, and also the corresponding
degeneracy indices. For instance, if the rank- @xmath tensor @xmath of
Eq. ( 3.40 ) is U(1)-invariant and has components

  -- -------- -- ---------
     @xmath      (3.110)
  -- -------- -- ---------

when expressed in the particles number basis @xmath , @xmath , @xmath ,
then tensor @xmath of Eq. ( 3.25 ), obtained from @xmath by permuting
the last two indices, has components

  -- -------- -- ---------
     @xmath      (3.111)
  -- -------- -- ---------

where

  -- -------- -- ---------
     @xmath      (3.112)
  -- -------- -- ---------

Notice that since we only need to permute the components of those @xmath
such that @xmath , implementing the permutation of indices requires less
computation time than a regular index permutation. This is shown in Fig.
3.16 , corresponding to a permutation of indices using matlab .

##### 3.2.4.6 Reshaping of Indices

The indices of a U(1)-invariant tensor can be reshaped (fused or split)
in a similar manner to those of a regular tensor. However, maintaining
the convenient canonical form of Eqs. ( 3.107 )–( 3.108 ) requires
additional steps. Two adjacent indices can be fused together using the
table @xmath of Eq. ( 3.69 ), which is a sparse tensor made of ones and
zeros. Similarly an index can be split into two adjacent indices by
using its inverse, the sparse tensor @xmath of Eq. ( 3.72 ).

Example 8: Let us consider again the rank- @xmath tensor @xmath of Eq. (
3.40 ) with components given by Eq. ( 3.110 ), where @xmath and @xmath
are outgoing indices and @xmath is an incoming index. We can fuse
outgoing index @xmath and incoming index @xmath into an (e.g. incoming)
index @xmath , obtaining a new tensor @xmath with components

  -- -------- -- ---------
     @xmath      (3.113)
  -- -------- -- ---------

where @xmath . (The sign in front of @xmath comes from the fact that
@xmath is an incoming index and @xmath is an outgoing index.) The
components of @xmath are in one-to-one correspondence with those of
@xmath and follow from the transformation

  -- -------- -- ---------
     @xmath      (3.114)
  -- -------- -- ---------

where only the case @xmath needs to be considered. To complete the
example, let us assume that the index @xmath is described by the vector
space @xmath with degeneracies @xmath , @xmath , and @xmath ; index
@xmath is described by a vector space @xmath without degeneracies, i.e.
@xmath ; and index @xmath is described by a vector space @xmath also
without degeneracies, @xmath . Then @xmath (and in this example, also
@xmath ) and Eq. ( 3.114 ) amounts to

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

where we notice that tensor @xmath is a matrix as in Eq. ( 3.62 ).
Similarly, we can split incoming index @xmath of tensor @xmath back into
outgoing index @xmath and incoming index @xmath of tensor @xmath
according to

  -- -------- -- ---------
     @xmath      (3.115)
  -- -------- -- ---------

which, again, is non-trivial only for @xmath and @xmath .

This example illustrates that fusing and splitting indices while
maintaining the canonical form of Eqs. ( 3.107 )–( 3.108 ) requires more
work than reshaping regular indices. Indeed, after taking indices @xmath
and @xmath into @xmath by listing all pairs of values @xmath , we still
need to reorganize the resulting basis elements according to their
particle number @xmath . Although this can be done by following the
simple table given by @xmath , it may add significantly to the overall
computational cost associated with reshaping a tensor. For instance,
Fig. 3.16 shows that fusing indices of invariant tensors can be more
expensive than fusing indices of regular tensors.

##### 3.2.4.7 Multiplication of Two Matrices

By permuting and reshaping the indices of a U(1)-invariant tensor, we
can convert it into a U(1)-invariant matrix @xmath , or simply

  -- -------- -- ---------
     @xmath      (3.116)
  -- -------- -- ---------

where @xmath . In components, matrix @xmath reads

  -- -------- -- ---------
     @xmath      (3.117)
  -- -------- -- ---------

where @xmath and @xmath . In particular, similar to the discussion in
Sec. 3.2.2.2 for regular tensors, the multiplication of two tensors
invariant under the action of U(1) can be reduced to the multiplication
of two U(1)-invariant matrices.

Let @xmath and @xmath be two U(1)-invariant matrices, with canonical
forms

  -- -------- -- ---------
     @xmath      (3.118)
  -- -------- -- ---------

Their product @xmath , ( 3.28 ), is then another matrix @xmath which is
also block diagonal,

  -- -------- -- ---------
     @xmath      (3.119)
  -- -------- -- ---------

such that each block @xmath is obtained by multiplying the corresponding
blocks @xmath and @xmath ,

  -- -------- -- ---------
     @xmath      (3.120)
  -- -------- -- ---------

Equations ( 3.116 ) and ( 3.120 ) make evident the potential reduction
of computational costs that can be achieved by manipulating
U(1)-invariant matrices in their canonical form. First, a reduction in
memory space follows from only having to store the diagonal blocks in
Eq. ( 3.116 ). Second, a reduction in computational time is implied by
only having to multiply these blocks in Eq. ( 3.120 ). This is
illustrated in the following example.

Example 9: Consider a U(1)-invariant matrix @xmath which is a linear map
in a space @xmath that decomposes into @xmath irreps @xmath , each of
which has the same degeneracy @xmath . That is, @xmath is a square
matrix of dimensions @xmath , with the block-diagonal form of Eq. (
3.116 ). Since there are @xmath blocks @xmath and each block has size
@xmath , the U(1)-invariant matrix @xmath contains @xmath coefficients.
For comparison, a regular matrix of the same size contains @xmath
coefficients, a number greater by a factor of @xmath .

Let us now consider multiplying two such matrices. We use an algorithm
that requires @xmath computational time to multiply two matrices of size
@xmath . The cost of performing @xmath multiplications of @xmath blocks
in Eq. ( 3.120 ) scales as @xmath . In contrast the cost of multiplying
two regular matrices of the same size scales as @xmath , requiring
@xmath times more computation time.

Figure 3.17 shows a comparison of computation times when multiplying two
matrices for both U(1)-symmetric and regular matrices.

##### 3.2.4.8 Factorization of a Matrix

The factorization of a U(1)-invariant matrix @xmath ( 3.116 ) can also
benefit from the block-diagonal structure. Consider, for instance, the
singular value decomposition @xmath of Eq. ( 3.35 ). In this case we can
obtain the matrices

  -- -------- -- ---------
     @xmath      (3.121)
  -- -------- -- ---------

by performing the singular value decomposition of each block @xmath
independently,

  -- -------- -- ---------
     @xmath      (3.122)
  -- -------- -- ---------

The computational savings are analogous to those described in Example 9
above for the multiplication of matrices. Figure 3.17 also shows a
comparison of computation times required to perform a singular value
decomposition on U(1)-invariant and regular matrices using matlab .

##### 3.2.4.9 Discussion

In this Section we have seen that U(1)-invariant tensors can be written
in the canonical form of Eqs. ( 3.107 )–( 3.108 ), and that this
canonical form is of interest because it offers a compact description in
terms of only those coefficients which are not constrained by the
symmetry. We have also seen that maintaining the canonical form during
tensor manipulations adds some computational overhead when reshaping
(fusing or splitting) indices, but reduces computation time when
permuting indices (for sufficiently large tensors) and when multiplying
or factorizing matrices (for sufficiently large matrix sizes).

The cost of reshaping and permuting indices is proportional to the size
@xmath of the tensors, whereas the cost of multiplying and factorizing
matrices is a larger power of the matrix size, for example @xmath . The
use of the canonical form when manipulating large tensors therefore
frequently results in an overall reduction in computation time, making
it a very attractive option in the context of tensor network algorithms.
This is exemplified in the next Section, where we apply the MERA to
study the ground state of quantum spin models with a U(1) symmetry.

On the other hand, however, the cost of maintaining invariant tensors in
the canonical form becomes more relevant when dealing with smaller
tensors. In the next Section we will also see that in some situations,
this additional cost may significantly reduce, or even offset, the
benefits of using the canonical form. In this event, and in the specific
context of algorithms where the same tensor manipulations are iterated
many times, it is possible to significantly decrease the additional cost
by precomputing the parts of the tensor manipulations that are repeated
on each iteration. Precomputation schemes are described in more detail
in Sec. 3.4 . Their performance is illustrated in the next Section.

#### 3.2.5 Tensor Network Algorithms With U(1) Symmetry: A Practical
Example

In previous Sections we have described a strategy to incorporate a U(1)
symmetry into tensors, tensor networks, and their manipulations. To
further illustrate how the strategy works in practice, in this Section
we demonstrate its use in the context of the multi-scale entanglement
renormalization Ansatz, or MERA, and present numerical results from our
reference implementation of the U(1) symmetry in matlab .

##### 3.2.5.1 Multi-Scale Entanglement Renormalization Ansatz

Figure 3.18 shows a MERA that represent states @xmath of a lattice
@xmath made of @xmath sites (see Sec. 3.2.2.5 ).

Recall that the MERA is made of layers of isometric tensors, known as
disentanglers @xmath and isometries @xmath , that implement a
coarse-graining transformation. In this particular scheme, isometries
map three sites into one and the coarse-graining transformation reduces
the @xmath sites of @xmath into two sites using two layers of tensors. A
collection of states on these two sites is then encoded in a top tensor
@xmath , whose upper index @xmath is used to label @xmath states @xmath
. This particular arrangement of tensors corresponds to the 3:1 MERA
described in Evenbly and Vidal ( 2009a ) .

In this Section we will consider a MERA analogous to that of Fig. 3.18
but with @xmath layers of disentanglers and isometries, which we will
use to describe states on a lattice @xmath made of @xmath sites. We will
use this variational Ansatz to obtain an approximation to the ground
state and first excited states of two quantum spin chains that have a
global internal U(1) symmetry, namely the spin- @xmath quantum @xmath
chain of Eq. ( 3.91 ) and the spin- @xmath antiferromagnetic quantum
Heisenberg chain of Eq. ( 3.92 ). Each spin-1/2 degree of freedom of the
chain is described by a vector space spanned by two orthonormal states
@xmath . Here we will represent them by the states @xmath corresponding
to zero and one particles, as in Example 1 of Sec. 3.2.3.1 . For
computational convenience, we will consider a lattice @xmath where each
site contains two spins, or states, @xmath . Therefore each site of
@xmath is described by a space @xmath , where @xmath and @xmath , as in
Example 2 of Sec. 3.2.3.1 . Thus, a lattice @xmath made of @xmath sites
corresponds to a chain of @xmath spins. In such a system, the total
particle number @xmath ranges from @xmath to @xmath . (Equivalently, the
@xmath component of the total spin @xmath ranges from @xmath to @xmath ,
with @xmath .)

##### 3.2.5.2 MERA With U(1) Symmetry

A U(1)-invariant version of the MERA, or U(1) MERA for short, is
obtained by simply considering U(1)-invariant versions of all of the
isometric tensors, namely the disentanglers @xmath , isometries @xmath ,
and the top tensor @xmath . This requires assigning a particle number
operator to each index of the MERA. Each open index of the first layer
of disentanglers corresponds to one site of @xmath . The particle number
operator on any such index is therefore given by the quantum spin model
under consideration. We can characterize the particle number operator by
two vectors, @xmath and @xmath : a list of the different values the
particle number takes and the degeneracy associated with each such
particle number, respectively. In the case of the vector space @xmath
for each site of @xmath described above, @xmath and @xmath . For the
open index of the tensor @xmath at the very top of the MERA, the
assignment of charges is also straightforward. For instance, to find an
approximation to the ground state and first seven excited states of the
quantum spin model with particle number @xmath , we choose @xmath and
@xmath . (In particular, a vanishing @xmath corresponds to @xmath .)

For each of the remaining indices of the MERA, the assignment of the
pair @xmath needs careful consideration and a final choice may only be
possible after numerically testing several options and selecting the one
which produces the lowest expectation value of the energy. Table 3.1
shows the assignment of particle numbers and degeneracies made to
represent the ground state and several excited states in a system of
@xmath sites (that is, @xmath spins) with total particle number @xmath
(or @xmath ).

Notice that at level @xmath of the MERA ( @xmath ), each index
effectively corresponds to a block of @xmath sites of @xmath . Therefore
having exactly @xmath particles in a block of @xmath sites corresponds
to a density of @xmath particle per site of @xmath . The assigned
particle numbers of Table 3.1 , namely @xmath for level @xmath , then
correspond to allowing for fluctuations of up to two particles with
respect to the average density. The sum of corresponding degeneracies
@xmath gives the bond dimension @xmath , which in the example is @xmath
.

In order to find an approximation to the ground state of either @xmath
or @xmath in Eqs. ( 3.91 )–( 3.92 ), we set @xmath and optimize the
tensors in the MERA so as to minimize the expectation value

  -- -------- -- ---------
     @xmath      (3.123)
  -- -------- -- ---------

where @xmath is the pure state represented by the MERA and @xmath is the
relevant Hamiltonian. In order to find an approximation to the @xmath
eigenstates of @xmath with lowest energies, we optimize the tensors in
the MERA so as to minimize the expectation value

  -- -------- -- ---------
     @xmath      (3.124)
  -- -------- -- ---------

The optimization is carried out using the MERA algorithm described in
Evenbly and Vidal ( 2009a ) , which requires contracting tensor networks
(by sequentially multiplying pairs of tensors) and performing singular
value decompositions. In the present example, all of these operations
will be performed exploiting the U(1) symmetry.

Figure 3.19 shows the error in the ground state energy as a function of
the bond dimension @xmath , for assignments of degeneracies similar to
those in Table 3.2 . The error is seen to decay exponentially with
increasing @xmath , indicating increasingly accurate approximations to
the ground state.

##### 3.2.5.3 Exploiting the Symmetry

We now discuss some of the advantages of using the U(1) MERA.

###### Selection of Particle Number Sector

An important advantage of the U(1) MERA is that it exactly preserves the
U(1) symmetry. In other words, the states resulting from a numerical
optimization are exact eigenvectors of the total particle number
operator @xmath ( 3.81 ). In addition, the total particle number @xmath
can be pre-selected at the onset of optimization by specifying it in the
open index of the top tensor @xmath .

Figure 3.20 shows the energy gap between the ground state and two
excited states of an @xmath chain with @xmath spins (or @xmath sites),
for @xmath particles ( @xmath ).

One is the first excited state which also has @xmath particles. The
other is the ground state in the sector with @xmath particles. The two
energy gaps are seen to decay with the system size as @xmath . The
ability to pre-select a given particle number @xmath means that only two
optimizations were required: one MERA optimization for @xmath with
@xmath in order to obtain an approximation to the ground state and first
excited state of @xmath in that particle number sector; and one MERA
optimization for @xmath with @xmath in order to obtain an approximation
to the ground state of @xmath in the particle number sector @xmath .

Similar results can be obtained with the regular MERA. For instance, one
can obtain an approximation to the ground state of a given particle
number sector by adding a chemical potential term @xmath to the
Hamiltonian and carefully tuning the chemical potential term @xmath
until the expectation value of the particle number @xmath is the desired
one. However, the regular MERA cannot guarantee that the states obtained
in this way are exact eigenvectors of @xmath . Instead the resulting
states are likely to have particle number fluctuations.

Figure 3.21 shows the low energy spectrum of the Heisenberg model @xmath
for a periodic system of @xmath sites (or @xmath spins), including the
ground state and several excited states both in the particle sector
@xmath (or @xmath ), and in neighboring particle sectors.

Recall that @xmath is actually invariant under a global internal SU(2)
symmetry, of which particle number is a U(1) subgroup. Correspondingly
the spectrum is organized according to irreps of SU(2), namely singlets
(total spin @xmath ), triplets (total spin @xmath ), quintuplets (total
spin @xmath ), etc. Again, using the U(1) MERA, the five particle number
sectors @xmath , and @xmath can be addressed with independent
computations. This implies, for instance, that in order to find the gap
between the first and fourth singlets, we can simply set @xmath and
@xmath on the open index of the top tensor @xmath , to accommodate the
first four spin-0 states and five spin-1 states in the @xmath sector, as
seen in Fig. 3.21 . In order to capture the fourth singlet using the
regular MERA, we would need to consider at least @xmath (at a larger
computational cost and possibly lower accuracy), since this state has
only the @xmath ^(th) lowest energy overall.

###### Reduction of Computational Costs

The use of U(1)-invariant tensors in the MERA also results in a
reduction of computational costs.

First, U(1)-invariant tensors, when written in the canonical form of
Eqs. ( 3.107 )–( 3.108 ), are block-diagonal and therefore require less
storage space. Table 3.2 compares the number of MERA coefficients that
need to be stored in the regular and symmetric case, for different
choices of particle number assignments relevant to the present examples.

Second, the computation time required to manipulate tensors is also
reduced when using U(1)-invariant tensors in the canonical form. Figure
3.22 shows the computation time required for one iteration of the energy
minimization algorithm of Evenbly and Vidal ( 2009a ) (during which all
tensors in the MERA are updated once), as a function of the total bond
dimension @xmath .

The plot compares the time required using regular tensors and
U(1)-invariant tensors. For U(1)-invariant tensors, we display the time
per iteration for three different levels of precomputation, as described
in Sec. 3.4 . The figure shows that for sufficiently large @xmath ,
using U(1)-invariant tensors leads to a shorter time per iteration of
the optimization algorithm.

In our reference implementation (written in matlab ), using the symmetry
without precomputation is seen to only reduce the computation time by
about a factor of 2 for the largest @xmath under consideration. This is
because maintaining the canonical form for U(1)-invariant tensors still
imposes a significant overhead for the values of @xmath considered. In
contrast, when using precomputation we obtained times shorter by a
factor of 10 or more.

The magnitude of the overhead imposed by maintaining the canonical form
will depend on factors such as programming language and machine
architecture, but in general more significant gains can be obtained by
making full use of precomputation. This option, however, requires a
significant amount of additional memory (see Sec. 3.4 ), and a more
convenient middle ground can be obtained by using a partial
precomputation scheme.

### 3.3 Conclusions

In this Chapter we have provided a detailed explanation of how a global
internal Abelian symmetry may be incorporated into any tensor network
algorithm. We considered tensor networks constructed from tensors which
were invariant under the action of the internal symmetry, and showed how
each tensor may be decomposed according to a canonical form into
degeneracy tensors (which contain all the degrees of freedom that are
not affected by the symmetry) and structural tensors (which are
completely determined by the symmetry). We then introduced a set of
primitive operations @xmath which may be used to carry out tensor
network algorithms using Ansätze such as MPS, PEPS, and MERA, and showed
how each of these operations can be implemented in such a way that the
canonical form is both preserved and exploited for computational gain.

We then demonstrated the implementation of this decomposition for
tensors with an internal U(1) symmetry, and computed multiple benchmarks
demonstrating the computational costs and speed-ups inherent in this
approach. We found that although maintaining the canonical form imposed
additional costs when combining or splitting tensor indices, for
simulations of a sufficiently large scale these costs can be offset by
the gains made when performing permutations, matrix multiplications, and
matrix decompositions.

Finally, we implemented the MERA on a quantum spin chain with U(1)
symmetry. We showed that exploitation of this symmetry can lead to a
decrease in the computational cost by a factor of 10 or more. These
gains may be used either to reduce overall computation time or to permit
substantial increases in the MERA bond dimension @xmath , and
consequently in the accuracy of the results obtained.

Although in this Chapter we have focused on an example which is a
continuous Abelian group, the formalism presented here may equally well
be applied to a finite Abelian group. In particular let us consider a
cyclic group @xmath , @xmath . ¹⁶ ¹⁶ 16 The fundamental theorem of
Abelian groups states that every finite Abelian group may be expressed
as a direct sum of cyclic subgroups of prime-power order. As in the case
of U(1), the Hilbert space decomposes under the action of the group into
a direct sum of one dimensional irreps which are each characterized by
an integer charge @xmath , and consequently most of the analysis
presented in this Chapter remains unchanged. In particular, matrices
which are invariant under the action of the group will be block diagonal
in the basis labeled by charge according to Eq. ( 3.59 ), and symmetric
tensors enjoy the canonical decomposition stated in Eqs. ( 3.107 )–(
3.108 ). The only objects which need modification are the fusion and
splitting maps, which need to be altered so that they encode the fusion
rules for @xmath instead for U(1). For a cyclic group @xmath , the
fusion of two charges @xmath and @xmath gives rise to a charge @xmath
according to @xmath where @xmath indicates that the addition is
performed modulo @xmath . For example, @xmath has charges @xmath , and
the fusion rules for @xmath take the form @xmath where the value of
@xmath is given in the following table:

                   @xmath   
  -------- --- --- -------- ---
               0   1        2
           0   0   1        2
  @xmath   1   1   2        0
           2   2   0        1

More generally, a generic Abelian group will be characterised by a set
of charges @xmath . When fusing two such sets of charges @xmath and
@xmath , each charge @xmath is combined with its counterpart @xmath
according to the fusion rule of the relevant subgroup. Once again, this
behaviour may be encoded in a single fusion map @xmath and its inverse
@xmath . The formalism presented in this Chapter is therefore directly
applicable to any Abelian group.

### 3.4 \nohyphensSupplement: Use of Precomputation in Iterative
Algorithms

We have seen that the use of the canonical form given in Eqs. ( 3.107
)–( 3.108 ) to represent U(1)-invariant tensors can potentially lead to
substantial reductions in memory requirements and in calculation time.
We also pointed out, however, that there is an additional cost in
maintaining an invariant tensor in its canonical form, and that this is
associated with the reshaping (fusing and/or splitting) of its indices.
In some situations this additional cost may significantly reduce, or
even offset, the benefits of using the canonical form.

In this Section we investigate techniques for reducing this additional
cost in the context of iterative tensor network algorithms. Many of the
algorithms discussed in Sec. 3.2.2.5 are iterative algorithms, repeating
the same sequence of tensor network manipulations many times over.
Examples include algorithms which compute tensor network approximations
to the ground state by minimizing the expectation value of the energy or
by simulating evolution in imaginary time, with each iteration yielding
an increasingly accurate approximation to the ground state of the
system.

The goal of this Section is to identify calculations which depend only
on the symmetry group, and are independent of the variational
coefficients of such algorithms. Where these calculations are repeated
in each iteration of the algorithm, we can effectively eliminate the
associated computational cost by performing them only once, either
during or prior to the first iteration of the algorithm, and then
storing and reusing these precomputed results in subsequent iterations.
We will illustrate this procedure by considering the precomputation of a
series of operations applied to a single tensor @xmath .

To do this, we begin by revisiting the fusion and splitting tables of
Sec. 3.2.3.3 and introducing a graphical representation of these
objects. We then introduce a convenient decomposition of a symmetric
tensor into a matrix accompanied by multiple fusion and/or splitting
tensors, and linear maps @xmath that map one such decomposition into
another. These linear maps are independent of the coefficients of the
tensor being reorganized, and consequently they are precisely the
objects which can be precomputed in order to quicken an iterative
algorithm at the expense of additional memory cost. Finally we describe
two specific precomputation schemes, differing in what is precomputed
and in how the precomputed data are utilized during the execution of the
algorithm, in order to illustrate the trade-off between the amount of
memory needed to store the precomputation data and the associated
computational speedup which may be obtained. In practice, the nature of
the specific implementation employed will depend on available
computational resources.

#### 3.4.1 Diagrammatic Notation of Fusing and Splitting Tensors

In describing how we can precompute repeated manipulations of this
tensor @xmath , we will find it useful to employ diagrammatic
representations of the fusion and splitting tables @xmath and @xmath
introduced in Sec. 3.2.3.3 . These tables implement a linear map between
a pair of indices and their fusion product, and thus can be understood
as trivalent tensors having two input legs and one output leg (or vice
versa) in accordance with Sec. 3.2.2.6 . We choose to represent them
graphically as shown in Fig. 3.23 (i), where the arrow within the circle
always points toward the coupled index.

The linear maps @xmath and @xmath are unitary, and consequently we
impose that the tensors of Fig. 3.23 (i) must satisfy the identities
given in Fig. 3.23 (ii), corresponding to unitarity under the action of
the conjugation operation employed in diagrammatic tensor network
notation (vertical reflection of a tensor and the complex conjugation of
its components, typically denoted @xmath ). Our notation also reflects
the property, first noted in Sec. 3.2.3.3 , that @xmath and @xmath may
be decomposed into two pieces [Fig. 3.23 (iii)]. For the fusion tensor,
we identify the first piece (represented by a circle containing an
arrow) with the creation of a composed index using the manner we would
employ in the absence of symmetry ( 3.26 ). The second piece,
represented by the small square, permutes the basis elements of the
composed index, reorganizing them according to total particle number.
The two components of the splitting tensor are then uniquely defined by
consistency with the process of conjugation for the diagrammatic
representation of tensors, and with the unitarity condition of Fig. 3.23
(ii).

These requirements have an important consequence. Suppose the first part
of @xmath implements @xmath by iterating rapidly over the values of
@xmath and more slowly over the values of @xmath , and @xmath lies
clockwise of @xmath on the graphical representation of @xmath . This
then means that on the graphical representation of @xmath which
implements @xmath , index @xmath must lie counterclockwise of @xmath .
It is therefore vitally important to distinguish between the splitting
tensor and a rotated depiction of the fusing tensor. To this end we
require that when using this diagrammatic notation, all tensors (with
the exception of the fusion and splitting tensors) must be drawn with
only downward-going legs, as seen for example in Fig. 3.24 , though the
legs are still free to carry either incoming or outgoing arrows as
before.

#### 3.4.2 Tree Decomposition

We find it convenient to decompose a rank- @xmath , U(1)-invariant
tensor @xmath , having components @xmath , as a binary tree tensor
network @xmath consisting of a matrix @xmath which we will call the root
node , and of @xmath splitting tensors @xmath as branching internal
nodes , with the leaf indices of tree @xmath corresponding to the
indices @xmath of tensor @xmath . We refer to decomposition @xmath as a
tree decomposition of @xmath . Figure 3.24 shows an example of tree
decomposition for a rank-6 tensor. It is of the form

  -- -------- -- ---------
     @xmath      (3.125)
  -- -------- -- ---------

where @xmath are the internal indices of the tree.

The same tensor @xmath may be decomposed as a tree in many different
ways, corresponding to different choices of the fusion tree. As an
example we show two different but equivalent decompositions of a rank-4
tensor in Fig. 3.25 .

Different choices @xmath of tree decomposition for tensor @xmath will
lead to different matrix representations @xmath of the same tensor.
Finally, Fig. 3.26 shows how to obtain the tree decompositions from
@xmath by introducing an appropriate resolution of the identity,
constructed from pairs of fusion operators @xmath and splitting
operators @xmath in accordance with Fig. 3.23 (ii).

The representation of a tensor @xmath by means of a tree decomposition
is particularly useful because many tensor network algorithms may be
understood as a sequence of operations carried out on tensors reduced to
matrix form. For example, consider tensor network algorithms such as
MPS, MERA, and PEPS. When tensors are updated in these algorithms, the
new tensor is typically created as a matrix, to which operations from
the primitive set @xmath of Sec. 3.2.2.4 are then applied. When they are
decomposed or contracted with other tensors, this may once again take
place with the tensor in matrix form. Any such matrix form may always be
understood as the matrix component of an appropriate tree decomposition
@xmath of tensor @xmath , where the sequence of operations reshaping
tensor @xmath to matrix @xmath corresponds to the contents of the shaded
area in Fig. 3.26 .

#### 3.4.3 Mapping Between Tree Decompositions

Suppose now that we have a tensor @xmath in matrix form @xmath , which
is associated with a particular choice of tree decomposition @xmath ,
and we wish to transform it into another matrix form @xmath ,
corresponding to another tree decomposition @xmath . As indicated, this
process may frequently arise during the application of many common
tensor network algorithms. The new matrix @xmath can be obtained from
@xmath by means of a series of reshaping (splitting/fusing) and
permuting operations, as indicated in Fig. 3.27 , and this series of
operations may be understood as defining a map @xmath :

  -- -------- -- ---------
     @xmath      (3.126)
  -- -------- -- ---------

The map @xmath is a linear map which depends only on the tree structure
of @xmath and @xmath , and is independent of the coefficients of @xmath
. Moreover, @xmath is unitary, and it follows from the construction of
fusing and splitting tensors and the behaviour of permutation of indices
(which serves to relocate the coefficients of a tensor) that @xmath
simply reorganizes the coefficients of @xmath into the coefficients of
@xmath in a one-to-one fashion.

Therefore, one way to compute the matrix @xmath from matrix @xmath is by
first computing the linear map @xmath , which is independent of the
specific coefficients in tensor @xmath , and by then applying it to
@xmath .

#### 3.4.4 Precomputation Schemes For Iterative Tensor Network
Algorithms

The observation that the map @xmath is independent of the specific
coefficients in @xmath is particularly useful in the context of
iterative tensor network algorithms. It implies that, although the
coefficients in @xmath will change from iteration to iteration, the
linear map @xmath in Eq. ( 3.126 ) remains unchanged. It is therefore
possible to calculate the map @xmath once, during the first iteration of
the simulation, and then to store it in memory and re-use it during
subsequent iterations. We refer to such a strategy as a precomputation
scheme . Figure 3.28 contrasts the program flow of a generic iterative
tensor network algorithm with and without precomputation of the
transformations @xmath .

Using such a precomputation scheme, a significant speed-up of
simulations can be obtained, at the price of storing potentially large
amounts of precomputed data (as a single iteration of the algorithm may
require the application of many different transformations @xmath ).
Therefore a trade-off necessarily exists between the amount of speed-up
that can be obtained and the memory requirement which this entails. In
this Section we describe two different precomputation schemes. The first
one fully precomputes and stores all maps @xmath , and is relatively
straightforward to implement. This results in the maximal increase in
simulation speed, but implementation requires a large amount of memory.
The second scheme only partially precomputes the maps @xmath , resulting
in a moderate speed-up of simulations, but with memory requirements
which are also similarly more modest.

##### 3.4.4.1 Maximal Precomputation Scheme

As noted in Sec. 3.4.3 , applying the map @xmath to a matrix @xmath
simply reorganizes its coefficients to produce the matrix @xmath .
Moreover, if the indices of matrices @xmath and @xmath are fused to
yield vectors @xmath and @xmath then the map @xmath may be understood as
a permutation matrix, and this in turn may be concisely represented as a
string of integers @xmath such that entry @xmath of @xmath is given by
entry @xmath of vector @xmath . Because all of the elements from which
@xmath is composed are sparse, unitary, and composed entirely of 0’s and
1’s, the permutation to which @xmath corresponds may be calculated at a
total cost of only @xmath , where @xmath counts only the elements of
@xmath which are not fixed to be zero by the symmetry constraints of
Eq. ( 3.107 ). In essence, for each element of the vector @xmath one
identifies the corresponding number and degeneracy indices @xmath on
each leg @xmath of matrix @xmath . One can then read down the figure,
applying each table @xmath or @xmath in turn to identify the
corresponding labels @xmath on the intermediate legs, until finally the
corresponding labels on the indices of @xmath are obtained. There is
then a further 1:1 mapping from each set of labels @xmath , @xmath on
@xmath to the corresponding entry in @xmath , completing the definition
of @xmath as a map from @xmath to @xmath .

Storing the map @xmath for a transformation such as the one shown in
Fig. 3.27 imposes a memory cost of @xmath . The application of this map
also incurs a computational cost of @xmath , but computational overhead
is saved in not having to reconstruct the map @xmath on every iteration
of the algorithm.

##### 3.4.4.2 Partial Precomputation Scheme

The @xmath memory cost incurred in the previous scheme can be
significant for large matrices. However, we may reduce this cost by
replacing the single permutation @xmath employed in that scheme with
multiple smaller operations which may also be precomputed. In this
approach @xmath is retained in matrix form rather than being reshaped
into a vector, and we precompute permutations to be performed on its
rows and columns.

First, we decompose all the fusion and splitting tensors into two pieces
in accordance with Fig. 3.23 (iii). Next, we recognise that any
permutations applied to one or more legs of a fusion or splitting tensor
may always be written as a single permutation applied to the coupled
index [Fig. 3.29 (i)].

We use this to replace all permutations on the intermediate indices of
the diagram with equivalent permutations acting only on the indices of
@xmath and the open indices, as shown for a simple example in Fig. 3.29
(ii). The residual fusion and splitting operations, depicted by just a
circle enclosing an arrow, then simply carry out fusion and splitting of
indices as would be performed in the absence of symmetry ( 3.26 )-( 3.27
). These operations are typically far faster than their symmetric
counterparts as they do not need to sort the entries of their output
indices according to particle number.

In subsequent iterations, the matrix @xmath is obtained from @xmath by
consecutively

1.  permuting the rows and columns of @xmath using the precomputed net
    permutations which act on the legs of @xmath ;

2.  performing any elementary (non-symmetric) splitting, permuting of
    indices, and fusing operations, as described by the grey-shaded
    region in Fig. 3.29 (ii);

3.  permuting the rows and columns of the resulting matrix, using the
    precomputed net permutations which act on the open legs of Fig. 3.29
    (ii).

When matrix @xmath is defined compactly, as in ( 3.107 ), so that
elements which are identically zero by symmetry are not explicitly
stored, a tensor @xmath is constructed from multiple blocks identified
by U(1) charge labels on their indices [ @xmath in Eq. ( 3.107 )]. Under
these conditions the elementary splitting, fusing, and permutation
operations of step 2 above are applied to each individual block, but
some additional computational overhead is incurred in determining the
necessary rearrangements of these blocks arising out of the actions
performed. This rearrangement may be computed on the fly, or may also be
precomputed as a mapping between the arrangement of blocks in @xmath and
that in @xmath .

The memory required to store the precomputation data in this scheme is
dominated by the size of the net permutations collected on the matrix
indices, and is therefore of @xmath . The overall cost of obtaining
@xmath from @xmath is once again of @xmath , but is in general higher
than the previous scheme as this cost now involves two complete
permutations of the matrix coefficients, as well as a reorganisation of
the block structure of @xmath which may possibly be computed at runtime.
Nevertheless, in situations where memory constraints are significant,
partial precomputation schemes of this sort may be preferred.

### 3.5 Supplement: Notes on the Implementation of Abelian Symmetries

As noted in Sec. 3.1.2 , a symmetric tensor may be decomposed into a
spin network and a collection of degeneracy tensors. This was seen again
in Secs. 3.2.3 – 3.2.4 for the symmetry group U(1), with the spin
network in this instance being trivial due to the Abelian nature of the
group. For an Abelian symmetry, we may consequently understand this
decomposition [( 3.59 ), ( 3.107 )] as dividing a tensor into a number
of blocks, labelled by the charges on each leg of the tensor, the
majority of which are systematically zero. An example of this is seen in
Eq. ( 3.62 ) for a two-legged tensor, and is reproduced here with the
different charge blocks highlighted and labelled:

  -- -------- -- ---------
     @xmath      (3.127)
  -- -------- -- ---------

For an Abelian tensor, this block decomposition may be trivially
extended over an arbitrary number of legs, with each block being indexed
by the associated charges on all legs ( 3.105 ), and this fact may be
exploited in implementation of the symmetry.

To illustrate how this is done, consider the fusing of two indices under
the action of the simplest Abelian symmetry group, @xmath , and recall
that fusing and splitting of indices may be thought of as consisting of
two stages (see Secs. 3.2.3.3 and 3.4.1 ). In the first stage, the
charges on the legs are combined according to a typical tensor product
process, as per Eq. ( 3.26 ), iterating rapidly over one charge and
slowly over the other:

  -- -------- -- ---------
     @xmath      (3.128)
  -- -------- -- ---------

In the second stage, the states of the tensor product space are
re-ordered to collect like charges:

  -- -------- -- ---------
     @xmath      (3.129)
  -- -------- -- ---------

Significantly, this entire process may be conducted at the level of
blocks, and no mixing or re-ordering of the elements within individual
blocks is required. If the entire tensor is maintained in the form of a
sparsely-populated array of blocks in this manner, and blocks are not
concatenated on fusion, then both fusion and splitting may be performed
without requiring the addressing of the contents of any individual
blocks. This is illustrated in Fig. 3.30 . [It is assumed either that
any reshaping of individual blocks is deferred, in keeping with the
philosophy of Sec. 3.4 , or that an efficient representation of @xmath
-dimensional tensors employed—such as that used by matlab —for which
reshape operations may be performed on the individual blocks at trivial
cost independent of the size of the block.] A similar treatment may be
applied to all the primitives of set @xmath (Sec. 3.2.2.4 ).

Using this approach, the author was able to implement the @xmath
-symmetric ternary 1D MERA with a demonstrable increase in performance
over the standard MERA for bond dimensions @xmath . Performance for
Abelian symmetries with greater numbers of charge sectors, such as U(1),
is anticipated to be even higher, effectively removing the need for the
precomputation techniques discussed in Sec. 3.4 . However, these
techniques remain important for the exploitation of non-Abelian
symmetries (Chapter 5 and Singh and Vidal in preparation ) and anyons
(Chapter 4 and Pfeifer et al. 2010 ).

One note of caution: When combining indices using a symmetry-preserving
ordering such as that given in Eq. ( 3.129 ), care must be taken to
ensure that fusion is implemented in an associative manner, i.e. @xmath
results in the same ordering of entries on index @xmath as the fusion
@xmath , if consistent results are to be obtained.

#### 3.5.1 Fermions

My initial development of efficient @xmath symmetry algorithms was
performed in parallel with Philippe Corboz, who adopted a similar
computational strategy. To avoid inappropriate duplication of research
efforts, it was decided that Philippe would then proceed to incorporate
fermionic exchange behaviour, while I would instead study the extension
of this technique to other symmetry groups, resulting in the present
work on Abelian symmetries, non-Abelian symmetries, and anyons. The
principle by which fermionic exchange statistics may be incorporated
into this scheme is, however, easily understood as follows:

Recall that we have chosen to enumerate the legs of a tensor
counterclockwise from the 9 o’clock position (Sec. 3.2.2.1 ). When a
tensor network diagram involves crossings of pairs of legs, these
crossings may be understood as a permutation operation which exchanges
the ordering of the indices on the tensor. However, for fermionic
statistics, we have a @xmath symmetry and thus for any given index, each
block in the tensor will have a charge label, @xmath or @xmath ,
associated with that index. The permutation operation corresponding to
index exchange is performed independently on each block in the normal
fashion for a @xmath -symmetric tensor, subject to one simple
modification: When the charge labels for a given block are @xmath on
both indices, in addition to the permutation operation, the block is
multiplied by @xmath . This is the essence of the “swap gate” formalism
introduced by Corboz et al. ( 2010b ) —see also Corboz and Vidal ( 2009
); Pineda et al. ( 2010 ); Barthel et al. ( 2009 ); Pižorn and
Verstraete ( 2010 ) .

I note that even greater efficiency may be attained not only by writing
each tensor as a grid of blocks, but also by associating with each block
a numeric multiplier, initially 1. To multiply a block by @xmath , it
now suffices to multiply the corresponding numeric factor by @xmath . In
this manner the cost of the swap gate is reduced, requiring only one
operation to change the sign of an entire block. Of course, this minus
sign must eventually be applied in order to obtain numerical results,
but if many swap gates are applied to a tensor before these numerical
results are computed, then deferring (and combining) their evaluation in
this manner may result in a significant saving in calculation time. A
generalised version of this technique is also used to reduce
computational cost in the study of anyonic systems, as described in Sec.
4.7

One caution is required when using the methods of Sec. 3.2 for the
simulation of fermions. Note that when contracting a pair of tensors as
per Sec. 3.2.2.2 , the counterclockwise ordering of the indices to be
contracted must be the same on each tensor. There will frequently be
index permutations required to set up this ordering, which will
introduce some exchange factors of @xmath . However, there are also
exchange factors associated with the contraction itself, as seen in e.g.
Fig. 3.31 , and these too must be taken into account. For an alternative
approach where exchange factors are associated only with the re-ordering
of indices, and there are no such hidden factors associated with the
contraction of two tensors, see the discussion in Sec. 5.2 .

## Chapter 4 Anyonic Tensor Networks

Having developed a formalism for the exploitation of internal symmetries
in tensor networks and exploited it for Abelian symmetry groups, it was
natural to seek to apply this formalism to more general classes of
physical systems. As a colleague (Sukhwinder Singh) was already working
on the implementation of SU(2) symmetries for the MPS and MERA, with
obvious extension to spin systems having any non-Abelian internal
symmetry group, it was decided that I would instead study whether our
approach could be extended to permit the study of systems of anyons.

Anyonic systems do not in general exhibit an internal symmetry group;
instead their behaviours and statistics are typically described by a
more general structure known as a Unitary Braided Tensor Category
(UBTC). \nomenclature UBTC Unitary Braided Tensor Category: The
mathematical structure used to describe a system of anyons (see Chapter
4 ). There is also a UBTC associated with any symmetry group (see
Chapter 5 ). To specify an anyon model using a UBTC, one must declare a
set of charge labels (including a vacuum charge), fusion rules
describing how they combine (which must be associative), a set of basis
transformations known as @xmath moves, and a tensor @xmath which
describes the exchange statistics of the charges. These properties will
all be described in detail in Sec. 4.2 . For any group @xmath , it is
possible to construct an associated UBTC where the @xmath moves are
derived from the 6- @xmath symbols, and the @xmath tensor reflects the
universal braid matrix for the irreps of the group. In fact, this UBTC
provides a complete description of the group at the level of
representations, up to (but not including) explicit construction of
representations of the irreps themselves (see Chapter 5 ). However,
although every group may be associated with a UBTC, the converse does
not hold, and UBTCs may also be used to describe more general structures
such as quantum doubles [e.g. D(D @xmath )], and quantum groups [e.g.
@xmath -deformed SU(2), or SU(2) @xmath ]. It is the UBTCs associated
with these more general structures which may be used to describe systems
with anyonic statistics.

Because of the close association between groups and UBTCs, it was
tempting to ask whether our internal symmetry formalism, which could be
applied to quantum systems with a group-based mathematical structure,
could be extended to address systems based on any UBTC. Of particular
interest are the UBTCs associated with quantum groups, as these are
associated with many interesting anyon models such as SU(2) @xmath , for
which the integer subalgebra describes a class of anyons known as
“Fibonacci anyons”, capable of supporting universal quantum computation
simply through particle exchange. However, many of the charges in these
models may not be associated with explicit matrix representations, as in
many cases the fusion rules imply that the dimensions of these irreps
must be non-integer, or even irrational. Consequently the problem could
not be viewed as one of decomposing a known system into charge sectors
as per Chapter 3 , but instead had to be formulated directly in the
graphical language of UBTCs which constitutes a natural description of
an anyonic system.

In this Chapter, I describe a formalism permitting exactly this, whereby
any tensor network Ansatz or algorithm may be constructed for a system
of anyons, or indeed for any other model describable in terms of a UBTC.
This Chapter will only concern itself explicitly with systems of anyons
on the disc, though a subsequent treatment of anyons on surfaces of
higher genus is planned.

(The same formalism may even be applied to 1D systems described by a
Unitary Tensor Category without including a notion of particle exchange,
though the author is as yet unaware of any physical systems of interest
which take such a form.)

Sections 4.1 – 4.6 of this Chapter have previously been published as
Pfeifer, Corboz, Buerschaper, Aguado, Troyer, and Vidal , Physical
Review B , 82 , 115126, 2010, © (2010) by the American Physical Society.

### 4.1 Introduction

The study of anyons offers one of the most exciting challenges in
contemporary physics. Anyons are exotic quasiparticles with non-trivial
exchange statistics, which makes them difficult to simulate. However,
they are of great interest as some species offer the prospect of a
highly fault-tolerant form of universal quantum computation (Kitaev,
2003 ; Nayak et al., 2008 ) , and it has been suggested that the
simplest such species may appear in the fractional quantum Hall state
with filling fraction @xmath = 12/5 (Xia et al., 2004 ) . Despite the
current strong interest in the development of practical quantum
computing, our ability to study the collective behaviour of systems of
anyons remains limited.

The study of interacting systems of anyons using numerical techniques
was pioneered by Feiguin et al. ( 2007 ) , using exact diagonalisation
for 1D systems of up to 37 anyons, and the Density Matrix
Renormalisation Group algorithm (DMRG) (White, 1992 ) for longer chains.
Also related is work by Sierra and Nishino ( 1997 ) , later extended by
Tatsuaki ( 2000 ) , which applies a variant of DMRG to spin chain models
having @xmath symmetry. Some of these models are now known to correspond
to @xmath anyon chains (Trebst et al., 2008a ) , and using this mapping
these systems may also be studied using the Bethe Ansatz (Alcaraz
et al., 1987 ) and quantum Monte Carlo (Todo and Kato, 2001 ) .

However, all of these methods have their limitations. Exact
diagonalisation has a computational cost which is exponential in the
number of sites, strongly limiting the size of the systems which may be
studied. DMRG is capable of studying larger system sizes, but is
typically limited to 1D or quasi-1D systems (e.g. ladders). Mapping to a
spin chain is useful in one dimension but is substantially less
practical in two. There are therefore good reasons to desire a formalism
which will allow the application of other tensor network algorithms to
systems of anyons. Many of these tensor networks, such as Projected
Entangled Pair States (PEPS) (Verstraete and Cirac, 2004 ; Nishino and
Okunishi, 1998 ; Gu et al., 2008 ; Xie et al., 2009 ; Jordan et al.,
2008 ) , and the 2D versions of Tree Tensor Networks (TTN) (Tagliacozzo
et al., 2009 ) and of the Multi-scale Entanglement Renormalisation
Ansatz (MERA) (Cincio et al., 2008 ; Evenbly and Vidal, 2009b , 2010a )
have been designed specifically to accurately describe two-dimensional
systems.

In one dimension, many previously studied systems of interacting anyons
display extended critical phases (e.g. Feiguin et al., 2007 ; Trebst
et al., 2008a ) , which are characterised by correlators exhibiting
polynomial decay (Di Francesco et al., 1997 ) . Whereas DMRG favours
accurate representation of short range correlators at the expense of
long-range accuracy, the 1D MERA (Vidal, 2007a , 2008 ) is ideally
suited to this situation as its hierarchical structure naturally encodes
the renormalisation group flow at the level of operators and
wavefunctions (Vidal, 2007a , 2008 , 2010 ; Chen et al., 2010a ) , and
hence accurately reproduces correlators across a wide range of length
scales (Vidal, 2007a , 2008 ; Giovannetti et al., 2008 ; Pfeifer et al.,
2009 ; Evenbly and Vidal, 2009a ) . The development of a general
formalism for anyonic tensor networks is therefore also advantageous for
the study of 1D anyonic systems.

This Chapter describes how any tensor network algorithm may be adapted
to systems of anyons in one or two dimensions using structures which
explicitly implement the quantum group symmetry of the anyon model. As a
specific example I demonstrate the construction of the anyonic 1D MERA,
which I then apply to an infinite chain of interacting Fibonacci anyons
at criticality. The approach which I present is completely general, and
can be applied to any species of anyons and any tensor network Ansatz.

### 4.2 Anyonic States

Consider a lattice @xmath of @xmath sites populated by anyons. In
contrast to bosonic and fermionic systems, for many anyon models the
total Hilbert space @xmath can not be divided into a tensor product of
local Hilbert spaces. Instead, a basis is defined by introducing a
specific fusion tree [e.g. Fig. 4.1 (i)]. The fusion tree is always
constructed on a linear ordering of anyons, and while the 1D lattice
naturally exhibits such an ordering, for 2D lattices some linear
ordering must be imposed. Each line is then labelled with a charge index
@xmath such that the labels are consistent with the fusion rules of the
anyon model,

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

For anyon types where some entries of the multiplicity tensor @xmath
take values greater than 1, a label @xmath is also affixed to the vertex
which represents the fusion process to distinguish between the different
copies of charge @xmath . The edges of the graph which are connected to
a vertex only at their lower end are termed “leaves” of the fusion tree,
and we will associate these leaves with the charge labels @xmath .
Different orderings of the leaves on a fusion tree may be interconverted
by means of braiding [Fig. 4.1 (ii)], and different fusion trees,
corresponding to different bases of states, may be interconverted by
means of @xmath moves [Fig. 4.1 (iii)] (Kitaev, 2006 ; Bonderson, 2007 )
. In some situations it may also be useful to associate a further index
@xmath with each of the leaves of the fusion tree. For example, if the
leaves are equated with the sites of a physical lattice, then this
additional index may be used to enumerate additional non-anyonic degrees
of freedom associated with that lattice. For simplicity we will usually
leave these extra indices @xmath implicit, as we have done in Fig. 4.1 ,
as they do not directly participate in anyonic manipulations such as
@xmath moves and braiding.

Let the total number of charge labels on the fusion tree be given by
@xmath , where @xmath . For Abelian anyons the fusion rules uniquely
constrain all @xmath for @xmath , and provided there are no constraints
on the total charge, the total Hilbert space reduces to a product of
local Hilbert spaces @xmath , such that @xmath . For non-Abelian anyons,
additional degrees of freedom arise because some fusion rules admit
multiple outcomes, permitting certain @xmath to take on multiple values
while remaining consistent with the fusion rules, and the resulting
Hilbert space does not necessarily admit a tensor product structure.

We will now associate a parameter @xmath with each charge on the fusion
tree, which we will term the degeneracy. This parameter corresponds to
the number of possible fusion processes by which charge @xmath may be
obtained at location @xmath . Where charge @xmath arises from the fusion
of charges @xmath and @xmath , then @xmath will satisfy

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

For systems where the only degrees of freedom are anyonic, degeneracies
on the physical lattice @xmath (i.e. @xmath , @xmath ) will take values
of 0 or 1 depending on whether a charge @xmath is permitted on lattice
site @xmath . Higher values of @xmath may be used on the physical
lattice if there is also a need to represent additional non-anyonic
degrees of freedom, enumerated by indices @xmath .

Up to this point we have parameterised our Hilbert space in terms of
explicit labellings of the fusion tree. We now adopt a different
approach: Consider an edge @xmath of the fusion tree which is not a
“leaf”. As well as labelling this edge with a charge @xmath we may
introduce a second index @xmath , running from 1 to @xmath . Each pair
of values @xmath may be associated with a unique charge labelling for
the portion of the fusion tree from edge @xmath out to the leaves, with
these labellings being compatible with the fusion rules in the presence
of a charge of @xmath on site @xmath (for an illustration of this, see
Fig. 4.2 ). Provided we know the structure of the fusion tree above
@xmath and have a systematic means of associating labellings of that
portion of the tree with values of @xmath , then in lieu of stating the
values of all @xmath for edges @xmath involved in that portion of the
tree, we may simply specify the value of the degeneracy index @xmath .
In this way we may specify an entire state in the form

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

where @xmath is the total charge obtained on fusing all the anyons. The
index @xmath , which is the degeneracy index associated with the total
charge of the fusion tree, may be understood as systematically
enumerating all possible labellings of the entire fusion tree including
charge labels, vertex labels, and any labels associated with additional
non-anyonic degrees of freedom. For an example, see Fig. 4.3 . Note that
for a given edge @xmath , the value of the degeneracy @xmath may vary
with the charge @xmath and consequently the range of the degeneracy
index @xmath in Eq. ( 4.3 ) is dependent on the value of the charge
@xmath .

The notation of Eq. ( 4.3 ) should be contrasted with that of Fig. 4.1
(i). In the latter, the number of indices on @xmath depends upon the
number of charge labels on the fusion tree, whereas in the former, the
tensor describing the state is always indexed by just one pair of
labels—charge and degeneracy—which will prove advantageous in
constructing a tensor network formalism for systems of anyons.

We now choose to restrict our attention to systems having the identity
charge. We may do this without loss of generality as a state on @xmath
lattice sites with a total charge @xmath may always be equivalently
represented by a state on @xmath lattice sites whose total charge is the
identity, with a charge @xmath on lattice site @xmath . This additional
charge annihilates the total charge @xmath of sites @xmath to give the
vacuum. The expression for @xmath then becomes

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

where @xmath ranges from 1 to the dimension of the Hilbert space of the
system of @xmath sites with total charge @xmath . Consequently we may
represent the state @xmath of a system of anyons by means of the vector
@xmath . For simplicity of notation, we will take greek indices from the
beginning of the alphabet to correspond to pairs of indices @xmath
consisting of a charge index and the associated degeneracy index. The
vector @xmath will therefore be denoted simply @xmath , with the
understanding that in this case the charge component @xmath of
multi-index @xmath takes only the value 1. (Multi-index @xmath is raised
as we will shortly introduce a diagrammatic formalism in which vector
@xmath is represented by an object with a single upward-going leg. In
this formalism, upward- and downward-going legs may be associated with
upper and lower multi-indices respectively.)

### 4.3 Anyonic Operators

We will divide our consideration of anyonic operators into two parts.
First we shall consider operators which map a state on some Hilbert
space @xmath into another state on the same Hilbert space. When applied
to a state represented by @xmath , such an operator leaves the
degeneracies of the charges in multi-index @xmath unchanged. We will
therefore call these degeneracy-preserving anyonic operators. Then we
will consider those operators which map a state on some Hilbert space
@xmath into a state on some other Hilbert space @xmath . These operators
may represent processes which modify the environment, for example by
adding or removing lattice sites, and also play an important part in
anyonic tensor networks, for instance taking the role of isometries in
the TTN and MERA. As these operators can change the degeneracies of
charges in a multi-index @xmath , we will call them degeneracy-changing
anyonic operators. More generally, the degeneracy-preserving anyonic
operators may be considered a subclass of the degeneracy-changing
anyonic operators for which @xmath .

#### 4.3.1 Degeneracy-Preserving Anyonic Operators

We begin with those operators which map states on some Hilbert space
@xmath into other states on the same Hilbert space @xmath . Examples of
these operators include Hamiltonians, reduced density matrices, and
unitary transformations such as the disentanglers of the MERA.

First, we introduce splitting trees. The space of splitting trees is
dual to the space of fusion trees. While the space of fusion trees
consists of labelled directed graphs whose number of branches increases
monotonically when read from bottom to top, the space of splitting trees
consists of labelled directed graphs whose number of branches increases
monotonically when read from top to bottom. An inner product is defined
by connecting the leaves of fusion and splitting trees which have
equivalent linear orderings of the leaves (braiding first if necessary),
then eliminating all loops as per Fig. 4.4 (i), with @xmath moves
performed as required.

Anyonic operators may always be written as a sum over fusion and
splitting trees, such as the two-site operator @xmath shown in Fig. 4.4
(ii), and for degeneracy-preserving anyonic operators it is always
possible to choose the splitting tree to be the adjoint of the fusion
tree. To apply an operator to a state the two corresponding diagrammatic
representations are connected as shown in Fig. 4.4 (iii), and closed
loops may be eliminated as shown in Fig. 4.4 (i). Sequences of @xmath
moves, braiding, and loop eliminations may be performed until the
diagram has been reduced once more to a fusion tree without loops on a
lattice of @xmath sites.

Much as the state of an anyonic system may be represented by a vector
@xmath , anyonic operators may be represented by a matrix @xmath . Each
value of @xmath corresponds to a pair @xmath where @xmath is a possible
charge of the central edge of the operator diagram [e.g. @xmath in Fig.
4.4 (ii)], and @xmath is a value of the degeneracy index associated with
charge @xmath . We will denote the degeneracy of @xmath by @xmath .
Similarly, values of @xmath correspond to pairs @xmath where @xmath has
degeneracy @xmath . For degeneracy-preserving anyonic operators the
charge indices @xmath and @xmath necessarily take on the same range of
values, and @xmath when @xmath . The values of @xmath may equivalently
be calculated from either the fusion tree making up the top half or the
splitting tree making up the bottom half of the operator diagram.

A well-defined anyonic operator @xmath must respect the (quantum)
symmetry group of the anyon model, and consequently all entries in
@xmath for which @xmath will be zero. However, in contrast with @xmath
we do not require that @xmath . When @xmath is a degeneracy-preserving
operator, matrix @xmath is therefore a square matrix of side length

  -- -------- -- -------
     @xmath      (4.5)
  -- -------- -- -------

which may be organised to exhibit a structure which is block diagonal in
the charge indices @xmath and @xmath , and for which the blocks are also
square. As an example consider Fig. 4.5 , which shows an operator acting
on four Fibonacci anyons. An example matrix @xmath for an operator of
this form is given in Table 4.1 , from which the entries of @xmath can
be reconstructed, e.g. @xmath .

#### 4.3.2 Degeneracy-Changing Anyonic Operators

We now introduce the second class of anyonic operators, which map states
in some Hilbert space @xmath into some other Hilbert space @xmath .
These operators may reduce or increase the degeneracy of any charge
present in the spaces on which they act, and may even project out entire
charge sectors by setting their degeneracy to zero. When these operators
are written in the conventional notation of Fig. 4.4 , the fusion and
splitting trees will not be identical. Further, we may choose to allow
combinations of degeneracies which do not naturally admit complete
decomposition into individual anyons. For example, a degeneracy-changing
operator may map a state on five Fibonacci anyons (having total
degeneracies @xmath , @xmath ) into a state having degeneracies @xmath ,
@xmath . As these degeneracies do not admit decomposition into an
integer number of nondegenerate anyons, it is necessary to associate an
index @xmath with the single open leg of the fusion tree. This index
behaves identically to the vertex indices @xmath of Fig. 4.1 , serving
to enumerate the different copies of each individual charge, and as with
the vertex indices of Fig. 4.1 , it is absorbed into the degeneracy
index @xmath .

As a further example, a state having degeneracies @xmath , @xmath could
be associated with a fusion tree having either one leg, or two legs each
with degeneracies @xmath , @xmath . Again, indices @xmath would have to
be associated with each open leg.

Matrix representations of degeneracy-changing anyonic operators may also
be constructed, and when they are written in block diagonal form, the
matrices and their blocks may be rectangular rather than square.
Degeneracy-changing anyonic operators therefore represent a
generalisation of the degeneracy-preserving anyonic operators discussed
in Sec. 4.3.1 . It is worth noting that the presence of indices @xmath
on the open legs of the fusion or splitting trees of an operator do not
automatically imply that it is a degeneracy-changing anyonic operator:
The defining characteristic of a degeneracy-preserving anyonic operator
is that it maps a state in a Hilbert space @xmath into a state in the
same Hilbert space @xmath , and consequently both the matrix as a whole
and all of its blocks are square. Thus a degeneracy-preserving anyonic
operator may act on states having additional indices @xmath on their
open legs, and the resulting state may be expressed in the form of the
same fusion tree, with the same additional indices on the open legs.

Operators which change degeneracies may represent physical processes
which change the accessible Hilbert space of a system. As we will see in
Sec. 4.5.1 , they may also be used in tensor network algorithms as part
of an efficient representation of particular states or subspaces of a
Hilbert space, for example the ground state or the low energy sector of
a local Hamiltonian.

This distinction between degeneracy-changing and degeneracy-preserving
anyonic operators is clearly seen with a simple example. Let @xmath be a
state on six Fibonacci anyons. This state can be parameterised by a
vector @xmath , which has five components. We now define two projection
operators, @xmath and @xmath (Fig. 4.6 ), each of which acts on the
fusion space of anyons @xmath and @xmath . Operator @xmath is
degeneracy-preserving, and projects @xmath into the subspace in which
anyons @xmath and @xmath fuse to the identity. Its matrix representation
is

  -- -------- -- -------
     @xmath      (4.6)
  -- -------- -- -------

where the first value of each multi-index corresponds to a charge of 1,
and the second to a charge of @xmath . Operator @xmath performs the same
projection, but is degeneracy-changing. Its matrix representation is
written

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

Both operators perform equivalent projections, in the sense that

  -- -------- -- -------
     @xmath      (4.8)
  -- -------- -- -------

When @xmath acts on @xmath it leaves the Hilbert space unchanged, and
hence the vector @xmath describing state @xmath is once again a
five-component vector, although in an appropriate basis some components
will now necessarily be zero. In contrast @xmath explicitly reduces the
dimension of the Hilbert space, and the vector @xmath describing state
@xmath is of length two, describing a fusion tree on only four Fibonacci
anyons (as both @xmath and @xmath have been eliminated). One consequence
of this distinction is that while @xmath , the value of @xmath is
undefined.

### 4.4 Anyonic Tensor Networks

#### 4.4.1 Diagrammatic Notation

The diagrammatic notation conventionally employed in the study of
anyonic systems, and used here in Figs. 4.1 and 4.4 , is well suited to
the complete description of anyonic systems, as it provides a physically
meaningful depiction of the entire Hilbert space. However, the number of
parameters required for such a description grows exponentially in the
system size, and because it is necessary to explicitly assign every
index to a specific charge or degeneracy, specification of a tensor
network rapidly becomes inconveniently verbose [for example see Fig. 4.4
(iii)].

In the preceding Sections, we developed techniques whereby anyonic
states and operators could be represented as vectors and matrices,
bearing only one or two multi-indices apiece. We now introduce the
graphical notation which complements this description, and in which we
will formulate anyonic tensor networks. Figure 4.7 (i) gives the
graphical representations of a state @xmath associated with a vector
@xmath , and of an operator @xmath associated with a matrix @xmath . The
circle marked @xmath corresponds to the vector @xmath , and the circle
marked @xmath corresponds to the matrix @xmath . In general, grey
circles correspond to tensors, and the number of legs on the circle
corresponds to the number of multi-indices on the associated tensor.
Each multi-index is also associated with a fusion or splitting tree
structure, which is specified graphically. For reasons to be discussed
shortly, we will require that no tensor ever have more than three
multi-indices. As the legs of the grey shapes are each associated with a
multi-index, they carry both degeneracy and charge indices. Consequently
it is not necessary to explicitly assign labels to the fusion/splitting
trees, as these labellings are contained implicitly in the degeneracy
index (for example see Table 4.1 , where specifying the values of @xmath
and @xmath is equivalent to fully labelling the fusion and splitting
trees of Fig. 4.5 ).

The fusion or splitting tree associated with a particular multi-index
may be manipulated in the usual way by means of braids and @xmath moves,
recalling that each component of the tensor is associated with a
particular labelling of the fusion and splitting trees via the
corresponding values of the multi-indices. Manipulations performed upon
a particular tree thus generate unitary matrices which act upon the
multi-index that corresponds to the labellings of that particular tree.

The application of an operator to a state is, unsurprisingly, performed
by connecting the appropriate diagrams, as shown in Fig. 4.7 (ii). For
operators of the type discussed in Sec. 4.3.1 , the outcome is
necessarily a new state in the same Hilbert space, which consequently
can be described by a new state vector @xmath , as shown. However, in
general an operator @xmath will not act on the entire Hilbert space of
the system, and so will be described by a tensor constructed on the
fusion space of some subset of lattice sites, and not on the system as a
whole. Operator @xmath acting on state @xmath in Fig. 4.7 (ii) is an
example of this. Because @xmath describes a six-site system but @xmath
is constructed on the fusion space of two sites, the multi-indices of
@xmath span a significantly smaller Hilbert space than that of @xmath
and we cannot simply write

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

(using Einstein notation, where repeated multi-indices are assumed to be
summed). Instead, we must understand how to expand the matrix
representation of an operator on some number of sites @xmath , to obtain
its matrix representation as an operator on @xmath sites, where @xmath .

#### 4.4.2 Site Expansion of Anyonic Operators

The multiplicity tensor @xmath describes the fusion of two charges
without degeneracies. It is easily extended to incorporate degeneracies
of the charges, and we will denote this expanded multiplicity tensor
@xmath where multi-indices @xmath , @xmath , and @xmath are associated
with the pairs @xmath , @xmath , and @xmath respectively, and for given
values of @xmath , @xmath , and @xmath , @xmath runs from 1 to @xmath .
The degeneracies associated with charges @xmath , @xmath , and @xmath
are denoted @xmath , @xmath , and @xmath respectively. As with @xmath ,
@xmath , and @xmath , there is an implicit additional index on each
degeneracy @xmath representing the edge of the tree on which charge
@xmath resides. The values of @xmath and @xmath may be chosen
arbitrarily (for example, @xmath may differ from @xmath ), but the
degeneracies associated with the values of @xmath must satisfy

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

in accordance with Eq. ( 4.2 ). When this constraint is satisfied, every
quadruplet of indices @xmath corresponding to a unique pair of choices
for @xmath and @xmath may be associated with @xmath distinct pairs of
indices @xmath for each @xmath . These pairs @xmath are enumerated by
the additional index @xmath . This defines a 1:1 mapping between sets of
values on @xmath and pairs @xmath , and we set the corresponding entries
in @xmath to 1, with all other entries being zero. A simple example is
given in Table 4.2 .

By virtue of their derivation from @xmath , the object @xmath and its
conjugate @xmath represent application of the anyonic fusion rules, and
may be associated with vertices of the splitting and fusion trees. Under
the isotopy invariance convention there is an additional factor of
@xmath associated with the fusion of charges @xmath and @xmath into
@xmath , where @xmath is the quantum dimension of charge @xmath , and
similarly for splitting, but we will account for these factors
separately (see Sec. 4.7.1 ). Thus constructed, the tensors @xmath
satisfy @xmath .

When used as a representation of the fusion rules, the generalised
multiplicity tensor @xmath and its conjugate @xmath permit us to
increase or decrease the number of multi-indices on a tensor in a manner
which is consistent with the fusion rules of the quantum symmetry group.
This process is reversible provided the symmetry group is Abelian or,
for a non-Abelian symmetry group, provided the total number of
multi-indices on the tensor does not at any time exceed three. In
constructing and manipulating a tensor network for a system of anyons,
we will require only objects which respect the fusion rules of the anyon
model. It is a defining property of such objects that when the number of
multi-indices they possess is reduced to 1 by repeated application of
@xmath and @xmath , non-zero entries may be found only in the vacuum
sector. We imposed this requirement for states in Sec. 4.2 , and it is
equivalent to the restriction we imposed on anyonic operators in Sec.
4.3.1 . In Singh et al. ( 2010a ) (Sec. 3.1 of this Thesis), an
equivalent condition was observed for tensors remaining unchanged under
the action of a Lie group, and these tensors were termed invariant .
When working with invariant tensors, we may separately evaluate the
components of the tensors acting on the degeneracy spaces (e.g. the
nonzero blocks of @xmath ), and the factors arising from loops and
vertices of the associated spin network (see Sec. 4.7.1 for details).
This property greatly simplifies the contraction of pairs of tensors.

In addition to increasing or decreasing the number of legs of a tensor,
we may also use @xmath to “raise” the matrix representation of an
operator from the space of @xmath sites to the space of @xmath sites.
This is shown in Fig. 4.8 , and the matrix representation of the raised
operator is given by

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

where multi-index @xmath describes the fusion space of all sites in
@xmath but not in @xmath . Because the numeric factors associated with
loops and vertices (and braiding where applicable) are handled
separately, no factors of quantum dimensions appear in Eq. ( 4.11 ).

To act an operator @xmath on a state @xmath in the matrix
representation, we therefore connect the diagrams for @xmath and @xmath
, eliminate all loops, and then raise the matrix representation of the
operator @xmath using Eq. ( 4.11 ), repeatedly if necessary, until the
resulting matrix @xmath may be applied directly to the state vector
@xmath . Similarly it is possible to combine the matrix representations
of operators, by connecting their diagrams appropriately, eliminating
loops, and performing any required raising so that both operators act on
the same fusion space. Their matrix representations can then be combined
to yield the matrix representation of the new operator:

  -- -------- -- --------
     @xmath      (4.12)
  -- -------- -- --------

and the fusion/splitting tree associated with this new operator is
obtained as shown in Fig. 4.8 .

Note that as yet, we have not described how two objects may be combined
if their multi-indices are both up or both down, and are connected by a
curved line. To contract such objects together, it is necessary to
understand how bends act on the central matrix of an operator. Once this
is understood, the bend can be absorbed into one of the central
matrices, so that the connection is once again between an upper and a
lower multi-index as in Eq. ( 4.12 ). This process is described in Sec.
4.4.3 .

#### 4.4.3 Manipulation of Anyonic Operators

As observed in Sec. 4.4.2 , when we describe a system entirely in terms
of objects invariant under the action of the symmetry group, we may
account separately for the numerical normalisation factors associated
with the spin network. However, as well as affecting these numerical
factors, transformations of the fusion or splitting tree of an anyonic
operator will typically also generate unitary matrices which act on the
matrix representation of the operator. These matrices respect the
symmetry of the anyon model, and thus can be written as block-diagonal
matrices where each block is a unitary matrix acting on a particular
charge sector. In terms of the diagrammatic notation of Sec. 4.4.1 ,
@xmath moves and braids therefore result in the insertion of a unitary
matrix, as shown in Fig. 4.9 . These matrices, whose entries are derived
from the tensors @xmath and @xmath respectively, are raised if required,
as described in Sec. 4.4.2 , and then contracted with @xmath , the
matrix representation of the operator. To compute the unitary matrices
involved, it suffices to recognise that @xmath moves and braids are
unitary transformations in the space of labelled tree diagrams.
Identifying the leg on which the unitary matrix is to be inserted, the
relevant region of the space of labelled diagrams is then enumerated by
the multi-index which can be associated with this leg (compare Fig. 4.2
).

Braiding is of particular importance when working in two dimensions, as
an operator will necessarily be defined with respect to some arbitrary
linear ordering of its legs, and when manipulating a tensor network it
may be necessary to map between this original definition and other
equivalent definitions, corresponding to different leg orderings. For
example, let @xmath be a four-site anyonic operator as shown in Fig.
4.10 (i), which we wish to apply to a 2D lattice. For the indicated
linearisation of this lattice, application of @xmath will require
braiding as shown in Fig. 4.10 (ii). By evaluating the unitary
transformations corresponding to these braids and absorbing them into
@xmath , we may define a new operator @xmath which acts directly on the
linearised lattice without any intervening manipulations of the
fusion/splitting trees.

We will also frequently wish to deal with tensor legs which bend
vertically through 180 @xmath . If working with an anyon model that has
non-trivial Frobenius–Schur indicators, then indicator flags must be
applied to all bends. Like @xmath moves and braiding, the reversal of a
Frobenius–Schur indicator flag is a unitary transformation, and once
again this leads to the introduction of a unitary matrix which can be
absorbed into a nearby existing tensor. However, we may wish to perform
other operations on bends, such as absorbing them into fusion vertices
or the central matrices of anyonic operators. We may also need to move a
matrix @xmath across a bend. We must therefore develop the description
of bends in the new diagrammatic formalism.

In Bonderson ( 2007 ) a prescription for absorbing bends into fusion
vertices is given in terms of tensors @xmath and @xmath , derived from
the @xmath moves, and corresponding to clockwise and counter-clockwise
bends respectively. The absorption of a clockwise or counterclockwise
bend into a fusion vertex is reproduced in Fig. 4.11 (i), and results in
a vertex fusing upward- and downward-going legs. We now assign new
tensors @xmath and @xmath to such vertices, such that writing these
transformations in the notation of Sec. 4.4.1 is trivial. This is shown
in Fig. 4.11 (ii).

Explicit expressions for the new vertex tensors @xmath and @xmath may be
obtained by recognising that Fig. 4.11 (i) describes the action of
unitary transformations on @xmath . When the bend is counterclockwise,
the corresponding unitary matrix is derived from @xmath , and when the
bend is clockwise, the unitary matrix is derived from @xmath . We will
denote these unitary matrices @xmath and @xmath respectively. We then
have

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (4.13)
     @xmath   @xmath   @xmath      (4.14)
  -- -------- -------- -------- -- --------

and conjugation describes equivalent vertices @xmath and @xmath when a
bend is absorbed into a splitting tree.

Knowing how the absorption of bends acts on a vertex tensor, we may
readily infer how the same process acts on the matrix representation of
an operator. In Fig. 4.11 (iii) we see a bend absorbed into the matrix
@xmath , resulting in a new object with two lower multi-indices, @xmath
. First we exploit the freedom to introduce fusion with the trivial
charge (denoted @xmath ), with degeneracy 1. The corresponding @xmath
object takes only one value on its upper left multi-index, and is fully
defined by @xmath . Absorbing the bend into this fusion vertex as per
Eq. ( 4.13 ) yields @xmath , which may be then combined with @xmath to
give

  -- -------- -- --------
     @xmath      (4.15)
  -- -------- -- --------

In conjunction with the relationships given in Fig. 4.12 , this gives us
the ability to move a matrix past a bend. An example of this is given in
Fig. 4.13 , for which @xmath and @xmath are related according to

  -- -------- -- --------
     @xmath      (4.16)
  -- -------- -- --------

where @xmath represents reversal of the Frobenius–Schur indicator flag
on the lower bend. Finally, bending may also allow more efficient
contraction of pairs of anyonic operators, as shown in Fig. 4.14 .

Having described the action of bends, it is customary also to introduce
a second type of @xmath move which is described by the tensor @xmath
(Fig. 4.15 ). This tensor may be derived from @xmath by bending, and as
with @xmath these @xmath moves perform a transformation of the fusion
tree, accompanied by the introduction of a unitary matrix which can be
absorbed into the matrix representation of the operator. These unitary
matrices correspond to the consecutive application of a bend, an @xmath
move of the original type, and a second bend whose action is the inverse
of the first.

#### 4.4.4 Constructing a Tensor Network

Now that we have developed a formalism for anyonic tensors, we may
convert an existing tensor network algorithm for use with anyons. First,
the tensor network must be drawn in such a manner that every leg has a
discernible vertical orientation. Although these orientations may be
changed during manipulation of the tensor network, an initial assignment
of upward or downward direction is required. Second, all tensors must be
represented by entirely convex shapes, such as circles or regular
polygons. For existing tensor network algorithms such as MERA and PEPS,
this requirement is trivial. However, it is conceivable that future
algorithms might involve superoperator-type objects whose graphical
representations interleave upward- and downward-pointing legs.
Concavities on these objects may be eliminated by replacing some of
their upward-pointing legs with downward-pointing legs (or vice versa),
followed by a bend [Fig. 4.16 (i)-(ii)]. A similar treatment may be
applied to any superoperators which arise during manipulations of the
tensor network, introducing a pair of bends as in Fig. 4.12 (i) and then
absorbing one into the matrix representation of the object.

If working with an anyon model that has non-trivial Frobenius–Schur
indicators, then indicator flags must be applied to all bends. Initial
choices are a matter of convenience, and it is frequently possible to
assign these indicators in opposed pairs, as shown in Fig. 4.12 . If
these paired indicators are not flipped or are only flipped in adjacent
opposed pairs during subsequent manipulations of the tensor network,
then they may frequently be left implicit.

Next, if there exist charges in the anyon model which are not self-dual,
a direction (represented by a solid arrow) must be assigned to every
multi-index. Any tensor with more than three legs (e.g. @xmath in Fig.
4.16 ) is then replaced by a trivalent tensor network consisting of a
core object, e.g. @xmath , which contains the free parameters of the
tensor, and as many copies of @xmath or @xmath as are required to
provide the correct output legs. These tensors @xmath , @xmath
correspond to vertices in the fusion and splitting trees associated with
@xmath , yielding the corresponding anyonic tensor. Objects with three
legs or less can be directly identified with an anyonic tensor object
carrying the appropriate number of indices (i.e. three multi-indices and
a vertex index @xmath ), though for consistency with the methods
described in Sections 4.4.2 and 4.4.3 we point out that it is possible
to similarly replace three-legged objects with anyonic operators
consisting of a central matrix @xmath and a fusion or splitting vertex,
if desired.

Any bends introduced earlier may now be reabsorbed, so that some
vertices now correspond to @xmath , @xmath , @xmath , and @xmath . This
step, however, is optional as it may be more convenient for subsequent
manipulations of the tensor network if the bends are left explicit. The
anyonic tensors are then connected precisely as in the original Ansatz.

Manipulations of the anyonic tensor network are equivalent to those
performed on the spin version of the Ansatz, differing only in that the
degrees of freedom of the tensor network are now expressed entirely by
the at-most-trivalent central objects, and certain topological elements
such as braids and vertical bends must be accounted for in accordance
with the prescriptions of Sec. 4.4.3 . These changes may naturally imply
minor changes to the manipulation algorithms, and we will see examples
of this in the 1D MERA. Similar considerations will apply to other
tensor network algorithms.

Our construction of an anyonic tensor network draws upon two important
elements which have previously been observed in other, simpler, physical
systems:

1.  Tensors in the Ansatz exhibit a global symmetry, which may be
    non-Abelian. Exploiting a non-Abelian symmetry requires that the
    Ansatz be written in the form of a trivalent tensor network. This
    has previously been observed and implemented for non-Abelian Lie
    group symmetries such as SU(2) ( Singh et al., 2010a , Sec. 3.1 of
    this Thesis; Singh and Vidal, in preparation ).

2.  Tensors in the Ansatz must be able to account for non-trivial
    exchange statistics. This has previously been observed in the
    simulation of systems of fermions (Corboz et al., 2010a ; Kraus
    et al., 2010 ; Pineda et al., 2010 ; Corboz et al., 2010b ; Barthel
    et al., 2009 ; Shi et al., 2009 ; Pižorn and Verstraete, 2010 ; Gu
    et al., 2010 ) , where efficient implementation of particle
    statistics can be achieved through the use of “swap gates” (Pineda
    et al., 2010 ; Corboz et al., 2010b ; Barthel et al., 2009 ; Pižorn
    and Verstraete, 2010 ) .

In both cases, anyonic tensor networks extend the concepts introduced in
previous work. The symmetry structure of an anyon model may be a quantum
group, for example a member of the series @xmath , @xmath , rather than
having to be a Lie group, and this permits representation of non-Abelian
anyonic systems whose Hilbert space does not admit decomposition into a
tensor product of local Hilbert spaces. Similarly, anyonic braiding may
be implemented using a generalisation of the fermionic “swap gate”
formalism. When braiding, particle exchange may introduce transformation
by a unitary matrix rather than by a sign, and efficient implementation
of the resulting swap gates is particularly important for the simulation
of 2D systems.

Although anyonic systems pose a number of unique challenges, we see that
these are addressed by developments based on existing techniques, and we
therefore anticipate that the resulting generalisations of existing
tensor network Ansätze should still be capable of accurately
representing the states of an anyonic system.

#### 4.4.5 Contraction of Anyonic Tensor Networks

The techniques described in Secs. 4.4.2 and 4.4.3 ( @xmath moves,
braids, bending of legs, elimination of loops, diagrammatic isotopy,
flipping of Frobenius–Schur indicator flags, and the use of @xmath
tensors) suffice to contract any network of anyonic tensors written in
the form of matrices with degeneracy indices, and unlabelled trees.
Through careful application of these techniques, and avoiding at all
times processes which would yield a tensor with more than three legs,
the matrix representations of any pair of contiguous tensors in a
network may always be brought into conjunction such that their
multi-indices can be contracted in the manner of Eq. ( 4.12 ), and any
tensor network may be contracted by means of a sequence of such pairwise
contractions.

That a tensor network may represent a system of anyons in this way is
possible because throughout the anyonic tensor network, each value of a
degeneracy index is associated with a specific labelling of the
corresponding unlabelled tree. Consequently it is always possible to
fully reconstruct any operation in terms of the more verbose
representation of Fig. 4.4 .

An anyonic tensor network is therefore fully specified merely by the
unlabelled tree (with Frobenius–Schur indicator flags if required), and
the values and locations of the matrix representations of its tensors,
written in the degeneracy index form.

### 4.5 Example: The 1D MERA

#### 4.5.1 Construction

To construct an anyonic MERA for a 1D lattice with @xmath sites, where
@xmath satisfies @xmath , we begin with a “top” tensor on a two-site
lattice @xmath whose matrix representation is of a computationally
convenient size. [The top tensor is named for its position in the usual
diagrammatic representation of the MERA, where diagrams with open legs
at the bottom correspond to a ket. For anyons the converse convention
applies, and consequently in Fig. 4.17 (i) the “top” tensor is
ironically located at the bottom.]

To each leg of the top tensor, we now append an isometry [Fig. 4.17
(ii)]. The matrix representations of the isometries consist of
rectangular blocks, as described in Sec. 4.3.2 , and we choose
isometries whose fusion trees have three legs, so as to construct a
ternary MERA (Evenbly and Vidal, 2009a ) . Next, disentanglers are
applied above the isometries. For periodic boundary conditions this must
be performed in a manner which respects the anyonic braiding rules, as
shown in Fig. 4.17 (iii). We identify the open legs of the resulting
network as the sites of a lattice @xmath , and the rows of disentanglers
and isometries may be understood as a coarse-graining transformation
taking a finer-grained lattice @xmath into a coarser-grained lattice
@xmath , similar to the standard MERA. Note that the geometry of the
periodic lattice is reflected by the connections of the disentanglers.
Specifically, whether the outside legs are braided over or under the
other lattice sites reflects whether the lattice closes towards or away
from the observer.

The application of anyonic isometries and disentanglers is now repeated
@xmath times [Fig. 4.17 (i)-(iii) corresponds to @xmath ], until the
Ansatz has @xmath legs. The final row of isometries should be chosen
such that each of their upper legs have the same charges and
degeneracies as the sites of the physical lattice @xmath , and the open
legs above the last row of disentanglers are identified with the
physical lattice. For coarse-grained lattices @xmath to @xmath , the
dimensions of the lattice sites correspond to the lower legs of the
isometries and are chosen for computational convenience, subject to the
requirement that each charge sector is sufficiently large to adequately
reproduce the physics of the low-energy portion of the Hilbert space.
For all other legs, their charges and degeneracies are determined by
requiring consistency with Eq. ( 4.2 ). Initial choices of which charges
to represent on the “top” tensor and on the lower legs of the
isometries, and with what degeneracies, must be guided either by prior
knowledge about the physical system, or by balancing computational
convenience against the inclusion of a broad and representative range of
possible charges. When used in a numerical optimisation algorithm, the
choice of relative weightings for the different charge sectors may often
be refined by examination of the spectra of the reduced density matrices
on the coarse-grained lattices, after initial optimisation of the tensor
network is complete.

This concludes construction of the MERA for a state on a finite,
periodic 1D anyonic lattice. That this tensor network does represent an
anyonic state is easily seen by sequentially raising tensors, performing
@xmath moves, and combining tensors, until the entire network is reduced
to a single vector whose length is equal to the dimension of the
physical Hilbert space, and an associated fusion tree. These then
represent the state of the system as per Eq. ( 4.4 ). The structure of
this tensor network closely resembles that of the normal MERA, according
to the identifications given in Fig. 4.17 , and consequently we
anticipate that it will share many of the same properties, including the
ability to reproduce polynomially decaying correlators in strongly
correlated physical systems. Open lattices may also be easily
represented by omitting the braided disentanglers at the edge of the
diagram.

We also note that in common with the MERA for spins, the anyonic MERA
may be understood as a quantum circuit, although one which carries
anyonic charges in its wires. Any junction in the fusion/splitting trees
may be associated with a @xmath or @xmath tensor, and the entire network
may be considered as the application of a series of gates to a Hilbert
space of fixed dimension beginning mostly (or entirely, if the top
tensor is considered to be the first gate) in the vacuum state, with
individual gates introducing entanglement across some limited number of
wires.

#### 4.5.2 Energy Minimisation

The anyonic MERA can be used as a variational Ansatz to compute the
ground state of a local Hamiltonian. The Hamiltonian is introduced as a
sum over nearest neighbour interactions, each term having the form of
Fig. 4.17 (iv), and optimisation of the tensor network is carried out in
the usual manner (Evenbly and Vidal, 2009a ) . Also as per usual,
Hamiltonians involving larger interactions, such as next-to-nearest
neighbour, can be accommodated by means of an initial exact @xmath
-into-one coarse-graining of the physical lattice.

As in Evenbly and Vidal ( 2009a ) , optimisation of the MERA then
consists of repeatedly lifting the Hamiltonian from @xmath to the
coarse-grained lattices, updating their isometries and disentanglers,
and lowering the reduced density matrix, or the top tensor and its
conjugate. When lifting the Hamiltonian or lowering the reduced density
matrix, then the diagrams in Evenbly and Vidal ( 2009a ) taken in
conjunction with the key given in Fig. 4.17 serve to describe networks
of anyonic operators which, when contracted to a single operator, yield
the lifted form of the Hamiltonian or lowered form of the reduced
density matrix respectively. Similarly, when optimising disentanglers or
isometries, the diagrams of Evenbly and Vidal ( 2009a ) and the
identifications in Fig. 4.17 indicate how to construct an anyonic
operator which constitutes the environment of the anyonic operator being
optimised. However, once the admissible ranges of charges and
degeneracies on each leg have been fixed, the only optimisable content
of an anyonic operator is its matrix representation. Consequently, the
fusion and splitting tree contributions should be evaluated and absorbed
into the operator and its environment, reducing them both to their
matrix representations, denoted @xmath and @xmath respectively (see Fig.
4.18 ). If the singular value decomposition of @xmath is written @xmath
, then the updated matrix content @xmath of the anyonic operator being
optimised is given by @xmath , minimising the value of @xmath subject to
the usual constraint for disentanglers and isometries that @xmath [Fig.
4.17 (v)-(vi)]. The fusion/splitting tree content of the operator can
then be restored, along with any appropriate numerical factors that may
be required.

As with the standard MERA, the “top” tensor is constructed by
diagonalising the total Hamiltonian on the most coarse-grained lattice,
@xmath on @xmath . As @xmath is a two-site lattice, the total
Hamiltonian @xmath is a sum of two terms, @xmath and @xmath . For the
translation-invariant anyonic MERA, we may formally define @xmath in
terms of @xmath as shown in Fig. 4.19 , and the top tensor @xmath
(together with any factors arising from the chosen normalisation scheme)
then corresponds to the lowest-energy eigenstate of @xmath .

#### 4.5.3 Scale-Invariant MERA

Having identified the anyonic counterparts of the tensors of the
standard MERA, and described how these tensors may be lifted, lowered,
and optimised, the algorithm for the scale-invariant MERA described in
Pfeifer et al. ( 2009 ) may also be implemented for anyonic systems,
simply by applying the dictionary of Fig. 4.17 and the techniques
described in Sec. 4.5.2 . As with optimisation of @xmath and @xmath ,
the computation of the top reduced density matrix (which is a descending
eigenoperator of the scaling superoperator with eigenvalue 1) may be
understood as a calculation of the matrix component @xmath of the
reduced density matrix @xmath . The ascending eigenoperators of the
scaling superoperator, or local scaling operators of the theory, may
also be computed in this manner.

#### 4.5.4 Results

To demonstrate the effectiveness of the anyonic generalisation of the
MERA, we applied it to a 1D critical system of anyons whose physical
properties are already well known: The golden chain (Feiguin et al.,
2007 ) . This model consists of a string of Fibonacci anyons subject to
a local interaction. Fibonacci anyons have only two charges, @xmath (the
vacuum) and @xmath , and one non-trivial fusion rule ( @xmath ). The
simplest local interactions for a chain of Fibonacci @xmath anyons are
nearest neighbour interactions favouring fusion of pairs into either the
@xmath channel (termed antiferromagnetic, or AFM), or the @xmath channel
(termed ferromagnetic, or FM). Both choices correspond to critical
Hamiltonians, associated with the conformal field theories @xmath and
@xmath for AFM and FM couplings respectively. Individual lattice sites
are each associated with a charge of @xmath . \nomenclature AFM
Antiferromagnetic. \nomenclature FM Ferromagnetic.

The AFM and FM Hamiltonians act on pairs of adjacent Fibonacci anyons.
On a pair of lattice sites each carrying a charge of @xmath , the matrix
representations of the AFM and FM Hamiltonians are written

  -- -------- -- --------
     @xmath      (4.17)
  -- -------- -- --------

where a multi-index value of 1 corresponds to the vacuum charge, 2
corresponds to @xmath , and the charges are non-degenerate. We optimised
a scale-invariant MERA on the golden chain for each of these
Hamiltonians, and computed local scaling operators using the tensor
network given in Fig. 4.20 . The operators calculated using this diagram
may be classified according to the values of the charge labels @xmath
and @xmath , and the scaling dimensions and conformal spins which we
obtained are given in Tables 4.3 and 4.4 , and Fig. 4.21 .

Comparison of the AFM case with existing results in the literature show
that the scaling dimensions obtained when @xmath correspond to those
obtained when studying a system of anyons with a toroidal fusion diagram
(Feiguin et al., 2007 ) . For a system of anyons on the torus it is
possible to define an additional topological symmetry (Feiguin et al.,
2007 ) and classify local scaling operators according to whether or not
they respect this symmetry. Operators satisfying @xmath correspond to
those which respect the topological symmetry, and those satisfying
@xmath do not. We will discuss the interpretation of the different
sectors and their relationship to anyons on the torus in a forthcoming
paper (Pfeifer and Vidal, 2010 ) .

When @xmath the scaling operators obtained are chiral, with those
obtained from @xmath and @xmath believed to form conjugate pairs.

### 4.6 Summary

Numerical study of systems of interacting anyons is difficult due to
their non-trivial exchange statistics. To date, study of these systems
has been restricted to exact diagonalisation, Matrix Product States
(MPS) for 1D systems, or special-case mappings to equivalent spin
chains. This paper shows how any tensor network Ansatz may be translated
into a form applicable to systems of anyons, opening the door for the
study of large systems of interacting anyons in both one and two
dimensions. As an example, this paper demonstrates how the MERA may be
implemented for a 1D anyonic system. This Ansatz is particularly
important as many 1D systems of anyons are known which exhibit extended
critical phases (see e.g. Feiguin et al., 2007 ; Trebst et al., 2008a ,
b ) . The structure of the MERA is known to be particularly well suited
to reproducing long range correlations, and the scale-invariant MERA has
the additional advantage of providing simple and direct means of
computing the scaling dimensions and matrix representations of local
scaling operators.

We applied the scale invariant MERA to infinite chains of Fibonacci
anyons under antiferromagnetic and ferromagnetic nearest neighbour
couplings, and identified a large number of local scaling operators. Our
results for the scaling dimensions are in agreement with those
previously obtained by exact diagonalisation of closely related systems,
and for the relevant primary fields they are within @xmath of the
theoretical values obtained from conformal field theory. We thus
demonstrate that an anyonic MERA with @xmath permits conclusive
identification of the relevant conformal field theory, and gives a level
of accuracy comparable to that of the scale invariant MERA on a spin
chain (Table 2.1 of this Thesis; Pfeifer et al., 2009 ).

The anyonic generalisation of the 1D MERA presented here is useful in
its own right, but the greatest significance of the approach described
is that it is equally applicable to 2D tensor network Ansätze, and hence
opens the door to studying the collective behaviour of large systems of
anyons in two dimensions by numerical means, in situations where
analytical solutions may not be possible.

Note— Simultaneous with the work presented in this Chapter, the 1D MERA
for systems of anyons was also independently constructed by König and
Bilgin ( 2010 ) on the torus. These authors provide proof of principle
by computing ground state energies and two-point correlators for finite
systems of Fibonacci anyons with @xmath ( @xmath in their notation),
with errors in the energy on the order of a few percent. Once again we
see that even for small values of @xmath , the anyonic MERA is capable
of providing an accurate description of the low-energy behaviour of a
system of interacting anyons.

### 4.7 Some Notes on Implementation

#### 4.7.1 Block Structure

Recall that in Sec. 3.5 we saw that an efficient way of storing Abelian
symmetric tensors was as a number of blocks, and that operations on
these tensors such as fusing and splitting legs amounted to nothing more
than a rearrangement of these blocks. It is possible to implement a
similar scheme for anyonic tensor networks, although with some important
differences.

As with the Abelian symmetric tensors of Chapter 3 , we construct the
central objects (e.g. @xmath ) of our tensors from a number of blocks.
However, in contrast to the Abelian tensors, we no longer simply
assemble these blocks into the final object. Instead, we give each block
a unique identifying index, and introduce a map . This map then records
the locations of the blocks, with each entry in the map corresponding to
a unique labelling of all fusion trees (Fig. 4.22 ). Note that a given
entry in the map may contain a list of more than one block, and each
block listed is associated with a numeric multiplier. To reconstruct the
tensor from the map and blocks, each entry in the map is assembled by
summing over the relevant blocks, each multiplied by their associated
numeric multiplier.

This system may at first seem unwieldly. However, it has a number of
advantages. First, @xmath moves may be performed quickly and
efficiently, with linear recombinations of the blocks of the tensor
being performed simply by modifying the map. Second, braiding will
permute the entries within individual blocks; efficiency gains may be
made by deferring these permutations for as long as possible, letting
them accumulate, and then determining and performing a single,
cumulative operation on each block. By being able to perform @xmath
moves without accessing the contents of individual blocks, it becomes
unnecessary to evaluate these deferred permutations when performing
@xmath moves (or indeed any other unitary operations depending only on
the charges, such as the reversal of Frobenius–Schur indicators), and
this can lead to greater computational efficiency.

Further, the numerical factors associated with this braiding may also be
introduced at the level of the map. This is typically an advantage when
a particular charge labelling of the fusion trees is multiply
degenerate, as is common at higher levels of the MERA. Each charge
labelling is still only associated with a single block, and so applying
these numerical factors at the level of the map requires less operations
than applying them directly to the numerical content of the tensor.

Finally, in Sec. 4.4.2 it was mentioned that it could be convenient to
separate out the handling of normalisation factors associated with the
diagrammatic isotopy convention. This takes place at two levels. First,
when performing an operation such as “raising” an anyonic tensor to act
on the fusion space of a larger number of sites, it is frequently
possible to observe that factors coming from the introduction of
vertices exactly cancel those arising due to the presence of loops; this
is the reason why no factors of quantum dimensions @xmath appear in (
4.11 ). Second, when drawing an anyonic tensor with fusion trees, the
vertex normalisation factors may be kept associated with the vertex .
They therefore do not enter into @xmath except when @xmath absorbs a
vertex during the process of splitting or fusion of multi-indices.
During splitting, when a vertex is absorbed, the associated factors of
@xmath are applied to the coefficients in the map, and not the entries
of the tensor itself. During fusion, again the coefficients are applied
only to the map, only this time they account for both the vertex itself
and the loop which it makes with the central object of the tensor.

With appropriate care, any effects of operations on the contents of
individual blocks may be deferred until an operation is performed which
by its nature must access the contents of the blocks, such as a singular
value decomposition or a matrix multiplication.

#### 4.7.2 Precomputation

If an algorithm employs repeated application of the same series of
tensor manipulations, for example the repeated iterations of
optimisation for the MERA, then many calculations involved in these
operations may be stored, and recycled on subsequent iterations.
Examples are the matrices generated by @xmath moves and braids (Fig. 4.9
), and the permutations of the numerical elements of each block which
are generated by braiding. It is now that the ability to defer
permutations of the elements of a tensor really comes into its own, as
it is only necessary to store the cumulative operation for subsequent
iterations, and not each individual step.

It should be recognised that this Section describes only one particular
scheme for the efficient implementation of anyonic tensors, and that
this approach is by no means necessarily the only means of achieving
this. However, for those interested in pursuing this approach further,
additional discussion of the philosophy of precomputation [in this
instance, as applied to U(1)-symmetric tensors] may be found in the
Appendix of Singh et al. ( 2011 ) , and Sec. 3.4 of this Thesis.

#### 4.7.3 Reminder: The Important Difference Between Fusion and
Splitting Trees

Given the implementation-oriented nature of this final Section of the
Chapter, it seems appropriate to include a timely reminder of the
difference between fusion and splitting trees. Recall that a tree
assembled from fusion vertices describes the state of a system, @xmath ,
and one assembled from splitting vertices describes a state in the dual
space, @xmath , where Hermitian conjugation is performed by vertical
reflection of a tree diagram and complex conjugation of its
coefficients.

Note well that this same rule for Hermitian conjugation applies also to
the vertex tensors @xmath and @xmath , and thus fusing and splitting of
legs takes place differently depending on whether it is acting on a
fusing or a splitting tree. This important distinction will affect not
only the arrangement of the charge blocks, but also of the entries
within each block itself, and the correct implementation of these
processes is one of the cornerstones for implementation of an anyonic
tensor network.

## Chapter 5 Non-Abelian Symmetries of Spin Systems

### 5.1 Unitary Braided Tensor Categories and Group Symmetries

The formalism developed in Chapter 4 constitutes a methodology for
performing tensor network simulations of any physical system which
admits a description in terms of a UBTC. As mentioned in the
introduction to that Chapter, it is also possible to associate a UBTC
with a group @xmath , where the @xmath moves are related to the 6-
@xmath symbols of the group, and the tensor @xmath is related to the
choice of universal braid matrix (which describes the exchange
properties of the irreps). Note that the choice of universal braid
matrix is not in general unique; in fact, we have already seen two
systems with @xmath symmetry but different braiding. The first was the
spin-0 formulation of the Ising model of Eq. ( 3.1 ),

  -- -------- -- ---------
     @xmath      ( 3.1 )
  -- -------- -- ---------

where the @xmath symmetry is associated with a @xmath -radian rotation.
If we write the charge labels of @xmath as 0 @xmath and 1 ( @xmath ),
then the fusion rules may be written

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

(where @xmath denotes that the addition is performed modulo 2), and the
nonzero entries in @xmath are given by

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

The second example of a system with @xmath symmetry is any system of
fermions, as discussed in Sec. 3.5.1 , for example the fermionic
formulation of the Ising model:

  -- -------- -- -------
     @xmath      (5.3)
  -- -------- -- -------

Here, the charges of @xmath correspond to parity, indicating the
presence or absence of a fermion at a site @xmath . The operators @xmath
and @xmath in the Hamiltonian are fermionic creation and annihilation
operators, and the non-zero entries in @xmath for the associated UBTC
are given by

  -- -------- -- -------
     @xmath      (5.4)
  -- -------- -- -------

As @xmath is Abelian, the non-zero @xmath moves in both examples are
simply

  -- -------- -- -------
     @xmath      (5.5)
  -- -------- -- -------

[There are no vertex indices @xmath , @xmath , @xmath , @xmath (see Fig.
4.1 ) in this expression, as all fusion products are non-degenerate.]

For a non-Abelian group such as SU(2), both the @xmath tensor and @xmath
may be more complicated. The charges in SU(2) are the non-negative
half-integers, with fusion rules

  -- -------- -- -------
     @xmath      (5.6)
  -- -------- -- -------

and we may write the @xmath moves for SU(2) as

  -- -- -- -------
           (5.7)
  -- -- -- -------

where @xmath denotes the 6- @xmath symbol

  -- -------- -------- -- -------
     @xmath               (5.8)
     @xmath   @xmath      (5.9)
  -- -------- -------- -- -------

with the sum running over all integer values of @xmath such that the
factorials are of non-negative numbers. When representing quantum
mechanical spin, the half-integer charges are fermionic, and thus the
@xmath tensor is given by

  -- -------- -- --------
     @xmath      (5.10)
  -- -------- -- --------

for any combination of @xmath , @xmath , and @xmath permitted by the
fusion rules ( 5.6 ).

Given the @xmath tensor and the particle exchange tensor @xmath , we may
apply the UBTC formalism of Chapter 4 to any quantum mechanical system
which exhibits a group symmetry. For Abelian symmetry groups, much of
this machinery is redundant and we may prefer the simpler approach
outlined in Chapter 3 . For non-Abelian symmetries, however, this
provides a useful means of exploiting those symmetries.

Expressions ( 5.1 ), ( 5.2 ), and ( 5.4 )–( 5.10 ) in this Section are
adapted from expressions found in Chapter 5 of Bonderson ( 2007 ) .

### 5.2 Fermions revisited

Before presenting the application of the formalism of Chapter 4 to an
example of a system exhibiting a non-Abelian symmetry, we will first
consider its application to a system of fermions. In Sec. 3.5.1 we saw
that adding fermionic statistics to a symmetric tensor network algorithm
involved introducing extra factors of @xmath into permutation
operations, and also into the multiplications used to perform tensor
contraction. This process is counter-intuitive and an approach would be
preferable in which factors associated with particle exchange arise only
during index permutation. This may achieved by representing fermionic
systems using the @xmath and @xmath tensors of Eqs. ( 5.5 ) and ( 5.4 )
in the UBTC formalism of Chapter 4 . However, in truth, the full
machinery of the UBTC tensor network formalism is not necessary.
Instead, it suffices to simply assign a vertical orientation to each leg
and to contract pairs of upgoing legs using the expanded multiplicity
tensor @xmath , and pairs of downgoing legs using its Hermitian
conjugate @xmath , as per Sec. 4.4.2 . When contracting two tensors
together, the counterclockwise ordering of indices on one of these
tensors is now the opposite of that on the other, e.g.

  -- -------- -- --------
     @xmath      (5.11)
  -- -------- -- --------

and there are no longer any concealed particle exchanges within the
equivalent of Fig. 3.8 (3),

  -- -------- -- --------
     @xmath      (5.12)
  -- -------- -- --------

which is now just a simple matrix multiplication.

### 5.3 The 1D Spin-@xmath Heisenberg (Anti)Ferromagnet

The procedure for studying systems with non-Abelian symmetries using a
UBTC-based tensor network is largely the same as that described in
Chapter 4 , with only a couple of minor differences. First, until now I
have implicitly employed a convention where every fusion tree has a
total charge of @xmath , and represents a single state. It is also
possible to use a fusion tree with a total charge @xmath to represent a
single state, where a total charge of @xmath is taken to imply the
existence of an ancillary system not explicitly considered, and having
charge @xmath (for example, for a system of anyons on a finite disc this
ancillary charge may live on the boundary). However, in this Section it
is instead preferable to use a fusion tree with a total charge of @xmath
to represent a subspace of the Hilbert space having dimension @xmath . ¹
¹ 1 There is a subtlety here, in that for a system with SU(2) symmetry,
we might choose to define the Hilbert space such that an orthonormal
basis is given by eigenstates of a complete set of SU(2)-symmetric
commuting operators. Under this choice, a fusion tree always corresponds
to a single state, regardless of total charge. More commonly, however,
for systems exhibiting a non-Abelian symmetry we define the Hilbert
space with respect to a complete set of commuting operators on the
microscopic degrees of freedom of the system. These measurements are not
necessarily SU(2)-symmetric, e.g. measurement of spin in the @xmath
basis for a Heisenberg spin chain. In the resulting basis of this
example, a pair of spin- @xmath fermions can have total spin 0 in
precisely one way, or total spin 1 in three orthogonal ways,
corresponding to the SU(2)-symmetry-breaking measurement of @xmath -axis
spin permitting resolution of a three-dimensional subspace for the
spin-1 triplet [which has a total SU(2) “charge” of 1, with @xmath ].
Second, if we are considering systems with only fermionic and/or bosonic
statistics, it is not necessary to specify the orientation of a braid.
Thus we may denote particle exchange simply by line crossings,

  -- -------- -- --------
     @xmath      (5.13)
  -- -------- -- --------

As examples of systems with SU(2) invariance, we shall consider the 1D
spin- @xmath Heisenberg antiferromagnet and ferromagnet, with periodic
boundary conditions. The former model exhibits a nearest-neighbour
Hamiltonian which favours neighbouring pairs of particles occupying the
singlet state, with total spin 0,

  -- -------- -- --------
     @xmath      (5.14)
  -- -------- -- --------

whereas the latter favours occupation of the triplet sector, with spin
1,

  -- -------- -- --------
     @xmath      (5.15)
  -- -------- -- --------

On two sites, the total Hamiltonians may be written as

  -- -------- -- --------
     @xmath      (5.16)
     @xmath      (5.17)
  -- -------- -- --------

and the diagrams

  -- -------- -- --------
     @xmath      (5.18)
  -- -------- -- --------

correspond to eigensubspaces of these Hamiltonians, with the former
representing the spin-0 singlet state [total SU(2) “charge” 0; @xmath ,
hence a state] and the latter representing the 3-dimensional spin-1
triplet sector [total SU(2) “charge” 1; @xmath , hence a 3-dimensional
subspace]. For the given antiferromagnetic Hamiltonian these diagrams
have eigenvalues @xmath and 0 respectively, whereas for the
ferromagnetic Hamiltonian these eigenvalues are reversed.

This concludes a very simple demonstration of the application of UBTC
tensor networks for the exploitation of non-Abelian symmetries. A fuller
treatment will be provided in Singh and Vidal ( in preparation ) .

## Chapter 6 Summary and Outlook

In this Thesis, we have seen how symmetries—both spatial and
internal—may be exploited in tensor network algorithms. We began in
Chapter 2 with the exploitation of scale invariance, constructing an
Ansatz which naturally reflects the entanglement structure present in
quantum critical systems. Applying this Ansatz to the critical Ising and
three-state Potts models in 1D, we were able to extract most of the
conformal data of the CFTs which describe the continuum limit of these
theories, namely the scaling dimensions, central charge, and the
coefficients of the operator product expansion.

In Chapter 3 we described the mathematical background behind internal
symmetries of lattice models, and developed basic techniques to exploit
Abelian symmetries in tensor network algorithms. These techniques
enabled us to address specific symmetry sectors of models such as the
@xmath and Heisenberg models, and to simulate these systems at a
substantially reduced computational cost. In their own rights, these
techniques therefore substantially increase the power of tensor network
algorithms as tools for the study of symmetric systems.

In Chapter 4 , we turned our attention to systems of anyons. The
simulation of anyonic systems is acknowledged to be challenging: Like
fermions, the study of anyons suffers from the sign problem, ¹ ¹ 1 The
sign problem may be paraphrased as “For the class of Hamiltonians
obtained by taking a bosonic system whose ground state energy may be
computed in polynomial time and introducing fermionic particle exchange
statistics, does there exist an algorithm similarly capable of computing
the ground state energy of these fermionic Hamiltonians in polynomial
time?” This question, which is stated more formally Troyer and Wiese (
2005 ) , remains unanswered, and as demonstrated by Troyer and Wiese ,
is in fact NP-hard. Specific solutions are known for many problems and
problem groups, but there exist many other such fermionic systems whose
ground state may at present only be computed for a cost exponential in
the system size, even though their bosonised equivalents may be solved
in polynomial time. There are of course bosonic systems (such as
frustrated systems, and spin glasses) which are also exponentially hard,
and their fermionic counterparts tend to be exponentially hard as well.
It is the presence of the middle ground, where fermions are “harder”
than bosons, to which the term “the sign problem” is usually applied.
and the problem of developing a general algorithm to compute the ground
state of arbitrary fermionic systems in polynomial time is therefore
known to be at least NP-hard (Troyer and Wiese, 2005 ) . Nevertheless,
by means of a non-trivial generalisation of the techniques introduced in
Chapter 3 , we were able to develop a formalism of tensor networks for
anyons, allowing us to compute a close approximation to the ground state
of an anyonic system in polynomial time, provided the entanglement
structure of that ground state may be effectively represented by an
appropriate tensor network algorithm (see the discussion on entanglement
in Sec. 2.1.2 ).

Finally, in Chapter 5 it was seen that the formalism developed for
anyons in Chapter 4 may also be applied to exploit non-Abelian
symmetries of spin systems, such as the SU(2) symmetry of the Heisenberg
(anti)ferromagnet, or indeed to exploit Abelian symmetries in the
presence of possibly non-trivial exchange statistics. In fact, the
formalism of Chapter 4 may be used to study systems of bosons, fermions,
Abelian and non-Abelian anyons, and to exploit the presence of Abelian
and non-Abelian internal symmetries of the Hamiltonian in any of these
systems. ² ² 2 With minimal modification, the formalism may even be
applied to systems where particle exchange is not possible, and the
system is a 1D open chain described by a unitary tensor category
admitting no solutions to the hexagon equation; however, the author is
as yet unaware of any interesting physical models of this form.

The exploitation of symmetries is a powerful tool, vastly increasing the
reach and power of tensor network algorithms as a condensed matter
technique. Of all the developments described above, perhaps the most
exciting is the extension of tensor network algorithms to anyons,
opening the door to the study of a vast array of condensed matter
systems, many of which have never been studied before. Many of the
questions to be asked are of great significance—for example, consider
the Fibonacci anyons studied in Sec. 4.5.4 . They can implement
universal quantum computation through braiding alone, are believed to
appear as quasiparticles in the @xmath fractional quantum Hall state,
and yet we are only beginning to understand their phase diagrams under
even the simplest of interactions (e.g. Trebst et al., 2008a ) . Anyonic
tensor networks are a powerful tool for asking fundamental questions
about such systems, and could be of vital importance to the coming
quantum revolution in information processing. They also provide an
unrivalled opportunity to gain insight into this fascinating and
comparatively little-understood area of condensed matter physics.

This is an exciting time to be working on anyons!