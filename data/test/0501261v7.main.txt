##### Contents

-    I Introduction
    -    1 Definitions and basic facts on Bermudan and American options
        -    1.1 Some classes of Bermudan option pricing algorithms
        -    1.2 Outline of this thesis
        -    1.3 Notation
    -    2 Exercise regions
-    II Bounds on the American-Bermudan barrier option price difference
    -    3 Scaling the difference between perpetual American and
        Bermudan barrier options
        -    3.1 The exercise boundary and its relevance for continuity
            corrections
        -    3.2 Continuity corrections in a one-dimensional setting
        -    3.3 One-dimensional continuity corrections outside the
            Black-Scholes model
        -    3.4 Continuity corrections in higher dimensions
    -    4 From perpetual to non-perpetual Bermudan barrier options
-    III Convergence of some approximate pricing algorithms
    -    5 Bermudan option pricing based on piecewise harmonic
        interpolation and the réduite
        -    5.1 Introduction
        -    5.2 Piecewise harmonic Bermudan option pricing for options
            on one asset
        -    5.3 Réduite-based approximation of Bermudan option prices
    -    6 Soundness and convergence rate of perpetual Bermudan option
        pricing via cubature
    -    7 Some convergence estimates for non-perpetual American option
        pricing based on cubature
-    IV Numerical analysis of cubature-based American pricing
    -    8 Motivating Bermudan pricing based on cubature
        -    8.1 The general setting
        -    8.2 Application to the Black-Scholes model
        -    8.3 Exploiting combinatorial aspects of Gaussian cubature
    -    9 Numerical results
-    V High-dimensional approximate @xmath -hedging
    -    10 Hedging options on multiple assets – a suggestion for
        further research
        -    10.1 Theoretical suggestions
        -    10.2 A @xmath -hedging algorithm based on a correlation
            analysis
-    VI Appendix
    -    A Re-formulation of the perpetual Bermudan pricing problem in
        @xmath and @xmath
        -    A.1 Non-applicability of the @xmath Spectral Theorem
        -    A.2 The @xmath operator equation: analyticity in the
            exercise mesh size
    -    B An algebraic perpetual Bermudan pricing method and its
        natural scaling

## Part I Introduction

### Chapter 1 Definitions and basic facts on Bermudan and American
options

In order to clarify terminology, we start by introducing the
mathematical notions corresponding to the financial concepts that we
shall allude to.

Our first definition is a notational convention.

###### Definition 1.1.

Let @xmath . By @xmath and @xmath we denote componentwise exponentiation
and taking natural logarithms componentwise, respectively.

###### Remark 1.1.

For any @xmath , @xmath is a Lie group with respect to componentwise
multiplication @xmath . Its Lie algebra is the vector space @xmath with
its usual (componentwise) addition. The exponential map from the Lie
algebra @xmath into the Lie group @xmath is componentwise exponentiation
@xmath . Therefore the abbreviation introduced in Definition 1.1 is
consistent with standard notation.

###### Definition 1.2.

Let @xmath be a positive real number. Consider a real-valued stochastic
process @xmath , adapted to a filtered probability space @xmath . We
will call @xmath a logarithmic price process for a non-dividend paying
asset (for short, a logarithmic price process or simply log-price
process ), if and only if there exists a probability measure @xmath
equivalent to @xmath on @xmath and a constant @xmath such that the
stochastic process @xmath is a martingale with respect to the filtration
@xmath and the probability measure @xmath . In this case, such a @xmath
is called a martingale measure and @xmath a market price of risk or a
dicsount rate for the stochastic process @xmath and the probability
measure @xmath .

###### Definition 1.3.

Let @xmath . A @xmath -dimensional basket is a @xmath -tuple of
logarithmic price processes such that there exists a probability measure
@xmath and a market price of risk @xmath such that @xmath is a
martingale measure and @xmath a market price of risk for all components
of the @xmath -tuple.

For the rest of this Chapter, we will adopt the terminology and the
notation for Markov processes of Revuz and Yor [ 26 ] .

In particular, for all probability measures @xmath on @xmath , @xmath is
the probability measure induced by the transition function @xmath via
the Ionescu-Tulcea-Kolmogorov projective limit construction, cf Revuz
and Yor [ 26 , Theorem 1.5] ).

For any @xmath , we will denote the @xmath -algebra of Borel subsets of
@xmath by @xmath .

###### Definition 1.4.

Let again @xmath . A family @xmath of @xmath -valued homogeneous Markov
processes @xmath adapted to a filtered probability space @xmath with
respect to @xmath , with transition function @xmath and initial measure
@xmath , is called a @xmath -dimensional Markov basket if and only if
there is a homogeneous transition function @xmath on the measurable
space @xmath and a constant @xmath such that the following three
assertions hold:

1.   The process @xmath is a Markov process with transition function
    @xmath with respect to @xmath for all @xmath .

2.   The process @xmath is a martingale with respect to @xmath and
    @xmath .

3.   The measures @xmath and @xmath are equivalent for all @xmath .

In this case, @xmath is called a family of martingale ( or:
risk-neutral) measures associated with @xmath , and @xmath is called the
discount rate for @xmath .

The expectation operator for the probability measure @xmath will be
denoted by @xmath for all @xmath .

If the transition function @xmath is a Feller semigroup, then we shall
refer to @xmath as a Feller basket .

If @xmath is a translation-invariant Feller semigroup, we shall call
@xmath a Lévy basket .

###### Remark 1.2.

A priori, it is not clear if there are logical connections between the
three assertions in the previous Definition 1.4 , in particular the
author does not know whether the third assertion implies the first one.

###### Notational convention 1.1.

If no ambiguity can arise, we will drop the superscript of a Markov
basket. Thus, in the notation of Definition 1.4 , we set

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath , @xmath and @xmath -tuples of stopping times @xmath
whenever @xmath is nonnegative or @xmath . Here we are using the term
“stopping time” as a synonym for @xmath -valued stopping time, that is a
stopping time with values in @xmath .

Also, since we are explicitly allowing stopping times (with respect to
the filtration generated by a process @xmath ) to attain the value
@xmath , we stipulate that the random variable @xmath (for any
Lebesgue-Borel measurable function @xmath ) should be understood to be
multiplied by the characteristic function of the event @xmath .
Formally, this can be done by introducing a constant @xmath , called
cemetery , and stipulating that @xmath on @xmath and @xmath for all
measurable functions @xmath (cf eg Revuz and Yor [ 26 , pp 84,102] ).

We will not formally define what we mean by an option itself, but we
will rather define what expected payoffs and prices of some classes of
financial derivatives are.

###### Definition 1.5.

Consider a @xmath -dimensional Markov basket @xmath with an associated
family @xmath of martingale measures and discount rate @xmath .

The expected payoff of a Bermudan option with (log-price) payoff
function @xmath on the underlying Markov basket @xmath with exercise
times in @xmath , log start-price @xmath and maturity @xmath is defined
to be

  -- -- --
        
  -- -- --

The expected payoff of a perpetual Bermudan option is the expected
payoff of a Bermudan option of maturity @xmath .

The expected payoff of a Bermudan option with exercise mesh size @xmath
is the expected payoff of a Bermudan option with exercise times in
@xmath .

The expected payoff of an American option is the expected payoff of a
Bermudan option with exercise times in @xmath .

We shall call the expected payoff of a Bermudan option (or an American
option) a Bermudan option price (or an American option price ) if and
only if the martingale measures associated with the underlying basket
are unique (that is, if the market model described by @xmath , @xmath
and @xmath is complete ).

In recent years, there has been increasing interest in incomplete market
models that are governed by general Lévy processes as log-price
processes, as is not only witnessed by a tendency in research papers to
focus on Lévy process settings (for instance Boyarchenko and
Levendorskii [ 5 ] ; Asmussen, Avram and Pistorius [ 4 ] ; Øksendal and
Proske [ 23 ] , to take a random sample). Even textbooks, such as
Karatzas’ [ 17 ] and Mel’nikov’s [ 22 ] introductory works, are putting
considerable emphasis on incomplete markets. Finally, “Lévy finance” has
already been treated in survey articles intended for a general
mathematical audience, e g Applebaum’s article [ 3 ] . We will try not
to deviate too much from this consensus that tries to accomplish as much
mathematical generality as possible, while stopping short of studying
Markov process models in their full generality. Instead we note that a
substantial proportion of our results is concerned with perpetual
Bermudan and American options, and it is precisely the medium and
long-term risk theory where Lévy finance seems to be applied most
frequently. As a last remark on this issue, we consider it as beyond the
scope of this thesis to question whether it is reasonable from an
economist’s point of view to study incomplete markets.

Whilst there are some points to be made about market failures on stock
markets that might entail arbitrage opportunities (for example, when
assets are traded simultaneously on several stock exchanges, or in the
event of insider trading), the transaction costs to exploit these
arbitrage opportunities usually tend to be close to the actual gain that
can be achieved through taking advantage of the arbitrage. Therefore we
shall, for the sake of mathematical simplicity, merely refer to the
works of Corcos et al [ 9 ] as well as Imkeller et al [ 14 , 15 ] , and
impose a strict no-arbitrage assumption (which under certain regularity
conditions on the basket is equivalent to the existence of a martingale
measure, cf Karatzas [ 17 , Theorem 0.2.4] ).

###### Example 1.1 (A few common examples).

1.   The price of a European call option on a single asset with maturity
    @xmath and strike price @xmath is the price of a Bermudan option
    with the set of exercise times being the singleton @xmath and the
    (log-price) payoff function @xmath .

2.   The price of a perpetual American put of exercise mesh size @xmath
    on the arithmetic average of two assets in an underlying basket with
    strike price @xmath is the price of a Bermudan put option with the
    set of exercise times being the whole of the half-line @xmath , the
    maturity being @xmath and the payoff function @xmath .

3.   Consider a perpetual Bermudan call option on a single asset that
    continuously pays dividends at a rate @xmath and whose logarithm
    follows a Markov process @xmath adapted to some probability space
    @xmath . Then, in order to exclude arbitrage, we will have to
    require the existence of a family of measures @xmath such that each
    @xmath is equivalent to @xmath (in particular, @xmath ) and such
    that @xmath is a @xmath -martingale for all @xmath . The expected
    payoff of the option will then be

      -- -------- --
         @xmath   
      -- -------- --

As an auxiliary result, let us remark

###### Lemma 1.1 (Lower semi-continuity of @xmath).

If @xmath is a set and @xmath is a family of real numbers, then

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

We have

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath and @xmath , therefore for all @xmath and @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

thus

  -- -------- --
     @xmath   
  -- -------- --

hence

  -- -------- --
     @xmath   
  -- -------- --

This is the assertion. ∎

This estimate enables us to prove the following Lemma that is asserting
the approximability of expected payoffs or prices of American options by
sequences of expected payoffs or prices of Bermudan options,
respectively.

###### Lemma 1.2.

Let @xmath , @xmath , @xmath , consider a bounded continuous function
@xmath (the payoff function ), and a @xmath -dimensional basket @xmath
having a modification with continuous paths. If the expected payoff of
an American option of maturity @xmath , log start-price @xmath and
payoff function @xmath on this basket @xmath is less than infinity, then
the limit

  -- -------- --
     @xmath   
  -- -------- --

exists and equals the American expected payoff.

###### Proof.

Consider a sequence @xmath such that @xmath as @xmath . Choose a
sequence of stopping times @xmath such that for all @xmath ,

  -- -- --
        
  -- -- --

and define

  -- -------- --
     @xmath   
  -- -------- --

Then, due to the continuity conditions we have imposed on @xmath and on
the paths of (a modification of) the basket @xmath , we get

  -- -------- --
     @xmath   
  -- -------- --

and hence by the lower semi-continuity of @xmath , one obtains

  -- -------- --
     @xmath   
  -- -------- --

Now we can use the Montone Convergence Theorem and Lebesgue’s Dominated
Convergence Theorem (this is applicable because of the boundedness of
@xmath ) to swap limits/suprema with the expectation operator. Combining
this with the specific choice of the sequence @xmath , this yields for
all @xmath ,

  -- -- -------- -------- --
                          
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

This finally gives

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

Since the left hand side does not depend on @xmath , we conclude that
@xmath exists and is equal to @xmath . ∎

#### 1.1 Some classes of Bermudan option pricing algorithms

Let @xmath and @xmath , as usual, denote the spaces of nonnegative
continuous functions defined on @xmath , and of nonnegative measurable
functions defined on @xmath , respectively.

The purpose of the following definitions is merely to introduce a façon
de parler which will allow us to quickly describe desirable properties
of approximative Bermudan pricing algorithms in the later parts of this
thesis.

###### Definition 1.6.

A map @xmath is said to be a sound iterative Bermudan option pricing
algorithm (for short, a sound algorithm ) for a payoff function @xmath
if and only if @xmath for all @xmath and the map @xmath is pointwise
monotone, that is

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

A sound iterative Bermudan option pricing algorithm @xmath is said to
have a perpetual limit if and only if

  -- -------- --
     @xmath   
  -- -------- --

(rather than this supremum being allowed to equal @xmath on a subset of
positive measure of its range). In that very case, the function in the
last line is simply referred to as the perpetual limit of the algorithm.
Finally, @xmath is said to converge linearly in @xmath to the perpetual
limit if and only if there exists a @xmath such that

  -- -- --
        
  -- -- --

@xmath being shorthand for @xmath for all @xmath .

###### Remark 1.3.

The elements of @xmath should be conceived of assigning the value – that
is, the expected payoff – of an option to the vector of logarithmic
start prices of the components of the basket (at least on the complement
of a Lebesgue null set).

###### Remark 1.4.

The monotonicity condition imposed on sound iterative Bermudan pricing
algorithms entail that the sequence of functions @xmath is always
pointwise increasing. Thus, this sequence has a limit:

  -- -------- --
     @xmath   
  -- -------- --

The infimum of all @xmath -fixed points is always an upper bound for the
perpetual limit:

###### Lemma 1.3.

Let @xmath be a sound iterative Bermudan pricing algorithm for @xmath
with a perpetual limit @xmath . Then the function @xmath is smaller than
any fixed point of @xmath ; moreover, @xmath .

###### Proof.

Any fixed point @xmath of @xmath is in the image of @xmath and
therefore, due to our assumptions on sound algorithms, pointwise greater
or equal @xmath . Now, as @xmath (and thus @xmath ) is pointwise
monotone,

  -- -------- --
     @xmath   
  -- -------- --

therefore

  -- -------- --
     @xmath   
  -- -------- --

where the right hand side is just the perpetual limit. Hence, any fixed
point of @xmath is greater or equal the perpetual limit. Furthermore,
observe that due to the pointwise monotonicity of @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

therefore for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

∎

Later on, it will turn out that if @xmath is based on either cubature or
piecewise harmonic interpolation or the réduite, the perpetual limit is,
in fact the minimal fixed point (cf Theorem 5.1 and Lemma 5.6 for
piecewise harmonic interpolation, Theorem 5.2 for réduite-based
approximation, and Theorem 6.1 for a result on a map @xmath which is
based on cubature).

Moreover, we shall show that the algorithm based on cubature converges
linearly in the sense of the definition above.

#### 1.2 Outline of this thesis

We will postpone giving a more informal account of our motivation to use
cubature formulae for Bermudan option pricing until we study the
numerical implementation of some Bermudan pricing algorithms. In this
thesis, we will first of all prepare the derivation of bounds on the
natural scaling of the difference between an American and a Bermudan
perpetual barrier option price (conceived of as a function of the
Bermudan’s exercise mesh size). Later on, we will show that this is
sufficient to obtain bounds on the natural scaling for the difference of
certain non-perpetual American and Bermudan barrier options. For the
one-dimensional setting, analogous results have been obtained by
Broadie, Glasserman and Kou [ 7 ] .

Furthermore, we will prove soundness and existence of a perpetual limit
for a number of Bermudan pricing algorithms, including pricing based on
cubature. In particular, as was previously remarked, we will obtain a
linear convergence rate for the latter class of algorithms.

Later on, we shall prove convergence bounds for a non-perpetual American
pricing algorithm in which one is computing non-perpetual Bermudan
prices of a certain exercise mesh size via cubature and successively
halves their exercise mesh size while leaving the number of paths at
which the option is evaluated constant.

The natural scaling for which bounds are derived in the first chapters
of this report, can be used to consistently extrapolate from a finite
number of Bermudan barrier prices to an approximation for the American
price. En passant, we will sketchily explain how some of the features of
object-oriented (C++) programming can be exploited to enhance the
efficiency of a Bermudan pricing algorithm based on cubature (that is to
say, how to circumvent exponential complexity by achieving recombination
through the map class template).

#### 1.3 Notation

We are following largely standard probabilistic notation, as can be
found for instance in the works by Itô and McKean jr. [ 16 ] or Revuz
and Yor [ 26 ] .

Both @xmath and @xmath for sets @xmath and @xmath will mean that @xmath
is a subset of @xmath (possibly @xmath ).

### Chapter 2 Exercise regions

In this Chapter, we will give a rigorous proof for the fact that an
American/Bermudan option price coincides with the payoff that is
expected if one exercises at the first possible entry of the log-price
process into the immediate exercise region ( @xmath ). Characterisations
of such regions for special cases have been proven in recent years [ 6 ,
24 ] .

###### Definition 2.1.

Given a countable subset @xmath and a Lebesgue-Borel measurable set
@xmath , often referred to as exercise region , we define the stopping
time

  -- -------- --
     @xmath   
  -- -------- --

(the superscript will be dropped when no ambiguity can arise) which is
just the first (nonnegative) entry time in @xmath into @xmath . If
@xmath is a subset of space-time, that is @xmath rather than space (ie
@xmath ) itself, we use the space-time process rather than just the
process itself to give an analogous definition:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the time-coordinate at which the space-time process was
started. Also, for @xmath we set

  -- -------- --
     @xmath   
  -- -------- --

to denote the first positive entry time in @xmath into @xmath or @xmath
, respectively, whilst finally @xmath and @xmath will denotes the first
nonnegative entry time into @xmath and @xmath , respectively.

For convenience, we will also adopt the following convention for this
Chapter:

###### Definition 2.2.

Let @xmath . A stopping time @xmath is called @xmath -valued if the
range of @xmath , denoted by @xmath , is a subset of @xmath .

###### Lemma 2.1.

Consider a countable subset @xmath . Let @xmath be a @xmath -dimensional
basket with an associated risk-neutral measure @xmath and discount rate
@xmath . Suppose @xmath , @xmath and @xmath is a @xmath -submartingale.
For all @xmath -valued stopping times @xmath there is a space-time
region @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath where

  -- -------- --
     @xmath   
  -- -------- --

If @xmath is @xmath -integrable, then the latter inequality will also
hold for @xmath .

The Lemma holds in particular for @xmath for arbitrary @xmath .

###### Proof.

Firstly, we will treat the case of @xmath . Define

  -- -------- --
     @xmath   
  -- -------- --

Let us first of all assume that

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

and let us also for the moment suppose

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

Both of these assumptions will be dropped at the end of the proof for
the case @xmath in order to show the Lemma in its full strength. Now,
from equations ( 2.2 ) and ( 2.1 ) one may derive

  -- -- -- -------- -------- -------
           @xmath            (2.3)
           @xmath   @xmath   
           @xmath   @xmath   
           @xmath   @xmath   
  -- -- -- -------- -------- -------

for all @xmath .

Furthermore, observe that @xmath a.s. Using Doob’s Optional Stopping
Theorem (see eg Varadhan [ 31 , Theorem 5.11] ), we infer from our
assumption of @xmath being a @xmath -submartingale with respect to the
canonical filtration @xmath the assertion that @xmath is a @xmath
-submartingale with respect to the filtration @xmath . Hence, if we
combine this with equation ( 2.3 ) and note that @xmath for all @xmath ,
we obtain for every @xmath ,

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

On the other hand, since @xmath , if @xmath , then also @xmath ,
entailing

  -- -------- --
     @xmath   
  -- -------- --

Summarising these last two remarks, one concludes

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
                 @xmath   
        @xmath   @xmath   
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

In order to complete the proof for the case of @xmath , let us show that
the assumptions ( 2.1 ) and ( 2.2 ) are dispensable.

If the assertion ( 2.1 ) failed to hold, we would simply define the
stopping time

  -- -------- --
     @xmath   
  -- -------- --

and based on this definition, we would set

  -- -------- --
     @xmath   
  -- -------- --

Then we would have on the one hand ( 2.1 ) for @xmath instead of @xmath
which, according to what we have been able to show under the assumption
of ( 2.1 ), yields

  -- -------- --
     @xmath   
  -- -------- --

provided the condition ( 2.2 ) is satisfied. However, in any case

  -- -------- --
     @xmath   
  -- -------- --

Thus, with @xmath we have found a set that can play the rôle of @xmath
in the Lemma’s statement, under the assumption ( 2.2 ).

Now suppose the condition ( 2.2 ) was not satisfied, and ( 2.1 ) may or
may not hold (in the former case, one may even replace @xmath by @xmath
in what follows). In this situation we consider the stopping time

  -- -------- --
     @xmath   
  -- -------- --

If one now defines

  -- -------- --
     @xmath   
  -- -------- --

then

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- -------- -------- --
     @xmath   @xmath            
              @xmath   @xmath   
  -- -------- -------- -------- --

hence ( 2.1 ) holds for @xmath instead of @xmath (since ( 2.1 ) holds
for @xmath instead of @xmath and as we have just seen @xmath for all
@xmath ). Furthermore,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
                       @xmath   
  -- -------- -------- -------- --

(the first line because of @xmath ). Therefore

  -- -------- --
     @xmath   
  -- -------- --

thus ( 2.2 ) holds for @xmath instead of @xmath and @xmath instead of
@xmath . But we have already seen that ( 2.1 ) holds for @xmath instead
of @xmath . Therefore, using what we have proven under the assumption of
both ( 2.1 ) and ( 2.2 ), we get

  -- -------- --
     @xmath   
  -- -------- --

On the other hand, however,

  -- -------- --
     @xmath   
  -- -------- --

(as @xmath on @xmath , as well as @xmath on @xmath , thus @xmath on
@xmath ) and we have already seen that

  -- -------- --
     @xmath   
  -- -------- --

Finally,

  -- -------- --
     @xmath   
  -- -------- --

whence with @xmath we have found a set that can play the rôle of @xmath
in the Lemma’s statement.

Finally, we need to consider the case where @xmath . The random variable
@xmath is an upper bound on @xmath for all @xmath . Therefore, as soon
as @xmath is @xmath -integrable, we are allowed to apply Lebesgue’s
Dominated Convergence Theorem and the assertion for @xmath follows by
letting @xmath tend to infinity in

  -- -------- --
     @xmath   
  -- -------- --

(which we have already proven for all @xmath ).

∎

###### Corollary 2.1 (Formula for an option price using hitting times).

Let @xmath be a @xmath -dimensional basket with an associated
risk-neutral measure @xmath and discount rate @xmath . Consider a
countable subset @xmath . Suppose @xmath , @xmath and assume that the
process @xmath is a @xmath -submartingale. Then one has

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

for all @xmath . If the random variable @xmath is @xmath -integrable,
then the equation

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

holds.

###### Definition 2.3.

Let @xmath be countable, @xmath and @xmath measurable, and @xmath a
@xmath -dimensional Markov basket with an associated family of
risk-neutral measures @xmath and discount rate @xmath . We define

  -- -------- --
     @xmath   
  -- -------- --

as well as

  -- -------- --
     @xmath   
  -- -------- --

Instead of @xmath , we shall often simply write @xmath . Also, the
subscript @xmath will be dropped when no ambiguity can arise. Also,
@xmath and @xmath will be shorthand for @xmath and @xmath ,
respectively.

As another notational convention, let us from now on use @xmath and
@xmath to denote @xmath and @xmath , respectively.

###### Theorem 2.1 (Optimality of the immediate exercise region).

Let @xmath be a @xmath -dimensional Feller basket with @xmath being an
associated family of risk-neutral measures and @xmath being the discount
rate belonging to @xmath . Suppose @xmath , @xmath is countable, and
@xmath . Assume, moreover, that @xmath is a @xmath -submartingale for
all @xmath . Define

  -- -------- --
     @xmath   
  -- -------- --

if @xmath (we may drop the superscript @xmath wherever this is
unambiguous) and else

  -- -------- --
     @xmath   
  -- -------- --

Then

  -- -------- --
     @xmath   
  -- -------- --

if @xmath , and @xmath for all @xmath such that the random variable
@xmath is @xmath -integrable.

###### Proof.

Let @xmath . Using Corollary 2.1 and recalling the definition of @xmath
, all we have to show is

  -- -------- --
     @xmath   
  -- -------- --

(where we recall that @xmath denotes the first nonnegative entry time
into @xmath ). However, after exploiting the special particular shape of
@xmath , we can – due to the boundedness of @xmath which yields @xmath
for all @xmath which allows us to apply Lebesgue’s Dominated Convergence
Theorem – swap @xmath and @xmath to get for all @xmath ,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

(where for notational convenience @xmath should denote the first
nonnegative entry time into @xmath ). Now, let us use the strong Markov
property of the Feller process @xmath , and for this purpose, let @xmath
denote the shift operator on the space-time path space @xmath (which is
the set of all càdlàg functions from @xmath into @xmath – recall that
all Feller processes have a càdlàg modification). We obtain

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

But @xmath is a @xmath -supermartingale for all @xmath , therefore by
Doob’s Optional Stopping Theorem, @xmath must also be a @xmath
-submartingale for all @xmath and @xmath (note that @xmath a.s. because
of the fact that @xmath is the shift operator for the space-time process
@xmath , rather than simply for @xmath ). Letting @xmath tend to
infinity, we can employ Lebesgue’s Dominated Convergence Theorem (as
@xmath yields @xmath for @xmath for all @xmath and @xmath ) in order to
get that the expected value of @xmath is always greater or equal than
the expectation of @xmath . Hence

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

The case @xmath can be dealt with analogously.

∎

###### Lemma 2.2 (Time-stationarity of immediate exercise regions for
perpetual Bermudans).

Let @xmath be a Lévy basket with @xmath being an associated family of
probability measures and discount rate @xmath . Then for all @xmath we
have

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath satisfying the condition that the random variable @xmath
be @xmath -integrable.

###### Proof.

Consider an integer @xmath , and an @xmath such that @xmath is @xmath
-integrable. Then we shift the time scale by @xmath to get

  -- -- -------- -------- --
                          
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

where @xmath denotes the shift operator on the space (as opposed to
space-time) path space @xmath . Because of the boundedness of @xmath
which entitles us to apply Lebegue’s Dominated Convergence Theorem, we
may swap @xmath and @xmath to obtain

  -- -- -------- -------- --
                          
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

for all @xmath . Thus we conclude

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath . If we insert this equality fact into the definition of
@xmath , we see that the condition determining whether a pair @xmath
belongs to @xmath does not depend on @xmath . On the other hand, by
Corollary 2.1 ,

  -- -------- --
     @xmath   
  -- -------- --

and the left hand side equals – by our previous observations in this
proof – the term featuring in the definition of @xmath .

∎

Summarising the two previous Lemmas and applying them to a more concrete
setting, we deduce that the expected payoff of a perpetual Bermudan
option of mesh size @xmath equals

  -- -------- --
     @xmath   
  -- -------- --

where @xmath .

###### Lemma 2.3.

Let us fix a Lévy basket with an associated family of risk-neutral
probability measures @xmath and discount rate @xmath , as well as a
region @xmath and a real number @xmath . Then we have

  -- -------- --
     @xmath   
  -- -------- --

In particular, using Lemma 2.2 , one has the following equation for the
expected perpetual Bermudan option payoff:

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Using the Markov property of @xmath , denoting by @xmath the shift
operator on the path space @xmath of a Lévy process @xmath , and taking
into account the fact that @xmath (i.e. @xmath ) in case @xmath , we
obtain:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

∎

## Part II Bounds on the American-Bermudan barrier option price
difference

### Chapter 3 Scaling the difference between perpetual American and
Bermudan barrier options

Embracing the terminology of Broadie, Glasserman and Kou [ 7 ] , we
shall refer to the difference between an American and the corresponding
Bermudan options (on the same basket and with the same payoff function)
as “continuity correction”.

#### 3.1 The exercise boundary and its relevance for continuity
corrections

###### Lemma 3.1.

Consider a @xmath -dimensional Feller basket @xmath with an associated
family of risk-neutral probability measures @xmath and discount rate
@xmath . Furthermore, let @xmath be a nonnegative continuous function
such that @xmath is a martingale, @xmath a nonnegative real number, and
define @xmath . Finally, let @xmath be a measurable set such that @xmath
on @xmath . Then we have for all @xmath and @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Similarly, if @xmath instead and one assumes that this @xmath is
nonnegative on @xmath , the identity

  -- -------- --
     @xmath   
  -- -------- --

holds for all @xmath and @xmath .

###### Proof.

Let @xmath and @xmath , and let us first set consider the case of @xmath
. Since @xmath on @xmath by assumption, one has the identity

  -- -------- --
     @xmath   
  -- -------- --

Moreover,

  -- -------- --
     @xmath   
  -- -------- --

@xmath -almost surely, therefore by Lebesgue’s Dominated Convergence
Theorem,

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

But since @xmath is a @xmath -martingale, we may apply Doob’s Optional
Stopping Theorem to get that @xmath is a @xmath -martingale, too, whence

  -- -------- --
     @xmath   
  -- -------- --

This finally yields, because of equation ( 3.1 ) and the monotonicity of
the sequence @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

which completes the proof as

  -- -- --
        
  -- -- --

for all @xmath .

The case of @xmath can be treated analogously. ∎

An important feature of the immediate exercise region for a perpetual
Bermudan option with payoff function of the form @xmath where @xmath is
monotonely increasing in each component, and exercise mesh size @xmath ,
is that – owing to the fact that @xmath , the option price as function
of the logarithmic start price, is monotonely decreasing in each
component – it is south-west connected in the following sense:

###### Definition 3.1.

A set @xmath is called north-east connected if and only if for all
@xmath such that @xmath componentwise, @xmath . Likewise, any set @xmath
is called south-west connected if and only if for all @xmath such that
@xmath componentwise, @xmath .

###### Remark 3.1.

If @xmath is a south-west connected subset of @xmath , then @xmath is an
element of the boundary of @xmath . Analogously, if @xmath is a
north-east connected subset of @xmath , then @xmath is an element of the
boundary of @xmath .

###### Lemma 3.2 (Characterisation of the American-Bermudan barrier
difference for perpetual puts).

Let @xmath be the logarithmic price process of the multidimensional
Black-Scholes model with constant volatility and interest rate, that is

  -- -------- --
     @xmath   
  -- -------- --

(where @xmath is the @xmath -dimensional Wiener process) for some @xmath
and @xmath . Let @xmath , wherein @xmath be a real number and @xmath be
a continuous function that is monotonely increasing in each component
and such that @xmath is a martingale. Finally, consider a measurable set
of the shape @xmath for some @xmath and some convex north-east connected
set @xmath (making @xmath convex and south-west connected) such that
@xmath is nonnegative on @xmath . Then we have for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Fix an @xmath . Introduce stopping times @xmath for @xmath , @xmath
through

  -- -------- --
     @xmath   
  -- -------- --

and define

  -- -------- --
     @xmath   
  -- -------- --

Now consider an arbitrary @xmath and @xmath . Due to Doob’s Optional
Stopping Theorem, applied to the two-component sequences of stopping
times @xmath and @xmath (which both are bounded by @xmath ), combined
with the fact that @xmath is a martingale,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

thus as @xmath on @xmath and @xmath ,

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
                 @xmath   
                 @xmath   
        @xmath   @xmath   
                 @xmath   
                 @xmath   
        @xmath   @xmath   
                 @xmath   
                 @xmath   
                 @xmath   
        @xmath   @xmath   
                 @xmath   
  -- -- -------- -------- --

Note, however that the subtractor converges to zero exponentially. For,
exploiting not only the Markov property, but also @xmath and that @xmath
is a martingale, we have

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
                       @xmath   
              @xmath   @xmath   
                       @xmath   
              @xmath   @xmath   
                       @xmath   
              @xmath   @xmath   
                       @xmath   
              @xmath   @xmath   
                       @xmath   
              @xmath   @xmath   
                       @xmath   
              @xmath   @xmath   
                       @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

on @xmath – and both of the conditioned probabilities in the last line
converge to nought as @xmath tends to @xmath .

Therefore, due to equation ( 3.1 ),

  -- -- -- -------- -------- -------
           @xmath            (3.2)
           @xmath   @xmath   
           @xmath   @xmath   
  -- -- -- -------- -------- -------

on @xmath .

However, we also know that

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

almost surely. Combining this convergence assertion with Lebesgue’s
Dominated Convergence Theorem yields

  -- -------- --
     @xmath   
  -- -------- --

and recalling equation ( 3.2 ), we finally arrive at

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

∎

Next we shall prove that the expression on the right hand side of the
last equation is monotonely increasing as @xmath componentwise.

###### Lemma 3.3.

Under the assumptions of Lemma 3.2 , one has for all @xmath and @xmath
that satisfy the relation

  -- -------- --
     @xmath   
  -- -------- --

the following lower bound:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
                       @xmath   
  -- -------- -------- -------- --

In the proof of Lemma 3.3 , we will apply the following

###### Auxiliary Lemma 3.1.

Consider a measurable set @xmath of positive Lebesgue measure and two
continuous positive integrable functions @xmath on @xmath , as well as a
nonnegative monotonely decreasing function @xmath , and assume that not
only is the function @xmath monotonely decreasing, but also @xmath .
Then

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

If the continuous monotonely decreasing function @xmath was either
@xmath or @xmath on all of @xmath , then one would get @xmath or @xmath
, respectively. Hence there exists a real number @xmath such that @xmath
as well as @xmath on @xmath and @xmath on @xmath . But since @xmath is
monotonely decreasing and nonnegative, these inequalities yield

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

thus

  -- -------- --
     @xmath   
  -- -------- --

∎

###### Proof of Lemma 3.3.

Let @xmath , which in particular entails @xmath . Let us define, for all
@xmath and @xmath , a measure @xmath on @xmath by

  -- -- --
        
  -- -- --

Then, due to our choice of @xmath as being a Brownian motion with drift,
@xmath will have a positive continuous Lebesgue density, denoted by
@xmath .

Now whenever @xmath such that

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

(in particular @xmath ), note the identity

  -- -- --
        
  -- -- --

(where @xmath shall for every symmetric positive semidefinite @xmath and
@xmath denote the density of @xmath , the Gaussian measure with
covariance matrix @xmath and mean @xmath ) for all @xmath . For, we
shall then obtain

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

and therefore

  -- -------- --
     @xmath   
  -- -------- --

This implies

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

whence we have shown that @xmath is monotonely increasing on the
ray-segment @xmath . But in addition,

  -- -- --
        
  -- -- --

will hold (since @xmath was assumed to be convex and south-west
connected). Thus @xmath decreases monotonely on any ray (or ray-segment)
@xmath (for @xmath componentwise) if we look at this ray (-segment) as a
linearly ordered set with respect to the componentwise real order
relation @xmath . From these two sets of monotonicity assertions (for
all @xmath componentwise), together with the fact that @xmath , one
deduces by means of the above Auxiliary Lemma 3.1 ,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

therefore

  -- -- -------- -------- --
                          
        @xmath   @xmath   
  -- -- -------- -------- --

(where @xmath denotes the @xmath -dimensional unit ball) which via the
Fubini-Tonelli Theorem amounts to

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

On the other hand, the definition of @xmath gives

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

for arbitrary @xmath . Hence inequality ( 3.6 ) becomes

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

The Markov property of @xmath , moreover, entitles us to state

  -- -- --
        
  -- -- --

for all @xmath and @xmath . Thus we can use inequality ( 3.7 ) to derive
the following estimate:

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
                 @xmath   
        @xmath   @xmath   
                 @xmath   
  -- -- -------- -------- --

which via Lemma 3.2 gives the desired result.

∎

###### Corollary 3.1.

Suppose the assumptions of the preceding Lemma 3.3 hold. Consider @xmath
and assume, in addition, there is a positive lower bound for the
sequence @xmath . Then one has

  -- -- --
        
  -- -- --

###### Proof.

Equation ( 3.1 ) and Lemma 3.2 yield

  -- -- -- -------- -------- --------
           @xmath            (3.10)
           @xmath   @xmath   
                    @xmath   
  -- -- -- -------- -------- --------

for every @xmath . Inserting @xmath and @xmath for @xmath in this
estimate, the Corollary can be deduced via the estimate of the preceding
Lemma 3.3 .

∎

We can explicitly state a partial differential equation that the said
difference @xmath obeys:

###### Lemma 3.4.

Consider a measurable set @xmath . Let again @xmath be the logarithmic
price process of the multidimensional Black-Scholes model, that is,

  -- -------- --
     @xmath   
  -- -------- --

and let us assume that @xmath and @xmath are as in Lemma 3.3 Then for
all @xmath , the partial differential equation

  -- -------- --
     @xmath   
  -- -------- --

holds on @xmath , wherein for all @xmath

  -- -------- --
     @xmath   
  -- -------- --

and (always following the notation introduced in Chapter 1 )

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Fix @xmath . We shall prove the Lemma by studying the space-time Markov
process @xmath and the functions

  -- -------- --
     @xmath   
  -- -------- --

where

  -- -------- --
     @xmath   
  -- -------- --

It is clear that

  -- -------- --
     @xmath   
  -- -------- --

hence if we employ the Markov property in two directions we can for all
@xmath get the following @xmath -almost sure identities:

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

Thus, the process @xmath is a martingale. The infinitesimal generator of
the space-time Markov process @xmath is

  -- -------- --
     @xmath   
  -- -------- --

due to the well-known result on the infinitesimal generator of Brownian
motion with drift (see eg Revuz and Yor [ 26 , p. 352] ). Therefore we
have proven

  -- -- --
        
  -- -- --

Now one observes that

  -- -------- --
     @xmath   
  -- -------- --

which yields

  -- -------- -------- -------- --
     @xmath            @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

and thus brings the proof of the Lemma to a close. ∎

###### Remark 3.2.

In the remainder of this Chapter, we will continue to largely focus on
put options, thus always setting @xmath for some componentwise
monotonely increasing @xmath and assuming the @xmath occurring in the
defition of @xmath , @xmath , to be south-west connected. However, one
can easily derive analogous results for call options on dividend-paying
assets, by simply cutting the interest rate to discount the dividends
and by replacing @xmath by @xmath and @xmath by some north-east
connected measurable subset of @xmath that is assumed to satisfy the
condition @xmath on @xmath .

#### 3.2 Continuity corrections in a one-dimensional setting

###### Definition 3.2.

For measurable @xmath and @xmath define

  -- -------- --
     @xmath   
  -- -------- --

###### Remark 3.3.

Note that in general, @xmath on @xmath , but always @xmath on @xmath .

###### Theorem 3.1.

Suppose @xmath , let @xmath and @xmath , and assume @xmath , in words:
@xmath is the logarithmic price process of the one-dimensional
Black-Scholes model with constant volatility @xmath and discount rate
@xmath . Set @xmath . Then one has for all @xmath the relations

  -- -- -- -------- -------- --------
           @xmath            (3.11)
           @xmath   @xmath   
           @xmath   @xmath   
  -- -- -- -------- -------- --------

Furthermore, if @xmath , there exist constants @xmath such that for all
sufficiently small @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

If both @xmath and @xmath , there exist constants @xmath such that for
all sufficiently small @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

###### Remark 3.4.

Although computing the constants @xmath explicitly is possible, we
refrain from it for the moment, as it is not required to find the right
scaling for an extrapolation for @xmath from @xmath to @xmath and it
would not provide any additional useful information for our
extrapolation purposes. The same remark applies to all examples and
generalisations that are studied subsequently.

###### Proof.

The existence of @xmath is a consequence of Lemma 1.2 . The first
identity in the statement of the Theorem is a consequence of the
previously established Lemma 3.1 , whereas the second equation in the
statement of the Theorem follows from a result by Feller [ 11 , p. 606,
Lemma 3] on processes with stationary and independent increments. For,
if we define

  -- -------- --
     @xmath   
  -- -------- --

then Feller’s identity [ 11 , p. 606, Lemma 3] reads

  -- -------- -- --------
     @xmath      (3.12)
  -- -------- -- --------

and holds whenever @xmath has stationary and independent increments, in
particular for all Lévy processes (note that our definition of a Lévy
process requires them to be Feller processes in addition). This entails

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (3.13)
              @xmath   @xmath      (3.14)
  -- -------- -------- -------- -- --------

which is enough to prove the second identity ( 3.11 ) in the Theorem.
This ushers in the derivation of the estimates on @xmath which are
needed in order to prove the inequalities of the second half of the
Theorem. We shall show that if @xmath , there exist constants @xmath
such that for all sufficiently small @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

and if both @xmath and @xmath , there exist constants @xmath such that
for all sufficiently small @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Now, the scaling invariance of Brownian motion yields for all @xmath and
@xmath :

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.15)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

We divide the remainder of the proof, which will essentially consist in
finding estimates for the right hand side of the last equation, into two
parts according to the sign of @xmath .
Case I: @xmath . In this case we use the estimates

  -- -------- --
     @xmath   
  -- -------- --

thus

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

hence by transformation for all @xmath

  -- -------- -- --------
     @xmath      (3.16)
  -- -------- -- --------

Due to equation ( 3.15 ), this entails for all @xmath , @xmath , @xmath
(if we insert @xmath for @xmath )

  -- -------- --
     @xmath   
  -- -------- --

Therefore for arbitrary @xmath ,

  -- -- -- --------
           (3.17)
  -- -- -- --------

The sums in equation ( 3.17 ) have got the shape of @xmath for @xmath .
Now one performs a standard elementary computation on this power series:

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (3.18)
  -- -------- -------- -------- -- --------

which immediately gives

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

when applied to equation ( 3.17 ). Due to de l’Hospital’s rule, the
differences in the brackets on the left and right hand sides of the last
estimate behave like @xmath when @xmath . This is sufficient to prove
the estimate in the Theorem for the case of @xmath .
Case II: @xmath and @xmath . In that case we employ the estimates

  -- -------- --
     @xmath   
  -- -------- --

and proceed analogously to Case I, to obtain

  -- -------- -- --------
     @xmath      (3.19)
  -- -------- -- --------

In the special case of @xmath , this leads to the esimate in the
statement of the Theorem via

  -- -------- -- --------
     @xmath      (3.20)
  -- -------- -- --------

Therefore in case @xmath the scaling exponent is exactly @xmath . ∎

###### Corollary 3.2.

Assume @xmath and let, as in the previous Theorem 3.1 , @xmath , in
words: @xmath be the logarithmic price process of the one-dimensional
Black-Scholes model with constant volatility @xmath and discount rate
@xmath . Furthermore, suppose @xmath and let @xmath denote the optimal
exercise region for a (one-dimensional) perpetual Bermudan put option of
exercise mesh size @xmath and strike price @xmath on the
(one-dimensional) basket @xmath . Define @xmath , @xmath and @xmath .
Then we have for all @xmath

  -- -------- --
     @xmath   
  -- -------- --

Moreover, there are constants @xmath , such that if @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

and if both @xmath and @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

for all sufficiently small @xmath .

###### Proof.

The first asymptotic identity in the statement of the Corollary follows
from equation ( 3.11 ) in Theorem 3.1 as soon as we have remarked that

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

for these equations yield

  -- -------- --
     @xmath   
  -- -------- --

which by the differentiability of @xmath means

  -- -------- --
     @xmath   
  -- -------- --

We can now use results on the exercise boundary for perpetual Bermudan
options obtained by Boyarchenko and Levendorskii [ 5 , equation (5.3)]
who showed

  -- -------- --
     @xmath   
  -- -------- --

for sufficiently small @xmath , and the estimates in the Corollary
follow directly from the estimates of Theorem 3.1 . ∎

###### Remark 3.5.

Up to this point, we have derived estimates for the American-Bermundan
option price difference at the boundary @xmath of the exercise region
@xmath (in case of a put) or @xmath (in case of a call with dividends).
We can extend these bounds of the American-Bermudan difference from the
exercise boundary to the complement of the exercise region: By
continuity, we can even extend the lower bounds or upper bounds,
respectively, to a neighbourhood of the exercise boundary: For, if we
consider a put for the moment, we get from Lemma 3.4 that @xmath is
continuous for all @xmath , implying that if @xmath

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is shorthand for @xmath Thus, if @xmath – where we have,
thanks to Theorem 3.1 estimates for the limit @xmath – we will for any
@xmath get a @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

Analogously, we can proceed to derive bounds for the American-Bermundan
call option price difference (for an option on a dividend-paying asset)
in a neighbourhood of the exercise boundary.

#### 3.3 One-dimensional continuity corrections outside the
Black-Scholes model

The identity ( 3.11 ) of Theorem 3.1 can be used to derive estimates in
the spirit of the second half of Theorem 3.1 in more general situations.
We will illustrate this by means of the following example:

###### Example 3.1 (Merton’s jump-diffusion model with positive jumps
and “moderate” volatility).

Suppose the logarithmic price process @xmath is governed by an equation
of the form

  -- -------- --
     @xmath   
  -- -------- --

where @xmath , @xmath , @xmath is the Poisson process (thus, in this
setting, only positive jumps are allowed for simplicity) and @xmath a
normalised one-dimensional Brownian motion, and the stochastic processes
@xmath and @xmath are assumed to be independent. Let @xmath be an
associated family of risk-neutral measures and @xmath the discount rate.
In order to employ ( 3.11 ), we shall compute the sum @xmath for all
@xmath . Since @xmath for arbitrary @xmath we may without loss of
generality take @xmath . Let us also assume @xmath ; note that since
@xmath is a martingale – as @xmath is a logarithmic price process – ,
@xmath must be such that @xmath (if @xmath and @xmath are given), hence
@xmath implies @xmath . Now, by definition of the Poisson distribution
together with the symmetry and scaling invariance of Brownian motion

  -- -- -- -------- -------- --------
           @xmath            (3.21)
           @xmath   @xmath   
           @xmath   @xmath   
  -- -- -- -------- -------- --------

(with the convention that @xmath ). Now let us first of all try and find
estimates for the probability in the last line. By equation ( 3.16 )
applied to @xmath ,

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

which yields, using the abbreviation @xmath ,

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

so

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

Thus, we can perform the following estimates to derive an upper bound of
the sum in ( 3.21 ):

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

where the last line uses that @xmath and we need to impose the condition
that @xmath (which, given @xmath and @xmath , will be satisfied if
@xmath is sufficiently small) to employ the identity

  -- -------- -- --------
     @xmath      (3.23)
  -- -------- -- --------

The lower bound follows simply from

  -- -- --
        
  -- -- --

(for @xmath recall that @xmath in this paragraph by our earlier
convention) as this entails (when exploiting the estimate ( 3.1 ) and
finally ( 3.23 ) ):

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

As a consequence of these estimates and using the Taylor expansion of
@xmath around @xmath , we now get the existence of two constants @xmath
and @xmath (which can be computed explicitly) such that for all
sufficiently small @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Finally, we may apply identity ( 3.11 ) from Theorem 3.1 – as this is an
immediate consequence of Feller’s identity [ 11 , p. 606, Lemma 3] and
our Lemma 3.1 – and conclude that if @xmath and @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

for all sufficiently small @xmath .

#### 3.4 Continuity corrections in higher dimensions

The proof of Theorem 3.1 relies heavily on the use of Feller’s result [
11 , p. 606, Lemma 3] which in turn is proven by means of elementary
Fourier analysis and a so-called “basic identity” [ 11 , p. 600,
equation (1.9)] .

Hence, if one aims at generalising Theorem 3.1 to higher dimensions, one
should first of all find a multi-dimensional analogue of the said basic
identity.

Indeed, we shall see that this is feasible. Let us for the following fix
a stochastic process @xmath on @xmath with stationary and independent
increments.

###### Lemma 3.5.

Suppose @xmath is a measurable subset of @xmath , and @xmath . Define
for all @xmath

  -- -------- --
     @xmath   
  -- -------- --

as well as

  -- -------- --
     @xmath   
  -- -------- --

(in particular @xmath and @xmath ). Then for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Consider a measurable @xmath . Clearly,

  -- -------- -- --------
     @xmath      (3.24)
  -- -------- -- --------

On the other hand, since @xmath is a Markov process, we have

  -- -------- --
     @xmath   
  -- -------- --

(where @xmath is the translation-invariant Markov semigroup of
transition functions for the process @xmath whose increments are
stationary and independent), thus

  -- -------- --
     @xmath   
  -- -------- --

for all nonnegative measurable functions @xmath . But this implies

  -- -------- -------- -------- --
     @xmath   @xmath            
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

and the right hand side of this equation coincides with the one of
identity ( 3.24 ). ∎

Applying Fourier transforms we obtain

###### Corollary 3.3.

Let us adopt the notation of the preceding Lemma and define the Fourier
transform of a countable sequence @xmath of finite measures on @xmath ,
denoted by @xmath , by

  -- -------- --
     @xmath   
  -- -------- --

Then for all @xmath , and @xmath the equation

  -- -------- --
     @xmath   
  -- -------- --

holds.

###### Proof.

The result of the previous Lemma reads

  -- -------- --
     @xmath   
  -- -------- --

when we apply the Fourier transform. After multiplication with @xmath
and summing up over @xmath , one arrives at

  -- -------- --
     @xmath   
  -- -------- --

hence

  -- -------- --
     @xmath   
  -- -------- --

This is our claim. ∎

###### Definition 3.3.

A subset @xmath is called @xmath -closed if and only if @xmath is
measurable and @xmath , that is sums of elements of @xmath are again
elements of @xmath .

###### Lemma 3.6 (à la Feller, Wiener, Hopf).

Suppose @xmath is a @xmath -closed set and its complement @xmath is a
@xmath -closed set as well. Assume furthermore @xmath (ensuring @xmath
), and let @xmath the main branch of the logarithm on @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath such that the left-hand side is well-defined. In general,
for all @xmath , one has at least

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Let @xmath . According to the previous Corollary 3.3 , we have

  -- -------- -- --------
     @xmath      (3.25)
  -- -------- -- --------

wherever this is defined. Due to the identities @xmath for all @xmath
(cf equation ( 3.18 ) in the proof of Theorem 3.1 above) and @xmath this
can also be written as

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

However, at least for @xmath and arbitrary choice of @xmath , one may
still state identity ( 3.25 ) as this follows from Corollary 3.3 more or
less directly: First we note that

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

(as in these statements the arguments of @xmath are positive, hence
surely in the domain of @xmath ) and written in series notation

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

But Corollary 3.3 implies

  -- -- --
        
  -- -- --

Combining these two equations yields ( 3.25 ). Next, note that

  -- -------- --
     @xmath   
  -- -------- --

is still a finite measure – concentrated on @xmath – and thus possesses
a Fourier transform. Analogously, the measure @xmath is concentrated on
@xmath and also has a Fourier transform as it is finite. Now, for
arbitrary @xmath , the properties of the Fourier transform imply

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

But since @xmath and @xmath are @xmath -closed sets, i.e. @xmath and
@xmath , the measures on the right hand sides of these two equations,
@xmath and @xmath , have to be (signed) measures on @xmath and @xmath ,
respectively. Let us now split the sum in ( 3.4 ) and insert the terms
we have previously identified:

  -- -- -- -------- -------- --------
           @xmath            (3.27)
                    @xmath   
           @xmath   @xmath   
  -- -- -- -------- -------- --------

It is the injectivity of the Fourier transform that yields from this

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

Either side of this equation equals the sum of two (signed measures),
and we recall that the first measure on the left hand side and first
measure on the right hand side are both concentrated on @xmath , whilst
the second measure on the left hand side as well as the second measure
on the right hand side are both concentrated on @xmath . The only way
for this to be true is that the two measures that are concentrated on
each of @xmath or @xmath are equal:

  -- -------- --
     @xmath   
  -- -------- --

and also

  -- -------- --
     @xmath   
  -- -------- --

the former identity being exactly what the statement of the Lemma
expresses in the language of Fourier transforms. ∎

Based on this result, we may partially generalise Theorem 3.1 to higher
dimensions when we require @xmath (the set that we refer to the exercise
region) to be @xmath -closed set.

###### Theorem 3.2.

Let us make the assumptions of Lemma 3.2 , viz: Let @xmath be the
logarithmic price process of the multidimensional Black-Scholes model
for independent assets with constant volatility and interest rate, that
is

  -- -------- --
     @xmath   
  -- -------- --

(where @xmath is the @xmath -dimensional Wiener process) for some @xmath
and @xmath . Let @xmath , wherein @xmath be a real number and @xmath be
a continuous function that is monotonely increasing in each component
and such that @xmath is a martingale. Finally, consider a measurable set
of the shape @xmath for some @xmath and some convex north-east connected
set @xmath (making @xmath convex and south-west connected) such that
@xmath is nonnegative on @xmath and @xmath . Suppose furthermore that
both @xmath and @xmath are @xmath -closed. Then for all @xmath ,

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

###### Proof.

The existence of @xmath is a consequence of Lemma 1.2 . The subsequent
identity follows directly from Lemma 3.6 (just as in the proof of the
corresponding equation in Theorem 3.1 , except that en lieu of Lemma 3.6
, the proof of Theorem 3.1 makes use of Feller’s original result [ 11 ,
p. 606, Lemma 3] ): For, the second equation in Lemma 3.6 may be read

  -- -------- --
     @xmath   
  -- -------- --

that is

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

for all @xmath , in particular for @xmath .

∎

Again, an analogous result can be accomplished when the function @xmath
is replaced by @xmath and the set @xmath by @xmath :

###### Remark 3.6.

In special cases, one can find estimates for @xmath that are strong
enough to establish multi-dimensional generalisations of the estimates
in the second half of Theorem 3.1 . We shall give a few examples.

In general our results can be used for the extrapolation from
(multi-dimensional) Bermudan to American barrier knock-in option prices
when the barrier regions and their complements are, up to a constant
factor, closed with respect to multiplication, and when, in addition,
the barrier region is convex as well as south-west connected (in the
case of put options) or north-east connected (in the case of call
options), and is contained in the immediate exercise region of the
corresponding American option.

###### Example 3.2.

Let @xmath be any natural number. Consider the convex, north-east
connected, @xmath -closed set @xmath (whose complement is also @xmath
-closed) and set @xmath . Let us impose the same assumptions on @xmath ,
@xmath , and @xmath as in the statement of the previous Theorem 3.2 .
Then one has, due to the independence of the components of @xmath , the
following bounds for all @xmath :

  -- -------- --
     @xmath   
  -- -------- --

where @xmath depend on @xmath . @xmath and @xmath if @xmath , @xmath and
@xmath if @xmath . Hence also,

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

for arbitrary @xmath and @xmath componentwise.

###### Example 3.3.

Suppose @xmath and let again @xmath , @xmath , @xmath and @xmath be as
in Theorem 3.2 , though we will later on have to impose the condition of
@xmath (componentwise). Furthermore consider the convex south-west
connected @xmath -closed set

  -- -------- --
     @xmath   
  -- -------- --

whose complement is also @xmath -closed for @xmath and set @xmath . Note
that in this situation @xmath (which one might refer to as the
non-logarithmic exercise region) equals

  -- -------- --
     @xmath   
  -- -------- --

( @xmath denoting componentwise exponentiation as before). Then we get
for all @xmath and arbitrary @xmath ,

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
              @xmath   @xmath      
              @xmath   @xmath      
              @xmath   @xmath      
              @xmath   @xmath      
              @xmath   @xmath      
              @xmath   @xmath      (3.29)
  -- -------- -------- -------- -- --------

(where @xmath is the normal Gaussian measure on @xmath and @xmath ).

Next observe that for any real number @xmath , from rotating the set
@xmath by @xmath , @xmath and @xmath , we obtain, via exploiting the
translation-invariance of the two-dimensional normal Gaussian measure
@xmath , the relation

  -- -------- --
     @xmath   
  -- -------- --

Using the trivial estimate @xmath , we arrive at

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

But of course, by a change of coordinates, viz @xmath , one has

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

Applying this to the equation ( 3.29 ) for @xmath and using the
assumption @xmath (componentwise) yields

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.30)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

for

  -- -------- --
     @xmath   
  -- -------- --

We can find the following bounds for the measure in the previous
estimate:

###### Lemma 3.7.

For all @xmath and @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

The elementary proof has two parts. Firstly, we observe that for all
@xmath and @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

which implies

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.31)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Secondly, we have for all @xmath and @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Thus,

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

which via

  -- -------- --
     @xmath   
  -- -------- --

and ( 3.31 ) gives

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

But

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

from which the Lemma follows.

∎

This Lemma’s inequalities admit by means of identity ( 3.30 ) the
following conlusion:

  -- -------- --
     @xmath   
  -- -------- --

By Theorem 3.2 and the formula @xmath for all @xmath , we conclude,
analogously to the deliberations in the proof of Theorem 3.1 that

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

After applying de l’Hospitals rule to the bases of the powers on each
side of this estimate, we get constants @xmath such that for all
sufficiently small @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

###### Example 3.4 (a special Extended Black-Scholes Model).

In this example we do not assume a multi-dimensional Black-Scholes
model, but we presume the discounted price process vector @xmath to
satisfy the stochastic differential equation

  -- -------- --
     @xmath   
  -- -------- --

where @xmath are mutually commuting symmetric random matrices and @xmath
is a one-dimensional Brownian motion, subject to the initial condition

  -- -------- --
     @xmath   
  -- -------- --

Then, due to Albeverio and Steblovskaya [ 1 , Proposition 4] , we have
got an explicit solution of that stochastic differential equation, given
by

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.32)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Then @xmath and hence the logarithmic non-discounted process @xmath are
Lévy processes, and thus Theorem 3.2 applies. In this setting we can
compute the expression in the last line ( 3.32 ) by applying the
Spectral Theorem to the symmetric matrices @xmath and @xmath .

### Chapter 4 From perpetual to non-perpetual Bermudan barrier options

Recall how the function @xmath , the expected payoff of a non-perpetual
Bermudan option on a Feller basket with validity @xmath , log-price
payoff function @xmath and exercise mesh @xmath as a function of the
logarithmic start price vector, given that the option is exercised on
the first entry into @xmath , was defined:

  -- -- --
        
  -- -- --

The purpose of the following Lemma 4.1 is to see see that for all @xmath
, the limiting behaviour of the difference @xmath as @xmath tends to
zero whilst @xmath remains constant must be the same as the one of the
difference @xmath . In words: In the (sub-optimal case) of a
non-stationary exercise policy for a non-perpetual option, the
American-Bermudan barrier option price difference has the same limiting
behaviour as the American-Bermudan difference for the corresponding
perpetual barrier options.

###### Lemma 4.1.

Suppose @xmath is a @xmath -dimensional Feller basket with @xmath and
@xmath being an associated family of risk-neutral probability measures
and discount rate, respectively. If we define

  -- -------- --
     @xmath   
  -- -------- --

then one has for all @xmath , @xmath , measurable @xmath and @xmath ,

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
                 @xmath   
  -- -- -------- -------- --

###### Proof.

For all @xmath , @xmath , @xmath we can use the Markov property of the
Feller process @xmath and the definition of the sequence of events
@xmath to obtain the following expressions for @xmath and @xmath :

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
                       @xmath   
              @xmath   @xmath   
                       @xmath   
  -- -------- -------- -------- --

as well as

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Then immediately for all @xmath , @xmath , @xmath ,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
                       @xmath   
  -- -------- -------- -------- --

Regarding the difference between @xmath and @xmath , observe that again
for all @xmath , @xmath and @xmath ,

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

where again we have exploited several times the Markov property of
@xmath , and – in order to interchange @xmath and @xmath – the
assumption that @xmath on @xmath .

∎

This proves

###### Corollary 4.1.

Under the assumptions of Lemma 4.1 as well as @xmath for nonnegative
@xmath and @xmath on @xmath , we have the identity

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
                 @xmath   
                 @xmath   
                 @xmath   
  -- -- -------- -------- --

###### Remark 4.1.

Put informally, this Corollary 4.1 means that as soon as one has
established order estimates (in @xmath ) on the difference @xmath (for
instance the ones from Theorem 3.1 ), one only needs to find estimates
on the probabilities @xmath and @xmath to obtain order estimates on the
difference @xmath .

## Part III Convergence of some approximate pricing algorithms

### Chapter 5 Bermudan option pricing based on piecewise harmonic
interpolation and the réduite

#### 5.1 Introduction

We intend to approximate the function that assigns the value of a
Bermudan option with payoff function @xmath and no dividends to the
logarithmic start prices of the underlying assets by piecewise harmonic
functions. In the first step, we will compute a piecewise harmonic
approximation to the function that assigns the European option price
associated with @xmath and the Bermudan’s maturity @xmath to the
logarithmic asset prices at the penultimate time @xmath where exercise
is possible. Then we iteratively compute the expectation of this
function after time @xmath , discount, take the maximum with the payoff
function @xmath , and perform a réduite-based interpolation (in the
one-dimensional setting: a piecewise harmonic interpolation).

Now we would like to answer the following questions: Given the
stationarity of perpetual Bermudan option prices, can we prove that
there exists a minimal fixed point of the iteration step described above
(which would then be an approximation to the perpetual Bermudan price)?
If so, can we characterise it explicitly? Is the iteration step
monotone?

First, we will discuss these questions in the one-dimensional setting –
very little knowledge of potential theory has to be assumed for the
proofs in that section. Second, we shall generalise that approach to
higher dimensions; this will entail a few technical subtleties.

#### 5.2 Piecewise harmonic Bermudan option pricing for options on one
asset

Consider @xmath , the set of (mutually distinct) support abscissas , and
let @xmath be the infinitesimal generator of a Markov semigroup of
operators on Lebesgue measurable functions from @xmath to @xmath . We
call a function @xmath @xmath -harmonic (or shorter: harmonic , if no
ambiguity can arise) if and only if @xmath . Let @xmath denote the
semigroup generated by @xmath .

For the following, assume @xmath to be a second-order differential
operator , that is, there are constants @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

A function @xmath is said to be subharmonic ( superharmonic ) if and
only if @xmath is right- and left-differentiable (thus, letting @xmath
become well-defined as a function from @xmath to @xmath ) and @xmath (
@xmath , respectively).

In particular, the supremum (infimum) of countably many harmonic
functions is subharmonic (superharmonic).

###### Lemma 5.1.

Given two support abscissas and ordinates, there is a unique harmonic
interpolation, provided @xmath is a second-order differential operator
with a non-trivial second-order part (i e @xmath ) or a non-zero
first-order part (i e @xmath ).

###### Proof sketch.

The uniqueness is a consequence of the maximum principle for harmonic
functions. The existence follows (in our one-dimensional setting) by
distinguishing the cases delineated in the statement of the Lemma. If
@xmath is a second-order operator and it has only a non-zero term of
second order, then the space of solutions are all affine-linear
functions from @xmath to @xmath . This space is two-dimensional. If
there are terms of different order, the space of solutions will have
basis elements of the form @xmath and we have to solve a linear or
quadratic equation to find the @xmath (or @xmath ’s) satisfying this
linear or quadratic equation. Since @xmath is sub-Markovian, there will
be at least one real solution to this equation for @xmath . ∎

The Lemma implies

###### Corollary 5.1.

There cannot be more than two linearly independent harmonic functions:
There is a canonical monomorphism from the space of functions to the –
two-dimensional – space of pairs of subordinates.

###### Lemma 5.2.

A subharmonic function from @xmath to @xmath is constantly zero if it
has three zeros.

###### Proof.

The left- and right-differentiablility of subharmonic functions entail
that for all subharmonic @xmath , @xmath will be defined as a function
from @xmath to @xmath . ∎

If there is only a first order non-zero term, the space of harmonic
functions will just coincide with the space of constant functions.

###### Lemma 5.3.

1.   Piecewise harmonic interpolation with respect to the support
    abscissas @xmath preserves subharmonicity on @xmath : The
    interpolating function dominates the interpolated function on @xmath
    , and if the interpolating function @xmath equals the harmonic
    function @xmath on @xmath for all @xmath , then we have @xmath .

2.   The interpolating function @xmath is strictly dominated by the
    interpolated function @xmath on the intervals @xmath and @xmath .

###### Proof sketch.

1.  The domination part follows from the maximum principle for harmonic
    functions. From the maximum principle, we also get for all @xmath
    that if @xmath , then

      -- -------- --
         @xmath   
      -- -------- --

    Now there are two possibilities: either @xmath on @xmath and @xmath
    on @xmath or the other way round @xmath on @xmath and @xmath on
    @xmath . However, in the former case, the interpolating function
    would equal @xmath on @xmath , which is superharmonic, and it would
    also dominate the subharmonic interpolated function @xmath on @xmath
    . Then, @xmath would be nonpositive and subharmonic on @xmath and it
    would have three zeroes, in @xmath , @xmath and @xmath . By Lemma
    5.2 , this can only be true if @xmath on @xmath . Thus, @xmath on
    @xmath . Since @xmath is subharmonic on @xmath , so must be @xmath
    then, and therefore, @xmath is harmonic on @xmath . This means
    @xmath (as both @xmath and @xmath are harmonic) which contradicts
    our assumption that @xmath . Therefore, @xmath on @xmath and @xmath
    on @xmath for all @xmath .

    Inductively, this yields @xmath on @xmath for all @xmath , hence
    @xmath on @xmath .

2.  The function @xmath is subharmonic on @xmath and it has two zeroes
    in @xmath and @xmath . Moreover, it is nonpositive on @xmath .
    Because of Lemma 5.2 , then @xmath has to be positive or negative on
    @xmath . In the former case, we are done. In the latter case, due to
    the maximum principle, @xmath must be decreasing and therefore in
    @xmath we would have @xmath , which is absurd. A symmetric argument
    works for the proof of the domination of @xmath by @xmath on the
    interval @xmath .

∎

###### Lemma 5.4.

Piecewise harmonic interpolation to a set of support absicssas @xmath is
monotone on @xmath in the sense that if @xmath on @xmath , then the
piecewise harmonic interpolation of @xmath will be dominated by the
piecewise harmonic interpolation of @xmath on @xmath .

###### Proof.

Use the maximum principle on each of the intervals @xmath for @xmath . ∎

###### Lemma 5.5.

Let @xmath denote the operator of piecewise harmonic interpolation with
respect to the set of support abscissas @xmath . Let @xmath be
subharmonic on @xmath . Consider a harmonic function @xmath , assumed to
dominate @xmath : @xmath on @xmath . Then @xmath on @xmath .

###### Proof.

From the previous Lemma 5.4 , we already know that @xmath holds for all
@xmath . However, @xmath , hence @xmath on @xmath and from Lemma 5.3 ,
we conclude that @xmath on the intervals @xmath and @xmath . ∎

###### Theorem 5.1.

Let @xmath again denote the operator of piecewise harmonic interpolation
with respect to the set of support abscissas @xmath . Let @xmath be a
subharmonic function, let @xmath be nonnegative and subharmonic, and let
@xmath be harmonic. Let @xmath be, moreover, harmonic on each of the
intervals @xmath for @xmath . Suppose @xmath on @xmath and @xmath on
@xmath , @xmath and let @xmath . Now define

  -- -------- --
     @xmath   
  -- -------- --

as well as

  -- -- --
        
  -- -- --

Then @xmath maps the convex and bounded subset @xmath of @xmath
continuously to itself. Moreover, due to Lemma 5.1 , @xmath is a subset
of a finite-dimensional subspace of @xmath (this subspace being the
space of all functions from @xmath that are harmonic on each of the
intervals @xmath for @xmath . By Brouwer’s Fixed Point Theorem, @xmath
has got a fixed point in @xmath . Finally, @xmath is a composition of
monotone functions on @xmath and therefore monotone as well.

###### Proof sketch.

We can divide the proof for @xmath into three parts:

1.  The cone of subharmonic functions is closed under @xmath , under
    @xmath , under multiplication by constants and under piecewise
    harmonic interpolation @xmath (cf Lemma 5.3 ), therefore the image
    of @xmath under @xmath can only consist of subharmonic functions.

2.  The upper bound on the elements of the image @xmath follows from the
    monotonicity of @xmath and @xmath (Lemma 5.4 ), combined with the
    equations @xmath and @xmath as well as the Lemma 5.5 : First, we may
    state @xmath for all @xmath , which by Lemma 5.5 allows us to deduce

      -- -------- --
         @xmath   
      -- -------- --

    for all @xmath .

3.  The lower bound follows again from the monotonicity of @xmath , but
    this time only by exploiting @xmath on @xmath and employing the fact
    that the space of those functions that are harmonic on each of the
    intervals @xmath for @xmath is invariant under the composition of
    @xmath with the restriction to @xmath (yielding @xmath on @xmath ).

Since @xmath is nonnegative, we get that @xmath is bounded by @xmath as
a subset of @xmath , and because @xmath is finite-dimensional, we may
apply Schauder’s Theorem, provided we are given the continuity of @xmath
. However, this last assertion follows from the maximum principle. ∎

The existence of a minimal fixed point for @xmath can be proven
constructively as well:

###### Corollary 5.2.

Let us adopt the notation of the previous Theorem. Then the sequence
@xmath is monotone on @xmath , bounded and dominated by @xmath .
Therefore we have the existence of a limit on @xmath given by

  -- -------- --
     @xmath   
  -- -------- --

This limit is an element of @xmath and therefore can be canonically
extended to the whole of @xmath . By the continuity of @xmath , @xmath
is a fixed point of @xmath . On @xmath , the convergence in the last
equation will be uniform.

###### Proof.

The only part of the Corollary that does not follow directly from the
preceding Theorem 5.1 is the uniformity of the convergence and that
@xmath will be harmonic on each of the intervals @xmath for @xmath .
However, monotone convergence on compact sets preserves harmonicity and
is always uniform (cf e g Meyer [ 22 ] – or, more directly, Port and
Stone [ 25 , Theorem 3.9] if @xmath is the Brownian semigroup). ∎

###### Lemma 5.6.

In the preceding Corollary’s notation, @xmath is the minimal nonnegative
fixed point of @xmath .

###### Proof.

The proof partly copies the one for Lemma 1.4 . Any nonnegative fixed
point @xmath of @xmath must be greater or equal @xmath on @xmath .
Therefore the monotonicity of @xmath on @xmath , implies

  -- -------- --
     @xmath   
  -- -------- --

yielding

  -- -------- --
     @xmath   
  -- -------- --

∎

###### Example 5.1 (Bermudan vanilla call on a dividend-paying asset in
a special Black-Scholes model).

Assume

  -- -------- --
     @xmath   
  -- -------- --

where

  -- -------- --
     @xmath   
  -- -------- --

thus @xmath can be perceived as the semigroup associated to the
logarithmic price process under the risk-neutral measure in the
one-dimensional Black-Scholes model). We will assume that (possibly
after a linear change of the time scale) @xmath and we assume that
@xmath has been cut to discount dividends. Define

  -- -------- --
     @xmath   
  -- -------- --

(the payoff on exercise of a one-dimensional call option with strike
price @xmath ). The infinitesimal generator of the Markov semigroup
@xmath is

  -- -------- --
     @xmath   
  -- -------- --

Thus we obtain

  -- -------- --
     @xmath   
  -- -------- --

hence @xmath is, @xmath -subharmonic. We can find the @xmath -harmonic
functions for @xmath (otherwise they are simply the affine linear
functions) by observing that for all @xmath ,

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

If @xmath , the functions @xmath and @xmath are two linearly independent
harmonic functions, thus by Corollary 5.1 , we have already found a
basis for the space of harmonic functions. If @xmath , the harmonic
functions are exactly the affine linear functions. In order to obtain
the setting of Theorem 5.1 , we will assume @xmath such that the sum
@xmath of @xmath and a sufficiently large positive constant is a
harmonic function dominating @xmath . In order to satisfy the conditions
on @xmath we could simply take @xmath for instance.

#### 5.3 Réduite-based approximation of Bermudan option prices

Suppose @xmath is a Markov semigroup on @xmath ( @xmath ) and @xmath is
the infinitesimal generator of @xmath . We will call a function @xmath
subharmonic if and only if

  -- -------- --
     @xmath   
  -- -------- --

holds pointwise. A function @xmath will be called superharmonic if and
only if @xmath is subharmonic, and @xmath will be called harmomic if it
is both super- and subharmonic.

Let @xmath denote the operator of upper-semicontinuous regularisation,
that is, for all functions @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

(of course, this is a priori only defined as a function taking values in
@xmath ). Consider a harmonic function @xmath and a closed (and
therefore @xmath ) set @xmath and define the réduite operator @xmath on
the set of all subharmonic functions @xmath dominated by @xmath via

  -- -------- --
     @xmath   
  -- -------- --

It is a well-known result from potential theory (cf e g the work of
Paul-André Meyer [ 22 , Théorème T22] ) that there will be a greatest
subharmonic function dominated by @xmath on @xmath and that this
function will be equal to @xmath . Moreover, we have that @xmath on
@xmath except on a set of potential zero, in
probabilistic/potential-theoretic jargon

  -- -------- --
     @xmath   
  -- -------- --

where “q.e.” is, as usual, short-hand for “quasi-everywhere”. Now define

  -- -------- --
     @xmath   
  -- -------- --

Then our definition of the réduite operator @xmath implies @xmath (as
@xmath is dominating the function whose upper-semicntinuous
regularisation is, according to our definition, the réduite @xmath of
@xmath ) and our potential-theoretic characterisation of the réduite –
as the greatest subharmonic function dominated by @xmath on @xmath –
ensures the subharmonicity of @xmath . Therefore,

  -- -------- --
     @xmath   
  -- -------- --

We also have that @xmath is monotone (in the sense that for all @xmath ,
@xmath ) so that @xmath must be monotone as well (from the @xmath
-monotonicity of @xmath and the definition of @xmath ).

Hence

###### Lemma 5.7.

Adopting the notation of the preceding paragaph, @xmath and whenever
@xmath , @xmath .

Let @xmath be a subharmonic function such that @xmath and let @xmath .
The next step is going to be the consideration of the following family
of operators:

  -- -------- --
     @xmath   
  -- -------- --

for @xmath . If @xmath , @xmath for all @xmath , since the operators
@xmath are positive and linear, and @xmath was assumed to be harmonic.
Thus, since @xmath and @xmath , one must have @xmath for all @xmath and
@xmath . Moreover, the operators @xmath preserve subharmonicity and the
maximum of two subharmonic functions is subharmonic again, therefore
@xmath must be subharmonic for all subharmonic @xmath . Finally, since
@xmath is monotone, @xmath has to be monotone for all @xmath Summarising
this, we obtain

###### Lemma 5.8.

Using the notation introduced previously, @xmath and whenever @xmath ,
@xmath for all @xmath .

As a consequence, we derive from the two Lemmas 5.7 and 5.8 the
following:

###### Corollary 5.3.

If we define @xmath (adopting the notation of the previous paragraph),
we have @xmath and whenever @xmath , @xmath .

###### Corollary 5.4.

The map @xmath is a sound iterative Bermudan option pricing algorithm
for the payoff function @xmath (in the sense of Definition 1.6 ).

This already suffices to prove the following

###### Theorem 5.2.

Let @xmath . Then for all @xmath ,

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

Furthermore,

  -- -------- --
     @xmath   
  -- -------- --

(which a priori is only defined as a function with range in @xmath ) is
an element of @xmath and indeed is the least nonnegative fixed point of
@xmath .

###### Proof.

1.  Relation ( 5.1 ) follows from the fact that @xmath is a sound
    algorithm and Remark 1.4 .

2.  Since @xmath maps @xmath to itself, the whole sequence @xmath is
    bounded by @xmath . This entails @xmath as well. Applying Beppo
    Levi’s Theorem on swapping @xmath and @xmath – for bounded
    monotonely increasing sequences of measurable nonnegative functions
    and an arbitrary measure @xmath – to the measures @xmath , @xmath
    and the sequence @xmath , we can exploit the subharmonicity of the
    functions @xmath , @xmath , to deduce

      -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
                  @xmath   @xmath   
      -- -------- -------- -------- --

    which is the subharmonocity of @xmath . As we have already seen,
    @xmath , so @xmath .

3.  If we employ Beppo Levi’s Theorem again, we can show that @xmath and
    @xmath commute for bounded monotonely increasing sequences of
    functions. Thereby

      -- -------- --
         @xmath   
      -- -------- --

4.  That @xmath is the least nonnegative fixed point is seen as in the
    proof of Lemma 1.3 . Any nonnegative fixed point @xmath of @xmath
    must be greater or equal @xmath . Therefore by the monotonicity of
    @xmath and @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

∎

###### Example 5.2 (Bermudan call option with equidistant exercise times
in @xmath on the weighted arithmetic average of a basket in a special
Black-Scholes model).

Let @xmath be a convex combination and for simplicity, assume that the
assets in the basket are independent and each follow the Black-Scholes
model with one and the same volatility @xmath , and let @xmath be the
interest rate of the bond. We may assume that, possibly after a linear
change of the time-scale, @xmath . Then @xmath is the semigroup of this
Markov (even Lévy) basket. Then one has

  -- -------- --
     @xmath   
  -- -------- --

(cf e g Revuz and Yor’s exposition [ 26 ] ), and for

  -- -------- --
     @xmath   
  -- -------- --

we obtain

  -- -------- --
     @xmath   
  -- -------- --

which is pointwise nonnegative if and only if

  -- -------- --
     @xmath   
  -- -------- --

Hence, if @xmath is sufficiently large, @xmath is subharmonic and we can
apply the theory developed earlier in this Chapter, in particular
Theorem 5.2 .

### Chapter 6 Soundness and convergence rate of perpetual Bermudan
option pricing via cubature

When Nicolas Victoir studied “asymmetric cubature formulae with few
points” [ 32 ] for symmetric measures such as the Gaussian measure, the
idea of (non-perpetual) Bermudan option pricing via cubature in the
log-price space was born. In the following, we will discuss the
soundness and convergence rate of this approach when used to price
perpetual Bermudan options.

Consider a convex combination @xmath (that is, @xmath ) and @xmath .
Then there is a canonical weighted arithmetic average operator @xmath
associated with @xmath given by

  -- -------- --
     @xmath   
  -- -------- --

Now suppose @xmath , @xmath , @xmath , @xmath and @xmath . Define an
operator @xmath on the cone of nonnegative measurable functions by

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath is positive and linear, thus monotone (in the sense that
for all @xmath , @xmath ), it follows that @xmath must be monotone as
well. Furthermore, whenever @xmath , we have that @xmath , as the
linearity and positivity of @xmath combined with our assumption on
@xmath imply

  -- -------- --
     @xmath   
  -- -------- --

Finally, due to our assumptions on @xmath and @xmath , we have for all
nonnegative @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Summarising this, we are entitled to state

###### Lemma 6.1.

Adopting the previous paragraph’s notation and setting

  -- -------- --
     @xmath   
  -- -------- --

we have that

  -- -------- --
     @xmath   
  -- -------- --

@xmath is monotone (i e order-preserving), and @xmath is nonnegative.

###### Corollary 6.1.

The map @xmath is a sound iterative Bermudan option pricing algorithm
for the payoff function @xmath (in the sense of Definition 1.6 ).

This is sufficient to prove

###### Theorem 6.1.

For all @xmath ,

  -- -------- -- -------
     @xmath      (6.1)
  -- -------- -- -------

Furthermore,

  -- -------- --
     @xmath   
  -- -------- --

and @xmath is the smallest nonnegative fixed point of @xmath .

###### Proof.

1.  Relation ( 5.1 ) follows from the soundness of @xmath is a sound
    algorithm and Remark 1.4 .

2.  Since @xmath maps @xmath itself, the whole sequence @xmath is
    bounded by @xmath . This entails @xmath as well. Using the linearity
    of @xmath and our previous observation that @xmath (Lemma 6.1 ), we
    can show

      -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
                  @xmath   @xmath   
      -- -------- -------- -------- --

    which means @xmath . As we have already seen, @xmath , so @xmath .

3.  Again, due to the linearity of @xmath and the special shape of
    @xmath that is based on a weighted arithmetic average operator,
    @xmath and @xmath commute for bounded monotonely increasing
    sequences of functions. Thereby

      -- -------- --
         @xmath   
      -- -------- --

4.  Just as in the proof of Lemma 1.3 , we see that @xmath is the
    minimal nonnegative fixed point. For, any nonnegative fixed point
    @xmath of @xmath must be greater or equal @xmath . Thus, by the
    monotonicity of @xmath and @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

∎

###### Lemma 6.2.

Using the previous Theorem’s notation, we have for all @xmath and @xmath
, if @xmath , then @xmath .

###### Proof.

By the monotonicity of the sequence @xmath (Theorem 6.1 ), we have

  -- -------- --
     @xmath   
  -- -------- --

∎

###### Theorem 6.2.

For all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

The preceding Lemma 6.2 yields

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

via the definition of @xmath as @xmath for @xmath and @xmath . But the
last equality implies

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Since @xmath is linear as well as an @xmath -contraction (and therefore
a @xmath -contraction, too), we finally obtain

  -- -------- --
     @xmath   
  -- -------- --

∎

###### Example 6.1 (Bermudan put option with equidistant exercise times
in @xmath on the weighted arithmetic average of a basket in a discrete
Markov model with a discount factor @xmath for @xmath).

Let @xmath be a convex combination and assume that @xmath is such that

  -- -------- -- -------
     @xmath      (6.2)
  -- -------- -- -------

then the functions

  -- -------- --
     @xmath   
  -- -------- --

and @xmath (where @xmath ) satisfy the equations @xmath and @xmath ,
respectively. Moreover, by definition @xmath . Then we know that the
(perpetual) Bermudan option pricing algorithm that iteratively applies
@xmath to the payoff function @xmath on the @xmath -price space, will
increase monotonely and will have a limit which is the smallest
nonnegative fixed point of @xmath . Moreover, the convergence is linear
and the contraction rate can be bounded by @xmath .

The condition ( 6.2 ) can be achieved by a change of the time scale
(which ultimately leads to different cubature points for the
distribution of the asset price)

One might also be interested in determining the convergence rate for the
approximation of non-perpetual American option prices based on
non-perpetual Bermudan option pricing via cubature. After proving a
series of Lemmas we will end up with a Theorem that asserts linear
convergence and also provides bounds for the convergence factor.

From now on, @xmath and @xmath will no longer be fixed but their rôle
will be played by @xmath and @xmath (for @xmath where @xmath shall be
fixed) respectively, where @xmath and @xmath describes a Markov chain on
@xmath (By the Chapman-Komogorov equation this is tantamount to @xmath
).

### Chapter 7 Some convergence estimates for non-perpetual American
option pricing based on cubature

For this Chapter, let us consider an arbitrary but fixed
translation-invariant finite-state Markov chain @xmath with state space
@xmath (for @xmath ) where @xmath for some real number @xmath , as well
as a real number @xmath (the time horizon, or maturity), a real number
@xmath , a continuous function @xmath that is monotone in each
coordinate, a nonnegative real number @xmath and let us set

  -- -------- --
     @xmath   
  -- -------- --

as well as defining a family of maps @xmath , @xmath , by

  -- -------- --
     @xmath   
  -- -------- --

(Note that @xmath will always be nonnegative for @xmath – hence, for all
@xmath , @xmath .) Furthermore, we shall denote by @xmath the set of
(distinct) states at time @xmath after starting the process at time
@xmath in @xmath and by @xmath the weights for each of these states,
thereby imposing on the sets @xmath for @xmath , in addition to it being
a subset of @xmath , the condition that they be a convex combination,
viz.

  -- -------- --
     @xmath   
  -- -------- --

Summarising this, we write

  -- -------- --
     @xmath   
  -- -------- --

For the whole of this section, the Lebesgue measure on @xmath shall be
denoted by @xmath , and @xmath will be shorthand for the
measure-theoretic power @xmath .

The operators @xmath and @xmath when applied to subsets of @xmath will
be understood to be taken componentwise. Analogously, we will interpret
the relations @xmath and @xmath componentwise on @xmath .

For convenience, we allow all @xmath -norms (including the @xmath norm)
of measurable functions to take values in the interval @xmath , thereby
extending the domain for each of the @xmath -norm to @xmath , the vector
lattice of measurable functions. Furthermore, any functions occurring in
this Chapter will be assumed to be measurable. Thus, eg the relation
@xmath should be read as shorthand for @xmath for all functions @xmath ;
analogously for the relation @xmath .

Finally, we will use the operation @xmath in such a way that it is
applied prior to @xmath , but only after @xmath and multiplication with
other functions or constants have taken place:

  -- -------- --
     @xmath   
  -- -------- --

In this Chapter we are aiming to understand the convergence behaviour of
the sequence @xmath . We will start by noting that this sequence is
monotonely increasing:

###### Lemma 7.1.

The sequence @xmath is monotonely increasing for all functions @xmath .
Furthermore, if there exists a function @xmath such that @xmath is
@xmath -harmonic (ie @xmath ) and @xmath , then for all @xmath , @xmath
.

###### Proof.

Consider @xmath and @xmath such that @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

and by the monotonicity of the operators @xmath for @xmath ,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

where the last line is a consequence of the Chapman-Kolmogorov equation.
This completes the proof for the monotonicity of the sequence @xmath .

Now suppose there exists such a function @xmath as in the statement of
the Lemma. Then @xmath for all @xmath and therefore @xmath for all
@xmath . Also, the map @xmath is monotone in the sense that @xmath
always implies @xmath (because it is the composition of two monotone
maps: @xmath and @xmath ) for all @xmath . Thus we see that for all
@xmath

  -- -- --
        
  -- -- --

∎

###### Lemma 7.2.

For all measurable functions @xmath , as well as for all @xmath and
@xmath one has

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

(with the usual convention that @xmath for all @xmath ).

###### Proof.

The map @xmath is monotone. Thus we have

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

for @xmath . Since @xmath , this implies

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

which yields the assertion as @xmath is an @xmath -contraction (for
@xmath this is immediate and for @xmath it follows from the
translation-invariance of both @xmath and the Lebesgue measure). ∎

###### Lemma 7.3.

Suppose

  -- -------- --
     @xmath   
  -- -------- --

componentwise, implying @xmath componentwise for all @xmath and @xmath .
Then for all @xmath , @xmath is nonnegative on @xmath .

###### Proof.

Recalling our notational convention that @xmath as relation on @xmath
and @xmath when applied to subsets of @xmath are to be interpreted
componentwise, we may write

  -- -------- --
     @xmath   
  -- -------- --

due to the componentwise monotonicity of @xmath , yields for all @xmath
the inclusion

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

∎

###### Lemma 7.4.

Suppose there is a @xmath (without loss of generality, @xmath ) such
that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath (where @xmath with @xmath whence it is sufficient that
this estimate holds for @xmath ). In addition, assume that @xmath on the
subset @xmath of @xmath for all @xmath (this assumption being, due to
Lemma 7.3 , satisfied in particular if @xmath ). Then for all @xmath ,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

###### Proof.

Let @xmath . Due to our assumption of @xmath on @xmath , one has

  -- -- -- -------- -------- -------
           @xmath            (7.1)
           @xmath   @xmath   
           @xmath   @xmath   
           @xmath   @xmath   
                    @xmath   
  -- -- -- -------- -------- -------

On the other hand

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

that is

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

where we have exploited @xmath . Now @xmath gives

  -- -------- --
     @xmath   
  -- -------- --

since @xmath . Combining this estimate with the previous inclusion, one
obtains

  -- -------- --
     @xmath   
  -- -------- --

and hence

  -- -------- --
     @xmath   
  -- -------- --

This result, combined with the first equation ( 7.1 ) in this Proof,
yields

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

However, one of our assumptions reads

  -- -------- --
     @xmath   
  -- -------- --

whence we conclude

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

∎

###### Remark 7.1.

The assumption of the existence of a @xmath such that @xmath for all
@xmath is natural: If @xmath was a Markov process evolving according to
@xmath , the (stronger) condition

  -- -------- --
     @xmath   
  -- -------- --

simply means that the process @xmath is, after discounting, a
martingale. Now, if @xmath was a Markov model for a vector of
logarithmic asset prices (a Markov basket in our terminology) and @xmath
would assign to each vector the arithmetic average of the exponentials
of its components, this is by definition true if @xmath governs the
process @xmath under a risk-neutral measure. Furthermore, the said
assumption

  -- -------- --
     @xmath   
  -- -------- --

trivially implies

  -- -------- --
     @xmath   
  -- -------- --

and therefore provides us with some vindication for assuming the last
assertion in some of the subsequent Lemmas of this Chapter.

###### Lemma 7.5.

Suppose there is a @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath (for which in our case of @xmath with @xmath it is
sufficient that this estimate holds for @xmath ), and let us assume
without loss of generality that this @xmath be @xmath . Then, setting

  -- -------- --
     @xmath   
  -- -------- --

we have found an @xmath such that for all @xmath and measurable @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Let @xmath and consider a measurable @xmath . Then by our assumption of
@xmath for @xmath , we firstly have (inserting @xmath for @xmath )

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

and therefore (using @xmath and @xmath as well as the monotonicity of
@xmath ),

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (7.2)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

Now,

  -- -------- --
     @xmath   
  -- -------- --

since @xmath is right-differentiable in zero with derivative @xmath .
Therefore

  -- -------- --
     @xmath   
  -- -------- --

Via estimate ( 7.2 ), we arrive at

  -- -------- -- -------
     @xmath      (7.3)
  -- -------- -- -------

But

  -- -------- --
     @xmath   
  -- -------- --

and – in combination with the linearity of @xmath and the
Chapman-Kolmogorov equation – this implies

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

hence by equation ( 7.3 )

  -- -- -------- -------- -------- -------
                 @xmath            (7.4)
                 @xmath   @xmath   
        @xmath   @xmath            (7.5)
  -- -- -------- -------- -------- -------

(where in ( 7.5 ) we have exploited the fact that @xmath is an @xmath
-contraction).

Now, again by the Chapman-Kolmogorov equation and the monotonicity of
@xmath for any @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

and therefore (due to the estimate @xmath which holds for arbitrary
@xmath and @xmath )

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

But

  -- -------- --
     @xmath   
  -- -------- --

thus the last inclusion yields

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

where the last line has used the estimate ( 7.5 ) derived previously. ∎

Later on, in Lemma 7.6 , we will see that it is impossible to obtain
estimates for @xmath that are both uniform in @xmath and of higher than
linear order in @xmath .

We can draw from the proof of Lemma 7.5 the following Corollary:

###### Corollary 7.1.

Suppose there is a @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath (for which in case @xmath with @xmath it is sufficient
that this estimate holds for @xmath ). Then for all measurable @xmath

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

We will continue to assume @xmath , @xmath and define

###### Definition 7.1.

  -- -------- --
     @xmath   
  -- -------- --

###### Remark 7.2.

Equivalent expressions for @xmath are:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

These formulae for @xmath imply, by the monotonicity of @xmath , that
@xmath is north-east connected (that is @xmath for all @xmath ) for all
@xmath . Furthermore, if one had for all @xmath and @xmath an index
@xmath such that @xmath componentwise (for instance if the set @xmath
could be written as the sum of a reflection symmetric subset of @xmath
and a componentwise nonpositive vector), then the north-east
connectedness of the @xmath ’s entails for all @xmath and @xmath ,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

where for the penultimate line we have used the Chapman-Kolmogorov
equation, of course. Therefore

  -- -------- --
     @xmath   
  -- -------- --

Also, if there exists an @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

one has – due to the monotonicity of @xmath in each coordinate – first
of all @xmath and thence for all @xmath the inclusion

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

This means

  -- -------- --
     @xmath   
  -- -------- --

and for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

The reason for @xmath not being the whole space is that the measure
@xmath on the Borel @xmath -algebra of @xmath has compact support.

If one interprets @xmath as a logarithmic payoff function (eg @xmath ,
@xmath in case of a vanilla one-dimensional put) and @xmath as a Markov
chain that models the stochastic evolution of the logarithmic prices of
assets in a given portfolio, then the set @xmath , for @xmath consists
of all those vectors of logarithmic start prices where the probability
of exercising the option at time @xmath is strictly positive.

###### Lemma 7.6.

Suppose there is a @xmath (without loss of generality, @xmath ) such
that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath (where @xmath with @xmath whence it is sufficient that
this estimate holds for @xmath ). Assume furthermore that @xmath ,
implying that @xmath on the set @xmath . Then for all @xmath there is an
@xmath independent of @xmath such that for all @xmath and @xmath (with
positive Lebesgue measure),

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

as well as

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

(the left hand side, following the usual convention, being @xmath if
@xmath , @xmath and @xmath ).

###### Proof.

Let us first remark that, due to Corollary 7.1 , we have

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

as well as

  -- -- -- -------- -------- -------
           @xmath            (7.6)
           @xmath   @xmath   
           @xmath   @xmath   
           @xmath   @xmath   
  -- -- -- -------- -------- -------

for all @xmath (using the translation invariance of @xmath ).

Next let us note that by our assumption of @xmath componentwise for all
@xmath , combined with the north-east connectedness of @xmath (which
entails south-west connectedness of @xmath ), we have

  -- -------- --
     @xmath   
  -- -------- --

Therefore we may conclude that for all @xmath and @xmath ,

  -- -------- -------- -------- -------- -------
                       @xmath            (7.7)
                       @xmath   @xmath   
                       @xmath   @xmath   
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
     @xmath   @xmath   @xmath            
  -- -------- -------- -------- -------- -------

Now, off @xmath one has due to Lemma 7.4 (which may be applied thanks to
our assumption @xmath ) the following situation:

  -- -- -- -------- -------- -------
           @xmath            (7.8)
           @xmath   @xmath   
           @xmath   @xmath   
           @xmath   @xmath   
           @xmath   @xmath   
  -- -- -- -------- -------- -------

However, one can also perform the calculation

  -- -- -------- -------- -------- --------
                 @xmath            (7.9)
                 @xmath   @xmath   
        @xmath   @xmath            (7.10)
  -- -- -------- -------- -------- --------

(where we have used the assumption @xmath to get from ( 7.9 ) to ( 7.10
)). Combining estimates ( 7.10 ) and ( 7.8 ), we arrive at

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

for every @xmath for some @xmath dependent on @xmath and finally (using
estimate ( 7.7 ), @xmath and @xmath )

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

for all @xmath and @xmath .

This yields – due to the translation-invariance of the Lebesgue measure
(which gave us estimate ( 7.6 )) – the first line of the Lemma’s @xmath
norm estimate. It also implies the @xmath norm estimate of the Lemma
since for all @xmath (in particular for @xmath ),

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

(a consequence of the monotonicity of @xmath ), and therefore

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

∎

###### Remark 7.3.

Assume @xmath is not strictly less than @xmath , say @xmath for some
@xmath . We can use the property of @xmath being monotonely increasing
in each component to see, via Remark 7.2 that

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

where the set in the last line has infinite Lebesgue measure.

Thus, @xmath whenever @xmath fails to hold.

Keeping Corollary 7.1 in mind, our next step shall consist in proving

###### Lemma 7.7.

Let @xmath . Suppose there is a @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath (where @xmath with @xmath and therefore it is sufficient
that this estimate holds for @xmath ). Let us define

  -- -------- --
     @xmath   
  -- -------- --

Then there is a constant @xmath given by

  -- -------- --
     @xmath   
  -- -------- --

such that for all @xmath and measurable @xmath ,

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

###### Proof.

For all @xmath , the following estimates hold on @xmath :

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

for some real constant @xmath that can be bounded by

  -- -------- --
     @xmath   
  -- -------- --

This gives a uniform pointwise estimate for the nonnegative function
@xmath on @xmath from which the Lemma’s estimate can be derived
immediately. ∎

###### Corollary 7.2.

Let @xmath . Assume there exists an @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

Then one has

  -- -------- --
     @xmath   
  -- -------- --

and for all @xmath and measurable @xmath ,

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

###### Proof.

The assumption about @xmath implies that @xmath as @xmath by Remark 7.2
), hence @xmath which suffices to prove the Corollary. ∎

###### Lemma 7.8.

If there is an @xmath such that @xmath componentwise (which entails
@xmath by the monotonicity of @xmath in each component), one will have
the following upper bound for the measure of the set occuring in the
preceding Lemma 7.7 :

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

for all measurable @xmath .

###### Proof.

We shall establish an upper bound for the set @xmath . Since by our
assumption

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath , we may, once again exploiting @xmath , derive

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

This implies

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

∎

This and Lemma 7.7 readily yield, via Corollary 7.1 , the following

###### Lemma 7.9.

Suppose @xmath and @xmath componentwise. Assume furthermore that there
exists a real number @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

Then for all @xmath ,

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
                 @xmath   
  -- -- -------- -------- --

wherein

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Consider @xmath . Via our assumption of @xmath componentwise, one has

  -- -------- --
     @xmath   
  -- -------- --

componentwise. Since the set @xmath is north-east connected, this yields
@xmath for all @xmath , which in turn – via @xmath for all @xmath –
gives

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

for all @xmath . This yields, replacing @xmath by @xmath ,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

for all @xmath . Therefore – using in addition the
translation-invariance of @xmath and @xmath (which makes @xmath a map
that preserves the @xmath -norm of nonnegative measurable functions) –
we deduce that for all measurable @xmath ,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

From this, using Corollary 7.1 , we derive

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

This is enough to prove the Lemma once one takes advantage of Lemma 7.7
and Corollary 7.2 . ∎

###### Remark 7.4.

Let the translation-invariant Markov semigroup @xmath be derived from a
cubature formula for the Gaussian measure with points @xmath in such a
way that a geometric Brownian motion with logarithmic drift @xmath (
@xmath and @xmath being the interest rate of the price process and the
volatility vector, respectively) shall be approximated, that is to say

  -- -- --
        
  -- -- --

Then the assumption that all the @xmath be componentwise nonpositive for
@xmath reads

  -- -------- --
     @xmath   
  -- -------- --

and therefore simply means that @xmath is componentwise at least as
small or even smaller than @xmath which, needless to say, equals @xmath
in case of an axis-symmetric cubature formula for the Gaussian measure.
This assumption is tantamount to

  -- -------- --
     @xmath   
  -- -------- --

that is

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

for all @xmath , entailing that @xmath models a basket of logarithmic
asset prices whose volatilities are bounded below by the positive number
@xmath .

Now, emphasising again that our investigations are only concerned with
discrete translation-invariant Markov chains @xmath (Markov chains which
are derived from cubature formulae, for instance), we can use rather
elementary inequalities to find upper bounds on the subsets of @xmath
occurring in the estimates of Lemma 7.7 .

We will start with the simple, nevertheless practically important,
example of a one-dimensional American vanilla put:

###### Lemma 7.10.

Suppose @xmath and @xmath . Under these assumptions there exists a
@xmath such that @xmath , and furthermore, one has for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

The real number @xmath is given by the relation

  -- -------- --
     @xmath   
  -- -------- --

that is

  -- -------- --
     @xmath   
  -- -------- --

Next we observe that on the one hand by Remark 7.2

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

and secondly

  -- -------- --
     @xmath   
  -- -------- --

thus

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

∎

Applying the preceding Lemmas and using Corollary 7.1 , we conclude by
stating

###### Theorem 7.1.

Suppose @xmath and @xmath . Under these assumptions there is a @xmath
such that @xmath for all @xmath . Assume, moreover, that

  -- -------- --
     @xmath   
  -- -------- --

Then there is a real constant @xmath such that for all @xmath and for
all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

We can compute @xmath explicitly as

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

One only has to apply Lemma 7.9 for @xmath , which one is entitled to by
Lemma 7.10 . ∎

Now we shall proceed to establish convergence estimates for the sequence
@xmath in the @xmath -norm, for all measurable @xmath and measurable
@xmath .

###### Lemma 7.11.

Suppose @xmath and @xmath . Under these assumptions there is a @xmath
such that @xmath . Assume, moreover, that

  -- -------- --
     @xmath   
  -- -------- --

Under these assumptions there exists a real number @xmath (the same as
in Theorem 7.1 ) such that for all @xmath , @xmath and measurable @xmath
, one has

  -- -------- --
     @xmath   
  -- -------- --

The proof is contrived inductively, the base step being Theorem 7.1 ,
and the induction step being the first part of Lemma 7.13 . However, the
second and more general part of Lemma 7.13 – which we will need later on
in this Chapter when we study options on multiple assets – requires the
following auxiliary result.

###### Lemma 7.12.

Let @xmath , @xmath measurable, and assume

  -- -------- --
     @xmath   
  -- -------- --

(which due to @xmath is equivalent to @xmath for all @xmath ). Then
@xmath by Remark 7.2 , and for all @xmath and @xmath ,

  -- -- -------- -------- --
                          
        @xmath   @xmath   
  -- -- -------- -------- --

###### Proof.

Consider a measurable set @xmath and measurable functions @xmath .
Similarly to the proof of Lemma 7.2 , we observe that due to the
monotonicity of @xmath and the fact that @xmath for all @xmath ,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

that is

  -- -------- --
     @xmath   
  -- -------- --

Combining this with the monotonicity of @xmath as well as the fact that
@xmath for all @xmath (which in turn is a consequence of the north-east
connectedness of @xmath – cf Remark 7.2 – and the assumption that @xmath
for all @xmath ), we obtain

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

Now, since

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath and @xmath , this means

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

Combining this pointwise estimate with the translation-invariance of the
Lebesgue measure yields

  -- -- -------- -------- --
                          
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

where we have used the inclusion @xmath which – owing to the north-east
connectedness of the sets @xmath and our assumption @xmath – holds for
arbitrary @xmath and @xmath as well as the assumption @xmath .
Similarly, the translation-invariance and the sub-linearity of the
@xmath -norm imply

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

where again one has exploited the inclusion @xmath that holds for any
@xmath and @xmath .

∎

###### Lemma 7.13.

Let @xmath and @xmath . Consider a real number @xmath and a measurable
set @xmath . Suppose one has an estimate of the kind

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

Assume, moreover, @xmath (which by the Chapman-Kolmogorov equation is
firstly equivalent to @xmath for all @xmath and @xmath and secondly also
entails @xmath ). Then we get for all measurable @xmath and for all
@xmath , @xmath such that @xmath , the estimate

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

Furthermore, if one assumes in addition

  -- -------- --
     @xmath   
  -- -------- --

then one has a related implication for @xmath instead of @xmath for all
measurable @xmath : If under these assumptions the assertion

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

holds, then the estimate

  -- -- -------- -------- --
                 @xmath   
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

holds for all @xmath and @xmath such that @xmath .

###### Proof.

For both parts of the Lemma, we will conduct an induction in @xmath ,
the initial (or base) step being tautological each time. We have for all
@xmath and @xmath the estimate

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
                 @xmath   
                 @xmath   
                 @xmath   
        @xmath   @xmath   
                 @xmath   
  -- -- -------- -------- --

which plays a crucial part in both the first and the second part of the
Lemma. For, we can first of all note that the induction hypothesis in
the situation of the first part of the Lemma reads

  -- -- -- -------- -------- --------
           @xmath            (7.11)
                    @xmath   
           @xmath   @xmath   
  -- -- -- -------- -------- --------

And if one now applies this induction hypothesis ( 7.11 ) for @xmath
(recalling that by assumption @xmath , thus @xmath ) to the previous two
equations and uses Lemma 7.12 , then one gets by the triangle inequality
for the @xmath -norm,

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
                 @xmath   
        @xmath   @xmath   
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

In order to be entitled to apply Lemma 7.12 in this situation we have
successively used the fact that

  -- -------- --
     @xmath   
  -- -------- --

This completes the induction step for the first part of the Lemma.

Turning to the proof of the second assertion in the Lemma (where @xmath
is assumed), we remark that

  -- -------- -------- -------- -------- --------
                       @xmath            
     @xmath   @xmath   @xmath            (7.12)
                       @xmath   @xmath   
                       @xmath   @xmath   
              @xmath   @xmath            (7.13)
  -- -------- -------- -------- -------- --------

In particular, if @xmath , @xmath is decreasing in @xmath :

  -- -------- --
     @xmath   
  -- -------- --

Similarly to proof of the first part of the present Lemma, we deduce

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
                 @xmath   
        @xmath   @xmath   
                 @xmath   
  -- -- -------- -------- --

from the triangle inequality. But by a successive application of Lemma
7.12 , combined with the properties ( 7.13 ) of @xmath , we have for all
@xmath ,

  -- -- -------- -------- --
                          
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath            
        @xmath            
        @xmath   @xmath   
  -- -- -------- -------- --

In light of the inclusion @xmath , we also have

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
                 @xmath   
  -- -- -------- -------- --

Combining the previous two sets of estimates leads to

  -- -- -------- -------- --
                          
        @xmath   @xmath   
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

where in the last line we have taken advantage of the induction
hypothesis

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

for the special case @xmath ∎

The assumption of @xmath while @xmath componentwise corresponds to the
volatility attaining a certain critical value:

###### Remark 7.5.

Consider a cubature formula for the one-dimensional Gaussian measure
with cubature points @xmath which will then give rise to a new Markov
chain via

  -- -------- --
     @xmath   
  -- -------- --

(if simply @xmath , then this was a discrete model for a logarithmic
asset price evolution that converges weakly to the Black-Scholes model
with volatility @xmath and discount rate @xmath when @xmath ). In this
setting, the set of pairs @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

has at most two elements, ie it is a Lebesgue null set. However, in
practice, we will not have the exact values of the volatility @xmath
(and if the maturity is sufficiently large, one will not even have an
exact value for the interest rate @xmath ), but we will only know that
@xmath for some @xmath . So, given @xmath the set of volatility
parameters that both fit the model and allow for the previous Lemma to
be applied will equal

  -- -------- --
     @xmath   
  -- -------- --

If @xmath and the equation characterising this set has a solution @xmath
, this set will at least have positive Lebesgue measure, so that there
is some hope that our condition of @xmath (which we had to impose in the
second part of the previous Lemma 7.13 ) can be satisfied in practice at
least occasionally.

With the first half of Lemma 7.13 , we have completed the proof of Lemma
7.11 . We shall now apply this result to finally get to a convergence
bound for @xmath – which can be conceived of as a sequence of
non-perpetual Bermudan option prices when successively halving the
exercise mesh size.

###### Lemma 7.14.

Let @xmath . Consider a real constant @xmath as well as a measurable set
@xmath and a set @xmath of nonnegative measurable functions, and suppose
one has an estimate of the kind

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

Then for all @xmath , @xmath and @xmath , the estimate

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

holds.

###### Proof.

With @xmath , @xmath , @xmath as in the statement of the Theorem, we
obtain by the triangle inequality

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

This suffices to prove the Theorem. ∎

Thus, if we combine this last Lemma 7.14 with Lemma 7.11 we arrive at

###### Theorem 7.2.

Suppose, as before, @xmath and @xmath . Under these assumptions there is
a @xmath such that @xmath , and let us suppose this @xmath . Assume,
moreover, that

  -- -------- --
     @xmath   
  -- -------- --

Under these assumptions there exists a real number @xmath such that for
all @xmath , @xmath and monotonely decreasing @xmath , one has

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

Analogously, we may proceed to prove convergence of higher order in
@xmath for @xmath , where @xmath is a convex combination (the weights
for a weighted average of the components/assets in a @xmath -dimensional
basket), as well as for the choices @xmath and @xmath . However, this
time, we shall employ different norms: @xmath for a compact subset
@xmath such that @xmath .

The first part of this endeavour will be to prove certain
generalisations of Lemmas 7.7 and 7.12 .

###### Lemma 7.15.

If @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

where

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

We have for all @xmath the estimate

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

in particular this estimate holds for @xmath . But this is to say

  -- -------- --
     @xmath   
  -- -------- --

hence we have proven the estimate in the Lemma for @xmath . This readily
suffices to prove the Lemma’s assertion in its full generality, as
@xmath is a Markov semigroup and by applying the Chapman-Komogorov
equation (and the monotonicity of @xmath ) inductively,

  -- -------- --
     @xmath   
  -- -------- --

∎

###### Lemma 7.16.

If @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

where

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

We have for all @xmath the estimate

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
                       @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

in particular the estimate holds for @xmath again. But this means

  -- -------- --
     @xmath   
  -- -------- --

hence we arrive at the estimate of the Lemma for @xmath . This readily
suffices to prove the Lemma’s assertion in its full strength, as @xmath
is a Markov semigroup and by applying the Chapman-Komogorov equation
inductively,

  -- -------- --
     @xmath   
  -- -------- --

∎

###### Lemma 7.17.

If this time @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

where

  -- -- --
        
  -- -- --

###### Proof.

We have for all @xmath the estimate

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

in particular the estimate holds for @xmath . But this is – as it was in
the proofs of the two preceding Lemmas – to say

  -- -------- --
     @xmath   
  -- -------- --

hence we have proven the estimate in the Lemma for @xmath . This readily
suffices to prove the Lemma’s assertion, as @xmath is a Markov semigroup
and by applying the Chapman-Komogorov equation inductively,

  -- -------- --
     @xmath   
  -- -------- --

∎

Thus at least for certain choices of @xmath – viz. weighted arithmetic
average of the exponential components, minimum of the exponential
components and maximum of the exponential components – we can apply
Lemma 7.7 .

Therefore we shall next turn our attention to deriving upper bounds for
the measures of the sets in the estimates of Lemma 7.7 for the said
examples of @xmath , @xmath and @xmath . We continue to use the notation
@xmath and

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the Markov chain generated by @xmath .

###### Lemma 7.18.

If @xmath , then for all @xmath ,

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

as well as

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

###### Proof.

Let @xmath . Then

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

and also

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

∎

###### Corollary 7.3.

If @xmath , then for all @xmath ,

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

###### Proof.

Let @xmath . We simply remark that

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

∎

###### Lemma 7.19.

If @xmath , then for all @xmath ,

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

as well as

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

###### Proof.

Let @xmath . Then

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

and also

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

∎

###### Corollary 7.4.

If @xmath , then for all @xmath ,

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

###### Proof.

Let @xmath . We simply remark that

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

∎

These estimates lead to the following Corollary that will enable us –
under the assumption of

  -- -------- --
     @xmath   
  -- -------- --

(in order to be entitled to apply eg Lemma 7.13 ) – to prove an @xmath
-convergence estimate (on a particular subset of @xmath ) for @xmath for
any measurable @xmath .

###### Corollary 7.5.

Suppose @xmath and consider any compact set @xmath . Then

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

for all @xmath .

###### Proof.

Let @xmath . Since

  -- -------- --
     @xmath   
  -- -------- --

by the monotonicity of @xmath , we only have to observe that

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

to arrive – after taking advantage of the preceding Corollary 7.4 – at

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
                 @xmath   
  -- -- -------- -------- --

However, by our assumption that @xmath be compact, there is some @xmath
such that @xmath . Thus

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
                 @xmath   
  -- -- -------- -------- --

and from this inclusion we may deduce the estimate given in the Lemma. ∎

The inequality we have just derived implies that the @xmath -volume of
the set occurring in Lemma 7.7 is of order @xmath for any compact @xmath
and for @xmath . Hence again by Lemma 7.7 (which is applicable because
of Lemma 7.17 ) we obtain that the difference @xmath is of order @xmath
(this time, however in the @xmath -norm). This estimate on the norm of
@xmath leads, via Lemmas 7.14 and 7.13 to the result that the analogon
of the difference in Theorem 7.2 is of order @xmath , too:

###### Theorem 7.3.

Suppose @xmath and consider a compact set @xmath . Assume that

  -- -------- --
     @xmath   
  -- -------- --

Under these assumptions there exists a real number @xmath such that for
all @xmath , @xmath and @xmath , one has

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

###### Remark 7.6.

This @xmath -convergence result has some (however, because of our
assumption @xmath , fairly limited) practical interest, as in practice
quite frequently the exact start price of the (multiple) asset on which
an option is issued, is unknown. Instead, one will have the logarithmic
start price vector @xmath a short time @xmath before the actual option
contract becomes valid. Now, asuming that @xmath has a continuous
density @xmath , this function @xmath will be bounded on @xmath by some
constant

  -- -------- --
     @xmath   
  -- -------- --

One will therefore have for all @xmath , @xmath and @xmath ,

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

## Part IV Numerical analysis of cubature-based American pricing

### Chapter 8 Motivating Bermudan pricing based on cubature

#### 8.1 The general setting

This Chapter has been designed to elaborate the idea of Bermudan option
pricing via cubature and to put it into a context of other Bermudan
option prcing algorithms. Given its informal character, it can also be
perceived as another introductory chapter.

Consider a basket of @xmath assets. A @xmath -dimensional Bermudan
option is an option that can be exercised at a discrete set of exercise
times, yielding payoff @xmath if @xmath is the vector of logarithmic
prices (of stocks in the @xmath -dimensional basket) at that time, for
some @xmath which will be called the payoff function . In the case of a
one-dimensional put option with strike price @xmath for example, one
would have @xmath . In case of a call on a stock index, @xmath would be
the positive part of the difference between a weighted sum of
exponential functions of the coordinate entries and the strike price.
Unless specified otherwise, we will from now on assume the exercise
times to be equidistant with an exercise mesh size @xmath . Adopting the
notation of Chapter 1 , this is to say @xmath .

We regard such a Bermudan option as a binary tree of European options.
This means that at each exercise time one has to decide whether it is
more rewarding to keep the option or to exercise it – in other words,
whether the payoff at that exercise time is less than the value of the
(European) option to exercise at the next exercise time. A recursive
algorithm is thus obtained. Note however that this binary tree of
European options has continuum many nodes at each level, one for each
price vector at the subsequent exercise time.

Let us now describe this recursion in detail. Suppose the option is
non-perpetual, i.e. it has a maturity time @xmath , and assume
furthermore that @xmath for some @xmath . Then the Bermudan option price
for a start price vector @xmath will be @xmath where the @xmath are
computed according to the following backward recursion (where for
simplicity we assume the logarithmic discount rate to be a constant
@xmath and @xmath denotes the @xmath -valued process of vectors
comprised of the asset prices in the basket):

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- -- -------
     @xmath      (8.1)
  -- -------- -- -------

Recursion formulae of this kind for the pricing of Bermudan options are
fairly standard and can be found for example in textbooks such as Hull’s
[ 13 ] or Wilmott, Howison and Dewynne’s [ 33 ] . To use these recursion
formulae practically, one needs a way to summarise or approximate the
state @xmath in a way that permits the equivalent summarisation or
approximation for @xmath (this for all positive integer @xmath ). One
method of achieving this in a one-dimensional setting is the application
of Fourier-Hermite expansions to the functions @xmath , as studied in
the paper by Chiarella, el-Hassan and Kucera [ 8 ] . Our goal is to
develop their approach; in particular we will employ cubature formulae
for symmetric measures. These methods of approximating integrals by
weighted (finite) averages can be computationally efficient, and with
increasing dimension may be superior to other approaches. Victoir [ 32 ]
introduced a vital improvement by constructing sequences that scale
well. This route to high-dimensional Bermudan and American option
pricing was proposed for the first time by my supervisor [ 20 ] .

#### 8.2 Application to the Black-Scholes model

Let us in this section work within the situation of the
multi-dimensional Black-Scholes model, that is to say that the
logarithmic price processes of the @xmath assets in the basket are
independent Brownian motions with drift. Let us assume the volatilities
of the assets to be constants @xmath for @xmath .

We set

  -- -------- --
     @xmath   
  -- -------- --

for @xmath and denote by @xmath for @xmath , @xmath the @xmath
-dimensional Gaussian probability measure of variance @xmath centered at
@xmath . We will assume that – possibly after an appropriate change of
the time scale and the discount rate @xmath (by a linear transformation
from the left) – we have @xmath for all @xmath . @xmath will change
according to its definition.

Then Itô’s Lemma implies that the logarithmic price process in the
@xmath -th coordinate is – with repect to the risk-neutral measure –
just a Brownian motion with drift @xmath and volatility @xmath , thus
the process of logarithmic prices of the assets in the basket is a
Brownian motion with drift @xmath and volatility @xmath . Therefore our
recursion fomula ( 8.1 ) becomes

  -- -------- -------- -------- --
                       @xmath   
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Now, if the points @xmath with respective weights @xmath , @xmath ,
determine cubature formulae for the standard Gaussian measure @xmath ,
we can approximate the previous recursion by the following formula:

  -- -------- -------- -------- -- -------
                       @xmath      
     @xmath   @xmath   @xmath      (8.2)
  -- -------- -------- -------- -- -------

#### 8.3 Exploiting combinatorial aspects of Gaussian cubature

Thanks to the work of Nicolas Victoir (which has later been extended by
Christian Litterer), there are “cubature formulae with few points” [ 32
] for the integration of polynomials with respect to the standard
Gaussian measure up to a certain degree. Although “asymmetric” [ 32 ] ,
their shape is quite regular and uniform. Since the recursion following
the previous recursion formula amounts to the evaluation of payoff
functions at (modified) sums of these cubature points and we therefore
desire recombination of these sums, this will turn out to be a
computationally palpable advantage.

The commutativity of @xmath and the equidistance of the exercise times
already enable us to perform a geometric argument based on the regular
and uniform shape of the cubature points, which results in

###### Theorem 8.1.

Let @xmath for some @xmath . The recursion according to ( 8.2 ), using
the cubature formula for the integration of degree 5 polynomials with
respect to a standard Gaussian measure from Victoir’s example [ 32 ,
5.1.1] , is polynomial in @xmath .

###### Proof.

The cubature points of the cubature formulae referred to in the Theorem
form a finite subset of @xmath . Sums of length @xmath (provided this
fraction is an integer) of the cubature points are therefore always
elements of @xmath (and this set has only @xmath elements), and the
points used in the recusion formula stated above are comprised of a
subset of @xmath . ∎

However, this is not the only recombination that can be accomplished in
the case where @xmath :

###### Remark 8.1.

Let us look at the tree obtained from starting at some point @xmath and
then at each node letting exactly @xmath branches leave (where @xmath in
Victoir’s notation is the set of cubature points he uses in the example
[ 32 , 5.1.1] we are referring to), exactly one branch for each element
of the set @xmath .

If we intend to find and eliminate the branches of the tree that are
computed “wastefully”, it is reasonable to divide the sums (of length
@xmath ) of the cubature points by @xmath and consider them
coordinate-wise modulo @xmath . Then one is looking at elements of the
vector space @xmath . For the sake of simplicity, let @xmath , that is
@xmath in the notation of the previous Theorem 8.1 . The coordinate-wise
projection of the @xmath -multiple of our set of cubature points @xmath
, where

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

(in the notation of [ 32 , 5.1.1] ), into the vector space @xmath now
contains only eight points (instead of @xmath as before).

Thus, using basic linear algebra in a @xmath -dimensional @xmath -vector
space, we are easily able to classify the non-trivial zero
representations from elements of the projected cubature points.

Perceiving @xmath as a @xmath -element subset of @xmath , we see that
@xmath is an invertible @xmath -matrix. Therefore we cannot expect any
recombination from representations of zero by nontrivial linear
combinations of elements of @xmath . Moreover, the fact that @xmath is
invertible, shows that @xmath can only be written trivially as a sum of
elements of @xmath . Hence we have shown that we exploit symmetries
optimally if we use: (i) the commutativity of @xmath ; (ii) the obvious
symmetries due to the construction of the cubature formulae by means of
the action of a reflection group on certain points; (iii) the fact that
addition of @xmath does nothing at all.

### Chapter 9 Numerical results

In this Chapter we shall present some numerical results. We have decided
to choose a @xmath -dimensional example, since (1) most previous
research has stopped short of numerically tackling American options on
baskets with more than @xmath assets, (2) it is the smallest dimension
@xmath in which Victoir’s cubature formulae for the normal Gaussian
measure of dimensions @xmath (where @xmath ) [ 32 , Example 5.1.1] hold.

We shall assume that the basket @xmath as a logarithmic price process
follows the Black-Scholes model for independent assets with discount
rate @xmath and volatilities @xmath , @xmath , that is

  -- -------- --
     @xmath   
  -- -------- --

(where @xmath is the @xmath -dimenional Wiener process). Given a payoff
function, a strike price, a maturity @xmath , and logarithmic start
price vector @xmath we shall vary the exercise mesh size @xmath (say
@xmath ) and compute approximate Bermudan prices @xmath . Then we will
extrapolate the function @xmath to @xmath by assuming assume that @xmath
is a polynomial of degree @xmath in @xmath for a given @xmath that
finally shall be varied as well.

Unfortunately, it is difficult to find data on American option prices
for dimension @xmath . However, one can of course use our algorithm
sub-optimally for @xmath through letting the payoff function only depend
on the first five coordinates. Then a comparison with the numerical
value computed by the 50S algorithm (as stated in Rogers [ 27 ] ) sadly
yields a 3.64 % difference after 9.87 seconds of computations on a 1.4
GHz Personal Computer (whereas 50S needed 14 seconds on a 600 MHz PC).

More extensive numerical experiments (on computers of better
performance) may find, however, that a cubature-based algorithm is
superior to a Monte-Carlo routine when higher dimensions than @xmath are
considered. On a different note, recall that in practice for the vast
majoriy of derivative options, pricing algorithms are only used as part
of hedging programs – and with hedging, the accuracy of the prices
computed is of lesser importance than the processor time the algorithms
actually requires.

We conclude this Chapter by stating some Bermudan and American option
prices computed through our cubature-based algorithm.

For a min-put on a basket of seven independent assets with discount rate
@xmath , maturity at time @xmath and strike price @xmath , one will get
the following numerical results. (Here, extrapolation I is the
extrapolation of @xmath from @xmath to @xmath with scaling exponent
@xmath , and extrapolation II is the corresponding extrapolation with
scaling exponent @xmath . The amount of time elapsed during each
computation is given in seconds.)

If all volatilities @xmath are equal to @xmath , then

+-------------+-------------+-------------+-------------+-------------+
| Start       |   -         | Ex          | Ex          | Time        |
| prices      | ----------- | trapolation | trapolation |             |
|             |   Bermudan  | I           | II          |             |
|             |             |             |             |             |
|             |  ( @xmath ) |             |             |             |
|             |   -         |             |             |             |
|             | ----------- |             |             |             |
+=============+=============+=============+=============+=============+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
|             | @xmath      | @xmath      | @xmath      | @xmath      |
|  ---------- |             |             |             |             |
|   @xmath ,  |             |             |             |             |
|   @xmath    |             |             |             |             |
|             |             |             |             |             |
|  ---------- |             |             |             |             |
+-------------+-------------+-------------+-------------+-------------+

In case the volatilities @xmath are all equal to @xmath , then

+-------------+-------------+-------------+-------------+-------------+
| Start       |   -         | Ex          | Ex          | Time        |
| prices      | ----------- | trapolation | trapolation |             |
|             |   Bermudan  | I           | II          |             |
|             |             |             |             |             |
|             |  ( @xmath ) |             |             |             |
|             |   -         |             |             |             |
|             | ----------- |             |             |             |
+=============+=============+=============+=============+=============+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
|             | @xmath      | @xmath      | @xmath      | @xmath      |
|  ---------- |             |             |             |             |
|   @xmath ,  |             |             |             |             |
|   @xmath    |             |             |             |             |
|             |             |             |             |             |
|  ---------- |             |             |             |             |
+-------------+-------------+-------------+-------------+-------------+

Finally, if @xmath , @xmath , @xmath , @xmath , @xmath , @xmath , @xmath
, one has the following figures:

+-------------+-------------+-------------+-------------+-------------+
| Start       |   -         | Ex          | Ex          | Time        |
| prices      | ----------- | trapolation | trapolation |             |
|             |   Bermudan  | I           | II          |             |
|             |             |             |             |             |
|             |  ( @xmath ) |             |             |             |
|             |   -         |             |             |             |
|             | ----------- |             |             |             |
+=============+=============+=============+=============+=============+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
|             | @xmath      | @xmath      | @xmath      | @xmath      |
|  ---------- |             |             |             |             |
|   @xmath ,  |             |             |             |             |
|   @xmath    |             |             |             |             |
|             |             |             |             |             |
|  ---------- |             |             |             |             |
+-------------+-------------+-------------+-------------+-------------+

These data suggest that the optimal scaling exponent @xmath for the
extrapolation from Bermudan to American min-put prices will have to
depend on both the volatility vector @xmath and the vector of
(logarithmic) start prices @xmath .

Our second example concerns itself with the pricing of Bermudan and
American put options on the arithmetic average of a basket of
independent assets.

If all volatilities @xmath are equal to @xmath , then

+-------------+-------------+-------------+-------------+-------------+
| Start       |   -         | Ex          | Ex          | Time        |
| prices      | ----------- | trapolation | trapolation |             |
|             |   Bermudan  | I           | II          |             |
|             |             |             |             |             |
|             |  ( @xmath ) |             |             |             |
|             |   -         |             |             |             |
|             | ----------- |             |             |             |
+=============+=============+=============+=============+=============+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
|             | @xmath      | @xmath      | @xmath      | @xmath      |
|  ---------- |             |             |             |             |
|   @xmath ,  |             |             |             |             |
|   @xmath    |             |             |             |             |
|             |             |             |             |             |
|  ---------- |             |             |             |             |
+-------------+-------------+-------------+-------------+-------------+

In case the volatilities @xmath are all equal to @xmath , then

+-------------+-------------+-------------+-------------+-------------+
| Start       |   -         | Ex          | Ex          | Time        |
| prices      | ----------- | trapolation | trapolation |             |
|             |   Bermudan  | I           | II          |             |
|             |             |             |             |             |
|             |  ( @xmath ) |             |             |             |
|             |   -         |             |             |             |
|             | ----------- |             |             |             |
+=============+=============+=============+=============+=============+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
|             | @xmath      | @xmath      | @xmath      | @xmath      |
|  ---------- |             |             |             |             |
|   @xmath ,  |             |             |             |             |
|   @xmath    |             |             |             |             |
|             |             |             |             |             |
|  ---------- |             |             |             |             |
+-------------+-------------+-------------+-------------+-------------+

And if @xmath , @xmath , @xmath , @xmath , @xmath , @xmath , @xmath , we
obtain the following results:

+-------------+-------------+-------------+-------------+-------------+
| Start       |   -         | Ex          | Ex          | Time        |
| prices      | ----------- | trapolation | trapolation |             |
|             |   Bermudan  | I           | II          |             |
|             |             |             |             |             |
|             |  ( @xmath ) |             |             |             |
|             |   -         |             |             |             |
|             | ----------- |             |             |             |
+=============+=============+=============+=============+=============+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
| @xmath      | @xmath      | @xmath      | @xmath      | @xmath      |
+-------------+-------------+-------------+-------------+-------------+
|             | @xmath      | @xmath      | @xmath      | @xmath      |
|  ---------- |             |             |             |             |
|   @xmath ,  |             |             |             |             |
|   @xmath    |             |             |             |             |
|             |             |             |             |             |
|  ---------- |             |             |             |             |
+-------------+-------------+-------------+-------------+-------------+

The first line of each of these sets of figures of course simply means
that immediate exercise is optimal if the start price of each asset is
at @xmath or below (and, in case @xmath for all @xmath , even if each
asset start price is at @xmath ).

Again it is apparent from these numerical data that the scaling exponent
needed for the extrapolation from Bermudan to American prices
put-on-the-average option prices has to be varied with the vector of
asset start prices and possibly the volatilities of the underlying
assets (otherwise the American price computed by extrapolation would be
at times very significantly below the approximate price of a Bermudan
option on the same basket and with the same payoff function).

## Part V High-dimensional approximate @xmath-hedging

### Chapter 10 Hedging options on multiple assets – a suggestion for
further research

#### 10.1 Theoretical suggestions

Up to this point, the subject of our investigation has been the pricing
of high-dmensional American and Bermudan options. In practice, there is
at least as much (if not even significantly more) interest in the heding
of such options as in finding out their price – the latter task often
being simply left to the markets. To this extent, any pricing algorithm
gains much of its practical interest merely from being employable as a
subroutine of a hedging algorithm.

The canonical way of hedging – that is replicating a portfolio, ideally
without risk – that does not need to introduce utility functions for
portfolios which sometimes may not be that easy to justify themselves is
@xmath -hedging. Unfortunately, however, there is no straightforward
multi-dimensional generalisation of @xmath -hedging in the discrete
binomial model (in the sense of eg Hull [ 13 ] or Wilmott, Howison,
Dewynne [ 33 ] ). For, if the price processes of all of the assets in a
portfolio of @xmath different types of shares each follow the binomial
model, then at each time step @xmath where the vector of current asset
prices equals @xmath there are @xmath possible states of the market that
may be encountered at the next time step (given by a set of the form

  -- -------- --
     @xmath   
  -- -------- --

as each asset @xmath is assumed to move either by a factor @xmath or by
a factor @xmath where without loss of generality one may assume @xmath
for all @xmath ) compared with only @xmath elements in the portfolio
(including the bond). Assuming translation-invariance of the Markov
chain, we introduce the notation

  -- -------- --
     @xmath   
  -- -------- --

Then the volatility @xmath of the @xmath -th asset is defined to be the
square root of the variance of the one-dimensional random walk with
steps @xmath and transition probabilities

  -- -------- --
     @xmath   
  -- -------- --

and @xmath respectively, on the set @xmath . This is to say,

  -- -------- --
     @xmath   
  -- -------- --

We shall define for each such vector @xmath in the set

  -- -------- --
     @xmath   
  -- -------- --

the overall absolute correlation by

  -- -------- --
     @xmath   
  -- -------- --

One can now think of various approximate @xmath -hedging algorithms –
previsible transaction policies that whilst being unable to eliminate
the @xmath altogether, reduce it significantly. At least three classes
of such algorithms come to one’s mind:

1.  At each time step @xmath -hedging of proper subsets of the
    portfolio, possibly changing the subset with time.

2.  Removing @xmath of the elements of @xmath (thereby making the market
    model a “ @xmath -nomial” one) via a correlation analysis (cf
    Section 10.2 ).

3.  The use of cubature formulae to achieve this elimination.

A natural method of comparing these hedging algorithms will be to look
at the @xmath -norms of the resulting sequences of @xmath ’s. The
algortihms of 1 . and 3 . are straightforward modifications of standard
@xmath -hedging algorithms for @xmath -component portfolios in market
models that only allow for @xmath possible states of the market at the
respective subsequent time step.

We will therefore dedicate the rest of this short Chapter to the
algorithm suggested in 2 .

#### 10.2 A @xmath-hedging algorithm based on a correlation analysis

Following suggestion 2 . of the preceding paragraph, we will now propose
an algorithm that constructs a new set @xmath with cardinality @xmath
from @xmath (in the notation of the previous Section). It will then be
possible to apply @xmath -hedging to the Markov chain market model that
is given by

  -- -------- -------- -------- --
     @xmath                     
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

In order to describe the algorithm that produces @xmath from @xmath ,
two cases according to the size of @xmath have to be distinguished.

Case I : @xmath .

In this situation,

  -- -------- --
     @xmath   
  -- -------- --

and one will determine the set of those @xmath vectors in the set @xmath
which will be removed, that is the elements of @xmath (as opposed to
finding the elements of @xmath themselves).

The set @xmath will comprise exactly the @xmath elements @xmath of
@xmath with the smallest overall absolute correlation @xmath .

Note that a priori there can be @xmath such that @xmath . Therefore, in
order to get the procedure of constructing @xmath from @xmath
well-defined, it is necessary to first define a well-ordering @xmath on
@xmath and to define a linear order @xmath on @xmath by

  -- -------- --
     @xmath   
  -- -------- --

Then, the product @xmath will be a well-ordering and we will define
@xmath to be the set of the @xmath -smallest @xmath elements of @xmath .

Case II : @xmath .

In that case

  -- -------- --
     @xmath   
  -- -------- --

For this reason it is faster to single out the elements of @xmath
directly, rather than determining the elements of its complement @xmath
first.

Using the well-ordering defined above, one will thus choose the @xmath
-greatest @xmath elements of the finite set @xmath (the elements of
@xmath with the largest overall absolute correlation @xmath , that is).

## Part VI Appendix

### Appendix A Re-formulation of the perpetual Bermudan pricing problem
in @xmath and @xmath

#### a.1 Non-applicability of the @xmath Spectral Theorem

Consider a @xmath -dimensional Lévy basket @xmath with associated family
of risk-neutral probability measures @xmath and discount rate @xmath .

Fixing @xmath and defining

  -- -------- --
     @xmath   
  -- -------- --

we can rewrite the result of Lemma 2.3 as follows:

  -- -------- -- -------
     @xmath      (A.1)
  -- -------- -- -------

where we assume that @xmath has a square-integrable extension from
@xmath to the whole of @xmath ; given this assumption, the @xmath of the
previous identity can be any such extension.

We will suppress the superscript of @xmath for the rest of this
paragraph.

Also, without loss of generality, we will assume in this Chapter that
the components of the basket @xmath when following the Black-Scholes
model all have volatility @xmath .

###### Lemma A.1.

Let @xmath be a Lévy basket with associated family of risk-neutral
probability measures @xmath and discount rate @xmath . Then @xmath and
@xmath are invertible. Furthermore, the @xmath norm of @xmath is bounded
by @xmath , if @xmath (thus @xmath ) where @xmath is a standard Brownian
motion. Moreover, @xmath is a contraction if @xmath .

###### Proof.

Suppose @xmath and @xmath is bounded. Then @xmath and we may choose a
set @xmath of positive Lebesgue measure such that @xmath for all @xmath
(this is possible because @xmath and therefore @xmath ), we deduce

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

which means that

  -- -------- --
     @xmath   
  -- -------- --

hence @xmath (for @xmath has positive Lebesgue measure). So

  -- -- --
        
  -- -- --

and we are done for the invertibility of @xmath . Similarly, one can
prove the invertibility of @xmath . Finally, @xmath is seen to be a
contraction by application of the Fourier transform: The Fourier
transform is an @xmath isometry (by Plancherel’s Theorem), thus

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

Now, the factor in front of @xmath in the last line can be bounded by
@xmath , and it is strictly less than one for @xmath . Using
Plancherel’s Theorem again, this yields the result. ∎

Now, this is sufficient to apply a Wiener-Hopf factorisation (for a
general treatment of this kind of factorisations, one may consult eg
Speck [ 28 ] , our application uses in particular [ 28 , 1.1, Theorem 1]
) and state

###### Theorem A.1.

Let @xmath and let @xmath be a Lévy basket with associated family of
risk-neutral probability measures @xmath and discount rate @xmath . Then
@xmath , the expected payoff of a perpetual Bermudan option for @xmath
with exercise mesh size @xmath and payoff function @xmath , is – using
the above notation – given by

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a Wiener-Hopf factorisation of @xmath .

We observe

###### Lemma A.2.

The Hilbert space operator @xmath is normal.

###### Proof.

We define @xmath (where @xmath is the @xmath -dimensional Lebesgue
measure) and via the Fubini Theorem one has for every @xmath

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

that is

  -- -------- --
     @xmath   
  -- -------- --

But since the convolution is associative and commutative, this implies

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

∎

However, it will not be possible to find a basic system of eigenvectors
and eigenvalues for this operator, since

###### Lemma A.3.

The operator @xmath fails to be compact.

###### Proof.

Any normalised basis provides a counterexample for the compactness
assertion. ∎

Therefore, the equation ( A.1 ) cannot easily be applied to compute the
expected option payoff by means of a spectral analysis. Thus, our
examination of the Hilbert space approach in the second part of this
Chapter has led to a negative outcome.

However, one can also conceive of the operators @xmath as operators on
the Banach space @xmath :

#### a.2 The @xmath operator equation: analyticity in the exercise mesh
size

From now on, @xmath will no longer be fixed and we will therefore write
@xmath instead of @xmath .

If we now assume @xmath to be an integrable extension of @xmath to the
complement of @xmath as an element of Quite similarly to A.1 , we can
prove

###### Theorem A.2.

Let @xmath and let @xmath be a Lévy basket with associated family of
risk-neutral probability measures @xmath and discount rate @xmath . Then
@xmath , the expected payoff of a perpetual Bermudan option for @xmath
with exercise mesh size @xmath and payoff function @xmath , is – using
the above notation – given by

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a Wiener-Hopf factorisation of @xmath .

It suffices to observe that @xmath is – due to the @xmath norm estimate
for the convolution of two integrable functions (as the product of the
norms of the convolved functions) – also a bounded operator on @xmath .

We shall now identify @xmath and @xmath .

###### Theorem A.3.

With the notation previously introduced, we define @xmath to be the
semigroup

  -- -------- --
     @xmath   
  -- -------- --

where

  -- -------- --
     @xmath   
  -- -------- --

is the distribution of the logarithmic price vector at time @xmath .
Suppose @xmath and, with the notation from the previous chapters, @xmath
is a (normalised) Brownian motion with (possibly zero) drift (
Black-Scholes model ). Then @xmath is real analytic in @xmath on @xmath
as function with range in the Banach space @xmath .

###### Proof.

It is obvious that @xmath is a semigroup. According to [ 10 , Theorem
1.48] , the set

  -- -------- --
     @xmath   
  -- -------- --

is dense in @xmath . Hence it is possible to approximate every @xmath by
a sequence @xmath in @xmath . Since

  -- -------- --
     @xmath   
  -- -------- --

we obtain @xmath for @xmath uniformly in @xmath on @xmath , where @xmath
is entire for every @xmath and @xmath . Thus, @xmath , and thereby
@xmath , is an analytic function on @xmath taking values in the Banach
space @xmath . Now observe that for arbitrary open @xmath (the symbol “
@xmath ” indicating that @xmath is contained in a compact subset of
@xmath ) the following equations hold:

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (A.2)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

since the sums converge uniformly in @xmath on @xmath , yielding the
analyticity of @xmath as a function whose range lies in the Banach space
@xmath . ∎

###### Lemma A.4.

Let @xmath , @xmath . Then the equation

  -- -- --
        
  -- -- --

holds (where @xmath denotes the transpose of a vector @xmath ). In
particular, if @xmath for some @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

According to Davies [ 10 , Proof of Theorem 2.39] , we have

  -- -------- -- -------
     @xmath      (A.3)
  -- -------- -- -------

where @xmath denotes the infinitesimal generator of the semigroup @xmath
. Now, define @xmath to be the convolution operator semigroup @xmath of
(normalised) Brownian motion with drift @xmath (as before denoting by
@xmath the Lebesgue density of the Gaussian distribution centered around
@xmath of variance @xmath for all @xmath and @xmath ). It is well-known
(cf e g [ 26 , p. 352] ) that the infinitesimal generator of this
semigroup @xmath is

  -- -------- --
     @xmath   
  -- -------- --

By our requirements on @xmath , @xmath on @xmath . Furthermore, @xmath
and @xmath commute:

  -- -------- --
     @xmath   
  -- -------- --

Thus,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

which due to equation ( A.3 ) already suffices for the proof of the
Lemma in the general case. And if @xmath is an eigenfunction of @xmath
for the eigenvalue @xmath , one has @xmath .

∎

###### Theorem A.4.

The Taylor series for the expected payoff of a perpetual Bermudan option
as a function of the exercise mesh with respect to a fixed exercise
region @xmath is for all @xmath :

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
                       @xmath   
  -- -------- -------- -------- --

where, in order to avoid confusion with pointwise exponentiation, @xmath
denotes @xmath for any operator @xmath .

###### Proof.

We know about the real analyticity of @xmath on @xmath and even, thanks
to the previous Lemma, the explicit Taylor series. Thereby we also have
the Taylor series for @xmath . So we can use equation ( A.2 ) and see by
means of a binomial expansion

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
                       @xmath   
  -- -------- -------- -------- --

∎

This Taylor series fails to provide any straightforward possibility for
the computation of @xmath . Instead we state the following immediate
Corollary of equation ( A.2 ):

###### Corollary A.1.

With the notation as in the previous Theorem,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

### Appendix B An algebraic perpetual Bermudan pricing method and its
natural scaling

In this Chapter, we will, in a more algebraically flavoured way, present
an approach that approximates the perpetual Bermudan option price as the
fixed point of some map on the space of polynomials that is defined by
means of not only the max operator, but also interpolation with respect
to a given, fixed, set of interpolation points, as well as convolution
with one and the same Gaussian (not necessarily normalised) measure.
This set of interpolation points could, for example, be a set of
cubature points for the distribution of the time @xmath increment of the
logarithmic price process (if this process is assumed to be Gaussian
with stationary increments).

Let us, for this purpose, adopt Victoir’s notation [ 32 ] and denote the
space of all polynomials of degree @xmath and degree at most @xmath by
@xmath and @xmath , respectively, for all @xmath . We will write
polynomials in the form @xmath and denote by @xmath the associated
polynomial function from @xmath to @xmath .

Now, let @xmath be any positive integer. We introduce the interpolation
map

  -- -------- --
     @xmath   
  -- -------- --

that assigns to each pair @xmath of a real-valued function @xmath and a
vector @xmath of interpolation points the well-defined (see e g Stoer
and Bulirsch [ 29 ] ) polynomial @xmath satisfying

  -- -------- --
     @xmath   
  -- -------- --

Now let @xmath be a positive real number (interpreted to be the discout
rate), @xmath (which is the drift of the logarithmic price process in a
Black-Scholes model with normalised time scale – ensuring the volatility
@xmath to be equal to one), and @xmath a continuous function
(interpreted to be the payoff function of an option defined on the space
@xmath of logarithmic underlying asset prices). Consider a vector-valued
function @xmath such that the range of the function @xmath consists
exclusively of vectors with mutually distinct entries. The elements of
the range of @xmath can in this case serve as sets of interpolation
points (these points also called support abscissas ). Hence using the
notation of previous paragraphs, we may define another map

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath by

  -- -------- --
     @xmath   
  -- -------- --

Note that

  -- -------- --
     @xmath   
  -- -------- --

This can be shown using the linearity of the convolution and the fact
that for all @xmath , the convolution @xmath of the measure @xmath with
the function @xmath is again a polyonmial function of degree @xmath .
Writing down an explicit formula for the map @xmath , we see that this
function is continuous with respect to the Euclidean topology on the
@xmath -dimensional real vector space @xmath .

This ushers in the proof of the following Lemma which is one of the
first observations leading to the fixed point equation mentioned at the
beginning of this Chapter.

###### Lemma B.1.

Consider any @xmath and arbitrary @xmath . @xmath is continuous with
respect to the Euclidean topology on the @xmath -dimensional real vector
space @xmath . Also, if and only if @xmath , that is, @xmath is a fixed
point of @xmath , there will exist a polynomial @xmath such that @xmath
(in the Euclidean toplogy of the @xmath -dimensional vector space @xmath
).

###### Proof.

The continuity of @xmath is a consequence of the continuity of the map
@xmath . For the second part of the Lemma observe that provided the
existence of such a @xmath as in the statement of the Lemma, we can
deduce the fixed point equation of the Lemma’s statement from the
continuity of the map @xmath . For the converse implication, simply take
@xmath . ∎

###### Notational convention B.1.

For any vector @xmath , @xmath will be understood to be the polynomial
@xmath , and for all @xmath , @xmath shall be understood to denote the
polynomial function

  -- -------- --
     @xmath   
  -- -------- --

and .

###### Lemma B.2.

For all @xmath and each @xmath , the function @xmath is a polynomial
function in both @xmath and @xmath . Its degree in @xmath is @xmath ,
its leading coefficient in @xmath being the leading coefficient of
@xmath . Furthermore, the function @xmath is @xmath for all @xmath .
Moreover, if @xmath , then the first two leading terms of @xmath as a
function of @xmath are @xmath and a term of order @xmath , respectively,
if @xmath is odd – and if @xmath is even, the first two leading terms of
@xmath are @xmath and some term of order @xmath , respectively.

###### Proof.

First, let us once again remark that @xmath is, as a convolution
operator, linear and that we therefore may restrict our attention to the
functions @xmath for @xmath . Consider any @xmath . Using the
transformation @xmath we find that

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

where the last line follows from expanding the binomial and using the
identity

  -- -------- --
     @xmath   
  -- -------- --

(an immediate consequence of the “oddness” of the integrand), which
entails that all odd terms in @xmath will be cancelled out (as they have
to be odd terms in @xmath as well). ∎

###### Remark B.1.

The assumption of @xmath will hold in particular for any non-zero
element of a set of cubature points for the measure @xmath derived from
a cubature formula for @xmath .

###### Definition B.1.

For all @xmath , @xmath (the symmetric group of @xmath ), and @xmath ,
we define the @xmath -matrix

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

and the following vector:

  -- -------- --
     @xmath   
  -- -------- --

We also define @xmath to be the solution @xmath of

  -- -------- --
     @xmath   
  -- -------- --

provided there exists a unique solution to this equation.

From the matrix formulation of the interpolation problem (again cf Stoer
and Bulirsch [ 29 ] ), the following Lemma is immediate:

###### Lemma B.3.

The polynomial @xmath is a fixed point of @xmath if and only if there is
an @xmath and a @xmath (the symmetric group of @xmath ) such that

  -- -------- --
     @xmath   
  -- -------- --

and, in addition,

  -- -- -------- -------- --
                 @xmath   
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

as well as

  -- -- -------- -------- --
                 @xmath   
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

###### Lemma B.4.

Suppose there is an @xmath such that for all @xmath , @xmath is a
non-constant polynomial in @xmath , except for possibly one @xmath where
@xmath for all @xmath . Then for all sufficiently small @xmath , @xmath
. The upper bound in @xmath on all those @xmath that satisfy the
previous inequality @xmath for all @xmath and @xmath shall be denoted by
@xmath .

###### Remark B.2.

According to Remark B.1 , the assumption of @xmath being non-constannt
polynomial in @xmath for all @xmath (apart from possibly one zero
coordinate) for some @xmath holds in particular for any set of cubature
points for the measures @xmath that is derived from a cubature formula
for the normalised Gaussian measure (in that case @xmath ). Similar
assertions hold if one replaces @xmath by @xmath where @xmath is the
convolution semigroup associated to some other symmetric stable process.

###### Proof sketch for Lemma b.4.

The function @xmath is, by our assumptions on the functions @xmath on
the one hand polynomial in @xmath , as one can see exactly as in the
proof of Lemma B.2 . On the other hand, one can show, using the
polyonmiality in @xmath and the assumpion that all the entries of @xmath
are mutually distinct for all @xmath , that the function @xmath is
non-constant. Hence, @xmath is a non-constant analytic function in
@xmath , and note that @xmath is a bijection on the unit interval @xmath
. Therefore, @xmath cannot be constantly zero on the open unit interval
@xmath , but it can also only have finitely many critical points on that
interval. Hence there must be an @xmath such that either @xmath for all
@xmath or @xmath for all @xmath . ∎

On the other hand, we have the following

###### Lemma B.5.

Suppose the real-valued function @xmath is a non-constant polynomial in
@xmath for all @xmath for some @xmath , except for possibly one @xmath
where @xmath for all @xmath . Furthermore, take @xmath to be analytic.
Then for all @xmath and all @xmath there exists a vector @xmath of
relations such that for all sufficiently small @xmath ,

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

The upper bound in @xmath on all those @xmath such that for all @xmath
the relations in the previous line hold for all @xmath and @xmath shall
be denoted by @xmath . (Here @xmath is the strictly positive constant of
Lemma B.4 .) Thus, @xmath .

###### Proof.

We have already defined @xmath to be the solution @xmath of

  -- -------- --
     @xmath   
  -- -------- --

From Cramer’s rule, Lemma B.2 , and our assumptions on @xmath
(coordinatewise polynomial in @xmath ) as well as @xmath (analyticity),
we derive that @xmath is analytic in @xmath and therefore has only
finitely many critical points on @xmath for all @xmath and arbitrary
choice of @xmath . Thus, when approaching zero, these functions must
eventually stay on either side of nought. Put more formally, there must
be for all @xmath and @xmath a vector @xmath such that for all
sufficiently small @xmath , and for all @xmath ,

  -- -- --
        
  -- -- --

∎

###### Corollary B.1.

Let the assumptions of the previous Lemma hold. If there is a fixed
point of @xmath for an @xmath (the strictly positive constant of Lemma
B.5 ), all @xmath with positive @xmath must have fixed points as well.
There exist a permutation @xmath as well as a natural number @xmath such
that the coefficient vectors @xmath to all these fixed points @xmath are
solutions @xmath to the linear equation

  -- -------- --
     @xmath   
  -- -------- --

and satisfy

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

as well as

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

###### Proof.

By virtue of Lemma B.3 , a fixed point is a polynomial @xmath whose
coordinate vector @xmath solves the linear equation

  -- -------- --
     @xmath   
  -- -------- --

and satisfies, moreover, inequalities of the form

  -- -------- --
     @xmath   
  -- -------- --

for some vector of relations @xmath . Now apply the previous Lemma B.5 .

∎

###### Theorem B.1.

Suppose there is a vector @xmath such that @xmath for all @xmath (where
the mutual distinctness of the entries of @xmath for all @xmath entails
that @xmath are mutually distinct as well), and assume furthermore that
@xmath is analytic and satisfies @xmath . Suppose, moreover, that there
exists a fixed point of @xmath for some @xmath (where @xmath is the
strictly positive constant from Lemma B.5 ). Then by Corollary B.1 the
maps @xmath do have a fixed point for every @xmath , and let @xmath be
the natural number @xmath and the permutation whose existence is stated
in Corollary B.1 , respectively. Also write @xmath for all @xmath . Then
there is a polynomial @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

componentwise.

###### Remark B.3.

Such a @xmath exists in particular whenever @xmath is a set of cubature
points for the measure @xmath derived from a cubature formula for @xmath
.

###### Proof of Theorem b.1.

Observe that by Lemma B.2 ,

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

Next, we will use Cramer’s rule to determine if there is a limit for the
solution of

  -- -------- --
     @xmath   
  -- -------- --

as @xmath tends to zero and if so, what the convergence rate will be.
For this purpose, we have to consider the determinant of the matrix
@xmath which is defined to be the matrix coinciding with @xmath in the
columns @xmath and having the vector

  -- -------- --
     @xmath   
  -- -------- --

as its @xmath -th column. Then, since @xmath is right-differentiable in
@xmath and by assumption @xmath ,

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

From this, we can conclude that for all @xmath there is a constant
@xmath such that

  -- -------- --
     @xmath   
  -- -------- --

∎

###### Example B.1 (@xmath, the quadratic case, when @xmath).

We have

  -- -------- -- -------- --
     @xmath      @xmath   
                 @xmath   
                 @xmath   
  -- -------- -- -------- --

implying

  -- -------- -- -------- --
     @xmath      @xmath   
                 @xmath   
                 @xmath   
  -- -------- -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

Suppose the parameters from Corollary B.1 are in our example @xmath and
@xmath . Furthermore, in our case, for all @xmath

  -- -------- --
     @xmath   
  -- -------- --

Therefore for all @xmath ,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
                       @xmath   
                       @xmath   
                       @xmath   
              @xmath   @xmath   
                       @xmath   
  -- -------- -------- -------- --

Note that

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the matrix of the interpolation problem with support
abscissas @xmath . Due to the unique solvability of the interpolation
problem (see again e g Stoer and Bulirsch [ 29 ] ), this determinant
@xmath never vanishes unless the support abscissas @xmath fail to be
mutually distinct.

###### Remark B.4.

Similarly one can prove that the function @xmath is differentiable in
@xmath if one assumes the support abscissas to be polynomial in @xmath
rather than @xmath .