# Acknowledgements

I would especially like to thank my supervisor Jesse Ratzkin for all the
mathematical advice, optimism and encouragement, and for his patience
during the generous gestation of this thesis. Thanks to Rob Kusner for
his interest in this project and the discussions about it. This thesis
has benefitted from his remarks and those of the other examiners,
Karsten Grosse-Brauckmann and Laurent Hauswirth.

I gratefully acknowledge the financial support of an Innovation
Scholarship from the National Research Foundation.

###### Contents

-    1 Introduction
    -    1.1 Minimal and constant mean curvature surfaces in @xmath
    -    1.2 Minimal and cmc @xmath surfaces in @xmath
    -    1.3 Main results and outline of the thesis
-    2 Constant mean curvature @xmath horizontal catenoids in @xmath
    -    2.1 Models of @xmath
    -    2.2 The horizontal catenoids
        -    2.2.1 Convergence to horocylinders in the small necksize
            limit
        -    2.2.2 Truncating the horizontal catenoids
-    3 Expansion of the Mean Curvature Operator
    -    3.1 Fermi coordinates
    -    3.2 Estimates for error terms
    -    3.3 Proof of Proposition 3.1
-    4 Linear Analysis of the Jacobi Operator
    -    4.1 Weights, indicial roots and Fredholm index: an example
    -    4.2 Perturbation of spectra and indicial roots
    -    4.3 Mapping properties on weighted Hölder spaces
    -    4.4 Geometric Jacobi fields
-    5 Construction of cmc @xmath ends asymptotic to horizontal
    catenoids
-    6 A proposed gluing construction
    -    6.1 Outline and obstacles
    -    6.2 Existence of cmc @xmath horizontal graphs close to
        horocylinders.

## 1 Introduction

We begin this thesis with a brief discussion of the field of minimal and
constant mean curvature surfaces in the classical setting of @xmath .
One of the central topics in the field has been the construction and
classification of complete, embedded, non-compact examples of such
surfaces. We focus on the rather general gluing techniques that have
been brought to bear on the construction problem. A central component of
these is the analytic construction of spaces of ends, by solving
boundary value problems for the non-linear mean curvature equation. The
main result of our thesis is a result of this type, in the context of
constant mean curvature @xmath surfaces in @xmath . In section 1.2 we
discuss some of the recent developments in the theory of these surfaces,
before stating our main result, giving an overview of the thesis, and
mentioning some future work.

### 1.1 Minimal and constant mean curvature surfaces in @xmath

#### Mean curvature and least area

Minimal and constant mean curvature surfaces are solutions to the
problem of finding surfaces of least area. The classical Plateau problem
asks one to find a surface having the smallest possible area amongst all
surfaces with boundary a prescribed closed curve. A minimiser for this
problem has vanishing mean curvature, and in general one calls a surface
with mean curvature zero at all points a minimal surface. On the other
hand, surfaces that minimise area subject to an enclosed volume
constraint must have constant, non-zero mean curvature. These minimal
and constant mean curvature (cmc) surfaces represent idealised models
for soap films and soap bubbles, respectively.

Minimal and cmc surfaces are in many ways distinguished objects. One
indication of this is the fact that, locally, they are solutions to a
quasilinear, elliptic, partial differential equation (pde). More
precisely, if one parametrises such a surface locally as a graph over
its tangent plane, the graphing function @xmath satisfies the pde

  -- -------- -- -----
     @xmath      (1)
  -- -------- -- -----

where @xmath is the mean curvature of the surface. Such a pde enjoys a
maximum principle, with the following striking geometric consequence.
Suppose two surfaces having the same constant mean curvature meet
tangentially at a point in such a way that their unit normals coincide,
and, locally, one surface lies entirely on one side of the other. Then
the surfaces coincide in a neighbourhood of that point. In fact, because
solutions to ( 1 ) are real-analytic, the surfaces are identical if
they’re connected.

#### Complete, embedded, non-compact examples

As a consequence of the maximum principle, it’s not hard to see that
there cannot exist closed (i.e. compact boundaryless) minimal surfaces
in @xmath . Indeed supposing such a surface existed, one could take a
plane disjoint from the surface and parallel translate it until it makes
a first, tangential point of contact with the minimal surface. Since the
plane has vanishing mean curvature, the argument of the previous
paragraph implies that the plane and our putative compact surface are
identical, a contradiction.On the other hand, the solution to Plateau’s
problem by Douglas and Rado guarantees that there is an abundance of
compact minimal surfaces with boundary in @xmath .

The minimal surfaces we’ll be most interested in are the complete,
embedded, non-compact surfaces which have finite topology, in the sense
that they are homeomorphic to a compact surface with finitely many
punctures. The central examples, apart from flat planes, are the
catenoid and helicoid. The catenoid is a surface of revolution,
homeomorphic to a sphere with two punctures. The helicoid is ruled by
lines, and homeomorphic to a once-punctured sphere. The helicoid and
catenoid were discovered in the @xmath ’s, and for a long time it was
not known if there exist complete, embedded minimal surfaces of finite
topology and positive genus. Only in the @xmath ’s did Costa discover
such a surface, with genus one and three ends. This and subsequent
examples with higher genus have been constructed using the
Enneper-Weierstrass representation. This representation produces minimal
surfaces from complex-analytic data, but is peculiar to mean curvature
zero, and other methods are required for other cmc surfaces.

As in the case of minimal surfaces, there are rather limited
possibilities for closed, embedded cmc surfaces in @xmath . In fact the
only possibilities are round spheres, as was shown by Alexandrov using
his ingenious method of moving planes. The argument used above to show
that there are no closed minimal surfaces in @xmath is a moving planes
argument, and illustrates some of the general features, particularly the
use of a maximum principle for an elliptic pde. As far as complete
examples are concerned, the most important embedded cmc surfaces are the
Delaunay unduloids. These are a one-parameter family of rotationally
invariant, periodic surfaces that interpolate between a cylinder and a
singular limit of tangentially intersecting spheres whose centers lie
along a line. The most powerful method for producing new examples of
embedded cmc surfaces has so far proved to be analytic gluing
techniques, which were pioneered in this setting by Kapouleas [ 15 ] , [
16 ] . We turn to these now.

#### Gluing and spaces of ends

The Delauney unduloids and catenoids play much deeper roles in the
theory of cmc and minimal surfaces than simply that of examples. This is
because they form the asymptotic models for embedded annular ends of
their respective types of surfaces. Indeed Schoen [ 32 ] proved that
every embedded, minimal end is asymptotic to a catenoid or a plane, and
it was later shown that every embedded annular cmc end is asymptotic to
the end of a Delaunay surface [ 17 ] . Therefore, one knows that any
complete embedded minimal or cmc surface one may construct will have
ends that are perturbations of these asymptotic models. This suggests a
procedure for building new examples by attaching perturbations of these
model ends to central building blocks.

One variant of this gluing procedure is the method of Cauchy data
matching developed by Mazzeo, Pacard, Pollack and others. In [ 19 ] the
authors construct new examples of complete, embedded cmc surfaces by
attaching Delaunay ends to certain truncated minimal surfaces called
@xmath -noids. Since this nicely illustrates the method we describe it
in some detail.

To begin with we’d like to give some motivation for why such a
construction may be possible. The crucial ingredient is the
one-parameter family of minimal catenoids: mean curvature scales like
@xmath under a homothety by @xmath , thus all homotheties of the
catenoid @xmath are also minimal surfaces. The parameter @xmath is the
necksize of the corresponding catenoid @xmath . The importance of this
family is that it provides asymptotic models both for (non-planar) ends
of @xmath -noids, and for neck regions of Delaunay unduloids. The
compact, central building blocks in [ 19 ] are truncations of @xmath
-noids with asymptotically catenoidal ends. To each end one associates a
necksize parameter @xmath which is that of the catenoid to which that
end is asymptotic. Similarly any Delaunay surface has a necksize
parameter @xmath , and as @xmath , neighbourhoods of the neck regions
are asymptotic to the catenoids @xmath . Thus if one shrinks the @xmath
-noid sufficiently, each of its ends may be closely matched to a neck
region of the unduloid that shares the same catenoidal model, or in
other words, the same necksize parameter.

The method of Cauchy data matching consists of producing families of cmc
@xmath surfaces close to each of the building blocks in the
construction. This is done by solving the Dirichlet problem for the mean
curvature equation for normal graphs off these pieces. The family is
then parametrised by the boundary data one can prescribe for this
problem. Since one is only interested in surfaces close to the given
models, the solution of the non-linear equation is perturbative, after
inverting the linearisation of the mean curvature equation. The next
step is then to show that it is possible to find a member of each family
such that, at each interface, the boundaries match to zeroth and first
order. The resulting surface is then a continuously differentiable weak
solution to the cmc @xmath equation, and hence by elliptic regularity, a
smooth cmc @xmath surface.

### 1.2 Minimal and cmc @xmath surfaces in @xmath

Whilst many deep open problems remain in the study of cmc surfaces in
@xmath , the field has also expanded to other ambient three-manifolds,
in particular the eight homogeneous geometries of Thurston. To put these
in context, recall that in two dimensions, the constant curvature
geometries @xmath and @xmath play something of a universal role. Indeed,
every compact, orientable two-manifold is homeomorphic to a quotient of
one of these spaces by a discrete group of isometries. In other words,
each of these topological spaces admits a canonical geometry . The
famous Geometrisation Conjecture of Thurston asserts that something
analogous is true in three dimensions; of course, this conjecture has
now been proven through the work of Hamilton and Perelman. Loosely
speaking, the Geometrisation Theorem asserts that every compact,
orientable three-manifold can be topologically decomposed in such a way
that each piece admits a canonical geometry. More precisely, each piece
is homeomorphic to the quotient of a ’model’ geometry by a discrete
group of isometries. Unlike the two-dimensional case where the models
have constant curvature, there are eight model geometries, which
constitute all the simply-connected, homogeneous three-manifolds. They
are

  -- -- --
        
  -- -- --

The constant curvature spaces @xmath and @xmath possess the maximum
possible symmetry, their isometry groups are six-dimensional. The
isometry groups of the product spaces @xmath and @xmath are
four-dimensional, as are those of the universal cover @xmath of @xmath ,
and the Heisenberg group @xmath . The least symmetric of the eight model
geometries is the Lie group @xmath , which has a three-dimensional
isometry group. See [ 29 ] for further details.

Before we focus on @xmath we would like to mention two highlights of the
theory of cmc surfaces in the Thurston geometries, which extend results
on cmc surfaces in space forms to the homogenous manifolds with
four-dimensional isometry group. One of the first such results was the
discovery by Abresch and Rosenberg [ 1 ] of a holomorphic quadratic
differential on cmc surfaces in @xmath and @xmath , which generalises
the classical Hopf differential for cmc surfaces in @xmath (and the
other space forms). The most famous application of Hopf’s differential
is the classification of immersed cmc spheres as round spheres. Using
their holomorphic quadratic differential, [ 1 ] classified the immersed
cmc spheres in @xmath and @xmath : they are all embedded and
rotationally invariant. For a survey of other global results in these
spaces, including analogs of Alexandrov’s classification of compact,
embedded cmc surfaces, and Bernstein’s classification of entire minimal
graphs, see [ 8 ] .

Another important result, due to Daniel [ 4 ] , is the solution of the
isometric embedding problem for surfaces in three-dimensional manifolds
with four-dimensional isometry group. In the space forms the Gauss and
Codazzi equations are necessary and sufficient conditions for a surface
to be locally isometrically embedded with prescribed shape operator. Due
to the reduced symmetry in @xmath and @xmath , the Gauss and Codazzi
equations must be augmented by two further equations relating to the
structure of these spaces as fibrations over a two-dimensional space
form. Using these integrability conditions, Daniel generalised the
Lawson correspondence between cmc surfaces in the space forms to this
setting. For example, under Daniel’s correspondence every
simply-connected, immersed, minimal surface in @xmath is isometric to a
simply-connected, immersed, cmc @xmath surface in @xmath , and a
conformal parametrisation of one of these gives rise to a conformal
parametrisation of the other.

#### Minimal surfaces in @xmath

One of the pioneering works on cmc surfaces in @xmath was that of Nelli
and Rosenberg [ 23 ] , in which they produced many examples of complete
minimal surfaces. These included a family of rotationally invariant
annuli, or catenoids, and a family of screw motion invariant surfaces
generated by a horizontal geodesic, that is, helicoids. They proved
that, given any @xmath vertical graph in the asymptotic boundary @xmath
, there is a unique, entire minimal graph asymptotic to the given curve.
They also proved a Jenkins-Serrin theorem, showing that over suitable
domains there exist minimal graphs with infinite boundary data on some
portions of the boundary, and arbitrary continuous boundary data on
others. Lastly, in conjunction with the work of Collin and Rosenberg [ 3
] , many Scherk-type surfaces have been constructed. These are minimal
graphs over domains bounded by finite polygons or ideal polygons with
@xmath sides, which have asymptotic boundary values alternating between
@xmath and @xmath on the edges of the polygon.

In addition to the helicoids just mentioned, Sa Earp and Toubiana [ 30 ]
constructed surfaces invariant under a screw motion but whose generating
curve is not a horizontal geodesic. In [ 31 ] these authors also
constructed disc-type minimal surfaces invariant under hyperbolic
translations.

The last examples of minimal surfaces we wish to mention are the
positive genus surfaces constructed using gluing techniques by Martin,
Mazzeo and Rodriguez [ 18 ] . The key building blocks in their
construction are the horizontal minimal catenoids constructed
independently by Pyo [ 28 ] and Morabito and Rodriguez [ 22 ] . These
embedded minimal annuli have ends asymptotic to vertical geodesic
planes, and finite total curvature. In fact, Hauswirth, Nelli, Sa Earp
and Toubiana [ 11 ] proved that these are the unique complete, immersed,
finite total curvature, minimal surfaces having two ends asymptotic to
vertical planes; a result analogous to Schoen’s [ 32 ] characterisation
of minimal catenoids in @xmath .

The gluing procedure of [ 18 ] relies on the geometry of the ends of the
horizontal catenoids through the following key observation. If two
horizontal catenoids each have an end asymptotic to a common vertical
plane, and if the distance between their necks is sufficiently large,
then these two ends will be very close to each other, and an
approximately minimal surface can be constructed by gluing the ends
together with a small cutoff function.

Now consider a collection @xmath of disjoint geodesics in @xmath , and a
collection @xmath of geodesic arcs connecting pairs of geodesics in
@xmath in such a way that each arc in @xmath realises the distance
between the pair it connects. Replace each arc in @xmath with a
horizontal catenoid that is asymptotic to the vertical planes over the
geodesics in @xmath that it connects. Using cut-off functions to glue
these catenoids together, they create an approximately minimal surface.
If the catenoids’ necks are all sufficiently far away from each other,
then the approximately minimal surface can be perturbed to a minimal
surface. By considering different configurations of geodesics @xmath and
connecting arcs @xmath , the authors construct surfaces of any positive
genus. In order to keep the neck separation sufficiently large, they
require many ends for each handle they attach.

#### Constant mean curvature @xmath surfaces in @xmath.

Apart from minimal surfaces, there is a second distinguished class of
cmc surfaces in @xmath , those with cmc @xmath . The value @xmath is the
critical mean curvature of @xmath , that is, the largest value of the
mean curvature for which there are no closed cmc surfaces. This can be
seen by an argument similar to the one for minimal surfaces in @xmath ,
once one has the appropriate analogue of a plane, namely a horocylinder.
A horocylinder is a vertical cylinder over a horocycle in @xmath . Since
horocycles have constant curvature @xmath , the horocylinders have
constant mean curvature @xmath . Like parallel planes in @xmath ,
horocylinders foliate @xmath . Now, if there was a closed cmc @xmath
surface in @xmath , then there would exist a horocylinder that makes
one-sided, tangential contact with it, with coinciding normal vector.
Once again, the geometric maximum principle implies that the surfaces
coincide, a contradiction. On the other hand, for every @xmath there do
exist closed cmc @xmath surfaces, for example the rotational spheres.

There are many examples of complete cmc @xmath surfaces in @xmath .
These include screw motion invariant surfaces, and both embedded and
immersed rotationally invariant annuli that are symmetric with respect
to reflection in a horizontal plane [ 30 ] . Another important example
is the hyperboloid, which is an entire, rotationally invariant vertical
graph whose height function tends to infinity on the end of the surface.
Nelli and Sa Earp [ 24 ] proved a half-space theorem with respect to the
hyperboloid: the only complete, properly immersed cmc @xmath surfaces
that lie on the mean convex side of a hyperboloid are vertical
translations of the hyperboloid. A different half-space theorem was
proved by Hauswirth, Rosenberg and Spruck [ 12 ] , with respect to
horocylinders. Their result is that a complete, properly immersed cmc
@xmath surface contained in the mean convex side of a horocylinder must
be another horocylinder.

A fair amout is known about vertical graphs of cmc @xmath . Hauswirth,
Rosenberg and Spruck [ 13 ] proved a Jenkins-Serrin theorem for them,
and [ 6 ] constructed examples by solving exterior Dirichlet problems,
obtaining surfaces whose ends are contained between two hyperboloids.
Cartier and Hauswirth [ 2 ] showed that it is possible to pertub the
ends of the hyperboloids to obtain entire graphs whose horizontal
distance from a hyperboloid in a slice @xmath and direction @xmath is
given, in the limit as @xmath , by an essentially arbitrary, small
function of @xmath . Entire examples have been constructed using the
harmonic Gauss map of Fernandez and Mira [ 7 ] and a result of [ 12 ] ,
and the entire examples have been completely classified (see [ 8 ] ).

Moving away from vertical graphs, Plehnert has constructed examples with
dihedral symmetry using a conjugate Plateau construction [ 26 ] , [ 27 ]
. This method proceeds by solving Plateau problems for minimal surfaces
in the Heisenberg group @xmath , then using the sister correspondence of
Daniel [ 4 ] to obtain cmc @xmath surfaces in @xmath , which are
extended by Schwarz reflection. Genus one surfaces with @xmath -ends,
and genus zero @xmath -noids have been constructed this way.

Lastly we mention the cmc @xmath surfaces that are the topic of this
thesis. Using the Gauss map of [ 7 ] , Daniel and Hauswirth constructed
a family of complete, properly embedded cmc @xmath annuli whose axes lie
in a horizontal plane; the horizontal catenoids [ 5 ] . The family is
parametrised by their necksizes. We discuss their geometry and the small
necksize limit in detail in section 2.2 .

### 1.3 Main results and outline of the thesis

The main result of this thesis is the construction of families of cmc
@xmath annular ends asymptotic to the ends of horizontal catenoids. Let
@xmath denote the horizontal catenoid of necksize @xmath , and @xmath
the truncation of the end (to be defined precisely later). Stated
somewhat imprecisely, our result is

###### Theorem 1.1.

There exists @xmath such that for all @xmath , there is a projection
@xmath on @xmath with two-dimensional kernel, and the following
property. For each @xmath there is a unique decaying solution to the
boundary value problem for the mean curvature operator

  -- -------- --
     @xmath   
  -- -------- --

In Section @xmath we introduce two models of @xmath , and Daniel and
Hauswirth’s original parametrisation of the horizontal catenoids. We
then map this to the upper half plane model of @xmath , and
reparametrise the catenoids with the coordinates that we will perform
our analysis with.

Our approach to constructing the families of cmc @xmath ends is
perturbative. In section @xmath we expand the mean curvature operator on
normal graphs in the form

  -- -------- --
     @xmath   
  -- -------- --

The linearisation @xmath is the well-known Jacobi operator, and our
primary focus is to understand the structure of the non-linear operator
@xmath which collects the quadratic and higher order terms in @xmath and
its derivatives. In particular we show that the coefficients of @xmath
are functions that are bounded on the ends of the catenoids,
independently of @xmath . To obtain these results we begin with an
expansion of the mean curvature equation in a more general setting using
Fermi coordinates. Then we incorporate information on the geometry of
@xmath and the horizontal catenoids to establish the final estimates.

Section @xmath is devoted to analysing the linear Jacobi operator. We
begin by calculating an explicit expression for the operator in our
chosen coordinates, which brings out a close similarity to the Jacobi
operator on minimal catenoids in Euclidean space. This similarity allows
us to obtain explicit expressions for several Jacobi fields that arise
from geometric deformations of the horizontal catenoids. Our two main
linear results provide solutions of homogeneous and inhomogeneous
boundary value problems for the Jacobi operator with estimates. This is
done in the setting of weighted Hölder spaces, using eigenfunction
expansions.

With the linear analysis in place, Section @xmath contains the solution
of the boundary value problems for the mean curvature operator, by
reformulating it as a fixed point problem. Our results on the structure
of the non-linear term allow us to show that for small neck-sizes, the
relevant operator is a contraction on a suitable small ball, proving
existence of the desired cmc surfaces.

The final Section @xmath outlines a gluing construction for cmc @xmath
surfaces in @xmath , in which the space of ends asymptotic to horizontal
catenoids would play an important role. We discuss the proposed
construction, as well the technical obstacles that have prevented us
from completing it.

## 2 Constant mean curvature @xmath horizontal catenoids in @xmath

### 2.1 Models of @xmath

We will use two models for @xmath , corresponding to the upper
half-plane and disc models of the hyperbolic plane. Hauswirth and
Rosenberg parametrise the horizontal catenoids in the disc model of
@xmath , but we will map the parametrisation to the upper half plane
model and conduct all our analysis there.

By the upper half plane model of @xmath we mean, of course, the upper
half-plane model of hyperbolic space crossed with the real line. This is
the set @xmath endowed with the product metric @xmath , which in these
coordinates takes the form

  -- -------- --
     @xmath   
  -- -------- --

On the other hand, the disk model is the set @xmath with the metric

  -- -------- --
     @xmath   
  -- -------- --

In both models the coordinate @xmath parametrises the @xmath factor,
which we shall refer to as the vertical factor, in contrast to the
horizontal hyperbolic factor. Similarly, there is an orthogonal direct
sum decomposition of the tangent bundle as

  -- -------- --
     @xmath   
  -- -------- --

Vectors lying in the first factor will be called horizontal , whilst
those in the second factor are vertical . The horizontal and vertical
components of a tangent vector @xmath we denote by @xmath and @xmath .

The isometries we will use to relate the two models act trivially on the
vertical factor. On the horizontal factor they are the Möbius
transformations that exchange the ordered triples @xmath and @xmath ,
when we identify the pairs @xmath and @xmath with the complex variables
@xmath and @xmath respectively. Explicitly, these maps are given by

  -- -------- -- -----
     @xmath      (2)
  -- -------- -- -----

  -- -------- --
     @xmath   
  -- -------- --

The isometry group of @xmath is isomorphic to the product of isometry
groups of @xmath and @xmath . The isometry group of the upper half plane
model of @xmath is generated by three families of orientation-preserving
isometries, as well as an orientation-reversing inversion. The
orientation preserving families are parabolic translations

  -- -------- --
     @xmath   
  -- -------- --

hyperbolic dilations

  -- -------- --
     @xmath   
  -- -------- --

elliptic rotations about @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

whereas inversion in the semi-circle @xmath reverses orientation. Each
of the one-parameter families of translations, dilations and rotations
give rise to a Killing field, and these are responsible for a certain
degeneracy of the linear operators we will study. The hyperbolic plane
is homogeneous and isotropic, so the next result is not surprising.

###### Lemma 2.1.

The sectional curvature @xmath of a plane @xmath is given in terms of a
unit normal vector @xmath to @xmath by

  -- -- --
        
  -- -- --

###### Proof.

The product structure of @xmath and the fact that @xmath has zero
curvature imply that the curvature tensor only depends on the horizontal
components of its arguments. Furthermore the horizontal factor has
constant curvature @xmath , so the curvature tensor of @xmath is given
by

  -- -------- --
     @xmath   
  -- -------- --

Given the plane @xmath , choose an orthonormal basis @xmath for @xmath
where @xmath is horizontal; this is always possible as we can choose
@xmath orthogonal (in P) to the projection of @xmath onto @xmath . With
this choice of basis we have

  -- -------- --
     @xmath   
  -- -------- --

Expressing the vertical vector @xmath in terms of the orthonormal basis
@xmath we have @xmath , so Pythagorous’ theorem implies that

  -- -------- --
     @xmath   
  -- -------- --

Combining the last two equations gives the result. ∎

###### Lemma 2.2.

Let @xmath be a unit normal for @xmath . Then @xmath and the Ricci
tensor of @xmath are related by

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

To see this note that @xmath is the sum of sectional curvatures of three
orthogonal planes (specifically @xmath and any two orthogonal planes
containing @xmath ). However @xmath is also half the scalar curvature,
so the sum is independent of the orthogonal planes chosen. Choosing the
three planes spanned by each pair of the coordinate vectors @xmath ,
@xmath we obtain the given value. ∎

### 2.2 The horizontal catenoids

In this section we describe our parametrisation of the horizontal
catenoids (Proposition 2.10 ). This is obtained from the original
parametrisation of Daniel and Hauswirth by, firstly, mapping it from the
ball to the half-plane model of @xmath , and secondly, making a change
of coordinates motivated by the resulting form of the Jacobi operator.
Thereafter we discuss their geometry and calculate some explicit
formulae for various curvatures that we will use in computing the Jacobi
operators.

The original, conformal parametrization of the horizontal catenoids
obtained by [ 5 ] is given in Proposition 2.4 . We begin by describing
the parameters and functions needed to define the parametrisation. For
details on the origin of these, see [ 5 ] .

The one-parameter family of catenoids is indexed by @xmath . We define
equivalent parameters

  -- -------- -- -----
     @xmath      (3)
  -- -------- -- -----

The length of the shortest closed geodesic on the horizontal catenoid
with parameter @xmath is on the order of @xmath , so we may think of
@xmath as a neck-size parameter. We will use both @xmath and @xmath as
convenient. Typically our results hold for all large @xmath , or
equivalently for all sufficiently small @xmath .

###### Definition 2.3.

The functions @xmath and @xmath are defined as the solutions to the
initial value problems

  -- -------- -- -----
     @xmath      (4)
  -- -------- -- -----

  -- -------- -- -----
     @xmath      (5)
  -- -------- -- -----

We also define

  -- -------- --
     @xmath   
  -- -------- --

Now we can state the original parametrization of the horizontal
catenoids.

###### Proposition 2.4 (Horizontal catenoids of [5]).

For each @xmath there is a constant mean curvature @xmath horizontal
catenoid parametrised in the ball model of @xmath by

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

The metric in these coordinates is

  -- -------- --
     @xmath   
  -- -------- --

Before we can map this to the upper half plane we need some preliminary
calculations to obtain reasonably simple expressions when we do so.

###### Lemma 2.5.

The following identities relate @xmath and @xmath :

  -- -------- --
     @xmath   
  -- -------- --

These allow us to replace @xmath and @xmath with functions of @xmath in
Lemma 2.6 below.

###### Proof.

This is proved in Lemma @xmath of [ 5 ] . Using the Definition 2.3 it is
straightforward to verify that the functions @xmath and @xmath both
solve the initial value problem

  -- -------- --
     @xmath   
  -- -------- --

This gives the first identity, and the second follows from the first
using the definitions 2.3 and standard trigonometric identities. ∎

###### Lemma 2.6.

The functions @xmath and @xmath satisfy the identity

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

We’ll show that @xmath . Expanding the squares and using hyperbolic
trigonometric identities we get

  -- -------- --
     @xmath   
  -- -------- --

Hence the claimed formula will follow once we show that the quantity in
@xmath is identically one. Direct calculation from the expression for
@xmath (see Definition 2.3 ) shows that this is equivalent to the
identity

  -- -------- --
     @xmath   
  -- -------- --

and the validity of this expression can be seen by using the identities
of Lemma 2.5 to replace @xmath on the left hand side and @xmath on the
right hand side. ∎

Now we are in a position to map the parametrisation in Definition 2.4 to
the upper half plane. The isometry we will use acts trivially on the
@xmath factor, so we only need to compute its effect on @xmath . In
terms of real and imaginary parts, the map ( 2 ) of the ball model to
the upper half plane model is given by

  -- -------- -- -----
     @xmath      (6)
  -- -------- -- -----

Note that this is not the most commonly used map between these models;
it is our choice because it places the ”axis” of the catenoids along the
@xmath axis, and the ends at @xmath and @xmath .

###### Lemma 2.7.

Composing @xmath with the mapping ( 2 ) to the upper half plane we have

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Given that @xmath and @xmath , the identity of Lemma 2.6 gives

  -- -------- --
     @xmath   
  -- -------- --

Another application of the identity gives

  -- -------- --
     @xmath   
  -- -------- --

Therefore the claim follows from ( 6 ). ∎

###### Lemma 2.8.

The denominators @xmath are given by

  -- -- --
        
  -- -- --

###### Proof.

Expanding the hyperbolic trigonometric functions in Proposition 2.4 as
sums of exponentials we obtain

  -- -------- --
     @xmath   
  -- -------- --

from which the claimed formula follows straightforwardly.∎

###### Lemma 2.9.

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

By calculation, using Definition 2.3 and Lemma 2.5 , we omit the
details. ∎

#### New coordinates @xmath and @xmath

We define coordinates @xmath on the horizontal catenoids by

  -- -------- --
     @xmath   
  -- -------- --

Making the change of variables from @xmath to @xmath and putting
together the results above we have the parametrisation of the catenoids
in the upper half plane that we will use for our analysis.

###### Proposition 2.10.

For each @xmath there is a constant mean curvature @xmath horizontal
catenoid paramatrised in the upper half-plane model of @xmath by

  -- -------- -- -----
     @xmath      (7)
  -- -------- -- -----

  -- -------- --
     @xmath   
  -- -------- --

The metric in these coordinates is given by

  -- -------- -- -----
     @xmath      (8)
  -- -------- -- -----

###### Proposition 2.11.

The catenoids are invariant under a finite group of ambient isometries
generated by three reflections. Indeed, reflection in the plane @xmath
interchanges @xmath and @xmath , and reflection in the plane @xmath
interchanges @xmath and @xmath . Inversion in the semi-circle @xmath
interchanges @xmath and @xmath ,

###### Proof.

The first two symmetries follow from the fact that @xmath and @xmath are
invariant under @xmath and @xmath . Due to the somewhat complicated
formula for inversion in the unit circle, it is more difficult to verify
the last symmetry in these coordinates. However, in the original
parametrization of the catenoids in the ball model (Proposition 2.4 ),
it is easy to see that reflection in the geodesic @xmath interchanges
@xmath and @xmath , which is exactly the symmetry we claim in our model
and coordinates. ∎

###### Lemma 2.12.

Let @xmath denote the sectional curvature in @xmath of the tangent plane
to the horizontal catenoids at @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

Note the surprising fact that this is independent of @xmath .

###### Proof.

We obtain this formula by expressing @xmath in terms of the gradient
@xmath of the height function @xmath . The gradient of @xmath in @xmath
is simply @xmath , so @xmath is the projection of @xmath onto @xmath .
In terms of the normal @xmath to the surface @xmath . From this and
Lemma 2.1 it follows that

  -- -------- --
     @xmath   
  -- -------- --

Now a straightforward, intrinsic calculation of @xmath using the
parametrisation ( 7 ) and metric ( 8 ) gives

  -- -------- --
     @xmath   
  -- -------- --

Together the last two equations give the result. ∎

By direct calculation from the metric ( 8 ), or changing variables in
Proposition @xmath of [ 5 ] , we have a formula for the intrinsic
curvature on the horizontal catenoids.

###### Lemma 2.13.

The intrinsic curvature @xmath on the horizontal catenoids is

  -- -------- --
     @xmath   
  -- -------- --

Note that the integral of the absolute value of @xmath is infinite.

###### Lemma 2.14.

The principal curvatures @xmath on @xmath satisfy the estimates

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

The constant mean curvature condition @xmath and the Gauss equation
@xmath imply that

  -- -------- --
     @xmath   
  -- -------- --

Using the cmc equation once more we see that the principal curvatures
satisfy the quadratic equation @xmath , whose roots are

  -- -------- --
     @xmath   
  -- -------- --

By Lemmas 2.12 and 2.13 we have the bound @xmath from which the
estimates follow. ∎

#### 2.2.1 Convergence to horocylinders in the small necksize limit

In this section we represent an annular domain in the horizontal
catenoids as a horizontal graph, that is to say a graph of the form
@xmath . We obtain an expansion of the graphing function @xmath which
implies that the end of horizontal catenoids converges uniformly to a
horocylinder on compact subsets of @xmath . It will be most convenient
to state the results in terms of the following polar coordinates.

###### Definition 2.15 (Polar coordinates @xmath).

The polar coordinates @xmath in the @xmath plane are defined by

  -- -------- --
     @xmath   
  -- -------- --

###### Lemma 2.16.

Fix a parameter @xmath . Then for all small @xmath (depending on @xmath
) the horizontal graph @xmath parametrising @xmath admits the expansion

  -- -------- -- -----
     @xmath      (9)
  -- -------- -- -----

Note that the dominant terms are independent of @xmath .

###### Proof.

We begin by defining an auxiliary variable @xmath which is approximately
the radial variable @xmath defined above:

  -- -------- --
     @xmath   
  -- -------- --

For each fixed @xmath this defines a diffeomorphism @xmath , which we
restrict to @xmath . Inverting this we obtain

  -- -------- -- ------
     @xmath      (10)
  -- -------- -- ------

Now we derive an expansion for @xmath in terms of @xmath . Firstly note
that @xmath uniformly on @xmath . Hence the expansion ( 10 ) implies

  -- -------- -- ------
     @xmath      (11)
  -- -------- -- ------

Note that this is essentially ( 9 ), except we want it in terms of
@xmath not @xmath .

We must find an expression for @xmath in terms of @xmath . To begin with
we compute

  -- -------- --
     @xmath   
  -- -------- --

thanks to @xmath and ( 11 ). Similarly one has

  -- -------- --
     @xmath   
  -- -------- --

From these and the relation @xmath it follows that

  -- -------- --
     @xmath   
  -- -------- --

Putting this back into ( 11 ) completes the proof. ∎

#### 2.2.2 Truncating the horizontal catenoids

###### Definition 2.17.

We will truncate the horizontal catenoids at @xmath , where @xmath is
defined as the positive solution to

  -- -------- --
     @xmath   
  -- -------- --

We think of the surface @xmath as the end of a horizontal catenoid. When
we construct cmc @xmath surfaces asymptotic to a horizontal catenoid
end, it will be as normal graphs over these surfaces.

Note that

  -- -------- -- ------
     @xmath      (12)
  -- -------- -- ------

Although this choice of cut-off appears to truncate the ends far from
the necks, an intrinsic calculation using the metric ( 8 ) shows that
the distance from the neck geodesic @xmath to the boundary curve @xmath
is realised by the geodesic @xmath , and equals @xmath .

By Lemma 2.14 , the principal curvatures are bounded uniformly in @xmath
and @xmath on the end. Therefore this choice of truncation avoids the
neck regions of the catenoids, where the curvatures blow up as @xmath .
Furthermore, it follows from Lemma 2.16 that as @xmath , these ends
converge to the horocylinder @xmath , the convergence being uniform on
compact subsets of @xmath .

For later reference we record here the behaviour of the boundary curve
@xmath as @xmath when regarded as a horizontal graph over a curve in the
@xmath plane.

###### Lemma 2.18.

On the boundary curve @xmath of an end we have

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (13)
     @xmath   @xmath   @xmath      (14)
     @xmath   @xmath   @xmath      (15)
  -- -------- -------- -------- -- ------

Thus the boundary curve is the graph over a curve in the @xmath plane
that converges to the unit circle @xmath , and the graph admits the
expansion ( 14 ), as @xmath .

###### Proof.

The proof is by straightforward calculation, using ( 12 ). ∎

## 3 Expansion of the Mean Curvature Operator

The goal of this section is to establish a first-order expansion for the
mean curvature operator for normal graphs off the ends of horizontal
catenoids. It’s a well-known fact ( [ 19 ] , [ 20 ] ) that the
linearisation of the mean curvature operator for a surface @xmath is the
Jacobi operator @xmath , given by

  -- -------- --
     @xmath   
  -- -------- --

Here @xmath is the Laplace-Beltrami operator on @xmath , @xmath is the
norm squared of the second fundamental form, and @xmath is the Ricci
tensor evaluated on the unit normal field @xmath of @xmath . The
emphasis in the following result is therefore on the bounds for the
coefficients of the non-linear error @xmath , or more precisely the
estimate ( 16 ) that we will use in the contraction mapping argument.

###### Proposition 3.1.

The mean curvature operator @xmath admits the expansion

  -- -------- --
     @xmath   
  -- -------- --

The error term @xmath has the properties that @xmath , and there exists
a constant @xmath , independent of @xmath , such that for all @xmath

  -- -------- -- ------
     @xmath      (16)
  -- -------- -- ------

To prove Proposition 3.1 we work partly abstractly, using Fermi
coordinates on a tubular neighbourhood of an embedded surface. This
simplifies the calculation of the zeroth and first order terms in the
expansion. To handle the higher order terms, and make estimates that are
valid uniformly on the ends, it is necessary to introduce various facts
about the geometry of @xmath and the ends of horizontal catenoids.

### 3.1 Fermi coordinates

Consider an embedding @xmath of a two-sided surface @xmath in a
three-manifold @xmath , with unit normal @xmath . Relative to any
(local) coordinates @xmath on @xmath , one defines the Fermi coordinates
on a tubular neighbourhood of (a coordinate neighbourhood in) @xmath ,
by the parametrisation

  -- -------- --
     @xmath   
  -- -------- --

Here @xmath denotes the exponential map on @xmath .

Given a sufficiently small and smooth function @xmath on @xmath , we
define the surface @xmath by the embedding

  -- -------- --
     @xmath   
  -- -------- --

With respect to Fermi coordinates, the embedding @xmath takes the simple
form

  -- -------- --
     @xmath   
  -- -------- --

This greatly facilitates the calculations, which proceed analogously to
those for the graph of a function in Euclidean space. Before we compute
the induced metric @xmath and second fundamental form @xmath on @xmath ,
we develop a few aspects of the theory of Fermi coordinates that we will
need.

#### Notation

The coordinate frame in @xmath determined by the Fermi coordinates
@xmath will be denoted by @xmath and @xmath . The level sets of the
coordinate @xmath will be denoted by @xmath , and refered to as tubular
hypersurfaces .

###### Lemma 3.2 (Tubular Gauss Lemma).

The @xmath -coordinate lines are unit speed geodesics that are
orthogonal to the tubular hypersurfaces.

###### Proof.

By definition of the exponential map, the @xmath -coordinate lines are
unit speed geodesics. Note that @xmath and @xmath are tangential to the
tubular hypersurfaces for all @xmath , so we need to show that @xmath is
orthogonal to them. Since @xmath , we have @xmath when @xmath .
Furthermore, for @xmath ,

  -- -- --
        
  -- -- --

because each term vanishes, since @xmath -coordinate lines are unit
speed geodesics. ∎

In the forthcoming calculations we find it convenient, following [ 10 ]
, to use the following two fields of operators, which satisfy a
differential equation.

###### Definition 3.3.

The operators @xmath and @xmath are defined by

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the curvature endomorphism of @xmath .

By the Tubular Gauss Lemma, for each fixed @xmath the operator @xmath is
nothing but the shape operator for the tubular hypersurface @xmath . The
relevance of @xmath is a little less clear, but illuminated by the next
two lemmas.

###### Lemma 3.4.

For each fixed @xmath the operators @xmath are a field of symmetric
linear operators on @xmath whose eigenvalues are sectional curvatures
(in @xmath ) of two-planes orthogonal to the tubular hypersurfaces.

###### Proof.

Note that

  -- -------- -- ------
     @xmath      (17)
  -- -------- -- ------

Taking @xmath , the anti-symmetry of @xmath in its last two arguments
shows that @xmath is tangential to the tubular hypersurfaces (by the
Tubular Gauss lemma), so the operators @xmath reduce to operators on
@xmath . Symmetry of @xmath also follows from the symmetries of the
curvature tensor @xmath . Hence the eigenvalues of @xmath are the
extrema of the associated quadratic form @xmath , when restricted to the
unit sphere in the tangent spaces for @xmath . By ( 17 ) these extrema
are the sectional curvatures of certain two-planes orthogonal to the
tubular hypersurfaces. ∎

The following differential equation for the shape operators @xmath is
the key fact about @xmath and @xmath . Recall that the connection on
@xmath is defined to respect the pairing of an operator with a tangent
vector. Hence for an operator @xmath we have

  -- -------- --
     @xmath   
  -- -------- --

###### Lemma 3.5 (Ricatti equation for shape operators ([10])).

The shape operators @xmath satisfy the Ricatti equation

  -- -------- -- ------
     @xmath      (18)
  -- -------- -- ------

###### Proof.

Equation ( 18 ) is tensorial, so we can verify it using any basis we
please. Using coordinate vectors @xmath , one has simply @xmath ,
therefore

  -- -------- --
     @xmath   
  -- -------- --

There is also a scalar version of this result for the eigenvalues of the
operators @xmath , which will be an important ingredient in our
estimates later.

###### Corollary 3.6.

Let @xmath be the eigenvalues of @xmath (which for fixed @xmath are also
the principal curvatures of the tubular hypersurfaces). We assume that
the eigenvalues are distinct. Then for @xmath they satisfy the equation

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the sectional curvature of the plane spanned by @xmath
and the @xmath -eigenspace of @xmath .

###### Proof.

@xmath is symmetric, therefore on each tubular hypersurface there is an
orthonormal frame of eigenvectors for @xmath which we denote @xmath ,
@xmath . Applying the equation ( 18 ) to @xmath we get

  -- -------- --
     @xmath   
  -- -------- --

Taking the inner product of this with @xmath gives the result, because
symmetry of @xmath implies that

  -- -- --
        
  -- -- --

Now we are ready to prove the main result we will use about Fermi
coordinates, which is the expansion of part of the ambient metric and
Christoffel symbols in the @xmath direction.

###### Proposition 3.7.

We work in Fermi coordinates. For @xmath the metric components @xmath
and Christoffel symbols @xmath have the expansions

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

where @xmath and @xmath are the metric and second fundamental form on
@xmath , and @xmath is a symmetric two-tensor given by @xmath .
Furthermore @xmath and @xmath satisfy the Lipschitz estimates

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

for all @xmath .

###### Proof.

We begin by computing the first three partial derivatives of @xmath with
respect to @xmath . In these calculations we repeatedly use standard
facts about the Levi-Civita connection, such as its symmetry and
compatibility with the metric, as well as the symmetry of @xmath . Thus

  -- -------- -- ------
     @xmath      (19)
  -- -------- -- ------

Then since @xmath we find

  -- -------- -- ------
     @xmath      (20)
  -- -------- -- ------

For the third derivative we begin with

  -- -- --
        
  -- -- --

Expanding

  -- -------- --
     @xmath   
  -- -------- --

and using the Ricatti equation to compute @xmath gives

  -- -------- -- ------
     @xmath      (21)
  -- -------- -- ------

Now any twice differentiable function @xmath on @xmath has an expansion

  -- -------- --
     @xmath   
  -- -------- --

where, by the mean value inequality, the function @xmath satisfies the
estimate

  -- -------- --
     @xmath   
  -- -------- --

The expansion for @xmath follows from this and ( 19 ). Finally the
standard formula for Christoffel symbols in terms of the inverse and
first derivatives of the metric gives us

  -- -------- --
     @xmath   
  -- -------- --

Hence we obtain the expansion for @xmath from the formulae ( 19 ), ( 20
). ∎

#### A preliminary expansion of @xmath and @xmath.

Now we return to the surface @xmath for which we require expansions of
the metric @xmath and second fundamental form @xmath . Recall that in
Fermi coordinates, a parametrisation of this surface is given by

  -- -------- --
     @xmath   
  -- -------- --

Therefore the tangent vectors are @xmath and @xmath , where we use the
notation @xmath for brevity. A succinct expression for the unit normal
@xmath to @xmath can be given by introducing the vector

  -- -------- --
     @xmath   
  -- -------- --

that is, the pushforward of the gradient @xmath of @xmath on @xmath by
the embedding @xmath . Then one can easily verify that

  -- -------- --
     @xmath   
  -- -------- --

A straighforward calculation using the Tubular Gauss Lemma proves

###### Lemma 3.8.

The metric and second fundamental of @xmath are

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath            
                       @xmath   
  -- -------- -------- -------- --

For the moment we will pause in the calculation of the mean curvature,
and collect certain estimates relating to the geometry of @xmath , the
horizontal catenoids, and Fermi coordinates defined relative to the
@xmath coordinates on the horizontal catenoids. This allows us to give a
simpler treatment of the otherwise complicated quantities that will
contribute to the non-linear error in the expansion of the mean
curvature.

### 3.2 Estimates for error terms

In computing an expansion for the mean curvature operator we’ll need
control over the errors @xmath and @xmath - and also the main terms - in
the expansions in Proposition 3.7 . More precisely, we need estimates on
these terms that hold both uniformly on the ends of horizontal
catenoids, and uniformly for all small @xmath . Our strategy to obtain
these begins with the observation that all of these terms are the
components of tensors defined in terms of the operators @xmath and
@xmath . Therefore, if one can estimate the eigenvalues of these
operators, one will have estimates for these terms.

###### Lemma 3.9.

The eigenvalues of @xmath are bounded in @xmath . The eigenvalues of
@xmath are bounded on a uniform tubular neighbourhood of the ends, and
uniformly for all small @xmath . More precisely, there exists a constant
@xmath such that for all small @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

In Lemma 3.4 we showed that the eigenvalues of @xmath are sectional
curvatures in @xmath , hence the first claim. For the second, note that
Lemma 2.14 implies the estimates @xmath and @xmath , uniformly on the
ends. To leverage these from @xmath to estimates for @xmath we use the
Ricatti equation satisfied by the eigenvalues (Corollary 3.6 ).

For brevity of notation write @xmath for @xmath . Now the functions
@xmath in the Ricatti equation

  -- -------- --
     @xmath   
  -- -------- --

satisfy the bounds @xmath , because they are sectional curvatures in
@xmath . Hence we define @xmath and @xmath as solutions to the initial
value problems

  -- -------- --
     @xmath   
  -- -------- --

By comparison of the equations they satisfy, @xmath for all small @xmath
, and @xmath for all small @xmath . Hence

  -- -------- --
     @xmath   
  -- -------- --

Integrating the equations we find

  -- -------- --
     @xmath   
  -- -------- --

Then since @xmath and @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

Clearly these are all bounded when @xmath is sufficiently small and
@xmath , and the result follows. ∎

###### Lemma 3.10.

Let @xmath be the eigenvalues of @xmath . Then @xmath , and

  -- -------- --
     @xmath   
  -- -------- --

Hence @xmath and @xmath are independent of @xmath , and @xmath vanishes
identically.

###### Proof.

Recall that the eigenvalues of @xmath are sectional curvatures of
two-planes spanned by @xmath and vectors tangent to the tubular
hypersurfaces. Now the first case to consider is that @xmath is
vertical, in which case the @xmath -geodesic is a vertical geodesic and
@xmath remains vertical for all @xmath . Therefore any two plane spanned
by @xmath is vertical, all of their sectional curvatures are zero, and
@xmath vanishes identically for all @xmath . In this case all the
assertions of the lemma certainly hold.

In the second case we assume that @xmath is not vertical. Since @xmath
is parallel we have

  -- -------- --
     @xmath   
  -- -------- --

and so the angle between @xmath and @xmath is independent of @xmath .
Now we know that the sectional curvatures of @xmath are bounded between
@xmath and @xmath , and it is clear that the @xmath -plane spanned by
@xmath and the tangential projection @xmath of @xmath onto the tangent
space of the tubular hypersurface contains @xmath . Therefore the
sectional curvature of this plane is zero, the quadratic form associated
to @xmath is maximised in the direction of @xmath , and so @xmath must
be an eigenvector of @xmath with eigenvalue @xmath .

To compute the second eigenvalue we observe that, since @xmath is
symmetric, the eigenspace for @xmath is orthogonal to @xmath , and
tangential to the tubular hypersurface. Hence the plane spanned by this
eigenspace and @xmath is orthogonal to @xmath , and @xmath is the
sectional curvature of this plane. But Lemma 2.1 tells us that we can
compute this sectional curvature as

  -- -------- --
     @xmath   
  -- -------- --

This establishes the formula for @xmath . Since it only depends on
@xmath and since this is independent of @xmath as we have mentioned,
this completes the proof of the assertions regarding the eigenvalues.

To prove the vanishing of @xmath we begin by completing the orthonormal
set of vector fields @xmath and @xmath to a smooth orthonormal frame
along the @xmath -geodesic with a vector field @xmath along the curve.
As already noted, @xmath is an eigenvector for @xmath . Moreover @xmath
is actually parallel along this geodesic, because @xmath and @xmath are.
Now expand any vector field along the geodesic that is tangential to
tubular hypersurfaces as @xmath . Using the facts just mentioned, and
that @xmath and @xmath are independent of @xmath , we compute

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

###### Corollary 3.11.

There exists @xmath and a constant @xmath independent of @xmath , such
that

  -- -------- --
     @xmath   
  -- -------- --

Note that this implies an estimate of the same form for @xmath and
@xmath too, by ( 19 ) and ( 20 ).

###### Proof.

We claim that once an estimate of this form is proved for @xmath , those
for the derivatives follow. Indeed the formulae ( 19 ), ( 20 ) and ( 21
) for the first three derivatives of @xmath with respect to @xmath show
that we can estimate them in terms of @xmath and the operator norms of
@xmath and @xmath , by Cauchy-Schwarz’ inequality. Moreover the operator
norms of @xmath and @xmath are bounded uniformly in all the variables,
by Lemma 3.9 , and @xmath by Lemma 3.10 . This proves the claim.

To obtain the estimate for @xmath , first note that

  -- -------- --
     @xmath   
  -- -------- --

where, by the mean value inequality,

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath , Lemma 3.9 gives

  -- -------- -- ------
     @xmath      (22)
  -- -------- -- ------

When @xmath , this yields @xmath , or in other words,

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath , we obtain the desired estimate in the case @xmath . The
general case then follows from the estimate when @xmath and ( 22 ). ∎

### 3.3 Proof of Proposition 3.1

For notational simplicity, we will use the symbols @xmath and @xmath to
denote ”quadratic” errors and the growth rates of their coefficients, as
follows.

###### Definition 3.12.

For @xmath , a quantity will be denoted @xmath if @xmath , and there is
a constant @xmath , independent of @xmath , such that for all @xmath we
have

  -- -------- --
     @xmath   
  -- -------- --

Note that every error of the form @xmath is also of the form @xmath if
@xmath .

###### Lemma 3.13.

The first and second fundamental form of @xmath are

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the Hessian of @xmath on @xmath .

###### Proof.

It follows from Lemma 3.8 that @xmath . And combining the estimates of
Proposition 3.7 with the those of Corollary 3.11 gives

  -- -------- --
     @xmath   
  -- -------- --

Together these prove the expansion for @xmath .

Going back to the preliminary expression of Lemma 3.8 we found

  -- -------- -------- -------- --
     @xmath   @xmath            
                       @xmath   
  -- -------- -------- -------- --

Once again combining the estimates of Proposition 3.7 with the those of
Corollary 3.11 gives

  -- -------- --
     @xmath   
  -- -------- --

Furthermore @xmath , whilst the expression on the second line in the
formula ( 3.3 ) is @xmath . Therefore we get

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath is the @xmath component of the Hessian of @xmath this
completes the proof. ∎

###### Proof of Proposition 3.1.

We work in matrix notation, so

  -- -------- --
     @xmath   
  -- -------- --

Inverting the expansion for @xmath one obtains

  -- -------- --
     @xmath   
  -- -------- --

Hence

  -- -------- --
     @xmath   
  -- -------- --

In applying the matrix trace, note that

  -- -------- --
     @xmath   
  -- -------- --

and @xmath , so

  -- -------- --
     @xmath   
  -- -------- --

Therefore, as claimed,

  -- -------- --
     @xmath   
  -- -------- --

The error is of type @xmath (rather than @xmath or @xmath ) because each
term that contributes to it is a product of a term from the expansion of
@xmath , which has coefficients that are @xmath , and a term from the
expansion of @xmath , which has coefficients that are @xmath . ∎

## 4 Linear Analysis of the Jacobi Operator

In the previous section we expanded the mean curvature operator on
normal graphs as

  -- -------- --
     @xmath   
  -- -------- --

Of course, @xmath is the mean curvature of the horizontal catenoids, and
thus identically @xmath . Therefore a normal graph has constant mean
curvature @xmath if and only if the function @xmath satisfies

  -- -------- --
     @xmath   
  -- -------- --

To solve this equation we will invert the Jacobi operator @xmath ,
reformulate it as a fixed point problem, and apply the contraction
mapping principle. This requires that we choose suitable Banach spaces
on which the Jacobi operator may be wholly or partially inverted.

We begin our treatment with an explicit calculation of the Jacobi
operator in @xmath coordinates.

###### Proposition 4.1.

The Jacobi operator @xmath is given by

  -- -------- -- ------
     @xmath      (24)
  -- -------- -- ------

where @xmath is a second order operator, independent of @xmath and
@xmath , given by

  -- -------- -- ------
     @xmath      (25)
  -- -------- -- ------

###### Proof.

As we have already shown, @xmath . From the coordinate expression ( 8 )
for the metric, we calculate the Laplacian to be

  -- -------- --
     @xmath   
  -- -------- --

To compute the potential we use the intrinsic curvature @xmath of @xmath
, and the Gauss equation to write

  -- -------- --
     @xmath   
  -- -------- --

Recall that @xmath are the principal curvatures, and @xmath is the
sectional curvature in @xmath of the tangent plane to @xmath . This and
the constant mean curvature condition @xmath give

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath , by Lemma 2.2 ,

  -- -------- --
     @xmath   
  -- -------- --

Finally, combining the formulae for @xmath and @xmath from Lemmas 2.12
and 2.13 , and making essential use of the identity @xmath , one
calculates that

  -- -------- --
     @xmath   
  -- -------- --

This and the expression for the Laplacian complete the proof. ∎

For applications to the mean curvature equation it will suffice to study
the operator

  -- -------- -- ------
     @xmath      (26)
  -- -------- -- ------

which we shall also call the Jacobi operator. Notice that @xmath is a
perturbation of

  -- -------- -- ------
     @xmath      (27)
  -- -------- -- ------

This operator is nothing other than the Jacobi operator on a minimal
catenoid in euclidean space with its standard parametrisation. A good
deal is known about this operator, and we will exploit these facts to
study @xmath . To begin with, a useful class of Banach spaces on which
to study such operators are the weighted Hölder spaces [ 33 ] . The ones
we shall work with are the standard ones on a half-cylinder.

###### Definition 4.2.

Fix @xmath , a non-negative integer @xmath , @xmath and @xmath . @xmath
is defined as the set of @xmath functions for which the following
weighted norm is finite

  -- -------- --
     @xmath   
  -- -------- --

Here @xmath is the usual @xmath norm on @xmath .

A function belonging to @xmath satisfies an estimate of the form

  -- -------- --
     @xmath   
  -- -------- --

Thus a positive weight allows functions of limited exponential growth, a
negative weight requires functions to decay at a sufficiently fast
exponential rate, and @xmath if @xmath .

The reason for considering these weighted Hölder spaces is that - except
for a discrete, countable set of weights - operators such as @xmath and
@xmath are Fredholm on these spaces. The exceptional set of weights is
the sequence of indicial roots of the operator. These indicial roots
describe all the possible asymptotic growth and decay rates of solutions
to the homogeneous equation. There is a fundamental relationship between
the weight, indicial roots and Fredholm index, which is an important
consideration in our analysis. To better understand this we’ll review
the relationship in the simplest setting of the flat Laplacian on a
half-cylinder.

### 4.1 Weights, indicial roots and Fredholm index: an example

This example is a slight modification of an example in [ 33 ] . We
denote the flat Laplacian on a half-cylinder @xmath by

  -- -------- --
     @xmath   
  -- -------- --

Later we will need to understand a perturbation of @xmath to analyse the
Jacobi operator.

Throughout we’ll work with a fixed orthonormal basis @xmath for @xmath ,
such that

  -- -------- --
     @xmath   
  -- -------- --

Let

  -- -------- --
     @xmath   
  -- -------- --

The purpose of this discussion is to illustrate two main facts. First,
consider the bounded operator

  -- -------- -- ------
     @xmath      (28)
  -- -------- -- ------

As alluded to above, the operator ( 30 ) is Fredholm, provided the
weight @xmath is not an indicial root. In Proposition 4.3 we show that
the indicial roots are the integers, and prove the Fredholm property in
the case most relevant to us, when @xmath and the operator is injective.
It turns out that the cokernel is always non-trivial when the kernel is
trivial. Furthermore the dimension of the cokernel increases each time
the weight crosses below a negative indicial root.

Proposition 4.3 implies that there is no choice of weight for which ( 30
) is invertible. In particular, when @xmath it is not possible to find a
solution @xmath to the boundary value problem

  -- -------- -- ------
     @xmath      (29)
  -- -------- -- ------

for every @xmath . However, in Proposition 4.4 we prove that it is
possible to partially invert it in the following sense. We can find, for
every @xmath , a unique @xmath with the properties that @xmath and
@xmath vanishes on the boundary, except on the certain ’low modes’
@xmath . The number of these low modes is the dimension of the cokernel
of ( 30 ).

In summary then, the closest one can come to inverting @xmath in this
setting, is with the solution operator @xmath of Proposition 4.4 . This
provides solutions to the equation @xmath for every @xmath , but the
price we pay is that @xmath may not have zero boundary data on the low
modes. Finally - and this is the key point - the dimension of this space
of low modes increases every time the weight crosses below a negative
indicial root.

###### Proposition 4.3.

( [ 33 ] ) If @xmath , then

  -- -------- -- ------
     @xmath      (30)
  -- -------- -- ------

is a Fredholm operator. Let @xmath be the largest integer such that
@xmath .

1.   If @xmath and not an integer, the operator is injective with @xmath
    -dimensional cokernel.

2.   If @xmath and not an integer, the operator is surjective with
    @xmath -dimensional kernel.

As we mentioned, we’ll only discuss the most relevant case @xmath . The
proof of injectivity is elementary, but the characterisation of the
cokernel requires Proposition 4.4 below.

###### Proof of injectivity.

Assume @xmath , @xmath and @xmath is a solution to the homogeneous
equation @xmath . Expand @xmath in a Fourier series as

  -- -------- --
     @xmath   
  -- -------- --

Since

  -- -------- --
     @xmath   
  -- -------- --

the equation @xmath is equivalent to the family of ordinary differential
equations

  -- -------- --
     @xmath   
  -- -------- --

The solutions are of course

  -- -------- --
     @xmath   
  -- -------- --

Admission to the weighted space requires that each solution @xmath decay
at infinity, because @xmath is negative. Hence @xmath for all @xmath ,
@xmath , and

  -- -------- --
     @xmath   
  -- -------- --

Applying the boundary condition @xmath forces the coefficients @xmath to
vanish, and with them @xmath . ∎

###### Proposition 4.4.

Let @xmath and not an integer, and let @xmath be the largest integer
such that @xmath . Then for each @xmath there exists a unique solution
@xmath to the boundary value problem

  -- -------- -- ------
     @xmath      (31)
  -- -------- -- ------

Hence there is a well-defined solution operator @xmath given by

  -- -------- --
     @xmath   
  -- -------- --

In fact @xmath is bounded, that is, there exists a constant @xmath such
that

  -- -------- -- ------
     @xmath      (32)
  -- -------- -- ------

###### Proof.

We begin with the proof of existence and the estimate ( 32 ). Expand
@xmath and @xmath in Fourier series as

  -- -------- --
     @xmath   
  -- -------- --

The equation we wish to solve is equivalent to the family of ordinary
differential equations

  -- -------- --
     @xmath   
  -- -------- --

These can be solved using ”variation of coefficients”, since the
solutions to the homogeneous equations are explicitly known. As we wish
to obtain decaying functions @xmath , we discard the unbounded solutions
to the homogeneous equation, taking the ansatz

  -- -------- --
     @xmath   
  -- -------- --

It follows from the equation for @xmath that @xmath must satisfy

  -- -------- --
     @xmath   
  -- -------- --

This is first order and linear in @xmath , so an integrating factor of
@xmath gives us

  -- -------- -- ------
     @xmath      (33)
  -- -------- -- ------

To integrate this, notice that the right hand side is @xmath as @xmath ,
because @xmath implies that

  -- -------- --
     @xmath   
  -- -------- --

Therefore we may integrate in from infinity, obtaining

  -- -------- --
     @xmath   
  -- -------- --

If instead we had integrated ( 33 ) from @xmath to @xmath , such an
estimate need not hold. In summary then,

  -- -------- -- ------
     @xmath      (34)
  -- -------- -- ------

Now there arises a dichotomy. If @xmath , the right hand side of ( 34 )
is exponentially decaying, whilst if @xmath it may grow exponentially.
In the first case we integrate ( 34 ) from @xmath to @xmath , as before,
whilst in the second we must integrate from @xmath to @xmath . Therefore
our solutions are

  -- -------- -- ------
     @xmath      (35)
  -- -------- -- ------

if @xmath and

  -- -------- -- ------
     @xmath      (36)
  -- -------- -- ------

if @xmath . In all cases we have the estimate

  -- -------- --
     @xmath   
  -- -------- --

These bounds on @xmath are summable in @xmath , therefore there exists a
constant @xmath independent of @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

Hence the full estimate ( 32 ) follows from Schauder’s estimates.

The proof of uniqueness follows the argument for injectivity in
Proposition 4.3 @xmath . Assume we have a function @xmath satisfying (
31 ) with @xmath , and belonging to @xmath . As above we find

  -- -------- --
     @xmath   
  -- -------- --

By assumption @xmath is not only decaying, but @xmath , as @xmath . This
forces @xmath for all @xmath , and @xmath for all @xmath . So

  -- -------- --
     @xmath   
  -- -------- --

and the boundary condition in ( 31 ) implies that @xmath for all @xmath
, hence @xmath . ∎

###### Proof of Proposition 4.3 (a).

Given @xmath , let @xmath be the unique solution to the boundary value
problem ( 31 ) furnished by Proposition 4.4 . Since @xmath is negative,
the mapping ( 30 ) is injective, so @xmath is the only candidate for a
pre-image of @xmath . In fact @xmath provides such a pre-image if and
only if it vanishes identically on the boundary, which occurs precisely
when @xmath for all @xmath . Intuitively this identifies the cokernel of
( 30 ) with the space of @xmath -tuples @xmath . Formally, define a
mapping

  -- -------- --
     @xmath   
  -- -------- --

Proposition 4.4 implies that this is well-defined and bounded. Therefore
we have a short exact sequence of bounded linear maps

  -- -------- --
     @xmath   
  -- -------- --

and the cokernel of ( 30 ) is isomorphic to @xmath . ∎

In our solution of the mean curvature equation by a contraction mapping
we must invert the Jacobi operator. As with @xmath this is only possible
when the weight is not an indicial root, and then only with a solution
operator analogous to that of Proposition 4.4 . Before we can prove such
a result we must understand the indicial roots of @xmath .

### 4.2 Perturbation of spectra and indicial roots

It turns out that for the linear analysis we can work with an operator
that is simpler than even @xmath . As we will see, the exponentially
decaying term in the potential can be absorbed in ’error’ terms, so we
will study

  -- -------- --
     @xmath   
  -- -------- --

As for the flat Laplacian @xmath discussed in the previous section, the
indicial roots of @xmath are determined by the eigenvalues of the
cross-sectional operator. In this case that operator is @xmath rather
than @xmath .

#### Spectrum of @xmath

Since @xmath is a perturbation of @xmath , whose spectrum we know, it is
natural to try to apply the perturbation theory of linear operators to
understand its spectrum. The results of this theory are most
straightforward in the setting of a family of self-adjoint operators on
a fixed Hilbert space [ 14 ] . Unfortunately @xmath is not self-adjoint
on @xmath . However the Jacobi operator is self-adjoint with respect to
the area measure on the catenoids, so @xmath ought to be self-adjoint
with respect to a suitable measure obtained from the catenoids’ metrics.
This is indeed the case, the measure being @xmath . Therefore we can
arrange to work with a family @xmath of self-adjoint operators on @xmath
by conjugating @xmath by the obvious isometry of this Hilbert space and
@xmath . Of course, @xmath and @xmath have exactly the same eigenvalues.

###### Definition 4.5.

Let @xmath be the isometry

  -- -------- --
     @xmath   
  -- -------- --

We’ll consider the family of self-adjoint operators on @xmath defined by

  -- -------- --
     @xmath   
  -- -------- --

Note that @xmath .

There is one final consideration to be made before discussing the
perturbation theory. Recall that the horizontal catenoids are invariant
under a reflection @xmath in the plane @xmath . Corresponding to this
ambient symmetry is a coordinate symmetry @xmath . That is, the
embeddings @xmath of the catenoids satisfy

  -- -------- --
     @xmath   
  -- -------- --

Therefore functions on the catenoids are invariant under vertical
reflection if, for each fixed @xmath , they belong to the following
space.

###### Definition 4.6.

Let

  -- -- --
        
  -- -- --

There’s a good geometric reason for considering such functions, which we
explain below when we discuss geometric Jacobi fields. On the other hand
much of our analysis does not depend on this symmetry. Nevertheless we
will go ahead and impose it on our function spaces, to avoid a
proliferation of notations.

The space @xmath has an orthonormal basis of eigenfunctions for @xmath ,
consisting of @xmath , where

  -- -- --
        
  -- -- --

From this description it follows that the operators @xmath , @xmath and
@xmath reduce to this subspace, so we can and will consider @xmath as a
family of self-adjoint operators on @xmath .

On @xmath the eigenvalues of @xmath are all simple; we’ll denote them by
@xmath . Therefore the eigenvalues of @xmath on @xmath are also simple,
and we denote them by @xmath .

###### Lemma 4.7.

For every @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

We invoke the perturbation theory of analytic families of self-adjoint
operators (see [ 14 ] , Chapter @xmath , section @xmath ). It tells us
that the eigenvalues and eigenvectors of @xmath may be chosen to have
(convergent) series expansions in powers of @xmath in such a way that
@xmath is an eigenfunction for the unperturbed operator @xmath
corresponding to the eigenvalue @xmath :

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

The eigenfunctions for @xmath are then @xmath ; clearly these may also
be expanded in a series in powers of @xmath . In particular we have

  -- -------- --
     @xmath   
  -- -------- --

and we wish to find @xmath . Substituting these into the eigenvalue
equation @xmath and equating coefficients of @xmath , we get

  -- -------- --
     @xmath   
  -- -------- --

Now, the left hand side is orthogonal to @xmath in @xmath , hence the
coefficient of @xmath on the right hand side must vanish. We claim that
the only contribution of @xmath to the @xmath Fourier mode is @xmath ,
from which the lemma then follows.

To prove the claim, write

  -- -------- --
     @xmath   
  -- -------- --

The first term on the right hand side is the @xmath term. All other
terms involve a factor of @xmath or @xmath multiplying @xmath or its
first or second derivative. By standard identities for products of the
form @xmath and @xmath , those terms only involve @xmath and @xmath .
Since @xmath the functions @xmath and @xmath are distinct from @xmath ,
and the claim is proved. ∎

For the low eigenvalues we have more precise information.

###### Lemma 4.8.

The first three eigenvalues of @xmath on @xmath (not @xmath ) are @xmath
and @xmath . Their eigenspaces are spanned by @xmath and @xmath
respectively. Hence the first two eigenvalues of @xmath on @xmath are
@xmath and @xmath .

###### Proof.

The proof is by direct calculation; note that @xmath , @xmath and @xmath
by ( 3 ). ∎

In other words, on the full space @xmath , the multiplicity two
eigenvalue @xmath for @xmath splits into two simple eigenvalues @xmath
and @xmath , but on the restricted space @xmath the eigenvalue @xmath is
simple and is perturbed to @xmath .

#### Indicial roots

As in the discussion of the indicial roots of @xmath , the indicial
roots of @xmath are essentially the square-root of the eigenvalues of
the cross-sectional operator @xmath . Just as the eigenvalues of @xmath
depend on whether or not we impose the symmetry discussed above, so do
the indicial roots. Therefore we make the following convention :

  Unless otherwise stated, when we refer to ’the indicial roots of
  @xmath ’ it refers to the operator acting on spaces of functions that
  are invariant under the vertical reflection symmetry discussed above.

By the same argument as described for @xmath at the beginning of section
4.2 , and the results of Lemmas 4.7 and 4.8 , we have the result we will
need on the indicial roots.

###### Lemma 4.9.

The indicial roots @xmath of @xmath are given by @xmath for all @xmath .
Hence, for all small @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

### 4.3 Mapping properties on weighted Hölder spaces

In this section we will establish the linear analysis that we will use
to prove our main theorem. As discussed above, we’ll analyse the
operator @xmath on weighted Hölder spaces, and we wish to begin by
discussing our choice of weight. The most basic requirement is that the
weight be negative, so that the normal graphs are decaying and define
surfaces that are asymptotic to the ends of horizontal catenoids. Beyond
this there are two competing factors that motivate the choice of weight.

The first factor is that when we ’invert’ the operator @xmath , we would
like to be able to prescribe as much of the boundary data to be zero as
possible. As discussed following Proposition 4.3 , when the weight is
negative we can only prescribe zero boundary data up to a
finite-dimensional subspace of the full set of boundary values. Each
time we decrease the weight across an indicial root the dimension of
this subspace increments. Thus, ideally, we would like to use a weight
below zero but above the first negative indicial root.

The second factor determining our choice of the weight is the
application to the mean curvature equation. Propositions 3.1 and 4.1
imply that the normal graph defined by @xmath has cmc @xmath if and only
if

  -- -------- --
     @xmath   
  -- -------- --

Our strategy to solve this equation is to ’invert’ @xmath and
reformulate it as a fixed point problem, formally

  -- -------- --
     @xmath   
  -- -------- --

This requires that the right hand side belong to the same weighted space
as @xmath originally does. If @xmath decays with weight @xmath , then
the right hand side belongs to the space with weight @xmath . (Recall
the notation @xmath means that there is no growth in the coefficients of
the operator as @xmath .) So our requirement amounts to @xmath and
therefore @xmath .

There is a tension between choosing the weight sufficiently negative,
but minimising the number of negative indicial roots crossed. Crossing
the first negative indicial root @xmath is unavoidable as @xmath (by
Lemma 4.9 ). On the other hand the second negative indicial root @xmath
. Therefore we can work with a weight of @xmath without crossing @xmath
, and we will do so.

###### Definition 4.10.

Let @xmath be an orthonormal basis of eigenfunctions for @xmath . We
define the projections @xmath by

  -- -------- --
     @xmath   
  -- -------- --

Now we prove the two main results of this section, which provide
solutions to homogeneous and inhomogeneous boundary value problems for
@xmath in weighted Hölder spaces on the ends of horizontal catenoids (as
defined in section 2.2.2 ).

###### Proposition 4.11.

There exists a bounded linear operator

  -- -------- --
     @xmath   
  -- -------- --

such that for each @xmath , the function @xmath is the unique solution
in @xmath of the boundary value problem

  -- -------- --
     @xmath   
  -- -------- --

Furthermore there exists a constant @xmath independent of @xmath such
that

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

We expand @xmath in terms of eigenfunctions for @xmath as

  -- -------- --
     @xmath   
  -- -------- --

Then the equation @xmath is equivalent to the family of ordinary
differential equations

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath and @xmath for @xmath these have solutions

  -- -------- --
     @xmath   
  -- -------- --

Imposing the decay condition forces @xmath and @xmath for all @xmath ,
because we require @xmath to decay at least as fast as @xmath , and by
Lemma 4.9 we know @xmath for @xmath , whilst

  -- -------- --
     @xmath   
  -- -------- --

Imposing the boundary condition we have the solution

  -- -------- --
     @xmath   
  -- -------- --

where @xmath are defined by @xmath .

To obtain the weighted Hölder estimate for @xmath we begin with a
weighted @xmath estimate and then invoke Schauder’s estimates. Consider
first when @xmath ; then

  -- -------- --
     @xmath   
  -- -------- --

Here we have used the fact that @xmath for all @xmath to compare the
series in the last inequality with a geometric series. From this we get

  -- -------- --
     @xmath   
  -- -------- --

In the domain @xmath we only require the estimate

  -- -------- -- ------
     @xmath      (37)
  -- -------- -- ------

the weighted estimate follows since @xmath here. The estimate ( 37 ) in
turn follows from the fact that

  -- -- -- ------
           (38)
  -- -- -- ------

Recall that @xmath is self-adjoint with respect to @xmath . We write the
inner product and norm on this Hilbert space as

  -- -------- --
     @xmath   
  -- -------- --

  -- -- --
        
  -- -- --

Therefore self-adjointness and Cauchy-Schwarz’ inequality give

  -- -------- --
     @xmath   
  -- -------- --

This proves ( 38 ), and ( 37 ) follows because @xmath by Lemma 4.7 .

This completes the proof of the weighted @xmath estimate

  -- -------- --
     @xmath   
  -- -------- --

and the full weighted @xmath estimate follows from this and Schauder’s
boundary and interior estimates. ∎

###### Proposition 4.12.

There exists a bounded linear operator

  -- -------- --
     @xmath   
  -- -------- --

such that, for each @xmath , the function @xmath solves the boundary
value problem

  -- -------- -- ------
     @xmath      (39)
  -- -------- -- ------

Furthermore there exists a constant @xmath independent of @xmath such
that

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Expand @xmath and @xmath as

  -- -- --
        
  -- -- --

The equation @xmath reduces to the family of ordinary differential
equations

  -- -------- --
     @xmath   
  -- -------- --

When @xmath we place no boundary condition on @xmath at @xmath , but
when @xmath we require @xmath .

The solutions @xmath can be obtained via variation of parameters. In
other words, since the solutions of the homogeneous equation are @xmath
, we take the ansatz

  -- -------- --
     @xmath   
  -- -------- --

However, since we wish to obtain a decaying solution, we set @xmath .
Our equation for @xmath implies the following equation for @xmath

  -- -------- --
     @xmath   
  -- -------- --

This is a linear, first order equation in @xmath , and we may solve it
using integrating factors. In the end we get

  -- -------- --
     @xmath   
  -- -------- --

The @xmath estimate for @xmath is obtained in a few steps. Firstly,
since @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

Hence

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

Similar arguments show that

  -- -------- --
     @xmath   
  -- -------- --

These satisfy the weighted @xmath estimates

  -- -------- --
     @xmath   
  -- -------- --

Now it follows from the weighted @xmath estimates and Schauder’s
estimates that there is a constant @xmath independent of @xmath such
that

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

Therefore

  -- -------- --
     @xmath   
  -- -------- --

By Lemma 4.7 the sum over all @xmath is bounded independently of @xmath
, whilst @xmath . Therefore the full sum is @xmath and the estimate is
complete. ∎

### 4.4 Geometric Jacobi fields

In the next section we will complete our construction of a space of
annular cmc @xmath surfaces asymptotic to the ends of horizontal
catenoids. The space will be parametrised by the boundary data we are
able to prescribe when solving the mean curvature equation. As our
solution to the mean curvature equation is perturbative, this is
determined by the boundary data we can prescribe for the linearised
equation. Propositions 4.11 and 4.12 only allow us to prescribe the
boundary data on the ”high modes”, that is, on the image of @xmath .
Thus we have no analytic control over the boundary data on the low modes
@xmath and @xmath that span the kernel of @xmath .

It turns out, however, that one has some geometric control over these
low modes, thanks to certain Jacobi fields. These Jacobi fields are
solutions to the homogeneous equation @xmath which arise from geometric
deformations of the horizontal catenoids through families of cmc @xmath
surfaces. Any such deformation has a variation vector field whose normal
component gives a Jacobi field. There are five independent Jacobi fields
that arise in this way - four from the four-dimensional isometry group
of @xmath , and one from varying the necksize parameter in the family of
horizontal catenoids. We call these geometric Jacobi fields .

In principle, one can compute explicit formulae for these Jacobi fields
by computing the variation vector field of each of these deformations,
and taking its inner product with the normal on the horizontal
catenoids. In practice, however, the formulae one obtains are very
complicated. However if one works directly with the equation @xmath one
obtains fairly explicit, and rather simple Jacobi fields on the low
modes.

###### Lemma 4.13.

The Jacobi fields on the low modes are given by the functions

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

Here @xmath are solutions to the ode

  -- -------- --
     @xmath   
  -- -------- --

such that @xmath is even, and decays at the rate @xmath as @xmath ,
whilst @xmath is odd, and grows at the rate @xmath as @xmath .

###### Proof.

The proof that these functions satisfy @xmath is a simple calculation
using Lemma 4.8 and the well-known geometric Jacobi fields of Euclidean
catenoids. ∎

The analogy with Euclidean catenoids, and an examination of the zero
sets and decay rates of geometric Jacobi fields, strongly suggests that
all but one of these solutions are in fact a geometric Jacobi field. We
suspect that

  -- -------- --
     @xmath   
  -- -------- --

arise from hyperbolic dilations, and varying the necksize parameter,
respectively, and that

  -- -------- --
     @xmath   
  -- -------- --

arise from hyperbolic translations and rotations, respectively. On the
other hand, the proof of Lemma 2.12 computes the Jacobi field arising
from vertical translations to be exactly @xmath . The only unexplained
Jacobi field is @xmath - we are not aware of any geometric deformation
that could be responsible for this Jacobi field.

If one grants that these Jacobi fields are indeed the geometric Jacobi
fields which we claim they are, then the next result shows that the
horizontal catenoids are non-degenerate, in the sense that there are no
decaying Jacobi fields that do not arise from geometric deformations.

###### Theorem 4.14.

Let @xmath , then the Jacobi operator

  -- -------- --
     @xmath   
  -- -------- --

has a two-dimensional kernel spanned by

  -- -------- --
     @xmath   
  -- -------- --

(Here we do not impose any symmetry requirements on the functions in
these spaces).

###### Proof.

Suppose that @xmath , for @xmath . Expanding @xmath as before, as

  -- -------- --
     @xmath   
  -- -------- --

the pde reduces to the odes

  -- -------- --
     @xmath   
  -- -------- --

Now for all @xmath we multiply through by @xmath and integrate over
@xmath . Given the exponential decay of @xmath we may use integration by
parts to find

  -- -------- --
     @xmath   
  -- -------- --

By Lemma 4.7 we have @xmath for all @xmath , therefore the integrand is
non-negative, and @xmath vanishes identically for all such @xmath .

When @xmath , the functions @xmath are linear combinations of the known
Jacobi fields of Lemma 4.13 . Only two of these Jacobi fields are
exponentially decaying on the ends, namely @xmath and @xmath , and this
proves the theorem. ∎

Lastly we would like to motivate why we introduced the symmetry
requirement in the results of the linear analysis in the preceding
sections. To produce a family of surfaces that can be used in a gluing
construction, one would like to have two free parameters for each of the
low modes, these parameters being determined by geometric manipulations.
Intuitively, two parameters is the correct number, as one wishes to
match the Dirichlet and Neumann data on each mode for a smooth gluing.
With each of the modes @xmath and @xmath , there are two geometric
deformations that provide the two parameters. However, on the @xmath
mode the absence of a second geometric deformation leaves us a parameter
short of being able to specify the Cauchy data.

Now @xmath is not invariant under reflection in the horizontal plane
@xmath . So by imposing this reflection symmetry we can eliminate the
degeneracy on this mode. Bearing in mind future applications to gluing,
this was our reason for requiring this symmetry in our results above.

## 5 Construction of cmc @xmath ends asymptotic to horizontal catenoids

With Propositions 3.1 , 4.11 and 4.12 in place, the proof of our main
result is a straightforward matter.

###### Theorem 5.1.

There exists @xmath , such that for all @xmath the following holds. If
@xmath and @xmath , then there is a solution @xmath to the boundary
value problem

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

By Propositions 3.1 and 4.1 the boundary value problem we wish to solve
takes the form

  -- -------- --
     @xmath   
  -- -------- --

We take as an approximate solution the function @xmath furnished by
Proposition 4.11 . Thus @xmath satisfies

  -- -------- --
     @xmath   
  -- -------- --

Now we look for a solution of the form @xmath , where we require @xmath
to solve

  -- -------- --
     @xmath   
  -- -------- --

Using the ’inverse’ @xmath to @xmath established in Proposition 4.12 ,
this boundary value problem for @xmath is equivalent to @xmath being a
fixed point for the operator

  -- -------- --
     @xmath   
  -- -------- --

That such a fixed point exists (and is unique), for all sufficiently
small @xmath , follows from the next two inequalities. We claim there
exist constants @xmath independent of @xmath such that for small @xmath
,

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (40)
     @xmath   @xmath   @xmath      (41)
  -- -------- -------- -------- -- ------

for all @xmath . Indeed, it follows from these that @xmath is a
contraction map, mapping the ball @xmath into itself, for all
sufficiently small @xmath .

Now we turn to the estimates ( 40 ), ( 41 ). First note that @xmath by
Proposition 4.11 , because @xmath . Since @xmath , we get @xmath . For
the rest of the proof we’ll just write @xmath in place of the weighted
norms @xmath and @xmath ; it should be clear which norm is used in each
term.

The estimate in Proposition 4.12 and the fact that @xmath on the end
imply

  -- -------- --
     @xmath   
  -- -------- --

For the Lipschitz estimate ( 41 ), let @xmath and begin with

  -- -------- --
     @xmath   
  -- -------- --

Then @xmath as before. Now the Lipschitz estimate for the error in
Proposition 3.1 gives us

  -- -------- --
     @xmath   
  -- -------- --

Estimating the local norms @xmath in terms of the weighted norms it
follows that

  -- -------- --
     @xmath   
  -- -------- --

Hence

  -- -------- --
     @xmath   
  -- -------- --

Putting this all together gives ( 41 ) and completes the proof. ∎

## 6 A proposed gluing construction

As discussed in the introduction, the construction of spaces of ends is
an important ingredient in gluing constructions by Cauchy data matching.
There is one particular gluing construction that we’ve had in mind, and
which might be possible using our results. It follows from Lemma 2.16
that the horizontal catenoids converge, in the small neck-size limit, to
two horocylinders intersecting tangentially in a vertical line. This
suggests that configurations of mutually tangent horocylinders may be
desingularised, by perturbing each end to an end asymptotic to some
horizontal catenoid. In fact, [ 12 ] showed that if a cmc @xmath end is
asymptotic to a horocylinder, then it lies in that horocylinder. Thus in
any scheme to desingularise configurations of horocylinders, the ends of
the resulting surfaces cannot be asymptotic to horocylinders, and
horizontal catenoids are natural asymptotic models.

In this final chapter we describe the gluing construction that we have
pursued to produce large families of cmc @xmath surfaces that are
symmetric with respect to a reflection in @xmath . These include
examples with arbitrary positive genus @xmath and @xmath ends. We also
highlight the technical obstacles to executing this program that we’ve
encountered.

### 6.1 Outline and obstacles

We start by outlining the approach we have pursued to produce cmc @xmath
surfaces with genus @xmath and @xmath ends. One could also try to
produce surfaces with a wider variety of topologies, for instance, with
@xmath ends for any @xmath . However, the @xmath case already contains
the obstacles we have not been able to resolve, so we will restrict the
discussion to that case.

#### Admissable configurations of tangent horocylinders

We consider admissable configurations of horocylinders, defined as
follows.

###### Definition 6.1.

A configuration of @xmath horocylinders is admissable if

-    their union is a connected set,

-    any two horocylinders that intersect do so tangentially, and

-    every horocylinder intersects at least two others.

When @xmath , an admissable configuration is unique up to an isometry of
@xmath , but when @xmath is large there are many different ones. Every
admissable configuration of @xmath horocylinders is constructed from a
configuration of @xmath horocylinders by adding a horocylinder that
intersects exactly two others. Hence, the total number of intersections
in an admissable configuration of @xmath horocylinders is @xmath .

The gluing scheme we outline desingularises a configuration by replacing
each of the @xmath horocylinder ends with an end of a horizontal
catenoid, and each of the @xmath intersections with a catenoidal neck.
Each time an end is added to a configuration, the desingularisation
produces two closed loops around the resulting necks, and one of these
may be removed without disconnecting the surface. Hence, the genus
increases by one each time the number of ends increases by one, and the
result is a cmc @xmath surface with @xmath ends and genus @xmath .

#### Summands

If one considers the cmc surfaces we aim to construct, each end is
connected to at least two others by catenoidal necks. However, the ends
we have constructed have only a single boundary component, so we require
a further type of surface in the construction. The three families of cmc
@xmath surfaces that are required are

-   annular ends asymptotic to an end of a horizontal cateniod,

-   certain compact horizontal graphs that are close to horocylinders
    and have multiple boundary components,

-   compact annuli with boundary that are close to the necks of
    horizontal catenoids.

We require that all of these families are symmetric with respect to a
reflection in @xmath .

The existence of the first of these families was proved in Theorem 5.1 ,
and it should be possible to construct the last family by similar
methods. The construction of compact horizontal graphs is carried out
below, in Theorem 6.2 . For now we continue the discussion with a
description of this family and the role it plays.

We propose to do the gluing in two steps. First, one would use surfaces
from the families (a) and (b) to produce surfaces that have multiple
boundary components, and one end asymptotic to an end of a horizontal
catenoid. Second, one would glue together such pieces using the family
(c). In the first step, the Cauchy data matching required to glue
surfaces has presented difficulties that we have not been able to
resolve. We describe our strategy to achieve the first step, and these
difficulties, in what follows.

Suppose we have an admissable configuration of horocylinders @xmath .
For the remainder of the discussion we fix a horocylinder @xmath . Let
@xmath be the number of horocylinders that are tangent to @xmath , so
@xmath . We wish to replace @xmath with a family of cmc ends that are
asymptotic to a catenoid end and have @xmath boundary components. The
family is to be parametrised by the boundary data on the @xmath
components.

To do this we first apply an isometry of @xmath that identifies @xmath
with the horocylinder @xmath . Then we take @xmath to be a domain in the
@xmath plane of the form

  -- -------- --
     @xmath   
  -- -------- --

where @xmath are the coordinates of the intersections of @xmath and
other horocylinders, the @xmath are chosen so that the balls @xmath are
disjoint, and finally @xmath is chosen sufficiently large that @xmath
contains @xmath . It will be convenient to decompose @xmath into the
outer and inner boundaries defined as

  -- -------- --
     @xmath   
  -- -------- --

Similarly we decompose @xmath into its factors on the outer and inner
boundaries as @xmath .

Now Theorem 6.2 below ensures that, given any sufficiently small @xmath
, there exists a cmc @xmath horizontal graph on @xmath which equals
@xmath on @xmath . The boundary data @xmath is prescribed on a scale of
@xmath that matches that of the horizontal catenoid on @xmath , and is
slightly smaller on the @xmath components of @xmath . Thus, the
expansion ( 14 ) for the boundary curve on a catenoidal end requires
@xmath to be of the order @xmath , hence @xmath should be @xmath , and
perhaps simply @xmath .

To produce the desired family of surfaces consider arbitrary Dirichlet
data @xmath on the scale just described. Fixing @xmath and allowing
@xmath to vary gives a family of graphs parametrised by their boundary
values on @xmath . If we can match the Cauchy data on this boundary with
that on the boundary of an end, then we will have produced families of
cmc @xmath surfaces with ends asymptotic to an end of a horizontal
catenoid, and parametrised by boundary data @xmath .

#### Cauchy data matching

To match Cauchy data on the outer boundary of a horizontal graph and the
boundary of an end, we must find a pair of surfaces, one from each of
these families, whose Dirichlet and Neumann data agree on this boundary.
If the surfaces match to zeroth and first order, then the resulting
surface is a continuously differentiable weak solution to a cmc @xmath
equation, and therefore smooth by elliptic regularity.

Let @xmath be as above, and @xmath be boundary data on the boundary of
an end. We denote their boundary derivatives by @xmath and @xmath (with
compatible orientations of the boundaries). Then we are trying to show
that for all small @xmath , the map

  -- -------- --
     @xmath   
  -- -------- --

has a zero. Ensuring that the first factor is zero poses no difficulty;
the issue is to find data for which this holds, and the boundary
derivatives agree.

To do this we need to understand the Dirichlet-to-Neumann operators for
the mean curvature equations that we have solved to construct the
surfaces in each family. Since we are only interested in solutions to
these non-linear equations that are small and have been produced
petrubatively, it should be sufficient to understand the
Dirichlet-to-Neumann operator for the linearisations of the mean
curvature equations. Thus far, however, we have not been able to obtain
a sufficiently refined understanding of the Dirichlet-to-Neumann maps
for the Jacobi operators.

### 6.2 Existence of cmc @xmath horizontal graphs close to
horocylinders.

Recall that in the upper half space model of @xmath , a horizontal graph
is defined as a graph of the form @xmath . A horizontal graph has mean
curvature @xmath if and only if @xmath , where the operator @xmath gives
twice the mean curvature on the graph of @xmath . It is given by ( [ 12
] )

  -- -------- -- ------
     @xmath      (42)
  -- -------- -- ------

with

  -- -------- --
     @xmath   
  -- -------- --

Note that the constant functions @xmath corresponding to the
horocylinders satisfy @xmath . In particular we would like to consider
the solution @xmath , to which the end of a horizontal catenoid
converges (Lemmas 2.16 , 2.18 ).

A straightforward calculation shows that the linearisation of the mean
curvature operator about this solution is the flat Laplacian:

  -- -------- --
     @xmath   
  -- -------- --

Therefore cmc surfaces in narrow slabs can be constructed using the
Implicit Function Theorem and classical results on the Laplace operator:

###### Theorem 6.2.

Let @xmath be a smooth, bounded domain in @xmath . Then for all
sufficiently small @xmath , there exists a unique solution to the
boundary value problem

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Define @xmath by

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the unique harmonic function on @xmath that equals
@xmath on @xmath . Now @xmath and, as above, the linearisation of @xmath
(in @xmath ) about this solution is the Laplace operator

  -- -------- --
     @xmath   
  -- -------- --

It’s a classical result that this last mapping is an isomorphism ( [ 9 ]
), so the theorem follows from the Implicit Function Theorem. ∎