#### Multi-Threading Classes

Most of the programs used in the framework consist of multiple threads
for two reasons. Primarily, the motivation is to prevent programs from
blocking completely when one part of a program blocks. This might occur
when a system function used to communicate with another program blocks,
for example because the communication partner has terminated. If on the
other hand the program is multi-threaded, only the thread executing the
communication can block. Care has to be taken of course that no other
thread in the program will block because of waiting for a certain
condition in the communication thread.

To facilitate multi-threaded programming and especially communication
between threads in one program, several classes have been implemented in
the MLUC library. MLUCThread , the first of these, has the purpose of
handling of the threads themselves, e.g. starting and stopping. It uses
the POSIX threads ( pthreads ) API [ 119 ] , [ 120 ] , [ 121 ] , [ 122 ]
and encapsulates it into a class providing methods for starting and
aborting threads. An abstract (pure virtual) member function Run
declared in the class is called when a thread is started, serving as the
actual thread function, so that the thread is terminated when the Run
function ends. Creating a new thread as a consequence involves deriving
a class from MLUCThread and overwriting the Run method with the code to
be executed by the thread. For integration with functionality in other
classes, the template class MLUCObjectThread has been derived from
MLUCThread . It uses another class type as its template parameter,
accepting an object and a member function of that class in its
constructor. In its Run method the specified member function of the
given object is called, making that method the actual thread function.

For purposes of signalling between threads the MLUCConditionSem class
implements a condition semaphore, also called a signal. Waiting for
signals from other threads is supported by the class either with a
specified timeout or without. When a timeout has been specified, the
Wait function returns even when no signal has been received. Otherwise
it will wait indefinitely for a signal to arrive before returning. To
prevent race conditions between waiting for a signal and signalling, the
MLUCConditionSem uses an internal mutual exclusion semaphore (mutex). It
is acquired by default and is released atomically before a wait is
entered. When an attempt is made to signal a thread waiting on this
object, the mutex is tried to be locked as well. If no thread is waiting
for a signal on the object, this will cause the signalling thread to
block until another thread calls one of the object’s Wait functions. To
support longer processing sections between waits, without blocking
signalling threads, the class supplies two member functions for manual
locking and unlocking of its internal lock.

In addition to these signalling features the MLUCConditionSem class also
supports a notification data structure. This queue consists of a list of
64 bit data items. Items are added to the end of the list and queried or
removed from its beginning. Using the queue makes it possible to provide
a thread with a list of items to be processed by signalling it whenever
a new item has been added to the list. To ensure thread safety while
maintaining efficiency the notification data is protected by a separate
lock, distinct from the internal lock associated with the signal/wait
functionality. Internally the class uses the MLUCVector class covered
later in this section, making use of the provided efficiency features of
that class.

For inter-thread communication where exchanging single 64 bit values is
not sufficient, a second First-In/First-Out (FIFO) communication class
is available. The MLUCFifo class offers an interface for the exchange of
data of any size between multiple threads. For efficiency reasons the
interface is optimized so that it is not necessary to have the data
available for a write call to be copied into the FIFO. Instead a
location of a specified size is allocated in the FIFO and the pointer to
that location is returned. The thread writing into the FIFO can then
write its data directly without having to store it in a temporary
location and copying it from there into the FIFO. After writing the
thread calls a commit function that updates the FIFO’s internal tables,
exposing the written data into the FIFO as available for reading. For
thread safety the allocation and commit member functions of the class
also perform locking/unlocking respectively so that only one thread at a
time is able to write into it.

Reading works in a similar manner: When data is available for reading
the responsible member function returns a pointer to the start of the
available data. After the reading thread has finished processing the
data it calls another function to free the accessed data. The free call
also updates the object’s internal tables, marking the freed space as
being available again for writing new data. Similar to writing, reading
also involves a locking mechanism ensuring that no two threads can
access the data simultaneously. To allow concurrent reading and writing
separate read and write mutexes are used.

An MLUCFifo object contains two buffers for data, an ordinary and an
emergency one, where the emergency buffer is typically rather small. On
writing data it is possible to specify into which of the two buffers the
data should be stored. On reading this is not possible, instead the
emergency buffer is always checked first for the availability of data,
and only if it is empty is the ordinary buffer checked. This mechanism
ensures that it will always be possible to send high priority messages
to a thread.

A FIFO’s size is initially set to a power of 2 and can be resized by
doubling its size if necessary. If the amount of the buffer used drops
below a specified threshold, e.g. a quarter of its size, the buffer is
reduced again to half its size. These measures ensure that a buffer will
not suffer from a yo-yo effect of constant expanding and shrinking when
it is used around a resizing threshold. If the resize ability of a
FIFO’s buffer is not desired, then it is also possible to disallow
resizing completely. In such a case writing to a full buffer will fail.

##### Timer Classes

In multi-threaded systems it can be necessary that threads wait for a
specified time while still being interruptible during the wait. When the
thread itself knows for how long it needs to wait, then the timeout Wait
method from the MLUCConditionSem class can be used. If however the
thread itself does not know how long it needs to wait, then some other
method must be used. To solve this problem MLUC provides the MLUCTimer
and MLUCTimerCallback classes.

The MLUCTimer class allows to set timeouts associated with a specific
instance of a class derived from MLUCTimerCallback . MLUCTimerCallback
is an abstract base class consisting of just one abstract member
function, TimerExpired . When a time set in MLUCTimer has passed the
TimerExpired function in the specified instance of the MLUCTimerCallback
derived class is called. To provide more information about the timer
that has expired, a 64 bit value, that can also hold a pointer, can be
passed to MLUCTimer when the timeout is started. This value is then
subsequently passed to the TimerExpired function as well. Additionally,
it is possible to remove set timeouts before their expiration and to set
new waiting times for active timeouts.

Internally the MLUCTimer class uses a thread class in which the main
timer loop runs. This thread also calls the TimerExpired functions of
the objects registered for each timeout. Implementations of these
functions as a consequence have to fulfill two requirements. Firstly,
since the function is called in most cases from a thread different from
the one in which the timeout was set, it has to be ensured that the
function is thread-safe and that all data accesses are properly
synchronized by mutex locks. Secondly, the function should not take too
long or even block, as this could slow or even stop the complete timer
loop and its functionality.

To address both of the above issues and also make the timer
functionality better accessible, a third specialized class has been
developed. The MLUCTimerSignal class is derived both from
MLUCTimerCallback and MLUCConditionSem described earlier. Its
implementation of the TimerExpired function, inherited from
MLUCTimerCallback , adds the supplied 64 bit data value to the
notification list inherited from MLUCConditionSem and calls that class’s
Signal function. By using the class it thus becomes easy to have a
thread wait for events from the timer and other sources at the same
time.

##### Monitoring Classes

Monitoring of a cluster node’s parameters is a functionality not
especially needed for a data transport framework, but is useful in many
other applications. A class hierarchy in MLUC allows to monitor many
relevant system parameters through a common interface, e.g. CPU load,
network throughput, or hard disk throughput. At the base of this
hierarchy is the MLUCValueMonitor class that declares an abstract method
GetValue to read out a 64 bit large system parameter and allows to
specify a description for that parameter. Derived classes overwrite the
GetValue method to read out and return specific system parameters.
Implementations exist, amongst others, for reading out different CPU
usage values, incoming and outgoing network traffic, separately on each
network interface or globally for all interfaces, and for measuring the
amount of data read from and written to hard disks.

In addition to the basic functionality of reading out these parameters
the MLUCValueMonitor class hierarchy also contains methods to calculate
averages of the last values read for each parameter, to print the values
to standard output, and to write each read value to a file together with
a timestamp. Especially the last capability has been very valuable for
performance and correlation analysis of programs used in the framework.

##### The Tolerance Handler Class

The MLUCToleranceHandler class is used internally in some of the fault
tolerance components presented in section 7.5 . It is able to manage a
given number of items that can be either functional or non-functional,
for example corresponding to processing nodes in the HLT. For a task,
identified by a 64 bit index number, one of the items to which it is
assigned can be determined. When all managed items are functional the
worker item is obtained by a simple modulo operation on the task’s index
number with the total number of items available (functional and
non-functional).

When one or more of the items are non-functional, additional steps have
to be taken. Using the above modulo operation’s result, a check is
always made whether the item found is functional or not. If it is
functional the task is assigned to it. Otherwise the next step is taken.
The task’s index number is divided by the total number of items
available. A second modulo operation is performed on this division’s
result, based upon the number of functional items. The number obtained
from this operation is used as the index for a map array. In this array
the indices of the available functional items are contained. From the
array the item to which the task is assigned is determined by the second
index. Fig. 4.8 shows the principle for five items, on the left with all
items functional and on the right with item 2 non-functional.

With the above rules, tasks assigned to functional items are not
affected, while all tasks that would have to be processed by
non-functional items are redistributed among the available ones. To show
that in the case of errors the distribution is done evenly, a small test
program has been written that simulates a number of item errors and
fills histograms for each item with the number of tasks assigned to it.
Sample distributions for a number of parameters are shown in Fig. 4.9 .

#### 4.2.2 Functionality Replacement Classes

##### The MLUCVector Class

In the framework many uses of a dynamic array class involve an almost
FIFO-like behaviour where new values are added to the end of an existing
list. Access and removal of values, however, is not always strictly from
the beginning but can in extreme cases be from the end as well.
Typically though, removal is done from the first few elements of a list.
Due to this non-strict removal from the head of the list, the queue
class from the C++ Standard Template Library (STL) is not suitable, nor
is any other queue class. A dynamic array class, allowing random access,
is used instead. Although the STL list class is also usable in
principle, tests have shown its performance to be slower than the vector
class, probably because of the large number of element allocation and
deallocation operations performed.

Unfortunately the STL vector class has a major drawback in this usage
pattern. Whenever an element is removed all elements located after it in
the list are shifted one slot forward. With a potentially very large
number of events in the system coupled with a high rate this leads to a
large number of copying operations that have to be executed in a node.
For example, with event sizes of 8 kB, an event buffer size of 256 MB,
an event descriptor size of 32 byte, and an event rate of 200 Hz, about
200 MB/s will be copied in memory just as a result of handling a list of
event descriptors. To overcome this problem the new dynamic array class
MLUCVector has been designed as a replacement class for the MLUC
library.

Like the STL vector class the MLUCVector class is a template class with
the template parameter defining the type of data stored. Unlike the STL
class the MLUCVector uses a preallocated array as a ring buffer with a
number of elements equal to a power of 2. In addition to the array for
storing the contained elements themselves another array for a similar
number of boolean elements is used to specify the validity of each
element slot in the primary array. If the valid flag for a corresponding
element slot is false, the slot is unused.

The advantage of using powers of 2 as sizes for a ring buffer is an easy
wrap around handling. All operations on indices, e.g. the increment of
an index for looping over all contents, are followed by applying a
bitwise AND operation with a specific mask. This mask is the number of
available elements minus one, corresponding to set bits for all valid
indices in the buffer. Indices that have become too high or low by a
previous operation are thus automatically truncated back to valid
values. As the AND operation is typically very cheap, it often can be
executed in one clock cycle by current processor types. It often is even
cheaper to apply than to use the ordinary check for index wrap around.
The combination of a comparison (or subtraction) operation coupled with
a conditional jump that corresponds mostly to this check make these
instructions typically at least as expensive as the AND operation
applied in this class.

When an element is added to an MLUCVector object, it is inserted at the
position specified by the end index of the ring buffer which is
subsequently increased by one. In addition, the valid flag for that
location is set to true, indicating that the slot is now used. For read
accesses the index of a specific slot may be given, allowing random
access to each element. To search for elements corresponding to specific
criteria search functions are available that iterate over all valid
elements using a caller supplied callback function for element
comparison.

Removal of elements is done by specifying ring buffer indices, whose
corresponding valid flags are then set to false. If the element to be
freed is the first or last element, the appropriate index is increased
or decreased respectively. When invalid elements are adjacent to such a
freed boundary element the corresponding index is increased until a
valid element is encountered.

When no free space remains between the ring buffer’s end and start
indices two different actions are possible. If the ring buffer contains
invalid elements, the buffer is compacted by shifting the valid elements
together. After this operation the ring buffer consists of two separate
blocks, containing all used and all unused element slots respectively. A
buffer without invalid elements can be resized by doubling the size of
its internal buffer arrays. Moving of elements can be necessary after a
resize if the used element block wraps around the end of the buffer. In
this case it has to be moved to the new end of the enlarged buffer. As
soon as the number of used slots in a previously enlarged MLUCVector
object drops to less than a quarter of the available slots the buffer is
compacted again and its size is halfed. A buffer is never shrunk below
its originally specified size. If resizing is not desired or necessary,
then it is possible to set a flag in the constructor that inhibits
resizing, both growing and shrinking, for the object concerned.

With the described features of the MLUCVector class it is ensured that
the available resources, especially memory bandwidth, are used optimally
for the dominant access pattern specified above. Correspondingly the
change from the STL vector class to the MLUCVector class has brought a
significant increase in the framework’s speed.

##### Allocation Cache Classes

To avoid copying large amounts of data, or small amounts very often,
many parts of the framework only store pointers to data instead of the
data itself. Only these pointers are passed between functions or
different threads. This approach, however, has another problem of
frequently issued allocation and deallocation calls, that usually are
costly as well if used in such large numbers. In response to this
problem two more classes, MLUCAllocCache and MLUCObjectCache , have been
introduced into the MLUC library to prevent these frequent calls to the
memory subsystem.

Both classes allocate a specified amount of elements on creation and
store them in a pool of available elements. Instead of calling the
normal memory allocation routines, e.g. malloc for C or new for C++, a
program calls the allocation routine of one of these classes. This
routine checks whether there is at least one element available in its
pool and returns a pointer to the first available element if this is the
case. The element’s pointer is then removed from the available pool and
stored in a list of used elements. If the pool of preallocated elements
is exhausted, the allocation objects use the system allocation routines
to obtain the requested element. A pointer to this allocated element is
stored in another list for additionally allocated elements.

When an element can be freed again the allocating object’s release
function is called. If the element was allocated additionally, because
the preallocated pool was exhausted, it is removed from the list it was
stored in and is freed again, using the appropriate system deallocation
call. For elements that originated from the preallocated pool, the
pointer to the element is removed from the list of used elements.
Subsequently it is reinserted into the pool to make it available for
further use.

For both of these classes the amount of elements to preallocate is
specified as a power of 2 and the MLUCVector class is used internally to
store all element lists. The lists of available and used pool elements
are both presized to the number of preallocated elements so that from
the beginning both lists have enough space available to store all pool
elements. No resize will be necessary for them. By using the MLUCVector
class in this manner, the two classes benefit from its low overhead
features and can handle operations on their internal lists efficiently.

The MLUCObjectCache class is a template class with the template
parameter defining the type of elements for which the allocation object
functions as a cache. Objects of the given type are preallocated,
including executing their default constructor, and are stored in the
allocation object’s pool. When a pool object is released again, it is
not directly reinserted into the pool. Before the insertion one of its
methods, ResetCachedObject , is called. This method’s task is to reset
the object into a clean state, corresponding to its state immediately
after creation, ensuring that a used object can be reused by the calling
program without a check for a usable state. A consequence of this
mechanism is that the ResetCachedObject method must be present in each
class to be managed by an MLUCObjectCache instance.

In contrast the MLUCAllocation object manages only blocks of memory of a
specified size. The block size is specified in the object’s constructor
together with the amount of elements to be stored. Elements contained in
an MLUCAllocation instance are neither explicitly overwritten with
zeroes on creation nor upon release, again to save memory bandwidth. A
program using an object of the MLUCAllocCache class thus can make no
assumptions about the content of memory blocks received from it.

##### The String Class

Many pieces of code in the framework have to store a name or another
type of textual information. Compared to the traditional C handling of
the string type C++ string classes allow a significantly easier handling
of these texts. In conjunction with the STL string class the discussed
multi-threading of the framework poses a serious problem, as this class
uses internal static members, globally shared between all objects of its
type. Presumably for performance reasons these members furthermore are
not protected by mutex semaphores. These global data items can thus be
accessed and even changed simultaneously by multiple threads in a
program. This behaviour has led to several very hard to trace bugs
during the development of the framework until the real cause of the
problem was found.

To work around the problem a primitive replacement string class,
MLUCString , has been written and included in MLUC. This class is very
simple, providing only the most basic functions to ease handling of
textual data. Its function names also do not conform to the standard
string class functions, as the aim was not to provide a complete
reimplementation of the STL standard string class. Instead the decision
was made for consistency reasons to have this string class conform to
the naming conventions used in the MLUC class library as a whole.

## Chapter 5 The Communication Class Library

### 5.1 Overview

One of the concepts of the ALICE High Level Trigger is to purchase the
necessary components rather late, to take advantage of new technological
developments. This concept should not only be applied to the cluster
nodes themselves but to the network used for the interconnection of the
nodes as well. To be able to support this aproach and, of equal
importance, to maintain the generality of the framework, the
communication technology and protocol used for communication between
processes on different nodes have not been fixed. Instead a C++ class
library has been developed that exports an abstract communication API
with implementations for two network technologies. The API provided by
this Basic Communication Library (BCL) has been designed to be generic
and independent of any specific network technology. Despite this
generality the API has also been designed so that implementations are
able to make use of low-overhead, high performance, or efficiency
capabilities present in the respective underlying network technology or
protocol used. After evaluation the communication packages described in
section 2.1 have not been considered as a basis for communication in the
framework due to their different requirements, characteristics, and
intended uses.

The API is split up into two parts, each optimized for a different
communication pattern. Its first part is designed for the transfer of
small amounts of data corresponding to the sending and receiving of
short messages. For these small amounts of data the use of special
transfer types like DMA is typically too much overhead so that they
should be sent via Programmed I/O (PIO) transfers. The other API part is
designed for the transfer of Binary Large OBjects (BLOBs), large blocks
of data, in one transfer. For these block transfers any overhead needed
for special transfers, e.g. DMA, is considered to be negligible compared
to the actual transfer of the data. These special transfer mechanisms
are thus acceptable and even desirable if they provide a lighter load on
the host CPU and/or memory system.

To demonstrate the actual generality of the library’s API and also to
provide a usable communication subsystem, two implementations providing
the API’s functionality have been written. The first is based on the
Transmission Control Protol (TCP) [ 22 ] as the most widespread network
protocol. Its prime advantage is its availability on practically every
computing platform and that most Unix variants, including Linux, contain
a very robust and efficient implementation. For the hardware used with
this protocol, Fast or Gigabit Ethernet are very widespread, with very
cost-effective adapters being available for standard PC nodes. TCP’s
disadvantage is its high overhead compared to dedicated System Area
Network adapters, partly due to its design as a Wide Area Network (WAN)
protocol over unreliable connections. The second communication
implementation is provided, although only in a prototype form, for the
SISCI API [ 126 ] on top of Dolphin SCI SAN [ 13 ] , [ 90 ] interface
cards. These SCI adapters are shared memory interface cards with a
network bandwidth of more than 650 MB/s and latencies of below @xmath .
Their primary disadvantages are the comparatively high price, which
almost doubles the cost of a node compared to Gigabit Ethernet, and
their weaker default reliability. Unlike TCP the SISCI API does not
provide a reliable data delivery and packet loss has been observed
although the physical layer specification guarantees packet delivery.

Various computers exchanging data can organize that data in different
formats. The most common problem being the byte-order of stored
multi-byte integer values. To avoid this type of problem when
transporting data a number of helper classes and structures have been
created that enable automatic translation of data between different
storage formats. To take advantage of these capabilities the data types
concerned have to be declared using a special type definition language.
This language is then translated into normal C++ code by utility
programs provided in the library.

### 5.2 Communication Paradigms

Several of the design decisions and paradigms chosen for the Basic
Communication Library are different from design characteristics found in
common network APIs, notably the socket API used for TCP. To avoid
misunderstandings and confusion in later chapters these design decisions
will be presented here.

#### 5.2.1 General Design Features

A primary general design feature of the BCL library is that all data
transport operations, both for message and block communications, can be
executed with or without a previously established connection. In the
library support is provided for establishing connections between two
communication objects, but its use is not mandatory. If a data transport
to a remote partner is performed without an explicitly established
connection, then a connection will be set up implicitly for that
transfer if required by the underlying protocol. After the transfer has
completed the connection is terminated again. For a transfer to a remote
partner, to which a connection is already established, this connection
will be used to transport the data. The motivation behind this scheme is
that in large systems it might be necessary to exchange messages with a
large number of communication partners at a low rate. For these
infrequent exchanges it would be too much overhead to establish open
connections to all potential partners, if it would be possible at all
and would not be restricted by the operating system. A user application
should not be required to establish a connection manually for each of
these transfers as that would complicate the application’s program
unnecessarily. On the other hand, there may be frequent exchanges of
data with other remote communication partners. For these the overhead of
establishing a connection for each transfer has to be avoided, and a
connection should be established only once. Therefore both explicitly as
well as implicitly established connections are supported by the library.

One additional property supported for explicit connections is the
on-demand connection. This means that a connection is not actually
established immediately when the connection attempt is made. Instead the
connection address is entered into an internal connection list but is
marked as not yet established. When the first data transfer to this
address is started, the connection is checked and found not to be
established yet. As for an implicit connection, it is then established
as part of the send operation. Unlike in the case of implicit
connections, however, the connection remains established and is not
terminated at the end of the operation. Like other explicitly
established connections it has to be closed explicitly by the calling
program as well when it is not needed anymore.

Another decision for the library is partially influenced by the above
requirement for both connection-less and connection-based data
transfers. Prior to any data send operation, each communication object
must have been assigned its own receive address and must have performed
a bind operation on it in order to make its address available to
external programs. There are two reasons for this demand. The first of
these, derived from the support for optional connections above, is that
each program should be able to receive answers to messages it transmits.
For the connection-less send mode the receiver cannot use the
back-channel of a connection established from a remote object to it. In
some instances sending of data requires a unique identifier in the
system, e.g. to regulate access to a resource on a remote node. This
remote identifier is trivially obtained by using a valid receive address
for a specific network technology, which has to be unique by design. To
ensure that a given address is actually valid and therefore unique a
successful bind operation has to be performed on each communication
object’s address before it can send data.

For some types of connections it might become necessary to perform some
handshaking or negotiating before sending data even when using an
already established connection. One example is SCI where multiple
senders have to regulate access to a shared memory segment provided by a
receiver process. Only in cases where a point-to-point connection is
used between two communication partners, it is possible to avoid that
overhead for each send operation. In these cases an already established
connection can be locked and later unlocked by a sending object via two
library functions. Locking of a connection makes the receiver object
exclusively available to the locking sender object. No other sender
object can send data to this receiver object even if a connection to it
is established. This negotiation requirement is specific for ShM
networks like SCI. Therefore the functions for locking and unlocking do
not have to contain any functionality. A further function is provided so
that an application program can determine whether a given communication
object supports locking or not and can make use of the other functions
as appropriate.

With regard to the handling of errors the choice has been made for a
combination of return values and error callbacks. Every function in the
library’s API returns an integer value of zero on success and a
non-negative value describing the error that occured otherwise. These
error values are taken from the standard C errno.h header file with the
advantage that the preexisting C standard functions to convert the
integer values to error descriptions can be used directly without any
additional effort. In addition to these return values, that have to be
evaluated explicitly, another method of detecting errors is available
based on error callback objects registered with communication objects.
The callback classes are derived from one common base class, that
exports an interface of methods called for the various error types. If
an error occurs inside a communication object it calls the appropriate
function for all its registered callback objects. Parameters passed to
these error callback methods include a pointer to the originating object
and an integer value describing the error, identical to the error return
value returned by the function. In addition to these two basic
parameters further arguments are passed if necessary, e.g. the remote
address for a failed connection attempt. After all callback objects have
been called, the communication object’s function returns with the
integer error described above. Exception handling has not been chosen
for error handling to keep its use optional and not make it mandatory.
It can be used by providing a callback object that throws an appropriate
exception when one of its error functions is called. Beyond the callback
objects registered statically with each communication object most of the
communication objects’ function calls can accept an optional argument
representing a pointer to an error callback object to be used in
addition to the registered objects.

#### 5.2.2 Message Communication Design Features

For the design of the message communication classes and their interface
only a few design choices have been made. The primary design choice is
to base the design on the pattern of sending and receiving of messages
rather than on a stream of bytes, as e.g. for the standard socket API.
Send and receive calls can include a timeout to be applied to the
operation, which can be infinite. In most such cases, however, a fixed
timeout of the underlying communication technology used, will expire and
cause a system function to return with an error.

Another feature is actually more a requirement than a design decision.
As a calling program cannot know in advance the size of a message
received the allocation of the memory space for that message has to be
made by a communication object’s receive method. By extension it also
has to free the message again after the calling code has finished
processing it, for which there are actually two reasons. The primary
reason is that a calling program does not have to make any assumptions
about the memory allocation function, e.g. new , new [] , or malloc .
Therefore the library has the liberty to choose which function to use
and to change it without affecting a user’s program. The second reason
is that for some network technologies it might be possible to store a
message in an internal buffer which may even be done directly by the
network hardware. The object then justs returns a pointer into that
buffer without the steps of allocating memory and copying the message.
Such a message is not freed using any system function but instead by the
buffer management for the object’s internal buffer.

#### 5.2.3 Blob Communication Design Features

For the blob communication mechanism more characteristics have been
specified than for the message classes. Initially, a user may set the
size of the buffer where received data will be stored so that user code
can access it. For some of the blob communication implementations it may
in addition be possible to specify the receive buffer itself. However,
this feature might not be supported by a specific implementation of the
blob interface, one example is the existing SCI implementation. Although
it might not be possible to specify the receive buffer directly, a user
always has the possibility to obtain a pointer to the receive buffer
from the communication object. This enables it to write any received
data into the receive buffer directly, from where it can be accessed by
user programs. No additional copy step is required to copy the data from
the communication object. Instead a user can directly access data that
has been received from a remote node via the receive buffer pointer.

To enable this type of direct transfers it is necessary that the sending
node knows beforehand where the data should be stored in the receive
buffer and whether it is not already full so that the data cannot be
stored at all. For this the sending process is split up into two parts
each contained in its own function. In the first step an ID for the
transfer is obtained by specifying the size of the data to be
transferred. This transfer ID is queried from the remote receiving
object, using an optional timeout, and then passed back to the user
program. With this transfer ID the program can then use the second
function by supplying it with the obtained ID and a pointer to the data
to be sent. The communication object now has a receive buffer location
associated with this transfer ID, and transfers the data to that
location in the remote node. For this sending process a gather call is
available where the data to be transferred is collected from multiple
blocks scattered in memory. The size of data for which the transfer ID
is obtained must of course be the sum of the sizes of all data blocks.

A transfer ID obtained for such a transfer is not automatically
transmitted to the receiver object or program. Instead a user program
has to pass it explicitly to its communication partner, most likely by
using a message communication object. In the receiving program it is now
possible to use the transmitted transfer ID to get access to the
transferred data. Using the ID one can either obtain a direct pointer to
the data or an offset to the data from the start of the receive buffer.
With these informations a user program can access the data and process
it as required. Once this is finished another communication object
function has to be called to free the buffer block in which the data was
stored. This block is again identified by passing the transfer ID used.

For the communication required between two blob objects, e.g. to
negotiate a transfer ID, each blob communication object makes use of the
facilities offered by the message communication mechanism instead of
using an internal one. A message communication object is assigned to
each blob object to be available exclusively to that blob object,
implying that this message object must not be used by the user program.
The advantage of this approach, besides avoiding duplicate development,
is that the message communication may use another network technology
than the blob communication. One example is if two technologies exist,
one with low latency but comparably high overhead and the other with low
overhead and higher latency. In such a case the
high-overhead/low-latency technology could be used for the message
exchange and the low-overhead/high-latency one for the blob transfers.

To achieve an even lower overhead of sending with the avoidance of the
additional latency incurred by the negotiation phase before each
transfer a special approach can be taken. A sender can allocate a large
block of a remote buffer in advance by requesting a large transfer
block. This block could be as large as the whole buffer which is made
possible by using a function that queries a remote node’s receive buffer
size. The transfer ID of this block is sent once to the remote object,
which stores it for future use. From this point on the sending program
can perform a completely local buffer management in its obtained block
and only sends the offsets in that buffer to its receiving node. This
completely avoids waiting for reply messages from the receiver and
decreases the time overhead associated with each data transfer by twice
the message sending latency.

### 5.3 Auxiliary Classes

#### 5.3.1 Data Format Translation

One of the main problems encountered in network communication on
potentially heterogeneous systems is the different format of stored
data, most often encountered in the form of different byte orders for
integer data. To work around this problem, a helper hierarchy with one
base class and structure is included in the BCL. The base structure,
BCLNetworkDataStruct , provides a header for derived data to be stored.
Derived types are used to actually store the data. Code and meta-data
required to execute the translations is contained in the base class as
well as its derived classes. In the header three elements are stored to
provide information about a structure’s original data format at
creation, its current data format, and the total length of the data
structure. Since the native data format of any given system node is
trivially known it is always possible to convert data, described by this
format, into a node’s native format. By also supplying the data’s
original format, it becomes possible to handle data not directly under
the control of this mechanism as well. This cannot be done automatically
anymore though. Including the total length of the structure furthermore
enables all software stages to know how much data they have to handle
without having to know its actual content.

In the basic class BCLNetworkData a number of static member functions
are provided to transform integer data of different sizes between data
formats. For each of the integer sizes 16 bit, 32 bit, and 64 bit two
functions are available to convert the data. One can be used to convert
the data in place. The other one works with separate source and
destination, copying the data during the translation process. In
addition to these static functions, the class provides further functions
to aid the handling of different formats of data. Importing of data
structures into a class is supported by different methods either at an
object’s creation using its constructor or by calling member functions
later. At creation only two possibilities for copying the data into the
class’s internal data structure exist: transforming it to the node’s
local format in the process or copying the data untransformed. For an
existing object it is possible either to copy the data structure, as on
creation, or to adopt it by setting an internal pointer to the
structure. Both approaches can optionally be combined with the same data
transformation possible for the constructor. The advantage of the second
approach is that it avoids the overhead of copying data, which for large
amounts of data and/or high frequencies of transforming data can be
quite significant. After data from a network data structure has been
imported into an object, functions exist, that allow to transform the
data, either to its original format, the node’s current format, or a
user specified data format. In addition it is possible to query both the
data’s original as well as its current data format.

Data types to be managed by this mechanism have to be declared using a
very simple type definition language (vstdl), translated into normal C++
code by a program in the BCL library. The language supports only plain
8, 16, 32, and 64 bit sized unsigned integer types. Each structure type
must be derived from another vstdl type, at least from BCLNetworkData ,
as its two respective C++ elements contain the translation
functionality. A sample of the declaration of such a datatype is given
in Fig. 5.1 , showing the three size options available for a structure
element: A single scalar type, a fixed size array, or a variable size
array. Fig. 5.2 shows the C++ structure type generated from the previous
vstdl definition. As for the base BCLNetworkData types the generated
class and structure differ by the Struct modifier appended to the
structure’s name. The base name for both is the name specified in the
vstdl type definition and both are derived from the corresponding C++
type for the vstdl parent type. Transformation is performed in the
inverse inheritance hierarchy. A derived class first converts its own
elements and then calls its parent class’s transformation functions.

Structure elements of the variable size array type can only be contained
as the last element in a structure. Otherwise the C++ declaration of the
structure would have to allow moving subsequent elements due to the
array’s changing size, this however is not supported by the C++
language. This also implies that each structure can only contain one
such element. For these elements the array is preceeded by an
automatically generated member, holding the number of actual elements
making up the array, to allow for the correct handling of the changing
array size.

A sample hierarchy of three generated vstdl data types from the BCL is
shown in Fig. 5.3 , with the vstdl types on the left side, the generated
C++ classes in the middle and the generated C++ structures on the right.
The vstdl BCLNetworkData type displayed in the figure exists only
virtually, since only C++ class and structure exist for the represented
base type. Of the three vstdl types BCLAbstrAddress and BCLMessage are
directly derived from BCLNetworkData while the third type, BCLTCPAddress
, is in turn derived from BCLAbstrAddress . In the resulting C++ classes
and structures the hierarchy of the respective vstdl types is reflected
directly. Also displayed in the figure is the mutual dependency of each
type’s class and structure with the class directly containing an
embedded structure type as well as a pointer to the structure. This
pointer is used to access structures that have not been copied into a
class object but that have been adopted for efficiency reasons as
discussed.

Using the interface provided by BCLNetworkData and generated classes for
other vstdl data types, it becomes possible for programs to handle the
parts of data it needs, independent of the data format they were
originally stored in. This is achieved without having to write the
conversion code for every data type explicitly, which can instead be
written as a vstdl type definition, from which the necessary C++ code is
subsequently generated. One drawback of the mechanism employed is that
code working with C++ types generated from a vstdl data type definition
is unable to transparently handle derived data types as well. It will
always handle only the data elements it was compiled for. A more mature
and flexible data format conversion scheme might be implemented and used
in later versions of the library and framework.

#### 5.3.2 Address Classes

Addresses used in the BCL library are based on a vstdl type hierarchy,
with the abstract address type BCLAbstrAddress at its root.
BCLAbstrAddress is shown in Fig. 5.3 . It contains only an integer
element that defines the type of address in addition to the
BCLNetworkData inherited header. Each communication implementation
defines its own constant to identify its address type, e.g. 1 for SCI
and 2 for TCP. The address types for each network technology are derived
from the BCLAbstrAddress type, included are implementations of addresses
for SCI as well as for TCP.

SCI addresses include three 16 bit elements, the first of which is used
to identify the specific node concerned. It is unique to each SCI
adapter card, making it rather an adapter than a node ID but is
sufficient to identify a node. Which adapter in a node is used for
transmission is defined by the second number in the structure, this
number is required if multiple adapters are present in a node and is 0
otherwise. The third number finally holds the ID of the shared memory
segment used to receive the data in the target node and must be a unique
identifier in each node. For TCP the address structure contains only two
elements, a 32 bit field with the target node’s IP number and a 16 bit
number for the port that the receiving communication object uses. Fig.
5.4 shows the three different vstdl types used for address handling. At
the top is the abstract address type definition with its single element
to define the network technology supported. In the middle is the SCI
address type with its three described 16 bit numbers, and at the bottom
the TCP address with the IP and port number required for a TCP
connection.

#### 5.3.3 Message Classes

For the message communication mechanism the basic datatype used to
define the message header, BCLMessage , is derived from BCLNetworkData .
It is thus also based on the data transformation mechanism from section
5.3.1 . The first of its three fields contains an ID that defines the
type of the message, outside the scope of the library and under the
control of the application. Unlike this field the second field contains
an ID to identify messages, reserved for use by the library itself. For
the current implementations this is just a counter increased for each
message sent. The final field allows the specification of flags to
affect the sending of a message. At the moment, though, the field is not
used by the library and no flags are specified, neither general message
flags nor flags specific to an implementation of the message
communication interface. Fig. 5.5 shows the vstdl type definition of the
BCLMessage type with the three fields described.

#### 5.3.4 Error Callbacks

Error handling is performed partly by a set of callback object classes
derived from the abstract base class BCLErrorCallback shown in Fig. 5.6
. Instances of this or derived classes can be registered with
communication objects and provide a number of callback functions, called
by a communication object when the corresponding error has occured.

As can be in seen in Fig. 5.6 , the class contains callback methods for
different types of errors:

-   General errors, applying to any communication object

-   Send and receive errors for message communication objects

-   Prepare, send, and receive errors for blob communication objects

All methods accept at least a set of three common parameters: an
indicator for the error that occured, a pointer to the originating
communication object, and a pointer to an address structure involved.
Depending on the context this last parameter may contain either a local
address, e.g. on a bind operation, or a remote address, e.g. for a
connect or send operation. In addition to these common parameters more
may be accepted or required as appropriate for the error type that
occured. For message errors this is a pointer to the message concerned
by the respective error and for blob transfers it is the transfer ID.
The available callback methods are not declared as abstract methods.
Instead each is provided as a default implementation that only returns a
default value described below so that derived classes have to implement
only those methods whose functionality is needed.

The value returned by the callback functions is an action indicator
containing a suggestion from the callback object how to handle the
error. This action can have one of three values indicating either to
ignore the error, abort the operation, or make a retry attempt of the
failed operation. Since the value is only treated as a suggestion the
communication object can ignore the values returned by all callback
objects and proceed differently, as an action might not be possible for
a specific case. In the current implementation of the communication
classes, the error callbacks’ return values are not evaluated at all,
but the option to do so is already present for later implementations of
the library.

Next to the base callback class two more derived classes are contained
in the communication library, shown in Fig. 5.7 . The first of these,
BCLErrorLogCallback , calls the logging system of the MLUC library with
an appropriate error message constructed from its parameters. In the
other class, BCLStackTraceCallback , a set of system debugging functions
is used to dynamically obtain a trace of the current call stack. This
trace is then also passed to the MLUC logging classes. Beyond these two
included classes an application can also derive its own classes from
BCLErrorCallback to implement any error callback handling necessary.

#### 5.3.5 Address URL Functions

To support future additions of communication classes to the library
without the need to recompile programs using the library, a set of
functions is included in the library that allows to specify BCL
communication addresses in a Uniform Resource Locator (URL) like format.
Addresses in this format specify the network technology to be used,
whether the message or blob communication mechanism is to be used, and
the specific address information required by the technology concerned.
In this way a generic separation of address handling and network
technology has been introduced into the library. The supported abstract
address format supported is now essentially a string type. Fig. 5.8
shows the syntax for TCP and SCI addresses. As can be seen in the
figure, the elements of the URL specifying the actual address correspond
to the elements of the respective address structure types described in
section 5.3.2 .

Address URLs are processed by four functions in the library, two for
creating BCL objects and two for releasing them. Objects can be
allocated for either local or remote addresses by the BCLDecodeLocalURL
or BCLDecodeRemoteURL function respectively. For local addresses an
address structure is returned together with an appropriate communication
object of a class derived from BCLCommunication . Additionally, a flag
is provided indicating whether the returned object is a message or blob
communication object. To release allocated objects two BCLFreeAddress
functions are available, one to release only an address structure and
the second one to also release the communication object. A fifth helper
function, BCLGetAllowedURLs , is provided to aid in providing lists of
allowed addresses to program users. It returns two list of strings
containing valid message and blob address URL formats.

### 5.4 Communication Classes

The seven primary communication classes in the library are organized in
a tree hierarchy, displayed in Fig. 5.9 . At the root of this class
hierarchy is the BCLCommunication class, providing basic services and
declaring interface functions common to both of the previously described
communication types. Derived from this class are BCLMsgCommunication and
BCLBlobCommunication , which declare interfaces and implement common
services for the message-like and data-block communication mechanisms
respectively.

Two classes are derived from each of the two communication type base
classes to provide implementations for the TCP protocol using the
standard socket API as well as for the shared memory interconnection
technology SCI by Dolphin using the SISCI API. All four implementation
classes are designed to be able to make use of as many performance and
efficiency optimizing features as possible for the specific network
technology used. In addition to these primary communication classes two
separate classes, BCLIntTCPComHelper and BCLIntSCIComHelper , are
present, used by the two implementation classes for each network type as
shown. They supply functions and variables common to both communication
mechanisms but specific to each network technology. These two classes
are not intended to be used directly in a program but are for the
library’s internal use only as signified by the Int specifier in their
names.

#### 5.4.1 The Basic Interface Classes

##### The BCLCommunication Class

At the base of the communication class hierarchy is the BCLCommunication
class containing functionality common to all communication types and
mechanisms. Primarily, however, it defines the common interface for the
different communication types using abstract member functions. A UML
diagram of the class with the main features described in the following
paragraphs is shown in Fig. 5.10 . The main functionality contained in
BCLCommunication is the handling of the error callback objects, that can
be registered with each communication object. Two public functions are
provided, allowing to add or remove callbacks to a communication object
plus a number of protected methods for internal use by this or derived
classes. Each of these functions corresponds to one of the different
error functions exported by the callback interface. They are called when
an error occurs and in turn call the appriopriate error function for
each registered callback object as well as for the optional callback
parameter object supported by most functions.

Common interface parts defined by the BCLCommunication class include
functions for binding, connecting, locking connections, handling
addresses, and one function for querying whether a communication object
is a message or a blob communication object. For all functions any
address needed must be specified in the form of a base address structure
BCLAbstrAddressStruct or a pointer to it, to keep the interface generic
from a specific network technology. The actual address used still has to
be of the type required by the corresponding communication object itself
and is thus specific to the network technology supported by that object.

The Bind function defined in the interface requires one argument only, a
structure pointer holding the address to which the communication object
should be bound. Data sent to this address must be received by the
communication object, and the address therefore must be a valid address
for the network technology chosen. No parameter is required by the
Unbind function, which releases a bind to an address, as each
communication object has exactly one address it is bound to. Both
functions return an integer value to indicate the success or error
status of the operation as described previously. Unlike the Bind call
the Connect call exists in two versions: one using a millisecond
granularity timeout value and the other without a timeout. Both calls
require a remote address to connect to and a boolean stating whether the
connection should be established immediately or as an on-demand type
connection, as explained in section 5.2.1 . The supplied default value
of false for the boolean parameter specifies that the connection has to
be established immediately. As a communication object can be connected
to multiple remote communication partners the Disconnect function
requires the remote address of the connection to be terminated. Similar
to the Connect function, the Disconnect function also exists in two
variants, one with a milliseconds timeout and one without. All four
connection related functions return the standard integer error
indicator.

The following set of five functions is responsible for locking
connections, with the first of these functions allowing to query whether
the locking feature is supported by this object, which depends on the
network technology implemented. Two functions are available to lock an
established connection, one with timeout value and one without. Like the
Connect function both need the address of the remote connection partner
and return an integer error value. The two unlock functions are also
similar with respect to their required arguments. Each needs the address
and one of them allows to use a timeout value.

Support for handling network addresses in an abstract manner is provided
by the final set of functions defined by BCLCommunication . In
conjunction with the set of helper functions that allow to specify
addresses in a string similar to Internet address URLs, any user program
working with address classes needs to know as little as possible about
the format of the underlying addresses and the specifics of the network
technology used by a communication object. The first two of the address
support functions allow to query two values relevant for address
handling, namely the actual length of an address and the value of the
communication ID. These two values allow to identify whether a given
address belongs to a specific communication implementation. The address
length is also required for storing, copying, or allocating addresses.

Allocating memory for addresses is also supported by the second set of
two functions, that allow to allocate memory for an address and to free
an allocated address. The allocation function NewAddress returns an
address structure object with header fields and the communication ID
initialized to values appropriate for the communication object.
DeleteAddress frees the memory allocated for an address structure by
using the free call corresponding to the allocation call used in
NewAddress . By using these two functions it is ensured, that the
allocate and free functions always match, the correct amount of memory
is allocated, and the basic fields are initialized correctly. A fifth
address helper function is used to compare address structures for
identity, to support comparing of addresses whose exact type and
contents might not be known at compile time. Comparing two structures
bytewise relying on their length is always possible, but addresses with
different byte contents might point to the same remote address, e.g.
because of different byte orders. The final helper function provided by
BCLCommunication specifies whether a communication object is a message
or a blob communication object so that it is possible to distinguish
between these two sub-hierarchies in a generic manner.

##### The BCLMsgCommunication Class

The BCLMsgCommunication class is derived from BCLCommunication . It
defines the interface for the message communication mechanism, as shown
in Fig. 5.11 . This class provides almost no additional functionality
beyond that provided by BCLCommunication but defines the interface only.
The one supplied functionality is the implementation of the helper
function to distinguish between message and blob classes, which returns
the indicator for a message communication object so that derived classes
for specific network technologies do not have to implement this function
themselves. Primarily, the BCLMsgCommunication class defines the
interfaces for sending and receiving of messages on top of the basic
interface defined in BCLCommunication . Beyond these send and receive
calls a Reset call is provided, that serves to reset a message
communication object to a clean defined state. Resetting an object might
cause some previously received data to be lost.

For all Send functions two parameters are needed, the remote address
where to send the message to and the message itself. The address is
supplied as a pointer to a BCLAbstrAddressStruct object, and the message
is specified as a pointer to a BCLMessageStruct structure. This message
object can be of the actual BCLMessageStruct type, or it can be of a
structure type derived either directly or indirectly from
BCLMessageStruct . An optional third respectively fourth parameter is a
pointer to a BCLErrorCallback object. In the case of an error this
object’s error reporting callback functions will be called prior to
those of the communication object. In order to always provide a Receive
function with a message’s originating address, a Send function
implementation should send its address prior to the actual message data.
This might not be necessary, if the remote communication partner has
other methods of finding out the originating address of a received
message, but a Receive function must always be able to provide a
message’s source address.

The Receive functions also accept two mandatory and one or two optional
arguments. In analogy to the Send functions the Receive function’s
optional parameter is a callback object whose functions will be called
in addition to the ones of registered objects. As their first parameter,
both Receive functions use a pointer to a memory location where the
address of the sending communication object will be stored. A check is
executed whether the structure has the correct length expected for
addresses used by this communication object. If that size is incorrect,
the Receive call fails. The second mandatory parameter is a reference to
a BCLMessageStruct pointer, used to return the received message itself
to the calling program. Memory to store a message is allocated by a call
to NewMessage , another function provided as the declaration for an
abstract member function, to be implemented by derived message
communication classes. The function is declared as a protected member
function so that it is only available for internal use by other methods
of this or derived classes and not as an interface to programs.
Allocated memory for a message is filled with the contents of the
message received. The pointer to the new message is returned via the
reference parameter. It is not mandatory for the Receive calls to
allocate memory for the message data, it could for example also be
stored in an internal buffer, or it might be written directly into a
reserved buffer by the sender, like in the case of SCI. Here it would be
sufficient to return a pointer to this internal buffer memory.

A message that has been received and its memory been allocated, has to
be released again after the program has finished using the message’s
data. To retain flexibility in the way the messages are allocated in the
Receive functions, the BCLMsgCommunication class declares the ReleaseMsg
method for this purpose. It accepts a pointer to an allocated message as
its argument. This pointer is then passed to the protected member
function, FreeMessage . As is the case for NewMessage , FreeMessage is
also declared as a pure virtual member function to be implemented by
derived message classes.

##### The BCLBlobCommunication Class

As the BCLMsgCommunication class, the BCLBlobCommunication class shown
in Fig. 5.12 is also derived from the BCLCommunication class. It
contains functionality for the blob communication mechanism and also
declares an abstract interface for it. Functionality is provided by this
class mainly for the blob receive buffer, that stores data received from
remote nodes for access by the program. One further helper function in
the class is an implementation of the function from the BCLCommunication
class that identifies objects of this and any derived class as blob
objects. The main function for the receive buffer is used to specify the
buffer’s size and optionally the receive buffer directly by using a
pointer to the respective area of memory. Setting the buffer’s size is
always allowed, it is allocated immediately with the new size. If the
buffer was already allocated, the old buffer is released and a new one
allocated with the given size. Specifying the buffer to be used itself
in a parameter to this function is not guaranteed to be supported. This
depends on the blob implementation. For the SCI implementation, for
example, this is not possible, since received data is written directly
into the buffer by the remote sender program. As the current
implementation of the SISCI API for SCI does not allow to specify
arbitrary memory locations into which data can be written remotely, the
allocation of the receive buffer has to be done by the SISCI driver or
library. To allow the required overriding of the original SetBlobBuffer
function in BCLBlobCommunication by blob implementation classes the
function is declared as a virtual function. Two additional helper
functions are available to obtain a pointer to the buffer for accessing
received data and to query the buffer’s size.

As was pointed out in section 5.2.3 , the blob communication mechanism
has to exchange handshaking and control messages with a remote
communication partner using exclusively reserved message communication
objects. The specification of the message object to be used by a blob
object can be done either in a BCLBlobCommunication constructor or it
can be done later using a separate function. Both approaches use a
pointer to the message object set in the blob object. A helper function
allows to set the timeout used to wait for answers from the remote node
via the associated message communication objects. When replies are not
received within the specified timeout they are treated as errors. An
exception is the special timeout value of 0 which disables timeouts,
resulting in an infinite wait.

The blob communication interface, declared by the BCLBlobCommunication
class, consists of two parts, the interface for the sender to transmit
the data and the one for the receiver to access received data. Each of
the declared sending and receiving interfaces consists of three pure
virtual functions. Before a transfer is started, a negotiation has to be
performed first to determine where to store the data in the receiving
buffer, if it can be stored at all. This negotiation is performed by the
PrepareBlobTransfer function, that uses the address of the remote
message communication object associated with the receiving blob object.
It is not necessary for the object to know the address of the blob
object itself. The second parameter required by this function is the
size of the data to be sent, the data itself is not passed to the blob
object for the preparation of the transfer. Optionally, a timeout can be
supplied as well, in order to restrict the time that the function waits
for the reply from the remote message communication object. On a
successful negotation the function returns an ID that can be used
subsequently to execute the transfer. Using the returned transfer ID the
data can be sent to the remote blob object with the help of one of the
two available TransferBlob functions. Both functions require the remote
message address as their first argument followed by the transfer ID
returned from PrepareBlobTransfer . Two optional arguments, supported by
both functions, are a timeout and a pointer to an additional error
callback object. This object is used in addition to the registered
callback objects. By default the timeout is disabled through the value
zero and the default callback pointer is empty.

In addition to these common parameters the simpler of the two transfer
functions accepts three more parameters that describe the data to be
transmitted. Of these parameters the first is a pointer to the actual
data to be transmitted itself, followed by an offset holding the
location where this data block should be stored in the previously
reserved receive transfer area. The offset is specified relative to the
start of the reserved transfer area and can be used to write multiple
data blocks into a transfer area with multiple calls. This is needed
especially when a larger portion of the receive buffer is reserved in
advance, and the sender performs its own buffer management in that area,
as discussed in section 5.2.3 . As the function’s final parameter, the
data size is specified. The sum of the size and the data offset must not
exceed the size parameter passed in the transfer preparation function
call. Explicitly specifying the size parameter is done to allow
splitting a transfer over multiple calls in conjunction with the
function’s offset parameter. Unlike the first transfer call, the more
complex second one allows to perform a scatter-gather-transfer with one
call. Therefore the three parameters describing the transfer, data
pointers, offsets in the receive buffer, and block sizes, are not
specified as scalars but instead as three vectors, or dynamic arrays,
each containing multiple values. The arrays holding the pointers to the
data and data block sizes must contain the same number of elements for
the transfer attempt to be valid, but the array of offsets can contain
less values than the other two arrays and may even be completely empty.
If it contains less offsets than needed, the remaining offsets will be
calculated so that each block will be immediately adjacent to the
previous block. In particular this means that if no offsets are
specified the blocks will start at the beginning of the transfer area
and will then continue directly adjacent to form a single large block.
Essentially this provides a merging or coalescing functionality for the
scatter-gather-transfer.

Three functions make up the receiving interface of the class, each of
which uses only one parameter, the ID of a received transfer. One of the
functions, GetBlobData , returns a pointer to the beginning of the area
of the receive buffer. With this pointer an application program can
access the data that was received for the particular transfer.
GetBlobOffset , the second function, returns the offset to the
transfer’s block in the buffer, relative to the buffer’s start. Together
with the pointer to the receive buffer itself this can also be used to
get access to the received data. The final receive interface function,
ReleaseBlob , is used to release the buffer space occupied by a received
transfer. No method is provided by the blob communication interface to
send a transfer ID to a receiving node or to wait for a completed
transfer. This has to be done explicitly by a user program, for example
using a second message communication object.

#### 5.4.2 The TCP Communication Classes

The classes presented in this section implement the message and blob
facilities on top of the widely used Transmission Control Protocol (TCP)
[ 22 ] . Both primary classes BCLTCPMsgCommunication and
BCLTCPBlobCommunication for the two different communication mechanisms
make use of the services provided by a third class, BCLIntTCPComHelper .
Communication is based upon the established POSIX socket API [ 119 ] , [
120 ] , [ 121 ] , [ 127 ] , available on basically every Unix system as
well as on Windows systems. TCP is a connection-oriented protocol, and
the use of the BCL communication functions without an explicitly
established connection results in a TCP connection being initiated for
the duration of the sending. On the other hand, the system API used to
access the TCP protocol trivially allows to receive data from multiple
connections simultaneously, making the locking functionality unnecessary
for TCP.

##### The TCP Communication Helper Class

In the BCLIntTCPComHelper class functions are provided for the TCP
protocol used by both communication mechanisms, and which therefore have
been placed in a separate class for reuse. Most of the functions
supplied by the TCP helper class follow the interface defined by the
top-level communication class BCLCommunication and differ primarily in
the number and type of parameters. Parameters supported by this class
can be made more specific for the TCP implementation compared to the
generic interface definition. For example, for the functionality to bind
to a valid address provided by the helper class’s Bind function, the
generic address pointer to a BCLAbstrAddressStruct instance is replaced
by a BCLTCPAddressStruct pointer.

In the Bind function the socket that will be used to accept incoming
connections is created using the socket function. The SO_REUSEADDR flag
is set by the setsockopt function to allow the socket to bind to an
address that has been in use before and not been properly cleaned up.
Since a bind to an address in actual use will still fail, the option can
be safely used here. After setting the flag the socket is bound to the
port number from the specified address using the bind API function. If
an IP number has been specified in the address the socket will be bound
to that particular address so that data can be received only from the
network interface associated with that address. Otherwise it will not be
bound to a particular IP number, and data sent to any IP address of the
current node can be received. When the bind call has completed
successfully, the socket is set to a state in which the system accepts
incoming connections using the listen system call. Finally, a background
thread is created to wait for and handle incoming connections as well as
data arriving on an already established connection. This accept thread
is described in more detail below. Analogous to the Bind function the
helper class also contains an Unbind function that ends the ability to
accept incoming connections and data. To unbind the socket created above
the function sets an internal flag and tries to establish a short
connection to the local receive socket itself to activate the created
accept thread. Upon activation the thread checks whether the flag has
been set. If this is the case, it cleans up and terminates, setting
another flag in the process. The connection established by the Unbind
function is closed again immediately, and the function waits a specific
time for the second flag to be set, signalling the termination of the
accept thread. If the flag is not set within the required timespan the
thread is aborted forcefully. After termination or abortion of the
accept thread, the accept socket created in Bind is closed to complete
the unbind operation.

The second type of functionality provided by the TCP helper class for
both communication mechanisms is the ability to connect to and
disconnect from remote communication objects. For this purpose two
interfaces are available in the helper class. One of these interfaces is
identical to the connection interface supplied by the BCLCommunication
class, again with the exception of TCP address structures being used
instead of abstract addresses. Its parameters are the address pointer
mentioned, a flag whether a timeout is to be used together with the
timeout value, and a boolean flag specifying the connection type. The
timeout value may be ignored depending on the timeout flag’s value and
the connection flag indicates whether to connect immediately or create
an on-demand connection. This interface is used for explicitly initiated
connects by the TCP communication classes. It makes use of the second
basic connection interface in the helper class.

This second helper connection interface is used by any sending function
in the TCP communication classes that needs to use a connection for the
duration of the send operation as well as by the explicit connection
interface. The function for initiating a connection, ConnectToRemote ,
requires as its primary arguments a pointer to the TCP address of the
remote connection partner and a pointer to a structure used internally
to store data for an established connection. In this structure the
remote address and the socket used for this connection are stored
together with a use count. It is filled by the connection function and
allows the code that initiated the connection to use it through the
stored socket descriptor. In addition to these arguments ConnectToRemote
supports two more input flags and one output flag, the first of which
specifies whether the connection should be established immediately or as
an on-demand connection. The second input flag controls whether the
connection is a permanent connection initiated by an explicit user
connection call or whether it is an implicitly established connection.
In the first case, the connection data as returned by the function is
stored in a list of connections if it has not been stored there already.
Related to this, the output flag of the function indicates whether the
connection was already stored in the connection list, indicating that
the connection had already been established before the ConnectToRemote
function call.

If the connection has been already established, only its data is copied
into the connection data structure, and the appropriate output flag is
set so that a connection can be used multiple times. For a connection
that needs to be established immediately, but that has been stored as an
on-demand connection, the call will cause the connection to be
established. The established connection’s data will be stored in the
slot already used. It is stored independently of whether the function
call causing the connection to be established, specifies a permanent
connection or not. When a connection cannot be found in the internal
connection list three possibilities have to be distinguished.

1.  An on-demand connection that has to be stored: In this case just the
    remote address is stored in the list with a usage count of one and
    an invalid socket descriptor. The invalid descriptor indicates later
    calls to this function that the entry is an on-demand connection
    which still has to be established.

2.  An immediate connection to be made permanent as well: In this case
    the connection is established as described below and, if successful,
    stored in the internal list.

3.  An implicit connection from a transmission operation: This
    connection needs to be established immediately but does not have to
    be stored in the internal list. It is also established as described
    below, but contrary to the other cases the connection data is only
    returned to the caller and is not placed in the list.

If a connection has to be established for one of the three cases listed
above, certain steps are executed in ConnectToRemote , starting with the
creation of the socket used as the local endpoint for the connection
concerned. After creation the TCP_NODELAY socket option is set as any
message should be sent immediately without waiting for further data that
might have to be sent. When these preparation steps are completed an
attempt is made to establish the connection using the connect system
call. If a timeout has been specified, it is used for the connection
attempt by setting the timeout with the SO_SNDTIMEO socket option. After
the connect function returns the old timeout value is restored. At this
point the connection has been established if the connect function
signals success and the connection data can be stored in the connection
list as required (see above). One combination of the two on-demand and
store input flags for ConnectToRemote has not been discussed in the
previous paragraphs: an on-demand connection that does not have to be
stored as well. This combination of flags does not have any significance
for establishing any kind of connection but the function will still
return whether a connection could be found in the internal list. As a
consequence this input flag combination can be used to check for the
existence of a connection with a given remote address in the list of a
communication helper object.

For terminating existing connections, two functions for the two
different connection interfaces exist in analogy to the two connection
functions. The Disconnect function of the explicit connection interface
requires three of the arguments of the corresponding Connect function,
the address of the remote connection partner and the timeout flag and
value. Using the input flag combination to the ConnectToRemote function
described in the previous paragraph, it determines whether a connection
to the specified address is active. In that case it uses the disconnect
function of the second interface type to terminate the connection. The
second disconnect function, DisconnectFromRemote , also accepts three
arguments, but unlike for the Disconnect function the first argument is
not the remote connection address. Instead it expects a pointer to the
connection data structure returned by ConnectToRemote that contains the
data of the connection to be closed, including the remote address. Its
two other parameters are again the timeout flag and value parameters
with the same meaning as for the other functions. Using the remote
address in the connection data structure the function searches the
connection list for a matching connection, and if it is found, its
reference count is decreased by one. For a reference count greater than
zero the function terminates without any further action. If the
reference count is zero or less, the connection’s socket is closed by
the close system call, again using the SO_SNDTIMEO option if the timeout
is specified. Following this the connection data item is removed from
the list of connections. Saving the socket’s previous timeout value is
not necessary in this case as the socket is closed and thus unusable at
the end of this block. If no connection data structure containing the
specified remote connection address could be found, it is presumed that
the connection is an implicitly established one that has not been stored
in the list. In this case the socket in the structure is simply closed
as before.

As introduced above, the Bind call starts a background thread with the
task of managing new incoming connections as well as data arriving on
established connections. This thread consists of a loop that runs until
the end flag is set by the Unbind function as described in the previous
paragraph. In the loop the select system function is used to check the
socket created and bound in Bind as well as any socket belonging to an
accepted connection for available data. To ensure regular checks of the
end flag the select function is called with a timeout of 500 ms as a
safety measure in addition to the short connection made from Unbind (cf.
above).

If a select function indicates that new data is available for the listen
socket, this signifies a new connection attempt, which is accepted by
the accept system call. The new socket returned by accept is passed to a
callback function of the helper object’s parent, an instance of either
BCLTCPMsgCommunication or BCLTCPBlobCommunication . If this function
returns the boolean value false, the connection is rejected and the
socket closed. Otherwise the connection is accepted and placed into the
list of currently established connections to be checked for new data by
the background loop. When new data is signalled to be available on an
accepted connection, a second callback function of the parent object is
called. In this callback function the steps required to read the
received data have to be performed, and its implementations differ
between the TCP message and blob communication classes. If this callback
function returns false this is an indicator that an error occured while
attempting to read and the socket is placed in a list of connections to
be closed. This is necessary because a receiver can only detect a
connection that has been closed by a sender when select returns
available data for a socket while a subsequent read call fails with no
available data. Errors that occur during a select call have a
configurable limit for the number of calls allowed to fail in a row. If
this limit is exceeded, the background thread handling the connections
is terminated, as it is assumed that either a fatal error occured with
the listen socket or that it has been closed from outside the loop. The
error count for select calls is reset after every successful call.

During the receiving of messages many read calls for small amounts of
data are executed, e.g. first the header of the sender’s address is read
to check for the correct address length, and then the rest of the
address is read. Following this, the header of the message and the
message data itself are read. Consequently four read calls have to be
made to receive one message. To reduce this overhead of many system read
calls, a special read aggregation mechanism has been implemented in the
TCP helper class. In this mechanism the communication classes do not use
the read system call directly but instead use a read function provided
by the helper class. When called, this function checks an internal read
buffer, currently 1 kB large, for the presence of data. With data being
present in that buffer, the data requested to be read is copied from
that buffer instead. If not enough or no data is present in the buffer,
then a read system call is made. In this call it is attempted to read as
much data as is available in one call, up to a maximum of the read
buffer’s size. Later read requests can then again be satisified from the
buffer’s internal memory. A read attempt that, after emptying the read
buffer, requires more data than would fit in the buffer will not result
in a read system call to fill the buffer again, but instead the read
call is made so that the whole amount of data is read directly into the
desired final location. This “override” was implemented to avoid read
requests for large amounts of data being “translated” into multiple
small read system calls to fill the buffer followed by memory copying
operations from the buffer into the final destination memory.

##### The TCP Message Communication Class

Making use of the functionality provided by the TCP helper class, the
BCLTCPMsgCommunication class contains the implementation of the message
communication functionality for the TCP protocol. The basic
functionalities for binding to a given receive address and establishing
connections to remote addresses are implemented based upon the helper
class functionality described in the previous subsection. In many
message class functions appropriate functions provided by the helper
class are simply called with the appropriate type casting of the
parameters from abstract to the respective TCP types. As written in the
TCP section’s introduction, TCP trivially supports receiving data
simultaneously from multiple data destinations. Connection locking is
therefore not supplied by the TCP message class. Since the pure virtual
functions inherited from the communication base class still have to be
implemented to be able to use the class, they are provided as empty
functions. An error is returned by these functions to indicate that the
functionality is not supported.

One of two primary functions of the class is the sending of messages to
remote processes using implementations of the two Send functions
declared in BCLMsgCommunication , both of which call the same internal
member function. In analogy to functions from the helper class this
function accepts a timeout flag and value to support both connection
functions that differ in the support for a timeout. In addition to these
two parameters it also requires the remote receiver address and the
message to be sent. An optional error callback object pointer can be
passed as well. Checks are performed first in this internal Send
function on the remote address for the correct communication ID and
minimum structure size of the BCLMessageStruct length. A further check
is performed on the message itself to reduce segmentation violations
during sending. The first and last byte of the message data are read
once so that potential violations occur before the sending is started at
all to avoid that a sending operation is aborted while in progress.
After these checks the connection is established or data of an existing
connection retrieved by calling the second connection function
ConnectToRemote in the helper class. If a timeout value has been
specified, it is set here after saving the old value for restoration
after the send is completed.

In the next step the address of the sending object is sent to the
message recipient to inform it about the message’s origin. Like all TCP
send operations in the library classes, sending itself is done in a loop
where all remaining data is passed to a write call for sending.
Depending upon the amount of data written, as returned by write , the
loop is either ended or a select call is made to wait for the connection
to become available for writing again, using a timeout as appropriate.
An additional inner loop is present around the select call to account
for uncaught signals that cause select to exit even though the
connection is not available again as required. When the address has been
written successfully, the actual message is sent using a similar loop.
If the connection had been established implicitly for this operation, it
is terminated by calling the second disconnect function,
DisconnectFromRemote , in the helper class. During message transfers it
may happen that a connection to a specific receiver address is
interrupted, for example due to a temporarily severed network
connection. A mechanism has been implemented in the TCP message class to
hide this fact from the calling code and prevent error handling at that
level as well as data loss. The mechanism is present in the form of a
loop in the Send function surrounding the parts of the function between
and including the establishing and termination of the connection. When a
lost connection is detected during a message send operation, the
connection is closed and the loop starts again, attempting to
reestablish the severed connection. If this fails due to a more severe
and/or permanent network error, the transfer is aborted and the error is
escalated to the calling program.

Data sent from other communication objects is received with the help of
the background accept thread started by the communication helper class.
When new data is available from an established connection, the thread
invokes a callback function of its parent message object with a
connection data structure. Using this connection data a check is made on
an internal list whether a data receive operation for this connection is
currently already in progress. If this is not the case, a new operation
data structure is created and filled with the necessary data. As
detailed in the TCP helper class description, the address header is read
first from the connection’s socket to check whether the received
sender’s address has the correct size. Like all data read operations,
this is performed by attempting to first read as much data as necessary
in one call using the helper class’s buffered read function. In case of
insufficient amounts of data being returned in order to satisfy the read
request, a select call with a zero timeout to return immediately is made
to determine whether there is more data available on the connection.
Similarly to its use in the Send function, the select call is surrounded
by a loop that handles interruptions by calling the function again. For
a select call that signals available data a new read is attempted for
the whole missing amount of data. This process is repeated until select
indicates that no more data is currently available for reading. In this
case the receive operation data is inserted into the appropriate
internal list, if not already present, and the callback function
terminates.

If an error occurs during a message read an operation that has been
continued by this call is removed from the operations list, while a
started operation is not entered into it. Read operations are thus fully
aborted upon an error. An address header that has been successfully read
is checked for the correct length of the whole structure. A mismatch
here causes the receive operation to be aborted as for a read error
above. For a correct address size the rest of the address itself is read
by a similar read loop and the complete address is checked for the
correct communication ID type. A failure of this check again causes the
termination of the receive operation. In the next step the reading of
the message’s header data is attempted, followed by a check for its
correct minimum size. After this check is passed, the memory to store
the complete message is allocated using the size specified in the
header. The header already read is copied into this memory and the
pointer to it is stored in the read operation’s data structure for later
calls, if the operation cannot be completed in this call. Following the
allocation, the rest of the message data is read into this new memory
using a similar read operation sequence. Upon successful completion of
the read operations the final receive steps are performed. The
receiver’s address and the received message are added to a list of
received messages. If this function call was the continuation of a
receive operation, the operation’s data structure is now removed from
the list of in-progress operations. Finally, a signal is sent to wake up
receive functions waiting for new data.

Receiving of messages is done through implementations of the two Receive
interface functions declared in the BCLMsgCommunication class from
section 5.4.1 . Both functions call an internal third receive function
that handles the two different cases with and without a specified
timeout. If a received message is already available when the function is
called, the corresponding sender address and message data pointer is
passed to the function’s caller using the corresponding parameters. The
address is copied into a structure provided by the caller, and the
pointer to the allocated message memory is returned as the message’s
address is known only after it has already been received. If no message
is available for return to the calling code, a wait is entered on a
signal object. An appropriate timeout is used if it has been specified
in the function’s parameters. When the wait call returns, either because
the timeout expired or because a signal has been sent by the background
receive thread, the list of messages is checked again and the first
available message is returned as above. If still no message is
available, the wait is entered again or the function terminates,
depending on whether the timeout has already fully expired or not.
Messages that have been received like this must be released by the user
with a call to the ReleaseMsg method to free the memory allocated in the
incoming data callback function described in the previous paragraphs.

##### The TCP Blob Communication Class

The purpose of the BCLTCPBlobCommunication class is to provide the
implementation of the blob communication mechanism on top of the TCP
protocol and socket API. Similar to the TCP message communication class,
basic functionality like binding or connecting makes use of the
facilities supplied by an internal instance of the TCP communication
helper class. The implementations of these functions in the blob class
also perform the necessary address type checking and casting before
calling the helper object’s corresponding function, but unlike the
message class’s implementations the functions perform some additional
steps. In the Bind call two further steps are executed after the helper
class’s function has been called, initialization of the list of free
blocks in the buffer and starting of a background thread to handle
requests issued to this communication object. The free block list is
initialized to contain one block for the whole buffer. It will later be
used to keep track of used and free areas in the buffer.

In the started request thread a wait is entered for incoming messages,
using the message communication object assigned to the blob object’s
exclusive use, as described in section 5.2.3 . Received messages are
handled according to their type, which in general is either one of five
different requests issued to this object or a reply from a remote object
to an issued request. Of these five requests the first is the most basic
one and has to be issued before any direct communication can take place
between two blob communication objects. This request queries via its
associated message object the address to which a remote blob object has
been bound. It is necessary to use the message object as only the
message address is passed to blob object functions. To avoid unnecessary
message traffic the address is queried only once when a connection is
established. It is stored in the blob object together with its
associated message address for later uses. Two more query requests
handled by the thread are a query for the blob buffer’s size and a
request for a free block in that buffer. Of these two the first is
handled by generating a simple reply message containing the desired
size. In order to answer the third request type a block has to be
obtained from the list of free blocks. If a free block of the desired
size is available, its starting offset is sent in the reply message,
otherwise a -1 is sent as a buffer full reply. In the search for a free
block the whole list is searched to find the smallest free block of at
least the requested size to reduce fragmentation of large blocks.

The remaining two request messages handled by the thread are
notifications about the connection status between the sender and
receiver object. When a blob object has established a connection to a
remote partner the first of these two messages is sent to the remote
object to trigger a reverse connection to the initiator object. This
connection is made both for the blob object itself and for its message
object to profit from the connection for the frequent reply messages
expected. No reply other than establishing the connection is sent in
response to receiving this connection message. A disconnect of such an
established connection between two blob objects and their associated
message objects is triggered by the last request message handled by the
background thread. The disconnect message is sent by the communication
partner that initiates the process to ensure that the connections in
both directions are terminated. Like the connection notification, this
message is not answered. Replies to one of the first three request types
are handled identically by placing the received reply in a list and by
triggering a signal object on which any thread expecting a reply waits.
When a thread is woken up by this signal the list is checked for the
expected reply using a reference number placed in the original request.
If the reply is found, it is extracted from the list and processed as
needed. Less additional work than in the Bind function is performed in
the class’s Unbind function, which just terminates the background thread
prior to calling the helper object’s Unbind function.

In the TCP blob class’s Connect function several steps are executed
before and after calling the helper function’s Connect function. After
the obligatory address type check and the check for an associated
message object, the function calls the Connect function of this object.
Using the established message connection the remote blob partner’s
address is obtained using the address query message mechanism described
above. The received address is passed to the helper object’s Connect
function to establish the blob connection. As the final step in this
function, a connection notification message is sent to the remote object
to initiate a reverse connection as discussed above. In the class’s
Disconnect function the same checks as in the Connect function are
performed initially for the address type and message object presence.
Using the remote blob object address from the cache list the blob object
connection is terminated by calling the helper object’s Disconnect
function. The message connection which is still established is used to
send a disconnect notification message to abort the established reverse
connections too. Afterward the message connection is closed as well, and
finally the remote blob address is removed from the cache list. In
analogy with the TCP message class, the locking feature is not supported
by this class. Implementations of the locking functions return the same
function-not-supported error indicator as in the message class.

As detailed in sections 5.2.3 and 5.4.1 , a blob data transfer is split
up into two phases. During the first phase the two communication
partners negotiate where the transfer data has to be placed in the
receiver’s data buffer, and the second phase is the actual transfer of
the data. Functionality for the first phase is contained in the TCP blob
class’s PrepareBlobTransfer method defined in its BCLBlobCommunication
base class. Like the previous functions this function also first
performs the address type check together with a check for a configured
message communication object. After the checks are passed, it requests a
block in the remote blob object’s buffer by sending an appropriate
request message with the size specified in the function’s parameter. The
block’s offset is then received in the request’s reply message and is
used as the transfer ID returned to the calling code. An error indicator
that has been received for the block request corresponds to an invalid
transfer ID and thus can be returned directly as well. After the
preparation phase the actual data transfer is performed by one of two
TransferBlob functions. These two functions differ in the argument types
and actions they have to perform, as described for the
BCLBlobCommunication class in section 5.4.1 . In the single block
version of the function the block parameters are simply passed to the
multiple block version by placing them into lists containing just one
element each. In the second, multi-block, transfer function the same
address checks as in the transfer preparations function are made,
followed by additional checks of the transfer parameters, starting with
the validity of the transfer ID. The two lists of block sizes and
pointers are checked for identical numbers of elements, while the offset
list is not checked since any missing offsets cause the blocks to be
placed consecutively.

No validity checks are made for the offsets and sizes of the data blocks
in this function, instead the values for each block are sent to the
receiver prior to the data itself to be validated there. The obvious
drawback of transferring data that will be discarded if the parameters
are wrong is reasonable as this should happen rarely. However, the
advantage is the avoidance of a validation message exchange, that would
otherwise increase the transfer latency. Actually the only occasion
where offsets could be wrong, apart from a bug in the library, arises
when an incorrect transfer ID is passed to the transfer function. After
the preliminary checks have been passed successfully, the remote blob
communication object’s address is queried, either from the cache list
for established connections or through a query message. Using this
address the helper class’s ConnectToRemote function is called to
establish a connection to the remote blob object. A timeout is set for
this connection, if one has been specified in the function’s parameters.
Following this is the main loop of the transfer function that iterates
over each block to be transferred. In the loop each block’s data
pointer, size, and offset are extracted from their respective list or
calculated in the case of a missing offset. These parameters are then
placed into a structure derived from BCLNetworkDataStruct described in
section 5.3.1 . This structure is then sent to the receiver blob object
prior to the data block itself. Sending of the header as well as the
data is done in loops similar to the one used for sending message data,
detailed in the preceeding message class section.

Errors that occur during the transfer can be separated in two classes.
Broken connections, the first category of error, are handled by
signalling a broken connection to the helper object and then
reestablishing the connection. Such an interrupted transfer is resumed
by retransmitting the block where the interruption occured, as it cannot
be determined exactly which amount of data was successfully sent. Other
types of errors that occur during the writing of data or during attempts
to reestablish a connection are handled by aborting the function. Once
all blocks of a transfer have been sent the function waits indefinitely
or for the specified timeout to read a 32 bit data word indicating that
the receiver has read all transmitted data successfully. After receiving
this indicator value, the connection is released by calling the helper
object’s DisconnectFromRemote function, to either decrease its usage
count or terminate it.

Receiving of blob data is achieved similar to receiving of messages with
the help of the background thread in the TCP helper class. From the
thread a data reading callback is invoked that functions similarly to
the corresponding message class function. With the help of the
connection data structure transfers are either started or resumed as
appropriate. The actual process of reading the data is also split into
two parts, for the transfer header containing the block’s destination
information and the data itself. Using the offset and size values from
the transfer header a check is made whether the transfer is valid and
can be placed into a reserved block in the object’s receive buffer. If
this is the case, the read operations for the data are performed such
that it is placed directly into the appropriate receive buffer area. All
reading steps are executed similarly to the reading of messages in small
inner loops with read and select calls. Uncompleted transfers for which
no more data is available for reading are placed into a list to be
resumed in later calls of the function when new data is available.
Errors that occur during the receive operation are signalled to the
calling accept thread via the functions’s return value and result in the
closing of the concerned connection. When all expected data has been
read from the socket, the 32 bit completion indicator expected by the
sending transfer function is written using a select call with a short
non-zero timeout to verify that the connection is available for writing,
followed by a single 32 bit write operation. At this stage errors are
ignored and not reported back to the accept thread. After writing the
completion indicator the function terminates, signalling success to the
calling accept thread.

Access to data received from a remote object is possible via the
GetBlobData function or a combination of the GetBlobOffset and
GetBlobBuffer functions, as described in section 5.4.1 . The starting
offset required to access the data, both for GetBlobData and
GetBlobOffset , is equal to the transfer ID passed as the functions’
only parameter. Both functions check the list of used blocks for a block
whose starting offset is equal to that transfer ID. If such a block is
found the transfer ID is presumed to be valid. Otherwise an error is
reported and no valid pointer or offset is returned. When a block of
received data can be released, the object’s ReleaseBlob function is
called to free the used block in the receive buffer. The block is
located in the list by comparing its starting offset with the transfer
ID passed as the function’s parameter. If the appropriate block could be
found, it is removed from the used block list and inserted into the free
block list to be used again.

#### 5.4.3 The SCI Communication Classes

In this section the three classes are described which implement the two
different communication facilities on top of the SISCI API [ 126 ] for
SCI adapter cards by Dolphin [ 90 ] . Functionality in these classes is
very similar to what is provided by the three TCP classes: one class
each for the message and blob communication facilities,
BCLSCIMsgCommunication and BCLSCIBlobCommunication , and the helper
class BCLIntSCIComHelper encapsulating functions for both communication
mechanisms.

Dolphin SCI cards are a high performance system area network (SAN)
technology designed to provide tightly coupled interconnects in clusters
of PCs or workstations. SCI is an IEEE standard [ 13 ] for a shared
memory interface. Nodes connected via SCI are able to write to or read
from a remote node’s memory directly. In addition, the Dolphin SCI
adapters contain DMA engines capable of copying data autonomously
between nodes without intervention by a host CPU. Low level details of
accessing the adapters are hidden by the SISCI C-API, based upon
supplied device drivers to access the adapter cards for controlling
connections to remote nodes and DMA transfers. Programmed I/O (PIO)
transfers, performed by a host CPU, are executed over an established
connection without any API or driver intervention. For the SCI
communication classes any data transfers, for messages as well as for
blobs, are performed via a memory segment in the receiver object to
which the remote sender writes the data to be transferred. This memory
segment, exported for remote node access, has to be allocated by the
SISCI API. It is not possible to specify a normal user allocated memory
block to the SISCI system for exporting.

##### The SCI Communication Helper Class

Analogous to the BCLIntTCPComHelper class the BCLIntSCIComHelper class
provides common services to the two classes implementing the message and
blob communication mechanisms on top of the SISCI API and SCI network.
Due to the more complex SISCI API being used to access the SCI cards,
this helper class needs to contain more support functions than its TCP
counterpart. The first interaction with the SISCI API is made by this
class already in its constructor, as each API function call requires a
handle to an SCI descriptor in its call. This descriptor is obtained
using an SCIOpen function call in the constructor. An error occuring
here is only logged without any further action, although subsequent
SISCI calls will fail due to the invalid descriptor. Therefore, all
functions in the class check the descriptor’s validity first before
performing any other action, particular prior to any API function call.

As for the TCP helper class the first functionality supported by this
class is binding to a valid address to enable an object to accept remote
connections and data. For this purpose a local memory segment is
allocated and exported under a given ID for remote access using the
SISCI API. The Bind function that executes this task accepts two
arguments, the size of the memory segment to be allocated and the
address under which the segment is to be made available in the form of a
pointer to an BCLSCIAddressStruct structure. In the Bind function the
local ID of the SCI adapter specified in the address is queried. Using
that ID’s lower 16 bit together with the 16 bit segment ID also
specified in the address, a 32 bit ID is generated. Since each segment
ID has to be unique on each node and each node ID has to be unique in a
cluster, the generated 32 bit ID is unique in a whole cluster as well. A
prerequisite is that only the lower 16 bits of a node ID are used in a
cluster. Next to the node ID restriction to be 16 bit, a further
requirement placed on the generated ID is that its final value must not
be 0xFFFF, which is reserved as an invalid ID.

A memory segment of the given size is created with the specified 16 bit
segment ID. It is mapped and exported so that the creating program as
well as remote programs can access it for reading and writing. Creation
of the segment is executed using the SCICreateSegment call, specifying a
size one page larger than the user specified amount. This additional
page, typically 4 kB, is needed to provide room for a header structure
located at the segment’s start. Mapping and exporting are done via
SCIMapLocalSegment and the combination of SCIPrepareSegment followed by
SCISetSegmentAvailable . Following the segment creation an SCI interrupt
is created by calling the SCICreateInterrupt function with no ID
specified so that an available one can be selected and returned by the
SCI system. This interrupt will be used to signal the availability of
new data that has been written into the memory segment to receiving
objects to avoid the need of polling for data. Before the segment is
made fully available for remote access using the SCISetSegmentAvailable
call, the header at its start is filled with the returned interrupt ID,
the segment’s size, and further management data required to handle the
memory as a FIFO ring buffer area. To ensure accessibility from remote
nodes the header structure is derived from BCLNetworkDataStruct and thus
allows to use the data transformation mechanism described in section
5.3.1 . The mapping function returns a standard C pointer, allowing the
use of the segment’s data in the local program like any other memory
area. This pointer is used to access the messages that have been written
into the segment from remote processes.

For errors occuring during any of these steps the error number returned
by the SCI subsystem is logged and transformed into a standard C system
error code passed to the calling function. Ultimately this number is
reported to the program using the library from an SCI message or blob
communication object, either as a return value or an error callback
parameter. After an error has occured, the steps that have been taken in
the binding process are reversed so that the object is brought into the
same state as before the call. The above reversal is achieved by calling
the Unbind function in the helper class, which iterates through each of
the steps of the bind process in the opposite order. For each of these
steps it is checked whether it has been performed by analysing the
allocated resources. First the segment is made unvailable for external
connections, and then the created interrupt is removed using the
interrupt number stored in the segment’s control structure. After this
the segment is unmapped and finally destroyed, freeing the allocated
memory. API functions called in this process are
SCISetSegmentUnavailable , SCIRemoveInterrupt , SCIUnmapSegment , and
SCIRemoveSegment in this order.

Like the TCP communication helper class BCLIntTCPComHelper from section
5.4.2 , the BCLIntSCIComHelper class also offers two sets of function
calls for establishing and terminating connections to remote objects.
Both interfaces are identical to the TCP functions in their respective
tasks and arguments, apart from the SCI and TCP differences, e.g. in the
addresses used. The first of the interfaces is used for explicitly
established connections while the other one is used to establish
implicit connections, as well as by the first helper class functions for
explicit connections. In adition to the remote address and an optional
additional error callback object, the explicit connection API supports
two flags, one specifying the use of a timeout value and the other to
specify whether a connection should be established immediately or as an
on-demand connection. The explicit Disconnect function also accepts four
of these parameters: the remote address, the error callback object, and
the timeout flag and value parameters.

Five of the eight arguments being used by the implicit connection
function ConnectToRemote are in principle similar to the Connect
function’s arguments. A small difference can be found in the flag that
differentiates between an immediate and an on-demand connection. Since a
set flag specifies that a connection is to be established immediately,
the flag’s meaning is reversed with respect to the flag for the Connect
function. Concerning the parameters specific to this function, the first
is used to return information about the established connection to the
calling function. This information includes a SISCI API descriptor for
the connected remote memory segment, a pointer to access the segment
mapped into the local program’s memory, and a handle used to trigger the
remote segment’s data notification interrupt. The first of the remaining
two parameters is a flag that specifies whether the connection is to be
stored and thus established as a permanent connection or whether it is
an implicit connection only established for a single send operation. A
return flag for the function’s caller is contained in the last
parameter, indicating whether the connection was already existing or
whether it had to be initiated by this function call. In the first case
the data for the connection is obtained from an internal list and is
returned to the calling function without further communication taking
place, while in the second case the connection has been established, if
this was specified in the function call.

In the first step of the connection attempt the unique global ID of the
remote segment is constructed from the node and segment IDs in the given
address as described for the bind process. If the resulting 32 bit ID is
invalid, the connection attempt is aborted with an error. Otherwise a
new SCI descriptor handler is obtained by calling the SCIOpen function.
This step is necessary because each SCI descriptor is only able to
handle one remote segment, one local segment, and one interrupt. Using
this new descriptor a connection attempt is made to the memory segment
on the remote node with SCIConnectSegment , specifying the remote node’s
ID and the ID of the segment to connect to. One retry is made for two
types of SCI errors reported back from this function. This has been
found experimentally to be necessary. After a successful connection
attempt the SCIMapRemoteSegment function is used to map the header part
of the remote segment into the local address space. From the header the
segment’s total size is obtained, which allows to map the whole segment
into the local address space. The segment descriptor and the pointer
returned by the two API functions are stored in the data structure for
the established connection.

After completion of the mapping, an SCI sequence is created for the
remote segment. A sequence is an SCI mechanism that allows to check for
errors while accessing a remote segment and to exercise some control
over local read and write buffers for a remote segment. It is created by
calling the SCICreateMapSequence function in two attempts, first for a
fast sequence type, of which only a limited number are available, and
for the normal type if the fast one fails. Then the created sequence is
cleared from old errors by calling the SCIStartSequence API call in a
loop until it indicates success or a specified timeout expires. In the
final step of the connection process the remote segment’s interrupt
number is read, and a connection attempt to the interrupt is made using
the SCIConnectInterrupt function with the returned descriptor being
stored in the connection data structure. As for the SCIConnectSegment
call, this function may fail the first time under specific circumstances
and is therefore attempted a second time upon failure. For an on-demand
connection it is possible that an unestablished connection is locked by
the user. Instead of establishing the connection when the lock call is
made, a flag is set in the connection data structure in the helper
class, and the flag is checked in ConnectToRemote when a connection has
been fully established. If the flag is set, the class’s internal
LockConnection function, described below, is called with the
connection’s data to establish the lock.

Closing of an established connection is performed by the
DisconnectFromRemote function, which requires the connection data
structure of the connection concerned. Additionally, an optional error
callback as well as timeout flag and value parameters are accepted by
the function. If a matching established connection is found, its usage
count is decremented. If the usage count subsequently is zero the
connection is terminated. To terminate a connection first a local
segment created by a DMA enabled blob class (see below) is destroyed,
and the connection’s lock flag is checked. If the flag is set, the
remote segment is unlocked. The SCI calls to release the connection’s
resources are made next, releasing the remote interrupt, unmapping the
segment and disconnecting from it. SCIDisconnectInterrupt ,
SCIUnmapSegment , and SCIDisconnectSegment respectively are used for
these steps. Afterwards the SCI descriptor used for the connection is
closed by calling SCIClose , and the stored connection data is removed
from the object. If no matching connection could be found in the helper
object, the connection is presumed to be an implicitly established one
and the steps are performed identically, except for the removal of the
connection’s data structure.

All PIO read and write operations executed on remote memory segments by
a node’s CPU, in the helper class as well as in the message and blob
classes, are contained in an error checking loop. This loop uses the SCI
sequence data created in the ConnectToRemote function with the help of
two internal functions of the class. These two functions, StartSequence
and CheckSequence , are called prior and after the access respectively.
StartSequence is used to initialize the sequence while CheckSequence
clears any pending errors and checks for errors that occured during the
operation. The return value from CheckSequence can be one of three
types: success, a fatal error, or a temporary error. For the last type
the operation has to be repeated until one of the two other cases
occurs. In the two functions the appropriate SCI API calls
SCIStartSequence and SCICheckSequence are encapsulated with the
appropriate temporary variables and error conversion required. The
SCIStartSequence call is also repeated until either success or a fatal
error is reported, after which the operation can be started or has to be
aborted respectively.

Unlike the TCP communication classes, the SCI message communication
class supports the locking feature defined in the BCLCommunication
class, as already detailed in the preceeding paragraphs. Like the
connection API two versions of the locking functions exist. One used for
explicit user initiated locking and a second one used internally by the
first version and to handle the write arbitration necessary for normal
send operations. The LockConnection function for explicit locking
requires the connection’s remote address, the usual timeout flag and
value, and an optional erorr callback parameter. For unestablished
on-demand connections only the lock flag in the connection’s data
structure is set as discussed above. If an established connection to the
specified address exists, the internal locking function is called with
the connection’s data structure to obtain the lock. When the lock
arbitration in that function has completed successfully, lock flags are
set in the connection data structure and in the remote segment’s initial
header structure. In the last locking step a local cached copy of the
remote segment’s control data is created to avoid remote read or write
calls. This caching is possible, as only the local process is allowed to
access that data after locking. In the corresponding UnlockConnection
function the same parameters as for the LockConnection function are
available for use. For locked but unestablished on-demand connections
the lock flag is simply cleared without any further action. If the
specified connection is established and locked, the locking flag in the
remote control structure is cleared and the internal UnlockConnection
function is called to relase the granted write access to the remote
segment. In a last step, the lock flag in the local connection data
structure is cleared as well.

The internal version of the LockConnection function accepts the
connection data structure in addition to the parameters required for the
explicit LockConnection function. For write arbitration two 32 bit large
fields in the segment control data are used, one to request write access
and the other to indicate the current owner of the write access. By
writing the sender’s own unique 32 bit ID into the first field and
triggering the remote segment’s interrupt, the remote receiving process
is notified that a process requests permission to write. After this
activation the remote process reads the requesting ID and if it is valid
and no other process currently owns the segment, the ID is written into
the field for the current owner. When the requesting process reads the
owner ID and finds its own ID, it knows that it has been granted
exclusive write access to the remote segment. If it reads another ID,
the process of writing its ID into the request field is repeated, with
small busy waits, until the request is granted or a specified timeout
expires. The busy wait uses a short loop to run for @xmath to keep the
intervals short and not delay too long. Normal system wait functions
which actually suspend a process without using processing time cannot be
used for this purpose as they work with a granularity of 10 ms, much too
long for this purpose. A granted write access is released in the
internal UnlockConnection function by writing the invalid 32 bit ID with
all set bits into the current owner field of the remote segment’s header
structure. After writing the function triggers the remote segment’s
interrupt to allow the receiver to update its arbitration state and ends
normally. For locked connections two helper functions exist to increase
a programs efficiency by handling a local cache copy of a remote
segment’s control data. Since only the message communication class has
to deal with reading and writing values from the control data structure,
when it writes messages to the remote segment, these two functions are
only used by that class and not by the blob class. The first of these
two functions, UpdateCachedCD , updates a local cache copy from the
remote segment’s data structure, while the second one, WriteBackCachedCD
, writes back a modified cache copy.

Two additional helper functions contained in BCLIntSCIComHelper are only
used by the SCI blob communication classes to compensate restrictions in
the SISCI API implementation. It is not possible to execute a DMA
transfer, where the SCI card copies the data autonomously to the remote
node, from normal user space program memory. Instead these transfers are
only possible if a local SCI memory segment is used as the data source.
This is an implementation issue with the supplied SISCI API which does
not allow to register ordinary memory as a segment to make it usable as
a data source location. The SCI adapters themselves are in principle
zero-copy DMA capable. To retain the desired DMA capability these helper
functions allow to create a local memory segment as a buffer for DMA
transfers. Blob Data to be transferred is copied from the ordinary
program memory into this local segment in chunks of up to the segment’s
size. Each chunk is then transferred by the DMA engine from the local
segment to the remote target memory segment. This approach still
requires a memory copy using the CPU, but as it is now only local it
should still be faster and more efficient than a PIO transfer of the
data to the remote memory. Creation of this buffer segment is done in
the CreateLocalSegment function with the size specified as a parameter.
The segment’s data is stored in a connection data structure to associate
it with a specific connection, either an implicit or explicit one. An ID
for the segment is obtained by using IDs from the part of the 32 bit
large SCI ID space that cannot be used by user segments for which only
16 bit are allowed. A call to SCICreateSegment is made for each possible
ID to test whether it is already used or available. If the call
succeeds, the segment created by it is used as the local segment.
Otherwise the process is repeated until a free ID is found or all
available segment IDs have been tested. In the latter case the function
cannot create a segment and fails with an error.

A successfully created segment has to be prepared so that it can be
accessed by the SCI adapter card for DMA and from the program through a
pointer, using the SCIPrepareSegment and SCIMapLocalSegment calls
respectively. Handles obtained from the three SCI API Calls are stored
in the data structure of the connection associated with the segment. In
the DestroyLocalSegment function a local segment’s usage counter is
decreased. If it is zero or less the segment is released. To release a
segment its user space pointer is invalidated and unmapped by calling
SCIUnmapSegment , and the segment itself is freed with SCIRemoveSegment
. In addition to these helper functions a number of smaller utility
functions are provided. These functions allow access to several of the
object’s fields so that the parents of helper class objects can use
SISCI API routines that require these parameters to expand the class’s
functionality as necessary.

##### The SCI Message Communication Class

Next to the SCI helper class the second important class for the SCI
communication implementation is the BCLSCIMsgCommunication class that
provides the message communication mechanism using the SCI network
technology. Like the TCP message class it relies on services from its
corresponding helper class but performs more actions beyond type
checking and calling helper class functions in its binding and
connection functions. In the Bind function, this additional action is
performed after the address type has been checked and cast and the Bind
function from the helper class has been called. A background thread
responsible for the arbitration of write access requests to the local
segment, as discussed in the previous section, is created as well. The
Unbind function in the message class first stops this started
arbitration thread and calls the helper object’s Unbind function
afterwards. Support for explicit connections, the next basic
functionality provided by the message communication class, is
implemented in the Connect and Disconnect functions, which have been
declared in the BCLCommunication base class. All of these functions
basically just call the corresponding helper Connect or Disconnect
function with the appropriately converted parameters. As detailed in the
preceeding section, the SCI message communication class, unlike the TCP
classes, supports the locking feature for established connections to
avoid the overhead of write arbitration for each message in cases where
this is possible. Both the LockConnection and the UnlockConnection
functions are implemented in the two versions defined in the base
communication class. All four functions also call their respective
counterpart in the helper class with the required parameters.

The first functionality specific to the message class is contained in
the implementations of the Send functions inherited from
BCLMsgCommunication , which call a third internal version that can send
with and without a specified timeout. A passed message is checked in
this internal Send function by reading its first and last byte to
prevent segmentation violations during the process of copying the
message into the remote memory segment, similar to the TCP message
class. In the next step the helper object’s ConnectToRemote function is
called to retrieve an existing connection’s data or to establish a new
one to the destination address. Depending on whether the connection is
locked or not, the header information for the remote segment is either
read from the cached copy maintained by the helper object or from the
remote memory itself.

If the connection is not locked already, the helper class’s internal
LockConnection function is called to obtain the required write
privileges for the segment. After the write access has been obtained,
the free amount of memory in the segment is calculated. If insufficient
space is available, according to the current cached header copy of a
locked connection, it is updated from the original remote header. If
still insufficient memory is available to write the specified message
into the remote segment, the function aborts after executing its cleanup
section. During the write process the sending object’s address is
written first, followed by the message data itself. Both steps are
surrounded by the SCI sequence error checking loops described in the
helper class’s section. After these two write steps have been completed,
the new write index is determined and written back, both to a locked
connection’s cache copy as well as to the remote segment’s header
structure. This last step is necessary even for locked connections so
that the remote receiver can determine the amount of new data. With the
write index written back correctly, the write privilege for an unlocked
connection is released and the remote segment interrupt is triggered to
inform the receiver about the availability of a new message. Finally,
the helper class DisconnectFromRemote function is called to relase the
connection, terminating it if it is no longer used.

The counterparts to the Send functions are the implementations of the
two Receive functions defined in the message class. Both functions also
call a third internal function to perform the required actions, either
with or without a timeout. In this third function the read and write
indices in the receive segment’s header structure are checked to
determine if data is already available. If no data is available, a wait
is entered on a signal object triggered by the background thread when
the SCI interrupt is triggered itself. When the function progresses past
the signal, either because data was already available or after the wait,
a small loop is run for a fixed number of iterations or until the read
and write indices indicate that data is available. Each loop iteration
performs a @xmath busy wait. This waiting loop is necessary as SCI does
not guarantee in-order delivery, and the SCI packet that triggers the
interrupt can arrive slightly before the packet holding the updated
write index. The function exits with a timeout error when no data is
available after the wait . If data is available in the receive memory
segment, a number of checks are performed on the data to ensure its
validity. For the expected address structure the correct size and type
are checked first, and for the message data itself the minimum length to
store a message header of type BCLMessageStruct is required. When these
conditions are met the address is copied into the function’s output
address parameter, and memory for the message is allocated. The message
data from the segment buffer is copied into that memory, and the pointer
to it is placed in the pointer reference parameter for output. As the
function’s last step the read index in the segment’s header structure is
updated to reflect the amount of data extracted from the receive buffer.
The above copy steps are required to be able to maintain a simple ring
buffer whose management can be simply split between local reader and
remote writer. If the data were to be accessed directly in the shared
memory a more elaborate buffer management would be required since an
application could not be relied upon to release the messages in the
correct order. The required more complex buffer management would most
likely require more (costly) remote read and write operations from the
writing process.

The final important method in the BCLSCIMsgCommunication class is the
interrupt handler function executed by the arbitration background
thread. It consists of a loop that runs while the object is bound to its
receive address. In the first part of the loop the request field in the
receive segment’s header structure is checked for a valid sender ID
indicating an active write request by a remote object. If such a request
is found and no sender is currently active, the request ID is placed
into the current sender ID field, granting write access to that object.
Following this arbitration part the function enters a wait for the
segment’s associated interrupt to be triggered using the
SCIWaitForInterrupt API function. When this functions returns because an
interrupt has been received, the segment header’s read and write indices
are checked. If they are unequal, the signal object on which the Receive
function waits is triggered to indicate new data to the function. As the
last part of the loop, a timer that has been started when write access
was granted to a sending object is checked. If the connection is not
locked explicitly and no change has been made on the segment’s write
index for a specific amount of time, the current sender field in the
header is reset, removing the sender’s write access. This timeout sender
reset has been introduced to cope with unexpectedly terminated
connections from remote objects during a sending process.

##### The SCI Blob Communication Class

An implementation of the blob communication mechanism defined by the
BCLBlobCommunication class from section 5.4.1 on top of the SCI network
technology is provided by the BCLSCIBlobCommunication class. Unlike the
other three classes implementing one of the communication mechanisms it
contains some network specific code already in its constructor. If DMA
functionality is enabled, it uses the SCIQuery function to determine
three parameters relevant for DMA transfers: the starting offset
alignment, the block size alignment, and the maximum blocksize. As
already introduced in the BCLIntSCIComHelper class’s section, the SISCI
API does not allow to specify an arbitrary user space buffer for
receiving but only SCI segments that have been allocated by
SCICreateSegment . To modify the default behaviour of the SetBlobBuffer
function inherited from BCLBlobCommunication the function is overwritten
to return an error when a user area is specified as a receive buffer.

In the class’s Bind function the helper objects Bind function is called
initially to create a segment and make it available for remote
connections. The pointer to the segment’s data part is then passed to
the BCLBlobCommunication parent class’s SetBlobBuffer function to set
the receive buffer pointer stored in that class. In the next step the
object’s free block list is initialized to contain one block describing
the whole buffer area, and a background thread is started to handle
request messages and replies for the object. To reverse these steps the
Unbind function terminates the background request thread and calls the
helper object’s Unbind function to release the allocated resources.

Similar to the BCLSCIMsgCommunication class, the Connect and Disconnect
functions exist in two public and one protected internal version each,
with the public versions calling the respective internal one. In the
internal Connect function the object’s associated message object is
first connected to its remote counterpart by calling its own Connect
function. After this connection is established the message object is
used to query the remote blob object’s own address if necessary. This
address can also be specified in a parameter to the function when it is
called in response to a connection request message from another object.
In that case the address query is skipped. Following the successful
address query the remote blob address is placed into a list of addresses
to avoid further query messages. If the function is called from one of
the public Connect functions, a connection request message is sent to
the remote blob object containing the local blob object’s own address.
As written above, in response to such a message a reverse connection to
the message’s originating object is established.

Like the internal Connect function the Disconnect function can also be
called with or without the remote blob object’s address. The second case
is used if it is called from one of the public Disconnect versions.
Similar to a connection request, the remote blob address is specified
when the function is called in response to a disconnect request message
received from a remote object. When the address is not specified, it is
obtained from the address cache or it has to be queried using an address
query message. Once the remote blob address is available, the Disconnect
function of the communication helper object is called to terminate the
blob object connection. During a locally initiated disconnect, the
result of a call to a public Disconnect function, a disconnect request
message is sent to the remote blob object to terminate the previously
established reverse connection. After sending that message the message
object’s Disconnect function is called to terminate that connection as
well, and the remote blob address is cleared from the object’s address
cache. Similar to the TCP blob communication class, the SCI blob class
does not require the connection locking feature as the space for each
data block is negotiated before the transfer. Therefore locking
functions are implemented as empty functions only. An implemented helper
function is GetRemoteBlobSize , which uses the associated message object
to send a message that queries the remote blob object’s buffer size. If
a reply is received, the size contained in the reply is returned to the
function’s caller.

The first function that implements functionality for the blob data
transfer is PrepareBlobTransfer . It sends a query message to the remote
destination object to request a free block of the specified size. In the
received reply message the destination object specifies the offset of
the block in the remote receive buffer segment or it indicates that no
free block of sufficient size is available. Similarly to the TCP blob
object, the offset in the buffer is equal to the transfer ID for that
particular transfer, as it is used for the TransferBlob function. Using
the returned transfer ID one of the two implemented TransferBlob
functions can be called to transmit the block or blocks to the receiver
object. Again like the TCP blob class, the transfer function that allows
only one block to be specified converts the block’s parameters into
parameter lists and calls the multi-block version of the function. This
function performs the same preliminary checks for the validity of the
transfer ID and equal number of elements in the data pointer and size
lists as the TCP function. After obtaining the remote blob address,
either from the cache list or by querying it with an appropriate
message, the helper object’s ConnectToRemote function is called to get
access to a connection to the remote object, either by establishing a
new or using an existing one. If the class’s DMA functionality is
enabled, a DMA queue is created using the SCICreateDMAQueue API function
to process DMA transfers. With the required preparations completed, a
loop is started that transfers each of the specified blocks to its
respective remote destination.

Without enabled DMA each block is simply copied to its destination in
the remote memory segment mapped into the current processes’ address
space. This is done by a normal memcpy surrounded by the SCI error
checking described previously. For DMA enabled transfers several steps
have to be executed for each block. It starts with the calculation of
slack areas for the beginning and end of the block, which result from
improper alignment with the values required by the SISCI system. Such
slack blocks have to be copied to the remote destination by a normal
memcpy call. In the framework these steps should not be necessary as all
memory and buffer management functions use a sufficiently large,
matching, alignment value. Support for unaligned blocks has been
included nonetheless to retain the generic usability of the class. The
remaining block data is split into smaller blocks, up to the size of the
local DMA segment used for the DMA transfer. Each of these sub-blocks is
then copied by memcpy into the local helper SCI segment. After copying
the block’s data, a DMA transfer is initiated by calling
SCIEnqueueDMATransfer to place the transfer into the DMA queue and
SCIPostDMAQueue to start the transfer. SCIWaitForDMAQueue is used to
wait for the transfer’s completion, after which the queue is reset for
the next transfer by calling the SCIResetDMAQueue function. If one of
the calls used in transmitting the block returns an error, the transfer
is presumed to have failed, and the block is copied explicitly by
another memcpy call. When all data of a block is transferred, the last
byte of the block in the remote destination is compared with the last
byte in the block’s source. For a more reliable check the destination
byte is filled with the bitwise inverted value from the data block’s
last byte before the transfer is started. The check is done in a loop
that runs until the bytes are equal or a timeout expires, to ensure that
the data block has been transmitted correctly when the function exits.
Problems with data arriving out of order, like for the message classes
have not been detected here, presumably due to the time needed to send
the explicit announce message before the data can be accessed. Finally,
after all data blocks have been transferred to the remote buffer, the
DMA queue is freed if it was created. The established connection is
released, and terminated if it is no longer used, by the communication
helper’s DisconnectFromRemote .

For the receiver of a blob data transfer the GetBlobData and
GetBlobOffset functions allow access to the received data by returning a
direct pointer to the data or the data’s offset from the blob buffer’s
beginning respectively. Both functions check the list of used blocks in
the buffer to verify the specified transfer ID by comparing it with the
block’s offset in the buffer. To free a block used by transferred data,
the ReleaseBlob function is used that searches for a used block with the
starting offset specified by the transfer’s ID. A found block is removed
from the used block list and merged into the free blocks list for
further use.

Much of the BCLSCIBlobCommunication class’s functionality is contained
in the RequestHandler function that runs in the background thread
started by the Bind function to handle request messages and replies. The
function consists of a loop in which messages are received by the
associated message communication object and then processed according to
their type. As for the TCP class, the request handler handles three
query messages with their respective replies and four other request
messages. Of the three queries, the first one is for a blob’s address,
and this is answered with a reply containing the three parameters of the
blob object’s own address. In reply to the second query for the blob’s
receive buffer size the appropriate size is sent to the query message’s
sender. The third query request type is a query for a block in the
object’s receive buffer. A matching block has to be searched first by
the object’s buffer manager in the free block list. If a matching block
is found, it is moved into the used block list. Its parameters are
placed into the reply message sent back to the requesting object.
Without a matching block a negative reply is sent back to inform the
sender that no block is currently available. Reply messages to queries
sent by the object are all handled in an identical manner. A reply is
placed into a list of received replies by the request handler, and the
signal object on which all functions expecting a reply wait is
triggered. Upon waking up, each of these functions checks the list of
received replies for its expected reply. The list is checked by
comparing a unique identifier tagging the queries upon sending and
copied into a reply message by the receiver. A matching reply is removed
from the list for processing by its corresponding function.

All four request messages handled by the request loop are for explicit
connections between blob objects and their associated message objects,
to establish, terminate, lock, and unlock connections. For each of these
four actions, a message is sent to the remote blob object concerned
after the respective action has already been performed by the sending
object to initiate the same action for the reverse connection to the
sender. If such a request message is received in the request handler
loop, the corresponding functions for the associated message object and
the blob object’s own communication helper object are called.
Establishing and terminating of connections is handled by calling the
object’s Connect or Disconnect function respectively. This function in
turn calls both the helper’s and the message object’s corresponding
functions. To distinguish this from the case of explicitly called
functions to initiate or terminate a connection, the remote blob
object’s address is passed directly to the object’s function. For
connection locking or unlocking the class’s internal LockConnection or
UnlockConnection function is called, which just calls the message
object’s locking function, if that object supports the locking feature.
As the SCI blob class itself does not support connection locking, no
further action has to be performed in this case.

## Chapter 6 The Publisher-Subscriber Framework Interface

### 6.1 The Publisher-Subscriber Interface Principle

For the exchange of data between the different components in the
framework an interface has been developed conforming to the requirements
specified in section 3.2 . This interface is made up of several classes
which together provide the communication between components following
the publisher-subscriber paradigm. This section details the actual
software architecture and implementation according to the requirements
outlined in section 3.2 .

Two separate class trees, displayed in Fig. 6.1 , make up the class
hierarchy for the publisher and subscriber part of the framework
respectively. At the root of each tree is an abstract base class that
defines the calling interface of the corresponding part of the component
interface. Each class in one of the two trees addresses its counterparts
in the other tree through this defined interface.

The model behind the component interface is a data producing component
containing a publisher object through which it makes its data available.
A data consumer component contains a subscriber object which receives
published data from a producer and performs the necessary processing on
it. Communication between these two object types should be encapsulated
so that they can communicate when they are situated in different
processes and address spaces as well as when they are present in the
same process, directly calling each others functions. For this reason
the implementations of the two classes use the interface of their
respective opposite tree and do not contain any built in communication
primitives. Communication between objects in different processes is
handled by proxy objects that implement the corresponding opposite
interface and do not process the calls but only forward them to their
own counterpart in the remote process who calls the target object’s
corresponding function. These communication classes are called publisher
and subscriber proxy classes respectively. The advantage of this
approach is that publisher and subscriber object can be either situated
in separate processes, calling functions of proxy objects for
communication with their counterpart, or in the same process, directly
calling each other’s functions.

An example of this principle is sketched in Fig. 6.2 which shows a
producer process containing a publisher object and a subscriber proxy as
well as a consumer process with a publisher proxy and a subscriber
object. When new data is available the publisher calls the subscriber
proxy’s new data function which collects the specified information and
sends it to the publisher proxy in the consumer process. This proxy
retrieves the received information and calls the subscriber object’s new
data function with this data. As soon as the subscriber object in the
consumer process has finished processing the data, it calls the
publisher proxy’s data finished function. The publisher proxy again uses
the specified data and sends it to its subscriber proxy counterpart in
the producer process. Using this information the subscriber proxy calls
the data finished function of the publisher object that can release the
produced data and continue. Fig. 6.3 shows the same process as a UML
sequence diagram.

The primary purpose of the interface is to allow a data producer to
announce newly available data, for example new ALICE events, to multiple
subscribers. Placing the data in shared memory in the producer process
implies that the management of the data’s buffers has to be handled by
the producer process which needs to know where to place new data. In
order for the producer to know when some specific data can be released
again, it has to be informed by each of its subscribers when they have
finished working on each received data block.

For efficiency reasons it should also be possible for consumers to
collect finished events and inform their producer about multiple
finished events in one call. During such an aggregation process by one
or multiple consumers their producer process might run out of buffer
space for new event data before the consumers have received and finished
enough events to inform the producer about their released events. In
such a case a producer has to be able to send a high watermark to its
consumers when it threatens to run out of buffer space. For a
non-blocking or transient type of consumer, a producer must also be able
to forcibly cancel a consumer’s access to an event’s data buffers. This
is necessary to free a buffer when the consumer uses and thus locks the
buffer too long.

In some cases it might be desirable for a consumer to send some data
about the processing of an event back to the producer along with the
finished event information. An example for this in the ALICE HLT are the
HLT trigger decisions for an event that have to be communicated back
along the path that the event data has taken. To support this ability,
the calls informing about finished events allow to attach arbitrary
information to each event. This information is treated as opaque to the
interface itself and is just transported from subscriber to publisher.
For some consumers it might also be of interest to receive this kind of
event finished information produced by other consumers attached to the
same producer. Support for this is provided by allowing the subscriber
to set a flag in their publisher to indicate interest in this
information. Whenever new event finished information becomes available
afterwards it will be forwarded to this consumer.

For consumers that want to reduce the amount of data or events they
receive from their producer two approaches are possible that can also be
used in combination. The simpler possibility is to set a modulo
restriction based on the event sequence number so that only events with
a sequence number evenly divisible by that specifier will be published
to the consumer. The more complex but also more flexible approach allows
to use tags called trigger words associated with each event. A consumer
has the possibility to specify a set of trigger words, and only events
that have a correct trigger word set will be announced to that consumer.

One final necessity for the interface is that each side has to be able
to query whether its partner process is still alive. This is
accomplished by using ping calls which must be answered within a
predefined time by an acknowledge call. If this answer is missing, the
partner can be presumed to be locked up or have terminated.

In the rest of this chapter first a number of basic data types used in
the interface’s implementation are presented. In the following the
definitions of the central interface classes as well as the proxy
classes, used for communication between publisher and subscriber
processes, are described. The final two parts respectively describe
implementations of publisher and subscriber classes providing actual
functionality.

### 6.2 Auxiliary Data Types

In the following section a number of datatypes will be described that
have been defined for use in the publisher-subscriber interface. These
types are used in the definitions of both publisher and subscriber class
interfaces. The first subsection 6.2.1 lists simple datatypes with no or
very little inner structure while the following subsection 6.2.2
contains the descriptions for more complex structured data types.

#### 6.2.1 Flat Types

##### Integer Types

To ensure type compatibility in a system that can use multiple node
architectures a number of unsigned integer types have been defined that
always have the same size and thus value range on different systems. The
definition is made from the basic datatypes defined for the C/C++
language using #ifdef preprocessor directives. Four different tpes are
defined with 1, 2, 4, and 8 bytes (or 8, 16, 32, and 64 bits), named
AliUInt8_t , AliUInt16_t , AliUInt32_t , and AliUInt64_t respectively.
For the 64 bit type an exception has to be made as a type of that size
is in general not supported by compilers on 32 bit platforms and so a
GNU Compiler extension, the unsigned long long datatype, had to be used.

##### The Event ID

For identification purposes each event has to be tagged with a unique
ID. The AliEventID_t datatype used for this task is simply defined to be
identical to the previously declared 64 bit unsigned integer type
AliUInt64_t . Depending on the application equal sized structures can be
overlaid over this flat datatype. The most simple possible use would be
to just encode a unique sequence number into this type. A more complex
use could store a timestamp, as for example returned by gettimeofday ,
using the higher 32 bits for the second portion and the lower 32 bits
for the microseconds.

##### The Node ID

Unique identification of the nodes in a cluster running a data flow
chain is provided by the AliHLTNodeID_t datatype. As it is assumed that
every node in such a cluster will be equipped with at least one TCP/IP
network interface, the 32 bit IP address of that interface is used as
the node’s ID. To store this address ID the AliHLTNodeID_t type has been
defined to be the 32 bit unsigned integer AliUInt32_t type. In the case
of multiple interfaces and IP addresses for one node the first address
returned by the gethostbyname system call will be used.

#### 6.2.2 Structure Types

##### The Data-Type Specifier

To allow the specification of the type of data stored in a memory block,
the AliHLTEventDataType type was introduced. It is basically again the
64 bit large unsigned integer AliUInt64_t overlayed with an additional
structure as an 8-character-array. This array overlay allows to print
the datatype, for example for debugging purposes. The prerequisite to
this is that the data type is specified in a format such that each of
the 8 characters is actually printable. For the application in the ALICE
High Level Trigger “ ADCCOUNT ”, “ CLUSTERS ”, or “ TRACSEGS ” are among
the possible values.

##### The Data-Origin Specifier

In analogy to AliHLTEventDataType , the AliHLTEventDataOrigin type
allows to specify the origin of the data stored in a memory block. It
uses the same principle of overlaying an array of 4 characters over a
32 bit AliUInt32_t ID. For the ALICE HLT this can contain the detector
where the data originated from. Possible values are “ TPC “ or “ DIMU ”
for the TPC and DiMuon arm respectively.

##### The Shared Memory Identifier Structure

This structure, named AliHLTShmID , holds the information required to
access the shared memory areas where event data published by a data
producer process is stored. It contains two fields, the first of which
is an AliUInt32_t member that defines the type of the shared memory
segment. Possible values currently define an invalid segment, a big
physical area shared memory segment (bigphys), or a shared physical
memory segment (physmem). In the second field the actual ID of the
shared memory segment is contained in an overlayed AliUInt64_t
/8-character-array organization as described above.

##### The Basic Structure Header

Every complex data structure that will be passed between processes will
contain at its beginning an instance of this AliHLTBlockHeader header
structure. To allow for an opaque transport of such structures, the
header contains as its first element a 32 bit unsigned integer holding
the length of the whole structure in bytes. The next two fields allow to
specify the type of the structure as a type and subtype combination.
Both use the unsigned integer/character array overlay principle of the
two preceeding types, with a 32 bit/4 byte size for the type field and
24 bit/3 byte size for the subtype. The last field in the header
structure is an 8 bit unsigned integer carrying a version number for
each structure type which allows to add elements to a structure.

##### The Sub-Event Data Block Descriptor

Information describing a block of data stored in shared memory is
contained in the AliHLTSubEventDataBlockDescriptor structure type. Since
a block descriptor will not be exchanged between processes by itself but
only as part of a AliHLTSubEventDataDescriptor structure described
below, it does not contain an instance of the header structure discussed
above. The first element of the structure is the ID of the shared memory
segment holding the described data in the form of an AliHLTShmID field.
Following this there are two 32 bit unsigned integer fields that contain
the starting offset of the block relativ to the beginning of the shared
memory segment and its size in bytes.

Behind these fields required to access the data, five more fields are
defined which describe the data in the shared memory itself. The first
of these is the ID of the node that produced the data, followed by two
fields with the type of the data and its origin. Each of the three
fields is of the corresponding type described above. Finally, two 32 bit
unsigned integers are available, the first of which can contain a
specification of the data under the control of each application while
the second contains the byte order that the data has been stored in.
Fig. 6.4 shows the definition of this type.

If the datatype of a block has the value “ COMPOSIT ”, then the data
block described contains another AliHLTSubEventDataDescriptor structure
described below, allowing hierarchical event descriptions.

##### The Sub-Event Data Descriptor

To describe the information for a whole subevent the
AliHLTSubEventDataDescriptor can be used. This structure’s first element
is a header of the AliHLTBlockHeader type followed by an AliEventID_t
field containing the ID of the event concerned. The next two elements
are 32 bit unsigned integers holding event time information, the seconds
and microseconds of the timestamp of the event’s creation. After these
elements another 32 bit unsigned integer is placed that contains a
timestamp specifying the maximum allowed event age in the system. This
timestap is specified in seconds as returned by the Unix time function.
Using it information can be broadcast through a system to purge events
from the system that have not been freed properly, preventing the
slowing down or filling up of the system.

The sixth field of the structure contains the datatype of the whole
event. If all the datablocks of the event are of the same type, then
this field can contain this datatype specifier. Otherwise this field
contains the specifier “ COMPOSIT ” to indicate a composite event.
Behind this field there is another AliUInt32_t element holding the
number of data blocks contained in this descriptor followed by the
corresponding number of AliHLTSubEventDataBlockDescriptor structures
containing the information for each of the data blocks. Fig. 6.5 shows
this type’s definition.

Hierarchical event descriptions are supported by allowing data blocks to
contain locations of further AliHLTSubEventDataDescriptor structures,
placed in shared memory as described in the previous section.

##### The Event Trigger Type

In the AliHLTEventTriggerStruct data is contained that characterizes a
particular event and allows a subscriber to select only a particular
subset of events for processing. For applications in high-energy or
heavy-ion physics this could be trigger information received from
preceeding trigger levels specifying a type of event. Since the
organization of this type of data cannot be known in advance by the
framework, this type has no complex inner structure. It contains only
the header field as well as an AliUInt32_t holding the number of 32 bit
data words in the structure and another AliUInt32_t marking the
beginning of the data word array. For determining matches between
structures a comparison function type is defined. A comparison function
which performs a bytewise comparison of two structures is supplied in
the framework.

##### The Event Done Data Type

The structures of the AliHLTEventDoneData type contain information
transferred back from a subscriber to a publisher about events whose
processing has been finished. This additional information in these
structures is opaque to the framework, which only transports the data
for interpretation by the higher layers of the system’s components.
Therefore, application specific data is implemented similarly to the
event trigger type described previously, as one 32 bit unsigned integer
holding the number of data words in the structure followed by the array
of 32 bit unsigned data words itself. Preceeding these fields are the
usual header structure and a field with the ID of the event concerned.
In the following text the expression “non-trivial event done data” will
be used to describe an event done data structure that contains at least
one 32 bit data word.

### 6.3 The Interface Definition Classes

#### 6.3.1 The Publisher Interface

Fig. 6.6 shows the interface for publisher classes,
AliHLTPublisherInterface , as a UML class diagram. This interface
defines abstract methods for each of the tasks described in section 6.1
. In the AliHLTPublisherInterface class one data member and one
non-abstract member function are contained. The data member holds the
name under which the publisher referred to by an object is known. It is
returned by the GetName member function.

The first two of the methods defining the publisher interface, Subscribe
and Unsubscribe , handle subscribing and unsubscribing to a publisher.
Both methods accept as only parameter a reference to the subscriber to
be subscribed to. Once a subscriber object is subscribed to a publisher,
its type can be set with the SetPersistent method. It requires a
reference to the subscriber concerned as its first parameter together
with a boolean flag specifying whether the subscriber should be treated
as persistent or transient. Initially after subscription, all
subscribers are marked as persistent requiring the method to be called
for transient subscribers only.

Using the two following SetEventType functions, it is possible to scale
down the number of events received by a subscriber from its publisher.
In the first of the functions the event sequence modulo specifier is set
that scales down the event rate to the given ratio. Only events whose
sequence number is evenly divisible by the specified modulo number will
be announced to this subscriber. The second variant of the functions
accepts a vector of trigger word structures, described in section 6.2.2
, together with a modulo specifier identical to the one passed in the
simpler function’s version. Specified trigger word structures are stored
associated with the subscriber, and each new event is checked for a
match with one of them.

A method of interest to transient subscribers only is
SetTransientTimeout . It allows to set the minimum timeout before an
event used by a transient subscriber can be cancelled. A publisher will
cancel a transient subscriber’s event only when all persistent
subscriber have released it and at least the amount of time specified in
this call has passed. To prevent transient subscribers from setting
arbitrarily large intervals it is suggested that publisher
implementations have a separate allowed maximum for this timeout.
Transient subscribers should thus be prepared for shorter timeouts than
specified. The last subscriber configuration method is the
SetEventDoneDataSend method, that also uses a boolean flag as its
parameter. This flag specifies whether the subscriber concerned is
interested in receiving the data sent along with finished events from
other subscribers. Since this information might be needed by the
subscriber to properly process the event, this information is forwarded
immediately after another subscriber has released a specific event in
the publisher.

To inform a publisher that processing of an event has finished, a
subscriber calls one of the two EventDone methods in the publisher
interface that differ only with respect to the arguments taken. In the
first call one AliHLTEventDoneData structure argument, described in
6.2.2 , is accepted to release only one event while the second version
allows to release multiple events in one call by accepting a vector of
these structures. As described previously, each of the structure
arguments contains the identifier of the event to be released.

By calling the StartPublishing method, a subscriber activates publishing
of new events for itself. After calling this method a subscriber will
receive all new events as they become available in the publisher. This
call will not return immediately but instead will enter a loop until
Unsubscribe has been called. The final two methods are used by a
subscriber to test a publisher’s availability ( Ping ) or to reply to a
publisher to acknowledge the subscriber’s own availability ( PingAck ).
An optional boolean flag, which can be given as the second parameter,
allows to specify that all events currently contained in the publisher,
already announced to other subscribers, should be announced to the
subscriber concerned. If this flag is not set, only events which arrive
in the publisher after the StartPublishing call will be announced.

#### 6.3.2 The Subscriber Interface

Fig. 6.7 shows the interface for subscriber classes
AliHLTSubscriberInterface as a UML class diagram. This interface defines
abstract methods for each of the subscriber tasks described in section
6.1 . Similar to the publisher interface class AliHLTSubscriberInterface
also contains the subscriber’s name as the only data member and the
non-abstract member function GetName to return that name. In analogy to
the publisher interface all defined abstract subscriber interface
functions require a reference to the publisher object calling the
corresponding subscriber function as their first argument.

NewEvent , the first of the abstract member functions defined for the
subscriber interface, will be called when the publisher to which the
respective subscriber is attached announces new data. It is therefore
the most important subscriber member function. In addition to the
calling publisher’s reference it accepts three more arguments. The first
of these is an AliEventID_t containing the ID of the event being
announced. Following is an AliHLTSubEventDataDescriptor holding the
information about the actual data blocks contained in the event, mostly
located in shared memory. The final argument is an event trigger
structure of the AliHLTEventTriggerStruct type, containing information
that more closely characterizes the event concerned. Event trigger data
is passed to the subscriber so that it can determine the processing of
the event based on this trigger information.

The second abstract subscriber interface function is EventCanceled . It
is called for transient subscribers whenever the publisher to which they
are attached cancels an event before the subscriber itself has finished
working on it. After this function has been called a subscriber should
not rely on any data located in shared memory to still be valid.
Processing on this event should be stopped as soon as possible after
this function has been called. Parameters to this function are the
calling publisher’s reference and the ID of the event being cancelled.

Next is another notification function, that can be called while any
subscriber has not yet finished processing an event. EventDoneData is
called when the publisher receives event done information from another
subscriber and this subscriber has requested to receive this kind of
data using the publisher’s SetEventDoneDataSend function. The function
will be called with the exact AliHLTEventDoneData structure that the
publisher received from the other subscriber, provided that the
structure is non-trivial, containing at least one data word.

When the publisher cancels a subscription the SubscriptionCanceled
function of the subscriber concerned is called. Such a cancellation
might happen because the subscriber has called the publisher’s
Unsubscribe function or because the producer process is ending. Each
subscriber’s ReleaseEventsRequest function is called when the publisher
threatens to run out of buffer space to signal that the subscribers
should release events as soon as possible. The final abstract member
functions defined for the subscriber interface are the Ping and PingAck
functions that have the same meaning as the respective functions in the
publisher interface described in section 6.3.1 .

### 6.4 The Proxy Classes

Beyond the functions defined already in the publisher and subscriber
interface definition classes, two derived classes exist defining
additional interface functions for the two types of proxy classes. These
two interface classes are shown in Fig. 6.8 with their respective,
additionally defined functions.

Both classes define a SetTimeout function allowing to specify a
communication timeout used by proxy implementations in a program. Since
only the proxy classes are intended for communication between processes,
but not publisher or subscriber classes in general, it is reasonable to
place this function in these classes. The AliHLTPublisherProxyInterface
class defines one additional abstract function, AbortPublishing . Its
purpose is to terminate the publishing loop started by the proxy classes
when their StartPublishing function is called as described below.
Usually this function ends when a SubscriptionCanceled message is
received from the publisher. If, however, the publisher process has died
or the connection between publisher and subscriber processes is
interrupted or broken, the publisher proxy cannot determine when to
leave the message loop.

Two additional functions are defined in the
AliHLTSubscriberProxyInterface class, MsgLoop and QuitMsgLoop . The
MsgLoop function is intended to contain the implementation of the
proxy’s loop for receiving and handling messages from the opposite
publisher proxy in derived classes. It should be called in a separate
thread from the publisher object to which the proxy is attached. When
the loop has to be terminated because the subscription has been
cancelled, the publisher calls the third defined function, QuitMsgLoop ,
whose purpose is to end the message loop.

In the following section the proxy classes present in the current
framework are described. They can be divided into two types categorized
by the type of communication between publisher and subscriber proxies:
pipe proxies and shared memory proxies. Communication for the two pipe
proxy classes is done via named pipe [ 124 ] system resources while the
shared memory proxies communicate via System V shared memory [ 120 ] , [
121 ] , [ 122 ] , [ 124 ] . In addition to these four proxy classes the
final part of this section covers the subscription loop, the mechanism
for subscribers to register with publisher objects in other processes.

#### 6.4.1 The Pipe Proxy Classes

##### The Pipe Communication Class

Named pipes used for communication between the pipe proxy classes are
encapsulated by the AliHLTPipeCom class. This class supports two pipes
simultaneously, one for reading and one for writing, as the pipe
communication between the proxies is executed via two named pipes. One
of these is used for communication from publisher to subscriber and the
other from subscriber to publisher. Two pipes are used to avoid lockup
situations. The naming of the pipes is based on a scheme that places all
pipes in the /tmp directory and assigns a base name identical to the
subscriber’s ID to them. Each pipe has an additional suffix, either
PublToSubs or SubsToPubl depending on the flow of communication. The
full file names for a subscriber whose ID is TestSubscriber then are
/tmp/TestSubscriberPublToSubs and /tmp/TestSubscriberSubsToPubl . The
pipe class offers an interface for reading and writing similar to the
system read / write function calls. This interface allows direct
specification of the number of bytes to read or write together with a
memory block where the write data was stored, respectively where read
data should be stored. Functions of this interface are shown in Fig. 6.9
.

In addition, a second interface is supported that directly allows to
read and write structures described by an element of the
AliHLTBlockHeader type, detailed above in section 6.2.2 . This interface
is shown in Fig. 6.10 . For writing, this version of the Write function
requires a pointer to such a block header structure as its primary
argument. It will write as many bytes from the memory pointed to as
specified in the header’s length field. In the Read function the header
structure is read first and then the required amount of bytes is
allocated as specified in the header’s length field. The header already
read is copied to the beginning of this memory block and the rest of the
data is read and placed directly into that memory. In the function’s
pointer reference argument the pointer to the allocated memory is then
returned to the calling code that later has to free the allocated memory
again through a call to delete [] .

To avoid blocking, e.g in case a process attempts to write to a pipe
that has been filled because the reader process has died or locked up,
all read and write function calls accept an additional boolean argument
specifying whether the call is to be blocking or non-blocking. For
non-blocking calls a member variable in the pipe object is used that
specifies the timeout to use when a call would block. This timeout can
be specified on a microsecond granularity.

In order to reduce the number of system read calls in a more efficient
reading mechanism, the pipe communication class implements a caching
strategy, similar to what is implemented in the BCLIntTCPComHelper class
described in section 5.4.2 . Each object contains a buffer of 4 kB size,
the maximum amount of data that can be read from or written to a pipe in
one system call. If this buffer is empty upon a read call, the object
tries to read the full amount of 4 kB into this buffer at once. Since
the pipes are opened in non-blocking mode the read attempt will read as
much data as is available up to the specified amount. Data that has been
requested by the calling code will be provided from this cache until it
is exhausted, saving a number of read calls.

##### The Publisher Pipe Proxy Class

Handling the publisher part of the communication based on named pipes is
the task of the AliHLTPublisherPipeProxy class, implementing the
abstract functions defined in the publisher interface. Two named pipes,
provided by an instance of the AliHLTPipeCom class described above, are
used for the two communication directions. Additionally, the proxy uses
a third named pipe, encapsulated in another pipe object, for connection
to a publisher’s subscription loop, as described below in section 6.4.3
. This pipe object uses just one of the two possible pipes as no reading
will be done from it, and only the subscription request will be written.
Fig. 6.11 shows the UML relationship of the proxy class and the pipe
communication class.

In the Subscribe function the publisher pipe proxy first tries to open
the subscription request pipe to the producer process’s subscription
loop. Then the two pipes used for the actual communication between the
publisher and subscriber processes are created. It is mandatory that
these pipes are created by the publisher proxy in the subscriber process
and that they are already present when the subscription request is
written into the request pipe. The subscription message sent to the
publisher contains the subscriber’s name field as its identifier,
preceeded by the string “ pipe: ” to specify that pipe proxies are used.
After its construction this message is written into the subscription
pipe and the Subscribe function ends.

To unsubscribe none of these steps have to be executed by the
Unsubscribe function. Instead it starts by creating the unsubscription
message. This message also contains the subscriber’s name as the
identifier. Again the function ends immediately after writing the
message into the pipe for subscriber to publisher communication. This
communication can also be seen as communication between a publisher
proxy in the subscriber process and a subscriber proxy in the publisher
process.

Both messages created in the functions described contain the block
header structure as their first element. They can be written into their
respective pipes via the block header write functions. For the rest of
the functions the messages follow the same principle. Any additional
parameters needed by the messages are encapsulated into data structures
that contain this header structure as well and can also be written using
the header write functions.

For efficiency reasons, calls which accept parameters and that thus have
to send multiple structures do not actually send each structure with a
separate write call. Instead a coalescing step is taken where a block of
memory is allocated with a size large enough for all data to be sent.
The message, all parameters, and any additional data are then copied
into that block, and the block is passed to the pipe object for writing
using a single call to the Write function. When the allocation of the
buffer block fails, seperate calls to the block header Write function
are used.

At the end of the StartPublishing function, after the message and
parameters have been written, the function does not return but instead
enters the message loop function. In this loop the publisher proxy waits
for messages that arrive on the publisher-to-subscriber pipe. Received
messages are checked for the correct header identification and are then
handled according to their respective type. For most of the messages,
reading of the necessary parameter structures and their decoding into
normal C++ parameters is done in separate functions. In these functions
the parameter structures are also checked for the correct header
identification. Once the C++ parameters have been obtained the
corresponding function in the attached subscriber object is called with
these parameters. When the message corresponding to the
SubscriptionCanceled method is received, the message loop is ended
normally.

To ensure the correct transport of the data through the named pipes, the
publisher proxy class has the ability of performing a 32 bit Cyclic
Redundancy Checksum (CRC) [ 128 ] over the data for each message
including its parameters. This checksum is sent after the actual data.
In the subscriber proxy’s receiving message loop the same checksum is
calculated using the received data. The subscriber proxy’s checksum is
then compared with the value calculated and sent by the publisher proxy.
If the comparison indicates that an error occured, this is reported, and
the received data is discarded without further action. The publisher
proxy message loop and the subscriber proxy functions implement the
identical error checking mechanism. This capability can be activated
using #define statements at compile time of the classes.

##### The Subscriber Pipe Proxy Class

In the producer process it is the AliHLTSubscriberPipeProxy ’s task to
handle the subscriber part of the named pipe communication. For this
purpose it implements all abstract functions defined in the subscriber
proxy interface. This class uses only one pipe communication object to
handle the same two publisher-subscriber pipes. The meaning of the pipes
with regard to reading and writing is of course reversed with respect to
the publisher proxy class. A subscriber proxy object is created in the
producer process only when a subscription has taken place, and the pipes
are created at the beginning of the publisher proxy’s Subscribe
function. Therefore the pipes will exist already upon creation of an
object of this class. In the class’s constructor they thus only have to
be opened. Fig. 6.12 shows a UML diagram of the relationship between the
proxy and pipe communication classes.

As for AliHLTPublisherPipeProxy , the implemented interface functions
mainly create a message object, identical to the publisher proxy’s
message object, and several parameter objects as required to hold the
necessary arguments. These data structures are then coalesced and sent
utilizing the same mechanisms as used in AliHLTPublisherPipeProxy . Only
if the coalesced sending call fails, the structures are sent with
separate Write calls for each of them. The subscriber proxy’s functions
and its message loop implement the same optional CRC error checks as the
publisher proxy, as has been described in the previous section.

Unlike the publisher proxy class, no call to one of the interface
functions causes a subscriber proxy object to enter the message loop
function defined for subscriber proxy objects. Instead the message loop
has to be called explicitly by a publisher object after a proxy object
has been subscribed to it, for concurrency preferably in a separate
thread. In analogy to the publisher proxy class the message loop calls
functions to handle the more complex messages with multiple arguments
for parameter extraction. Using the extracted parameters, the proxy
calls the appropriate interface functions in the publisher object to
which it is attached. A further difference to the publisher proxy class
is that the message loop does not automatically end when a specific
message is sent or received. As is the case for starting the loop, it
has to be terminated explicitly by a call to the QuitMsgLoop function
from the parent publisher object.

#### 6.4.2 The Shared Memory Proxy Classes

##### The Shared Memory Communication Classes

In analogy to the pipe communication class for the pipe proxies, the two
shared memory proxy classes also rely on a common base class,
AliHLTShmCom , to handle the interaction with the System V shared memory
functions. In addition, the class also handles the buffer management for
the shared memory blocks used for communication between the proxies.

Since System V shared memory segments cannot be identified by names but
only by integer IDs, such an ID has to be passed to the communication
object together with the segment’s size. These two arguments are passed
to the object’s constructor where the shared memory segment will be
created or opened. To support the case where a communication partner
connects to a segment already created by its partner, the class’s
constructor accepts a flag argument. This flag allows to specify whether
the object should create the segment specified or whether it should just
try to connect to an existing segment.

One problem encountered in communication via shared memory is that it
does not support suspending a process while waiting for data, notifying
and waking it up when data has become available. Similarly, it is not
possible to wait when no space is available for writing, to be notified
after enough space for the attempted write operation has again become
available. If one were to use continuous polling of the parameters that
indicate available data, this would result in a high CPU load on the
system. As a compromise the approach chosen for this class uses a number
of read or write attempts followed by usleep calls. The usleep calls use
the minimum granularity available to processes on a Linux system of
10 ms. To reduce the impact of this rather high sleep time, the number
of poll retries executed before sleeping can be configured for each
object during runtime.

Similar to the pipe communication class the shared memory communication
class also supports multiple kinds of read and write calls, although in
this case they do not just differ in ease of use but also in the degree
of efficiency supported. The first function set for reading and writing
in Fig. 6.13 works identical to the basic functions provided in the pipe
communication class. Both functions accept a size specifier for the
amount of data to read or write and a pointer to the data buffer. Their
final parameter is a flag, specifying whether the call should block
indefinitely or use a specified timeout. Using this interface the data
is copied by the CPU from the source memory to the destination by memcpy
calls.

The second call interface, shown in Fig. 6.14 , only provides two
functions to allow reading, while no support for data writing is
present. These functions contain support for reading data structures
described by a block header structure at their beginning, analogous to
the second interface type of the pipe communication class. Unlike the
two functions from the first set, this interface does not always perform
copy operations for the data. Instead the first of the two functions can
return a direct pointer to the data structure located in the shared
memory segment, avoiding the copy steps. To prevent overwriting of data
while it is still in use, the occupied memory is not directly marked as
available again. This is performed by the second function of this set,
which can be called when the read data can be released. Copy steps are
only necessary if the read data, described by the header structure, is
wrapped around in the buffer. In such a case the first part lies at the
buffer’s end and the second at its beginning. This situation is handled
by allocating a further memory block of the required size and copying
the data from the shared memory buffer into that memory by two memcpy
calls. The allocated memory is returned as the pointer to the block
header structure. When the free function is called in this case it does
not only release the memory in the buffer but also frees the
specifically allocated memory again. In the first of the two functions,
Read , five arguments are accepted. Only the first and last of these
arguments are relevant to the user. The first is a reference in which
the pointer to the structure will be returned and the last is an
optional flag that indicates whether or not the function should block
while waiting for data. Two pointers and a size specifier make up the
remaining three arguments in which information is returned from the
function that the second function, FreeRead , uses to determine whether
the memory with the data has been allocated or is in the shared memory.
Except for the block argument, which does not apply to the free
operation, the remaining four parameters supported by the Read function
have to be passed in the call to the FreeRead function to provide it
with the required information to release the block or blocks.

The third interface, shown in Fig. 6.15 , also allows a more efficient
approach to communication by using a set of two functions for writing
and three for reading. For writing only the size of the data to be
written is passed to the first of the two functions, CanWrite , together
with a flag specifying blocking or non-blocking mode. Three parameters
are returned by the function, two pointers and another size specifier.
When the memory block for writing the specified amount of data is
present as one block in the shared memory buffer, then the second
pointer and the returned size specifer are both zero. In the first
pointer parameter a pointer to the target block in shared memory is
returned. In contrast, when the block for writing wraps, then the first
pointer points to the part of the block located at the buffer’s end and
the second pointer to the one at the buffer’s beginning. The returned
size specifier contains the size of the block’s first part located at
the buffer’s end. Using the two pointer arguments, the data can then be
written into the shared memory buffer. To avoid copying, the data can
even be directly created in the shared memory, taking into account the
two parts of the block. Once the data is present in the buffer, the
second function HaveWritten can be used to commit it and make it
available for reading. HaveWritten requires the first four of CanWrite
’s parameters, the fifth blocking parameter is not applicable. Using
these parameters it determines whether the block is in one piece or
wrapped around and then accordingly sets the amount of data written in
the object’s internal structures.

Of the three functions available for the read part of this interface,
two CanRead functions are used to determine whether data is available
for reading. These functions only partly differ in their arguments,
having five of them in common: a size parameter specifying how many
bytes to read, two pointer arguments, a second size specifier, and a
blocking/non-blocking flag. In combination the two pointer arguments and
the additional size specifier basically have the same function as in the
CanWrite function, specifying the memory where the data to be read is
located either as one block or as two wrapped around parts. The
difference between the two functions is an initial flag argument,
available in one of the functions. The flag specifies whether the read
indices should be updated and mark the data whose pointers are returned
as read, or whether the function should just peek for available data and
not modify any indices. In the function without this additional flag
argument the first CanRead function is called with all its specified
parameters and the peek flag set to false. This function will thus mark
the data as read so that the next CanRead call will return pointers to
the next available data block. Once the data has been accessed and can
be released, the third function, FreeRead , is available to release the
data and make the memory blocks usable for writing new data. This
function requires the two size and pointer arguments of the CanRead
functions and updates the indices of the object to mark the block
concerned as free.

##### The Publisher Shared Memory Proxy Class

Similar to the AliHLTPublisherPipeProxy , the task of the
AliHLTPublisherShmProxy class is to handle the publisher side of the
shared memory communication in the data consumer processes. It uses two
shared memory communication objects for the two communication directions
from publisher proxy to subscriber proxy and vice-versa. Each of these
requires its own shared memory key, although both use the same size. The
communication objects and their memory segments are created by the
class’s constructor. In addition to the two shared memory segments, one
pipe communication object is used to execute the subscription through
the publisher’s subscription loop described below in section 6.4.3 .
AliHLTPublisherShmProxy ’s hierarchy and its relation to the two
communication classes is shown in Fig. 6.16 .

The subscription pipe is opened in the class’s Subscribe function using
the publisher’s name specified in the object’s constructor, in order to
build the pipe name as described for the publisher pipe proxy in section
6.4.1 . Since the shared memory segments have already been created the
subscription request message can be sent directly. This message includes
the subscriber’s name preceeded by ’ shm: ’ to indicate that a shared
memory proxy is used. Following the name the message contains the rest
of the information needed by the publisher to establish a communication:
the two shared memory keys and the segments’ common size.

In the implementations of the functions defined in the subscriber
interface, the approach used is basically always identical. The function
determines the total amount of data that it has to write for the message
and its required parameters. This size is then passed to the call of the
CanWrite function of the publisher-proxy to subscriber-proxy shared
memory object to obtain the shared memory block for writing the message.
If this block in the shared memory segment consists of only one part and
is not wrapped around, then the message and its parameters are created
directly in the memory block just allocated. Additional data can be
copied from function parameters as necessary. If the data is not in one
block but wrapped around, an additional memory block is allocated and
the message and parameter creation functions are called using this local
memory block. Once all the required message data is present in this
block it is copied into the two parts of the shared memory block through
two memcpy calls. After these steps the HaveWritten function is called
to commit the data and update the write indices in the communication
object appropriately. Once this is done all functions except for
StartPublishing end, indicating successful completion to the caller.

The StartPublishing function does not terminate once the message has
been written into the communication object, but calls the class’s
message loop to process messages received from the producer processes’
subscriber proxy, similar to the publisher pipe proxy. In the loop the
messages received from the subscriber proxy are read using the block
header read function of the subscriber-proxy to publisher-proxy
communication object described earlier. For the three messages NewEvent
, EventCanceled , and EventDoneData that require multiple parameters,
handler functions are called to read the necessary parameter data from
the communication object. The parameters are subsequently decoded and
the appropriate function in the attached subscriber object is called
with them. For the other, simpler, messages the subscriber functions can
be called directly without the need for further parameters. After
calling the appropriate subscriber’s functions the data in the shared
memory is released again using the appropriate FreeRead calls for the
message and its parameters.

##### The Subscriber Shared Memory Proxy Class

The subscriber shared memory proxy class AliHLTSubscriberShmProxy
performs the subscriber proxy functions in the producer process,
analogous the subscriber pipe proxy class described in section 6.4.1 .
For this purpose it implements the functions defined in the subscriber
interface in a similar manner as in the shared memory publisher proxy
class’s functions described in the previous section. Each function
determines the size of the message to send to the publisher proxy
together with necessary parameters and other data. Message data is
created either directly in the shared memory buffer used for the
subscriber-proxy to publisher-proxy communication, or it is created in
an intermediate memory block and then copied into the shared memory
segment.

As for the pipe subscriber proxy, no interface function called will
cause a message loop to be entered. Instead the publisher to which this
subscriber is attached to has to call the MsgLoop function, defined in
the AliHLTSubscriberProxyInterface class, to run in a separate thread.
Once the subscription has been cancelled the publisher has to call
QuitMsgLoop to exit the message loop and terminate the thread.

#### 6.4.3 The Subscription Loop Function

Related to the proxy classes is the subscription loop function
PublisherPipeSubscriptionInputLoop which should be called by any
producer process in a separate thread to wait for incoming subscription
requests. Its only parameter is a reference to the publisher object
whose subscription requests it should handle. From the object it obtains
the publisher’s name used to create the full name of the subscription
pipe so that it is located in the /tmp directory and consists of the
name of the publisher with the appended SubsService identifier.

A pipe communication object as described in 6.4.1 is used to create and
open a pipe with the constructed name. In the loop a blocking read is
entered to wait for incoming messages with subscription requests. As
each subscription request is contained in a single message described by
a block header structure, a single Read call is sufficient to retrieve
the data needed for a subscription. When a subscription request has been
received, the function strips the type specifier, either pipe or shared
memory, from the subscriber’s name to determine which type of proxy to
create.

For a pipe proxy the only information needed is the name so that an
AliHLTSubscriberPipeProxy object can be created directly. In the case of
a shared memory proxy the function additionally has to extract the size
of the two shared memory segments as well as the two keys for them from
the message. Using these three parameters and the subscriber’s name an
AliHLTSubscriberShmProxy object is created.

After the correct subscriber proxy object has been created the function
calls its publisher object’s Subscribe function with the created proxy
as its argument, subscribing the proxy and its associated subscriber
object in the consumer process. Following this, the function releases
the message that has been allocated in the pipe communication object and
reenters the read call waiting for the next request.

When the subscription loop has to exit because the producer process
ends, a global flag variable is set to be evaluated in the function’s
loop. Subsequently, a quit message is sent to the loop’s named pipe.
Upon reading that message and detecting the global quit flag set, the
function leaves the loop and terminates.

### 6.5 The Publisher Implementation Classes

Only one publisher implementation class directly derived from the
publisher interface definition AliHLTPublisherInterface exists,
implementing the basic publisher functionality of managing a number of
subscribers and events. Other publisher classes are in turn derived from
this base class to extend its functionality. The most important of these
classes, shown in Fig. 6.18 , are described in the following section.

#### 6.5.1 The Sample Publisher Class

AliHLTSamplePublisher is the base class for all other publisher
implementation classes. It is the only class that implements the basic
functionality of managing multiple subscribers, announcing events to
them, and freeing the events again once all subscribers have released
them. All other publisher classes inherit this functionality from
AliHLTSamplePublisher . In addition to implementing the required
abstract methods defined in the publisher interface it provides a set of
other functions that serve as the external API of this class. It also
supports a number of callback functions that allow for further
customization of a derived publisher, e.g. by implementing an action
when an event has been released. These callback functions are not
defined as abstract methods so that not every derived class has to
implement all of them but instead are present as empty virtual method
bodies. The provided external API allows other programs or classes to
use the features present in the sample publisher class, e.g. subscriber
and event handling, management, and accounting.

##### Internal Architecture

Internally the AliHLTSamplePublisher class makes use of a number of
different threads and two main lists that store the data for each
subscriber and each event respectively. An entry in the subscriber list
contains a pointer to the subscriber object or proxy, in the form of a
pointer to a subscriber interface, together with pointers to two thread
objects. These two threads are used for communication with the
subscriber object. The first is used for the subscriber proxy’s message
loop from which the publisher interface functions are called and the
second for calling the subscriber object’s interface functions. This
second thread object also implements the subscriber interface and can
thus be accessed by the publisher class similar to a subscriber. Calls
to the subscriber interface functions place the required data in a
memory FIFO of the thread class. The thread itself runs a loop which
waits for data from its FIFO object and calls the interface functions in
the subscriber object, decoupling the publisher’s main functions from
the timing behaviour of the subscriber object’s functions. This is of
particular importance when the subscriber object actually is a proxy
object that communicates with other processes and could block waiting
for them.

A subscriber data structure furthermore contains a number of fields
relevant to the subscriber’s status corresponding to the parameters that
can be set using the respective publisher interface functions. Three
flags define whether a subscriber is persistent or transient, whether it
is active and receives events, and whether a subscriber receives event
done data received from the publisher’s other subscribers. Additionally,
three fields related to the data that can be set using the publisher’s
SetEventType functions are present. Of these three elements the first is
a list holding the event trigger type structures that can be specified.
The other two are the event modulo number used to restrict the rate of
events and the number of events that have been announced while the
specific subscriber has been active. These two numbers are used to
determine whether a specific event is announced to a subscriber with a
set event modulo number. The final element in the subscriber structures
is the number of ping calls that have been made to this subscriber. This
number is increased when a Ping call is made to the subscriber object
and decreased when a PingAck call is received from it. When the number
reaches a configurable maximum the subscriber is presumed to be unable
to process any calls and is removed in the publisher.

In each element of the event list a number of fields are stored as well.
The first of these is the ID of the event whose data is stored in that
particular element. This is followed by two reference counters, one for
the total number of subscribers and one for the number of transient
subscribers to which this event has been announced. Two more fields
contain data regarding event timeouts, the maximum timeout used for that
event and the ID of the currently active timeout for that event. Two
sublists hold a list of subscribers which have not released the event
that has been announced to them and a list of all event done data
structures that have been received for that event. This last list is
used for one of the callback functions presented below.

The event list itself is organized in a manner similar to the principle
of the MLUCVector class described in section 4.2.2 : a fixed size array
used as a ring buffer. This approach is applied since events are
typically processed in an approximate first-in-first-out manner. As the
releasing of events is not guaranteed to be sequential, each entry in
the list contains a used flag that specifies whether the data contained
in the element is valid. Free slots for new events are always searched
from the end of the used space while the search for events to be freed
is started at its beginning. If no size for the array is specified in
the class’s constructor, a normal dynamic array class, the vector class
from the STL library, is used instead of the ring buffer.

Beyond the two communication threads for each subscriber, each sample
publisher object uses four more threads in addition to a program’s main
thread. The first of these is used to handle expired timeouts for each
event. It runs in a loop waiting for signals from the timer object for
expired timeouts. Any transient subscriber still locking an event with
an expired timeout is forced to release the event. A loop waiting for
timer signals is used in the second thread as well, but these timeouts
signal expirations of wait times for ping messages. After a certain
number of ping acknowledge replies have not been received, the
subscriber concerned will be removed from the publisher’s list. Cleanup
of subscribers in the process of being removed from a pulisher’s list is
the task of the third publisher thread. Subscriber data structures are
passed to this thread using a signal object. When the thread detects
that a subscriber is not used anymore, it will free any data structures
that have been allocated for this subscriber. It is necessary to use
this approach to prevent a thread from releasing a subscriber when
another thread still accesses that subscriber’s data structures. The
final of these four threads runs the timer used for every timeout in the
publisher, including event and ping timeouts.

##### External Sample Publisher Interface

Beyond the standard publisher interface for use by subscriber objects
the sample publisher class provides a second function interface, shown
in Fig. 6.19 . This API consists of seven additional functions to be
called from external functions, some of which correspond loosely to
functions defined in either the publisher or subscriber interface.
AnnounceEvent , the first of these seven functions, accepts the same
three arguments as the subscriber’s NewEvent function: an event ID, a
sub-event descriptor for the event’s data, and its event trigger type
structure. The event described by these three parameters will be
announced to subscribers currently attached to this publisher object,
depending on the trigger types and modulo counters set for each
subscriber.

To forcibly remove an event from a publisher’s internal list the
publisher’s AbortEvent function can be called. The ID of the event to be
removed is the function’s first parameter. An optional second parameter
is a flag specifying whether an EventCanceled call is made to all
subscribers that still use the event. By default this flag is true so
that the EventCanceled calls are made. When a producer program starts to
run out of buffer space for events, the RequestEventRelease function can
be called to make ReleaseEventsRequest calls to all attached subscribers
informing them of the imminent buffer shortage. To terminate all
subscriptions or call the ping function for all attached subscribers
respectively the CancelAllSubscriptions and PingActive functions are
available, both without any parameters as for ReleaseEventsRequest .

In the case that releasing events in a publisher object has to be
inhibited for a time, the PinEvents function can be called. Its argument
is a flag that specifies whether events are to be freed normally when
all subscribers have released a particular event, or whether the event
should be kept in the publisher nonetheless. A possible application case
for the function could be a subscriber that has terminated unexpectedly
and should be reattached. Any events still in the system should be kept
available so that they can be reannounced to this subscriber once it is
subscribed again. When the pinning is released, any events not in use by
at least one publisher are released immediately.

The final of the sample publisher API functions, AcceptSubscriptions ,
is called to start the subscription loop for the publisher to wait for
incoming subscription requests. It calls the function that has been
specified for the publisher using the SetSubscriptionLoop function
described below, in most cases the loop function described in 6.4.3 .

##### Configurable Functions

Customization of sample publisher objects is supported by two
configurable functions in the class. Fig. 6.20 shows the two function
pointers together with the two methods used to set them. The first of
these, SetSubscriptionLoop , allows to specify a function to be called
as a subscription loop when the publisher’s AcceptSubscriptions function
is called. This lets programs use subscription loop functions different
from the one described in section 6.4.3 , to support subscription
requests through other mechanisms than named pipes.

A feature in the framework that has not been fully specified is the
evaluation of the event trigger type structures defined in section 6.2.2
. As these structures can be used to determine which events are
announced to subscribers, the sample publisher has to be able to
determine when an event’s trigger type structure is matched by a
structure restricting events for a subscriber. On the other hand, to
leave the relevance and interpretation to a particular application, the
meaning and content of these structures has not been specified. To work
around the problem presented by these two conflicting requirements, the
sample publisher class supports a second configurable function used to
compare two event trigger type structures. The first of the two
structures is used to restrict events for this subscriber and the second
one is the trigger structure that has been specified for the event
concerned. When a match is found the configured comparison functions
returns true , otherwise false . The SetEventTriggerTypeCompareFunc
function can be used to set this comparison function.

##### Callback Functions

To allow further customization of the sample publisher class through
derived classes, the class contains four callback functions invoked when
specific events occur. These functions, shown in Fig. 6.21 , are
implemented as empty function bodies and can be overwritten by classes
derived from AliHLTSamplePublisher to adapt or extend the class’s
behaviour.

The first two of these functions, AnnouncedEvent and CanceledEvent , are
called when an event has been announced to all interested subscribers or
when it has been released from the publisher respectively. Parameters
passed to the AnnouncedEvent function include the three parameters used
in the call to the AnnounceEvent function that has been used to announce
a particular event. Additionally, the reference count for the number of
subscribers to which this event has been announced is passed as the
function’s fourth parameter. CanceledEvent is called with two
parameters, the first is the ID of the event that has been released. A
vector of event done data structures is passed to the function in its
second parameter. This list holds all non-trivial event done data
structures that have been received from attached subscribers for that
specific event. When such a structure has been received from a
subscriber, the third callback function, EventDoneDataReceived , is
called. Arguments passed to this function are the name of the subscriber
from which the data has been received, the ID of the respective event,
and a pointer to the event done data structure that has been received.

The final of the four callback functions, GetAnnouncedEventData , is
called by the publisher object when a subscriber specifies that it wants
to receive events that have already been announced to other subscribers
in the StartPublishing call. To avoid the duplicate effort of storing
each event’s sub-event descriptor and event trigger structure in both
the sample publisher object and the calling application code, the
callback is used to obtain these two data structures from a derived
class for each event. The referenced parameters to the two structures
have to be filled with pointers to the event’s actual data inside the
function. If this is not possible, the function has to return false.
Otherwise it has to return true so that the publisher knows that the
data has been filled in and that the event can be announced.

#### 6.5.2 The Detector Publisher Class

The publisher class AliHLTDetectorPublisher is intended for producer
programs that address detector hardware. It is derived from and enhances
AliHLTSamplePublisher to provide a framework for programs that access a
hardware device and insert its data into a processing chain. For this
purpose it implements three of the callback methods introduced in the
sample publisher class and provides six additional abstract callback
methods that have to be provided by actual implementation classes. The
class’s main feature is an event loop that runs in a separate thread and
that calls three of the abstract callbacks at different times. Two
functions, StartEventLoop and EndEventLoop , are called respectively at
the beginning and end of the event loop, while a third WaitForEvent is
called repeatedly to retrieve new events for announcement. Two further
callbacks are the EventFinished functions that differ in the arguments
accepted. They are called when an event is in the process of being
released under different circumstances. One is used when the sub-event
descriptor for the event could be found in a wrapper class that handles
the descriptors, and the other if the descriptor could not be found. The
final callback method QuitEventLoop is called when the event loop has to
be terminated. This call is necessary because the event loop might be
blocked inside the WaitForEvent method and the QuitEventLoop is intended
to make that function return to the calling event loop. A UML diagram of
the callback functions can be seen in Fig. 6.22 .

In addition to the event loop the class provides a number of other
features intended to reduce the amount of implementation work that has
to be done for each new data producer program. It has support for a
shared memory manager class that facilitates dereferencing of shared
memory segments used for data exchange between a producer and its
consumers. Access is provided to a buffer manager class as well as to a
descriptor handler class, as detailed in the preceeding paragraph. The
former of these two allows to use a buffer manager class from inside the
publisher with a minimum of effort while the second basically functions
as a higher level allocation cache for sub-event data descriptor
structures. Producer specific code in the WaitForEvent method can use
this handler object to obtain descriptor structures as needed in an
efficient manner.

To implement a data producer based on this class, one first has to
create a derived class that implements the class’s six abstract callback
methods, and an object of this class has to be created in the producer
program. Properly configured instances of the shared memory and buffer
manager classes as well as the descriptor handler class have to be
specified to the publisher object. Once this is done the event loop and
the publisher’s normal subscription loop have to be started, which will
also cause the StartEvent callback method to be called, followed by
multiple calls to the WaitForEvent method to retrieve events as needed.
The managing and accounting of events and subscribers will be handled by
the sample publisher base class, as described.

#### 6.5.3 The Detector RePublisher Class

The AliHLTDetectorRePublisher class is derived from the
AliHLTDetectorPublisher class and is intended to be used in conjunction
with the AliHLTDetectorSubscriber class presented below in section 6.6.1
. It can be used with that class to republish events that have been
received by a detector subscriber instance for reannouncement to other
subscribers. Examples where this is used are the EventGatherer ,
EventScatterer , and EventMerger components described in section 7.1
below.

To store the sub-event descriptor and event trigger structures of
announced events in the class, it overwrites the sample publisher’s
AnnouncedEvent method so that these structures can be reused when a
subscriber requests already announced events. In addition it also
overwrites the six abstract callbacks defined by the detector publisher
class, although they are just empty implementations, except for the two
EventFinished methods. The EventFinished methods first attempt to
release any buffer blocks and shared memory segments still allocated and
locked for an event. Subsequently the EventDone method of the event’s
originating publisher is called to propagate the event’s release through
its originating producers. This call is made using the aggregated event
done data structures that have been received from the subscribers
attached to the republisher class.

#### 6.5.4 The Processing Component Publisher Class

In analogy to the AliHLTDetectorRePublisher and AliHLTDetectorPublisher
classes the AliHLTProcessingRePublisher class is intended to be used
together with the AliHLTProcessingSubscriber class (section 6.6.2 ). It
overrides three of the callback methods provided by the sample publisher
class, which are CanceledEvent , EventDoneDataReceived , and
GetAnnouncedEventData . They are forwarded to correspondingly named
methods in the subscriber class for actual processing. The two classes
are intended to be used in analysis components, as described in sections
7.2 to 7.4 , that contain a subscriber for receiving data, processing
it, and producing new data. This produced data is then subsequently
announced by another publisher in the process. A sample calling sequence
for an event that has been announced to a program’s publisher proxy
class, reannounced, and released by a processing republisher class is
shown in Fig. 6.23 .

#### 6.5.5 The Publisher Bridge Head Class

Like the two preceeding classes the AliHLTPublisherBridgeHead class is
also designed to be used in cooperation with another class, the
AliHLTSubscriberBridgeHead , introduced in section 6.6.3 and described
in more detail in 7.1.4 . Unlike the two other cases, however, the two
bridge head classes are not situated in the same process, but instead
each is present in its own process. In most cases these two processes
will not even be running on the same node but on two separate nodes, as
they together provide a transparent connection between components on
different nodes. The connection mechanism as well as the publisher and
subscriber bridge head classes are described more detailed in section
7.1.4 .

### 6.6 The Subscriber Implementation Classes

Unlike the publisher classes there is no single basic implementation
class at the root of the subscriber class hierarchy. An
AliHLTSampleSubscriber class also exists but its functions are mainly
just empty bodies. The only function that performs any action is the
class’s Ping method that calls the calling publisher’s PingAck method as
a response. The reason for this lack of a basic subscriber
implementation is that unlike the publisher’s event and subscriber
management and accounting there is little or no general overlap of
functionality between the different subscriber classes. Therefore the
AliHLTSampleSubscriber class is primarily a useful base class for
subscribers that implement only some of the calls defined in the
subscriber interface. Most of the subscriber classes are derived from
AliHLTSubscriberInterface directly, rather than from an intermediate
subscriber implementation class. Fig. 6.24 shows the class hierarchy for
the three classes described in the following sections, including the
subscriber interface and the sample subscriber class.

#### 6.6.1 The Detector Subscriber Class

One of the classes derived from AliHLTSampleSubscriber is the
AliHLTDetectorSubscriber class that was originally intended as the
companion to the AliHLTDetectorPublisher class in 6.5.2 . For the
primary purpose as the subscriber object in data analysis programs of
the type described in section 7.2 , it has been superseded by the
AliHLTProcessingSubscriber class described in the following section
6.6.2 . This class provides a more advanced framework for receiving,
processing, and reannouncing events. It reduces the additional tasks
required for the writing of application specific programs to the
implementation of one function. The detector subscriber class is still
used as the receiving subscriber for a number of data flow components,
like the event gathererer and scatterer programs presented in 7.1 .

In the detector subscriber class three of the subscriber interface
functions provided by the AliHLTSampleSubscriber are overwritten with
additional functionality. The class also starts a thread for the class’s
main processing loop, that uses a signal to wait for incoming events,
prepares them, and calls the processing function for the event. This
processing function is defined as an abstract method that has to be
implemented by derived classes to provide actual processing
functionality. Included in the preparation is a dereferencing step to
convert the shared memory ID/offset combination for each data block into
an actual C pointer passed to the processing function. When events are
cancelled before they reach the processing step they are removed from
the queue where they have been placed to be processed. For events
cancelled while being processed, a flag is set that should be checked
periodically in the processing function to avoid working on data that
has been overwritten. Resources that have been allocated for these
events are released after processing has finished or aborted. Events are
added to the notification queue of the signal used in the main loop by
the implementation of the NewEvent function.

#### 6.6.2 The Processing Component Subscriber Class

The AliHLProcessingSubscriber class is the successor to the
AliHLTDetectorSubscriber class. It is designed to be used as a
subscriber object in either analysis components, with a subscriber and
publisher, or in data sink components with only a subscriber for
receiving data. For this purpose it implements all defined subscriber
interface functions and starts two internal threads as well as a timer
thread. Of these two internal threads one executes the class’s main
processing loop while the other one contains a cleanup loop.

In the class’s NewEvent function the specified sub-event descriptor and
trigger structures are copied. Pointers to these copies are added to the
data queue of a signal object before it is triggered. In the main loop
the processing subscriber waits for this signal to be triggered and as
soon as this happens, it retrieves these two event meta-data pointers
from the signal’s queue and prepares them for processing. As for the
detector subscriber from the previous section this preparation includes
the conversion of the shared memory ID/block offset pairs into pointers
to each data block in the event. Unlike in the detector subscriber, a
memory block for output data is also obtained from attached buffer
manager and shared memory objects. The prepared and dereferenced block
descriptors as well as the output memory block are used in the call to
the event processing function, which again is defined as an abstract
function that has to be overwritten by derived classes. If this function
completes processing successfully and produces new output data in the
output shared memory, and if the object is part of an analysis component
and not a data sink, a sub-event descriptor is built for this data and
announced via an associated AliHLTProcessingRePublisher object (cf.
section 6.5.4 ) to any interested subscriber. For subscribers in data
sink components event done data produced by the processing function is
used to send the event done message to the event’s originating
publisher. In analysis components a flag decides when an event done
message is sent to the originating publisher, either when the event has
been processed and new output data produced, similar to the data sink
case, or when the associated republisher object informs the subscriber
that the produced event data has been released by its subscribers. In
the latter case event done messages will propagate back through a whole
processing chain from the last processing component. Any event done data
produced by the processing function is stored in this case with the
event’s other meta-data and is attached to the event done data that has
been received from the republisher’s attached subscribers. This
assembled event done data is then used in the event done message sent to
the event’s originating subscriber. Fig. 6.25 and Fig. 6.26 show
sequence diagrams of the two cases for sending an event done message
back to the originating publisher. To prevent event losses in the system
the main loop contains error detection logic at each stage of the
preparation, processing, and announcing steps. This is coupled with
retry handling that ensures that an event with an error occuring
anywhere in the stages is processed until it succeeds or until a
permanent unresolvable error occurs.

When an event is ready to be freed, any blocks reserved for its output
data are released and a pointer to its event done data is placed into
the queue of a further signal object. The subsequent triggering of this
signal causes the cleanup loop running in the second thread to be
activated. In this loop, the EventDone call to the event’s publisher is
made using the assembled event done data. In addition, the event’s
meta-data is removed from the internal structures of the object and
further cleanup is performed as needed.

#### 6.6.3 The Subscriber Bridge Head Class

At the sending end of a data bridge to an AliHLTPublisherBridgeHead
object from section 6.5.5 and 7.1.4 is an instance of the
AliHLTSubscriberBridgeHead class implementing the subscriber interface
functions. This class is described in more detail in section 7.1.4
together with the other classes used in the bridging components.

## Chapter 7 The Framework Components

Based upon the publisher-subscriber interface classes described in the
previous chapter, a number of software components and component
templates have been developed as the main part of the framework to allow
the construction of complex data flow chains in PC cluster systems. The
components can be separated by their purpose into several categories
described in the following sections. For the configuration of the data
flow in such a system a set of fully functional components exists, which
are described in section 7.1 . Section 7.2 details a number of template
programs without actual functionality, whose purpose it is to ease the
writing of components for specific tasks. Templates are provided for
data sink, source, and processing components. Several worker components
to create, modify, or otherwise process event data, some based upon
these templates, are described in the following sections 7.3 to 7.4 .
The second of these includes analysis components which have been written
for use in the ALICE HLT or its prototypes. The final section 7.5
contains descriptions of components dedicated to ensuring the fault
tolerance of systems created using this framework. They function in
conjunction with components from 7.1 .

For the program components described below, a number of additional
classes and functions beyond the interface classes described in chapter
6 , have been written that contain some of their key functionality.
These classes are described where appropriate.

### 7.1 Data Flow Components

The components described below are intended to configure the flow of
data in a system constructed using the framework. Amongst others,
components exist to merge parts belonging to the same event, to connect
components on different computers, and to split up and rejoin a stream
of events into multiple smaller event streams. None of these components
modify the data specified by the event descriptors exchanged between the
programs through the publisher-subscriber interface. Some modify the
descriptors while they are forwarded unchanged by others.

#### 7.1.1 Event Merger Component

Since multiple data sources may exist that produce data blocks belonging
to one event, the EventMerger component exists to merge the multiple
event descriptors for these parts into a single descriptor containing
all blocks. For this purpose the program uses multiple subscribers,
derived from the class AliHLTDetectorSubscriber , to receive the event
parts. One output publisher, derived from AliHLTDetectorRePublisher , is
used to announce merged descriptors. The component’s main functionality
is contained in an object of the AliHLTEventMerger class to which the
subscriber objects forward received events. Fully merged events are
passed to the republisher object for announcement to attached consumer
components. Fig. 7.1 and 7.2 show the relation of the classes in the
component and a sample calling sequence of these classes respectively.
The subscriber and publisher classes do not contain any significant
functionality beyond calling the merger class’s corresponding functions.

In addition to the component’s main thread, one thread is started as a
subscription thread for the republisher object. One more thread is
started for the message loop for each configured subscriber in addition
to the two cleanup and processing threads started internally by each
AliHLTDetectorSubscriber instance. In the program’s main thread a loop
is entered that waits for all parts of an event to be completely
received after which the assembled event is announced again. A timeout
is configurable that will cause events to be announced when one or more
parts were not received within a specified amount of time.

##### The Event Merger Class

The two main parts of the AliHLTEventMerger class are its list of
configured input subscribers and the list for partially received and
unannounced events. Of these two the subscriber list is the more simple
one. It just stores pointers to the configured subscriber objects of the
AliHLTEventMergerSubscriber class. Among the four most important
elements stored in the event list structures are the number of
contributing subscribers expected for this event as well as the number
of subscribers from which parts have already been received. In addition,
the event trigger structure from the first received subevent is also
stored for each event. Another possibility might be to concatenate the
event trigger structures from all event parts for the event’s
reannouncement. The fourth important element of these structures is a
list of descriptors for each data block contained in the received
subevents.

When an event part is received by one of the configured subscribers, the
event list is checked whether an entry for that particular event already
exists. If no existing entry can be found, a new one is created with the
event trigger data that has been received for this part, otherwise the
existing entry is used. In both cases the number of subscribers from
which data has already been received is increased, and the block
descriptions contained in the received sub-event descriptor are added to
the event’s data block list. As soon as the number of received
sub-events is equal to the number of configured subscribers, the list
entry for the event concerned is placed into a signal object. The
subsequent triggering of this signal object activates the merger
components’s main thread to retrieve the block list from the event data
structure. A new event descriptor for the aggregated list will be
constructed and announced through the republisher object in the program.
During these steps the event data will not be removed from the event
data list. It is kept in the list until the republisher object declares
that the event has been cancelled through its appropriate callback
function. When the specified timeout expires, the event list is also
searched for the triggered event. If it is found, the event’s data
structure will be signalled to the main thread as well, irrespective of
the number of sub-events that have been received so far.

After the republisher has released an event, it informs the
AliHLTEventMerger object by calling its EventDone method. The merger
object searches for the event in its event list. If it is found it is
removed, and all used resources are freed. Finally, an EventDone message
is sent to all upstream publishers the merger component is subscribed
to, allowing them to release the event as well.

#### 7.1.2 Event Scatterer Component

One CPU executing one analysis component will not always be sufficient
to alone perform a specific processing step of a chain at the required
rate. The processing load of the steps concerned will thus have to be
distributed among a number of CPUs. To provide this functionality in the
framework the EventScatterer component has been created, which splits up
an incoming stream of sub-events into multiple streams consisting of
correspondingly lower rate of sub-events. Splitting of the stream is
executed on an event-by-event basis, distributing whole events, and not
by splitting up data from one event. In a manner analogous to the
EventMerger component, the EventScatterer component uses one input
subscriber and multiple output republishers. The input subscriber is
derived from AliHLTDetectorSubscriber and is used to receive the input
event stream, while the output publishers derived from
AliHLTDetectorRePublisher make the multiple output streams available to
other components. Unlike in the case of the merger component the
scatterer’s main functionality is not contained in one specific class to
allow the possibility of different algorithms for the distribution of
the incoming events. The scatterer base class AliHLTEventScatterer has
been defined to provide parts of the required functionality together
with a number of callback functions that define an interface for
scatterer classes to be used in the scatterer component. Currently only
one derived class, AliHLTRoundRobinEventScatterer , is implemented for
use in this component together with one class for use in the fault
tolerance scatterer described below in section 7.5.6 . It uses a simple
round-robin algorithm for the distribution of the events among the
configured output publishers. Neither the subscriber nor the republisher
classes provide significant additional functionality beyond interfacing
with the central scatterer object.

In addition to the program’s main thread and the two threads started by
each subscriber, one subscription loop thread is started for each
republisher object. In the main thread the subscriber’s message
communication loop for the publisher-subscriber interface is executed.
As soon as this message loops ends the scatterer component will be
terminated as well. Fig. 7.3 and Fig. 7.4 show the relationship of the
different classes in the scatterer and a sample calling sequence
respectively.

##### The Event Scatterer Base Class

Internally the AliHLTEventScatterer class mainly consists of the list of
output publishers which have been configured to be used. The interface
functions it provides and defines are shown in Fig. 7.5 . Among these
are three main functions for use by programs, derived base classes, and
its related subscriber and republisher classes: AddPublisher , NewEvent
, and EventDone . The first of these, AddPublisher , has to be called to
add an output publisher to a scatterer object to make its part of the
received data available. It has to be called during the initialization
of the scatterer component in its main thread and calls the
PublisherAdded callback with the publisher object that has been added.
The next of these functions, NewEvent , is called by the
AliHLTEventScattererSubscriber object when a new event is received.
EventDone on the other hand is called by one of the
AliHLTEventScattererRePublisher objects when an event is released. Both
of these functions call a further function declared or defined by this
class. NewEvent calls the abstract function AnnounceEvent to dispatch an
event to one of the available publishers to be announced, and EventDone
calls the empty ReleasingEvent notification callback. Following this
notification call, EventDone calls the EventDone method of the
AliHLTEventScattererSubscriber object to allow the event to be released
in its originating producer.

Among the six defined abstract methods in the AliHLTEventScatterer class
three are public methods which are called directly by the subscriber
object in the component. Two more public methods are provided for the
case of publisher errors in the component. The final one is called
internally by the class’s NewEvent method. There are two public abstract
methods, EventCanceled and RequestEventRelease , called by the
subscriber object when a particular event has been cancelled or when a
request to release events has been received respectively.
CancelAllSubscriptions , the third of these methods, is called when the
subscriber’s own subscription has been terminated by its publisher. The
two publisher error methods are called PublisherDown and
PublisherSendError . PublisherDown is called from outside the class,
either by a republisher object or by an external supervising instance,
in response to a non-trivial error. Its purpose is to mark that
publisher as unavailable, preventing the scatterer from sending any data
to it. In contrast PublisherSendError is called whenever an error
occured announcing an event for a specific publisher object. This is not
considered a severe error and does not necessitate the removal of the
publisher concerned. The final abstract method AnnounceEvent is the
central method for each scatterer class. It is called by NewEvent
whenever a new event is received to determine to which output publisher
an event is dispatched for publishing. This is handled according to each
scatterer type’s specific algorithm.

##### The Round-Robin Event Scatterer Class

In the basic EventScatterer component the AliHLTEventScatterer interface
implementation is provided by its derived class
AliHLTRoundRobinEventScatterer . It provides implementations of the six
abstract methods defined in the base class. It neither overrides the
default behaviour of other base class methods, nor does it implement any
of the two callback methods provided by the base class.

A simple round-robin algorithm is used by the central AnnounceEvent
method to select an output publisher for each event. To ensure
consistency for multiple parts of an event passing through different
parts of a system, this algorithm is not based on the event sequence
number but uses an event’s ID instead. The same algorithm is also used
by the implementation of the EventCanceled method to determine the
republisher to which the notification about an event’s cancelation has
to be forwarded. RequestEventRelease just forwards the release request
to all publishers and CancelAllSubscriptions cancels all publishers’
subscriptions. Empty implementations without any functionality are
provided for the two publisher error notification functions, effectively
disabling handling of errors occuring in one of the scatterer’s
publishers.

#### 7.1.3 Event Gatherer Component

Most event streams that have been split up with the help of the
EventScatterer component described in the previous section will have to
be united into a single stream again at a later point of a data
processing chain. This task is performed by the EventGatherer component,
which can be seen as the inverse component to the scatterer, with
multiple input subscribers and one output publisher in place of the
scatterer’s multiple output publishers and single input subscriber. As
for the merger component the subscribers’ class is derived from
AliHLTDetectorSubscriber and the publisher’s is derived from
AliHLTDetectorRePublisher . Fig. 7.6 shows the relationship of the
classes used in the EventGatherer component. The merger and the gatherer
components are very similar in their internal architecture. Their main
difference is in the gatherer not having to receive one part of an event
from each of its input subscribers. Instead it just has to forward each
received event to its output publisher unchanged. Fig. 7.7 shows a
sample sequence of events for this component.

As in the EventScatterer component a base class, called
AliHLTBaseEventGatherer , is used to define the central interface for
the main gatherer class in the component with one data structure and
five abstract methods. Actual gathering functionality is contained in a
derived class AliHLTEventGatherer that provides implementations of these
methods. As for the previous merger and scatterer components neither the
subscriber nor the republisher component class contain significant
functionality beyond the forwarding of function calls to the central
gatherer object.

Internally, the gatherer’s primary data structures are its list of
configured subscribers as well as a list of received and forwarded
events which the ouput republisher could not yet release because they
are still in use by at least one of its subscribers. This event list is
necessary in the gatherer as it has to keep track of the event’s
originating publishers, to be able to send EventDone messages for
released events. In this respect it differs from the EventMerger , as
that component receives parts of one event from each of the publishers
it is attached to and thus has to send EventDone messages to each of
them as well. Allocation and work task assignment for threads in the
gatherer component is identical to the merger. One thread is started for
each subscriber as the message loop for the publisher-subscriber
interface communication plus each subscriber’s two internal threads for
processing and cleanup. One further thread is created as a subscription
request thread for the republisher object. In the component’s main
thread a loop is entered that waits for received events to announce them
via the republisher to any further components.

##### The Event Gatherer Base Class

In the central gatherer object base class AliHLTBaseEventGatherer an
interface is defined consisting of one structure data type and five
abstract methods, both shown in Fig. 7.8 . The EventGathererData type is
used to store the data required to associate each event correctly with
its originating subscriber. Primarily, this includes the event’s ID and
the index number of and pointer to the originating subscriber object.
Also available are an event’s trigger data as well as any event done
data structures received for the event. These last two elements,
however, are not used in the standard gatherer component. Finally,
descriptors for the event’s datablocks are stored as well to construct a
new subevent descriptor from them. This descriptor is used for the
event’s announcement by the republisher object. Constructing a new
subevent descriptor is necessary, as announcing runs in a separate
thread from the receiving thread and the original descriptor may already
have been released when the event is announced.

The abstract method WaitForEvent is intended to be called externally to
wait for an event to arrive. In the EventGatherer component this is done
in the program’s main thread. One further function, EventDone , is
called by the component’s output publisher when an event has been
released. Two of the remaining three functions, NewEvent and
SubscriptionCanceled , are called by the subscribers configured for the
component in response to a stimulus from the publisher they are
subscribed to. The stimuli are either the arrival of a new event or
respectively the cancellation of their subscription. The last function,
SubscriberError , is called in response to an error that occurs in one
of the specified subscribers, e.g. when attempting to send an event done
notification back.

##### The Event Gatherer Class

In the class AliHLTEventGatherer , derived from the class
AliHLTBaseEventGatherer , the two central data structures are a list of
configured subscribers and a list of data structures of the base
gatherer’s EventGathererData structures. The second list is used to
store information about each event which has been received by one of the
subscriber objects and announced through the republisher object, but is
not yet released. Events are added to this list in the class’s
implementation of the NewEvent function. In this function a pointer to
the event data structure in this list is added as notification data to a
signal object before it is triggered. A wait for this signal to be
triggered is entered in WaitForEvent . Upon return from the wait the
first available event structure in the notification data is returned to
the function’s caller. In the EventGatherer component this caller is the
function’s main thread, which uses this data to announce the event. When
the provided implementation of the EventDone function is called by the
republisher to signal a released event, the list is searched for the
event concerned. If the event is found, its structure is removed from
the list, and an event done message is sent to the subscriber from which
the event has been received. A subscriber is removed from the object’s
list of subscribers if its subscription is cancelled through the
SubscriptionCanceled function. Each event that has been received through
that subscriber subsequently has to be cancelled in all subscribers
attached to the republisher as well. The final of the five abstract
methods defined in the AliHLTBaseEventGatherer class, SubscriberError ,
is only implemented as an empty function body with no functionality.
Subscriber error handling is not supported by this class and thus
neither by the component.

#### 7.1.4 Bridge Components

All components in the framework rely on the publisher-subscriber
interface for communication between components. Due to the used
mechanisms of named pipes and shared memory any communication in the
framework is restricted to be local on one node. To lift this
restriction and enable inter-node communication and data-exchange of
components a set of two specialized bridging components has been
developed. In the first, the SubscriberBridgeHead , data is accepted
from a producer component and sent via a network to its partner
component, the PublisherBridgeHead . The PublisherBridgeHead places the
received data in a shared memory segment and announces it via its
publisher object to further components subscribed to it. Fig. 7.9 shows
the relation of the different publisher-subscriber and communication
classes in these two components. A sample of the calls that occur
between the classes in the components is displayed in Fig. 7.10 . Using
the standard subscriber and publisher interface objects for receiving
and reannouncing data supports transparent connections of other remote
framework components without special measures required in any of them.

In the SubscriberBridgeHead component the major part of the
functionality is provided by an instance of the
AliHLTSubscriberBridgeHead class together with two instances of classes
derived from the BCLMsgCommunication class and one instance of a
BCLBlobCommunication derived class, all three described in section 5.4.1
. Of these communication classes one message class object is used for
the application level communication between the two bridge components,
and the second message object is used for the required communication
between the blob objects in the two components. Internally the
SubscriberBridgeHead uses two threads in addition to its main thread and
any background threads that may be created internally by the different
communication classes. In the main thread the message loop responsible
for the publisher-subscriber interface communication is run. The first
additional thread is used for the network message loop that accepts and
handles network messages received from the remote PublisherBridgeHead
component. In the third thread the transfer loop for events is run that
receives sub-events from the subscription loop through a signal object,
accesses their data, and sends it over the network to the
PublisherBridgeHead together with the parts of the sub-events’
descriptors necessary to announce the event. The class uses the approach
of reserving the whole receive blob buffer and performing buffer
management on it locally, as described in section 5.2.3 . Local buffer
management in the SubscriberBridgeHead is possible as each
PublisherBridgeHead component receives its data from only one
SubscriberBridgeHead , which thus can use the receive buffer
exclusively. This approach has been chosen to minimize the number of
messages exchanged between the two components and thus reduce the
latency time needed to transfer an event.

On the receiving side the primary constituents of the
PublisherBridgeHead component are an instance of the
AliHLTPublisherBridgeHead class together with the same three
communication class instances as in the SubscriberBridgeHead . The
purposes of these communication objects are identical to the ones in the
SubscriberBridgeHead : application level communication, blob message
communication, and blob data transfer. Also similar to its sending
counterpart, the PublisherBridgeHead uses two additional threads beyond
the main thread and the threads started internally by its communication
objects. One of the two additional threads executes the common loop for
accepting new subscriptions for the publisher object while the other is
the retry thread. This retry thread is responsible for trying to resend
event done messages to the SubscriberBridgeHead where previous sending
attempts for an event have been unsuccessful. In the program’s main
thread the message loop to receive and handle network messages from the
sending component is executed, similar to the message loop in the
subscriber bridge head.

##### The Subscriber Bridge Head Class

The main data element of the AliHLTSubscriberBridgeHead class is a list
of data structures for events that have been received from its publisher
object. Pointers to the corresponding sub-event’s descriptor and trigger
structures are stored in each event’s structure as well as a pointer to
its originating publisher proxy object. Additionally, the number of
retries that have been made to send the event to the publisher bridge
head component are stored together with data about the event’s
destination location in the receive buffer. This last information is
required to release the part of the buffer used by that event, as the
receive buffer’s management is performed in the sender component as
described above. An event structure’s first three elements are the event
descriptor, trigger structure, and publisher interface pointer. They are
set when the event has been received from the publisher in the
subscriber object’s NewEvent method before it is added to a signal
object. This signal object is then triggered subsequently, to inform the
transfer loop described below that a new event is available for sending.
Buffer management data for an event is only set when the event’s block
in the receive buffer has been successfully allocated, which takes place
during the attempt to transfer it. The retry counter is increased every
time a send attempt of the event to the remote partner fails. In
addition to the event list the AliHLTSubscriberBridgeHead class stores
pointers to the three BCL communication objects used for the network
communication with the PublisherBridgeHead component. A pointer to the
buffer manager object used for the receive buffer is also contained in
the class.

Next to the functions implemented for the subscriber interface there are
two functions that perform the major tasks of the subscriber bridge head
class. In the MsgLoop function any messages received from the remote
AliHLTPublisherBridgeHead partner object are handled. These are
primarily connect and disconnect request messages as well as event done
messages. Connection messages contain the addresses of the remote
program message and blob message communication objects. If no connection
is established, these addresses are extracted from the message and are
used to establish a connection to the remote component. When a
connection has been established successfully, the remote blob buffer
size is queried, and the whole buffer is reserved as a transfer buffer
for the events. The buffer size is also used to initialize the buffer
manager object correctly. Events already stored in the object’s event
list are now added again to the transfer loop signal object. After these
additions the signal object is then triggered to activate the transfer
loop. For disconnect requests not much action is required except for
initiating the actual disconnection of the three communication objects.
Received event done messages contain the event’s ID as well as any
non-trivial event done data that has been received from the publisher
bridge head object. This event done data is extracted from the message
and is used to send an event done notification to the publisher that the
component is subscribed to. Further actions in response to a received
event done message include the cleanup of all object internal data
related to that event, especially releasing the block occupied by the
event in the buffer manager object.

The second main function of the AliHLTSubscriberBridgeHead class is the
TransferLoop function, that is responsible for the transfer and
announcement of an event and its data to the remote
AliHLTPublisherBridgeHead object. In this function a wait is entered on
a signal object triggered when new events are available for transfer, as
described above. Available events are extracted from the signal object’s
notification queue for processing. For each event a first check is
performed whether a connection to the remote publisher bridge head is
established, otherwise an attempt is made to establish one. If that
connection attempt fails as well, the event transfer attempt is aborted.
When a transfer is aborted the event concerned is entered into the
object’s event retry list for a later send attempt. As soon as a
connection is available, a block for the event data is allocated in the
buffer manager. The event data is then transferred into this block in
the remote receive buffer by the blob communication object’s multi-block
transfer function described in section 5.4.1 . After the successful
transfer of the data an event descriptor message is constructed from the
event’s original descriptor and the buffer manager data. This message is
then sent to the remote component to announce the event. If the event
has been cancelled by its originating publisher before the send process
is complete, a special abort message is sent as the validity of the
transferred event data cannot be assurred. Otherwise the announce
message is sent normally and the event is kept in the list until the
event done message for it is received from the publisher bridge head.

##### The Publisher Bridge Head Class

In the AliHLTPublisherBridgeHead class the two main data members are the
list of events that have been received over the network from the
subscriber bridge head and a retry list of released events for which the
sending of the event done message to the remote
AliHLTSubscriberBridgeHead object has failed. For each received event
the sub-event data descriptor and the event trigger structure received
from the sender component are stored in the event list. Each event’s
done data obtained from attached subscribers is stored in the retry
list. This data is sent in each attempt to the subscriber bridge head.
In a retry loop failed event done data structures are attempted to be
sent again when a retry timeout has expired. In addition to these two
main data lists each AliHLTPublisherBridgeHead object also stores
pointers to the three communication objects used.

Three of the functions from the callback interface provided for derived
classes by the AliHLTSamplePublisher class are implemented in the
publisher bridge head class: CanceledEvent , AnnouncedEvent , and
GetAnnouncedEventData . Of these three functions AnnouncedEvent has a
notification purpose only without actual functionality.
GetAnnouncedEventData ’s purpose is to obtain an event’s stored data
descriptor and trigger structure for the reannouncement of events.
CanceledEvent initiates the sending of released events’ done data to the
AliHLTSubscriberBridgeHead .

Besides these three callback functions one further function, MsgLoop ,
contains the main functionality of this class. Similar to the subscriber
class from the previous section, this function’s purpose is to receive
network messages from its remote counterpart. The most important
messages handled in this function are connection and disconnection
requests as well as new event announcement messages. Connection request
messages are handled somewhat in the same way as in the subscriber
bridge head class. The address of the remote partner is extracted from
the message, and then a connection to this component is established if
it is not existing already. No send attempt of event done data
accumulated before the connection is made, these attempts are only
triggered by their respective timeouts, unlike for the subscriber bridge
head’s event announcement sends. For disconnect requests the connection
to the partner is simply aborted. NewEvent messages are the most
complicated messages handled in the function. An event’s trigger
structure and descriptor data are extracted from the message. The event
trigger structure is subsequently used unchanged but the event
descriptor is modified to use the correct shared memory segment ID,
since this is not available in the sending component. When the correct
data structures are assembled, they are added to the event data list and
following this the event is announced by the component’s publisher to
its subscribers.

#### 7.1.5 Trigger Filter Component

One further functionality that has to be executed by a component is the
triggered filtering of events. This means for the TriggeredFilter
component that it has to receive events from a publisher and store them
until a trigger decision for each is received. Based upon this trigger
decision it determines which blocks of an event to forward and announces
these blocks via its own publisher object to further subscribers. The
mechanism by which the trigger data is received is the one provided by
the SetEventDoneDataSend and EventDoneData functions, defined in the
AliHLTPublisherInterface and AliHLTSubscriberInterface classes
respectively. Trigger decisions are arrays of structures of the
AliHLTTriggerDecisionBlock type described below.

Components that make the trigger decision for a particular event
encapsulate vectors of these AliHLTTriggerDecisionBlock data structures
into AliHLTEventDoneData structures. These structures are then
transported back along the path that the event has been announced on.
Components like the TriggeredFilter which have requested this will
receive event done data originating from a publisher’s other
subscribers. Each of the blocks in the trigger decision is then compared
to an event’s data descriptor to determine which blocks are to be
forwarded. Based upon this result a new event descriptor is constructed
from the original one, and the event is announced to the filter’s
subscribers. Fig. 7.11 shows a schematic sequence of events in the
trigger filter component, a description of the classes follows below.
For events where no block is selected through the received trigger
decision two kinds of behaviour can be configured via command line
options: Either the event concerned is not announced by the filter
component at all or it can be announced as an empty event without any
data blocks.

Internally the TriggeredFilter component consists of three main objects:
a TriggeredFilterSubscriber object, a TriggeredFilterPublisher object,
and an AliHLTTriggerEventFilter object. Its main logic is contained in
the subscriber object which makes use of the event filter object for
evaluating each event’s trigger data. The publisher object does not
contain much functionality beyond the one provided by its
AliHLTSamplePublisher base class. It starts a thread that contains the
standard subscription loop and implements two of its base class’s
callback functions, CanceledEvent and GetAnnouncedEventData . Calls to
both functions are only forwarded to corresponding functions in the
subscriber object. No threads apart from the mentioned subscription
thread and those started internally by the AliHLTSamplePublisher class
are started in this component. Fig. 7.12 shows the relation of the
different classes in the component.

##### The Trigger Filter Subscriber Class

The TriggeredFilterSubscriber class contains the component’s main
functionality. Its main data parts are two lists. One of them holds
events that have been announced to the component, but for which no
trigger decision has been sent so far and which thus have not been
announced yet. The other contains the descriptors and trigger structures
of events which have already been announced by the component’s own
publisher and which have not been released yet. Both lists contain
structures of the same type, storing an event’s descriptor and trigger
data as well as a pointer to its originating publisher proxy.

The class’s functionality is contained primarily in the implementations
of the NewEvent and EventDoneData subscriber interface functions.
Supplementary functionality is contained in the subscriber interface
function EventCanceled as well as in the CanceledEvent function called
by the component’s publisher object. In the EventCanceled function the
respective cancelled event is searched in the two event lists and is
removed if found. If the event has already been announced through the
component’s own publisher, it is aborted in the component itself, and
the event cancelled message is forwarded to its subscribers as well. In
the CanceledEvent function the event is also searched in the lists. If
it is found, the event done data that has been received by the
TriggeredFilterPublisher object is used in the EventDone call to the
event’s originating publisher.

An event is added to the list of received events in the NewEvent
function by placing copies of its descriptor and trigger data into the
list. No further event processing or announcing is performed in this
function as this only happens upon receipt of event done data from the
event’s publisher in the EventDoneData function. When event done data is
received, the trigger decision for the event concerned is extracted from
it and the event is searched in the list of received events. The trigger
decision data and the event descriptor are then passed to the
AliHLTTriggerEventFilter object to appropriately filter the event’s
descriptor. Depending on the results and the current setting the
resulting descriptor is then used to announce the event through the
component’s publisher to further subscribers. If the event is not
announced any further, an event done message is sent to its originating
publisher to release the event.

##### The Trigger Decision Block

Three data elements are contained in the AliHLTTriggerDecisionBlock
structure that specify which data blocks of an event are to be read out:
the block’s data type, its data origin and its data specification. These
three fields directly correspond to the three fields of the same name
and function in the AliHLTSubEventDataBlockDescriptor described in
section 6.2.2 and are of the same respective type.

##### The Trigger Event Filter Class

In the AliHLTTriggerEventFilter class the main functionality is
contained in the FilterEventDescriptor function. This function accepts
an event’s data descriptor and a list of trigger decision blocks as its
parameters. The trigger decision blocks are used to filter the data
blocks from the event descriptor to be forwarded according to the
trigger decision. Upon return from this function the event descriptor
only contains those blocks that have not been filtered out so that it
can be used directly to announce these events.

Matching of an event’s data blocks with the information in the trigger
decision blocks is performed differently for the data type and origin
and for the data specification field. For the type and origin a match is
made if one of three conditions is met: The corresponding fields in the
data block and the decision block are identical or one of the two fields
contains the wildcard pattern of all 64 bit respectively 32 bit set. For
the event data specification field matching modes are differentiated in
the class by specifying a matching function in the filter object. Two
predefined functions for this purpose are provided in the library. More
matching modes are also possible by specifying user-defined matching
functions instead of these predefined ones. In the first and simpler of
the existing matching modes a match is found when the specification
values from the descriptor and decision blocks are identical. This is
similar to the matching for the data type and origin fields, although
without the possibility for wildcards.

The second data specification matching mode is more complex and specific
to the framework’s use in the ALICE High Level Trigger. It currently
exists only as a first draft version and is still subject to
modification. In this mode the data specification field is used to
indicate an event data’s origin in the detector given in the data origin
field. For data originating from the ALICE TPC the data specification
contains the minimum and maximum numbers of the slice and patch
specifiers as defined in section 2.2.2 . If a data block’s specification
overlaps with a decision block’s in both slice and patch numbers, then
the block is marked for readout. All four fields (mininum and maximum
slice and patch) in a trigger decision block are allowed to take the
value of all 8 bit set, which corresponds to a wildcard for that number.
Data originating from ALICE’s DiMuon arm contains the numbers of the
DDLs used for readout of the data. A trigger decision block contains the
minimum and maximum number of the DDLs to be read out for an event. For
both the minimum and maximum DDL number for readout in the decision
block 8 set bits again corresponds to a wildcard value for the number in
that decision block.

### 7.2 Application Component Templates

To ease the programming of worker components for tasks other than those
currently provided, three templates have been included in the framework.
In general, application components can be divided into three types
according to their position in a chain:

-   Data source components that obtain data from a source outside of the
    chain and make it available via a publisher object to other
    framework components. They are located at the beginning of a chain

-   Data processing components that receive data via a subscriber
    object, process it to produce some new output data, and make the new
    produced data available again from a publisher object. They are
    located in the middle of a chain.

-   Data sink components that receive data using a subscriber object and
    then either process the data and/or forward it to some destination
    outside of a framework chain. They are located at the end of a
    chain.

Fig. 7.13 shows the principle of the three application component types
with their respective position in a chain. For each of these three types
one template is present, written and commented to be adapted easily to a
particular task at the intended position in a data processing chain. The
following descriptions of the templates also contain instructions on how
to proceed in adapting the templates to their intended tasks.

#### 7.2.1 Data Source Template

Of the three component templates the data source component is the most
complex one to implement due to the largest number of requirements and
potential uses. The template is mainly intended for implementations that
access a specialized readout hardware, e.g. in the form of a PCI card.
Its main constituent part is an instance of a class derived from the
AliHLTDetectorPublisher class described in 6.5.2 . All six virtual
functions defined in the detector publisher class are implemented by the
template publisher class, although the functionality provided by the
WaitForEvent method generates random data for publishing only.
Functionality in the class’s other methods can be used as provided for
software-only components that do not have to access hardware. An
exception here are constants, like the block size for an event, which
most likely differ for real tasks.

For data sources that have to access and communicate with special
hardware devices more code will have to be added to the six detector
publisher interface methods. In the WaitForEvent and EventFinished
methods the functionality of the buffer manager object has to be
replaced, if this task is performed already by the hardware. In this
case a block in the output shared memory will not have to be allocated
using the buffer manager in WaitForEvent . Instead the location of the
data will have to be read out from the hardware. Similarly, in
EventFinished the block will not have to be released in the buffer
manager but the hardware has to be informed that it can now reuse the
occupied memory. In StartEventLoop code has to be inserted to initialize
the hardware device, while in EndEventLoop the device has to be
deactivated. Finally, in QuitEventLoop an interface between the hardware
and the component could be required to abort the event loop in
WaitForEvent while it is still waiting for the device to provide
information and/or data for a valid event.

#### 7.2.2 Data Processor Template

In the data processing template two classes are used directly, one
derived from AliHLTProcessingSubscriber described in 6.6.2 and one
derived from the AliHLTProcessingComponent class described in more
detail below. Only two functions have to be implemented in the
AliHLTProcessingSubscriber derived class to be able to use the class in
the template. The first of these functions is the class’s constructor,
which has to supply required parameters to the base class’s constructor.
Additional parameters that have to be passed to the new derived class
can be added to those parameters as well. ProcessEvent is the second
function that has to be implemented, defined as an abstract function in
the AliHLTProcessingSubscriber class. It is called by the parent class
when a new event is available for processing by the object. Input
parameters to this function include structures containing the event’s
data block descriptor with dereferenced pointers, the event’s trigger
data, and a pointer to a preallocated output shared memory block as well
as its size. Two primary output parameters of the function are a list of
created output data blocks as well as a pointer to an event done data
structure used in the EventDone message to the event’s originating
publisher. In the function the input data blocks can be immediately
accessed and processed. Output data can be placed directly into the
provided output shared memory block.

For the class derived from AliHLTProcessingComponent two cases have to
be distinguished, whether or not the processing subscriber class
requires a set of parameters for its constructor different from the one
for AliHLTProcessingSubscriber ’s constructor. If the constructor
parameters are identical for the two classes, a template class derived
from AliHLTProcessingComponent can be used with the subscriber class’s
name as the template parameter. This class contains an implementation of
the abstract subscriber creation method described below, that supplies
the default parameters to the constructor. To supply additional
parameters required by the subscriber constructor a custom class has to
be derived from AliHLTProcessingComponent that implements the abstract
subscriber creation function with the necessary parameters. Both of
these approaches are present in the sample data processor component, a
#define statement selects one of them.

##### The Processing Component Class

AliHLTProcessingComponent is a complex class that encapsulates almost
all functionality needed to set up a processing component. It parses the
program’s command line parameters to extract necessary arguments and
optional specifiers. Based upon these it creates all required objects
and initializes them. Among the objects being created are cache classes
for frequently needed data types, a buffer manager object, a republisher
object, and objects for accessing shared memory. Creation of the
subscriber class required for processing is not directly contained in
the component class. Instead an abstract function, CreateSubscriber , is
defined and called with the purpose of creating and returning a new
subscriber object. This object must be of a class derived from
AliHLTProcessingSubscriber to supply all functionality assumed by the
component class. Also all necessary threads for the operation of a
processing component are started so that amongst others the publisher’s
subscription loop, the subscriber’s message handler loop, and a
processing thread can operate without any further actions.

To make use of the functionalities of this class, a derived class has to
be defined that implements the abstract CreateSubscriber function. For
processing subscriber classes whose constructors do not require any
special arguments the AliHLTDefaultProcessingComponent class can be
used. This template class implements a subscriber creation function
using the template parameter as the type of class to create with the
processing subscriber default parameters. With a suitable derived
processing component class available an object of that class has to be
created with its required arguments in the component’s main function,
and the class’s Run method has to be called to activate the processing
component and start the processing of data.

#### 7.2.3 Data Sink Template

The data sink template is very similar to the data processing component
and uses the same two primary objects of classes derived from
AliHLTProcessingSubscriber and AliHLTProcessingComponent . New events
arriving are also handed to the user code in the ProcessEvent function
that has to be implemented in the subscriber class. The difference
between the two component types is attained by calling the NoRePublisher
function of the AliHLTProcessingComponent derived class. This function
specifies to the component class object that no republisher object is to
be created, inhibiting the publishing of any produced data. Mostly,
however, this component will not produce additional data but only
perform a specific task with its received input data, e.g. writing to a
file.

### 7.3 Generic Worker Components

In the following section a number of worker components are described not
dedicated to a specific task of the framework. Most of them are intended
to be used in debugging new components or chain setups, although they
can also be used in small chains with limited functionality.

#### 7.3.1 Random Trigger Decision Component

To aid in debugging the TriggerFilter component’s functionality,
discussed in section 7.1.5 , the AliRandomTriggerDecisionUnit component
was created. It is a data sink component that does not process in any
way the input data it receives. Instead it generates a random trigger
decision consisting of multiple trigger decision blocks for each event.
The generated trigger decision blocks are used as the event done data
payload when the event is released.

For each of the generated trigger blocks one of the available block
types is chosen at random. Seven trigger block types are available:

-   Empty or untriggered events

-   Completely triggered events

-   A specified TPC slice region, defined by a minimum and maximum slice
    number

-   A specified TPC patch region, defined by a minimum and maximum patch
    number

-   A specific type of data

-   A specific type of data in a certain TPC slice region

-   A specific type of data in a certain TPC patch region

If a block with one of the first trigger types is selected, no other
decision block is allowed for the event concerned. The available
datatypes as well as the valid slice and patch numbers are specified to
the component via command line parameters. These parameters also allow
to specify the trigger types to be used as well as a statistical weight
for each of them.

#### 7.3.2 Block Comparer Component

Testing the functionality of different paths in an event chain is the
purpose of the BlockComparer component. This component will compare the
data of all blocks in an event it receives and will provide a detailed
report of the differences found. Its most simple and also most important
application is to attach it to an event merger component with one input
subscriber attached directly to an event’s originating publisher and the
other to a publisher that publishes the same data after it has passed
through a more complex chain setup of multiple components. If the data
has been incorrectly transferred at one point of this chain, then the
block comparer component will detect and report this error. Fig. 7.14
shows a sample setup of the principle. A publisher component announces
data to a merger and a subscriber bridge head. From the subscriber
bridge head the event data is sent via two publisher bridge heads and
one subscriber bridge head to the second input subscriber of the merger.
The merger announces the received events to the block comparer that
compares their two blocks and thus can detect errors that have occured
during the data’s transmission.

#### 7.3.3 Event Dropper Component

By using the EventDropper component it is possible to test the behaviour
of components and complete chain setups when events are lost in the
system due to an error and are thus not released. This component is a
very simple program with a subscriber object that sends EventDone
messages back for most of its received events. Using a configurable
rate, e.g. every hundredth event, the EventDone message is not sent at
all and the event is just dropped. For the producing component to which
the EventDropper is attached this means that the event will never be
released by one of its subscribers and can only be removed when timeouts
expire to force its release.

#### 7.3.4 Event Keeper Component

In a manner similar to the event dropper, the EventKeeper component is
designed to simulate the behaviour of components that require a specific
amount of time to process events. It contains a subscriber object that
starts a timer for each received event. When the timer expires an event
done message is sent back to the event’s publisher. A constant amount of
time to wait between the receiving of the event and its EventDone
message can be specified via a command line parameter.

#### 7.3.5 Event Supervisor Component

During the testing of components, especially in complex setups, events
can become lost due to errors. Detection of such lost events is the task
of the EventSupervisor component. It keeps track of every event
received, and when a certain number of events has been received after a
missing event a warning is issued. The number of events before the
warning is configurable through command line parameters. If for instance
the alarm intervall has been set to 50 and if event 100 is not received,
an error will be reported when event 150 is received. Due to the fact
that the component requires event IDs to be consecutive numbers, it
cannot be used in configurations that encode other information in the
ID.

#### 7.3.6 File Publisher Component

In order to be able to simulate chains without having any special
readout hardware available, a functionality was created to publish data
contained in normal system files into a chain. The MultFilePublisher
component reads data from multiple files and publishes events using that
data. Each event contains data from one file. Files are alternated in a
round-robin fashion. IDs of the events are numbered consecutively,
starting with a configurable offset. The number of events to publish as
well as the time interval to wait between the publishing of events can
also be specified on the command line as well as the three data
characteristics: type, origin, and specification.

In order to make the component more efficient, the data is read from the
files directly into shared memory from which the events are published.
This avoids file I/O and/or copy steps into shared memory for each
event, reducing the CPU load. The component is mostly used in the
simulation and testing of chain configurations without special readout
hardware available.

#### 7.3.7 Dummy Load Component

Simulation of chain setups without actual processing components is the
purpose of the DummyLoad component based on the data processing template
with AliHLTProcessingComponent and AliHLTProcessingSubscriber based
classes. It simulates a processing component that receives input data
and publishes new output data. To simulate different analysis
components, a number of parameters in the DummyLoad can be configured
via command line arguments. The most important of these parameters are
the size of the output data and the simulated processing time for the
data. Specification of the output data’s size is made as the percentage
of the input data’s size, the value of this can be greater than 100 %,
inflating the original data. Processing time can be specified in two
ways, either as a constant value or proportional to the size of the
input data. Similar to the file publisher component, it is also possible
to specify the output data’s three characteristics type, origin, and
specification.

Main parts of the component are two classes derived from
AliHLTProcessingComponent and AliHLTProcessingSubscriber . The
processing subscriber class implements the ProcessEvent method to copy
the necessary amount of input data into the output shared memory and
simulate processing for the specified amount of time. In the processing
component class the main task is the evaluation of the additional
command line arguments to extract the parameters that specify the
simulation parameters.

#### 7.3.8 Data Writer Component

Data that has been produced by components in a chain may be required to
be stored permanently for later access. A very simple method for this is
provided by the DataWriter component that creates a file using a
configurable name prefix for each block in each event. Files are
enumerated by the event’s ID and the block number in the event. For a
large number of events this results in a correspondingly large number of
files so that a periodic means of reducing the amount of files, e.g. by
creating archives of events, becomes necessary. But for short and/or
slow running setups this approach is sufficient.

#### 7.3.9 Event Rate Component

The final generic worker component, the EventRateSubscriber , is a very
simple data sink component. It receives events and immediately sends
event done messages back to its publisher. After a configurable amount
of events has been received, the component calculates the rate of events
averaged over this number as well as the global average rate over all
received events and prints these results to the logging system. This
component is intended as a simple way to monitor the performance of a
system in the absence of a more complete control and supervision system.

### 7.4 TPC Analysis Components for the ALICE High Level Trigger

There exist a number of analysis components ready to be used for the
application of the framework in the ALICE High Level Trigger. Together
these components allow to process run-length encoded ADC values, as read
out from ALICE’s TPC, via space-points and tracklets to complete tracks
of the whole TPC. After running a properly defined chain with these
components the result is a completely reconstructed event of the whole
TPC with all available particle tracks. The processing components in
appropriate order are the ADC Unpacker, the ClusterFinder, the
VertexFinder, the Tracker, optionally the patch internal track merger,
the patch track merger, and the slice patch merger, all of which are
described below. The analysis parts of these components have been
written by collaborating partners from the University of Bergen, Norway
[ 129 ] , [ 130 ] , [ 131 ] , and have been integrated into the
framework in Heidelberg.

#### 7.4.1 The ADC Unpacker

The initial component in a processing chain for the ALICE TPC data is
the ADCUnpacker component. It accepts input data in the form of
zero-suppressed and run-length encoded ADC values as they are read out
from one TPC patch. This data is uncompressed by filling in the
suppressed zero values to create the component’s output data. During the
uncompression process the data is inflated to about 2 to 3 times its
previous size. Due to the fact that the data origin and specification
fields in the event data block descriptor structures were not present at
the creation of this component the slice and patch number that can be
placed there for TPC data are not yet evaluated. Instead it is necessary
to specify them using command line parameters. The slice and patch
specifiers are placed at the beginning of the output data block together
with the values for the minimum and maximum ADC padrows contained in the
data so that the next components also have access to these numbers.

#### 7.4.2 The Cluster Finder

Following the ADCUnpacker component and processing its output data is
the ClusterFinder component. Using the unpacked ADC values, it
calculates three-dimensional space coordinates of charge distributions,
called clusters, produced in the TPC by the passage of charged
particles. For each space point the produced output data contains the
three cartesian coordinate values of the distribution’s
center-of-gravity, the cluster’s width, and the amount of charge
contained in it. The array of space points in the output data is
preceeded by the originating data’s patch, slice numbers, as well as
minimum and maximum numbers of the padrows read out. Additionally, the
number of clusters found in the ADC values is also contained in this
preceeding data block.

#### 7.4.3 The Vertex Finder

One of two components that accept cluster data as its input is the
VertexFinder component that uses the clusters from a slice’s outermost
patch to provide a first calculation of an event’s reaction vertex
position along the beampipe. It produces the cartesian coordinates of
the determined vertex together with calculation error information for
each coordinate. This is preceeded again by the first four numbers,
patch, slice, minimum and maximum padrow number, extracted from the
cluster data’s information block.

#### 7.4.4 The Tracker

The Tracker component requires one or two input data blocks, cluster
data from one patch, optionally together with the vertex data that has
been calculated for the patch’s slice by the VertexFinder . When the
vertex data is omitted, a central vertex position in the middle of the
detector is assumed corresponding to the coordinates @xmath . Tracking
is possible without vertex data although the result is more exact when
it is available. The Tracker uses its input data to calculate segments
of tracks, called tracklets, corresponding to paths of particles
throught the TPC detector. Each tracklet is determined from the model of
a helix, the path that charged particles follow in the TPC due to the
magnetic field inside. The relevant parameters for this track model are
stored for each found track. Among them are the center coordinates and
radius of the helix when it is projected as a circle, the initial
transverse momentum of the particle, as well as the start and end-point
coordinates of the tracklet. Output track data is again preceeded by the
patch, slice, and both padrow numbers as well as the number of tracks
that have been found.

#### 7.4.5 The Patch Internal Track Merger

The next step after tracking one patch’s data, the IntTrackMerger , is
an optional component working on tracklet data from the Tracker .
Multiple tracklets are merged into one tracklet, if their parameters are
corresponding so that they belong to the same track. The output data
format is identical to its input format as tracklets are read and
produced. Due to this it is possible to insert this component
transparently after a tracking component, although it is not mandatory.
The following components cannot distinguish whether they work on data
that was produced directly by the Tracker or by the IntTrackMerger . For
high track densities the reduction in the number of tracks from one
patch can speed up the following merging steps of multiple patches and
slices.

#### 7.4.6 The Patch Track Merger

Merging of the tracks of the six patches belonging to the same slice is
the task of the PatchMerger component. It requires six blocks of
tracklet data as its input, each block belonging to one patch. Using the
tracklets from these patches the patch merger attempts to merge them
across patch boundaries if they belong to a track with the same
parameters. As its output the merger also produces tracklet data using
the same track data structures as they are used for the output data of
the tracker and patch internal track merger components. Unlike the
previous components the patch merger’s output data is preceeded by only
one of the four numbers, the slice number. As the ouput data does not
belong to a patch subset but a whole slice, the other three numbers are
not needed anymore. In addition to this location specifier the data is
also preceeded by the number of tracklets contained in the following
data section of the data block. Optionally, data from less than the
slice’s full six patches can be merged. The number of patches on which
to operate has to be specified as a command line argument. Missing
patches in this case are assumed to contain no tracklets.

#### 7.4.7 The Slice Track Merger

The last step in the TPC processing chain is the SliceMerger component
that performs track merging using the tracklet data of multiple slices
up to the TPC’s full number of 36. Tracklets are merged across the
boundaries between adjacent slices to finally form full tracks passing
through the whole detector. The format of the output data is still the
same tracklet structure which contains all parameters to describe a full
track as well. Preceeding the track array is just the number of tracks
contained in the output data.

#### 7.4.8 Future Steps

Following the final SliceMerger processing component the next required
component is a trigger decision component. This component needs to
analyse an event’s data to make a trigger decision to be passed back
along the analysis chain through its event done data structure. The
approach is similar to the AliRandomTriggerDecisionUnit component,
although of course with a real analysis part to generate the trigger
decision.

#### 7.4.9 The Whole Chain

Fig. 7.15 shows a sketch of the sequence of data through the analysis
components described in the preceeding sections. The vertex finder
component only runs on the data of the outermost patch 5, although its
output data, the vertex location, is used by all six trackers for the
patches of the same slice.

### 7.5 Fault Tolerance Components

#### 7.5.1 Framework Fault Tolerance Concept Overview

As has been pointed out in the introductory sections, a major challenge
of the framework is the behaviour in case of errors, especially with
respect to the intended operation of large clusters like the ALICE HLT.
Errors can be single component failures on nodes or in extreme cases
even failures of complete nodes, and both can be software or hardware
related. Although the fault tolerance (FT) part of the EU DataGrid
fabric management software [ 132 ] is intended to be responsible for the
handling of errors concerning the system software and a node’s hardware,
the HLT system still has to be able to react to node failures. This
reaction as well as its triggering should work closely together with the
GRID software framework.

In this section a set of components is presented that allows for such a
reaction to the failure of any component in the system, which are
handled on the granularity of complete nodes. On a failure the complete
data stream to the node concerned is rerouted to other nodes and if
possible a spare node is activated. The model is only applicable for
data distributed by a scatterer to multiple nodes, one or more of which
may fail, and is then collected again by a gatherer. In the current
proof-of-concept implementation seven components are required: four data
flow components, basically extensions of components described in section
7.1 , one fault detection component, and two components that supervise
and orchestrate the system’s reaction to the fault condition.

The four components extended with fault tolerance functionality are the
Publisher - and SubscriberBridgeHead and the EventScatterer - and -
Gatherer components, to form the TolerantPublisherBridgeHead ,
TolerantSubscriberBridgeHead , TolerantEventScatterer , and
TolerantEventGatherer respectively. For the two bridge head components
the added functionality is primarily the capability to perform remotely
triggered connect and disconnect operations from their respective remote
partners. The scatterer’s fault tolerant capability is to activate and
deactivate output publisher paths, also remotely triggered. Similarly
the gatherer is able to activate and deactivate its input subscribers
for event done messages and to handle the case of multiple subscribers
receiving the same event, which can happen if events are redistributed
by a scatterer. In the following discussion of the components’
principles, a worker or spare worker node can also be a group of nodes
connected together. One node in this group receives the data from the
scatterer and passes it to the next one for processing, which continues
until the last one sends its data to the gatherer. The two nodes
connected to the scatterer and gatherer act as endpoints to the FT
components.

ToleranceDetectionSubscriber , the fault detection component, basically
consists of a simple subscriber object that receives events and
immediately releases them again. For every event a retriggerable timer
is started. The timeout used is configured by the command line. When the
timer expires, indicating that no event has been received in that time,
the detection component sets its own status accordingly and informs the
first of the two supervising components of the status change.

In this supervision component, the ToleranceSupervisor , the status data
of multiple fault detection subscriber components is checked regularly.
When a change in the status of one of the subscribers is detected, the
supervisor sends commands to the scatterer and gatherer components to
deactivate the publishers and subscribers concerned. When an error is
removed the publishers or subscribers can also be activated instead.
After this a command is sent to the second supervision component, the
BridgeToleranceManager .

In response to this message the bridge tolerance manager searches
through its list of active and spare nodes and tries to activate a spare
node if one is available. This activation is done by sending disconnect
messages to the two bridge head components in contact with partners on
the failed node. Once the disconnect is complete, another command is
sent to the two bridge heads to reset their internal state by removing
all event data left over from the severed connection. The reset step is
necessary as the event data has already been resent to other nodes by
the scatterer. Finally, a third command is sent to initiate a new
connection to their new bridge head partners on the spare node. As soon
as it detects this connection as established in the participating bridge
heads, the bridge tolerance manager sends its final commands to the
scatterer and gatherer components to reactivate their output publisher
and input subscriber objects for the failed data path.

In a summary, the sequence of events is as follows:

1.  A node fails.

2.  The ToleranceDetectionSubscriber on a receiving node detects that no
    data arrives from the publisher bridge head connected to this node
    and sends a message to the ToleranceSupervisor .

3.  The ToleranceSupervisor checks the status of all configured
    ToleranceDetectionSubscriber s and detects that the path between
    scatterer and gatherer containing the faulty node is broken.

4.  The ToleranceSupervisor sends messages to the TolerantEventScatterer
    and - Gatherer components on the sending and receiving nodes to
    disable the path concerned. A message is also sent to the
    BridgeToleranceManager to inform it of the failure.

5.  The scatterer and gatherer disable the path concerned. The Scatterer
    distributes all events that have been sent to that path and not
    received back among the remaining nodes. Incoming events for the
    failed path are also distributed among the remaining paths.

6.  The BridgeToleranceManager sends messages to the subscriber and
    publisher bridge heads on the sending and receiving nodes that
    communicate with the failed node, instructing them to disconnect
    from that node and to reset their internal state.

7.  Once the bridge heads are disconnected and reset the bridge
    tolerance manager determines an available spare node and sends
    commands to the bridge heads to connect to that node. (In a more
    complex real system it would also have to be ensured that the
    requires processes are available on the spare node. In this setup
    the worker and spare nodes are configured identically. )

8.  When this new connection is established on both sides the manager
    sends commands to the scatterer and gatherer components to
    reactivate the broken path.

9.  The status change of the path is detected by the path’s tolerance
    detector and the tolerance supervisor.

10. The system functions normally as before, although with the number of
    available spare nodes reduced by one.

Fig. 7.16 shows the components in a sample setup using one data source
and sink each, three processing or worker nodes, and one spare worker
node. The ToleranceSupervisor and BridgeToleranceManager components can
run on a separate node or on either the sink or source node.

For implementations that exceed this prototype a number of extensions to
the above concept will be desirable or even necessary, mainly on the
supervisor level of the concept. At least the two existing supervisor
components, ToleranceSupervisor and BridgeToleranceManager , should be
merged into one component. To avoid single points of failure in the
system this supervisor component should exist in multiple instances in a
system setup, with these instances ideally monitoring each other for
failure. In addition the granularity of the system should be made finer,
so that not only whole nodes can be replaced but also faults in single
components can be recovered, e.g. by terminating and restarting the
component and reattaching it to its communication partners.

#### 7.5.2 Control and Monitoring Communication Classes

A central role in the fault tolerance functionality is taken by the
classes that enable communication between a supervising and a supervised
component. This is provided by two primary and a number of auxiliary
classes, that together allow to send commands to supervised components
and to query their status. An important characteristic supported by the
classes is that supervised components are not purely passive but can
also send interrupts, called LookAtMe or LAM, to supervising components
to indicate a special condition. Of the primary classes
AliHLTSCController is used in the supervisor and AliHLTSCInterface in
the controlled component. Fig. 7.17 shows the six most important
classes. The underlying mechanism used for communication between
components by these classes are the BCL message communication classes.
Communication is performed primarily without explicit connects, although
supervisor initiated connections between components are possible as
well.

##### The Status Structures

Two datatypes exist that help to define structures used to hold status
data for components, AliHLTSCStatusHeader and AliHLTSCStatusBase . The
first of these structures basically is a container for structures
derived from the second. As both are expected to be communicated over
the network they make use of the data format translation mechanism
defined in section 5.3.1 . They are consequently derived from the
BCLNetworkDataStruct type.

AliHLTSCStatusHeader contains three fields, a 64 bit long ID, the number
of actual status structures it contains, and the offset in bytes of the
first status structure, counted from the beginning of the status header.
The ID field holds an identifier specific to the combination of status
structures contained in the header structure. AliHLTSCStatusBase itself
defines only two fields, another 64 bit long type ID and the offset of
the following status structure in bytes, also counted from the beginning
of the status header structure.

Actual structures containing status data are derived as vstdl types from
AliHLTSCStatusBase and contain the status information as their fields.
One example is the AliHLTSCProcessStatus structure which defines common
status data for all components, like the current state and logging level
of a process as well as the last update time of the status structure.
This type is used as the first status structure contained in an
AliHLTSCStatusHeader container by most components.

##### The Controlled Component Interface Class

The primary class used in supervised and controlled components is the
AliHLTSCInterface class. It contains functions to provide a status
header structure for readout by supervising components, to attach
command processor objects handling received commands, and to send LAM
messages to controllers. A number of commands are defined to be
processed by the interface class itself, other commands are forwarded to
the registered command processor classes. Internally the class uses two
threads, one as the communication listening thread and the other for
command processing. Its main data structures are pointers to the
communication object with its local address, a list of addresses of
connected supervisors, a pointer to the registered status header, and
the list of registered command handler objects.

To use the class in the monitored component it first has to be bound to
a listening address, to enable communication, and the two background
threads have to be started using the Start function. Binding is done by
calling the class’s Bind function, which accepts the local address and
an error callback object as its parameters. The object’s local address
is passed in the form of a string holding an address URL as defined in
section 5.3.5 . To release the bound address and stop the two threads
the Unbind and Stop functions are used. Status data for a component is
defined by specifying the address of the status header structure holding
the component’s data to the class’s SetStatusData function. Status data
can only be unset by passing a NULL pointer to the function.

For the handling of command processor objects two functions exist in the
AliHLTSCInterface class. AddCommandProcessor adds an object to the list
and DelCommandProcessor removes it. Both functions accept the pointer to
the handler object to be added or removed as the only parameter. Command
handler objects are described in more detail in the following section
about the AliHLTSCCommandProcessor class. For LookAtMe notifications to
supervising objects two functions exist as well, differing in their
parameters and the intended receivers of the notifications. The first
LookAtMe function sends the interrupt message to all supervisors that
have established connections to this interface object, while the second
version sends it to that supervisor object only whose address is
specified as the function’s parameter.

##### The Command Processor Classes

Command processor objects are instances of classes derived from
AliHLTSCCommandProcessor . This class just defines one abstract
function, ProcessCmd . This function is called for registered handlers
by their interface class instance when commands are received. The
function receives as its only parameter the command structure that was
sent by the controlling instance. Similar to the status structures the
command structure AliHLTSCCommandStruct also makes use of the data
translation mechanism and is thus derived from BCLNetworkDataStruct . It
contains four fields: three 32 bit numbers holding the command itself as
well as two parameters and a variable length array of 32 bit items. This
array is available for holding additional required data which does not
fit into the two numerical parameters.

##### The LookAtMe Handler Class

Similar to the command processor class, the AliHLTSCLAMHandler class is
also an abstract class defining only one abstract function, LookAtMe .
This function is called by the controller class for registered LAM
handlers when a LAM request is received from a monitored component. The
only parameter passed to the LookAtMe function is the address of the
LAM’s originating component in the form of a BCL address structure

##### The Controller Class

The AliHLTSCController class is the main class to be used in supervising
components, providing functions to register LAM handlers, establish
connections to controlled components, and interact with supervised
components. This interaction includes sending commands to, querying the
status of, and setting the logging verbosity of components. Like the
interface class described above the controller class also uses a BCL
message communication object for communication with controlled
components. One thread is used to receive and handle messages in this
communication object. The class’s main data structures are a list of
received messages that have to be handled, primarily replies from
supervised components, the list of registered LAM handler objects,
pointers to the communication object with its local address, and the
address of a controlled object to which a connection has been
established.

LAM handlers can be added or removed from a controller object using the
two AddLAMHandler and DelLAMHandler functions respectively. Both
functions require the pointer to the handler object to be added as their
single parameter. As for the interface class, to be able to use an
instance of this class its communication object first has to be bound to
a valid address, and the background listening thread has to be started
by calling the class’s Start function. Binding is performed analogous to
the interface class by calling the Bind function with a string URL
specifying the address and an optional error callback object as
parameters. To release the bound address and stop the thread, the
functions Unbind and Stop are available.

Once the controller object is ready, a connection can be established to
one supervised component by calling the Connect function with the
component’s address. Termination of a connection is achieved with the
Disconnect function, also requiring the remote address as its parameter.
All functions in the class that interact with a remote controlled
component exist in two versions, one which requires the remote address
of the component and one without an address. The second versions perform
the corresponding task with the component to which the connection is
established. If no connection is established they fail.

Three function pairs are available that operate on the remote controlled
components. The first of these are the SetVerbosity functions that allow
to set the logging verbosity as described in 4.2.1 . As their only
parameter, besides the remote address in one of the versions, they use
the 32 bit large value for the verbosity, corresponding to the list of
set flags for each verbosity level. This flag value is directly assigned
to the global verbosity specifier in the remote component and takes
effect immediately after it is received. Sending commands to remote
components is the purpose of the second set of functions, called
SendCommand , with the command to be sent as the only (additional)
parameter. It is specified in the form of a pointer to the network
transparent AliHLTSCCommandStruct structure passed to the command
handler objects in the receiving components.

The final interaction function set consists of the two GetStatus
functions for querying a remote component’s status data. They return a
pointer reference to a status header structure containing the data that
has been read out from the monitored component. Memory for the structure
is allocated with the required size in the function when the reply
message containing the status data is received. To release the allocated
memory the FreeStatus function in the class has to be called.

#### 7.5.3 Fault Tolerance Detection Subscriber

In the described situation a fault will be noticed first by the
ToleranceDetectionSubscriber . This is a simple data sink component that
notices when no events are received for a specified amount of time and
signals an error condition to supervising components. Its internal main
parts are a subscriber object of the ToleranceDetectionSubscriber class,
a status data structure, and an instance of the AliHLTSCInterface class
for communication with supervisor components. No threads are started
explicitly by the component besides those started by its constituent
objects.

In addition to its header two structure members are present in the
component’s status data. The first of these is the common component
status data, with the component type and status update time of main
importance for the detection subscriber. Following this is an
AliHLTSCToleranceDetectorStatus structure containing three fields
specific to this component: the index of the path between scatterer and
gatherer to which the subscriber is attached, the current state of the
subscriber, and the ID of the last event that was received. A 32 bit
unsigned integer is used as the state specifier field, holding either a
0 or a 1 for a faulty or functioning path respectively. The value
contained in the path index field has to be specified to the component
on its command line.

##### The Tolerance Detection Subscriber Class

As the primary class of the fault detection subscriber component the
ToleranceDetectionSubscriber class is used. It is derived from the
subscriber interface class and implements all its functions. Except for
the NewEvent and Ping method all functions are implemented as empty
function bodies only, without any functionality. In the Ping method the
calling publisher’s PingAck function is called to acknowledge the
received ping. The most important data structures in the class are the
pointer to the component status structure and a list of supervisor
addresses to which LookAtME messages are sent when an error is detected.

In the class’s NewEvent function the component’s status information is
updated with the received information, including the timestamp, the
event’s ID, and the state of the event path. If the component has not
been paused as described below, a timer is set with a timeout value
specified on the component’s command line. As the last action of the
NewEvent function an event done message is sent back to the event’s
originating publisher. When the timer started in the NewEvent function
expires, then the class’s TimerExpired function is called. In this
function the status data is updated by setting the last update time to
the current time and the path’s state to faulty. Following this, a
LookAtMe message is sent to each configured supervisor component
address. Any error occuring during the send is ignored.

The last function of the class containing important functionality is the
ProcessCmd function called when a command message is received for the
component. Using these commands it is possible to initiate a paused mode
for the component when no events are expected to arrive, to suppress
raising of alarms. This pause state is necessary if the chain is still
functioning, but the component delivering events to the fault detection
subscriber cannot send events. Reasons for this might be errors in some
readout hardware that have to be handled in a different manner or
configuration changes in parts of the chain before this component.

#### 7.5.4 Fault Tolerance Supervisor

As described above, the ToleranceSupervisor component is used as the
location of the central supervising and decision making for the dataflow
in a chain setup. When an interrupt is received from a fault tolerance
detection component this component checks the state of all attached
detection components to determine the status of the different data
paths. A discovered faulty data path is removed from the active dataflow
by sending the appropriate commands to the components responsible for
routing the data. Similarly it is possible to reactivate a path once it
has recovered from a fault.

Two primary classes are used in this component, ToleranceSupervisor and
AliHLTSCController . Apart from any background threads started by the
controller class and its internal communication classes, no further
threads are started by the component. The controller object is used in
the supervisor object to monitor and control the external detection and
dataflow components.

##### The Tolerance Supervisor Class

Inside the ToleranceSupervisor class the primary data structures are
lists for the detection and dataflow component addresses, the current
and previous states and the last received event IDs of each detection
component. Of these, the lists for the last event IDs, previous and
current states have to be of the same size.

The main loop of the supervisor program is the class’s Supervise
function that runs in a loop until a signal is caught to terminate the
program. At the beginning of each loop iteration a wait is entered for
the triggering of a signal object with a timeout of the interval between
checks of the supervised detection components. A timeout is used in
waiting so that asynchronous loop iterations outside of the fixed
intervals are possible by triggering the signal object. Such a signal
trigger is executed by the LAM handler function when an interrupt is
received from a supervised component. When the signal’s wait function
returns, the Check function is called to determine the channel state
data from the configured detection and dataflow components. If a state
change in one of the components is detected, pause commands are sent to
all detection components, and the path concerned by the change is set to
enabled or disabled accordingly by calling the class’s Set function.
After these steps the next loop iteration is started.

Inside the Check function the status of each supervised component is
read out using the supervisor’s controller class method GetStatus .
Depending on whether or not a supervised component is a dataflow
component or a tolerance detection subscriber, the channel state data in
the status read is evaluated differently. For detection subscribers the
read channel state is accepted to be the current state, while for
dataflow components a channel state is only updated when the read state
is faulty. This is necessary as the dataflow components have very little
ability to determine a faulty channel state, particularly when the fault
occurs on other nodes.

The first task in the class’s Set function is to send commands to the
dataflow components, informing them that a specific monitored channel,
or path, has been reported as faulty and should not be used anymore.
After building the appropriate command structure, it is sent to all
configured dataflow components. In addition the function sends another
command to the configured bridge tolerance manager component. This
component also has to be informed of the path’s failure, to terminate
bridge connections to any nodes concerned and if possible activate a
spare node replacing the failed one.

#### 7.5.5 Bridge Fault Tolerance Manager

Complementing the fault tolerance supervisor component from the previous
section is the BridgeToleranceManager component, that controls the
bridge connections for the specific paths. To do so it maintains a list
of required connections between data source, sink, and worker nodes for
each of the paths. Additionally, it maintains a list of spare nodes in
the form of available connection endpoints as well as lists of the
supervised dataflow and detection subscriber components. The component
contains two primary classes: BridgeConfig , responsible for reading and
storing the configuration and providing access to its parameters, and
CommandHandler , mainly responsible for communication with outside
components. Outside components include the supervisor as well as the
bridge, dataflow, and detection subscriber components. No threads are
created by the component apart from those created implicitly by its
objects, such as the controller and interface classes described in
section 7.5.2 .

In the component’s main function the command line options are evaluated
first and the necessary objects are created, configured, and activated
as needed. After this a loop is entered in which the component remains
until it receives a signal to terminate. During each loop iteration two
different types of status events with respect to the bridge components
are checked. If a path has been deactivated and the bridge connections
to its nodes have been terminated, new connections have to be
established to a spare node, if there is one available. When these
conditions are met, the bridge head components on the data source and
sink node are checked whether the connections to the broken path have
already been interrupted completely. This is necessary to ensure that
the bridge heads have been able to reset their internal state and remove
any old events from their internal lists. Once the connection
termination has completed successfully, commands are sent to the sink
and source bridge head components, containing the commands to connect to
the corresponding partner components on the activated spare node(s).

The second check performed by the main loop is executed prior to the
first check described above. When a reconnection attempt has been
started it is necessary to periodically check the bridge components on
the source and sink nodes whether the connection has been established
successfully. If this the case, then commands are sent to the dataflow
components responsible for routing the data to reactivate the path
concerned. Once this is done, the broken path has been handled, and the
system functions as before.

##### The Bridge Connection Configuration

A configuration for the bridge fault tolerance manager is described in a
file using six different types of entries, with each entry consisting of
one line:

1.  Data source entries describe connection parameters for a connection
    from the data source node to a worker node. Each of these
    connections is identified by a unique number corresponding to its
    path. To account for the fact that multiple data sources may be
    present, each entry also contains a subnumber. Entries with
    identical major numbers must have different subnumbers. They belong
    to one path which has multiple data sources that have to be merged
    in the path. The number of data source entries with the same major
    identifier and different subnumbers must be identical for each of
    the different source numbers.

    Four parameters are specified for each data source entry: the
    control address URLs of the subscriber and publisher bridge head
    components as well as the message and blob-message address URLs of
    the publisher bridge head component. The subscriber bridge head
    component runs on the data source node, while the publisher bridge
    head runs on a worker node. It is not necessary here to specify the
    message and blob-message address URLs of the subscriber bridge head
    as they do not change, contrary to the worker node address which can
    change due to a node’s failure and replacement by a spare node.

    Entry format (On one line):
                    'source' <number> <subnumber> <subscriber-control-address-URL> \               
                    <publisher-control-address-URL> <publisher-msg-address-URL> \               
                    <publisher-blobmsg-address-URL>               

2.  Data sink entries describe the connection parameters for the
    opposite chain ends from a worker node to the data sink node. Each
    of these connections is also identified by a unique number which
    corresponds to the path that feeds the connection. Unlike the data
    source connections described previously, multiple data sink
    connections that belong to the same path are not supported. A
    one-to-one relation exists between a connection and a path, as it is
    assumed that data has been merged before it is sent to the sink
    node.

    The parameters that have to be specified for a data sink entry are
    the control address URLs of the subscriber and publisher bridge
    heads as well as the message and blob-message address URLs of the
    subscriber bridge head, in analogy to the data source entry
    parameters. Here the subscriber bridge head component runs on the
    worker node while the publisher bridge head is located on the data
    sink node. Similar to the source entries’ subscriber bridge head
    message and blob-message address, it is not necessary to specify
    these addresses for the publisher bridge head, since they do not
    change either.

    Entry format (On one line):
                    'sink' <number> <publisher-control-address-URL> \               
                    <subscriber-control-address-URL> <subscriber-msg-address-URL> \               
                    <subscriber-blobmsg-address-URL>               

3.  Spare data source entries contain the worker node parameters
    required for the connection of a data source node to a specific
    spare worker node. A data source worker node is identified like a
    normal data source entry by a unique major number in combination
    with a subnumber. This number is located in the same address space
    as the numbers for the normal source entries and is thus not allowed
    to conflict with them. For each spare source entry the amount of
    subnumbers must also be identical to the one specified for the
    active source entries. Parameters that have to be specifed for a
    spare data source entry are the three parameters for the publisher
    bridge head in the source node to worker node connection: its
    control, message, and blob-message address URLs. It is not necessary
    to specify the control message of the subscriber bridge head on the
    source node as this is obtained from an active source entry when a
    connection is established to the spare node.

    Entry format (On one line):
                    'sparesource' <number> <subnumber> <publisher-control-address-URL> \               
                    <publisher-msg-address-URL> <publisher-blobmsg-address-URL>               

4.  Spare data sink entries are the analogue of the spare data source
    entries for the worker to data sink node connection. They are
    identified similarly to the normal data sink entries by a unique
    number, located in the same address space as the normal sink
    entries. Three parameters for the subscriber bridge head on the
    worker node have to be specified: the control, message, and
    blob-message address URLs. Analogous to the spare source entry the
    parameters for the publisher bridge head on the sink node do not
    have to be specified.

    Entry format (On one line):
                    'sparesink' <number> <subscriber-control-address-URL> \               
                    <subscriber-msg-address-URL> <subscriber-blobmsg-address-URL>               

5.  Target entries specify control address URLs of command targets to
    which command messages will be sent when a broken connection has
    been reestablished using a spare node. Such a message instructs the
    targets to reactivate the path that has failed. Typically, these are
    tolerant event scatterer and gatherer components controlling the
    data flow. The only parameter that needs to be specified here is the
    control address URL used by the target component concerned.

    Entry format (On one line):
                    'target' <target-control-address-URL>               

6.  Detector entries specify the fault detection subscriber components
    used for each path. A detection subscriber is identified by the
    number of the path it belongs to. The parameter that has to be
    specified for such a component is the control address URL used.
    Using these entries start commands are sent to detection subscribers
    when a faulty path is reactivated after a failed node has been
    replaced by a spare node. Reactivation is required as the detection
    subscriber has been paused by the fault tolerance supervisor when
    the fault occured.

    Entry format (On one line):
                    'detector' <nr> <tolerance-detector-control-address-URL>               

##### The Bridge Configuration Class

Reading a configuration file, storing the read configuration, and
providing easy access to its data is the task of the BridgeConfig class
in the bridge tolerance manager component. To read a configuration the
class’s ReadConfig function has to be called with the name of the file
in which the configuration is stored. Access to the configuration data
is provided by a number of member functions that return different parts
of the configuration in a structured manner.

GetActivePath and GetPath return data about an active path or one path
from the whole set of active and spare paths, respectively. In both
cases the path is selected by the number specified in the configuration
file. For active paths only entries specified by the data source and
sink entries are searched, while for the whole set of paths the spare
source and sink entries are searched as well. The information returned
for a path includes the connection data for the data source connections
and the data sink connection. There may be multiple data source
connections between the configured number of sources and worker nodes,
but only a single data sink connection between one worker and data sink
node. Furthermore, the path’s absolute and active path numbers and the
type of the path are contained in the returned data field as well. A
path’s absolute and active number can differ, e.g. for spare nodes that
have been activated. The type of a path specifies whether it is active
or down or whether it is a spare path.

The class’s GetTargets function returns the list of address structures
that have been specified in target component entries.
GetToleranceDetector provides a structure for the fault detection
subscriber component that has been configured for the path number
specified to the function’s call. Included in the returned data is the
control address URL under which the detection subscriber can be
addressed.

Two functions are provided to set certain parameters of the
configuration. The first of these, SetPathStatus , is used to set the
state of a specific path in the stored configuration to either down or
up. An active or spare path’s state can be set to down, while only a
down path’s state can be set to up. When an active path is set to down,
the list of spare paths is searched for an available path that can be
used to replace the broken path. If a spare path is found, the control
addresses of each data sources’s subscriber bridge head and of the data
sink’s publisher bridge head are copied into the spare path’s data
structure and reset in the original active path’s data. The original
path’s active number is also copied into the spare path, and the states
of the paths are set to down and active respectively. A spare path which
is set to down triggers no further action, while a down path which is
available is placed into the list of spare paths.

When a new spare node has just become available it can be used to
reactivate a broken path by the SetSpareActive function called from the
command handler class. This function searches for the path with the
specified active number in the list of paths requiring replacement. It
also searches the list of spares for an available path to be used as a
replacement. If both an active path to be replaced and an available
spare are found, the source subscriber bridge head’s and sink publisher
bridge head’s control addresses are moved from the original path to the
spare one. The active number of the new path is set according to the old
active path’s one, and the new paths’ state is set to active.

##### The Bridge Command Handler Class

The CommandHandler class in the component is derived from the
AliHLTSCCommandProcessor class described in section 7.5.2 . Its
ProcessCmd function is called by the controller object in the component
when a command is received, its four other functions are called from the
component’s main loop.

In the ProcessCmd function only the command to set a bridge node’s state
is handled, which specifies a change in the state of a path in the
system. The command is handled by interfacing with the component’s
BridgeConfig instance. When a path is set to up or available an internal
list of paths that need to be replaced is searched to determine whether
a failed and unrecovered path exists. If such a path is found, it is
placed into a list of paths that need to be connected to their source
and sink node endpoints, using the spare path’s endpoints. The list of
paths requiring reconnection can be queried by calling the class’s
GetConnectsNeeded function. This is done in the component’s main loop,
as described above.

For a path whose state is set to down, different steps are taken in the
command processing function. First the path’s parameters are queried
from the configuration object. Then three commands are sent to the
bridge head components on the source and sink nodes belonging to the
path: A disconnect command to abort the connection to the broken path,
preceeded and followed by a purge events command to clear all events
that have remained in these components. Following this, the path’s state
is set to down, and a spare path to activate is searched. If no
available spare path is found, the path is placed into the list of
path’s that need to be replaced. The list is searched when a path
becomes active again, as described above. Otherwise, if a spare path is
available, it is activated in the configuration and placed in the
command handler object’s list of connections that have to be
established. This is the same list into which a newly activated path is
placed when it has to replace a broken path (see above).

When the program’s main loop has retrieved a set of connections that
have to be established, it calls the command handler’s MakeConnection
function for each of them after the disconnection from their previous
remote partners has completed. In this function connection command
messages are assembled containing the message and blob-message addresses
of the new bridge head components in the path to be activated. After
these messages have been sent successfully to the bridge heads on the
data source and sink nodes, the path is removed from the list of paths
that have to be connected and placed into a list of paths to which
reconnect commands have been sent. This second list can be queried by
calling the class’s GetReconnectPaths function. In the main loop this
function is called, and the states of all connections in the subscriber
and publisher bridge head components of the path are queried. If all
connections are established successfully, the command handler’s
SetTarget function is called to reactivate the replaced path. In this
function a start command is sent to the fault detection subscriber
monitoring the path concerned. Further commands are sent to all
configured target components to set the path’s state back to active,
instructing the data flow components to send events to that path again.

#### 7.5.6 Fault Tolerant Event Scatterer

One of the two components responsible for routing the dataflow in a
fault tolerant chain setup, the TolerantEventScatterer component is an
extension of the EventScatterer from section 7.1.2 . It replaces the
AliHLRoundRobinEventScatterer object in the original scatterer with an
AliHLTTolerantRoundRobinEventScatterer object and adds a number of
objects for the fault tolerance tasks in the component. Fig. 7.18 gives
an overview of the most important classes used. The component’s main
program does not differ significantly from the event scatterer’s. Major
differences are the use of another central class and the configuration,
creation, and setup of the required auxiliary objects for the FT tasks.
Primarily, these are an instance of the AliHLTSCInterface class from
section 7.5.2 and a status structure derived from the ones described in
the same section.

##### The Fault Tolerant Round Robin Event Scatterer Class

Similar to the original AliHLTRoundRobinEventScatterer class, the
AliHLTTolerantRoundRobinEventScatterer class is also derived from the
AliHLTEventScatterer class, described in section 7.1.2 , implementing
the six abstract functions defined by that class. It also overwrites the
two callback functions provided by the base class. The main difference
between the basic round-robin scatterer class and this class is that
AliHLTTolerantRoundRobinEventScatterer uses an MLUCFaultToleranceHandler
object, described in section 4.2.1 . This object distributes the work
load of events only between functional output publisher paths, taking
into account their respective status. In addition, it is possible to
control this class to a certain degree from external components via a
control and monitoring interface class instance.

The primary data structures on which the class operates are lists of
events, one for each of the configured output publishers and one for
retry events. Events are entered into the retry list when all output
publisher paths are marked as broken and no publisher is available to
announce the event. An event is entered into a specific publisher’s list
when it has been announced by that publisher. The data placed into each
of the lists contains everything required to announce, or reannounce,
the event: its ID, sub-event descriptor, and event trigger structure,
plus the time at which it arrived. Storing this data in the component is
necessary in case the event has to be reannounced due to a failure of
the output path to which it was assigned.

Beyond these lists two auxiliary objects are used in the class as well
as objects of two proxy classes. The proxy class objects function as
forwarders between a command processor and the scatterer object on one
hand and the scatterer object and an FT interface object, described in
section 7.5.2 , on the other hand. In the first case a function in the
scatterer is called to handle received commands, while in the second LAM
requests are forwarded from the scatterer to an interface object. The
fault tolerance handler object is only used internally in the class,
while the status structure is passed from the component’s main program
and is just updated in the class.

Although all of the functions implemented in the class contain some code
related to or affected by the fault tolerance functionality of the
component, the most important work of correctly distributing the events
is done in two functions: AnnounceEvent and SetPublisher . AnnounceEvent
is called whenever a new event has to be announced to a publisher. The
publisher to be used is first determined with the help of the fault
tolerance handler object. When a publisher is found, the event is
announced to it and is placed with the required data in the event list
for that specific publisher. If no publisher could be found for the
event, all publisher paths must be broken and the event is added to the
list of retry events. The retry list is accessed when at least one
publisher becomes available again. Events are removed from a publisher’s
list when the event is released by the scatterer’s (re-)publisher or
when it is cancelled by its original publisher from which the scatterer
received it. In the first case an EventDone message is sent via the
scatterer’s subscriber to the event’s originating publisher.

The second important new function, SetPublisher , is called by the
command processor function when the state of one of the output publisher
paths in the system changes. If events have to be announced or
reannounced as a result of such a change, the function calls
AnnounceEvent after updating the concerned publisher’s state. After a
status change command has been received, a publisher with the given
index is searched and its status in the fault tolerance handler is
compared to the specified state. If these states are equal, the change
has already been processed and the function performs no further action,
otherwise the state is updated in the handler object.

When a publisher path has become functional again and the retry list
contains events, these events are processed. They are removed from the
retry list and passed to the AnnounceEvent function to be announced. If
no events are contained in the retry list, the publisher is only used
for the respective fraction of new incoming events. For a state change
to non-functional the event list of the concerned publisher is accessed.
Each event is removed and handed to the announce event function. Due to
the status change in the fault tolerance handler object these events are
assigned to a different, still functional, publisher able to process
them. After these events have been reannounced, they are aborted in the
faulty publisher to which they had been assigned originally, using the
publisher’s AbortEvent function so that no event is contained in two
publisher objects.

One more function contains functionality related to the fault tolerance
operation. This is the PublisherDown method called by one of the
scatterer’s republisher objects when an error occurs in one of its
subscribers. In the function the state field in the status data
structure corresponding to this publisher’s state is set to faulty, and
a LookAtMe request is sent to a supervising component. The decision to
remove this publisher’s path is not taken locally in this component but
instead in the supervisor component when it has checked the respective
state.

#### 7.5.7 Fault Tolerant Event Gatherer

The second component responsible for routing the dataflow in a fault
tolerant chain setup is the TolerantEventGatherer component, an
extension of the EventGatherer component from section 7.1.3 . Similar to
the fault tolerant event scatterer component, this component replaces
the AliHLTEventGatherer class from the original gatherer with the
extended AliHLTTolerantEventGatherer class and adds a number of
additional helper objects. Fig. 7.19 gives an overview of the most
important classes used.

As in the case of the fault tolerant scatterer component, the fault
tolerant gatherer’s main program does not differ significantly from the
original gatherer one’s. The main differences are also the
initialization of the added classes, primarily status structures and
instances of the control and monitoring classes described in section
7.5.2 .

##### The Fault Tolerant Event Gatherer Class

Identical to the AliHLTEventGatherer class, the
AliHLTTolerantEventGatherer class is also derived from the
AliHLTBaseEventGatherer class and implements its five abstract defined
functions. Compared to the standard gatherer class it uses a number of
additional objects or object pointers. One of these is an
MLUCFaultToleranceHandler object, as described in section 4.2.1 . Unlike
the scatterer, the fault tolerant gatherer does not use this object to
determine where to send an event or event done data, but only to keep
track of the states of the paths associated with its input subscribers.
To select an output path for an event done data structure, it uses the
information from which subscriber the respective event has been
received.

The class’s primary data structures are event lists, one for active
events that have been received and announced but not yet released and a
backlog list holding a configurable amount of event done data structures
for already released events. In the first list EventGathererData
structures are stored, similar to the original gatherer class’s event
data list. Included in this data structure is a pointer for each event
to the subscriber object from which it has been received and the index
number associated with that subscriber. Events are added to the active
event list in the NewEvent function when a new event is received and in
the EventDone function when an error occurs sending the event done data
back to an event’s original publisher. Additionally, events are added to
the backlog list in the EventDone function after their event done data
structure has been sent successfully to its originating publisher.

Fault tolerance functionality provided by the class is primarily located
in three of its functions: NewEvent , EventDone , and SetSubscriber .
NewEvent is called by one of the component’s subscriber objects when a
new event has been received and EventDone is called by the republisher
after an event has been released. SetSubscriber is called by the
ProcessCmd function upon receipt of a command indicating that the state
of one of the input paths has changed.

When the NewEvent function is called with a new event by one of the
component’s subscribers, the list of active events is first searched
whether the event has been received already. If the event is found in
that list, the subscriber pointer and index in its data structure are
changed to the corresponding ones of the subscriber from which it has
just been received again. When a flag is set in the event’s data
structure to signal that the event has been released already, the
respective event done data that has been received will be sent back to
the subscriber from which the event has just been received. This is done
immediately without announcing the event again through the component’s
output publisher.

If the event is not contained in the active event list, the backlog list
is searched as well and when the event is found in that list, its event
done data contained in the list is sent back immediately as well. Events
not contained in either list have been received for the first time or
have already been removed from the backlog list. This last case should
not happen if the backlog list is large enough. An EventGathererData
structure for the event is created, filled, and added to the active
event list. The pointer to that data is added to the signal object used
to notify the component’s main loop about new events. After adding the
event’s data to the signal object it is triggered to wake up the main
loop and initiate the republishing of the event.

In the class’s EventDone function, called when an event has been
released, the data structure for it is searched in the list of active
events first. Once its data has been found, the pointer to its
subscriber and the subscriber’s index number are extracted. The
subscriber index is used together with the fault tolerance handler
object to determine whether the subscriber and its associated path are
in a working state. If this is the case, then the received event done
data is passed to the subscriber’s EventDone function to signal the
event’s original publisher that it can be freed. Following the
successful completion of this, the event’s data is removed from the list
and any resources occupied by its data are released. Finally, the event
done data is added to the end of the backlog list of released events.
That list’s first element is then removed if it has become too long.
When an error has occured in EventDone , the data structure remains in
the list of active events. For an event whose subscriber is marked as
faulty in the fault tolerance handler object, a flag is set in the
event’s data to indicate that it is already released and the received
done data is stored in its data structure as well. In this case neither
of the two lists is modified. When the event is subsequently received
again through a different subscriber, it is not forwarded through the
republisher again, but the event done data is immediately sent back, as
written in the description of the NewEvent function.

The last of the important fault tolerance related functions is the
SetSubscriber function, called when a command is received to change a
subscriber’s and its associated path’s state. Unlike the
TolerantEventScatterer ’s function of the analogue name, this function
does not contain much functionality. It searches for the subscriber
specified in its arguments to determine its index. Using the index it
sets the state of that subscriber in the fault tolerance handler object
to the one specified. As the events concerned will be resent through
other paths by the event scatterer, they will be received again through
different subscribers as well. When an event is received again, it can
be released using the new subscriber, thus no further action is
necessary in this function.

A fourth, additional, function that performs a task related to the fault
tolerance is the SubscriberError function. This function is called for a
subscriber object when an error occurs while sending an event done
message back to that specific subscriber’s publisher in EventDone . The
function determines the subscriber’s index number and then sets the
subscriber’s state to faulty in the component’s status data structure.
To inform a supervising instance of this change it triggers a LookAtMe
interrupt for all specified supervisor components, which will later
cause the corresponding path to become disabled.

#### 7.5.8 Fault Tolerant Bridge Components

The last fault tolerance components are the TolerantSubscriberBridgeHead
and the TolerantPublisherBridgeHead , extensions of the
SubscriberBridgeHead and PublisherBridgeHead components from section
7.1.4 respectively. Unlike the fault tolerant scatterer and gatherer
components they do not replace their central AliHLTSubscriberBridgeHead
and AliHLTPublisherBridgeHead classes. Instead the necessary
functionality is contained in additional classes used in these
components. Parts of the functionality of the two bridge head classes
have not been covered in the classes’ sections in 7.1.4 as they are used
in conjunction with the control and monitoring classes from 7.5.2 .
These parts of the classes are explained in the following paragraphs.
Differences between each component and its respective basic counterpart
are principally identical in the two bridge head types with only minor
deviations. As this section focuses on the differences between the basic
and fault tolerance components, both bridge head types are described
together with comments on their respective deviations.

Compared to the basic bridge components the major additional tasks in
the two components’ main programs are the parsing of command line
parameters for the fault tolerant relevant configuration as well as the
creation, configuration, and activation of required additional objects.
In addition to the objects of the classes described below, this includes
primarily an instance of the AliHLTSCInterface class, described in
section 7.5.2 , to allow the external control of the components.

##### Additional Bridge Head Class Functionality

The ability to change remote message and blob-message addresses during
runtime of the component is a feature contained in both bridge head
classes. To ensure that this does not happen at times when the
communication classes or addresses are in use, locks have been
introduced in the classes to protect the regions in the class methods
where they are used. These locks have to be acquired by an external
entity before it attempts to modify the remote addresses of a bridge
head object. An additional feature that should also be used before
changing addresses, is the ability to pause processing in the classes.
If a bridge head object is paused, no events are processed, new events
are not announced to the publisher bridge head component, and neither
are event done messages sent to the subscriber bridge head. Pausing the
objects before modifying the communication addresses ensures that no
communication attempt is made during the process of modification.

To support communication with the fault tolerance parts without
introducing customized bridge head classes or adding optional
functionality to the bridge head classes themselves, three additional
classes and structures can be attached to bridge head objects. The first
of these external structures is a LAM proxy object derived from a common
abstract base class, AliHLTBridgeHeadLAMProxy . Its only function,
LookAtMe , is called by the bridge head objects when an error occurs in
their Connect function while trying to establish a connection to their
remote partner. In the LAM proxy derived class used in the fault
tolerant bridge components the LookAtMe function sends LAM requests to a
configured list of supervisor component addresses. This address list is
provided to the component via command line parameters and can contain
multiple target components.

Next to the LAM proxy object an instance of the AliHLTBridgeStatusData
structure is the second external object used in the bridge classes. This
structure contains status data for a bridge component and consists of a
header field, a generic process status field, and a bridge status field.
In this last field two elements are contained, signalling the connection
status of the message communication object as well as the combined
connection status of the blob and blob-message objects. Both
communication status fields are updated in the bridge classes’ Connect
and Disconnect functions as required.

The third external entity that can be used is an error callback, derived
from the AliHLTBridgeHeadErrorCallback template class. It contains two
abstract functions, ConnectionError and ConnectionEstablished . The
first is called when a communication error occurs, including message
sends, blob transfers, or connection or disconnection attempts. In the
derived class used in the bridge head components this function pauses
the bridge object in the component and calls its Disconnect function to
abort the connection. A later communication attempt automatically
initiates a reconnection attempt of the communication objects by calling
the classes’ Connect function. When this function completes successfully
with a new established connection, the callback object’s second
function, ConnectionEstablished , is called to signal the new
connection. The fault tolerant bridge component callback object then
acknowledges the connection by restarting the paused object.

In addition to the address change ability and the use of the above three
external classes one more function is contained in the class used for
the fault tolerance functionality, PURGEALLEVENTS . When this function
is called it will access all data fields in the corresponding bridge
object and remove any contained event data structures. After this
function has been called, the object is in a state of not having
received any event, making this function inherently dangerous since
calling it can cause events to be lost in a system. This functionality
is required to bring the objects into a known clean state after a
connection to one remote partner has been aborted and before a new
connection to another remote bridge component is established. In this
situation no events will be lost as the scatterer takes care to resend
them.

##### Fault Tolerant Bridge Command Handler

In the two fault tolerant bridge components the most important objects
in addition to the three external ones attached to the bridge objects
are the command processors that handle commands received from external
supervisor components. The ProcessCmd functions are able to handle five
commands related to the bridge’s connections: Disconnect , Connect ,
Reconnect , NewConnection , and NewRemoteAddress . For the first two
just the bridge objects’ Disconnect and Connect functions are called
respectively. A Reconnect command causes Disconnect and Connect calls in
succession, with the bridge object being paused before and restarted
after the calls. For the Reconnect command the communication lock is
acquired before and released after the two connection function calls,
the two simple Connect and Disconnect commands do not use the locks.

The last two commands both contain new message and blob-message
addresses for the remote partner bridge head of the component. It is not
necessary to transmit the remote blob address as it is queried using the
blob-message object. After extracting the two addresses from the command
structure the bridge head object is paused, its communication lock is
acquired, and a currently established connection is terminated using the
Disconnect function. With the connection interrupted the new address is
passed to the bridge head object, and in case of the NewConnection
command, Connect is called to reestablish the connection. Following
this, the communication lock is released again and the object is
restarted.

In addition to the five connection-related commands, one more command is
available for processing by the command handler objects. The
PURGEALLEVENTS command clears all events from the bridge object by
calling the object’s function of the same name described above.

## Chapter 8 Benchmarks and System Tests

Results of tests executed with the developed software are presented in
this section. First short micro benchmarks are presented, followed by
network reference tests. These reference tests are used for comparison
and evaluation of the subsequent benchmarks of the two TCP communication
classes. In the next section benchmarks and scalability evaluations of
the basic publisher-subscriber interface are presented. The final
section contains descriptions and results of two tests of the complete
framework: A performance test using simulated ALICE TPC data with ALICE
analysis components and a test of the framework’s fault tolerance
functionality. The operating system used in all tests except for the
logging overhead measurement (section 8.1.1 ) was a SuSE Linux [ 133 ] [
134 ] version 7.2 running a Linux kernel version 2.4.18 [ 135 ] with the
precise accounting [ 141 ] (cf. section 8.2 below) and bigphysarea
patches [ 118 ] applied. For the logging overhead measurement a standard
SuSE Linux 8.0 was used. The corresponding data can be found in appendix
B .

### 8.1 Micro-Benchmarks

#### 8.1.1 Logging Overhead

The logging system available in the MLUC class library was designed so
that logging calls for messages with deactivated severity levels impose
as little overhead for the calling program as possible (see also section
4.2.1 ). For calling the logging system with multiple severity levels
two major variants are possible. The first is a function call using all
required parameters. Whether a specified message has to be logged is
decided inside the function based upon its severity and the activated
severity levels. The other variant for calling the system is by using an
if -statement to decide whether logging takes place followed by a
function call to execute the actual logging process in the case of a
positive decision.

To evaluate the effects of these two variants, a small program has been
written to determine the amounts of time required to execute an if
-statement and a function call. The program itself is listed in appendix
A.1.1 . Results obtained from the program are shown in Table 8.1 without
compiler optimization and Table 8.2 with compiler optimization level 2
(-O2) for a gcc 2.95.3 compiler for an i686 (-march=i686) processor.
Execution of the program was performed on a 700 MHz Pentium III
processor. Absolute execution times, though, are not as important as the
values relative to each other. The first column of each table, labeled
Reference Loop , includes only the time for a loop with just one pointer
dereference increment ( *n++ ). This pointer dereference increment is
used as the test instruction for the if -statement tests and the
function calls. It is included in the reference loop to prevent the
compiler from removing it during optimization and is therefore also kept
in each examined statement for comparison. Each of the different loop
tests is executed @xmath times to obtain good accuracy. In addition each
test has been run ten times with the values shown averaged over these
ten runs. For the exact instructions executed in each case see appendix
section A.1.1 .

Taking into account the reference loop overhead one can compare the time
required for a loop using an if -statement with the time for a loop
using a function call and an if -statement. As a result of these
comparisons one can conclude that a function call containing an if
-statement, corresponding to the first logging call option, is 8 to 14
times slower than just an if -statement, corresponding to the second
option. The efforts in making the logging system handle calls with
disabled levels efficiently are thus justified. Even with activated
logging, corresponding to the case for an if -statement followed by a
function call, the approach chosen is more efficient than a function
call containing an if -statement.

#### 8.1.2 Cache and Memory Reference Tests

In addition to the framework tests, three reference PCs, described
below, have been examined with a cache testing program [ 136 ] to obtain
the different amounts of time required to access data stored in the
level 1 cache, level 2 cache, and main memory respectively. These times
are necessary for the evaluation of the scaling behaviour of the
programs tested. They are measured by accessing memory arrays of varying
sizes with different distances between the array fields (strides)
accessed. From the graphs obtained by plotting the access times in
dependance of array size and stride one can determine, amongst others,
the different access times. Figures 8.1 , 8.2 , and 8.3 contain the
three different plots that have been obtained by running this program on
the reference PCs. Table 8.3 summarizes the values for the different
access times. During the tests the number of background processes, e.g.
system daemons, has been restricted to a minimum to exclude outside
interference effects as far as possible. The list of remaining processes
is shown in appendix A.2 . For similar reasons any networks in the
machine have been disabled and unplugged for the duration of the tests.

All of the three reference PCs listed below have dual Pentium III
processors with a 133 MHz front side bus using PC133 SDRAM memory in a
two bus interleaved mode, doubling the theoretical memory bandwidth.

-   733 MHz PC with the Katmai P3 version with 16 kB Level 1 data cache
    and 256 kB unified Level 2 cache running at half the CPU’s clock
    frequency. This PC uses a Tyan Thunder 2500 motherboard with the
    Serverworks III HE chipset.

-   800 MHz PC with the Coppermine P3 version with 16 kB Level 1 data
    cache and 256 kB unified Level 2 cache running at full CPU clock
    frequency. This PC uses a Tyan Thunder HEsl motherboard with the
    Serverworks III HEsl chipset, the successor to the III HE chipset.

-   933 MHz PC with the Coppermine P3 version with 16 kB Level 1 data
    cache and 256 kB unified Level 2 cache running at full CPU clock
    frequency. This PC uses a Tyan Thunder HEsl motherboard with the
    Serverworks III HEsl chipset, similar to the motherboard in the
    800 MHz PC.

All cache measurements have been executed with the operating system
running in single CPU mode, as exchanging the processes between the two
CPUs would influence and distort the results. The effects of the three
different types of PCs can clearly be seen in the measured access times
shown in Table 8.3 . The level 1 cache access times scale very well with
the CPUs’ clock frequencies. Level 2 cache times also show good scaling
with the respective level 2 clock frequencies, taking into account the
factor of 2 in the 733 MHz PC. Memory access times are primarily
influenced by the chipsets as the access times on the two similar
motherboards are basically identical, irrespective of the CPU clock
frequencies. These measured times will be used later in the scaling
evaluations of the publisher-subscriber communication properties.

One results of the cache and memory benchmarks is not fully understood
so far. This is the linear behaviour of the last four points of each
curve between 16 B and 4 kB array sizes, with decreasing times for
larger strides. These behaviours can best be observed in the logarithmic
plots for each reference PC. Due to the displayed behaviour cache
properties (e.g. cache size, cache line size, or associativity) are
unlikely to be the causes for this phenomenon. Possible explanations
include pipelining or queueing effects in the processor, e.g. branch
prediction or data forwarding effects. However, an exact explanation
would require a very intimate and detailed knowledge of the internal
processor architecture, which could not be obtained as part of this
thesis. A qualitatively similar behaviour can be observed on a HP
V-Class machine, while differing effects in this array size range can be
observed on an AMD Athlon, a Pentium 4, and a SUN Enterprise 10000. For
the measurements the plateau results before the decrease have been used.
The actual value displayed on the plateau corresponds very well to the
documented [ 137 ] 3 cycle load latency for a L1 cache hit of the Intel
PentiumPro system architecture on which the Pentium III is based [ 138 ]
.

### 8.2 CPU Usage Measurements

For several measurements in this chapter CPU loads on PCs running Linux
had to be measured. During some of these tests strange behaviours could
be observed which could be traced after some careful examination to the
method of timeslice (and thus CPU usage) accounting in the Linux kernel
[ 139 ] , [ 140 ] . These values are exported by the kernel via its
/proc interface and are used as the basis for all CPU accounting and
usage programs, e.g. top or xosview as well as the MLUC monitoring
classes in section 4.2.1 . As part of [ 139 ] a Linux kernel precise
accounting patch [ 141 ] has been written which allows a global as well
as process CPU usage accounting using the time stamp counter of the
processor. Using this patch CPU usage accounting can thus be done on the
granularity of the CPU’s clock frequency, much more accurate than the
default kernel accounting based on a 10 ms timeslice granularity. This
patch has been used to determine the CPU load in all measurements in
this chapter.

### 8.3 Network Reference Tests

To determine the influence of the network hardware and the operating
system on the TCP communication class benchmarks, a number of
measurements have been performed using a C program that directly
accesses the socket API to perform TCP communication. The tests have
been executed four times, for Fast and Gigabit Ethernet with and without
the TCP_NODELAY socket option set respectively. For each of these four
test types the message sending latency as well as the throughput in the
mode of a continuous stream of packets to a receiver have been measured.
In a preparatory measurement the number of blocks has been determined
which is to be sent in a stream for each block size in the throughput
tests. This block count has been determined by sending varying numbers
of 32 byte large blocks in a continuous stream to a receiver and
plotting the achieved sending rate for each block count.

The tests have been performed on four pairs of the 800 MHz reference PCs
examined in section 8.1.2 , using the PC’s onboard Intel EEPro 100 Fast
Ethernet interfaces as well as 3Com 3C996T Gigabit Ethernet adapter
cards (based on a Broadcom chip) in 64 bit/66 MHz PCI slots on the
boards. As the maximum transmission unit (MTU) 1500 B has been specified
for both interfaces. For the Fast Ethernet interfaces the standard
kernel drivers were used while for Gigabit Ethernet a driver supplied by
Broadcom [ 142 ] , [ 143 ] was used in version 2.2.19. For each
measuring point 10 measurements have been made with the average used as
the result for that point.

#### 8.3.1 TCP Network Reference Throughput

##### Plateau Determination

To determine the block count for the throughput measurement varying
numbers of blocks of the same size have been sent in direct succession
to a remote receiver. Blocks of 32 bytes are transmitted in a varying
number from 1 to @xmath (8388608 / 8 M). To calculate the sending rate
the time required for all blocks to be sent has been measured as the
program’s main output. The expected shape of the resulting curve is a
rise that flattens to slowly approach an asymptotic value. Actual
obtained results are shown in Fig. 8.4 . None off the four tests
displays the expected behaviour. The one that most closely follows the
predicted form is the Gigabit Ethernet test without the TCP_NODELAY
option. Instead each test shows the same approximate form of its curve,
a steep rise to a maximum value followed by a decrease that levels off
to approach an asymptotic value for large message sizes. For the two
tests with the TCP_NODELAY option set the decreases even reach a minimum
and rise again slighty towards their asymptotic values. The respective
peak and asymptotic values for the achieved sending rate are shown in
Table 8.4 together with the block count for each rate’s peak value. An
interesting fact observed is that both pairs of Gigabit and Fast
Ethernet tests reach approximately the same asymptotic value for large
counts, showing no effect of the TCP_NODELAY option for large block
counts. This behaviour is explained by the fact that the Linux Kernel
only evaluates this option if specific preconditions are met, e.g. all
large blocks queued have been actually sent. The results indicate that
these preconditions are met only rarely in these tests, thereby reducing
the option’s effect on the measurements.

One immediate fact that can be derived from this measurement is the
overhead involved in doing a write call for a 32 byte block (or
message). As the overhead for writing the actual amount of data can be
neglected, this approximates the minimal overhead of a write call. The
overhead can be determined by taking the inverse of the measured maximum
sending rate. For Gigabit Ethernet this is approximately 1.8 @xmath and
for Fast Ethernet it is about 2.7 @xmath . Note that this overhead only
definitely includes the TCP protocol overhead until the network packet
has been generated and enqueued for transmission. The actual
transmission until the packet reaches the physical network medium will
mostly not be included here.

##### Plateau Throughput Measurement

A message count of 524288 (512 k) blocks has been chosen for the plateau
throughput measurements as all four tests have approached their
asymptotic plateau value closely at this count in the prerequisite
measurement. The results obtained from these tests are displayed in Fig.
8.5 to 8.9 .

Fig. 8.5 displays the block sending rate achieved in the tests. As can
be seen the Fast and Gigabit Ethernet test pairs are almost identical
with only slight deviations at small message sizes. Starting at about
64 B for Fast Ethernet (FE) and 512 B for Gigabit Ethernet (GbE) the
curves become basically linear with identical slopes. Both sets differ
by about a factor of 6. The similarity of each pair can be explained by
the same reasoning as for the limited effect of the TCP_NODELAY option
in the first measurement in this section. In theory the difference
between the two sets for Gigabit Ethernet (1 Gbps) and Fast Ethernet
(100 Mbps) should be a factor of 10. The reason why this factor is only
6 is that the used GbE cards are not able to saturate the GbE link,
whereas the FE cards are able to saturate their link. This is also shown
in comparison with the curves showing the maximum theoretical sending
rate for each of the two networks. For FE the measured sending rate
approaches the theoretical limit very quickly. In contrast for GbE the
limit is approached for larger blocks and the measured rate is limited
by another factor, as it runs parallel to but does not approach the
theoretical curve. The factor between the theoretical and the measured
curves is about 1.7, indicating that the used GbE adapter cards only
utilize about 60 % of the network’s theoretical bandwidth.

Test results obtained for the network throughput can be seen in Fig. 8.6
. They have been obtained by multiplying the measured sending rate with
the respective block sizes so that they reflect application level
throughput that can be achieved by a program. In analogy with the rate
test results, the initial rise levels off to a constant plateau at 64 B
for FE and 512 B for GbE, and the same approximate factor of 6 between
the two sets becomes apparent. Two effects of both Gigabit Ethernet
tests could not be observed so clearly in the rate curves. The first of
these are deviations between 16 kB and 64 kB blocks. In this interval
they rise above the surrounding plateau level and display a peak at
64 kB blocks of around 86 MB/s. The second effect is that the network
throughput actually drops slightly with increasing block sizes, going
from 77 MB/s for 512 B blocks to 68 MB/s for 4 MB blocks. This seems to
indicate that for Fast Ethernet the limit is set by the saturated
network link, while for Gigabit some effect on the PC, e.g. from the
network card, the operating system, or the PCs memory, limits the
throughput. This can again be seen in comparison with the theoretically
achievable throughputs. For FE one can see in this figure that in the
plateau the measured throughput is about 94 % of the theoretical
maximum. The missing 6 % are due to the TCP/IP protocol overhead, the
protocol headers for each network packet which also require some of the
available bandwidth. For GbE in contrast only between about 72 % and
about 58 % of the available bandwidth are used. Accounting for the 6 %
TCP protocol overhead, as determined from FE, one can see that with the
GbE cards used in the test between 22 % and 36 % of the available
bandwidth are not used.

CPU usage measured during the tests is shown in Fig. 8.7 , on the left
hand side for the sender and on the right for the receiver. The first
fact to become apparent is the identity of the sets of Fast and Gigabit
Ethernet respectively, with the FE curves lying practically on top of
each other. For Fast Ethernet on the sender as well as on the receiver
the shape of the curve is an initial steep decrease that levels off to
an almost flat plateau, with only a slight “bathtub” minimum at its
center. An exception to the initial decrease are the 8 B block
measurements whose results are slightly lower than the ones of the
following 16 B blocks. As can be expected, absolute usage values on the
receiver are slightly higher than those on the sender. Maximum values
are at 102 % and 96 % for 16 B blocks on receiver and sender
respectively and minimum values at 18 % between 4 kB and 32 kB and 12 %
between 2 kB and 32 kB. For Gigabit Ethernet the absolute values are
higher and the “bathtub” shape is more pronounced. Maximum, minimum, and
final values on receiver and sender respectively are at 114 %, 78 %, and
84 % and 100 %, 64 %, and 72 % respectively. Corresponding block sizes
are 256 B, 8 kB, 4 MB, 128 B, 8 kB, and 4 MB. In these GbE test results
the curves on both nodes also display irregularities between 16 kB and
64 kB, with the 64 kB values being at a local maximum. One result from
these tests that has to be considered is the fact that the CPU usage
during the GbE test reaches values larger than 100 %. This means that a
single CPU computer will be fully saturated in these parts of the test
and will not be able to reach the data transfer rates displayed in Fig
8.5 and 8.6 .

For a better comparison CPU usages of the different tests have been
divided by the respective sending rates and network throughputs, with
the results displayed in Fig. 8.8 and Fig. 8.9 . The first of these two
is a measure for the CPU overhead per send call or per message, while
the second one measures the overhead per transferred byte/s. In the
plots in Fig. 8.8 showing the usage relative to the sending rate no
difference can be observed between the two pairs of FE and GbE curves
each. In a comparison of the two different sets the Fast and Gigabit
Ethernet curves are almost identical for small messages. At about 1 kB
on the sender and 128 B on the receiver the curves start to diverge and
the values for GbE become smaller and thus better. The difference for
the largest blocks is about a factor of 1.35 on the sender and 1.45 on
the receiver. One can see that the minimum CPU overhead on the sender
for each message (per second) is about @xmath for the smallest messages,
increasing to about @xmath and @xmath for the largest messages on Fast
and Gigabit Ethernet respectively. On the receiver the values are about
@xmath and @xmath for FE and @xmath and @xmath for GbE. Using the
results for a specific block size it is also possible to calculate the
expected CPU usage resulting from transferring blocks of that size with
a given rate.

The graphs in Fig. 8.9 display CPU usage normalized with achieved
throughput. They show that up to 512 B blocks on the sender and about
64 B on the receiver the four respective curves are basically identical.
They start to diverge for increasing block sizes, with the Gigabit
Ethernet curves at lower values than the Fast Ethernet ones. Seven of
the eight curves exhibit an initial sharp decrease, that flattens to a
minimum in a slight “bathtub” and then develops into a plateau. The
exception is Fast Ethernet without the TCP_NODELAY option on the
receiver. In this test a pronounced plateau does not set in at all, just
an indication of it is showing at the last two block sizes of 2 MB and
4 MB. From these results one obvious conclusion can be drawn: Gigabit
Ethernet is more efficient than Fast Ethernet concerning its use of CPU
cycles per megabyte of transferred data per second. In a second
conclusion one can see that, since some of the measured values are
larger than @xmath , a single CPU machine will be fully busy already at
values below 100 MB/s, before a GbE link will be saturated. A third
result to be deduced is that it is not necessarily the most efficient
approach to send at the largest possible block sizes. The minimum CPU
overhead per byte transferred can be found at the medium block sizes,
between about 2 kB and 32 kB on the sender and 8 kB and 32 kB on the
receiver.

##### Peak Throughput Measurement

In addition to measuring the network transfer characteristics at the
plateau values the same measurements should be performed at those block
counts where the curves in Fig. 8.4 show their peak values. Unlike the
throughput plateau the peak values of the four curves do not show at
identical message counts. Block counts of 4 k, 4 k, 128, and 16 k
therefore have been used for Fast Ethernet with and without the
TCP_NODELAY option and Gigabit Ethernet with and without TCP_NODELAY
respectively, as determined in section 8.3.1 . Results obtained from
these tests are shown in Fig. 8.10 to 8.14 .

Measured rates of these tests are shown in Fig. 8.10 . As can be seen,
the Gigabit Ethernet rates are slightly higher than for the respective
plateau test, although more deviations are present. Particularly
noticeable is the drop at 128 B for the TCP_NODELAY GbE test. For Fast
Ethernet only the values around 32 B for the TCP_NODELAY test and the
small block measurements of the test without the TCP_NODELAY option have
gained discernibly from the changed block count. Up to about 256 B the
FE test using the TCP_NODELAY option is at lower values compared to the
test without the option. Comparing measured and theoretical rates one
can see that for GbE the theoretical limit is approached earlier and
closer than in the plateau tests. For FE one can notice that for 32 B to
128 B block sizes the theoretical limit is actually exceeded by the
measured curve. This result indicates that the blocks written can all be
stored in local buffers, by the operating system, the network card, or
both, and do not immediately reach the physical network medium. Only for
larger blocks are the local buffers exceeded and the packets reach the
network. This could also explain the performance increase for Gigabit
Ethernet, in particular for the curve with the TCP_NODELAY option set,
as this measurement uses a very small block count of only 128.

In the network throughput plots in Fig. 8.11 the differences between the
plateau and peek tests are more clearly pronounced. For Fast Ethernet a
peak at 32 B drops towards the same plateau as in the previous test,
reached between 512 B and 1 kB large blocks. Up to about 512 B the
TCP_NODELAY measurements provide values below the ones obtained without
using the option. The Gigabit Ethernet curves for the TCP_NODELAY test
also display the drop at 128 B. Before that drop the values are above
the ones from the plateau measurement and afterwards below, up to
between 1 kB and 2 kB. At larger values a local maximum is present with
a peak at about 32 k, and from about 128 kB on the two test’s curves are
again basically identical. For the GbE test without the TCP_NODELAY
option set the values are higher than for the plateau test up to about
32 kB. Between 1 kB and 8 kB the peak results are considerably higher in
the local maximum present in the peak test, the difference for the
values at 8 kB is 106 MB/s compared to 75 MB/s. The comparison of the
measured and theoretical curves show more clearly that the FE
measurements partly exceed the theoretical limits. One can also see that
the GbE curves approach their network limit much more closely than in
the plateau test. This behaviour though may be just a measuring
artifact, similar to the detailed FE “ superperformance ”.

CPU usage for the sender and receiver is shown in Fig. 8.12 . On the
receiver the measured values for small blocks are not shown because of
large inaccuracies in the respective measurements. These inaccuracies
are caused by the method of measuring the usage combined with the short
running times of the tests due to the small block counts. For FE the
limit seems to be between 1 kB and 2 kB, while for GbE measurements up
to 32 kB appear to be unreliable. At larger block sizes both FE and GbE
values are basically identical to the ones from the plateau tests (Fig.
8.7 ). On the sending node the drop of the Fast Ethernet curves is less
steep so that for block sizes below 8 kB the usage is higher than for
the corresponding plateau tests. GbE results on the sender are fairly
irregular and depending on the block size can be higher or lower
compared to the corresponding results from the plateau measurements.
Neither curve displays the “bathtub” shape as clearly as in the plateau
test curves.

In the plots of CPU usage normalized to the sending rate and network
throughput, respectively in Fig. 8.13 and 8.14 , the results of the CPU
usage test are reflected partially. On the receiver the measurements up
to block sizes of about 2 kB for FE and 64 kB for GbE indicate
unreliable values. With increasing block sizes the GbE results are
approximately identical to the ones from the plateau tests, while the FE
results display a slightly irregular behaviour, although at lower values
than in the plateau test. The previous tests’ values are only approached
for block sizes above 256 kB. On the sender the GbE curves, in
particular the one from the TCP_NODELAY measurement, display erratic
behaviour for small message sizes, which as on the receiver might also
be caused by measurement inaccuracies. For larger blocks the curves
again become basically identical to the ones from the plateau test. For
FE the TCP_NODELAY curve is at higher values than its counterpart
without the option up to 128 B blocks. At higher block sizes they become
identical and display higher values than the respective plateau test
results. Starting between 8 kB and 16 kB the FE peak test curves also
become basically identical to the ones from the plateau throughput
tests. Therefore, as far as the CPU efficiency is concerned, there is no
significant advantage over the plateau tests, the differences on the
sending and receiving nodes should approximately balance.

#### 8.3.2 TCP Network Reference Latency

As the final network reference test the message latency has been
determined by sending messages between an originating sender and a
receiver. The sender transmits a number of messages to the receiver and
waits for an identical reply after each message before sending the next
message. By measuring the time required to send all messages and receive
all replies the average message latency is determined. The results are
shown in Fig. 8.15 .

All four curves in the test display the same behaviour. With increasing
message counts a drop to an asymptotically approached plateau is
observed. This reflects the decrease of the relative overhead per
message due to infrastructure overhead, e.g. connection establishing.
The values of the two Fast Ethernet and Gigabit Ethernet test pairs are
identical in the plateaus. At smaller numbers the respective results
obtained without the set TCP_NODELAY option are slightly higher than the
ones obtained using the option. Latency times for Fast Ethernet are
consistently smaller than for Gigabit Ethernet, in the plateau the
values are about @xmath compared to @xmath .

#### 8.3.3 Network Reference Summary

As a summary Table 8.5 and 8.6 list the parameters obtained from the
network reference plateau and peak throughput measurements respectively.
Each entry holds the minimum and maximum values with their respective
block sizes as well as the average of all values. In this table the
whole range covered by the tests from 8 B to 4 MB is included. One
observation can be made regarding the rule of thumb that 1 % of one CPU
is used for every megabyte transferred per second. With the tested
configuration this is an approximate lower bound, as almost every value
of CPU usage divided by throughput is above that limit. The only
exceptions are the minimal Gigabit Ethernet values on the sender.

### 8.4 Communication Class Benchmarks

For the communication classes benchmarks have been carried out with the
TCP message and blob class implementations. The SCI classes have not
been tested due to the prototype status of the implementation. Two
different measurements have been executed for the message classes,
measuring the message latency as well as the achievable continuous
throughput during message sending as a function of the message size. For
the blob classes these two tests have been performed twice, using the
standard on-demand type allocation where a block is remotely allocated
for each transfer as well as the preallocation method where the whole
remote buffer is allocated before any transfer, and buffer management is
executed locally in the sender. The hardware and system software used
for the tests is identical to that used in the network reference tests,
described in section 8.3 . For both network adapters sending has been
performed twice, with and without explicit connect calls, to determine
the influence of establishing implicit connections on the transfers.
Again the results of the measuring points have been obtained as the
average of ten measurements each. Each test’s result is described first
in detail, and then two summaries for the message and communication
classes are given as well as an overall summary for the TCP
communication class implementations.

#### 8.4.1 TCP Message Class Throughput

In order to benchmark the TCP message class, measurements have been made
to determine the maximum rate achievable by streaming a continuous
sequence of messages to a target without waiting for a reply. This test
is relevant for the comunication classes’ use in the framework which has
been designed to not require a reply from a remote side anywhere. As for
the network reference tests a prior measurement is used to determine the
number of messages to be sent for each size by measuring the sending
rate for different numbers of messages streamed to the receiver.

##### Plateau Determination

To determine the number of messages to be used for the following
throughput measurements, a prerequisite measurement has been made for
each of the four test types (FE, GbE, explicit or implicit connects (cf.
sections 5.2.1 and 5.4.2 )) in order to establish the influence of this
number on the throughput. For the test the smallest message of 32 bytes
is transmitted in a varying number from 1 to @xmath . A plateau with an
asymptotic value was expected. The throughput tests were then to be
performed using a message count on the plateau. To restrict the running
times of the tests, the start of the plateau was intended to be used.

The actual results of these tests are shown in Fig. 8.16 . Connected as
well as unconnected tests display increasing curves to a first plateau
followed by a steeper decrease to a second plateau. Peak values for all
curves are reached at about 2048 (2 k) messages while asymptotic values
are reached at 262144 (256 k) messages. These values are therefore used
as the counts for two separate throughput measurements. The exact reason
for the observed sudden decrease has not been determined yet. A possible
explanation are overflows of system or network interface buffers, e.g.
socket send or receive buffers, causing packet loss and retransmits, but
this hypothesis has not been verified yet. One test to determine or at
least narrow the cause of this drop would be to modify the benchmark
program to use different socket buffer sizes, vary these over a certain
range, and observe whether the drop occurs at different message counts.
A variation of the message size to determine its effect on the behaviour
could also be performed in separate measurements as well as in
combination with the buffer size variation. Due to the large parameter
space and correspondingly large amount of measurements, and the time
required for them, these investigations have not been executed as part
of this thesis. For the use of the framework the observed drops do not
present a problem, as the communication classes are only used with
explicit connections. In this mode even the values after the drop are
sufficiently high for the given requirements, as will be detailed in the
following sections. However, in the long run research into the
phenomenon as well as modifications of the communication classes to work
around it, if possible, are certainly desirable.

##### Plateau Throughput Measurement

At the message count of 262144 (256 k) messages the plateau has been
reached for all four sending types and the first throughput measurement
has been performed using this message count. The message size varied
from 32 B to 1 MB with the results obtained shown in Fig. 8.17 to 8.21 .

Fig. 8.17 shows the message sending rates achieved in the four tests
together with the results from the reference tests. As can be seen, the
message sending rate is considerably higher for the tests with explicit
than with implicit connections, both for Fast and Gigabit Ethernet. This
result could be expected, as in the implicit connection measurements a
new connection has to be established and terminated for each message,
adding the connection overhead every time. On the plateau of the two
unconnected (implicitly connected) tests the rate is only limited by the
overhead of establishing the connection for all messages. Only for
larger messages does the rate become limited by the network limit. For
the GbE test the overhead is big enough that it does not even approach
the limit fully but only starts to be limited by it. In the connected
test the overhead introduced by the protocol between the sender and
receiver communication objects also adds overhead, decreasing the
achievable message rates in comparison with the reference tests. This
decrease can be primarily seen for small message sizes, for Fast
Ethernet up to 256 B and for Gigabit Ethernet up to about 4 kB. For the
smallest message sizes the decrease is fairly significant, a factor of 6
for FE and almost one order of magnitude for GbE. This indicates that
the message class code still has some potential for optimizations. But
even with these results the achieved rates in the connected mode are
still easily high enough to allow the classes’ use in the framework.

In order to show the network throughput that can be reached by an
application, the achieved sending rates have been multiplied with the
respective message sizes. The resulting curves are shown in Fig. 8.18 .
As was to be expected these curves correspond very closely to the rates
from Fig. 8.17 . Both Fast Ethernet as well as the connection Gigabit
Ethernet curve approach the curve from their respective reference
measurement, while the unconnected Gigabit Ethernet curve still rises
slowly towards it. Similar to the rate curves one can see the overhead
from the communication classes by the fact that they reach the hardware
limit later than the corresponding reference measurement. An interesting
point can be observed in the connected GbE curve. For the largest
measured block sizes, from about 128 kB on, this curve even exceeds the
reference curve. This behaviour indicates that the communication
approach used in the class is more effective at utilizing the systems’
resources than the relatively simple reference program. Both graphs in
Fig. 8.19 below support this thesis. In the receiver plot on the right
hand side the receiver CPU usage of the message class is higher than the
one from the reference benchmark, and in particular it is greater than
100 %, indicating that due to its multi-threaded design it is able to
utilize the system’s two CPUs better. In the sender plot on the left
hand side the CPU usage of the connected GbE curve is lower than the one
from the GbE reference measurement for most of the test. This in turn
could indicate that on the sender the communication class uses the CPU
or memory system more efficiently, being therefore less constrained by
it and allowing higher sending rates.

CPU usage for the sending and receiving nodes is displayed respectively
on the left and right hand sides of Fig. 8.19 . One obvious result that
can be seen is the high CPU usage on the sending side and the very low
usage on the receiver for the two unconnected measurements at small
block sizes, up to about 8 kB. The reason for the very low rates at
small block sizes in the unconnected mode therefore seems to be the high
CPU load produced from initiating the connections on the sending node.
Accepting connections on the receiving node does not seem to be so CPU
intensive. A second interesting feature, as already remarked above, is
the fact that on the sender the communication class CPU usage in the
connected GbE test is mostly lower than the reference GbE usage. For the
lower block sizes this could, in addition to the potential reasons
outlined above, also be caused by the lower sending rate of the
communication class in that block range. At higher block sizes, however,
the throughput achieved by the communication class was higher and this
reasoning cannot be applied. As outlined above at these rates it is
therefore more likely that the sending approach used in the
communication classes, using write preceeded by select calls, is more
efficient than the simple approach of using blocking write calls in the
reference benchmark program. On the receiver the behaviour of the two
respective curves is reversed, the communication class consistently uses
between 10 % and 20 % more CPU cycles compared to the reference
benchmark. Here the communication class introduces more overhead than
the reference benchmark. One likely cause of this overhead are the
allocation and deallocation calls of the memory for each message as well
as its copying. Similar to the reference test, the measured CPU load
reaches more than 100 % and therefore uses both of the nodes’ CPUs, in
particular on the receiving side. As for the reference test this implies
that single CPU nodes will only be able to handle lower throughputs than
measured in this benchmark.

For a better comparison all CPU usage measurements should be evaluated
with respect to the sending rates or network throughputs as shown in
Fig. 8.20 and 8.21 respectively. The rate and bandwidth plots are
correlated by the message size due to the way the network throughput is
determined, as detailed above. Fig. 8.20 clearly shows the high relative
overhead caused by establishing a connection for each message,
particularly on the sender but on the receiver as well. This relative
overhead per message can be several orders of magnitude above that for
just sending a message over an established connection. In the sender
graph one can also see that both connected message class measurements
initially are higher than their respective reference measurement. For
larger message or block sizes, however, they fall below the respective
reference curve. This is a further indication for the behaviour already
observed in the rate and sender side CPU usage plots (Fig. 8.17 and 8.19
). In both of these single plots the message class showed better values
(higher rate, smaller CPU usage) than the reference measurement for
large messages. On the receiver side this behaviour is not present, here
the CPU overhead outlined above is high enough that the per-message
overhead of both connected message class curves is higher than the
respective reference one. Similar to the reference measurement of CPU
cycles per sending rate from Fig. 8.8 one can again see that Gigabit
Ethernet is more efficient in its use of CPU cycles per transfer than
Fast Ethernet, both on the sending and the receiving nodes. On both
nodes the unconnected measurements approach the connected ones with
increasing block sizes, showing that the overhead per message for
establishing the connections decreases with increasing message size, as
could be expected.

The plots of CPU cycles per megabyte of data transferred per second in
Fig. 8.21 show mostly the same information as the ones in Fig. 8.20 .
One additional item can be observed in Fig. 8.21 . Unlike the reference
curves the connected message class measurement curves do not show a “
bathtub ” curve shape on the sender. At the points where the reference
curves rise again the message class curves remain constant. The GbE
curve even shows a slight drop. This behaviour again underlines the fact
that the sending approach used in the class makes a more efficient use
of CPU cycles than the one used in the reference benchmark program. On
the receiver the measured message class values show the same behaviour
as the reference curves, although at higher values. At small blocks the
values are considerably higher, more than one magnitude for some message
sizes. This shows again the high overhead added on the receiver side,
presumably at least partly due to the message allocation and release
calls.

##### Peak Throughput Measurement

At the message count of 2048 (2 k) all four sending types have reached
their peak value for the measured rate. As for the plateau measurement
the message size has been varied from 32 B to 1 MB with the results
obtained shown in Fig. 8.22 to 8.26 . Due to the short measuring times
involved and the details of the measurement the CPU related values for
small message sizes, particularly on the receiving node, could not be
measured accurately, similar to the problems in the network reference
test. These values have therefore been excluded from the measurement
results.

For the maximum achieved sending rate, shown in Fig. 8.22 , one can see
that for smaller message sizes the achieved rates are higher than for
the plateau measurement in Fig. 8.17 , differing by factors of about 3.6
and 8 for the connected and unconnected tests respectively. The
connected Gigabit Ethernet curve runs close to the FE one up to the
256 B message size and starts to diverge for higher sizes. Both of these
curves have reached their bandwidth limit at about 8 kB. For both Fast
Ethernet tests as well as the connected Gigabit one the curves are
identical with their corresponding plateau throughput curves for message
sizes exceeding certain limits: 512 B for the connected GbE, 256 B for
the connected FE, and 16 kB for the unconnected FE curve. Below that
limit each peak curve features higher values than its plateau
counterpart. Compared to the peak reference tests one can see that the
communication class’s connected Fast Ethernet test reaches the reference
sending rate earlier than in the plateau test, showing that it is less
constrained by the limit at small messages encountered in that test. In
the connected Gigabit Ethernet measurement the communication class
reaches the respective reference limit later than the FE and later than
its plateau counterpart. At about 256 B to 512 B it reaches the same
values as the connected plateau GbE curve, and therefore already at
these sizes seems to be limited by the same factor as the plateau
measurement. The two unconnected curves initially again run almost
constant, but at higher values than in the plateau test and they start
to decrease earlier. At small messages the limits between these two
tests thus seem to be different while the limit that causes the later
decrease, most likely the available bandwidth, is approached sooner.

Fig. 8.23 shows the application level network throughput that has been
achieved in these four tests. The respective plateau results are
represented in Fig. 8.18 . In consistency with the achieved rates one
can see that the maximum network throughput is reached in the connected
FE test for 64 B messages already. It is closely approached by the
unconnected one between 8 kB and 16 kB. In both cases this happens for
smaller message sizes than for the related plateau test. All four tests
also start at higher throughput values and correspondingly reach the
bandwidth limit earlier. The connected GbE curve is identical to the
plateau curve for messages of at least 1 kB with the exception of the
somewhat higher peak shifted towards the lower range between 16 kB to
32 kB. Apart from this peak the maximum values are not higher than the
ones from the plateau tests, as expected. Similar to the FE reference
peak measurement the message class curve reaches more than the
theoretically possible network throughput. It must therefore be assumed
that the data is buffered to a large degree as well. As detailed in the
peak reference test this buffering is also a potential explanation for
the performance increase in the peak tests. Further remarkable
properties in this graph are the kinks in several of the curves. No
clear explanation has been found for them yet, a possible explanation
for at least some of them are buffer limits which are encountered. With
full buffers the measurements then again display different behaviour as
when all or a large amount of data can be buffered. Similar to reference
peak tests the peak message class tests are mostly at higher throughput
values than the respective plateau measurements. An exception are the FE
curves where they have already reached the hardware limit.

CPU usages for the tests are shown in Fig. 8.24 . As for the peak
reference tests some values on the receiver could not be measured
accurately. These values have therefore been excluded from the graphs.
As can be seen in the figures, CPU usage on the sender for the two
explicit connection tests for small message sizes is much higher
compared to the respective plateau test (Fig. 8.19 ). This continues up
to the point where the usage levels become equal, for FE in the range
between 32 kB to 64 kB and for GbE at about 64 kB messages sizes,
similar to the observed behaviour for the sending rates and network
throughputs. In the unconnected tests a different behaviour is
displayed, with the FE test showing continuously lower CPU usage values
coupled with an earlier decrease compared to the related plateau test.
The unconnected GbE test initially displays lower values than its
respective plateau throughput test, with both curves running basically
flat at about 30 % and 45 % respectively. For messages sizes higher than
32 kB, though, the plateau test usage is lower. Both Fast as well as
Gigabit Ethernet tests reach the same final value for the 1 MB message
size. On the receiver the Gigabit Ethernet tests are mostly separated
and only approach similar values for the 1 MB message size as well. The
two Fast Ethernet measurements run much closer and are approximately
similar. They also reach the identical values at 1 MB messages. One can
also see that, similar to the plateau tests, the connected message class
measurements on the sender display lower CPU usage values than the
reference test; the GbE one almost over the whole test range and the FE
one for large messages only. In contrast on the receiver the usage of
the reference measurements, where present, is considerably lower than
the message class’s, by at least 10 %. This is again similar to the
behaviour in the plateau tests. On the sender the unconnected
measurements are mostly at lower values than in the plateau test, while
they are higher on the receiver. The difference between the unconnected
usage on the two nodes is therefore not as extreme as it was in the
plateau measurement where most of the load was produced on the sending
node. Compared to the plateau test measurement all peak test curves can
be lower or higher, depending on measurement type and message size.

For better comparison the CPU usage values of each test are normalized
to the achieved sending rate as well as to the measured network
throughput, as for the plateau tests. The resulting graphs are shown in
Fig. 8.25 and 8.26 . For the CPU usage per rate measurements of the
connected tests on the sender side one can see that the FE and GbE
curves are almost identical. GbE values are slightly smaller than FE
values, similar to the same measurement for the plateau tests from Fig.
8.20 . Between 128 B and 2 kB inclusively the plateau curves are at
lower and therefore more efficient values. From 4 kB on the curves for
peak and plateau tests are almost identical, although the FE peak curve
runs at slightly higher values than the plateau one. In the unconnected
tests on the sender the peak test curves are a factor of about 10 lower
than the plateau ones for sizes from 32 B to about 4 kB. All four curves
are basically constant for this whole interval. Between 8 kB and 32 kB
the peak and plateau curves show different behaviours, with the peak
measurements still at lower values. From there on all four curves again
show an identical linear rise. Where reliable values are available on
the receiver, the four curves rise continuously with the peak curves at
slightly lower values compared to the plateau ones. This trend continues
up to 2 kB for FE and 32 kB for GbE, after which the respective curves
in the peak and plateau tests are basically identical. For the
unconnected test the corresponding peak and plateau curves appear mostly
similar, although the peak curves are at slightly lower values for sizes
up to about 2 kB. All four curves reach the same approximate final value
for the largest messages as the respective plateau curve. In Fig. 8.25
one can see that over the whole test range each message class (and
reference) GbE curve on the sender is at lower values than the
corresponding FE curve. This shows that in this mode, where at least
part of the data is buffered, GbE makes more efficient use of the
available CPU cycles compared with FE. Also one can see again that at
about 32 kB messages the connected message class measurements on the
sender reach lower and better values than the corresponding reference
measurement. Therefore even in this partially buffered mode the sending
method in the communication class is more efficient. On the receiver the
behaviour is also as before, the message class curves are higher than
the reference measurements where present.

Comparing the CPU usage per network bandwidth in Fig. 8.26 and 8.21 it
can be noticed that for the unconnected tests on the sender the plateau
and peak test curves are almost identical in shape. However, the
starting values of the two peak curves are again a factor of 10 lower
than the corresponding plateau values, and only for messages greater
than 64 kB do the curves show identical values. Differing from this, the
peak results in the connected tests on the sending side show higher
initial values. The respective curves are again almost identical after
32 kB. On the receiver node the results for the peak and plateau tests
also have the same general shape, where values are available, showing a
steady decrease. The peak measurements are at slightly lower values and
display a slightly more unsteady behaviour. For messages exceeding 16 kB
the corresponding peak and plateau test curves again run at basically
identical constant values. Results obtained for the unconnected tests
are almost indistinguishable in the peak and plateau tests, both in
behaviour and the measured values. They show a constant decrease that
flattens to constant values of about @xmath for Fast Ethernet and
between @xmath and @xmath for Gigabit Ethernet. Apart from these values
the same conclusions can be drawn from these graphs as already derived
from Fig. 8.25 .

#### 8.4.2 TCP Message Class Latency

To determine the latency of message send operations for the different
configurations a number of messages are transmitted from a sender to a
receiver. For each of these messages the receiver sends a reply message
to the originating sender. The sender in turn waits for this reply
before sending its next message. A ping-pong message pattern is thus
established between the two programs, similar to the network reference
latency test principle. Results obtained from this test are shown in
Fig. 8.27 .

As can be seen the measured latency decreases and approaches an
asymptotic plateau value for all tests, although the test remains on
this plateau for the connected tests only. As expected the two
unconnected tests have a latency much higher than the connected ones,
due to the overhead of establishing a new connection for each new
message in the unconnected (or implicitly connected) tests. The
difference for the plateaus is almost a factor of 3. An unexpected and
currently unexplained characteristic is displayed in both unconnected
tests, starting between 8192 (8 k) and 32768 (32 k) messages, where the
latency increases abruptly from about @xmath to @xmath , a factor of
about 2.5. It continues to increase at a slower pace to about @xmath for
524288 (512 k) messages. The initial higher start and decrease to the
plateau value is most likely explained by the measurement and
infrastructure overhead that dominates the timing results obtained from
the measurements so that the plateau values reflect the actual latency
present in applications. An obvious exception is the unexplained rise
for high message counts in the unconnected tests, which might be related
to the drops shown in the graph of message rates as a function of
message counts in section 8.4.1 and Fig. 8.16 . But as for that graph
the cause of the latency increase could not be determined in this
thesis.

Table 8.7 summarizes the minimum latency measured in each of the four
tests and the reference measurements. The unconnected tests, as already
detailed, are higher than the connected ones by a factor of about 3.
Gigabit Ethernet tests are between 5.0 % ( @xmath ) and 7.8 % ( @xmath )
slower than Fast Ethernet ones for the unconnected and connected cases
respectively. One can see that the differences between the respective
Fast and Gigabit Ethernet measurements are, to a first order, constant.
The differences between a message class measurement and the
corresponding reference measurement can therefore be most probably
attributed to overhead introduced in the class itself. This overhead
therefore amounts to approximately @xmath and @xmath for explicit and
implicit connection modes respectively.

#### 8.4.3 TCP Message Benchmark Summary

One conclusion that can be drawn from these benchmarks is that the TCP
message classes can be used in the data flow framework, especially in
the ALICE HLT. The framework is designed so that message sending rates
do not have to be higher than the actual event rate, and all event
related message transmissions are performed over explicitly established
connections. Therefore handling the 1 kHz maximum event rate required
for the ALICE HLT presents no principal problem. Fulfilling this
requirement is in particular aided by the fact that no reply messages
are needed and thus no restriction on the minimum latency exists. At the
expected message sizes between 64 B and 512 B the CPU usage is expected
to be between about 0.5 % and 1.0 % on the sender and 1.3 % and 3.3 % on
the receiver at the 1 kHz message rate required for proton-proton
operation. These usage values are scaled to one 800 MHz CPU so that
100 % corresponds to one fully used CPU. For the 200 Hz rate of
heavy-ion operation the CPU load is expected to be lower by the
corresponding factor of 5. Message transfers are mainly dominated by the
available memory bandwidth which does not increase as fast as CPU power,
as described in section 2.1 . CPU usage will most likely not decrease
proportionally to the available CPU power in the future as a result. As
shown in the two different throughput tests, the classes seem to perform
better when the messages can be sent in short bursts than for a constant
stream of messages to the receiver.

A further result that can be inferred from the tests is related to the
use of Fast or Gigabit Ethernet for message transfers. If latency is not
the deciding factor, as for the classes’ use in the framework, Gigabit
Ethernet is the favored choice due to its lower CPU usage in sending the
same amount of messages or data (Fig. 8.20 and 8.21 ), even if the
network throughput of Fast Ethernet would already be sufficient to
fulfill the requirements. Fast Ethernet cards should be chosen if
latency is the primary concern, due to their lower latencies compared
with Gigabit Ethernet. This of course is only possible when they are
able to fulfill the bandwidth requirements. These conclusions, however,
should be treated with care as they depend on the respective network
adapters used and the measurements should be repeated for adapters
concerned. On the other hand, due to their higher maximum throughput
Gigabit Ethernet adapters are generally more efficient in their use of
CPU cycles, and the technology itself sets some restrictions on the
minimum latency that can be reached. Cost differences between the
adapters have to be considered as well, but due to its rapid evolution
this is beyond the scope of this document. As a note on the cost
calculation: Using a Gigabit Ethernet card to profit from the better
efficiency does not necessitate a Gigabit Ethernet switch port as well.
If its bandwidth is sufficient, a Fast Ethernet switch can also be used,
reducing the cost of this solution.

In a direct comparison with the network reference tests several features
can be noticed, also partially reflected in the summaries of the peak
and plateau throughput measurements in Table 8.8 and 8.9 . A first
difference can be observed in the behaviour of the sending rate in
dependence of the block or message count. In the reference measurements
the decrease after the initial plateau is not as obvious as in the
message test. Concerning the rate and throughput it can be seen that the
results from the reference measurements are much better. The plateau
values differ by a factor of 9 for Fast Ethernet and a factor of 6 for
Gigabit Ethernet. Differences in the peak measurements are not as large,
factors of 2 and 3 can only be observed here. These comparisons only
apply to the connected message tests, as expected the results of the
unconnected ones are far poorer. Examining the CPU usage as well as the
efficiency of CPU usage divided by throughput it can be noticed that the
reference tests are better, except for the Gigabit Ethernet measurements
on the sending nodes. For this case the minimum values measured are
actually below the ones from the reference tests. The most likely
explanation for this unexpected behaviour is that the use of write calls
with timeouts followed by select calls in the message classes is more
efficient for large blocks than the blocking write calls used in the
reference program. Looking at the message or block latencies it is again
apparent that there is a certain time penalty associated with the
functionality contained in the message classes, as the results are
almost a factor 2 worse for FE and 1.5 for GbE. Part of the overhead and
performance decrease introduced by the message classes is certainly
unavoidable due to the more elaborate checks and actions that have to be
performed in them compared to the reference program. For example the
reference program knows the block size in advance and can discard the
data immediately after reading it. But even taking this into account,
the results indicate that there still seem to be opportunities for
improvement.

#### 8.4.4 TCP Blob Class Throughput with On-Demand Allocation

Similar to the message classes the throughput benchmark using on-demand
allocation for the blob class also consists of two parts, the initial
evaluation of the number of blocks or blobs sent for each size and the
actual throughput measurement as a function of the block size.

##### Plateau Determination

The results of the plateau measurements that have been made for the blob
communication mechanism are shown in Fig. 8.28 . It can be realized that
for the two connected tests the rate rises steadily with the number of
blocks to a plateau reached between 8 k and 32 k blocks. Unlike the
message communication these tests show no peak over the measured
spectrum. For the unconnected tests a similar rise is displayed which
drops off again and reaches a lower plateau starting at about 8 kB. For
the blob throughput tests’ evaluation of the plateau value the number of
32 k blocks was chosen as a common point of reference. A 2 k (2048) blob
count was chosen for the peak values of the unconnected tests, while for
the connected tests the values at 32 k blobs were reused as no real peak
exists for these tests.

##### Plateau Throughput Measurement

Fig. 8.29 to 8.33 display the results that have been obtained during the
on-demand allocation throughput tests using a blob count of 32768 (32 k)
for each size, going from 256 B to 4 MB. The first result, the
achievable sending rate, is shown in Fig. 8.29 . As can be seen, the
maximum values are about 1.5 kHz to 2 kHz for the connected tests and
about 120 Hz for the unconnected tests. In the Fast Ethernet test with
explicit connections the maximum value is somewhat higher than 2 kHz,
reached at the smallest block size of 256 B. With increasing block sizes
the rate continously decreases. From about 16 kB to 32 kB the decrease
in rate becomes linear with block size as the available network
bandwidth becomes the limiting factor, as can be seen in Fig. 8.30 ,
too. For the connected Gigabit Ethernet test an initial plateau with
only a slight decrease can be observed from 256 B to 8 kB block sizes.
At 8 kB blocks a steeper decrease starts which also develops into a
linear decrease when the network starts to become the bottleneck. Except
for the initial two values at 256 B and 512 B, where it is slightly
lower than Fast Ethernet, the connected GbE test constantly shows the
highest achievable sending rate, as could be expected due to its higher
available bandwidth. The fact that the FE curve is higher at 256 B could
be explained by its lower latency in exchanging the allocation messages
which dominates the rate at these small sizes. Between 64 kB and 128 kB
the unconnected Gigabit Ethernet rate starts to exceed the connected
Fast Ethernet curve, which closely approaches the unconnected Fast
Ethernet curve after 256 kB block sizes. For these sizes the overhead of
establishing the connection for each block thus is becoming negligible
compared to the sending of the large blocks. In comparison with the
reference measurements one can see that the connected blob tests reach
the reference values later than the corresponding message class curves.
This is most likely due to the overhead of allocating a block in the
remote buffer and sending of an additional message to announce the block
to the receiver. But, similar to the message class, for large blocks the
achieved sending rate exceeds the one of the reference benchmark. The
explanation for this is the same as given in the message class section:
The message class usage of preceeding the read and write calls with
select class seems to be more efficient than the simple use of blocking
read and write calls.

The measured network throughputs of the tests are shown in Fig. 8.30 .
One can see that the blob class throughput curves rise linearly with
three separate slopes to a specific point for each curve. At these
points the increase slows down as the hardware limit is approached,
which can be seen by comparison with the reference measurements. In this
comparison one can also see again that the connected GbE blob class
throughput exceeds the respective reference throughput for large blocks,
as already observed in the rate measurement above.

CPU usage during the blob transfers can be seen in Fig. 8.31 , for the
sender on the left and the receiver on the right hand side. On the
sender one can see that the connected Fast and Gigabit Ethernet blob
class measurements are lower than the corresponding reference
measurements; the FE one for large blocks and the GbE one for the whole
test range. This corresponds to behaviour already observed in the
message class tests. The reason why the GbE curve displays less CPU
usage even at small block sizes is very likely related to the fact that
the achieved rates and throughputs at these block sizes are noticeably
lower than in the reference measurements. The other notable feature in
the sender graph is the very high load of the two unconnected blob
measurements at small blocks. This is also similar to the observed
message class behaviour and is most likely caused by the connection
establishing overhead. On the receiver node the blob class connected
Fast Ethernet curve is constantly at higher values than its reference
counterpart. The connected GbE blob class curve is lower than the
reference measurement at small block sizes and rises above it for large
blocks, approximately where its rate and throughput also exceed the
reference ones. At small block sizes the low usage therefore seems to
be, at least in part, caused by the low sending rate and throughput
values.

To allow a better interpretation of the CPU usage values they are
normalized to the achieved blob sending rate and network throughput,
similar to the message throughput tests. The results are displayed in
Fig. 8.32 and Fig. 8.33 respectively. These curves are very similar in
appearance to those for message sending in Fig. 8.20 and Fig. 8.21 ,
with the exception of the higher blob class usages compared to the
corresponding message classes usages. This is the case for sender and
receiver, connected and unconnected, as well as Fast and Gigabit
Ethernet tests. A possible explanation for this is that for each blob to
be transferred three messages (block allocation request and reply as
well as block announcement) have to be sent as well, which speaks in
favor of the preallocation method examined below in section 8.4.5 .

##### Peak Throughput Measurement

To measure the peak blob class throughput the implicit connection tests
have been run with a blob count of 2048 (2 k) where the throughput for
256 B blocks has reached its peak value. Unlike these unconnected tests,
the connected ones have not been rerun as the respective curves from
Fig. 8.28 do not show a peak value. Instead the 32 k count measurements
from the previous section have been used for this measurement as well.
The discussion of the measurements and their differences is therefore
primarily focussed on the implicit connection tests. Results that have
been obtained from these measurements are shown in Fig. 8.34 to 8.38 .

For the achieved blob sending rates shown in Fig. 8.34 it can be seen
that the results for the peak tests are higher than those from the
previous plateau tests of Fig. 8.29 . The results of the tests differ by
a factor of about 3 and 4 for the Fast and Gigabit Ethernet measurements
respectively. A comparison of the connected tests with the respective
reference measurements yields the same qualitative results as for the
plateau test. The reference rates are reached only for relatively large
blocks, but the connected GbE measurement exceeds the GbE reference
curve for the largest blocks, as before. Both blob class Fast Ethernet
measurements initially have higher rates than their respective Gigabit
Ethernet counterpart. As for the plateau test this is again presumed to
be due to Fast Ethernet’s lower latency. Since at the operating system
level messages have to be exchanged for connection establishing and
termination the increased latency should influence the unconnected tests
more than the connected ones. This is reflected in the graph, as the
connected GbE curve exceeds the connected FE curve earlier than the
unconnected GbE curve exceeds the unconnected FE curve. One interesting
point to be found in the plot is the transition from 1 MB to 2 MB blocks
for the unconnected Gigabit Ethernet curve, where the new curve displays
a bend and for large blocks becomes identical to the curve from the
previous plateau measurement. The reason for this bend are presumably
buffers filled by the larger blocks which causes the same behaviour for
the peak test as for the plateau test.

From the measurements of the application level network throughput in
Fig. 8.35 the same tendencies as for the blob sending rate can be
derived. The throughput for the unconnected tests is higher by factors
of about 3 to 4 for small blocks up to the point where the available
bandwidth becomes the limit. One can also see the bend in the
unconnected GbE curve that marks the transition from the peak to the
plateau throughput measurement. This bend appears even more pronounced
than in the rate plot of Fig. 8.34 .

In the CPU usage measurements of the peak throughput tests in Fig. 8.36
one can see that compared to the plateau tests the sender CPU usage in
general reaches lower values. For the receiver on the other hand the
measured peak test usages are higher than the ones from the
corresponding plateau tests. As for the previous communication class
measurements it can be seen that the connected CPU usages on the sender
are still lower than the reference ones; for FE only for large blocks
but for GbE over the whole test range. On the receiver the plateau
behaviour is also repeated. Connected FE blob class usage is always
higher than the reference measurement and connected GbE usage exceeds
the respective reference usage only for large blocks, approximately in
the range where the class reaches a higher throughput than the reference
measurement. One can also see, on the sender as well as on the receiver,
that the unconnected GbE blob class measurement transitions between
512 kB and 1 MB blocks from the peak test behaviour to the same
behaviour that has been exhibited in the plateau test. This is similar
to what was seen in the rate and throughput measurements (Fig. 8.34 and
8.35 ).

For a better comparison normalized CPU usages, i.e. CPU usage divided by
the achieved rate respectively network throughput, have been plotted in
Fig. 8.37 and 8.38 instead of absolute CPU usage. As can be expected due
to the higher sending rates, both peak test curves show significantly
lower CPU usage per rate values on the sending node compared to the
corresponding plateau test results in Fig. 8.32 . The improvement
between the two tests is about a factor of 5 for each of the curves. An
interesting point can be seen for the two curves at the end of the
initial constant plateau in Fig. 8.32 . After a short transition at
32 kB for FE and 128 kB for GbE the curve from the previous plateau
throughput measurement becomes identical to the smoothly increasing
curve from the peak throughput measurement. In this case the bend is
clearly seen in the curve of the plateau tests and not the in peak
tests’, as for the Gigabit Ethernet rate, network throughput, and CPU
usage measurements. The reason for this behaviour has not been
determined and no plausible explanation could be found in the course of
this thesis. On the receiver node the curves from the two different
throughput tests are nearly indistinguishable. Compared to the reference
benchmarks the behaviour already observed in the preceeding
communication class tests is seen: On the sender the connected FE and
GbE blob class tests are better for larger blocks, while on the receiver
they are less efficient over the whole test range.

As before, the measurement of the CPU usage per network throughput shown
in Fig. 8.38 results in a similar improvement factor of roughly 5
compared with the plateau throughput test in Fig. 8.33 . The transition
in the plateau test curve towards the smooth curve from the peak test
can also be seen, although less pronounced, between the 32 kB and 64 kB
and 128 kB and 256 kB blocks for Fast and Gigabit Ethernet respectively.
The comparison between the blob class and the reference measurements
leads to the same conclusions as discussed for the usage per rate
graphs.

#### 8.4.5 TCP Blob Class Throughput with Preallocation

As for the previous throughput measurements the benchmark for the blob
class in preallocation mode also splits in two parts, the initial
determination of the number of blocks (blobs) sent for each block size
and the actual throughput measurement in dependence of the block size.

##### Plateau Determination

The plot of the blob sending rate in dependence of the number of blocks
sent in preallocation mode is shown in Fig. 8.39 . The same shapes of
the curves as in the corresponding on-demand allocation tests from the
previous section can be made out, although the absolute rates are higher
by a factor of 2 for the connected final values, 1.5 for the peaks of
the unconnected curves, and about 1.2 for their final values. As the
final plateaus for the unconnected tests are reached at higher values
than in the on-demand allocation rate measurements, the blob counts of
131072 (128 k) and 2048 (2 k) have been chosen for the plateau and peak
measurements respectively. Since the connected curves display no peak
the plateau results at 128 k messages are reused as the values for the
peak tests.

##### Plateau Throughput Measurement

Results of the throughput plateau measurements with block counts of
128 k are displayed in Fig. 8.40 to Fig. 8.44 . The rate measurements in
Fig. 8.40 show the same basic shape as the equivalent curves in
on-demand allocation mode from Fig. 8.29 . Achieved rates are higher
than in the on-demand tests though, initially about 160 Hz unconnected
and 4.8 kHz connected. These rates are higher by a factor of 1.3 for the
unconnected and about 3 for the connected tests respectively. For the
two Gigabit Ethernet curves even the final results where the available
network bandwidth already influences the rate are higher than the ones
from the on-demand tests. In the Fast Ethernet measurements the two
tests produce identical results from 64 kB blocks on. At these sizes the
network sets the absolute limit and is not only a limiting influence as
for GbE. Comparing the two connected curves one sees that in this test
Gigabit Ethernet has a higher transfer rate than Fast Ethernet for all
block sizes. The higher initial rate for FE in on-demand mode is thus
due to the lower message latency that influences the round-trip time for
the allocation messages as presumed. Since these messages are not
required in preallocation mode the effect is not seen in this test.
Comparing the blob class and reference measurements one can see that for
larger blocks both connected curves reach the respective reference curve
and in the case of Gigabit Ethernet even exceed it, as also observed in
the previous communication class measurements. Absolute rates are not
considerably different than in the on-demand allocation blob class
measurement, since the hardware is the primary limit for large blocks,
but the block sizes where the reference curves are reached or exceeded
are about a factor of 2 smaller than in the on-demand allocation
measurement. The performance increases, compared to the on-demand
allocation measurements, should be due to the use of the preallocation
mode, with its lack of the allocation request-reply message sequence.

Similar results can be deduced from the network throughput measurement
in Fig. 8.41 . The achieved throughput is higher for all curves, with
Fast Ethernet at the smaller blocksizes up to about 64 kB and with
Gigabit Ethernet over the whole test range. The available bandwidth
starts to limit the throughput already at about 256 kB blocks for the
unconnected Fast, at 16 kB for the connected Fast, and at 256 kB for the
connected Gigabit Ethernet test. Unconnected Gigabit Ethernet is not
limited by the available bandwidth up to the maximum tested blocksizes
of 4 MB.

As can be seen in Fig. 8.42 the CPU usage during blob transfers on the
sending node increases for both Gigabit Ethernet tests by about 5 % to
10 % over the whole test range, compared to the equivalent on-demand
allocation tests. This is presumably due to the absolute higher sending
rates observed in this mode. For the two Fast Ethernet measurements on
the sender the opposite effect is observed. CPU usage is lower than for
the equivalent on-demand allocation tests by about 5 % for smaller block
sizes. This decrease is present up to the largest block sizes where the
network limits the throughput and thus the CPU usage too. The FE network
limit also starts to affect the tests for smaller block sizes than for
the on-demand tests. The reason for this decrease is presumably caused
again by the lack of the allocation request-reply messages. In addition
to increasing the rate the CPU usage decreases as the additional
messages do not have to be sent and received on each node. In comparison
with the respective reference measurements one again can see that the
communication class GbE curve is at lower values over the whole test
range while the FE curve is higher, but only for small block sizes. This
is identical to previously observed behaviour, and the presumed causes
are similar as well. However, the communication class GbE curve
approaches the reference curve closer than the GbE blob on-demand
allocation measurement, due to its higher absolute usage values.

On the receiver the effect on the connected Gigabit Ethernet test is
identical to that on the sender, except that on the receiver the
increase is between 5 % up to almost 20 % for small blocks. For the Fast
Ethernets test the opposite effect compared to the sender sets in for
small block sizes, CPU usage is increased by almost 10 %. The curves for
FE are again identical to those from the on-demand test at large blocks.
The unconnected GbE test only shows a small increase at small block
sizes of less than 5 %. With growing block sizes, though, the difference
grows to about 10 % as well. These observed increases in CPU usage are
most probably caused by the increases in blob rates/network throughput,
as correspondingly more data has to be handled by the receiver. On the
sender the increased rate does not necessarily cause a CPU usage
increase, as less data, practically none, has to be received there
without the allocation reply messages from the blob receiver node.

Inspecting the CPU usage normalized with the event rate and network
throughput in Fig. 8.43 and 8.44 respectively, the comparison with the
on-demand allocation mode is more favorable for the preallocation tests.
The basic forms of the curves are identical for both measurements on the
sender as well as on the receiver. In a comparison of the two transfer
modes, the CPU usage to rate (or throughput) ratios of the preallocation
tests are better by factors of 2.5 to 2.2 for the connected Fast and
Gigabit Ethernet tests respectively and 1.25 to 1.17 for the unconnected
FE and GbE measurements. These ratios are measured for 256 B blocks,
towards the largest blocks the ratios become almost equal with a
relative difference of only a few percent. This respective approach of
the two tests’ curves can be expected, as for these large blocks the
main influence is by the actual transfer itself and the allocation
message exchange becomes negligible. Qualitatively the behaviour
relative to the reference tests is similar to the ones of the on-demand
allocation measurements. Due to the lower values in preallocation mode,
however, the relative differences are distinct; when the communication
class measurements are higher than the reference, the difference has
become smaller and when the class measurements are lower, the difference
has become larger.

##### Peak Throughput Measurement

The final blob class throughput measurement is performed in
preallocation mode with a block count of 2048 (2 k) for the unconnected
tests, with results shown in Fig. 8.45 to 8.49 . As for the on-demand
allocation peak throughput tests the connected plateau throughput tests
with a 128 k block count from the previous section are reused here. The
following discussion will therefore focus on the two unconnected tests.

For the measured rates and network throughputs, shown in Fig. 8.45 and
8.46 respectively, the relation to the preallocation plateau throughput
tests is in principle identical to the relation of the plateau and peak
on-demand allocation tests. The rates (and therefore also throughput
values) achieved are the highest of all four unconnected block transfer
tests: a maximum rate of more than 650 Hz and 533 Hz at 256 B blocks for
Fast and Gigabit Ethernet respectively. Network throughput for Fast
Ethernet is higher up to about 64 kB blocks where the network bandwidth
becomes the limit for all FE tests. The achieved results differ by
factors of 3 to more than 4 relative to the preallocation plateau tests,
and for the on-demand peak tests the factors are 1.3 to 1.4. At larger
block sizes this effect is decreasing, and at the largest blocks this
test has only a small advantage for Gigabit Ethernet and none for Fast
Ethernet. The reasons for this increase relative to the on-demand peak
test are again the lack of the request-reply allocation message
sequence. With regard to the preallocation plateau test the increase is
presumably caused by buffers which are able to accept a large part of
the small blobs, analogous to the other peak test increases. As for the
other comunication class tests the connected FE curve approaches its
appropriate reference curve and the connected GbE curve exceeds it for
large blocks.

CPU usage on the sending node is displayed in Fig. 8.47 . For the
connected measurements it is approximately equal to the one from the
preallocation plateau test described above, despite the higher rates.
Concerning the unconnected tests, their usage is considerably higher at
small blocks than in the plateau test, reflecting the already observed
overhead of establishing connections on the initiating (or sending)
node, coupled with the higher rates in this test. Compared to the
on-demand peak test the usage is slightly higher, most likely because of
the higher sending rates.

On the receiver measured connected usage is again roughly the same as in
the preallocation plateau test and the unconnected usage is again
considerably higher at small blocks. The factor for the unconnected
measurements is between 3 and 4 for both tests relative to the plateau
test. Compared to the on-demand allocation peak tests the results are
identical or higher, up to a factor of 2 for the connected GbE curve at
small block sizes. This increase is again caused most probably by the
increase in sending rate relative to the on-demand test. Where
measurements are available both connected measurements considerably
exceed their respective reference measurement, at least in part due to
the additional block announcement message which has to be received by
the communication class, as already discussed.

In the efficiency comparison of CPU usage per rate respectively network
throughput, the curves on both sender and receiver have almost identical
forms in the two peak throughput tests, with the results of the
preallocation mode tests at lower (and better) values. For small block
sizes the preallocation test results are better by a factor of about 1.5
for the two unconnected tests and by a factor of about 2.3 for the
connected ones. With growing block sizes the difference between the
tests becomes smaller. On a small scale the results are even reversed so
that the on-demand test partially has better values. At the largest
block sizes the difference between the tests is at most 1 % for the
unconnected tests in favour of the preallocation tests. For the
connected tests at these block sizes the on-demand allocation mode tests
are better by about 0.5 % to 2 %. As the block sizes increase the
block’s actual transfer increasingly dominates the overhead and the
difference caused by the different allocation messages becomes smaller
and smaller, causing the different tests’ results to become similar. A
possible explanation for the on-demand allocation test’s efficiency
being higher than the preallocation test’s could be that in
preallocation mode buffer can be filled more quickly, due to the missing
allocation sequence latency. Therefore buffers can also overflow more
quickly, leading to packet losses and retransmits. These retransmits do
not increase the throughput but still have to be processed, decreasing
the transfer’s efficiency. The effect is probably not seen in the
plateau tests’ due to the larger amount of data transferred, causing
overflows in both allocation modes. On the other hand, these differences
are not large and could therefore just be noise respectively measurement
uncertainties.

#### 8.4.6 TCP Blob Class Latency with On-Demand Allocation

To determine the latency of the TCP blob class, measurements similar to
those for the message class from section 8.4.2 have been executed in
on-demand mode. Corresponding measurements in preallocation mode are
described in section 8.4.7 . In this test varying numbers of data blocks
are transferred from sender to receiver. After each block the sender
waits for the block to be sent back by the receiver before continuing
with the next block. Fig. 8.50 shows the results that have been obtained
from this ping-pong pattern.

As can be seen in the figure, the latency curves display the same
general pattern as those for messages in Fig. 8.27 . A sharp decrease
turns into a plateau and rises sharply to a second plateau in the
unconnected tests. The jump to the second plateau in the unconnected
tests sets in at lower counts than for the message tests, between 2 k
and 8 k for the blobs compared to between 8 k and 32 k for messages. In
comparison with the message tests the plateaus are at higher values.
These higher values are to be expected as sending a blob with on-demand
allocation requires the sending of three messages on both nodes, two for
the buffer space allocation and one for the notification that a blob is
available. Similar to the message tests and also as expected the
unconnected tests display again much higher latencies compared to the
connected tests. The jump between the two unconnected test plateaus is
just by a factor of 2 instead of 2.5 as found in the message test.

Table 8.10 summarizes the minimum latency times measured for each of the
four different configurations. Each connected test is about 4.5 times
faster than the respective unconnected test, and the Fast Ethernet tests
are between 23 % (connected) and 29 % (unconnected) faster than their
corresponding Gigabit test counterparts, in each case a higher relative
difference than in the message latency tests. Compared to the reference
measurements the latencies are considerably increased, almost by an
order of magnitude for the connected and about a factor of 40 for the
unconnected tests. One part of the explanation for this is most
definitely the fact that three times the respective message latency
(allocation request, allocation reply, blob announcement) is included in
these times. When these message latencies are subtracted the remaining
“pure” blob latencies are considerably lower, but still higher than the
message and in particular the reference latencies. This can in part be
caused by the larger block used in the blob test, 256 B instead of 8 B
for the reference and 32 B for the message class. A second potential
cause can be the 32 bit value that is written back to the sender by the
receiver blob class after each transfer, to indicate completion, which
contributes approximately one respective reference latency to the blob
latency.

#### 8.4.7 TCP Blob Class Latency with Preallocation

The same test as in section 8.4.6 has been performed for the blob
classes in preallocation mode as well, with the results shown in Fig.
8.51 . As can be seen the shape of the four curves is as good as
identical to the ones in the on-demand allocation latency measurements
in Fig. 8.50 . A major difference between the plots is that the values
in preallocation mode are lower than those in on-demand allocation mode,
which also can be seen when comparing Table 8.11 and Table 8.10 . The
unconnected tests are faster roughly by a factor of 1.5, the connected
ones even by a factor of about 1.8, compared to the values from the
on-demand allocation tests. For the preallocation values themselves a
comparison of the unconnected and connected tests yields factors of 5.1
and 5.4 for Fast and Gigabit Ethernet respectively. Compared to Gigabit
Ethernet, Fast Ethernet is about 22 % and 29 % faster for connected and
unconnected tests respectively, basically identical to the on-demand
allocation latency differences. Relative to the reference latency
results the measured latencies are still fairly high. Taking into
account the latency corresponding to the one remaining message (blob
announcement), the resulting “pure” times are identical to a first
approximation with the respective times in on-demand allocation mode. In
preallocation mode, however, all values are slightly lower. These time
differences could be due to the added allocation and release
functionality that has to be executed locally on the receiver in
on-demand allocation mode.

#### 8.4.8 TCP Blob Benchmark Summary

The primary conclusion to be drawn for the TCP blob classes is that,
just as the message classes, they are able to handle the requirements
presented by the ALICE HLT within the scope of the current hardware used
in the tests. The requirements are particularly fulfilled in
preallocation mode, as it is used in the framework’s bridge components.
In heavy-ion mode average block sizes of around 300 kB are expected for
the largest parts of data, the ADC values read-out from the detector. At
these block sizes the classes are still able to handle a rate of more
than the required 200 Hz over Gigabit Ethernet. The 1 kHz rate required
for operation in pp mode is possible up to 64 kB large blocks. These
evaluations all refer to the connected mode, as the blob classes will
not be used with implicit connections in the HLT. A reduction of CPU
usage during transfers is still desirable, and it should also be
achievable to a certain degree by more powerful CPUs and more efficient
network adapters. But optimization potential in this respect should also
still exist in the communication library itself so that further tuning
measures of its classes can be undertaken as well.

Just as for the message classes, if latency is of lesser importance, the
use of Gigabit Ethernet recommends itself due to the lower relative CPU
usage values per network throughput, even when the absolute throughput
required does not necessitate its use. If latency is a concern, the use
of Fast Ethernet is suggested whenever possible because its latencies
are lower than those of Gigabit. For the framework components this is of
no concern as the conditions allow to treat latency with secondary
priority. For the HLT the amount of data to be transferred over the
network, however, implies the use of at least Gigabit Ethernet even
without its efficiency advantages. Since the sizes of the different
types of data passed between the HLT’s stages are not yet known,
predictions of the CPU usage incurred by the transfers cannot be made at
the current stage.

A direct comparison with the network reference measurements reveals
similar results as in the message class – reference comparison. Again
the first noticeable details are the different characteristics in the
graphs showing rate as a function of count. For the connected tests the
decrease is not present at all, while in the unconnected ones it is
again more obvious than in the reference tests. As far as latency is
concerned, this is considerably increased in the blob classes because of
the need to send the blob data itself as well as the message informing
the receiver about the transmission. Results from the plateau and peak
throughput tests are shown in a summary in Tables 8.12 and 8.13
respectively. In the following discussion only the connected tests are
regarded, as the results of the unconnected ones are considerably poorer
in turn. Concerning the achieved block sending rate the reference tests
are mostly much faster, particularly for small block sizes. The
respective differences are actually between one and two orders of
magnitude. An exception are the Gigabit Ethernet measurements at the
largest block sizes, where the minimum achieved rates, with the blob
measurements are slightly higher than those for the reference’s. For the
network throughput the reference tests consistently show better values
than each of the blob tests, at least for the minimum and maximum values
listed in the tables. As a further exception the GbE measurements of CPU
usage and efficiency for sender and receiver differ from the expected
characteristics that the reference tests always results in lower values.
These differences also vary between the plateau and peak throughput
tests. In the plateau measurements the absolute CPU usages for GbE are
always lower on the sender, while for the efficiency the minimum values
at large block sizes are lower. On the receiver the absolute CPU usage
minima are lower, and maxima are roughly equal or slightly higher. As
expected, efficiency values on the receiver are always higher than in
the reference tests. In the peak tests only values on the sender are
better in the blob tests. Absolute CPU usage is always better for the
blob classes, while for the efficiency only the minimum values are
lower. The conclusion that can be drawn is also quite similar to the one
for the message classes. Some of the overhead and performance loss in
the blob classes certainly has to be accepted as part of the added
functionality and in particular flexibility compared to the simple
reference test program. However, the potential for optimization is
definitely greater than in the message classes, as can be seen in the
considerably lower sending rates of the blob classes compared with the
message class rates, so that the need for tuning measures is a definite
must. The efficiency measurements that, at least on the sender, indicate
better results compared to the reference program, again stand out
positively, with the same possible explanation as for the message class.

#### 8.4.9 TCP Communication Class Benchmark Summary

As an overall summary for the two types of TCP communication classes one
can repeat the separate communication classes’ conclusions that they are
suited for use in the ALICE High Level Trigger in the present version,
even though some room for optimization is still present. The separation
into different classes, optimized for small and large transfers, does
not produce significant advantages in the tested version if both types
of communication are handled by the same physical communication medium.
An advantage might be seen when the message communication is run over
Fast Ethernet, utilizing its lower latency, and the blob communication
over Gigabit Ethernet using the higher bandwidth and better efficiency.
One further reason why the separation does not produce any clearly
visible effects could also be found in the fact that the current
implementations of the communication code are not yet optimized enough
for each of their specific tasks. Additionally, the combination of
Gigabit Ethernet and the TCP network protocol does not provide enough
features that allow to optimize for transfer efficiency. Also, TCP is
not able to take enough advantage of many features provided by the
networking hardware. The use of other network protocols and technologies
could therefore provide clearer effects of the separation. Possible
optimization measures for the communication classes are the use of the
writev Linux system call that allows to pass several blocks to write
into a connection socket in one system call as well as the reduction of
memory allocation and release calls in the message classes. Preliminary
tests of these modifications in the publisher-subscriber interface from
section 6 indicate good benefits from these measures as described below
in section 8.5.3 .

Although all the above measurements are of course highly specific for
each network device on which the corresponding test was executed, the
results indicate for message as well as for blob classes that depending
on the optimization goal, e.g. throughput, absolute CPU usage, or CPU
usage relative to throughput, different block or message sizes are the
optimum choice. The largest block size is not necessarily always the
best choice.

### 8.5 Publisher-Subscriber Interface Benchmarks

#### 8.5.1 Timing Measurements

To evaluate the performance of the publisher-subscriber framework and
provide data and estimates of the current and expected future overhead
incurred by the framework, a number of measurements have been performed.
A set of benchmark publisher and subscriber programs has been written to
execute the basic functions associated with announcing and freeing
events only. No additional functionality, e.g. shared memory mapping or
accessing, is contained in these programs. The tests have been performed
on the three reference PCs evaluated and described in section 8.1.2 to
obtain measurements about the scaling properties of the software. All
benchmarks have been performed with almost no user processes or daemons
running on the system to exclude interference effects from other
processes, e.g. (de-) scheduling, as far as possible. The list of
remaining processes is shown in appendix A.2 .

In the two benchmark processes four different parts of the framework
have been instrumented for timing measurements using the gettimeofday
system call that delivers a microsecond resolution. The timing overhead
of a gettimeofday call itself is small. In a test program on an 800 MHz
PC the time needed to execute 100000 calls was 61275 @xmath , so one
call requires about 600 ns.

The four benchmarked parts of the framework are executed for each event,
as it is announced to a subscriber and released again, as detailed in
chapter 6 :

-   The main publisher object’s AnnounceEvent function that stores an
    event’s management data into the publisher’s internal tables and
    dispatches the data’s descriptor to the write threads for each
    subscriber.

-   The write thread’s function that writes the data into the named
    pipe.

-   The NewEvent function in the subscriber that writes the data release
    message ( EventDone ) into the pipe to the publisher.

-   The publisher’s EventDone function that releases the event
    management data from its internal tables.

One of the principal problems of measuring a program’s (processing)
overhead is that a program’s running time for a particular code section
does not provide an adequate measure of its overhead. This inadequacy
results from the fact that a program may be suspended while executing
the section, increasing the section’s runtime but not the overhead.
Reasons for a suspension might be that the operating system deschedules
it, allowing other programs to execute, or because it has to wait for an
operation to complete, e.g. disk or network I/O, or for a lock to become
available. While the case of explicit sleeps can in principle be
accounted for during measurement by deducting the corresponding sleep
time from the runtime, the other cases cannot be predicted. Even in the
explicit sleep case there are problems, as the operating system may let
a program sleep longer than the specified time. Therefore a way has to
be found to exclude these sleep times from the overhead measurement for
a given program, as these represent time where the active thread sleeps
and thus does not use the CPU producing overhead.

In the publisher-subscriber measurements lock calls have been excluded
from the timing, by starting and stopping the timing measurement before
and after it explicitly. For the other cases the presumption can be made
that code section runtimes during which a program has been suspended,
which therefore are unsuitable for overhead measurements, will be
significantly longer than those where the section could be executed
uninterrupted. These longer values can then be excluded from the
overhead measurement. A precondition for this is of course that the
examined code sections are sufficiently short so that a minimal
descheduling will actually be longer than a normal section execution.

@xmath events have been processed on each of the three PCs and the four
timing values for each event have been entered into a runtime histogram
for later analysis. For the analysis a cut-off has been made so that the
contents of the bins used for the analysis amount to at least 90 % of
the histogram entries. This cut-off is made under the assumption,
detailed above, that longer times only occur when the examined process
is inactive, which has no influence on the framework’s overhead. The
mean values with and without the cut-off are shown in Table 8.14 , all
values are in microseconds. For both mean values the scaling constants
from 733 MHz to 800 MHz and 800 MHz to 933 MHz are shown as well. The
complete timing analysis plots are shown in Fig. 8.52 , 8.53 , 8.54 ,
and 8.55 , containing the superimposed time distribution for the three
reference PCs, 733 MHz values in green, 800 MHz values in blue, and
933 MHz values in red. 800 MHz and 933 MHz values are scaled with
factors of @xmath and @xmath respectively for clarity. Each plot also
shows the respective bins where the 90 % cut-off was made. As can be
seen from the times which make up the majority of the measurements, the
values presumed to be descheduled are indeed significantly longer than
the majority.

A final measurement that has been performed is the global average
announce rate that can be sustained over the @xmath events. These values
are shown for the three different PCs with the derived time overheads (
@xmath ) in Table 8.15 . The average processing overhead is scaled by a
factor 2 with respect to the transaction rate period as there are two
processors in the tested computers, which both have been fully busy
during the tests. It should be noted, however, that these averages
include overhead introduced by waits from the operating system, which
would be present even in an idle system, but become less likely in case
of a system operating at a much lower transaction rate and performing
trigger algorithms. The numbers stated here should be taken as an all
inclusive upper limit. As can be seen the achieved rates on the
reference PCs are already high enough to easily allow the use of the
framework in the ALICE HLT. No performance problem should therefore be
encountered in running the interface on PCs available when the HLT
becomes operational.

These performance tests were specifically made using a multi-processor
architecture in order to include scheduling effects of the Linux system.
For instance, even the best data locality in the communication algorithm
can be destroyed, if the rescheduling results in a job often being
assigned to different CPUs and thus requiring the cache coherency
protocols to copy the cached data between the CPUs across their front
side bus. The results seem to indicate that this problem only occurs
with a negligible frequency in the framework interface.

#### 8.5.2 Scaling Behaviour

To gain an impression of how the publisher-subscriber framework will
perform on future CPUs with their large expected increases in clock
frequency, an analysis of the software’s scaling behavior on the three
reference PCs has been made. Since the software handles to a large
fraction inter-process communication one would expect that a high
fraction of the data accesses address the system’s main memory
accessible by both the reference systems’ CPUs. Such a behavior would
result in a very bad scaling behavior with regard to the clock
frequency, as the memory bandwidth and access latency increase much
slower than the CPU frequency. This effect can also be seen in the
access time measurements of the three test PCs in Table 8.3 .

For comparison a scaling plot has been produced, shown in Fig. 8.56 , in
which the relative values of various measurements are plotted as
determined for the three different PCs over their clock frequencies. All
values are scaled relative to the values of the 800 MHz PC. The red
reference curve shows the clock frequencies. For clarity the curves have
been offset slightly to prevent overlapping.

The green curve for the level 1 cache access times shows a perfect
scaling, which can be expected as this cache works with the CPU’s core
frequency. In the blue level 2 access curve one can see the influence of
the level 2 frequency for the 733 MHz CPUs, which is only half the CPU’s
frequency unlike for the other CPUs. Folding in this factor of 2 for the
733 MHz PC the pink curve again shows the same perfect scaling property.
The cyan memory access curve shows the influence of the chipset (733 MHz
to 800 MHz transition) and that for identical motherboards the CPU
frequency basically has no influence on the memory access time (800 MHz
to 933 MHz transition).

The orange curve shows the scaling behavior of the sum of the times
measured in the previous section 8.5.1 without the 90 % cut-off, while
the black curve shows the same sum using 90 % cut-off. One can see, as
also shown in Table 8.14 , that both values somewhat under-scale
compared to the theoretical values of 1.091 and 1.166 for 733 MHz to
800 MHz and 800 MHz to 933 MHz respectively. But even taking into
account this scaling behavior, the results indicate that the framework
can utilize and profit from more than 90 % of CPU performance increases.

Based on those measurements it is assumed that the processing overhead
for a complete event announce and release loop is going to drop to
@xmath or less during the next four to five years, before ALICE (and its
HLT) starts to operate. The value of @xmath per event announcement is a
useful metric that can be used to calculate the overhead in a more
complex chain of multiple processes. Even given all scaling
uncertainties, however, the existing framework is fast enough to fulfill
all ALICE HLT requirements to operate at full speed, already to date. On
the other hand, scaling uncertainties are minimized for CPU bound
processes, and the interface’s architecture is optimized for smalls
amounts of data exchange, making it CPU bound as much as possible.

#### 8.5.3 Future Optimization Options

Preliminary tests with two optimizations of the low-level pipe
communication and the pipe proxy classes used in the
publisher-subscriber interface show very promising results. The
optimizations in question are the replacing of multiple write calls with
one writev call that allows to specify multiple blocks to be written
with one system call as well as the reduction of new and delete
allocation and release calls in the communication functions.
Measurements of the interface with these optimizations in place are
currently very preliminary and by far not as exhaustive as the ones
presented above, but the performances measured so far indicate that a
factor of 4 improvement of the maximum performance, and by deduction
also overhead, in favour of the new optimized version could be possible.

### 8.6 Framework System Tests

#### 8.6.1 ALICE HLT Proton-Proton Performance Test

In a large-scale test of the framework in an ALICE High Level Trigger
configuration, a setup with 19 nodes has been used to simulate the
readout and online data processing of one slice of the ALICE Time
Projection Chamber. A slice is one of 18 sectors in one readout plane
and therefore represents @xmath of the total data volume of the TPC, as
described in section 2.2.2 . For the test simulated piled-up
proton-proton events have been used as the processing power available
for the test would not have been sufficient to enable the intended
operation at the maximum readout frequency of the TPC of 200 Hz using
simulated Pb-Pb events. The limit in this case is the time required to
reconstruct the tracks in the event. A schematic view of the cluster
configuration used for this test is shown in Figure 8.57 .

The data sources for the HLT are the FEPs (see section 2.2.2 ) connected
via optical fibers with the readout electronics mounted on the
detectors. Data from each TPC slice is shipped to the HLT over six
fibers, the sub-sectors associated with each fiber are called patches.
In the present test setup these FEPs are replaced by software in the
form of MultFilePublisher components. For each patch the zero-suppressed
and run-length encoded simulated ADC data is read from files by the
MultFilePublisher s and published into the start of the chain. This type
of data is similar to the data shipped from the detector. On average the
size of the encoded ADC data files is about 14 kB per patch. Encoded ADC
data is expanded to sequences of ADC values by the ADCUnpacker
components, increasing the size of the data by a factor of about 2 to 3.
These values in turn form the input for the ClusterFinder , which
reconstructs the three-dimensional coordinates of deposited charges in
the detector, called space points. Together with each space point the
amount of charge associated with that respective cluster is stored. The
MultFilePublisher , ADCUnpacker , and ClusterFinder components run on
one node for each patch, called Hierarchy-Level (HL) 0. From this node,
the space point data are shipped via bridge components to the next
Hierarchy-Level, responsible for combining the space points into track
segments, performed by the Tracker component. Since tracking is the most
time consuming process in the chain, data is distributed to three
trackers on separate nodes using an EventScatterer . Due to the usage of
two-processor machines it is possible to run two trackers in parallel on
each node, each processing data belonging to the same event but from
different patches. At the output of each Hierarchy-Level 1 node the data
stream is merged by an EventMerger component and forwarded to
Hierarchy-Level 2. On this level the data streams of the six patches are
merged into a data stream consisting of events with six blocks of track
segments per event. For load balancing reasons this data stream is
processed by six PatchMerger components running on three nodes. Each
PatchMerger combines the track segments of tracks crossing boundaries
between the patches. Since Hierarchy-Level 2 contains three nodes
running PatchMerger components the data streams of these nodes are
merged in Hierarchy-Level 3 using a SliceMerger component. Again, for
load balancing reasons two SliceMerger processes are running on each
node. The output obtained from running the processing chain described
are reconstructed tracks of one TPC slice.

Operation at a sustained rate of more than 430 events/s has been
achieved by using the described setup consisting of 19 nodes with twin
CPUs operating at 733 MHz and 800 MHz and connected via Fast Ethernet.
The bottlenecks in this setup were the nodes in HL0, especially the
ADCUnpacker components. In the final setup of the HLT, these steps will
be performed by FPGAs implemented on the RORC cards and thus will not
consume time on the FEP CPUs. The maximum TPC readout rate intended for
p-p mode in ALICE is 1 kHz and CPUs with more than 3 times the clock
frequency relative to those used in the test are already available
today. Therefore the use of the framework with these software-only
analysis components for online tracking in p-p mode seems to be a
practicable option for the ALICE High Level Trigger. Given the necessary
increases in CPU processing power and an adequate number of CPUs and
thus financial resources, the use in Pb-Pb mode is possible as well.

#### 8.6.2 Framework Fault Tolerance Test

##### Fault Tolerance Test Setup

In order to demonstrate the fault tolerance capabilities of the
framework described in section 7.5 a test setup has been created using
seven computers. The setup used is the sample setup shown in Fig. 7.16
and detailed in section 7.5.1 . It is briefly described and shown again
here for convenience. Two of the seven computers function as a data
source and sink, one supervisor hosts two control programs, three
perform identical worker tasks, and one acts as a spare worker node, as
displayed in Fig. 8.58 .

On the data source computer one process publishes data from a file to a
TolerantEventScatterer component (see section 7.5.6 ) with three output
publishers. Each of these publishers in turn supplies its data to one
TolerantSubscriberBridgeHead (section 7.5.8 ) that sends the received
data to a TolerantPublisherBridgeHead (section 7.5.8 ) on one of the
worker nodes. Attached to this TolerantPublisherBridgeHead is a dummy
processing component (section 7.3.7 ) that publishes any received data
unchanged to a further TolerantSubscriberBridgeHead . This
TolerantSubscriberBridgeHead on the worker node in turn sends its input
data to a TolerantPublisherBridgeHead on the data sink computer.

Each of the three TolerantPublisherBridgeHead processes on the sink node
has two subscribers attached. The first one is an instance of the
ToleranceDetectionSubscriber (section 7.5.3 ), controlling each of the
three data streams for continuous operation and the second is one of
three subscribers belonging to a TolerantEventGatherer component
(section 7.5.7 ). These gatherer-subscribers merge the three parts of
the data stream into one stream again. Attached to the gatherers output
publisher is one subscriber process checking for lost events in the data
stream, an instance of the EventSupervisor component (section 7.3.5 ).

##### Normal Setup Operation

During normal operation the flow of data is basically the one outlined
above. Data is published from a file, split up by the scatterer
components, and distributed evenly to the three worker nodes, which send
their data to the data sink node for data collection and merging into
one data stream. This data stream is then checked for lost data.

##### Node Failure Scenario

If during normal operation of the setup described above one of the three
worker nodes fails, the following sequence of events should take place.
Due to the node’s failure no more events arrive at the corresponding
TolerantPublisherBridgeHead on the data sink node. This causes the
timeout of the fault detection component attached to that
TolerantPublisherBridgeHead to expire after the specified interval,
resulting in a message being sent to one of the control programs. In
this supervisor the status of all configured fault detection programs is
now checked to determine which of the three data streams is broken. It
subsequently sends messages to the scatterer and gatherer components,
informing them about the broken link. Now the scatterer marks the output
publisher concerned as bad and checks for events that have been sent to
that publisher’s path and have not been received back. These events are
presumed to be lost and are distributed evenly to the remaining output
publishers. All new events arriving after this are also distributed to
the remaining publishers. The publisher associated with the broken path
does not receive any new events until further notice.

No special action is taken by the Gatherer component upon receiving the
notification other than marking the path concerned as broken. EventDone
messages for events received from that path are now processed by just
marking the event as already done, as the gatherer expects the scatterer
to send these events again. Any new event received is first checked
against the backlog of EventDone messages that have already been
received as well as the list of events marked as done. An event is
entered internally into this last list when event done data is received
and it belongs to the broken path. If such an event is found, the
EventDone message is sent back immediately and the event is removed from
the internal tables. An event that cannot be found in these two tables
is presumed to be a new event and is handled as usual.

After notifying the scatterer and gatherer components about the failure
of the broken data stream, the first control program also informs the
second supervisor program of the failure. This program now sends
disconnect messages to the corresponding bridge head components on the
data source and sink nodes and checks whether a spare node is available.
If there is an available spare node it waits for the bridge heads on the
sink and source nodes to be properly disconnected and then sends connect
commands to them with the addresses of the corresponding bridge head
components on the spare node. After it detects that this new connection
has been properly established it sends a message to the scatterer and
gatherer components to reintegrate the broken path. From this point on
the system functions as before with the role of the broken node taken
over by the spare. As soon as the functioning node is available again it
can be reintegrated into the system as a new spare node.

##### Fault Tolerance Test Results

To test the fault-tolerance functionality of the system described above,
the test setup has been activated with communication between the
computers being done via Fast Ethernet. When the data flow chain had
been running for a time the network cable was unplugged from one of the
worker nodes. This caused the corresponding TolerantSubscriberBridgeHead
, that was trying to send from the data source to that node, to block in
the TCP code until the specified sending timeout expired. The
TolerantPublisherBridgeHead on the data sink did not block while trying
to send its accumulated event done messages back to the node. This was
presumably because the messages were small enough that they could be
placed in buffers of the kernel’s network code or the network interface
hardware.

After the timeout in the fault detector component for the broken node’s
data path expired, but before the TCP network timeout expired, the first
control program was informed of the failure. It subsequently notified
the gatherer and scatterer components on the data source and sink,
causing lost events to be resent along the remaining two nodes. At the
same time, the second control program was notified as well, which then
sent disconnect commands to the appropriate bridge head components on
the source and sink, and waited for them to become disconnected. Because
of mutex semaphores regulating access to the communication classes, the
disconnection only happened after the network send timeout expired. When
the bridge heads had disconnected from their partners on the ”broken”
node, commands were sent to them to clear all events from their internal
data structures and to reconnect them to the BridgeHeads on the spare
node. As soon as the second control program had determined that the new
connection was properly established, it sent commands to the scatterer
and gatherer to reactivate the broken path and send new events again to
all three data streams.

Fig. 8.59 shows results of measurements that were made on the different
nodes during the test. In the curves the amount of network traffic going
in or coming out of the corresponding nodes is displayed. The
measurements were made locally on each of the nodes. Note that the five
curves shown are scaled independently to arbitrary values for a better
visualization. Real values of the plateaus for the data source and sink
node are between 11 MB/s and 12 MB/s and the four other nodes’ (two
normal worker/one faulty worker/one spare worker) plateaus are at about
4 MB/s with the peak in the worker node’s curve going to about 6 MB/s.
This shows that the network load going out from the data source is
evenly distributed to the three active nodes at first and after recovery
as well as to the two remaining nodes during the recovery process.

At the points marked 1 the cable is unplugged from the faulty node,
causing its incoming network traffic to fall to zero immediately. At the
same time or shortly afterwards the network traffic going out of the
data source decreases to about two thirds as the
TolerantSubscriberBridgeHead sending to the unplugged node blocks. The
reason for the decrease to somewhat less than two thirds of the previous
might be due to buffers filling up on the source as they do not get
freed by the “faulty” node. At point 2 the faulty node has been taken
out of the path, and events are passed only to the two remaining worker
nodes. The amount of data leaving the data source increases to its
previous value, and the amount of data going into the worker node
increases by about a factor 1.5, as expected. Finally, at point 3 the
spare node has been connected into the chain and the third path has been
activated again. Data starts to go into the spare node at the same rate
as for the faulty node before the cable was unplugged, and the amount of
data going into the regular worker node decreases back to the value
before the simulated failure. At this point the data flow chain has
fully recovered. During the running time of the test, including some
time after the recovery, the event supervisor component has issued no
warning about missing events, so not a single event was lost due to this
simulated node failure.

##### Fault Tolerance Test Summary

The presented framework includes a number of components that make a data
flow chain tolerant against faults in components of the chain, even
against hardware faults of complete nodes. Failures that occur while
spare nodes are available cause no further impact except for a short
performance decrease until the spare node is activated. With no spares
to activate, the system will continue to run with at most a performance
decrease corresponding to the processing power loss due to the
failure(s). Even in the case of multiple failures no events will be
lost. If no output path is available the scatterer component stores
events in a list. As soon as a path becomes available again, all events
will be sent via this path. Neither of these cases results in the loss
of just one single event in the chain. Every event inserted at the
beginning arrives at the end of the chain.

#### 8.6.3 System Tests Summary

In the two system tests described above the framework has been
demonstrated to be operational and usable in its current form. It has
been shown to handle data rates, including processing, within a factor
of 2.4 of the highest requirements for operation in the ALICE High Level
Trigger. Furthermore, the fault tolerance test has proven that the
current concept to ensure fault tolerance works and can in principle
also be used in production systems already, despite its proof-of-concept
status.

## Chapter 9 Conclusion and Outlook

In this thesis a framework has been presented, that has been developed
for data flow oriented applications with a particular emphasis on its
use in trigger systems of high-energy and nuclear physics. Design and
implementation of the framework have been carried out for the data
transport software to be used in the High Level Trigger of the future
ALICE heavy-ion experiment. To allow flexible configurations the
framework is composed of distinct components, that communicate via a
defined interface and can be combined in various configurations.
Configuration changes are even possible during the runtime of a system.

A first conclusion to be drawn from the framework’s development and its
use in a number of setups is, that the composition into multiple
independent modules has been proven to be highly functional and
efficient. As requirements for specific tests have evolved the
modularity has enabled to add functionality in new as well as in
existing components and to provide proof-of-concept implementations and
prototypes of new characteristics quite fast and easy. Furthermore it
has allowed to vary the configurations of tests in a simple and rapid
way and therefore to change test setups and to introduce new ones
easily. In the two system tests described in section 8.6 it has been
demonstrated that the framework is able to operate in conditions closely
approaching the ones expected for the operation of the ALICE High Level
Trigger. In addition, the current fault tolerance capability also has
been shown to be able to handle failures of complete nodes in a running
system. The performance impact caused by such a failure is only
temporary provided that enough spare nodes are available, otherwise it
is at most proportional to the amount of processing power lost.

The separation of the network code into an individual communication
class library has turned out to be advantageous, too, since it has
allowed to implement and test the communication related functionality of
the framework without the need to decide upon a network technology at
the current stage. For the tests and developments the currently
widespread available and comparatively cheap Gigabit Ethernet TCP/IP
solution could be used, however, at the obvious processing overhead.
Once the decisions for a network technology and protocol have been made,
the appropriate classes have to be implemented only in the communication
class library used.

Concerning the performance the framework already meets the requirements
set by the conditions of the ALICE High Level Trigger in the existing
implementation and with the tested hardware. As the available CPU power
does not yet reach the projected level a correspondingly, and quite
considerably, larger number of nodes and CPUs would be required to
perform the necessary analysis steps of the HLT would it be built today.
However, in principle it could be realized at the moment and will be
able to operate at the start of the LHC and ALICE. With the potential
optimizations discussed in section 8.4.9 and 8.5.3 it should be possible
to further enhance the performance and particularly the efficiency of
the framework, reducing the CPU power required for the operation at a
given rate.

In addition to these performance improvements a number of further tasks
will be useful for a full working trigger system. The first of these is
a configuration program that provides a plain manner to graphically
connect the functional components for a system. The required framework
components should be automatically inserted by this configuration
program. A more pressing need exists for a process startup, control, and
supervision system that can monitor and control the components in a
framework configuration. It also has to react to changes in their state
by sending appropriate commands, effectively functioning as a Detector
Control System (DCS). For this task the framework components have to be
modified using the monitoring and control classes described in section
7.5.2 so that they can react as finite state machines (FSM), shifting
between states as a result of received commands or other external
stimuli. Supervising processes can monitor the states of a number of
components, e.g. all components on one node, and react to changes by
sending commands. A summary status can be derived from the supervised
components’ states and is reported to a further supervisor component
that controls multiple nodes. This component in turn can send commands
to its subordinate supervisors, which translate them into appropriate
commands for the actual framework components. Furthermore, for the use
in the ALICE HLT such a system requires an interface to the global ALICE
DCS, translating and forwarding its commands and providing it with
status information. A final item needed for the framework is a good
packaging and distribution mechanism that allows an easy installation of
the framework by users not involved in its development. With these
enhancements in place, the framework will provide a toolbox from which
cluster applications, in particular trigger related ones, can be
constructed easily.

## Appendix A Benchmark Supplement

### a.1 Microbenchmark Programs

#### a.1.1 Logging Overhead

    #include <sys/time.h>
    #include <stdio.h>

    #define COUNT 1000000000

    void test_function1( int* a )
        {
        (*a)++;
        }

    void test_function2( unsigned long flags, int* a )
        {
        if ( flags & 1 )
            (*a)++;
        }

    unsigned long long calc_tdiff( struct timeval* start, struct timeval* stop )
        {
        unsigned long long tmp;
        tmp = (stop->tv_sec - start->tv_sec);
        tmp *= 1000000;
        tmp += (stop->tv_usec - start->tv_usec);
        return tmp;
        }

    int main( int argc, char** argv )
        {
        struct timeval start, stop;
        unsigned long i;
        int n;
        int *p = &n;
        unsigned long flags = 1;
        unsigned long long loopoverhead;
        unsigned long long loop_if;
        unsigned long long loop_iffunc;
        unsigned long long loop_func;
        unsigned long long loop_funcif;

        gettimeofday( &start, NULL );
        for ( i = 0; i < COUNT; i++ )
            {
            (*p)++;
            }
        gettimeofday( &stop, NULL );
        loopoverhead = calc_tdiff( &start, &stop );
        printf( "Loop overhead:          %Lu us\n", loopoverhead );

        gettimeofday( &start, NULL );
        for ( i = 0; i < COUNT; i++ )
            {
            if ( flags & 1 )
                (*p)++;
            }
        gettimeofday( &stop, NULL );
        loop_if = calc_tdiff( &start, &stop );
        printf( "Loop with if:           %Lu us\n", loop_if );

        gettimeofday( &start, NULL );
        for ( i = 0; i < COUNT; i++ )
            {
            test_function1( &n );
            }
        gettimeofday( &stop, NULL );
        loop_func = calc_tdiff( &start, &stop );
        printf( "Loop with func:         %Lu us\n", loop_func );

        gettimeofday( &start, NULL );
        for ( i = 0; i < COUNT; i++ )
            {
            if ( flags & 1 )
                test_function1( &n );
            }
        gettimeofday( &stop, NULL );
        loop_iffunc = calc_tdiff( &start, &stop );
        printf( "Loop with if and func:  %Lu us\n", loop_iffunc );

        gettimeofday( &start, NULL );
        for ( i = 0; i < COUNT; i++ )
            {
            test_function2( flags, &n );
            }
        gettimeofday( &stop, NULL );
        loop_funcif = calc_tdiff( &start, &stop );
        printf( "Loop with func with if: %Lu us\n", loop_funcif );

        return 0;
        }

### a.2 Minimal Benchmark Process List

      PID TTY      STAT   TIME COMMAND
        1 ?        S      0:05 init [3]
        2 ?        SW     0:00 [keventd]
        3 ?        SWN    0:00 [ksoftirqd_CPU0]
        4 ?        SW     0:00 [kswapd]
        5 ?        SW     0:00 [bdflush]
        6 ?        SW     0:00 [kupdated]
       34 ?        SW     0:00 [kreiserfsd]
      229 ?        S      0:00 /sbin/syslogd
      233 ?        S      0:00 /sbin/klogd -c 1
      264 ?        SW     0:00 [khubd]
      508 tty1     S      0:00 login -- root
      509 tty2     S      0:00 /sbin/mingetty tty2
      510 tty3     S      0:00 /sbin/mingetty tty3
      511 tty4     S      0:00 /sbin/mingetty tty4
      512 tty5     S      0:00 /sbin/mingetty tty5
      513 tty6     S      0:00 /sbin/mingetty tty6
      965 tty1     S      0:00 -bash
     1259 tty1     R      0:00 ps x

## Appendix B Benchmark Result Tables

The following tables were generated automatically. Layout and number
formats may therefore not be optimal. Errors are given as standard
deviations where present.

### b.1 Micro-Benchmarks

#### b.1.1 Cache and Memory Reference Tests

### b.2 Network Reference Tests

#### b.2.1 TCP Network Reference Throughput

##### Plateau Determination

##### Plateau Throughput Measurement

##### Peak Throughput Measurement

#### b.2.2 TCP Network Reference Latency

### b.3 Communication Class Benchmarks

#### b.3.1 TCP Message Class Throughput

##### Plateau Determination

##### Plateau Throughput Measurement

##### Peak Throughput Measurement

#### b.3.2 TCP Message Class Latency

#### b.3.3 TCP Blob Class Throughput with On-Demand Allocation

##### Plateau Determination

##### Plateau Throughput Measurement

##### Peak Throughput Measurement

#### b.3.4 TCP Blob Class Throughput with Preallocation

##### Plateau Determination

##### Plateau Throughput Measurement

##### Peak Throughput Measurement

#### b.3.5 TCP Blob Class Latency with On-Demand Allocation

#### b.3.6 TCP Blob Class Latency with Preallocation

### b.4 Publisher-Subscriber Interface Benchmarks

#### b.4.1 Scaling Behaviour

## Appendix C Obsolete Framework Components

### c.1 ALICE DAQ Interface Components

For a previous software only version of an interface between the ALICE
High Level Trigger and the ALICE Data Acquisition system DATE for the
recording of events two data sink components have been created. Both
interfaces contain a subscriber object of a class derived from the
common AliHLTDATEBaseSubscriber parent class. This class contains
functionality to initialize a DATE interface library, pass events for
recording to DATE, and query already recorded events that can be
released.

#### c.1.1 The DATE Subscriber Base Class

Basic functionality for interfacing from the publisher-subscriber
interface framework to the DATE system is defined in the
AliHLTDATEBaseSubscriber class. It is derived from
AliHLTSubscriberInterface but defines none of the functions needed to
implement the interface, which has to be done by its own derived
classes, DirectDATESubscriber and TriggeredDATESubscriber . In the class
four primary and one auxiliary functions are provided for the interface
with the DAQ system. Fig. C.1 shows the relation of the classes, more
detailed explanations are contained in the following paragraphs.

The first of the primary functions is Run , which has the purpose of
initializing the library provided by DATE and starting the background
thread that runs the DATEEventsDone function described later. It also
interacts with the DATE run-control to inform it about the activation of
a recording program. Run ’s counterpart is the Stop method called at the
end of a program. It terminates the started background thread, informs
the run-control about the program’s end, and deinitializes the DATE
library. To pass an event to the DATE system for recording the third
function, SendEventToDATE , has to be called. Its main parameters are
the ID of the event concerned, its corresponding sub-event descriptor,
and a 32 bit unsigned integer containing additional event flags to pass
to DATE.

In the current version this function imposes one restriction on the
event data. The program is currently only able to create one type of
DATE events, called streamlined events, in which the event data has to
be prefixed directly by the DATE event header. As a consequence space
has to be available in front of the data in shared memory for the
appropriate amount of bytes so that an event header can be created
there. In addition to the event ID the header contains the number of the
GDC node where the event will be assembled as well as the additional
flags that have been passed in the function’s parameter. The GDC ID is
obtained by calling the class’s GetGDCID helper function. After calling
the DATE function to record the event described by the constructed
header, the function updates the run-control’s event and byte counters
appropriately.

Running as a continuous loop in a background thread the DATEEventsDone
function’s task is to query DATE via its library periodically for events
that have already been recorded. For these finished events the publisher
proxy’s EventDone function is called to allow the event’s original
publisher to release it. This polling is done in configurable intervals,
by default in half a seconds intervals. Next to the periodic check for
recorded events the loop also queries run-control status flags to
determine whether the program should continue running or whether it
should terminate.

GetGDCID is a helper function to encapsulate the determining of the GDC
ID to be used for an event. For tests the function currently implements
a round-robin scheme based on an event’s ID and the number of active
GDCs.

#### c.1.2 The Direct DATE Subscriber

In the DirectDATESubscriber component each event received by the
components’s subscriber object is directly passed to DATE for recording.
The NewEvent method implemented by the DirectDATESubscriber class
directly calls the SendEventToDATE method provided by its
AliHLTDATEBaseSubscriber parent class. Apart from setting up all
necessary objects there is not much more functionality contained in this
component.

#### c.1.3 The Triggered DATE Subscriber

Unlike the previous DirectDATESubscriber , the TriggeredDATESubscriber
component does not forward each event to DATE immediately. Instead it
uses an approach similar to the TriggeredFilter component from section
7.1.5 . A new received event is entered into a list, and only upon
receipt of event done data for it a decision is made which parts of the
event are to be passed to DATE. It is possible that an event is not
announced at all or only as an empty event with no data. Event done data
for an event is received from another subscriber via the publisher
component. The event trigger decision is made using an object of the
AliHLTTriggerEventFilter class, also described in 7.1.5 . In the
EventDoneData function implemented by TriggeredDATESubscriber the filter
object’s FilterEventDescriptor function is called to determine the data
blocks to record. Similar to the trigger filter component the triggered
DATE subscriber can also be configured to either forward untriggered
events as empty events with no data blocks or to simply release them
without invoking DATE.

## Appendix D Glossar

  ACM  

    Association for Computing Machinery

  ADC  

    Analog to Digital Converter

  ALICE  

    A Large Ion Collider Experiment — Future heavy-ion experiment at
    CERN’s LHC Collider

  API  

    Application Programmer’s Interface

  ATLAS  

    A Toroidal LHC ApparatuS — Future general purpose experiment at
    CERN’s LHC Collider

  ATOLL  

    ATOmically Low Latencies — A SAN for the PCI bus being developed at
    the University of Mannheim

  BAR  

    Base Address Register

  BCL  

    Basic Communication Library — C++ communication class library
    covered in this thesis

  BLOB  

    Binary Large OBject

  BNL  

    Brookhaven National Laboratory

  C  

    Procedural programming language well suited to system programming

  C++  

    Programming language based on C with object-oriented extensions

  CAMAC  

    Computer Automated Measurement and Control — Industry standard
    instrumentation bus

  CBM  

    Compressed-Baryonic-Matter — Planned experiment at the future HESR
    accelerator at GSI

  CERN  

    European Organisation for Nuclear Research in Geneva, Switzerland

  CMS  

    Compact Muon Solenoid — Future general purpose experiment at CERN’s
    LHC Collider

  COTS  

    Commodity-Off-The-Shelf

  COW  

    Cluster Of Workstations

  CRC  

    Cyclic Redundancy Check

  CSMA/CD  

    Carrier Sense Multiple Access with Collision Detection — Ethernet
    technology for regulating access to physical transmission medium

  CSR  

    Configuration Space Register

  DAQ  

    Data AcQuisition

  DARPA  

    Defense Advanced Research Projects Agency

  DATE  

    ALICE Data Acquisition and Test Environment

  DCS  

    Detector Control System

  DDL  

    Detector Data Link

 @xmath 

    Specific energy loss of charged particles per distance travelled

  DMA  

    Direct Memory Access

 @xmath 

    Distribution of particles per unit of pseudo-rapidity

  EDM  

    Event Destination Manager

  EG  

    Event Gatherer

  EM  

    Event Merger

  ES  

    Event Scatterer

  FE  

    Fast Ethernet or Front End

  FEE  

    Front End Electronics

  FEP  

    Front End Processor

  FIFO  

    First-In-First-Out

  FMD  

    Forward Multiplicity Detector — One of ALICE’s detectors

  FPGA  

    Field Programmable Gate Array

  FSM  

    Finite State Machine

  FT  

    Fault Tolerance

  gcc  

    GNU C Compiler or GNU Compiler Collection

  GDC  

    Global Data Concentrator

  GNU  

    GNU’s Not Unix — Project to provide a freely available version of a
    Unix like operating system

  GSI  

    Gesellschaft für Schwerionenforschung in Darmstadt, Germany

  GbE  

    Gigabit Ethernet

  HADES  

    High Acceptance Di-Electron Spectrometer — Detector at GSI

  HESR  

    High Energy Storage Ring — Future Accelerator at GSI

  HI  

    Heavy-Ion

  HL  

    Hierarchy Level

  HLT  

    High Level Trigger

  HMPID  

    High Momentum Particle IDentification — One of ALICE’s detectors

  HPC  

    High Performance Computing

  I/O  

    Input & Output

  IEEE  

    Institute of Electrical and Electronics Engineers, Inc.

  IETF  

    Internet Engineering Task Force

  IIS-A  

    Fraunhofer-Institut für Integrierte Schaltungen

  IP  

    Internet Protocol

  ISA  

    Industry Standard Architecture — PC extension bus

  ITS  

    Inner Tracking System — One of ALICE’s detectors

  k  

    As a prefix usually @xmath , however in this thesis when used as
    prefix for bits or bytes or when used for counts of multiples of 2
    (e.g. 32 or 128), means @xmath

  kB  

    @xmath Bytes

  L0  

    Level 0 Trigger

  L1  

    Level 1 Trigger

  L2  

    Level 2 Trigger

  L3  

    Level 3 Trigger

  LAM  

    Look At Me — An interrupt signal

  LAM/MPI  

    MPI implementation

  LDC  

    Local Data Concentrator

  LHC  

    Large Hadron Collider — Future accelerator at CERN

  LHCb  

    Large Hadron Collider beauty experiment — Future experiment
    dedicated to b-physics at CERN’s LHC Collider

  LSF  

    Load Sharing Facility

  M  

    As a prefix usually @xmath , however in this thesis when used as
    prefix for bits or bytes or when used for counts of multiples of 2
    (e.g. 32 or 128), means @xmath

  MB  

    @xmath Bytes

  MCP  

    Micro Channel Plate — A technology for particle detectors

  MLUC  

    More or Less Useful Class Library — C++ utility class library
    covered in this thesis

  MP3  

    MPEG Audio Layer 3 — Compressed audio file format

  Molière radius  

    Material characteristic used to describe the transversal dimension
    of electromagnetic particle showers

  MPEG  

    Motion Picture Experts Group

  MPI  

    Message Passing Interface – Standard for parallel program
    communication

  MPICH  

    MPI implementation

  MTU  

    Maximum Transmission Unit

  Mutex  

    Mutual Exclusion Semaphore

  MWPC  

    Multi Wire Proportional Chamber — A technology for particle
    detectors

  NOW  

    Network Of Workstations

  Ogg Vorbis  

    Compressed audio file format

  OO  

    Object Oriented

  OOP  

    Object Oriented Programming

  PANDA  

    Proton-ANtiproton-at-DArmstadt — Planned experiment at the future
    HESR accelerator at GSI

  PBH  

    Publisher Bridge Head

  PC133  

    Specification for SDRAM modules with 133 MHz clock frequency

  PCI  

    Peripheral Component Interconnect — PC extension bus

  PCISIG  

    Peripheral Component Interconnect Special Interest Group — PCI
    standardization body

  PDS  

    Permanent Data Storage

  PHOS  

    PHOton Spectrometer — One of ALICE’s detectors

  PID  

    Particle IDentification

  PIO  

    Programmed I/O

  PM  

    Patch Merger

  PMD  

    Photon Multiplicity Detector — One of ALICE’s detectors

  PPC  

    Parallel Plate Counters — A technology for particle detectors

  Pseudo-rapidity  

    Variable for particles in a collision. Defined as @xmath , where
    @xmath is the angle between the particle and the direction of the
    undeflected beam. Approximates the relativistic rapidity of a
    particle.

  PSI  

    PCI and Shared memory Interface — Driver and library for PCI
    hardware and shared memory access covered in this thesis

 @xmath 

    Transversal Momentum

  PVM  

    Parallel Virtual Machine — Library for parallel program
    communication

  QGP  

    Quark-Gluon Plasma

  RFC  

    Request For Comment — Informal Internet standard

  RHIC  

    Relativistic Heavy Ion Collider — Accelerator at BNL

  RICH  

    Ring Image Čerenkov (or Cherenkov) Counter — A technology for
    particle detectors

  RORC  

    Read Out and Receiver Card

  RPC  

    Resistive Plate Chamber — A technology for particle detectors (In
    computing also Remote Procedure Call , but not used as such in this
    thesis)

  SAN  

    System Area Network

  SBH  

    Subscriber Bridge Head

  SCI  

    Scalable Coherent Interface — SAN technology

  SDD  

    Silicon Drift Detector — A technology for particle detectors

  SI95  

    SpecInt95 — Unit to measure computing speed

  SISCI  

    Software Infrastructure for SCI — SCI programming API

  SM  

    Slice Merger

  SMP  

    Symmetric Multi-Processor system - A system with multiple processors
    (CPUs) accessing the same memory

  SPD  

    Silicon Pixel Detector — A technology for particle detectors

  SPS  

    Super Proton Synchrotron — Accelerator at CERN

  SSD  

    Silicon Strip Detector — A technology for particle detectors

  SSI  

    Single System Image

  SSIC  

    Single System Image Cluster

  STAR  

    Solenoidal Tracker at RHIC — Detector at the RHIC accelerator at BNL

  STL  

    Standard Template Library

  ShM  

    Shared Memory

  Si  

    Silicon

  TCP  

    Transmission Control Protocol

  TDR  

    Technical Design Report

  TOF  

    Time Of Flight — One of ALICE’s detectors

  TPC  

    Time Projection Chamber — A technology for particle detectors as
    well as one of ALICE’s detectors

  TRD  

    Transition Radiation Detector — A technology for particle detectors
    as well as one of ALICE’s detectors

  UML  

    Unified Modelling Language

  VITA  

    VMEbus International Trade Association

  VMEbus  

    VERSAmodule Eurocard extension bus — Industry standard
    instrumentation bus

  WAN  

    Wide Area Network

  ZDC  

    Zero Degree Calorimeter — One of ALICE’s detectors

  ZN  

    Zero degree Neutron calorimeters

  ZP  

    Zero degree Proton calorimeters