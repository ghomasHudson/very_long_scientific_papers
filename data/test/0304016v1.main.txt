##### Contents

-    1 Phenomenology of Strong Interactions
-    2 Classical Field Theories
-    3 Quantization
    -    3.1 Non-relativistic Quantum Mechanics
    -    3.2 The Axioms of Relativistic Quantum Field Theories
    -    3.3 The Path Integral
        -    3.3.1 Construction Principle
        -    3.3.2 Computing Observables
        -    3.3.3 Euclidean Field Theory
        -    3.3.4 Evaluation of Path Integrals
    -    3.4 Ensembles
-    4 Gauge Theories
-    5 Quantum Chromodynamics
    -    5.1 Running Coupling and Energy Scales
    -    5.2 Factorizable Processes
    -    5.3 Lattice QCD
-    6 Discretization
    -    6.1 Scalar Fields
    -    6.2 Gauge Fields
    -    6.3 D-Theory
        -    6.3.1 Globally Symmetric Models
        -    6.3.2 Models with Local Gauge Symmetries
        -    6.3.3 Simulation Algorithms
    -    6.4 Fermion Fields
        -    6.4.1 Even-Odd Preconditioning
    -    6.5 Yang-Mills Theory
-    7 Monte-Carlo Algorithms
    -    7.1 Markov Chains
-    8 Autocorrelation
    -    8.1 Autocorrelation Function
    -    8.2 Exponential Autocorrelation Time
    -    8.3 Integrated Autocorrelation Time
    -    8.4 Scaling Behavior
    -    8.5 Short Time Series
        -    8.5.1 Windowing Procedure
        -    8.5.2 Lag-Differencing Method
        -    8.5.3 Jackknife Method
-    9 Measuring Hadron Masses
-    10 Bosonic Sampling algorithms
    -    10.1 Metropolis Algorithm
        -    10.1.1 Local Metropolis Update
        -    10.1.2 Global Metropolis Update
    -    10.2 Heatbath Algorithm
        -    10.2.1 Heatbath for Gauge Fields
        -    10.2.2 Heatbath for Scalar Fields
    -    10.3 Overrelaxation
-    11 Fermionic Sampling Algorithms
    -    11.1 Sampling with the Wilson Matrix
    -    11.2 Hybrid Monte-Carlo Algorithm
    -    11.3 Multiboson Algorithms
        -    11.3.1 Even-Odd Preconditioning for MB Algorithms
        -    11.3.2 Exact Multiboson Algorithms
        -    11.3.3 Non-Hermitian Variant
        -    11.3.4 TSMB Variant
-    12 Matrix Inversion Algorithms
    -    12.1 Static Polynomial Inversion
        -    12.1.1 Quadratically Optimized Polynomials
        -    12.1.2 Ultra-Violet Filtering
    -    12.2 Conjugate-Gradient Iteration
    -    12.3 GMRES Algorithm
    -    12.4 Stabilized Bi-Conjugate Gradient Algorithm
-    13 Eigenvalue Algorithms
-    14 Optimizing the Polynomial Approximation
    -    14.1 Tuning the Quadratically Optimized Polynomials
        -    14.1.1 Measures of Accuracy
        -    14.1.2 Fixing the Lower Limit
    -    14.2 Algorithm for Polynomials
    -    14.3 Computing the Reweighting Correction
-    15 Tuning the Dynamical Parameters
    -    15.1 Practical Determination of Autocorrelations
        -    15.1.1 Case I: Low Fluctuations
        -    15.1.2 Case II: Large Fluctuations
    -    15.2 Acceptance Rates vs. Polynomial Approximation Quality
        -    15.2.1 Behavior of the Correction Factor
        -    15.2.2 Fermionic Energy
        -    15.2.3 Gauge Field Plaquette
    -    15.3 Dynamical Reweighting Factor
    -    15.4 Updating Strategy
        -    15.4.1 Boson Field Updates
        -    15.4.2 Gauge Field Updates
        -    15.4.3 Choice of Updating Sequence
-    16 Implementation Systems
    -    16.1 APE Platform
    -    16.2 ALiCE Cluster
    -    16.3 Accuracy Considerations and Test Suites
        -    16.3.1 Local Fermionic Action
        -    16.3.2 The Inverse Square-root
    -    16.4 Architectures and Efficiency
-    17 Summary
-    18 Simulation Runs at Different Parameters
-    19 Efficiency of Multiboson Algorithms
    -    19.1 Tuning the MB-HB Algorithm
    -    19.2 Tuning the MB-OR Algorithm
    -    19.3 Direct Algorithmic Comparison
    -    19.4 Scaling Behavior of Algorithms
-    20 Summary
-    21 The Non-Zero Temperature Crossover
-    22 The Chiral Limit
-    23 Explorative Studies
    -    23.1 The Case @xmath
    -    23.2 The Case @xmath
        -    23.2.1 Locating the Shielding Transition
        -    23.2.2 Computing @xmath
        -    23.2.3 Locating the Critical Point
        -    23.2.4 Prospects for Future Simulations
-    24 Summary and Outlook
-    A Dirac Matrices
-    B Groups and Representations
-    C The U @xmath Group
-    D The SU @xmath Groups
    -    D.1 The SU @xmath Group
    -    D.2 The SU @xmath Group
-    E The Poincaré Group
-    F Spin-Statistics Theorem
-    G Grassmann Algebras
    -    G.1 Definitions
    -    G.2 Derivatives
    -    G.3 Integration
-    H General Expressions
-    I Local Forms of Various Actions
    -    I.1 Pure Gauge Fields
    -    I.2 Lattice Fermion Fields
        -    I.2.1 Wilson Fermions (Hermitian)
        -    I.2.2 Wilson Fermions (non-Hermitian)
-    J Design of the Database

###### List of Figures

-    1 Fundamental representations of the SU @xmath flavor group. The
    left graph shows the quark triplet ( @xmath , @xmath , @xmath ) and
    the right graph shows the anti-quark triplet ( @xmath , @xmath ,
    @xmath ).
-    2 Pseudoscalar meson octet together with the singlet (the @xmath
    state) as classified by the parameters of the SU @xmath group.
-    3 Vector meson octet together with the singlet (the @xmath state)
    as classified by the parameters of the SU @xmath group.
-    4 Baryon octet as classified by the parameters of the SU @xmath
    group.
-    5 Relations of different axiomatic frameworks for quantum field
    theory.
-    6 Relations between the different kinds of @xmath -point functions
    consisting of vacuum expectation values of products of field
    operators. The figure has been taken from [ 28 ] .
-    7 Different methods for obtaining prediction in QCD.
-    8 Spectrum of the hopping matrix, @xmath in Eq. (), in the limit
    @xmath .
-    9 Spectrum of the hopping matrix, @xmath in Eq. (), in the limit
    @xmath .
-    10 Histogram of the @xmath smallest eigenvalues of @xmath .
-    11 Histogram of the @xmath largest eigenvalues of @xmath .
-    12 Test function @xmath for a quadratically optimized polynomial.
-    13 Norms @xmath and @xmath vs. the lower interval limit @xmath .
-    14 Norms @xmath and @xmath as defined in Eq. ( 205 ) vs. the lower
    interval limit @xmath .
-    15 Residual norms for @xmath vs. the lower interval limit @xmath .
-    16 Polynomial @xmath which has been obtained by applying the GMRES
    algorithm to a thermalized gauge field configuration together with
    the corresponding quadratically optimized polynomial.
-    17 Residual vector norm for both the GMRES and the quadratically
    optimized polynomials.
-    18 Individual correction factors as computed using the @xmath
    lowest eigenvalues of @xmath for the quadratically optimized
    polynomial @xmath .
-    19 Similar to Fig. 18 , but with the cumulative correction factor
    from the @xmath lowest eigenvalues.
-    20 Plaquette history of HMC run.
-    21 Normalized autocorrelation function and integrated
    autocorrelation time vs. the cutoff for the HMC plaquette history.
-    22 Variance @xmath vs. the bin size @xmath using the Jackknife
    method for the HMC plaquette history.
-    23 Order- @xmath -lag- @xmath differenced series of the HMC
    plaquette history.
-    24 Correlation function @xmath together with its integral vs. the
    cutoff.
-    25 Integrated autocorrelation time as obtained from the
    lag-differencing method for varying lags @xmath .
-    26 Similar to Fig. 24 , but with a differencing lag @xmath .
-    27 Time series of the average plaquette from the TSMB algorithm.
-    28 Autocorrelation function and the corresponding integral as a
    function of the cutoff for the plaquette history from the TSMB run.
-    29 Jackknife variance calculated for different bin sizes for the
    time series from the TSMB run.
-    30 Integrated autocorrelation time vs. the differencing lag for the
    TSMB run.
-    31 Dependence of the exponential correction factor on the number of
    boson fields, @xmath .
-    32 Dependence of the standard deviation of the exponential
    correction factor on the number of boson fields, @xmath .
-    33 Integrated autocorrelation time @xmath vs. the number of boson
    fields @xmath .
-    34 Integrated autocorrelation times of the plaquette together with
    their standard errors as a function of the differencing lag for
    various @xmath .
-    35 Jackknife variances as a function of the bin size for various
    values of @xmath .
-    36 Integrated autocorrelation times of the plaquette vs. @xmath .
-    37 Polynomials @xmath for the three different values of the lower
    limit.
-    38 Reweighting factors for the three values of @xmath .
-    39 Accumulated eigenvalue histograms for the @xmath lowest
    eigenvalues of @xmath for the three runs.
-    40 Boson field thermalization using local boson field updating
    algorithms.
-    41 Integrated autocorrelation times of the plaquette together with
    their standard errors as a function of the differencing lag for the
    different update sequenced.
-    42 Jackknife variances as a function of the bin size for the
    different updating sequences.
-    43 Maximum numerical error between the different implementations of
    the fermionic actions for local changes of the gauge field.
-    44 Maximum numerical error between the implemented actions for
    local changes of the boson field.
-    45 Residual error of the noisy correction @xmath vs. the number of
    iterations @xmath for two lattice sizes. Both single ( @xmath bit)
    and double precision ( @xmath bit) have been used.
-    46 Autocorrelation function and the corresponding integral as a
    function of the cutoff for the plaquette history from the HMC run.
    This figure is identical to Fig. 21 .
-    47 Autocorrelation function and integrated autocorrelation time for
    the plaquette histories of the MB-HB algorithm.
-    48 Autocorrelation function and integrated autocorrelation time for
    the plaquette histories of the MB-OR algorithm.
-    49 Autocorrelation function and integrated autocorrelation time for
    the plaquette histories of the HMC algorithm.
-    50 Autocorrelation function and integrated autocorrelation time for
    the plaquette histories of the MB-OR algorithm.
-    51 Sketch of the FSE-induced shielding transition. The squared
    pseudoscalar meson mass, @xmath , is plotted vs. the inverse hopping
    parameter @xmath .
-    52 Average plaquettes for runs with three dynamical flavors at
    @xmath .
-    53 Inverse condition number of @xmath vs. @xmath for three
    dynamical fermions at @xmath .
-    54 Plaquette history of the run at @xmath and @xmath with @xmath .
-    55 History of reweighting factors for the run at @xmath and @xmath
    with @xmath .
-    56 Average plaquettes for runs with three dynamical flavors at
    @xmath .
-    57 Polyakov loops along the shortest length, @xmath , for the
    simulation of three dynamical fermion flavors at @xmath .
-    58 Symmetrized correlation function for the pseudoscalar meson with
    three dynamical fermions at @xmath and @xmath .
-    59 Symmetrized correlation function for the vector meson with three
    dynamical fermions at @xmath and @xmath .
-    60 Jackknife variances for different bin sizes for the mass of the
    pseudoscalar meson with three dynamical fermions at @xmath and
    @xmath . The mass is obtained from a fit to an interval @xmath .
-    61 Jackknife variances for different bin sizes for the mass of the
    vector meson with three dynamical fermions at @xmath and @xmath .
    The mass is obtained from a fit to an interval @xmath .
-    62 Meson masses in lattice units as a function of the fitting
    interval for three dynamical fermions at @xmath .
-    63 Inverse condition number of @xmath vs. @xmath for three
    dynamical fermions at @xmath .
-    64 Square of the pion mass, @xmath , (black circles) and the rho
    mass, @xmath , (black squares) for @xmath with @xmath .
-    65 Entity-relation diagram for the configuration database.

###### List of Tables

-    1 List of selected hadrons with their quantum numbers and their
    masses in MeV.
-    2 Different flavors of quarks together with their associated
    quantum numbers.
-    3 Bare and physical parameters for most runs presented.
-    4 Extremal eigenvalues and the condition number of @xmath .
-    5 General algorithmic parameters for high-statistics TSMB runs.
-    6 Runs for the parameter tuning of the TSMB algorithm.
-    7 Comparison of acceptance rates as a function of @xmath from the
    predictions of Eq. ( 209 ) and from the actuals runs in Tab. 6 .
-    8 Integrated autocorrelation times of the plaquette together with
    their standard errors as read off from Fig. 34 .
-    9 Physical parameters for the investigation of the gauge field
    updating sequence.
-    10 Algorithmic parameters for the investigation of the gauge field
    updating sequence.
-    11 Exponential correction with standard deviation and the
    corresponding acceptance rates as a function of the number of gauge
    field Metropolis sweeps.
-    12 Polynomial and lattice volume for runs with different updating
    sequences.
-    13 Different updating sequences used for a single trajectory.
-    14 Integrated plaquette autocorrelation times from the
    lag-differencing and the Jackknife methods together with the total
    costs for a statistically independent configuration.
-    15 Execution times of several parts of the TSMB algorithm on the
    ALiCE cluster and the CRAY T3E.
-    16 Execution times of the parts of the TSMB algorithm on an APE
    board.
-    17 Bare and physical parameters for the comparison between HMC and
    TSMB. Also cf. Sec. 3 .
-    18 Machines and statistics for the test runs at different physical
    parameters.
-    19 Average plaquette values with their standard errors obtained
    from the different samples.
-    20 Different polynomial parameters and the resulting acceptance
    rates for the MB-HB algorithm.
-    21 Updating sequence for the MB-HB algorithm.
-    22 General algorithmic parameters for MB-OR run.
-    23 Average plaquette and hadronic masses for the three different
    sampling algorithms for Lattice QCD used.
-    24 Numerical efforts for meson masses obtained by employing three
    different sampling algorithms. Courtesy B. Orth .
-    25 Autocorrelation times and efforts for independent plaquette
    measurements for the three different algorithms at @xmath and @xmath
    .
-    26 Statistics and average plaquette for the HMC and the MB-OR
    algorithms at @xmath and @xmath .
-    27 Autocorrelation times and efforts for independent plaquette
    measurements for the two algorithms at @xmath and @xmath .
-    28 Hopping parameter, @xmath , number of trajectories, and
    plaquette values with resulting autocorrelation times for the runs
    with @xmath and @xmath .
-    29 Algorithmic parameters for the runs with three dynamical quark
    flavors at @xmath .
-    30 Average extremal eigenvalues and condition numbers for runs with
    three dynamical flavors at @xmath .
-    31 Algorithmic parameters for each configuration for the runs at
    @xmath with @xmath .
-    32 Simulation runs using three dynamical fermion flavors at @xmath
    .
-    33 Jackknife variances together with the corresponding variances
    for bin size @xmath for the simulation run at @xmath and @xmath .
    From there, an estimate for the integrated autocorrelation time is
    obtained.
-    34 Masses and their autocorrelation times for the determination of
    the ratio @xmath for three dynamical fermions at @xmath .
-    35 Average extremal eigenvalues and condition numbers for runs with
    three dynamical flavors at @xmath .
-    36 The suggested working point for future spectroscopic studies on
    lattices with @xmath and beyond.
-    37 Most important semi-simple groups together with their dimension
    and rank. The table is taken from [ 245 ] .
-    38 All four kinds of homogeneous Poincaré-transformations
    compatible with ().
-    39 Attributes of the Configurations entity.
-    40 Attributes of the Polynomials entity.
-    41 Attributes of the Ensembles entity.
-    42 Attributes of the Machines entity.

## Chapter \thechapter Introduction

  The works of the LORD are great,

  sought out by all them

  that have pleasure in them.

  Psalm 111, Verse 2.

Throughout history one of the fundamental driving forces of man has been
the desire to understand nature. In the past few centuries, natural
sciences have paved the way to several revolutionary insights to the
structures underlying our world. Some of them founded fundamental
benefits for the quality of life and the advance of civilization. In
particular, in the recent decades, computer technology has set the base
for a major leap of several important facets of human existence.

A major challenge is posed by fundamental research, which does not
directly aim towards developing industrial applications, but instead
examines structures and relations relevant for future technologies. The
goal of fundamental research is to formulate theories which comprise as
many different phenomena as possible and which are, at the same time, as
simple as they can be.

The branch of natural sciences which concentrates on the structures
underlying matter and energy is termed particle physics. This field
lives on contributions from experiments, providing insights of how the
particles which constitute our world interact. A further driving force
behind particle physics is the desire to find a simple description of
the mechanisms underlying these experiments. The field of particle
physics has benefitted from the evolution in computer science, but on
the other hand theoretical physicists have triggered many pivotal
developments in technology we have today.

Current physical theories categorize the interactions between matter and
energy into four different types of fundamental forces. These forces are
gravitation, electromagnetism, and finally the weak and the strong
interactions. The strong interaction is responsible for the forces
acting between hadrons, i.e. between neutrons, protons and nuclei built
up from these particles. Its name originates from the fact that it is
the strongest among the other forces on the energy scale of hadronic
interactions. The strengths of the four forces can be stated in terms of
their coupling constants [ 1 ] :

  -- -------- -------- -------- -- -----
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (1)
  -- -------- -------- -------- -- -----

It is apparent, that gravitation is many orders of magnitude weaker than
the other forces and thus is expected to play no role on the energy
scales important for particle physics so far [ 2 ] .

In contrast to electromagnetic interactions and gravity, strong and weak
forces do not have infinite ranges. They exhibit finite ranges up to at
most the size of a nucleus. Thus, they only play a role in nuclear
interactions, but are almost completely negligible on the level of atoms
and molecules. Any particular process can be dissected into a multitude
of processes acting at a smaller scale. Hence, a process with a given
action @xmath may be described by a set of subprocesses @xmath with
smaller actions @xmath . We can tell from observations that processes
are essentially deterministic if the action of the process in question
@xmath is far larger than some action @xmath which is known as “Planck’s
constant” [ 3 ] . However, if the action of the process is of the order
of @xmath , then the system cannot be described by a deterministic
theory any longer and a non-deterministic theory known as quantum
mechanics must be employed. Today, all interactions can be described
within a quantum-mechanical framework, with the exception of gravity,
which has so far not been successfully formulated as a consistent
quantum theory. For a discussion of these topics the reader may consult
[ 4 ] and references therein.

A necessary requirement for the above iteration to make sense is that it
must be possible to recover a classical (non-probabilistic) theory in a
certain limit (naturally the @xmath limit) from a quantum theory. This
limit is given by the WKB approximation [ 5 ] . One finds that an
expansion exists, where the amplitude of a process can be formulated as
a power-series in @xmath :

  -- -------- -- -----
     @xmath      (2)
  -- -------- -- -----

In the limit @xmath the classical amplitude is recovered. But Eq. ( 2 )
also shows that the quantum theory contains more information than the
classical theory: Different choices for the series coefficients @xmath
obviously lead to the same classical theory. Thus, if one intends to
construct a quantum theory starting from the classical theory, there is
always an ambiguity of how to proceed [ 5 ] . The correct prescription
can only be found by experimental means.

There exist several quantization prescriptions to build a quantum
theory. All of these have in common that they describe a system which
exhibits a probabilistic interpretation of physical observables and a
deterministic evolution equation of some underlying degrees of freedom.
The quantum theory which is believed to constitute the correct quantum
theory of the strong interaction is called quantum chromodynamics (QCD).

In general, a quantum mechanical system can not be solved exactly, but
only by use of certain approximations. The approximation most commonly
employed is known as perturbation theory and is applicable in a large
variety of cases. It is known to fail, however, when applied to the
low-energy regime of QCD, where a diversity of interesting phenomena
occurs. Hence, different techniques commonly called “non-perturbative”
methods must be employed. One of these non-perturbative methods is the
simulation of the system in a large-scale numerical calculation, an
approach which is referred to as “Lattice QCD”. This approach is in the
focus of this thesis.

When performing numerical simulations within Lattice QCD, one finds that
the simulation of the bosonic constituents of QCD, the “gluons”, stand
at the basis of research efforts, see [ 6 ] for a pioneering
publication.

The inclusion of dynamical fermions poses a serious problem. Although it
has become clear in [ 7 , 8 ] that without dynamical fermions the
low-energy hadron spectrum is reproduced with @xmath accuracy, several
important aspects of low-energy QCD require the inclusion of dynamical
fermions. One case where the inclusion of dynamical fermions is
phenomenologically vital is given by the mass of the @xmath meson, cf. [
9 ] .

The numerical simulation of dynamical fermions is plagued by severe
difficulties. In particular, the requirement that the fermions must be
light is to be met, since only in this case the chiral behavior of QCD,
i.e. the behavior at light fermion masses, is reproduced correctly. But
in this particular limit, the algorithms suffer from a phenomenon known
as critical slowing down, i.e. a polynomial decrease in efficiency as
the chiral point is approached.

Furthermore, the majority of algorithms in use today can only treat two
mass-degenerate dynamical fermion flavors, a situation not present in
strong interactions as observed in nature. In fact, one has to use three
dynamical fermion flavors [ 10 ] .

Thus, the demands on an algorithm for the simulation of dynamical
fermion flavors must consist of (i) the suitability for the simulation
of three dynamical fermion flavors, and (ii) the efficiency of the
algorithm with regard to critical slowing down. These two requirements
are not met by the commonly used algorithm in Lattice QCD, the hybrid
Monte-Carlo (HMC) algorithm.

The topic of this thesis is the exploration of a new type of algorithm,
known as the multiboson algorithm. This algorithm is expected to be
superior to the HMC algorithm with regard to the above properties. In
particular, it will be examined how to tune and optimize this class of
algorithms and if these algorithms are suitable for the simulation of
three light, dynamical, and mass-degenerate fermion flavors.

The thesis is organized as follows: the theoretical background of the
strong interaction, the quantization of field theories, and the
definition of lattice gauge theories is given in chapter Advanced
Algorithms for the Simulation of Gauge Theories with Dynamical Fermionic
Degrees of Freedom . The tools required to perform numerical simulations
in lattice theories and the analysis of time series are formulated in
chapter Advanced Algorithms for the Simulation of Gauge Theories with
Dynamical Fermionic Degrees of Freedom . The optimization and tuning of
the algorithm is discussed in chapter Advanced Algorithms for the
Simulation of Gauge Theories with Dynamical Fermionic Degrees of Freedom
.

A direct comparison of the multiboson algorithm with the hybrid
Monte-Carlo method is performed in chapter Advanced Algorithms for the
Simulation of Gauge Theories with Dynamical Fermionic Degrees of Freedom
. In particular, the scaling of the algorithms with the quark mass has
been focused at.

A particularly useful application of the multiboson algorithm appears to
be the simulation of QCD with three dynamical fermion flavors. Such a
simulation allows to assess the suitability of multiboson algorithms for
future simulations aimed at obtaining physically relevant results. A
first, explorative investigation of the parameter space which might be
relevant for future simulations is presented in chapter Advanced
Algorithms for the Simulation of Gauge Theories with Dynamical Fermionic
Degrees of Freedom .

Finally the conclusions are summarized in chapter Advanced Algorithms
for the Simulation of Gauge Theories with Dynamical Fermionic Degrees of
Freedom .

Appendix Advanced Algorithms for the Simulation of Gauge Theories with
Dynamical Fermionic Degrees of Freedom contains a short overview of the
notation used in this thesis. An introduction to group theory and the
corresponding algebras is given in App. Advanced Algorithms for the
Simulation of Gauge Theories with Dynamical Fermionic Degrees of Freedom
. The explicit expressions used for the local actions required for the
implementation of multiboson algorithms are listed in App. Advanced
Algorithms for the Simulation of Gauge Theories with Dynamical Fermionic
Degrees of Freedom . At last, App. Advanced Algorithms for the
Simulation of Gauge Theories with Dynamical Fermionic Degrees of Freedom
explains the concepts required for running large production runs, where
a huge amount of data is typically generated.

I have to thank many colleagues and friends who have accompanied me
during the completion of this thesis and my scientific work. I am
indebted to my parents, Astrid Börger, Claus Gebert, Ivan Hip, Boris
Postler, and Zbygniew Sroczynski for the time they invested to
proof-read my thesis. For the interesting scientific collaborations and
many useful discussions I express my gratitude to Guido Arnold, Sabrina
Casanova, Massimo D’Elia, Norbert Eicker, Federico Farchioni, Philippe
de Forcrand, Christoph Gattringer, Rainer Jacob, Peter Kroll, Thomas
Moschny, Hartmut Neff, Boris Orth, Pavel Pobylitza, Nicos Stefanis, and
in particular to István Montvay, Thomas Lippert, and Klaus Schilling.

## Chapter \thechapter Quantum Field Theories and Hadronic Physics

This chapter provides a general introduction into the topic of particle
physics. It covers both the phenomenological aspects, the mathematical
structures commonly used to describe these systems, and the particular
methods to obtain results from the basic principles.

Section 1 gives a general overview of the phenomenology of the strong
interaction without making direct reference to a particular model.

A short overview of classical (i.e. non-quantum) field theories is given
in Sec. 2 . With this basis, the general principles of constructing a
quantum field theory starting from a classical field theory are
presented in Sec. 3 . The case of non-relativistic theories is covered
in Sec. 3.1 , while the generalization to relativistic quantum field
theories requires far more effort. This is described in Sec. 3.2 , where
the basic axiomatic frameworks of relativistic quantum field theories
are stated.

Particular emphasis will be placed on the path integral quantization
which allows for a rigorous and efficient construction of a quantum
theory. Section 3.3 provides a detailed treatise of this method and also
contains a discussion of how one can perform computations in practice.
An important tool for the evaluation of path integrals is the concept of
ensembles, which is introduced in Sec. 3.4 . It will turn out to be
essential in numerical simulations of quantum field theories.

Section 4 introduces an important class of quantum field theories,
namely the class of gauge theories. These models will be of central
importance in the following.

With all necessary tools prepared, Sec. 5 will introduce a gauge theory
which is expected to be able to describe the whole phenomenology of the
strong interaction. This theory is known as quantum chromodynamics (QCD)
and it is the main scope of this thesis. After a general introduction to
the properties of QCD in Sec. 5.1 , a method known as factorization is
discussed in Sec. 5.2 . This method allows to combine information from
the different energy scales and thus provides an essential tool for
actual predictions in QCD calculations. Finally, the method of Lattice
QCD is discussed in Sec. 5.3 . Lattice simulations exploit numerical
integration schemes to gain information about the structure of QCD, and
represent the major tool for the purposes of this thesis.

The construction of a quantum field theory based on path integrals
requires a certain discretization scheme. This scheme is particularly
important in lattice simulations. Therefore, Sec. 6 covers the common
discretizations for the different types of fields one encounters in
quantum field theories. The case of scalar fields allows for a simple
and efficient construction as will be shown in Sec. 6.1 . The case of
gauge fields is more involved since there exist several proposals how
this implementation should be done. Contemporary simulations focus
mainly on the Wilson discretization, although recently a new and
probably superior method has been proposed. This method, known as
D-theory, is reviewed in Sec. 6.3 .

The discretization of fermion fields is even more involved. The
necessary conditions such a scheme has to fulfill are given in Sec. 6.4
and the scheme used in this thesis, namely the Wilson-fermion scheme, is
constructed. In contrast to the cases of scalar and gauge fields, a
large number of different fermion discretization schemes are used in
actual simulations today, and each has its particular advantages and
disadvantages.

This chapter is concluded by the application of the previously discussed
discretization schemes to a gauge theory containing both fermions and
gauge fields in Sec. 6.5 . Such a model is expected to be the lattice
version of gauge theories with fermions, and in particular of QCD.

### 1 Phenomenology of Strong Interactions

Until 1932 only the electron @xmath , the photon @xmath and the proton
@xmath have been known as elementary particles (for overviews of the
history of particles physics see [ 11 , 12 , 13 ] ). The only strong
process known was the @xmath -decay of a nucleus. The milestones in this
period were the detection of the neutron by Chadwick in 1932 and the
prediction of the @xmath -meson (today it is customary to call it simply
“pion”) by Yukawa in 1935 as the mediator of the strong force. However,
it took until 1948 before the charged pion was actually detected by
Lattes . In 1947 particles carrying a new type of quantum number called
“strangeness” have been detected by Rochester . In 1950 the neutral pion
was detected by Carlson and Bjorkland . It was soon realized that the
hadrons were not point-like objects like the leptons, but had an
internal structure and accordingly a finite spatial extent.

The experiments to observe the structure of hadrons usually consist of
scattering two incoming particles off each other, producing several
outgoing particles of possibly different type. If one considers a
particular subset of processes where all outgoing particles are of a
determined type, one speaks of exclusive reactions . A sub-class of
exclusive reactions are the elastic scattering processes, where the
incoming and outgoing particles are identical.

The inclusive reactions are obtained by summing over all possible
exclusive reactions for given incoming particles. Inclusive
electron-nucleon scattering at very large energies is called
deep-inelastic scattering (DIS) and played an important role in the
understanding of the structure of hadrons. The prediction of scaling by
Bjorken in 1969 was confirmed experimentally and led to the insight that
the hadrons consist of point-like sub-particles. In 1968 Feynman
proposed a model which exhibited this feature, the parton model .

A collection of hadrons, as known in the early 60’s, is given in Tab. 1
together with their properties in the form of quantum numbers. These
quantum numbers are known as spin, parity, electric charge @xmath ,
baryon number @xmath , and strangeness @xmath . They are conserved by
the strong interaction ¹ ¹ 1 Note, however, that the weak interaction
violates both the baryon number and the strangeness. While the latter
phenomenon has been observed in experiment so far [ 11 ] , the former
violation may never be observed directly in earth-bound experiments [ 14
] .

The particles may be divided into several groups according to their spin
and parity: the particles with even spin are called mesons and the
particles with odd spin baryons . Because of their parity and spin, the
particles @xmath , @xmath , @xmath , @xmath , @xmath , @xmath and @xmath
are usually called “pseudoscalar mesons”. Similarly, the particles
@xmath , @xmath , @xmath , @xmath , @xmath , @xmath and @xmath are named
“vector mesons”. The group @xmath , @xmath , @xmath , @xmath , @xmath ,
@xmath and @xmath is simply called “baryons”. In each group, the members
have roughly similar masses (with the exception of the @xmath in the
group of the pseudoscalar mesons).

In 1963 Gell-Mann and Zweig independently proposed a scheme to classify
the known particles as multiplets of the Lie group SU @xmath . It turned
out that the classification is indeed possible with the exception that
there were no particles corresponding to the fundamental triplets of the
group. This would imply that the corresponding particles carry
fractional charges; particles with such a property have never been seen
in any experiment. However, experiments at SLAC in 1971 involving
neutrino-nucleon scattering clearly indicated that the data could be
accounted for if the parton inside the nucleon had the properties of the
particles in the fundamental triplet of the SU @xmath group. This led
finally to the identification of the (charged) partons from Feynman’s
model with the particles from the classification scheme of Gell-Mann and
Zweig . These particles are known today as quarks .

Today a huge number of particles subject to the strong interaction is
known from different kinds of experiments. For a complete overview see [
1 ] . To classify them the quark model had to be extended to six kinds
of quarks. They are referred to as having “different flavors”, which are
new quantum numbers. Thus, they are conserved by the strong interaction.
The quarks cover a huge range of masses. Their properties are listed in
Tab. 2 .

To classify the multiplets of the flavor group SU @xmath , two numbers
are required which are usually called @xmath (the strong hyper-charge)
and @xmath (the isospin). They are defined by

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

The three light quarks together with the corresponding anti-particles
are shown in Fig. 1 . The classifications for the pseudoscalar mesons,
the vector mesons and the baryons are given in Figs. 2 , 3 , and 4 .

The multiplets containing the particles are irreducible representations
of the SU @xmath group; they may be built from tensor products of the
fundamental triplet in the following way:

  -- -------- -------- -------- -- -----
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (3)
  -- -------- -------- -------- -- -----

This explains why there are always nine particles in each of the meson
groups: the first eight belong to an octet and the remaining one is the
singlet state. In the group of the pseudoscalar mesons, the singlet
state @xmath deserves special attention because its mass is extremely
heavy.

The particle content to lowest order of the pseudoscalar mesons (Fig. 2
) in terms of the different quark flavors is given by

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

Similarly the lowest-order particle content of the vector mesons (Fig. 3
) is given by

  -- -------- -------- -------- -- -----
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (5)
  -- -------- -------- -------- -- -----

A very important step was the discovery of the “color” degree of
freedom. A first indication towards this feature was the observation
that the @xmath baryon is a particle with flavor content of three @xmath
-type quarks in the ground state and spin pointing in the same
direction. Without an additional quantum number this would imply that
all quarks building up these particles are in the same quantum state
which is not possible with Fermi-Dirac particles since their
wave-functions should anti-commute. Consequently, a further quantum
number should exist which indeed has been found in experiments [ 11 ] .
This quantum number has been termed color and can take on three values.
If this property is described again in terms of an SU @xmath group, then
the quarks must transform as the representation of the fundamental
multiplet. The hadrons, however, are color singlets since they display
no color charge. This and the observation that the quarks have never
been seen as free particles outside of hadrons led to the hypothesis of
confinement which will be examined more closely in Sec. 5.1 . According
to this hypothesis, free quarks could never be observed in nature
directly since the force between them grows infinitely.

### 2 Classical Field Theories

There are quite a few frameworks for the description of a classical ² ²
2 The meaning of the word “classical” in the title of this section and
in the context of this paragraph is referring to any system described by
a finite or infinite number of degrees of freedom with deterministic
dynamics regardless of the symmetries of the underlying space-time or
the system itself. It should be noted, that the term “classical” is
perhaps the word with the largest variety of meanings in the literature
of physics — it is used for non-relativistic, non-quantum systems, in a
different context for relativistic quantum field theories and also for
anything in between. physical system. For a general review of such
frameworks, the reader may consult [ 15 ] , for the generalization to
field theories [ 16 ] . For the later generalization to quantum
mechanical systems, the Lagrangian method will attract our attention.

This approach has the advantage that it may be formulated in a
coordinate-invariant manner, i.e. it does not depend on any specific
geometry of the physical system. The basic postulate underlying the
dynamics of this formulation is (see e.g. [ 17 ] for textbook overview):

  Principle of extremal action:  

    A set of classical fields @xmath , @xmath , is described by a local
    @xmath function @xmath which is called the Lagrangian density. The
    integral @xmath over a region in Minkowski-space @xmath is defined
    as the action of the system:

      -- -------- --
         @xmath   
      -- -------- --

    The equations of motion follow from the requirement that the action
    becomes minimal. A necessary condition is

      -- -------- --
         @xmath   
      -- -------- --

From this postulate, we obtain a necessary condition which the functions
@xmath have to fulfill in order to describe the motion of the physical
system

  -- -------- -- -----
     @xmath      (6)
  -- -------- -- -----

This set of equations is called the Euler-Lagrange equations . Using
@xmath , we can define the momentum conjugate @xmath of @xmath via

  -- -------- --
     @xmath   
  -- -------- --

Then the Hamiltonian @xmath is defined by a Legendre-transformation

  -- -------- --
     @xmath   
  -- -------- --

From Eq. ( 6 ) the canonical equations of motion follow:

  -- -------- -------- -------- -- -----
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (7)
  -- -------- -------- -------- -- -----

Similarly to the situation in classical mechanics, the Lagrangian
framework and the Hamiltonian framework provide the bases of two
different formulations for the quantum mechanical description of a
system.

Since the particles of a quantum theory should transform as
representations of multiplets of certain symmetry groups, the
representations of the Poincaré group (see App. E ) will require special
attention. The lowest representations are given by

1.  the singlet representation, described by a scalar field @xmath . The
    field transforms as

      -- -------- --
         @xmath   
      -- -------- --

2.  the doublet representation, described by a Weyl spinor field @xmath
    , transforming as the fundamental representation under @xmath . In
    this thesis we deal with Dirac spinors composed of two Weyl spinor
    fields

      -- -------- --
         @xmath   
      -- -------- --

3.  the vector representation, described by a four-vector field @xmath ,
    transforming as

      -- -------- --
         @xmath   
      -- -------- --

The Lagrangians describing the corresponding particles should obey
Lorentz- and @xmath -invariance and possibly transform according to an
internal symmetry group. As an excellent introduction see [ 18 ] . So
far these requirements fix at least the free field Lagrangians. Examples
for Lagrangians obeying these principles are listed in the following:

1.  The Lagrangian for a complex scalar field @xmath is given by

      -- -------- -- -----
         @xmath      (8)
      -- -------- -- -----

    where the free field (i.e. the Lagrangian describing a field
    propagating without an external force) is given by @xmath .

2.  The spin-statistics theorem (see App. F ) suggests that a quantum
    mechanical spin- @xmath particle should be described by
    anticommuting field variables. Thus, the fields should be Grassmann
    variables (cf. Appendix G ). The free field Lagrangian is given by

      -- -------- -- -----
         @xmath      (9)
      -- -------- -- -----

    A generalization is the free @xmath -component Yang-Mills field
    described by an @xmath -component vector @xmath of independent
    fields @xmath . Its Lagrangian is the sum of the single-field
    Lagrangians and thus given by

      -- -------- -- ------
         @xmath      (10)
      -- -------- -- ------

3.  The vector field @xmath with a field strength @xmath is described in
    the non-interacting case by

      -- -------- -- ------
         @xmath      (11)
      -- -------- -- ------

    The generalization to a vector field with several components will be
    discussed in Sec. 4 .

### 3 Quantization

As it has been pointed out in Chapter Advanced Algorithms for the
Simulation of Gauge Theories with Dynamical Fermionic Degrees of Freedom
, the world of elementary particle physics is essentially of quantum
mechanical nature. However, any theory of particle physics should also
obey the principle of Lorentz invariance. Therefore the need arises to
find a quantum mechanical model which is at least globally invariant
under the Poincaré-group. This is hard to do within the framework of
quantum mechanics for pointlike-particles. In fact, the single-particle
interpretation of the relativistic Dirac equation is subjected to
several paradoxes (see e.g. [ 19 ] ) which can only be resolved if one
considers instead fields (or rather generalized concepts called
operator-valued distributions ) [ 18 ] .

#### 3.1 Non-relativistic Quantum Mechanics

Before embarking on the definition of a relativistic quantum field
theory, we should recall the concepts of a non-relativistic quantum
theory. In general, a quantum theory has the following general structure
[ 20 , 21 ] :

  Hilbert space @xmath :  

    The discussion will now be limited to pure states , which are given
    by unit rays of a complex Hilbert space @xmath with scalar product
    @xmath .

  Observables:  

    An operator @xmath on @xmath is called an observable if it is a
    self-adjoint operator on @xmath . Thus, its eigenvalues are real.
    Observables correspond to quantities which can be measured in an
    experiment. If the system is in a state @xmath , the expectation
    value @xmath of the observable @xmath is given by

      -- -------- --
         @xmath   
      -- -------- --

  Symmetries:  

    The symmetries of the system are represented by unitary (or
    anti-unitary) operators on @xmath .

  Evolution:  

    The evolution equation of the states, the Schrödinger equation , is
    given by

      -- -------- -- ------
         @xmath      (12)
      -- -------- -- ------

    The Hamiltonian operator @xmath is a Hermitian operator acting in
    @xmath .

The state space @xmath may be finite, infinite or even uncountably
infinite. In the case that it is finite, the solution of Eq. ( 12 ) is
well-defined and can be found by diagonalizing the operator @xmath .

But already in the case of an infinite but countable state space, there
may be physically equivalent observables which are not unitarily
equivalent [ 19 , 22 ] . However, we will see below that information
about the quantum field theory can be extracted even without knowing the
complete state space. In fact, only few cases are known, where the state
space has been constructed in a mathematically rigorous manner. So far,
this does not include any interacting quantum field theory in four
space-time dimensions.

#### 3.2 The Axioms of Relativistic Quantum Field Theories

The mathematically rigorous formulation of relativistic quantum field
theories requires the introduction of operator-like objects replacing
the classical fields; however, it turns out to be impossible to use
operator-valued functions to define @xmath since the relativistic
quantum field is too singular at short distances [ 21 ] . Rather, a
quantum field must be defined as an operator-valued distribution @xmath
; the only objects with physical meaning are thus given by the smeared
fields @xmath , where @xmath is a smooth test function in the Schwartz
space @xmath and

  -- -------- -- ------
     @xmath      (13)
  -- -------- -- ------

The fact that the @xmath -operators only get a meaning in conjunction
with the functions @xmath already displays the need to regularize a
quantum field theory — a requirement that must be implemented by all
methods striving to compute observables. With this notation, a
relativistic quantum field theory can be postulated using the
Gårding-Wightman axioms [ 21 ] (where the discussion is now limited to
the case of a single scalar field in four dimensions):

  States:  

    The states of the system are the unit rays of a separable Hilbert
    space @xmath . There is a distinguished state @xmath , called the
    vacuum .

  Fields:  

    There exists a dense subspace @xmath , and for each test function
    @xmath in @xmath there exists an operator @xmath with domain @xmath
    , such that:

    1.  The map @xmath is a tempered distribution @xmath .

    2.  For all @xmath , the operator @xmath is Hermitian.

    3.  The vacuum @xmath belongs to @xmath .

    4.  @xmath leaves @xmath invariant: given an arbitrary @xmath
        implies that @xmath .

    5.  The set @xmath of finite linear combinations of vectors of the
        form @xmath with @xmath and @xmath is dense in @xmath .

  Relativistic covariance:  

    There is a continuous unitary representation @xmath of the proper
    orthochroneous Poincaré group @xmath such that

    1.  @xmath implies @xmath .

    2.  @xmath .

    3.  @xmath .

  Spectral condition:  

    The joint spectrum of the infinitesimal generators of the
    translation subgroup @xmath is contained in the forward light cone

      -- -------- --
         @xmath   
      -- -------- --

  Locality:  

    If @xmath and @xmath have spacelike-separated supports, then @xmath
    and @xmath commute:

      -- -------- --
         @xmath   
      -- -------- --

The quantities of major interest are the vacuum expectation values of
products of field operators @xmath . These objects are called Wightman
distributions :

  -- -------- -- ------
     @xmath      (14)
  -- -------- -- ------

It can be shown [ 23 ] using the so-called “reconstruction theorem” that
all information of the quantum theory can be obtained from these vacuum
expectation values. Essentially it allows to construct the state space
as well as the field operators. Since the @xmath are numerical-valued
quantities, they are much easier to work with than the operator-valued
fields @xmath . This allows for a simpler treatment of the problem.
Assuming that some smearing functions @xmath , @xmath , peaked around
the points @xmath have been chosen, one can speak of Wightman functions
and introduce the more convenient notation

  -- -------- -- ------
     @xmath      (15)
  -- -------- -- ------

A very powerful observation is the non-trivial fact that the Wightman
functions may be analytically continued from the Minkowski space @xmath
to Euclidean space @xmath . This can be done by applying the following
transformation of a four-vector @xmath in Minkowski-space

  -- -------- -- ------
     @xmath      (16)
  -- -------- -- ------

The Minkowski-metric @xmath is changed to @xmath . This transformation
is also known as the Wick rotation . This allows for the definition of
the Schwinger functions ³ ³ 3 Again one should not forget that the
objects under consideration are in fact distributions.

  -- -------- -- ------
     @xmath      (17)
  -- -------- -- ------

As discussed in [ 24 ] this analytic continuation is possible in the
whole complex plane, i.e. the Schwinger distributions exist if all
@xmath are distinct. It can be shown using the Gårding-Wightman axioms [
21 ] that the @xmath have the following properties:

  Reflection positivity:  

    Let @xmath denote reflection on the real axis. The @xmath satisfy
    the condition

      -- -------- --
         @xmath   
      -- -------- --

  Euclidean invariance:  

    The @xmath are invariant under all Euclidean transformations @xmath
    :

      -- -------- --
         @xmath   
      -- -------- --

  Positive definiteness:  

    Define the composition of test functions @xmath , @xmath , by (with
    @xmath )

      -- -------- --
         @xmath   
      -- -------- --

    The @xmath then obey the following condition:

      -- -------- --
         @xmath   
      -- -------- --

  Permutation symmetry:  

    The @xmath are symmetric in their arguments.

Now there is another important theorem due to Osterwalder and Schrader :
given the Schwinger distributions ( 17 ) satisfying the above
conditions, one can reconstruct the whole quantum field theory in
Minkowski space [ 25 , 26 , 27 ] . So the axioms due to Gårding-Wightman
and Osterwalder-Schrader are equivalent and one can use the
Osterwalder-Schrader framework to actually define a relativistic quantum
field theory.

Consequently, it is sufficient to compute the Schwinger functions for a
Euclidean quantum field theory and then reconstruct the Minkowski theory
from them. As it will be discussed later on, the Schwinger functions are
easier to handle than the Wightman distributions. However, this has to
be taken with a grain of salt: physical observables calculated in the
Euclidean theory must afterwards be analytically continued back to
Minkowski space to allow for a comparison with experiments. After all,
the physical quantities are defined in the Minkowski theory and not in
the Euclidean domain. In some cases (e.g. for the correlation lengths of
the two-point function which is the inverse of the particle mass), the
results are identical, i.e. the inverse Wick-rotation does not change
the value obtained in the calculation. However, there exist a lot of
cases, where the analytic continuation is non-trivial. For details the
reader is encouraged to consult [ 24 ] .

The relations between the different axiomatic settings discussed so far
are given in Fig. 5 . From the Wightman distributions, the whole QFT can
be constructed. However, the Osterwalder-Schrader axioms are an
equivalent formulation. The Schwinger distributions and the Wightman
distributions are related by analytic continuation.

The permutation symmetry of the Schwinger functions allows for the
construction of a generating functional. In contrast, the Wightman
functions are only symmetric for spacelike-separated arguments. Thus,
they can not be computed in terms of a generating functional. Another
elegant way to define generating functions in Minkowski space is the
introduction of Feynman functions which can be defined as “time-ordered”
products of field operators:

  -- -- -- ------
           (18)
  -- -- -- ------

where the time-ordering is defined as the product with factors arranged
so that the one with the last time-argument is placed leftmost, the
next-latest next to the leftmost etc. [ 18 ] . There is also an
alternative definition in [ 28 ] . It is given by applying a Fourier
transform to the Schwinger function and performing an analytic
continuation back to Minkowski space afterwards. This is displayed in
Fig. 6 .

By construction, the Feynman functions are also symmetric for
timelike-separated arguments and thus they are symmetric for arbitrary
arguments.

Hence, both the Schwinger and the Feynman functions allow for the
construction of a generating functional @xmath and @xmath . Only the
Schwinger functions will be considered here — formally the generating
functional (sometimes it is also called the vacuum-vacuum functional) is
given by

  -- -------- -------- -------- -------- ------
     @xmath   @xmath   @xmath            (19)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- ------

where the functions @xmath are taken from the Schwartz space @xmath .
Using @xmath , the Schwinger functions can be recovered by a functional
derivative

  -- -------- -- ------
     @xmath      (20)
  -- -------- -- ------

Knowledge of @xmath is thus equivalent to solving the quantum field
theory.

#### 3.3 The Path Integral

Having now discussed what a quantum field theory is , one needs a recipe
of how to construct it. In fact, there exist several prescriptions of
how to build a quantum theory if the Lagrangian of the classical field
theory to which the quantum theory should reduce is known. The two most
commonly used quantization schemes are the canonical quantization scheme
(which is described in detail in standard textbooks like [ 5 , 20 , 22 ,
18 ] ) and the path-integral quantization which will be used in this
thesis. Both of these schemes break the general covariance of the
classical theory discussed in Sec. 2 . The quantum theory still stays
invariant under global Lorentz transformations (and there even exist
generalizations to curved, but fixed spacetimes, see e.g. [ 29 ] and
references therein), but the quantization prescriptions implicitly
assume the existence of a global, canonical basis. However, local
Lorentz invariance is only of importance for a quantum theory of
gravity, so all to be said in the following can be applied to any
quantum theory of strong interactions discussed in Sec. 1 .

##### 3.3.1 Construction Principle

Before attempting to define a prescription for a quantum field theory,
let us go back to the case of non-relativistic quantum mechanics. The
notion of a path integral is closely related to the notion of a random
walk. To make this relation obvious, consider the expectation value
@xmath of the evolution operator applied to a single particle in one
dimension between two states @xmath :

  -- -------- -- ------
     @xmath      (21)
  -- -------- -- ------

@xmath is the probability amplitude for the particle to move from
position @xmath to position @xmath in time @xmath . If the Hamiltonian
corresponds to a free particle,

  -- -------- --
     @xmath   
  -- -------- --

then the solution to ( 21 ) can be given immediately [ 24 ] :

  -- -------- -- ------
     @xmath      (22)
  -- -------- -- ------

On the other hand, the probability for a one-dimensional random walk to
go from position @xmath to position @xmath in time @xmath is given by [
28 ] :

  -- -------- -- ------
     @xmath      (23)
  -- -------- -- ------

with @xmath being the diffusion constant. The quantum mechanical
expectation value is obtained by analytic continuation of Eq. ( 23 ) to
imaginary time and the identification @xmath . Thus, the quantum
mechanical amplitude may be computed by considering a classical random
walk and analytically continuing the result to imaginary time. If one
adopts this interpretation, the amplitude can be computed via a
path-integral using a conditional Wiener measure , see [ 28 ] for a
rigorous mathematical treatment.

To extend Eq. ( 22 ) also to the case of non-Gaussian Hamiltonians, we
decompose the full Hamiltonian @xmath into a Gaussian and a non-Gaussian
part:

  -- -------- -- ------
     @xmath      (24)
  -- -------- -- ------

and perform a time-slicing procedure. Consider the evolution operator
@xmath for small imaginary times @xmath . In the leading order, @xmath
coincides with the operator @xmath which is defined in the following
way:

  -- -------- -- ------
     @xmath      (25)
  -- -------- -- ------

The operator @xmath is known as the transfer matrix ; its matrix
elements can be computed to yield:

  -- -------- -- ------
     @xmath      (26)
  -- -------- -- ------

Using the Lie-Trotter formula, one gets:

  -- -------- -- ------
     @xmath      (27)
  -- -------- -- ------

Inserting @xmath times the identity @xmath into Eq. ( 27 ) finally
yields the expression:

  -- -------- -------- -------- -------- ------
     @xmath   @xmath   @xmath            (28)
                                @xmath   
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
  -- -------- -------- -------- -------- ------

In the continuum limit this equation can now be interpreted as a path
integral over a set of random walks with the weight function given in
the exponential. If we denote all paths @xmath with fixed end-points
from @xmath to @xmath , then we can write (using the Wiener measure
@xmath ):

  -- -------- -------- -------- -------- ------
     @xmath   @xmath   @xmath            (29)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- ------

where

  -- -------- --
     @xmath   
  -- -------- --

Now there is an important difference between the forms of Eqs. ( 28 )
and ( 29 ): In the latter, the exponential weight which connects
neighbor points of the paths is already a part of the measure @xmath ,
while in the former the exponential weight is contained in the
expression for @xmath . What is then the interpretation of this weight
factor? From Eq. ( 2 ) we know, that any amplitude can be expanded in a
power series of @xmath with the amplitude for the classical process
being the leading amplitude. Thus, in the limit @xmath , only the
classical (leading) contribution should contribute to the expression (
28 ). The exponential weight factor should thus be peaked around the
classical solution, i.e. the exponential factor will become minimal for
the classical trajectory, just like minimizing the action @xmath yields
the classical path. Thus, the exponential weight factor in the limit
@xmath should coincide with the classical action if one inserts a
differentiable trajectory. However, there is an important difference:
the classical action @xmath is only defined for differentiable paths,
while the exponential factor @xmath in ( 28 ) is defined for any
continuous path (which is a superset of the set of all differential
paths). This gives rise to a certain freedom in the choice of @xmath .
The actual choice should thus be guided by the desire to simplify the
problem at hand. Especially in the case of chiral fermions, a wide class
of possible actions has been proposed, see Sec. 6.4 .

Symbolically we can thus introduce a functional @xmath which projects
any continuous path to a real number and write

  -- -------- -- ------
     @xmath      (30)
  -- -------- -- ------

where the integral measure is given by

  -- -------- -- ------
     @xmath      (31)
  -- -------- -- ------

The resulting expression Eq. ( 30 ) can be analytically continued back
to imaginary times using @xmath which yields the desired transition
amplitude Eq. ( 22 ). Such a Wick-rotated form of Eq. ( 30 ) is known as
the Feynman-Kac formula. Sometimes the derivation is directly carried
out in Minkowski space, but the problem is that the integrand is highly
oscillatory and not well-defined, for further reading cf. [ 30 ] .

As already pointed out, the difference between ( 29 ) and ( 30 ) lies in
the interpretation of the measure. One can perform substitutions @xmath
to ( 31 ), giving rise to different integral measures. Since the path
integral has close resemblance to a system of statistical mechanics (via
its affinity to the random walk), we will classify the different classes
of paths which can be used in ( 30 ) by the means of ensembles . This
topic is discussed in detail in Sec. 3.4 . For the time being, we want
to interpret ( 28 ) as an integral over random paths with a weight given
by the entire exponential. This amounts to choosing the measure

  -- -------- --
     @xmath   
  -- -------- --

Later in Sec. 3.4 it will be argued that these paths are taken from the
random ensemble . In contrast, in the expression ( 29 ) using the Wiener
measure @xmath , the paths are taken from the canonical ensemble . This
integral measure already contains the kinetic term, but not the
potential term. In this way, the problem of assigning a meaning to the
derivative from the classical action is circumvented. This procedure is
not possible in the case for quantum field theories which will be
discussed below since in that case there is no such thing as a Wiener
measure.

##### 3.3.2 Computing Observables

As discussed in Sec. 3.2 , one is interested in ground-state expectation
values of certain operators, @xmath . Consider a (countable) Hilbert
space @xmath with Hamiltonian @xmath . Let @xmath , @xmath , be the
eigenvalues and @xmath be the corresponding eigenvectors of @xmath in
ascending order. Taking the trace of the evolution operator and @xmath
provides us with

  -- -------- --
     @xmath   
  -- -------- --

In the limit @xmath only the term with @xmath in the exponential
survives and we are left with

  -- -------- -- ------
     @xmath      (32)
  -- -------- -- ------

with the partition function

  -- -------- -- ------
     @xmath      (33)
  -- -------- -- ------

For the application to field theory, the operators @xmath will require
special attention, since they are analogous to the Schwinger functions
encountered in Euclidean quantum field theories in Sec. 3.2 ). Using
@xmath , we consider the @xmath -point correlation function @xmath ,
with @xmath . It is straightforward [ 24 ] to show that

  -- -------- -------- -------- -------- ------
     @xmath   @xmath   @xmath            (34)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- ------

where the paths obey periodic boundary conditions,

  -- -------- --
     @xmath   
  -- -------- --

and the partition function @xmath can be written as

  -- -------- -- ------
     @xmath      (35)
  -- -------- -- ------

Hence, the @xmath -point correlation function @xmath is written in ( 34
) as the moment of the measure @xmath . There is another possibility to
obtain the correlation function from a generating functional @xmath with
@xmath being a continuous path by means of the following definition:

  -- -------- -------- -------- -------- ------
     @xmath   @xmath   @xmath            (36)
                                @xmath   
  -- -------- -------- -------- -------- ------

Using

  -- -- -- -------- -------- ------
           @xmath            (37)
                    @xmath   
           @xmath   @xmath   
                    @xmath   
  -- -- -- -------- -------- ------

one recovers Eq. ( 34 ). The meaning of the derivatives can be
understood by considering again the subset of differentiable paths. The
expression then reduces to the functional derivative . Thus, @xmath can
be considered to be the generating functional for the @xmath -point
correlation functions and we can write symbolically:

  -- -------- -- ------
     @xmath      (38)
  -- -------- -- ------

Here the notation of the conventional functional derivative has been
employed, but with a meaning corresponding to Eq. ( 37 . This expression
is similar to the generating functional of the Schwinger functions ( 20
), implying that generalizing @xmath to the case of Euclidean fields is
the key to find a quantization prescription for quantum field theories.

##### 3.3.3 Euclidean Field Theory

The generalization of Eq. ( 38 ) to the case of Euclidean fields is very
difficult, however. As a starting point one can expect that the
expectation values for the Schwinger function in ( 17 ) can also be
written using a path integral just like the @xmath -point functions in (
34 ). They would then be moments of some suitably defined measure

  -- -------- -- ------
     @xmath      (39)
  -- -------- -- ------

The Schwinger functions @xmath can hence be written

  -- -------- -- ------
     @xmath      (40)
  -- -------- -- ------

where the generating functional @xmath is the field theory analogue of
Eq. ( 35 ). It can symbolically be written as

  -- -------- -- ------
     @xmath      (41)
  -- -------- -- ------

The functional @xmath appearing in Eq. ( 39 ) is again a suitable
generalization of the Euclidean action to a superset of continuous, but
non-differentiable fields. The vacuum expectation value @xmath of a
general operator, @xmath , is then defined by the path integral

  -- -------- -- ------
     @xmath      (42)
  -- -------- -- ------

where the partition function @xmath is given by ( 41 ). However, this
definition encounters severe difficulties because of the fact that the
@xmath are not pointwise-defined objects.

By inverting the logic which led to the path-integral formula Eq. ( 29
), one can define a prescription to formulate a quantum field theory
starting from a classical action @xmath . This procedure which gives
meaning to Eq. ( 39 ) is called renormalization theory and consists of
the following steps [ 21 ] :

1.  Regularize the theory by imposing an ultraviolet cutoff @xmath
    (where @xmath is a distance short compared to the intrinsic scales
    of the theory) so that ( 39 ) is a well-defined measure. This can
    e.g. be done by discretizing the Euclidean space @xmath to describe
    the system using a (finite) lattice in @xmath such that all @xmath .
    Find a functional @xmath with parameters @xmath on the lattice which
    reduces to the classical action @xmath for differentiable continuum
    fields. This prescription is not unique. In any case, however,
    either Euclidean invariance or Osterwalder-Schrader positivity or
    both are broken. Let @xmath be the @xmath -point functions of the
    discrete theory.

2.  Perform the infinite volume limit @xmath for the system with @xmath
    held fixed. This limit must exist and be unique.

3.  Allow the parameters @xmath of @xmath to be functions of @xmath :
    @xmath . The parameters occurring in ( 39 ) are then called the bare
    parameters .

4.  Perform the continuum limit @xmath . A continuum quantum field
    theory is obtained from the sequence of lattice theories by
    rescaling the lengths by a factor @xmath and rescaling the fields by
    a factor @xmath :

      -- -------- -- ------
         @xmath      (43)
      -- -------- -- ------

    For each choice @xmath check the convergence properties of @xmath
    and if they satisfy the Osterwalder-Schrader axioms.

5.  Consider all possible choices of @xmath and @xmath ; classify all
    limiting theories @xmath and study their properties.

This procedure may give rise to continuum theories which can be
categorized as follows:

  No limit:  

    For at least one @xmath , the limit ( 43 ) does not exist.

  Unimportant limit:  

    All resulting @xmath exist, but are devoid of information (like
    @xmath etc.)

  Gaussian limit:  

    The limiting theory @xmath is Gaussian, i.e. a generalized free
    field. This situation is commonly referred to as triviality .

  Non-Gaussian limit:  

    The limiting theory is non-Gaussian giving rise to a nontrivial
    theory. This may, however, still imply that the scattering matrix is
    the identity.

For a non-trivial limit to exist, the lattice theories should have
correlation lengths @xmath as @xmath (otherwise the physical lengths
would get rescaled to @xmath ). Thus, the parameters @xmath should
approach or sit on the critical surface and the theory must undergo a
phase transition of second order where the correlation lengths diverge.
This is expected to be the case for most interesting quantum field
theories whose critical behavior can be handled using the
renormalization group of Wilson , see [ 31 ] for the historical paper
and [ 24 , 32 ] for standard textbooks. There is also a second very
interesting case where @xmath for all @xmath , i.e. the parameters
@xmath already sit on the critical surface for finite lattice spacings.
This is e.g. the case in non-compact U @xmath pure gauge theories. For
compact U @xmath the situation is less clear so far, consult for a
description of simulation results the work of Arnold [ 33 ] and
references therein..

Despite the huge phenomenological successes of quantum field theories in
practice, a rigorous proof that the resulting theory exists in the sense
defined above, has been stated so far only for a few special cases. In
four dimensions, so far only free fields have been proven with
mathematical rigor to give rise to a relativistic quantum field theory.

##### 3.3.4 Evaluation of Path Integrals

Having now a definition for the path integral, we also need a way to
evaluate it. In principle there are two different ways to compute
expressions of the form ( 30 ) and ( 42 ):

-   Consider a series of weight factors @xmath , @xmath , which
    converges to the desired weight factor @xmath . The path integrals (
    30 ) should be computable for each @xmath .

-   Compute an approximation to ( 30 ) for finite @xmath in the measure
    ( 31 ). The resulting approximation will depend on @xmath . Then
    perform the limit @xmath .

As has already been mentioned, taking the limit in ( 43 ) is only
possible in some simple models, or in the case that the resulting
integrals have Gaussian shape. One way to also extend the applicability
to non-Gaussian models is thus to approximate the “true” function @xmath
by integrable Gaussian models which reduce to the @xmath in some
suitable limit.

The most popular form to do this is to expand the exponential into a
Gaussian part and a small, non-Gaussian part:

  -- -------- -- ------
     @xmath      (44)
  -- -------- -- ------

The idea is then to form the path-integral of the r.h.s. of Eq. ( 44 )
and take the result to be the sum of all contributions. The problem
behind the series obtained this way is that in several cases the sum
fails to converge. This is the case of the common four-dimensional
models, as has first been noted by Dyson in [ 34 ] .

As an example consider the “field theory” at a single site with
“partition function” [ 35 ]

  -- -------- -- ------
     @xmath      (45)
  -- -------- -- ------

The function @xmath contains an essential singularity at the origin.
Performing the perturbative expansion ( 44 ) yields

  -- -------- --
     @xmath   
  -- -------- --

which has a convergence radius of @xmath . Performing a semi-classical
expansion around the saddle point @xmath and integrating over the
quadratic deviations yields

  -- -------- --
     @xmath   
  -- -------- --

Obviously the @xmath are divergent (and the divergence is in fact
logarithmic), but the power series is at least asymptotic in the complex
@xmath plane cut along the negative real axis since

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

This means that for fixed @xmath the right hand side can be made
arbitrarily small by choosing @xmath small enough. It may even be
possible to recover the full partition function @xmath from the series
expansion @xmath using resummation. For recent reviews of the
application of resummation techniques consult [ 36 , 37 ] .

Despite these conceptional difficulties, perturbation theory turns out
to be the most effective approach to treat many problems in quantum
field theory provided the expansion parameter is sufficiently small.
However, in several situations of interest, the latter condition is not
fulfilled and the perturbative expansion is not even asymptotic, or the
expansion parameter is too large, causing it to diverge already in the
lowest orders. In these situations, one has to resort to different ways
to approximate the Schwinger functions. One possibility is a numerical
simulation of Euclidean QFT on the finite, discrete lattice @xmath . It
has some very intriguing advantages: it does not resort to any
assumptions of the dynamics of the model one is examining other than the
information underlying the regularized action and it is directly based
on the definition of the quantities under consideration. In essence, any
operator @xmath corresponding to a physical observable can be written
via ( 42 ) as the corresponding moment of a measure @xmath on the
underlying space. The ensemble of field configurations @xmath is
distributed according to the partition function ( 41 ). Consequently,
the latter is the quantity which one tries to access in numerical
simulations.

However, this approach has the shortcoming that the actual continuum
limit can never be performed and at best one has to resort to
extrapolation techniques giving rise to further uncertainties. Since the
actual shape of the Schwinger functions is not recovered, an analytic
continuation to Minkowski-space is not possible either and objects like
distribution amplitudes are not directly accessible. Nonetheless it is
possible to compute integrals over these functions and their moments,
which help to shed light on their behavior. This approach has been used
in e.g. [ 38 , 39 , 40 , 41 , 42 , 43 ] to extract information about
form factors and structure functions from lattice simulations.

One important question is if the theory is renormalizable if one uses a
perturbative expansion. There are models which are renormalizable
non-perturbatively, but are non-renormalizable when employing a
perturbative expansion. This is the case for the Gross-Neveau model at
large @xmath in three dimensions [ 21 ] .

However, due to the great importance of perturbative methods, the models
which are perturbatively renormalizable are considered in most practical
applications. This means that one has to choose Lagrangians with mass
dimension @xmath [ 35 ] , where the mass dimension for the scalars
@xmath , Dirac spinors @xmath and vector fields @xmath and their
derivatives are given by:

  -- -------- -- ------
     @xmath      
     @xmath      
     @xmath      (46)
  -- -------- -- ------

The mass dimension of a composite term in the Lagrangian is given by
adding the mass dimensions of its factors. A dimension-four term then
corresponds to a renormalizable interaction, less than four is
super-renormalizable and greater than four is non-renormalizable.

#### 3.4 Ensembles

Following its definition, Eq. ( 42 ), the quantum mechanical vacuum
expectation value @xmath of some functional @xmath of the fundamental
fields in the theory, @xmath , can be written as the moment of the
measure ( 39 ). As discussed in Sec. 3.3.4 , the analytic treatment of
equation ( 42 ) is only possible in case the path integral has the shape
of a Gaussian or in some toy models. If one does not want to recourse to
expansion techniques or simplifying assumption at this stage, the only
alternative method known today is the numerical treatment of ( 42 ).
However, a straightforward integration does not appear to be feasible,
since the dimensionality of the integral in simulations as they are run
today is easily exceeding @xmath [ 44 ] . The only alternative is
therefore a Monte-Carlo integration . To define possible techniques for
treating this problem, the concept of ensembles of configurations has
turned out to be extremely useful [ 24 ] :

  Ensembles:  

    An ensemble @xmath consists of an infinite number of field
    configurations @xmath with a density @xmath defined on the measure
    @xmath .

A simple example is the micro-canonical ensemble, which is defined by

  -- -------- -- ------
     @xmath      (47)
  -- -------- -- ------

with a constant @xmath . Thus, this ensemble only consists of
configurations with a fixed action. Obviously, this ensemble cannot be
used for the evaluation of ( 42 ), since the majority of configurations
appearing in the path-integral are not members of @xmath . To take
account of the need to include any possible configuration in the
ensemble, we also have to introduce the notion of ergodicity :

  Ergodicity:  

    An ensemble @xmath is called ergodic if

      -- -------- --
         @xmath   
      -- -------- --

An example of an ergodic ensemble is given by the random ensemble, where
each possible field configuration enters with equal probability:

  -- -------- -- ------
     @xmath      (48)
  -- -------- -- ------

With the measure @xmath from the random ensemble, the expression ( 42 )
becomes

  -- -------- -- ------
     @xmath      (49)
  -- -------- -- ------

Switching to different ensembles in path integrals consists of a
re-parameterization of the measure. It is therefore equivalent to the
substitution rule in ordinary integrals.

Another example of an ergodic ensemble is given by the canonical
ensemble (also known as the “equilibrium ensemble”) which is defined by

  -- -------- -- ------
     @xmath      (50)
  -- -------- -- ------

The measure in ( 39 ) is corresponding to the canonical ensemble and
therefore underlying the path integral definition in Eq. ( 42 ). Due to
this simple form of the operator expectation value, the canonical
ensemble ( 50 ) plays a huge role in numerical simulations of quantum
field theories.

Finally an important generalization of the canonical ensemble is given
by the multi-canonical ensemble . Suppose the underlying action in Eq. (
50 ) is replaced by an action @xmath , with some parameter @xmath . The
ensemble @xmath with density

  -- -------- -- ------
     @xmath      (51)
  -- -------- -- ------

leads to the following shape of ( 42 ):

  -- -------- -- ------
     @xmath      (52)
  -- -------- -- ------

The reason why ( 51 ) is useful is that it is often possible to find an
action @xmath which is numerically simpler to handle and simulate than
the original action @xmath and with the ensembles ( 50 ) and ( 51 )
being close enough to each other such that the “reweighting correction”
in ( 52 ) is small. A situation where this is the case is given in this
thesis in the framework of the TSMB algorithm to be discussed in Sec.
11.3 .

The ensemble is given by an infinite set of field configurations @xmath
. The introduction of ensembles thus apparently made the problem of
integrating a complicated multi-dimensional system even worse instead of
simplifying it. However, the re-formulation of the problem allows for a
solution by a different integration technique, the Monte-Carlo
integration [ 24 , 32 , 45 ] . This numerical method is going to be
discussed in Sec. 7 .

### 4 Gauge Theories

The guiding principle of the construction of quantum field theories in
Sec. 3.2 was the idea of locality. For a start, consider the @xmath
-component ( @xmath ) Yang-Mills theory described by the Lagrangian:

  -- -------- -- ------
     @xmath      (53)
  -- -------- -- ------

which is invariant under global transformations @xmath :

  -- -------- -- ------
     @xmath      (54)
  -- -------- -- ------

However, a global transformation on the fields living in @xmath is not
consistent with the idea of locality. Rather we want a theory which is
invariant under local gauge transformations @xmath :

  -- -------- -- ------
     @xmath      (55)
  -- -------- -- ------

A theory invariant under these transformations is called a gauge theory
. It is possible to add to Eq. ( 53 ) a term containing a new set of
fields @xmath such that it stays invariant under the transformation ( 55
). The simplest way to do this is to choose

  -- -------- -- ------
     @xmath      (56)
  -- -------- -- ------

where the covariant derivative @xmath is given by

  -- -------- -- ------
     @xmath      (57)
  -- -------- -- ------

and the transformation of @xmath must be given by

  -- -------- -- ------
     @xmath      (58)
  -- -------- -- ------

meaning that the @xmath lie in the adjoint representation of SU @xmath
and that @xmath . Thus, the resulting theory will now contain the fields
@xmath , @xmath , and @xmath . The new fields @xmath are termed gauge
fields and their coupling to the fields @xmath , @xmath is given by the
dimensionless coupling strength @xmath .

By postulating the fields to be invariant under the transformations ( 55
) and ( 58 ) and requiring that the Lagrangian only contains
perturbatively renormalizable terms (see Sec. 3.3.4 ), one is finally
led to the general form

  -- -------- -- ------
     @xmath      (59)
  -- -------- -- ------

with the field strength

  -- -------- -- ------
     @xmath      (60)
  -- -------- -- ------

There is an important difference between the pure gauge part in the SU
@xmath Lagrangian ( 59 ) and the single gauge field Lagrangian ( 11 )
corresponding to an Abelian gauge group: The former contains
interactions between different components of the gauge field @xmath ,
while the latter describes a true free field. Thus, the @xmath
-component vector theory contains interactions even in the case of a
purely gauge theory without coupling to a matter field. It is argued
below, that this phenomenon leads to the dynamical generation of a mass
scale in the case of the quantized theory. This phenomenon is also known
as dimensional transmutation .

Since the group SU @xmath is non-Abelian — their elements don’t commute
— Eq. ( 59 ) is referred to as a non-Abelian gauge theory . For
alternative ways to define a gauge theory cf. [ 46 , 24 ] and references
therein.

In addition to the SU @xmath symmetry, the Lagrangian ( 59 ) is also
invariant under axial rotations of the fermion fields, provided, the
Dirac part is massless ( @xmath ):

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (61)
  -- -------- -------- -------- -- ------

The question arises, whether this symmetry exists also on the quantum
level, or if it is broken by an anomaly. As has been realized by Adler [
47 ] and Bell and Jackiw [ 48 ] , for an Abelian gauge theory this is
indeed the case. The anomaly responsible for breaking the axial current
corresponding to the symmetry ( 4 ) is known as the Abelian anomaly or
ABJ-anomaly. It is present once the theory contains fermions and is
independent of the fermion masses. This result has also been derived
non-perturbatively by Fujikawa [ 49 ] . An extension to non-Abelian
theories has been given in [ 50 ] . For a textbook containing a rigorous
mathematical treatment consult [ 51 ] .

### 5 Quantum Chromodynamics

Now the ground has been prepared to formulate quantum chromodynamics
(QCD) as the theory underlying the strong interaction. It is a
Yang-Mills gauge theory (see Sec. 4 ) symmetric under the SU @xmath
group (as discussed in Sec. 1 ), where the latter symmetry group refers
to the color degree of freedom of the quarks. It contains six flavors of
quarks with masses @xmath , with each flavor of quarks transforming as
the fundamental triplet representation of the color group. The
accompanying vector bosons, the ‘‘gluons’’ transform according to the
adjoint representation. Furthermore we require the theory to be
perturbatively renormalizable. Thence, the resulting Lagrangian @xmath
in Minkowski-space is given by (the number of colors is denoted by
@xmath )

  -- -------- -- ------
     @xmath      (62)
  -- -------- -- ------

It is also possible (without violating perturbative renormalizability)
to add a term of the form

  -- -------- --
     @xmath   
  -- -------- --

to ( 62 ). This term is known as the “ @xmath -term” and would be a
source of @xmath violation [ 11 ] . The experimental limit for @xmath is
@xmath . Thus, this term will not be considered in this thesis.

Several important properties of QCD can be learned by considering the
symmetries of ( 62 ) [ 35 ] . For @xmath massless quark flavors, @xmath
is invariant under several global symmetry transformations. In
particular, one can decompose the Dirac spinors into left- and
right-handed quark fields and perform independent rotations on the
resulting Weyl spinors. This yields a global @xmath symmetry (this
symmetry is also known as chiral ). Furthermore one can make independent
global vector and axial rotations on the full Dirac spinors resulting in
a global @xmath symmetry. When looking at the masses of the different
quark flavors, one can indeed consider the masses of the @xmath - and
@xmath - quark flavors to be almost zero compared to the typical scales
of hadronic resonances. To a lesser extent this is also valid for the
@xmath -quark flavor. Thus, QCD contains three almost massless fermion
flavors and should consequently have a global @xmath symmetry.

According to the Noether theorem, there should be conserved charges
corresponding to each symmetry of the Lagrangian. The U @xmath -symmetry
is indeed associated with a conserved quantum number, namely the baryon
number which is conserved exactly by the strong interaction. The current
corresponding to the axial U @xmath -symmetry is, however, explicitly
broken by the ABJ anomaly (cf. Sec. 4 ) if the theory is quantized.
Nonetheless, one can find a modified, conserved current albeit it will
be gauge-dependent and thus not represent a physical current.

From the remaining chiral symmetry, one half is indeed present in the
hadron spectrum, namely as the flavor SU @xmath symmetry discussed in
Sec. 1 . This half corresponds to a vector symmetry transformation of
the Dirac spinors. The other half, however, which corresponds to an
axial vector transformation would result in a parity degeneracy of the
particles which is clearly not observed. To be specific, there are no
parity degeneracies present in the hadron spectrum at all. Thus, the
quantization of QCD must break this symmetry. Since there is no anomaly
which could attribute for this symmetry breaking, it must be broken in a
spontaneous manner, i.e. the ground state of the theory will not be
invariant. Due to the Goldstone theorem [ 35 ] , consequently there
exist massless particles corresponding to the pseudoscalar mesons whose
masses are much smaller than those of the other hadrons. The fact that
they are not zero can be attributed to the explicit breaking of chiral
symmetry due to the small masses of the light quarks. Within the
framework of Chiral Perturbation Theory ( @xmath PT) (see Sec. 5.3 ), it
can indeed be shown that for small quark masses, the effect can be
treated perturbatively.

But there does not seem to exist any Goldstone boson corresponding to
the breaking of the axial U @xmath charge. The only particle with the
correct symmetries is the @xmath -meson whose mass is far too large (see
Tab. 1 ). The solution of this problem is related to the topology of the
gauge field. Topological transitions can produce the @xmath -mass via
the axial anomaly. A possible explanation is that instanton transitions
(see below) are responsible for these topological charge fluctuations.

#### 5.1 Running Coupling and Energy Scales

The quantum theory build upon ( 62 ) is characterized by a running
coupling (for details see e.g. [ 35 ] ). Performing a leading order
perturbative analysis and renormalizing the theory, the behavior of the
running coupling ‘‘constant’’ is found to be

  -- -------- -- ------
     @xmath      (63)
  -- -------- -- ------

with @xmath , where @xmath is the number of active flavors [ 11 ] . This
defines the coupling at an energy scale @xmath . There are two important
lessons to be learned from ( 63 ):

-   The coupling @xmath decreases for increasing values of @xmath . The
    interaction vanishes for @xmath and the particles becomes free in
    this limit. This property is referred to as asymptotic freedom .

-   The coupling becomes infinite for a certain finite value of @xmath ,
    @xmath . This happens also in case of an Abelian gauge theory (where
    the underlying group is U @xmath ) and shows an intrinsic
    inconsistency under which ( 63 ) has been derived: The assumption
    that @xmath is small becomes invalid for increasing @xmath at some
    point and the series starts to diverge already at the first order
    beyond tree level. This singularity is called the Landau pole and is
    considered to be an unphysical remnant only present due to the fact
    that perturbation theory cannot be applied for too large expansion
    parameters. The appearance of the Landau pole thus sets a limit to
    the applicability of perturbative calculations. On the other hand
    one can expect the calculation to be valid at energies far larger
    than @xmath .

It is usually assumed that, when “solving” full QCD by the methods
sketched in Sec. 3.3 , one also obtains the whole low-energy
phenomenology with minimal input. There is no reason why the failure of
a single method, namely the perturbative expansion around the free
field, should imply that QCD is not valid at low energy scales. However,
a concise solution of interacting quantum field theories is not in
sight, so one has to stick with a number of models parameterizing the
low-energy behavior. One of these parameterizations is @xmath PT [ 52 ]
. Besides the latter, there are also different effective theories which
parameterize the behavior of the strong interaction at low energies:
models like the Nambu-Jona-Lasignio model [ 53 ] , the skyrmion model [
54 ] , or models based on instantons (see below) are different attempts
to describe the properties of low energy strong interactions. The
hadrons built up from one of the three heavy quarks can be described
using Heavy-Quark Effective Theory (HQET), see [ 55 , 56 ] for
introductions.

However, all these theories are only able to predict the low-energy
properties of the strong interaction; they do not incorporate an
adequate mechanism for the description of the parton content of hadrons.
For the high energy regime, the perturbative treatment of QCD has to be
used, which describes the interaction using the color group with the
gluons being the mediators of the strong force. However, if the strong
interaction is described using the flavor group as an interaction
between the baryons (the octet multiplet in the flavor SU @xmath group),
then the mediating particles are the pseudoscalar mesons.

One particularly important concept in the development of QCD is the
hypothesis of confinement . The common understanding of confinement is
that in a world without sea quarks, the static potential of two quarks
would be linear growing without limit. This leads to bound quarks not
being separable and thus free quarks being unobservable. One consequence
of this picture of confinement could be that the classical limit, Eq. (
2 ), may not exist. Thus, the consequences of confinement could be
wide-reaching. The best tools which have so far been used to address
this particular issue are lattice simulations. For a recent discussion
of lattice simulations regarding confinement, see [ 57 ] and references
therein.

There is another very important property of QCD shared with other
non-Abelian gauge theories: Consider ( 59 ) without fermions. Then it
can be shown [ 58 ] that there exist gauge field configurations which
vanish at spatial infinity, but fall into different topological classes.
They may be characterized by the winding number @xmath , which is given
by the Chern-Simons three form on the gauge fields [ 59 , 51 ] . The
transition between the different topological sectors may be performed
using the instanton solutions . These are solutions of the classical
equations of motion and they may also contribute significantly in the
quantized theory. For recent overviews consult [ 60 , 61 , 62 ] . The
importance of instantons for hadron physics has also been demonstrated
on the lattice in [ 63 , 64 ] . Recently, a method to examine a
prediction of the instanton model with lattice simulations has been
proposed in [ 65 ] . This method has been applied in [ 66 ] , confirming
the predictions of the instanton model. Indications for this picture
have also been found in an earlier publication [ 67 ] and in later works
[ 68 , 69 ] .

One particularly important point is that the quantum field theory built
upon ( 59 ) puts a lower limit to the magnitude of the instanton actions
resulting in a certain mass scale of the theory. Thus, a mass-scale is
generated although the classical theory is scale-free (and has no free
parameters except for the coupling @xmath which can be rescaled to any
value). As has already been mentioned in Sec. 4 , this phenomenon is
known as dimensional transmutation. A different widely discussed
manifestation of dimensional transmutation is the existence of
glue-balls (see e.g. [ 70 ] for a recent overview).

#### 5.2 Factorizable Processes

Several observables in QCD (like structure functions and form factors
etc., see e.g. [ 71 , 72 ] and for a more recent review [ 73 ] and
references therein) depend on input from both regimes. For several
interesting processes involving these observables, a method known as
factorization is applicable. The formal framework of factorization is
the operator-product expansion, whenever it applies. Consider two local
operators @xmath , @xmath . The Wilson expansion of the time ordered
product of the composite operator for short distances @xmath can then be
performed as [ 74 ]

  -- -------- -- ------
     @xmath      (64)
  -- -------- -- ------

This relation is only established perturbatively, however. The
singularities of the composite operator @xmath are then contained in the
@xmath which are @xmath -numbers. They are called Wilson coefficients
and contain the high-energy physics. Consequently, they can be computed
perturbatively. The operators @xmath are local operators containing
information about the low-energy regime and hence are usually not
accessible by perturbative methods. The individual terms in the sum ( 64
) can be arranged in such an order that the single terms behave as a
power series in @xmath , where @xmath characterizes the order of the
associated term. This is done by ascribing a certain “twist” to each
term. The first term (which vanishes slowest) is called the “leading
twist contribution” and the higher terms are consequently “higher twist
contributions”. The series then takes a form reminiscent of the
perturbative expansion, Eq. ( 44 ).

In the form of ( 64 ), the high energy regime and the low energy part
can be treated separately, and the object under consideration factorizes
in the two separate contributions. The major ingredient to a
factorization scheme is the factorization scale , i.e. the scale
describing which contributions belong to the low-energy regime and thus,
to the operators @xmath , and which contributions belong to the high
energy part, i.e. the functions @xmath . This leaves a certain freedom
in the application of the factorization approach. This freedom should be
exploited to keep higher-order corrections in the perturbative series as
small as possible, shifting the majority of contributions into the
leading order.

Naturally the question arises to what extent it is possible to ascribe
any meaning to a series like ( 64 ) if it involves a running coupling (
63 ) which is singular at some point in the physical parameter space.
This question has been addressed in e.g. [ 75 ] . From a pragmatic point
of view one can adopt the series despite the conceptional problems.
However, one has to circumvent the Landau singularity; to achieve this,
a number of proposals have been made: one is to apply a “freezing”
prescription, i.e. simply hold the coupling constant fixed below a
certain point [ 76 ] . Another consists of introducing an effective
gluon mass [ 77 ] . A different approach relies on the application of an
analytization procedure (first applied to QED by Lehmann and then
Bogoliubov , see [ 78 , 79 , 80 ] and references therein), which was
originally invented to extent ( 63 ) also to the regime where @xmath is
a timelike momentum transfer [ 81 , 82 ] . Later a framework of analytic
perturbation theory has been founded on this bases by Shirkov and
Solovtsov in [ 83 , 84 , 85 , 86 ] . In essence, the Landau singularity
in Eq. ( 63 ) can be compensated in a minimal way by adding a unique
power-term replacing the running coupling by

  -- -------- -- ------
     @xmath      (65)
  -- -------- -- ------

In contrast to the conventional expansion, the contribution of higher
terms appears to be suppressed (cf. [ 84 ] ). This observation together
with a renormalization and factorization scheme optimized for putting
most higher order contributions into the leading order should allow for
a consistent and efficient description of factorizable processes.
Indeed, it has been found that this program works for the cases of the
electromagnetic form factor of the pion and the @xmath transition form
factor [ 87 , 88 , 89 ] and yields an excellent agreement with the
experimental data while providing a consistent framework for the
computation of hadronic observables.

#### 5.3 Lattice QCD

The approach to perform a numerical simulation on a finite lattice
yielding an approximation to the Schwinger functions @xmath in discrete
Euclidean space @xmath is referred to as lattice gauge theory and
provides in principle the only means known so far to access the complete
structure of both the low and high energy regime of QCD. Anyhow, due to
the technical difficulties inherent to this method, the quality of
results is poor when compared to perturbation theory (whenever the
latter is applicable). Thus, contemporary lattice investigations always
concentrate on the non-perturbative regime of QCD calculating the
properties of the low-energy parameterizations.

As will be shown in Sec. 6.4 , there are problems concerning the
formulation of massless fermions on the lattice. On the other hand,
@xmath PT, as a low energy model of QCD, performs an expansion in the
quark mass around the point @xmath and thus allows for a systematic
treatment of near-massless fermions; for this reason, it is of
particular interest for lattice investigations, since one is usually
interested in performing extrapolations in the quark mass (see e.g. [ 42
] for a recent proposal of how to do this). @xmath PT is, however,
limited to the continuum theory. Thus, the continuum extrapolation
should precede the application of @xmath PT.

Since in lattice simulations one often chooses quark masses occurring in
virtual quark loops (the so-called ‘‘sea-quarks’’) different from the
quark masses appearing in hadrons (the so-called “valence-quarks”), an
extension of the original @xmath PT-formulation is necessary to handle
also these models. The first extension was to set the sea-quark mass
equal to zero (the quenched approximation ) yielding “quenched chiral
perturbation theory” (for a short discussion and the references, see [
90 ] ). This model allows for the extraction of phenomenology from
lattice simulations if one completely disregards dynamical fermion
contributions.

With the advent of dynamical fermion simulations, a further extension of
this model introducing different masses for sea and valence quarks was
proposed by Bernard and Golterman in [ 90 ] resulting in the “partially
quenched chiral perturbation theory”. In principle, partially quenched
chiral perturbation theory should allow for the first time to gain
direct access to phenomenological quantities from lattice simulations
provided a number of conditions is met [ 10 ] . In essence, one has to
perform simulations with three dynamical quark flavors (which may be
even mass-degenerate) at rather small masses of about @xmath . This goal
is out of reach with the resources available to the lattice community
today, but it may pave the way for future lattice simulations aiming at
precise measurements of hadron properties.

While quenched simulations already allow for a rather precise
determination of many phenomena in QCD [ 7 , 8 ] , there are observables
which depend also on dynamical fermion contributions. For example, the
mass of the @xmath meson (see above) is only properly accessible in
unquenched simulations (see [ 9 ] for a discussion).

The different methods for computations in QCD are visualized in Fig. 7 .

### 6 Discretization

As discussed in 3.3.3 , for the construction of a quantum field theory
on a lattice, the functional @xmath is required. This functional should
reduce to the Euclidean action @xmath in the continuum limit and for
differentiable paths. Before applying the limit prescription, it will
thus differ by @xmath -effects from the continuum expression — meaning
that in general the choice of @xmath is not unique but still leaves
freedom to choose all terms of order @xmath with @xmath . This freedom
should be used to find the form best suited for numerical calculations.

#### 6.1 Scalar Fields

Consider the complex field @xmath defined on the sites @xmath . The
continuum Lagrangian corresponding to this situation is given by Eq. ( 8
). One candidate for the lattice version of the action is then given by
[ 24 ] :

  -- -------- -- ------
     @xmath      (66)
  -- -------- -- ------

There are certainly other ways to replace the derivative, but the
present choice is the simplest way to incorporate neighbor fields.
Consequently, this choice is suitable for numerical investigations and
will be used in this thesis.

A particularly interesting model is the so-called @xmath -model, where
one sets

  -- -------- --
     @xmath   
  -- -------- --

This model appears to be an interacting, nontrivial field theory at
first sight, but already early it has been conjectured [ 31 ] that it
might only give rise to a non-interacting theory of free particles. In
later investigations this surmise has been corroborated [ 91 , 92 ] .
However, a rigorous proof is still missing.

#### 6.2 Gauge Fields

In the continuum form ( 59 ), the gauge field is given in terms of
parallel transporters along infinitesimal distances. By putting the
system on a lattice, the shortest (non-zero) distance is the lattice
spacing @xmath . The parallel transporter connecting a point @xmath with
its neighbor @xmath is denoted by @xmath . It is an element of SU @xmath
. The simplest gauge-invariant object one can construct is a closed loop
with a side length of one lattice unit usually called the plaquette .
Starting from the point @xmath , one can construct the plaquette lying
in the @xmath -plane by considering

  -- -------- -- ------
     @xmath      (67)
  -- -------- -- ------

Due to the fact that @xmath one can rewrite ( 67 ):

  -- -------- -- ------
     @xmath      (68)
  -- -------- -- ------

The suggestion of Wilson [ 93 ] was to use real part of the trace of
@xmath summed over all plaquettes as the action of the system,

  -- -------- -------- -------- -------- ------
     @xmath   @xmath                     (69)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- ------

which is the discretized form of the non-Abelian gauge field part in
Eq. ( 59 ). This form, however, is also applicable to the case of
Abelian gauge fields with the Lagrangian given by Eq. ( 11 ).

For later applications, the more convenient notation @xmath will be used
from now on; hence, the plaquette, Eq. ( 68 ), is written as

  -- -------- -- ------
     @xmath      (70)
  -- -------- -- ------

The path integral form of the partition function, Eq. ( 41 ), on a
finite lattice @xmath is given by

  -- -------- -- ------
     @xmath      (71)
  -- -------- -- ------

where the measure @xmath is the Haar measure on the gauge group (see
App. B ).

One of the properties of the Haar measure is that the total integral
over the group space on a single lattice point @xmath is finite. Thus,
any gauge-fixing procedure is unnecessary [ 24 ] . However, if one
attempts to apply a saddle-point approximation to ( 71 ) (see Sec. 3.3.4
), the presence of zero modes will spoil the inverse of the two-point
function [ 35 ] . This requires the introduction of a gauge fixing
procedure and auxiliary fields known as Faddeev-Popov ghosts [ 35 ] .

It is important to point out that the entire partition function ( 71 )
is manifestly gauge invariant since it is composed of gauge invariant
loops @xmath only. It can be shown [ 46 , 24 ] that it is impossible to
break gauge invariance spontaneously. This fact implies that the
expectation value of the @xmath will always vanish, a fact which is also
known as the Elitzur theorem:

  -- -------- --
     @xmath   
  -- -------- --

Eq. ( 69 ) has two limits, where the form of the gauge fields can be
written down explicitly: If one considers @xmath , the situation
resembles the hot temperature limit in thermodynamics. Thus, the
resulting gauge field configurations are called hot configurations — the
values of the gauge field variables are arbitrary and they can take any
random values from their domain of definition. The limit @xmath , is
referred to as the zero-temperature limit. The correlation between
neighbor points increases, therefore the total correlations length will
increase. Finally, the values of the field variables are given by @xmath
. If the system undergoes a second order phase transition, i.e. if the
correlation length diverges, this describes the continuum quantum field
theory.

#### 6.3 D-Theory

There exists also a discretization technique entirely different from the
methods discussed in Sec. 6.2 . It is based on a quantum link model,
which is constructed in such a way that it is still locally gauge
invariant and should thus reduce to the correct continuum form of the
Yang-Mills theory, just as ( 69 ) is expected to do. Quantum link models
with local gauge invariance have been formulated by Horn in [ 94 ] for
the first time, where a model with a local SU @xmath U @xmath gauge
invariance has been formulated. They have been extended to the case of
SU @xmath by Orland and Rohrlich in [ 95 ] . These models, however, did
not yet relate to the quantum field theories with continuous symmetries
as discussed in this thesis. Only recently it has been realized by
Chandrasekharan and Wiese in [ 96 ] , how one can relate the discrete
quantum link models with continuum field theories. For a review of the
ingredients of quantum link models see [ 97 ] .

##### 6.3.1 Globally Symmetric Models

The basic ideas behind the construction of D-theory become clear if one
considers a spin model, namely the O @xmath -model in two space
dimensions. The action is given by (cf. ( 69 ))

  -- -------- -- ------
     @xmath      (72)
  -- -------- -- ------

with @xmath , @xmath , being three-component unit vectors. The coupling
constant is given by @xmath . After quantizing this spin system
(cf. Sec. 3.3 ) by considering the partition sum

  -- -------- -- ------
     @xmath      (73)
  -- -------- -- ------

one arrives at a model which is asymptotically free and has a
non-perturbatively generated mass-gap. The question arises, whether it
is possible to find a different lattice system (with no resemblance to
Eq. ( 72 )) which still reduces to the same continuum field theory in
the sense discussed in Sec. 3.3 . This construction is indeed possible
and can be done as follows:

1.  Replace the classical vectors @xmath by quantum spin operators
    @xmath which are elements of the algebra @xmath , i.e. they are the
    generators of the SU @xmath -group (cf. Appendix D ).

2.  Replace the classical action ( 72 ) by the Hamiltonian

      -- -------- -- ------
         @xmath      (74)
      -- -------- -- ------

    which yields the quantum Heisenberg model. The partition sum ( 73 )
    is therefore replaced by the state sum

      -- -------- -- ------
         @xmath      (75)
      -- -------- -- ------

    It is important to point out that the particular representation of
    the group is not important — the trace can be taken over any
    representation, although in practice one usually adopts the
    fundamental representation [ 97 ] ⁴ ⁴ 4 This choice allows one to
    restrict to the smallest possible Hilbert space . In the following,
    the discussion is restricted to the case @xmath , i.e. the
    anti-ferromagnetic Heisenberg system.

3.  By using a Suzuki-Trotter discretization, the state sum ( 75 )
    becomes a partition function of a three-dimensional model with
    continuous symmetry with a certain lattice spacing @xmath . This
    model is invariant under a global SO @xmath -symmetry since the
    Hamiltonian ( 74 ) is also invariant. The low-energy properties of
    the resulting model can be described using chiral perturbation
    theory [ 98 ] . The symmetry is spontaneously broken in the ground
    state, resulting in two Goldstone bosons which are represented by
    fields in the coset SO @xmath /SO @xmath . Thus, they describe the
    same kind of three-component unit vectors which appear in the
    original action, Eq. ( 72 ). The low-energy effective action of the
    Goldstone bosons can be formulated using chiral perturbation theory:

      -- -------- -- ------
         @xmath      (76)
      -- -------- -- ------

    with @xmath being the extend of the third dimension which has been
    introduced by the Suzuki-Trotter discretization. The parameters
    @xmath and @xmath constitute the spin-wave velocity and the
    stiffness, respectively.

4.  Finally, there exists a mapping of the two systems, which has been
    suggested by Hasenfratz and Niedermayer [ 99 ] . This is achieved by
    a block spin transformation, which maps subvolumes of size @xmath to
    a new lattice system. The new lattice will then have a lattice
    spacing given by @xmath and the coupling constant @xmath of the
    transformed system is given by

      -- -------- -- ------
         @xmath      (77)
      -- -------- -- ------

    Thus, the continuum limit of the new lattice model is obtained in
    the limit @xmath . The correlation length (and thus the inverse mass
    scale of the system) is given in terms of @xmath by

      -- -------- -- ------
         @xmath      (78)
      -- -------- -- ------

    In the limit @xmath , the correlation length thus diverges
    exponentially and the extent @xmath becomes negligible and hence the
    system undergoes dimensional reduction.

In conclusion, one can say that D-theory introduces a substructure to
the original system. The lattice spacing of this substructure is much
smaller than the corresponding lattice spacing of the original theory.
However, the resulting lattice action is obtained from exact blocking of
the continuum fields, implying that the lattice artifacts are of order
@xmath . This means that in practical simulations, one can use lattice
spacings of the same order of magnitude as with the Wilson
discretization and the resulting theory has a lattice spacing @xmath .

##### 6.3.2 Models with Local Gauge Symmetries

The construction principle underlying D-theory can be applied to other
models as well. The important cases of U @xmath and SU @xmath gauge
theories have been discussed in [ 96 ] . The application to the case of
QCD has been considered in [ 100 ] . For a review consult [ 101 ] .

In [ 102 ] , it has been conjectured how the parameter space of the
D-theory formulation is related the coupling of the conventional theory.
Also the principal chiral model could have been formulated in this way
and has been shown to reduce to the conventional discretization
formulation [ 103 ] . From these discussions it becomes clear that in
fact D-theory is an alternative formulation of the discretization of
quantum field theories with local gauge symmetries.

##### 6.3.3 Simulation Algorithms

For the simulation of quantum spin systems, a particular efficient class
of algorithms is available, known as cluster algorithms . While the most
efficient algorithms to be discussed in Chapter Advanced Algorithms for
the Simulation of Gauge Theories with Dynamical Fermionic Degrees of
Freedom which are applicable to the Wilson action Eq. ( 69 ) are all
local, the cluster algorithms are global.

Cluster algorithms have first been introduced to quantum spin systems by
Swendsen and Wang in [ 104 ] . These algorithms exploit the mapping
introduced by Fortuin and Kasteleyn [ 105 ] to rewrite the partition
function and to formulate a global algorithm which is able to flip a
large cluster of spins at once. In this way, critical slowing down which
will be discussed in Sec. 8.4 is effectively reduced, provided the
average cluster size scales proportional to the correlation length of
the system. For a general overview of cluster algorithms see [ 106 ] . A
useful generalization of cluster algorithms which might be applicable to
D-theory is given by the world-line Monte-Carlo algorithms, see [ 107 ,
108 , 109 ] and [ 110 ] for a new implementation.

If indeed locally gauge symmetric models can be simulated efficiently
using a quantum spin system, the inclusion of dynamical fermions would
be straightforward [ 97 ] . Thus, full Yang-Mills theory might be
efficiently simulated. There is furthermore reason to believe, that the
fermionic sign problem for a discussion) may be handled better in the
framework of quantum spin systems. For an overview see [ 111 ] . For
further readings consult [ 112 ] .

This benefit could then be used to overcome the limitations of current
algorithms regarding the sign of the fermionic determinant. This problem
occurs whenever an odd number of dynamical fermion flavors is being
simulated very close to massless fermion flavors. This point will be
discussed in Secs. 6.4 and 20 .

#### 6.4 Fermion Fields

The Euclidean space version of ( 56 ) is given by

  -- -------- -- ------
     @xmath      (79)
  -- -------- -- ------

where the @xmath -matrices in Euclidean space must be employed, cf. App.
A . The representation of Eq. ( 79 ) on the lattice is a very
complicated task. As shown in App. G , the basic fields @xmath are
elements of a Grassmann algebra. These fields admit a representation as
four-component vectors with the choice of @xmath as given in App. A .
Thus, the task of putting an @xmath -component Yang-Mills field in
Euclidean space, on the lattice is equivalent to finding a matrix @xmath
with @xmath , @xmath , and @xmath , giving rise to the action

  -- -------- -- ------
     @xmath      (80)
  -- -------- -- ------

To simplify the notation, the indices @xmath , and @xmath will be
suppressed from now on. The corresponding path integral defining the
quantum partition function, Eq. ( 41 ), is then given by (cf. Eq. ( 262
) in App. G.3 )

  -- -------- -- ------
     @xmath      (81)
  -- -------- -- ------

For the discretization of the fermionic action, a number of choices is
available. However, the Nielsen-Ninomiya theorem [ 113 , 114 ] puts a
general limit on any lattice fermion action; under some natural
assumptions on the lattice action, it follows that there is an equal
number of left- and right-handed particles for every set of quantum
numbers.

This implies that on the lattice the fermion spectrum consists of pairs
of fermions and fermion-mirrors. Thus, apparently it appears to be
impossible to implement the structure of Dirac fermions on a discrete
space-time. However, one can evade the physical consequences by
decoupling the superfluous fermion states. In QCD this can be achieved,
for instance, by giving the fermion doublers a mass proportional to the
cut-off @xmath . This procedure, however, does not work in a chirally
symmetric model; in fact, it is a general consequence of the topological
character of lattice theory that there does not exist a regularized
chiral fermion theory that has the following properties (see for a proof
of this no-go theorem [ 115 ] ):

1.  global invariance under the gauge group,

2.  a different number of left- and right-handed species for given
    charge combinations,

3.  the (correct) Adler-Bell-Jackiw anomaly,

4.  and an action bilinear in the Weyl field.

The absence of the Adler-Bell-Jackiw anomaly displays the fact that the
axial U @xmath current is conserved because of the cancellation of
opposite-handed species.

Of course, in the continuum formulation any gauge invariant
regularization scheme yields the same expression for the axial anomaly.
Thus, this should also be valid for the lattice regularization, too.
Consequently, any candidate for the lattice discretization of gauge
theories should reproduce the axial anomaly in the continuum limit.
Indeed it has been shown in [ 116 ] that the Wilson discretization [ 117
] does reproduce the chiral anomaly in the continuum limit. The Wilson
action breaks chiral symmetry on the lattice explicitly thus removing
the unwanted doublers from the propagators. The chiral symmetry breaking
term is actually an irrelevant contribution to the lattice Ward
identity, i.e. it is proportional to the lattice spacing, @xmath .
However, it does not disappear in the limit @xmath , but rather accounts
precisely for the anomaly. For a discussion of the phase structure
associated with Wilson fermions on the lattice consult [ 118 ] .

A theorem showing that, under the rather general conditions of locality,
gauge covariance and the absence of species doubling, the lattice action
gives rise to the axial anomaly has been given in [ 119 , 120 ] for
Abelian gauge theories and generalized to the case of QCD (which can in
principle be generalized to any non-Abelian gauge theory) in [ 121 ] .
However, the axial flavor mixing current should be non-anomalous. That
this is indeed the case has been shown in [ 122 ] . The proofs have all
been done perturbatively on the lattice using the expansion from [ 123 ]
.

The problem of representation of chiral symmetry on the lattice has been
resolved only recently, when it was realized that a solution of the
Ginsparg-Wilson relation (GWR) introduced in [ 124 ] has an exact chiral
symmetry on the lattice, as has first been discussed in [ 125 ] . The
first fermionic action which actually satisfies the GWR was the perfect
action of [ 126 ] . For practical purposes, the solution of Neuberger [
127 , 128 ] is the most widely used today (for a historical overview of
the development leading to the Neuberger representation, see [ 129 ] ).
Finally it is important to point out that the theorem in [ 121 ] also
applies to Ginsparg-Wilson fermions thus ensuring that they reduce to
the correct fermionic action in the continuum limit.

However, since the numerical effort for the evaluation of Neuberger
fermions increases by @xmath orders of magnitude compared to Wilson
fermions, the calculation with dynamical Neuberger fermions is still
prohibitively expensive.

As argued above, the Wilson action breaks chiral symmetry on the lattice
with a term of order @xmath . Thus, the action depends linearly on the
cutoff and physical observables might show sizable lattice artifacts
when approaching the continuum limit. As has been put forward by
Sheikholeslami and Wohlert in [ 130 ] the cancellation of the @xmath
dependence can be calculated perturbatively up to a prefactor, the
parameter @xmath . Observables computed using this fermionic action with
a non-perturbatively calculated @xmath indeed show weaker artifacts with
an @xmath -dependence as has been demonstrated in e.g. [ 131 , 132 ] .
This program is also called clover-improvement , since the perturbative
correction has the shape of a four-leaf clover. Clover-improvement
turned out to be useful in a number of studies employing the hybrid
Monte-Carlo (HMC) algorithm (see Sec. 11.2 ). When applying it to
multiboson (MB) algorithms (cf. Sec. 11.3 ), however, the required local
staples (see App. Advanced Algorithms for the Simulation of Gauge
Theories with Dynamical Fermionic Degrees of Freedom ) would soon become
extremely complicated and the merits of the improvement might become
obscured by the increased algorithmic demands.

Since the major focus of this thesis lies on Wilson fermions, a few
words about its explicit breaking of chiral symmetry are in order.
Having no chiral symmetry means that there is explicit symmetry breaking
by the non-chiral fermion mass. Thus, the physics of spontaneous chiral
symmetry breaking may be shadowed. In fact, it turns out to be extremely
difficult to perform lattice calculations with light quarks since the
numerical effort increases polynomially in the inverse quark mass [ 133
] . However, when performing the continuum limit at sufficiently small
quark masses (where the precise meaning of “sufficient” can only be
given very roughly within @xmath PT [ 10 ] ), one can afterwards
extrapolate to the desired quark mass and still be able to extract
correct continuum physics from numerical lattice simulations. This is
the method usually adopted in actual calculations employing light
fermions.

With the conventions used in this work, the Wilson action for a single
fermion flavor reads:

  -- -------- -- ------
     @xmath      (82)
  -- -------- -- ------

where the Wilson matrix @xmath is defined to be

  -- -------- -------- -------- -------- ------
     @xmath   @xmath   @xmath            (83)
                                @xmath   
  -- -------- -------- -------- -------- ------

with @xmath being a function of the bare mass parameter which is called
hopping parameter . Due to the anticommutivity of the fermion field, one
also has to include antiperiodic boundary conditions in the coupling to
the gauge field, see [ 24 ] for a thorough discussion. This usually
proceeds by choosing all @xmath , with @xmath restricted to a single
timeslice when applying the matrix multiplication with ( 83 ). This sign
is not explicitly written here. For the local form to be discussed in
App. Advanced Algorithms for the Simulation of Gauge Theories with
Dynamical Fermionic Degrees of Freedom , however, it is necessary to
treat this factor separately.

The matrix @xmath in ( 83 ) consists of the local @xmath -function
contribution and a “derivative” term containing nearest-neighbor
interactions. This is often called the hopping matrix , @xmath , and can
be considered to be the lattice version of the covariant derivative in
the continuum Dirac matrix Eq. ( 56 ), @xmath . The “mass” has been
taken to unity and the hopping parameter @xmath has been written in
front of the lattice derivative term which can be achieved by a
redefinition of the fields @xmath . Thus, the Wilson matrix explicitly
breaks chiral symmetry on the lattice. As will soon become clear, one
can nonetheless recover the correct chiral behavior by fine-tuning the
@xmath parameter. In terms of the hopping matrix, Eq. ( 83 ) can be
written as

  -- -------- -------- -------- -------- ------
     @xmath   @xmath   @xmath            
     @xmath   @xmath   @xmath            (84)
                                @xmath   
  -- -------- -------- -------- -------- ------

The Wilson matrix, @xmath , fulfills the @xmath -hermiticity property

  -- -------- -- ------
     @xmath      (85)
  -- -------- -- ------

as can be seen from inspection. This leads to the following properties:
the eigenvalues are either real or come in complex conjugate pairs. If
one takes the determinant of @xmath , it can therefore only change sign
if an odd number of purely real eigenvalues becomes negative. At this
point it should also be remarked that the total number of real
eigenvalues is in the continuum related to the topological charge via
the Atiyah-Singer-index theorem [ 51 ] . For an investigation of the
validity of the index theorem on the lattice see [ 134 , 135 ] .

The spectrum of the hopping matrix @xmath in Eq. ( 6.4 ) has been
examined in [ 136 ] . For recent overviews and results obtained from
eigenvalue methods, see [ 137 , 138 , 139 ] . In general, the following
picture emerges: For a configuration with @xmath (cf. Eq. ( 69 )), the
spectrum fills a disc centered at the origin with radius two (see Fig. 8
). In the small coupling regime, the structure is more complicated
(consult Fig. 9 ): The outer shape of the eigenvalues forms an ellipse
which has a large radius of eight and a small radius of four. However,
four circles with radius two each, centered on the real axis, are left
out. At intermediate values of @xmath , one finds spectra interpolating
between these two situations: the spectrum starts to spread and the
holes start to form, but the eigenvalue density is not yet completely
zero in the holes. Especially, the real eigenvalues tend to populate the
bulks for a rather long time compared to the imaginary ones (see [ 134 ]
). When measuring the lattice spacing in physical units, @xmath , one
finds that @xmath effects manifest themselves prominently in the real
eigenvalues still lying in the holes [ 134 ] .

Considering then the complete Wilson matrix, @xmath , one finds that the
lower bound of the spectrum becomes zero if (in the free case) @xmath .
A derivation of this result for free configurations can also be found in
[ 24 ] . In this case, the Wilson matrix describes massless Dirac
fermions. This point is called the chiral point and the associated value
of @xmath is called the critical value @xmath .

Hence, if @xmath increases from zero to @xmath , the values of @xmath
decrease from @xmath down to @xmath . For practical determinations of
@xmath , see Sec. 22 .

The Wilson matrix given in equation ( 6.4 ) is not only non-Hermitian,
it is even non-normal, i.e.

  -- -------- --
     @xmath   
  -- -------- --

Thus, it cannot be diagonalized by a unitary matrix; however, it is
possible to diagonalize the Wilson matrix by a similarity transformation
with non-unitary matrices,

  -- -------- -- ------
     @xmath      (86)
  -- -------- -- ------

A further consequence of non-normality is that @xmath will in general
have different left- and right eigenvectors [ 140 ] , a property which
should be respected in the definition of matrix elements in terms of the
eigenvectors [ 66 , 141 ] . In several cases (as it is the case for the
sampling algorithms to be discussed in Chapter Advanced Algorithms for
the Simulation of Gauge Theories with Dynamical Fermionic Degrees of
Freedom ), one only needs the determinant @xmath . Therefore it is often
convenient to use the Hermitian variant of the Wilson action which can
be obtained by replacing the matrix @xmath by the Hermitian Wilson
matrix @xmath :

  -- -------- -- ------
     @xmath      (87)
  -- -------- -- ------

It is easy to show that @xmath is in fact self-adjoint:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

The spectrum of @xmath is more complicated than the spectrum of @xmath
and determining the sign of the determinant is a non-trivial task.
Exploiting the fact that @xmath , one can, however, always use

  -- -------- --
     @xmath   
  -- -------- --

##### 6.4.1 Even-Odd Preconditioning

A simple transformation allows the Wilson action to be rewritten [ 142 ,
143 ] such that the condition number is reduced. To do this we divide
the lattice into two distinct subsets of “even” and “odd” coordinates:

  Even-odd splitting:  

    If the coordinates of a given lattice site are given by @xmath then
    a point belongs to the “odd” subset iff

      -- -------- --
         @xmath   
      -- -------- --

    Otherwise they belong to the “even” subset.

If we rearrange the components of the vector in ( 83 ) in such a way
that the color spinor is given by @xmath with the first half being
“even” sites and the second half “odd” sites, then the Wilson matrix (
83 ) takes the following shape:

  -- -------- -- ------
     @xmath      (88)
  -- -------- -- ------

Using the Schur decomposition [ 144 ]

  -- -------- -- ------
     @xmath      (89)
  -- -------- -- ------

one arrives at the preconditioned action

  -- -------- -- ------
     @xmath      (90)
  -- -------- -- ------

Since this matrix has the same determinant as ( 83 ) it yields the same
action ( 82 ). However, the smallest eigenvalue is about a factor of two
larger, making the inversion simpler. On the other hand, ( 90 ) has a
more complicated shape (it now contains next-to-nearest neighbor
interactions). Therefore the total effort for a matrix multiplication
stays the same, but the memory requirement for a color spinor has been
reduced.

#### 6.5 Yang-Mills Theory

Finally, one can write down the total discretized form of the continuum
Yang-Mills action whose Lagrangian is given in Eq. ( 59 ):

  -- -------- -------- -------- -------- ------
     @xmath   @xmath   @xmath            (91)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- ------

with @xmath being the Wilson matrix ( 83 ). The bare mass parameter
@xmath appearing in @xmath refers to the contribution of dynamical sea
quarks (i.e. the virtual quark loops). It is therefore termed @xmath .
The evaluation of the determinant becomes increasingly difficult as
@xmath approaches @xmath , whose precise value can only be determined
non-perturbatively (see Sec. 22 ). Since the evaluation of the
determinant of such a huge matrix is highly difficult, it is sometimes
being set equal to one (which corresponds to @xmath ), resulting in the
fermionic contribution to ( 91 ) being totally absent. If this is only
being done in the generation of configurations (i.e. the ensemble is
sampled with the pure gauge action) this amounts to removing the
contributions of sea quarks. This defines the quenched approximation
mentioned in Sec. 5.3 .

## Chapter \thechapter Numerical Methods

In this chapter, the numerical methods are introduced which are required
to simulate lattice gauge theories with and without dynamical fermion
contributions.

The properties of Monte-Carlo algorithms are introduced in Sec. 7 . They
make use of Markov chains, as it will be explained in Sec. 7.1 .

Section 8 introduces into the subject of time series analysis, in
particular the analysis of autocorrelations in a Monte-Carlo time
series.

The measurement of hadronic masses is discussed in Sec. 9 .

The particular algorithms used for the Monte-Carlo integration scheme
are given in Secs. 10 and 11 . The former concentrates on the algorithms
required for scalar and gauge fields, while the latter introduces
algorithms applicable to simulations with dynamical fermionic
contributions. The most important bosonic algorithms are the Metropolis
algorithm (Sec. 10.1 ), the heatbath algorithm (Sec. 10.2 ) and the
overrelaxation technique (Sec. 10.3 ).

The algorithms for sampling contributions of dynamical fermions are
treated in Sec. 11 . First the general problems one encounters when
evaluating the determinant of the Wilson matrix are introduced in Sec.
11.1 . It will become clear, that any algorithm dealing with the
fermionic determinant requires the inversion of a large matrix
describing the contribution of the discretized fermionic degrees of
freedom. Then the most widely used algorithm for the simulation of
dynamical fermion flavors, the hybrid Monte-Carlo (HMC) algorithm, is
reviewed in Sec. 11.2 .

In this thesis, however, a more advanced algorithm for this subject will
be used, namely a variant of the multiboson (MB) algorithms. This class
of algorithms is discussed in Sec. 11.3 . These algorithms are able to
overcome several limitations and shortcomings of the HMC, but at the
cost of far more complexity.

As has been mentioned above, matrix inversion is an essential tool for
the implementation of fermion algorithms. The tools required for the
implementation of matrix inversion algorithms are described in Sec. 12 .
The inversion algorithms presented are static algorithms in Sec. 12.1 ,
the Conjugate-Gradient iteration (Sec. 12.2 ), the GMRES algorithm (Sec.
12.3 ), and the BiCGStab scheme (Sec. 12.4 ).

Finally, the tools for the computation of eigenvalues of matrices are
shortly reviewed in Sec. 13 . They are important for the application of
static matrix inversion schemes and thus for the implementation of
multiboson algorithms.

### 7 Monte-Carlo Algorithms

The path integral definition introduced in Sec. 3.3 allows for an
evaluation using ensembles of field configurations as discussed in Sec.
3.4 . This definition, however, requires to perform an integration on an
infinite space of operator-valued distributions @xmath with a given
probability distribution @xmath and a measure @xmath . An approach
different to the reformulation in terms of Gaussian integrals discussed
in Sec. 3.3 is the application of a numerical integration using a
Monte-Carlo scheme. That such an endeavor can indeed yield physical
results in quantum field theories was first demonstrated in [ 145 , 6 ]
. In this section it will be demonstrated how an algorithm can be
designed in such a way that it generates a finite set of independent
gauge field configurations which can be used as an estimator to the
ensemble averages and thus to the path integral ( 42 ).

A Monte-Carlo integration algorithm is an algorithm which computes a
finite set of mesh points and yields a statistical approximation @xmath
to the given problem. The error of the approximation is given by the
statistical error of the integration scheme. To be specific, let’s
consider an algorithm which generates a finite sequence @xmath , @xmath
, of @xmath statistically independent configurations. These must be
distributed according to the probability density @xmath of the
underlying ensemble. The finite sequence is called the sample of
configurations . If they have been taken randomly from the ensemble ( 48
), the sample average

  -- -------- -- ------
     @xmath      (92)
  -- -------- -- ------

is an estimator for the ensemble average @xmath with an error given by
the variance of the statistical estimate ( 42 ). Consequently, the error
of the Monte-Carlo integration behaves as @xmath . This is different
from the standard integration schemes like Simpson’s rule [ 44 ] whose
error behaves as @xmath , with @xmath being the dimension of the
underlying space. Obviously this method is better for low dimensions
(for @xmath ) and worse for higher dimensions ( @xmath ). There are
better algorithms than Simpson’s rule, but none is competitive with
Monte-Carlo integrations in very large dimensions. On the other hand, no
Monte-Carlo integration is competitive with deterministic algorithms at
lower dimensions.

It is obvious how to generalize Eq. ( 92 ) if the sample configurations
have been drawn from the canonical ensemble ( 50 ). Then the estimator
is given by the sample average

  -- -------- -- ------
     @xmath      (93)
  -- -------- -- ------

Since such an integrand may be peaked rather narrowly around its average
value, the sampling algorithm should generate only the relevant
contributions. Such a procedure is called importance sampling .

In the following, the theoretical basis needed to design Monte-Carlo
algorithms from Markov chains is founded.

#### 7.1 Markov Chains

An important concept for the design of an algorithm yielding the desired
sample of configurations is the Markov chain :

  Markov chain:  

    A Markov chain @xmath consists of a set of states @xmath defined on
    a base space. For the purposes in this thesis, this is the space of
    discretized fields, @xmath . A specific element @xmath is generated
    from the previous element @xmath by a stochastic process @xmath :

      -- -------- --
         @xmath   
      -- -------- --

    The associated transition probability is given by the matrix element
    @xmath . It solely depends on the state @xmath . The Markov density
    @xmath is a unit vector in the state space spanned by all @xmath ,
    in which the matrix @xmath acts.

If the states @xmath have the probability distribution @xmath , applying
@xmath once to the end of the chain may change the probability
distribution. With the initial distribution given by @xmath , one
obtains a new distribution @xmath via

  -- -------- --
     @xmath   
  -- -------- --

Using this language one can define the following notions related to
Markov chains:

  Irreducibility:  

    Denote @xmath for @xmath repeated applications of @xmath on @xmath ,
    yielding @xmath . A chain is called irreducible if for any states
    @xmath , there exists an @xmath such that

      -- -------- --
         @xmath   
      -- -------- --

  Aperiodicity:  

    Define @xmath to be the @xmath -step transition probability to reach
    @xmath from the starting element @xmath in @xmath steps. A chain is
    called irreducible and aperiodic if for each pair @xmath there
    exists an @xmath such that @xmath for all @xmath .

  Recurrence time:  

    Take a state @xmath . Let @xmath be the probability to reach @xmath
    after @xmath applications of @xmath on @xmath . Then the mean
    recurrence time @xmath is given by

      -- -- --
            
      -- -- --

  Positivity:  

    A state @xmath is called positive iff @xmath is finite.

  Stationary distribution:  

    A probability distribution @xmath is called stationary distribution
    of the Markov chain if it stays invariant under application of
    @xmath :

      -- -------- --
         @xmath   
      -- -------- --

A particularly important class of Markov chains is given by the
irreducible, aperiodic chains whose states are positive [ 32 , 44 ] .
Indeed one can prove the following theorem:

  Existence and uniqueness of the stationary point:  

    Take an irreducible, aperiodic Markov chain with positive states
    @xmath and transition function @xmath . Hence, the chain has the
    starting distribution @xmath . Then the limiting probability
    distribution @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

    exists and is unique. It is thus a fixed point of @xmath .

From now on we will only consider Markov chain with this property. The
transition probability @xmath has to be normalized, i.e. for all @xmath
the following equation must hold:

  -- -------- -- ------
     @xmath      (94)
  -- -------- -- ------

Now we can generate the desired sample of field configurations @xmath as
the states of a Markov chain by the repeated application of @xmath on
the last state @xmath of the sample thus generating new members of the
sample and improving the approximation of Eq. ( 92 ). However, we must
ensure that the transition probability is designed in such a way that
the samples are taken from the desired ensemble of configurations,
i.e. the density of the sample, @xmath , must equal the density of the
ensemble, @xmath .

Since the application of @xmath on a state @xmath may change the
probability density @xmath , we have to design the process in such a way
that the stationary distribution of the Markov chain is given by the
ensemble density of the ensemble under consideration:

  -- -------- -- ------
     @xmath      (95)
  -- -------- -- ------

Knowing from above that the fixed point exists and that it is unique,
one can formulate the following sufficient (but not necessary) condition
for the transition probability @xmath of the transition matrix @xmath :

  -- -------- -- ------
     @xmath      (96)
  -- -------- -- ------

Summing on both sides over the complete state space @xmath and using (
94 ) one arrives at

  -- -------- --
     @xmath   
  -- -------- --

which is identical to Eq. ( 95 ). Relation ( 96 ) is known as detailed
balance . It does not determine the transition probability uniquely and
thus one can design different algorithms sampling the field
configurations. However, since Eq. ( 95 ) is not a sufficient condition,
it may happen that the algorithm gets ‘‘stuck’’ in a local maximum of
the density. Such a situation is difficult to detect and even more
difficult to handle. The only way to proceed in such cases is by using
multicanonical sampling (see Sec. 3.4 ). If one manages to find an
action @xmath which no longer has several distinct local maxima, this
problem is avoided. A typical situation where this may happen is if the
system is close to a first-order phase transition, where the system has
comparable probabilities to exist in either one of two different phases
[ 33 ] .

For an infinitely long Markov chain, we define the mean value @xmath by

  -- -------- -- ------
     @xmath      (97)
  -- -------- -- ------

with @xmath being the stationary distribution of the Markov chain. Then
the mean value @xmath coincides with the expectation value @xmath from
Eq. ( 42 ). For a finite sample @xmath , @xmath , the estimator @xmath
approximates @xmath with an error of order @xmath as discussed above.

### 8 Autocorrelation

Although the Markov chain generates a new state only from the previous
one without any knowledge of older states, the new state may be rather
similar to the old one. Thus, the sample of configurations generated as
states of the Markov chain will in general not be statistically
independent. The correlation in the sequence of generated configurations
can be made mathematically precise using the autocorrelation function of
a time series. In the following @xmath denotes the measurement of @xmath
on a configuration @xmath . The time series then consists of the set of
@xmath , @xmath .

#### 8.1 Autocorrelation Function

The autocorrelation function is defined by

  -- -------- -- ------
     @xmath      (98)
  -- -------- -- ------

where the average of the infinite series is denoted as @xmath . The set
of states underlying Eq. ( 98 ) is infinite. However, as already noted
above, in practical calculations one deals with finite samples and
therefore is unable to compute the exact averages but only estimators.
The estimator based on a finite sample of length @xmath for ( 98 ) is
given by

  -- -------- -- ------
     @xmath      (99)
  -- -------- -- ------

The autocorrelation function with @xmath is the standard deviation of
the series. The normalized autocorrelation function @xmath is defined by

  -- -------- -- -------
     @xmath      (100)
  -- -------- -- -------

#### 8.2 Exponential Autocorrelation Time

One important information the autocorrelation function yields is the
time the system needs to equilibrate, i.e. the time needed until the
system goes from an arbitrary starting point @xmath to the stationary
probability density @xmath . To study this behavior, let @xmath be a
given probability measure on @xmath at an arbitrary intermediate state
taken from the Markov chain and @xmath the equilibrium distribution of
the Markov chain. Let @xmath denote the Banach space of complex-valued
functions @xmath on the state space @xmath having finite norm

  -- -------- -- -------
     @xmath      (101)
  -- -------- -- -------

The inner product in this space is given by

  -- -------- -- -------
     @xmath      (102)
  -- -------- -- -------

Then we define the deviation of @xmath from @xmath by [ 146 , 44 , 133 ]
:

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (103)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

In general, the transition @xmath for an irreducible, positive-recurrent
Markov chain has the following properties:

  Contraction:  

    The spectrum of @xmath lies in the closed unit disc. Consequently,
    @xmath is a contraction.

  Eigenvalues of the stationary distribution:  

    The eigenvalue @xmath of @xmath is simple. The operator @xmath has
    the same properties.

  Uniqueness:  

    If the chain is aperiodic, then @xmath is the only eigenvalue of
    @xmath (and of @xmath ) on the unit circle. The eigenvector is the
    unit vector in @xmath .

If @xmath has been obtained from a starting distribution @xmath by a
single application of @xmath , it follows that

  -- -------- -- -------
     @xmath      (104)
  -- -------- -- -------

The spectral radius formula [ 146 ] yields:

  -- -------- -- -------
     @xmath      (105)
  -- -------- -- -------

Thus, @xmath is the spectral radius of @xmath on the orthogonal
complement of the identity, i.e. the largest modulus of the eigenvalues
of @xmath with @xmath . The definition of @xmath , Eq. ( 105 ), maps the
spectral radius @xmath onto @xmath . Hence, a scale in the Markov chain
has been introduced. After @xmath applications of @xmath one arrives at

  -- -------- -- -------
     @xmath      (106)
  -- -------- -- -------

The meaning of @xmath is that of a relaxation parameter. The number of
steps required for the system to reach the fixed point distribution
starting from an arbitrary distribution is characterized by this time
scale. It may happen that @xmath even becomes infinite [ 44 ] . In such
a case, one can never reach the equilibrium by starting from an
arbitrary configuration in finite time.

To actually compute @xmath for a given algorithm, one must find a good
test function, i.e. an appropriate observable in ( 100 ) with sufficient
overlap to the slowest mode of the system. Thus, one can define @xmath
via

  -- -------- -- -------
     @xmath      (107)
  -- -------- -- -------

where several different observables @xmath must be considered. Of
course, in practice one can never be sure that the slowest mode of the
system is captured by the set of observables chosen.

In practical situations, however, one does not work with the total
density vector @xmath of the system, but rather one considers only the
finite sample of configurations obtained by repeated application of
@xmath to a single starting configuration. The probability of this
configuration in the equilibrium density @xmath may be rather small, but
it cannot be zero. The way to estimate a given density vector in the
state space of the Markov chain is then to histogram an observable and
examine its distribution. For a gauge theory on the lattice this could
e.g. be the gluonic action. Unless the system hasn’t thermalized, the
histogram will still change its shape when adding new configurations.

For the starting configuration it is common to either use a homogeneous
set of variables, the cold start , or a set of random variables, the hot
start .

#### 8.3 Integrated Autocorrelation Time

Once the Markov chain has reached the equilibrium density, there is
still an autocorrelation between subsequent measurements. This
autocorrelation can be assessed by considering the integrated
autocorrelation time , @xmath . For an observable @xmath , the latter is
defined via [ 44 , 133 ]

  -- -------- -- -------
     @xmath      (108)
  -- -------- -- -------

The factor of @xmath in ( 108 ) is a matter of convention. It ensures
that @xmath if @xmath for @xmath . When applied to a finite sample of
lengths @xmath , one obtains an estimate via

  -- -------- -- -------
     @xmath      (109)
  -- -------- -- -------

@xmath characterizes the statistical error of an observable @xmath .
This can be seen by considering the variance @xmath of the mean ( 92 ):

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (110)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

Thus, the error in case of stochastically dependent configurations is
decreased by the factor @xmath if autocorrelations are present. It is
obvious, that the integrated autocorrelation time will in general depend
on the observable @xmath , meaning that some quantities are harder to
measure than others from finite samples. This also depends on the
algorithm underlying the Markov chain, i.e. on the choice of the
transition matrix @xmath .

As it is discussed in [ 133 ] , the autocorrelation function @xmath may
be composed of several different exponentials. The fast decaying modes
lead to a decrease of the contribution of the slower modes in the
integral. Therefore, observables with only a small overlap on the slowly
decaying modes will usually exhibit a smaller @xmath than those
dominated by the slower modes. Large spatial correlations on the lattice
may induce modes in the autocorrelation functions which are also large
(since the information has to propagate a larger distance through the
lattice along the Markov chain). This results to the fact that large
correlation lengths which one encounters for smaller masses exhibit
larger integrated autocorrelation times — a result which was clearly
visible in the samples contained in [ 133 ] .

Recalling that @xmath is associated with the slowest mode in the system,
one concludes that @xmath for any observables @xmath . This can also be
shown by considering again the spectrum of @xmath . If detailed balance
holds, @xmath is self-adjoint on the space @xmath . Hence, the spectrum
is real and lies in an interval @xmath with

  -- -------- -------- -- -- -------
     @xmath   @xmath         
     @xmath   @xmath         (111)
  -- -------- -------- -- -- -------

Using the spectral radius formula ( 105 ) again yields

  -- -------- --
     @xmath   
  -- -------- --

where the slowest mode is associated with @xmath . By considering an
estimator @xmath for @xmath , one can write it in form of a spectral
representation

  -- -------- -- -------
     @xmath      (112)
  -- -------- -- -------

The largest and slowest modes contributing to @xmath have been denoted
by @xmath and @xmath . They form a subinterval of @xmath . Summing ( 112
) over @xmath one finally arrives at

  -- -------- --
     @xmath   
  -- -------- --

This leads to

  -- -------- --
     @xmath   
  -- -------- --

#### 8.4 Scaling Behavior

As has been discussed in Sec. 3.3.3 , a quantum field theory usually
will undergo a second order phase transition as the continuum limit is
approached. This implies that the correlation length, @xmath ,
associated with the system diverges. This divergence claims an increase
in the lattice size, @xmath , and usually also means that the
autocorrelation time increases rapidly. This phenomenon is known as
critical slowing down . In particular, the autocorrelation time diverges
as [ 44 ] :

  -- -------- -- -------
     @xmath      (113)
  -- -------- -- -------

which defines the dynamic critical exponent @xmath . Critical slowing
poses a problem for the numerical simulation of dynamical systems since
especially the critical points are points of major physical interest.

#### 8.5 Short Time Series

When using Eq. ( 109 ) to estimate @xmath for an observable on a finite
time series, one still needs a sufficient amount of measurements. The
particular problem is that large @xmath values of @xmath will have large
noise, but only small signals since the function does approach zero
while the errors don’t [ 44 ] . To be specific the error can be computed
using the approximation @xmath :

  -- -------- -- -------
     @xmath      (114)
  -- -------- -- -------

If the sum in ( 109 ) is cut off at a point @xmath (introducing a
“window” of size @xmath ), one obtains @xmath via

  -- -------- -- -------
     @xmath      (115)
  -- -------- -- -------

The trade-off is that by using ( 115 ), one introduces a bias

  -- -------- -- -------
     @xmath      (116)
  -- -------- -- -------

Thus, the bias will only be a finite-length effect of the time series
which will vanish once the series is long enough.

The choice of @xmath should be guided by the desire to make @xmath small
while on the other hand still keeping the @xmath small.

##### 8.5.1 Windowing Procedure

One way to choose the window parameter @xmath is to apply the following
recipe [ 44 , 133 ] : Find the smallest integer @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

If @xmath was a pure exponential, then it would suffice to take @xmath .
This implies that @xmath would have decayed by @xmath since @xmath .
However, if @xmath does not show a clear exponential behavior, then one
has to consider @xmath or still larger. For time series of the order of
@xmath this algorithm works fine [ 44 ] , however it is not clear how
stable this procedure is for much smaller samples. Sadly, in the
numerical simulation of Euclidean field theories, one usually only has
@xmath or even less, so this method alone is insufficient for obtaining
a reliable estimate of @xmath .

##### 8.5.2 Lag-Differencing Method

A typical indicator of a systematic bias might be that the
autocorrelation function does not converge to zero but rather approaches
a constant before dropping to zero in a non-exponential manner. It could
also be that the autocorrelation function exhibits linear behavior.
Being conservative, one would conclude that in such a situation the time
series is simply too short to give answers and that there is no way to
extract further information from it. If one is more practical, one may
try to extract only the exponential modes from the series and discard
the linear behavior. This is what differencing does. In [ 133 ] , this
new method for eliminating, or at least reducing the bias ( 116 ) of the
time series has been suggested by Lippert . The idea is to apply a
differencing prescription to the original series in order to reveal the
true autocorrelation behavior. This approach is justified, because once
the Markov density @xmath becomes stationary, the system will be
unaffected by a shift in the time origin.

Define the order- @xmath -lag- @xmath -differenced time series by

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (117)
  -- -------- -------- -------- -- -------

Examining the estimator for the average @xmath shows, that the
first-order-differenced series indeed goes to zero:

  -- -------- --
     @xmath   
  -- -------- --

One possible way to apply definition ( 8.5.2 ) is to examine the
correlation between the original series @xmath and the order- @xmath
differenced series @xmath :

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (118)
                                @xmath   
  -- -------- -------- -------- -------- -------

A constant bias will be removed for @xmath , while the modes with scales
below @xmath should not be affected. However, when choosing @xmath too
small, Eq. ( 118 ) will destroy also exponential modes larger than
@xmath . On the other hand, the procedure will be ineffective if @xmath
is too large since the statistical quality of the sample will get worse.
For this reason, we also believe that higher order differencing will not
be useful for practical purposes.

In practice one has to examine the autocorrelation function for a number
of different lags. In the ideal case, a plateau should form when
plotting the estimated value for @xmath from Eq. ( 118 ) vs. the lag
@xmath . This fortunate case is, however, only rarely given since one
would not need to apply the differencing procedure in the first place if
the statistics were good enough.

The recipe to apply this procedure which is used in this thesis consists
of the following steps: (i) Get a first rough estimate about the
autocorrelation time @xmath . This may be obtained by comparison to
different time series or by the other methods for computing
autocorrelation times. (ii) Vary the lag @xmath and measure a the
function @xmath for the different lags. (iii) If the function exhibits a
plateau with @xmath , the estimate for @xmath at the plateau is taken.
If no plateau is formed even when going to @xmath , the method fails to
give any reasonable answer.

##### 8.5.3 Jackknife Method

As an independent consistency check, one can also exploit relation ( 110
) to obtain an estimate for @xmath . The method discussed in the
following is called Jackknife binning and allows to find the “true”
variance of a sample. In addition, it allows to estimate the variance of
“secondary quantities”, i.e. a function obtained from the average of the
original sample. In the context of quantum field theories, secondary
quantities are given by observables which are defined to be expectation
values and thus require an averaging over the ensemble.

Reference [ 147 ] contains an introduction to the Jackknife procedure;
for a complete discussion and further applications consult Ref. [ 148 ]
.

The Jackknife method consists of the following steps:

1.  Choose a block size @xmath and partition the series in a number of
    blocks of size @xmath . The total number of blocks is then given by
    @xmath . In the following it will be assumed that all blocks have
    equal size (if @xmath is not a divisor of @xmath , one can simply
    make the last block smaller; this has no practical influence).

2.  Define the averages @xmath , @xmath , by

      -- -------- -- -------
         @xmath      (119)
      -- -------- -- -------

    with @xmath and @xmath . Thence, @xmath is the average of the sample
    @xmath with the @xmath th block of size @xmath (ranging from @xmath
    to @xmath , included) being left out.

3.  Then define the Jackknife estimator for the average and its variance
    for bin size @xmath by

      -- -------- -------- -------- -- -------
         @xmath   @xmath   @xmath      
         @xmath   @xmath   @xmath      (120)
      -- -------- -------- -------- -- -------

4.  Repeat the above procedure for different values of @xmath and take
    the limit @xmath . The corresponding value of @xmath is the true
    variance of the sample. In practice, one has to plot the variance
    @xmath vs. the bin size @xmath until a plateau emerges. The
    resulting plateau will then give an estimate of the true variance.
    However, in general the resulting variances will fluctuate strongly,
    making a precise determination impossible. The best one can do is
    then to take the average value of the plateau as an estimate and the
    fluctuations as the errors on the variances.

After knowing the true variance, the integrated autocorrelation time can
be estimated by

  -- -------- -- -------
     @xmath      (121)
  -- -------- -- -------

This approach, however, only allows for a crude estimate of @xmath ,
since one has no systematic control of the error (see above). This has
to be contrasted to the autocorrelation function where one can use Eq. (
114 ).

The generalization of the Jackknife method to secondary quantities,
i.e. functions of the sample average, @xmath , is straightforward.
Starting from the averages defined in ( 119 ), one defines the functions
@xmath of @xmath and their variances analogously to Eq. ( 3 ) by

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (122)
  -- -------- -------- -------- -- -------

With the obtained variances, one can proceed as before and apply ( 121 )
to get the autocorrelation time of the secondary quantity.

The Jackknife method is applied in this thesis both to obtain an
independent estimate of the autocorrelation time and to obtain the true
variance and thus the true error of both primary and secondary
quantities.

### 9 Measuring Hadron Masses

In order to measure hadronic masses on the lattice, one needs to compute
correlation functions of operators carrying the same quantum numbers as
the hadron under consideration. For general reviews see [ 46 , 149 , 32
, 24 ] and [ 150 ] . On the lattice one has again a certain freedom for
the construction of these operators. In this thesis the simplest
operators are taken in accordance with [ 149 ] . For instance, in the
case of the charged pion and rho-meson (cf. Eqs. ( 1 ) and ( 1 )), one
obtains

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (123)
  -- -------- -------- -------- -- -------

where @xmath is the @xmath -flavored quark field with color index @xmath
, and @xmath the @xmath -flavored quark field, respectively. @xmath
means that summation over the three spatial @xmath -matrices has to be
performed.

As discussed in [ 149 ] , one can use the Källen-Lehmann representation
of two-point functions in the Euclidean region to derive the mass
formula. In the case of a scalar field this is done via

  -- -------- --
     @xmath   
  -- -------- --

where the spectral weight function is positive and has the shape of a
@xmath -peak for single-particle states. The Euclidean propagator,
@xmath , is given by

  -- -------- --
     @xmath   
  -- -------- --

Integrating over three-space yields a single “time-slice”, defining the
correlation function

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (124)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

For large time separations, @xmath , the lowest mass state @xmath
dominates.

In order to extent this construction to fermionic correlation functions,
one needs the generalization of ( 262 ) to arbitrary integrals of the
Gaussian type (see [ 24 ] for a mathematical derivation):

  -- -------- -- -------
     @xmath      
     @xmath      (125)
  -- -------- -- -------

with

  -- -------- --
     @xmath   
  -- -------- --

The sign factor from Eq. ( 262 ) has been dropped. The evaluation of a
hadronic matrix element thus requires one to recourse to the fermionic
matrix, @xmath . The bare mass which enters here is related to the
valence quark content of the hadron in question and is therefore termed
@xmath . It is therefore possible, as already argued in Sec. 5.3 , to
choose the valence quark mass appearing in the hadronic operators
different from the sea quark mass appearing in the measure which is used
for the sampling process, Eq. ( 91 ). In fact, for quenched simulations
this is a necessity to derive hadronic masses. See Sec. 5.3 for a
discussion of these methods.

A meson correlation function

  -- -------- -- -------
     @xmath      (126)
  -- -------- -- -------

for large Euclidean times will then yield the desired mass. However, on
a lattice with a finite extent, one has to take into account finite size
effects. Since periodic boundary conditions are usually implemented, the
lattice correlation function will be symmetric and the lattice masses
will have to be extracted using:

  -- -------- -- -------
     @xmath      (127)
  -- -------- -- -------

where the temporal lattice extension is taken to be @xmath . The case of
baryons is more involved, however. See for the latest methods and
results [ 148 ] .

Combining Eqs. ( 9 ), ( 9 ), and ( 126 ), the correlation function for
the pion is given by

  -- -------- -- -------
     @xmath      (128)
  -- -------- -- -------

Fitting the resulting @xmath to ( 127 ) for large values of @xmath will
then yield the lattice pion mass, @xmath .

### 10 Bosonic Sampling algorithms

The task of this section is to describe several algorithms realizing a
Markov chain for the field configurations @xmath . Any algorithm should
thus generate a new configuration @xmath from a given configuration
satisfying ergodicity and detailed balance Eq. ( 96 ). Once the new
configuration @xmath has been generated by updating all degrees of
freedom (d.o.f.), one denotes this procedure as a single sweep .

In general, one can divide the algorithms into two different classes:

  Local algorithms:  

    The local algorithms consider a subset @xmath of sites — usually
    only a single site at a time — and change this point according to a
    certain prescription. Then a different subset will be considered
    until the whole space @xmath has been processed at least once. There
    is no global decision taking place on the lattice. Usually local
    algorithm are constructed such that they satisfy detailed balance
    and ergodicity locally, thus ensuring that the total sweep also
    satisfies these properties.

  Global algorithms:  

    All sites are being updated at once according to a prescription not
    depending on any sublattice or subset. These global update
    algorithms usually induce larger autocorrelations than the local
    ones since the changes which can be applied to all sites at once
    will only be small compared to a change which can be applied at a
    single site only.

There are also several hybrid forms of algorithms. The multiboson
algorithms discussed in this thesis are usually a mixture of several
local sweeps combined with a global step. Furthermore, local forms of
the multicanonical algorithms [ 151 , 152 ] , also may require the
evaluation of the global action.

To estimate the dynamical critical exponent for a local algorithm, one
has to remember that in a single step the “information” is transmitted
from a single site to its neighbors [ 44 ] . Consequently, the
information performs a random walk around the lattice. In order to
obtain a “new” configuration, the information must travel at least a
distance of @xmath , the correlation length. Therefore one would expect
@xmath near criticality, i.e. @xmath .

The potential advantage of global algorithms is that they may have a
critical scaling exponent smaller than for local algorithms. This can be
attributed to the fact that since all sites are update at once, the
information need not travel stepwise from one lattice site to its
neighbor, as it was the case for a local algorithm.

#### 10.1 Metropolis Algorithm

The Metropolis algorithm has been introduced in [ 153 ] . It can be
implemented both locally and globally and has the following general form
which has been formulated in [ 154 , 155 ] ): The transition probability
@xmath is the product of two probabilities @xmath , where

1.  @xmath generates a given probability density for the proposed change
    of the configuration. A convenient choice may be that @xmath is
    taken from the random ensemble Eq. ( 48 ) independent of @xmath .

2.  The transition probability @xmath is then given by

      -- -------- -- -------
         @xmath      (129)
      -- -------- -- -------

where @xmath is the probability of @xmath in the equilibrium density of
the Markov process, @xmath .

##### 10.1.1 Local Metropolis Update

As an example we consider a lattice with field variables @xmath , @xmath
, which can take on continuous variables from the interval @xmath ,
@xmath . The task is to design a Markov process which generates field
configurations distributed according to a canonical ensemble, Eq. ( 50
), i.e. according to @xmath , where @xmath is a multiquadratic action as
discussed in App. Advanced Algorithms for the Simulation of Gauge
Theories with Dynamical Fermionic Degrees of Freedom . A simple
algorithm which implements the local Metropolis update sweep is designed
as follows:

1.  For each lattice site @xmath compute the local staple @xmath
    corresponding to @xmath . For a definition and actual computations
    of such a staple see App. Advanced Algorithms for the Simulation of
    Gauge Theories with Dynamical Fermionic Degrees of Freedom .

2.  Suggest a randomly chosen new field variable @xmath from @xmath with
    staple @xmath , @xmath . Accept the new variable @xmath with
    probability

      -- -------- -- -------
         @xmath      (130)
      -- -------- -- -------

    otherwise keep the old value @xmath .

3.  Iterate step 2 a number of times.

4.  Continue to next loop in item 1 .

Afterwards, the entire lattice will have been updated. This algorithm is
obviously ergodic since any configuration can be reached due to the
random proposal of @xmath . Furthermore it satisfies detailed balance (
96 ) by construction. This form is the special case of the general
algorithm, where @xmath and thus @xmath alone.

The algorithm discussed above is applicable to almost any system with
multiquadratic action, but it may not be efficient. It may happen that
those values of @xmath which have a high chance of being accepted are
strongly peaked around a small subinterval and consequently most
suggestions are rejected. In such cases it is therefore preferable to
take @xmath from a non-uniform distribution which is very close (or even
identical) to the desired distribution. In this case, the Metropolis
decision will have to be modified accordingly. In the latter case, if
@xmath has already been taken from the correct distribution, the test
can even be skipped (since this situation would correspond to the case
@xmath and consequently @xmath . This is the idea of the heatbath
algorithm which is discussed in Sec. 10.2 .

##### 10.1.2 Global Metropolis Update

In contrast to the algorithm above, it is also possible to postpone the
Metropolis decision until all lattice sites have been processed. This is
the idea of the global Metropolis update. This may be necessary in a
situation where the action cannot be written in the form of a local
staple, or if this step is too costly. In general, the Metropolis
decision will take the following form

  -- -------- -- -------
     @xmath      (131)
  -- -------- -- -------

However, when choosing @xmath to be a random configuration, the action
@xmath will usually be widely different from @xmath , and thus the
exponential will become huge. To be specific, the probability of
acceptance is given by the @xmath th power of the single-site acceptance
rate, where @xmath is the lattice volume. For any reasonable lattice
size, this number will consequently be prohibitively small, if even the
single-site acceptance was of the order of @xmath .

Therefore a global Metropolis step can only be applied in the following
situations:

-   The distribution of @xmath is close to the desired one. Hence, the
    sampling process was able to generate almost the “correct”
    distribution and one merely has to correct a small residual error.

-   The proposal @xmath is very close to the old configuration @xmath .
    In such a case one has to make sure that ergodicity still holds and
    even if it does, the danger of running into metastabilities may be
    larger. Furthermore the autocorrelation times may not be very
    favorable in this situation since the evolution in phase space is
    rather slow. For the effort of processing all sites a much smaller
    path has been traversed than in the case of the local algorithms;
    this explains why e.g. the HMC algorithm is not competitive to local
    algorithms when the local form of the action is available (see
    below).

The global form of the Metropolis algorithms therefore usually appears
in combination with some other algorithm (either of global or local
nature) which generates a suitable proposal @xmath such that the
acceptance rate, Eq. ( 131 ), stays reasonably large.

#### 10.2 Heatbath Algorithm

As has already been pointed out in the previous section, the heatbath
algorithm generates a sample from a distribution which is identical to
the equilibrium distribution @xmath . The name of the algorithm
expresses the procedure of bringing the system in contact with an
infinite heatbath. If there exists a global heatbath algorithm, then it
will immediately generate the new configuration independent of the old
one, thereby eliminating all autocorrelations. This fortunate situation
is only seldom given, however. In many situations, it is possible to
apply the heatbath at least locally, i.e. to generate a candidate @xmath
at a site @xmath independent from the old value @xmath such that @xmath
is distributed according to

  -- -------- -- -------
     @xmath      (132)
  -- -------- -- -------

Since repeated application of the local Metropolis update prescription
generates a Markov chain for @xmath at lattice site @xmath , which also
satisfies detailed balance, it will have a fixed point distribution
which is precisely given by Eq. ( 132 ). Thence, repeating the local
Metropolis an infinite number of times on a single site is identical to
the local heatbath algorithm.

Finding the distribution ( 132 ) is possible once its integral is known,
i.e. [ 24 ]

  -- -------- -- -------
     @xmath      (133)
  -- -------- -- -------

Then one can generate the distribution of @xmath from a random number
@xmath by

  -- -------- -- -------
     @xmath      (134)
  -- -------- -- -------

Often, it is not possible to directly generate the desired distribution
( 134 ), but rather only an approximation. Call this approximation
@xmath with its integral @xmath . Now generate the new variable @xmath
and correct for the difference to the desired distribution, @xmath ,
with a Metropolis step with probability [ 24 ]

  -- -------- --
     @xmath   
  -- -------- --

The total transition probability matrix for this process is given by

  -- -------- --
     @xmath   
  -- -------- --

with @xmath being the average acceptance rate which depends on the
quality of the approximation of @xmath . Iterating this step for @xmath
times yields the transition probability matrix

  -- -------- --
     @xmath   
  -- -------- --

In the limit @xmath the desired distribution is recovered. However, it
is sufficient to just iterate this step @xmath times (where the optimal
value of @xmath should be determined such that the algorithm has the
highest efficiency) since the stationary distributions of @xmath and
@xmath coincide by virtue of the properties of the Markov process.

One can also choose to iterate the transition @xmath as long as the
proposed change is accepted, i.e. stop the iteration once one proposal
has been rejected. This procedure will also have the same stationary
distribution. Again, considerations of numerical efficiency should
decide which choice is optimal.

In the following, several implementations of local heatbath algorithms
which are needed for the multiboson algorithm are presented.

##### 10.2.1 Heatbath for Gauge Fields

First consider the case of the Wilson action, Eq. ( 268 ) from App.
Advanced Algorithms for the Simulation of Gauge Theories with Dynamical
Fermionic Degrees of Freedom , for an SU @xmath gauge theory [ 6 , 156 ]
. The distribution to be generated for a single gauge variable @xmath
then takes the form

  -- -------- -- -------
     @xmath      (135)
  -- -------- -- -------

The link variable @xmath can be parameterized as

  -- -------- --
     @xmath   
  -- -------- --

The unitarity condition implies

  -- -------- --
     @xmath   
  -- -------- --

where @xmath , and @xmath . The Haar measure @xmath in Eq. ( 135 ) can
be parameterized as

  -- -------- --
     @xmath   
  -- -------- --

In the present form, all parameters depend in a non-linear way on the
distribution and the precise form of the staple @xmath . Thus, it
appears that generating the desired distribution is a tough problem.
However, it is possible to exploit the invariance of the Haar measure
@xmath on the gauge group and to rotate the l.h.s. of ( 135 ) to a
distribution which only depends on @xmath . This step significantly
simplifies the problem; consider the SU @xmath -projection @xmath
(cf. Eq. ( 249 ) in App. D.1 ). Obviously, @xmath holds, so the Haar
measure stays invariant under right multiplication with @xmath ,

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (136)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

In this form, the distribution only depends on the determinant of the
staple, @xmath , and the point @xmath has a non-trivial distribution
alone. Once it has been chosen, the remaining components, @xmath , are a
random point on the unit sphere in three-dimensional space, @xmath , and
can be chosen, for instance according to @xmath . The distribution for
@xmath is given by (with @xmath ):

  -- -------- -- -------
     @xmath      (137)
  -- -------- -- -------

By applying the transformation @xmath one obtains

  -- -------- -- -------
     @xmath      (138)
  -- -------- -- -------

This distribution can be generated by the method from Eq. ( 134 ) by
choosing the proposal for @xmath from the interval @xmath . An
alternative method has been introduced by Kennedy and Pendleton in [ 157
] . This method is superior if the distribution for @xmath is peaked
close to one, a situation which is typically encountered in multiboson
algorithms. While the method from Eq. ( 138 ) becomes less efficient for
sharply peaked distributions, the latter choice will soon become
superior.

Once the new @xmath have been obtained in this way, the new link
proposal can be obtained by applying the inverse rotation in Eq. ( 136 )
thus yielding

  -- -------- -- -------
     @xmath      (139)
  -- -------- -- -------

An extension of this procedure to the case of SU @xmath gauge theories
with @xmath is more difficult since they do not share the property that
any sum of group elements is proportional to a group element. A possible
generalization has been proposed in [ 158 ] . The basic idea is to
decompose the whole SU @xmath group into an appropriate set of SU @xmath
subgroups such that no subgroup is left invariant. Call this set @xmath
, @xmath . A possible choice is @xmath with

  -- -------- --
     @xmath   
  -- -------- --

The new field variable @xmath is finally chosen to be

  -- -------- --
     @xmath   
  -- -------- --

Defining

  -- -------- --
     @xmath   
  -- -------- --

one obtains the recursion

  -- -------- --
     @xmath   
  -- -------- --

Now each multiplication with @xmath gives rise to a heatbath
distribution of the SU @xmath group, Eq. ( 135 ). Hence, one has to take

  -- -------- -- -------
     @xmath      (140)
  -- -------- -- -------

where @xmath now takes over the role of the SU @xmath -staple in Eq. (
135 ). For the proof that this procedure does indeed generate the
desired distribution consult [ 24 , 158 ] .

##### 10.2.2 Heatbath for Scalar Fields

In the case of scalar fields one encounters actions of the type ( 267 ).
One prominent example is the evaluation of the fermion matrix in
sampling algorithms (see Sec. 11.1 ). Another case of major importance
is the evaluation of correlation functions like Eq. ( 128 ). These
systems allow for a rather simple implementation of both local and
global heatbath algorithms. In fact, this is one of the few cases, where
a global heatbath algorithm exists. The application of the local
algorithm is straightforward: For each site @xmath generate a Gaussian
random number @xmath with width @xmath , i.e.

  -- -------- --
     @xmath   
  -- -------- --

The new field variable, @xmath is then given by

  -- -------- -- -------
     @xmath      (141)
  -- -------- -- -------

There also exists a more powerful variant which is applicable if the
total action admits the following form (as it is the case for fermionic
actions):

  -- -------- -- -------
     @xmath      (142)
  -- -------- -- -------

Similarly to the local case, the generation of the @xmath proceeds by
taking a random Gaussian vector @xmath with unit width, i.e.

  -- -------- --
     @xmath   
  -- -------- --

Then solve the equation

  -- -------- -- -------
     @xmath      (143)
  -- -------- -- -------

Thus, the global heatbath requires a matrix inversion for each new
sample @xmath . This is rather costly compared to the local variant;
however, the advantage is that there is no autocorrelation at all for
the whole sample of @xmath generated.

Several methods how to perform the matrix inversion are discussed in
detail in Sec. 12 . All these methods provide an approximation with a
residual error @xmath . The question arises, how small this error should
be made. Choosing the residual error too large will result in a bias
introducing systematic errors beyond control. One could make the
residual error extremely small, i.e. several orders of magnitude below
the statistical error inherent in the Monte Carlo integration. But this
will waste computer time in generating an inverse with too large
accuracy. This question has been addressed in several publications, see
[ 159 , 160 , 161 ] and references therein. An improvement to these
standard methods has been suggested in [ 162 ] , which allows for a
reduction of the computer time required by a factor of about @xmath
while still generating the correct distribution. The idea is again to
sample an approximate distribution and apply a Metropolis correction
step. Consider a vector distributed according to @xmath . Then consider
the joint distribution

  -- -------- --
     @xmath   
  -- -------- --

By virtue of

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

the distribution of @xmath is unchanged. Now one can update @xmath and
@xmath with the following alternate prescription:

1.  Perform a global heatbath on @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

    where @xmath is a random Gaussian vector with unit width.

2.  Perform the reflection

      -- -------- -- -------
         @xmath      (144)
      -- -------- -- -------

    which yields the new vector @xmath .

The second step conserves the probability distribution of @xmath but is
not ergodic. The first step ensures ergodicity. The matrix inversion in
( 144 ) now can be performed with a finite accuracy @xmath yielding the
approximate solution

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the residual. Now the second step can be considered as a
proposal for @xmath . It will be accepted in a Metropolis step with
probability (cf. Eq. ( 129 ))

  -- -------- -- -------
     @xmath      (145)
  -- -------- -- -------

where

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (146)
                                @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

If the matrix inversion is solved exactly, i.e. @xmath , then one will
recover the original global heatbath algorithm. It has been discussed in
[ 162 ] that there exists an optimal choice of @xmath which reduces the
computer time by a factor of @xmath over the older methods.

#### 10.3 Overrelaxation

A particular method to improve the behavior of the system near
criticality consists of overrelaxation . It is similar to the technique
of overrelaxation in differential equation algorithms [ 163 , 164 ] .
The idea can also be generalized to gauge theories [ 156 , 165 ] . An
overrelaxation step performs a reflection in the space of field
elements, which keeps the action invariant. When applying the local
Metropolis decision, Eq. ( 130 ), the change is thus always accepted.
Since the action does not change, the algorithm is non-ergodic and
generates the microcanonical ensemble, Eq. ( 47 ); it does, however,
satisfy detailed balance, Eq. ( 96 ). Consequently, it cannot be used as
the only updating scheme, but it can increase the motion of the system
in phase space if mixed with an ergodic algorithm. In this way, the
expected improvement may result in a dynamical critical scaling exponent
of about @xmath , cf. [ 24 ] .

For a multiquadratic action of the form ( 267 ), a local overrelaxation
step may simply be implemented by choosing the new field @xmath to be

  -- -------- -- -------
     @xmath      (147)
  -- -------- -- -------

For the Wilson action of the SU @xmath gauge theory, the overrelaxation
step can be performed by choosing the new element @xmath as

  -- -------- -- -------
     @xmath      (148)
  -- -------- -- -------

where @xmath is given by @xmath . This replacement leaves the action
invariant since (note that no summation over the index @xmath must take
place!):

  -- -------- --
     @xmath   
  -- -------- --

Eq. ( 148 ) is equivalent to the following transformation:

  -- -------- -- -------
     @xmath      (149)
  -- -------- -- -------

It is possible to generalize ( 149 ) to the case of SU @xmath , @xmath ,
with the same Cabibbo-Marinari decomposition as discussed in Sec. 10.2 .

### 11 Fermionic Sampling Algorithms

The algorithms discussed in the previous sections have for a long time
only been applicable to the case of theories without dynamical fermions,
i.e. the quenched approximation. The typical cost one has to pay if one
includes dynamical fermion contributions is a factor of about @xmath .
It was not before the mid-90’s when sufficient computer power became
available to treat also dynamical fermions numerically. One further
problem is that in a Yang-Mills theory including dynamical fermion
contributions, Eq. ( 91 ), the fermion determinant is a non-local
object. Therefore global algorithms like the HMC had to be employed. A
possible way to rewrite ( 91 ) to obtain a purely local action has been
put forward by Lüscher [ 166 ] . This was the key to also use local
sampling algorithms for systems with dynamical fermions.

#### 11.1 Sampling with the Wilson Matrix

The essential problem of lattice fermions is the evaluation of the
determinant from Eq. ( 81 ). This can be achieved by using a Gaussian
integral over boson fields @xmath [ 167 ]

  -- -------- -- -------
     @xmath      (150)
  -- -------- -- -------

where the field @xmath has the same indices as the Grassmann field
@xmath . The prefactor from the integration in Eq. ( 150 ) is a constant
which cancels in any observable and will hence be dropped from now on.
The determinant can be evaluated using a stochastic sampling process
similar to the measurement of observables using ( 92 ) for the
evaluation of ( 42 ). Thus, the fermionic contributions can also be
written as a part of a measure. The prefactor is a constant and will
cancel for any observable. Therefore, it will be disregarded in the
following.

However, there are some problems with the application of ( 150 ) to the
Wilson matrix, Eq. ( 83 ). The former is only defined for a Hermitian
and positive-definite matrix, a condition clearly not fulfilled by the
Wilson matrix. Nonetheless, the product @xmath is Hermitian and positive
definite, so ( 150 ) is applicable. This expression corresponds to two
dynamical, degenerate fermionic flavors.

A second problem regards the fact that the Wilson matrix will have
eigenvalues close to zero when describing sufficiently light fermions
(cf. Sec. 6.4 ). Since ( 150 ) computes the inverse determinant, a
single noisy estimate may oscillate over several magnitudes and in sign,
see e.g. [ 149 ] . Therefore, one instead tries to compute the
determinant instead of its inverse.

The above arguments result in the expression

  -- -------- -- -------
     @xmath      (151)
  -- -------- -- -------

When approximating the determinant in ( 151 ) with a finite sample of
configurations, @xmath , one can use the global heatbath applied to the
scalar boson fields @xmath as discussed in Sec. 10.2.2 . This requires a
matrix inversion. Algorithms to perform this inversion will be discussed
in Sec. 12 .

#### 11.2 Hybrid Monte-Carlo Algorithm

The idea behind the molecular dynamics -based algorithms is different
from those discussed in the previous sections. The key feature consists
of using quantities obtained from averages of the microcanonical
ensemble ( 47 ) as an approximation to the average as given in Eq. ( 42
) obtained from the canonical ensemble. This identification works in the
thermodynamic limit, i.e. in the case of large lattices. The first time
such an algorithm was used in the context of pure gauge field theory was
in [ 168 ] . This class of algorithms turned out to be applicable to the
case of dynamical fermions and became the standard method for this type
of systems. Closely related to this line of thinking is the idea of
stochastic quantization [ 169 ] .

In order to simulate the pure gauge action ( 69 ) using some classical
Hamiltonian formalism, consider the partition function ( 41 ) for the
random ensemble ( 48 ), applied to quenched action,

  -- -------- --
     @xmath   
  -- -------- --

Inserting a unit Gaussian integration with a field @xmath carrying the
same indices as @xmath into the partition function introduces an overall
constant which does not change observables,

  -- -------- -- -------
     @xmath      (152)
  -- -------- -- -------

where @xmath is given by

  -- -------- -- -------
     @xmath      (153)
  -- -------- -- -------

The phase space has been enlarged by the introduction of the new fields.
Now the idea of the molecular dynamics methods is to simulate a
classical system, interpreting the function @xmath in Eq. ( 153 ) as the
corresponding Hamiltonian and thus the new fields @xmath as the
canonical conjugate momenta of @xmath . This method, however, will only
simulate the microcanonical ensemble with the fixed “energy” @xmath .
Since the microcanonical ensemble can be used as an approximation to the
canonical ensemble as one approaches the thermodynamic limit, one can
take the samples from sufficiently long classical trajectories for very
large lattices to compute observables.

Since the canonical momenta appearing in ( 153 ) have a Gaussian
distribution independent of the fields, one can extend the algorithm by
not only considering a single classical trajectory, but several of them,
all starting with Gaussian distributed initial momenta. This would
clearly solve the problem of lacking ergodicity of the purely
microcanonical approach. If the momenta are refreshed regularly during
the molecular dynamics evolution, one arrives at the Langevin algorithms
[ 170 ] . The extreme case is to refresh the momenta at each step which
would imply that one performs a random walk in phase space. The other
extreme is the purely molecular dynamics evolution which moves fastest
without ever changing direction by refreshing the momenta, but lacking
ergodicity. A combination of both approaches are the hybrid classical
Langevin algorithms [ 171 ] , where at each step a random decision takes
place whether to reshuffle the momenta or not.

The culmination point of the molecular dynamics algorithms is the hybrid
Monte-Carlo algorithm (see for the foundations [ 172 , 173 ] , for a
more detailed discussion [ 174 ] and for recent reviews [ 175 , 133 ] ).
The idea is again to simulate the classical equations of motion along a
trajectory of a certain length; this is easily achieved by integrating
the canonical equations of motion, Eq. ( 2 ),

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (154)
  -- -------- -------- -------- -- -------

The integration of the equations of motion can be done with various
algorithms available for molecular dynamics. Of particular interest are
the symplectic integration schemes, see e.g. [ 176 , 177 ] for an
introduction. A scheme which is of second order and which requires only
a single force evaluation per step is the leap-frog integration scheme.
The integration of the equations of motion proceeds with a finite step
length, @xmath . The leap-frog method has a systematic error or the
order of @xmath , so the actual trajectory in the simulation may differ
from the exact solution of ( 11.2 ). This deviation can be corrected for
by a global Metropolis step similar to Eq. ( 131 ), but with the action
@xmath replaced by the “Hamiltonian” @xmath . The integration of ( 11.2
) is done for a certain number of steps, @xmath , which is thus the
length of an HMC trajectory. After the Metropolis decision has taken
place, a new set of Gaussian random “momenta” is shuffled and the whole
integration is started again.

A crucial point for the application of the molecular dynamics evolution
is the reversibility of the trajectory, i.e. replacing @xmath by @xmath
should return to the system to exactly the same point in parameter
space, where it has started. This condition is necessary for detailed
balance, Eq. ( 96 ) to hold.

The generalization to fermionic field theory was suggested in [ 178 ] .
It proceeds by considering the partition function

  -- -------- -- -------
     @xmath      (155)
  -- -------- -- -------

where the “Hamiltonian” @xmath is now given by

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (156)
                                @xmath   
  -- -------- -------- -------- -------- -------

The explicit form of the resulting equations of motion can be found
e.g. in [ 137 , 133 ] .

By adjusting the step length, @xmath , and the trajectory length, @xmath
between two Metropolis decisions, one can tune the acceptance rate and
optimize the algorithm to achieve best performance. In general, the
larger the total trajectory length, @xmath , the lower the acceptance
rate, since a longer trajectory introduces larger numerical errors. This
can be compensated by making @xmath smaller (and consequently @xmath
larger), but this will increase the required computer time per
trajectory by the same factor.

The suggestion by Creutz [ 179 , 180 ] was to choose @xmath and modify
the two parameters such that the acceptance rate is about @xmath . This
proposal has been tested numerically in [ 181 ] . A different
investigation has been performed in the case of compact QED by Arnold in
[ 151 ] . Of particular interest is also the impact of @xmath -bit
precision on the feasibility of the algorithm. As has been shown in [
182 , 183 ] , the systematic error (i.e. the non-reversibility of the
HMC trajectory) in case of QCD with two dynamical fermion flavors is of
the order of @xmath on a @xmath lattice. This error should in any case
be small compared to the statistical error of the quantities under
consideration.

In conclusion, the advantages of the HMC are that it has only two
parameters namely @xmath and @xmath , that its optimization and tuning
is well understood and under control, and that it is rather simple to
implement even for more complicated systems. In direct comparison to the
local algorithms, it is, however, less efficient. This is related to the
fact that a local sweep changes each variable by a greater amount than a
global sweep, while it may still have a similar computational cost. In
the quenched case of QCD it soon became clear that algorithms like the
HMC are not competitive with heatbath algorithms, in particular if they
are used together with overrelaxation techniques, see for a recent
algorithmic review e.g. [ 184 ] .

There exist extensions of molecular dynamics-based algorithms which
allow to handle also odd numbers of dynamical quark flavors. One method
is the @xmath -algorithm, which has a residual systematic error which
has to be kept smaller than the statistical error of observables [ 185 ]
. A method which is free of systematic errors has been proposed by
Lippert in [ 186 ] , but its efficiency may be rather limited due to the
presence of a nested iteration ⁵ ⁵ 5 It is possible, that the
quadratically optimized polynomials discussed in Sec. 12.1 are able to
handle this iteration in an efficient way . A different approach has
been suggested in [ 187 ] and exploited in [ 188 , 189 ] .

One potential problem of the HMC scheme is related to the question of
ergodicity. Although the algorithm is exact and ergodic in the
asymptotic limit, for finite time series it may get “stuck” in certain
topological sectors. In particular, in a study of dense adjoint matter,
it has been shown in [ 190 ] that the HMC method is not ergodic, while
the MB algorithm retains ergodicity. This question has also been raised
by Frezzotti and Jansen [ 191 ] who introduced a variant of the HMC
algorithm [ 192 , 193 ] , using a static polynomial inversion similar to
those discussed in Sec. 12.1 (see also [ 187 ] ). For a recent
comparison of efficiencies of current algorithms see [ 194 ] .

#### 11.3 Multiboson Algorithms

As discussed in the previous subsection, the standard HMC allows to
simulate the situation with an even number of degenerate, dynamical
fermion flavors, at the expense of having a global algorithm.
Furthermore, since ergodicity is only ensured in an asymptotical sense,
one may ask whether it is possible to use a different approach for the
same problem. As has been shown by Lüscher [ 166 ] , it is possible to
rewrite the action ( 91 ) in such a way that a purely local action is
obtained which can be treated by more efficient algorithms like local
heatbath and overrelaxation. The algorithms based on this idea are
called multiboson algorithms (MB). For an overview of recent
investigations consult [ 195 ] . For theoretical estimates of efficiency
especially compared to the HMC consider [ 196 ] .

Consider a similarity transformation Eq. ( 86 ), but applied to the
non-Hermitian Wilson matrix @xmath . The resulting diagonal matrix,
@xmath will have all eigenvalues of @xmath , @xmath , on its diagonal.
Then consider a polynomial of order @xmath ,

  -- -------- -- -------
     @xmath      (157)
  -- -------- -- -------

which approximates the function @xmath over the whole spectrum of @xmath
with a certain accuracy, @xmath . Applying this polynomial to the matrix
@xmath will yield an approximation to @xmath as can be seen by applying
( 157 ) to the diagonal matrix from Eq. ( 86 ), since the resulting
matrix will have the inverse eigenvalues, @xmath , on its diagonal. This
allows the fermionic action to be rewritten:

  -- -------- -- -------
     @xmath      (158)
  -- -------- -- -------

where the @xmath are the roots of the @xmath . The determinant is then
computed via

  -- -------- --
     @xmath   
  -- -------- --

This action has the form of Eq. ( 263 ) and thus can be treated by local
heatbath and overrelaxation techniques, as they are discussed in Sec. 10
. The system now incorporates the gauge fields @xmath as before, but in
addition also @xmath scalar fields @xmath since the polynomial has
@xmath roots and each field has the same indices as a Dirac spinor times
the Yang-Mills group number @xmath . In the following these fields will
be referred to as “boson fields”. Hence, it is apparent that the system
of ( 158 ) has both a huge memory consumption and may have a relatively
complicated phase space. In any case, one will have to deal with @xmath
additional fields and the computational effort will still be enormous.

The central question now regards the optimal choice of the polynomial.
Clearly, its order @xmath should be kept as small as possible while
still maintaining a sufficiently good approximation. In any case, the
polynomial approximation in ( 157 ) is a static inversion (cf. Sec. 12
). This means that once the choice has been fixed, one cannot alter the
polynomial during the sampling process anymore. For an overview of the
choices available, see Sec. 12.1 .

##### 11.3.1 Even-Odd Preconditioning for MB Algorithms

It is possible to incorporate the preconditioning technique introduced
in Sec. 6.4.1 to the multiboson approximation ( 158 ). However, since
the matrix in ( 90 ) contains next-to-nearest neighbor interactions, the
square in ( 158 ) would introduce an even more complicated action which
may have up to fourth-neighbor terms and thence would be almost
impossible to implement:

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (159)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

This problem has been solved in [ 197 ] by applying the Schur
decomposition from Eq. ( 89 ) again to the preconditioned action:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denotes the projector on “odd” sites ( @xmath , which
contains @xmath on the first half diagonal and @xmath on the second
half). The resulting preconditioned action is then given by

  -- -------- -- -------
     @xmath      (160)
  -- -------- -- -------

This is the action which will be considered from here on.

##### 11.3.2 Exact Multiboson Algorithms

The multiboson algorithm as discussed so far only uses an approximate
polynomial with a residual error @xmath . One could decide to stay with
this error and try to minimize it by increasing the order of the
polynomial @xmath . But this would indeed be a bad idea since the
computer time and memory requirement would become enormous. Thus,
different proposals have been made to get rid of the residual error. The
original proposal [ 166 ] was to generate a sample of configurations
using the action ( 158 ) as an approximation to the “real” action in the
sense of ( 51 ). Then one performs a reweighting of the observables
using ( 52 ). This procedure is free of systematic errors but it may
introduce additional noise in the measurement of observables if the
initial approximation of ( 157 ) is bad. Therefore, this approach has
been abandoned in practical simulations.

The method which is used in current simulations is to apply a Metropolis
step ( 131 ) after a set of local sweeps [ 198 , 199 , 200 , 201 ] . In
this way the algorithm is free of any systematic error provided the
correction factor is computed with sufficient accuracy. The exact
acceptance probability is given by

  -- -------- -- -------
     @xmath      (161)
  -- -------- -- -------

with @xmath being the gauge field configuration after the local update
sweeps and @xmath being the gauge field configuration prior to the
sweeps.

Still the problem remains to actually compute the ratio of the
determinants in ( 161 ). The straightforward evaluation with a noisy
estimate vector @xmath using a global heatbath as discussed in Sec. 10.2
will result in a nested iteration of an inversion algorithm and the
polynomial @xmath . In this sense, the polynomial will act as a
preconditioner.

Another approach has been suggested in [ 198 ] : One can obtain an
estimate to the determinants by computing the low-lying eigenvalues for
which the chosen polynomial was only a bad approximation. This allows to
compute the correction factor directly. For the smallest @xmath
eigenvalues @xmath , @xmath , this yields

  -- -------- -- -------
     @xmath      (162)
  -- -------- -- -------

This approximation is reasonable if the approximation @xmath is
inaccurate only for small @xmath . Nonetheless there is no way to limit
the systematic error if one doesn’t want to determine @xmath
dynamically. Furthermore, this approach can be expected to scale badly
with the volume since the eigenvalue density is proportional to volume
@xmath and the total effort will at best scale as @xmath .

For a discussion of the effect of the polynomial quality on the
acceptance factor, see [ 201 ] .

Another suggestion lies at at the basis of the Two-Step Multiboson
(TSMB) algorithm proposed by Montvay in [ 202 ] . This is discussed
below.

##### 11.3.3 Non-Hermitian Variant

One can also use the non-Hermitian Wilson matrix, @xmath , instead of
@xmath for the construction of the polynomial approximation. In this
case, the action ( 158 ) takes on the following form:

  -- -------- -- -------
     @xmath      (163)
  -- -------- -- -------

This suggestion has first been put forward by Boriçi and de Forcrand in
[ 203 ] . It is directly applicable to the case of an even number of
mass-degenerate fermion flavors, just like the HMC. However, the
approximation ( 157 ) fails once a real eigenvalue gets negative. This
problem is avoided as long as the fermion masses are still large. It is
unclear, however, what will happen if the masses get small enough, so
that fluctuations may eventually cause the smallest real eigenvalue to
cross the imaginary axis.

Since the effort of inverting the non-Hermitian matrix is lower than in
the Hermitian case, the algorithm is in principle more efficient,
whenever the aforementioned problem is avoided.

It is important to realize that also an algorithm based on the expansion
( 163 ) will be “exact” even if a real eigenvalue gets negative,
whenever it uses a correction step as discussed above. The correction
step will correct any errors in the polynomial approximation. However,
the algorithm may become inefficient since the acceptance rate would
drop almost to zero once a point in phase space is reached where the
approximation becomes invalid.

##### 11.3.4 TSMB Variant

An extension of multiboson algorithms which allows to handle situations
with an arbitrary number of fermion flavors has been suggested by
Montvay in [ 202 ] . In particular, supersymmetric Yang-Mills theory on
the lattice has been examined (see for early reviews [ 204 , 205 ] ).
For the physical results consult [ 206 , 207 , 208 , 209 , 210 , 211 ] .

This approach can immediately be generalized to the case of an arbitrary
number of dynamical fermions, in particular the physically interesting
case (cf. Sec. 5.3 ) with three dynamical quark flavors [ 212 ] ; this
is done by choosing a polynomial @xmath (the reason for calling the
polynomial order @xmath instead of @xmath will become clear soon) which
approximates @xmath , where @xmath is allowed. For @xmath one generally
requires larger order @xmath to achieve the same accuracy while for
@xmath one gets along with smaller @xmath . The value of @xmath
determines the number of dynamical fermion flavors via @xmath since the
polynomial is still applied to @xmath . Thence, for gluinos one has to
choose @xmath leading to @xmath [ 202 ] . The case of three dynamical
fermion flavors, as discussed in Chapter Advanced Algorithms for the
Simulation of Gauge Theories with Dynamical Fermionic Degrees of Freedom
, requires the choice @xmath .

The central idea regards the computation of the correction factor ( 161
). The generalized correction factor for @xmath takes the form:

  -- -------- -- -------
     @xmath      (164)
  -- -------- -- -------

The evaluation with a noisy estimate is highly difficult since now a
(possibly non-integer) power of the matrix @xmath will have to be
inverted. The idea of Ref. [ 202 ] was to employ the multicanonical
sampling (cf. Sec. 3.4 ) to get an approximate action

  -- -------- -- -------
     @xmath      (165)
  -- -------- -- -------

where the polynomial @xmath satisfies

  -- -------- --
     @xmath   
  -- -------- --

This can be achieved by replacing the TSMB noisy correction step ( 164 )
by

  -- -------- -- -------
     @xmath      (166)
  -- -------- -- -------

In order to compute this ratio using a noisy correction vector, one uses
the sampling prescription as discussed in Sec. 11.1 . This requires the
application of a global heatbath, as aforementioned in Sec. 10.2.2 ,
which is very expensive since it would again require a nested inversion
for the polynomial @xmath . Therefore, the suggestion of [ 202 ] was to
use a third polynomial @xmath with order @xmath which approximates the
inverse square root of @xmath ,

  -- -------- -- -------
     @xmath      (167)
  -- -------- -- -------

When applied to a matrix, one obtains

  -- -------- -- -------
     @xmath      (168)
  -- -------- -- -------

The reason for this procedure becomes clear, if one evaluates ( 166 )
using a noisy estimate. In practice, a single noisy vector is usually
sufficient [ 202 ] . Then the acceptance probability becomes

  -- -------- -- -------
     @xmath      (169)
  -- -------- -- -------

with @xmath being a random Gaussian vector with unit width.

The approximation of @xmath in ( 167 ) determines the total residual
error of the algorithm. There is no way to correct for this error after
the sampling has taken place since the error appears in the correction
step and cannot be rewritten as an extra term in the action. It is of
vital importance to keep this influence small. A precise investigation
of the effects associated with this residual error can be found in Sec.
14.1 .

After @xmath has been chosen sufficiently large, the total systematic
error is governed by the second polynomial, @xmath . This systematic
error, however, is present in the action ( 165 ) and can therefore be
corrected by the measurement correction, Eq. ( 52 ). As shown in [ 213 ]
, this can be done by considering yet a further polynomial, @xmath ,
defined by

  -- -------- -- -------
     @xmath      (170)
  -- -------- -- -------

The calculation of the expectation value of an operator @xmath then
proceeds by applying Eq. ( 52 ):

  -- -- -- -------
           (171)
  -- -- -- -------

with

  -- -------- --
     @xmath   
  -- -------- --

The interval of the polynomial approximation for @xmath must be
sufficiently large to cover the entire eigenvalue spectrum of @xmath for
all gauge fields in the sample. Since this may be problematic if
exceptional configurations with extremely small eigenvalues are present,
one can combine the noisy estimation of the correction factor in ( 171 )
with an exact computation of the corresponding factor for the smallest
eigenvalues (see [ 213 ] ):

  -- -------- -- -------
     @xmath      (172)
  -- -------- -- -------

with @xmath being the @xmath th eigenvalue of the matrix @xmath .

This procedure can also act as a preconditioner to the computation of
@xmath . The accuracy of @xmath can be adjusted until the correction
factor has converged. However, as will be shown in Sec. 14.3 , it is in
general not necessary to compute both the smallest eigenvalues and the
correction factor using the sampling in ( 171 ). Since the eigenvalue
approximation of the quadratically optimized polynomials converges
extremely fast, it is sufficient to approximate the correction factor
using the smallest eigenvalues only.

### 12 Matrix Inversion Algorithms

For the computation of the polynomials and the global heatbath in the
previous sections, an inversion of the fermion matrix is required. The
problem is to find a solution vector @xmath which solves the equation

  -- -------- -- -------
     @xmath      (173)
  -- -------- -- -------

for a given matrix @xmath and a given vector @xmath . The numerical
effort of this problem depends cubically on the size of the matrix [ 45
] and monotonically on the condition number (see Sec. 13 ). If the
inverse condition number is of the order of or smaller than the machine
precision, the matrix is said to be “ill-conditioned”, because the
algorithms will in general be unable to yield a stable solution,
although the matrix entries may not pose any direct problem themselves.
The aim of preconditioning techniques is thus to reduce the condition
number of the matrix @xmath without altering the solution. Often
techniques like those discussed in Sec. 6.4.1 also go by the name
“preconditioner”, although the even-odd preconditioned matrix is
different from the original one.

For the case under consideration in the thesis, inversion of the lattice
Dirac matrices ( 158 ) or ( 160 ) is required. These matrices typically
have sizes of the order @xmath which for a lattice of size @xmath is
@xmath . Storage of the complete matrix would thus require about @xmath
TBytes and is out of reach for current computer technology.
Consequently, for the inversion of @xmath only an iterative solver may
be considered. These solvers do not require the whole matrix to be
stored in memory, but rather require the presence of a matrix-vector
multiplication. This step typically consumes most of the computer time
of the algorithm.

From the repeated application of the matrix-vector multiplication, an
approximation @xmath of order @xmath to the solution vector @xmath is
generated. Thus, these algorithms apply a polynomial @xmath of order
@xmath with the matrix @xmath as its argument to the starting vector
@xmath yielding the solution vector:

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (174)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

where the order of the polynomial is given by @xmath (which is thus the
number of iterations required).

Sometimes the iteration prescription can be cast in the form

  -- -------- -- -------
     @xmath      (175)
  -- -------- -- -------

where the matrix @xmath and the vector @xmath are independent of the
iteration number @xmath . Such methods are called “stationary”. The
Jacobi method , the Gauss-Seidel method and the (S)SOR methods are
examples of such cases (cf. [ 214 , 45 ] ).

A measure of the quality of the approximation in equation ( 174 ) is
given by the norm of the residual vector

  -- -------- -- -------
     @xmath      (176)
  -- -------- -- -------

where @xmath is defined to be

  -- -------- -- -------
     @xmath      (177)
  -- -------- -- -------

which should converge to zero as @xmath approaches infinity. In some
cases the exact solution is already found after a finite number of
steps. In most practical situations, however, the exact solution cannot
be found due to the limited accuracy of the machines and one is
interested only in finding the solution in as few steps @xmath as
possible up to a certain accuracy @xmath .

The solver determines the coefficients @xmath of the polynomial @xmath
in Eq. ( 174 ) or, in some cases, the recurrence coefficients of a
recurrence relation. The algorithms may be divided into two classes:

-   The coefficients of the polynomial are fixed prior to the iteration
    and do not depend on the shape of the matrix @xmath . This does not
    allow to exploit any knowledge gained by the algorithm during the
    iteration process and it does not allow to compensate for any
    rounding errors. Rather, the rounding errors will usually add up
    causing the iteration to saturate at some point where further
    iterations do not increase the accuracy of the solution. This class
    of solvers is called non-adaptive and is of great importance for
    multiboson algorithms; generally they are important in those cases
    where an approximate inverse is required with a fixed series of
    coefficients. This is the case e.g. for reweighting purposes.

-   The coefficients are determined dynamically during the iteration
    itself. Thus, the solver may adapt to the specific form of the
    matrix @xmath . These algorithms are called adaptive solvers and are
    in general superior to the non-adaptive algorithms in terms of
    required matrix-vector operations. Furthermore they are able to
    compensate better for rounding errors so the accuracy which may be
    achieved is higher than for non-adaptive ones. The reaction on
    ill-conditioned matrices is consequently improved as well. These
    algorithms are the method of choice if the inverse up to a fixed
    accuracy is required.

For a complete overview of iterative solvers consult [ 45 , 214 ] . The
algorithms which have been employed in this thesis are discussed in the
following sections; all of them are efficiently parallelizable both on
MIMD (Multiple Instruction, Multiple Data) and on SIMD (Single
Instruction, Multiple Data) machines. For an explanation of the
architectures see e.g. [ 215 ] .

#### 12.1 Static Polynomial Inversion

The choice of the polynomial @xmath in Eq. ( 157 ) is crucial for the
applicability of multiboson algorithms. The construction of any
polynomial requires one to know at least the condition number of the
Wilson matrix. Usually more information is available regarding the
spectrum, cf. Sec. 6.4 , and also the spectral density plots in Sec. 14
. The original proposal of Lüscher [ 166 ] is to use an approximation
build from Chebyshev polynomials [ 45 ] . This approximation does not
take care of the peculiarities of the Wilson matrix and thus this choice
is not the optimal one. It is, however, a safe method which is
applicable to any fermion representation if only the condition number is
known.

##### 12.1.1 Quadratically Optimized Polynomials

The quadratically optimized polynomials have been introduced by Montvay
[ 202 ] . For a thorough discussion and comparison to the Chebyshev
polynomials see [ 216 ] and for further technical details [ 217 , 218 ]
. The basic idea is to find the polynomial @xmath which approximates a
function @xmath (with @xmath , cf. Sec. 11.3 ) in a given interval
@xmath in such a way that the relative deviation norm @xmath defined via

  -- -------- -- -------
     @xmath      (178)
  -- -------- -- -------

is minimized. If @xmath is expanded in coefficient form,

  -- -------- --
     @xmath   
  -- -------- --

the coefficients @xmath of the polynomial minimizing ( 178 ) are given
by [ 216 ]

  -- -------- -- -------
     @xmath      (179)
  -- -------- -- -------

with

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

A straightforward computation of the @xmath in terms of the expansion
coefficients ( 179 ) is not practical, however. The coefficients will
soon become arbitrarily large and the computation of larger polynomial
orders is not feasible anymore, since then typically orders of @xmath
are required. Fortunately, the polynomials can be computed in terms of a
recurrence relation which is stable even for orders of @xmath and
beyond, at least if @xmath -bit precision is used.

Take a set of polynomials @xmath (e.g. Jacobi polynomials are possible
choices [ 217 ] ) satisfying the orthogonality relation

  -- -------- -- -------
     @xmath      (180)
  -- -------- -- -------

The weight function @xmath can be chosen. Then @xmath can be expanded in
terms of the @xmath with coefficients @xmath ,

  -- -------- -- -------
     @xmath      (181)
  -- -------- -- -------

The coefficients @xmath are given by

  -- -------- -- -------
     @xmath      (182)
  -- -------- -- -------

The polynomials @xmath can be constructed by the three-term recurrence
relation (see [ 216 , 219 ] )

  -- -------- -- -------
     @xmath      (183)
  -- -------- -- -------

with

  -- -------- -- -------
     @xmath      (184)
  -- -------- -- -------

The factors @xmath are given by

  -- -------- --
     @xmath   
  -- -------- --

The advantages of the quadratically optimized polynomials are that they
only require the knowledge of the eigenvalue interval @xmath of the
matrices whose inverse one is interested in. They provide a very good
approximation which is worse at the lower end of the interval where the
eigenvalue density is decreasing, cf. Sec. 14 . Furthermore, the
quadratically optimized polynomials give a very simple way to control
the number of dynamical fermions to be simulated by a multiboson
algorithm. This can directly be done by by adjusting the value of @xmath
. Of great value is also the fact that they are very stable even for
large orders. Finally, they can efficiently be implemented on parallel
computers since they only require matrix-vector-multiplications and
vector-vector-additions.

The disadvantage is that they may not take into account all information
which is available about the matrix under consideration. In particular,
the eigenvalue density is also decreasing on the upper end of the
interval, although the quadratically optimized polynomials have good
accuracy at this point. In this sense, one might hope to achieve better
results by modifying the weight function @xmath . This still leaves room
for further improvement in the future.

##### 12.1.2 Ultra-Violet Filtering

An important preconditioning technique which has been introduced to the
field of multiboson algorithms by de Forcrand [ 220 ] is known as
UV-filtering . It makes use of the identity

  -- -------- --
     @xmath   
  -- -------- --

so that

  -- -- -- -------
           (185)
  -- -- -- -------

The order @xmath of the hopping parameter expansion can be adjusted to
minimize the total effort. The effect of UV-filtering on the order of
the polynomial approximation has been examined in [ 220 ] and shown to
be superior to standard HMC in [ 221 , 222 ] . It turns out that the
order @xmath can be reduced by a factor of about two.

In order to find the polynomial @xmath one applies an adaptive inverter
(Ref. [ 201 ] uses the GMRES method for this purpose) to a thermalized
gauge field configuration. The polynomial will then approximate

  -- -------- -- -------
     @xmath      (186)
  -- -------- -- -------

However, for larger orders @xmath , the iterations used to fix the
coefficients of the polynomial become numerically unstable. This is the
reason why one needs the recursion form of the quadratically optimized
polynomials. The instability will thus limit the applicability of the
expansion ( 185 ).

Concluding, UV-filtering is a highly effective way to reduce the order
of the polynomial and thus to improve the algorithm to a large extend.
On the other hand, one needs a thermalized configuration (or even
several of them) at the physical point one is interested in. In this
respect, the method to fix the polynomial @xmath discussed in [ 222 ]
will only become optimal after a certain run-time once thermalization is
achieved.

#### 12.2 Conjugate-Gradient Iteration

The simplest adaptive iterative inverter is the Conjugate Gradient (CG)
scheme, see e.g. [ 214 ] for a reference implementation. It is also the
oldest and best-known method for this problem. It requires that the
matrix @xmath is Hermitian and positive definite. The idea is to
minimize the function

  -- -------- -- -------
     @xmath      (187)
  -- -------- -- -------

This function is minimized when the gradient

  -- -------- --
     @xmath   
  -- -------- --

vanishes which is simply equivalent to Eq. ( 173 ). The iteration
prescription is to choose orthogonal search directions @xmath and
minimize the function ( 187 ) along this direction in any iteration
step:

  -- -------- -- -------
     @xmath      (188)
  -- -------- -- -------

Correspondingly, the residuals @xmath are updated as

  -- -------- -- -------
     @xmath      (189)
  -- -------- -- -------

The coefficients @xmath are computed as to minimize the function

  -- -------- --
     @xmath   
  -- -------- --

at each iteration step. Note that the existence of this minimum requires
@xmath to be positively definite — this is the reason why the CG
algorithm only works for positively definite matrices. The minimization
is performed by choosing

  -- -------- --
     @xmath   
  -- -------- --

with @xmath denoting the following norm of a vector @xmath :

  -- -------- --
     @xmath   
  -- -------- --

The search directions are iterated via

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (190)
  -- -------- -------- -------- -- -------

This choice of @xmath makes it possible that @xmath is orthogonal to all
previous @xmath and that @xmath is orthogonal to all previous @xmath (
@xmath ) (cf. [ 214 ] ). This is also the reason why the algorithm is
called CG, since it generates a series of orthogonal (or “conjugate”)
vectors. The iterate @xmath is chosen from the @xmath -dimensional
subspace spanned by these vectors which is known as the “Krylov”
subspace @xmath

  -- -------- -- -------
     @xmath      (191)
  -- -------- -- -------

It can be shown [ 144 , 223 ] that for a Hermitian matrix @xmath an
orthogonal basis for the Krylov subspace can be constructed using only a
three-term recurrence relation. Thus, such a recurrence is also
sufficient for constructing the residuals. In the CG algorithm this
relation is replaced by two two-term recurrences: one for the residuals
@xmath and one for the search direction @xmath .

The starting points of the iterations are chosen to be

  -- -------- --
     @xmath   
  -- -------- --

Of course it is possible to choose a different vector as starting vector
for @xmath , e.g. a good guess if possible or a random vector if all
else fails.

The convergence of CG depends on the distribution of eigenvalues. With
@xmath being the spectral condition number, an upper bound for the
effort can be given [ 144 ] :

  -- -------- --
     @xmath   
  -- -------- --

Thus, the number of iterations to achieve a relative reduction of @xmath
in the error is at most proportional to @xmath . In the case of
well-separated eigenvalues, however, often a better convergence can be
observed. This can be explained by the fact that the CG tends to
optimize the solution in the direction of extremal eigenvalues first,
thereby reducing the effective condition number of the residual
subspace. For a discussion cf. [ 224 ] .

This method can also be extended to the case of non-Hermitian matrices:
if Eq. ( 173 ) is multiplied from the left by the conjugate matrix
@xmath , the resulting equation becomes

  -- -------- -- -------
     @xmath      (192)
  -- -------- -- -------

In this form the iteration is done using the new matrix @xmath . Thence,
this method requires two matrix multiplications per iteration. But the
situation is even worse: Since the new matrix @xmath has a condition
number @xmath exponentially larger than @xmath , the number of
iterations required is increased by a factor of @xmath . Consequently,
the CG algorithm is much worse for these applications and should only be
considered as a last resort if other methods fail.

#### 12.3 GMRES Algorithm

In the case of a non-Hermitian matrix @xmath an orthogonal basis of the
Krylov space can no longer be constructed by a recurrence relation among
the residues @xmath . Thus, the whole space has to be orthogonalized;
this can be done using the Gram-Schmidt construction:

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (193)
  -- -------- -------- -------- -- -------

From the orthogonal basis of the Krylov space

  -- -------- --
     @xmath   
  -- -------- --

the iterate @xmath can be constructed via

  -- -------- -- -------
     @xmath      (194)
  -- -------- -- -------

where the coefficients minimize the residual norm

  -- -------- -- -------
     @xmath      (195)
  -- -------- -- -------

This method is known as the “Arnoldi method” [ 225 ] . Thus, the
Generalized Minimal Residual (GMRES) algorithm minimizes the function (
195 ) instead of ( 187 ) in case of the CG iteration.

The advantages of this method are that it can be used to minimize
non-Hermitian functions and that the residual norms @xmath can be
determined without computing the iterates @xmath . The major
disadvantage is its huge memory consumption if the iteration number
@xmath and the problem size @xmath are large. Although this method
converges exactly in @xmath steps, this point is out of reach in the
cases of interest in this thesis. Hence, only iterates up to a certain
order @xmath can be formed. In case higher accuracy is required, the
method should be restarted several times discarding the previous Krylov
subspace. Furthermore, the convergence properties can be improved by
replacing the Gram-Schmidt orthogonalization by the Householder method.
Thus, greater computer time consumption can be traded for higher
stability.

This method has been proposed in [ 220 ] for the generation of the
polynomial @xmath to be used in multiboson algorithms, Eq. ( 157 ).

#### 12.4 Stabilized Bi-Conjugate Gradient Algorithm

The Bi-Conjugate Gradient (Bi-CG) method is an extension to the CG
algorithm which is also applicable to non-Hermitian matrices. Unlike the
proposal in Eq. ( 192 ) it does not square the original matrix and thus
does not worsen the condition number. Instead it requires the
computation of the Hermitian conjugate matrix @xmath to a conjugate set
of residual and direction vectors, doubling the memory requirements of
the CG algorithm.

The updating prescription for the residuals then becomes

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (196)
  -- -------- -------- -------- -- -------

while for the search directions one gets

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (197)
  -- -------- -------- -------- -- -------

Now the choices

  -- -------- --
     @xmath   
  -- -------- --

enforce the bi-orthogonality relations

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

This method allows inversion of non-Hermitian matrices but does not show
a stable convergence pattern in all cases. It may converge irregularly
or even fail completely. Therefore several modifications have been
proposed to make the convergence smoother (for an overview see [ 214 ]
). The method known as Stabilized Bi-Conjugate Gradient (Bi-CGStab) as
introduced by van der Vorst in [ 226 ] does not require the Hermitian
conjugate matrix to be used, but has an overall cost similar to the BiCG
method just discussed.

### 13 Eigenvalue Algorithms

An important ingredient of the application of multi-boson algorithms as
described in this thesis is the knowledge of how to tune the polynomials
to the eigenvalue spectrum of the matrix. Thus, it is of great
importance to have methods available to correctly compute at least the
borders of the eigenvalue spectrum. Another possible application is the
preconditioning of the matrix to make evaluation of observables more
simple. This approach has only been used in the measurement of the
correction factor ( 162 ) in this thesis. But a different application
also covers the measurement of other observables as discussed in Sec. 9
. This approach has been examined in [ 139 ] .

The matrix @xmath is said to have an eigenvector @xmath with
corresponding eigenvalue @xmath iff

  -- -------- -- -------
     @xmath      (198)
  -- -------- -- -------

A necessary condition for ( 198 ) is

  -- -------- -- -------
     @xmath      (199)
  -- -------- -- -------

which translates to a polynomial of degree @xmath which has exactly
@xmath complex roots. These roots need not be distinct. Furthermore,
Eq. ( 199 ) implies that to every eigenvalue @xmath there corresponds an
eigenvector since the matrix @xmath is singular and thus has a kernel
with dimension @xmath [ 45 ] . One important property is that the
eigenvalues may be shifted by some constant @xmath by adding @xmath to
both sides of Eq. ( 198 ).

Some matrices fulfill the normality condition ⁶ ⁶ 6 As discussed in Sec.
6.4 , the Wilson matrix does not share this property

  -- -------- -- -------
     @xmath      (200)
  -- -------- -- -------

The eigenvectors of a matrix fulfilling Eq. ( 200 ) span the whole
vector space @xmath . Applying Gram-Schmidt orthogonalization to this
set of eigenvalues yields an orthonormal basis and thus a matrix
fulfilling the unitarity condition

  -- -------- -- -------
     @xmath      (201)
  -- -------- -- -------

Although an arbitrary matrix has exactly @xmath eigenvalues and
consequently @xmath eigenvectors, these eigenvectors do not necessarily
span the whole vector space @xmath . In such a case the matrix is said
to be defective .

The order- @xmath Krylov subspace of a matrix @xmath on a certain
starting vector @xmath defined in Sec. 12.2 in Eq. ( 191 ) can be used
for this purpose in the following way: Given the eigenvectors of @xmath
, @xmath , which span a space of dimension @xmath , then choosing a
vector

  -- -------- --
     @xmath   
  -- -------- --

will result in a Krylov space whose dimension @xmath can be at most
@xmath . Repeated application of @xmath on the starting vector will
yield for the @xmath th element of the Krylov space

  -- -------- --
     @xmath   
  -- -------- --

This recipe will increase the projection of @xmath on the eigenvector
whose corresponding eigenvalue has the largest magnitude @xmath . Thus,
in the limit @xmath , the iteration will converge to the largest
eigenvector.

For a properly chosen starting vector @xmath which has an overlap will
all eigenvectors, the Krylov iteration will consequently yield the
eigenvector corresponding to the eigenvalue with largest magnitude.
Repeated application of this procedure with orthogonalization of the
starting vector to previously found eigenvectors allows in principle to
restore the complete spectrum.

However, the straightforward application is quite cumbersome. In
practice it has turned out to be more economical to compute the subspace
of several eigenvalues from the border of the spectrum together and
afterwards to determine the largest eigenvector from these iterates. One
of the methods achieving this goal is called Arnoldi iteration [ 144 ] ⁷
⁷ 7 This algorithm is already coded in the ARPACK package and can be
found in
http://www.caam.rice.edu/software/ARPACK/ . If more than a single
eigenvalue/-vector are required, this method is the most efficient way
to determine the spectrum of a matrix.

Once the eigenvalue(s) closest to the origin are known, one can also use
this knowledge to simplify the inversion of a matrix using any of the
algorithms discussed in Sec. 12 . With @xmath eigenvectors @xmath and
their corresponding eigenvalues @xmath known, one can compute

  -- -------- -- -------
     @xmath      (202)
  -- -------- -- -------

and then use @xmath as a starting point for the inversion. The resulting
inverse @xmath is then given by

  -- -------- -- -------
     @xmath      (203)
  -- -------- -- -------

The problem to compute @xmath may now have a significantly reduced
condition number since the @xmath smallest eigenvalues have been
removed. For a highly singular matrix @xmath , the cost to compute the
eigenvalues (which is independent of the condition number) may be lower
than the cost for the complete inversion. However, it has been shown in
[ 139 ] that for the condition numbers used in the SESAM project [ 161 ,
133 , 227 , 148 ] (which are similar or even higher than those
considered in this thesis), this is not yet the case.

## Chapter \thechapter Tuning of Multiboson Algorithms

The main focus of this chapter is the optimization and tuning of
multiboson algorithms with an emphasis on the TSMB algorithm introduced
by Montvay [ 202 ] . The details of the algorithm have been discussed in
Sec. 11.3 . Throughout this chapter, the focus lies mainly on the survey
of QCD with two degenerate, dynamical fermion flavors on various lattice
sizes with fixed physical parameters given in Tab. 3 (for a precise
measurement see [ 148 ] , also cf. [ 228 , 227 ] ). These numbers are an
excerpt from Tab. 17 .

In Sec. 14 , the static aspects of the polynomial approximations are
discussed. The question to be answered is how to choose the
approximation of an inverse power of the Wilson matrix in the most
efficient way if one recourses to a static approximation (cf. Sec. 12.1
).

Section 15 investigates the tuning of the dynamical aspects of
multiboson algorithms. After a detailed presentation of the tools used
for the efficiency analysis in 15.1 , the practical application to an
aspect of major importance, namely the dependence of the performance on
the order @xmath of the polynomial ( 157 ) is investigated in 15.2 . The
results presented here should be independent of the particular
implementation of the algorithm and thus apply to other variants of MB
algorithms apart from TSMB as well. The impact of reweighting is
analyzed in Sec. 15.3 , and finally the updating strategy is discussed
in Sec. 15.4 . The updating strategy consists of the proper combination
of local updating sweeps which make up a single trajectory . A
trajectory is then the logical partition after which an iteration of
update sweeps restarts.

The practical implementations of multiboson algorithms are discussed in
Sec. 16 . The two major platforms, where the multiboson algorithm has
been implemented are compared and performance measurements are
presented.

Section 17 summarizes the results from this chapter.

### 14 Optimizing the Polynomial Approximation

In order to find the required approximations for the TSMB algorithm, one
has to focus first on the behavior of the polynomial approximation in
the static case. This regards the application of the inversion to a
single gauge field configuration with known condition number and
eigenvalue distribution.

In the following, a particular thermalized gauge field configuration at
the physical point given in Tab. 3 on an @xmath lattice will be
considered. The extremal eigenvalues and the condition number of the
Wilson matrix @xmath for this gauge field configuration are given in
Tab. 4 .

A histogram of the lowest @xmath eigenvalues is shown in Fig. 10 .
Figure 11 shows the corresponding histogram of the largest eigenvalues.
As it is evident from these plots, the eigenvalue density is small at
the lower and upper ends of the interval and increases towards the
middle.

#### 14.1 Tuning the Quadratically Optimized Polynomials

The quality of the approximation provided by the polynomial ( 157 ) does
not only depend on its order, but also on the choice of the interval,
where it should approximate the function under consideration. Now the
optimal choice of the approximation interval, @xmath , will be
determined for a quadratically optimized polynomial introduced in Sec.
12.1 . Figure 12 displays the function

  -- -------- --
     @xmath   
  -- -------- --

of a quadratically optimized polynomial with @xmath , @xmath and @xmath
. The quality of the approximation is best at the upper end of the
interval, while already slightly above the upper limit it will soon
become useless. At the lower end of the interval the approximation is
worse, but the limit is not as stringent as in the former case.

These observations fix the strategy for finding the optimal interval:
The upper limit must be chosen very conservatively — large enough that
during the simulation runs an eigenvalue never leaves this interval. In
the following, the choice @xmath will be adopted unless otherwise
stated. The lower end may be chosen more freely, in particular it may be
chosen larger than the smallest eigenvalue since the eigenvalue density
is largest in the middle of the interval. Raising the lower limit will
make the approximation for the smallest eigenvalues worse, but will
increase the quality of the polynomial in the middle, where the majority
of eigenvalues is located.

##### 14.1.1 Measures of Accuracy

To find a measure for the quality of the polynomial approximation for a
particular matrix (in this case the square of the Hermitian Wilson
matrix, @xmath , for the gauge field configuration discussed above), the
following two definitions of matrix norms will be adopted: Consider the
matrix @xmath defined by

  -- -------- -- -------
     @xmath      (204)
  -- -------- -- -------

Then the following two definitions of matrix norms will be used:

1.  Measure the vector norm of @xmath defined by

      -- -------- -- -------
         @xmath      (205)
      -- -------- -- -------

    where @xmath is a Gaussian random vector with unit width. The
    average vector norm @xmath for a sample of @xmath will be denoted by
    @xmath .

2.  Measure the expectation value

      -- -------- -- -------
         @xmath      (206)
      -- -------- -- -------

    where @xmath is again a Gaussian random vector with width one. This
    quantity is not a norm, however. Since it is not positive definite
    the absolute value of @xmath will be used in the following and will
    be denoted by @xmath .

These definitions can also be applied to the case of the inverse square
root defined in Eq. ( 167 ). This is done by replacing @xmath by @xmath
, which is defined by

  -- -------- -- -------
     @xmath      (207)
  -- -------- -- -------

In particular, @xmath is the exponential factor in the noisy correction
step of the TSMB algorithm, Eq. ( 169 ), if the old configuration is
chosen equal to the new one.

When computing the matrix norms of @xmath for too small orders @xmath ,
the fluctuations of the norms will be large. In particular, the matrix
norm ( 206 ) at small @xmath will be close to ( 150 ), i.e. the inverse
of the determinant of @xmath . The opposite limit @xmath will correspond
to the determinant itself. As has been discussed in Sec. 11.1 , the
fluctuations of ( 150 ) are huge, while those of its inverse are small.
Thus, the fluctuations will decrease for increasing values of @xmath .
Therefore, the optimization of the static approximation should be
performed for comparatively large orders.

##### 14.1.2 Fixing the Lower Limit

As has been argued, it is of importance to have a recipe for fixing the
lower limit of a quadratically optimized polynomial for a given order.
First consider the choice @xmath for which the two matrix norms together
with their standard errors are displayed in Fig. 13 for varying values
of @xmath . For each point a sample of @xmath Gaussian vectors has been
considered. While @xmath displays a minimum at the lower end of the
interval (where the smallest eigenvalue is located), @xmath stays more
or less constant over a range of more than one order of magnitude. Thus,
for small orders, one cannot rule out that a choice @xmath is practical.

Next consider the case @xmath which should already provide a very good
approximation to the inverse function. Figure 14 again shows the two
matrix norms for varying values of @xmath . The curve of @xmath clearly
displays a minimum at @xmath , which is about @xmath smaller than @xmath
. The curve of @xmath shows a more or less continuous increase with
larger errors.

Finally the situation regarding the third polynomial must be clarified.
In general, the systematic error of a simulation run should be bounded
to be much smaller than the statistical error of any quantity measured.
The magnitude of the error can be estimated by considering a noisy
estimate for the determinant, Eq. ( 169 ), with the old configuration
being equal to the new one, i.e.

  -- -------- --
     @xmath   
  -- -------- --

If the approximation was exact the acceptance probability would be equal
to one. However, any deviation in the exponential could cause spurious
acceptances or rejections. Since any negative value in the exponential
in ( 169 ) would cause the configuration to be accepted in any case, the
case of large negative values of the exponential factor can be
completely disregarded. On the other hand, for large positive values of
the argument, the influence of any error on the acceptance rate will be
minor due to the flat tail of the exponential function. Thence, the
largest influence is to be expected for values around zero.

To quantify the influence of this systematic error one can consider the
following model for the exponential correction factor:

  -- -- -- -------
           (208)
  -- -- -- -------

The resulting acceptance rate can be computed to yield

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (209)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

Using Eq. ( 209 ), one can compute the actual systematic error by
measuring @xmath , @xmath and @xmath in a given run and considering the
resulting change in acceptance rates

  -- -------- -- -------
     @xmath      (210)
  -- -------- -- -------

The resulting number is the systematic error for a single trajectory. As
a rule of thumb one should not allow @xmath to exceed values of @xmath .
In most situations, however, it is possible to make it as small as
@xmath . Any systematic error will then be negligible.

Figure 15 shows plots of the two norms with @xmath (the value @xmath has
been chosen compatible to the situation discussed above) vs. the lower
limit of the interval, @xmath . Both norms obtain their minimal values
at lower interval limits of @xmath . However, the important norm in this
case is @xmath — already if @xmath is varied by a factor of @xmath , one
approaches a region where the systematic error may become significant.
One has to keep in mind that in a dynamical simulation fluctuations may
cause the smallest eigenvalue to become smaller than in the present
case. Therefore, the interval for the third polynomial has to be chosen
far more conservatively than for the other polynomials and the residual
norm must be adjusted by increasing the order @xmath .

In some cases, there is a different problem related to this strategy,
cf. Sec. 16.3 below.

One can conclude that the choice of the approximation interval for
quadratically optimized polynomials can have a large impact on the
quality of the approximation. While in the case of the first polynomial
(where one deals with a comparatively small order), the choice of the
lower limit only has a small impact, the situation changes as the order
is increased. For orders as large as the second polynomial in the
two-step approximation, the optimal choice for the lower limit is
slightly smaller than the smallest eigenvalue of the matrix, while for
the third polynomial, the choice of the lower limit should be made
extremely conservative. The interval should always cover every single
eigenvalue and ensure that @xmath in Eq. ( 210 ) is sufficiently small
for the algorithm to be free of systematic errors.

In the case of a dynamical simulation, the choice of the interval should
be guided by the average values of the smallest eigenvalue. However,
this information is only rarely available prior to the run. It may
therefore be necessary to readjust the polynomials during a run as more
information becomes available. In this way one can reduce the total
runtime, but at the price of more effort and logistics. Appendix
Advanced Algorithms for the Simulation of Gauge Theories with Dynamical
Fermionic Degrees of Freedom gives a framework for handling this type of
runs.

In any case, one has to make sure that the third polynomial is
sufficiently good by making a very conservative decision regarding the
lower limit and making the order sufficiently large as to keep the
systematic error bounded to at most a few percent.

#### 14.2 Algorithm for Polynomials

As has been discussed in Sec. 11.3 , different methods are possible for
finding the polynomial, Eq. ( 157 ), which approximates the inverse of
the Wilson matrix. The quadratically optimized polynomials do not
require explicit knowledge of the eigenvalue density in the
approximation interval. On the other hand, the method proposed by de
Forcrand in [ 220 ] requires a thermalized gauge field configuration to
be available. As has been noted in the latter publication, taking a
single gauge field configuration may already be sufficient since the
results from several configurations are similar.

Figure 12 already showed the deviation of a quadratically optimized
polynomial. The GMRES algorithm discussed in Sec. 12.3 will now be used
to construct the polynomial dynamically on a (different) thermalized
configuration. The resulting plot of @xmath is displayed in Fig. 16 .
For comparison the quadratically optimized polynomial from Fig. 12 is
also shown.

It is apparent that the quadratically optimized polynomial performs
worse in the middle of the spectrum, where most eigenvalues are located.
In contrast, the GMRES polynomial respects the spectral density of the
Wilson matrix and thus results in a better approximation. The
disadvantage is that the underlying algorithm will become unstable when
computing the coefficients of the polynomials for larger orders if it is
only run on a machine with @xmath -bit precision. This instability will
already become apparent for orders slightly beyond @xmath . A further
problem is that usually one does not have a thermalized gauge field
configuration for a particular physical point prior to the calculation.
It is therefore necessary to perform the optimization process
dynamically during the sampling and readjust the polynomials after a
certain number of trajectories. Similar to the case of quadratically
optimized polynomials, this requires more effort.

The influence of the qualitative difference is displayed in Fig. 17 .
The GMRES results are compared to the quadratically optimized
polynomials for a number of different orders. The polynomial interval
for the quadratically optimized polynomials has been chosen to be @xmath
. The former choice clearly exhibits smaller residuals and is thus
superior. However, since the computation has only been performed with
@xmath -bit precision, the numerical instabilities are already visible
at @xmath .

This scheme can easily be extended to the case of any rational number
@xmath , i.e. any rational number of fermion flavors. However, the
instability of this method will also grow as the number of
multiplications required increases. Therefore, this method has not been
applied in the following. This is a place where further research is in
demand. One way to solve this problem would be to implement the
polynomial algorithm using very high precision arithmetics, similar to
what has been done in [ 229 ] . Another way could consist of using a
different scheme which does not recourse directly to the expansion
coefficients in Krylov space.

#### 14.3 Computing the Reweighting Correction

When using the TSMB algorithm for the correction step as discussed in
Sec. 11.3 , one will perform a noisy estimate for the inverse
determinant using a static inversion algorithm with the polynomial
@xmath . Any residual systematic error in the ensemble of gauge field
configurations generated will then have to be repaired with a
multicanonical reweighting. An observable will be computed using either
( 171 ) or ( 172 ).

In order to find an efficient way to perform this reweighting, it is
assumed that the configuration under consideration has been computed
using the TSMB algorithm with action ( 165 ), where the quadratically
optimized polynomial (cf. Sec. 14.1 ) @xmath has been employed with the
interval @xmath . The overall systematic error from the simulation run
is thus determined from the polynomial @xmath alone. The observable
under consideration is now @xmath , i.e. the correction alone is being
measured. The correction factor from the individual @xmath lowest
eigenvalues of @xmath is plotted in Fig. 18 . Obviously, the correction
is mostly related to the lowest eigenvalue alone ⁸ ⁸ 8 The situation
changes if the GMRES polynomials had been used since they also perform a
worse approximation on the upper end of the interval, cf. Fig. 16 .

This finding is confirmed when examining the convergence behavior of the
correction factor with respect to the number of eigenvalues computed.
The cumulative factor in Eq. ( 172 ) as a function of the number of
eigenvalues taken into account is shown in Fig. 19 and gives an average
value of @xmath . Although larger eigenvalues still introduce
fluctuations, the major impact comes from the smallest eigenvalue alone.

The alternative way to compute the correction factor is provided by the
evaluation of ( 171 ). From @xmath noisy vectors one observes that the
approximation already has converged at order @xmath . The total
correction factor from this method is

  -- -------- --
     @xmath   
  -- -------- --

which is completely consistent with the value obtained from Fig. 19 .

In conclusion, when estimating the correction factor on the basis of
eigenvectors on an @xmath lattice alone, it makes sense to use only a
small fraction (definitely less than @xmath ) of the lowest eigenvalues.
The fluctuations introduced from the larger eigenvalues do not have a
significant influence on the total result. The evaluation of the
correction factor using a fourth polynomial is a practical alternative
and avoids having to compute a fraction of all eigenvalues. The only
potential problem is that the smallest eigenvalue of @xmath may lie
outside the interval of @xmath , which could result in incorrect
results. This problem can again be controlled by choosing an extremely
conservative lower limit @xmath or even @xmath ⁹ ⁹ 9 As has been
discussed in [ 218 ] , the convergence will no longer be exponential in
this case. Since the total runtime of the correction step is negligible
compared to the whole run, this approach still appears to be justified .
Once one considers larger lattices up to @xmath and beyond, the
eigenvalue approach may become too costly since the eigenvalue density
increases linearly with the volume and consequently a larger number of
eigenvalues need to be computed to cover an equivalent fraction of the
spectrum.

### 15 Tuning the Dynamical Parameters

After the optimal matrix inversion using a non-adaptive polynomial for a
single (or a limited set of) gauge field configurations has been found,
there remains the task to examine the dynamical behavior of these
approximations. This is a question of major interest for the practical
implementation of any multiboson algorithm, since after all one is
interested in using the approximations in a dynamical updating process.
It may happen, that fluctuations of the eigenvalue density may
temporarily cause eigenvalues to run out of the approximation interval.
This can have a dramatic impact on the performance of the algorithm. It
is therefore of considerable importance to assess the size of these
fluctuations and what impact they could have on a simulation run. It is
important to notice that these aspects may still be explored on rather
small lattices since they will exhibit larger fluctuations and will
thence show a larger sensitivity to these vulnerabilities.

#### 15.1 Practical Determination of Autocorrelations

Before proceeding further, the tools must be prepared to compute the
primary measure of efficiency in the dynamical case, namely the
autocorrelation time of a time series. Since the aim of any simulation
algorithm is to generate statistically independent gauge field
configurations with minimal effort, the autocorrelation time is the key
monitor for the cost determination of a particular algorithm. The
theoretical bases of methods to compute autocorrelations of time series
have been laid in Sec. 8 . The purpose of this section is to apply them
to two different time series obtained from actual simulation runs.

In the first case, the series has a low fluctuation and is sufficiently
long for the autocorrelation time to be measured. The second situation
is less suitable: the time series exhibits large fluctuations and a
rather large autocorrelation time. Furthermore, it shows a contamination
of a very long mode which introduces fluctuations on a time scale
comparable to the length of the series itself. This mode appears to be
separated from the other modes contained in the series. Given the fact
that the total lattice size is given by @xmath fm (cf. Tab. 3 ), one may
suspect that the simulation is already very close to the shielding
transition, see also Sec. 21 . This could explain the observed behavior
and the presence of the long-ranged mode. This mode contaminates the
results and unless it is possible to perform simulations on a series at
least two orders of magnitude longer, no statement can be made about its
length. In this case it will become evident, that the lag-differencing
method as discussed in Sec. 8.5 is still able to extract information
from the series although the other methods fail.

##### 15.1.1 Case I: Low Fluctuations

This time series has been taken from a simulation run using the physical
parameters displayed in Tab. 3 on an @xmath -lattice. The algorithm
employed is the HMC algorithm with SSOR-preconditioning. The molecular
dynamics integration algorithm is the leap-frog scheme with a time step
of @xmath and a trajectory length of @xmath . The resulting acceptance
rate is @xmath .

The total size of the sample consists of @xmath trajectories, from which
the leading @xmath trajectories have been discarded. The complete time
series is given in Fig. 20 .

Figure 21 shows the normalized autocorrelation function (dotted curve),
together with the integrated autocorrelation time (blue curve) as a
function of the cutoff. The windowing procedure discussed in Sec. 8.5
has been applied with @xmath resulting in the green and red lines,
respectively. From the @xmath line, one can read off an integrated
autocorrelation time of @xmath . The dashed-dotted line displays the
maximum of the curve, which is clearly compatible with the @xmath
window.

A particular problem is already visible in the behavior of the
normalized autocorrelation function. It does not approach zero
exponentially (as one would expect), but appears to reach a plateau
above zero, before it suddenly drops. The curve is not compatible with
zero at this point since it is almost two standard deviations too high.
This is a typical case of a linear bias mentioned in Sec. 8.5 . The
lag-differencing method which will be applied below is able to handle
this situation.

Next, the variance is estimated using the Jackknife method (cf. Sec.
8.5.3 ). Figure 22 shows the variance @xmath as a function of the bin
size @xmath . The variance reaches a plateau (red line) at @xmath which
yields the true variance of the plaquette. The result for @xmath is
given by @xmath . Applying Eq. ( 121 ) now yields @xmath , which is
slightly below the result from the previous methods, but with a much
larger uncertainty. It must be realized that this procedure should only
give a rough estimate of the “true” value of @xmath , see [ 148 ] for a
thorough discussion.

Finally the lag-differencing method (cf. Sec. 8.5 ) is applied to the
time series. As a first step, the order- @xmath -lag- @xmath
-differenced series, @xmath , is computed using the definition ( 8.5.2
). It is displayed in Fig. 23 . As the next step, the correlation
between the plaquette and @xmath is being computed, cf. ( 118 ). The
normalized correlation function, @xmath , together with its integral as
a function of the cut-off is shown in Fig. 24 . The former is given by
the dotted curve, while the latter is visualized by the blue line. The
windowing method proposes the values @xmath (green line) and @xmath (red
line) for windows of @xmath and @xmath , respectively. The maximum of
the autocorrelation function, however, is reached at @xmath . It is
obvious that there is no significant improvement from the differencing
prescription and that the resulting function @xmath is not compatible
with a single exponential mode, just like the original function @xmath
was not. Furthermore, the maximum of the curve has shifted to the left
and the windowing prescription does no longer give the maximum at @xmath
. In fact, the differencing prescription impairs the statistics if the
lag is large compared to the “true” autocorrelation time.

To obtain a better result from the lag-differencing method, one has to
repeat the procedure leading to the estimate for @xmath using a series
of different lags and look for the stability of results. As long as the
lag @xmath stays above the autocorrelation time, no physical modes
should get lost. Once the autocorrelation time obtained becomes as large
as or larger than the lag @xmath , one may cut off physical modes. Thus
— in accordance with the discussion in Sec. 8.5 — one would look for a
plateau at some intermediate values of @xmath , where the
autocorrelation function should exhibit an exponential behavior.

The results from this analysis are displayed graphically in Fig. 25 .
Indeed, one finds a plateau reaching from @xmath up to @xmath giving
rise to @xmath . The self-consistency criterion @xmath is clearly met.
The question arises, whether the differencing prescription does indeed
result in a correlation function where the linear bias is suppressed. To
address this question, Fig. 26 shows the correlation function for the
case @xmath . Now the function indeed decays to zero for already a short
value of the cutoff, but still increases later on. This may be no
exponential mode, but a polynomial mode giving rise to a higher order
bias. Although in theory one could get rid of this bias by considering a
higher-order differencing scheme, the impact of this procedure on the
quality of statistics would invalidate this approach pretty soon. In
particular, if the quality of the series was good enough to allow for
higher-order differencing, the impact of the bias would be significantly
smaller in the first place.

The lesson from this investigation is that the linear bias can be
removed by applying the lag-differencing prescription and the result
obtained in this way is consistent with the one obtained from the
original autocorrelation function and from the Jackknife method. The
analysis shows that for the HMC with dynamical fermions one has to use a
time series with a length of at least @xmath trajectories to gain
accurate information about the true autocorrelation behavior.

##### 15.1.2 Case II: Large Fluctuations

The second series has been obtained from the history of the average
plaquettes using the TSMB algorithm discussed in Sec. 11.3 . The
simulation has been performed using the same physical parameters as in
the previous case except for the volume and the algorithmic parameters
given in Tab. 5 . The first polynomial order was @xmath . This run is
part of the tuning series discussed in Sec. 15.2 below. Reweighting of
the observables has been neglected, since this would have introduced
another source of autocorrelation effects, see [ 230 ] for a discussion.
The total length of the series was @xmath trajectories, where the
thermalization phase has already been subtracted.

As has already been pointed out, a very long mode possibly related to
the shielding transition is present in the series which cannot be
examined in a time series of such a length. However, since this mode
does not appear to have any connection to the other, short-range
fluctuations, it may be questioned if it has any significance for the
efficiency considerations of the multiboson algorithms.

As in the previous case, first the autocorrelation function is
visualized together with the corresponding integral in Fig. 28 . The
estimated autocorrelation time is @xmath . Again, the problem is that
the autocorrelation function does not go to zero exponentially and that
it appears to reach a plateau, before it drops to zero and then
decreases linearly. This behavior may indicate a linear bias, which
could be removed by the lag-differencing method.

The variance obtained from the Jackknife analysis for different bin
sizes is shown in Figure 29 . The plateau can be estimated to lie at
about @xmath . Together with @xmath one obtains an estimate of @xmath .
This number is compatible with the previous estimate, however it should
not be trusted since the original time series was contaminated with the
mode too long to be reliably examined.

Finally, the lag-differencing method has to shed some light on the
behavior of the autocorrelation time. Figure 30 displays the results
from measuring the integrated autocorrelation time with various lags. If
the long-range mode is indeed separated from the other modes, one should
be able to see a plateau from the other modes after the long-range mode
has been cut out. There is a clear signal for the formation of such a
plateau at lags between @xmath and @xmath . Using the error bars from
the single points and making a linear fit yields @xmath . This result is
about a factor of four below the previous estimates.

In conclusion, the lag-differencing method allows to get rid of a linear
bias and thus enables the evaluation of a series with large
fluctuations. The stability criterion is met, i.e. the estimated
autocorrelation time exhibits a plateau when plotted as a function of
the lag @xmath . The self-consistency criterion is also met, i.e. the
resulting value for @xmath is larger than the current value of @xmath
used. A large autocorrelation mode associated with a fluctuation that is
expected to vanish with increasing volume, has successfully been cut
off. In the following section, a series of these runs will be presented,
which are all identical except that a single parameter has been changed
during all runs.

#### 15.2 Acceptance Rates vs. Polynomial Approximation Quality

It has already been argued that the number of boson fields enters
linearly into the autocorrelation time of a multiboson algorithm. On the
other hand, one can expect that a small number of boson fields gives
rise to a small acceptance rate and thence to an increase in the
autocorrelation time again for small numbers of fields. The number of
boson fields and thus the first polynomial order is of critical
importance for a multiboson algorithm. Until today, however, no
systematic analysis of this effect has been performed and the impact of
this choice on practical simulations is unclear. This is certainly
related to the fact that any systematic analysis is exacerbated by the
requirement to measure autocorrelation times with a reasonable accuracy.
Therefore, we base our study on very long runs. Beyond that, we employ
the efficient tools described in detail in the previous section.

The algorithmic parameters shared by all runs are displayed in Tab. 5 .
Only the order of the first polynomial, @xmath , has been varied.

Table 6 shows the statistics generated together with the acceptance
rates of the noisy correction step and the cost of a single trajectory.
These runs have been performed on the ALiCE computer cluster installed
at Wuppertal University ¹⁰ ¹⁰ 10 See
http://www.theorie.physik.uni-wuppertal.de/Computerlabor/ALiCE.phtml for
technical details and further information . The machine configurations
were both a single node configuration (with no parallelization) and a
four-node partition with the lattice parallelized in @xmath - and @xmath
-direction. The local lattice size was consequently @xmath . The
numerical efforts are given for the latter situation.

It is specified in terms of a multiplication by the preconditioned
fermion matrix @xmath with an arbitrary colorspinor @xmath . Since the
TSMB algorithm uses non-adaptive polynomials in the noisy correction
step, the number of explicit matrix-vector multiplications is
straightforwardly given by @xmath . In the case under consideration we
thus have @xmath . To estimate the total effort we assume that the
efficiency of the implementation for the local algorithms is roughly
equivalent to the efficiency of the matrix-vector multiplication routine
¹¹ ¹¹ 11 This assumption is only roughly valid leading to a machine- and
compiler-dependence of the effort defined in this way. See Sec. 16.4 for
a thorough discussion . Thencefrom, we measure the time needed for a
complete trajectory, @xmath , and the time needed for the noisy
correction alone, @xmath . Using these times, we can define the total
effort @xmath as ¹² ¹² 12 Actually the total time may fluctuate due to
the load of the whole communication. Therefore, the results in Tab. 6
have been averaged

  -- -------- -- -------
     @xmath      (211)
  -- -------- -- -------

##### 15.2.1 Behavior of the Correction Factor

As a first step, the dependence of the acceptance rate on the magnitude
of the exponential correction factor @xmath should be clarified. Figure
31 shows the exponential correction factor together with its standard
deviation. It depends exponentially on the order @xmath .

The function approximating the average value is given by

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (212)
                                @xmath   
  -- -------- -------- -------- -------- -------

This behavior is in line with the expectations that the convergence of
the first polynomial is exponential. On the other hand, the standard
deviation of the exponential correction does not follow a precise
exponential dependence; this is shown in Fig. 32 .

Restricted to the intermediate regime, nevertheless an exponential
function yields a good fit to the data points:

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (213)
                                @xmath   
  -- -------- -------- -------- -------- -------

Finally, inserting the models ( 212 ) and ( 213 ) into Eq. ( 209 )
allows to check their validity by comparing them to the measured
acceptance rates from Tab. 6 . Table 7 compares the predicted and the
measured acceptance rates. The numbers are obviously in perfect
agreement.

In conclusion, the exponential correction factor shows an exponential
dependence on the order @xmath . Its standard deviation also
approximately follows an exponential decay. Therefore, the acceptance
rate can be predicted as a function of the polynomial order @xmath once
at least two points have been determined which allow to fit the
functions ( 212 ) and ( 213 ).

##### 15.2.2 Fermionic Energy

First consider the fermionic action @xmath . This quantity is not
affected by the correction step, since in the trajectory in Tab. 5 the
correction step may only reject an update of the gauge field. However,
it is still linked to the full dynamics of the system by its coupling to
the gauge field. Thus, it is expected to display a linear dependence on
the number of boson fields, @xmath . Figure 33 shows the integrated
autocorrelation times @xmath versus the polynomial order, @xmath . The
autocorrelation times have been measured using the windowing
prescription from Sec. 8.5 on the integrated autocorrelation function.

One finds a linear dependence on the number of boson fields. Certainly,
the small absolute values of the fermionic autocorrelation times help to
make the measurement very precise. However, the fermionic energy does
not directly give rise to any useful physical information. Rather
quantities computed directly from the gauge fields (like the plaquette)
give rise to physical information about the system. Since the “fermionic
force” on the gauge field is directly related to the boson fields, one
can nonetheless expect the influence on the autocorrelation time of
gauge-field related quantities to be linear in @xmath . Unfortunately,
the situation is far more involved in that case.

##### 15.2.3 Gauge Field Plaquette

Since the plaquette is a purely gauge-field dependent quantity, its
autocorrelation time will be affected by the acceptance rate. It can be
expected to increase at too small @xmath because of the correlations
caused by identical configurations. Furthermore, as already noted above,
the plaquette contains a strong noise and is thus very difficult to be
measured. The desired behavior will therefore be embedded in huge
fluctuations. A standard analysis of the effect is therefore bound to
fail. It is in this situation, where the lag-differencing method becomes
important and provides a useful source of information.

The integrated autocorrelation time as a function of the differencing
lag is shown in Fig. 34 for all available values of @xmath . The errors
are larger than in the case of @xmath since the autocorrelation times
are now larger and thus the statistics for this observable is worse. The
left diagram in the second row has already been discussed in Sec. 15.1 ,
Fig. 30 .

In the case @xmath a plateau is clearly visible, indicating that the
lag-differencing yields a stable solution. In the case @xmath the
situation is less clear. A pseudo-plateau may be suspected around @xmath
, but in general the method is unstable and the result should be
disregarded. The case @xmath has been discussed in Sec. 15.1 , while
@xmath is again very stable with a plateau determined around @xmath .
Absolutely nothing can be learned from @xmath ; there is no plateau and
obviously any attempt to find one is futile. For @xmath a clear plateau
is again visible, starting at about @xmath . Regarding @xmath a plateau
can be found from @xmath to @xmath . For @xmath , again no result can be
found.

For comparison the Jackknife procedure as discussed in Sec. 8.5 has been
applied to all samples. The variances vs. the bin sizes are displayed in
Fig. 35 . The straight lines give the plateau values. Again, the first
graph in the second row is identical to Fig. 29 in Sec. 15.1 .

The resulting integrated autocorrelation times read off from Figs. 34
and 35 are given in Tab. 8 . Figure 36 displays these results
graphically.

The conclusion is that there is no measurable increase in the
autocorrelation time as @xmath is increased if one relies solely on the
lag-differencing method. The increased acceptance rate from larger
@xmath compensates for the loss of mobility in phase space. From this
point of view it appears reasonable to simulate at comparatively small
acceptance rates.

The Jackknife method exhibits a similar behavior, but it is compatible
with a decrease of the autocorrelation time with increasing @xmath .
There is no indication, however, that this decrease exceeds a factor of
two, see the cases @xmath and @xmath . In fact, it is very likely that
there is a non-trivial dependence (which results in differences of the
order of a factor of two, but not much larger) which will become visible
if one performs the same runs with far larger statistics. This is
impossible with current computer technology, and would not be worth the
effort given the fact that its influence is so small. The conclusion is
therefore unchanged.

#### 15.3 Dynamical Reweighting Factor

Of particular importance is the stability of the polynomial
approximation with respect to the interval @xmath chosen for the TSMB
algorithm. To access this problem, three runs have been performed with
different choices for the lowest limit @xmath . The physical parameters
were again chosen according to Tab. 3 on an @xmath lattice using the
TSMB algorithm with polynomial order @xmath and parameters as in Tab. 5
apart from the value of @xmath . The lower limit has been varied to be
@xmath in the first, @xmath in the second, and @xmath in the final case.
These polynomials are visualized in Fig. 37 for the lower end of the
approximation intervals.

The histories of the reweighting factors measured during the runs are
displayed in Fig. 38 . They have been computed from the lowest @xmath
eigenvalues (cf. Sec. 14.3 ).

In the first case, @xmath , the correction factor shows the largest
fluctuation of all cases: @xmath . For the second case, @xmath , one
obtains a correction factor of @xmath . In the third case with @xmath
the correction factor drops down to @xmath meaning that the lowest
eigenvalue left the region where the polynomial approximation is good
enough; although this appears to be a problem, it only happens at a
single place and never repeats. If this outlier is not disregarded, the
correction factor is found to be @xmath . Dropping the first @xmath
trajectories from the sample — which is certainly justified, since it
makes up only @xmath of the total runtime — one arrives at @xmath . When
computing any observable, however, the configurations must not be
disregarded, since the algorithm would otherwise not be ergodic. If it
is included in the measurement via Eq. ( 171 ), the algorithm will be
both exact and ergodic.

As it is apparent from Fig. 37 (see also Fig. 12 ), the correction
factor oscillates for the contributions from the smallest eigenvalues
when using the static inversion of the TSMB algorithm. Consequently, an
eigenvalue which is located in a “valley” will contribute with a smaller
factor than an eigenvalue located on a local maximum. The static
inversion of the TSMB algorithm may prefer to accumulate the eigenvalues
in the local minima compared to the distribution sampled by an adaptive
inverter. Since the distributions from the adaptive inversion (or from
the reweighted static inversion, respectively) do not know about the
existence of the oscillations, one may suspect that this distortion of
phase space introduces notable stochastic forces . If these forces were
present, they could result in larger autocorrelation times since motion
of the eigenvalues between separate valleys would be suppressed. Figure
39 shows the histograms of the @xmath lowest eigenvalues from the three
histories of the runs, computed every @xmath trajectories.

There does not appear to be any correlation between the eigenvalue
fluctuations and the shape of the polynomials. As a result, one can say
that the static matrix inversion employed in the TSMB algorithm does not
result in significant stochastic forces. Furthermore, it is possible to
make a choice of @xmath guided by the optimal lower limits found in Sec.
14.1 . If the lowest eigenvalue ever leaves this interval, the
correction factor will suppress these configurations. On the average, it
will be smaller (with all other parameters fixed) than any choice with
smaller @xmath . The magnitude of the reweighting factor is then
controlled by the choice of @xmath . Since a large fluctuation of the
reweighting factor impairs the statistics, the value of @xmath should be
chosen such that the reweighting factor is always below the statistical
error. This choice ensures that its influence is so small that the
statistical quality of the sample is not perturbed too much.
Alternatively, one can also compute the determinant norm @xmath given by
Eq. ( 206 ) for the second polynomial and keep its error below the
statistical one. The latter procedure is simpler and still gives a good
handle of the quality attained.

On the other hand, one should refrain from making @xmath far too large
and the lower limit @xmath too small — this choice will simply increase
the computer time required and make the algorithm inefficient.

#### 15.4 Updating Strategy

After the recipe for choosing the polynomials and their orders are
fixed, finally the focus is placed on the tuning of the updating
algorithms for sampling a new configuration (the transition matrix
@xmath in the Markov process). The available algorithms have all been
discussed in detail in Sec. 10 .

##### 15.4.1 Boson Field Updates

The boson field updates should be performed using a global heatbath
(cf. Sec. 10.2 ) to achieve optimal decorrelation. This fact is already
known theoretically from [ 196 ] and has been confirmed numerically in [
221 ] . The global updates may, however, be accompanied by local
updates, see e.g. [ 197 ] .

Prior to a simulation run, the boson fields should be thermalized. This
can best be achieved by holding the gauge field configuration fixed and
updating the boson fields only. Observing the efficiency of
thermalization methods also allows to shed some light on the best
combination of local updating algorithms. Figure 40 shows a history of
the fermionic energy, @xmath , starting from a random boson field
configuration. They display the effect of local overrelaxation sweeps to
the thermalization. The runs have been performed using an @xmath
lattice. The physical parameters have been chosen as in Tab. 3 and the
polynomial order has been chosen to be @xmath . This makes no sense for
a production run on this lattice size, but since the local boson field
updates factorize, there should be no dependence on the number @xmath
chosen. The interval of the polynomial has been chosen to be @xmath . A
trajectory always consisted of a local boson heatbath update and either
@xmath , @xmath or @xmath local boson field overrelaxations. It is
obvious, that local boson overrelaxations improve the thermalization
rate and thus should also be expected to decrease the exponential
autocorrelation time.

Of course, in practical implementations, one should not use the local
updating algorithms for the thermalization, but instead directly use the
global boson heatbath.

##### 15.4.2 Gauge Field Updates

For the gauge field updates one can either use a local Metropolis
algorithm (Sec. 10.1.1 ), a local heatbath (see Sec. 10.2 ), or local
overrelaxations (Sec. 10.3 ). It turns out that after a certain number
of gauge field updates has been applied the acceptance rate of the noisy
correction step stays essentially constant.

To demonstrate this behavior, a simulation run at the physical
parameters given in Tab. 9 on a lattice with @xmath has been performed.
This run is a part of the investigations performed in Sec. 23.1 . Table
10 shows the algorithmic parameters in this study. The only parameter
varied is the number of gauge field Metropolis sweeps, where a
Metropolis algorithm with eight hits per single link has been used.

The resulting values of the exponential correction together with their
standard deviations and the corresponding acceptance rates are shown in
Tab. 11 .

The average exponential correction increases slightly with an increasing
number of sweeps between the noisy corrections. However, this increase
is accompanied by a slight increase in the standard deviation. Already
after six sweeps have been performed, the changes can be attributed to
fluctuations. The net effect is that the acceptance rate does not vary
more than @xmath . Hence, one finds that indeed the acceptance rate will
saturate once a certain amount of updates has been applied to the gauge
field.

Therefore it is possible to choose a rather large number of gauge field
updates between the noisy corrections.

##### 15.4.3 Choice of Updating Sequence

During a single trajectory, all d.o.f. of the system need to be updated.
Therefore, a trajectory always consists of a certain number of boson
field updates and a certain number of gauge field updates followed by a
noisy correction step. The optimal sequence, however, might also depend
on the architecture used for the configuration sampling. The reason is
that the ratio of local to global update sweeps itself depends on the
architecture used, see Sec. 16 below for details.

The acceptance rate is only slightly influenced by the number of local
gauge field sweeps, as has been shown in Sec. 15.2 . Hence, it is
obvious that the noisy correction does not contribute to any reduction
of the autocorrelation time while the update sweeps do. Consequently,
one should always keep the number of local updates in a trajectory as
large as possible, such as to minimize the contribution of the noisy
correction step to the total runtime.

Now two different kinds of trajectories are proposed:

-   Perform a number of boson field updates, followed by a number of
    gauge field updates and a correction step.

-   Perform an alternating sequence of gauge and boson field updates
    prior to a correction step.

Since the correction step only depends on the gauge field configuration,
but not on the boson fields, a rejection in the first proposal does not
imply the necessity of restoring the boson field configuration since it
has always been obtained in the background of a “valid” gauge field
“background” configuration. Conversely, the second scheme will have to
restore both the old boson and gauge field configurations in case the
update is rejected. Hence, the memory requirements of the second
proposal will be significantly larger than in the first case.

These considerations do not yet fix the optimal mixture of gauge and
boson field updates. A very simple proposal is to use a single local
gauge overrelaxation sweep followed by a single local boson
overrelaxation sweep. The overrelaxation sweeps allow for a very fast
movement through phase space, but do not ensure ergodicity, see Sec.
10.3 . So this sequence has to be complemented by at least one ergodic
heatbath and/or Metropolis sweep. This simple sequence turns out to be
quite efficient in practice, see Sec. 19.2 for an application.

However, given the fact that the fermionic energy decorrelates much
faster than the gauge field plaquette (see Sec. 15.2 above), one might
hope that subsequent updates of the gauge field alone might still
decrease the plaquette autocorrelation, although essentially only a
small subset of all d.o.f. is being updated. The reason why this could
be efficient is, as will be shown below in Sec. 16.4 , that the caching
of the boson field contributions as proposed in App. Advanced Algorithms
for the Simulation of Gauge Theories with Dynamical Fermionic Degrees of
Freedom in Eqs. ( 279 ) and ( I.2.1 ) allows for a larger number of
subsequent gauge-field-only update sweeps. On the other hand it is clear
that this effect will very soon lead to a saturation since the gauge
field will thermalize with the fixed boson field background.

The question when this saturation occurs can only be answered in a
practical simulation. The physical parameters have again been chosen
from Tab. 3 , and the algorithmic parameters are given in Tab. 12 . They
are identical to the parameters given in Tab. 5 .

Table 13 shows the different sequences used for a number of simulations
and the number of trajectories computed together with the total cost in
MV-Mults for a single trajectory. The machine configuration used for all
runs was an eight-node partition of the ALiCE computer cluster;
parallelization was used in the @xmath - and @xmath -direction resulting
in local lattices of @xmath .

Sequence I corresponds to an intermediate number of boson sweeps and
gauge field sweeps. For the gauge field local overrelaxations have been
used. Hence, ergodicity is ensured by the boson field heatbath only.
Sequence II applies a small number of boson field sweeps, but a large
number of gauge field sweeps. Sequence III applies a large number of
boson field updates and an intermediate number of gauge field updates.
Sequence IV consists of a small number of boson field updates and only
an intermediate number of gauge field updates. The latter run makes
direct contact with the run in Sec. 15.2 at @xmath . It will be denoted
by “Sequence 0”.

In the following, we will not only consider the autocorrelation times in
terms of trajectories, @xmath , but also the efficiencies of the
algorithms, @xmath . These efficiencies are defined as the number of
MV-Mults required per statistically independent gauge field
configuration,

  -- -------- -- -------
     @xmath      (214)
  -- -------- -- -------

where @xmath has been defined in Eq. ( 211 ) and @xmath is the
integrated autocorrelation time of the observable under consideration,
in this case the plaquette. Thus, the quantity @xmath is a measure for
the total cost which is, to a large extent, independent of the technical
details of the underlying algorithm.

The resulting plaquette autocorrelation times have been computed using
both the lag-differencing and the Jackknife method. Figure 41 shows the
results for the former method. For Sequence I one finds that a plateau
emerges beyond @xmath , while for Sequence II it starts already around
@xmath . Sequence III exhibits a stable region between @xmath and @xmath
, while Sequence IV becomes steady beyond @xmath . Hence, in all cases,
the differencing method was able to yield conclusive results.

The Jackknife variances as a function of the bin size are displayed in
Fig. 42 . In all cases, plateaus can be identified. Note again, that
there is no systematic control of the errors when using this method.

The resulting autocorrelation times and the total numerical efforts (as
computed from the lag-differencing method) are summarized in Tab. 14 .
The data for Sequence 0 has been taken from Tab. 8 .

When examining the integrated autocorrelation times one finds that
indeed some gain can be achieved by increasing the number of local gauge
field sweeps. However, the effect clearly saturates already after as few
as four consecutive sweeps have been performed in Sequence IV. On the
other hand, increasing the number of boson field sweeps in Sequence III
did not produce any practical gain.

In contrast, the picture is different if the total efforts are
considered. Apparently a decrease in the autocorrelation time is
accompanied by an increase in the effort for a single trajectory. From
this finding one can conclude that one should better mix the local
update sweeps. There is, however, one subtlety which is not reflected in
Tab. 14 : As it will be shown later in Tab. 16 , on the APE -100
architecture a caching of the boson fields allows for an efficient
implementation of subsequent gauge field sweeps. This would reduce the
total cost in Sequence IV compared to Sequence 0 by the amount of three
gauge field sweeps. The net effect of such an implementation is that the
cost would only slightly increase for the execution of subsequent gauge
field sweeps. Hence, one would find that for the APE -implementation an
updating sequence like Sequence IV is superior to the sequence used
previously.

In conclusion, the optimal updating sequence consists of a mixture of
local gauge and boson field sweeps. One can update the gauge fields for
several sweeps while holding the boson field background fixed. It does
not appear to be efficient, however, to perform more than four gauge
field sweeps in this way. In case the caching discussed in App. Advanced
Algorithms for the Simulation of Gauge Theories with Dynamical Fermionic
Degrees of Freedom is available, this method is indeed effective in
reducing the total effort for a single trajectory, @xmath , and hence
also the total cost for a statistically independent configuration,
@xmath .

### 16 Implementation Systems

The discussion so far has been limited to the machine-independent part
of multiboson algorithms. In practical simulations, however, a
particular architecture for the large-scale simulations has to be
selected. This choice will have a considerable impact on the project
since the complexity of a multiboson algorithm is huge compared to other
algorithms in use today and the program is expected to run for several
months.

For the purposes of this thesis, two platforms have been given major
focus: The first platform was the APE -100 platform [ 231 ] which is
also compatible with the APE -1000 [ 232 ] architecture ¹³ ¹³ 13 Further
material on these machines can also be found on the web under
http://chimera.roma1.infn.it/ape.html . The machines used are installed
at DESY /Zeuthen and at the Forschungszentrum Jülich in Jülich, Germany.
The second target platform for the implementation was the ALiCE computer
cluster [ 233 ] installed at Wuppertal University. In the following, the
machines together with their specific merits and drawbacks will be
presented. The properties of the implementations are discussed and the
influence of the numerical precision on the calculations is examined.

#### 16.1 Ape Platform

The APE is a SIMD machine which executes a program in parallel on a
number of nodes arranged in a three-dimensional mesh. The smallest
configuration of nodes is a @xmath -partition, the largest configuration
available is an @xmath -machine.

The APE -100 architecture can only execute single-precision floating
point numbers efficiently in parallel. However, for special operations
like global sums, a library for double-precision addition is available [
234 ] . Given that a global sum only consumes a fraction of the total
runtime of a program, there is no performance degradation to be
expected. In contrast to floating point operations, integer calculations
are being done globally on a single CPU (with no parallelization
possible) with less efficiency. Especially integer operations on array
indices should be kept at a minimum for the program to run efficiently.
One further obstacle is the fact that the complete multi-boson program
would be too large to fit into the memory of the machine — thus only a
portion of the complete code can be written on the APE machines and the
remaining parts must be run on conventional parallel computers.

One further problem is the bad I/O-performance of the machine. A
save/restore of the complete machine state requires about @xmath -
@xmath hours of time (for a typical lattice of size @xmath and @xmath )
which means that about @xmath of the whole runtime of a job (which is
usually about @xmath hours) would be wasted for I/O operations. This
problem can be overcome, however, by not storing the boson fields on
disc, but rather performing a global heatbath thermalization sweep (see
Sec. 15.4 ) to initialize them prior to a run. This strategy is more
efficient (and also is more effective in terms of autocorrelation times
of observables) than saving and restoring the complete machine state
each time.

The compiler and optimizer technologies lag behind the industry
standards of conservative parallel computers — the CPUs have no caches
(only the registers of the floating point processors serve as a kind of
@xmath st level cache). Most optimization strategies (like loop
unrolling, prefetching etc.) have to be implemented manually using the
high-level language of the platform. This language is called TAO [ 235 ]
and is a language build on Zz , which is a compiler construction
language. However, due to the fact that Zz is still accessible (to
extend the features of TAO and to implement manual loop-unrolling etc.),
the system is effectively using a dynamic grammar, which is known to
bear a lot of responsibility on the implementor. The drawback is that
more complicated programs developed on the machine cannot easily be
ported to different architectures and thus the maintenance costs will
soon become a reasonable factor. This is no concern for trivial
algorithms like the HMC, but will become a serious problem once a larger
source code base is to be established on the machine.

One particular problem is that the sources of the compilers are not
publicly available, meaning that bugs are hard to locate and fix
compared to e.g. the GNU compilers ¹⁴ ¹⁴ 14 Further information and
resources related to this system can be found under
http://www.gnu.org/software/gcc/gcc.html . This implies that the
development tools could not be run on modern and fast machines — the
typical compile times during the early phases of the project were of the
order of @xmath minutes resulting in turn-around times of more than half
an hour.

The advantages of the APE architecture are that a lot of computer time
is available and it has proven to be the ideal platform for simple
algorithms like the HMC, which essentially rely only on the
implementation of an efficient matrix-vector multiplication. Furthermore
the platform scales very efficiently since the communicational overhead
is minimal. This results in a rather small latency and is thus a
counterpoint to the workstations clusters available today, see [ 236 ]
for a different application of workstation clusters which demonstrates
the same properties. Several of these shortcomings have improved with
the advent of the APE -1000 architecture [ 232 ] , but experience is
still too sparse to include major results in this thesis. The APE -1000
architecture still has problems regarding the maximum machine size and
the fact that double precision calculations will introduce a performance
hit of a factor of four in the peak performance.

The first implementation of the TSMB algorithm used in this thesis has
been written on the Q4open machine located at the NIC in Jülich, Germany
¹⁵ ¹⁵ 15 See http://www.fz-juelich.de/nic/ for further information on
the John von Neumann — Institut für Computing . The machine had a
configuration of @xmath nodes and served as the major development
platform until Spring 2000. Sadly, it went out of service due to a
defect board.

#### 16.2 ALiCE Cluster

At a later stage of this project, development was shifted to the ALiCE
computer cluster installed at Wuppertal University ¹⁶ ¹⁶ 16 A large
contribution to this program has been provided by Prof. I. Montvay, DESY
, Hamburg , where modern compilers and development tools are available.
Most results have in fact been obtained on this machine. The cluster
consists of @xmath Compaq DS 10 workstations, each equipped with a 21264
Alpha processor running at @xmath MHz. The size of the second level
cache is @xmath Mbyte. The network is based on a Myrinet network with a
peak performance of @xmath Gbit/ @xmath .

The coding done on this platform was immediately usable on other
parallel machines, like the CRAY T3E located at the ZAM , Jülich ¹⁷ ¹⁷
17 The official homepage of the Central Institute for Applied
Mathematics can be found at
http://www.fz-juelich.de/zam/ and the Nicse -cluster which is also
located at the NIC institute. The program has been proven to run also on
a cluster of standard, Intel-based workstations installed at Wuppertal
University. The network of these machines is based on standard Ethernet
which made the installation not competitive from a performance point of
view, but very attractive for development and debugging purposes. This
illustrates the particular advantage of standard tools over proprietary
solutions: although the hardware costs might be smaller (the situation
might be less clear once development costs are included, however), the
Total Cost of Ownership (TCO) may outweigh the former price. In fact,
the total costs for maintenance and software may become larger than the
pure hardware costs.

#### 16.3 Accuracy Considerations and Test Suites

The complexity of code for the multiboson algorithm is high compared to
the case of other algorithms in use today like the HMC. The multiboson
code on the APE machine (together with the production environment)
amounted to more than @xmath lines of code; the program on the ALiCE
cluster consisted of @xmath lines of code (for the single-node and the
parallel version) and the administrative software required another
@xmath lines. For the measurement of hadronic masses, a program with a
size of @xmath lines of code was required. This clearly asks for having
efficient test suites available to track down possible sources of
errors.

##### 16.3.1 Local Fermionic Action

An important part of the program consists of the implementation of the
fermionic action. Explicit forms of the different expressions required
for the local action are given in App. Advanced Algorithms for the
Simulation of Gauge Theories with Dynamical Fermionic Degrees of Freedom
. The local forms have to be consistent with the implemented
matrix-vector multiplication ¹⁸ ¹⁸ 18 Strictly speaking, this is not a
necessity for the program to be correct. One can implement the global
matrix-vector multiplication @xmath with another convention than that
used for the local actions. However, in this case the tests suggested
here will fail. Thus, it appears to be a good idea to keep the actions
consistent and proceed as discussed . Then one can alter a single link
at an arbitrary site and compare the results of Eqs. ( 160 ), ( I.2.1 )
and ( I.2.1 ). The second test consists of changing a single
color-spinor with an arbitrary index @xmath , @xmath , and again
comparing the results of ( 160 ) and ( I.2.1 ). The residual error
should only be limited by the machine precision. This can also act as a
test on whether the single precision of the APE machines is a real
limitation.

In fact, for the application of ( I.2.1 ) one has to sum up @xmath terms
in single precision to get a complex @xmath matrix which may introduce
already difficulties at moderate values of @xmath . To examine the
errors as they occur in practical computations, one can already get
along with a very small lattice since the major source of numerical
errors occurs in the local update part. Therefore, a simulation has been
performed using a thermalized configuration on a @xmath lattice at the
physical parameters given in Tab. 3 on the QH1-board at DESY /Zeuthen.
The polynomial in question has been chosen to be @xmath with @xmath .
The maximum numerical error in the three expressions is displayed in
Figure 43 , where the distribution of the inaccuracies are shown. They
have been obtained by considering separately each site on a single node
during a gauge field updating sweep. The error obviously is bounded from
above and only scarcely exceeds @xmath . In the development phase such a
plot turned out to be extremely useful since identifying the sites which
give a huge numerical error can help to track down program bugs rather
easily.

The second message to be learned from Fig. 43 is that the @xmath -bit
precision used is not an obstacle in actual simulations: The
systematical error introduced by the local gauge field updates is
obviously under control.

The same can be done for local changes of the boson field and
considering the expressions Eqs. ( 160 ) and ( I.2.1 ). The
corresponding results are displayed in Fig. 44 . Apparently, the same
can be said about the boson field case as has been stated before in the
gauge field case.

##### 16.3.2 The Inverse Square-root

Another problem is posed by the residual error of the inverse square
root required for the TSMB algorithm, Eq. ( 167 ). As has already been
discussed in Sec. 14.1 , the systematic error can be computed via ( 207
) and must be less than one percent.

The question arises, how accurate the approximation can be at best .
Given the fact that in case of the HMC algorithm one has a residual
error of @xmath if one uses @xmath -bit floating point numbers on an
@xmath lattice (see Sec. 11.2 ), the question arises, how large the
lattice may be in the TSMB case if one only has access to single
precision on a particular architecture. This question is answered by
Fig. 45 , where the residual error @xmath of the noisy correction step
is plotted vs. the order of the third polynomial, @xmath (the polynomial
@xmath has been chosen to be @xmath ). The calculation has been
performed using both @xmath -bit (single precision) and @xmath -bit
(double precision) algebra. The lattice sizes which have been considered
were @xmath and @xmath .

On the @xmath lattice the results coincide up to orders of about @xmath
. Beyond this point, the single precision result deviates from the
double precision curve. Finally, the single precision numbers saturate
at @xmath . The accuracy which can be reached is still satisfactory
since it is bounded by @xmath . Hence, one can conclude that single
precision is adequate on an @xmath lattice.

On the @xmath lattice, the single precision result saturates already at
@xmath . This is more than on the @xmath lattice, but it is still
sufficiently small. The double precision curve also shows a saturation,
but not before @xmath and an accuracy of @xmath has been achieved.
Again, this is completely acceptable.

The conclusion to be drawn from this test is that single precision
arithmetic is not at all a problem on an @xmath lattice, and is still
acceptable on lattices as large as @xmath . On larger lattices, however,
single precision may no longer be feasible and one should refrain from
using @xmath -bit arithmetics.

#### 16.4 Architectures and Efficiency

All architectures discussed so far have certain advantages and
disadvantages. In this section, the efficiencies of the implementations
are compared against each other. Table 15 shows the execution times of
the different parts of the multiboson implementations for a selection of
architectures. The physical parameters of the runs are given in Tab. 3 ,
while the algorithmic parameters are listed in Tab. 5 with @xmath . The
lattice size was again @xmath .

In case of the ALiCE cluster and the CRAY T3E, an eight-node partition
with parallelization in @xmath - and @xmath -direction has been employed
which results in local lattices of @xmath . Table 15 displays the
execution times of the different algorithms employed.The last line shows
the ratio of local update sweeps to the global matrix-vector
multiplications taking place in the noisy correction step. This is the
key quantity of interest, where the influence of a particular
architecture is most clearly exposed.

For the APE -100, an eight-node Q1 board has been used with a lattice
size of @xmath . The local lattices were @xmath per node. The resulting
execution times of these algorithms are quoted in Tab. 16 . In contrast
to the above implementation, the APE program uses an efficient caching
strategy, where the contributions of the boson fields to the local
staples, Eq. ( I.2.1 ), which are unchanged by a gauge field sweep are
held in memory, see App. Advanced Algorithms for the Simulation of Gauge
Theories with Dynamical Fermionic Degrees of Freedom for details. This
allows to reduce the computational costs for repeated local gauge field
sweeps. The time required for the initialization of the gauge field
sweep is given in the fifth line of Tab. 16 . This strategy has only
been implemented on the APE , but in principle this caching scheme is
machine-independent and could also be implemented in the other program.

When comparing the ALiCE and the CRAY T3E, one realizes that the CRAY
T3E architecture has a more efficient network, but lacks the large
second-level caches of the ALiCE nodes. This explains why the
communication-intensive performance of the matrix-vector multiplication
is very efficient on the CRAY T3E. On the other hand, the local update
sweeps are very cache intensive and communication between nodes only
plays a minor role. Therefore, the local update sweeps contribute a much
smaller fraction to the total runtime on the ALiCE cluster, while they
account for about @xmath of the total sweep time on the T3E.

The APE -100 architecture shows an even more prominent dominance of the
local update sweeps, which consume about @xmath of the total runtime.
This can be attributed to the fact that the CPUs have no second-level
cache at all and the data has to be fetched from memory each time.
Therefore the local update sweeps are rather inefficient, while the
global matrix-vector multiplications are very efficient.

### 17 Summary

The TSMB algorithm requires three approximations to an inverse power of
the Hermitian Wilson matrix. All approximations are being performed with
static inversion algorithms as discussed in Sec. 12.1 . The first is a
crude approximation to an inverse power of @xmath with order @xmath .
The second is a refined approximation to the same function with order
@xmath . The third one approximates the inverse square root of the
polynomial with which the second approximation is performed.

The best method to find the first polynomial consists of applying the
GMRES algorithm (see Sec. 12.3 ) to one or more thermalized gauge field
configurations at the physical point one is interested in. This method
is limited by the numerical precision of the architecture used to
generate the polynomial. The other method consists of using a
quadratically optimized polynomial. The latter choice only requires
rough knowledge of the spectral bounds.

The second and third polynomials have to be quadratically optimized
polynomials. The value of @xmath for the product of the first and the
second polynomial should be chosen such that it is slightly smaller than
the average smallest eigenvalue. Its order has to be adjusted such that
the reweighting factor is not fluctuating more than a few percent. Since
the convergence is exponential, this can be achieved without too much
effort.

The third polynomial must have a sufficiently high order such that its
corresponding determinant norm, Eq. ( 207 ), or rather the resulting
systematic error, Eq. ( 210 ), is never exceeding values of @xmath .

The order of the first polynomial influences the acceptance rate of the
correction step. It appears to be safe to make the acceptance rate
somewhat smaller than @xmath . The motion in phase space depends
linearly on the number of boson fields. The decreased acceptance rate
counteracts the increased mobility in phase space and the resulting
efficiency does not appear to depend on the acceptance rate. Since the
numerical cost of a single trajectory is proportional to the number of
boson fields, the total cost for a statistically independent gauge field
configuration, Eq. ( 214 ), is then given by

  -- -------- -- -------
     @xmath      (215)
  -- -------- -- -------

This formula should apply for runs at different parameters and different
orders @xmath if the acceptance rates are held constant and if the field
updates dominate the time needed for a single trajectory.

A trajectory is given by the updating sequence, i.e. the transition
function of the Markov chain. It consists of a number of update sweeps
for the boson fields and local update sweeps of the gauge field. It has
turned out that the boson field updates should be mixed with the gauge
field updates, but a number of subsequent gauge field updates allows a
similarly efficient decorrelation. Each trajectory is completed by a
noisy correction step. In view of the fact that the acceptance rate is
rather independent from the local gauge field updates, the optimal
efficiency can be achieved by performing a larger number of local gauge
field updates between two noisy corrections. Thus, the sequence should
be arranged in such a way that the field updates dominate the total
runtime.

Furthermore, it is important to identify the machine architecture which
meets the specific demands of MB algorithms. While a conventional
massive-parallel machine with a network similar to the CRAY
architecture, but small caches on the nodes does not perform well with
respect to the local update sweeps, its efficient network allows for a
rather efficient matrix-vector multiplication. However, as it has been
discussed in Sec. 15.4 , one can (and, in fact, one should) always
arrange the updating sequence in such a manner that the local updating
sweeps dominate the total runtime of the code. Hence, the machine best
suited for TSMB calculations is found to be a cluster of workstations
with large cache and standard programming tools.

The APE -100 system does not perform very well in the local update
sweeps and suffers from the problem that the coding of the algorithm can
until now not be used on any other machines. The former problem may be
overcome with the advent of the APE -1000 architecture which may (due to
the large number of CPU registers) reduce the number of memory accesses
required. The latter problem can not be expected to be solved before the
advent of the APE -Next platform [ 237 ] . Due to the complexity of the
algorithm it appears reasonable to implement first a reference
implementation in a standard language on a different architecture before
starting with the coding on the APE platform.

There is still room for further improvements, however. In particular,
improving the approximation scheme of the third polynomial would allow
to overcome the limitations of the current implementations and should
make the algorithm applicable to larger lattices even with single
precision arithmetics. A different place where further improvements are
in order is the first polynomial. Although the optimal way to find its
coefficients has been identified, this method still requires high
numerical accuracy of the implementation system.

## Chapter \thechapter Comparison of Dynamical Fermion Algorithms

In the first section, Sec. 18 , the variant of the multiboson algorithm
with TSMB correction step, which has been studied in detail in Chapter
Advanced Algorithms for the Simulation of Gauge Theories with Dynamical
Fermionic Degrees of Freedom , is applied at different physical
locations in parameter space. At all points, results from the HMC method
are available. This allows for a direct comparative study.

Section 19 directly compares the efficiencies of MB algorithms with that
of the HMC. This investigation is not limited to the MB-variant
discussed so far, but also covers the non-Hermitian variant with
UV-filtering and gauge field overrelaxation as proposed by de Forcrand
in [ 220 ] (cf. Sec. 11.3 ). These results might be of importance for
future simulations of gauge theories with dynamical fermions, see e.g. [
238 ] . The study is carried out on equal lattice volumes and with
identical physical parameters. Hence, it will allow to probe the scaling
of the algorithms as the continuum limit is approached.

### 18 Simulation Runs at Different Parameters

The TSMB variant of the multiboson algorithm is applied to situations at
different physical points in parameter space with two dynamical fermion
flavors. A direct comparison with the HMC algorithm is given. The latter
acts as a benchmark for the alternative proposals.

The HMC simulations have all been carried out on volumes @xmath . The
physical parameters of the various runs performed here and in the rest
of this chapter are compiled in Tab. 17 and have been taken from [ 148 ]
, see also [ 228 , 227 ] . The second line is identical to Tab. 3 in
Chapter Advanced Algorithms for the Simulation of Gauge Theories with
Dynamical Fermionic Degrees of Freedom .

The multiboson algorithm has been operated on rather small lattices with
volumes @xmath and @xmath . Measurements of the physical masses can be
expected to differ from those on the larger volumes due to finite-size
effects [ 148 ] . Hence, runs on such small lattice sizes can only be
preliminary studies which need to be supplemented later by runs on
larger volumes. The quantity under consideration here is the average
plaquette. This observable will exhibit only a weak dependence on the
lattice volume. These runs corroborate the tests performed in Sec. 16.3
of Chapter Advanced Algorithms for the Simulation of Gauge Theories with
Dynamical Fermionic Degrees of Freedom .

Table 18 gives the total statistics in numbers of trajectories entering
the analysis. The complete data set as given in Tab. 6 has been
exploited, therefore the statistics in this particular case is enormous.
Furthermore, the machines on which the data have been sampled are shown.

Table 19 lists the resulting values for the average plaquette together
with their standard errors for the different algorithms used. The
standard errors have been obtained using the Jackknife method on the
plaquette time series.

The plaquette values obtained with the HMC coincide with those generated
by the TSMB algorithm up to three digits ¹⁹ ¹⁹ 19 The residual deviation
is caused by finite-size effects. However, it may also indicate that the
autocorrelation times are underestimated and the actual errors of the
plaquettes are still larger .

In conclusion, the TSMB implementation indeed produces identical
plaquettes. This comparison demonstrates the correct implementation and
the correct execution of the MB algorithm.

### 19 Efficiency of Multiboson Algorithms

In this section, three different algorithms for the simulation of
Lattice QCD with two dynamical fermion flavors are compared. The
physical parameters are the ones given in the second line of Tab. 17 .
In all cases, the lattice volume has been chosen to be @xmath . This
allows for a measurement of hadronic masses and opens the stage for a
direct comparison of both algorithms.

The HMC algorithm has been introduced in the previous section, see Tab.
18 . The two variants of MB algorithms used are the implementation with
quadratically optimized polynomials discussed in the previous section
Sec. 18 and an implementation based on the UV-filtered non-Hermitian
approximation. The latter code has been written by M. D’Elia and Ph. de
Forcrand . Both programs have been implemented on the APE -100 QH4
installed at DESY /Zeuthen. While the former variant uses heatbath
sweeps for the gauge field updates (it will be called “MB-HB” in the
following) and the TSMB correction step, the latter uses overrelaxation
sweeps (in the following abridged with “MB-OR”) for the gauge field and
an exact correction step. To reflect the different updating strategies
used, the algorithms are named after the corresponding local gauge field
updates.

#### 19.1 Tuning the MB-HB Algorithm

For the MB-HB algorithm, the question arises how the acceptance rate of
the correction step changes with the volume and what the consequences
for the polynomial orders are. Table 20 shows two different choices of
parameters and the corresponding acceptance rates. Thus, the choice
@xmath gives an acceptance rate of about @xmath , while for smaller
values of @xmath the acceptance rate is decreasing rather fast. We
observe a significant volume dependence since in the case @xmath one
only needs @xmath to get similar acceptance rates (consult Tab. 6 ). For
the simulation run to be presented below the parameters from the second
line have been taken.

The updating strategy is shown in Tab. 21 and is chosen similar to Tab.
5 in the previous chapter. Note, as has been discussed in Sec. 15.4 ,
the boson fields do not have to be restored if the correction step
rejects a proposed gauge field configuration.

The heatbath algorithm (cf. Sec. 10.2 ) has been employed for the gauge
field updates. It is important to notice that a single-hit heatbath
algorithm is sufficient if the scheme from [ 157 ] is used. In fact,
acceptance rates exceeding @xmath have been observed when generating the
distribution for @xmath (consult Sec. 10.2 for the notation).

In the version of the program employed, the contribution of the
unit-submatrix from the even points in the noisy vectors has been
included in the noisy correction step. This resulted in a systematic
error of about @xmath when applying Eq. ( 210 ). If this had not been
done, one could have reduced the polynomial orders @xmath and @xmath .
We do expect this to influence neither the stochastic averages nor the
autocorrelation times in terms of sweeps, however.

#### 19.2 Tuning the MB-OR Algorithm

In contrast to the former variant, the other MB algorithm does not make
use of a polynomial approximation in the correction step but makes an
adaptive inversion. As has been discussed in Sec. 11.3 , this requires a
nested iteration of an adaptive inversion and the polynomial @xmath ,
which acts as a preconditioner. This approach has the great practical
advantage that no multicanonical reweighting for the measurement of
observables is necessary, but has the shortcoming that one has an
increased effort once configurations with exceptionally small
eigenvalues are encountered. In the present case, however, we do not
expect this to have a major influence.

The power of the GMRES polynomials in conjunction with UV-filtering for
the non-Hermitian Wilson matrix is demonstrated if one considers the
order of the polynomial @xmath required to arrive at an acceptance rate
of @xmath . The polynomial needed in this case has an order of only
@xmath .

Thus, the number of boson fields could have been reduced by a factor of
@xmath (and even more if one aims for an acceptance rate of about @xmath
). This number takes into account the combined effect of using the
non-Hermitian Wilson matrix, employing the expansion of Eq. ( 186 ), and
using the GMRES algorithm instead of quadratically optimized
polynomials. To actually find this polynomial, however, a thermalized
gauge field configuration had to be provided from the HMC run. Had this
configuration not been available prior to the run, the run would have
had to be performed with a non-optimal polynomial instead for
thermalization. This would have increased the total investment into the
algorithm.

The precise update sequence for a single trajectory is given in Tab. 22
. In contrast to the former multiboson implementation discussed in Sec.
19.1 , only overrelaxation sweeps (see Sec. 10.3 ) have been used for
the gauge field. Although this algorithm alone is non-ergodic,
ergodicity is ensured by the boson field global quasi-heatbath (this
method has been discussed in Sec. 10.2 ). In particular, instead of only
two gauge field updates between a correction step, in total @xmath gauge
field updates are being run. However, the mixing of gauge and boson
field updates requires to restore both kinds of fields in case the
correction step rejects the current configuration. This results in much
larger memory requirements. As has been argued in Sec. 15.4 , one can
expect that this updating sequence results in a faster decorrelation
than the updating sequence in Tab. 21 .

In conclusion one can expect that the MB-OR implementation may perform
better since both the number of boson fields is reduced significantly
and the updating sequence ensures a faster decorrelation.

#### 19.3 Direct Algorithmic Comparison

The observables under consideration were the average plaquette, the
(non-singlet) pseudoscalar meson mass (denoted as pion @xmath ) and the
(non-singlet) vector meson mass (denoted as rho-meson @xmath ). Their
expectation values (for the three different algorithms) together with
their standard errors are shown in Tab. 23 . The hadronic masses have
been taken from Orth [ 148 ] .

The plaquette values agree within errors, while the meson masses agree
within at most two standard deviations. The statistics for the MB-HB
algorithm is worse than in the other cases.

Table 24 shows the resulting total efforts as defined in Eq. ( 214 ) for
the three algorithms employed. The quantities under consideration are
the meson masses. The efforts have been computed by Orth in [ 148 ]
using the Jackknife method. See also [ 239 ] for the latest results.

Finally, the plaquette is investigated. The time series for the HMC
method at these physical parameters has already been examined in Sec.
15.1 . Figures 46 (this figure is identical to Fig. 21 ), 47 and 48 show
the autocorrelation functions and the corresponding autocorrelation
times computed for the plaquette histories from the HMC, the MB-HB and
the MB-OR algorithms respectively.

The efforts for each single trajectory, the corresponding
autocorrelation times, and the total efforts to obtain one statistically
independent plaquette measurement are listed in Tab. 25 . Note that — as
has been pointed out in Sec. 19.1 — the effort for a single trajectory
could have been reduced in the case of the MB-HB algorithm. The
integrated autocorrelation times have been determined using the
windowing procedure which has been discussed in Sec. 8.5 .

The time series from the HMC algorithm contains @xmath autocorrelation
times which is sufficient to obtain a reliable estimate for the
autocorrelation time. The MB-OR algorithm was run for @xmath
autocorrelation times, which should be enough for a good estimate. The
MB-HB algorithm, however, has only accumulated of the order of @xmath
autocorrelation times if the value of @xmath is correct. This is too
short for a safe determination of @xmath , therefore, these numbers have
to be taken with a grain of salt. One cannot be sure that already the
longest mode has been measured in the time series, but one can consider
the autocorrelation mode giving rise to this value as a lower limit of
the true autocorrelation time.

As it has already been anticipated, the MB-HB algorithm can not compete
with the MB-OR algorithm at this point in parameter space. The observed
autocorrelation time for the plaquette in terms of trajectories is a
factor of about @xmath larger than for the MB-OR algorithm. However, the
statistics which went into the MB-HB run is not yet sufficient. Given
the large difference in the number of boson fields and the small number
of gauge field updates between the noisy corrections during each
trajectory compared to the MB-OR algorithm, the efficiency may
consequently be even worse than what is expressed in Tab. 25 . The
results for the meson masses (cf. Tab. 24 ) are compatible with the
these findings. Again, the numbers should only be considered to be lower
limits and may not capture the longest mode of the time series in
question.

When comparing the MB-OR and the HMC algorithms regarding the plaquette
autocorrelation times, one finds that the algorithms are similarly
efficient. In the case of the meson masses, the problem occurs that for
the HMC only the configuration at every @xmath th trajectory has been
analyzed. There is no residual autocorrelation in the sample, therefore
the actual autocorrelation times may be even smaller than the numbers
given in Tab. 24 .

In conclusion, at the physical point given in Tab. 3 , the MB-OR
algorithm performs for the decorrelation of the hadronic masses at least
as good as the HMC. For the measurement of hadronic masses, the results
are similar. However, one finds that the tuning of MB algorithms is
crucial for their performance.

#### 19.4 Scaling Behavior of Algorithms

The ultimate goal of Lattice QCD simulations has been formulated in [ 10
] (cf. Sec. 5.3 ), namely the demand to simulate with three light
fermionic flavors at quark masses of about @xmath . For this goal to be
reached, an algorithm is required which has a sufficiently weak critical
scaling exponent when approaching the chiral regime (see Eq. ( 113 )).
The challenge is now to apply the algorithms from the previous
comparison to a point in phase space with lighter fermion masses. The
point has been chosen from the third line in Tab. 17 , i.e. @xmath and
@xmath . It corresponds to lighter quark masses and should allow to shed
some light on the scaling behavior of the algorithms under
consideration.

The updating sequence of the multiboson algorithm has been chosen
identical to the previous run, see Tab. 22 . The number of boson fields
had to be increased, however, and is now @xmath . This results in an
acceptance rate of @xmath .

As has been found in Eq. ( 215 ), the total cost for a single trajectory
should depend quadratically on the number of boson fields, @xmath . From
the cost obtained in Tab. 25 for @xmath we read off that an estimate for
the cost with @xmath is given by

  -- -------- -- -------
     @xmath      (216)
  -- -------- -- -------

This estimate neglects the non-quadratic contribution of the correction
step to the trajectory, but should still be a good approximation given
the fact that the updating sweeps dominate the total cost.

The number of trajectories performed in each case together with the
average plaquette is listed in Tab. 26 . The plaquettes coincide within
their standard errors.

The autocorrelation functions corresponding to the plaquette histories
are displayed in Figs. 49 and 50 .

The corresponding efforts for a single trajectory, the integrated
autocorrelation times for the plaquettes and the resulting efforts are
given in Tab. 27 . The statistics for the HMC algorithm are now more
than @xmath autocorrelation times, while the MB-OR has generated about
@xmath autocorrelation times. These numbers should allow for a reliable
estimate of the efficiencies in both cases. In addition, the cost
estimate from Eq. ( 216 ) is in excellent agreement with the measured
effort @xmath in Tab. 27 .

In light of these results, it is clear that the MB-OR gained a lot of
ground in comparison to the HMC. For the former, the total effort to
generate one statistically independent configuration only increased by a
factor of about @xmath , while for the latter the effort has increased
by a factor of @xmath . The MB algorithm has become an overall factor of
almost three more effective than the HMC. For the simulation of two
light, degenerate fermion flavors, the algorithm of choice is therefore
definitely a multiboson algorithm.

### 20 Summary

It has been shown that all implementations of MB algorithms considered
indeed produce the same physical results as the HMC algorithm. However,
MB algorithms are more complicated to operate and tune and it has turned
out that a suboptimal choice can easily lead to a degradation in
performance. The efficiency of MB algorithms depends strongly on the
polynomial and the updating sequence. The optimal setup for the
polynomial at the chosen working points has been identified in Sec. 14.2
. Furthermore, it has been discussed in Sec. 15.4 that one should apply
sufficiently many gauge field updates between the correction steps to
ensure a fast decorrelation. In this way, the field updates will
dominate the runtime of the algorithm and lead to an optimal
exploitation of resources.

For intermediate quark masses the HMC is able to perform equivalently to
a well-tuned multiboson algorithm. When going to lighter quark masses,
however, the MB will pretty soon outrival the HMC. It still remains to
be seen, to what extend a non-Hermitian polynomial approximation is a
viable candidate for further simulations in the deep chiral regime as
they are planned in [ 238 ] . One may have to switch to a Hermitian
approximation after one starts to encounter “exceptional” configurations
with negative real eigenvalues to get reasonable acceptance rates. This
step might be accompanied with an increase of @xmath . However, first
indications regarding the behavior of the smallest real eigenvalues in
simulations in the deep chiral regime are given in [ 240 ] and
references therein. These preliminary results hint that in actual
simulations the sign problem may be absent unless one gets extremely
close to the chiral limit.

The optimal tuning of the MB algorithm can only be found after a certain
runtime has already been invested since the best polynomial
approximating the fermionic contribution to the action can only be
gained from one or more thermalized gauge field configurations. This
additional effort requires more logistics and should also be considered
when estimating the efficiencies.

Due to the price in complexity one has to pay, the HMC can consequently
still be the preferred choice whenever it can be expected to be
comparable or only slightly inferior to MB algorithms. Nonetheless, for
simulations at very light quark masses close to the physical regime, it
cannot be expected that the HMC is competitive anymore. An excellent
candidate for future simulations at such masses is therefore the MB
algorithm.

## Chapter \thechapter Exploring the Parameter Space with Three
Degenerate Dynamical Flavors

Up to this point, the emphasis has been put on the simulation of two
degenerate dynamical fermion flavors. However, as has been argued in
Sec. 5.3 , realistic numerical simulations of Lattice QCD require a
simulation with three dynamical fermionic degrees of freedom. Reference
[ 10 ] shows that it is sufficient to concentrate first on the case of
three mass-degenerate dynamical fermion flavors with dynamical quark
masses of the order of @xmath . One possible goal is to obtain the
Gasser-Leutwyler coefficients from those runs. However, such an endeavor
requires lattice sizes and Wilson-matrix condition numbers beyond what
we are capable of handling today.

In this section, a first step in such type of program will be taken,
namely the application of a multiboson algorithm with TSMB correction
step to this physically interesting situation.

In order to prepare the stage, we will work on @xmath and @xmath
lattices. This will help to acquire some insight onto the chances of
doing more realistic simulations on @xmath lattices, as previously
carried out for @xmath in the SESAM -project [ 228 , 133 ] . So the
question is whether, in the @xmath scenario, we can establish an
operational window to achieve a reasonably large pion correlation length
without hitting the shielding transition that has been found in @xmath
at finite volumes and fixed @xmath , as @xmath was increased towards
@xmath .

Section 21 gives a short overview of the determination of the non-zero
temperature crossover and the shielding transition. It is important to
avoid this region in parameter space since the physical properties of
the non-hadronized region are different from the zero-temperature phase
of QCD. In particular, no hadrons are expected to exist and consequently
one cannot extract useful information on their masses.

The physically interesting point in parameter space in an infinite
lattice volume @xmath is the critical point where the Wilson matrix
describes massless fermions. This property has been discussed in Sec.
6.4 . The practical ways to find this chiral limit are reviewed in Sec.
22 .

The application to two different values of @xmath is discussed in Sec.
23 . These runs have been performed with the TSMB algorithm and might
allow to identify a potential working point for future simulations.

At this stage we would like to mention some previous algorithmic work on
@xmath physics, which was mainly carried out at finite temperatures.
Reference [ 241 ] presents a detailed study of the thermodynamical
properties of three flavor QCD. It employs the @xmath -algorithm for the
numerical simulations.

Note that, algorithmically, extensions of the HMC can also be used for
these kinds of simulations [ 186 , 188 , 189 , 242 ] .

### 21 The Non-Zero Temperature Crossover

It is expected that the phase space of QCD contains a “deconfined
phase”, where chiral symmetry is restored and the quarks and gluons form
a plasma with color-charges being Debye-screened. This transition takes
place at some critical temperature. For general introductions to this
topic consult [ 24 , 32 ] . This phase is interesting for the
description of hadronic matter at high temperatures and densities.
However, when performing simulations relevant for the low-temperature
phase of QCD — where the phenomenology is dominated by hadronized
particles — this phase should be avoided.

This transition is accompanied by a jump in the free energy of the
system. An order parameter is given by the Polyakov loop, which is
defined to be [ 243 ] :

  -- -------- -- -------
     @xmath      (217)
  -- -------- -- -------

with @xmath being a point in three-space, and @xmath and @xmath the
spatial and temporal lattice sizes, respectively. The physical picture
of @xmath is the description of the average world line of a static
quark. Information about the free energy of a static quark-antiquark
pair can be obtained from the correlation of two such loops having
opposite direction

  -- -------- -- -------
     @xmath      (218)
  -- -------- -- -------

One can show [ 32 ] that this quantity is related to the free energy
@xmath of a static quark-antiquark pair via

  -- -------- -- -------
     @xmath      (219)
  -- -------- -- -------

Assuming that @xmath satisfies clustering, one finds

  -- -------- -- -------
     @xmath      (220)
  -- -------- -- -------

Hence, one obtains that if @xmath , the free energy increases for large
@xmath with the separation of the quarks. This is a signal for the
hadronization phase.

Therefore the order parameter indicates the phase of the system via

  -- -------- -- -------
     @xmath      (221)
  -- -------- -- -------

This argumentation so far is only valid in the absence of dynamical
quarks. It may, however, also be extended to the case of dynamical
quarks with finite mass, see [ 32 ] . In this case, the Polyakov loop
might similarly indicate the non-zero temperature crossover.

Up to this point, the discussion has always considered the case where
the temporal lattice extension is smaller than the spatial one, @xmath .
In actual simulations, the situation can also arise that a transition
similar to the non-zero temperature crossover occurs for too small
lengths @xmath , even if @xmath is sufficiently large. In this case, one
is similarly unable to measure hadronic masses properly. This phenomenon
is called the shielding transition .

### 22 The Chiral Limit

Of particular importance for any simulation of QCD is the critical line
in parameter space, where the mass of the pion vanishes. The vicinity of
this point allows for a treatment using @xmath PT, as it has been argued
in Sec. 5.3 . As explained in Sec. 6.4 , the Wilson matrix then contains
a zero-mode.

The critical line can be found by varying the hopping parameter @xmath
appearing in the action Eq. ( 91 ) at a fixed value of the gauge bare
parameter @xmath . Then one has to find the critical value @xmath ,
where the fermionic contribution to the action describes massless
fermions. Repeating this procedure for several values of @xmath yields
the critical line in parameter space. This procedure is impeded once the
shielding transition sets in.

A qualitative illustration of the shielding transition and the critical
behavior is given in Fig. 51 . The figure shows the squared pseudoscalar
meson mass, @xmath , at a fixed value of @xmath , as a function of
@xmath . The solid curve shows the mass in the infinite volume limit,
@xmath . The dotted line corresponds to a correlation length @xmath .

As has already been pointed out, finite-size-effects (FSE) will induce
the shielding transition which might inhibit a reliable extraction of
zero-temperature physics. We illustrate this scenario by sketching the
FSE for two different volumes, @xmath , with lengths @xmath . When
measuring the mass on the smaller lattice volume, @xmath , one finds
that the curve can be followed reliably up to the point @xmath . Beyond
this point, the shielding transition sets in and the mass can no longer
be measured correctly on the smaller volume. The larger lattice volume,
@xmath , allows to go closer to the critical point, but will still run
into finite-size-effects at some higher value, @xmath . The “true” value
of @xmath (as defined in the limit @xmath ) can be estimated the better
the larger the available volume.

Lattice results become meaningful, once the pseudoscalar correlation
length, stays larger than unity, @xmath . This condition is impossible
to fulfill on the lattice @xmath — the shielding transition sets in
before the desired parameter region is reached. On the lattice @xmath ,
however, it is in fact possible to go beyond @xmath before shielding is
observed. Hence, for a given set of parameters one has to increase the
lattice volume until one reaches a “window”, where the FSE are under
control while the mass already became sufficiently small.

But how to estimate @xmath ? On a given large enough lattice, one can
use the following recipes (see also Sec. 6.4 )

1.  the point in @xmath -space where the condition number of the
    Hermitian Wilson matrix @xmath diverges,

2.  the point where the smallest real eigenvalue of the non-Hermitian
    Wilson matrix @xmath reaches the imaginary axis,

3.  the point where the pseudoscalar meson mass @xmath vanishes. This is
    the physical definition of the chiral limit.

Comment: From a physical point of view, the last criterion is the
approach of choice for estimating the critical point. The first two
definitions will coincide and give identical results since, in both
cases, the matrix contains a zero-mode. Furthermore, the mass of the
pseudoscalar mesons is strongly dominated by the smallest eigenvalues
and this dominance becomes more pronounced as the chiral limit is
approached, cf. [ 139 ] . Hence, the results from all these methods will
coincide sufficiently close to the chiral limit ²⁰ ²⁰ 20 However, it is
extremely difficult to actually work “sufficiently close” to the chiral
limit . For larger masses, however, one can expect that the results from
the methods differ in practical simulations.

To properly apply the physical definition, one can use an extrapolation
inspired by @xmath PT. To be specific, one employs (see e.g. [ 228 ] )

  -- -------- -- -------
     @xmath      (222)
  -- -------- -- -------

Strictly speaking, @xmath PT only applies in the continuum. However, it
is customary to nonetheless use such type of fitting function at a fixed
value of @xmath , see again [ 228 ] and also [ 227 ] for latest results.
Furthermore, this relation might have to be modified by logarithmic
corrections which could cause the linear behavior predicted by Eq. ( 222
) to be inaccessible in current simulations [ 10 ] . For the moments of
structure functions it has indeed been shown in [ 42 ] that a
logarithmically modified extrapolation formula appears to yield best
agreement with experimental data. Therefore, one should be careful when
interpreting all predictions obtained by linear fits only.

### 23 Explorative Studies

In this section, results from simulations at two different values of
@xmath are presented, namely at @xmath (Sec. 23.1 ) and @xmath (Sec.
23.2 ). For several values of @xmath , the average plaquette is
determined. In both cases, the critical value, @xmath , is measured. In
the latter case, both methods discussed in Sec. 22 are applied, while in
the former case only a single method is used.

A discussion about prospects for future simulations concludes these
investigations.

#### 23.1 The Case @xmath

The simulations discussed here have been run at a value of @xmath on
lattices with volume @xmath and varying values of @xmath .

The different values of @xmath , the number of trajectories after
thermalization, and the resulting average plaquette values are listed in
Tab. 28 together with an estimate for the integrated autocorrelation
time of the average plaquette. The standard errors on the plaquettes
together with the estimate for @xmath have been determined using the
Jackknife method. This data has been obtained from runs on both the
Nicse and the ALiCE clusters, see Sec. 16.2 for further details. The
algorithmic parameters have been varied in the runs. Table 29 shows the
algorithmic parameters together with the resulting acceptance rates.

The average plaquette is visualized in Fig. 52 . Between @xmath and
@xmath a large jump in the plaquette occurs which indicates the presence
of the shielding transition. The values beyond this transition are
therefore not particularly interesting and hence less statistics has
been generated. An estimate for the autocorrelation time has not been
obtained here. Therefore, the statistical error may be underestimated.

For the determination of the critical value, @xmath , the first method
from Sec. 22 is adopted. Table 30 shows the average smallest and largest
eigenvalues of @xmath . The eigenvalues have been computed every @xmath
trajectories, and the errors have again been estimated using the
Jackknife method.

Figure 53 shows the resulting plot of @xmath vs. the inverse condition
number @xmath of @xmath .

The straight line is a fit to the points between @xmath and @xmath which
is parameterized by

  -- -------- -- -------
     @xmath      (223)
  -- -------- -- -------

From the point where @xmath diverges (and thus @xmath ) one finds

  -- -------- -- -------
     @xmath      (224)
  -- -------- -- -------

This method requires little effort and has a rather small error on the
critical value of @xmath . However, the estimate ( 224 ) still contains
a systematic uncertainty due to the fact that one is still rather far
from the chiral regime.

#### 23.2 The Case @xmath

The point considered in the previous section already showed signs of the
shielding transition as the condition number of @xmath still was below
@xmath . Hence, this value of @xmath does not allow to probe the chiral
regime further if one is limited to such small lattices. It can,
however, be considered as a working point for future studies on larger
lattices. As a different starting point, the focus will now be placed on
the point @xmath with lattice sizes of @xmath instead. This lattice size
might already allow for a measurement of the ratio @xmath for degenerate
sea and valence quark masses and hence for an independent estimate of
the chiral transition. Again, the finite-temperature phase of QCD has to
be avoided.

For the actual simulation, again several values for @xmath have been
chosen. The polynomial parameters are given in Tab. 31 . The runs have
been performed on the ALiCE -cluster with a partition of eight nodes for
each run.

In general, one can expect that the polynomial orders and intervals are
chosen somewhat conservatively and one could achieve some gain by
adapting them manually with respect to the spectrum of @xmath obtained
during the production. Despite the lengths of the runs, it might still
make sense to improve the statistics further.

The working points chosen are listed in Tab. 32 together with the
acceptance rate of the noisy correction step, the number of performed
trajectories, and the average plaquette with the error determined from
the Jackknife method. From the Jackknife estimate, the plaquette
autocorrelation time has been determined. Finally the correction factor
with its standard deviation is shown.

The plaquette for the run at @xmath showed a fluctuation between two
different points and is plotted in Fig. 54 . This is an indication that
the shielding transition takes place around this point. Since the series
is too short to make any statement about this fluctuation, the standard
error is not shown here.

In the cases @xmath and @xmath the autocorrelation time appears to be
very large. Hence, the statistics are still comparatively small at these
working points.

The magnitude of the reweighting factors in Tab. 32 confirms the
expectation that the polynomial has been chosen very conservatively in
most cases. However, the run at @xmath has a large fluctuation in the
reweighting factor, which means that the smallest eigenvalue went off
the polynomial interval. The precise situation is displayed in Fig. 55
after the thermalization phase has been subtracted. If this run was to
be continued, one may consider to use polynomials with a smaller value
of the lower limit for the approximation interval. The properly
reweighted values may still be used for this analysis, but the
statistics may be worse for this case. For the other simulation runs,
one can conclude that reweighting can safely be disregarded.

Figure 56 shows the resulting values of the average plaquette as a
function of the hopping parameter @xmath . This plot corroborates that
the shielding transition is located around @xmath .

##### 23.2.1 Locating the Shielding Transition

As a first guideline of where the crossover to the shielded phase takes
place, the plaquette fluctuation in Fig. 54 and the jump in the average
plaquette in Fig. 56 have been considered. To gain further insight one
can investigate the behavior of the average Polyakov loop, see Sec. 21 .
However, in this case it should now be measured in spatial (i.e. @xmath
, @xmath , and @xmath ) direction since the @xmath -direction is now the
longest. The Polyakov line in that direction can be expected to show no
sign of the finite-temperature phase.

The Polyakov loops have been measured every @xmath trajectories. The
resulting values are shown in Fig. 57 . Starting with @xmath , one
clearly sees a clustering in one of the three sectors. It is surprising
that despite the rather long runs, in each case the values are clustered
in only one sector. This indicates that the samples are not decorrelated
with respect to this observable. At @xmath the shielding transition is
not yet apparent in the Polyakov loop. However, when considering the
previous indications, it appears safer to disregard the latter run from
the following analysis.

##### 23.2.2 Computing @xmath

The details for the measurement of hadronic masses have been given in
Sec. 9 . As has been discussed above, only the points @xmath should be
considered for this analysis. The reweighting factor has been included,
although it had no practical influence in these productions.

In the run with @xmath the correlation functions for the (non-singlet)
pseudoscalar and the vector mesons are visualized in Figs. 58 and 59 .
These functions have already been symmetrized, i.e. the plot shows
(cf. Eq. ( 126 ))

  -- -------- --
     @xmath   
  -- -------- --

with @xmath being the lattice extension in @xmath -direction.

These functions should follow the behavior given in Eq. ( 127 ).
However, for small values of @xmath , one expects the results to be too
large (due to the contamination with higher modes, cf. Sec. 9 ), while
for larger values of @xmath , larger autocorrelations of the greater
lengths may result in worse statistics.

To obtain an estimate for the autocorrelation time of these masses, the
Jackknife method has again been employed. The case which is considered
in detail is the run at @xmath . Figures 60 and 61 show the variances of
the masses for a fit interval from timeslice @xmath to timeslice @xmath
.

The resulting values are given in Tab. 33 together with the variance
estimate for bin size @xmath . By exploiting Eq. ( 121 ), one can as
usual obtain an estimate for the autocorrelation time of the quantity
under consideration.

Since the correlators have been computed every @xmath trajectories, the
results imply that the @xmath - and @xmath -mesons have autocorrelation
times of @xmath and @xmath trajectories, respectively. These numbers are
slightly better than what the plaquette has indicated, albeit still
large. A source of this problem is the choice of the first polynomial.
If the GMRES method had been used instead, one might have achieved a
faster decorrelation by reducing the polynomial order, @xmath , see Sec.
14.2 and also Sec. 19.3 .

Figure 62 shows the resulting values for masses in lattice units. The
lower limit of the fit is given by the timeslice @xmath , while for the
upper limit, always the next-to-last limit has been used, i.e. @xmath .
The error is again taken to be the standard error, which has been
computed using the Jackknife procedure as above in Tab. 33 . The method
follows the results discussed in [ 150 , 148 , 227 , 9 ] .

The plateaus in Fig. 62 are reached at @xmath . Therefore, the values
obtained at this point will be used in the following. Table 34
summarizes all results together with the autocorrelation times
determined using the Jackknife scheme. In the case @xmath , no plateau
could have been identified and the results are compatible with an
integrated autocorrelation time below @xmath trajectories.

##### 23.2.3 Locating the Critical Point

To locate the critical point, again the smallest and largest eigenvalues
have been computed and the condition numbers have been determined for
the runs at @xmath . The eigenvalues have been computed every @xmath
trajectories. Table 35 summarizes the findings.

The inverse condition number is plotted vs. the inverse quark mass in
Fig. 63 .

The fit to all three data points yields

  -- -------- -- -------
     @xmath      (225)
  -- -------- -- -------

The zero of the line gives

  -- -------- -- -------
     @xmath      (226)
  -- -------- -- -------

Finally, the fitting function from Eq. ( 222 ) is applied to the
situation at hand with the pion masses given by Tab. 34 . In Fig. 64 the
inverse value of the quark mass, @xmath , is plotted versus the square
of the pion mass, @xmath . In addition, the rho mass, @xmath , is also
included in this plot. The former is visualized as circles, while the
latter is pictured by squares. The shielding transition is shown as a
magenta bar.

The linear fit to @xmath is given by the solid green line in Fig. 64 .
The curve is parameterized by

  -- -------- -- -------
     @xmath      (227)
  -- -------- -- -------

The critical value of @xmath is then found to be

  -- -------- -- -------
     @xmath      (228)
  -- -------- -- -------

The result from Eq. ( 228 ) agrees within the errors with the previous
result from Eq. ( 226 ).

Furthermore, a quadratic curve has been drawn through the values for
@xmath , given by the green dashed line. It is parameterized by

  -- -------- -- -------
     @xmath      (229)
  -- -------- -- -------

When using this curve, the resulting value for @xmath is found to be

  -- -------- -- -------
     @xmath      (230)
  -- -------- -- -------

The linear fit to @xmath is given by the blue line in Fig. 64 . The
curve is parameterized by

  -- -------- -- -------
     @xmath      (231)
  -- -------- -- -------

Obviously, it is not possible to reach values of @xmath before the
shielding transition sets in on the current lattice size, cf. Sec. 22 .
Therefore, the linear extrapolation in Eq. ( 227 ) may be biased with an
uncontrolled systematic uncertainty. To estimate this effect, one may
compare the resulting critical point, Eq. ( 228 ), with the result
obtained from the quadratic fit, Eq. ( 230 ). This uncertainty makes
further investigations closer to the chiral point necessary and
consequently implies the need to go to larger lattices.

##### 23.2.4 Prospects for Future Simulations

Up to this point, one could only achieve ratios of @xmath with @xmath .
When going to larger lattices, the shielding transition will set in at
larger values of @xmath , allowing to probe lighter quark masses.

A procedure for continuing along this line of research consists of going
to @xmath lattices, starting from @xmath until the shielding transition
for the new lattice sets in. In light of the fact that the shielding
transition in Fig. 64 is located shortly before one arrives at @xmath ,
a lattice size of @xmath might already be sufficiently large to obtain a
set of data points all fulfilling the requirement @xmath . In such a
situation, one could obtain an extrapolation to the chiral point, @xmath
, with reduced systematic uncertainty.

With the available information we can, however, still try to locate a
working point at this particular value of @xmath in the @xmath - @xmath
-plane with properties similar to the point chosen for the SESAM
-project [ 133 ] .

This working point will now be denoted @xmath . With the uncertainties
discussed above in mind, we imposes the following constraints:

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (232)
  -- -------- -------- -------- -- -------

The actual parameters can be identified from the extrapolations Eqs. (
227 ) and ( 231 ). First, from setting @xmath , we obtain

  -- -------- -- -------
     @xmath      (233)
  -- -------- -- -------

From the requirement ( 23.2.4 ) that the value of the finite-size
parameter should be @xmath , one finds, in accordance with the SESAM
-data from [ 227 ] , that one has to go to lattices with at least @xmath
if one wants to explore this region in parameter space.

The parameters for this working point are summarized in Tab. 36 . The
estimated values for @xmath , @xmath , and @xmath have been computed
from the fit ( 231 ). The total physical lattice size @xmath would then
be @xmath . A possible criticism against this working point might be
that this lattice spacing is rather coarse. To actually increase the
resolution, one would need to go to higher values of @xmath , thus
moving closer to the continuum limit.

Finally, the question arises how large the total effort might be for
such a project. For the case of quadratically optimized polynomials, it
has been argued in [ 218 ] , that the required increase in @xmath when
going from @xmath to @xmath is only of the order of about @xmath .
Reference [ 240 ] confirms this finding by stating that going from
@xmath to @xmath will only increase @xmath by about @xmath . Taking — as
a very conservative estimate — the latter number to be applicable also
to the simulations performed for @xmath in chapter Advanced Algorithms
for the Simulation of Gauge Theories with Dynamical Fermionic Degrees of
Freedom , we find by applying Eq. ( 215 ) that the total cost for an
independent configuration (with respect to the plaquette) is about

  -- -------- -- -------
     @xmath      (234)
  -- -------- -- -------

when considering the lightest quark mass, where @xmath . Hence, this
estimate marks the upper limit for the effort required in a simulation
similar to the SESAM -project, provided one decides to take recourse to
an MB algorithm. The total cost quoted in Eq. ( 234 ) is still smaller
than the corresponding cost for the HMC run with @xmath . Therefore, one
can expect the simulations at the lighter quark masses to be even
cheaper than they were in the case of the SESAM -project.

### 24 Summary and Outlook

A first step towards the simulation of QCD with three degenerate
dynamical quark flavors has been taken. The TSMB algorithm been applied
successfully to this physically interesting situation. The simulations
have yielded first results for the shielding transitions on the current
lattice size with @xmath and the critical points at two values of @xmath
. It has become clear that there is no window for doing spectroscopy at
the parameters chosen.

Prospects for future simulations have been given and a potential working
point has been estimated, although with large systematic uncertainty. It
has been argued that a study with physical masses similar to the SESAM
-project is feasible today and might even cost slightly less than the
HMC-based program.

In an ongoing research project, such type of simulations will be
performed on larger lattices and closer to the chiral limit. For the
current status of the comprehensive project see [ 240 ] .

A potential obstacle for future simulations with three dynamical fermion
flavors may still be posed by the fermionic sign problem. As has been
noted in Sec. 6.4 , the fermionic determinant will change its sign if an
odd number of real eigenvalues becomes negative. The polynomial
approximations in Sec. 12.1 , however, are applied to the square of the
Hermitian Wilson matrix. Hence, they will always yield a positive sign.
Consequently, the sign would have to be included into the measurement of
observables which may eventually spoil the statistical quality of the
sample. A similar problem is known to occur in the simulation of gauge
theories with a non-zero chemical potential, see [ 190 ] .

Such a problem does not show up for an even number of degenerate fermion
flavors since in that case squaring the Wilson matrix will always yield
a positive sign. The only known way to overcome this obstacle directly
in a sampling process has been found for some quantum spin systems
(cf. Sec. 6.3 ). It is yet unclear, if any quantum spin system similar
to gauge theories with dynamical fermion flavors can be simulated
efficiently in such a manner. However, as has already pointed out in
Sec. 20 , it may be that this sign problem is not significant in actual
simulations of QCD.

## Chapter \thechapter Summary and Outlook

[]

A foresighted strategy can help to find

a winning move in a superior position.

In this thesis algorithms for the simulation of quantum field theories
with dynamical fermionic degrees of freedom have been presented. Special
emphasis has been put on a new class of algorithms, namely the
multiboson algorithms which represent the fermionic determinant by a
number of boson fields. They allow for the implementation of local
updating algorithms, which are known to be superior to global schemes
applicable to gauge theories so far.

A particular variant of these algorithms, the TSMB method, has been
implemented on several machines. This scheme relies on the computation
of powers of matrices using static polynomials. The parameters which fix
a given polynomial are the order and the interval of the approximation.
Beyond that, it has turned out that the choice of the updating scheme is
important.

The optimal settings for these parameters have been determined and the
sensitivity of the system to sub-optimal tuning has been analyzed.
Furthermore, different updating schemes have been examined with respect
to their efficiency and recommendations for the implementations of
multiboson algorithms in general have been given.

Due to the complexity of MB schemes, however, there is still room for
improvement. MB algorithms remain open for refinements in the future,
but can already be used for large-scale simulations today.

Major emphasis has been put on how multiboson algorithms compare to
their competitors in the field of dynamical fermion simulations. We have
shown that, with sufficient tuning, MB algorithms appear to be superior
to the HMC algorithm in the case of light quark masses. We would expect
that further improvements in MB algorithms will be found with growing
experience in future simulations. This might help the MB scheme to
replace the HMC method as the standard algorithm in Lattice QCD.

The final part of this thesis has considered the application of the TSMB
algorithm to the case of three dynamical fermionic flavors, a situation
which is of great importance for realistic simulations of QCD. Based on
this experience, a proposal for future simulations has been formulated.
In fact, one can be optimistic to perform a project similar to SESAM at
reasonable cost. A working point for such type of simulations on @xmath
lattices has been estimated, where semi-realistic simulations with good
statistics should be run. This would provide an assessment of an
operating window in the @xmath scenario.

In conclusion, we find that multiboson algorithms provide a great leap
forward in the simulation of Lattice QCD and give us the means to
perform simulations in realistic scenarios.

## Chapter \thechapter Notations and Conventions

Unless otherwise explicitly stated, natural units have been adopted
throughout this thesis by setting

  -- -------- -- -------
     @xmath      (235)
  -- -------- -- -------

The four-dimensional Minkowski-space is denoted by @xmath and has the
canonical flat-space metric

  -- -------- -- -------
     @xmath      (236)
  -- -------- -- -------

The Wick rotation

  -- -------- -- --------
     @xmath      ( 16 )
  -- -------- -- --------

transforms vectors from Minkowski-space to the Euclidean space @xmath
with the metric given by the Kronecker symbol @xmath :

  -- -------- -- -------
     @xmath      (237)
  -- -------- -- -------

The discrete space of the lattice theory is denoted by @xmath . Vectors
in space are always denoted by @xmath . Unit vectors are only used in
@xmath , where they are written as @xmath with @xmath . If the lattice
volume is finite, the lengths @xmath in direction @xmath are also
denoted by @xmath and @xmath , where @xmath is the lattice size in
“time” and @xmath in “space” direction. The total volume is denoted by
@xmath and the corresponding space is @xmath . For different lengths
@xmath , @xmath , and @xmath , the notation @xmath will be used. For a
bosonic field, @xmath , periodic boundary conditions are imposed:

  -- -------- --
     @xmath   
  -- -------- --

Here @xmath denotes the point adjacent to @xmath in direction @xmath .

The totally antisymmetric @xmath -tensor @xmath obeys

  -- -------- --
     @xmath   
  -- -------- --

The commutator of two objects @xmath for which multiplication and
addition are defined is denoted by

  -- -------- --
     @xmath   
  -- -------- --

The anti-commutator is denoted by

  -- -------- --
     @xmath   
  -- -------- --

## Appendix A Dirac Matrices

The Dirac matrices @xmath , @xmath , in Minkowski space are defined by

  -- -------- -- -------
     @xmath      (238)
  -- -------- -- -------

The matrix @xmath is defined by

  -- -------- -- -------
     @xmath      (239)
  -- -------- -- -------

It satisfies @xmath , and @xmath . The chirality projectors @xmath are
given by

  -- -------- -- -------
     @xmath      (240)
  -- -------- -- -------

When performing the Wick rotation, the Euclidean Dirac matrices are
given by

  -- -------- -- -------
     @xmath      (241)
  -- -------- -- -------

They are related via [ 24 ]

  -- -------- --
     @xmath   
  -- -------- --

The Euclidean @xmath -matrix is given by @xmath .

The matrices with smallest dimension satisfying ( 238 ) are @xmath
matrices [ 11 ] . The representation of the @xmath -matrices employed in
this thesis has been chosen to be the ‘‘chiral’’ one ²¹ ²¹ 21 It should
be remarked that this is different from the representation which has
been employed in the standard TAO -libraries [ 235 ] , where the Dirac
form has been used , where the Minkowski-space matrices are given by

  -- -------- -- -------
     @xmath      
     @xmath      (242)
  -- -------- -- -------

The Euclidean @xmath matrices are then given by

  -- -------- -- -------
     @xmath      
     @xmath      (243)
  -- -------- -- -------

The contraction of a four-vector @xmath with the Dirac matrices is
denoted by

  -- -------- -- -------
     @xmath      (244)
  -- -------- -- -------

## Chapter \thechapter Groups and Algebras

One of the central concepts of particle physics is the notion of
symmetry groups . All particles transform according to a symmetry of
space-time and the symmetries of the Lagrangian. In the relativistic
case this will mean that they transform as representations of the proper
orthochroneous Poincaré-group, see below.

## Appendix B Groups and Representations

A group is a pair @xmath of a set @xmath and a relation @xmath ,
satisfying the following axioms

1.  The operation @xmath is associative, i.e. @xmath holds @xmath .

2.  There is a unit element @xmath satisfying: @xmath holds @xmath .

3.  For every @xmath such that @xmath .

The group is called Abelian (or commutative ) if additionally @xmath
holds. It follows immediately that the unit element is uniquely
determined. Furthermore it follows that the inverse element @xmath for
each @xmath is unique.

A representation @xmath of the group @xmath is a group homomorphism from
@xmath to the group of vector space endomorphisms of a representation
space @xmath , @xmath , @xmath and @xmath with the following properties:

1.  @xmath , i.e. the representation respects the group multiplication
    of @xmath ,

2.  @xmath , i.e. the image of the unit element in @xmath is the
    identity in @xmath ,

3.  @xmath , i.e. the image of the inverse element is the inverse of the
    group element.

A representation is called irreducible if it can not be written as the
direct sum of other representations. Thus, there are no invariant
subspaces under the action of the @xmath for all @xmath . In the
following, a matrix in @xmath (with an appropriate basis) with the above
properties will be called a representation of @xmath .

Particles, as observed in nature, should certainly be independent of the
way we choose our coordinate system, i.e. how we choose the basis for
the representation space @xmath (this requirement parallels the
requirement of the theories to be coordinate invariant). Thus, they
should always be classified by irreducible representations of a group.
These irreducible representations also go under the name multiplet .

Of particular interest to physics are the Lie groups , see for a
textbook [ 244 ] . A Lie group is a group for which the multiplication
law and taking the inverse are smooth functions. Thus, the group space
must be a manifold and one can form the tangent space on any point in
the group. The tangent space on the unit element is called the Lie
algebra of the group. A basis of the Lie algebra is called the set of
generators of the group. Accordingly, an element @xmath of a Lie group
@xmath can be written in terms of the generators @xmath , @xmath as:

  -- -------- -- -------
     @xmath      (245)
  -- -------- -- -------

where the element @xmath is parameterized using the @xmath as
coordinates. The dimension of a Lie group is thus the dimension of the
underlying manifold of the group space. A Lie algebra can be specified
by the structure constants @xmath , which are defined via

  -- -------- -- -------
     @xmath      (246)
  -- -------- -- -------

For the integration over the group space, there exists a unique measure
on @xmath called the Haar measure , @xmath , which obeys:

1.  Consider a function @xmath . Then @xmath obeys for all @xmath

      -- -------- --
         @xmath   
      -- -------- --

2.  The integral is normalized, i.e. @xmath .

It satisfies

  -- -------- --
     @xmath   
  -- -------- --

The rank of a group is the number of generators that simultaneously
commute among themselves. It is thus the maximum number of generators
which can simultaneously be diagonalized.

A Lie algebra is called semisimple , if for some @xmath , there are
@xmath with @xmath . It can be shown, that for any compact Lie group,
the algebra can always be written as the direct sum of a semisimple Lie
algebra and an Abelian one. The semisimple Lie algebras can be
decomposed into a set of groups which are called simple . The latter
cannot be written as sums of anything else. The simple groups fall into
the following categories ²² ²² 22 A compact and readable introduction to
the subject of simple, finite groups and this classification can also be
found in http://math.ucr.edu/home/baez/week63.html ,
http://math.ucr.edu/home/baez/week64.html , and
http://math.ucr.edu/home/baez/week66.html :

1.  The algebra sl @xmath , the @xmath complex matrices with vanishing
    trace. The compact real form of sl @xmath is su @xmath and the
    corresponding Lie group is SU @xmath , the @xmath unitary matrices
    with unit determinant. In the case @xmath , we speak of the group U
    @xmath , which consists of the complex numbers on the unit circle.

2.  The Lie algebra so @xmath , the @xmath skew-symmetric complex
    matrices with vanishing trace. The compact real form is so @xmath ,
    and the Lie group generated is SO @xmath , the @xmath real,
    orthogonal matrices with determinant one. They form the rotation
    group in @xmath -dimensional Euclidean space. The rotation group in
    Minkowski space whose metric changes sign on the diagonal is usually
    denoted with SO @xmath , but still belongs to this category.

3.  The Lie algebra sp @xmath , the @xmath complex matrices of the form

      -- -------- --
         @xmath   
      -- -------- --

    where @xmath and @xmath are symmetric, and @xmath is the negative
    transpose of @xmath . The compact real form is sp @xmath and the Lie
    group is Sp @xmath , which forms the group of @xmath quaternionic
    matrices which preserve the inner product on the space @xmath of
    @xmath -tuples of quaternions.

4.  The Lie algebra so @xmath , the @xmath skew-symmetric complex
    matrices with vanishing trace. This is the even-dimensional analog
    of item 2 for even @xmath . These groups are to be distinguished,
    since the physics in these cases may differ.

Apart from these classical algebras, there are also the groups @xmath ,
@xmath , @xmath , @xmath , and @xmath . Some of these also have
applications in physics, however, so far they are not considered to play
any role for the purposes of this thesis.

The dimensions and ranks of the three important kinds of semi-simple
groups in this thesis are shown in Tab. 37 .

## Appendix C The U@xmath Group

The U @xmath group is a special case of the SU @xmath groups. It
consists of the group of complex numbers on the unit circle. It is a
commutative group since the complex numbers commute under
multiplications.

## Appendix D The SU@xmath Groups

The SU @xmath groups consist of elements isomorphic to the @xmath
unitary matrices with unit determinant:

  -- -------- -- -------
     @xmath      (247)
  -- -------- -- -------

Obviously the matrices @xmath in ( 247 ) form already the fundamental
representation of the SU @xmath group. In this thesis the groups SU
@xmath and SU @xmath play a central role. The generators chosen for the
specific realizations used in this thesis are listed in the following
sections.

#### d.1 The SU@xmath Group

The standard choice for the generators of the SU @xmath group are the
Pauli matrices :

  -- -------- -- -------
     @xmath      (248)
  -- -------- -- -------

The peculiarity of SU @xmath is that these matrices together with the
unit matrix,

  -- -------- --
     @xmath   
  -- -------- --

form a basis of the complex @xmath matrices. The expansion coefficients
form a hypersurface in the space of complex @xmath matrices, where the
expansion coefficients are real. A consequence of this observation is
that any sum of SU @xmath matrices is again proportional to an SU @xmath
matrix. This property only exists in the case @xmath . The
proportionality factor can be computed by considering the inverse of a
matrix @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Then the inverse is given by

  -- -------- --
     @xmath   
  -- -------- --

Consequently, the proportionality factor is given by @xmath , i.e. the
matrix

  -- -------- -- -------
     @xmath      (249)
  -- -------- -- -------

is an SU @xmath matrix.

From Tab. 37 it follows that the group has rank one, thus the
representations correspond to the eigenvalues of a single operator.
Usually, the eigenvalues of @xmath are taken to classify the multiplets.
SU @xmath is locally isomorphic to SO @xmath [ 15 ] , which means that
the algebras of the two groups are identical, although this does not
hold for their global topology. To be specific, SU @xmath is the
double-cover of SO @xmath . While the latter is not simply connected,
the former is.

#### d.2 The SU@xmath Group

In this thesis the Gell-Man matrices @xmath , @xmath , as defined in [
24 ] have been chosen as the generators of the SU @xmath group:

  -- -------- -- -------
     @xmath      (250)
  -- -------- -- -------

## Appendix E The Poincaré Group

The space-time manifold underlying the physical theories discussed in
this thesis is given by the Minkowski-space. The metric is
pseudo-Euclidean and can be transformed globally to the form

  -- -------- -- ---------
     @xmath      ( 236 )
  -- -------- -- ---------

An event is associated with a point in space-time, and the distance
between two events is defined as

  -- -------- -- -------
     @xmath      (251)
  -- -------- -- -------

Here and in the following the Einstein summation convention that
identical indices are to be summed over is understood. The quantity
@xmath is called the norm of @xmath . This norm, however, is not
positive definite. Depending on the sign of @xmath , one defines the
following classes of vectors:

  Timelike region:  

    If @xmath , the distance is called timelike . In such a case, the
    two events at @xmath and @xmath may have a causal influence on each
    other and there exists a unique Lorentz transformation which reduces
    the spatial components of @xmath to zero. However, there is no
    transformation which rotates the @xmath component to @xmath .

  Spacelike region:  

    If @xmath , the distance is spacelike . In this case, two events at
    @xmath and @xmath cannot be causally related. This requirement is
    equivalent to the colloquial saying that “no information can travel
    faster than the speed of light”. There is a unique Lorentz
    transformation which rotates the @xmath component to @xmath , but
    there is none which reduces the spatial components of @xmath to
    zero.

  Likelight region:  

    If @xmath , the distance between @xmath and @xmath is lightlike .
    The two events can be causally related if the interaction happens by
    exchanging information using massless particles traveling at the
    velocity @xmath .

The Poincaré group consists of the four-dimensional rotations in
Minkowski space, the group SO @xmath , and the translation group. An
element of the Poincaré group is denoted by @xmath and transforms a
four-vector @xmath in the following manner:

  -- -------- -- -------
     @xmath      (252)
  -- -------- -- -------

The inverse transformation of @xmath is given by @xmath . The
multiplication law is given by

  -- -------- --
     @xmath   
  -- -------- --

Thus, the set of Poincaré-transformations form a non-Abelian group.

According to the postulates of special relativity, the coordinate
transformations are linear and real. When changing the frame of
reference, the distance of two events will be unchanged, which implies
that the norm of a vector is conserved. This means that for @xmath (this
subgroup is called the homogeneous Poincaré-group):

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (253)
  -- -------- -------- -------- -- -------

Consequently, one finds

  -- -------- --
     @xmath   
  -- -------- --

and one can distinguish four kinds of transformations as displayed in
Tab. 38 . From the four subsets, only the proper, orthochroneous set
contains the unit element and is therefore the only subgroup. This
subgroup is connected, while the entire homogeneous Poincaré group is
not connected.

The Poincaré group has six generators for rotations in the @xmath
-plane, @xmath (which are antisymmetric, @xmath ), and four generators
@xmath for translations. Their commutation relations give rise to the
Poincaré algebra [ 245 ] :

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (254)
  -- -------- -------- -------- -- -------

The algebra admits a representation in Minkowski space in terms of
differential operators:

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (255)
  -- -------- -------- -------- -- -------

Defining the Pauli-Lubanski tensor by

  -- -------- -- -------
     @xmath      (256)
  -- -------- -- -------

one finds [ 245 ] that the Poincaré group has two Casimir operators:
@xmath , and @xmath . This allows to classify all irreducible
representations [ 22 ] :

 @xmath  , @xmath :  

    The energy states lie on the hyperboloid in the forward light cone.
    This describes massive particles with spin @xmath , @xmath , @xmath
    ,

 @xmath  , @xmath :  

    The energy states lie on the forward cone. This describes massless
    particles with helicities @xmath , @xmath , @xmath , s counts as
    above,

 @xmath  :  

    This is the single point at the origin.

 @xmath  , @xmath :  

    The energy states lie on the surface of the backward light cone. The
    quantum number @xmath is continuous.

 @xmath  , @xmath :  

    The energy states lie on the hyperboloid in the backward light cone.

 @xmath  :  

    The particles lie on a spacelike hyperboloid. This would describe
    tachyonic particles with velocities greater than @xmath .

Only the first two classes are realized for observable particles in
nature. If there was no lower bound to the energy of a particle as it
would be the case if the last class did correspond to any physical
particle, an arbitrary amount of energy could spontaneously be created
from any point in spacetime. According to the rules of quantum mechanics
this would happen with finite probability. Thus, this possibility seems
to be incompatible with the formulations of quantum field theories known
so far.

## Appendix F Spin-Statistics Theorem

An important relation between the particles of different spins is the
spin-statistics theorem [ 22 , 18 ] . For a relativistic quantum field
theory the observable particles (i.e. the physical states) must have the
following properties if one requires that causality holds: Taking @xmath
to be a spacelike distance in Minkowski space, the fields @xmath must
satisfy the following (anti-) commutativity relations:

  Bose fields:  

    @xmath , if the fields @xmath transform according to a particle with
    even spin. The particles described by @xmath are called bosons .
    Consequently, a single state may be occupied by an arbitrary amount
    of bosons.

  Fermi fields:  

    @xmath , if the fields described by @xmath transform as a
    representation with odd spin. The corresponding particles are called
    fermions and a single state may only be occupied by a single or none
    fermion.

## Appendix G Grassmann Algebras

As has been noted in Sec. F , the fields describing fermions anticommute
for spacelike distances. The anticommutativity is an essential property
of Grassmann fields. Thus, Grassmann algebras are an important
ingredient for the description of fermionic degrees of freedom. The
discussion follows Ref. [ 28 ] .

#### g.1 Definitions

Consider a map from @xmath coordinates in @xmath , @xmath , @xmath ,
onto the complex numbers,

  -- -------- --
     @xmath   
  -- -------- --

@xmath is called @xmath -linear if @xmath is separately linear in each
argument. It is called antisymmetric if, for any permutation @xmath , we
have

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denotes the signature of the permutation @xmath .

Now we consider the space @xmath of @xmath -linear antisymmetric
functions on @xmath . By definition, we set @xmath . For @xmath , one
finds

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

The Grassmann product map assigns to any two vectors @xmath and @xmath a
vector @xmath via

  -- -------- -- -------
     @xmath      
     @xmath      (257)
  -- -------- -- -------

The Grassmann product is associative,

  -- -------- --
     @xmath   
  -- -------- --

and the commutation law becomes

  -- -------- -- -------
     @xmath      (258)
  -- -------- -- -------

The direct sum of vector spaces,

  -- -------- --
     @xmath   
  -- -------- --

together with the Grassmann product Eq. ( G.1 ) form a graded algebra,
called the Grassmann algebra over @xmath . An element of @xmath can
always be written as a sum @xmath such that @xmath . The dimension of
the algebra is given by

  -- -------- --
     @xmath   
  -- -------- --

@xmath may be decomposed into an even and an odd part,

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (259)
  -- -------- -------- -------- -- -------

Using the decomposition ( G.1 ) allows to write the product rule ( 258 )
as follows:

  -- -------- -- -------
     @xmath      (260)
  -- -------- -- -------

Thus, the even part @xmath is a commutative subalgebra.

Let @xmath , @xmath , be a basis of @xmath . Any vector @xmath has then
the coordinates @xmath . Then define special elements @xmath via

  -- -------- --
     @xmath   
  -- -------- --

The following properties then express the fact that the @xmath generate
the Grassmann algebra:

1.  The @xmath anticommute: @xmath .

2.  Each vector @xmath may be represented as

      -- -------- --
         @xmath   
      -- -------- --

    where @xmath are complex expansion coefficients which are
    antisymmetric with respect to permutations of their indices.

The above definitions still make sense when the limit @xmath is
considered. This is the interesting situation when applying Grassmann
variables to continuum field theories. However, when constructing the
Schwinger functions @xmath of Grassmann fields on the lattice, cf. Sec.
6.4 , the behavior of the fermionic degrees of freedom in the continuum
limit will also matter.

#### g.2 Derivatives

The derivative , @xmath , is a map

  -- -------- --
     @xmath   
  -- -------- --

which is given by

  -- -------- -- -------
     @xmath      (261)
  -- -------- -- -------

with respect to the basis @xmath . It obeys the following rules

1.  @xmath ,

2.  @xmath

3.  @xmath .

#### g.3 Integration

The integral @xmath has the following properties:

1.  The integral @xmath is a complex number and the map @xmath is
    linear,

2.  @xmath , @xmath ,

3.  @xmath .

It is straightforward to proof the following rules:

1.  The relation between integration and differentiation is given by

      -- -------- --
         @xmath   
      -- -------- --

2.  Integration by parts is performed via ( @xmath , @xmath )

      -- -------- --
         @xmath   
      -- -------- --

3.  Consider a linear transformation @xmath of the coordinates @xmath of
    @xmath . Then the following rule holds:

      -- -------- --
         @xmath   
      -- -------- --

    This integral is the counterpart of the corresponding integral in a
    real vector space, @xmath , @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

4.  The exponential integral of the linear transformation @xmath is
    given by

      -- -------- -- -------
         @xmath      (262)
      -- -------- -- -------

    This rule is again the counterpart of the exponential integral in a
    real vector space. However, in the latter case, the integral only
    exists for a positive definite transformation @xmath , while the
    former exists for any @xmath .

In fact, the generating functional ( 41 ) for bosonic fields can be
generalized to an integral over Grassmann fields @xmath if fermions are
considered. Then Eq. ( 9 ) is the central tool for evaluating the path
integral on a finite lattice @xmath . It should be pointed out that the
sign-factor in Eq. ( 262 ) drops out in the case of Dirac fermions since
a Dirac spinor is composed of two Weyl spinors which are separately
described by Grassmann variables. This in turn implies that @xmath will
always be even in case of Dirac fermions. Thus, the overall sign is
@xmath .

## Chapter \thechapter Local Forms of Actions Used

For the local updating algorithms on the lattice discussed in Sec. 10
the lattice actions have to be cast into a form where the contribution
of a single site factorizes from the contributions of the other sites.
This is not possible for all actions, but in many cases it is possible
to find an approximative action which fulfills the above condition and
which has sufficient overlap with the original action under
consideration. This idea is in fact the basis of Lüscher’s original
proposal for a multiboson algorithm [ 166 ] . After the action has been
rearranged in the form above, the local “staples” can be used for the
local updating algorithms.

## Appendix H General Expressions

Consider a lattice action of the following general form

  -- -------- -- -------
     @xmath      (263)
  -- -------- -- -------

i.e. on a given space @xmath with coordinate vectors denoted by @xmath
we have a discretized field @xmath . The action is given by a sum of
@xmath terms containing products of the field @xmath such that each
coordinate appears in the action only once; i.e. the functions of the
coordinates @xmath (with @xmath , and @xmath , @xmath ) must be
distinct:

  -- -------- -- -------
     @xmath      (264)
  -- -------- -- -------

Furthermore the functions @xmath must be invertible.

Then we can choose the functions @xmath such that @xmath without loss of
generality. If the action contains @xmath different fields @xmath ,
@xmath , each field-type @xmath has to be considered separately in Eq. (
263 ). The other fields @xmath are then contained in the constants
@xmath .

From Eq. ( 263 ) we can compute the staples of the action, i.e. the
change @xmath in the action @xmath if we vary the field @xmath at a
single point @xmath about @xmath , with the following formula:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
                       @xmath   
  -- -------- -------- -------- --

The above form may also be generalized to the case where @xmath denotes
a field with several components, e.g. a complex @xmath matrix in the
case of gluon fields @xmath . The action ( 263 ) will then be the trace
over the resulting matrix; however, equation ( H ) will have to be
modified to account for the non-commutativity of the fields. Since the
trace is not invariant under commutation, but under cyclic permutations,
the expression reads

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
                       @xmath   
                       @xmath   
  -- -------- -------- -------- --

There is another important situation where the field @xmath at site
@xmath appears quadratically in the lattice action. In this case the
action can be rewritten as a Gaussian and the heatbath algorithm
discussed in Sec. 10.2.2 can immediately be applied. Such an action will
have the following form:

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (267)
                                @xmath   
                       @xmath   @xmath   
                                @xmath   
  -- -------- -------- -------- -------- -------

where @xmath . The remaining terms independent of @xmath are of no
importance for the updating algorithm and their precise form does not
matter. The case where @xmath is a complex field or an @xmath -component
field (where the trace has to be taken to compute the action) is
straightforward. However, the matrix @xmath must be invertible for this
method to work.

## Appendix I Local Forms of Various Actions

To implement the local algorithms for gauge fields in Sec. 10 , one has
to find the plaquette staples @xmath for a given action. The local
action then takes the form

  -- -------- -- -------
     @xmath      (268)
  -- -------- -- -------

In the following subsections, this form will be examined for the cases
needed in this thesis. Please note that in the following no implicit
summation over the external index must be performed.

#### i.1 Pure Gauge Fields

As a first example, consider the pure gauge action given by Eq. ( 69 ),

  -- -------- -- --------
     @xmath      ( 69 )
  -- -------- -- --------

with the plaquette @xmath given by

  -- -------- -- --------
     @xmath      ( 70 )
  -- -------- -- --------

Then one immediately finds for the local staple form of the action:

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (269)
                                @xmath   
  -- -------- -------- -------- -------- -------

#### i.2 Lattice Fermion Fields

The Wilson matrix @xmath describing a single, massive fermion flavor is
given by the expression ( 83 ):

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            ( 83 )
                                @xmath   
  -- -------- -------- -------- -------- --------

where @xmath . Up to now the boundary conditions have been chosen
implicitly to be periodic in the lattice @xmath -, @xmath - and @xmath
-directions and anti-periodic in the lattice @xmath -direction. For the
actual implementation of the local action, it is more convenient to
impose periodic boundary conditions in all four lattice directions, and
consequently have a symmetric treatment of the lattice volume @xmath .
Respecting the anti-periodicity can be done by introducing an explicit
factor which implements the anti-periodic boundary conditions in the
lattice @xmath -direction (also called @xmath -direction). We define the
fermionic sign function to be:

  -- -------- -- -------
     @xmath      (270)
  -- -------- -- -------

i.e. the function @xmath is equal to @xmath on the hyperslice with
@xmath for @xmath only and @xmath everywhere else. With this convention
the Wilson matrix takes the form

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (271)
                                @xmath   
  -- -------- -------- -------- -------- -------

This staple can be used directly for the implementation on a computer.

##### i.2.1 Wilson Fermions (Hermitian)

Using the Hermitian fermion matrix the fermionic energy is given by

  -- -------- -- ---------
     @xmath      ( 158 )
  -- -------- -- ---------

where the @xmath are the roots of the polynomial in ( 157 ). If one uses
even-odd preconditioning, the fermionic energy is given by Eq. ( 160 ):

  -- -------- -- ---------
     @xmath      ( 160 )
  -- -------- -- ---------

Inserting the Wilson matrix ( 83 ) into Eq. ( 158 ) one gets the action
in the form of Eq. ( 263 ):

  -- -------- -------- -------- -- -------
                                   
                                   
                       @xmath      
              @xmath   @xmath      
     @xmath                        (273)
                                   
  -- -------- -------- -------- -- -------

This expression can be cast into the form ( 267 ) to yield

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

With the chiral representation of the @xmath -matrices, cf. Eq. ( 243 ),
the matrix @xmath takes a very simple form

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

A local boson field heatbath is then computed by (see also Eq. ( 141 ))

  -- -------- -- -------
     @xmath      (275)
  -- -------- -- -------

with @xmath being a random number taken from a Gaussian distribution
with unit width. A local boson field overrelaxation is performed by

  -- -------- -- -------
     @xmath      (276)
  -- -------- -- -------

In both cases, the order of the sites being updated matters.

For the local gauge field updates, expression ( I.2.1 ) has to be cast
into the form ( H ). Then @xmath takes the form

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
                                   (277)
                                   
  -- -------- -------- -------- -- -------

This expression can be implemented efficiently for the case of repeated
local gauge field sweeps, as has already been noted in Sec. 16.4 .
Equation ( I.2.1 ) admits a representation in the following form

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (279)
                                @xmath   
  -- -------- -------- -------- -------- -------

with the cache fields @xmath given by

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (280)
  -- -------- -------- -------- -- -------

Any expression similar to ( I.2.1 ) can be written in the form ( 279 ).

By inserting the matrix ( 83 ) into Eq. ( 160 ), one arrives at the
corresponding expressions in the preconditioned case. @xmath designates
the projector to odd, and @xmath the projector to even sites:

  -- -------- -------- -- -- -------
     @xmath   @xmath         
     @xmath                  (281)
                             
  -- -------- -------- -- -- -------

The corresponding staple @xmath takes the form

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
                                   (283)
                                   
  -- -------- -------- -------- -- -------

##### i.2.2 Wilson Fermions (non-Hermitian)

The fermionic action in terms of the non-Hermitian Wilson fermions is
obtained by replacing @xmath with @xmath in ( 158 ). Without even-odd
preconditioning one arrives then at

  -- -------- -- ---------
     @xmath      ( 163 )
  -- -------- -- ---------

The local fermionic action becomes

  -- -------- -------- -- -- -------
     @xmath   @xmath         
     @xmath                  (285)
                             
  -- -------- -------- -- -- -------

The gauge action staples become

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
                                   (287)
                                   
  -- -------- -------- -------- -- -------

Finally the even-odd preconditioned form of ( 163 ) is given by

  -- -------- -- -------
     @xmath      (289)
  -- -------- -- -------

This leads to the local fermionic action

  -- -------- -------- -- -- -------
     @xmath   @xmath         
     @xmath                  (290)
                             
  -- -------- -------- -- -- -------

The fermionic contribution to the gauge field staple for non-Hermitian
even-odd preconditioned Wilson fermions is then given by

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
                                   (292)
                                   
  -- -------- -------- -------- -- -------

This concludes the discussion of local forms of the actions used.

## Chapter \thechapter Logistics for Running Large Numerical Productions

It has become clear in the discussion of the TSMB algorithm in Chapter
Advanced Algorithms for the Simulation of Gauge Theories with Dynamical
Fermionic Degrees of Freedom , that the effort for maintaining and
running a production run to generate a sufficiently large sample of
field configurations is enormous. A large amount of data is being
generated. But already in the simpler case of the HMC, a large number of
gauge field configuration is generated which will have to be stored in a
large file-server. In particular, for the SESAM / T @xmath L -projects [
161 , 133 ] , several TBytes of data have been accumulated.

In the case of a large-scale multiboson production, one may in addition
want to change the polynomials during the production run and thus end up
with a selection of sub-samples all distributed with a different
multicanonical action. Therefore it is inevitable to have a powerful
machinery available which allows to maintain and use a TByte-sized
archive over several years and conserve the data for potential later use
by other groups ²³ ²³ 23 Already the SESAM / T @xmath L groups have
realized that an efficient, standardized system for the storage and
handling of their configurations was in demand. The contributions
discussed in the following originally were developed as a solution to
their problems .

To meet these goals, an SQL -based database system has been devised. The
design has been a part of the TSMB development in this thesis and it has
turned out to be very useful for practical applications. In particular,
the following components have been developed:

1.  A library to read and write gauge field configurations on variable
    lattice sizes in the standardized Gauge Connection format ²⁴ ²⁴ 24
    See for a definition and description http://qcd.nersc.gov . The
    library is usable both from C and Fortran and allows to access the
    gauge fields in the form of a comfortable data structure.
    Furthermore, a selection of different and proprietary formats is
    supported which is used mainly for data exchange with the APE
    -machines. The majority of gauge field configurations generated on
    these machines is still available in this format only ²⁵ ²⁵ 25 The
    program can be downloaded from
    http://www.theorie.physik.uni-wuppertal.de/~ @xmath
    wolfram/publications/downloads/unic.tar.gz .

2.  A database programmed in SQL which employs the fast and efficient
    MySQL -database engine ²⁶ ²⁶ 26 The database can be found at
    http://www.mysql.com/ . Albeit its lack of certain features of
    modern databases, it is very suitable for the purpose of storing
    information from numerical simulations. The reason is that write
    accesses (which usually consist of adding a new configuration) only
    take place once every minutes or even hours during a production run
    and almost never concurrently. The same is valid for queries:
    queries are used to request information for measurements and are
    unlikely to happen concurrently. Thus, usage of the MySQL engine
    appears to be perfectly justified for the purposes of lattice field
    theory simulations.

3.  Programs to support adding configurations to the database and to
    support specific types of queries. The database can be accessed
    using a high-level language via their corresponding interfaces. This
    allows a direct combination with the conversion library discussed
    above. A further alternative is the access to the database using
    script languages like shell scripts or Perl scripts.

## Appendix J Design of the Database

A number of text books is available which describe the design process of
a database in detail, see e.g. [ 246 ] . The basic structure of a
database is characterized by a set of entities , their corresponding
properties , and relations between the entities. Important design goals
are

-   avoidance of UPDATE-anomalies,

-   elimination of redundancies,

-   the creation of an understandable model,

-   and the minimization of restructuring the relations for the
    introduction of new data types. This should prolong the life
    expectancy of the applications.

The above points can be satisfied, if the underlying database is
normalized . There exist a number of properties the relations need to
satisfy for the database to be normalized. The most important are ones
are given by the first five normal forms.

Figure 65 shows the entities together with the relations between them.
These ingredients will now be discussed in detail.

The entities in the database are given by

  Configurations:  

    Any single gauge field configuration needs to be stored separately.
    Several pieces of information are required for the configurations to
    be reproduced correctly. The Gauge Connection format stores all
    necessary information as a part of the file in the header. The
    Configurations entity therefore needs to have similar properties.
    Table 39 lists all attributes of this entity.

  Polynomials:  

    The TSMB algorithm (see Sec. 11.3 ) requires a multicanonical
    reweighting with a correction factor depending on the choice of the
    polynomial used (see Sec. 14.3 ). Hence, it is important to know the
    polynomial the configuration has been sampled with. Therefore, the
    Polynomials entity will contain all necessary information about up
    to three polynomials used. However, if reweighting is not required —
    if either the configuration has been sampled using an algorithm like
    the HMC or with a multiboson algorithm using an exact correction
    step — no polynomial will be associated with the configurations. The
    relation R1 between the Polynomials and the Configurations entity is
    thus @xmath . The attributes implemented for Polynomials are
    displayed in Tab. 40 .

  Ensembles:  

    For the Monte-Carlo integration schemes as discussed in Sec. 7 one
    has to compute a sample of gauge field configurations which can then
    be used to measure a physical quantity with a certain statistical
    error. For this procedure it is important to categorize all
    configuration in the database into distinct classes according their
    physical parameters, the people who contributed to them etc. This
    classification is implemented using the Ensembles entity. It is
    important to realize that this entity need not classify the
    configurations only by their physical properties, but can also
    categorize the configurations by certain “organizational”
    considerations, i.e. the origin of the configurations, the projects
    they are intended for etc. The relation R3 between Ensembles and
    Configurations is @xmath , i.e. each configuration must be part of
    one and only one ensemble, but each ensemble can contain several
    configurations. The corresponding attributes are shown in Tab. 41 .

  Machines:  

    It is useful to know on which particular machine a certain ensemble
    has been sampled. This is one example of the categorization of the
    Ensembles entity, and the only example which has been implemented in
    this thesis. The practical use of this information is the evaluation
    of efficiency analysis, where one usually performs simulations at
    equivalent physical parameters, but on different implementation
    systems (cf. Sec. 16 ). This is again an @xmath relation (see
    relation R2 ), since each ensemble has to be created on a particular
    implementation system, but each implementation can give rise to
    several ensembles. Attributes relating to the Machines entity are
    given in Tab. 42 .

Beyond what has been done here, it is possible to introduce further
entities to categorize the Ensembles further, like different research
groups or different projects where the configurations are to be used.
This topic has so far been outside the scope of this thesis and has
therefore not been implemented.

In the practical implementation, the gauge field configurations cannot
be stored in the database itself. In fact, the storage requirements are
enormous — a typical configuration on an @xmath lattice will use about
@xmath MB of RAM, and a typical sample consists of several thousands of
these. It is clear that a dedicated storage device is required. The
solution was to store the configurations on a tape archive installed at
the Forschungszentrum Jülich , Germany. The database contains only the
path to the configurations in the archive. If the configurations are in
Gauge Connection format, they will contain redundant information about
their physical and logical affiliation. This redundancy ensures that the
archive can also be used independently from the database. For the same
reason, the information about the lattice volume are stored in the
Configurations table and not in the Ensembles table, in contrast to what
one would expect from a normalized relation.

With the extended definition of the Configurations entity which also
includes the Format and Ordering properties in Tab. 39 , one is also
able to store configurations in formats different from the Gauge
Connection scheme. In particular, all other structures used by the SESAM
/ T @xmath L -collaboration are supported by the current design. In this
case, the information in the table is not redundant and is required to
successfully access a particular configuration. Furthermore the
Link_Trace and Plaquette properties are simple and efficient checksum
implementations for these applications.

There is an important subtlety regarding the approximation interval for
quadratically optimized polynomials discussed in Sec. 12.1 as used in
Tab. 40 : the interval applies to the first and second polynomials and
it is assumed that these intervals are identical. If this is not the
case, one will have to store two sets of @xmath values for the two
polynomials. The corresponding information about the third polynomial is
not required since it is not used for reweighting purposes. The
information about @xmath can therefore also be considered optional.

In all cases, the Location entry should contain sufficient information
to uniquely locate a file in the archive. Therefore, the format
user@host:/complete-path-to-file has been used, which allows the file to
be accessed directly using the scp program ²⁷ ²⁷ 27 The program and
documentation can be obtained from http://www.openssh.com/ .

In conclusion, the configuration database allows to store all necessary
information about gauge field configurations. It supports different
formats and allows to salvage all data from the SESAM / T @xmath L
projects. It uses a modern database design which can be accessed from a
diversity of different implementation systems. The configurations in
Gauge Connection format can also be accessed independently from the
database.