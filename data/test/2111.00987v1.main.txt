##### Acknowledgements. First, I would like to express my gratitude to
my supervisors Dr Matthew Forshaw and Dr Stephen McGough. Who, without
their support, guidance and insight I would have been unable to develop
my skills in an academic context. Through their help and encouragements
I have been able to exceed my own expectations. Secondly, I would like
to thank my parents and brother for supporting me during this time.
Especially my father who has proved a guiding light during the most
challenging parts. For example, inspiring my work on how disruption
could be modelled with computational methods and pushing me to go
further. Thirdly I would like to thank Sumiré Moncholi for putting up
with me during these years and providing daily support and care. Since
joining Newcastle University I have been helped and inspired by the vast
range of problems and applications tackled within the School of
Computing and the School of Mathematics, Statistics and Physics. In
particular Junyang Wang, George Stamatiadis, Adam Cattermole, Kathryn
Garside, Alexander Brown, Michael Dunne-Willows, Ashleigh McLean, Lauren
Roberts, Thomas Cooper, Shane Halloran, Jonny Law, Peter Michalák, Saleh
Mohamad and Mario Parreno. I would also like to thank all my friends
outside of the department during this time, especially Thomas Smith,
Clement Venard, Sam Major, Marta Fernandez, Wenijan Yang, Alessandro
Boussalem, Lars Eriksson, Tom Brunt, Owen Jones, Marianne Amor, Kevin
Amor and Connor Scott. Finally, I would like to thank my colleagues at
the University of Cambridge’s Centre for Science and Policy, for
providing a challenging and stimulating environment to undertake an
internship: Nicola Buckley, Katie Cohen, Rob Doubleday, Su Ford, Kate
McNeil, Lauren Milden, Jackie Ouchikh, Erica Pramauro and Laura Sayer.

###### Contents

-    1 Introduction
    -    1.1 Motivation
    -    1.2 Research questions
    -    1.3 Methodology
    -    1.4 Contributions
    -    1.5 Thesis organisation and structure
    -    1.6 Related publications
-    2 Background and Literature Review
    -    2.1 Background and Introduction
    -    2.2 Literature Review
-    3 ElecSim model
    -    3.1 Introduction and Motivation
    -    3.2 Architecture
    -    3.3 Validation and performance
    -    3.4 Scenario Testing
    -    3.5 Sensitivity Analysis
    -    3.6 Limitations
    -    3.7 Conclusions
-    4 Electricity demand prediction
    -    4.1 Introduction
    -    4.2 Literature review
    -    4.3 Methods
    -    4.4 Short-term demand forecasting
    -    4.5 Day-ahead forecasting
    -    4.6 Conclusion
-    5 Carbon optimization
    -    5.1 Introduction
    -    5.2 Literature review
    -    5.3 Optimization methods
    -    5.4 Results
    -    5.5 Conclusion
-    6 Strategic bidding within Electricity Markets
    -    6.1 Introduction
    -    6.2 Literature Review
    -    6.3 Methodology
    -    6.4 Experimental Setup
    -    6.5 Results
    -    6.6 Discussion
    -    6.7 Conclusion
-    7 Conclusion
    -    7.1 Thesis summary
    -    7.2 Conclusions
    -    7.3 Future research direction

###### List of Figures

-    2.1 Number of articles published per year which work on AI in
    electricity focused ABMs.
-    2.2 Number of machine learning types surveyed in papers.
-    2.3 Application number per paper.
-    2.4 Applications per application type.
-    3.1 Overview of a wholesale electricity market.
-    3.2 High level overview of the ElecSim model.
-    3.3 Example Load Duration Curve ( LDC )for a single year.
-    3.4 Detailed ElecSim simulation overview
-    3.5 Visualisation of a single day of dispatched supply.
-    3.6 Price duration curve which compares real electricity prices to
    those paid in ElecSim (2018).
-    3.7 Number of clusters compared to error metrics for clustering
    methods. a) Displays the correlation metric, b) displays the
    normalised root mean squared error metric, c) displays the relative
    energy error metric.
-    3.8 Comparison of actual electricity mix vs. ElecSim vs. BEIS
    projections and taking three coal power plants out of service. The
    legend here is split into two: technologies and scenario type, with
    the coloured lines denoting technologies, and dotted/straight lines
    denoting scenario.
-    3.9 Predicted price duration curve for investment for most accurate
    run against simulated run in 2018.
-    3.10 Electricity generation mix simulated by ElecSim from 2013 to
    2018 compared to observed electricity mix in 2018.
-    3.11 Comparison of ElecSim and BEIS’ reference scenario from 2018
    to 2035.
-    3.12 Comparison between optimal price duration curves and simulated
    price duration curve in 2018 for optimisation of ElecSim and BEIS’
    reference scenario from 2018 to 2035.
-    3.13 Scenarios with varying carbon taxes and decreasing demand
    (-1%/year) for 20 time steps per year.
-    (a) £26 to £150 linearly increasing carbon tax.
-    (b) £40 carbon tax
-    3.14 Scenarios from 2020 to 2050 with varying carbon tax for 20
    time steps per year..
-    (a) Network2
-    (b) £20 carbon tax.
-    (c) Network2
-    3.15 Optimal carbon tax strategy to reduce both electricity cost
    and carbon emissions for 192 time steps per year (8 days with 24
    time steps per day).
-    3.16 Scenarios from 2018 to 2035 with varying demand for 192 time
    steps per year (8 days with 24 time steps per day).
-    (a) Demand does not increase or decrease.
-    (b) Demand reduces by 1% per year.
-    (c) Demand reduces by 2% per year
-    3.17 Scenarios from 2018 to 2035 with varying demand for 192 time
    steps per year (8 days with 24 time steps per day).
-    (a) Demand increases by 1% per year.
-    (b) Demand increases by 2% per year.
-    (c) Demand increases by 2.5% per year
-    3.18 Run times of different sized countries for 192 time steps per
    year (8 days with 24 time steps per day).
-    3.19 Sensitivity analysis where Weighted Average Cost of Capital (
    WACC )was varied. Results compare electricity mix in 2035.
-    3.20 Sensitivity analysis where weighted average cost of capital
    (WACC) was varied. Results compare relative carbon emissions in
    2035.
-    3.21 Sensitivity analysis where percentage of down payment was
    varied. Results compare electricity mix in 2035.
-    3.22 Sensitivity analysis where percentage of down payment was
    varied. Results compare relative carbon emissions in 2035.
-    4.1 Types of resources for electricity generation.
-    4.2 Types of resources for electricity generation per demand.
-    4.3 A three-layer feed forward neural network.
-    4.4 Daily smart meter data load profiles of a single customer over
    a week between 20th July 2009 and 27th July 2009.
-    4.5 Daily smart meter data load profiles of different customers
    over a single day on the 22nd June 2009.
-    4.6 Figure showing similar smart meter data load profiles for four
    different customers on the 22nd July 2009.
-    4.7 RMSE vs Number of variables randomly sampled as candidates at
    each split in the Random Forest model.
-    4.8 Comparison of accuracy of models 30-minute ahead forecasting
    electricity with varying number of clusters.
-    4.9 Average load profile for each cluster of the smart meter data.
-    4.10 Comparison of accuracy of models with or without calendar
    attributes for 30-minute ahead forecasting.
-    4.11 Offline models mean absolute error comparison, with 95%
    confidence interval for 5 runs of each model. Forecasting day-ahead.
-    4.12 Best offline machine learning algorithm (Extra Trees
    Regressor) distribution for day-ahead forecasting.
-    4.13 Time taken to train the offline models for day-ahead
    forecasting versus mean absolute error. Error bars display standard
    deviation between points.
-    4.14 Time taken to score the offline models versus mean absolute
    error. Error bars display standard deviation between points.
-    4.15 Comparison of mean absolute errors (MAE) for different online
    regressor models for day-ahead forecasting. MLP results for all
    parameters are shown in a single barchart due to the very similar
    MAEs for the differing hyperparameters.
-    4.16 Best online model (Box-Cox Regressor) distribution for
    day-ahead forecasting.
-    4.17 Online machine learning algorithm distribution. (Passive
    Aggressive Regressor (C=0.1, fit intercept = true, maximum
    iterations = 1000, shuffle = false, tolerance = 0.001), chosen as it
    was the worst result for the passive aggressive model for day-ahead
    forecasting.
-    4.18 Best offline model compared to the best online model over a
    one week period for day-ahead forecasting.
-    4.19 Residuals of best offline model compared to the best online
    model over a one week period for day-ahead forecasting.
-    4.20 Time taken to test the online models versus mean absolute
    error for day-ahead forecasting.
-    4.21 Time taken to train the online models versus mean absolute
    error for day-ahead forecasting.
-    4.22 Mean outputs of various technologies vs. mean absolute error
    from 2018 to 2035 in ElecSim.
-    (a) Photovoltaic output
-    (b) Coal output
-    (c) Nuclear output
-    (d) Offshore output
-    (e) Reciprocal gas engine output
-    4.23 Total technologies invested in vs. mean absolute error from
    2018 to 2035 in ElecSim
-    (a) Total CCGT
-    (b) Total Coal
-    (c) Total Onshore
-    (d) Total Offshore
-    (e) Total nuclear
-    (f) Total photovoltaics.
-    4.24  a) Investments in reciprocal gas engine technologies vs. mean
    absolute error from 2018 to 2035 in ElecSim and d) mean carbon
    emissions between 2018 and 2035.
-    (a) Total Reciprocal gas engine
-    (b) Mean carbon emitted
-    5.1  a) Schematic of non-dominated sorting with solution
    layering b) Schematic of the NSGA-II procedure
-    5.2 Development of genetic algorithm rewards for non-parametric
    carbon tax policy results in 2035.
-    5.3 Density plot of points with an average electricity price
    <£5/MWh for non-parametric carbon tax policy results in 2035.
-    5.4 Development of genetic algorithm rewards in 2035 for linear
    carbon strategy.
-    5.5 Density plot of average electricity price in 2035 over
    generation number of genetic algorithm for both linear and
    non-parametric policy.
-    5.6 Linear carbon policies under £4.5MW/h visualised.
-    (a) All carbon policies.
-    (b) Highlighted policies.
-    5.7 Electricity mixes under selected linear carbon policies.
-    5.8 Electricity mixes under UK Government’s carbon tax.
-    6.1 Example 30 bus system.
-    6.2 Reward over time for different groups of GenCos using
    Reinforcement Learning, max bid = £ @xmath /MWh.
-    6.3 Reward over time for different groups of GenCos using
    Reinforcement Learning, max bid = £ @xmath /MWh.
-    6.4 Capacity of agents using RL vs. average electricity price
    accepted, for unbounded agents.
-    6.5 Capacity of agents using RL vs. average electricity price
    accepted, for bounded agents.
-    6.6 Bids made by generator companies using Reinforcement Learning
    with a market cap of £600/MWh.
-    (a) Largest group of GenCos with a total controlled capacity of
    46929.2MW with a market cap of £600/MWh.
-    (b) Smallest company with a total controlled capacity of 2738.7MW
    with a market cap of £600/MWh.
-    6.7 Bids made by generator companies using Reinforcement Learning
    with a market cap of £150/MWh.
-    (a) Largest group of GenCos with a total controlled capacity of
    46929.2MW with a market cap of £150/MWh.
-    (b) Smallest company with a total controlled capacity of 2738.7MW
    with a market cap of £150/MWh.
-    6.8 Capacity of agents using RL vs. average electricity price
    accepted, for bounded agents.

###### List of Tables

-    2.1 Different reviews of energy system models
-    2.2 Features of electricity market agent-based models.
-    2.3 Model schema and presentation of various energy models Hall2016
-    2.4 Articles relating to reinforcement learning algorithm type.
-    2.5 Articles relating to supervised learning algorithm type.
-    2.6 Articles relating to unsupervised learning algorithm type.
-    2.7 Articles relating to optimisation algorithm type.
-    2.8 Articles relating to game theoretic algorithm type.
-    2.9 Frequency of market type for papers reviewed.
-    2.10 Frequency of market type and application for papers reviewed.
-    2.11 Frequency of algorithms for papers reviewed.
-    3.1 Power plant parameters.
-    3.2 Validation performance metrics of comparing price simulated in
    ElecSim compared to true price values in 2018 for model with 20 time
    steps.
-    3.3 Error metrics for time series forecast from 2013 to 2018 for
    validation.
-    3.4 Error metrics for BEIS’ scenario from 2013 to 2018.
-    4.1 Input smart meter data features for 30-minute ahead
    forecasting.
-    4.2 Prediction accuracy based on type of kernel for support vector
    regression for 30-minute ahead forecasting.
-    4.3 Hyperparameters for offline machine learning regression
    algorithms for day-ahead forecasting.
-    5.1 Resulting average electricity price and relative carbon density
    under a scenario using the UK Government’s carbon tax policy.
-    6.1 Groups of GenCos that used reinforcement learning bidding
    strategies, number of plants and total electricity generating
    capacity.

## Chapter 1 Introduction

### 1.1 Motivation

The impacts of global warming on the earth may have profound effects on
land and ocean ecosystems IPCC2018 . The release of carbon emissions
into the atmosphere increases the likelihood of the most severe impacts
and increases the likelihood that tipping points are reached, where
runaway carbon emissions and average temperature rises are likely.

Therefore a transition to a low-carbon energy supply is required to
prevent the impacts of climate change. A low-carbon electricity supply
is one which releases a lower amount of carbon dioxide over its lifetime
than the current, fossil-fuel based system.

However, such a transition is complex and contains multiple
uncertainties. For instance, what carbon tax should the UK government
set over the next 30 years? Are poor short-term electricity demand
forecasts locking us in to higher emissions over the long-term? Can we
limit the market power of generator companies? And can we rely on these
models to make decisions of such importance?

Whilst much work has been carried out investigating energy models in
different electricity and energy markets, these models are not often
fully validated against real-world data. This thesis seeks to validate a
novel agent-based model, ElecSim, by calibrating with real-world data.
Through this calibration, confidence can be gained in the underlying
dynamics of the model and provide policy makers with the opportunity to
better understand the system with which they are dealing with.

Secondly, much work has been undertaken to understand certain aspects of
electricity markets using agent-based models and machine learning.
However, this work, often, does not place these findings into a wider
context. For instance, whilst a high degree of focus is placed on the
ability of reinforcement learning to bid strategically within an
agent-based setting, how to limit this behaviour has not been
investigated to the same extent.

Similarly, machine learning has been used to predict electricity demand
at various time intervals. However, the effect that different prediction
methods have on the long-term electricity market has not been explored.
Finally, machine learning and simulation has the ability to optimise an
entire system. However, this ability received much research attention,
instead the focus has been on smaller scale changes to models. In this
thesis we aim to fill this research gap by first calibrating our model,
and secondly reducing electricity price and carbon emissions from the
United Kingdom’s electricity mix by optimising carbon tax strategies.

### 1.2 Research questions

The central question of this thesis is: how can artificial intelligence
(AI) and machine learning (ML) answer fundamental questions of the
energy transition using an agent-based model of the electricity system?

This thesis aims to go beyond small-scale improvements to agent-based
models and answering scope-limited questions by understanding first:
what challenges can AI and ML tackle, and secondly: how do these methods
relate back to the wider energy system?

By taking this approach, we answer multiple subquestions, which are
explored below:

1.   Can a simulation model an electrcity market over the long-term?
    Traditional electricity market models mimic the behaviour of
    centralised actors with perfect foresight and information. Other
    models which model actors as having imperfect foresight and
    information lack the ability to model multiple time-steps over a
    long time horizon. In Chapter 3 , a novel open-source agent-based
    model called ElecSim is presented which challenges these issues. We
    show that it is possible to create an electricity model which can
    simulate multiple time-steps over a long-time horizon and generate
    realistic electricity mixes as model outputs.

2.   Is it possible to model the variability of an electricity system?
    Intermittent renewable energy can produce electricity at both
    maximum capacity and at zero capacity in short time intervals. It,
    therefore, becomes important to model these variations in power
    output over a long-term horizon. Otherwise, the model may
    overestimate the production of energy from renewables and
    underestimate the variability of such technologies. This is achieved
    in Chapter 3 by showing that with representative days, we are able
    to accurately model an entire year with a reduced computational
    burden. Without this additional feature, an overestimation of
    intermittent renewable energy and underestimation of dispatchable
    generation is observed.

3.   Can we trust an electricity model’s outputs? Whilst long-term
    energy models can provide quantitative advice to experts,
    policymakers and stakeholders, the veracity of these models are
    rarely validated. The validation of long-term electricity models can
    highlight problems with the dynamics of the model, important
    components, and provide confidence in the outputs. Here, an approach
    is presented to provide confidence in such outputs in Chapter 3 . We
    achieve this through the use of a distributed genetic algorithm
    optimisation algorithm to calibrate our model. Through such a
    calibration we are able to observe a real-world transition of the
    UK’s electricity market from coal to gas over a 5-year period.

4.   Do poor short-term forecasts of electricity demand have a long-term
    impact? Forecasting of electricity demand within electricity markets
    is critical. The settlement of markets occurs prior to the time in
    which the demand must be supplied. However, the long-term effect on
    the markets of poor forecasts has not been investigated. In Chapter
    4 we investigate the long-term impact of poor short-term
    predictions. We find that poor short-term demand forecasts leads to
    increased investments in coal, gas and nuclear power, with a
    reduction in both onshore and offshore wind.

5.   Is it possible to use an algorithm to set carbon policy? Setting
    carbon taxes has been proposed as a solution to reduce our reliance
    on fossil fuels. However, the impact of such carbon taxes are
    unknown, as are the optimal strategies from different perspectives.
    Such a problem can be solved using optimisation based techniques.
    Here, a solution to finding optimal strategies from the perspective
    of a benevolent government is presented in Chapter 5 . We find that
    it is possible to find a variety of different carbon tax strategies
    to minimise both electricity price and carbon emissions, as long as
    a high carbon price is set (around £200).

6.   Is it possible to limit the power of large generator companies? It
    is known that oligopolies have a negative effect on markets for
    consumers. However, what has been explored to a lesser degree, is
    the proportion of capacity that generation company must own before
    they have market power. In addition, what would the effect be of a
    market cap on such electricity markets? Would a market cap reduce
    the ability for Generation Companies ( GenCos )to inflate
    electricity prices artificially? Chapter 6 investigates this issue.
    We find that if a generator company, or group of colluding generator
    companies own over 11% of the total generation capacity, electricity
    prices start to increase. However, the impact of this market power
    can be limited through the setting of a market cap. In the case of
    the UK, a market cap of £190/MWh suffices.

Through these questions we not only answer whether AI and ML methods can
be used with electricity market agent-based models, but also what is the
wider impact of the behaviours on the market.

### 1.3 Methodology

Primarily, in this work, simulation is used as a tool to better
understand and make projections for electricity markets. Specifically,
in this thesis, the agent-based modelling paradigm is used. This enables
us to model generator companies as individual agents, with heterogeneous
strategies and characteristics. These agents have access to imperfect
information and imperfect foresight. This methodology differentiates
this work from the traditional centralised optimisation approach.
Agent-based models are critical to model the behaviour of individual
actors within an electricity system. Without this distinction, the
system must be modelled as a homogenous system, which does not
accurately reflect the real world. Through this approach, we hope to
learn that it is possible to accurately model the UK’s national
electricity system with an agent-based approach.

Machine learning and statistical techniques are used to make short-term
forecasts of electricity demand. We use both deep learning, offline
learning and online learning to further improve our methods. Online
learning is a machine learning approach which utilises new data to
update model weights, and does not require the model to be completely
retrained, which is the case for offline learning. In comparison, deep
learning utilises neural networks with many different layers. We utilise
these methods due to their data-driven approach and strong ability to
forecast time-series data. Online learning is used as over the
time-periods with which we are forecasting, the underlying time-series
changes in structure. Online learning is able to continually use new
data points to retrain the model. Deep learning, on the other hand, uses
many layers to learn more complex patterns from the training set.
Through taking this approach, we hope to learn that it is possible to
improve predictions for energy demand data when compared to the
traditional machine learning methods.

Once our simulation model is built, we are able to answer different
questions using several approaches. For example, we perturb the
exogenous electricity demand by the error distribution generated by the
aforementioned electricity demand forecasting methods. This provides an
insight into how small errors can have large impacts on long-term
electricity markets in terms of both investments made and generator
utilisation. This approach was taken due to its ability to mimic the
behaviour of generator companies in a simple manner. We hope to learn
what the impact of short-term decisions are on the long-term market.

Multi-objective genetic algorithms are used to explore carbon tax
policies which will reduce both carbon emissions and average electricity
price. We find that we are able to achieve both of these goals by
setting a median carbon tax of @xmath £200 per tonne of carbon dioxide.
This methodology is chosen due to the genetic algorithm’s distributed
nature. We are able to run the algorithm in parallel and reduce training
time significantly. From this, we hope to learn that there is an
automatic method to reduce the search space for policy makers when
coming up with complex policy in a high parameter space.

Finally, we explore the ability for deep reinforcement learning (DRL) to
make strategic bidding decisions within a day-ahead electricity market.
This work enables us to see the proportion of capacity that must be
controlled to artificially inflate the electricity price in the market
using market power . Deep reinforcement learning was chosen due to its
ability to quickly form a policy on the expected environment. The
bidding environment with multiple competing agents is highly complex and
difficult to solve through a rule-based approach, and so DRL is chosen
to simplify the approach of forming a policy. Through this, we hope to
learn the parameters which allow for the manipulation of the market,
such as the size of generator companies and total capacity controlled,
and how to reduce this impact of it occurs.

### 1.4 Contributions

The work in this thesis makes a number of key contributions:

1.  Development of the open-source, generalised long-term agent-based
    model for decentralised electricity markets, ElecSim Kell . This can
    be accessed at: https://github.com/alexanderkell/elecsim . This
    model is parametrised to the UK electricity market, and contains the
    major pertinent features to model this market. This answers research
    question 1 by modelling an electricity market over the long-term.

2.  Validation of the aforementioned model through the use of
    cross-validation through five years and comparison with the
    established UK Government model until 2035 Kell2020 . Through this
    validation we are able to answer research questions 2 and 3 by
    modelling the inter-year variability and verify the outputs of the
    model.

3.  Forecasting of electricity demand using machine learning models and
    exploration of the impact of the prediction errors on the long-term
    electricity market Kell2018a . This contribution answers research
    question 4 by showing that short-term errors do have a large impact
    on the final electricity mix.

4.  Optimisation of a carbon tax policy to reduce electricity cost and
    carbon emissions for the UK electricity market using a
    multi-objective genetic algorithm, from the perspective of a
    benevolent government Kell2020a . This answers research question 5
    by showing that it is possible to come up with an optimal strategy
    for setting a carbon price to reduce carbon emissions and
    electricity price.

5.  Exploration of the long-term impact of strategic bidding and
    collusion on decentralised electricity markets Kell2020d . This
    answers research question 6 by showing that if generator companies
    control a large part of the market, market power occurs. However, it
    is possible to limit these market powers significantly by setting a
    market cap on electricity price.

This work directly addresses the aim of the research. We use AI and ML
to answer fundamental questions of the energy transition through the use
of an agent-based model of the electricity system. With the development
of a novel agent-based model, we are able to answer targeted questions
on how the electricity system behaves under different pressures. We do
not simply stop at validating the ability of an algorithm to perform a
specific task, but rather relate this back to the wider market.

### 1.5 Thesis organisation and structure

-   [itemindent=3em]

-   describes the motivations behind this thesis and highlights the main
    contributions of the research. Finally, the peer-reviewed
    publications produced during this PhD are presented.

-   describes the technical background material that relates to the rest
    of this work and investigates the different types of solutions that
    have been used in the current literature and differentiate this from
    this work.

-   introduces the simulation framework developed within this work. This
    includes the technical details of the simulation tool, how this
    simulation is validated and the difficulties of validating such
    simulation models. Finally, a sensitivity analysis to show the
    impact of various variables is displayed, and some example future
    scenarios are produced. This chapter contains contributions 1 and 2.

-   explores the literature on electricity demand forecasting, how this
    can be improved with online learning, and what the long-term impact
    of errors are on decentralised electricity markets. This chapter
    contains contribution 3.

-   demonstrates the ability for the model to come up with optimal
    strategies and scenarios through the use of machine learning
    techniques. Specifically, a carbon tax strategy between 2018 and
    2035 is optimised to reduce both electricity cost and carbon
    emissions. This chapter contains contribution 4.

-   demonstrates the ability for large or colluding generator companies
    to influence the price of electricity in their favour using deep
    reinforcement learning, as well as an approach to prevent this from
    occurring through the use of price caps. This chapter contains
    contribution 5.

-   summarises the conclusions of the work and motivates future
    directions for work in this area.

### 1.6 Related publications

During the course of my PhD I have authored the following peer-reviewed
publications:

-    Kell, A., Forshaw, M., & McGough, A. S. (2019). ElecSim :
    Monte-Carlo Open-Source Agent-Based Model to Inform Policy for
    Long-Term Electricity Planning. The Tenth ACM International
    Conference on Future Energy Systems (ACM e-Energy 2019), 556–565.

    This work introduces the agent-based model, ElecSim. The current
    state-of-the-art of agent-based models is reviewed, and the
    technical foundations of how ElecSim works is detailed. An initial
    validation method of comparing the price duration curve of the model
    to that observed in real life is displayed. Finally, some example
    scenarios are presented. This work forms the basis for Chapter 3 .

-    Kell, A., Forshaw, M., & McGough, A. S. (2019). Modelling carbon
    tax in the UK electricity market using an agent-based model.
    E-Energy 2019 - Proceedings of the 10th ACM International Conference
    on Future Energy Systems, 425–427.

    In this paper, further scenarios are explored by varying the carbon
    tax level. The effect of carbon tax on investments is demonstrated
    in the electricity market. This work augments the work done in
    Chapter 3 .

-    Kell, A. J. M., Forshaw, M., & McGough, A. S. (2020). Long-Term
    Electricity Market Agent Based Model Validation using Genetic
    Algorithm based Optimisation. The Eleventh ACM International
    Conference on Future Energy Systems (e-Energy’20).

    In this paper, further improvements are made to the ElecSim model.
    Through the addition of representative days, the model is validated
    between 2013 through 2018 by optimising for long-term predicted
    electricity price. The results are compared to those of the UK
    Government, for both a long-term and short-term validation. The
    results are comparable to those of the UK Government. This work
    further extends Chapter 3 .

-    Kell, A., McGough, A. S., & Forshaw, M. (2018). Segmenting
    residential smart meter data for short-Term load forecasting.
    e-Energy 2018 - Proceedings of the 9th ACM International Conference
    on Future Energy Systems.

    In this work, various machine learning and deep learning techniques
    are used to predict electricity demand 30 minutes ahead using smart
    meter data. Various households are clustered using a k -means
    clustering technique to further improve the accuracy. This paper
    forms the basis for Chapter 4 .

-    Kell, A. J. M., McGough, A. S., & Forshaw, M. (2020). The impact of
    online machine-learning methods on long-term investment decisions
    and generator utilization in electricity markets. 11th International
    Green and Sustainable Computing Conference, IGSC 2020.

    This paper expands on the work carried out in Kell2018a . However,
    instead of predicting 30 minutes ahead, electricity demand over the
    next day is predicted, over a 24-hour horizon. To improve results,
    online learning is used, which is able to update the parameters of
    the models as new data points become available. The results are
    significantly improved using this method. Finally, the errors of
    these predictions are taken and the long-term effects of these are
    shown on the electricity market using the ElecSim models, both in
    terms of generator utilisation and long-term investment decisions.

-    Kell, A. J. M., McGough, A. S., & Forshaw, M. (2020). Optimising
    carbon tax for decentralised electricity markets using an
    agent-based model. The Eleventh ACM International Conference on
    Future Energy Systems (e-Energy’20), 454–460.

    In this paper, different carbon tax strategies are trialled using a
    multi-objective genetic algorithm. With the aim to minimise both the
    electricity price and carbon emissions. It is found that it is
    possible to achieve both of these goals through different carbon tax
    strategies. This work builds on Kell2019 by using a similar
    multi-objective genetic algorithm optimisation approach.

-    Kell, A. J. M., Forshaw, M., & McGough, A. S. (2020). Exploring
    market power using deep reinforcement learning for intelligent
    bidding strategies. The 4th IEEE International Workshop on Big Data
    for Financial News and Data at 2020 IEEE International Conference on
    Big Data (IEEE BigData 2020).

    This paper uses reinforcement learning to control the bidding
    behaviour of a single GenCo, or a group of colluding GenCos in the
    day-ahead market. It is found that if the other agents bid using
    their short run marginal costs, the GenCos which use the
    reinforcement learning algorithm are able to artificially inflate
    the market price using their market power. The work in this paper
    forms the work done in Chapter 6 .

#### Papers not forming part of this thesis

-    Kell, A. J. M., Forshaw, M., & McGough, A. S., (2019). Optimising
    energy and overhead for large parameter space simulations. 2019 10th
    International Green and Sustainable Computing Conference, IGSC 2019.

    In this work, a multi-objective genetic algorithm is used to reduce
    both overhead and energy consumption of a cluster of computers at
    Newcastle University. This is achieved by varying different
    parameters of a reinforcement learning algorithm. The methods used
    in this paper influence much of the work presented in Chapters 3 , 4
    and 6 .

-    Kell, A. J. M., McGough A. S., Forshaw, M., Mercure, J. F.,
    Salas, P. (2020). Deep Reinforcement Learning to Minimize Long-Term
    Carbon Emissions and Cost in the Investment of Electricity
    Generation. 34th NeurIPS 2020, Workshop on Tackling Climate Change
    with Machine Learning.

    This paper modifies the FTT:Power model by using reinforcement
    learning as the electricity generator investment algorithm
    Mercure2012 . FTT:Power is a global power model which uses logistic
    differential equations to simulate competition between different
    electricity generating technologies. These logistic differential
    equations are replaced with the deep deterministic policy gradient
    reinforcement learning method. It is found find that, if the goal is
    to reduce both carbon emissions and electricity price, a transition
    to renewables occurs.

## Chapter 2 Background and Literature Review

### Summary

This chapter provides an overview of the relevant material which
motivates and underpins the work carried out in this thesis and places
the work in context with a literature review. Section 2.1.1 , introduces
electricity markets and how they are regulated. In Section 2.1.2 , an
introduction into how electricity markets are modelled is presented. We
provide an introduction in Section 2.1.3 to simulation and in Section
2.1.4 to machine learning. Finally, we conclude this chapter in Section
2.1 .

We also review the literature by giving an introduction to the relevant
energy models and provide a systematic review on how AI has been applied
to agent-based models. Section 2.2.1 gives an introduction to the field
of energy modelling. In Section 2.2.2 we present different types of
energy models, as well as a detailed view on agent-based models, the
focus of this thesis. We present a table in Section 2.2.4 , which
displays a high-level overview of the major models in the literature.
Section 2.2.9 details a systematic review on how AI has been utilised
with agent-based models in the electricity sector. We conclude this
chapter in Section 2.2.18 , where we discuss the limitations and
benefits of modelling types and how AI can be integrated into
agent-based models in the future.

### 2.1 Background and Introduction

#### 2.1.1 Electricity Markets

Electricity markets are complex. One of the principal reasons for this
is the expense and difficulty of storing electricity. This is because
electricity must be consumed the moment it is generated. Additionally,
as electricity travels over high voltage transmission lines, electricity
doesn’t always follow simple or unique paths, especially when the
transmission lines become congested. In addition there is power loss
during transmission. Finally, electricity markets require technical
overseers to ensure that the entire transmission system operates safely
and reliably.

Another aspect to consider is the fact that electricity is homogeneous.
A single unit of electricity produced by a wind turbine is equivalent to
a unit of electricity produced by a gas turbine. However, the
functioning of different electricity producers, or generators, are not
homogenous. Coal, gas and oil power plants can be dispatched at the will
of a human operator. Their ramp rates are well understood, as is the
amount of fuel that is available. Where the ramp rate is defined as the
increase or decrease in output per minute and is usually expressed as
megawatts per minute (MW/min). Intermittent Renewable Energy Sources (
IRES ), however, such as solar, wind and tidal are dependent on the
supply of solar irradiance, wind speed and the tide at any moment.
Whilst these can be predicted, predictions are often wrong, and perfect
knowledge is impossible. Therefore, at times where there is too much
supply from Intermittent Renewable Energy Sources (IRES) generators must
be curtailed if all regulator generators are already offline or can not
be shut down or ramped down. In the opposite case, where there is too
little supply from IRES , supply must be made up from other sources,
such as coal, gas or hydro.

The environmental impact from different electricity generators differs
significantly. Whilst gas and coal can be dispatched at a time
convenient to the grid operators, they emit \ce CO2 along with other
toxic substances. In this context, dispatched refers to electricity that
can be generated on demand at the request of power grid operators,
according to market needs. Wind and solar do not dispatch such gases and
substances, and therefore can not be controlled as easily. Storage
technologies can be used to fill these gaps; however, large-scale
storage depends on large pumped reservoirs that can move water to a
higher position when demand for electricity is low, and supply is high.
Not all geographies have access to such reservoirs, and therefore would
rely on battery technology made from chemicals. However, reaching such
high storage capabilities are expensive and are yet to have been done in
the real world cole2019cost . Another option is converting electricity
to hydrogen. However, this technology is also expensive and
uncompetitive with traditional fossil fuels such as coal, gas and oil.
Currently, peaker plant are used to fill these gaps. Peaker plants are
plants which are used in times of high demand and low supply. However,
these plants are expensive to operate, highly polluting and use fossil
fuels lin2011energy . It is expected that these peaker plant s will be
used increasingly due to the intermittent nature of renewable
technologies lin2011energy .

The electricity grid must match supply with demand at all times. Failure
to do so results in an imbalance of supply and demand, and affects the
frequency of the electricity network. Large differences between supply
and demand can lead to blackouts or oversupply and may damage equipment.
A number of different markets exist to regulate the supply and demand,
running from within seconds, to days-ahead and bilateral contracts which
settle electricity for years ahead conejo2010electricity .

There are a number of different market mechanisms that can be used to
balance the supply and demand of electricity. Largely these can be
divided between ancillary services and wholesale transactions. Wholesale
transactions can occur as bilateral trades or on a day-ahead market
conejo2010electricity . Bilateral trades can occur between two
electricity suppliers and those that have an electricity demand
conejo2010electricity . In this case, suppliers and customers create
contracts for electricity in advance. Typically, these agents must let
the market operator know of their trades conejo2010electricity . In a
day-ahead market, the system price is, in principle, determined by
matching offers from generators to bids from consumers at each node to
develop a supply and demand equilibrium price conejo2010electricity .

Ancillary markets, on the other hand, provide a method to facilitate and
support the continuous flow of electricity so that supply continually
meets demand conejo2010electricity . These include markets to regulate
power and voltage control as well as frequency control. These markets
make use of increasing supply or reducing demand at the times where this
is required conejo2010electricity .

There exist many markets to better regulate the supply and demand at
different time intervals. For example the intra-day market, capacity
market. These markets ensure that electricity supply continues to meet
demand as more intermittent renewable energy comes online. The intra-day
market is able to dynamically meet fluctuations in supply and demand
which were not matched using the day-ahead and bilateral markets.
Whereas, the capacity market pays participants a per MW rate for the
capacity they offer to the market.

Other mechanisms exist, such as the contracts for difference and the
carbon price floor to transform the UK’s electricity system. Contracts
for difference pays a fixed price to generators irrespective of the
market price. The carbon price floor places a minimum tax on carbon.

An imbalanced system can also be resolved through the means of demand
response (the reduction of electricity demand), interconnection with
other countries, and through storage for example with vehicle to grid or
by other means. In this thesis, however, we focus primarily on the
day-ahead market and carbon price floor due to these markets having the
largest impact on the macro-scale of the electricity market.

#### 2.1.2 Introduction to Electricity Market Modelling

Energy models are useful tools for insight into the functioning of
electricity markets. For example, modellers and analysts can develop an
intuition of how such a system works, and through modelling, they can
challenge untested hypothesise. The argument that these models are used
for insight and not just informational data is as old as the models
themselves Huntington1982 ; Pfenninger2014 , and is true for models in
many different disciplines Geoffrion1976 . We argue that energy models
should not be taken as truth, because as previously mentioned, no model
can perfectly model the real world. However, the intuition that can be
learned can prove to be a valuable resource.

Energy modelling and energy policy as a distinct field began after the
oil crisis in the 1970s, where long-term planning was deemed as
important in the electricity field Craig2002 . Models which make use of
optimisation techniques have been used since these times for diverse
applications, from the global energy market to small off-grid systems.
Optimisation techniques are models in which a central actor invests in
the cost-optimal (cheapest) energy system over a multi-year time period.

The utilisation of energy models has lately been redirected to ensuring
that there exists a security of supply, the resilience of the energy
system, affordability and that there is a transition to a low-carbon
supply. These models can also be used to investigate the impact that
different technologies have on investments made in the future.

However, since the traditional models have been established, various
changes in the energy industry have occurred. Originally, electricity
systems were built upon large-scale centralised electricity production
based on fossil fuels foxon2010developing . Since then there has been a
transition towards decentralised, distributed, intermittent renewable
energy sources, such as solar and wind IEA2015a . In addition, there has
been an advent in flexible demand driven by new technologies such as
smart meters avancini2019energy . This paradigm shift requires models
which can work with higher temporal and spatial detail to account for
fluctuations in demand, supply and distributed electricity generators.

Modelling electricity markets is a complex task. There exist many
variables, actors, services and behaviours within electricity markets
which make it impossible to perfectly model the system. Often
simplifications must be made, where models are designed for a specific
task Pfenninger2014 . Large established models exist which model every
possible detail; however, with the increase in temporal and spatial
resolution required, the computational tractability of these models can
be negatively impacted. Many of the large models used today have existed
for a long time, before the advent of the Internet Pfenninger2014 .
Therefore, these models and modellers risk becoming outdated, as their
models are not updated.

Energy and electricity models generally follow two approaches: bottom-up
or a top-down approach Ringkjob2018 . Bottom-up models are often
referred to as the engineering approach and are based on detailed
technological representations of the energy system. Top-down models, on
the other hand, follow an economic approach and consider the long-term
changes and macroeconomic relationships Mai2013 . It is possible to
combine both the technological properties and long-term changes by
creating a hybrid approach Fortes2014 .

#### 2.1.3 Introduction to Simulation

A computer simulation is a virtual model of a real-world system which is
programmed into computer software. These models can be used to study how
such a system works. One is able to change parameters in the system and
make predictions as to how a system might behave. These simulations are
particularly beneficial when the system one is analysing is difficult to
experiment with fishman1978principles . For example, the system exhibits
a high financial costs of experimentation, or negative consequences may
have large impacts mitrani1982simulation . Additionally, for systems
that operate on long timescales, such as energy markets, one may not
have the ability to repeat experiments in a controlled environment.

Digital twins are a particular instance of a simulation. Digital twins
have often been instantiated to a particular system, as opposed to a
general system. For example, a digital twin can be made of the UK
electricity market, whilst a simulation can be generalised to any
decentralised market. By having a digital twin of a particular system,
we are able to remove the risks associated with interacting with a
system and iterate many experiments within a short time to find an
optimal set of parameters. In addition, a digital twin is able to behave
more like the system in question.

Whilst simulations must be built with expert knowledge, and through a
thorough understanding of the system that one would like to model,
machine learning is a data-driven approach. Data-driven approaches need
not require an understanding of the system in which they are trying to
model. Rather, they infer properties of the system entirely from data.
These models are desirable in cases where a system is too complicated to
have a full understanding of how the system works. These models have
been shown to generate accurate results in many different disciplines
Covington2016 ; WarrenLiao2005 ; Wiens2009 .

Simulations can be used to learn how a system may evolve, find optimal
input parameters, learn the key features of a system and to extract data
from the system. Another key use of simulations is to interact with a
physical system. This list is non-exhaustive, as there exist many
different uses for simulations.

#### 2.1.4 Introduction to Machine Learning

Machine learning (ML) is the study of computer algorithms that improve
automatically with the use of data. By using training data, these
algorithms are trained to make predictions or decisions without being
explicitly programmed to do so.

Machine learning methods can be split into three different categories:
(1) supervised learning, (2) unsupervised learning and (3) reinforcement
learning. Each of these methods can be used in the following cases:

1.  Supervised learning is used where the data used has labelled data.
    Labelled data is where the true value that one is trying to predict
    is available in the data.

2.  Unsupervised learning is where there are no labels associated with
    the data. The model must, therefore infer from distinct clusters in
    the data where the divides in the values may lie.

3.  Reinforcement learning is concerned with how software agents must
    take actions in an environment in order to maximise a cumulative
    reward.

In addition to the aforementioned machine learning methods, there exists
an additional paradigm: optimisation methods. These methods explore a
mathematical or software function to find a minimum or maximum value of
an objective. These can be used to minimise an expected error, minimise
total cost or maximise total return from a system, for example.

In the following sections we explore an overview of the different
machine learning techniques that are applicable to the problems
addressed in this thesis:

##### Supervised Learning

Supervised learning uses training data, which contains both the inputs
and desired outputs, to predict the output associated with new inputs. A
functioning supervised learning model, will therefore be able to
correctly determine the output for inputs that were not part of the
original training data. Supervised learning can be used for both
regression and classification. Regression is where a continuous output
is returned, whereas classification is where a discrete value is
returned.

The model widely used supervised learning algorithms are:

-   Support vector machines

-   Linear regression

-   Logistic regression

-   Decision trees

-   Neural networks

Support vector machines, neural networks and decision trees can be used
for both regression and classification, whereas linear regression and
logistic regression are used for regression and classification
respectively. Therefore, decisions must be made when choosing the
appropriate algorithm for the respective task.

Once a decision has been made with respect to whether classification or
regression is required, it is often the case that a number of different
algorithms are trialled to see which yield the best results.

However, these algorithms have different characteristics which should be
taken into account. For instance, neural networks and support vector
machines are able to learn complex patterns in data to a higher level
than linear regression and so are often chosen for more complex datasets
and problems.

Secondly decision trees are interpretable, in that, one is able to
graphically visualise why a specific output has been returned by the
model through a series of decision criteria. This is a feature which is
lacking in neural networks, which can often yield good results, but
remain a black-box.

Linear and logistic regression can be used to understand the most
important variables to influence the output variables. So whilst, they
are unable to model complex problems as well as neural networks or
support vector machines, they can be used for better model
interpretation.

In this thesis, we use a variety of different supervised learning
techniques for specific problems, as previously discussed.

##### Unsupervised Learning

As previously mentioned, unsupervised learning learns patterns from
unlabelled data. Unsupervised learning can therefore exhibit
self-organisation that can capture patterns in data that were previously
unknown.

Some of the most common algorithms include:

-   Hierarchical clustering

-    k -means clustering

-   Self-organizing maps

Hierarchical clustering, similar to decision trees, can be graphically
visualised to observe the decisions made by the algorithm. This can be
useful, particularly when it is required to see which groups are more
similar to one another, and different parent groupings.

k -means partitions observations into k clusters, in which each
observation belongs to each cluster. k -means provides a cluster centre,
which serves as a prototype of the cluster, and provides useful
information about the mean of the cluster.

A self-organising map produces a low-dimensional representation of
higher dimensional data while preserving the topological structure of
the data. This can make high dimension data easier to visualise.
Self-organising maps are a type of neural network.

##### Reinforcement Learning

Reinforcement learning (RL) is an algorithm which allows intelligent
agents to take actions in an environment in order to maximise a
cumulative reward. Reinforcement learning does not require labelled
input/output data. The basis of RL is to find a balance between
exploration and exploitation.

A large amount of research has been dedicated in recent times to
improving the performance of reinforcement learning algorithms
Arulkumaran2017 ; Hunt2016a . Various different methodologies have been
tried and tested, from the use of neural networks in deep reinforcement
learning to updating a lookup table.

Some of the algorithms used in the literature are:

-   Q-learning

-   Deep Deterministic Policy Gradient (DDPG)

-   Deep Q Network (DQN)

A major difference between the algorithms presented are the action and
state spaces. For example, Q-learning requires both a discrete action
and state space. Whereas, DDPG and DQN operate with a continuous action
and state space. Q-learning updates a look-up table to map observations
to actions, whereas DDPG and DQN use deep neural networks to learn a
policy.

The type of algorithm chosen can therefore be dependent on the different
features required. Due to Q-learning working with a look-up table, they
are more interpretable than DDPG and DQNs, as one is able to simply look
at the lookup table to see which action is taken with different
observations. However, due to the discrete nature of the action and
state space, this methodology is less useful if one would like to have
more precise actions.

It is possible to discretise the action and state space of Q-learning,
but the speed and efficiency of the algorithm is greatly reduced with
increasing discretised steps. Deep reinforcement learning techniques,
similar to neural networks, are able to learn complex patterns within
data, which can lead to better results. Care must therefore be taken
when making decisions on the type of algorithm used.

#### Conclusion

In this section, we have introduced key concepts that have been used as
part of this thesis: electricity markets and their modelling, simulation
and digital twins, machine learning methods such as supervised learning
and reinforcement learning as well as optimisation techniques. All of
the mentioned methods have been used in this thesis for the purpose of
increasing our understanding of electricity markets over both a long,
and short time periods.

We have motivated the need for novel approaches to be used when
understanding and modelling energy systems due to the fundamental
changes that have occurred since the 1970s. From a system with a
centralised actor and power stations which run on fossil fuels to one
built on decentralised generation capacity and many heterogeneous
actors.

Additionally, we have discussed the complexity of modelling a system
such as electricity markets. However, the insight that can be gained is
invaluable and can provide further understanding to those who need to
make decisions under large uncertainties. The ramifications of such
decisions go far beyond the energy sector, and therefore any help that
can be given to decision-makers is of utmost importance. Further details
presented in this chapter will be presented in future chapters.

### 2.2 Literature Review

#### 2.2.1 Energy Modelling

Energy modelling is a broad field, so there have been multiple reviews
that attempt to separate these models into different classifications
Ringkjob2018 ; Savvidis2019a ; Sensfub2007 . Examples of the metrics for
classification are the mathematical underpinning, the underlying
methodology, analytical approach or data requirements. This thesis
focuses specifically on agent-based models and AI applied to electricity
markets and not the wider literature of energy models. We, therefore,
refer the reader to the papers presented in Table 2.1 for a more
thorough investigation of this broader field.

#### 2.2.2 Model types

Large, detailed bottom-up optimisation models have long been used for
energy system modelling. These optimisation models are typically based
upon a detailed description of the technical components of the energy
system.

The ultimate goal of optimisation models is to optimise a given
quantity, for example, the minimisation of cost or the maximisation of
welfare. In this context, welfare can be designed as the material and
physical well-being of people Keles2017 . However, these models do not
state how likely each of these scenarios is to develop.

There are limitations to optimisation based models: traditional
centralised optimisation models are not designed to describe a system
that is out of equilibrium. Optimisation models assume perfect foresight
and risk-neutral investments with no regulatory uncertainty
Pfenninger2014 . The core dynamics which emerge from equilibrium remain
a black-box. For example, the model assumes a target will be reached and
does not provide information when this is not the case. Reasons for this
could be investment cycles which move the model away from equilibrium
Chappin2017 .

Equilibrium models take an economic approach. They model the energy
sector as a part of the whole economy and study how it relates to the
rest of the economy Ringkjob2018 . POLES Soria2012 is a global detailed
econometric model developed by the European Commission. E3MG is an
econometric simulation developed by Cambridge Econometrics Dagoumas2010
. MARKAL-MACRO is a hybrid model. Where MARKAL is bottom-up, and MACRO
is top-down.

Simulation models simulate an energy system based upon specified
equations, characteristics and rules. These are often bottom-up models,
and are designed with a high level of technological description
Ringkjob2018 . Agent-based models are a specific case of simulation
models, where actors are modelled explicitly as agents with
heterogeneous strategies and behaviours.

Additionally, there exist a set of power system models which can help
with decisions such as investment planning or decisions about generator
dispatch. An example of a large power systems model is WASP
jenkins1974wein .

#### 2.2.3 Agent-based models

In this subsection, we outline current agent-based models available and
motivate why the model, ElecSim, is required. Part of the literature
review outlined here has been previously published in Kell .

Electricity market liberalisation in many western democracies has
changed the framework conditions Praca2003 . Centralised, monopolistic,
decision making entities have given way to multiple heterogeneous agents
acting for their own best interest Most2010 . Policy options must,
therefore, be used to encourage changes to attain a desired outcome. It
has been proposed that these complex agents are modelled using
agent-based models (ABMs) due to their non-deterministic nature Kell .

A number of ABM tools have emerged over the years to model electricity
markets: SEPIA Harp2000 , EMCAS Conzelmann , NEMSIM Batten2006 , AMES
Sun2007 , GAPEX Cincotti2013 , PowerACE Rothengatter2007 , EMLab
Chappin2017 and MACSEM Praca2003 . Table 2.2 shows, however, that these
do not suit the needs of an open source, long-term market model.

Table 2.2 contains five columns: tool name, whether the tool is open
source or not, whether they model long-term investment in electricity
infrastructure and the markets they model and we determine how the
stochasticity of real life is modelled.

We chose these columns to compare the models due to their importance in
the application of understanding long-term electricity market scenarios.
Firstly, an open-source tool enables independent users to verify the
code developed for these models. It is important that this can be
achieved, so that the outputs are not obfuscated by a closed system.
Long-term investment allows for the endogenous propagation of an
electricity market.

We believe that a model which models both day-ahead and futures markets
is of importance. This is because these markets are interlinked and
correlated. If we did not take into account the day-ahead market, we
would not be able to determine the economic success of the GenCos, where
profitable GenCos are able to invest further, and those which are less
successful invest less. The futures market enables GenCos to invest for
the long-term, which determines the trajectory of a scenario. Other
aspects, such as intra-day markets are important for the short-term, but
less important for the long-term.

In addition, the ability to model stochasticity is of importance, due to
the non-deterministic nature of the real-world. For instance, the price
paid by GenCos is non-deterministic and can vary at any point in time
between GenCos. In reality, a large number of features are stochastic,
but we discuss those that, we believe, are most pertinent to long-term
electricity markets.

There have been several recent studies using ABMs which focus on
electricity markets. However, they often utilise ad-hoc tools designed
for a particular application hadar2019 ; Kunzel2018 ; Saxena2019 . In
our work, we develop the model ElecSim, which has been built for re-use
and reproducibility. The survey Weidlich2008 cites that many of these
tools do not release source code or parameters, which is a problem that
ElecSim seeks to address by being open source and releasing parameters.

SEPIA Harp2000 is a discrete event ABM. SEPIA models plants as being
always on. SEPIA does not model a spot market, instead focusing on
bilateral contracts. As opposed to this, ElecSim has been designed with
a merit-order, spot market in mind. As shown in Table 2.2 , SEPIA does
not include a long-term investment mechanism.

EMCAS Conzelmann is a closed source ABM. EMCAS investigates the
interactions between physical infrastructures and economic behaviour of
agents. However, ElecSim focuses on the dynamics of the market, and
provides a simplified, transparent model of market operation, whilst
maintaining the robustness of results.

NEMSIM Grozev2005 is an ABM that represents Australia’s National
Electricity Market (NEM). Participants are able to grow and change over
time using learning algorithms. NEMSIM is non-generalisable to other
electricity markets, unlike ElecSim.

AMES Sun2007 is an ABM specific to the US Wholesale Power Market
Platform. GAPEX Cincotti2013 is an ABM framework for modelling and
simulating power exchanges. However, neither of these model the
long-term dynamics for which ElecSim is designed.

PowerACE Rothengatter2007 is a closed source ABM of electricity markets
that integrates short-term daily electricity trading and long-term
investment decisions. PowerACE models the spot market, forward market
and a carbon market. Similarly to ElecSim, PowerACE initialises GenCos
with each of their power plants. However, as shown in Table 2.2 , unlike
ElecSim, PowerACE does not consider stochasticity of price risks in
electricity markets Most2010 .

EMLab Chappin2017 is an open-source ABM toolkit for the electricity
market. Like PowerACE, EMLab models an endogenous carbon market;
however, they both differ from ElecSim by not taking into account
stochasticity in outages and operating costs.

MACSEM Praca2003 has been used to probe the effects of market rules and
conditions by testing different bidding strategies. MACSEM does not
model long term investments or stochastic inputs.

As shown in Table 2.2 , none of the tools fill each of the
characteristics that we require. We therefore propose ElecSim to
contribute an open-source, long-term, stochastic investment model. For
this work, we decided that a novel agent-based simulation was required
to fill all of these categories, as the use-cases developed for our work
is specific towards the requirements of our model. Whilst it is true
that no other model meets each of our defined characteristics, however,
it is not the case that these models are not useful. In fact, they are
useful for their own specific purposes.

#### 2.2.4 Energy models classification

In this section, we present a high-level overview of the various models
that are in existence in a table format: Table 2.3 . These models fall
into various categories. However, for simplification we classify these
as either agent-based models (ABM) or non-ABM. The time horizon details
what the main purpose of the model is, is it a long-term model. Finally,
the time-step column details the time resolution granularity.

#### 2.2.5 Validation

This subsection covers the difficulties inherent in validating energy
models and the approaches taken in the literature to validate these
models.

#### 2.2.6 Limits of Validating Energy Models

Beckman et al. state that questions frequently arise as to how much
faith one can put in energy model results. This is due to the fact that
the performance of these models as a whole are rarely checked against
historical outcomes Beckman2011 .

Under the definition by Hodges et al. Hodges long-range energy forecasts
are not validatable Craig2002 . Under this definition, validatable
models must be observable, exhibit constancy of structure in time,
exhibit constancy across variations in conditions not specified in the
model and it must be possible to collect ample data Hodges .

Whilst it is possible to collect data for energy models, the data
covering important characteristics of energy markets are not always
measured. Furthermore, the behaviour of the human population and
innovation are neither constant nor entirely predictable. This leads to
the fact that static models cannot keep pace with long-term global
evolution. Assumptions made by the modeller may be challenged in the
form of unpredictable events, such as the oil shock of 1973 Craig2002 .

This, however, does not mean that energy-modelling is not useful for
providing advice in the present. A model may fail at predicting the
long-term future because it has forecast an undesirable event, which led
to a pre-emptive change in human behaviour—thus avoiding the original
scenario that was predicted. This could, therefore, be viewed as a
success of the model.

Schurr et al. argued against predicting too far ahead in energy
modelling due to the uncertainties involved Schurr_1961 . However, they
specify that long-term energy forecasting is useful to provide basic
information on energy consumption and availability, which is helpful in
public debate and in guiding policymakers.

Ascher concurs with this view and states that the most significant
factor in model accuracy is the time horizon of the forecast; the more
distant the forecast target, the less accurate the model. This can be
due to unforeseen changes in society as a whole gillespie_1979 .

It is for these reasons that we focus on a shorter-term (5-year) horizon
window when calibrating our model for validation. This enables us to
have increased confidence that the dynamics of the model work without
external shocks and can provide descriptive advice to stakeholders.
However, it must be noted that the UK electricity market exhibited a
fundamental transition from natural gas to coal electricity generation
during this period, meaning that a simple data-driven modelling approach
would not work.

In addition to this short-term cross-validation, we compare our
long-term projections to those of BEIS from 2018 to 2035. It is possible
that our projections and those of BEIS could be wrong. However, this
allows us to thoroughly test a particular scenario with different
modelling approaches, and allow for the possibility to identify
potential flaws in the models.

#### 2.2.7 Validation Examples

In this Section, we explore a variety of approaches used in the
literature for energy model validation.

The model OSeMOSYS Howells2011 is validated against the similar model
MARKAL/TIMES through the use of a case study named UTOPIA. UTOPIA is a
simple test energy system bundled with ANSWER Hunter2013 , a graphical
user interface packaged with the MARKAL model generator Noble2004 .
Hunter et al. use the same case study to validate their model Temoa
Hunter2013 . In these cases, MARKAL/TIMES is seen as the "gold
standard". In this work, however, we argue that the ultimate gold
standard should be real-world observations, as opposed to a hypothetical
scenario.

The model PowerACE demonstrates that realistic prices are achieved by
their modelling approach. However, they do not indicate success in
modelling GenCo investment over a prolonged time period Ringler2012 .

Barazza et al . validate their model, BRAIN-Energy, by comparing their
results with a few years of historical data; however, they do not
compare the simulated and observed electricity mix Barazza2020 . This
reduces the confidence that one may have in the results of the produced
electricity mix.

Work by Koomey et al. expresses the importance of conducting
retrospective studies to help improve models Koomey2003 . In this case,
a model can be rerun using historical data in order to determine how
much of the error in the original forecast resulted from structural
problems in the model itself, or how much of the error was due to
incorrect specification of the fundamental drivers of the forecast
Koomey2003 .

A retrospective study published in 2002 by Craig et al. focused on the
ability of forecasters to accurately predict electricity demand from the
1970s Craig2002 . They found that actual energy usage in 2000 was at the
very lowest end of the forecasts, with only a single exception. They
found that these forecasts underestimated unmodelled shocks such as the
oil crises which led to an increase in energy efficiency.

Hoffman et al. also developed a retrospective validation of a
predecessor of the current MARKAL/TIMES model, named Reference Energy
System Hoffman_1973 , and the Brookhaven Energy System Optimization
Model ERDA_48 . These were studies applied in the 70s and 80s to develop
projections to the year 2000. This study found that the models had the
ability to be descriptive but were not entirely accurate in terms of
predictive ability. They found that emergent behaviours in response to
policy had a strong impact on forecasting accuracy. The study concluded
that forecasts must be expressed in highly conditioned terms Hoffman2011
.

#### 2.2.8 Modelling Conclusion

In this section, we have introduced various electricity market models
and the categories that they fall into. However, it can prove to be
challenging to place models within a clear boundary, as many models fall
within a continuous spectrum. We introduced the concept that traditional
models may not have the ability to detail every single component of an
electricity market without losing tractability.

The need for a new paradigm in which decentralised agents act within an
environment was discussed. So was the need for a model with high
temporal resolution to more accurately model the intermittency of
renewable energy. Traditional optimisation models work in a normative,
prescriptive way. However, it is not possible to describe a system which
is out of equilibrium. Another limitation of the traditional
optimisation models is that they assume perfect foresight, with
risk-neutral investments and no regulatory uncertainty. It assumes that
certain scenarios are possible, but does not highlight the way a target
may not be reached.

It is for these reasons that in this thesis, we focus on agent-based
models, which move away from the traditional optimisation approach, and
allow for a more dynamic solution without rigid mathematical
expressions.

Additionally, we found that there was a gap in the literature for an
open-source agent-based model that could model long-term investments,
was generalisable to many countries and modelled stochastic inputs. It
is for this reason that we developed the model ElecSim.

#### 2.2.9 Machine Learning and Agent-Based Models

In this section, we review the literature that investigates how
artificial intelligence and machine learning can be integrated into
agent-based models for the electricity sector. To select the related
articles to review, we conduct a systematic analysis of relevant
research in the field. We limited our search to literature published in
the five most recent years (2016-2021). As a result of this, we provide
a comprehensive status of the applications of ML and AI in agent-based
models for the electricity sector. For this purpose we used the Elsevier
Scopus database. To find the articles, we used the following set of
search terms to select our articles:

1.  Machine Learning, Artificial Intelligence, Deep Learning, Neural
    Networks, Decision Tree, Support Vector Machine, Clustering,
    Bayesian Networks, Reinforcement Learning, Genetic Algorithm, Online
    Learning, Linear regression.

2.  Agent-based modelling.

3.  Electricity.

We searched using each of the keywords in each of the bullet points. For
instance, the first keyword search was: Machine Learning, Agent-Based
Modelling and Electricity. The second was: Artificial Intelligence,
Agent-based modelling and Electricity. We selected these search terms to
focus this review on agent-based models applied to the electricity
sector and machine learning, which is the focus of this thesis.

These search terms resulted in 149 research articles. However, not all
of these were related to our research focus. For instance, a number of
electric vehicle, buildings and biological papers were returned. After a
further manual review, these 149 papers were reduced to 55 papers which
were specifically related to agent-based modelling, electricity,
artificial intelligence and machine learning.

Figure 2.1 shows the amount of articles published each year between 2016
and the present date. Whilst the number of articles published in this
field has increased per year since 2018 to 2020, the number of papers
published in 2017 was lower when compared to the other years, with a
large number published in 2016. We reviewed these 55 papers
systematically in the following sections.

#### 2.2.10 Market Type

In this literature review, we make three different market type
distinctions: international/national energy market, local energy market
and a microgrid. The international/national energy market typically
considers a country, multiple countries or the world. A local energy
market is a smaller region than the international/national energy
market, for instance, a city or region. Whereas a microgrid serves a
discrete geographic footprint, such as a university campus, business
centre or neighbourhood. Whilst there is some cross-over between a local
energy market and microgrid, a microgrid can be disconnected from the
traditional grid and operate autonomously.

Tables 2.4 , 2.5 and 2.6 , 2.7 and 2.8 categorise each of the market
types respectively. The papers have been displayed in chronological
order and categorise the market type, machine learning (ML) type used,
the application in which it was used and the algorithm used. These
different criteria are explored in the following subsections.

#### 2.2.11 Machine Learning Types

Within this work, we have covered five different type of artificial
intelligence paradigms. These are: supervised learning, unsupervised
learning, reinforcement learning, optimisation and game theory

Each of these techniques have been utilised in the papers surveyed.
However, a particular focus has been placed on reinforcement learning
within the research community. As shown by Figure 2.2 , 37 out of the 55
papers used a reinforcement learning algorithm. This greatly outweighs
the other machine learning types. The second most used machine learning
type was supervised learning, used by eight papers. The fact that
reinforcement learning has been used so extensively within the
agent-based modelling community for electricity highlights the
usefulness of this technique within this field.

Within each of the different machine learning types there exist many
algorithms. The algorithms used in the papers surveyed are now
presented. Within reinforcement learning the deep deterministic policy
gradient (DDPG), Deterministic Policy Gradient (DPG) Deep Q-Network
(DQN), Deep Q-Learning, Fitted Q-iteration (FQI), long short-term memory
neural network (LSTM), Multi-Agent Deep Deterministic Policy Gradient
(MADDPG), Markov Decision Process (MDP), Novel WoLF-PHC, Policy
Iteration (PI), Probe and Adjust, Q-Learning, Roth-Erev, SARIMAX and
Variant Roth-Erev are used.

Within supervised learning, the following algorithms were used:
Artificial Neural Network (ANN), Bayesian networks, Classification
trees, Extreme Machine Learning, Lasso regression, Linear regression and
Support Vector Machine (SVM) were used. Fewer algorithms were used for
both unsupervised learning and optimisation. For supervised learning,
the following algorithms were used: Bayesian classifier, K-Means
Clustering, Naive Bayes classifier. For optimisation the following
algorithms were trialled: Bi-level coordination optimisation, Genetic
Algorithm, Iterative algorithm and Particle Swarm Optimisation. For the
game theory method, a game theoretic algorithm was used.

#### 2.2.12 Applications

Within this work, we classified each paper by the problem domain which
they are trying to solve, or the application. The applications are:
agent behaviour, bidding strategies, bilateral trading, demand
forecasting, demand response, electricity grid control, expansion
planning, forecasting carbon emissions, load scheduling, market
investigation, microgrid management, peer to peer trading, price
forecasting, risk management, scheduling of flexibility, secure demand
side management and tariff design.

Figure 2.3 displays the number of applications used by each machine
learning type. The most utilised application was bidding strategies,
with price forecasting and tariff design following behind. However, the
bidding strategies application was investigated 27 times, with price
forecasting investigated only 8 times. This demonstrates a considerable
research effort in this area.

Figure 2.4 displays the number of applications per machine learning type
area. We can see that bidding strategies is highly used within the
reinforcement learning machine learning type. The reinforcement learning
algorithm, however, is shown to be highly versatile, with different
applications investigated, from demand response, flexibility scheduling
to expansion planning. This is due to the ability for reinforcement
learning to learn different policies based upon solely the reward and
observations within an environment.

Within supervised learning, a large amount of research effort has been
put into price forecasting. This is likely due to the strong ability of
supervised learning techniques at making predictions. However, outside
of classification and making predictions, supervised learning is not so
versatile, when compared to reinforcement learning.

Optimisation is used for five different applications. This is because
optimisation requires a problem domain to be maximised or minimised.
This may not be the case for all applications.

Unsupervised learning and the game theoretic machine learning types are
shown to be used less than the other machine learning types. This is
because unsupervised learning is preferential when there is no labelled
data. However, with labelled data, supervised learning can yield more
accurate results. Within simulations it is often the case that data is
available, and so supervised learning is used in preference to
unsupervised learning. The game theoretic approach has been used in a
single application in the papers surveyed: bidding strategies. The
application of game theory is possible for the problem of bidding
strategies, however, the assumptions of a Nash equilibrium and perfect
information may not always exist in an electricity market.

#### 2.2.13 Reinforcement Learning

In this section we review the papers that utilised reinforcement
learning for the applications shown in Figure 2.4 . Firstly, we cover
the papers which consider the bidding strategies problem.

Liu et al. Liu2020 establish non-cooperative and cooperative game models
between thermal power companies. They show that, compared with other RL
algorithms, the MADDPG algorithm is more accurate, with an increase in
revenue of 5.2%. They show that thermal companies are more inclined to
use physical retention methods to make profits in the medium and
long-term power market.

Liang et al. Liang use the DDPG algorithm to model the bidding
strategies of GenCos. They show that the DDPG algorithm is able to
converge to a Nash equilibrium even with imperfect information. They
find that they are able to reflect collusion through adjusting the
GenCos’ patience parameter. Purushothaman et al. Purushothaman model the
learning capabilities of power generators through the use of the
Roth-Erev RL algorithm. They find that the agents are able to exhibit
market power through this approach. Kiran et al. Kiran use a variant of
the Roth-Erev algorithm to investigate the ability for a generator to
bid strategically within a market. They find that agents have the
ability to bid strategically, and increase their net earnings. However,
the impact on the wider market and how to limit this ability was not
explored in these papers.

Sousa et al. Sousa use an ABM to model the Iberian electricity market,
with a focus on hydropower plants. They use Q-Learning to bid in the
day-ahead market. Poplavskaya et al. Poplavskaya model the balancing
services market, and investigate the effect of different market
structures on price. They find that in an oligopoly, prices can deviate
from the competitive benchmark by a factor of 4-5. They conclude that
changing market type would not solve this issue.

Nunna et al. Nunna use a simulated-annealing-based Q-learning algorithm
to develop bidding strategies for energy storage systems. They observe
that they can effectively reinforce the balance between supply and
demand in the microgrids using a mixture of local and global energy
storage systems.

Lin et al. Lin investigate herding behaviours of electricity retailers
on the monthly electricity market. Herding behaviours are where
individuals act collectively as part of a group. They model herding
behaviours mathematically based on the relationship network of
electricity retailers which are imbedded in an ABM. They use the
Roth-Erev RL to model these behaviours. They find that the herding
behaviours might bring positive or negative effects to electricity
retailers depending on the differing bidding strategies.

Wang et al. Wang investigate the bidding behaviour of all players in the
electricity market. They propose a hybrid simulation model and integrate
RL to bid. They find that with the hybrid simulation model, the dynamics
of the entire market remain stable, the market clearing prices converge,
and the market share is relatively uniform.

Machado et al. MacHado investigate how the energy price is affected when
a government intervention is observed through the increase in number of
public companies participating in auctions. They find that they are able
to model the impact of public companies on the overall electricity
market.

Ye et al. Ye propose a novel multi-agent deep RL algorithm, where they
combine the DPG algorithm with LSTM for multi-agent intelligence. Their
algorithm achieves a significantly higher profit than other RL methods
for a GenCo.

Pinto et al. Pinto1 investigate the ability for collaborative RL models
to optimise energy contract negotiation strategies. The results show
that the collaborative learning process enables players’ to correctly
identify which negotiation strategy to apply in each moment, context and
opponent.

Feng et al. Feng explore the effect of a transition from a monopoly to a
competitive electricity market. They simulate a day-ahead market and use
RL to bid strategically. They find that they can characterise the risk
characteristics and decision-making objectives of market participants.

Calabria et al. Calabria simulate the Brazilian power system, using 3
years worth of data which covers 98% of the total hydro capacity of
Brazil. They use RL to simulate the behaviour of virtual reservoirs in
this market. They find that through the management of these virtual
reservoirs, they can save water, while maintaining current efficiency
and security levels.

Gaivoronskaia et al. Gaivoronskaia present a modification of the
classical Roth-Erev algorithm to represent agents’ learning in an ABM of
the Russian wholesale electricity market, in a day-ahead market. Staudt
et al. Staudt investigate the effect of the number of energy suppliers
needed for a competitive market. They find that several suppliers are
required to ensure a welfare optimal pricing. Where welfare is defined
as a benefit to the community.

Esmaeili Aliabadi et al. Esmaeili study the effect of risk aversion on
GenCos’ bidding behaviour in an oligopolistic electricity market. They
find that the change in the risk aversion level of even one GenCo can
significantly impact on all GenCo bids and profits.

Rashedi et al. Rashedi use Q-Learning to learn optimal bidding
strategies of suppliers in a day-ahead market. They compare the
competitive behaviour of players in both the multi-agent and
single-agent case. Chrysanthopoulos et al. Chrysanthopoulos take a
similar approach, by modelling the day-ahead market as a stochastic
game. They use an ABM to model GenCos to maximise their profit using RL.

Skiba et al. Skiba model the bidding behaviour on the day-ahead and
control reserve power markets using an RL algorithm in an ABM. They
investigate the resulting market prices. Tang et al. Tang investigate
the bidding strategies of generators under three pricing mechanisms.
They find they are able to achieve market equilibrium results through
their novel RL approach.

Bakhshandeh et al. Bakhshandeh assess the ability for GenCos to withhold
capacity to increase the market price using RL. The results demonstrate
the emergence of capacity withholding by GenCos, which have an effect on
the market price.

Xu et al. Xu simulate a proactive residential demand response in a
day-ahead market using RL. They use residential data in China, and test
a case with 30,000 households. They find that a proactive residential
demand response may yield significant benefits for both the supply and
demand side.

Deng et al. Deng use a DDPG algorithm to also model residential demand
response. They find that the goal of peak load-cutting and valley
filling can be achieved through this method.

Shafie-Khah et al. Shafie-Khah develop a novel decentralised demand
response model. In their model, each agent submits bids according to the
consumption urgency and a set of parameters by the RL algorithm,
Q-Learning. Their studies show that their decentralised model drops the
electricity cost dramatically, which was nearly as optimal as a
centralised approach. Najafi et al. Najafi also propose a decentralised
demand response model. They use Q-Learning and consider small scale
GenCos. Similarly to Shafie-Khah et al. , they show the effectiveness of
this technique.

Tomin et al. Tomin propose an RL method to interact with the electricity
grid for active grid management. They demonstrate the effectiveness of
this approach on a test 77-node scheme and a real 17-node network
diagram of the Akademgorodok microdistrict (Irkutsk).

Huang et al. Huang propose a generation investment planning model for
GenCos. They use a genetic algorithm and Q-learning to improve their
optimisation ability, and show that the model is effective and could
provide support for plant expansion planning. Manabe et al. Manabe also
consider the generation expansion planning problem. They find through
their simulation that their agents can generate higher profit using RL.

Foruzan et al. Foruzan use an ABM to study distributed energy management
in a microgrid. They use an RL algorithm to allow generation resources,
distributed storages and customers to develop optimal strategies for
energy management and load scheduling. They are able to reach a Nash
equilibrium, where all agents benefit through this approach.

Viehmann et al. Viehmann analyse the different markets: Uniform Pricing
(UP) and Discriminatory Pricing. UP is where all agents pay the maximum
accepted price, and DP pay their accepted bid. Through the use of RL,
they find that UPs lead to higher prices in all analysed market
settings.

Pinto et al. Pinto introduce a learning model to enable players to
identify the expected prices of bilateral agreements. For this they use
a Q-Learning algorithm, and they use real data from the Iberian
electricity market.

Mbuwir et al. Mbuwir explore two model-free RL techniques: policy
iteration (PI) and fitted Q-iteration (FQI) for scheduling the operation
of flexibility providers - battery and heat pump in a residential
microgrid. Their simulation results show that PI outperforms FQI with a
7.2% increase in photovoltaic self-consumption in the multi-agent
setting and a 3.7% increase in the single-agent setting.

Naseri et al. Naseri propose an autonomous trading agent which aims to
maximise profit by developing tariffs. They find that designing tariffs
with usage-based charges and fixed periodic charges can help the agent
segment the retail market resulting in lower peak demand and capacity
charges.

Bose et al. Bose simulate a local energy market as a multi-agent
simulation of 100 households. Through the use of the Roth-Erev
reinforcement learning algorithm to control trading, and demand response
of electricity. They are able to achieve self-sufficiency of up to 30%
with trading, and 41.4% with trading and demand response. Kim et al. Kim
consider a prosumer that consumes and produces electric energy with an
energy storage system. They use the DQN reinforcement learning algorithm
to maximise profit in peer-to-peer energy trading. Liu et al. Liu2021
also investigate the peer-to-peer trading problem in a local energy
market. They find that the community modelled are able to increase
profitability through the use of DQN RL.

#### 2.2.14 Supervised Learning

In this section we review the papers that used a supervised learning
approach with their agent-based models, which focus on electricity.
First we consider papers which focus on price forecasting.

Fraunholz et al. Fraunholz use ANNs to forecast electricity price
endogenously within the long-term energy model, PowerACE. They find that
the ANN method outperforms the linear regression method and that this
endogenous method has a significant impact on simulated electricity
prices. This is of importance since these are major results for
electricity market models. Pinto et al. Pinto3 uses SVMs and ANNs for
price forecasting using real data from MIBEL, the Iberian market
operator. They show an ability to return promising results in a fast
execution time.

Goncalves et al. Goncalves use multiple different methods to understand
the main drivers of electricity prices. These include lasso and standard
regression, and causal analysis such as Bayesian networks and
classification trees. Their results are coherent and show the impact of
different generators on final electricity price.

Opalinski et al. Opalinski propose a hybrid prediction model, where the
best results from a possible large set of different short-term load
forecasting models are automatically selected based on their past
performance by a multi-agent system. They show an increase in prediction
accuracy with their approach.

Bouziane et al. Bouziane forecast carbon emissions using a hybrid ANN
and ABM approach from different energy sources from a city. They
forecast energy production using agents and calculate the benefits of
using renewable energy as an alternative way of meeting electricity
demand. They find they are able to reduce emissions by 3% per day using
this approach.

Pinto et al. Pinto4 propose an approach to addressing the adaptation of
players’ behaviour according to participation risk. To do this, they
combine the two most commonly used approaches of forecasting: internal
data analysis and sectorial data analysis. They show that their proposed
approach is able to outperform most market participation strategies and
reach a higher accumulated profit by adapting players’ actions according
to the participation risk.

Maqbool et al. Maqbool investigate the impact of feed-in tariffs and the
installed capacity of wind power on electricity consumption from
renewables. They use linear regression to understand the outputs from an
agent-based model to achieve this. They find that the effect of
increasing installed capacity of wind power is more significant than the
effect of feed-in tariffs.

El Bourakadi et al. Bourakadi propose the use of an Extreme Machine
Learning (EML) algorithm to make decisions about selling/purchasing
electricity from the main grid and charging and discharging batteries
from an ABM. The EML algorithm predicts wind and photovoltaic power
output from weather data and then makes a classification decision on
whether to buy or sell power.

Babar et al. Babar propose a secure demand-side management engine to
preserve the efficient utilisation of energy based on a set of
priorities. A model is developed to control intrusions into the smart
grid using a naive Bayes classifier. Simulation is used to test the
efficiency of the system, and the result reveal that the engine is less
vulnerable to intrusion.

#### 2.2.15 Unsupervised learning

In this section we discuss the papers which use an unsupervised learning
approach with agent-based models.

Imran et al. Imran develop a novel strategy for bilateral negotiations.
They enable each GenCo to estimate the reservation price of its opponent
using Bayesian learning. They show that the agents which use Bayesian
learning gain an advantage over non-learning agents.

Čaušević et al. Causevic propose a novel clustering algorithm to cluster
agents into virtual cluster members for the application of load
scheduling. They show that large-scale centralised energy systems can
operate in a decentralised fashion when only local information is
available.

Gomes et al. Gomes propose a management system for the operation of a
microgrid by an electricity market agent. They use K-means clustering
for scenario reduction and a stochastic mixed-integer linear programming
problem to manage the system.

#### 2.2.16 Optimisation

In this subsection we review the papers which use optimisation as a
basis for investigation.

Bevilacqua et al. Bevilacqua compare three optimisation methods to
implement agent rationality in the Italian electricity market with an
ABM. Through this, they observe that the moddel exhibits a very good fit
to real data.

Duan et al. Duan propose a bi-level coordination optimisation integrated
resource strategy to unify supply-side and demand side resources across
China. They do this for mid-long term planning.

Gao et al. Gao use a genetic algorithm to determine an optimal bidding
strategy. They verify their approach by modelling a 30-bus system as an
example. Meng et al. Meng also use a genetic algorithm optimisation
approach to make dynamic pricing decisions. They model the day-ahead
market and propose a two-level optimisation model. They model the price
responsiveness of different customers using the optimisation algorithm,
and then optimise the dynamic prices that the retailer sets to maximise
its profit. They confirm the feasibility and effectiveness of their
technique through simulation.

#### 2.2.17 Game theory

Filho et al. Filho is the only paper in this review which takes a game
theoretic approach. They deal with a comparative analysis of individual
strategies of generating units in auctions, using. non-cooperative game
theory approach. They find that their method is best suited for
second-price auctions and can be extended to more complicated networks
with high precision. However, they find that it is not possible to use
this methodology in some, more complex, systems. For example, bidding
within a stochastic and uncertain environment, where a predictions are
required for the behaviour of other actors.

#### 2.2.18 AI and ABMs Conclusions

Artificial intelligence (AI) and machine learning (ML) has been
integrated with agent-based models to model the electricity sector with
increasing frequency over the last years. This is due to the ability of
AI to optimise agent behaviour, system parameters and add functionality
to agent-based models (ABMs). This study, therefore, reviewed recent
papers regarding applications of AI and ML in this space. We categorised
these papers into four different criteria: market type, application,
algorithm type and algorithm used. To do this, we used different search
terms on Scopus and reviewed all 55 articles in the field over the past
five years. It was found that the majority of papers used reinforcement
learning applied to bidding strategies. However, a range of applications
were investigated through a wide variety of means. This included price
forecasting, demand forecasting, microgrid management and risk
management. We highlight the major findings from this study in the
following sections.

##### Market Type

Table 2.9 displays the frequency of market types for the papers
reviewed. Whilst Table 2.10 shows the frequency of different
applications per market type.

1.  68.9% of the papers reviewed relate to an international/national
    electricity market. The availability of data and the relative
    importance of the subject of whole system transitions in current
    affairs may explain why such research effort has been dedicated to
    this.

2.  32.8% of the papers for the international/national focus on bidding
    strategies, as is shown by Table 2.10 . This is due to the ability
    of reinforcement learning to make strategic decisions under
    uncertainty. In addition, the ability to model strategic bidding is
    of significance importance for international/national energy models
    due to the appearance of oligopolies in national energy markets.
    However, whilst many studies have explored on strategic bidding and
    a few have focused on oligopolistic bidding strategies, a study on
    the impact on the wider market does not exist.

3.  Price forecasting is the second largest category which is explored,
    and is investigated in 13.1% of papers. This is due to the ability
    for supervised learning to make predictions in time-series data. It
    is also of importance for agents to make realistic predictions of
    future prices. However, there exists a gap in the literature on the
    long-term effects of the different accuracies of these forecasts.

4.  18% of papers focus on the local energy market. A significant
    reduction when compared to the international/national electricity
    market. This is because of the limited availability of publicly
    available data for these local energy markets. ABMs require a high
    amount of data to inform the behaviour of the agents and
    environment, and so data collection for local energy markets can be
    expensive and difficult to obtain. Microgrids are explored in 13.1%
    of papers. Similarly to local energy markets this is because of a
    smaller amount of publicly available data. It is also possible that
    researchers place an increasing focus on international/national
    electricity markets due to the availability of these models and
    perceived impact from a large system.

##### Applications

1.  A significant proportion of papers have focused on bidding
    strategies, with 44.3% of papers investigating this. Bidding
    strategies is top for each of the market types, indicating the
    versatility and applicability of this technique in electricity
    markets.

2.  The application of control, for instance grid control, load
    scheduling and demand response has seen a significant amount of
    promising research. Particular in the application of decentralised
    control Najafi ; Shafie-Khah ; Causevic . With the advent of
    distributed data collection and internet of things, it has become
    possible to achieve various welfare objectives through a distributed
    control algorithm. That is, to optimise for individuals or
    distributed generators from the perspective of those respective
    individuals and generators. Studies have shown efficiencies close to
    centralised algorithms. However, centralised algorithms may reduce
    the agency of individuals and therefore must overcome an additional
    barrier in uptake if the individuals do not perceive that the
    algorithm is working in their own best interest.

3.  Various applications have been explored within the research
    community. In total, 17 different applications were explored. This
    demonstrates the versatility of ML applied to ABMs. This, however,
    highlights a significant gap in the literature as the majority of
    applications have only been explored by one or two papers. For
    instance, the ability to optimise the electricity system parameters
    in question has not been explored to the same level of detail as
    forecasting or trading behaviour.

4.  Whilst there are many studies which look at optimising certain
    aspects of an electricity market, there is no study which looks at
    optimising the whole electricity system as a whole.

##### Algorithm Type

Table 2.11 displays the frequency of each of the algorithms used.
Q-Learning is the most used algorithm, with 29% of papers using this
algorithm. Second is the Roth-Erev algorithm, which is used by 9.7% of
papers. Whilst this shows the versatility of these algorithms, further
research could be placed into the use of deep reinforcement learning
(DRL) to improve results. DRL uses deep neural networks to act as
function approximators. DDPG, DQN and DPG are examples of DRL
algorithms.

The majority of the algorithms have only been used in a single paper,
and so, there remains a significant gap in the literature to apply these
algorithms to the different applications to investigate with results can
be improved through other algorithms.

#### 2.2.19 Future research direction

Significant and increasing research interest has been placed into
agent-based models applied to the electricity sector. However, this work
has shown that a lot of research has clustered around similar subjects.
For instance, the top application investigated for each market type is
bidding strategies, largely exploring whether it is possible to use
reinforcement learning to bid into an electricity market. However, the
impact of these bidding strategies on the wider market has been
investigated to a lesser extent. This is a gap in the literature and
something we attempt to address in Chapter 6 . In this chapter, we look
at the effect of strategic bidding on the wider electricity market, and
how to reduce the effect of monopolies.

Another aspect that has been explored to a lesser extent is the
optimisation of the electricity system parameters as a whole as applied
to policy. We attempt to fill this gap in Chapter and 3 and 5 . In these
chapter, we minimise electricity price and carbon emissions using a
genetic algorithm to optimise a carbon tax policy and also to calibrate
our model, ElecSim.

Finally, a large amount of research has been placed into forecasting
time series within agent-based models. However, the research typically
stops here and does not consider the impact of these forecasts on the
wider market. Therefore, we attempt to fill this gap in Chapter 4 .

This chapter provides a systematic review of AI applied to agent-based
models in the electricity sector. Through this analysis, various
research gaps have been found, as the majority of papers focus on
similar themes and algorithms. This, therefore, highlights a wide range
of areas in which the research community can focus on in future work.

## Chapter 3 ElecSim model

### Summary

In this chapter, we motivate and introduce the agent-based model,
ElecSim. The majority of the work presented here was published in Kell
and Kell2020 . The contribution of this chapter is a new open-source
framework for the long-term modelling of electricity markets. We provide
curated data for the simulation, to improve realism with Monte Carlo
sampling, and calibrate our model using genetic algorithm-based
optimisation. We validate our model after this calibration, using
observed data between 2013 and 2018, as well as compare our model to the
UK Government’s baseline scenario up until 2035. We also provide a
sensitivity analysis of important variables. For example, these
variables include the Weighted Average Cost of Capital ( WACC )as well
as the percentage of down payment required for a particular investment.
The sensitivity analysis, as well as additional scenarios shown in
Section 3.5 and Subsection
LABEL:elecsim:sec:representative-days-scenarios were previously
unpublished in Kell and Kell2020 . We show, in this work, that it is
possible to validate an agent-based model which focuses on the long-term
electricity market by extracting the pertinent features of the market in
question.

We introduce our work in Section 3.1 , including why a simulation model
is required to aid in a low carbon transition. In addition to the
literature review on energy-based simulations presented in Chapter 2 ,
we provide a literature review of work done to validate energy models in
Section 2.2.5 . The architecture of the model is presented in Section
3.2 . The model is validated in Section 3.3 . Various scenarios are
presented in Section 3.4 , where we vary demand until 2035. A
sensitivity analysis is provided in Section 3.5 , where we vary the
weighted average cost of capital, as well as the down payment required
for investments. Finally, we present the limitations of our model in
Section 3.6 and conclude our work in Section 3.7 .

The ElecSim model, and all related code can be accessed at:
https://github.com/alexanderkell/elecsim .

### 3.1 Introduction and Motivation

#### 3.1.1 Transition to a low-carbon energy supply

Global carbon emissions from fossil fuels have significantly increased
since 1900 boden2017global . Fossil-fuel based electricity generation
sources such as coal and natural gas currently provide 65% of global
electricity BP2018 . Low-carbon sources such as solar, wind, hydro and
nuclear provide 35% . To halt this increase in \ce CO2 emissions, a
transition of the energy system towards a renewable energy system is
required.

Due to the long construction times, operating periods and high costs of
power plants, investment decisions can have long term impacts on future
electricity supply Chappin2017 . Governments, therefore, have a role in
ensuring that the negative externalities of emissions are priced into
electricity generation. This is most likely to be achieved via a carbon
tax and regulation to influence electricity market players such as
Generation Companies ( GenCos ).

Decisions made in electricity markets may have unintended consequences
due to their complexity. A method to test hypothesise before they are
implemented would, therefore, be useful.

To aid in such a transition, energy modelling can be used by
governments, industry and agencies to explore possible scenarios under
different variants of government policy, future electricity generation
costs and energy demand. These energy modelling tools aim to mimic the
behaviour of energy systems through different sets of equations and data
sets to determine the energy interactions between different actors and
the economy Machado2019 .

#### 3.1.2 ElecSim: Modelling and simulation

Live experimentation of physical processes is not often practical. The
costs of real-life experimentation can be prohibitively high, and can
require a significant amount of time in order to fully ascertain the
long-term trends. There is also a risk that changes can have detrimental
impacts and lead to risk-averse behaviour. These factors are true for
electricity markets, where decisions can have long term impacts. For
instance, by investing in a coal power plant today, this will lock in
the carbon emissions emitted by the coal power plant for the next 25+
years. Simulation, however, can be used for rapidly prototyping ideas.
The simulation is parametrised by real-world data and phenomena. Through
simulation, the user is able to assess the likelihoods of outcomes under
certain scenarios and parameters Law:603360 .

Simulation is often used to increase understanding as well as to reduce
risk and reduce uncertainty. Simulation allows practitioners to realise
a physical system in a virtual model. In this context, a model is
defined as an approximation of a system through the use of mathematical
formulas and algorithms. Through simulation, it is possible to test a
system where real-life experimentation would not be practical due to
reasons such as prohibitively high costs, time constraints or risk of
detrimental impacts. This has the dual benefit of minimising the risk of
real decisions in the physical system, as well as allowing practitioners
to test less risk-averse strategies.

Agent Based Models ( ABMs )are a class of computational simulation
models composed of autonomous, interacting agents. These can be built to
model systems with many heterogeneous agents. Due to the numerous and
diverse actors involved in electricity markets, ABMs have been utilised
in this field to address phenomena such as market power Ringler2016a .

The work presented in this chapter develops ElecSim, an open-source ABM
that simulates GenCos in a wholesale electricity market. ElecSim models
each GenCo as an independent agent. An electricity market facilitates
trades between the two. Figure 3.1 displays an overview of a wholesale
electricity market. The GenCos invest in power generation facilities,
such as coal power plants, renewables or gas turbines which produce
electricity. Typically, a reseller will purchase the produced
electricity through bilateral contracts, or on the day-ahead market who
then sell the electricity to end-users. Where end-users could be
residential users, industry or the automotive sector.

GenCos make bids for each of their power plants. Their bids are based on
the generators short run marginal cost Perloff2012 , which excludes
capital and fixed costs. The electricity market accepts bids in cost
order, also known as merit-order dispatch. GenCos invest in power plants
based on expected profitability.

ElecSim is designed to provide quantitative advice to policymakers,
allowing them to test policy outcomes under different scenarios. A
scenario is a single future which could occur under different
circumstances. To achieve this, policymakers are able to modify a file
to realise a scenario of their choice. For example, by changing
anticipated future energy costs, carbon taxes or electricity demand. It
can also be used by energy market developers who can test new
electricity sources or policies, enabling the modelling of changing
market conditions.

This model can be used by the following players:

-    Policy experts to test policy outcomes under different scenarios
    and provide quantitative advice to policymakers. They can provide a
    simple script defining the policies they wish to use along with the
    parameters for these policies.

-    Energy market developers who can use the extensible framework to
    add elements such as new energy sources, policies, consumer profiles
    and storage types. Thus allowing ElecSim to adapt to a changing
    ecosystem.

##### Current approaches

Optimisation based solutions are the dominant approach for analysing
energy policy Chappin2017 . However, the results of these models should
be interpreted in a normative manner. For example, how investment and
policy choices should be carried out, under certain assumptions and
scenarios. The processes which emerge from an equilibrium model remain a
black-box, making it difficult to fully understand the underlying
dynamics of the model Chappin2017 .

In addition to this, optimisation models do not allow for endogenous
behaviour to emerge from typical market movements, such as investment
cycles Chappin2017 ; Gross2007 . By modelling these naturally occurring
behaviours, policy can be designed that is robust against movements away
from the optimum/equilibrium. Thus, helping policy to become more
effective in the real world.

Agent-based models differ from optimisation-based models by the fact
that they are able to explore ‘ what-if ’ questions regarding how a
sector could develop under different prospective policies, as opposed to
determining optimal trajectories. ABM s are particularly pertinent in
decentralised electricity markets, where a decentralised actors dictate
investments made within the electricity sector. ABM s have the ability
to closely mimic the real world by, for example, modelling irrational
agents, in this case, Generation Companies (GenCos) with incomplete
information in an uncertain future Ghorbani2014 .

By undertaking calculations for each day of the simulation in ElecSim,
we would potentially gain accuracy. For instance, by modelling the
variations within days of solar irradiance and wind speed. However, this
would have the effect of slowing the simulation down significantly.
Therefore, as a compromise between speed and accuracy, we use a set of
representative days to model an annual time period.

Similar to Nahmmacher et al. we demonstrate how clustering of multiple
relevant time series such as electricity demand, solar irradiance and
wind speed can reduce computational time by selecting representative
days Nahmmacher2016 . In this context, representative days are a subset
of days that have been chosen due to their ability to approximate the
weather and electricity demand in an entire year. Similar to Nahmacher
et al. we use a Ward hierarchical clustering algorithm
doi:10.1080/01621459.1963.10500845 , However, we also try a @xmath
-means clustering approach forgy65 . We chose the @xmath -means
clustering approach due to the previous success of this technique in
clustering time series Kell2018a .

#### 3.1.3 Validation of long-term models

There is a desire to validate the ability of energy-models to make
long-term predictions. Validation increases confidence in the outputs of
a model and leads to an increase in trust amongst the public and
policymakers. Energy models, however, are frequently criticised for
being insufficiently validated, with the performance of models rarely
checked against historical outcomes Beckman2011 .

In answer to this, we postulate that ABM s can provide accurate
information to decision-makers in the context of electricity markets by
using cross-validation of observed electricity mix compared to simulated
electricity mix. We increase the temporal granularity of the work in our
work in this chapter and use genetic algorithms to tune the model to
observed data enabling us to perform calibration of the model. This
enables us to understand the parameters required to observe certain
phenomena, as well as use these fitted parameters to make inferences
about the future.

We use a genetic algorithm approach to find an optimal set of price
curves predicted by generation companies (GenCos) that adequately model
observed investment behaviour in the real-life electricity market in the
United Kingdom. Similar techniques can be employed for other countries
of various sizes Kell .

We measure the accuracy of projections for our improved ABM with those
of the UK Government’s Department for Business, Energy and Industrial
Strategy (BEIS) for the UK electricity market between 2013 and 2018. In
addition to this, we compare our projections from 2018 to 2035 to those
made by BEIS in 2018 DBEIS2019 . We select five years for the
calibration and validation period as we believe that the transition from
coal to gas in this period reflects the kind of dynamics that ElecSim
must capture over the long-term. Whilst it can be argued that a five
year period is too short a time to adequately model a long-term energy
model, it is argued here that long-term validation is unfeasible due to
the large amount of difficult to capture features over the long-term. In
other words, a long-term model provides scenarios and not forecasts,
where each of the scenarios could feasibly occur, but it is not feasible
to predict which of these scenarios will occur due to the stochastic
nature of the real-world in which we are attempting to model.

We use numerical validation in this case to reduce the complexity
required for theoretical validation. Theoretical validation requires the
validation of many complex processes, and even after this process, we
would not know the complex whether the interaction of these processes
are realistic. We argue in this thesis, that both numerical and
theoretical validation are required.

#### 3.1.4 Model outputs

Through this validation process, we are able to adequately model the
transitional dynamics of the electricity mix in the United Kingdom
between 2013 and 2018. During this time there was an @xmath drop in coal
use, @xmath increase in Combined Cycle Gas Turbine ( CCGT ), @xmath
increase in wind energy and increase in solar from near zero to @xmath
MW. We are therefore able to test our model in a transition of
sufficient magnitude.

We show in this chapter, that agent-based models are able to mimic the
behaviour of the UK electricity market under the same specific scenario
conditions. Concretely, we show that under an observed carbon tax
strategy, fuel price and electricity demand scenario, the model,
ElecSim, closely matches the observed electricity mix between 2013 and
2018. We achieve this by determining an exogenous predicted price
duration curve using a genetic algorithm to minimise the error between
observed and simulated electricity mix in 2018. The predicted price
curve is an arrangement of all price levels in descending order of
magnitude. The predicted price duration curve achieved is similar to
that of the simulated price duration curve in 2018, increasing
confidence in the underlying dynamics of our model.

In addition, we compare our projections to those of the BEIS reference
scenario from 2018 to 2035 DBEIS2019 . To achieve this, we use the same
genetic algorithm optimisation technique as during our validation stage,
optimising for predicted price duration curves for calibration. Our
model demonstrates that we are able to closely match the projections of
BEIS by finding a set of realistic price duration curves which are
subject to investment cycles. Our model, however, exhibits a more
realistic step-change in nuclear output than that of BEIS. This is
because, whilst BEIS projects a gradual increase in nuclear output, our
model projects that nuclear output will grow instantaneously at a single
point in time as a new nuclear power plant comes online.

This allows us to verify the scenarios of other models, in this case,
BEIS’ reference scenario, by ascertaining whether the optimal parameters
required to achieve such scenarios are realistic. In addition to this,
we are able to use these parameters to analyse ‘ what-if ’ questions
with further accuracy.

#### 3.1.5 Contributions of this Chapter

As part of this work, we contribute a validated open-source agent-based
model called ElecSim. Whilst we have used the United Kingdom as a
use-case for this thesis, ElecSim is able to model decentralised markets
of various sizes, and can therefore be used to model other countries.

To improve our results as well as making the execution time tractable,
we increased the temporal granularity of the model using a @xmath -means
clustering approach to select a subset of representative days for wind
speed, solar irradiance and electricity demand. This subset of
representative days enabled us to approximate an entire year and only
required a fraction of the total time-steps that would be necessary to
model each day of a year independently. We show that we are able to
provide an accurate framework, through this addition, to allow
policymakers, decision-makers and the public to explore the effects of
policy on investment in electricity generators.

We demonstrate that with a genetic algorithm approach, we are able to
optimise parameters to improve the accuracy of our model. Namely, we
optimise the predicted electricity price, the uncertainty of this
electricity price and nuclear subsidy. We validate our model using the
observed electricity mix between 2013-2018 through a process of
calibration. That is, we find the predicted electricity price,
uncertainty of this electricity price and nuclear subsidy which match
with the stated scenario. We then use these values to project further.

A major contribution of this work is to demonstrate that it is possible
for agent-based models to accurately model transitions in the UK
electricity market. This was achieved by comparing our simulated
electricity mix to the observed electricity mix between 2013 and 2018.
In this time, a transition from coal to natural gas was observed. We
demonstrate that a high temporal granularity is required to accurately
model fluctuations in wind and solar irradiance for intermittent
renewable energy sources.

### 3.2 Architecture

In this Section, we detail how the architecture of ElecSim has been
designed. ElecSim is made up of six parts:

-    GenCos which are made up of agents.

-    Demand

-    Power plants .

-   A power exchange , which controls an electricity spot market.

-   The time-steps.

-   The data for parametrisation. This includes both the configuration
    file and data sources .

A schematic of ElecSim is displayed in Figure 3.2 . We have placed the
items in bold which relate to each part of the figure. Next, we will
describe each of the elements described in Figure 3.2 in further detail.

##### Data parametrisation

ElecSim contains a configuration file and a collection of data sources
for parametrisation. These data sources contain information such as
historical fuel prices, historical plant availability, wind and solar
capacity.

The configuration file allows for rapid changes to test different
hypothesis and scenarios, and points to the different data sources. The
configuration file enables one to change the demand growth and shape,
future fuel and carbon prices, capital costs, plant availability,
investment costs and simulation time.

##### Demand Agent

The demand agent is a simplified representation of aggregated demand in
a country. The demand is represented as a Load Duration Curve (LDC) . An
example load duration curve for a year is demonstrated in Figure 3.3 .
This Figure effectively shows the baseload level at 460 hours, and the
peak load at 2400 hours and above, with the profile displayed between
these points. An LDC is an arrangement of all load levels in descending
order of magnitude. where the lowest segment demand demonstrates
baseload, and the highest segment represents peak demand. Each year, the
demand agent changes each of the LDC segments proportionally, by
multiplying each of the levels by the factor to increase or decrease
these load levels. In other words, whilst the magnitude of the LDC may
change, the profile does not.

##### Generation Company Agents

The GenCos have two main functions. Investing in power plants and making
bids to sell their generation capacity. We will first focus on the
buying and selling of electricity, and then cover the investment
algorithm.

The power exchange (or spot market) runs every year, accepting the
lowest bids until supply meets demand. Once this condition is met, the
spot price or system marginal price (SMP) is paid to all generators
regardless of their initial bid. Generators are motivated to bid their
SRMC, to ensure that their generator is being utilised, and reduce the
risk of overbidding.

###### Power Plants

Power plants are made up of various parameters which dictate their
behaviour within the electricity market. For example, cheaper power
plants will undercut the costs of more expensive power plants, or solar
will displace fossil-fuel based plants at times of high solar
irradiance.

Costs form an important element of markets and investment, and publicly
available data for power plant costs for individual countries can be
scarce, and when available is presented in different formats. Thus,
extrapolation and interpolation is required to estimate costs for power
plants of differing sizes, types and years of construction.

Users are able to initialise costs relevant to their particular country
by providing detailed cost parameters. They can also provide an average
cost per MWh produced over the lifetime of a plant, known as the
Levelised Cost of Electricity ( LCOE ). They do this within the data
sources file.

The parameters used to initialise the power plants are detailed in Table
3.1 . Periods have units of years and costs in unit of currency per MW
unless otherwise stated.

If specific parameters are not known, the LCOE can be used for parameter
estimation, through the use of linear optimisation. Constraints can be
set by the user, enabling, for example, varying operation and
maintenance costs per country as a fraction of LCOE. This includes
future costs.

To fully parametrise power plants, availability and capacity factors are
required. Availability is the percentage of time that a power plant can
produce electricity. This can be reduced by forced or planned outages.
We integrate historical data to model improvements in reliability over
time.

The capacity factor is the actual electrical energy produced over a
given time period divided by the maximum possible electrical energy it
could have produced. The capacity factor can be impacted by regulatory
constraints, market forces and resource availability. For example,
higher capacity factors are common for photovoltaics in the summer and
lower in winter.

To model the intermittency of wind and solar power, we allow them to
contribute only a certain percentage of their total capacity (nameplate
capacity) for each load segment. This percentage is based upon empirical
wind and solar capacity factors. In this calculation, we consider the
correlation between demand and renewable resources.

When initialised, @xmath is selected from a uniform distribution, with
the ability for the user to set a maximum percentage increase or
decrease. A uniform distribution was chosen to capture the large
deviations that can occur in @xmath , especially over a long time
period. By doing this, the variance in costs between individual power
plants for processes such as preventative and corrective maintenance,
labour costs and skill, health and safety and chance are different per
plant instant. Future work would include finding data to better inform
the @xmath price.

Fuel price is controlled by the user; however, there is inherent
volatility in fuel price. To take into account this variability, an
ARIMA ARIMA model was fit to historical gas and coal price data. The
standard deviation of the residuals was used to model the variance in
price that a GenCo will buy fuel in a given year. This considers
differences in chance and hedging strategies.

##### Spot market

The spot market is run by a power exchange. The power exchange
effectively ranks all power plant bids on the market from lowest to
highest for each demand segment, as shown by Figure 3.3 . All bids are
accepted until supply meets demand. This dispatch method is known as
merit order dispatch.

This method effectively ranks ensures that power plants with the lowest
marginal cost are brought online first, and those with the highest
marginal cost brought on last. For example, expensive peaker plants will
only be used at peak times, where all cheaper electricity generation
supply is being used.

###### Investment.

Investment in power plants is made based upon a Net present value
calculation. NPV is a summation of the present value of a series of
present and future cash flow. NPV provides a method for evaluating and
comparing investments with cash flows spread over many years, making it
well suited for evaluating power plants which have a long lifetime. NPV
is based upon the fact that current cash flow is worth more than future
cash flow. This is due to the fact that money today can be invested and
have a rate of return. This means that, for example, $50,000 today is
worth more than $50,000 in 10 years time. The value in which future cash
flow is worth less than present cash flow is denoted by the discount
rate.

Equation 3.1 is the calculation of NPV , where @xmath is the year of the
cash flow, @xmath is the discount rate, @xmath is total number of
periods, or lifetime of power plant, and @xmath is the net cash flow at
time @xmath .

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

A discount rate set by a GenCo’s weighted average cost of capital (WACC)
is often used KincheloeStephenC1990TWAC . WACC is the rate that a
company is expected to pay on average for its stock and debt. Therefore
to achieve a positive NPV, an income larger than the WACC is required.
However, a higher WACC is often selected to adjust for varying risk
profiles, opportunity costs and rates of return
KincheloeStephenC1990TWAC . To account for these differences we sample
from a Gaussian distribution, giving us sufficient variance whilst
deviating from the expected price. Gaussians are often observed within
real-life distributions, and therefore chosen for this work.

To calculate the NPV, future market conditions must be considered. For
this, each GenCo forecasts @xmath years into the future, which we assume
is representative of the lifetime of the plant. As in the real world,
GenCos have imperfect information, and therefore must forecast expected
demand, fuel prices, carbon price and electricity sale price. This is
achieved by fitting functions to historical data. Each GenCo is
different in that they will use differing historical time periods of
data for forecasting.

This is a notoriously difficult task, and there are multiple different
ways in which this can be achieved Tao2021 . However for this work, we
forecast fuel and carbon price using two different linear regression
functions. Linear regression was chosen due to the simplicity of the
function, and the absence of unrealistic growth seen in other functions.
Demand, however, is forecast using an exponential function, which
considers compounded growth, as seen in the real-world. Linear
regression is used if an exponential function is found to be
sub-optimal. We define sub-optimal as the case that a solution can not
be found. We use the linear regression for this, due to its
characteristics of reducing unrealistic growth.

The forecasted electricity price @xmath years ahead is difficult to
ascertain accurately. We therefore use two methods for forecasting
these. The first is to simulate a market @xmath years ahead. The second
is to optimise for the predicted Price Duration Curve ( PDC )using a
genetic algorithm. We describe this optimisation in Section 3.3 .

For the simulated market, the forecasted data is used to simulate a
market @xmath years into the future using the electricity market
algorithm. We simulate a market based on the expected bids – based on
short run marginal cost (SRMC) – that every operating power plant will
make. SRMC is the cost for a power plant to produce one unit of
electricity, excluding fixed and capital costs. We include the removal
of plants that will be past their operating period, and the introduction
of plants that are in construction or pre-development stages. This
effectively runs a simulation within a simulation here. Where the GenCos
must use a simulation technique to understand future market conditions.

There may be scenarios where demand is forecast to grow significantly,
and limited investments have yet been made to meet that demand. The
expected price would be that of lost load. Lost load is defined as the
price customers would be willing to pay to avoid disruption in their
electricity supply. To avoid GenCos from estimating large profits, and
under the assumption that further power plant investments will be made,
the lost load price is replaced with a predicted electricity price using
linear regression based on prices at lower points of the demand curve.
If zero segments of demand are met, then the lost load price is used to
encourage investment.

Once this data has been forecasted, the NPV can be calculated. GenCos
must typically provide a certain percentage of upfront capital, with the
rest coming from investors in the form of stock and shares or debt
(WACC). The percentage of upfront capital can be customised by the user
in the configuration file. The GenCos then invest in the power plants
with the highest NPV.

###### Time-steps

Time-steps are the steps made in a simulation which simulate time. Each
time-step moves the simulation forward a single iteration, and mimics
time in our case. For the time-steps, two approaches were taken. For the
first approach, as per Chappin et al. Chappin2017 , we modelled the LDC
of electricity demand with twenty segments. Twenty segments enabled us
to capture the variation in demand throughout the year to a high degree
of accuracy, whilst reducing computational complexity. However, as we
show later in Section 3.4 , this led to an overestimation of the supply
of IRES .

For the second approach, we used representative days to model a year.
Representative days in this context are a subset of days which have
characteristics, that when scaled proportionally can accurately model an
entire year. To select these representative days, we used a @xmath
-means approach. We describe this in full detail in Section 3.2.1

Figure 3.4 demonstrates the simulation and how it co-ordinates runs. The
world contains data and brings together GenCos, the Power Exchange and
demand. The investment decisions are based on future demand and costs,
which in turn influence bids made.

To explain the simulation, we will start from the left side of Figure
3.4 and move to the right. A database of power plants is parametrised by
a database of parameters, as discussed in Table 3.1 . This ensures that
the plants have realistic properties.

Generation companies (GenCos) own and invest in these power plants
through the use of investment decisions. The GenCos than submit bids to
dispatch these power plants. They do this using the parameters of the
power plants, which for renewable energy inclues the renewable energy
capacity factors. For instance, wind, solar and hydro have varying
profiles throughout the day and year.

These bids are then augmented with the \ce CO2 price (tax) and fuel
price to create the actual bid. The yearly electricity spot market then
takes the demand and matches this to the submitted bids in merit order.
The demand grows (or falls) each year, depending on the scenario.

Exogenous variables include fuel and \ce CO2 prices as well as demand
growth. Once the data is initialised, the model calls on the Power
Exchange to operate the yearly electricity spot market. The world also
settles the accounts of the GenCos, by paying bids, and removing
operating and capital costs as well as loans and dividends.

#### 3.2.1 Representative days

In this Subsection, we describe the work which allowed us to determine
the granularity of time-steps. In this problem, there is a trade-off
between accuracy and compute time. We visualise this trade-off and
choose a cut-off point, in which we receive diminishing returns for an
increase in temporal granularity, in terms of accuracy. Specifically,
for this, we use representative days. Representative days, in this
context, are a subset of days which, when scaled up to 365 days can
adequately represent a year.

In this work, we initialised the model to a scenario of the United
Kingdom as an example. However, the fundamental dynamics of the model
remain the same for other decentralised electricity markets.

Similar to findings of other authors, using a relatively low number of
time-steps from the representative days leads to an overestimation of
the uptake of intermittent renewable energy resources (IRES) and an
underestimation of flexible technologies Haydt2011 ; Ludig2011 . This is
due to the fact that the full intermittent nature of renewable energy
could not be accurately modelled in such a small number of time-steps.
For instance, by modelling the entire year in only 20 time-steps, a
reduction in the ability for ElecSim to model hourly lulls in wind
speed, the absence of solar irradiance at night time and differences in
electricity demand. This is critically important for a power system with
high amounts of electricity generated from renewable electricity.

To address this problem, whilst maintaining a tractable execution time,
we approximated a single year as a subset of proportionally weighted,
representative days. This enabled us to reduce computation time whilst
maintaining accuracy. Each representative day consisted of 24 equally
separated time-steps, which model hours in a day. Hourly data was
chosen, as this was the highest resolution of the dataset available for
offshore and onshore wind and solar irradiance Pfenninger2016 . A lower
resolution would allow us to model more days; however, we would lose
accuracy in terms of the variability of the renewable energy sources.

Similar to Nahmmacher et al. Nahmmacher2016 we used a clustering
technique to split similar days of weather and electricity demand into
separate groups. We then selected the historic day that were closest to
the centre of the cluster, known as the medoid, as well as the average
of the centre, known as the centroid Nahmmacher2016 . Similar to
Nahmmacher, we used Ward’s clustering algorithm and selected the
centroid doi:10.1080/01621459.1963.10500845 . However, we also used the
@xmath -means clustering algorithm forgy65 . This was due to the ability
for the @xmath -means algorithm to cluster time-series into relevant
groups Kell2018a . These days were scaled proportionally to the number
of days within their respective cluster to approximate a total of 365
days. A representative day is one where solar and wind are chosen on the
same day. This is because on a particular day, wind and solar are
correlated. The Ward’s clustering algorithm is an extension of the work
published in Kell2020 .

Equation 3.2 shows the series for a medoid or centroid, selected by the
clustering algorithms:

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

where @xmath is the medoid for series @xmath , where @xmath refers to
offshore wind capacity factor, onshore wind capacity factor, solar
capacity factor and electricity demand, @xmath is the hour of the day
and @xmath is the respective cluster. @xmath refers to the capacity
values at each hour of the representative day. In other words, @xmath is
a set of offshore wind capacity factor, onshore wind capacity factor,
solar capacity fator and electricity demand.

We then calculated the weight of each cluster. This gave us a method of
assigning the relative importance of each representative day when
scaling the representative days up to a year. The weight is calculated
by the proportion of days in each cluster. This gives us a method of
determining how many days within a year are similar to the selected
medoid or centroid. The calculation for the weight of each cluster is
shown by Equation 3.3 :

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

where @xmath is the weight of cluster @xmath , @xmath is the number of
days in cluster @xmath , and @xmath is the set of days that have been
used for clustering.

The next step was to scale up the representative days to represent the
duration curve of a full year. A duration curve is similar to the load
duration curve shown in Figure 3.3 , however can be defined for other
processes such as wind speed and solar irradiance. We achieved this by
using the weight of each cluster, @xmath , to increase the number of
hours that each capacity factor contributed in a full year. Equation 3.4
details the scaling process to turn the medoid or centroid, shown in
Equation 3.2 , into a scaled day. Where @xmath is the scaled day:

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

where @xmath . @xmath is the set of weights, @xmath is the weight for
the first day, @xmath the weight for the second day, and @xmath the
@xmath day. Equation 3.4 effectively extends the length of the day,
proportional to the amount of days in the respective cluster.

Finally, each of the scaled representative days were concatenated to
create a single series used for the calculations in the simulation. This
concatenated number contains information of the required capacity
factors and the respective duration curve. Equation 3.5 displays the
total time series of a series @xmath , where each scaled medoid is
concatenated to produce an approximated time series, @xmath :

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

the total number of hours in the approximated time series, @xmath , is
equal to the number of hours in a day multiplied by the number of days
in a year, which gives the total number of hours in a year ( @xmath ),
as shown by Equation 3.6 :

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

where @xmath the Set of clusters.

#### 3.2.2 Error Metrics

To measure the validity of our approximation using representative days
and also compare the optimum number of days, or clusters, we used a
technique similar to Poncelet et al. Dhaeseleer2015 ; Poncelet2017 . We
trialled the number of clusters against three different metrics:
Correlation ( CE )( @xmath ), Normalised Root Mean Squared Error ( NRMSE
)and Relative Energy Error ( REE )( @xmath ).

@xmath is the average value over all the considered time series @xmath
compared to the observed average value of the set @xmath . Where @xmath
are the observed time series and @xmath are the scaled, approximated
time series using representative days. @xmath is shown formally by
Equation 3.7 :

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

where @xmath is the duration curve for @xmath and @xmath is the duration
curve for @xmath . In this context, the duration curve can be
constructed by sorting the capacity factor and electrical load data from
high to low. The @xmath axis for the duration curve exhibits the
proportion of time that each capacity factor represents, similar to the
load duration curve shown in Figure 3.3 . The approximation of the
duration curve is represented in this text as @xmath . @xmath refers to
a specific time step of the original time series. @xmath refers to the
approximated duration curve for @xmath . Note that in this text @xmath
refers to the absolute value, and @xmath refers to the cardinality of a
set and @xmath refers to the total number of of considered time series.

Specifically, the sum of the observed values, @xmath , and approximated
values, @xmath , for all of the time series are summed. The proportional
difference is found, which is summed for each of the different series,
@xmath , and divided by the number of series, to give @xmath .

Another requirement is for the distribution of load and capacity factors
for the approximated series to correspond to the observed time series.
It is crucial that we can account for both high and low levels of demand
and capacity factor for IRES generation. This enables us to model for
times where flexible generation capacity is required.

The distribution of values can be represented by the duration curve (
@xmath ) of the time series. Therefore, the average normalised
root-mean-square error ( @xmath ) between each @xmath is used as an
additional metric. The @xmath is shown formally by Equation 3.8 :

  -- -------- -- -------
     @xmath      (3.8)
  -- -------- -- -------

Specifically, the difference between the approximated and observed
duration curves for each time-step @xmath is calculated. The average
value is then taken of these differences. This average value is then
normalised for the respective time series @xmath . The average of these
average normalised values for each time series are then taken to provide
a single metric, @xmath .

The final metric used is the correlation between the different time
series. This is used due to the fact that wind and solar output
influences the load within a single region, solar and wind output are
correlated, as well as offshore and onshore wind levels within the UK.
This is referred to as the average correlation error ( @xmath ) and
shown formally by Equation 3.9 :

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

where @xmath is the Pearson correlation coefficient between two time
series @xmath , shown by Equation 3.10 . Here, @xmath represents the
value of time series @xmath at time step t, and @xmath refers to the
mean of the values of the time series @xmath at time @xmath :

  -- -------- -- --------
     @xmath      (3.10)
  -- -------- -- --------

##### Integrating higher temporal granularity

To integrate the additional temporal granularity of the model using the
representative days, extra time-steps were taken per year. This did not
require a significant change to the ElecSim code, simply stepping
through each hour of a representative day to gauge renewable energy
capacity factors and electricity demand. This meant that we had to take
an additional number of time-steps when compared to the 20 LDC case.
Specifically, as we modelled each time-step as an hour, and a day
contains 24 hours, we ran 24 time-steps per representative day.

The difference between using the hourly representative days and 20
segments, is that peaks and troughs can be modelled more accurately with
representative days. An LDC with 20 load segments can accurately model
yearly intervals where each interval has an average of 5%, or 18.25 days
(365 days @xmath days). It is true that peaks and troughs occur for less
than 18.25 days, and therefore this granularity is lost with a 20
segment LDC. This amount of granularity may have been acceptable with
predictable generation technologies, but this becomes significantly more
challenging when trying to model intermittent renewable electricity.

It is true that we could have modelled more than 20 segments in an LDC
to have a similar number time-steps as in a set of representative days.
However, this would have given us an average of a year, in regular
intervals. This would not provide a diverse set of days which are
significantly different enough from each other. We did not want to
capture solely the averages, but also the extremes that can occur in a
year, which would have been lost through a simple LDC averaging
approach.

The higher temporal granularity of the model enabled us to accurately
model the hourly fluctuations in solar and wind which leads to more
accurate expectations of the investment opportunities of these
technologies Haydt2011 ; Ludig2011 .

GenCos make bids at the beginning of every time-step, and the Power
Exchange matches demand with supply in merit-order dispatch using a
uniform pricing market. An example of the electricity mix in a single
representative day is shown in Figure 3.5 . Figure 3.5 displays the high
utilisation of low marginal-cost generators such as nuclear, wind and
photovoltaics. At hour 19, an increase in offshore wind leads to a
direct decrease in CCGT. In contrast to this, a decrease in offshore and
onshore between the hours of 8 and 12 lead to an increase in dispatch of
coal and CCGT. One would expect this behaviour to prevent blackouts and
meet demand at all times. This process has enabled us to more closely
match fluctuations in IRES.

### 3.3 Validation and performance

In this Section, we detail the validation approaches taken in our model.
For this, we take two approaches. One is to compare the price duration
curve of the actual vs our simulated price duration curve in 2018 using
the 20 time-steps per year approach. The other is to use
cross-validation between the years 2013 and 2018, using our
representative days approach.

#### 3.3.1 Price Duration Curve Validation

Validation of models is important to ascertain that the output is
accurate. However, it should be noted that these long-term simulations
are not predictions of the future, rather possible outcomes based upon
certain assumptions. Jager posits that a certain outcome or development
path, captured by empirical data, might have developed in a completely
different direction due to chance. However, the processes that emerge
from a model should be realistic and in keeping with expected behaviour
Jager2006a .

We begin by comparing the price duration curve in the year 2018 for the
case with 20 time-steps. Figure 3.6 shows the N2EX Day Ahead Auction
Prices of the UK nordpool_2019 , the Monte-Carlo simulated electricity
prices, and the non-Monte-Carlo electricity price throughout the year
2018. Fuel prices varying throughout a year, as does @xmath and WACC.
WACC is sampled from a Gaussian distribution with a standard deviation
of @xmath %. @xmath is sampled from a uniform distribution between 30%
and 200% of the mean @xmath price, whilst fuel price is sampled from the
residuals of an ARIMA model fit on historical data. The N2EX Day Ahead
Market is a day ahead market run by Nord Pool AS. Nord Pool AS runs the
largest market for electrical energy in Europe, measured in volume
traded and in market share nordpool_2019 .

We ran the initialisation of the model 40 times to capture the price
variance. Outliers were removed as on a small number of occasions large
jumps in prices at peak demand occurred which deviated from the mean. We
did this, as although this does occur in real life, it occurs at a
smaller fraction of the time than 5% of the year (twenty time-steps per
year). Therefore the results would be unreasonably skewed for the
highest demand segment.

Figure 3.6 demonstrates very little variance in the non-stochastic case.
This is due to the fact that combined cycle gas turbines (CCGTs) set the
spot price. These CCGTs have little variance between one another as they
were calibrated using the same dataset. By adding stochasticity of fuel
prices and operation and maintenance prices, a curve that more closely
resembles the actual data occurs. The stochastic curve, however, does
not perfectly fit the real data, which may be due to higher variance in
fuel prices and historical differences in operation and maintenance
costs between power plants. One method of improving this would be
fitting the data used to parametrise to the curve. Figure 3.6 shows that
up to 4,000 hours there is an overestimation. Following this, there is
an underestimation. This is due to the fact that some power generators
are more expensive than the average, and some are less expensive, by
chance.

Table 3.2 shows performance metrics of the stochastic and non-stochastic
runs versus the actual price duration curve. The stochastic
implementation improves the Mean Absolute Error ( MAE )of the
non-stochastic case by @xmath .

#### 3.3.2 Validation of model with representative days

In this Section, we detail the approach taken in this work to validate
our model using representative days as time-steps.

Figure 3.7 displays the error metrics versus number of clusters, and
therefore days. This is because from each cluster, the most central day
of the cluster is chosen and used. Therefore, from eight clusters, eight
representative days are chosen. Centroids are the days at the centre of
the cluster, medoids are the average day of the cluster and Ward’s is
another clustering technique.

Both @xmath and @xmath display similar behaviour for the @xmath -means
approach (centroids and medoids), namely the error improves
significantly from a single cluster to eight clusters for both centroids
and medoids. For the number of clusters greater than eight, there are
diminishing returns. For @xmath , however, the error metric is best at a
single cluster, and gets worse with the number of clusters. The Wards
approach Salkind2013 , however, performs significantly worse for all
metrics at every number of clusters.

We chose eight clusters, for centroids and medoids, as a compromise
between accuracy of the three error metrics and compute time of the
simulation. This is because eight was the largest number of clusters
that gave us the lowest score for @xmath , @xmath and @xmath without
significantly increasing compute time. Whilst there was little
significant difference between centroid and medoid, we chose to use the
medoids due to the fact that the extreme high and low values would not
be lost due to averaging Hilbers2019 . Therefore, a total of eight
representative days were chosen.

To achieve this, we use a Genetic Algorithm ( GA )to find the predicted
price duration curves, which lead to the smallest error between our
simulated electricity mix and the scenarios tested. The scenarios
examined here are the observed electricity mix of the UK between 2013
and 2018 and the UK Government Department for Business, Energy and
Industrial Strategy ( BEIS )reference scenario projected in 2018 till
2035. When projecting the BEIS reference scenario, we also optimise for
nuclear subsidy and uncertainty in the price duration curves.

As mentioned in Section 3.2 , GenCos make investments based upon the net
present value. As shown in Equation 3.1 , an expectation of the net cash
flow, @xmath is required.

The net cash flow, @xmath , is calculated by subtracting both the
operational and capital costs from revenue over the expected lifetime of
the prospective plant. The revenue gained by each prospective plant is
the expected price they will gain per expected quantity of MWh sold over
the expected lifetime of the plant. This is shown formally in Equation
3.11 :

  -- -------- -- --------
     @xmath      (3.11)
  -- -------- -- --------

where @xmath is the expected quantity of megawatts sold in hour @xmath
of year @xmath . @xmath is the Predicted Price Duration Curve ( PPDC )at
year @xmath and hour @xmath . @xmath is the variable cost of the power
plant, which is dependent on expected megawatts of electricity produced,
@xmath is the capital cost.

The predicted price duration curve ( @xmath ) is an expectation of
future electricity prices over the lifetime of the plant. The @xmath is
a function of supply and demand. However, with renewable electricity
generator costs falling IRENA2014 , future prices are uncertain and
largely dependent upon long-term scenarios of electricity generator
costs, fuel prices, carbon taxes and investment decisions IRENA2014 .
Due to the uncertainty of future electricity prices over the horizon of
the lifetime of a power plant, we have set future electricity prices as
an exogenous variable that can be set by the user in ElecSim.

To gain an understanding of expected electricity prices that lead to
particular scenarios, we use a genetic algorithm optimisation approach.
This enables us to understand the range of future electricity prices
that lead to certain scenarios developing. In addition, it allows us to
understand whether the parameters required for certain scenarios to
develop are realistic. This enables us to check the assumptions of our
model and the likelihood of scenarios. Further, using these optimised
parameters, we are better able to further explore ‘ what-if ’ scenarios.

To verify the accuracy of the underlying dynamics of ElecSim, the model
was initialised with data available in 2013 and allowed to develop until
2018. We used a genetic algorithm to find the optimum price duration
curve predicted ( @xmath ) by the GenCos ten years ahead of the year of
the simulation. This @xmath was used to model expected rate of return of
prospective generation types, as shown in Equations 3.1 and 3.11 .

The genetic algorithm’s objective was to reduce the error of simulated
and observed electricity mix in the year 2018 by finding a suitable
@xmath used by each of the GenCos for investment evaluation.

##### Scenario

For this experiment, we initialised ElecSim with parameters known in
2013 for the UK. ElecSim was initialised with every power plant and
respective GenCo that was in operation in 2013 using the BEIS DUKES
dataset dukes_511 . The funds available to each of the GenCos was taken
from publicly released official company accounts at the end of 2012
companies_house .

To ensure that the development of the electricity market from 2013 to
2018 was representative of the actual scenario between these years, we
set the exogenous variables, such as carbon and fuel prices, to those
that were observed during this time period. In other words, the scenario
modelled equated to the observed scenario.

The data for the observed EU Emission Trading Scheme (ETS) price between
2013 and 2018 was taken from Ember Climate eu-ets . Fuel prices for each
of the fuels were taken from the department for business, energy and
industrial strategy beis_fuel_price . The electricity load data was
modelled using data from Elexon portal ad Sheffield University
gbnationalgridstatus2019 , offshore, and onshore wind and solar
irradiance data was taken from Pfenninger et al. Pfenninger2016 . There
were three known significant coal plant retirements in 2016. These were
removed from the simulation at the beginning of 2016.

##### Optimisation problem

The price duration curve was modelled linearly in the form @xmath ,
where @xmath is the cost of electricity, @xmath is the gradient and
refers to the relative increase in price with respect to demand, @xmath
is the demand of the price duration curve, and @xmath is the intercept,
or the price of electricity as demand approaches zero. A linear price
duration curve was chosen due to its simplicity, small amount of
parameters required for optimisation and also the similarity to the
actual price duration curve.

Equation 3.12 details the optimisation problem formally:

  -- -- -- --------
           (3.12)
  -- -- -- --------

where @xmath refers to the average percentage electricity mix during
2018 for wind (both offshore and onshore generation), nuclear, solar,
CCGT, and coal, where @xmath refers to the set of these values. @xmath
refers to observed electricity mix percentage for the respective
generation type in 2018. @xmath refers to the simulated electricity mix
percentage for the respective generation type, also in 2018. The input
parameters to the simulation are @xmath and @xmath from the linear
@xmath , previously discussed, ie. @xmath . @xmath refers to the
cardinality of the set.

#### 3.3.3 Long-Term Scenario Analysis

In addition to verifying the ability for ElecSim to mimic observed
investment behaviour over five years, we compared ElecSim’s long-term
behaviour to that of the UK Government’s Department for Business, Energy
and Industrial Strategy (BEIS) DBEIS2019 . This scenario shows the
projections of generation by technology for all power producers from
2018 to 2035 for the BEIS reference scenario. This is the same scenario
as discussed in the next Section.

##### Scenario

We initialised the model to 2018 based on our previous work Kell . The
scenario for the development of fuel prices and carbon prices were
matched to that of the BEIS reference scenario DBEIS2019 .

##### Optimisation problem

The optimisation approach taken was a similar process to that discussed
in Subsection 3.3.2 , namely using a genetic algorithm to find the
optimum expected price duration curve. However, instead of using a
single expected price duration curve for each of the agents for the
entire simulation, we used a different expected price duration curve for
each year, leading to 17 different curves. This enabled us to model the
non-static dynamics of the electricity market over this extended time
period. The agents are dynamic, however, not heterogenous in this
regard. This is due to the exponentially increasing computational time
of calculating predicted price duration curves for each agent.

In addition to optimising for multiple expected price duration curves,
we optimised for a nuclear subsidy, @xmath . Further, we optimised for
the uncertainty in the expected price parameters @xmath and @xmath ,
named @xmath and @xmath respectively, where @xmath is the standard
deviation in a normal distribution. @xmath and @xmath are the parameters
for the predicted price duration curve, as previously defined, of the
form @xmath . Effectively, @xmath and @xmath represent the distribution
around the price duration curve, that is sampled in a monte-carlo
manner.

This enabled us to model the different expectations of future price
curves between the independent GenCos. The addition of a nuclear subsidy
as a parameter is due to the likely requirement for government to
provide subsidies for new nuclear Suna2016 .

A modification was made to the reward algorithm for the long-term
scenario case. Rather than using the discrepancy between observed and
simulated electricity mix in the final year (2018) as the reward, a
summation of the error metric for each simulated year was used. This is
detailed formally in Equation 3.13 :

  -- -------- -- --------
     @xmath      (3.13)
  -- -------- -- --------

where @xmath and @xmath are the sets of the 17 parameters of @xmath and
@xmath respectively for each year, @xmath . @xmath refers to each year
between 2018 and 2035. @xmath and @xmath refer to the parameters for the
predicted price duration curve, of the form @xmath for the year @xmath .
@xmath refers to the actual electricity mix percentage for the year
@xmath and generation type @xmath . Finally, @xmath refers to the
simulated electricity mix percentage with the input parameters to the
simulation of @xmath and @xmath for the year @xmath .

#### 3.3.4 Genetic Algorithms

Genetic Algorithms (GAs) are a type of evolutionary algorithm which can
be used for optimisation. We chose the genetic algorithm for this
application due to its ability to find good solutions with a limited
number of simulation runs, the ability for parallel computation and its
ability to find global optima. These characteristics are useful for our
application, as a single simulation can take up to 36 hours.

In this Section, we detail the genetic algorithm used in this work.
Initially, a population @xmath is generated for generation 0. This
population of individuals is used for the parameters to the simulation.
The output of the simulations for each of the individuals are then
evaluated. A subset of these individuals @xmath are chosen for mating.
This subset is selected proportional to their fitness, with ‘fitter’
individuals having a higher chance of reproducing to create the
offspring group @xmath . @xmath have characteristics dependent on the
genetic operators: crossover and mutation. The genetic operators are an
implementation decision FogelDavidB2009 .

Once the new population has been created, the new population @xmath is
created by merging individuals from @xmath and @xmath . See Algorithm 1
for detailed pseudocode.

We used the DEAP evolutionary computation framework to create our
genetic algorithm Gagn2012 . This framework gave us sufficient
flexibility when designing our genetic algorithm. Specifically, it
enabled us to persist the data of each generation after every iteration
to allow us to verify and analyse our results in real-time.

1: @xmath

2: initialize @xmath

3: evaluate structures in @xmath

4: while termination condition not satisfied do

5: @xmath

6: select reproduction @xmath from @xmath

7: recombine and mutate structures in @xmath forming @xmath

8: evaluate structures in @xmath

9: select each individual for @xmath from @xmath or @xmath

10: end while

Algorithm 1 Genetic algorithm FogelDavidB2009

##### Parameters for Validation with Observed Data

The parameters chosen for the problem explained in Section 3.3.3 was a
population size of @xmath , a crossover probability of @xmath , a
mutation probability of @xmath and the parameters, @xmath and @xmath ,
as per Equation 3.12 , were given the bounds of @xmath and @xmath
respectively.

The bounds for @xmath and @xmath were calculated to ensure a positive
price duration curve, with a maximum price of £300 for 50,000MW. The
population size was chosen to ensure a wide range of solutions could be
explored, whilst limiting compute time to @xmath 1 day per generation to
allow for sufficient verification of the results. The crossover and
mutation probabilities were chosen due to suggestions from the DEAP
evolutionary computation framework Gagn2012 .

##### Parameters for Long-Term Scenario Analysis

The parameters chosen for the genetic algorithm for the problem
discussed in Section 3.3.3 are displayed here. The population size was
@xmath , a crossover probability of @xmath , a mutation probability of
@xmath . The parameters @xmath , @xmath were given the bounds @xmath and
@xmath respectively, whilst @xmath and @xmath were both given the bounds
of @xmath . Effectively, we did not allow these parameters to be larger
or smaller than these bounds.

The population size was slightly increased, and the bounds reduced when
compared to the parameters for Section 3.3.4 . This was to increase the
likelihood of convergence to a global optimum, which was more
challenging to achieve due to the significantly higher number of
parameters.

#### 3.3.5 Results

Here we present the results of the problem formulation of Section 3.3.3
. Specifically, we compare the ability of our model to that of BEIS in
the context of a historical validation between 2013 and 2018 of the UK
electricity market. We also compare our ability to generate scenarios up
to 2035 with that of BEIS.

##### Validation with Observed Data

Figure 3.8 displays the output of ElecSim under the validation scenario,
BEIS’ projections and the observed electricity mix between 2013 and
2018, as explained in Section 3.3.2 . We calibrated our data with the
BEIS projection to generate these results.

The observed electricity mix changed significantly between 2013 and
2018. A continuous decrease of electricity production from coal
throughout this period was observed. 2015 and 2016 saw a marked decrease
of coal, which can be explained by the retirement of 3 major coal power
plants. The decrease in coal between 2013 and 2016 was largely replaced
by an increase in gas. After 2016, renewables play an increasingly large
role in the electricity mix and displace gas.

Both ElecSim and BEIS were able to model the fundamental dynamics of
this shift from coal to gas as well as the increase in renewables. Both
models, however, underestimated the magnitude of the shift from coal to
gas. This could be due to unmodelled behaviours such as consumer
sentiment towards highly polluting coal plants, a prediction from
industry that gas would become more economically attractive in the
future or a reaction to The Energy Act 2013 which aimed to close a
number of coal power stations over the following two decades
uk_energy_act .

ElecSim was able to closely model the increase in renewables throughout
the period in question, specifically predicting a dramatic increase in
2017. This is in contrast to BEIS, who predicted that an increase in
renewable energy would begin in 2016. However, both models were able to
accurately predict the proportion of renewables in 2018.

ElecSim was able to better model the observed fluctuation in nuclear
power in 2016. BEIS, on the other hand, projected a more consistent
nuclear energy output. This small increase in nuclear power is likely
due to the decrease in coal during that year. BEIS consistently
underestimated the share of nuclear power.

We display the error metrics to evaluate our models 5-year projections
in Table 3.3 . Where MAE is mean absolute squared error, MASE is Mean
Absolute Squared Error ( MASE )and Root Mean Squared Error ( RMSE )is
the root mean squared error.

We are able to improve the projections for all generation types when
compared to the naive forecasting approach using ElecSim, as shown by
the MASE. Where the naive approach is simply predicting the next
time-step by using the last known time-step, in this case, the last
known time-step is the electricity mix percentage for each generation
type in 2013.

Table 3.4 displays the same set of error metrics for BEIS’ projections.
The technologies displayed here are slightly different to those
displayed in Table 3.3 due to the presentation of the data by BEIS. As
shown in Figure 3.8 , we are able to predict the rise in renewables and
nuclear better than that of BEIS, however, perform slightly worse for
coal and gas.

Figure 3.9 displays the optimal predicted price duration curve ( @xmath
) found by the genetic algorithm. This price curve was used by the
GenCos to achieve the results shown in Figure 3.11 .

The orange points show the simulated price duration curve for the first
year of the simulation (2018). The red line (Simulated Fit 2018) is a
linear regression that approximates the simulated price duration curve
(PDC 2018). The blue line shows the price duration curve predicted (
@xmath ) by the GenCos to be representative of the expected prices over
the lifetime of the plant.

The optimal predicted price duration curve ( @xmath ) closely matches
the simulated fit in 2018, shown by Figure 3.9 . However, the @xmath has
a slightly higher peak price and lower baseload price. This could be due
to the fact that there is a predicted increase in the number of
renewables with a low SRMC. However, due to the intermittency of
renewables such as solar and wind, higher peak prices are required to
generate in times of low wind and solar irradiance at the earth’s
surface.

To generate Figure 3.10 , we ran 40 scenarios with the @xmath to observe
the final, simulated electricity mix. The error bars are computed based
on a Normal distribution 95% confidence interval.

ElecSim was able to model the increase in renewables and stability of
nuclear energy in this time. ElecSim was also able to model the
transition from coal to gas, however, underestimated the magnitude of
the transition. This was similar to the projections BEIS made in 2013 as
previously discussed.

##### Long-Term Scenario Analysis

In this Section, we discuss the results of the analysis of the BEIS
reference scenario explained in Section 3.3.3 . Specifically, we created
a scenario that mimicked that of BEIS in ElecSim and optimised a number
of parameters using a genetic algorithm to match this scenario. Through
this, we are able to gain confidence in the underlying dynamics of
ElecSim to simulate long-term behaviours. Further, this enables us to
verify the likelihood of the scenario by analysing whether the
parameters required to make such a scenario are realistic.

Figure 3.11 displays the electricity mix projected by both ElecSim and
BEIS. To generate this image, we ran 60 scenarios under the optimal
collection of predicted price duration curves, nuclear subsidy and
uncertainty in predicted price duration curves. The optimal parameters
were chosen by choosing the parameter set with the lowest mean error per
electricity generation type and per year throughout the simulation, as
shown by Equation 3.13 .

Figure 3.12 displays the optimal predicted price duration curves (
@xmath s) per year of the simulation, shown in blue. These are compared
to the price duration curve simulated in 2018, as per Figure 3.9 . The
optimal nuclear subsidy, @xmath , was found to be @xmath £ @xmath /MWh,
the optimal @xmath and @xmath were found to be @xmath and @xmath
respectively. In this context, optimal means the values which produced
the required electricity mix scenarios.

The BEIS scenario demonstrates a progressive increase in nuclear energy
from 2025 to 2035, a consistent decrease in electricity produced by
natural gas, an increase in renewables and decrease to almost 0% by 2026
of coal. ElecSim is largely able to mimic the scenario by BEIS. A large
increase in renewables is projected, followed by a decrease in natural
gas. A significant difference, however, is the step-change in nuclear
power in 2033. This led to an almost equal reduction in natural gas
during the same year. In contrast, BEIS project a continuously
increasing share of nuclear. We argue that the ElecSim projection of
nuclear power is more realistic than that of BEIS due to the
instantaneous nature of large nuclear power plants coming on-line. This
is largely due to ElecSim’s discrete nature of power plants.

Figure 3.12 exhibits the price curves required to generate the scenario
shown in Figure 3.11 . The majority of the price curves are similar to
the simulated price duration curve of 2018 (Simulated Fit 2018).
However, there are some price curves which are significantly higher and
significantly lower than the predicted price curve of 2018. These cycles
in predicted price duration curves may be explained by investment cycles
typically exhibited in electricity markets Gross2007 .

In this context, investment cycles reflect a boom and bust cycle over
long timescales. When electricity supply becomes tight relative to
demand, prices rise to create an incentive to invest in new capacity.
Price behaviour in competitive markets can lead to periods of several
years of low prices (close to short-run marginal cost)
white2005concentrated .

As plants retire or demand increases, the market becomes tighter until
average prices increase to a level above the threshold for investment in
new power generators. At this point, investors may race to bring new
plants on-line to make the most out of the higher prices. Once adequate
investments have been made, the market returns to a period of low prices
and low investment until the next price spike Gross2007 .

The nuclear subsidy, @xmath , of @xmath £ @xmath /MWh in 2018 prices is
high compared to similar subsidies, but this may reflect the difficulty
of nuclear competing with renewable technology with a short-run marginal
cost that tends to £ @xmath . However, subsidising nuclear may be an
important option to gain reliable, low-carbon, base load, irrespective
of the price. The low values of @xmath and @xmath demonstrates that the
expectation of prices does not necessarily have to differ significantly
between GenCos. This may be due to the fact that GenCos have access to
the same market information. However, it may be true that agent’s
portfolios and cash reserves affect their decisions.

### 3.4 Scenario Testing

In this section we display scenario runs of ElecSim using both 20
time-steps per year and using representative days.

#### 3.4.1 Scenarios for 20 time-steps

In this Section, we vary the carbon tax and grow or reduce total
electricity demand. This enables us to observe the effects of a carbon
tax on investment. In this work, we have presented scenarios where
electricity demand decreases 1% per year, due to the recent trend in the
UK.

For the first scenario run displayed, we have approximated the
predictions by the UK Government, where carbon tax increases linearly
from £18 to £200 by 2050 Department2016 . Figure 3.12(a) demonstrates a
significant increase in gas turbines in the first few years, followed by
a decrease, with onshore wind increasing.

Figure 3.12(b) displays a run with a £40 carbon tax. This run
demonstrates a higher share of onshore wind than in the previous
scenario.

We experimented with the following levels of carbon tax: £10 ($13), £20
($26) and £70 ($90) with demand decreasing 1% per year. This was chosen
due to the increasing efficiency of homes, industry and technology, as
this was the recent trend in the UK. We run each scenario eight times to
capture the stochastic nature of the process. Via the observation of the
emergent investment behaviour until 2050, an understanding of how
real-life investors may behave emerges.

Figure 3.13(a) shows that with a carbon tax of £10, whilst renewable
technology does grow, gas power plants provide the majority of supply in
each year. However, at a level of £20 the increase in wind turbines is
enough to match gas turbines. A carbon tax of £70, however, shows a near
100% uptake of wind turbines.

These results demonstrate the large impact that a carbon tax can have on
the final electricity mix over the long term. Policy makers can
effectively remove high proportions of carbon emitting technology
lock-in by setting a carbon tax. The limitations to this work, from the
perspective of policy makers, is the inability to know for certain what
the final electricity mix will be as the results show a large amount of
variance.

The reason that onshore wind and CCGT become dominant is due to their
perceived low-cost over the lifetime of these projects when compared to
the other electricity generation technologies. The capacity factor of
onshore wind is lower than that of CCGT, and therefore a high investment
in onshore wind is required to meet increasing shares of electricity
demand. For example, if onshore wind has a capacity factor of 0.2, an
installed capacity @xmath the total electricity demand would be required
to meet 100% of demand. This is more expensive than investing in CCGT,
which has a higher capacity factor and can therefore meet more demand
with a lower total installed capacity.

This trade-off between CCGT and onshore wind becomes more nuanced with
different sized carbon taxes. For instance, in the £70 carbon tax case,
it becomes economically efficient to install large amounts of onshore
wind.

It is currently infeasible, however, for the power supply to be provided
solely by wind turbines today. This overestimation, however, is due to
the low time granularity of the model Collins2017 . This scenario
therefore assumes perfect storage capabilities. This is because the
model assumes that onshore wind’s capacity factor is spread evenly
across each of the LDC segments. However, this is not the case in the
real world. For instance, the capacity factor of wind may be 0 at some
times, and 1 at other times. This means that simply increasing the
number of wind turbines will never meet electricity demand at times when
there is no wind. Therefore, at these times other technologies are
required, leading to a mix of different electricity generators. More
detail from the representative days can capture days of zero wind and
high demand.

These runs demonstrate that a consistent, but relatively low carbon tax
can have a larger impact in the uptake of renewable energy than
increasing carbon tax over a long time frame. We hypothesise that an
early carbon tax affects the long-term dynamics of the market for many
years. We, therefore, suggest early action on carbon tax to transition
to a low-carbon energy supply.

#### 3.4.2 Discussion

Agent-based models provide a method of simulating investor behaviour in
an electricity market. We observed that an increase in carbon tax had a
significant impact on investment. These findings enable policymakers to
better understand the impact that their decisions may have. For a high
uptake of renewable energy technology, rapid results can be seen after
ten years with a carbon tax of £70 ($90).

#### 3.4.3 Scenarios for representative days

In this Section, we discuss various scenarios under the model which uses
representative days as time-steps. This work builds upon the work in
Section 3.3 ; we used the same predicted price duration curves as
modelled on BEIS’ scenario. We selected an optimal carbon tax level
which would reduce both electricity price and carbon emissions, as shown
later in Chapter 5 . The optimal carbon tax strategy found in Chapter 5
is shown by Figure 3.15 . Each of the scenarios were run ten times to
display any variability in the results. We chose ten runs to limit both
computation time and cost.

Figures 3.16 and 3.17 show the electricity mixes of various demand
scenarios. Figure 3.16 displays the scenarios in which demand either
stays flat, or decreases by 1% and 2%. For these scenarios, it can be
seen that solar is the dominant electricity supply, supplying @xmath
50%, with nuclear power second supplying between 20% and 30%. With a
decreasing demand scenario of 1% per year, as shown by Figure 3.15(b) ,
nuclear provides a higher proportion by the year 2034, of @xmath 30%,
however before the year 2033, provides a similar proportion to the other
scenarios as shown by Figures 3.15(a) and 3.15(c) .

For the scenarios shown in Figure 3.16 , CCGT, coal and onshore provide
around @xmath 10% each by 2034. Coal and CCGT, however, progress towards
0% whereas onshore wind increases. This is to be expected due to the
high carbon price, as shown by Figure 3.15 . Offshore does not exhibit a
high amount of investment. We believe this is the case as offshore wind
is more expensive than onshore wind, and in our scenario subsidies other
than for nuclear are not modelled.

Figure 3.17 shows scenarios where demand increases per year. Whilst
electricity mix distribution is similar to the scenarios shown in Figure
3.16 , solar plays a significantly increased role than nuclear. This may
be down to the large expense of nuclear, and the long time of deployment
of this type of technology. Solar power, on the other hand, is able to
be installed much more quickly to meet the high demand. This is
especially true for the scenarios shown in Figures 3.16(b) and 3.16(c)
where demand rises by 2% and 2.5% per year respectively.

These results show the impact that different demand scenarios can have
on the long-term electricity mix. It demonstrates that the electricity
mix may change significantly dependent on a few factors, such as the
electricity demand profile. Policy makers should therefore be conscious
of the different electricity mixes that could occur, and provide support
to ensure that demand is met at all times. For instance, a high nuclear
subsidy may be required to meet the base-load demand, or tax credits for
solar to ensure that increasing demand is met.

It is also possible for generator companies to understand which
technologies will be useful in different demand scenarios. This would
ensure that generator companies can make better decisions in the face of
uncertainty.

The limitations, however, are that these scenarios do not place a
probability to any of these scenarios. As, these results present the 2%
increase and 2% decrease scenarios as equally likely. This may not be
the case, and are dependent on more exogenous variables. This prevents
policy makers and generator companies from making informed decisions
about how they should adapt for relative scenarios.

#### 3.4.4 Performance

Figure 3.18 shows the running time for ElecSim with varying installed
capacity. We varied demand between 2GW and 320GW to see the effect of
different sized countries on running time. The makeup of the electricity
mix was achieved through stratified sampling of the UK electricity mix.
The results show a linear time complexity.

### 3.5 Sensitivity Analysis

In this Section, we investigate a sensitivity analysis of ElecSim, where
we vary the weighted average cost of capital and the down payment
required for investment. We used the reference scenario discussed in
Section 3.4 , with the optimal carbon tax to reduce both emissions and
electricity price. The work presented in this section is in addition to
the work published in Kell2020 .

We ran ten iterations per weighted average cost of capital and down
payment element. We did this due to the monte-carlo nature of the
simulation. We chose ten runs to give us sufficient variance in results,
but reduce compute power, to reduce both time and cost of calculation.
We show the results in the year 2035 due to this being where the largest
change in electricity mix is likely to be observed over the whole
horizon. Including more years in the analysis is possible, but would
lead to many graphs with limited additional information.

#### 3.5.1 Results

Figure 3.19 displays the results of the sensitivity analysis for the
Weighted average cost of capital for non-nuclear power generators. For
this, we trialled nine different Weighted Average Cost of Capital ( WACC
)values, where a value of 5.9% is the reference case
KincheloeStephenC1990TWAC .

It can be seen that the WACC has an effect on the total investment in
solar, nuclear and CCGT. With a WACC equal to or greater than 7.4%,
nuclear increases significantly, whilst solar decreases. Nuclear has a
WACC of 10%; therefore 7.4% may be the point where nuclear becomes more
competitive than solar in an environment where a low-carbon electricity
supply is elicited from an optimal carbon tax.

Offshore and coal do not change significantly over different levels of
WACC . This may be due to the fact that CCGT and onshore are more
competitive than coal and offshore, respectively, without external
subsidies.

Onshore seems to play a larger role at the lowest WACC , 3.9%. This may
be due to onshore wind’s high competitiveness when compared to nuclear.

Figure 3.20 displays the relative carbon emissions in 2035. With a low
WACC of 3.9%, the relative carbon emissions falls, on average, to zero.
This seems to be due to the high levels of solar, onshore and nuclear.

As WACC increases, so does carbon emissions, until there is a WACC of
0.5, where it reduces slightly. This is seemingly due to the higher
levels of CCGT and coal, which is able to displace solar. However, it
must be noted that in this scenario with the optimal carbon tax, the
relative carbon emissions remains low.

Figure 3.21 displays the sensitivity analysis results for different
levels of down payment required for all investments. We varied the down
payment required between the values of 10% and 40%.

As down payment required increases, so does nuclear and onshore, whilst
solar decreases. CCGT also shows an increase with down payment required.
The increase in down payment may help nuclear, due to the high costs of
WACC . With a higher down payment, the total costs of the project will
fall when compared to other, cheaper, generators. It is likely that
nuclear displaces solar in this case. CCGT increases up until a 35% down
payment required. This may be due to the increased use of onshore wind,
where CCGT and coal is required to fill for times of low wind speeds.

Figure 3.22 displays the relative carbon emissions versus down payment
required for investors. As down payment increases, so does relative
carbon emissions. This is down to the increasing role that CCGT and coal
play in the electricity mix and decreasing solar capacity. A
down-payment of 10% seems to have the lowest carbon emissions. This is
due to the high investment in solar, and low investment in CCGT and
coal.

These results demonstrate the importance of parameters on the final
electricity mix. For instance, the weighted average cost of capital can
have a large impact on the long-term electricity mix. As government and
policy makers have the ability to access capital at low-cost, it adds an
important lever that policy makers can have to influence the final
electricity mix. For instance, providing financial backing for solar,
which are highly capital intensive, could significantly reduce the
barriers to entry for this technology. Generator companies also have the
ability to analyse the attractiveness of each technology based on their
access to capital.

However, these results do not show a sensitivity analysis on different
costs. For example, capital expenditure may reduce in the future for
solar. This would change the results significantly. And so, generator
companies and policy makers should consider a wider range of future
technology cost scenarios in addition to the sensitivity analysis
presented here.

### 3.6 Limitations

As with all models, ElecSim features a number of limitations. That is,
ElecSim can not model the real world perfectly, and thus should be
relied on only with a spectrum of other quantitative and qualitative
methods.

A number of limitations exist of ElecSim, which have been listed in this
Section. This list, however, is not exhaustive, due to the possibility
for unknown unknowns.

1.  Requirement of exogenous price prediction curve.

2.  Challenges with long-term validation

3.  National model

4.  Lack of spatial resolution

5.  Lack of detailed supply curves (such as maximum wind and solar
    irradiance)

6.  Comparing to other models may not be correct as all models are wrong

7.  Future may change dramatically

8.  Lack of contracts for difference modelling and other subsidies
    (apart from nuclear)

#### 3.6.1 Exogenous price duration curve

The prediction of electricity prices in the long-term future is
difficult to predict with certainty. In ElecSim, we are required to
predict electricity prices over the lifetime of a plant. These plants
often have lifetimes of 25 years or more. Two methods, which were
utilised by the GenCos, were used by ElecSim: simulating a market ten
years into the future and optimising for predicted price duration curve.

The first method of simulating a market ten years into the future has
two major limitations. Firstly, the investments made ten years into the
future, the costs of fuel, tax all have a large effect. Predictions
which may seem perfectly reasonable at the time, may be unreasonable by
the time the market occurs in real life. For instance, investments in
another country may significantly reduce the cost of renewables in the
country where the simulation is being run. Secondly, even if the
simulation ten years into the future is correct, it does not mean that
it will be representative of the entire 25 years. However, to fix this
problem would require many more simulations, which may not make the
model more accurate.

To overcome this limitation, we removed the simulation to generate a
predicted price curve and used optimisation to find an optimal predicted
price duration curve to generate a scenario. We were then able to check
whether the predicted price duration curve had reasonable values. Whilst
this enabled us to remove the problem of simulating an uncertain market
in the future, it removed the ability to rapidly and quickly trial
significantly different scenarios. This is because the predicted price
duration curves were optimised for a certain scenario. Whilst it
remained possible to change multiple parameters, it became difficult to
trial many different carbon prices, as these have a significant impact
on the predicted price duration curve.

#### 3.6.2 Long-term validation

Long-term validation, as mentioned in Section 3.3 is particularly
difficult due to the uncertainties involved. We, therefore, do not
propose that ElecSim is used as a ground truth. Instead, it should be
used as a way to understand possibilities that could arise from
scenarios under certain assumptions and conditions. It is only with this
knowledge that the model should be used. However, we argue that, whilst
limited, quantitative advice has a large role to play in understanding
the future, and reducing the uncertainty through understanding complex
systems, such as electricity markets. It is only through building and
understanding such models that the uncertainty and complexity involved
can be understood.

Additionally, when modelling the future, many other models compare their
outcomes to other models for a specific scenario. Whilst this has the
benefit of checking that the underlying dynamics of the models are
similar, or the same, it does not test whether the models fit correctly
to real-life. It is for this reason that we used cross-validation as
well as comparing our model to that of the UK Government, BEIS
department.

However, a significant problem with using historical data to validate a
model, is that, we know more exogenous information about what happened
during the past than we do about the future. Therefore the comparison
between the past and future is not equal. For instance, we knew that
certain coal power plants were taken out of service during the 2013-2018
period. We do not know for certain that this will happen in the future.
It is therefore not possible to equate the past and forward scenarios.
This fundamental truth will always remain a problem.

However, the modelling process is stronger by using as much available
data as possible. If we did not use historic data to calibrate and
validate our model, we would be losing valuable information for
verifying our model, as well as finding problems within our model. For
instance, we may have introduced coding errors that would not have been
found without using historic data. It is for this reason that we chose
this approach.

#### 3.6.3 National model

Due to the high granularity of ElecSim on a technical and temporal
level, it becomes difficult to model electricity markets on a global
scale. Modelling a single run of the United Kingdom can take @xmath 16
hours, therefore modelling the rest of the world would take an
infeasible amount of time and computing power to function. We run these
on a Microsoft Azure, Dsv3-series virtual machine, which are made up of
Intel Xeon Platinum 8272CL (Cascade Lake), Intel Xeon 8171M 2.1GHz
(Skylake), Intel Xeon E5-2673 v4 2.3 GHz (Broadwell), or the Intel Xeon
E5-2673 v3 2.4 GHz (Haswell) processors in a hyper-threaded
configuration.

#### 3.6.4 Lack of spatial resolution

Within an electricity market, a component of space becomes important.
This is because electricity is not created in the locations that it is
consumed. For instance, offshore wind is typically clustered on banks
which are shallow enough to install wind turbines, whilst demand is
typically clustered on land in residential and urban areas. In addition
to this, different areas may have different demand profiles, for
instance with high income earners or industry, typically consuming more
than low-income areas.

By accounting for these variations in spatial resolution, one is able to
optimise the location of power plants, to minimise electricity transfer
losses. In addition, different distribution networks can be designed, to
account for differences in demand.

#### 3.6.5 Supply curves

As ElecSim is a national model, and the UK is a relatively small
country, we did not take into account the supply-demand curve for
resources such as natural gas, coal and other fossil fuels. This is due
to the fact that the fossil fuel market is much bigger than solely the
United Kingdom. In other words, an increase in demand in the UK would
not increase the price on a global scale.

We did not take into account limits of land usage for solar panels and
wind turbines in the model either. This is due to the fact that future
technologies may overcome such challenges. For example, currently,
offshore wind turbines must be placed in geographies with suitable
floors. However, with the advent of floating wind turbines, these
limitations could be reduced. Additionally, solar panels may become more
effective at being placed offshore.

#### 3.6.6 Model comparison

Whilst it is possible to compare multiple model outputs that look at a
horizon of 20+ years, it is not possible to know whether all or any of
these models are correct, or incorrect. Therefore, by comparing multiple
models, it may be the case that all of the models are wrong and that no
correct inference can be made. This is a significant challenge of long
term energy modelling. Other approaches include cross-validation, where
historic data is used to compare model outputs. However, whilst all
models are wrong, insights can be gained from the functioning of these
models to provide additional information that would not be available
without these quantitative models.

#### 3.6.7 A changing future

Whilst it is possible that a model can closely match a scenario, it is
not possible to know which scenario will occur in the future. Therefore,
whilst one is able to model many different scenarios, it is reasonable
to expect that none of these modelled scenarios actually occur. This
means that although a model accurately models the system it is designed
to replicate, none of the model outputs reflect that of real-life.

This is a significant challenge of scenario modelling, to which there is
no perfect solution. Increasing the number of scenarios and placing
likelihoods to each of these scenarios is one option, however, it is not
perfect.

#### 3.6.8 Subsidy modelling

In this work, we did not model subsidies such as the contracts for
difference auction, which is an auction found in the UK, where
generators are able to sell their electricity at a stable level,
irrespective of the daily price of electricity. This is because we
focused on marking ElecSim a generalisable model for different
countries. To ensure this, we had to focus on global parameters and
reduce complexity. We did, however, model a simple subsidy where an
extra price is paid for every MWh for Nuclear.

### 3.7 Conclusions

Liberalised electricity markets with many heterogeneous players are
suited to be modelled with ABMs. ABMs incorporate imperfect information
as well as heterogeneous actors. ElecSim models imperfect information
through forecasting of electricity demand and future fuel and
electricity prices. This leads to agents taking risk on their
investments, and model market conditions more realistically.

In this chapter, we have demonstrated that it is possible to use ABMs to
simulate liberalised electricity markets. Through validation, we are
able to show that our model, ElecSim, is able to accurately mimic the
observed, real-life scenario in the UK between 2013 and 2018. This
provides confidence in the underlying dynamics of ElecSim, especially as
we are able to model the fundamental transition between coal and natural
gas observed between 2013 and 2018 in the UK.

In addition to this, we were able to compare our long-term scenario to
that of the UK Government, Department for Business, Energy & Industrial
strategy. We show that we are able to mimic their reference scenario;
however, demonstrate a more realistic increase in nuclear power. The
parameters that were gained from optimisation show that the BEIS
scenario is realistic, however, a high nuclear subsidy may be required.

To improve the accuracy of our model, we used eight representative days
of solar irradiance, offshore and onshore wind speed and demand to
approximate an entire year. The particular days were chosen using a
@xmath -means clustering technique, and selecting the medoids. This
enabled us to accurately model the daily fluctuations of demand and
renewable energy resources.

In addition to this, a method of dealing with the non-validatable nature
of electricity markets, as per the definition of Hodges et al. is to
vary input parameters over many simulations and look for general trends
Hodges . This could be achieved using ElecSim through the analysis of a
reference case, and a limited set of scenarios which include the most
important uncertainties in the model structure, parameters, and data,
i.e. alternative scenarios which have both high plausibility and major
impacts on the outcomes.

Additionally, we showed a number of scenarios, and shows that total
demand has an effect on the electricity mix. An increasing demand,
year-on-year, can lead to an increase in solar to accommodate this
demand. However, if demand reduces, there is a higher investment in
nuclear, which contributes to the electricity mix by 2035.

We ran a sensitivity analysis of Weighted Average Cost of Capital ( WACC
)and down payment required. We showed that these two variables have a
large effect on the total electricity mix by the year 2035, which in
turn affects the total carbon emissions. We, therefore, show that the
input assumptions have an effect on the simulation, which must be
considered when analysing model outputs.

## Chapter 4 Electricity demand prediction

### Summary

In this chapter, we use several different machine learning and
statistical methods to predict electricity demand 30 minutes ahead as
well as a day ahead. By predicting electricity demand ahead, GenCos are
able to better schedule their power plants to dispatch to meet this
demand. The 30 minutes ahead methodology is used in the work on
day-ahead work. We utilise the errors from the day ahead predictions to
see what the impact of such errors are on the electricity market over
the long-term using ElecSim. The work looks specifically at the
difference in electricity mix with prediction error, as well as carbon
emitted. Whilst much of the literature investigates the short-term
benefits of using machine learning in making short-term forecasts, this
work goes further in that it integrates the short-term characteristics
of machine learning into the long-term model, ElecSim. This helps to
identify how the long-term market can be changed through algorithms with
differing accuracies. This directly relates to the aims of the thesis,
which is to look beyond the short-term impact of using machine learning,
but to look at the wider impacts on the long-term market. The work on
30-minute ahead forecasting was published in Kell2018a .

We introduce this work in Section 4.1 . Section 4.2 provides a
literature review on the topic of demand forecasting. We introduce the
methods used in Section 4.3 . Sections 4.4 and 4.5 look at 30-minute
ahead predictions and day-ahead predictions respectively. Additionally,
Section 4.5 integrates these day ahead projections into the ElecSim
model.

### 4.1 Introduction

The need for accurate load forecasting is essential for control and
planning of electricity generation in electrical grids due to the fact
that supply must meet demand Lu1993 . Short-term electricity demand
forecasting has become increasingly important due to the introduction of
competitive energy markets. Accurate estimates of demand are required so
that the correct amount of electricity is purchased on the wholesale
market Dillon1991 . Electricity is unique to other commodities in that
it must be either consumed the moment that it is generated or stored.
The difficulties in storing electricity arise from high installation and
maintenance costs, inefficiencies and low capacity Poonpun2008 . It is
therefore important to match demand to supply, and thus regulate
frequency. This is because, if there is a mismatch between supply and
demand, frequency will either increase or decrease. Failure to
accurately forecast electricity demand can lead to financial loss and/or
system-wide blackouts Hines2008 . We focus on electricity demand in this
work and not supply, solar or wind due to limited computing power and
time. However, the conclusions found here could be extended to these
timeseries.

The integration of higher proportions of intermittent renewable energy
sources (IRES) in the electricity grid will mean that the forecasting of
electricity demand will become increasingly important and challenging.
Examples of IRES are solar panels and wind turbines, which fluctuate in
terms of power output based on localised wind speed and solar
irradiance. However, as supply must meet demand at all times and the
fact that IRES are less predictable than dispatchable energy sources
such as coal and combined-cycle gas turbines (CCGTs), this means that
extra attention must be made in predicting future demand if we wish to
keep, or better reduce, the current frequency of blackouts Lu1993 . A
dispatchable source is one that can be turned on and off by human
control and therefore, able to adjust output just in time, at a moment
convenient for the grid.

Typically, peaker plants, such as reciprocal gas engines, are used to
fill fluctuations in demand that had not been previously planned for.
Specifically, peaker plants meet the peaks in demand where other cheaper
options are at full capacity. These peaker plants are typically
expensive to run and have higher greenhouse gas emissions than their
non-peaker counterparts Mahmood2014 . Whilst peaker plants are also
dispatchable plants; not all dispatchable plants are peaker plants.

Figure 4.1 shows the differences between baseload generation, peaking
resources and those in between. Peaker power plants will match peak
demand, as shown by the Figure, whereas baseload generation is run at
all times. This includes some gas power plants and nuclear plants, which
have a low short run marginal cost. It is unlikely that a nuclear power
plant will be used as a peaker plant due to its inflexibility and slow
ramp up and ramp down times, and low short run marginal cost.

Figure 4.2 displays an example load profile with different types of
electricity generation. As can be seen, the electricity generation types
with the lowest marginal costs, such as nuclear and water power are
dispatched first. Next, natural gas is dispatched which has a lower cost
than coal and pump power, and finally the top power refers to the
expensive peaker power plants, which are used to match peak demand only
due to their high costs.

To reduce reliance on peaker plants, it is helpful to know how much
electricity demand there will be in the future so that more efficient
plants can be used to meet this expected demand. This is so that these
more efficient plants can be brought up to speed at a time suitable to
match the demand. Forecasting a day into the future is especially useful
in decentralised electricity markets which have day-ahead markets.
Decentralised electricity markets are ones where electricity is provided
by multiple generation companies, as opposed to a centralised source,
such as a government. To aid in this prediction, machine learning and
statistical techniques have been used to accurately predict demand based
on several different factors and data sources Kell2018a , such as
weather Hong2014 , day of the week Al-Musaylh2018 and holidays
Vrablecova2017 .

The introduction of smart meters in many countries (USA, Europe, Canada
and South Korea) has led to an influx of high granularity electricity
consumption data that can be used for load forecasting Depuru2011a .
Smart meters are digital devices that measure the electricity
consumption of individual households at regular intervals (intervals of
an hour or less) and offer two-way communication between the meter and
utility company. Smart meters aid customers to understand precisely how
much electricity they consume at different time intervals, and enable
dynamic pricing Abreu2012a . Dynamic pricing allows utilities to charge
varying prices at different times, for instance, charging a higher price
when costly generation sources are used in times of peak demand, and
lower prices at night time or weekends when demand is low Ito2013 ;
Liu2016 . In this chapter, we forecast both 30-minutes ahead using smart
meter data, as well as 24-hours ahead to simulate the process made for a
day-ahead market.

Firstly, we explore short term load-forecasting at an interval of 30
minutes ahead and cluster similar users based on their electricity
usage. A variety of different forecasting techniques were evaluated such
as Random Forests ho1995random , Long-Short Term Memory neural networks
(LSTM) Hochreiter1997 , Multilayer Perceptron neural networks
book:984557 and Support Vector Regression ( SVR ) Drucker1997 .

Random Forests are an ensemble-based learning method for classification
and regression, and are made up of many decision trees. LSTM networks
are recurrent neural networks which remember values over arbitrary time
intervals. Multilayer Perceptrons are a popular type of neural network
which are made up of a minimum of three layers and can be used to make
non-linear predictions. SVR s are supervised learning models which
analyse data used for regression analysis.

To improve forecasting results, the clustering of smart meter data was
evaluated to identify similar customers, which may have a more similar,
and therefore easier to predict, profile. The technique used for this
was k -means clustering. An average 24-hour electricity load profile was
calculated, and the result used for clustering. The clustered sub-system
is then aggregated and separate models trained on this aggregate. The
yearly, weekly and daily periodicity of electricity load is accounted
for by input variables into the models. Once forecasts for each cluster
are made using the individual models, the results are aggregated for the
final predictions. These predictions are compared to the actual results,
and the accuracy measured using mean absolute percentage error (MAPE).

Whilst the work carried out for short-term forecasting does not relate
directly to the long-term characteristics of ElecSim and the work
presented previously, we used the methodology to inform the long-term
characteristics of the electricity market. This work therefore provided
the foundation for looking at the long-term using ElecSim.

Therefore, secondly, we introduce day-ahead forecasting and observe the
impact errors have on the long-term dynamics of the market. Various
studies have looked at predicting electricity demand at various horizons
Andersen2013 ; Huang2003 ; Singh2012 . However, the impact of poor
demand predictions on the long-term electricity mix has been studied to
a lesser degree.

We compare several machine learning and statistical techniques to
predict the energy demand for each hour over the next 24-hour horizon.
We chose to predict over the next 24 hours to simulate a day-ahead
market, which is often seen in decentralised electricity markets.
However, our approach could be utilised for differing time horizons. In
addition to this, we use our long-term agent-based model, ElecSim Kell ;
Kell2020 , to simulate the impact of different forecasting methods on
long-term investments, power plant usage and carbon emissions for the
years 2018 through 2035 in the United Kingdom. Our approach, however, is
generalisable to any country through parametrisation of the ElecSim
model.

As part of our work, we utilise online learning methods to improve the
accuracy of our predictions. Online learning methods can learn from
novel data while maintaining what was learnt from previous data. Online
learning is useful for non-stationary datasets and time-series data
where recalculation of a model would take a prohibitive amount of time.
Offline learning methods, however, must be retrained every time new data
is added. Online approaches are constantly updated and do not require
significant pauses while the offline training is being re-run. By
training, in an offline manner, on data that has already been used for
training, the computational load and time required increases.

We trial different algorithms and train different models for different
times of the year. Specifically, we train different models for the
different seasons. We also split weekdays and train both weekends and
holidays together. This is due to the fact that holidays and weekend
exhibit similar load profiles due to the reduction in industry
electricity use and an increase in domestic. This enables a model to
become good at a specific subset of the data which share similar
patterns, as opposed to having to generalise to all of the data.
Examples of the algorithms used are linear regression, lasso regression,
random forests, support vector regression, multilayer perceptron neural
network, box-cox transformation linear regression and the passive
aggressive model.

We expect a-priori that online algorithms will outperform the offline
approach. This is due to the fact that the demand time-series is
non-stationary, and thus changes sufficiently over time. In terms of the
models, we presume that the machine learning algorithms, such as neural
networks, support vector regression and random forests will outperform
the statistical methods such as linear regression, lasso regression and
box-cox transformation regression. We expect this due to the fact that
machine learning has been shown to be able to learn more complex feature
representations than statistical methods Singh2012 .

However, it should be noted, that such a-priori intuition, is no
substitute for analytical evidence and can (and has) been shown to be
wrong in the past, due to imperfect knowledge of the data and
understanding of some of the black box models, such as neural networks.

Using online and offline methods, we take the error distributions, or
residuals, and fit a variety of distributions to these residuals. We
choose the distribution with the lowest sum of squared estimate of
errors (SSE). SSE was chosen as the metric to ensure that both positive
and negative errors were treated equally, as well as ensuring that large
errors were penalised more than smaller errors. We fit over 80 different
distributions, which include the Johnson Bounded distribution, the
uniform distribution and the gamma distribution. The distribution that
best fits the respective residuals is then used and sampled from to
adjust the demand in the ElecSim model. We then observe the differences
in carbon emissions, and which types of power plants were both invested
in and utilised, with each of the different statistical and machine
learning methods. To the best of our knowledge, this is the most
comprehensive evaluation of online learning techniques to the
application of day-ahead load forecasting as well as assessing the
impacts of the errors that these models produce on the long-term
electricity market dynamics.

We show that online learning has a significant impact on reducing the
error for predicting electricity consumption a day ahead when compared
to traditional offline learning techniques, such as multilayer
artificial neural networks, linear regression, extra trees regression
and support vector regression, which are models used in the literature
Ahmad2017 ; Chen2004 ; Lu1993 . For a full list of algorithms used in
this thesis see Table 4.4 .

We show that the forecasting algorithm has a non-negligible impact on
carbon emissions and use of coal, onshore, photovoltaics, reciprocal gas
engines and CCGT. Specifically, the amount of coal, photovoltaics, and
reciprocal gas used from 2018 to 2035 was proportional to the median
absolute error, while both onshore and offshore wind are inversely
proportional to the median absolute error.

Total investments in coal, offshore and photovoltaics are proportional
to the median absolute error, while investments in CCGT, onshore and
reciprocal gas engines are inversely proportional.

The contributions of this work are:

1.  A methodology to forecast smart-meter electricity demand using the k
    -means clustering technique.

2.  The evaluation of different online and offline learning models to
    forecast the electricity demand profile 24 hours ahead.

3.  Evaluation of poor predictive ability on the long-term electricity
    market in the UK through the perturbation of demand in the ElecSim
    simulation.

### 4.2 Literature review

In this Section, we carry out a literature review on 30-minute ahead
forecasting, day-ahead forecasting and online forecasting methods. In
addition, we cover the literature on the impact of forecasting on
electricity markets.

#### 4.2.1 30-minute ahead forecasting

The forecasting of aggregated and clustered electricity demand has been
the focus of a considerable amount of research in recent years. The
research can generally be classified into two classes, Artificial
Intelligence (AI) methods Kim2000 ; Tiong2008 ; Quilumba2014 and
classical time series approaches Huang2003 ; Nguyen2017 . We investigate
both approaches in this chapter.

Singh et al. Singh2012 produced a review of load forecasting techniques
and methodologies and reported that hybrid methods, which combine two or
more different techniques, are gaining traction, as well as soft
computing approaches (AI) such as genetic algorithms. Our work presents
a hybrid method which combines k -means clustering with multiple
different learning algorithms.

##### Artificial Intelligence Methods

Dillon et al. Dillon1991 presented a neural network for short term load
forecasting. Their neural network consisted of three-layers and used
adaptive learning for training. They proposed the use of weather
information to augment their electricity load data. They found better
results with the adaptive neural network than with a linear model, or
non-adaptive neural network. In contrast to Dillon our work focuses on a
non-adaptive neural network and does not take into account weather
information.

Chen et al. Chen1996 used an Artificial neural network to predict
electricity demand of three substations in Taiwan. They integrated
temperature data and reported that the best results when forecasting
residential and commercial substations were during the week due to the
influence of weather. In contrast to the work done by Chen et al. , we
focus on client-side prediction using smart meter data as opposed to
substation data. We were, therefore, able to cluster the data based on
load profile, as opposed to geographical location.

##### Time Series Methods

Al-Musaylh et al. Al-Musaylh2018 proposed the use of Support Vector
Regression ( SVR ), an Auto Regressive Integrated Moving Average ( ARIMA
)model and a Multivariate Adaptive Regression Spline ( MARS )in their
short term electricity demand forecasting system. They found that for a
half, and one-hour forecasting horizons, the MARS model outperformed
both the ARIMA and SVR.

Taylor Taylor2008 evaluates different statistical methods including
ARIMA, an adaptation of Holt-Winters’ exponential smoothing Holt2004 ,
and an exponential smoothing method which focuses on the evolution of
the intra-day cycle. He found that the double seasonal adaptation of the
Holt-Winters’ exponential smoothing method was the most accurate method
for short lead times between 10 and 30 minutes.

In contrast to Taylor, Fard et al. Fard2014 proposed a novel hybrid
forecasting method based on both artificial intelligence and classical
time series approaches. They utilised the wavelet transform, ARIMA and
ANNs for short term load forecasting. The ARIMA model is created by
finding the appropriate order using the Akaike information criterion
Akaike1974 . The ARIMA model models the linear component of the load
time series, and the residuals contain the non-linear components. These
residuals are then decomposed by the discrete wavelet transform into its
sub-frequencies. ANNs are then applied to these sub-frequencies, and the
outputs of both the Artificial Neural Network ( ANN )and ARIMA models
are summed to make the final prediction. They found that this hybrid
technique outperformed traditional methods. Our work does not integrate
artificial intelligence and classical time series techniques, due to
time constraints.

##### Clustering

Multiple techniques have been proposed for the clustering of electricity
load data prior to forecasting. Both Shu and Luonan Shu2006 , and Nagi
et al Tiong2008 propose a hybrid approach in which self-organising maps
are used to cluster the data, and Support Vector Regression is used to
make predictions. This technique proved robust for different data types,
and was able to tackle the non-stationarity of the data. Shu showed that
this hybrid approach outperformed a single SVR technique, whilst Nagi
showed superior results to a traditional ANN system. In contrast to both
Nagi et al. and Shu and Luonan our work utilises k -means as the
clustering algorithm.

Quilumba et al. Fard2014 also apply machine learning techniques to
individual households’ electricity consumption by aggregation. To
achieve this aggregation, they use k -means clustering to aggregate the
households to improve their forecasting ability. The authors also use a
neural network-based model for forecasting, and show that the number of
optimum clusters for forecasting is dependent on the data, with three
clusters optimal for the Con Edison Launches Smart Grid Pilot Program in
Queens queens_dataset , and four for the Commission for Energy
Regulation (CER) dataset CER_DATASET .

Wijaya et al. Wijaya2010 demonstrated that implementing clusters
improved load-forecasting accuracy up to a certain level. Whilst, a
study by Ilić et al. Ilic2013 showed that increasing the number of
clusters did not improve accuracy.

Humeau et al. Humeau2013 compare MLPs, SVRs and linear regression at
predicting smart meter data. They aggregate different households and
observe which models work the best at each aggregate level. They find
that linear regression outperforms both MLP and SVR when forecasting
individual households. However, after aggregating over 32 households,
SVR outperforms linear regression.

#### 4.2.2 Online learning

Whilst multiple papers have looked at demand-side forecasting, as shown
by the review paper by Singh et al. Singh2012 , to the best of our
knowledge, the impact of online learning has been discussed with less
frequency. In addition to this, our research models the impact of the
performance of different algorithms on investments made, electricity
sources dispatched and carbon emissions over a 17 year period. To model
this, we use the model ElecSim. In our work, we trial a set of different
algorithms to our problem. Due to time and compute constraints, we do
not trial the additional techniques discussed in this literature review
within this thesis.

Johansson et al . Johansson2017 apply online machine learning algorithms
for heat demand forecasting. They find that their demand predictions
display robust behaviour within acceptable error margins. Finding that
artificial neural networks (ANNs) provide the best forecasting ability
of the standard algorithms and can handle data outside of the training
set. Johansson et al. , however, do not look at the long-term effects of
different algorithms on their application.

Baram et al . Baram2003 combine an ensemble of active learners by
developing an active-learning master algorithm. To achieve this, they
propose a simple maximum entropy criterion that provides effective
estimates in realistic settings. Their active-learning master algorithm
is empirically shown to, in some cases, outperform the best algorithm in
the ensemble on a range of classification problems.

Schmitt et al . Schmitt2008 also extends on existing algorithms through
an extension of the FLORA algorithm Widmer1996 . The FLORA algorithm
generates a rule-based model, which has the ability to make binary
decisions. Their FLORA-MC enhances the FLORA algorithm for
multi-classification and numerical input values. They use this algorithm
for an ambient computing application. Ambient computing is where
computing and communication merges into everyday life. They find that
their model outperforms traditional offline learners by orders of
magnitude.

Similarly to our work, Pindoriya et al . Pindoriya2008 trial several
different machine learning methods such as adaptive wavelet neural
network (AWNN). They find that AWNN has good prediction properties when
compared to other forecasting techniques such as wavelet-ARIMA,
multilayer perceptron (MLP) and Radial Basis Function ( RBF )neural
networks as well as the fuzzy neural network (FNN).

Goncalves Da Silva et al . GoncalvesDaSilva2014 show the effect of
prediction accuracy on local electricity markets. To this end, they
compare forecasting of groups of consumers in comparison to single
individuals. They trial the use of the Seasonal-Naïve and Holt-Winters
algorithms and look at the effect that the errors have on trading in an
intra-day electricity market of consumers and prosumers. They found that
with a photovoltaic penetration of 50%, over 10% of the total generation
capacity was uncapitalised and roughly 10, 25 and 28% of the total
traded volume were unnecessary buys, demand imbalances and unnecessary
sells respectively. This represents energy that the participant has no
control. Uncapitalised generation capacity is where a participant could
have produced energy; however, it was not sold on the market.
Additionally, due to forecast errors, the participant might have sold
less than it should have. Our work, however, focuses on a national
electricity market, as opposed to a local market.

### 4.3 Methods

In this section, we explore the principles behind the methods used in
this chapter.

#### 4.3.1 Error Metrics

##### Mean Absolute Percentage Error

The Mean Absolute Percentage Error ( MAPE )is a measure of prediction
accuracy which is used in this thesis. It can be defined as follows:

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

where @xmath is the actual value, @xmath is the forecast value and
@xmath is the number of points forecast Li2016 .

##### Root Mean Squared Error

The root mean squared error (RMSE) is a measure between the values
predicted by a model and the observed values. The RMSE is the sample
standard deviation of the differences between the predicted and observed
values.

The RMSE is defined as follows:

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

where @xmath are the predicted values, @xmath are the observed values,
and @xmath is the number of observations.

##### Mean Absolute Scaled Error

The mean absolute scaled error (MASE) is a measure of the accuracy of
forecasts Hyndman2006 . It is defined as the mean absolute error of the
forecast values, divided by the mean absolute error of the in-sample
one-step naive forecast. MASE can be scaled across different scales, has
symmetry for both positive and negative errors, is interpretable and has
predictable behaviour for a value of zero.

MASE can be defined as follows:

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

where @xmath is the forecast error for a given period, @xmath is the
number of forecasts. Where @xmath is defined as the actual value (
@xmath ) minus the forecast value ( @xmath ) for that period. The
denominator is the mean absolute error of the one-step naive forecast
method on the training set. This naive forecast is the actual value from
the prior period, or @xmath . @xmath is the total number of forecasts.

#### 4.3.2 Machine learning

Machine learning is a methodology for finding and describing structural
patterns in data Witten2011 . Offline learning models are trained with
the data available at a single point in time. With non-stationary data –
where underlying distributions change – the model must be retrained at
periodic intervals, determined by how quickly the model goes out of step
with the true data. With online learning, the model is able to retrain
every time a new data point becomes available, without having to retrain
the entire model. This makes these models good for time-series data
which exhibit moderate to significant non-stationary properties, such as
electricity demand profiles.

#### 4.3.3 Online learning

Examples of online learning algorithms are Passive Aggressive (PA)
Regressor Gzik2014 , Linear Regression, Box-Cox Regressor Box1964 ,
K-Neighbors Regressor forgy65 and Multilayer perceptron regressor
Hinton1989 . For our work, we trial the stated algorithms, in addition
to a host of offline learning techniques. The offline techniques
trialled were Lasso regression Tibshirani1996a , ridge regression
GeladiPaul1994Mrac , Elastic Net Geostatistics2010 , Least Angle
Regression Fike1988 , Extra Trees Regressor Fike1988 , Random Forest
Regressor Breiman2001 , AdaBoost Regressor Freund1997 , Gradient
Boosting Regressor 316 and Support vector regression Cortes1995 . We
chose the boosting and random forest techniques due to our previous
successes of these algorithms when applied to electricity demand
forecasting Kell2018a . We trialled the additional algorithms due to the
availability of these algorithms using the scikit-learn package and
online learning package, Creme creme ; scikit-learn .

#### 4.3.4 Linear regression models

Linear regression is an approach to modelling linear relationships
between a dependent variable and one or more independent variables.
Linear regressions can be used for both online and offline learning. In
this thesis, we use them for both online and offline learning. Linear
regression models are often fitted using the least-squares approach. The
least-squares approach minimises the sum of the squares of the
residuals.

Other methods for fitting linear regressions are by minimising a
penalised version of the least-squares cost function, such as in ridge
and lasso regression GeladiPaul1994Mrac ; Tibshirani1996a . Ridge
regression is a useful approach for mitigating the problem of
multicollinearity in linear regression. Multicollinearity is where one
predictor variable can be linearly predicted from the others with a high
degree of accuracy. This phenomenon often occurs in models with a large
number of parameters.

In ridge regression, the OLS loss function is augmented so that we not
only minimize the sum of squared residuals but also penalized the size
of parameter estimates, in order to shrink them towards zero:

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

we solve for @xmath , where @xmath are the OLS parameter estimates.
@xmath is a vector. @xmath is the number of observations of the response
variable, @xmath , Where @xmath . With a linear combination of @xmath
predictor variables, @xmath , where @xmath . @xmath is the
regularisation penalty which can be chosen through cross-validation, or
the value that minimises the cross-validated sum of squared residuals,
for instance.

Lasso is a linear regression technique which performs both variable
selection and regularisation. It is a type of regression that uses
shrinkage. Shrinkage is where data values are shrunk towards a central
point, such as the mean. The lasso model encourages models with fewer
parameters. This enables the selection of models with fewer parameters,
or automate the process of variable selection.

Under lasso the loss is defined as:

  -- -------- -- -------
     @xmath      (4.5)
  -- -------- -- -------

The only difference between lasso and ridge regression is the penalty
term. Elastic net is a regularization regression that linearly combines
the penalties of the lasso and ridge methods. Specifically, Elastic Net
aims to minimize the following loss function:

  -- -------- -- -------
     @xmath      (4.6)
  -- -------- -- -------

where @xmath is the mixing parameter between ridge ( @xmath ) and lasso
( @xmath ). The two parameters @xmath and @xmath can be tuned. Least
Angle Regression (LARS) provides a means of producing an estimate of
which variables to include in a linear regression, as well as their
coefficients.

#### 4.3.5 Decision tree-based algorithms

The decision tree is a model which goes from observations to output
using simple decision rules inferred from data features Quinlan . To
build a decision tree, recursive binary splitting is used on the
training data. Recursive binary splitting is a greedy top-down algorithm
used to minimise the residual sum of squares. The RSS, in the case of a
partitioned feature space with @xmath partitions, is given by:

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

Where @xmath is the value to be predicted, @xmath is the predicted value
for partition @xmath .

Beginning at the top of the tree, a split is done into two branches.
This split is carried out multiple times and the split is chosen that
minimises the current RSS. To obtain the best sequence of subtrees cost
complexity, pruning is used as a function of @xmath . @xmath is a tuning
parameter that balances the depth of the tree and the fit to the
training data. This parameter is used to see how deep the tree should
be. This parameter can be tuned using cross-validation.

The AdaBoost training process selects only the features of a model known
to improve the predictive power of the model Freund1997 . By doing this,
the dimensionality of the model is reduced and can improve compute time.
This can be used in conjunction with multiple different models. In this
thesis, we utilise the decision tree-based algorithm with AdaBoost.

Random Forests are an ensemble learning method for classification and
regression Breiman2001 . Ensemble learning methods use multiple learning
algorithms to obtain better predictive performance. They work by
constructing multiple decision trees at training time, and outputting
the predicted value that is the mode of the predictions of the
individual trees.

To ensure that the individual decision trees within a Random Forest are
not correlated, bagging is used to sample from the data. Bagging is the
process of randomly sampling with replacement of the training set and
fitting the trees. This has the benefit of reducing the variance of the
model without increasing the bias.

Random Forests differ in one way from this bagging procedure. Namely,
using a modified tree learning algorithm that selects, at each candidate
split in the learning process, a random subset of the features, known as
feature bagging. Feature bagging is undertaken due to the fact that some
predictors with a high predictive ability may be selected many times by
the individual trees, leading to a highly correlated Random Forest.

ExtraTrees adds one further step of randomisation Fike1988 . ExtraTrees
stands for extremely randomised trees. There are two main differences
between ExtraTrees and Random Forests. Namely, each tree is trained
using the whole learning sample (and not a bootstrap sample), and the
top-down splitting in the tree learner is randomised. That is, instead
of computing an optimal cut-point for each feature, a random cut-point
is selected from a uniform distribution. The split that yields the
highest score is then chosen to split the node.

#### 4.3.6 Gradient Boosting

Gradient boosting is a machine learning technique which is used for both
regression and classification problems 316 . Typically, decision trees
are used to produce a prediction model in the form of an ensemble of
weak prediction models. The model is built in a stage-wise fashion.
These model types are generalisable by allowing for an optimisation of
an arbitrary differentiable loss function.

The outline of the algorithm is defined here. As discussed, in many
supervised learning problems, one has an output variable @xmath and a
vector of input variables @xmath . These two variables are described via
a joint probability distribution @xmath . The goal of supervised
learning is to use a training set @xmath of known @xmath and @xmath
values to find an approximation of @xmath , @xmath . @xmath is found by
minimizing the expected value of some specified loss function @xmath :

  -- -------- -- -------
     @xmath      (4.8)
  -- -------- -- -------

The gradient boosting method assumes a real-valued @xmath and seeks an
approximation @xmath , where @xmath is a weighted sum of function @xmath
from some class @xmath . Where @xmath are called weak leaners:

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

This method tries to find an approximation @xmath that minimizes the
average value of the loss function on the training set. It does so, by
starting with a constant function model. @xmath and incrementally
expands it in a greedy fashion:

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

where @xmath is a weak learner function. Due to the fact that choosing
the best function @xmath is computationally infeasible, an approximation
must be sought. This approximation is found using functional gradient
descent to this minimization problem. If we consider the continuous
case, i.e. where @xmath is the set of arbitrary differentiable functions
on @xmath , we would update the model in accordance with the following
equations:

  -- -------- -- --------
     @xmath      (4.12)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (4.13)
  -- -------- -- --------

where the derivatives are taken with respect to the function @xmath for
@xmath and @xmath is the step length.

#### 4.3.7 Support vector regression

Support vector regression is an algorithm which finds a hyperplane and
decision boundary to map an input domain to an output Cortes1995 . The
hyperplane is chosen by minimising the error within a certain tolerance.

Suppose we have the training set: @xmath , where @xmath is the input,
and @xmath is the output value of @xmath . Support Vector Regression
solves an optimization problem Chen2004 ; Shu2006 , under given
parameters @xmath and @xmath , the form of support vector regression is
Drucker1997 :

  -- -------- -- --------
     @xmath      (4.14)
  -- -------- -- --------

subject to:

  -- -- -- --------
           (4.15)
  -- -- -- --------

@xmath is mapped to a higher dimensional space using the function @xmath
. The @xmath -insensitive tube @xmath is a range in which errors are
permitted. @xmath and @xmath are slack variables which allow errors for
data points which fall outside of @xmath . This enables the optimisation
to take into account the fact that data does not always fall within the
@xmath range Smola2004 .

The constant @xmath determines the trade-off between the flatness of the
support vector function. @xmath is the model fit by the SVR. The
parameters which control regression quality are the cost of error @xmath
, the width of the tube @xmath , and the mapping function @xmath
Chen2004 ; Shu2006 .

#### 4.3.8 k-Neighbours Regressor

k -Neighbours regression is a non-parametric method used for regression
forgy65 . One such k -NN algorithm uses a weighted average of the k
nearest neighbours, weighted by the inverse of their distance.

The algorithm works as follows:

1.  Compute the Euclidean or Mahalanobis distance from the test example
    to the labelled examples.

2.  Order the labelled examples by increasing distance.

3.  Find a heuristically optimal number k of nearest neighbours, based
    on cross-validation.

4.  Calculate an inverse distance weighted average with the k -nearest
    multivariate neighbours.

#### 4.3.9 Neural Networks

A neural network can be used in both offline and online cases. In this
thesis, we used them for both online and offline.

Artificial Neural Networks are a model which can model non-linear
relationships between input and output data Akaike1974 . A popular
neural network is a feed-forward multilayer perceptron. Fig. 4.3 shows a
three-layer feed-forward neural network with a single output unit, k
hidden units, @xmath input units. @xmath is the connection weight from
the @xmath input unit to the @xmath hidden unit, and @xmath is the
connecting weight from the @xmath hidden unit to the output unit Pao2007
. These weights transform the input variables in the first layer to the
output variable in the final layer using the training data.

For a univariate time series forecasting problem, suppose we have N
observations @xmath in the training set, and @xmath observations in the
test set, @xmath , and we are required to predict m periods ahead
Pao2007 .

The training patterns are as follows:

  -- -------- -------- -- --------
     @xmath   @xmath      (4.16)
     @xmath   @xmath      (4.17)
     @xmath   @xmath      (4.19)
  -- -------- -------- -- --------

where @xmath represents the MLP network and @xmath are the weights. For
brevity we omit @xmath . The training patterns use previous time-series
points, for example, @xmath as the time series is univariate. Where
@xmath is the total number of points in the univariate time-series. That
is, we only have the time series in which we can draw inferences from.
In addition, these time series points are correlated, and therefore
provide information that can be used to predict the next time point.

The @xmath testing patterns are:

  -- -------- -------- -- --------
     @xmath   @xmath      (4.20)
     @xmath   @xmath      (4.21)
     @xmath   @xmath      (4.23)
  -- -------- -------- -- --------

The training objective is to minimise the overall predictive mean sum of
squared estimate of errors (SSE) by adjusting the connection weights.
For this network structure the SSE can be written as:

  -- -------- -- --------
     @xmath      (4.24)
  -- -------- -- --------

where @xmath is the prediction from the network. The number of input
nodes corresponds to the number of lagged observations. Having too few
or too many input nodes can affect the predictive ability of the neural
network Pao2007 .

It is also possible to vary the hyperparameter or the number of input
units. Typically, various different configurations of units are
trialled, with the best configuration being used in production. The
weights @xmath in @xmath are trained using a process called
backpropagation rumelhart1986learning , which uses labelled data and
gradient descent to update and optimise the weights.

#### 4.3.10 Long-Short Term Neural Networks

Long Short Term Memory Neural Network ( LSTM ) Hochreiter1997 is an
artificial recurrent neural network (RNN) architecture, which is used in
the field of deep learning. They are able to process entire sequences of
time-series data. A common LSTM unit is composed of a cell, an input
gate, an output gate and a forget gate. The cell is able to remember
values over arbitrary time intervals and the three gates regulate the
flow of information into and out of the cell.

LSTMs are well suited to time-series data, due to the presence of lags
between important events, with an unknown duration. LSTMs have an
advantage over traditional RNNs by solving the vanishing gradient
problem. This is where the gradients which are back-propagated can tend
to zero or infinity. This is due to the computations involved in the
process, which use finite-precision numbers. LSTMs partially solve this
problem by allowing gradients to flow unchanged in the back-propagation
process.

#### 4.3.11 Online Algorithms

In this section we discuss the algorithms which were used exclusively
for online learning in this thesis.

#### 4.3.12 Box-Cox regressor

In this subsection, we discuss the Box-Cox regressor. Ordinary least
squares is a method for estimating the unknown parameters in a linear
regression model. It estimates these unknown parameters by the principle
of least squares. Specifically, it minimises the sum of the squares of
the differences between the observed variables and those predicted by
the linear function.

The ordinary least squares regression assumes a normal distribution of
residuals. However, when this is not the case, the Box-Cox Regression
may be useful Box1964 . It transforms the dependent variable using the
Box-Cox Transformation function and employs maximum likelihood
estimation to determine the optimal level of the power parameter lambda.
The Box-Cox Regression requires that no dependent variable has any
negative values.

Variable selection and ordinary least squares output dialogues are
identical to that of linear regression.

The Box-Cox regression will transform the dependent variable as follows:

  -- -------- -- --------
     @xmath      (4.25)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (4.26)
  -- -------- -- --------

Where @xmath is the power parameter, and the data vectors are @xmath .
The optimal value of ( @xmath ) is determined by maximising the
following log-likelihood function:

  -- -------- -- --------
     @xmath      (4.27)
  -- -------- -- --------

where @xmath is the estimate of the least squares variance using the
transformed y variable.

#### 4.3.13 Passive-Aggressive regressor

The goal of the Passive-Aggressive (PA) algorithm is to change itself as
little as possible to correct for any mistakes and low-confidence
predictions it encounters Gzik2014 . Specifically, with each example PA
solves the following optimisation Ma2009 :

  -- -------- -- --------
                 (4.28)
     @xmath      (4.29)
  -- -------- -- --------

Where @xmath is the input data and @xmath the output data, and @xmath
are the weights for the passive aggressive algorithm. Updates occur when
the inner product does not exceed a fixed confidence margin - i.e.,
@xmath . The closed-form update for all examples is as follows:

  -- -------- -- --------
     @xmath      (4.30)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (4.31)
  -- -------- -- --------

@xmath is derived from a derivation process which uses the Lagrange
multiplier. For full details of the derivation see Gzik2014 .

### 4.4 Short-term demand forecasting

In this section we present the work undertaken for short-term demand
forecasting. We forecast 30-minutes ahead using smart meter data.

#### 4.4.1 Methodology

##### The Data

Smart meter data obtained from the Irish Social Science Data Archive
(ISSDA) on the 28th of September 2017 was used in this study cer_2012 .
The Commission for Energy Regulation released a public dataset of
anonymised smart meter data from the " Electricity Smart Metering
Customer Behaviour Trials " cer_2012 . This dataset is made up of over
5,000 Irish homes and businesses and is sampled at 30-minute intervals.

The data was recorded between the 14th July 2009 and 31st December 2010,
providing 17 months worth of data. For the purposes of cross-validation,
the data was split into two partitions, the training set and the testing
set. The training set made up the first 11 months of data and was used
to parametrise the models, whereas the test set is made up of the
remaining six months of data. This split was chosen to balance the
amount of training data with the test data and to give the models a
chance to learn the periodicity inherent in a twelve month period of
electricity load. The test set was used for evaluation of the models
proposed. Due to the long training times for these algorithms, we worked
with a sub-sample of 709 individual Irish homes from the whole dataset.
However, we believe that our results would hold over the full dataset.

Figure 4.4 demonstrates the electricity consumption profile of a single
week for a single user. Whilst it can be seen that electricity usage
changes significantly between days, a pattern of behaviour is exhibited.
There is a large peak displayed each day in the evening, as well as a
peak earlier during the day. It can, therefore, be assumed that this
customer has some form of habitual behavioural pattern.

Figure 4.5 shows eight different residential customer load profiles on
the 22nd June 2009. It can be seen that the daily load profile changes
between each customer. The consumers use varying quantities of
electricity and at different times.

These figures display that electricity consumption changes per person
per day. To capture this variability between customer types, these
customers are clustered and then aggregated. Each of the different
aggregated electricity consumptions should provide a less stochastic
load profile, and therefore increase the accuracy of the models.

##### Clustering

We propose that clustering similar customer load profiles and
aggregating each cluster’s electricity consumption improves the accuracy
of the models.

Figure 4.6 displays four different customers with similar load profiles.
Each of the users display a strong peak in electricity consumption
during the evening and less consumption during the day. These customers
may potentially be clustered together by the k -means clustering
algorithm.

To cluster the load profiles, different options were considered.
Hierarchical clustering using metrics such as Euclidean and wavelet
distance metrics were evaluated BIMJ:BIMJ4710240520 , as was k -means
forgy65 . K -means was demonstrated to be the most robust and
best-performing clustering algorithm, and thus was chosen for use in
this thesis.

To select the optimum number of clusters ( k ) cross-validation was
explored. This allowed us to compare the results of each of the models
and select k with the highest MAPE accuracy.

The cross-validation method proposed, worked by trying a different
number of clusters per model, and testing for the resulting MAPE. The
optimum number of clusters with a low MAPE is then chosen. In this
thesis we varied k between 1 and 7; this range was chosen due to the
fact that the error did not vary greatly past seven clusters. We fit
multiple models per cluster and predicted six months of electricity
consumption.

With k -means clustering, it is possible that with the same
initialisation number of clusters, different clusters are formed. This
is due to the algorithm converging at a local minima. To overcome local
minima, the k -means algorithm is run multiple times, and the partition
with the smallest squared error is chosen Jain2010 . In our case, the k
-means clustering algorithm is run 1,000 times to reduce the chance of
finding a local minima.

The clustering technique utilised in our work was a scaled input
approach. The daily load profile was averaged for each customer based on
each day of the training data. The data was then scaled so that
households of different sizes, but with similar usage profiles were
clustered together. This data, which is made up of a m-by-n matrix,
where m is equal to the total number of meters and n is equal to 48 (two
readings for each hour in the day).

To find the optimum number of clusters it is recommended that the user
selects a value of @xmath that is high enough that distinct average load
profiles are displayed, however, not so high that well-clustered
customers are split. By doing this, the stochasticity of the load
profiles in each of the clusters will be reduced, and thus lead to the
best results.

##### Aggregating Demand

Once each customer is assigned to their respective cluster using k
-means clustering, the total electricity consumed per cluster is
aggregated. This is achieved by summing the electricity consumed at each
time interval per cluster. This creates a partial system load. A
different model is trained on each of the different partial system
loads, and the resultant forecasts are aggregated to generate the total
system load forecast. The total system load forecast is then used to
evaluate the accuracy of each of the different models using MAPE.

Random Forests, Support Vector Regression, Multilayer Perceptron neural
networks and Long-Short Term Memory neural networks were evaluated, and
a comparison between the different models were made.

These models were chosen due to their ability to model multivariate
non-linear relationships. They are data-driven methods and therefore
suited to this type of problem.

##### Feature Selection

Each component of the training data is known as a feature. Features
encode information from the data that may be useful in predicting
electricity consumption.

##### Calendar Attributes

Due to the daily, weekly and annual periodicity of the electricity
consumption daily calendar attributes may be useful to model the
problem. The calendar attributes included are as follows:

-   Hour of day

-   Day of the month

-   Day of the week

-   Month

-   Public holidays

These attributes enable the daily, weekly and annual periodicity to be
taken into account by the model. We chose these due to the daily,
monthly and weekly variations for electricity demand. It is also
observed that electricity demand is changed on a public holiday.

It is noted that electricity consumption changes on a public holiday
such as Christmas or New Year’s Eve. For example, a hotel may consume
more electricity and an office space less. It is therefore proposed that
public holidays in Ireland are input into the model as features.

For testing purposes, two sets of models for Random Forests, Multilayer
Perceptrons and Support Vector Regression were fit. One set omitted
these calendar attributes whilst the other didn’t. This is done to
evaluate the importance of periodicity in electricity consumption
prediction.

##### Time Series Data

As well as the calendar attributes, it is important to consider the
historical load demand. This allows the time-series element to be
modelled.

To do this, a lagged input of the previous 3 hours, the equivalent three
hours from the previous day, and the equivalent 3 hours from the
previous week were used. For example, to predict the electricity
consumed on the 21st December 2010 at 12:00 pm the electricity between
9:00 pm and 11:30 pm on the 21st of December are used as inputs, as are
the times between 9:00 pm and 12:00 pm on the 20th and 14th of December.

Long-Short Term Memory neural networks remember values over arbitrary
time intervals. They can remember short-term memory over a long period
of time; for this reason, five lagged inputs of the previous two and a
half hours were used as features to the Long-Short Term Memory network.

##### Data Representation

Once useful information is selected, we must encode the data for input
into the models. To encode the day of the week, seven binaries are
utilised in the form of one-hot encoding. Six of the binaries are for
Monday through to Saturday. When all six binaries are equal to zero
Sunday is encoded. A single binary for public holidays is included.
Eleven binaries are used for month of the year, with the first eleven
representing January to November, with December represented by all zeros
in the calendar binaries. The current hour and date are input using a
numerical attribute. The lagged data inputs, such as previous hour’s
electricity usage are also inputted using a numerical attribute for each
entry, totalling 20 attributes (six half-hourly entries for each 3 hour
period multiplied by three days plus 2 entries for the time to be
predicted on the previous day and week). Table 4.1 displays these
features.

#### 4.4.2 Overfitting

Overfitting can be a significant problem within the machine learning
domain. Overfitting is where a model is fit too closely to the training
data. Therefore, whilst the model is able to predict the original
dataset well, it is unable to predict data it has not seen before to the
same degree of accuracy.

To avoid this problem, we used both a training and a test set. The
training set is used to fit the data, however, to see the performance of
the model, we tested the model using previously unseen data, which is
called the test set. This therefore allowed us to verify that the models
trained were not overfitting solely to the training data.

#### 4.4.3 Experiments

This section explores the different methods used to select the model
parameters, and the tests to evaluate our models. Once the parameters
were chosen the models were trained on the different clusters of
residential customers. Each model was run five times to explore the
variance of the results.

##### Support Vector Regression

To implement a Support Vector Regression model, a variety of parameters
must be chosen. These parameters influence the performance of the model.
The parameters were evaluated using cross-validation. To do this, the
data was split 75% into training data, and the remaining 25% into test
data. This split was chosen to balance the trade-off between having
enough training data so that the model can accurately learn the
underlying form of the data, but also to have enough data to validate
each model.

To choose the optimum support vector machine kernel cross-validation was
also used. Again, with 75% acting as the training data and 25% as the
test. The kernels compared were polynomial, radial basis function (RBF)
and the linear kernel Chang2010 ; theodoridis2009pattern . These were
chosen due to their popularity, support and relative speed of
computation.

The parameter values selected are shown in Table 4.2 . These parameter
values were chosen from the results of cross-validation for each of the
different kernels. From the cross-validation, the linear kernel was
found to be the best performing. For this reason, the linear kernel was
utilised for prediction of electricity consumption in this thesis.

##### Random Forest

To initialise the Random Forest algorithm with the number of variables
randomly sampled as candidates at each split, cross-validation was used.
Once again, 75% of the data was used for training and the remaining 25%
for testing due to the trade-off between training and testing.

Figure 4.7 shows the results of tuning the parameter of the number of
variables randomly sampled as candidates at each split. The optimum
number was found to be 23. Either side of this value the RMSE increases.
Therefore the value 23 was selected to be the number of variables
randomly sampled as candidates at each split in the Random Forest model.
It is proposed that the value 23 was found to be optimum due to the 20
lagged inputs, as this data is crucial for the Random Forest to learn
the underlying nature of electricity load.

##### Neural Networks

A feed-forward Multilayer Perceptron is a common neural network
architecture used for the prediction of time-series data, which has
comparable, and in some cases better, results than statistical models
Hill1994 .

The first step when designing a Multilayer Perceptron neural network is
to design the architecture. For this case, the number of input neurons
is set to 41 (see Table 4.1 ). Once an input for each neuron is entered,
the output layer must be designed. Due to the fact that we are
forecasting only one-time step ahead (30 minutes ahead) one output
neuron is required.

The next step is to design the architecture of the hidden layers. To
accomplish this, cross-validation is utilised as per the previous
models. A maximum of 3 hidden layers were tested, and the results
analysed. A similar method to Fan et al. Fan2009 was evaluated to choose
the number of neurons and hidden layers, a technique known as the
Levenberg-Marquardt technique more1978levenberg . The
Levenberg-Marquardt is a technique suitable for training medium-sized
Artificial Neural Networks with a low mean-squared error. The
fundamental rule is to select the minimum number of neurons in the
hidden layer so as to capture the complexity of the model, but not too
many as to introduce over-fitting, which results in a loss in the
generalisation of the algorithm.

The method begins by choosing a small number of neurons and gradually
increasing the number each time the model is trained, and the forecast
error obtained. The forecast error is monitored until an optimum value
is found, to which no further improvement is noted. Once the optimum
number of neurons in the layer is obtained, an additional layer is
added, and the same technique is used.

Using this technique, an optimal architecture with three layers is
obtained. The first layer contained two neurons, the second contained
five, and the third contained four.

##### Lstm

To initialise the LSTM, cross-validation was used to select the number
of stacked layers and memory units. We trialled the number of stacks
from the following list: @xmath and the number of memory units from the
following list: @xmath .

The optimum number of layers was found to be 2, with a total of 50
memory units. Different combinations of layers and memory units
displayed worse results.

#### 4.4.4 Results

To test the accuracy of the trained model, the data was split into a
training and test set. The data between the 14th of July 2009 and the
15th of June 2010 was used as the training data, whilst the data between
the 15th of June 2010 and 31st of December 2010 was used for testing
purposes. The test set is separate from the training set and not used
during training. We did not, however, consider seasonal issues for this
work.

28 independent forecasting models are constructed for each of the Random
Forests, Support Vector Regression, LSTMs and Multilayer Perceptron
neural networks for each of the groups with k varying from 1 to 7. This
was done to determine the optimal number of clusters. Each of the 28
models are trained independently, five times each so that the standard
deviation results of MAPE for each cluster could be displayed. We
evaluated the MAPE of the overall prediction.

Figure 4.10 demonstrates the impact of using calendar attributes such as
month, day of the month, and day of the week on prediction accuracy. The
results show an increase in prediction accuracy of 6% for neural
networks, 4% for Random Forests and 1% for support vector regression
when taking into account these variables. It is proposed that the
ability for the models to take into account the yearly, monthly and
weekly seasonal behaviour improves the results.

Figure 4.8 displays the accuracy of the models trained at different
numbers of clusters ( k ). The results demonstrate that introducing
clusters to group similar customers improve results in all cases. The
optimum value for k for Random Forests, Support Vector Regression and
neural networks was shown to be four for our dataset. After this, the
accuracy diminishes slightly. The error bars shown in Figure 4.8 show a
small variance in MAPE in SVR s, ANNs and Random Forests. However, the
MAPE of the LSTMs seem to vary by up to 11% in the five models run.

Figure 4.9 shows the average load profiles of different clusters when
@xmath . It is proposed that the optimum number of clusters is four due
to the distinct load profiles that can be seen in Figure 4.9 . The four
different distinct patterns seen are high night time use in cluster 1, a
typical residential load profile is shown in cluster 2, a spread of
usage in cluster 3, and high daytime usage in cluster 4. At @xmath these
distinct patterns are not adequately clustered, and at @xmath one of the
distinct clusters is split, leading to an increase in stochasticity.

It is true that the optimum number of clusters will vary for different
datasets. Whilst residential smart meter datasets may be similar, it is
entirely possible that different geographies display different usage
characteristics based on factors such as culture, temperature and
economic reasons. It is therefore important to choose an optimal number
of clusters for each dataset.

The results demonstrate that SVR, Random Forests and the Multilayer
Perceptrons have a similar overall accuracy. The LSTM shows a similar
pattern in increasing accuracy with a number of clusters. However, the
Random Forest seems to outperform each of the models at every point.
This may be due, in part, to the internal operation of the Random Forest
which undertakes its own cross-validation using out-of-bag samples and
only having a few tuning parameters. This implies that neural networks
and support vector regressions require more tuning.

It has been shown that neural networks, SVR and Random Forests all
perform within an adequate range of predicting electricity consumption.
Whilst LSTMs perform poorly. This may be due to the features given to
the LSTM, which only had previous two and a half hours of data as input.

However, it is well known that the best machine learning technique for
predicting energy consumption cannot be chosen a priori . Therefore it
is necessary to compare different techniques to find the best solution
to a particular regression problem Ahmad2017 .

For this work, the training time was tested by timing how long the
models would take to fit to one cluster (single model trained on the
training set). The Support Vector Regression took much less time than
all of the other methods, whereas the LSTM took the longest. The
Artificial Neural Network required 9 minutes and 5 seconds to run. The
Support Vector Regression model required 3 minutes and 32 seconds to
run. The Random Forest, on the same data, required 9 minutes and 44
seconds to run, whilst the LSTM took 12 minutes 55 seconds.

### 4.5 Day-ahead forecasting

In this section, we expand on the work undertaken in Section 4.4 by
utilising further time-series prediction algorithms, including online
machine learning methods. We take the error distributions and perturb
the exogenous electricity demand of ElecSim, and observe the long-term
impacts of poor error forecasts on the UK electricity market. It should
be noted that this could be applied to any decentralised electricity
market.

#### 4.5.1 Methods

##### Data preparation

Similarly to our previous work in Chapter 3 Kell2018a , we selected a
number of calendar attributes and demand data from the GB National Grid
Status dataset provided by the electricity market settlement company
Elexon, and the University of Sheffield gbnationalgridstatus2019 . This
dataset contained data between the years 2011-2018 for the United
Kingdom. The calendar attributes used as predictors to the models were
hour, month, day of the week, day of the month and year. These
attributes allow us to account for the periodicity of the data within
each day, month and year.

It is also the case that electricity demand on a public holiday which
falls on a weekday is dissimilar to load behaviours of ordinary weekdays
Kim2000 . We, therefore, marked each holiday day to allow the model to
account for this.

As demand data is highly correlated with historical demand, we lagged
the input demand data. In this context, the lagged data is where we
provide data of previous time steps at the input. For example, for
predicting t+1, we use @xmath inputs: t, t-1, t-2, @xmath , t-n. This
enabled us to take into account correlations on previous days, weeks and
the previous month. Specifically, we used the previous 28 hours before
the time step to be predicted for the previous 1st, 2nd, 7th and 30th
day. We chose this as we believe that the previous two days were the
most relevant to the day to be predicted, as well as the weekday of the
previous week and the previous month. We chose the previous 28 hours to
account for a full day, plus an additional 4 hours to account for the
previous day’s correlation with the day to be predicted. We could have
increased the number of days provided to the algorithm, as well as
marking holiday days. However, due to time and computational
constraints, we used our previously described intuition for lagged data
selection. The number of lagged inputs to trial increases exponentially
with each additional day added, therefore making the problem intractable
when also trialling such a high number of algorithms and
hyperparameters.

In addition to this, we marked each of the days with their respective
seven seasons. These seasons were defined by the National Grid Short
Term Operating Reserve (STOR) Market Information Report ESO2019 . These
differ from the traditional four seasons by splitting autumn into two
further seasons, and winter into three seasons. Finally, to predict a
full 24-hours ahead, we used 24 different models, 1 for each hour of the
day.

The data is standardised and normalised using min-max scaling between -1
and 1 before training and predicting with the model. This is due to the
fact that the inputs such as day of the week, hour of day are
significantly smaller than that of demand. Therefore, the demand will
influence the result more due to its larger value; however, this does
not necessarily mean that demand has greater predictive power.

##### Algorithm Tuning

To find the optimum hyperparameters, cross-validation is used. As this
time-series data was correlated in the time-domain, we took the first
six years of data (2011-2017) for training and tested on the remaining
year of data (2017-2018).

Each machine learning algorithm has a different set of parameters to
tune. To tune the parameters in this thesis, we used a grid search
method. Grid search is a brute force approach that trials each
combination of parameters at our choosing; however, for our search
space, this was small enough to make other approaches not worth the
additional effort.

Tables 4.4 and 4.4 display each of the models and respective parameters
that were used in the grid search. Table 4.4 shows the offline machine
learning methods, whereas Table 4.4 displays the online machine learning
methods. Each of the parameters within the columns “Values” are trialled
with every other parameter.

Whilst there is room to increase the total number of parameters, due to
the exponential nature of grid-search, we chose a smaller subset of
hyperparameters, and a larger number of regressor types. Specifically,
with neural networks, there is a possibility to extend the number of
layers as well as the number of neurons, to use a technique called deep
learning. Deep learning is a class of neural networks that use multiple
layers to extract higher levels of features from the input.

##### Prediction Residuals in ElecSim

Each of the previously mentioned models trialled will have a certain
degree of error. Prediction residuals are the difference between the
estimated and actual values. We collect the prediction residuals to form
a distribution for each of the models. We then trial 80 different
closed-form distributions to see which of the distributions best fits
the residuals from each of the models. These 80 distributions were
chosen due to their implementation in scikit-learn scikit-learn .

Once each of the prediction residual distributions are fit with a
sensible closed-form distribution, we sample from this new distribution
and perturb the demand for the electricity market at each time step
within ElecSim.

By perturbing the market by the residuals, we can observe what the
effects are of incorrect predictions of demand in an electricity market
using the long-term electricity market model, ElecSim. We are able to
understand the differences that prediction residuals have on long-term
investment decisions as well as generators utilised.

#### 4.5.2 Results

In this section, we detail the accuracy of the algorithms and
statistical models to predict 24 hours ahead for the day-ahead market.
In addition to this, we display the impact of the errors on electricity
generation investment and electricity mix from the years 2018 to 2035
using the agent-based model ElecSim.

##### Offline Machine Learning

To generate these results, we use a training set to train the data, and
a test set to see how well each algorithm performs on the testing data.
That is, how well the algorithm can predict data it is yet to see. In
our case, the training data was from 2011 to 2017, and the testing data
was from 2017 to 2018.

Figure 4.11 displays the mean absolute error of each of the offline
statistical and machine learning models on a log scale. It can be seen
that the different models have varying degrees of success. The least
accurate models were linear regression, the Multilayer Perceptron ( MLP
)model and the Least Angle Regression (LARS). These all have mean
absolute errors over 10,000MWh. This error would be prohibitively high
in practice; the max tendered national grid reserve is 6,000MWh, while
the average tendered national grid reserve is 2,000MWh ESO2019 .

A number of models perform well, with a low mean absolute error. These
include the lasso, gradient Boosting Regressor and K-neighbours
regressor. The best model, similar to the work presented in the
30-minute ahead forecasting Kell2018a , was the decision tree-based
model, Extra Trees Regressor, with a mean absolute error of @xmath MWh.
This level is well within the average national grid reserve of 2,000MWh.

Figure 4.12 displays the distribution of the best offline machine result
(Extra Trees Regressor). It can be seen that the max tendered national
grid reserve falls well above the 5% and 95% percentiles. However, there
are occasions where the errors are greater than the maximum tendered
national grid reserve. In addition, the majority of the time, the
model’s predictions fall within the average available tendered national
grid reserve.

Figures 4.13 and 4.14 display the time taken to train the model and time
taken to sample from the model versus the absolute error respectively
for the offline algorithms. Multiple fits are trialled for each
parameter type for each model. The error bars indicate the results of
multiple cross-validations.

It can be seen from Figure 4.13 that the time to fit varies
significantly between algorithms and parameter choices. The multilayer
perceptron consistently takes a long time to fit, when compared to the
other algorithms and performs relatively poorly in terms of MAE. There
are many models, such as the random forest regressor, and extra trees
regressors which perform well, however, take a long time to fit,
especially when compared to the K-Nearest neighbours.

For a small deterioration in MAE it is possible to decrease the time it
takes to train the model significantly. For example, by using the
K-Nearest neighbours or support vector regression (SVR).

The scoring time, displayed in Figure 4.14 , also displays a large
variation between model types. For instance, the MLP regressor takes a
shorter time to sample predictions when compared to the K-Neighbors
algorithm and support vector regression. It is possible to have a
cluster of algorithms with low sample times and low mean absolute
errors. However, often a trade-off is required, with a fast prediction
time requiring a longer training time and vice-versa.

##### Online Machine Learning

To see if we can improve on the predictions, we utilise an online
machine learning approach. If we are successful, we should be able to
reduce the national grid reserves, reducing cost and emissions.

Figure 4.15 displays the comparison of mean absolute errors for the
different trialled online regressor models. To produce this graph, we
performed various hyperparameter trials. Where the hyperparameters had
the same results, we removed them. For the multilayer perceptron (MLP),
we aggregated all hyperparameters, due to the similar nature of the
predictions. This is due to the fact that no additional information
would be gained from presenting this work, as the results are the same
or very similar.

It can be seen that the best performing model was the Box-Cox regressor,
with an MAE of 1,100. This is an improvement of over 30% on the best
offline model. The other models perform less well. However, it can be
seen that the linear regression model improves significantly for the
online case when compared to the offline case. The passive aggressive
(PA) model improve significantly with the varying parameters, and the
MLP performs poorly in all cases.

Figure 4.16 displays the best online model. We can see a significant
improvement over the best online model distribution, shown in Figure
4.12 . We remain within the max tendered national grid reserve for 98.9%
of the time, and the average available tendered national grid reserve is
close to the 5% and 95% percentiles.

Figure 4.17 displays the residuals for a model with poor predictive
ability, the passive aggressive regressor. It displays a large period of
time of prediction errors at -20,000MWh, and often falls outside of the
national grid reserve. These results demonstrate the importance of
trying a multitude of different models and parameters to improve
prediction accuracy.

Figure 4.18 displays a comparison between the actual electricity
consumption compared to the predictions. It can be seen that the Box-Cox
model better predicts the actual electricity demand in most cases when
compared to the best offline model, the Extra Trees regressor. The Extra
Trees regressor often overestimates the demand, particularly during
weekdays. Whilst the Box-Cox regressor more closely matches the actual
results. During the weekend (between the hours of 120 and 168), the
Extra Trees regressor performs better, particularly on the Saturday
(between hours of 144 and 168).

Figure 4.19 shows the differences between predicted values and actuals.
Where the horizontal line @xmath would be perfect predictions. This
figure shows that the Extra Trees over predicts during the week. The
Box-Cox however, struggles with the weekends.

Figures 4.20 and 4.21 display the mean absolute error versus test and
training time respectively. In these graphs, a selection of models and
parameter combinations are chosen. Clear clusters can be seen between
different types of models and parameter types, with the passive
aggressive (PA) model performing the slowest for both training and
testing. Different parameter combinations show different results in
terms of mean absolute error.

The best performing model is the Box-Cox model, which is also the
fastest to both train and test. The linear regression, which performs
worse in terms of predictive performance, is as quick to train and test
as the Box-Cox model. Additionally, the multilayer perceptron (MLP) is
relatively quick to train and test when compared to the PA models.

It is noted that when compared to the offline models, the training time
is a good indicator to the testing time. In other words, models that are
fast to train are also fast to test and vice-versa. This also tells us
that these algorithms are highly suited to this problem domain, as the
training and testing time are much shorter than the available time to
solve these problems.

#### 4.5.3 Scenario Comparison

In this section we explore the effect of these residuals on investments
made and the electricity generation mix. To generate these graphs, we
perturbed the exogenous demand in ElecSim by sampling from the
best-fitting distributions for the respective residuals of each of the
online methods. We did this for all of the online learning algorithms
displayed in Figure 4.15 . We let the simulation run for 17 years from
2018 to 2035.

Running this simulation enabled us to see the effect on carbon emissions
on the electricity grid over a long time period. For instance, does
underestimating electricity demand mean that peaker power plants, such
as reciprocal gas engines, are over utilised when other, less polluting
power plants could be used?

##### Mean Contributed Energy Generation

In this section we display the mean electricity mix contributed by
different electricity sources over the years 2018 to 2035.

Figure 4.21(a) displays the mean photovoltaic (PV) contributed between
2018 and 2035 vs. mean absolute error of the various online regressor
models displayed in Figure 4.15 . A positive correlation can be seen
with PV contributed and mean absolute error. This is similar for coal
and nuclear output, shown in Figures 4.21(b) and 4.21(c) respectively.
However, as shown by Figure 4.21(d) , offshore wind reduces with mean
absolute error. Figure 4.21(e) displays the mean reciprocal gas engine
output vs mean absolute error between the same time period. Output for
the reciprocal gas engine also increases with mean absolute error.

The reciprocal gas engine was expected to increase with times of high
error. This is because, traditionally, reciprocal gas engines are peaker
power plants. Peaker power plants provide power at times of peak demand,
which cannot be covered by other plants due to them being at their
maximum capacity level or out of service. It may also be the case, that
with higher proportions of intermittent technologies, there is a larger
need for these peaker power plants to fill in for times where there is a
deficit in wind speed and solar irradiance.

It is hypothesised that coal and nuclear output increase to cover the
predicted increased demands of the service. As these generation types
are dispatchable, meaning operators can choose when they generate
electricity, they are more likely to be used in times of higher
predicted demand.

Photovoltaics may be used more with higher errors due to the times at
which the errors were greatest. For example, during the day, where
demand is higher, as is solar irradiance.

##### Total Energy Generation

In this Section, we detail the difference in total technologies invested
in over the time period between 2018 to 2035, as predicted by ElecSim.

CCGT, onshore, and reciprocal gas engines are invested in less over the
time period, as shown by Figures 4.22(a) , 4.22(d) , 4.23(a)
respectively. While coal, offshore, nuclear and photovoltaics all
exhibit increasing investments.

It is hypothesised that coal and nuclear increase in investment due to
their dispatchable nature. While onshore, non-dispatchable by nature,
become a less attractive investment when compared to the other
technologies.

CCGT and reciprocal gas engines may have decreased in capacity over this
time, due to the increase in coal. This could be because of the large
consistent errors in prediction accuracy that meant that reciprocal gas
engines were perceived to be less valuable.

Figure 4.23(b) shows an increase in relative mean carbon emitted with
mean absolute error of the predictions residuals. The reason for an
increase in relative carbon emitted could be due to the increased output
of utility of the reciprocal gas engine, coal, and decrease in offshore
output. Reciprocal gas engines are peaker plants and, along with coal,
can be dispatched. By being dispatched, the errors in predictions of
demand can be filled. It is therefore recommended that by improving the
demand prediction algorithms, significant gains can be made in reducing
carbon emissions.

#### 4.5.4 Discussion

From our results, it can be seen that different algorithms yield
differing prediction accuracies. Online models can result in a decrease
in 30% of prediction error on the best offline models. We calculated
this by comparing the MAE for Extra Trees to the MAE for the Box-Cox
regressor. We, therefore, recommend the use of online machine learning
for predicting electricity demand in a day-ahead market.

Similar to our assumptions, the online learning algorithms were able to
outperform the offline models. This is due to the non-stationary nature
of the data. An online method is able to use the most up-to-date
knowledge of the complex system of energy demand. For instance, a
certain year may have a particularly warm winter when compared to
previous years, reducing the amount of electricity used for heating.

However, contrary to our assumptions, the online linear regression
techniques outperformed the online machine learning techniques. This may
be due to their simpler nature and ability to learn from a smaller
subset of new data as opposed to relying on a large historic subset. For
the offline models, the best performing algorithms were the decision
tree approaches such as extra trees and random forests. This is a
similar outcome to our previous work, which showed that the best
performing method for demand forecasting were random forests Kell2018a .
Contrary to our assumptions, however, the lasso and ridge regression
outperformed the machine learning techniques support vector regression
and multilayer perceptron. This may be due to the ability of feature
selection by lasso and ridge regression, which only uses the most
important features.

To the best of our knowledge, more work has been done using offline
learning to predict electricity demand. This may be due to the
additional complexity of running online algorithms, and a smaller number
of available models to run in an online fashion.

In terms of computing power, finding the optimal input parameters,
hyperparameters and models to use can be a large undertaking. This is
due to the exponential growth of the number of choices that can be made.
This can be an issue where accuracy is of importance, especially when
the data changes over time, meaning it may be necessary to retest
previous results. However, due to the financial and sustainability
implications, we believe the trade-off between compute time and accuracy
is balanced towards compute time. There are also large implications if
the model were to break at a certain point in time. We, therefore,
recommend the reliance on multiple well-performing models, as opposed to
solely the best performing model at any one time. That is, qualitatively
compare and contrast models. However, a quantitative approach could be
undertaken here. For example, taking the mean or median of decisions of
various outputs of models.

In addition, the time it takes to make a prediction using the algorithm
is much smaller than the time you have to make the decision. That is,
where an algorithm can be run in seconds, we have hours to submit bids
onto the day ahead market. This therefore gives the user the ability to
use these predictions in a real setting.

For training time and prediction time, there is often a trade-off
between training and predicting. For instance, the k-nearest neighbours
is fast to train, but slow to sample from. Therefore stakeholders must
make a decision based upon accuracy, speed of training and sampling.

Additionally, the impact on the broader electricity market has been
shown to be significant. Principally, the investment behaviours of
generation companies change as well as the dispatched electricity mix.
The relative mean carbon emitted over this time period increases, due to
an increase in the utilisation of coal and reciprocal gas engines, at
the expense of offshore wind.

This work shows the importance for generator companies to predict
electricity demand with high accuracy. Without this work, it is possible
to have a sub-optimal electricity mix over the long-term from the
perspective of cost minimisation. The generator companies can use these
results to improve their predictions, and policy makers can use these
results to ensure that a framework is in place to provide companies with
high accuracy forecasts.

The limitations of this work, from the perspective of a generator
company or policy maker, is that it does not provide a fool-proof way of
improving accuracy of predictions with online learning. Rather, a
methodology to follow to improve results. In addition, we do not model
the impact of weather for solar and onshore renewable energy sources.
This may have a long-term significant impact on the electricity mix.
Particularly with climate change effects, which may change weather
patterns.

### 4.6 Conclusion

The availability of high granularity data produced by the smart grid
enables network operators to gain greater insights into their customer
behaviour and electricity usage. This enables them to improve customer
experience, utility operations and power management. In the first piece
of work, we demonstrated that implementing the k -means clustering
algorithm to group similar customers improved the accuracy of every one
of the different models tested. Distinct models were trained for each of
the clusters and the individual forecasts aggregated for the total
aggregated forecast. It was found that Random Forests outperformed the
other models at all levels of clustering and that the optimum number of
clusters was 4. Whilst the dataset used focused on residential data it
is expected that applying a similar clustering technique on commercial
properties would have a similar effect.

Next, we evaluated 16 different machine learning and statistical models
to predict electricity demand in the UK for the day-ahead market.
Specifically, we used both online and offline algorithms to predict
electricity demand 24 hours ahead. We compared the ability for the
offline models: lasso regression, random forests, support vector
regression, for both online and offline learning: linear regression,
multilayer perceptron and for just online learning: the Box-Cox
transformation and the passive aggressive regressor, amongst others. The
Box-Cox, as well as the passive aggressive regressors, were used as
online learning algorithms, the multilayer perceptron and linear
regression were used as both, whereas the rest were used as offline
learning algorithms.

We measured the errors and compared these to each model as well as the
national grid reserve. We found that through the use of an online
learning approach, we were able to significantly reduce error by 30% on
the best offline algorithm. We were also able to reduce our errors to
significantly below the national grid’s mean and maximum tendered
reserve, thus significantly reducing the chances of blackouts.

In addition to this, we took these errors, or residuals, and perturbed
the electricity demand within the market of the agent-based model
ElecSim. This enabled us to see the impact of different error
distributions on the long-term electricity market, both in terms of
investment and in terms of the electricity mix.

We observed that with an increase in prediction errors, we get a higher
proportion of electricity generated by coal, offshore, nuclear,
reciprocal gas engines and photovoltaics. This could be due to the fact
that more peaker and dispatchable plants are required to fill in for
unexpected demand. In addition, a higher proportion of intermittent
renewable energy sources leads to a higher use of peaker power plants to
fill in the gaps of intermittency of wind and solar irradiance. However,
by reducing the mean absolute error, we are able to significantly reduce
the amount of reciprocal gas engines and coal usage.

In future work, we will look into the features that best aid in the
forecasting of electricity consumption, try a wider variety of models in
an ensemble manner and try different clustering techniques such as
Self-Organizing Maps ( SOM )to obtain better accuracy measures. We will
also compare different prediction error measures.

We would also like to trial a different selection of algorithms and
statistical models and trial different inputs to the models, for
instance, by providing the model with two months worth of historical
data as dependent variables. Additionally, we would like to see the
impact of predicting wind speed and solar irradiance to see how these
impact the overall investment patterns and electricity mix.

## Chapter 5 Carbon optimization

### Summary

Carbon taxes have been shown to be an efficient way to aid in a
transition to a low-carbon electricity grid. In this work, we
demonstrate how to find optimal carbon tax policies through a genetic
algorithm approach, using the ElecSim model. To achieve this, we use the
NSGA-II genetic algorithm Valkanas2014 to minimise average electricity
price and relative carbon intensity of the electricity mix. We
demonstrate that it is possible to find a range of carbon taxes to suit
differing objectives.

In previous chapters we investigated the building of a long-term
agent-based model for electricity markets in the UK, and investigated
the impact of short-term predictions on long-term investments. However,
a policy maker may have specific objectives within the electricity
market that they are overseeing. For example, two key objectives in the
current climate are a reduction in carbon emissions, as well as a
reduction in electricity prices. A common tool for this are carbon
taxes.

However, a policy maker is unable to trial all carbon taxes and know the
interactions between carbon price, carbon emissions and electricity
price. Therefore, in this chapter we propose a machine learning method
that carries out this process automatically. This goes beyond the manual
work of setting carbon taxes exhibited in Chapter 3 . It fits within
this thesis by exploring the ability for machine learning to have an
impact on the wider electricity market over the long-term.

Our results show that we are able to minimise electricity cost to below
£10/MWh as well as carbon intensity to zero in every case. In terms of
the optimal carbon tax strategy, we found that an increasing strategy
between 2020 and 2035 was preferable. Each of the Pareto-front optimal
tax strategies are at least above £81/tCO2 for every year. The mean
carbon tax strategy was £240/tCO2. Whilst this work was undertaken for
the UK, it could be applied elsewhere.

This chapter is structured as follows: we introduce our work in Section
5.1 . Section 5.2 covers examples of optimisations using genetic
algorithms and different carbon strategies. Section 5.3 details the
optimization techniques applied. We present our results in Section 5.4 ,
and conclude in Section 5.5 . The work in this chapter has been
published in Kell2020a .

### 5.1 Introduction

In this work, we use the electricity market agent-based model ElecSim to
find an optimum carbon tax policy Kell . Specifically, we use a genetic
algorithm to find a carbon tax policy to reduce both average electricity
price and the relative carbon density by 2035 for the UK electricity
market. We compare this optimal strategy to the carbon tax policy of the
UK British government.

Carbon taxes have been shown to quickly lower emissions and lower the
costs to the public Wittneben2009 . Carbon taxes are able to send clear
price signals, as opposed to a cap-and-trade scheme, such as the EU
Emissions Trading System (ETS), which has been shown to be unstable
Wittneben2009 .

In this work, we use the reference scenario projected by the UK
Government’s Department for Business & Industrial Strategy (BEIS) with
model parameters calibrated by Kell et al. DBEIS2019 ; Kell2020 . This
reference scenario projects energy and emissions until 2035. We
undertake various carbon tax policy interventions to see how we can
reduce relative carbon density whilst at the same time, reduce the
average electricity price.

We did not model the EU ETS within ElecSim due to the significant
additional complexity required to model it in a simulation. However, the
carbon tax strategy that we took allows for policy makers to understand,
in principle, the range of carbon tax price that are required. This
information is directly relevant to both an ETS scheme or a carbon tax
approach, as these schemes can be designed with the appropriate range to
achieve the goals of the policy makers.

The parameter space we optimise over is the carbon tax price over a 17
year period from 2018 to 2035. The carbon price is used to influence the
objectives of average electricity price and relative carbon intensity in
2035. Grid and random search are approaches which trial parameters at
evenly distributed spaces and random spaces respectively. These
approaches are often inefficient, however, and require an increased
number of simulations due to their static nature. They are inefficient
due to the likelihood that they will continue to search a poor parameter
space area, even with consistent negative feedback. Genetic Algorithms,
in contrast, use an evolutionary computing approach to find global
optimal solutions faster, however, this is not guaranteed, as there are
some scenarios where grid search or random search may be quicker to find
an optimal solution. This is of particular importance in cases with a
large number of parameters or in simulations which require a long
compute time, which is the case for ElecSim.

In order to optimise over two potentially competing objectives, i.e.
average electricity price and relative carbon intensity, we use the
Non-Dominated Sorting Genetic Algorithm II (NSGA-II) Valkanas2014 . The
NSGA-II algorithm can approximate a Pareto frontier Pareto1927 ;
Stadler1979 . A Pareto frontier is a curve in which there is no solution
which is better than another along the curve for different sets of
parameters. In this context, better means that a solution is closer to
the optimal for a particular combination of objectives.

We find that the rewards of average electricity price and relative
carbon intensity are not mutually exclusive. That is, it is possible to
have both a lower average electricity price and a lower relative carbon
price. This is due to the low short-run marginal cost of renewable
technology, which has been shown to lower electricity prices
OMahoney2011 .

The main contribution of this work is to explore carbon tax strategies
using genetic algorithms for multi-objective optimisation.

### 5.2 Literature review

Multi-objective optimisation problems are commonplace. In this section,
we review multiple applications that have used multi-objective
optimisation, as well as explore the literature which focus on finding
optimal carbon tax strategies.

#### 5.2.1 Examples of Optimization

Similar to our work, Ascione et al . Ascione2016 use the NSGA-II
algorithm to generate a Pareto front to optimise for two objectives:
operating cost for space conditioning and thermal comfort. The aim of
their work is to optimise the hourly set point temperatures with a
day-ahead planning horizon. A Pareto front is generated, which allows a
user to select a solution according to their comfort needs and economic
constraints. This work showed a reduction in operating costs by up to
56% as well as improved thermal comfort.

Gorzałczany et al . Gorzaczany2016a also apply the NSGA-II algorithm.
However, they apply it to the credit classification problem. The
objectives optimised over were accuracy and interpretability when making
financial decisions such as credit scoring and bankruptcy prediction.
This technique was able to significantly outperform the alternative
methods in terms of interpretability while remaining competitive or
superior in terms of the accuracy and speed of decision making in
comparison with the existing classification methods.

Ma et al . Ma2015 use the multi-objective artificial immune optimisation
algorithm for land use allocation (MOAIM-LUA model). They balance land
use supply and demand based on the future dynamic demands from different
land-use stakeholders in the region at the macro-level in Anlu County,
China. The objectives to optimise were economic and ecological benefits.
They found that for this application, they were able to obtain better
solution sets than the NSGA-II algorithm.

In our previous work, not presented here, we use the NSGA-II algorithm
to optimise average overhead and energy consumption of a condor system
Kell2019 . We use the genetic algorithm to trial different parameters of
a Q-learning reinforcement learning algorithm, which acted as a job
scheduler. We found that we were able to generate a Pareto-front which
would allow stakeholders to select an optimum for their use case.

Whilst these works use a similar approach to solving an optimisation
problem using the NSGA-II algorithm, they optimise a different model and
application.

#### 5.2.2 Carbon Tax Strategies

In this section, we explore different strategies employed in the
literature to analyse the benefits and consequences of a carbon tax. To
the best of our knowledge, we are the first to employ a multi-objective
optimisation algorithm to minimise average electricity price and
relative carbon density.

Levin et al . Levin2019 use an optimisation model to analyse market and
investment impacts of several incentive mechanisms to support investment
in renewable energy and carbon emission reductions. Carbon tax was found
to be the most cost-efficient method of reducing emissions.

Zhou et al. Zhou2019 construct a social welfare model based on a
Stackelberg game. The differences and similarities between a flat carbon
tax and an increasing block tariff carbon tax are analysed using a
numerical simulation. This work shows that an increasing block tariff
carbon tax policy can significantly reduce tax burdens for manufacturers
and encourage low-carbon production. In contrast to Zhou et al . we
trial multiple different carbon tax strategies using a machine learning
approach.

Li et al . Li2017 use a hierarchical carbon market scheduling model to
reduce carbon emissions. Multi-objective optimisation was applied to
discover optimal behaviours for policymakers, customers and generators
to minimise the costs incurred by these actors. Our work, however,
focuses on the different strategies of carbon tax as opposed to optimal
actor behaviour.

These works do not use optimisation to find a carbon tax directly,
instead seeing the impact of several different options. Our work has the
ability to trial a vastly greater number of carbon tax strategies, to
hopefully find an optimum.

### 5.3 Optimization methods

Multi-objective optimisation allows practitioners to overcome the
problems with optimising multiple objectives with classical optimisation
techniques, such as the genetic algorithm used in Chapter 3 .
Multi-objective optimisation algorithms are able to generate
Pareto-optimal solutions as opposed to converting the multiple
objectives into a single-objective problem. A single-objective problem
assumes that there is only a single optimum, or combination of results,
and that other combinations are inferior. This may not be the case, as
different solutions are superior for a different set of circumstances. A
Pareto frontier is made up of many Pareto-optimal solutions which can be
displayed graphically. A user is then able to choose between various
solutions and trade-offs according to their wishes. The NSGA-II
algorithm, a multi-objective genetic optimisation algorithm, is able to
generate a Pareto frontier in a single optimisation run.

In the following sub-sections, we detail the NSGA-II algorithm. For an
overview of the standard Genetic Algorithm, please refer to subsection
3.3.4 .

#### 5.3.1 Nsga-Ii

NSGA-II Valkanas2014 is efficient for multi-objective optimisation on a
number of benchmark problems and finds a better spread of solutions than
Pareto Archived Evolution Strategy (PAES) Knowles1999 and Strength
Pareto EA (SPEA) Zitzler2006 when approximating the true Pareto-optimal
front.

The majority of multi-objective optimisation algorithms use the concept
of domination during population selection Burke2014 . A non-dominated
algorithm, however, seeks to achieve the Pareto-optimal solution. This
is where no single solution should dominate another. An individual
solution @xmath is said to dominate another @xmath , if and only if
there is no objective of @xmath that is worse than the same objective of
@xmath and at least one objective of @xmath is better than the same
objective of @xmath Bao2017 .

Non-domination sorting is the process of finding a set of solutions
which do not dominate each other and make up the Pareto front. See
Figure 5.1 a for a visual representation, where @xmath and @xmath are
two objectives to minimise and L1, L2 and L3 are dominated layers.

In this section, we define the processes used by the NSGA-II algorithm
to determine which solutions to keep:

##### Non-dominated sorting

We assume that there are @xmath objective functions to minimise, and
that @xmath and @xmath are two solutions. @xmath implies solution @xmath
is better than solution @xmath on objective @xmath . A solution @xmath
is said to dominate the solution @xmath if the following conditions are
true:

1.  The solution @xmath is no worse than @xmath in every objective. I.e.
    @xmath .

2.  The solution @xmath is better than @xmath in at least one objective.
    i.e. @xmath .

Next, each of the solutions are ranked according to their level of
non-domination. An example of this ranking is shown in Figure 5.1 a.
Here, @xmath and @xmath are the objectives to be minimised. The Pareto
front is the first front. All of the solutions in the Pareto front are
not dominated by any other solution. The solutions in layer 1, L1, are
dominated only by those in the Pareto front, and are non-dominated by
those in L2 and L3.

The solutions are then ranked according to their layer. For example, the
solutions in the Pareto front are given a fitness rank ( @xmath ) of 1,
solutions in L1 have an @xmath of 2.

##### Density Estimation

( @xmath ) is calculated for each solution. This is the average distance
between the two closest points to the solution in question.

##### Crowded comparison operator

( @xmath ) is used to ensure that the final frontier is an evenly spread
out Pareto-optimal front. This is achieved by using the two attributes:
@xmath and @xmath . The partial order is then defined as:
@xmath if @xmath or @xmath and @xmath Valkanas2014 .

This order prefers solutions with a lower rank @xmath . For solutions
with the same rank, the solution in the less dense area is preferred.

##### Overall algorithm

Similarly to the standard GA presented in subsection 3.3.4 , a
population @xmath is created with random parameters. The solutions of
@xmath are then sorted according to non-domination. The child population
@xmath of size @xmath is then created by binary tournament selection,
recombination and mutation operators. In this case, tournament selection
is the process of evaluating and comparing the fitness of various
individuals within a population. In binary tournament selection, two
individuals are chosen at random, their fitnesses are evaluated, and the
individual with the better fitness is selected AbdRahman2016 .

1: @xmath combine parent and child population

2: @xmath fast-non-dominated-sort @xmath where @xmath

3: @xmath

4: while @xmath

5: Calculate the crowding distance of @xmath )

6: @xmath

7: end while

8: Sort( @xmath ) sort in descending order using @xmath

9: @xmath select the first @xmath elements of @xmath

10: @xmath make-new-population @xmath using selection, crossover and
mutation to create the new population @xmath

11: @xmath

Algorithm 2 NSGA-II algorithm Valkanas2014

Next, a new combined population is formed @xmath . @xmath has a size of
@xmath . @xmath is then sorted according to non-domination. A new
population is then formed @xmath . Solutions are added from the sorted
@xmath in order of non-domination. Solutions are added until the size of
@xmath exceeds @xmath . The solutions from the last layer are
prioritised based on having the largest crowding distance Valkanas2014 .

This process is shown in Figure 5.1 b, which is repeated until the
termination condition is met. A termination condition could be: no
significant improvement over @xmath iterations or a specified number of
iterations have been performed. The full procedure is detailed formally
by Algorithm 2 .

#### 5.3.2 Carbon Optimization Application

In this section, we describe how the genetic algorithm is applied in our
carbon optimisation case. We use multi-objective optimisation to find a
solution which has both a low carbon emission and low average
electricity price. The parameter that we adjust is the carbon tax
between the years 2018 and 2035.

The mating steps work by, initially, taking the sets of carbon prices
over the 17 year period (2018 to 2035) which have the best rewards
(lowest relative carbon emissions and average electricity price). These
carbon prices are then mated with a probability of 90%, creating child
carbon prices. The children are mutated with a probability of 5%.
Therefore, 5% of children have a carbon price which is not inherited
from the parents. Over time, the mutations and inherited properties tend
to a population with more desirable rewards.

#### 5.3.3 Simulation Environment

In order to evaluate the different carbon strategies, we used ElecSim
Kell ; Kell2020 . For this work, we parametrised the model to data for
the UK in 2018 to act as a digital twin of the UK electricity market.
This includes the power plants in operation in 2018, and the funds
available to their respective companies dukes_511 ; companies_house .
ElecSim is validated by being instantiated by data from 2013 and
projected forward to 2018, with a mean absolute scaled error (MASE)
below or equal to 0.701 for all generator types Kell2020 .

The yearly income for each power plant is estimated for each generation
company by running a merit-order dispatch electricity market ten years
into the future. However, the expected cost of electricity ten years
into the future is uncertain. We, therefore, use the reference scenario
projected by BEIS and use the predicted costs of electricity calibrated
by our previous work in Chapter 3 DBEIS2019 ; Kell2020 .

#### 5.3.4 Optimization

In this section, we detail the optimisation approach taken. We modify
the carbon tax each year, as we believe this is the most likely process
taken by governments, giving generator companies and consumers the
ability to understand market conditions during each year.

##### Non-parametric carbon policy

The optimization approach has two stages. First, we initialize the
population of the NSGA-II algorithm @xmath with 18 attributes. These
correspond to a separate carbon tax for each year, shown by Equation 5.1
:

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

where @xmath is the first population, @xmath is the carbon price in year
@xmath . The constraints of the algorithm are that each of the carbon
prices are bound between the values of £ @xmath and £ @xmath . This
provides the optimisation algorithm with the highest degree of freedom.
The value £ @xmath was chosen due to the relative costs of electricity,
where £ @xmath would be the upper bound for the cost of electricity.
This high degree of freedom enables a high number of strategies to be
trialled due to its non-parametric nature. This, however, comes with a
large search space requiring a large number of iterations.

##### Linear carbon policy

To reduce the search space for the carbon strategy, we also trial a
linear carbon strategy, of the form:

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

where @xmath is the carbon price, @xmath is the year, @xmath is the
gradient or first attribute and @xmath is the intercept or second
attribute. The constraints of the optimisation problem are that @xmath
is bound by @xmath and @xmath , and @xmath by 0 and 250. These bounds
are chosen to ensure that the carbon price does not exceed @xmath £500
in the year 18 (2035) and is greater than about -£250, as well as
ensuring that the carbon tax in the first year is greater than £0 but
smaller than £250. The bounds for @xmath was chosen to make the
mathematics simpler, whilst remaining in range.

### 5.4 Results

In this section, we explore the results of the optimisations, the
optimum carbon strategies and the resultant electricity mixes.

#### 5.4.1 Non-parametric carbon policy

Figure 5.2 displays the development of the genetic algorithm against the
rewards, relative carbon density and average electricity price. Darker
colours display higher generation numbers. The first generation shows a
widespread in relative carbon density and average electricity price.
However, over successive generations, the solutions converge to a
relative carbon density of 0 and an average electricity price under
£10MW/h.

The rewards of relative carbon density and average electricity price are
not mutually destructive. This could be seen as counter-intuitive, as
historically low-carbon electricity generation was more expensive than
fossil fuels. However, these results could be shown due to the low
short-run marginal cost of renewable energy which reduces both
electricity prices and carbon emissions OMahoney2011 .

To understand the optimum carbon strategies, we visualised the
parameters that produced the lowest average electricity prices in Figure
5.3 . Specifically, we filtered for electricity prices under £5/MWh and
displayed the results using a heat map. The darker colours represent a
higher density of points.

Figure 5.3 displays a general trend, where carbon tax starts at @xmath
£100 until the year 2030, where it increases to @xmath £200 by 2035.
This may be due to the fact that a lower initial carbon tax of @xmath
£100 encourages investment in low-carbon technologies before the higher
rate of @xmath £200 comes into force. This higher rate of carbon tax
would allow GenCos to outcompete higher carbon-emitting generators over
the lifetime of the plants.

#### 5.4.2 Linear carbon policy

Figure 5.4 displays the development of the genetic algorithm against the
rewards: relative carbon density and average electricity price.
Similarly to the non-parametric carbon policy shown in Figure 5.2 , the
first generation shows a wide spread of results. However, the spread is
smaller than that of the linear carbon policy. This may be due to the
fact that it is easier for the GenCos to predict the carbon policy,
which increases confidence in the NPV calculations. The linear carbon
policy also converges to a relative carbon density of 0, and an average
electricity price smaller than £10MW/h.

Figure 5.5 compares the distributions of average electricity price for
both techniques. Both methods show improvements as the number of
generations of the genetic algorithm increase. The linear policy,
however, is able to more quickly converge to a low average electricity
price, with a mode of @xmath £5.4MW/h. The non-parametric policy has a
number of poorer performing parameters, and Generation Number 4 has a
bimodal distribution, with a mode of @xmath £6.3MW/h.

Figure 5.5(a) displays the linear carbon policies which had an average
electricity price under £4.5MW/h. There is no single ‘optimum’ carbon
policy; a range of policies are able to achieve low carbon and a low
average electricity price.

We explore the electricity mix generated of three different strategies
shown in Figure 5.5(b) . We selected the highest, lowest, and the lowest
flat carbon strategy to show a range of possible strategies. This means
that whilst a single carbon tax may be sought, in reality there are a
number of different strategies which each perform equally well.

It is therefore up to stakeholders and decision makers to select a
strategy within the bounds presented here. For example, a government may
decide to impose the lowest carbon tax policy, to avoid impacting
different groups and stakeholders. Or, they may choose a flat carbon tax
strategy to prevent an unwelcome increase.

Figure 5.7 displays the generated electricity mixes for each of the
selected strategies. To generate these figures, we ran 80 scenarios to
capture the variability between scenarios.

Whilst there does not seem to be a significant difference between
scenarios, with solar providing @xmath of the electricity mix by 2035,
there is an observable difference with the other generator types.

The ‘highest’ carbon strategy exhibits a higher uptake in nuclear,
possibly due to the fact that nuclear becomes more competitive when
compared to coal or gas. The ‘lowest’ carbon strategy shows a higher
uptake in Combined Cycle Gas Turbines (CCGT) during the years of 2026 to
2031 as it outcompetes nuclear. The ‘flat’ carbon policy shows a higher
percentage of solar energy than any of the other scenarios, albeit with
a lower percentage of nuclear. Onshore wind is shown to be consistent
for these scenarios.

#### 5.4.3 UK Government Policy

We compare our optimal carbon tax strategy to a scenario based on the UK
Government’s policy in this section from 2018 to 2034. Whilst it is not
possible to know the future carbon tax strategy over such a long time
horizon, we used a naive model approach to project a static carbon tax
strategy. That is, the strategy is maintained at the current level of
£18.08 throughout the projected horizon. We chose this level, due to the
carbon price at the time of writing. We ran 40 simulations to capture
the variance of the electricity mix. The work in this subsection is
additional work to that which was published in Kell2020a .

Figure 5.8 displays the resultant electricity mix of this carbon tax
strategy. A significant difference can be seen when compared to the
optimal carbon strategy. 50% of the electricity mix is provided by
nuclear energy by 2034. This is almost double that in the optimal carbon
tax strategy.

Solar, the second-largest source of electricity, provides 30% of
electricity supply, with CCGT providing @xmath 15%. This is a marked
difference to the electricity mix shown by Figure 5.7 , where solar
provides over 50% of electricity supply, followed by nuclear, which
provides @xmath 26%.

A larger variance can be seen in this scenario when compared to the
optimal carbon tax. This may be because of the less defined differences
between generator costs due to the lower carbon price, which does not
distinguish so strongly between carbon-emitting and non-carbon emitting
generators.

Table 5.1 displays the average electricity price and relative carbon
density of this scenario. As is expected, both average electricity price
and relative carbon density are above that of the optimal carbon price
scenario. This is because of the high short-run marginal cost of
nuclear, and the carbon intensity of CCGT.

These results show the importance of carbon tax in deciding the
electricity mix, where a difference is shown between high carbon taxes
and low-carbon taxes. However, solar continues to do well without a
carbon tax, as does nuclear, due to the high nuclear subsidy provided.

These results can be used by policy makers who require a range of
possible carbon tax levels to achieve certain objectives. These results
do not prescribe a single solution which should be used, rather we
present a range of different options and outcomes which could be chosen.
The limitations of this work are the limited scenarios that were
explored. For instance, different electricity demand trajectories could
be explored. This would provide the policy makers with an increased
level of confidence that the carbon tax policies would be robust in the
face of uncertainty.

### 5.5 Conclusion

In this work, we have demonstrated that it is possible to use the
genetic algorithm technique NSGA-II to optimise carbon tax policy using
an electricity market agent-based model.

We trialled a non-parametric carbon policy by allowing the genetic
algorithm to optimise a carbon price for each year. These results showed
us that a linear carbon tax might be appropriate. We then used a linear
model as a carbon tax policy to reduce the total number of parameters
for the genetic algorithm to optimise.

We were able to show that a range of linear carbon taxes were able to
achieve both low average electricity price and a relative carbon
intensity of zero in 2035. By exploring three different carbon tax
policies, we saw that @xmath 60% of electricity consumption in the UK
would be provided by solar. The difference between these ‘optimal’
carbon tax policies was largely shown by competition between CCGT, coal
and nuclear.

This was largely due to the low short-run marginal cost of solar and
nuclear energy, which means that they are often dispatched ahead of the
fossil-fuel based generators. CCGT and coal, however, are useful for
filling demand when there is low solar irradiance.

Additionally, we ran a carbon tax scenario based on that of the UK
Government’s. We found that an optimal carbon tax strategy had a much
lower average electricity price due to the low short-run marginal cost
of renewable energy. We also showed that we were able to achieve a low
relative carbon density through a higher carbon tax. The majority of the
electricity was provided by nuclear, which, in this scenario, had a high
subsidy. Therefore, we believe that a lower carbon density, and lower
average electricity price can be obtained without subsidies, but with a
higher carbon tax.

In future work, we would like to try additional scenarios with varying
future generation costs and calculate a sensitivity analysis to carbon
taxes. In addition to this, we would like to model the uncertain
reactions by consumers and generation companies with regards to carbon
taxes. The linear carbon tax approach is an introductory approach which
can be expanded upon. In addition, we would like to trial this approach,
as well as the future work to other countries, not just the United
Kingdom.

Additionally, we believe that this work could allow for optimisation of
other policies, and factors within the simulation. For example, it would
be possible to optimise for a range of possibilities for subsidies,
market designs and R&D investment. Questions such as, should solar
photovoltaics take investment priority for both R&D and installation
when compared to offshore wind in parts of the US.

We believe that we have created a framework in which different policy
options can be explored, and the use of computational optimisation can
trial many differing variants.

## Chapter 6 Strategic bidding within Electricity Markets

### Summary

Decentralised electricity markets are often dominated by a small set of
generator companies who control the majority of the capacity. In this
work, we explore the effect of collusion, or an oligopoly on electricity
market prices. We demonstrate this through ElecSim. We develop an agent,
representing a generator company, which uses a deep deterministic policy
reinforcement learning algorithm to bid in a uniform pricing electricity
market strategically. A uniform pricing market is one where all players
are paid the highest accepted price. ElecSim is parametrised to the
United Kingdom for the year 2018. However, this work could be applied to
other countries. This work can help inform policy on how to best
regulate a market to ensure that the price of electricity remains
competitive.

We find that capacity has an impact on the average electricity price in
a single year. If any single generator company, or a collaborating group
of generator companies, control more than @xmath 11 @xmath of generation
capacity, prices begin to increase by @xmath 25 @xmath . The value of
@xmath 25% and @xmath 11% may vary between market structures and
countries. For instance, different load profiles may favour a particular
type of generator or a different distribution of generation capacity.
Once the capacity controlled by a generator company is higher than
@xmath 35% of the total capacity, prices increase rapidly. The use of a
market cap of approximately double the average market price has the
effect of significantly decreasing this effect and maintaining a
competitive market.

In this chapter, we do not utilise the long-term features of the ElecSim
model, focusing, instead on a single year. However, the objective of
this thesis is to explore the wider impact of machine learning on the
electricity market. The discoveries and recommendations found in this
chapter can have a long-term impact on the wider market, through the
design of an efficient market type for the long-term. Additionally,
these types of recommendations are made more efficient through the
methodology explored in previous chapters. Specifically Chapter 3 . For
instance, agent-based models are required to model individual generator
companies, and reinforcement learning is a useful methodology to make
predictions and actions within an uncertain environment.

We introduce this work in Section . In Section 6.2 we review the
literature, and explore other uses of Reinforcement Learning ( RL )in
electricity markets. In Section 6.3 we introduce the agent-based model
used and the DDPG algorithm. Section 6.4 explores the methodology taken
for our case study. The results are presented in Section 6.5 . We
discuss and conclude our work in Sections 6.6 and 6.7 respectively.

### 6.1 Introduction

Under perfectly competitive electricity markets, generator companies
(GenCos) tend to bid their short-run marginal costs (SRMC) when bidding
into the day-ahead electricity market. However, electricity markets are
often oligopolistic, where a small subset of GenCos provide the majority
of the capacity to the market. Under these conditions, it is possible
that the assumption that GenCos are price-takers does not hold. That is,
large GenCos artificially increase the price of electricity to gain an
increased profit using their market power. If they were price-takers,
they would have to accept the competitive price set by the market.

Reduced competition within electricity markets can lead to higher prices
to consumers, with no societal benefit. It is, therefore, within the
interests of the consumer and that of government to maintain a
competitive market. Low energy costs enable innovation in other
industries reliant on electricity, and in turn, make for a more
productive economy.

In this work, we explore the effect of total control over capacity on
electricity prices. Specifically, we model different sizes of GenCos and
groups of colluding GenCos, to bid strategically to maximise their
profit using a reinforcement learning algorithm. This is in contrast to
the strategy of bidding using the SRMC of their respective power plants,
which would occur under perfect market conditions. To model this, we use
deep reinforcement learning (RL) to generate a bidding strategy for
GenCos in a day-ahead market. These GenCos are modelled as agents within
the agent-based model, ElecSim Kell ; Kell2020 . We use the UK
electricity market instantiated on 2018 as a case study, similar to our
work in Kell2019a . That is, we model each GenCo with their respective
power plants in the year 2018 to 2019. In total, we model 60 GenCos with
1,085 power plants between them. It is possible, however, to generalise
this approach and model to any other decentralised electricity market.

We use the Deep Deterministic Policy Gradient ( DDPG )deep RL algorithm,
which allows for a continuous action space Hunt2016a . Conventional RL
methods require discretisation of state or action spaces and therefore
suffer from the curse of dimensionality Ye2020a . As the number of
discrete states and actions increases, the computational cost grows
exponentially. However, too small a number of discrete states and
actions will reduce the information available to the GenCos, leading to
sub-optimal bidding strategies. Additionally, by using a continuous
approach, we allow for GenCos to consider increasingly complex bidding
strategies.

Other works have considered a simplified model of an electricity market
by modelling a small number of GenCos or plants EsmaeiliAliabadi2017 ;
Tellidou2007 . We, however, model each GenCo as per the UK electricity
market with their respective power plants in a day-ahead market. In
addition, further work focuses on a bidding strategy to maximise profit
for a GenCo. However, in our work, we focus on the impact that large
GenCos, or colluding groups of GenCos, can have on electricity price.

Our approach does not require GenCos to formulate any knowledge of the
data which informs the market-clearing algorithm or rival GenCo bidding
strategies, unlike in game-theoretic approaches Wang2011 . This enables
a more realistic simulation where the strategy of rival GenCos are
unknown.

This work fits into the wider scope of this thesis through using machine
learning to have an impact on the wider electricity market. In addition,
it utilises the work carried out in Chapter 3 for an additional
application. It is true that the long-term components of ElecSim are not
required for this work, but utilising every aspect of ElecSim was not
the central aim of this thesis.

### 6.2 Literature Review

Intelligent bidding strategies for day-ahead electricity markets can be
divided into two broad categories: simulation and game-theoretic models.
Agent-based models (ABMs) allow for the simulation of heterogenous
irrational actors with imperfect information. Additionally, ABMs allow
for learning and adaption within a dynamic environment
EsmaeiliAliabadi2017 . Game-theoretic approaches may struggle in complex
electricity markets where Nash equilibriums do not exist Wang2011 .

#### 6.2.1 Game-theoretic approaches

Here, we explore game-theoretic approaches. Kumar et al. VijayaKumar2014
propose a Shuffled Frog Leaping Algorithm (SFLA) to find bidding
strategies for GenCos in electricity markets. SFLA is a meta-heuristic
that is based on the evolution of memes carried by active individuals,
as well as a global exchange of information among the frog population.
They test the effectiveness of the SFLA algorithm on an IEEE 30-bus
system and a practical 75-bus Indian system. A bus in a power system is
a vertical line which several components are connected in a power
system. For example, generators, loads, and feeders can all be connected
to a bus, an example is shown in Figure 6.1 .

Kumar et al. find superior results when compared to particle swarm
optimisation and the genetic algorithm with respect to total profit and
convergence with CPU time. They assume that each GenCo bids a linear
supply function, and model the expectation of bids from rivals as a
joint normal distribution. In contrast to their work, we do not require
an estimation of the rivals bids.

Wang et al. Wang2011 propose an evolutionary imperfect information game
approach to analysing bidding strategies with price-elastic demand.
Their evolutionary approach allows for GenCos to adapt and update their
beliefs about an opponents’ bidding strategy during the simulation. They
model a 2-bus system with three GenCos. Our work, however, models a
simulation with 60 GenCos across the entire UK, which would require a
28-bus system model Bell2010 .

Our work differs from both of these approaches, in that their work
investigates what each GenCo should do, but not what would happen to the
entire market from a macro perspective.

#### 6.2.2 Simulation based approaches

Next, we explore simulation based approaches, which utilise
reinforcement learning to make intelligent bidding decisions in
electricity markets. RL is a suitable method for analysing the dynamic
behaviour of complex systems with uncertainties. RL can, therefore, be
used to identify optimal bidding strategies in energy markets Yang2020 .
Simulations are often used to provide an environment for the
reinforcement learning algorithm, which is true for the following papers
reviewed here.

Aliabadi et al. EsmaeiliAliabadi2017 utilise an ABM and the Q-learning
algorithm to study the impact of learning and risk aversion on GenCos in
an oligopolistic electricity market with five GenCos. They find that
some level of risk aversion is beneficial, however excessive risk
degrades profits by causing an intense price competition. Our work
focuses on the impact of the interaction of many GenCos within the UK
electricity market. In addition, we extend the Q-learning algorithm to
use the DDPG algorithm, which uses a continuous action space. This
allows for a continuous action space for bids, providing a higher
granularity to the possible actions taken by GenCos. This allows for a
more nuanced, and precise understanding of the optimal behaviour.

Bertrand et al. Bertrand2019 use RL in an intraday market. An intraday
market, in this context, allows for bids to be made during the day, to
aid in balancing supply and demand. Specifically, they use the REINFORCE
algorithm to optimise the choice of price thresholds. The REINFORCE
algorithm is a gradient-based method. They demonstrate an ability to
outperform the traditionally used method, the rolling intrinsic method,
by increasing profit per day by 4.2%. The rolling intrinsic method
accepts any trade, which gives a positive profit if the contracted
quantity remains in the bounds of capacity. In our work, we model a
day-ahead market and use a continuous action for price bids.

Ye et al. Ye2020a propose a novel deep RL based methodology which
combines the DDPG algorithm Hunt2016a with a prioritised experience
replay (PER) strategy Schaul2016 . The PER samples from previous
experience, but samples from the “important” ones more often Schaul2016
. The PER is a modification of the often used experience buffer, which
is a buffer which stores previous transitions and samples uniformly.
This helps to reduce the correlations between recent experiences. They
use a day-ahead market with hourly resolution and show that they are
able to achieve approximately 41%, 20% and 11% higher profit for the
GenCo than the MPEC, Q-learning and DQN methods, respectively. In our
work, we instead look at how to prevent GenCos (or sets of colluding
GenCos) from forcing higher prices above market rates.

Zhao et al. Zhao2016 propose a modified RL method, known as the gradient
descent continuous Actor-Critic (GDCAC) algorithm. This algorithm is
used in a double-sided day-ahead electricity market simulation. Where in
this case, a double-sided day-ahead market refers to GenCos selling
their supply to distribution companies, retailers or large consumers.
Their approach performs better in terms of participant’s profit or
social welfare compared with traditional table-based RL methods, such as
Q-Learning. Our work also looks at improving on table-based methods by
using function approximators, however our work looks at the entire
electricity market as a whole, instead of from the point of view of
GenCos.

### 6.3 Methodology

In this section, we describe the RL methodology used for the intelligent
bidding process as well as the simulation model used as the environment.

#### 6.3.1 Reinforcement Learning background

In reinforcement learning (RL) an agent interacts with an environment to
maximise its cumulative reward. RL can be described as a Markov Decision
Process ( MDP ). An MDP includes a state-space @xmath , action space
@xmath , a transition dynamics distribution @xmath and a reward
function, where @xmath . At each time step, an agent receives an
observation of the current state which is used to modify the agent’s
behaviour.

An agent’s behaviour is defined by a policy, @xmath . @xmath maps states
to a probability distribution over the actions @xmath . The return from
a state is defined as the sum of discounted future reward:

  -- -------- -- -------
     @xmath      (6.1)
  -- -------- -- -------

Where @xmath is a discounting factor @xmath . The return is dependent on
the action chosen, which is dependent on the policy @xmath . Where
@xmath is the reward for state @xmath and action @xmath . The goal in
reinforcement learning is to learn a policy that maximizes the expected
return from the start distribution @xmath .

The expected return after taking an action @xmath in state @xmath after
following policy @xmath can be found by the action-value function. The
action-value function is used in many reinforcement learning algorithms
and is defined in Equation 6.2 .

  -- -------- -- -------
     @xmath      (6.2)
  -- -------- -- -------

The action-value function defines the expected reward at time @xmath ,
given a state @xmath and action @xmath when under the policy @xmath .

#### 6.3.2 Q-Learning

An optimal policy can be derived from the optimal @xmath -values @xmath
by selecting the action corresponding to the highest Q-value in each
state.

Many approaches in reinforcement learning use the recursive relationship
known as the Bellman equation, as defined in Equation 6.3.2 : {dmath} Q_
π (s_t,a_t)= E _r_t,s_t+1∼E [r(s_t,a_t)+ γ E _a_t+1∼ π [Q_ π (s_t+1, π
(s_t+1))]]. The Bellman equation is equal to the action which maximizes
the reward plus the discount factor multiplied by the next state’s
value, by taking the action after following the policy in state @xmath
or @xmath .

The Q-value can therefore be improved by bootstrapping. This is where
the current value of the estimate of @xmath is used to improve its
future estimate, using the known @xmath value. This is the foundation of
Q-learning Gay2007 , a form of temporal difference (TD) learning
Sutton2015 , where the update of the Q-value after taking action @xmath
in state @xmath and observing reward @xmath , which results in state
@xmath is:

  -- -------- -- -------
     @xmath      (6.3)
  -- -------- -- -------

where,

  -- -------- -- -------
     @xmath      (6.4)
  -- -------- -- -------

@xmath is the step size, @xmath represents the correction for the
estimation of the Q-value function and @xmath represents the target
Q-value at time step @xmath .

It has been proven that if the Q-value for each state action pair is
visited infinitely often, the learning rate @xmath decreases over time
step @xmath , then as @xmath , @xmath converges to the optimal @xmath
for every state-action pair Gay2007 .

However, it is often the case that Q-learning suffers from the curse of
dimensionality. This is because Q-learning stores the Q-value function
in a look-up table. This therefore requires the action and state spaces
to be discretised. As the number of discretised states and actions
increases, the computational cost increases exponentially, making the
problem intractable. Many problems are naturally discretised which are
well suited to a Q-learning approach, however this is not always the
case.

#### 6.3.3 Deep Deterministic Gradient Policy

It is not straightforward to apply Q-learning to continuous action
spaces. This is because in continuous spaces, finding the greedy policy
requires an optimisation of @xmath at every time step. Optimising for
@xmath at every time step would be too slow to be practical with large,
unconstrained function approximators and nontrivial action spaces
Hunt2016a . To solve this, an actor-critic approach based on the
deterministic policy gradient (DPG) algorithm is used Silver2014 .

The DPG algorithm maintains a parameterized actor function @xmath which
specifies the current policy by deterministically mapping states to a
specific action. The critic @xmath is learned using the Bellman equation
as in Q-learning. The actor is updated by applying the chain rule to the
expected return from the start distribution @xmath with respect to the
actor parameters:

  -- -------- -- -- -------
     @xmath         (6.5)
  -- -------- -- -- -------

This is the policy gradient. The policy gradient is the gradient of the
policy’s performance. The policy gradient method optimises the policy
directly by updating the weights, @xmath , in such a way that an optimal
policy is found within finite time. This is achieved by performing
gradient ascent on the policy and its parameters @xmath .

Introducing non-linear function approximators, however, means that
convergence is no longer guaranteed. However, these function
approximators are required in order to learn and generalise on large
state spaces. The Deep Deterministic Gradient Policy (DDPG) modifies the
DPG algorithm by using neural network function approximators to learn
large state and action spaces online.

A replay buffer is utilised in the DDPG algorithm to address the issue
of ensuring that samples are independently and identically distributed.
The replay buffer is a finite-sized cache, @xmath . Transitions are
sampled from the environment through the use of the exploration policy,
and the tuple @xmath is stored within this replay buffer. @xmath
discards older experiences as the replay buffer becomes full. The actor
and critic are trained by sampling from @xmath uniformly.

A copy is made of the actor and critic networks, @xmath and @xmath
respectively. These are used for calculating the target values. To
ensure stability, the weights of these target networks are updated by
slowly tracking the learned networks. Pseudo-code of the DDPG algorithm
is presented in Algorithm 3 .

1: Initialize critic network @xmath and actor @xmath with random weights
@xmath and @xmath

2: Initialize target network @xmath and @xmath with weights @xmath

3: Initialize replay buffer @xmath

4: for episode=1,M do

5: Initialize a random process @xmath for action exploration

6: Receive initial observation state @xmath

7: for t=1,T do

8: Select action @xmath according to the policy and exploration noise,
@xmath

9: Execute action @xmath and observe reward @xmath and new state @xmath

10: Store transition @xmath in @xmath

11: Sample a random minibatch of @xmath transitions @xmath from @xmath

12: Set @xmath

13: Update critic by minimizing the loss:

  -- -------- --
     @xmath   
  -- -------- --

14: Update the actor policy using the sampled policy gradient:

  -- -------- --
     @xmath   
  -- -------- --

15: Update the target networks:

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

16: end for

17: end for

Algorithm 3 DDPG Algorithm Hunt2016a

#### 6.3.4 Simulation

We utilized the long-term electricity market agent-based model, ElecSim
Kell ; Kell2020 discussed in Chapter 3 . The model was run using a short
term approach by only iterating through a single year (2018), composed
of eight representative days, each of 24-time steps.

In this work, we explore whether large GenCos, or group of GenCos, can
manipulate the price of the electricity market through virtue of their
size. We achieve this by allowing a subset of GenCos to bid away from
their SRMC and allow them to learn an optimal bidding strategy for
maximising their income. The GenCo agents adopt a DDPG RL algorithm to
select their bids. This is to explore whether large GenCos, or a group
of GenCos can manipulate the price of the electricity market through
market power. The remaining GenCos, which fall outside of this group,
maintain a bidding strategy based upon their SRMC.

For the purpose of this work, we do not consider flow constraints within
the electricity mix. This is because we model the entire UK with more
than one thousand generators, and many nodes and buses. This would make
the optimisation problem intractable for the purpose of our simulation,
especially when considering the many episodes required for training. It
takes @xmath seconds to run a single year in the simulation, or episode
with our current setup. By increasing the simulation time further, we
would make the compute time intractable due to the many episodes
required for reinforcement learning to learn an effective policy.
Additionally, we must train several reinforcement learning policies to
account for each GenCo and market cap. Therefore a simulation that takes
@xmath 2 minutes to run, which needs to be executed one thousand times
takes @xmath 33 hours to complete, multiplied by 12 different GenCos
takes @xmath 16 days, and with two market cap scenarios it would take
@xmath 1 month.

### 6.4 Experimental Setup

To parametrise the simulation, we used data from the United Kingdom in
2018. This included 1,085 electricity generators and power plants with
their respective GenCos. The data for this was taken from the BEIS DUKES
dataset dukes_511 . The electricity load data was modelled using data
from Elexon portal and Sheffield University gbnationalgridstatus2019 ;
offshore and onshore wind and solar irradiance data from
renewables.ninja by Pfenninger et al. Pfenninger2016 . It would be
possible to adopt this approach to other decentralised markets in other
countries.

By modelling bidding decisions as an RL algorithm, we hoped to observe
the ability for RL to find the point at which market power artificially
inflates electricity prices. To achieve this, we chose the six largest
GenCos in the UK, as well as a smaller GenCo as a control. Groups of
GenCos are modelled as a single GenCo with a single RL strategy for the
purpose of this work. Table 6.1 displays the groups of GenCos, as well
as individual GenCos, with their respective capacity and number of
plants.

For the reinforcement learning problem we have the following tuple:
@xmath , where @xmath is the state at time @xmath and @xmath
respectively, @xmath is the action at time @xmath and @xmath is the
reward at time @xmath . For our problem the state space is given by the
tuple shown in Equation 6.6 :

  -- -------- -- -------
     @xmath      (6.6)
  -- -------- -- -------

where @xmath is the segment hour to bid into at timestep @xmath , @xmath
is the demand of the segment hour at timestep @xmath , @xmath is the
price of gas, @xmath the price of coal, @xmath is the carbon tax price,
and @xmath is the clearing price. We set the reward, @xmath to be the
average electricity price of that time step, @xmath .

For the action space, @xmath , we modelled two scenarios. Where there
was a price cap of £ @xmath /MWh and £ @xmath /MWh. Only these two
values were chosen to reduce computational load. We chose £ @xmath /MWh
as a reasonable price cap that may be introduced by a Government. This
was roughly double the average accepted price in 2018, therefore allowed
for higher prices in times of high demand or low supply. The £ @xmath
/MWh was chosen to simulate a realistic unbounded price cap. This
enabled us to see the price that an equilibrium is reached within a
market with agents with market power.

For this work, we assume that the action space @xmath only bids price,
and not how much capacity to bid on the market. We assume this to reduce
the dimensionality of @xmath , and simplify the training process.

In this work, we assume that the GenCo groups have no information about
the generation capacity, marginal cost, bid prices or profits of other
GenCos EsmaeiliAliabadi2017 . They learn the maximum profit that can be
made through experience within a particular market. We assume this
because in real-life GenCo groups have little information on their
competitors sensitive bidding data. If they were to have perfect
information on all their competitors, they would be able to devise a
perfect strategy which would always maximise their profit.

### 6.5 Results

In this section, we detail the results of the RL algorithm, and the
effect that capacity has on average electricity price within the UK. Our
approach could be generalised to any other decentralised electricity
market in other countries.

Figures 6.2 and 6.3 show the rewards over a number of time steps for the
unbounded and bounded cases respectively. Figure 6.2 shows a clear
difference between agents which use the DDPG RL strategy and have a
large capacity (green and yellow) compared to those which have a smaller
capacity (dark purple). The axis in Figure 6.2 are much larger than
those of 6.3 , highlighting the effect of market power on an unbounded
market.

The average electricity price for a capacity below 10,000MW, or @xmath
of total capacity, remains stable between £70/MWh and £100/MWh. This
range may be due to the stochasticity in calculating the weights for the
DDPG algorithm. The average electricity price does not change over the
time steps or training. We, therefore, hypothesise that there is no
market power as long as an individual GenCo owns below @xmath of total
electrical capacity.

On the other hand, once the capacity of a GenCo or groups of GenCos is
above 30,000MW, there is a significant increase in the average price for
capacity. The average electricity price for capacity falls between the
range of £ @xmath /MWh and £ @xmath /MWh.

Figure 6.4 displays the capacity controlled by the agents that use the
RL strategy versus the average electricity price for the unbounded case.
The colour displays the number of steps. The step-change, as shown in
Figure 6.2 can be seen clearly here, with agents with a capacity larger
than @xmath 25,000MW causing a step change in electricity price.
Electricity prices seems to cluster below @xmath 10,000MW. However,
after this point, the average electricity price begins to increase.

Figure 6.3 shows a cluster between @xmath £ @xmath /MWh and @xmath £
@xmath /MWh irrespective of the capacity of the agents. This is verified
by Figure 6.5 . This seems to suggest that setting a lower market cap
reduces the ability for generators, irrespective of size, from
influencing the electricity price.

Figures 6.6 and 6.7 display the actual bids made at the end of training
within the electricity market for all of their power plants. The number
of bids made by each GenCo changes dependent on the number of plants
that they own, with the Orsted GenCo only making eleven bids per
segment, and the largest group making 216 bids per segment. Figure 6.6
displays the uncapped scenario (£600/MWh) and 6.7 displays the capped
scenario (£150/MWh).

Figure 6.5(a) shows the bids made by the largest group of GenCos as
shown in Table 6.1 . A bimodal distribution can be seen, where the group
of GenCos tend to bid either the maximum or the minimum bid. We
hypothesise that they bid the maximum amount as this ensures that the
market price is artificially raised, and that they are able to utilise
their market power. The minimum price is bid the rest of the time to
ensure that generators bids are always accepted, regardless of whether
the market price has been artificially raised or not in each respective
clearing segment.

Figure 6.5(b) displays the bids of the small company with a market cap
of £600/MWh. The small company also seems to have a bimodal
distribution; however, bids the higher price more often. This may be due
to the fact that it is able to influence the price at certain market
segments, and the reward of the higher accepted reward outweighs the
times in which it is not accepted on the market segments. Again, bidding
low seems to be the strategy in which to take if the GenCo does not
believe it will be able to influence the final price. As the market
simulated is a uniform pricing market, having a £0 bid accepted does not
mean that the GenCo will be paid £0. Rather, the GenCo will be paid the
market clearing segment, which is set by the most expensive power plant
accepted onto that market segment.

Figures 6.6(a) shows the bids made by the largest group of GenCos in
each market segment. It seems to take a similar strategy to that of the
large company in the uncapped scenario, as shown in Figure 6.5(a) . We
believe, as similar in the uncapped scenario, that this is due to the
market power that this group possesses. Being able to influence the
market price enables the GenCo group to inflate the prices. It also
takes a conservative strategy to bid the minimum price allowed, to
ensure that the bid is accepted regardless of price.

Figure 6.6(b) displays the strategy of the smallest company in the
capped scenario. Here, it seems that the GenCo is unable to influence
the price at all, and therefore bids £0/MWh for the majority of the
time. This is similar to the expected strategy of GenCos, who tend to
bid their short-run marginal cost to ensure that they do not miss out on
potential profit. The short-run marginal cost can often change based
upon fuel, carbon and generator type. However, for renewable energy, it
is near £0, and for fossil-fuel based plants it is near the cost of fuel
and carbon at that point in time.

We ran a sensitivity analysis to observe the effects of the market cap
on final average accepted bid price. The largest GenCo group used a
strategy for this sensitivity analysis. Figure 6.8 displays the results.
It seems that whilst the average accepted bid price increased with the
capped bid level; there is a significant increase after a market cap of
£190/MWh. This may be due to the case that the GenCos begin to outbid
the SRMC bidding GenCos at this price point.

These results demonstrate a methodology for policy makers to investigate
the ability for generator companies to artificially increase the
electricity price. It gives quantitive advice on the level of capacity
that a generator company can control before it becomes a problem from
the perspective of consumers of the electricity market. These results
also provide a method to control excessive electricity prices with the
provision of a market cap.

The limitations from the perspective of the policy maker, is that we do
not provide any information on how to prevent an oligopoly from
occurring. We only provide advice on what to do once one has occurred.

### 6.6 Discussion

Our results demonstrate the ability for GenCos to artificially increase
the electricity price through market power in an uncapped market. Our
results have shown that in an uncapped market, any single agent or
groups of agents who make bids using the same strategy and information,
should have less than @xmath 10,000MW. This defines the optimal capacity
of any single GenCo to have a fair level of competition. After this, the
electricity price begins to rise with the same outcome and welfare. It
is also worth noting that when the market is capped, the average
accepted price does not simply become the capped price.

However, if there is an electricity market with a few large or colluding
players, it is possible to remove their advantage through the
introduction of a price cap. Our results show that whilst average
accepted bids increase with market cap level, the value does not
increase significantly until £ @xmath is reached.

This information and approach can help to inform government policy to
ensure fair competition within electricity markets, as well as run the
model for their own scenario. It is hypothesised that the findings in
this work are generalisable to other decentralised electricity markets
in other geographies due to their similar market structures. Whilst the
figures presented here may not be the same; we hypothesise that the
region of interest will be similar.

### 6.7 Conclusion

In this work, we investigated the ability for GenCos to make strategic
bids within an electricity market. We did this using the deep
deterministic policy gradient (DDPG) reinforcement learning method. We
utilised the agent-based model ElecSim to model the UK electricity
market. We utilised the DDPG algorithm only for a certain subset of
agents, from small individual generation companies (GenCos) to large
groups of GenCos.

This enabled us to explore the ability for GenCos with a large capacity
to artificially increase the price in the electricity market within the
UK if they are in control of a sufficiently large generation capacity.
Our results show that the optimum level of control of any single GenCo
or groups of GenCo is below @xmath 10,000MW or @xmath 11% of the total
capacity. Above this, prices begin to increase with no real additional
benefit to the consumer. After @xmath 25,000MW, or @xmath 35% of the
total capacity, the prices begin to increase substantially, to @xmath
£200, over triple the original cost without this market power. The
introduction of a market cap of £ @xmath reduces all market power and
maintains electricity price at a reasonable level.

We found through a sensitivity analysis that the average electricity
price in the market over a year remains low with a price cap smaller
than £190. However, after this level, the average electricity price
begins to increase.

Our work has shown the ability for reinforcement learning to learn an
optimal bidding strategy to maximise a GenCo’s profit within an
electricity market. The ability for GenCos to use their market power is
also highlighted, and is dependent on electricity generation capacity of
the respective GenCo.

In future work, we would like to enable GenCos to withhold the capacity
on offer to the electricity market. This would enable further market
power by reducing competition further. Additionally, we would like to
assess the market power in different countries with different market
structures and total electricity supply. An other option would be to
assess the effect of two large competing GenCos on the market.

## Chapter 7 Conclusion

### 7.1 Thesis summary

In this thesis, we presented an open-sourced, long-term agent-based
model for electricity markets called ElecSim. We were able to validate
the model through two methods: cross-validation using historic data from
2013 and projecting forward, and comparing our scenario to that of the
UK Government to 2035. We found positive results in both: we were able
to accurately model the transition from coal to gas between 2013 and
2018, and closely match the scenario of the UK Government.

For this validation, we used optimisation to find realistic parameters
that would generate the desired scenarios. The primary parameter that
was optimised was the predicted price duration curve. We found a
predicted price duration curve which closely matched the price found in
2018 for the cross-validation using historic data. For the UK government
forward scenario, we found a variety of predicted price duration curves,
with a few boom and bust cycles in the price of electricity.

In addition to designing and developing ElecSim, we used the forward
scenario generated to optimise a carbon tax strategy. That is, to reduce
both carbon emissions and electricity price from 2018 to 2035. To
achieve this we used a multi-objective genetic algorithm. We found that
we were able to reduce both electricity price and carbon emissions
through a combined use of solar, nuclear and onshore energy.

Further, we predicted electricity demand consumption on two time scales:
30-minutes ahead and a day-ahead to use in ElecSim. We found that we
were able to predict electricity demand at these time scales well.
Through the use of an online learning method, we were able to reduce the
required reserve capacity of the national grid in the UK. We also looked
at the long-term impact of poor forecasting errors on the long-term
market, and found that poor forecasts led to a high carbon density of
electricity grid over the long-term.

Our final chapter focuses on the use of deep reinforcement learning to
model a bidding strategy of a single generation company or group of
generation company using ElecSim. The rest of the generation companies
bid based upon the respective generator’s short run marginal cost. We
found that after a capacity controlled by a generation company of
10,000MW, the generation company with the reinforcement learning bidding
strategy was able to influence electricity prices in their favour;
effectively demonstrating market power within the electricity market.
After a controlled capacity of 30,000MW the average electricity price
triples from the equilibrium price.

We utilised agent-based models in this work for several reasons:

1.  Ability to model scenarios in a way that emerges from the generator
    companies.

2.  The process which emerge a not a black box, unlike in equilibrium
    models.

3.  Optimisation based methods should be interpreted in a normative
    manner, for example, how policy choices should be carried out.
    Whereas agent-based models make no assumption on outcomes of
    scenarios.

4.  Ability to model decentralised agents with different desires.

5.  Ability to model outcomes which are not in equilibrium, for example
    boom and bust cycles.

An additional reason was for the limited availability of an open-sourced
agent-based model, and to ensure that this model is available to the
community. Open-sourced models allow for transparency, and to garner
greater acceptance and understanding within the wider community.

Our contributions are the following:

1.  Developed a novel, open-source agent-based model for the electricity
    market called ElecSim. ElecSim and all related code can be accessed
    at: https://github.com/alexanderkell/elecsim .

2.  Validated this model by the means of cross-validation, and compared
    our model to that of the UK Government.

3.  Investigated the effect of online learning to improve electricity
    demand forecasting a day-ahead, and looked at the long-term impacts
    that this had on the electricity markets.

4.  Predicted electricity consumption 30-minutes ahead.

5.  Found a variety of optimal carbon tax strategies using genetic
    algorithm based optimisation and the ElecSim model.

6.  Investigated the impact of collusion within a oligopolistic
    electricity market.

### 7.2 Conclusions

In this section we explore the conclusions that arose from this thesis.
We explore each of the chapters where we present our primary technical
contributions. We place our work in the context of our research
question: what challenges can AI and ML tackle, and how do these methods
relate back to the wider energy system? Each of the chapters explore a
research subquestion, which are highlighted in Chapter 1 . We provide
answers to these subquestions here.

#### 7.2.1 ElecSim Model

Can a simulation model an electricity market over the long-term? Through
the recreation of the salient features of the UK electricity market into
an agent-based model, we show that we are able to derive similar outputs
to an established, governmental model. Therefore, even though the
underlying methodology between these two models are different, we have
shown that it is possible to reach similar conclusions.

This has important ramifications for future work, as features that could
not previously be modelled with traditional, optimisation based models,
such as heterogenous generation companies, can be modelled. This finding
adds a significantly different tool to the established models, and
allows modellers and policy makers to make findings from an alternative
kind of methodology. Whilst this does not provide a ground truth model
to the energy modelling community, it does provide an additional tool to
understand complexity within an electricity market.

Is it possible to model the variability of an electricity system? The
variability within a modern day electricity system is significant. With
the increase in solar photovoltaics and wind turbines a modern day
electricity model must be able to model fluctuations in solar irradiance
and wind speeds. This variability can change in minutes and therefore
adds significant complexity for electricity system models. This is
because a high temporal resolution is required to model this
variability.

To take into account this variability, we used a k-means clustering
approach to select representative days in Chapter 3 . Through the use of
these representative days, we found that we were able to model
significant peaks and troughs in demand, wind speed and solar
irradiance. This was shown by comparing three different metrics: the
normalised root mean squared error, correlation and relative energy
error. This was demonstrated in Chapter 3 . We recommend that a similar
approach is undertaken by other modellers for a realistic representation
of the real-world, without compromising on runtime.

Can we trust an electricity model’s outputs? Fundamentally, the ability
to trust an electricity model’s outputs is through the verification of
its outputs with observed data. For long-term models this becomes a
significant challenge because of all the uncertainties inherent in
long-term investments. Further, a long-term electricity model produces
scenarios and not predictions. This makes this problem even more
difficult, as an output scenario could be both possible and realistic,
but it does not mean that it is a certainty that it will occur.

To answer the subquestion of whether we can trust an electricity model’s
outputs, we investigated an observed shift in electricity mix from coal
to gas from the previous five years. We calibrated our model to find
parametric data that would generate the outputs required. We found that
the parametric data required to generate these outputs were realistic,
and we could model the shift in electricity mix from coal to gas over
this time period.

Whilst this approach has limitations, in that it raises an additional
question: can short-term validation translate to long-term verification?
However, this is the best solution we found without trying to predict
many years in the future which is both infeasible and unrealistic.

From this work, we recommend that models undertake some form of
validation using real-world, observed data. Without this, it is possible
that some aspects of the real-world system is not accurately modelled.
And so, an iterative process can be used to improve the functioning of
the model, with this approach.

#### 7.2.2 Electricity Demand Prediction

Do poor short-term forecasts of electricity demand have a long-term
impact? Previous work on short-term forecasts of electricity demand has
focused on improving the accuracy of such models. However, the long-term
impact of these predictions has been studied to a lesser degree. To
explore this question, we used a variety of different statistical and
machine learning techniques to predict electricity demand. We then took
the errors of these predictions and investigated the impact that these
errors have on the long-term electricity market by perturbing the
electricity demand perceived by the generator companies.

Our results show that with an increase in mean absolute error, there is
an increase in CCGT, coal, nuclear, photovoltaics and reciprocal gas
engines, with a decrease in onshore and offshore wind. Such a scenario
changes the electricity mix landscape in the future, and is suboptimal
if a low-carbon electricity mix is desired.

We recommend that a high amount of research is placed into improving
short-term forecasting for electricity demand. Whilst this can be
expensive in the short-term, the long-term benefits are significant
through the use of a more optimal and efficient system.

#### 7.2.3 Carbon Optimization

Is it possible to use an algorithm to set carbon policy? A significant
goal for policy makers is to achieve a low-cost and and low-carbon
system. However, in practice, this can be a difficult system to achieve.
Policy makers have limited levers in which to test, and those levers can
have an uncertain impact on the market.

To simplify the process of finding a cost and carbon optimal carbon tax
in the UK electricity market, we used a multi-objective genetic
algorithm, NSGA-II, to vary carbon tax between 2018 and 2035. We found
that it is possible to use a genetic algorithm to find a variety of
carbon policies to minimise both carbon and cost of the electricity
system. These carbon policies can be presented to policy makers to
choose from, depending on their requirements and preferences.

The recommendations that arise from this work is that a full
quantitative analysis should be undertaken for long-term policy options,
and that the search space can be explored using algorithms. This enables
policy makers to find a variety of different options, from which they
can choose from. Whilst the algorithm may not tell you the precise
strategy, it can give a range of feasible options, in which policy
makers can use their judgement with additional information.

#### 7.2.4 Strategic Bidding

Is it possible to limit the power of large generator companies?
Electricity markets can often be dominated by a small subset of
generator companies. If given enough power, these generator companies
can then use strategic bidding to artificially inflate the electricity
price for their own benefit.

With the use of reinforcement learning, we were able to simulate the
strategic bidding behaviour that generator companies may exhibit. We
found that if a generator company controls more than 11% of total
capacity, it is likely that price inflation will occur. However, we
found that such price inflation can be limited through the means of a
cap on bid levels. We found that a relatively high bid price cap of
£190/MWh prevents price inflation for the UK market.

We therefore recommend that policy makers place a price cap if they
believe that an oligopoly or monopoly is beginning to form. This can
limit the impact that these companies can have on consumers and the
overall market.

#### 7.2.5 AI and ML in a Wider Electricity Market Context

What challenges can AI and ML tackle, and how do these methods relate
back to the wider energy system? In this thesis, we explored the
different methods that AI and ML can tackle within the British
electricity system. Whilst much work has focused on incremental
improvements to targeted problems, such as short-term load forecasting
or the possibility of using reinforcement learning for short-term
bidding, the wider impact on the market has been explored to a lesser
extent.

We found that AI and ML can be used for a wide range of different
application domains, which are not limited to solely improving the
accuracy of these domains. The subquestions answered previously in this
section attest to this, and demonstrate that large-scale problems and
systems can be optimised, improved and modelled with the use of
state-of-the-art methods.

### 7.3 Future research direction

There remains significant research that can be undertaken as part of
this work that we would like to carry out in the future. A major aspect
of intermittent renewable energy sources such as wind and solar is their
distributed nature. On a geographic scale as large as the UK, weather
conditions change. Therefore, at any one time, capacity factors may
change across the country. It would, therefore, be beneficial to model
this, by integrating a higher temporal resolution into the model. This
would increase the complexity of the model, and therefore compute time,
however, the optimal placement of intermittent renewable energy sources
could be modelled to ensure the maximum supply at times when it was
required from such sources. Work could be done to find simplifications
that could be made to other aspects of the model to maintain
tractability.

Additionally, the integration of a larger amount of countries could be
carried out. For instance, coupling the UK market with that of Ireland,
or with the EU. Whilst this would increase the compute time, the results
would be more inclusive of other markets and enable for a larger
temporal resolution still. Another area of interest is predicting the
price of electricity in the future. Scenarios could be run to predict
this price and integrate it into the model. Whilst increasing compute
time, this would allow us to explore a wider range of scenarios.

Another aspect is the comparison to further models. The traditional
approach of optimisation models offer a variety of “gold-standard”
scenarios that we could explore. These comparisons could be undertaken
to find weaknesses in both our model as well as other models.

An interesting area of research is the area of investment using
optimisation or control algorithms such as reinforcement learning.
Reinforcement learning suffers from a requirement to have multiple
training epochs, and therefore we found this a difficult task to achieve
in this work. However, through either speed improvements or use of
another algorithm, this could be integrated into our work.

Finally, the integration of fuel supply-demand curves could help us in
better modelling electricity markets on both a local and global scale.
Firstly, land is a limited resource, especially in the United Kingdom,
and therefore limits to the amount of solar or wind that can be produced
a real. Secondly, on a global scale, as demand for gas or coal reduces
there are large effects on price. Firstly this may reduce due to the
supply and demand relationship. However, if demand reduces sufficiently,
the price may increase, due to the expense of extraction. This may turn
out to be a large inflection point, where gas and coal are used much
less.