### Foreword

  “ The greatest glory in living lies not in never falling, but in
  rising every time we fall. ”

  — Nelson Mandela

  “ Adults keep saying we owe it to the young people, to give them hope,
  but I don’t want your hope. I don’t want you to be hopeful. I want you
  to panic. I want you to feel the fear I feel every day. I want you to
  act. I want you to act as you would in a crisis. I want you to act as
  if the house is on fire, because it is. ”

  — Greta Thunberg

Climate Change
The Intergovernmental Panel on Climate Change (IPCC) set the global net
anthropogenic CO ₂ emission trajectories and targets, depicted in Figure
1 , to limit climate change ( ipcc2018sr15 ) . It is a summary for
policymakers (SPM) that presents the key findings of the special report
published in 2018 ⁶ ⁶ 6 The IPCC released in August 2021 the AR6 SPM (
ipcc2021ar6spm ) . It presents key findings of the Working Group I
contribution to the IPCC’s Sixth Assessment Report (AR6) on the physical
science basis of climate change. It is a must to read for every
decision-maker, professor, researcher, or person that wants to
understand the challenges at stake. , based on the assessment of the
available scientific, technical and socio-economic literature relevant
to Global Warming of 1.5°C and for the comparison between Global Warming
of 1.5°C and 2°C above pre-industrial levels. The SPM presents the
emission scenarios consistent with 1.5°C Global Warming:

  ’In model pathways with no or limited overshoot of 1.5°C, global net
  anthropogenic CO ₂ emissions decline by about 45% from 2010 levels by
  2030 (40–60% interquartile range), reaching net-zero around 2050
  (2045–2055 interquartile range).’ ( ipcc2018sr15 ) [C.1] ⁷ ⁷ 7
  https://www.ipcc.ch/sr15/chapter/spm/

The IPCC proposes different mitigation strategies to achieve the net
emissions reductions required to follow a pathway to limit Global
Warming to 1.5°C with no or limited overshoot. These strategies require
a tremendous amount of effort from all countries.

  ’Pathways limiting Global Warming to 1.5°C with no or limited
  overshoot would require rapid and far-reaching transitions in energy,
  land, urban and infrastructure (including transport and buildings),
  and industrial systems (high confidence). These systems transitions
  are unprecedented in terms of scale, but not necessarily in terms of
  speed, and imply deep emissions reductions in all sectors, a wide
  portfolio of mitigation options and a significant upscaling of
  investments in those options (medium confidence).’ ( ipcc2018sr15 )
  [C.2] 7

Trajectories to achieve IPCC targets
Several reports propose pathways and strategies to achieve these
targets. The International Energy Agency (IEA ⁸ ⁸ 8 https://www.iea.org/
) presents a comprehensive study of how to transition to a net-zero
energy system by 2050 in the special report ’Net-zero by 2050: A roadmap
for the global energy system’ ( iea2021 ) . It is consistent with
limiting the global temperature rise to 1.5 °C without a temperature
overshoot (with a 50 % probability). The Net‐Zero Emissions by 2050
Scenario (NZE) shows what is needed for the global energy sector to
achieve net‐zero CO ₂ emissions by 2050:

  ’In the NZE, global energy‐related and industrial process CO ₂
  emissions fall by nearly 40% between 2020 and 2030 and to net-zero in
  2050. Universal access to sustainable energy is achieved by 2030.
  There is a 75% reduction in methane emissions from fossil fuel use by
  2030. These changes take place while the global economy more than
  doubles through to 2050 and the global population increases by 2
  billion.’ ( iea2021 , Chapter 2: Summary)

The key pillars of decarbonization of the global energy system proposed
are (1) energy efficiency, (2) behavioral changes, (3) electrification ,
(4) renewables , (5) hydrogen and hydrogen‐based fuels, (6) bioenergy,
and (7) carbon capture, utilization, and storage.

First, concerning electrification, the direct use of low‐emissions
electricity in place of fossil fuels is one of the most significant
drivers of emissions reductions in the NZE, accounting for around 20% of
the total reduction achieved by 2050. The share of the electricity in
the final consumption increases from 20% in 2020 to 49% in 2050. It
concerns the sectors of the industry, transport, and buildings. Second,
concerning renewables:

  ’At a global level, renewable energy technologies are the key to
  reducing emissions from electricity supply. Hydropower has been a
  leading low‐emission source for many decades, but it is mainly the
  expansion of wind and solar that triples renewables generation by 2030
  and increases it more than eightfold by 2050 in the NZE. The share of
  renewables in total electricity generation globally increases from 29%
  in 2020 to over 60% in 2030 and to nearly 90% in 2050. To achieve
  this, annual capacity additions of wind and solar between 2020 and
  2050 are five‐times higher than the average over the last three years.
  Dispatchable renewables are critical to maintain electricity security,
  together with other low‐carbon generation, energy storage and robust
  electricity networks. In the NZE, the main dispatchable renewables
  globally in 2050 are hydropower (12% of generation), bioenergy (5%),
  concentrating solar power (2%) and geothermal (1%).’ ( iea2021 ,
  Section 2.4.5)

The IEA report is not the ground truth but has the merit to propose
guidelines and directions. There are many other reports and
organizations that present strategies and scenarios to achieve the IPCC
targets. For instance, The Shift Project (TSP) is a European think tank
⁹ ⁹ 9 https://theshiftproject.org/en/home/ advocating the shift to a
post-carbon economy. It proposes guidelines and information on energy
transition in Europe. The take-home message is that there are still
pathways to reach net-zero by 2050 . They remain narrow and challenging,
requiring all stakeholders, governments, businesses, investors, and
citizens to take action this year and every year after so that the goal
does not slip out of reach.

Gap between rhetoric and reality
However, the current gap between rhetoric and reality on emissions is
still huge :

  ’We are approaching a decisive moment for international efforts to
  tackle the climate crisis – a great challenge of our times. The number
  of countries that have pledged to reach net‐zero emissions by
  mid‐century or soon after continues to grow, but so do global
  greenhouse gas emissions. This gap between rhetoric and action needs
  to close if we are to have a fighting chance of reaching net-zero by
  2050 and limiting the rise in global temperatures to 1.5 °C.’ (
  iea2021 , Foreword)

Figure 2 illustrates humorously this gap.

The UNEP Emissions Gap Report provides every year a review of the
difference between the greenhouse emissions forecast in 2030 and where
they should be to avoid the worst impacts of climate change. Figure 3
depicts the global GHG emissions under different scenarios and the
emissions gap in 2030.

2009-2019 was particularly intense in rhetoric tackling the climate
crisis like the 2015 United Nations Climate Change Conference, COP 21.
However, the carbon dioxide emissions at the world scale constantly rose
from 29.7 (GtCO ₂ ) in 2009 to 34.2 in 2019 ( looney2020statistical ) ¹⁰
¹⁰ 10
https://www.bp.com/content/dam/bp/business-sites/en/global/corporate/pdfs/energy-economics/statistical-review/bp-stats-review-2020-full-report.pdf
. In the meantime, the primary energy consumption increased from 134 000
TWh to 162 000. In 2019, the primary energy consumption by fuel was
composed of oil 53 600 (33.1%), coal 39 300 (24.2%), natural gas 43 900
(27.0%), nuclear energy 6 900 (4.3%), hydro-electricity 10 500 (6%), and
renewables 8 100 (5%), as depicted in Figure 4 . The share of renewables
in the energy mix progressed and reached 5% in 2019 (a record). However,
the total consumption of fossil fuels such as oil, coal, and natural gas
also rose. Oil continues to hold the largest share of the energy mix,
coal is the second-largest fuel, and natural gas grew to a record share
of 24.2%.

The Covid‐19 pandemic has delivered a shock to the world economy. It
resulted in an unprecedented 5.8% decline in CO ₂ emissions in 2020. The
IPCC targets for 2050 require a reduction of 5% each year from 2020 to
2050. However, the IEA data shows that global energy‐related CO ₂
emissions have started to climb again since December 2020. Nevertheless,
there is still hope, and every action to decrease the CO ₂ emissions to
gain a reduction of 0.1°C is a winning!

### Chapter 1 General introduction {infobox}

Overview This chapter introduces the context, motivations, content, and
contributions of the thesis. Two main parts compose the manuscript: (1)
forecasting; (2) planning and control. Finally, it lists the
publications.

  “ Life has no meaning a priori… It is up to you to give it a meaning,
  and value is nothing but the meaning that you choose. ”

  — Jean-Paul Sartre

#### 1.1 Context and motivations

###### Assumption 1.

Let suppose a utopian world where the current gap between rhetoric and
reality on GHG emissions has been drastically decreasing to limit
Climate Change and achieve the ambitious targets prescribed by the IPCC.

Therefore, according to the IPCC targets, the transition to a
carbon-free society goes through an inevitable increase in the renewable
generation’s share in the energy mix and a drastic decrease in the total
consumption of fossil fuels.

###### Assumption 2.

This thesis does not debate or study whether and where renewables should
be implemented.

Renewables have pros and cons and are not carbon-free. We take the
scenarios proposed by organizations such as the IPCC or IEA that
evaluate the relevance of a particular type of renewable energy in the
energy transition pathways. In addition, the development of variable
renewable energies systems results from and produces many interactions
between several actors such as citizens, electricity networks, markets,
and ecosystems ( labussiere2018energy ) . This thesis does not discuss
energy transformations’ political and social aspects and focuses on
integrating renewable energies into an existing interconnected system.

###### Assumption 3.

This thesis studies the integration of renewables in power systems by
investigating forecasting and decision-making tools based on machine
learning.

In contrast to conventional power plants, renewable energy is subject to
uncertainty. Generation technologies based on renewable sources, with
the notable exception of hydro and biomass, are non-dispatchable , i.e.
, their output cannot or can only partly be controlled at the will of
the producer. Their production is stochastic ( morales2013integrating )
and therefore, hard to forecast. A high share of renewables is
challenging for power systems that have been designed and sized for
dispatchable units ( ueckerdt2015analyzing ) . Variable renewable
energies depend on meteorological conditions and thus pose challenges to
the electricity system’s adequacy when conventional capacities are
reduced. Therefore, it is necessary to redefine the flexible power
system features ( impram2020challenges ) .

Machine learning can contribute on all fronts by informing the research,
deployment, and operation of electricity system technologies. High
leverage contributions in power systems include ( rolnick2019tackling )
: accelerating the development of clean energy technologies, improving
demand and clean energy forecasts , improving electricity system
optimization and management , and enhancing system monitoring. This
thesis focuses on two leverages: (1) the supply and demand forecast; (2)
the electricity system optimization and management.

Since variable generation and electricity demand both fluctuate, they
must be forecast ahead of time to inform real-time electricity
scheduling and longer-term system planning. Better short-term forecasts
enable system operators to reduce reliance on polluting standby plants
and proactively manage increasing amounts of variable sources. Better
long-term forecasts help system operators and investors to decide where
and when to build variable plants. Forecasts need to become more
accurate, span multiple horizons in time and space, and better quantify
uncertainty to support these use cases. In this context, probabilistic
forecasts ( gneiting2014probabilistic ) , which aim at modeling the
distribution of all possible future realizations, have become an
important tool to equip decision-makers, hopefully leading to better
decisions in energy applications ( morales2013integrating ;
hong2016probabilistic ; hong2020energy ) .

When balancing electricity systems, system operators use scheduling and
dispatch to determine how much power every controllable generator should
produce. This process is slow and complex, governed by NP-hard
optimization problems ( rolnick2019tackling ) such as unit commitment
and optimal power flow that must be coordinated across multiple time
scales, from sub-second to days ahead. Scheduling becomes even more
complex as electricity systems include more storage, variable
generators, and flexible demand. Indeed, operators manage even more
system components while simultaneously solving scheduling problems more
quickly to account for real-time variations in electricity production.
Thus, scheduling must improve significantly, allowing operators to rely
on variable sources to manage systems.

Therefore, the two main research questions are:

1.  How to produce reliable probabilistic forecasts of renewable
    generation, consumption, and electricity prices?

2.  How to make decisions with uncertainty using probabilistic forecasts
    to improve scheduling?

Modeling tools for energy systems differ in terms of their temporal and
spatial resolutions, level of technical details, simulation horizons. In
particular, two classes of models can be distinguished depending on
their focus: (1) system operation and (2) system design (
collins2017integrating ) . ( limpens2021generating , Chapter 1) depicts
both of these classes.

In the first class, the operational models of energy systems generally
optimize a single sector’s operation, such as the electricity sector,
and do not consider investment costs. They describe the constraints of
the studied system with a high level of accuracy and can model rapid
variations in renewable energy production, forecast errors, or reserve
markets. The energy system must be controlled at any time, which
requires an excellent technical resolution of the components and
accounts for the risks of failure. The system must decide how the load
is shared between the different units below the hour at a short time
horizon. Solving this problem involves accurately representing
production units, such as the power ramps up or load constraints.
Therefore, models will decide which units must be committed to optimally
dispatching the load or generating shortly (hours - days).

In the second class, the long-term planning models generate scenarios
for an energy system’s long-term evolution. They include investments,
optimize the system design over multiple years, and have a lower
technical resolution of operational constraints. With a horizon of one
year, new units can be built in existing sites or former units
modernized. With a longer horizon, the overall system can be changed.
When the horizon is long enough to neglect the existing system, the
models can then optimize the future design of the energy system from
scratch and assess the impact of different long-term policies on the
design of the system.

Each model class is essential and answers different needs to tackle the
energy transition and help integrate renewable energies. This thesis
addresses the first class of models. An example of a thesis
investigating the second one is limpens2021generating .

###### Assumption 4.

This thesis considers the energy management of "small" systems such as
microgrids at a residential scale on a day-ahead basis.

Indeed, the development of microgrids provides an effective way to
integrate renewable energy sources and exploit the available flexibility
in a decentralized manner. Microgrids are small electrical networks
composed of decentralized energy resources and loads controlled locally.
They can be operated either interconnected or in islanded mode. Figure
1.1 depicts a microgrid composed of PV generation, diesel generator
(Genset), storage systems, load, and an energy management system (EMS).
Energy storage is a crucial component for the stable and safe operation
of a microgrid. Storage devices can compensate for the variability of
the renewable energy sources and the load to balance the system. The
reader is referred to zia2018microgrids that proposes a comparative and
critical analysis on decision-making strategies and their solution
methods for microgrid energy management systems.

###### Assumption 5.

This thesis considers the energy management of grid-connected microgrids
on a day-ahead basis.

Therefore, we are interested in producing reliable day-ahead
probabilistic forecasts of renewable generation, consumption, and
electricity prices ¹ ¹ 1 This thesis considers only the imbalance prices
in the Belgian case study. for a microgrid composed of PV or wind
generation and electrical consumption. However, in some specific cases,
this perimeter is not strictly respected. For instance, the sizing of a
grid-connected PV plant with a battery energy storage system is studied
in the specific framework of capacity firming or the day-ahead planning
of an energy retailer.

#### 1.2 Classification of forecasting studies

One of the first works of this thesis was to conduct a literature review
of forecasting studies. The forecasting literature is vast and composed
of thousands of papers, even when selecting a particular field such as
load or PV forecasting. Therefore, a classification into two dimensions
of load forecasting studies is proposed by dumas2018classification to
decide which forecasting tools to use in which case. The approach can be
extended to electricity prices, PV, or wind power forecasting. This
classification aims to provide a synthetic view of the relevant
forecasting techniques and methodologies by forecasting problem. This
methodology is illustrated by reviewing several papers and summarizing
the leading techniques and methodologies’ fundamental principles.

The classification process relies on two parameters that define a
forecasting problem: a temporal couple with the forecasting horizon and
the resolution and a load couple with the system size and the load
resolution. Each article is classified with key information about the
dataset used and the forecasting tools implemented: the forecasting
techniques (probabilistic or deterministic) and methodologies, the data
cleansing techniques, and the error metrics. The process to select the
articles reviewed was conducted into two steps. First, a set of load
forecasting studies was built based on relevant load forecasting reviews
and forecasting competitions. The second step consisted of selecting the
most relevant studies of this set based on the following criteria: the
quality of the description of the forecasting techniques and
methodologies implemented, the description of the results and the
contributions. For the sake of clarity, this manuscript does not detail
this study. It can be read in two passes.

1.  The first one identifies the forecasting problem of interest to
    select the corresponding class into one of the four classification
    tables. Each one references all the articles classified across a
    forecasting horizon. They provide a synthetic view of the
    forecasting tools used by articles addressing similar forecasting
    problems. Then, a second level composed of four Tables summarizes
    key information about the forecasting tools and the results of these
    studies.

2.  The second pass consists of reading the key principles of the main
    techniques and methodologies of interest and the reviews of the
    articles.

#### 1.3 Content and contributions

The manuscript is divided into two main parts: Part I forecasting; Part
II planning and control. Figure 1.2 depicts the thesis skeleton. Part I
provides the forecasting tools and metrics required to produce and
evaluate reliable point and probabilistic forecasts to be used as input
of decision-making models in Part II . The latter proposes approaches
and methodologies based on optimization for decision-making under
uncertainty using probabilistic forecasts on several case studies.

##### Part I content and contributions

The content and main contributions of the forecasting part are:

-   Chapter 2 introduces different types of forecasts to characterize
    the behavior of stochastic variables, such as renewable generation,
    electrical consumption, and electricity prices.

-   Chapter 3 provides the tools to assess the different types of
    forecasts. For predictions in any form, one must differentiate
    between their quality and their value. This Chapter focus on
    forecast quality. Part II addresses the forecast value. An example
    of forecast quality assessment is conducted on PV and electrical
    consumption point forecasts. They are computed using common
    deep-learning models such as recurrent neural networks and used as
    inputs of day-ahead planning in Chapter 9 .
    References: The point forecast quality evaluation is an extract of
    \bibentry dumas2021coordination.

-   Chapter 4 investigates PV quantiles forecasts using deep-learning
    models such as the encoder-decoder architecture. Then, Chapter 12
    uses the PV intraday point and quantiles forecasts as inputs of a
    robust optimization planner.
    References: This chapter is an adapted version of \bibentry
    dumas2020deep.

-   Chapter 5 proposes imbalance prices density forecasts with a
    particular focus on the Belgian case. A two-step density
    forecast-based approach computes the net regulation volume state
    transition probabilities to infer the imbalance prices.
    References: This chapter is an adapted version of \bibentry
    dumas2019probabilistic.

-   Chapter 6 studies the generation of scenarios for renewable
    production and electrical consumption by implementing a recent class
    of deep generative models, normalizing flows. It provides a fair
    comparison of the quality and value of this technique with the
    state-of-the-art deep learning generative models, Variational
    AutoEncoders, and Generative Adversarial Networks. Chapter 13
    assesses the forecast value.
    References: This chapter is an adapted version of \bibentry
    dumas2021nf.

-   Finally, Chapter 7 draws the general conclusions and perspectives of
    Part I .

##### Part Ii content and contributions

The content and main contributions of the planning and control part are:

-   Chapter 8 introduces different types of optimization strategies for
    decision-making under uncertainty: stochastic and robust
    optimization. Then, it presents succinctly two decomposition methods
    to address the two-stage robust non-linear optimization problem: the
    Benders dual-cutting plane and the column and constraints generation
    algorithms.

-   Chapter 9 presents a value function-based approach as a way to
    propagate information from operational planning to real-time
    optimization.
    References: This chapter is an adapted version of \bibentry
    dumas2021coordination.

-   Chapter 10 addresses the energy management, using a stochastic
    approach, of a grid-connected renewable generation plant coupled
    with a battery energy storage device in the capacity firming market.
    This framework has been designed to promote renewable power
    generation facilities in small non-interconnected grids.
    References: This chapter is an adapted version of \bibentry
    dumas2020stochastic.

-   Chapter 11 extends Chapter 10 and proposes a sizing methodology of
    the system.
    References: This chapter is an adapted version of \bibentry
    dumas2020probabilistic.

-   Chapter 12 extends Chapter 10 and investigates the day-ahead
    planning using robust optimization.
    Python code: https://github.com/jonathandumas/capacity-firming-ro
    References: This chapter is an adapted version of \bibentry
    dumas2021probabilistic.

-   Chapter 13 , is the extension of Chapter 6 , and presents the
    forecast value evaluation of the deep learning generative models by
    considering the day-ahead market scheduling of electricity
    aggregators, such as energy retailers or generation companies.
    Python code: https://github.com/jonathandumas/generative-models
    References: This chapter is an adapted version of \bibentry
    dumas2021nf.

-   Finally, Chapter 14 draws the general conclusions and perspectives
    of Part II .

##### Publications

The thesis is mainly based on the following studies, all available in
open-access on arXiv, listed in chronological order:

-    \bibentry
    dumas2018classification
-    \bibentry
    dumas2019probabilistic
-    \bibentry
    dumas2021coordination
-    \bibentry
    dumas2020stochastic
-    \bibentry
    dumas2020deep
-    \bibentry
    dumas2020probabilistic
-    \bibentry
    dumas2021probabilistic
-    \bibentry
    dumas2021nf.

## Part I Forecasting {infobox}

Overview Part I presents the forecasting techniques and metrics required
to produce and evaluate reliable point and probabilistic forecasts to be
used as input of decision-making models in Part II . Then, it
investigates the various types of forecasts in several case studies:
point forecasts, quantile forecasts, prediction intervals, density
forecasts, and scenarios.

  “ I never think of the future — it comes soon enough. ”

  — Albert Einstein

  “ We have two classes of forecasters: Those who don’t know — and those
  who don’t know they don’t know. ”

  — John Kenneth Galbraith

Figure 1.3 illustrates the organization of Part I , which can be read in
two passes depending on the forecasting knowledge.

First, a forecasting practitioner may identify the forecasting type of
interest and select the corresponding Chapter. For instance, Chapter 6
studies the scenarios of renewable generation and electrical
consumption. Second, a forecasting entrant should be interested in
reading Chapters 2 and 3 to acquire the forecasting basics. Then, the
following Chapters are the application of each type of forecast on case
studies.

### Part I general nomenclature

##### Acronyms

##### Variables and parameters

##### Symbols

##### Sets and indices

### Chapter 2 Forecasting background {infobox}

Overview This chapter introduces the main concepts in forecasting as a
background for the work developed in the following chapters of this
manuscript. It presents the various types of forecasts: point forecasts,
quantile forecasts, prediction intervals, density forecasts, and
scenarios. In addition, it provides some knowledge on how to train a
forecasting model.
General textbooks ( morales2013integrating ; hastie2009elements ;
zhang2020dive ; Goodfellow-et-al-2016 ) provide further information for
the interested reader. Two courses on this topic also provide
interesting material: (1) "Renewables in Electricity Markets" ¹ ¹ 1
http://pierrepinson.com/index.php/teaching/ given by professor Pierre
Pinson at the Technical University of Denmark. It covers some basics of
electricity markets, the impact of renewables on markets, participation
of renewable energy producers in electricity markets, and renewable
energy analytics (mainly forecasting); (2) "INFO8010 - Deep Learning" ²
² 2 https://github.com/glouppe/info8010-deep-learning , ULiège, Spring
2021, given by associate professor Gilles Louppe at Liège University. It
covers the foundations and the landscape of deep learning.

  “ If life were predictable it would cease to be life, and be without
  flavor. ”

  — Eleanor Roosevelt

Following morales2013integrating power generation from renewable energy
sources, such as wind and solar, are referred to as stochastic power
generation in this thesis. Electrical consumption and electricity prices
are also modeled as stochastic variables. Predictions of renewable
energy generation, consumption, and electricity prices can be obtained
and presented differently. The choice of a particular kind of forecast
depends on the process characteristics of interest to the decision-maker
and the type of operational problem. The various types of forecasts and
their presentation are introduced in the following, starting from the
most common point forecasts and building up towards the more advanced
products that are probabilistic forecasts and scenarios.

This Chapter is organized as follows. Section 2.1 introduces the point
forecasts. Section 2.2 presents the various types of probabilistic
forecasts. Section 2.3 proposes an abstract formulation of a model-based
forecaster. Section 2.4 provides some knowledge on how to train a
forecasting model. Finally, conclusions are drawn in Section 2.5 .

#### 2.1 Point forecast

Let @xmath be the variable of interest, e.g. , renewable energy
generation, consumption, or electricity prices, measured at time @xmath
, which corresponds to a realization of the random variable @xmath .

###### Definition 2.1.1 (A model-based forecast).

( morales2013integrating , Chapter 2) A (model-based) forecast @xmath is
an estimate of some of the characteristics of the stochastic process
@xmath issued at time @xmath for time @xmath given a model @xmath , with
parameters @xmath and the information set @xmath gathering all data and
knowledge about the processes of interest up to time @xmath , such as
weather forecasts, historical observations, calendars variables, etc .

In the above definition, @xmath is the lead time , sometimes also
referred to as forecast horizon . The ’hat’ symbol expresses that @xmath
is an estimate only. It models the presence of uncertainty both in our
knowledge of the process and inherent to the process itself. The
forecast for time @xmath is conditional on our knowledge of stochastic
process up to time @xmath , including the data used as input to the
forecasting process and the models identified and parameters estimated.
Therefore, a forecaster somewhat makes the crucial assumption that the
future will be like the past.

Forecasts are series of consecutive values @xmath , that is, for
regularly spaced lead times up to the forecast length @xmath . That
regular spacing @xmath is called the temporal resolution of the
forecasts. For instance, when one talks of day-ahead forecasts with an
hourly resolution, forecasts consist of a series gathering predicted
power values for each of the following 24 hours of the next day.

###### Definition 2.1.2 (Point forecast).

( morales2013integrating , Chapter 2) A point forecast @xmath is a
single-valued issued at time @xmath for @xmath , and corresponds to the
conditional expectation of @xmath

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

given @xmath , and the information set @xmath .

A forecast in the form of a conditional expectation translates to
acknowledging the presence of uncertainty, even though it is not
quantified and communicated.

###### Definition 2.1.3 (Multi-output point forecast).

A multi-output point forecast computed at @xmath for @xmath to @xmath is
the vector

  -- -------- -------- -- -------
     @xmath   @xmath      (2.2)
  -- -------- -------- -- -------

Depending on the problem formulation, it can be computed directly as a
vector or as an aggregate of single output forecasts.

#### 2.2 Probabilistic forecasts

In contrast to point predictions, probabilistic forecasts aim at
providing decision-makers with the full information about potential
future outcomes. Let @xmath and @xmath be the probability density
function (PDF) and related cumulative distribution function (CDF) of
@xmath , respectively.

###### Definition 2.2.1 (Probabilistic forecast).

( morales2013integrating , Chapter 2) A probabilistic forecast issued at
time @xmath for time @xmath consists of a prediction of the PDF (or
equivalently, the CDF) of @xmath , or of some summary features.

Various types of probabilistic forecasts have been developed: quantile ,
prediction intervals , scenarios , and density forecasts.

##### 2.2.1 Quantiles

###### Definition 2.2.2 (Quantile forecast).

( morales2013integrating , Chapter 2) A quantile forecast @xmath with
nominal level @xmath is an estimate, issued at time @xmath for time
@xmath of the quantile @xmath for the random variable @xmath

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

given @xmath , and the information set @xmath . Or equivalently @xmath ,
with @xmath the estimated cumulative distribution function of the
continuous random variable @xmath .

By issuing a quantile forecast @xmath , the forecaster tells at time
@xmath that there is a probability @xmath that @xmath will be less than
@xmath at time @xmath . Quantile forecasts are of interest for several
operational problems. For instance, the optimal day-ahead bidding of
wind or PV generation uses quantile forecasts whose nominal level is a
simple function of day-ahead and balancing market prices (
bitar2012bringing ) . Furthermore, quantile forecasts also define
prediction intervals that can be used for robust optimization.

###### Definition 2.2.3 (Multi-output quantile forecast).

A multi-output quantile forecast of length @xmath with nominal level
@xmath computed at @xmath for @xmath to @xmath is the vector

  -- -------- -------- -- -------
     @xmath   @xmath      (2.4)
  -- -------- -------- -- -------

##### 2.2.2 Prediction intervals

Quantile forecasts give probabilistic information in the form of a
threshold level associated with a probability. Even though they may be
of direct use for several operational problems, they cannot provide
forecast users with a feeling about the level of forecast uncertainty
for the coming period. For that purpose, prediction intervals define the
range of values within which the observation is expected to be with a
certain probability, i.e. , its nominal coverage rate ( pinson2007non )
.

###### Definition 2.2.4 (Prediction interval).

( morales2013integrating , Chapter 2) A prediction interval @xmath
issued at @xmath for @xmath , defines a range of potential values for
@xmath , for a certain level of probability @xmath , @xmath . Its
nominal coverage rate is

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

###### Definition 2.2.5 (Central prediction interval).

( morales2013integrating , Chapter 2) A central prediction interval
consists of centering the prediction interval on the median where there
is the same probability of risk below and above the median. A central
prediction interval with a coverage rate of @xmath is estimated by using
the quantiles @xmath and @xmath . Its nominal coverage rate is

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

For instance, central prediction interval with a nominal coverage rate
of 90%, i.e. , @xmath , are defined by quantile forecasts with nominal
levels of 5 and 95%.

###### Definition 2.2.6 (Multi-output central prediction interval).

A multi-output central prediction interval with a coverage rate of
@xmath computed at @xmath for @xmath to @xmath is the vector

  -- -------- -- -------
     @xmath      (2.7)
  -- -------- -- -------

##### 2.2.3 Scenarios

Let us introduce the multivariate random variable

  -- -------- -- -------
     @xmath      (2.8)
  -- -------- -- -------

which gathers the random variables characterizing the stochastic power
generation process for the @xmath following lead times. Hence, it covers
their marginal densities as well as their interdependence structure.

###### Definition 2.2.7 (Scenarios).

Scenarios issued at time @xmath and for a set of @xmath successive lead
times, i.e. , with @xmath consist of a set of @xmath time trajectories

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

The resulting time trajectories comprise scenarios like those commonly
used in stochastic programming.

##### 2.2.4 Density forecasts

All the various types of predictions presented in the above, i.e. ,
point, quantile, and interval forecasts, are only partly describing the
complete information about the future of @xmath at every lead time.
Density forecasts would give this whole information for each point of
time in the future.

###### Definition 2.2.8 (Density forecast).

( morales2013integrating , Chapter 2) A density forecasts @xmath (
@xmath ) issued at time @xmath for @xmath , is a complete description of
the pdf (or cdf) of @xmath conditional on a given model @xmath , and the
information set @xmath .

#### 2.3 Model-based formulation

Let assume the information set @xmath is composed of @xmath independent
and identically distributed samples from the joint distribution @xmath
of two continuous variables @xmath and @xmath . @xmath is the variable
of interest, e.g. , renewable energy generation, consumption, or
electricity prices, and @xmath is the context, e.g. , the weather
forecasts, calendar variables, or exogenous variables. Generically, any
prediction of a random variable @xmath issued at time @xmath , being
point or probabilistic forecast is a linear or nonlinear function of
@xmath . The goal of Part I is to generate multi-output context-based
forecasts @xmath that are distributed under @xmath .

###### Definition 2.3.1 (Multi-output model-based forecasts).

A multi-output model-based forecasts of length @xmath computed by @xmath
at @xmath for @xmath to @xmath is the vector

  -- -------- -------- -- --------
     @xmath   @xmath      (2.10)
  -- -------- -------- -- --------

given the context @xmath , and observations @xmath up to time @xmath .

Its purpose is to generate synthetic but realistic data @xmath whose
distribution is as close as possible to the unknown data distribution
@xmath . When considering point forecasts, quantile forecasts, and
scenarios, @xmath is defined by ( 2.2 ), ( 2.4 ), and ( 2.9 ),
respectively. This abstract formulation is used in Section 3.3 and
Chapters 4 and 6 where the forecasting models and the related inputs
including the context are specified.

#### 2.4 Model training

This section provides the basics of supervised learning that is used in
Part I to train the forecasting models @xmath . It relies mainly on
Lecture 1 of INFO8010 - Deep Learning ³ ³ 3
https://github.com/glouppe/info8010-deep-learning , ULiège, Spring 2021,
from associate professor Gilles Louppe ( louppe2014understanding ) at
Liège University. The interested reader may found interesting materials
in duchesne2021machine and sutera2021importance . They introduce the
different types of machine learning problems, describe their
characteristics, and the procedure to apply supervised learning.

##### 2.4.1 Regression with supervised learning

Consider the unknown joint probability distribution @xmath of two
continuous variables @xmath , e.g. , renewable energy generation, and
@xmath , e.g. , the weather forecasts, introduced in the previous
Section. Let assume some training data @xmath composed of @xmath
independent and identically distributed samples.

Supervised learning is usually concerned with the two following
inference problems: classification and regression . Classification
consists of identifying a decision boundary between objects of distinct
classes. Regression aims at estimating relationships among (usually
continuous) variables. In this thesis, we focus on regression.

###### Definition 2.4.1 (Regression).

Given @xmath , for @xmath we would like to estimate for any new @xmath

  -- -------- -- --------
     @xmath      (2.11)
  -- -------- -- --------

For instance, let assume @xmath is parameterized with a neural network
where the last layer does not contain any final activation. If we make
the assumption that @xmath , we can perform maximum likelihood
estimation to estimate the model’s parameters @xmath that is equivalent
to minimize the negative log likelihood

  -- -------- -------- -- ---------
                          
     @xmath   @xmath      (2.12a)
                          (2.12b)
                          (2.12c)
              @xmath      (2.12d)
  -- -------- -------- -- ---------

which recovers the common squared error loss @xmath that will be used in
Section 3.3 for point forecasting.

##### 2.4.2 Empirical risk minimization

Consider a function @xmath produced by a learning algorithm. The
predictions of this function can be evaluated through a loss @xmath ,
such that @xmath measures how close the prediction @xmath from @xmath
is. For instance, in point forecasting @xmath is the mean squared error
or the pinball loss for quantile forecasting.

The key idea of the model training relies on the empirical risk
minimization . Let @xmath denote the hypothesis space, i.e . the set of
all functions @xmath than can be produced by the chosen learning
algorithm.

###### Definition 2.4.2 (Empirical risk minimization).

( vapnik1992principles ) We are looking for a function @xmath with a
small expected risk

  -- -------- -- --------
     @xmath      (2.13)
  -- -------- -- --------

also called the generalization error.

Therefore, for a given data generating distribution @xmath and for a
given hypothesis space @xmath , the optimal model is

  -- -------- -- --------
     @xmath      (2.14)
  -- -------- -- --------

However, since @xmath is unknown, the expected risk cannot be evaluated
and the optimal model cannot be determined. Nevertheless, if we have
some training data @xmath composed of @xmath independent and identically
distributed samples, we can compute an estimate that is the empirical
risk ( vapnik1992principles ) (or training error)

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

This estimator is unbiased and can be used for finding a good enough
approximation of @xmath , resulting in the empirical risk minimization
principle

  -- -------- -- --------
     @xmath      (2.16)
  -- -------- -- --------

Note: most machine learning algorithms, including neural networks,
implement empirical risk minimization. Under regularity assumptions,
empirical risk minimizers converge: @xmath .

The capacity of a hypothesis space induced by a learning algorithm
intuitively represents the ability to find a suitable model @xmath for
any function, regardless of its complexity. In practice, capacity can be
controlled through hyper-parameters @xmath of the learning algorithm.
Then, the goal is to adjust the capacity of the hypothesis space @xmath
such that the expected risk of the empirical risk minimizer gets as low
as possible. To this end, it is essential to understand the concept of
the bias-variance trade-off ( geman1992neural ) . First, reducing the
capacity makes @xmath fit the data less on average, which increases the
bias term (under-fitting). Second, increasing the capacity makes @xmath
vary a lot with the training data, which increases the variance term
(over-fitting). Therefore, the bias-variance trade-off implies that a
model should balance under-fitting and over-fitting: rich enough to
express underlying structure in data, simple enough to avoid fitting
spurious patterns. It is summarized in the classical U-shaped risk curve
( belkin2019reconciling ) , shown in Figure 2.2 .

##### 2.4.3 The "double descent" curve

In the past few years, several studies have raised questions about the
mathematical foundations of machine learning and their relevance to
practitioners. Indeed, very complex machine learning models such as
neural networks are trained to exactly fit the data. Classically,
following the concept of bias-variance trade-off, these models should be
considered over-fit. However, they often obtain high accuracy on test
data. It is illustrated by Figure 2.3 from belkin2019reconciling that
discuss empirical evidence for the double descent curve.

This study demonstrated that the capacity of the function class does not
necessarily reflect how well the predictor matches the inductive bias
appropriate for the problem at hand. By training models on a range of
real-world datasets and synthetic data, the inductive bias that seems
appropriate is the regularity or smoothness of a function as measured by
a specific function space norm. Then, by considering larger function
classes, which include more candidate predictors compatible with the
data, the training allowed to determine interpolating functions with
smaller norms and are thus "simpler". Therefore, increasing function
class capacity improves the performance of classifiers. This connection
investigated by belkin2019reconciling between the performance and the
structure of machine learning models delineates the limits of classical
analyses. It has implications for both the theory and practice of
machine learning.

##### 2.4.4 Training methodology

In practice, one has a finite dataset of input-output pairs @xmath and
no further information about the joint probability distribution @xmath .
Then, the model is selected by minimizing the empirical risk ( 2.15 )
over the dataset. Therefore, it is typically strongly biased in an
optimistic way and is a bad estimate of the generalization error, also
called the testing error. A "good" model should predict well data
independent from the dataset used for training but drawn from the same
distribution.

##### 2.4.5 Learning, validation, and testing sets

A good practice in machine learning is to use a dedicated part of the
dataset as an independent testing set that is not used to train the
models but only to estimate the generalization error. Ideally, the
dataset @xmath is divided randomly into three parts, as depicted in
Figure 2.4 .

1.  The models are trained on the learning set.

2.  The validation set is used: (1) to evaluate the generalization error
    of the trained models to select among several learning algorithms
    the one more suited to the studied problem; (2) to optimize some
    algorithm’s hyper-parameters @xmath and to avoid over-fitting.

3.  Finally, the testing set is kept until the end of the process. It
    allows assessing the performance of the selected model on
    independent data.

A common rule is to build the training set as large as possible to
obtain good predictors while keeping enough samples in the validation
and testing sets to correctly conduct the hyper-parameters selection and
estimate the generalization error properly.

##### 2.4.6 k-fold cross-validation

When the dataset is composed of only a few months of data, dividing it
into three parts can lead to a too "small" learning set to learn good
predictors regarding both the empirical risk and the generalization
error. In this case, the @xmath -fold cross-validation methodology
allows evaluating the generalization error of the algorithm correctly (
hastie2009elements ) .

It consists of dividing the dataset into @xmath -folds. Then, the model
is trained on @xmath folds, and one fold is left out to evaluate the
testing error. In total, the model is trained @xmath times with @xmath
pairs of learning and testing sets. The generalization error is
estimated by averaging the @xmath errors computed on @xmath testing
sets. This procedure is adopted in Chapters 4 and 5 where the dataset is
composed of only a few months. Figure 2.5 depicts a 5-folds
cross-validation.

However, @xmath -fold cross-validation may suffer from high variability,
which can be responsible for bad choices in model selection and erratic
behavior in the estimated expected prediction error. A study conducted
by bengio2004no demonstrated there is no unbiased estimator of the
variance of @xmath -fold cross-validation. Therefore, the assessment of
the significance of observed differences in cross-validation scores
should be treated with caution.

#### 2.5 Conclusions

This Chapter introduces the forecasting basics with the various types of
forecasts, starting from the most common point forecasts and building up
towards the more advanced probabilistic products: quantiles, prediction
intervals, scenarios, and density forecasts. It also provides some
basics on how to train and evaluate a forecasting model properly. The
next Chapter presents the methodologies to evaluate the quality of these
various types of forecasts.

### Chapter 3 Forecast evaluation {infobox}

Overview This chapter introduces the main concepts in forecasting
evaluation as a background for the work developed in the following
chapters of this manuscript.
General textbooks such as ( morales2013integrating ) provide further
information to the interested reader. In addition, the course
"Renewables in Electricity Markets" ¹ ¹ 1
http://pierrepinson.com/index.php/teaching/ given by professor Pierre
Pinson at the Technical University of Denmark proposes valuable
materials on this topic.

  “ Everything that can be counted does not necessarily count;
  everything that counts cannot necessarily be counted. ”

  — Albert Einstein

For predictions in any form, one must differentiate between their
quality and their value ( morales2013integrating ) . The forecast
quality corresponds to the ability of the forecasts to genuinely inform
of future events by mimicking the characteristics of the processes
involved. The forecast value, studied in Part II , relates to the
benefits of using forecasts in decision-making, such as participation in
the electricity market. Consequently, forecast quality is independent of
the current operational problem while not forecast value. Intuitively,
it is relevant to evaluate the forecast quality before using the
predictions as input to operational problems. Even if a good quality
does not necessarily imply a good forecast value, the quality assessment
provides interesting information about the model at hand.

This Chapter adopts the general framework of morales2013integrating to
provide the basics required to appraise the quality of predictions. It
is organized as follows. Sections 3.1 and 3.2 introduce the metrics for
point and probabilistic forecasting, respectively. Finally, conclusions
are drawn in Section 3.4 .

#### 3.1 Metrics for point forecasts

The base quantity for evaluating point forecasts of a continuous random
variable is the forecast error.

###### Definition 3.1.1 (Forecast error).

( morales2013integrating , Chapter 2) The forecast error @xmath is the
difference between observed and predicted values

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

Note that the forecast error may be normalized so that verification
results can be comparable for different time series. If so,
normalization is most commonly performed by the nominal capacity of the
site of interest. The first error criterion that may be computed is the
bias of the forecasts, which corresponds to the systematic part of the
forecast error. It may be corrected in a straightforward manner using
simple statistical models.

###### Definition 3.1.2 (Bias).

( morales2013integrating , Chapter 2) The bias is the mean of all errors
over the evaluation period of length @xmath , considered indifferently.
For lead time @xmath , it is

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

This summary measure does not tell much about the quality of point
forecasts but only about a systematic error that should be corrected.
Therefore, for a better appraisal of the forecasts, it is advised to use
scores such as the mean absolute and root mean square errors.

###### Definition 3.1.3 (Mean absolute error).

( morales2013integrating , Chapter 2) The mean absolute error (MAE) is
defined as the average of the absolute forecast errors over an
evaluation set of length @xmath

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

###### Definition 3.1.4 (Root mean square error).

( morales2013integrating , Chapter 2) The root mean square error (RMSE)
is defined as the square root of the sum of squared errors over an
evaluation of length @xmath

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

All the above error criteria are independent of the length of the
evaluation set. The nominal capacity of the renewable energy site of
interest can be used to normalize them, and then they are referred to
Nbias, NRMSE, and NMAE.

#### 3.2 Metrics for probabilistic forecasts

##### 3.2.1 Calibration

The first requirement of probabilistic forecasts is to consistently
inform about the probability of events. It leads to the concept of
probabilistic calibration , also referred to as reliability . The
assessment of probabilistic calibration only informs about a form of
bias of probabilistic forecasts. A frequentist approach, based on an
evaluation set of sufficient length, can be performed by assessing the
reliability of each of the defining quantile forecasts using the
indicator variable.

###### Definition 3.2.1 (Indicator variable).

( morales2013integrating , Chapter 2) The indicator variable @xmath ,
for a given quantile forecast @xmath and corresponding realization
@xmath is

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

@xmath is a binary variable indicating if the quantile forecasts cover,
or not, the measurements. The empirical level of quantile forecasts can
be defined, estimated, and eventually compared with their nominal one by
using this indicator variable.

###### Definition 3.2.2 (Empirical level).

( morales2013integrating , Chapter 2) The empirical level @xmath , for a
nominal level @xmath and lead time @xmath , is obtained by calculating
the mean of the @xmath time-series over an evaluation set of length
@xmath

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

The difference between nominal and empirical levels of quantile
forecasts is to be seen as a probabilistic bias. Then, probabilistic
calibration may be appraised visually by using reliability diagrams
plotting empirical vs. nominal levels of the quantiles defining density
forecasts.

##### 3.2.2 Univariate skill scores

Perfectly calibrated probabilistic forecasts do not guarantee that the
forecasts are "good". For instance, they may not discriminate among
situations with various uncertainty levels, while these aspects are
crucial in decision-making. The overall quality of probabilistic
forecasts may be assessed based on skill scores. First, we consider the
univariate skill score that can only assess the quality of the forecasts
with respect to their marginals (time period). Second, we focus on the
multivariate skill score that can directly assess multivariate
scenarios.

###### Continuous ranked probability score

The continuous ranked probability score (CRPS) ( gneiting2007strictly )
is a univariate scoring rule that penalizes the lack of resolution of
the predictive distributions as well as biased forecasts. It is
negatively oriented, i.e. , the lower, the better, and for deterministic
forecasts, it turns out to be the mean absolute error (MAE). Thus, it
can be directly compared to the MAE criterion used for point forecasts
since the CRPS is its generalization in a probabilistic forecasting
framework. The CRPS is used to compare the skill of predictive marginals
for each component of the random variable of interest. In our case, for
the twenty-four time periods of the day.

###### Definition 3.2.3 (CRPS-integral).

( gneiting2007strictly ) The CRPS for predictive densities @xmath and
corresponding measurement @xmath , is calculated over an evaluation set
of length @xmath

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

However, it is not easy to estimate the integral form of the CRPS
defined by ( 3.7 ).

###### Definition 3.2.4 (CRPS-energy).

( gneiting2007strictly ) gneiting2007strictly propose a formulation
called the energy form of the CRPS since it is just the one-dimensional
case of the energy score, defined in negative orientation as follows

  -- -------- -------- -- -------
     @xmath   @xmath      (3.8)
  -- -------- -------- -- -------

where X and X’ are independent random variables with distribution P and
finite first moment, and @xmath is the expectation according to the
probabilistic distribution P.

The CRPS can be computed for quantile forecasts and scenarios. Let
@xmath be the set of @xmath scenarios generated at time @xmath for lead
time @xmath . The estimator of ( 3.8 ) proposed by zamo2018estimation ,
over an evaluation set of length @xmath for lead time @xmath , is

  -- -------- -------- -- -------
     @xmath   @xmath      (3.9)
  -- -------- -------- -- -------

Let @xmath be the set of @xmath quantiles generated at time @xmath for
lead time @xmath . ( 3.8 ) is over an evaluation set of length @xmath
for lead time @xmath

  -- -------- -------- -- --------
     @xmath   @xmath      (3.10)
  -- -------- -------- -- --------

###### Quantile score

The quantile score (QS), also known as the pinball loss , is
complementary to the CRPS. It permits obtaining detailed information
about the forecast quality at specific probability levels, i.e. ,
over-forecasting or under-forecasting, and particularly those related to
the tails of the predictive distribution ( lauret2019verification ) . It
is negatively oriented and assigns asymmetric weights to negative and
positive errors for each quantile.

###### Definition 3.2.5 (Pinball loss).

The pinball loss function for a given quantile @xmath is

  -- -------- -- --------
     @xmath      (3.11)
  -- -------- -- --------

###### Definition 3.2.6 (Quantile score).

The quantile score , over an evaluation set of length @xmath for lead
time @xmath , is

  -- -------- -- --------
     @xmath      (3.12)
  -- -------- -- --------

The quantile score, over an evaluation set of length @xmath for a given
quantile @xmath over all lead times @xmath , is

  -- -------- -- --------
     @xmath      (3.13)
  -- -------- -- --------

###### Interval score

The interval score (IS) ( gneiting2007strictly ) assesses the quality of
central prediction interval forecasts specifically. It rewards narrow
prediction intervals. It penalizes the forecasts where the observation
is outside the interval thanks to the penalty term that depends on
@xmath

###### Definition 3.2.7 (Is).

The interval score , for a central prediction interval with a coverage
rate of @xmath , over an evaluation set of length @xmath and for lead
time @xmath is

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (3.14)
  -- -------- -------- -- --------

Note: the skill scores can be normalized by the total installed capacity
for a renewable generation plant.

##### 3.2.3 Multivariate skill scores

###### Energy score

The energy score (ES) is the most commonly used scoring rule when a
finite number of trajectories represents distributions. It is a
multivariate generalization of the CRPS and has been formulated and
introduced by gneiting2007strictly . The ES is proper and negatively
oriented, i.e. , a lower score represents a better forecast. The ES is
used as a multivariate scoring rule by golestaneh2016generation to
investigate and analyze the spatio-temporal dependency of PV
generations. They emphasize the ES pros and cons. It is capable of
evaluating forecasts relying on marginals with correct variances but
biased means. Unfortunately, its ability to detect incorrectly specified
correlations between the components of the multivariate quantity is
somewhat limited.

###### Definition 3.2.8 (Energy score).

gneiting2007strictly introduced a generalization of the continuous
ranked probability score defined in negative orientation as follows

  -- -------- -------- -- --------
     @xmath   @xmath      (3.15)
  -- -------- -------- -- --------

where and X and X’ are independent random variables with distribution P
and finite first moment, @xmath is the expectation according to the
probabilistic distribution P, and @xmath the Euclidean norm.

The ES is computed following gneiting2008assessing over an evaluation
set of length @xmath as follows

  -- -- -------- -- --------
        @xmath      (3.16)
  -- -- -------- -- --------

Note: when we consider the marginals of @xmath , it is easy to recognize
that ( 3.16 ) is the CRPS.

###### Variogram score

An alternative class of proper scoring rules based on the geostatistical
concept of variograms is proposed by scheuerer2015variogram . They study
the sensitivity of these variogram-based scoring rules to inaccurate
predicted means, variances, and correlations. The results indicate that
these scores are distinctly more discriminative concerning the
correlation structure. Thus, the Variogram score (VS) captures
correlations between multivariate components in contrast to the Energy
score.

###### Definition 3.2.9 (Variogram score).

For a given day @xmath of the testing set and a @xmath -variate
observation @xmath , the Variogram score metric of order @xmath is
formally defined as

  -- -------- -------- -- --------
     @xmath   @xmath      (3.17)
  -- -------- -------- -- --------

where @xmath and @xmath are the @xmath -th and @xmath -th components of
the random vector @xmath distributed according to P for which the @xmath
-th absolute moment exists, and @xmath are non-negative weights. Given a
set of @xmath scenarios @xmath for this given day @xmath , the forecast
variogram @xmath can be approximated @xmath by

  -- -------- -------- -- --------
     @xmath   @xmath      (3.18)
  -- -------- -------- -- --------

Then, the VS is averaged over the testing set of length @xmath

  -- -- -------- -- --------
        @xmath      (3.19)
  -- -- -------- -- --------

In this thesis, we evaluate the Variogram score with equal weights
across all hours of the day @xmath and using a @xmath of 0.5, which for
most cases provides a good discriminating ability as reported in
scheuerer2015variogram .

#### 3.3 Point forecasting evaluation example

This section proposes an example of point forecasting quality
evaluation. It is based on an extract of dumas2021coordination . In
addition, it introduces multi-output weather-based point forecasting by
defining the problem formulation and implementation on a real-case study
to compute PV and electrical consumption forecasts. They are used by a
day-ahead deterministic planner presented in Chapter 9 . Note: PV
intraday point forecast will also be considered in this thesis using an
encoder-decoder architecture developed in dumas2020deep . Chapter 4
details the approach and the results.

##### 3.3.1 Formulation

Point forecasting corresponds to the conditional expectation of the
stochastic process for every lead time ( 2.1 ). This definition is
linked to the so-called loss function @xmath . Loss functions assign a
penalty to forecast errors as a proxy of the cost of these errors for
those making decisions based on such forecasts. There exist various
types of loss functions, such as the mean squared error or the pinball
loss. A special relevant case of a loss function to compute point
forecasts is the mean squared error

  -- -------- -- --------
     @xmath      (3.20)
  -- -------- -- --------

At the time @xmath a point forecast @xmath for time @xmath is the value
of the process such that it minimizes the expected loss for the forecast
user for all potential realizations of the process, given our state of
knowledge at that time. Therefore, when considering multi-output point
forecasts of length @xmath the estimation of the model parameters is
performed by solving

  -- -------- -------- -- ---------
                          
     @xmath   @xmath      (3.21a)
     @xmath   @xmath      (3.21b)
  -- -------- -------- -- ---------

given the information set @xmath of length @xmath , and @xmath the
Euclidean norm.

##### 3.3.2 Forecasting models

Two standard forecasting techniques are implemented to forecast the PV
production and the consumption, with a dedicated model per variable of
interest. The first model uses a Recurrent Neural Network (RNN) of the
Keras Python library ( chollet2015keras ) . The RNN is a Long Short Term
Memory (LSTM) with one hidden layer composed of @xmath neurons with
@xmath the number of input features. The second model is a Gradient
Boosting Regressor (GBR) of the Scikit-learn Python library (
scikit-learn ) . They both use past values of the PV production and
consumption, and the weather forecasts provided by the Laboratory of
Climatology of the Liège University, based on the MAR regional climate
model ( fettweis2017reconstructions ) . It is an atmosphere model
designed for meteorological and climatic research, used for a wide range
of applications, from km-scale process studies to continental-scale
multi-decade simulations. The study dumas2021coordination focuses on the
real-time control of microgrids based on planning that requires a
forecast horizon of a few hours up to a few days. Both models are
trained by solving ( 3.21a ) that becomes

  -- -------- -------- -- ---------
                          
     @xmath   @xmath      (3.22a)
     @xmath   @xmath      (3.22b)
     @xmath   @xmath      (3.22c)
  -- -------- -------- -- ---------

with @xmath the variable to forecast, i.e , PV, consumption, and @xmath
the forecast of the @xmath weather variable, e.g. , direct solar
irradiance, wind speed, air temperature. In the case study considered,
the point forecasts are computed each quarter for the subsequent @xmath
periods with a resolution @xmath minutes. The forecasting process is
implemented by using a rolling forecast methodology where the learning
set is updated every six hours, with a fixed size limited to the week
preceding the forecasts, to maintain a reasonable computation time.

##### 3.3.3 Results

Figure 3.2 illustrates the PV point forecasts of the RNN and GBR models
compared to the observations (black) on a particular day of the testing
set. It is interesting to point out the sudden drop in the PV production
around 10 a.m. that the models do not accurately forecast.

The point forecasts are evaluated each quarter over the entire
forecasting horizon, the next 96 periods, using the Normalized Mean
Absolute Error (NMAE), the Normalized Root Mean Squared Error (NRMSE),
and the Normalized Energy Measurement Error (NEME). The normalizing
coefficients for both the NMAE and NRMSE are the mean of the absolute
values of the PV and consumption over all the simulation data set. The
NEME is the NMAE of the energy summed over the entire forecasting
horizon. Figures 3.4 and 3.4 provide the scores for both GBR and LSTM
models for each quarter, plain lines, and the average over the entire
simulation dataset, dashed lines. On average, the LSTM model yields
slightly smaller average NMAE, NRMSE, and NEME values. However, the
differences are not significant, and the forecast value (see Chapter 9 )
needs to be evaluated to decide which model is the best.

#### 3.4 Conclusions

This Chapter provides the basics of forecast verification to acquire
background knowledge on forecast quality that corresponds to the ability
of the forecasts to genuinely inform of future events by mimicking the
characteristics of the processes involved. In contrast, the forecast
value, investigated in Part II , relates to the interests of using
forecasts in decision-making, such as participation in the electricity
market. However, it is intuitively expected that higher-quality
forecasts will yield better policies and decisions. Therefore, quality
evaluation is complementary to value evaluation and provides an insight
into forecasting model skills. The Chapter illustrates the evaluation
methodology with a multi-output point forecasting model and an
implementation of a real-case study using two standard techniques. The
weather-based forecasting models are trained to compute quarterly
day-ahead PV and electrical consumption point forecasts. Then, the
quality is assessed by computing point forecasts metrics. In the
following Chapters, the quality metrics are used to evaluate the various
probabilistic forecasts on several case studies.

### Chapter 4 Quantile forecasting {infobox}

Overview Overall, the Chapter contributions can be summarized as
follows.

1.  A deep learning-based multi-output quantile architecture computes
    prediction intervals of PV generation on a day-ahead and intraday
    basis. The goal is to implement an improved probabilistic intraday
    forecaster, the encoder-decoder, to benefit from the last PV
    generation observations. This architecture is compared to a
    feed-forward neural network.

2.  The weather forecasts are used to directly take into account the
    impact of the weather forecast updates generated every six hours.

3.  A proper assessment of the quantile forecasts is conducted by using
    a @xmath -fold cross-validation methodology and probabilistic
    metrics. It allows computing average scores over several testing
    sets and mitigating the dependency of the results to specific days
    of the dataset.

4.  Finally, a comparison of deep learning quantile regression models is
    conducted with quantiles derived from deep learning generative
    models.

References: This chapter is an adapted version of the following
publications:
\bibentry dumas2020deep.
\bibentry dumas2021nf.

  “ Prediction is very difficult, especially if it’s about the future. ”

  — Niels Bohr

This Chapter focuses on quantile forecasts that provide probabilistic
information about future renewable power generation, in the form of a
threshold level associated with a probability. This approach is
investigated in dumas2020deep with probabilistic PV forecasters that
exploit recent breakthroughs in the field of data science by using
advanced deep learning structures, such as the encoder-decoder
architecture ( bottieau2019very ) . It is implemented to compute
day-ahead and intraday multi-output PV quantiles forecasts, to
efficiently capture the correlation between time periods, and to be used
as input of a robust optimization model. For instance, to address the
energy management system of a grid-connected renewable generation plant
coupled with a battery energy storage device detailed in
dumas2021probabilistic and Chapter 12 . The case study is composed of PV
production monitored on-site at the University of Liège (ULiège),
Belgium. The weather forecasts from the MAR climate regional model (
fettweis2017reconstructions ) provided by the Laboratory of Climatology
are used as inputs of the deep learning models.

The remainder of this Chapter is organized as follows. Section 4.1
presents the non-parametric quantile forecasting framework. Section 4.2
provides the related work. Section 4.3 details the forecasting
techniques. Section 4.4 describes the case study and presents the
results. Section 4.5 proposes a comparison of the quantile regression
models with quantiles derived from deep learning generative models.
Finally, Section 4.6 summarizes the main findings and highlights ideas
for further work.

#### 4.1 Formulation

Quantile regression ( koenker1978regression ) is one of the most famous
non-parametric approach. It does not assume the shape of the predictive
distributions. It can be implemented with various types of forecasting
techniques, e.g. , neural networks, linear regression, gradient
boosting, or any other regression techniques. The estimation of the
model parameters @xmath to compute multi-output quantile forecasts of
length @xmath is performed by minimizing the pinball loss ( 3.11 ) over
all lead times

  -- -------- -------- -- -------
     @xmath   @xmath      (4.1)
     @xmath   @xmath      (4.2)
  -- -------- -------- -- -------

given the information set @xmath of length @xmath , and a set of @xmath
quantiles.

#### 4.2 Related work

The literature on quantile forecasting is broad. We present a few papers
that have gained our attention in quantile PV probabilistic forecasting.
At the Global Energy Forecasting Competition 2014 (
hong2016probabilistic ) solar forecasts are expressed in the form of 99
quantiles with various nominal proportions between zero and one. The
models are evaluated by using the pinball loss function. This study
summarizes the recent research progress on probabilistic energy
forecasting at that time, and this competition made it possible to
develop innovative techniques. A systematic framework for generating PV
probabilistic forecasts is developed by golestaneh2016very . A
non-parametric density forecasting method based on Extreme Learning
Machine is adopted to avoid restrictive assumptions on the shape of the
forecast densities. A combination of bidirectional Recurrent Neural
Networks (RNNs) with Long Short-Term Memory (LSTM), called Bidirectional
LSTM (BLSTM), is proposed by toubeau2018deep . It has the benefits of
both long-range memory and bidirectional processing. The BLSTM is
trained by minimizing the quantile loss to compute quantile forecasts of
aggregated load, wind and PV generation, and electricity prices on a
day-ahead basis. Finally, an innovative architecture, referred to as
encoder-decoder (ED), is developed by bottieau2019very to generate
reliable predictions of the future system imbalance used for robust
optimization.

#### 4.3 Forecasting models

This Section presents the forecasting techniques implemented to compute
multi-output point and quantile forecasts of PV generation.

##### 4.3.1 Gradient boosting regression (GBR)

Gradient boosting builds an additive model in a forward stage-wise
fashion ( hastie2009elements ) . It allows for the optimization of
arbitrary differentiable loss functions. In each stage, a regression
tree is fit on the negative gradient of the given loss function. The
gradient boosting regressor (GBR) from the Scikit-learn ( scikit-learn )
Python library is trained by minimizing the quantile loss. The learning
rate is set to @xmath , the max depth to 5, and the number of estimators
to 500. There is a GBR model trained per quantile as this library does
not handle a single model for several quantiles.

##### 4.3.2 Multi-layer perceptron (MLP)

A description of the most widely used "vanilla" neural network, the
multi-layer perceptron (MLP), is provided by ( hastie2009elements ) . A
MLP with a single hidden layer is considered for the day-ahead forecasts
and as the benchmark for the intraday forecasts. Note, MLPs with two and
three hidden layers did not provide any significant improvement on the
case study considered. The activation function is the Rectified Linear
Unit (ReLU). The number of neurons of the hidden layer is @xmath , with
@xmath and @xmath the number of neurons of the input and output layers,
respectively. The learning rate is set to @xmath and the number of epoch
to 500 with a batch size of 8. It is implemented using the PyTorch
Python library ( paszke2017automatic ) .

##### 4.3.3 Encoder-decoder (ED)

Several technical information about recent advances in neural networks
is provided by toubeau2018deep ; bottieau2019very . In particular,
recurrent neural networks have shown a high potential in processing and
predicting complex time series with multi-scale dynamics. However, RNNs
are known to struggle in accessing time dependencies more than a few
time steps long due to the vanishing gradient problem. Indeed,
back-propagated errors during the training stage either fades or blows
up over time. Long Short-Term Memory and Gated Recurrent Units networks
tackle this problem by using internal memory cells ( bottieau2019very )
. A neural network composed of a LSTM and feed-forward layers, referred
to as LSTM in the rest of the Chapter, is implemented for the day-ahead
and intraday forecasts. The number of LSTM units is @xmath , and the
number of neurons of the feed-forward layer @xmath .

An innovative architecture referred to as encoder-decoder (
bottieau2019very ) , is composed of two different networks and has
recently shown promising results for translation tasks and speech
recognition applications and imbalance price forecasting. The
encoder-decoder, depicted in Figure 4.2 , processes features from the
past, such as past PV observations, to extract the relevant historical
information that is contained into a reduced vector of fixed dimensions,
based on the last hidden state. Then, the decoder processes this
representation along with the known future information such as weather
forecasts.

A version of the encoder-decoder architecture (ED-1) is implemented with
a LSTM as the encoder and a MLP as the decoder. In a second version
(ED-2), the decoder is a LSTM followed by an additional feed-forward
layer. Both versions of the encoder-decoder are used as intraday
forecasters. In ED-1, the encoder has @xmath units with @xmath the
number of neurons of the encoder input layer, features from the past.
Then, the encoder output is merged with the weather forecasts becoming
the decoder input layer that has @xmath neurons. In ED-2, the decoder
has the same number of cells as the encoder, and the feed-forward layer
is composed of @xmath neurons. The LSTM, ED-1, and ED-2 models are
implemented using the TensorFlow Python library (
tensorflow2015-whitepaper ) . The activation functions are the ReLU. The
learning rate is set to @xmath , the number of epoch to 500 with a batch
size of 64 for the three models.

A sensitivity analysis has been conducted to select the hyperparameters:
number of hidden layers, neurons, epochs, and learning rate. Overall,
increasing the number of hidden layers and neurons increases the model
complexity. It can enhance the accuracy, but only up to a limited number
of layers and neurons due to overfitting issues. In addition, the
hyperparameter solution is closely related to the size of the historical
database ( toubeau2018deep ) . A deep learning model with more
significant hidden layers and neurons requires a large amount of data to
estimate the parameters accurately. In the case study considered, there
are only 157 days of data with a 15 minutes resolution. Thus, we decided
to restrict the number of layers and neurons to select a smaller model
that performs better with the available information.

#### 4.4 The ULiège case study

##### 4.4.1 Case study description

The ULiège case study is composed of a PV generation plant with an
installed capacity of 466.4 kW. The PV generation has been monitored on
a minute basis for 157 days. The data is resampled to 15 minutes. The
set of quantiles is @xmath for both the day-ahead and intraday
forecasts. Numerical experiments are performed on an Intel Core i7-8700
3.20 GHz based computer with 12 physical CPU cores and 32 GB of RAM
running on Ubuntu 18.04 LTS.

##### 4.4.2 Numerical settings

The MAR regional climate model ( fettweis2017reconstructions ) provided
by the Laboratory of Climatology of the Liège University is forced by
GFS (Global Forecast System) to compute weather forecasts on a six hours
basis, four-time gates per day at 00:00, 06:00, 12:00, and 18:00 with a
10-day horizon and a 15 minutes resolution. The solar irradiance and air
temperature at 2 meters are normalized by a standard scaler and used as
inputs to the forecasting models.

A @xmath -fold cross-validation strategy computes average scores over
several testing sets to mitigate the dependency of the results to
specific days of the dataset. The dataset is divided into @xmath parts
of equal length, and there are @xmath possible testing sets @xmath . For
a given testing set @xmath , the models are trained over the @xmath
parts of the dataset. Eleven pairs of fixed lengths of 142 and 15 days
are built. One pair is used to conduct the hyperparameters sensitivity
analysis, and the ten others for testing where the scores are averaged.
The Mean Absolute Error and Root Mean Squared Error are introduced to
evaluate the point forecasts. The MAE, RMSE, CRPS, and IS are normalized
by the PV total installed capacity with NMAE and NRMSE the normalized
MAE and RMSE.

The day-ahead models, MLP, LSTM, and GBR compute forecasts at noon for
the next day. Four intraday time gates are considered at 00:00, 06:00,
12:00, and 18:00. The intraday forecasts of time gate 00:00 are computed
by the day-ahead models using only the weather forecasts. Then, the
following three intraday forecasts are computed by intraday models where
the MLP, ED-1, and ED-2 models use the weather forecasts and the last
three hours of PV generation.

The day-ahead and the first intraday predictions are delivered for the
96 quarters of the next day from 00:00 to 23:45 indexed by time steps
@xmath . The prediction horizons span from 12 to 36 hours, for the
day-ahead gate 12:00, and 0 to 24 hours, for the intraday gate 00:00.
The prediction horizon is cropped to @xmath because the PV generation is
always 0 for time steps @xmath and @xmath on the ULiège case study. The
next three intraday predictions are performed for the 72, 48, and 24
next quarters of the day corresponding to the gates 06:00, 12:00, and
18:00. Therefore, the prediction horizons span from 0 to 18 hours, 0 to
12 hours, and 0 to 6 hours. The intraday forecasting time periods are
@xmath , @xmath , and @xmath . Table 4.1 compares the mean and the
standard deviation of the computation times over the ten learning sets
to train the point and quantile forecast models ¹ ¹ 1 The day-ahead and
intraday LSTM training times are identicals for both point and quantile
forecasts as they only take the weather forecasts as inputs. .

##### 4.4.3 Day-ahead results

Figure 4.2(a) compares the NMAE (plain lines), NRMSE (dashed lines), and
Figure 4.2(b) the CRPS per lead time @xmath of the day-ahead models of
gate 12:00. Table 4.2 provides the mean and standard deviation of the
NMAE, NRMSE, and CRPS. The LSTM achieved the best results for both point
and quantile forecasts. Figures 4.4(a) , 4.4(c) , and 4.4(e) compare the
MLP, LSTM, and GBR day-ahead quantile and point forecasts (black line
named dad 12) of gate 12:00 on @xmath with the observation in red. One
can see that the predicted intervals of the LSTM model better encompass
the actual realizations of uncertainties than the MLP and GBR.

##### 4.4.4 Intraday results

Table 4.3 provides the averaged NMAE, NRMSE, and CRPS per gate of
intraday models. The LSTM achieved the best NMAE and NRMSE for the 06:00
gate, and the ED-1 achieved the best NMAE and NRMSE for the noon gate
and the best CRPS for both gates. Figure 4.4 compares the CRPS per lead
time @xmath of the intraday models. The ED-1 benefits from the last PV
generation observations. Indeed, some CRPS values for both 06:00 and
12:00 gates are below the ones of 00:00 gate. Table 4.4 provides the
interval score of intraday models for 80%, 60%, 40%, and 20% width of
central intervals. The ED-1 model achieved the best results except for
the prediction interval width of 80%, where it is ED-2 and LSTM for the
06:00 and 12:00 gates, respectively. Overall, The LSTM achieved close
results to the ED-1. Figures 4.4(b) , 4.4(d) , and 4.4(f) compare the
ED-1, LSTM, and ED-2 intraday quantile and point forecasts (black line
named intra 6) of 06:00 gate on @xmath with the observation in red.
Generally, one can see that the predicted intervals of ED-1 and LSTM
models better encompass the actual realizations of uncertainties than
ED-2.

##### 4.4.5 Conclusions

An encoder-decoder architecture is implemented on the intraday scale to
produce accurate forecasts. It efficiently captures the contextual
information composed of past PV observations and future weather
forecasts while capturing the temporal dependency between forecasting
periods over the entire forecasting horizon. The models are compared by
using a @xmath -fold cross-validation methodology and quality metrics on
a real case study composed of the PV generation of the parking rooftops
of the Liège University. The best day-ahead model for point and quantile
forecasts is a neural network composed of a LSTM cell and an additional
feed-forward layer. Then, the encoder-architecture composed of a
LSTM-MLP yields accurate and calibrated forecast distributions learned
from the historical dataset compared to the MLP and LSTM-LSTM models for
the intraday point and quantile forecasts. However, the LSTM produced
similar results.

Several extensions are under investigation. First, using a larger
dataset of at least one full year to consider the entire PV seasonality.
Second, developing a PV scenario approach based on the encoder-decoder
architecture.

#### 4.5 Comparison with generative models

This Section proposes a quality evaluation of the normalizing flows
(NFs) and LSTM PV quantiles, used in Chapter 12 for robust optimization,
using the quantile score, the reliability diagram, and the continuous
ranked probability score. Indeed, the two-phase engagement control of
the capacity firming framework requires day-ahead and intraday
top-quality forecasts. The more accurate the forecasts, the better the
planning and the control. To this end, the Normalizing Flows technique
computes quantile day-ahead forecasts compared to a common alternative
technique using a Long Short-Term Memory neural network. The controller
requires intraday point forecasts computed by an encoder-decoder
architecture. NFs are investigated in Section 6.2 of Chapter 6 . Both
NFs and LSTM models use as input the weather forecasts of the MAR
climate regional model provided by the Laboratory of Climatology of the
Liège University ( fettweis2017reconstructions ) . The NFs model
generates day-ahead scenarios, and the quantiles are derived. The LSTM
model computes the quantiles directly and is trained by minimizing the
quantile loss. The set of PV quantiles considered for the assessment is
@xmath .

In this study, the class of Affine Autoregressive flows is implemented ²
² 2 https://github.com/AWehenkel/Normalizing-Flows (
wehenkel2019unconstrained ) . A five-step Affine Autoregressive flow is
trained by maximum likelihood estimation with 500 epochs and a learning
rate of @xmath . The LSTM learning rate is @xmath , the number of epoch
to 500 with a batch size of 64.

Figure 4.6 provides the results for these quality metrics computed over
the entire dataset normalized by the total installed capacity. The NFs
model outperforms the LSTM model with average values of 1.49% and 2.80%
vs. 1.69% and 3.15% for the QS and CRPS, respectively. The NFs quantiles
are also more reliable, as indicated by the reliability diagram. These
results motivate the use of the NFs as they outperform common deep
learning approaches such as LSTM models.

#### 4.6 Conclusions

This Chapter proposes a formulation of the quantile forecasts problem
using deep learning models trained by quantile regression to compute
multi-output PV quantiles. The forecast quality is evaluated on a real
case study composed of the PV generation of the parking rooftops of the
Liège University. In addition, these quantile regression models are
compared to PV quantiles derived from deep learning generative models,
which will be investigated in detail in Chapter 6 . In terms of forecast
quality, the generative models outperform on this case study the
quantile regression models. However, it does not mean they are better in
terms of forecast value which will be assessed in Chapter 12 where a
robust planner uses the PV quantiles in the form of prediction
intervals.

### Chapter 5 Density forecasting of imbalance prices {infobox}

Overview The contributions of this Chapter are two-fold.

1.  A novel two-step probabilistic approach (TSPA) is proposed for
    forecasting the Belgium imbalance prices. The TSPA uses a direct
    forecasting strategy ( taieb2012review ) . It consists of
    forecasting an imbalance price for each quarter of the horizon
    independently from the others, requiring a model per quarter.

2.  It sets a reference for other studies as this subject is rarely
    addressed.

References: This chapter is an adapted version of the following
publication:
\bibentry dumas2019probabilistic.

  “ If you don’t know where you’re going any road will do. ”

  — Lewis Carroll

This Chapter presents the probabilistic forecasting of imbalance prices
methodology developed in dumas2019probabilistic . A novel two-step
probabilistic approach is proposed, with a particular focus on the
Belgian case. The first step consists of computing the net regulation
volume (NRV) state transition probabilities . It is modeled as a matrix
estimated using historical data. This matrix is then used to infer the
imbalance prices. Indeed, the NRV can be related to the level of
reserves activated, and the corresponding marginal prices for each
activation level are published by the Belgian Transmission System
Operator (TSO) one day before electricity delivery. The model is
compared to: (1) a multi-layer perceptron, implemented in a
deterministic setting; (2) the widely used probabilistic technique, the
Gaussian Processes (GP).

This Chapter is organized as follows. Section 5.1 details the related
work. Section 5.2 introduces the novel two-step probabilistic approach
formulation and the assumptions made. Section 5.3 describes the
numerical tests on the Belgian case. Section 5.4 reports the results.
Conclusions are drawn in Section 5.5 . Appendix 5 lists the acronyms,
parameters, and forecasted or computed variables. Finally, Annex 5.6
provides a short reminder of the imbalance market and the Belgian
balancing mechanisms.

#### 5.1 Related work

The progressive, large-scale integration of renewable energy sources has
altered electricity market behavior and increased the electricity price
volatility over the last few years ( de2015negative ; green2010market ;
ketterer2014impact ) . In this context, imbalance price forecasting is
an essential tool the strategic participation in short-term energy
markets. Several studies take into account the imbalance prices as
penalties for deviation from the bids to compute the optimal bidding
strategy ( giannitrapani2016bidding ; pinson2007trading ;
bitar2012bringing ; boomsma2014bidding ) . However, these penalties are
known only a posteriori . A forecast indicating the imbalance prices and
the system position, short or long, with an interval to inform about the
range of potential outcomes is a powerful tool for decision making.
Probabilistic forecasting usually outperforms deterministic models when
used with the appropriate bidding strategies ( pinson2007trading ) .
Whereas the literature on day-ahead electricity forecast models is
extensive, studies about balancing market prices forecast have received
less attention. We recommend three papers related to this topic. First,
a statistical description of imbalance prices for shortage and surplus
is presented by saint2002wind . Second, a combination of classical and
data mining techniques to forecast the system imbalance volume is
addressed by garcia2006forecasting . Finally, a review and benchmark of
time series-based methods for balancing market price forecasting are
proposed by klaeboe2015benchmarking . One-hour and one-day-ahead
forecasts are considered for state determination, balancing volume, and
price forecasting on the Nord Pool price zone NO2 in Norway.

#### 5.2 Formulation

This study focuses on the intraday market time scale that requires a
forecasting horizon from a few minutes to a few hours with a resolution
@xmath . The day-ahead time scale requires forecasts of the imbalance
prices from 12 to 36 hours, which is not realistic at this stage. The
input data are the imbalance price history, the NRV, and the marginal
prices for activation published by the TSO. The probabilistic approach
consists of forecasting the imbalance prices in two steps: computing the
NRV state transition probabilities, then forecasting the imbalance
prices, as depicted in Figure 5.2 . It is motivated by the ELIA
imbalance price mechanisms detailed in Appendix 5.6 .

##### 5.2.1 Net regulation volume forecasting

Let consider the @xmath forecasting horizons @xmath with @xmath the
market period, 15 minutes for Belgium. The NRV historical data is
discretized into @xmath bins, @xmath , centered around @xmath . Note:
this discretization has been determined after a statistical study of the
NRV distribution. The @xmath NRV transition matrices @xmath , of
dimensions @xmath , from a known state at time @xmath to a future state
at time @xmath are estimated by using the NRV historical data, and
referred to as @xmath . They are composed of the following conditional
probabilities @xmath

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

with @xmath the measured NRV at time @xmath , and @xmath @xmath . The
conditional probabilities ( 5.1 ) are estimated statistically over the
learning set (LS) @xmath

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

Figure 5.3 illustrates the matrices @xmath and @xmath with 2017 as
learning set.

The estimated mean @xmath and standard deviation @xmath of the NRV at
time @xmath for @xmath are calculated as follows

  -- -------- -------- -------- -------
     @xmath   @xmath            (5.3)
              @xmath   @xmath   
  -- -------- -------- -------- -------

with @xmath such as @xmath .

##### 5.2.2 Imbalance price forecasting

The NRV can be related to the level of reserves activated and the
corresponding marginal prices for each activation level, published by
the TSO one day before electricity delivery. We thus first forecast the
NRV and its spread among the Gross Upward regulation Volume (GUV) and
Gross Downward regulation Volume (GDV). Then, we forecast the reserve
products activated (contracted or not) to select the most probable MIP
and MDP into the ARC table. Finally, the mean and the standard deviation
of the imbalance price forecast are derived.

However, the ARC table contains only the contracted reserve products.
Most of the time, the first activated reserve products come from the non
contracted International Grid Control Cooperation platform (IGCC-/+),
the contracted secondary reserve (R2-/+) and the non contracted
regulation reserves (Bids-/+) ¹ ¹ 1 More information about the reserve
products is available at http://www.elia.be . . For instance, consider a
quarter of an hour with an NRV of 150 MW, spread into 170 MW of GUV and
20 MW of GDV. Suppose ELIA activated 80 MW of IGCC+ and 90 MW of R2+.
Then, the MIP is given in the marginal activation price of R2+ in the
ARC table at the range @xmath MW. Suppose that ELIA has activated 20 MW
of IGCC+, 20 MW of R2 +, and 130 MW of Bids+. Then, the MIP is given in
the marginal activation price of Bids+. However, this is not a
contracted reserve, and its price is not in the ARC table. Then, it is
more complicated to predict the MIP and consequently the imbalance
prices. Therefore, we introduce several simplifying assumptions
justified by a statistical study on the 2017 ELIA imbalance data.

###### Assumption 6.

The NRV is entirely spread into the GUV if the NRV is positive or GDV if
the NRV is negative.

The mean and standard deviation of the GUV and GDV are @xmath MW vs.
@xmath MW when the NRV is positive, while it is @xmath MW vs. @xmath MW
when the NRV is negative. This assumption enables to select directly in
the ARC table the marginal price for activation corresponding to the
range of activation equal to the NRV, minus IGCC.

###### Assumption 7.

We do not consider the Bids reserve product. Thus, we assume the NRV
spreads over the IGCC and reserve products of the ARC table.

The percentage of Bids reserve product, positive or negative, activated
over each quarter of 2017 is 11.5%.

###### Assumption 8.

The level of activated IGCC reserve product is modeled by a function
@xmath of the NRV.

@xmath assigns for a given value of NRV a range of activation @xmath
into the ARC table. @xmath is the ARC marginal price at @xmath and for
the activation range @xmath , with @xmath . If @xmath falls into the
activation range @xmath , then @xmath is equal to @xmath . Due to the
2017 statistical distribution of the IGCC versus the NRV, @xmath is
defined as follows

  -- -------- -- -------
     @xmath      (5.4)
  -- -------- -- -------

The mean and standard deviation (MW) of the IGCC+ and IGCC- are

  -- -------- --
     @xmath   
  -- -------- --

Generally, ELIA first tries to activate the IGCC product to balance the
system. However, when the system imbalance is too high other reserve
products are required.

###### Assumption 9.

The positive imbalance price is equal to the negative one.

The positive and negative imbalance means prices are @xmath and @xmath
€/MWh. They are different @xmath % of the time, but the NMAE and NRMSE
are @xmath and @xmath %. Indeed, the positive and negative prices differ
only by a small correction parameter if the system imbalance is greater
than 140 MW, cf. Appendix 5.6.2 .

Under these assumptions, the estimated mean @xmath and standard
deviation @xmath of the imbalance prices at time @xmath for @xmath are
calculated as follows

  -- -------- -------- -------- -------
     @xmath   @xmath            (5.5)
              @xmath   @xmath   
  -- -------- -------- -------- -------

with @xmath such as @xmath . Finally, on a quarterly basis a forecast is
issued at time @xmath and composed of a set of @xmath couples @xmath .

#### 5.3 Case study

This approach is compared to a widely used probabilistic technique, the
Gaussian Processes, and a "classic" deterministic technique, a
Multi-Layer Perceptron (MLP). Both techniques are implemented using the
Scikit-learn Python library ( scikit-learn ) . The GP uses Matérn,
constant and white noise kernels. The MLP has one hidden layer composed
of @xmath neurons with @xmath input features. The dataset comprises the
2017 and 2018 historical Belgium imbalance price and NRV, available on
Elia’s website. Both the MLP and GP models forecast the imbalance prices
based on the previous twenty-four hours of NRV and imbalance prices,
representing a total of @xmath input features. The MLP uses a
Multi-Input Multi-Output (MIMO) strategy, and the GP a Direct strategy (
taieb2012review ) ² ² 2 GP regression with multiple outputs is
non-trivial and still a field of active research ( wang2015gaussian ;
liu2018remarks ) . . The Direct strategy consists of training a model
@xmath per market period

  -- -------- -- -------
     @xmath      (5.6)
  -- -------- -- -------

and the forecast is composed of the @xmath predicted values computed by
the @xmath models @xmath . In contrast, the MIMO strategy consists of
training only one model @xmath to directly compute the @xmath values of
the variable of interest

  -- -------- -- -------
     @xmath      (5.7)
  -- -------- -- -------

For both MIMO and Direct strategies, the forecast is computed quarterly
and composed of @xmath values. The forecasting process uses a rolling
forecast strategy where the training set is updated every month,
depicted in Figure 5.4 . Its size increases by one month for both the
MLP and TSPA techniques, with the initial training set being 2017.
However, it is limited to the month preceding the forecast for the GP
technique to maintain a reasonable computation time. The validation set
is 2018, where each month is forecasted by a model trained on a
different learning set.

#### 5.4 Results

The probabilistic forecasts are evaluated using the Pinball Loss
Function (PLF) and the Continuous Ranked Probability Score. They are
compared to the deterministic metrics with the Normalized Mean Absolute
Error (NMAE), and the Normalized Root Mean Squared Error (NRMSE) of the
mean predicted imbalance prices. The scores NMAE @xmath , NRMSE @xmath ,
PLF @xmath , and CRPS @xmath for a lead time @xmath are computed over
the entire validation set. The normalizing coefficient for both the NMAE
and NRMSE is 55.02 € @xmath , the mean absolute value of the imbalance
prices over 2018.

The forecaster computes the mean and standard deviation of the imbalance
prices per market period. Then, a Gaussian distribution generates
samples, and the percentiles @xmath are derived. They are used to
compute the PLF. The CRPS is computed using the crps_gaussian function
of the Python properscoring ³ ³ 3
https://pypi.org/project/properscoring/ library. Table 5.1 presents the
average scores over all lead times @xmath for the horizons of 15, 60 and
360 minutes. Figure 5.4(a) provides the average scores over all lead
times @xmath for each forecasting horizon, and Figure 5.4(b) depicts the
score per lead time @xmath for the forecasting horizon of 360 minutes.

Two days, depicted in Figure 5.6 , from the validation set are selected
to illustrate the results. On @xmath , the ELIA system was short on
average, leading to a high NRV and imbalance prices. On @xmath , the
ELIA system was alternatively short and long leading to fluctuating NRV
and imbalance prices. The 15 minutes horizon forecasts are depicted in
Figure 5.7 , where only the last forecasted value for each quarter is
shown. The 60 and 360 minutes horizon forecasts are depicted in Figure
5.8 in Appendix 5.6.3 . On @xmath the GP provides better results on
average as it accurately follows the actual prices. On @xmath , there is
no clear winner. Other figures are reported in Appendix 5.6.3 for other
forecasting horizons. The MLP provides the best NMAE and NRMSE, except
for the horizon of 360 minutes, and the TSPA the best CRPS and PLF
scores for the three forecasting horizons considered. However, to select
the best forecasting model, it would be necessary to measure the
accuracy of the global bidding chain composed of the forecasting and
decision-making modules.

#### 5.5 Conclusions

This study addressed the problem of forecasting the imbalance prices in
a probabilistic framework. The novel two-step probabilistic approach
consists of computing the net regulation volume state transition
probabilities. Then, it infers the imbalance price from the ELIA ARC
table and computes a probabilistic forecast. It is compared to the MLP
and GP techniques in the Belgium case. The results indicate it
outperforms them using probabilistic metrics, but it is less accurate at
predicting the precise imbalance prices.

Learning models could improve this novel probabilistic approach in three
directions: (1) To avoid making our simplifying assumptions. (2) By
adding input features to describe the market situation better. (3)
Extending the approach to implement the whole bidding strategy chain
would allow determining which approach is the best.

#### Appendix: notation

##### Acronyms

##### Parameters

##### Forecasted or computed variables

#### 5.6 Appendix: balancing mechanisms

##### 5.6.1 Balancing mechanisms

A balancing mechanism aims to balance a given geographical area and
control sudden imbalances between injection and off-take. Generally,
this mechanism relies on exchanges with neighboring TSOs, the
responsible balance parties, and the usage of reserve capacities. Each
party that desires to inject or off-take to the grid must be managed by
a Balancing Responsible Party (BRP). The BRP is responsible for
balancing all off-takes and injections within its customer’s portfolio.
The TSO applies an imbalance tariff when it identifies an imbalance
between total physical injections, imports, and purchases on the one
hand and total off-takes, exports, and sales on the other. When the BRPs
cannot balance their customer’s portfolios, the TSO activates reserves
to balance the control area. These reserves are mainly from conventional
power plants, which can be quickly activated upward or downward to cover
real-time system imbalances. The main types of reserve are the Frequency
Containment Reserve (FCR), the Automatic Frequency Restoration Reserve
(aFRR), the Manual Frequency Restoration Reserve (mFRR), and the
Replacement Reserve (RR) ⁴ ⁴ 4 https://www.entsoe.eu/ . The activation
of these reserves results from a merit order representing the activation
cost of reserve capacity. If the system faces a power shortage, the TSO
activates upward reserves that result in a positive marginal price on
the reserve market. Then, the TSO pays the Balancing Service Provider.
The cost of this activation is transferred to the BRPs. BRPs facing
short positions are reinforcing the system imbalance. They must pay the
marginal price to the TSO. BRPs facing long positions are restoring the
system imbalance. They receive the marginal price from the TSO. This
mechanism incentives market players to maintain their portfolios in
balance, as well as to reduce the net system imbalance.

##### 5.6.2 Belgium balancing mechanisms

This section describes the ELIA imbalance price mechanisms and the data
publication part of the TSPA inputs. On a 15 minutes basis, the NRV is
defined as the sum of the GUV and GDV. The Gross Upward Volume (GUV) is
the sum of the volumes of all upward regulations. The Gross Downward
Volume (GDV) is the sum of the volumes of all downward regulations. If
the NRV is positive, the highest price of all upward activated products,
the Marginal price for Upward Regulation (MIP), is applied for the
imbalance price calculation. If the NRV is negative, the lowest price of
all downward activated products, the Marginal price for Downward
Regulation (MDP), is applied. The definitions of the positive @xmath and
negative @xmath imbalance prices are provided in Table 5.2 . The
correction parameters @xmath and @xmath are zero when the system
imbalance is lower than 140 MW and proportional to it when greater than
140 MW.

The MIP and MDP prices are in the third Available Regulation Capacity
(ARC) table most of the time. The ARC publication considers the
applicable merit order, i.e. , the order in which Elia must activate the
reserve products. Then, the volumes are ranked by activation price
(cheapest first). The marginal price is the highest price for every
extra MW upward volume and the lowest for every extra MW downward
volume. The ARC table, showing the activation price of the contracted
reserves per activation range of 100 MW, displays the estimated
activation price considering a certain NRV. For a given quarter-hour
@xmath there are @xmath marginal prices for activation @xmath , @xmath ,
each one of them related to the activation range @xmath . @xmath is
equal to 22 with 11 negatives ranges and 11 positives ranges. The first
activation range, @xmath , corresponds to the interval @xmath MW, the
second one to @xmath , …, @xmath , @xmath up to @xmath . The data of day
@xmath are published on @xmath at 6 pm based on the nomination of
day-ahead and intraday programs and bids submitted by the concerned
parties. The values of each quarter-hour of the day are refreshed
quarterly. Therefore, the published values are an estimation. However,
they are likely to include the MIP and MDP prices. At the condition to
determine the NRV and its spread between the GUV and GDV. The TSPA takes
as input the third ARC table to determine the most probable MIP and MDP
prices.

##### 5.6.3 Additional results

### Chapter 6 Scenarios {infobox}

Overview The main contributions of this Chapter are three-fold:

1.  We present to the power systems community a recent class of deep
    learning generative models: the Normalizing Flows (NFs). Then, we
    provide a fair comparison of this technique with the
    state-of-the-art deep learning generative models, Generative
    Adversarial Networks (GANs) and Variational AutoEncoders (VAEs). It
    uses the open data of the Global Energy Forecasting Competition 2014
    (GEFcom 2014) ( hong2016bprobabilistic ) . To the best of our
    knowledge, it is the first study that extensively compares NFs,
    GANs, and VAEs on several datasets, PV generation, wind generation,
    and load with a proper assessment of the quality and value based on
    complementary metrics, and an easily reproducible case study;

2.  We implement conditional generative models to compute improved
    weather-based PV, wind power, and load scenarios. In contrast to
    most of the previous studies that focused mainly on past
    observations;

3.  Overall, we demonstrate that NFs are more accurate in quality and
    value, providing further evidence for deep learning practitioners to
    implement this approach in more advanced power system applications.

This study provides open-access to the Python code :
https://github.com/jonathandumas/generative-models .

References: This chapter is an adapted version of the following
publication:
\bibentry dumas2021nf.

  “ I won’t say ’See you tomorrow’ because that would be like predicting
  the future, and I’m pretty sure I can’t do that. ”

  — Ludwig Wittgenstein

#### 6.1 Introduction

This Chapter focuses on scenario generation , a popular probabilistic
forecasting method to capture the uncertainty of load, photovoltaic (PV)
generation, and wind generation. It consists of producing sequences of
possible load or power generation realizations for one or more
locations.

Forecasting methodologies can typically be classified into two groups:
statistical and machine learning models. On the one hand, statistical
approaches are more interpretable than machine learning techniques,
sometimes referred to as black-box models. On the other hand, they are
generally more robust, user-friendly, and successful in addressing the
non-linearity in the data than statistical techniques. We provide in the
following a few examples of statistical approaches. More references can
be found in khoshrou2019short and mashlakov2021assessing .

Multiple linear regression models ( wang2016electric ) and
autoregressive integrated moving average ( de200625 ) are among the most
fundamental and widely-used models. The latter generates spatiotemporal
scenarios with given power generation profiles at each renewables
generation site ( morales2010methodology ) . These models mostly learn a
relationship between several explanatory variables and a dependent
target variable. However, they require some expert knowledge to
formulate the relevant interaction between different variables.
Therefore, the performance of such models is only satisfactory if the
dependent variables are well formulated based on explanatory variables.
Another class of statistical approaches consists of using simple
parametric distributions, e.g. , the Weibull distribution for wind speed
( karaki2002probabilistic ) , or the beta distribution for solar
irradiance ( karaki1999probabilistic ) to model the density associated
with the generative process. In this line, the (Gaussian) copula method
has been widely used to model the spatial and temporal characteristics
of wind ( pinson2009probabilistic ) and PV generation (
zhang2019coordinated ) . For instance, the problem of generating
probabilistic forecasts for the aggregated power of a set of renewable
power plants harvesting different energy sources is addressed by
camal2019scenario .

Overall, these approaches usually make statistical assumptions
increasing the difficulty to model the underlying stochastic process.
The generated scenarios approximate the future uncertainty but cannot
correctly describe all the salient features in the power output from
renewable energy sources. Deep learning is one of the newest trends in
artificial intelligence and machine learning to tackle the limitations
of statistical methods with promising results across various application
domains.

##### 6.1.1 Related work

Recurrent neural networks (RNNs) are among the most famous deep learning
techniques adopted in energy forecasting applications. A novel
pooling-based deep recurrent neural network is proposed by shi2017deep
in the field of short-term household load forecasting. It outperforms
statistical approaches such as autoregressive integrated moving average
and classical RNN. A tailored forecasting tool, named encoder-decoder,
is implemented in dumas2020deep to compute intraday multi-output PV
quantiles forecasts. Guidelines and best practices are developed by
hewamalage2020recurrent for forecasting practitioners on an extensive
empirical study with an open-source software framework of existing RNN
architectures. In the continuity, toubeau2018deep implemented a
bidirectional long short-term memory (BLSTM) architecture. It is trained
using quantile regression and combined with a copula-based approach to
generate scenarios. A scenario-based stochastic optimization case study
compares this approach to other models regarding forecast quality and
value. Finally, salinas2020deepar trained an autoregressive recurrent
neural network on several real-world datasets. It produces accurate
probabilistic forecasts with little or no hyper-parameter tuning.

Deep generative modeling is a class of techniques that trains deep
neural networks to model the distribution of the observations. In recent
years, there has been a growing interest in this field made possible by
the appearance of large open-access datasets and breakthroughs in both
general deep learning architectures and generative models. Several
approaches exist such as energy-based models, variational autoencoders,
generative adversarial networks, autoregressive models, normalizing
flows, and numerous hybrid strategies. They all make trade-offs in terms
of computation time, diversity, and architectural restrictions. We
recommend two papers to get a broader knowledge of this field. (1) The
comprehensive overview of generative modeling trends conducted by
bond2021deep . It presents generative models to forecasting
practitioners under a single cohesive statistical framework. (2) The
thorough comparison of normalizing flows, variational autoencoders, and
generative adversarial networks provided by ruthotto2021introduction .
It describes the advantages and disadvantages of each approach using
numerical experiments in the field of computer vision. In the following,
we focus on the applications of generative models in power systems.

In contrast to statistical approaches, deep generative models such as
Variational AutoEncoders (VAEs) ( kingma2013auto ) and Generative
Adversarial Networks (GANs) ( goodfellow2014generative ) directly learn
a generative process of the data. They have demonstrated their
effectiveness in many applications to compute accurate probabilistic
forecasts, including power system applications. They both make
probabilistic forecasts in the form of Monte Carlo samples that can be
used to compute consistent quantile estimates for all sub-ranges in the
prediction horizon. Thus, they cannot suffer from the issue raised by
ordiano2020probabilistic on the non-differentiable quantile loss
function. Note: the generative models such as GANs and VAEs allow
directly generating scenarios of the variable of interest. In contrast
with methods that first compute weather scenarios to generate
probabilistic forecasts such as implemented by sun2020probabilistic and
khoshrou2019short . A VAE composed of a succession of convolutional and
feed-forward layers is proposed by zhanga2018optimized to capture the
spatial-temporal complementary and fluctuant characteristics of wind and
PV power with high model accuracy and low computational complexity. Both
single and multi-output PV forecasts using a VAE are compared by
dairi2020short to several deep learning methods such as LSTM, BLSTM,
convolutional LSTM networks, and stacked autoencoders. The VAE
consistently outperformed the other methods. A GAN is used by
chen2018model to produce a set of wind power scenarios that represent
possible future behaviors based only on historical observations and
point forecasts. This method has a better performance compared to
Gaussian Copula. A Bayesian GAN is introduced by chen2018bayesian to
generate wind and solar scenarios, and a progressive growing of GANs is
designed by yuan2021multi to propose a novel scenario forecasting
method. In a different application, a GAN is implemented for building
occupancy modeling without prior assumptions ( chen2018building ) .
Finally, a conditional version of the GAN using several labels
representing some characteristics of the demand is introduced by
lan2018demand to output power load data considering demand response
programs.

Improved versions of GANs and VAEs have also been studied in the context
of energy forecasting. The Wasserstein GAN enforces the Lipschitz
continuity through a gradient penalty term (WGAN-GP), as the original
GANs are challenging to train and suffer from mode collapse and
over-fitting. Several studies applied this improved version in power
systems: (1) a method using unsupervised labeling and conditional
WGAN-GP models the uncertainties and variation in wind power (
zhang2020typical ) ; (2) a WGAN-GP models both the uncertainties and the
variations of the load ( wang2020modeling ) ; (3) jiang2021day
implemented scenario generation tasks both for a single site and for
multiple correlated sites without any changes to the model structure.
Concerning VAEs, they suffer from inherent shortcomings, such as the
difficulties of tuning the hyper-parameters or generalizing a specific
generative model structure to other databases. An improved VAE is
proposed by qi2020optimal with the implementation of a @xmath
hyper-parameter into the VAE objective function to balance the two parts
of the loss. This improved VAE is used to generate PV and power
scenarios from historical values.

However, most of these studies did not benefit from conditional
information such as weather forecasts to generate improved PV power,
wind power, and load scenarios. In addition, to the best of our
knowledge, only ge2020modeling compared NFs to these techniques for the
generation of daily load profiles. Nevertheless, the comparison only
considers quality metrics, and the models do not incorporate weather
forecasts.

##### 6.1.2 Research gaps and scientific contributions

This study investigates the implementation of Normalizing Flows (
rezende2015variational , NFs) in power system applications. NFs define a
new class of probabilistic generative models. They have gained
increasing interest from the deep learning community in recent years. A
NF learns a sequence of transformations, a flow , from a density known
analytically, e.g. , a Normal distribution, to a complex target
distribution. In contrast to other deep generative models, NFs can
directly be trained by maximum likelihood estimation. They have proven
to be an effective way to model complex data distributions with neural
networks in many domains. First, speech synthesis ( oord2018parallel ) .
Second, fundamental physics to increase the speed of gravitational wave
inference by several orders of magnitude ( green2021complete ) or for
sampling Boltzmann distributions of lattice field theories (
albergo2021introduction ) . Finally, in the capacity firming framework
by dumas2021probabilistic .

This present work goes several steps further than ge2020modeling that
demonstrated the competitiveness of NFs regarding GANs and VAEs for
generating daily load profiles. First, we study the conditional version
of these models to demonstrate that they can handle additional
contextual information such as weather forecasts or geographical
locations. Second, we extensively compare the model’s performances both
in terms of forecast value and quality. The forecast quality corresponds
to the ability of the forecasts to genuinely inform of future events by
mimicking the characteristics of the processes involved. The forecast
value relates to the benefits of using forecasts in decision-making,
such as participation in the electricity market. Third, we consider PV
and wind generations in addition to load profiles. Finally, in contrast
to the affine NFs used in their work, we rely on monotonic
transformations, which are universal density approximators (
huang2018neural ) .

Given that Normalizing Flows are rarely used in the power systems
community despite their potential, our main aim is to present this
recent deep learning technique and demonstrate its interest and
competitiveness with state-of-the-art generative models such as GANs and
VAEs on a simple and easily reproducible case study. The research gaps
motivating this study are three-fold:

1.  To the best of our knowledge, only ge2020modeling compared NFs to
    GANs and VAEs for the generation of daily load profiles.
    Nevertheless, the comparison is only based on quality metrics, and
    the models do not take into account weather forecasts;

2.  Most of the studies that propose or compare forecasting techniques
    only consider the forecast quality such as ge2020modeling ,
    sun2020probabilistic , and mashlakov2021assessing ;

3.  The conditional versions of the models are not always addressed,
    such as in ge2020modeling . However, weather forecasts are essential
    for computing accurate probabilistic forecasts.

With these research gaps in mind, the main contributions of this Chapter
are three-fold:

1.  We provide a fair comparison both in terms of quality and value with
    the state-of-the-art deep learning generative models, GANs and VAEs,
    using the open data of the Global Energy Forecasting Competition
    2014 (GEFcom 2014) ( hong2016bprobabilistic ) . To the best of our
    knowledge, it is the first study that extensively compares the NFs,
    GANs, and VAEs on several datasets, PV generation, wind generation,
    and load with a proper assessment of the quality and value based on
    complementary metrics, and an easily reproducible case study;

2.  We implement conditional generative models to compute improved
    weather-based PV, wind power, and load scenarios. In contrast to
    most of the previous studies that focused mainly on past
    observations;

3.  Overall, we demonstrate that NFs are more accurate in quality and
    value, providing further evidence for deep learning practitioners to
    implement this approach in more advanced power system applications.

In addition to these contributions, this study also provides open-access
to the Python code ¹ ¹ 1
https://github.com/jonathandumas/generative-models to help the community
to reproduce the experiments. Figure 6.2 provides the framework of the
proposed method and Table 6.1 presents a comparison of the present study
to three state-of-the-art papers using deep learning generative models
to generate scenarios.

##### 6.1.3 Applicability of the generative models

Probabilistic forecasting of PV, wind generation, electrical
consumption, and electricity prices plays a vital role in renewable
integration and power system operations. The deep learning generative
models presented in this Chapter can be integrated into practical
engineering applications. We present a non-exhaustive list of five
applications in the following. (1) The forecasting module of an energy
management system (EMS) ( silva2021optimal ) . Indeed, EMSs are used by
several energy market players to operate various power systems such as a
single renewable plant, a grid-connected or off-grid microgrid composed
of several generations, consumption, and storage devices. An EMS is
composed of several key modules: monitoring, forecasting, planning,
control, etc . The forecasting module aims to provide the most accurate
forecast of the variable of interest to be used as inputs of the
planning and control modules. (2) Stochastic unit commitment models that
employ scenarios to model the uncertainty of weather-dependent
renewables. For instance, the optimal day-ahead scheduling and dispatch
of a system composed of renewable plants, generators, and electrical
demand are addressed by camal2019scenario . (3) Ancillary services
market participation. A virtual power plant aggregating wind, PV, and
small hydropower plants is studied by camal2019scenario to optimally bid
on a day-ahead basis the energy and automatic frequency restoration
reserve. (4) More generally, generative models can be used to compute
scenarios for any variable of interest, e.g. , energy prices, renewable
generation, loads, water inflow of hydro reservoirs, as long as data are
available. (5) Finally, quantiles can be derived from scenarios and used
in robust optimization models such as in the capacity firming framework
( dumas2021probabilistic ) .

##### 6.1.4 Organization

The remainder of this Chapter is organized as follows. Section 6.2
presents the generative models implemented: NFs, GANs, and VAEs. Section
6.3 provides the quality and assessment methodology. Section 6.5 details
empirical results on the GEFcom 2014 dataset, and Section 6.6 summarizes
the main findings and highlights ideas for further work. Chapter 13 in
Part II provides the forecast value assessment. Appendix 6.7 presents
the justifications of Table 6.1 , and Appendix 6.8 provides additional
information on the generative models.

#### 6.2 Background

This section formally introduces the conditional version of NFs, GANs,
and VAEs implemented in this study. We assume the reader is familiar
with the neural network’s basics. However, for further information,
goodfellow2016deep ; zhang2020dive provide a comprehensive introduction
to modern deep learning approaches.

##### 6.2.1 Multi-output forecasts using a generative model

Let us consider some dataset @xmath of @xmath independent and
identically distributed samples from the joint distribution @xmath of
two continuous variables @xmath and @xmath . @xmath being the wind
generation, PV generation, or load, and @xmath the weather forecasts.
They are both composed of @xmath periods per day, with @xmath and @xmath
. The goal of this work is to generate multi-output weather-based
scenarios @xmath that are distributed under @xmath .

A generative model is a probabilistic model @xmath , with parameters
@xmath , that can be used as a generator of the data. Its purpose is to
generate synthetic but realistic data @xmath whose distribution is as
close as possible to the unknown data distribution @xmath . In our
application, it computes on a day-ahead basis a set of @xmath scenarios
at day @xmath for day @xmath

  -- -------- -------- -- -------
     @xmath   @xmath      (6.1)
  -- -------- -------- -- -------

For the sake of clarity, we omit the indexes @xmath and @xmath when
referring to a scenario @xmath in the following.

##### 6.2.2 Deep generative models

Figure 6.3 provides a high-level comparison of three categories of
generative models considered in this Chapter: Normalizing Flows,
Generative Adversarial Networks, and Variational AutoEncoders.

###### Normalizing flows

A normalizing flow is defined as a sequence of invertible
transformations @xmath , @xmath , composed together to create an
expressive invertible mapping @xmath . This composed function can be
used to perform density estimation, using @xmath to map a sample @xmath
onto a latent vector @xmath equipped with a known and tractable
probability density function @xmath , e.g. , a Normal distribution. The
transformation @xmath implicitly defines a density @xmath that is given
by the change of variables

  -- -------- -------- -- -------
     @xmath   @xmath      (6.2)
  -- -------- -------- -- -------

where @xmath is the Jacobian of @xmath regarding @xmath . The model is
trained by maximizing the log-likelihood @xmath of the model’s
parameters @xmath given the dataset @xmath . For simplicity let us
assume a single-step flow @xmath to drop the index @xmath for the rest
of the discussion.

In general, @xmath can take any form as long as it defines a bijection.
However, a common solution to make the Jacobian computation tractable in
( 6.2 ) consists of implementing an autoregressive transformation (
kingma2016improving ) , i.e. , such that @xmath can be rewritten as a
vector of scalar bijections @xmath

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (6.3a)
     @xmath   @xmath      (6.3b)
     @xmath   @xmath      (6.3c)
     @xmath   @xmath      (6.3d)
  -- -------- -------- -- --------

where @xmath is partially parameterized by an autoregressive conditioner
@xmath with parameters @xmath , and @xmath the union of all parameters
@xmath .

There is a large choice of transformers @xmath : affine, non-affine,
integration-based, etc . This work implements an integration-based
transformer by using the class of Unconstrained Monotonic Neural
Networks (UMNN) ( wehenkel2019unconstrained ) . It is a universal
density approximator of continuous random variables when combined with
autoregressive functions. The UMNN consists of a neural network
architecture that enables learning arbitrary monotonic functions. It is
achieved by parameterizing the bijection @xmath as follows

  -- -------- -------- -- -------
     @xmath   @xmath      (6.4)
  -- -------- -------- -- -------

where @xmath is the integrand neural network with a strictly positive
scalar output, @xmath an embedding made by the conditioner, and @xmath a
neural network with a scalar output. The forward evaluation of @xmath
requires solving the integral ( 6.4 ) and is efficiently approximated
numerically by using the Clenshaw-Curtis quadrature. The pseudo-code of
the forward and backward passes is provided by wehenkel2019unconstrained
.

papamakarios2017masked ’s Masked Autoregressive Network (MAF) is
implemented to simultaneously parameterize the @xmath autoregressive
embeddings @xmath of the flow ( 6.3 ). Then, the change of variables
formula applied to the UMMN-MAF transformation results in the following
log-density when considering weather forecasts

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (6.5a)
              @xmath      (6.5b)
  -- -------- -------- -- --------

that can be computed exactly and efficiently with a single forward pass.
The UMNN-MAF approach implemented is referred to as NF in the rest of
the Chapter. Figure 6.4 depicts the process of conditional normalizing
flows with a three-step NF for PV generation. Note: Appendix 6.8.1
provides additional information on NFs.

###### Variational autoencoders

A VAE is a deep latent variable model composed of an encoder and a
decoder which are jointly trained to maximize a lower bound on the
likelihood. The encoder @xmath approximates the intractable posterior
@xmath , and the decoder @xmath the likelihood @xmath with @xmath .
Maximum likelihood is intractable as it would require marginalizing with
respect to all possible realizations of the latent variables @xmath .
kingma2013auto addressed this issue by maximizing the variational lower
bound @xmath as follows

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (6.6a)
     @xmath   @xmath      (6.6b)
     @xmath   @xmath      (6.6c)
  -- -------- -------- -- --------

as the Kullback-Leibler (KL) divergence ( perez2008kullback ) is
non-negative. Appendix 6.8.2 details how to compute the gradients of
@xmath , and its exact expression for the implemented VAE composed of
fully connected neural networks for both the encoder and decoder. Figure
6.5 depicts the process of a conditional variational autoencoder for PV
generation.

###### Generative adversarial networks

GANs are a class of deep generative models proposed by
goodfellow2014generative where the key idea is the adversarial training
of two neural networks, the generator and the discriminator , during
which the generator learns iteratively to produce realistic scenarios
until they cannot be distinguished anymore by the discriminator from
real data. The generator @xmath maps a latent vector @xmath equipped
with a known and tractable prior probability density function @xmath ,
e.g. , a Normal distribution, onto a sample @xmath , and is trained to
fool the discriminator. The discriminator @xmath is a classifier trained
to distinguish between true samples @xmath and generated samples @xmath
. goodfellow2014generative demonstrated that solving the following
min-max problem

  -- -------- -------- -- -------
     @xmath   @xmath      (6.7)
  -- -------- -------- -- -------

where @xmath is the value function, recovers the data generating
distribution if @xmath and @xmath are given enough capacity. The
state-of-the-art conditional Wasserstein GAN with gradient penalty
(WGAN-GP) proposed by gulrajani2017improved is implemented with @xmath
defined as

  -- -------- -------- -- --------
                          
     @xmath               (6.8a)
     @xmath   @xmath      (6.8b)
  -- -------- -------- -- --------

where @xmath is implicitly defined by sampling convex combinations
between the data and the generator distributions @xmath with @xmath .
The WGAN-GP constrains the gradient norm of the discriminator’s output
with respect to its input to enforce the 1-Lipschitz conditions. This
strategy differs from the weight clipping of WGAN that sometimes
generates only poor samples or fails to converge. Appendix 6.8.3 details
the successive improvements from the original GAN to the WGAN and the
final WGAN-GP implemented, referred to as GAN in the rest of the
Chapter. Figure 6.6 depicts the process of a conditional generative
adversarial network for PV generation.

##### 6.2.3 Theoretical comparison

Normalizing flows are a generative model that allows exact likelihood
calculation. They are efficiently parallelizable and offer a valuable
latent space for downstream tasks. In contrast to GANs and VAEs, NFs
explicitly learn the data distribution and provide direct access to the
exact likelihood of the model’s parameters, hence offering a sound and
direct way to optimize the network parameters ( wehenkel2020graphical )
. However, NFs suffer from drawbacks ( bond2021deep ) . One disadvantage
of requiring transformations to be invertible is that the input
dimension must be equal to the output dimension, making the model
difficult to train or inefficient. Each transformation must be
sufficiently expressive while being easily invertible to compute the
Jacobian determinant efficiently. The first issue is also raised by
ruthotto2021introduction where ensuring sufficient similarity of the
distribution of interest and the latent distribution is of high
importance to obtain meaningful and relevant samples. However, in our
numerical simulations, we did not encounter this problem. Concerning the
second issue, the UMNN-MAF transformation provides an expressive and
effective way of computing the Jacobian.

VAEs indirectly optimize the log-likelihood of the data by maximizing
the variational lower bound. The advantage of VAEs over NFs is their
ability to handle non-invertible generators and the arbitrary dimension
of the latent space. However, it has been observed that when applied to
complex datasets such as natural images, VAEs samples tend to be
unrealistic. There is evidence that the limited approximation of the
true posterior, with a common choice being a normally distributed prior
with diagonal covariance, is the root cause ( zhao2017towards ) . This
statement comes from the field of computer vision. However, it may
explain the shape of the scenarios observed in our numerical experiments
in Section 6.5 .

The training of GANs relies on a min-max problem where the generator and
the discriminator are jointly optimized. Therefore, it does not rely on
estimates of the likelihood or latent variable. The adversarial nature
of GANs makes them notoriously difficult to train due to the saddle
point problem ( arjovsky2017towards ) . Another drawback is the mode
collapsing, where one network stays in bad local minima, and only a
small subset of the data distribution is learned. Several improvements
have been designed to address these issues, such as the Wasserstein GAN
with gradient penalty. Thus, GANs models are widely used in computer
vision and power systems. However, most GAN approaches require
cumbersome hyperparameter tuning to achieve similar results to VAEs or
NFs. In our numerical simulations, the GAN is highly sensitive to
hyperparameter variations, which is consistent with (
ruthotto2021introduction ) .

Each method has its advantages and drawbacks and makes trade-offs in
terms of computing time, hyper-parameter tuning, architecture
complexity, etc . Therefore, the choice of a particular method is
dependent on the user criteria and the dataset considered. In addition,
the challenges of power systems are different from computer vision.
Therefore, the limitations established in the computer vision literature
such as bond2021deep and ruthotto2021introduction must be addressed with
caution. Therefore, we encourage the energy forecasting practitioners to
test and compare these methods in power systems applications.

#### 6.3 Quality assessment

For predictions in any form, one must differentiate between their
quality and their value ( morales2013integrating ) . Forecast quality
corresponds to the ability of the forecasts to genuinely inform of
future events by mimicking the characteristics of the processes
involved. Forecast value relates, instead, to the benefits from using
forecasts in a decision-making process, such as participation in the
electricity market. The forecast value is assessed in Part II Chapter 13
by considering the day-ahead market scheduling of electricity
aggregators, such as energy retailers or generation companies.

Evaluating and comparing generative models remains a challenging task.
Several measures have been introduced with the emergence of new models,
particularly in the field of computer vision. However, there is no
consensus or guidelines as to which metric best captures the strengths
and limitations of models. Generative models need to be evaluated
directly to the application they are intended for ( theis2015note ) .
Indeed, good performance to one criterion does not imply good
performance to the other criteria. Several studies propose metrics and
make attempts to determine the pros and cons. We selected two that
provide helpful information. (1) 24 quantitative and five qualitative
measures for evaluating generative models are reviewed and compared by
borji2019pros with a particular emphasis on GAN-derived models. (2)
several representative sample-based evaluation metrics for GANs are
investigated by xu2018empirical where the kernel Maximum Mean
Discrepancy (MMD) and the 1-Nearest-Neighbour (1-NN) two-sample test
seem to satisfy most of the desirable properties. The key message is to
combine several complementary metrics to assess the generative models.
Some of the metrics proposed are related to image generation and cannot
directly be transposed to energy forecasting.

Therefore, we used eight complementary quality metrics to conduct a
relevant quality analysis inspired by the energy forecasting and
computer vision fields. They can be divided into four groups: (1) the
univariate metrics composed of the continuous ranked probability score,
the quantile score, and the reliability diagram. They can only assess
the quality of the scenarios with respect to their marginals; (2) the
multivariate metrics are composed of the energy and the variogram
scores. They can directly assess multivariate scenarios; (3) the
specific metrics composed of a classifier-based metric and the
correlation matrix between scenarios for a given context; (4) the
Diebold and Mariano statistical test. The univariate and multivariate
metrics are already defined in Chapter 3 . Therefore, only the specific
metrics and statistical test are introduced in this Section.

##### 6.3.1 Classifier-based metric

Modern binary classifiers can be easily turned into robust two-sample
tests where the goal is to assess whether two samples are drawn from the
same distribution ( lehmann2006testing ) . In other words, it aims at
assessing whether a generated scenario can be distinguished from an
observation. To this end, the generator is evaluated on a held-out
testing set that is split into a testing-train and testing-test subsets.
The testing-train set is used to train a classifier, which tries to
distinguish generated scenarios from the actual distribution. Then, the
final score is computed as the performance of this classifier on the
testing-test set.

In principle, any binary classifier can be adopted for computing
classifier two-sample tests (C2ST). A variation of this evaluation
methodology is proposed by xu2018empirical and is known as the 1-Nearest
Neighbor (NN) classifier. The advantage of using 1-NN over other
classifiers is that it requires no special training and little
hyper-parameter tuning. This process is conducted as follows. Given two
sets of observations @xmath and generated @xmath samples with the same
size, i.e. , @xmath , it is possible to compute the leave-one-out (LOO)
accuracy of a 1-NN classifier trained on @xmath and @xmath with positive
labels for @xmath and negative labels for @xmath . The LOO accuracy can
vary from 0 % to 100 %. The 1-NN classifier should yield a 50 % LOO
accuracy when @xmath is large. This is achieved when the two
distributions match. Indeed, the level 50 % happens when a label is
randomly assigned to a generated scenario. It means the classifier is
not capable of discriminating generated scenarios from observations. If
the generative model over-fits @xmath to @xmath , i.e. , @xmath , and
the accuracy would be 0 %. On the contrary, if it generates widely
different samples than observations, the performance should be 100 %.
Therefore, the closer the LOO accuracy is to 1, the higher the degree of
under-fitting of the model. The closer the LOO accuracy is to 0, the
higher the degree of over-fitting of the model. The C2ST approach using
LOO with 1-NN is adopted by qi2020optimal to assess the PV and wind
power scenarios of a @xmath VAE.

However, this approach has several limitations. First, it uses the
testing set to train the classifier during the LOO. Second, the 1-NN is
very sensitive to outliers as it simply chose the closest neighbor based
on distance criteria. This behavior is amplified when combined with the
LOO where the testing-test set is composed of only one sample. Third,
the euclidian distance cannot deal with a context such as weather
forecasts. Therefore, we cannot use a conditional version of the 1-NN
using weather forecasts to classify weather-based renewable generation
and the observations. Fourth, C2ST with LOO cannot provide ROC curve but
only accuracy scores. An essential point about ROC graphs is that they
measure the ability of a classifier to produce good relative instance
scores. In our case, we are interested in discriminating the generated
scenarios from the observations, and the ROC provides more information
than the accuracy metric to achieve this goal. A standard method to
reduce ROC performance to a single scalar value representing expected
performance is to calculate the area under the ROC curve, abbreviated
AUC. The AUC has an important statistical property: it is equivalent to
the probability that the classifier will rank a randomly chosen positive
instance higher than a randomly chosen negative instance (
fawcett2004roc ) .

To deal with these issues, we decided to modify this classifier-based
evaluation by conducting the C2ST as follows: (1) the scenarios
generated on the learning set are used to train the classifier using the
C2ST. Therefore, the classifier uses the entire testing set and can
compute ROC; (2) the classifier is an Extra-Trees classifier that can
deal with context such as weather forecasts.

More formally, for a given generative model @xmath , the following steps
are conducted:

1.  Initialization step: the generative model @xmath has been trained on
    the LS and has generated @xmath weather-based scenarios per day of
    both the LS and TS: @xmath and @xmath . For the sake of clarity the
    index @xmath is omitted, but both of these sets are dependent on
    model @xmath .

2.  @xmath pairs of learning and testing sets are built with an equal
    proportion of generated scenarios and observations: @xmath and
    @xmath . Note: @xmath and @xmath .

3.  For each pair of learning and testing sets @xmath a classifier
    @xmath is trained and makes predictions.

4.  The @xmath curves and corresponding @xmath are computed for @xmath .

This classifier-based methodology is conducted for all models @xmath ,
and the results are compared. Figure 6.7 depicts the overall approach.
The classifiers @xmath are all Extra-Trees classifier made of @xmath
unconstrained trees with the hyper-parameters " " set to "None", and “ "
to 1 000.

##### 6.3.2 Correlation matrix between scenarios

The second specific metric consists of computing the correlation matrix
between the scenarios generated for given weather forecasts. Formally,
let @xmath be the set of @xmath scenarios generated for a given day of
the testing set. It is a matrix ( @xmath ) where each row is a scenario.
Then, the Pearson’s correlation coefficients are computed into a
correlation matrix ( @xmath ). This metric indicates the variety of
scenario shapes.

##### 6.3.3 Diebold-Mariano test

Using relevant metrics to assess the forecast quality is essential.
However, it is also necessary to analyze whether any difference in
accuracy is statistically significant. Indeed, when different models
have almost identical values in the selected error measures, it is not
easy to draw statistically significant conclusions on the outperformance
of the forecasts of one model by those of another. The Diebold-Mariano
(DM) test ( diebold2002comparing ) is probably the most commonly used
statistical testing tool to evaluate the significance of differences in
forecasting accuracy. It is model-free, i.e. , it compares the forecasts
of models, and not models themselves. The DM test is used in this study
to assess the CRPS, QS, ES, and VS metrics. The CRPS and QS are
univariate scores, and a value of CRPS and QS is computed per marginal
(time period of the day). Therefore, the multivariate variant of the DM
test is implemented following ziel2018day , where only one statistic for
each pair of models is computed based on the 24-dimensional vector of
errors for each day.

For a given day @xmath of the testing set, let @xmath be the error
computed by an arbitrary forecast loss function of the observation and
scenarios. The test consists of computing the difference between the
errors of the pair of models @xmath and @xmath over the testing set

  -- -------- -- -------
     @xmath      (6.9)
  -- -------- -- -------

and to perform an asymptotic @xmath -test for the null hypothesis that
the expected forecast error is equal and the mean of differential loss
series is zero @xmath . It means there is no statistically significant
difference in the accuracy of the two competing forecasts. The statistic
of the test is deduced from the asymptotically standard normal
distribution as follows

  -- -------- -- --------
     @xmath      (6.10)
  -- -------- -- --------

with @xmath the number of days of the testing set, @xmath and @xmath the
sample mean and the standard deviation of @xmath . Under the assumption
of covariance stationarity of the loss differential series @xmath , the
DM statistic is asymptotically standard normal. The lower the @xmath
-value, i.e. , the closer it is to zero, the more the observed data is
inconsistent with the null hypothesis: @xmath the forecasts of the model
@xmath are more accurate than those of model @xmath . If the @xmath
-value is less than the commonly accepted level of 5 %, the null
hypothesis is typically rejected. It means that the forecasts of model
@xmath are significantly more accurate than those of model @xmath .

When considering the ES or VS scores, there is a value per day of the
testing set @xmath or @xmath . In this case, @xmath or @xmath . However,
when considering the CRPS or QS, there is a value per marginal and per
day of the testing set @xmath or @xmath . A solution consists of
computing 24 independent tests, one for each hour of the day. Then, to
compare the models based on the number of hours for which the
predictions of one model are significantly better than those of another.
Another way consists of a multivariate variant of the DM-test with the
test performed jointly for all hours using the multivariate loss
differential series. In this case, for a given day @xmath , @xmath ,
@xmath are the vectors of errors for a given metric of models @xmath and
@xmath , respectively. Then the multivariate loss differential series

  -- -------- -- --------
     @xmath      (6.11)
  -- -------- -- --------

defines the differences of errors using the @xmath norm. Then, the
@xmath -value of two-sided DM tests is computed for each model pair as
described above. The univariate version of the test has the advantage of
providing a more profound analysis as it indicates which forecast is
significantly better for which hour of the day. The multivariate version
enables a better representation of the results as it summarizes the
comparison in a single @xmath -value, which can be conveniently
visualized using heat maps arranged as chessboards. In this study, we
decided to adopt the multivariate DM-test for the CRPS and QS.

#### 6.4 Case study

The quality and value evaluations of the models are conducted on the
load, wind, and PV tracks of the open-access GEFCom 2014 dataset (
hong2016bprobabilistic ) , composed of one, ten, and three zones,
respectively. See Chapter 13 for the value evaluation and the energy
retailer problem statement. Figure 6.8 depicts the methodology to assess
both the quality and value of the GAN, VAE and NF models implemented in
this study.

##### 6.4.1 Implementation details

By appropriate normalization, we standardize the weather forecasts to
have a zero mean and unit variance. Table 6.2 provides a summary of the
implementation details described in what follows. For the sake of proper
model training and evaluation, the dataset is divided into three parts
per track considered: learning, validation, and testing sets. The
learning set (LS) is used to fit the models, the validation set (VS) to
select the optimal hyper-parameters, and the testing set (TS) to assess
the forecast quality and value. The number of samples ( @xmath ),
expressed in days, of the VS and TS, is @xmath , with @xmath the number
of zones of the track considered. The 50 days are selected randomly from
the dataset, and the learning set is composed of the remaining part with
@xmath samples, where @xmath is provided for each track in Table 6.2 .
The NF, VAE, and GAN use the weather forecasts as inputs to generate on
a day-ahead basis @xmath scenarios @xmath .

##### Wind track

The zonal @xmath , @xmath and meridional @xmath , @xmath wind components
at 10 and 100 meters are selected, and six features are derived
following the formulas provided by landry2016probabilistic to compute
the wind speed @xmath , @xmath , energy @xmath , @xmath and direction
@xmath , @xmath at 10 and 100 meters

  -- -------- -------- -- ---------
                          
     @xmath   @xmath      (6.12a)
     @xmath   @xmath      (6.12b)
     @xmath   @xmath      (6.12c)
  -- -------- -------- -- ---------

For each generative model, the wind zone is taken into account with one
hot-encoding variable @xmath , and the wind feature input vector for a
given day @xmath is

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (6.13)
  -- -------- -------- -- --------

of dimension @xmath .

##### PV track

The solar irradiation @xmath , the air temperature @xmath , and the
relative humidity @xmath are selected, and two features are derived by
computing @xmath and @xmath . For each generative model, the PV zone is
taken into account with one hot-encoding variable @xmath , and the PV
feature input vector for a given day @xmath is

  -- -------- -------- -- --------
     @xmath   @xmath      (6.14)
  -- -------- -------- -- --------

of dimension @xmath . For practical reasons, the periods where the PV
generation is always 0, across all zones and days, are removed, and the
final dimension of the input feature vector is @xmath .

##### Load track

The 25 weather station temperature @xmath forecasts are used. There is
only one zone, and the load feature input vector for a given day @xmath
is

  -- -------- -------- -- --------
     @xmath   @xmath      (6.15)
  -- -------- -------- -- --------

of dimension @xmath .

##### 6.4.2 Hyper-parameters

Table 6.3 provides the hyper-parameters of the NF, VAE, and GAN
implemented. The Adam optimizer ( kingma2014adam ) is used to train the
generative models with a batch size of 10% of the learning set.

The NF implemented is a one-step monotonic normalizer using the UMNN-MAF
² ² 2 https://github.com/AWehenkel/Normalizing-Flows . The embedding
size @xmath is set to 40, and the embedding neural network is composed
of @xmath layers of @xmath neurons ( @xmath ). The same integrand neural
network @xmath @xmath is used and composed of 3 layers of @xmath neurons
( @xmath ). Both the encoder and decoder of the VAE are feed-forward
neural networks ( @xmath ), ReLU activation functions for the hidden
layers, and no activation function for the output layer. Both the
generator and discriminator of the GAN are feed-forward neural networks
( @xmath ). The activation functions of the hidden layers of the
generator (discriminator) are ReLU (Leaky ReLU). The activation function
of the discriminator output layer is ReLU, and there is no activation
function for the generator output layer. The generator is trained once
after the discriminator is trained five times to stabilize the training
process, and the gradient penalty coefficient @xmath in ( 6.7 ) is set
to 10 as suggested by gulrajani2017improved .

Figure 6.9 illustrates the VAE, GAN, and NF structures implemented for
the wind dataset where the number of weather variables selected and the
number of zones is 10, and 10, respectively. Recall, @xmath weather
forecasts, @xmath scenarios @xmath wind power observations, @xmath
latent space variable, @xmath Normal variable (only for the VAE).

#### 6.5 Quality results

First, a thorough comparison of the models is conducted on the wind
track. Second, all tracks are considered for the sake of clarity. Note
that the model ranking slightly differs depending on the track.

##### 6.5.1 Wind track

In addition to the generative models, a naive approach is designed
(RAND). The scenarios of the learning, validation and testing sets are
sampled randomly from the learning, validation, and testing sets,
respectively. Intuitively, it boils down to assume that past
observations are repeated. These scenarios are realistic but may not be
compatible with the context. Each model generates a set of 100 scenarios
for each day of the testing set. And the metrics are computed. Figure
6.10 compares the QS, reliability diagram, and CRPS of the wind
(markers), PV (plain), and load (dashed) tracks. Overall, for the wind
track in terms of CRPS, QS, and reliability diagrams, the VAE achieves
slightly better scores, followed by the NF and the GAN. The ES and VS
multivariate scores confirm this trend with 54.82 and 18.87 for the VAE
vs 56.71 and 18.54 for the NF, respectively.

Figure 6.13 provides the results of the DM tests for these metrics. The
heat map indicates the range of the @xmath -values. The closer they are
to zero, dark green, the more significant the difference between the
scores of two models for a given metric. The statistical threshold is
five %, but the scale color is capped at ten % for a better exposition
of the relevant results. For instance, when considering the DM test for
the RAND CRPS, all the columns of the RAND row are in dark green,
indicating that the RAND scenarios are always significantly outperformed
by the other models. These DM tests confirm that the VAE outperforms the
NF for the wind track considering these metrics. Then, the NF is only
outperformed by the VAE and the GAN by both the VAE and NF. These
results are consistent with the classifier-based metric depicted in
Figure 6.10(a) , where the VAE is the best to mislead the classifier
followed by the NF, and GAN.

The left part of Figure 6.12 provides 50 scenarios, (a) NF, (c) GAN and
(e) VAE, generated for a given day selected randomly from the testing
set. Notice how the shape of the NF’s scenarios differs significantly
from the GAN and VAE as they tend to be more variable with no
identifiable trend. In contrast, the VAE and GAN scenarios seem to
differ mainly in nominal power but have similar shapes. This behavior is
even more pronounced for the GAN, where the scenarios rarely crossed
over the periods. For instance, there is a gap in generation around
periods 17 and 18 where all the GAN’s scenarios follow this trend. These
observations are confirmed by computing the corresponding time
correlation matrices, depicted by the right part of Figure 6.12
demonstrating there is no correlation between NF’s scenarios. On the
contrary, the VAE and GAN correlation matrices tend to be similar with a
time correlation of the scenarios over a few periods, with more
correlated periods when considering the GAN. This difference in the
scenario’s shape is striking and not necessarily captured by metrics
such as the CRPS, QS, or even the classifier-based metric and is also
observed on the PV and load tracks, as explained in the next paragraph.

##### 6.5.2 All tracks

Table 6.4 provides the averaged quality scores. The CRPS is averaged
over the 24 time periods @xmath . The QS over the 99 percentiles @xmath
. The MAE-r is the mean absolute error between the reliability curve and
the diagonal, and @xmath is the mean of the 50 AUC.

Overall, for the PV and load tracks in CRPS, QS, reliability diagrams,
AUC, ES, and VS, the NF outperforms the VAE and GAN and is slightly
outperformed by the VAE on the wind track. On the load track, the VAE
outperforms the GAN. However, the VAE and GAN achieved similar results
on the PV track, and the GAN performed better in terms of ES and VS.
These results are confirmed by the DM tests depicted in Figure 6.13 .
The classifier-based metric results for both the load and PV tracks,
provided by Figure 6.11 , confirm this trend where the NF is the best to
trick the classifier followed by the VAE and GAN.

Similar to the wind track, the shape of the scenarios differs
significantly between the NF and the other models for both the load and
PV tracks as indicated by Figure 6.14 . Note: the load track scenarios
are highly correlated for both the VAE and GAN. Finally, Figure 6.15
provides the average of the correlation matrices over all days of the
testing set for each dataset. The trend depicted above is confirmed.
This difference between the NF and the other generative model may be
explicated by the design of the methods. The NF explicitly learns the
probability density function (PDF) of the multi-dimensional random
variable considered. Thus, the NF scenarios are generated according to
the learned PDF producing multiple shapes of scenarios. In contrast, the
generator of the GAN is trained to fool the discriminator, and it may
find a shape particularly efficient leading to a set of similar
scenarios. Concerning the VAE, it is less obvious. However, by design,
the decoder is trained to generate scenarios from the latent space
assumed to follow a Gaussian distribution that may lead to less
variability.

#### 6.6 Conclusions and perspectives

This Chapter proposed a fair and thorough comparison of NFs with the
state-of-the-art deep learning generative models, GANs and VAEs, both in
quality and value. The numerical experiments employ the open data of the
Global Energy Forecasting Competition 2014. The generative models use
the conditional information to compute improved weather-based PV, wind
power, and load scenarios. This Chapter demonstrated that NFs can
challenge GANs and VAEs as they are, overall, more accurate both in
terms of quality and value (see Chapter 13 ). In addition, they can be
used effectively by non-expert deep learning practitioners. In addition,
NFs have several advantages over more traditional deep learning
approaches that should motivate their introduction into power system
applications:

1.  NFs directly learn the stochastic multivariate distribution of the
    underlying process by maximizing the likelihood. Therefore, in
    contrast to VAEs and GANs, NFs provide access to the exact
    likelihood of the model’s parameters, hence offering a sound and
    direct way to optimize the network parameters (
    wehenkel2020graphical ) . It may open a new range of advanced
    applications benefiting from this advantage. For instance, to
    transfer scenarios from one location to another based on the
    knowledge of the probability density function. A second application
    is the importance sampling for stochastic optimization based on a
    scenario approach. Indeed, NFs provide the likelihood of each
    generated scenario, making it possible to filter relevant scenarios
    in stochastic optimization.

2.  In our opinion, NFs are easier to use by non-expert deep learning
    practitioners once the libraries are available, as they are more
    reliable and robust in terms of hyper-parameters selection. GANs and
    VAEs are particularly sensitive to the latent space dimension, the
    structure of the neural networks, the learning rate, etc . GANs
    convergence, by design, is unstable, and for a given set of
    hyper-parameters, the scenario’s quality may differ completely. In
    contrast, it was easier to retrieve relevant NFs hyper-parameters by
    manually testing a few sets of values, satisfying training
    convergence, and quality results.

Nevertheless, their usage as a base component of the machine learning
toolbox is still limited compared to GANs or VAEs.

#### 6.7 Appendix: Table 6.1 justifications

wang2020modeling use a Wasserstein GAN with gradient penalty to model
both the uncertainties and the variations of the load. First, point
forecasting is conducted, and the corresponding residuals are derived.
Then, the GAN generates residual scenarios conditional on the day type,
temperatures, and historical loads. The GAN model is compared with the
same version without gradient penalty and two quantile regression
models: random forest and gradient boosting regression tree. The quality
evaluation is conducted on open load datasets from the Independent
System Operator-New England ³ ³ 3 https://www.iso-ne.com/ with five
metrics: (1) the continuous ranked probability score; (2) the quantile
score; (3) the Winkler score; (4) reliability diagrams; (5) Q-Q plots.
Note: the forecast value is not assessed.

qi2020optimal propose a concentrating solar power (CSP) configuration
method to determine the CSP capacity in multi-energy power systems. The
configuration model considers the uncertainty by scenario analysis. A
@xmath VAE generates the scenarios. It is an improved version of the
original VAE However, it does not consider weather forecasts, and the
model is trained only by using historical observations. The quality
evaluation is conducted on two wind farms and six PV plants using three
metrics. (1) The leave-one-out accuracy of the 1-nearest neighbor
classifier. (2) The comparison of the frequency distributions of the
actual data and the generated scenarios. (3) Comparing the spatial and
temporal correlations of the actual data and the scenarios by computing
Pearson correlation coefficients. The value is assessed by considering
the case study of the CSP configuration model, where the @xmath VAE is
used to generate PV, wind power, and load scenarios. However, the VAE is
not compared to another generative model for both the quality and value
evaluations. Note: the dataset does not seem to be in open-access.
Finally, the value evaluation case study is not trivial due to the
mathematical formulation that requires a certain level of knowledge of
the system. Thus, the replicability criterion is partially satisfied.

ge2020modeling compared NFs to VAEs and GANs for the generation of daily
load profiles. The models do not take into account weather forecasts but
only historical observations. However, an example is given to illustrate
the principle of generating conditional daily load profiles by using
three groups: light load, medium load, and heavy load. The quality
evaluation uses five indicators. Four to assess the temporal
correlation: (1) probability density function; (2) autocorrelation
function; (3) load duration curve; (4) a wave rate is defined to
evaluate the volatility of the daily load profile. Furthermore, one
additional for the spatial correlation: (5) Pearson correlation
coefficient is used to measure the spatial correlation among multiple
daily load profiles. The simulations use the open-access London smart
meter and Spanish transmission service operator datasets of Kaggle. The
forecast value is not assessed.

#### 6.8 Appendix: background

##### 6.8.1 NFs

###### NF computation

Evaluating the likelihood of a distribution modeled by a normalizing
flow requires computing ( 6.2 ), i.e. , the normalizing direction, as
well as its log-determinant. Increasing the number of sub-flows by
@xmath of the transformation results in only @xmath growth in the
computational complexity as the log-determinant of @xmath can be
expressed as

  -- -------- -------- -- ---------
                          
     @xmath   @xmath      (6.16a)
              @xmath      (6.16b)
  -- -------- -------- -- ---------

However, with no further assumption on @xmath , the computational
complexity of the log-determinant is @xmath , which can be intractable
for large @xmath . Therefore, the efficiency of these operations is
essential during training, where the likelihood is repeatedly computed.
There are many possible implementations of NFs detailed by
papamakarios2019normalizing ; kobyzev2020normalizing to address this
issue.

###### Autoregressive flow

The Jacobian of the autoregressive transformation @xmath defined by (
6.3 ) is lower triangular, and its log-absolute-determinant is

  -- -------- -------- -- ---------
                          
     @xmath   @xmath      (6.17a)
              @xmath      (6.17b)
  -- -------- -------- -- ---------

that is calculated in @xmath instead of @xmath .

###### Affine autoregressive flow

A simple choice of transformer is the class of affine functions

  -- -------- -------- -- --------
     @xmath   @xmath      (6.18)
  -- -------- -------- -- --------

where @xmath is parameterized by @xmath , @xmath controls the scale, and
@xmath controls the location of the transformation. Invertibility is
guaranteed if @xmath , and this can be easily achieved by e.g. taking
@xmath , where @xmath is an unconstrained parameter in which case @xmath
. The derivative of the transformer with respect to @xmath is equal to
@xmath . Hence the log-absolute-determinant of the Jacobian becomes

  -- -------- -------- -- --------
     @xmath   @xmath      (6.19)
  -- -------- -------- -- --------

Affine autoregressive flows are simple and computation efficient.
However, they are limited in expressiveness requiring many stacked flows
to represent complex distributions. It is unknown whether affine
autoregressive flows with multiple layers are universal approximators (
papamakarios2019normalizing ) .

##### 6.8.2 VAEs

Figure 6.16 illustrates the VAE process with @xmath the encoder and
@xmath the decoder.

###### Gradients computation

By using ( 6.6 ) @xmath is decomposed in two parts

  -- -------- -------- -- --------
     @xmath   @xmath      (6.20)
  -- -------- -------- -- --------

@xmath is estimated with the usual Monte Carlo gradient estimator.
However, the estimation of @xmath requires the reparameterization trick
proposed by kingma2013auto , where the random variable @xmath is
re-expressed as a deterministic variable

  -- -------- -------- -- --------
     @xmath   @xmath      (6.21)
  -- -------- -------- -- --------

with @xmath an auxiliary variable with independent marginal @xmath , and
@xmath some vector-valued function parameterized by @xmath . Then, the
first right hand side of ( 6.20 ) becomes

  -- -------- -------- -- --------
     @xmath   @xmath      (6.22)
  -- -------- -------- -- --------

@xmath is now estimated with Monte Carlo integration.

###### Conditional VAE implemented

Following kingma2013auto , we implemented Gaussian multi-layer
perceptrons (MLPs) for both the encoder @xmath and decoder @xmath . In
this case, @xmath is a centered isotropic multivariate Gaussian, @xmath
and @xmath are both multivariate Gaussian with a diagonal covariance and
parameters @xmath and @xmath , respectively. Note that there is no
restriction on the encoder and decoder architectures, and they could as
well be arbitrarily complex convolutional networks. Under these
assumptions, the conditional VAE implemented is

  -- -------- -------- -- ---------
                          
     @xmath   @xmath      (6.23a)
     @xmath   @xmath      (6.23b)
     @xmath   @xmath      (6.23c)
     @xmath   @xmath      (6.23d)
     @xmath   @xmath      (6.23e)
  -- -------- -------- -- ---------

Then, by using the valid reparameterization trick proposed by
kingma2013auto

  -- -------- -------- -- ---------
                          
     @xmath   @xmath      (6.24a)
     @xmath   @xmath      (6.24b)
  -- -------- -------- -- ---------

@xmath is computed and differentiated without estimation using the
expressions

  -- -------- -------- -- ---------
                          
     @xmath   @xmath      (6.25a)
     @xmath   @xmath      (6.25b)
  -- -------- -------- -- ---------

with @xmath the dimensionality of @xmath .

##### 6.8.3 GANs

###### Gan

The original GAN value function @xmath proposed by
goodfellow2014generative is

  -- -------- -------- -- ---------
                          
     @xmath   @xmath      (6.26a)
     @xmath   @xmath      (6.26b)
  -- -------- -------- -- ---------

where @xmath is the cross-entropy, and @xmath the probability the
discriminator wrongly classifies the samples.

###### Wgan

The divergences which GANs typically minimize are responsible for their
training instabilities for reasons investigated theoretically by
arjovsky2017towards . arjovsky2017wasserstein proposed instead using the
Earth mover distance, also known as the Wasserstein-1 distance

  -- -------- -------- -- --------
     @xmath   @xmath      (6.27)
  -- -------- -------- -- --------

where @xmath denotes the set of all joint distributions @xmath whose
marginals are respectively @xmath and @xmath , @xmath indicates how much
mass must be transported from @xmath to @xmath in order to transform the
distribution @xmath into @xmath , @xmath is the L1 norm, and @xmath
represents the cost of moving a unit of mass from @xmath to @xmath .
However, the infimum in ( 6.27 ) is intractable. Therefore,
arjovsky2017wasserstein used the Kantorovich-Rubinstein duality (
villani2008optimal ) to propose the Wasserstein GAN (WGAN) by solving
the min-max problem

  -- -------- -------- -- --------
     @xmath   @xmath      (6.28)
  -- -------- -------- -- --------

where @xmath is the 1-Lipschitz space, and the classifier @xmath is
replaced by a critic function @xmath . However, the weight clipping used
to enforce @xmath 1-Lipschitzness can lead sometimes the WGAN to
generate only poor samples or failure to converge (
gulrajani2017improved ) . Therefore, we implemented the WGAN-GP to
tackle this issue.

### Chapter 7 Part I conclusions

  “ If you’re lonely when you’re alone, you’re in bad company. ”

  — Jean-Paul Sartre

Part I presents the forecasting tools and metrics required to produce
and evaluate reliable point and probabilistic forecasts used as input of
decision-making models. The forecasts take various forms: point
forecasts, quantiles, prediction intervals, density forecasts, and
scenarios. An example of forecast quality evaluation is given with
standard deep learning models such as recurrent neural networks that
compute PV and electrical consumption point forecasts on a real case
study. Then, several case studies are used to assess the forecast
quality of probabilistic forecasts.

-   Deep learning models such as the encoder-decoder architecture and
    recurrent neural networks compute PV quantile forecasts. These
    models are trained by quantile regression. The forecast quality is
    evaluated on a real case study composed of the PV generation of the
    parking rooftops of the Liège University. The quantile regression
    models are compared to quantiles derived from deep learning
    generative models. In terms of forecast quality, the latter models
    outperform, in this case, the quantile regression models. The
    forecast value is assessed in Part II where the quantile forecasts
    are used as input of robust optimization planners in the form of
    prediction intervals.

-   A density forecast-based approach produces probabilistic forecasts
    of imbalance prices, focusing on the Belgian case. The two-step
    probabilistic approach computes the net regulation volume state
    transition probabilities. Then, it infers the imbalance price. A
    numerical comparison of this approach to standard forecasting
    techniques is performed on the Belgium case. The proposed model
    outperforms other approaches on probabilistic error metrics but is
    less accurate at predicting the imbalance prices. Deep learning
    models could improve this probabilistic approach to avoid
    simplifying assumptions and adding input features to describe the
    market situation better.

-   Finally, a recent class of deep learning generative models, the
    normalizing flows, is investigated. A fair and thorough comparison
    is conducted with the state-of-the-art deep learning generative
    models, generative adversarial networks and variational
    autoencoders. The experiments employ the open data of the Global
    Energy Forecasting Competition 2014. The models use the conditional
    information to compute improved weather-based PV, wind power, and
    load scenarios. The results demonstrate that normalizing flows can
    challenge generative adversarial networks and variational
    autoencoders. Indeed, they are overall more accurate both in terms
    of quality and value (see Chapter 13 ). Furthermore, they can be
    used effectively by non-expert deep learning practitioners.

## Part II Planning and control {infobox}

Overview Part II presents the decision-making tools under uncertainty
required to address the day-ahead planning of a microgrid. It
investigates several approaches: deterministic planning using linear
programming based on point forecasts, stochastic programming using
scenarios, and robust optimization using quantile forecasts. Several
case studies are considered: a grid-connected microgrid using a value
function approach to propagate the information from planning to
real-time optimization, a grid-connected microgrid in the capacity
firming framework, and an energy retailer on the day-ahead market. The
case studies use the forecasting techniques studied in Part II .

  “ The straight line, a respectable optical illusion which ruins many a
  man. ”

  — Victor Hugo

Figure 7.1 illustrates the organization of the second part. It addresses
the energy management of a grid-connected microgrid by using the
forecasting techniques studied in Part I . Chapter 8 provides the
optimization basics. Chapter 9 proposes a value function-based approach
as a way to propagate information from operational planning to real-time
optimization. Chapters 10 , 11 , and 12 propose stochastic, sizing, and
robust approaches to address the energy management of a grid-connected
renewable generation plant coupled with a battery energy storage device
in the capacity firming market, respectively. The capacity firming
framework has been designed for isolated markets, such as the Overseas
France islands. Chapter 13 is the extension of Chapter 6 . It
investigates the forecast value assessment of the deep generative models
by considering an energy retailer portfolio on the day-ahead market.
Finally, Chapter 14 draws the conclusions of Part II .

### Chapter 8 Decision-making background {infobox}

Overview This Chapter introduces some basics of linear programming and
optimization methodologies to address uncertainty in decision-making
used to formulate the problems considered in Part II .
The interested reader is referred to more general textbooks for further
information ( morales2013integrating ; bertsimas1997introduction ;
birge2011introduction ) and the lectures of the courses "Renewables in
Electricity Markets" ¹ ¹ 1 http://pierrepinson.com/index.php/teaching/
and "Advanced Optimization and Game Theory for Energy Systems" ² ² 2
https://www.jalalkazempour.com/teaching given by professor Pierre Pinson
and associate professor Jalal Kazempour, respectively, at the Technical
University of Denmark.

  “ Nothing is more difficult, and therefore more precious, than to be
  able to decide. ”

  — Napoleon Bonaparte

Section 8.1 introduces the mathematical formulation of a linear
optimization problem. Sections 8.2 and 8.3 present the mathematical
formulations of a linear optimization problem when considering
uncertainty in the parameters of the problem. First, by considering a
stochastic programming approach. Second, by using a robust methodology.

#### 8.1 Linear programming

##### 8.1.1 Formulation of a linear programming problem

The most straightforward instance of an optimization problem is a linear
programming problem.

###### Definition 8.1.1 (Linear programming).

Linear programming (LP) minimizes a linear cost function subject to
linear equality and inequality constraints.

In a LP problem, we are given a cost vector @xmath and we seek to
minimize a linear cost function @xmath over all @xmath -dimensional
vectors @xmath subject to a set of linear equality and inequality
constraints. Suppose that there is a total of @xmath such constraints
and let @xmath and @xmath be the @xmath matrix. The constraints can be
expressed compactly in the form @xmath .

###### Definition 8.1.2 (LP in standard form).

( bertsimas1997introduction , Chapter 1) A linear programming problem of
the form

  -- -------- -------- -- -------
     @xmath   @xmath      (8.1)
              @xmath      
              @xmath      
  -- -------- -------- -- -------

is said to be in the standard form .

LP problems model a wide variety of real-world problems, such as
day-ahead planning of a microgrid.

##### 8.1.2 Duality in linear programming

Duality theory deals with the relation between a LP problem, called the
primal , and another LP problem, called the dual . It uncovers the
deeper structure of a LP problem and is a powerful tool that has various
applications. The duality theory is motivated by the Lagrange multiplier
method, where a price variable is associated with each constraint. It
explores prices under which the presence or absence of the constraints
does not affect the optimal cost. Then, the correct prices can be found
by solving a new LP problem, called the original’s dual (
bertsimas1997introduction ) .

###### Definition 8.1.3 (Dual of a LP problem).

( bertsimas1997introduction , Chapter 4) Given a primal problem in the
standard form ( 8.1 ), its dual is defined as

  -- -------- -------- -- -------
     @xmath   @xmath      (8.2)
              @xmath      
              @xmath      
  -- -------- -------- -- -------

with @xmath the price vector of the same dimension as @xmath .

Note that the dual of the dual problem is the primal problem. The strong
duality theorem is the central result of linear programming duality.

  ’If a linear programming problem has an optimal solution so does its
  dual, and the respective optimal costs are equal.’ (
  bertsimas1997introduction , Chapter 4, Theorem 4.4)

#### 8.2 Stochastic optimization

Most decision-making problems are subject to uncertainty due to the
intrinsic stochasticity of natural events conditioning our choices, e.g.
, the weather, or, more generally, to the inaccurate knowledge of input
information. Therefore, decision-makers are interested in methods and
tools that provide solutions less sensitive to environmental influences
or inaccurate data while simultaneously reducing cost, increasing
profit, or improving reliability ( morales2013integrating , Appendix C)
.

Let consider a LP in the standard form ( 8.1 ). If parameters @xmath ,
@xmath , and @xmath are perfectly known, solution algorithms for linear
optimization problems, e.g. , the simplex method, can be used to find
the best value of the decision variable vector @xmath .

However, suppose some of these parameters are contingent on the
realization @xmath of a particular random vector @xmath . In that case,
determining the optimal solution to the problem ( 8.1 ) may become
further challenging. First, the issue of how to guarantee the
feasibility of decision vector @xmath becomes remarkably more involved
when optimizing under uncertainty because @xmath and @xmath are not
completely known in advance. Second, the issue of how to guarantee the
optimally of decision vector @xmath is at stake because @xmath is not
completely known in advance. Finally, the LP problem ( 8.1 ) needs to be
recast so that solution algorithms for linear programming problems can
be used to obtain the optimal value of decision vector @xmath taking
into account the uncertainty of the parameters.

Stochastic programming provides the concepts and tools required to deal
with the implications of having uncertain data in an optimization
problem for decision making to address the three issues previously
stated. It assumes that an accurate probabilistic description of the
random variable is assumed available, under the form of the probability
distributions or densities. In the following, we consider stochastic
programming with recourse where the set of decisions is divided into two
groups: (1) decisions have to be taken before the realization of
uncertain parameters. These decisions are known as first-stage , @xmath
, or here-and-now decisions and do not depend on the realization of the
random parameters; (2) decisions can be taken after the realization of
uncertain parameters. These decisions are called second-stage , @xmath ,
or recourse decisions and are dependent on each plausible value of the
random parameters. Note: the term recourse points to the fact that
second-stage decisions enable the decision-maker to adapt to the actual
outcomes of the random events.

###### Definition 8.2.1 (Two-stage program with fixed recourse).

( birge2011introduction , Chapter 2) A classical two-stage stochastic
linear program with fixed recourse is

  -- -------- -------- -- -------
     @xmath   @xmath      (8.3)
              @xmath      
              @xmath      
  -- -------- -------- -- -------

with

  -- -------- -------- -- -------
     @xmath   @xmath      (8.4)
              @xmath      
              @xmath      
  -- -------- -------- -- -------

the second-stage value function.

Note: the uncertainty involved in problem ( 8.3 ) and ( 8.4 ) is assumed
to be properly represented by means of a finite set @xmath of scenarios
@xmath , with a probability @xmath such that @xmath .

###### Definition 8.2.2 (Deterministic equivalent problem).

( birge2011introduction , Chapter 2) The deterministic equivalent
problem of the stochastic programming problem ( 8.3 )-( 8.4 ) is

  -- -------- -------- -- -------
     @xmath   @xmath      (8.5)
              @xmath      
              @xmath      
              @xmath      
              @xmath      
  -- -------- -------- -- -------

and can be directly processed by off-the-shelf optimization software for
linear programs.

#### 8.3 Robust optimization

Robust optimization considers optimization problems with uncertain
parameters not modeled using probability distributions but with
uncertainty sets. A robust optimization strategy investigates a solution
to an optimization problem that is feasible for any realization of the
uncertain parameters within the uncertainty set and optimal for the
worst-case realization of these uncertain parameters (
morales2013integrating , Appendix D) .

###### Definition 8.3.1 (Two-stage robust optimization with fixed
recourse).

The general form of two-stage robust optimization formulation is

  -- -------- -------- -- -------
     @xmath   @xmath      (8.6)
              @xmath      
              @xmath      
  -- -------- -------- -- -------

with @xmath the uncertainty set and

  -- -------- -------- -- -------
     @xmath   @xmath      (8.7)
  -- -------- -------- -- -------

The objective of the problem ( 8.6 ) is to make the best decisions
represented by variable vector @xmath for the worst realization of
parameters in vector @xmath and considering the recourse decisions
described by variable vector @xmath . If the right-hand-side problem,
the @xmath -problem, is convex, it can be replaced by its dual and
merged with the middle @xmath -problem rendering it a conventional
single-level maximization problem. Overall, the resulting problem is a
min-max problem that in some cases can be solved using decomposition
such as the Benders-dual cutting plane method ( bertsimas2012adaptive )
or column-and-constraint generation algorithm ( zeng2013solving ) . Both
the Benders-dual method and the column-and-constraint generation
procedure are implemented in a master sub-problem framework.

##### 8.3.1 Benders-dual cutting plane algorithm

The key idea of the Benders-dual cutting plane (BD) algorithm is to
gradually construct the value function of the first-stage decisions
using dual solutions of the second-stage decision problems.

Consider the case where the second-stage decision problem is a linear
programming (LP) problem in @xmath . We first take the relatively
complete recourse assumption that this LP is feasible for any given
@xmath and @xmath . Let @xmath be its dual variables. Then, we obtain
its dual problem, which is a maximization problem and can be merged with
the maximization over @xmath . As a result, we have the following
dispatch problem, which yields the sub-problem (SP) in the BD algorithm.

###### Definition 8.3.2 (BD sub-problem).

  -- -------- -------- -- -------
     @xmath   @xmath      (8.8)
              @xmath      
              @xmath      
  -- -------- -------- -- -------

Note: that the resulting problem in ( 8.8 ) is a bilinear optimization
problem. Several approaches have been developed to address this issue
and depend on the case study.

Suppose, we have an oracle that can solve ( 8.8 ) for a given first
stage variable @xmath . Then, the optimal solution is @xmath , and a
cutting plane in the form of

  -- -------- -- -------
     @xmath      (8.9)
  -- -------- -- -------

can be generated, and included in the master problem (MP).

###### Definition 8.3.3 (BD master problem).

At iteration @xmath of the BD master problem is

  -- -------- -------- -- --------
     @xmath   @xmath      (8.10)
              @xmath      
              @xmath      (8.11)
              @xmath      
  -- -------- -------- -- --------

which can compute an optimal first stage solution @xmath .

The MP and SP provide the lower and upper bounds with @xmath and @xmath
, respectively, at iteration @xmath . When considering the relatively
complete recourse assumption, ( zeng2013solving ; bertsimas2012adaptive
; bertsimas2018scalable ) demonstrated the BD algorithm converges to the
optimal solution of the two-stage robust optimization problem. The lower
and upper bounds converge in a finite number of steps by iteratively
introducing cutting planes ( 8.9 ) and computing the MP ( 8.10 ). Note:
if the SP is a MILP, this result is not straightforward. It is the case
in the capacity firming problem studied in Chapter 12 where this issue
is addressed.

##### 8.3.2 Column and constraints generation algorithm

The column and constraints generation (CCG) algorithm does not create
constraints using dual solutions of the second-stage decision problem.
It dynamically generates constraints with recourse decision variables in
the primal space for an identified scenario. Let assume the uncertainty
set U is a finite discrete set with @xmath and @xmath are the
corresponding recourse decision variables. Then, the 2-stage RO ( 8.6 )
can be reformulated as follows

  -- -------- -------- -- --------
     @xmath   @xmath      (8.12)
              @xmath      (8.13)
              @xmath      (8.14)
              @xmath      (8.15)
              @xmath      (8.16)
  -- -------- -------- -- --------

Thus, it reduces to solve an equivalent, probably large-scale, MILP,
which is very close to a 2-stage stochastic programming model if the
probability distribution over @xmath is known. When the uncertainty set
is large or is a polyhedron, enumerating all the possible uncertain
scenarios in @xmath is not feasible. Constraints ( 8.14 ) indicate that
not all scenarios, and their corresponding variables and constraints,
are necessary for defining the optimal value @xmath of the 2-stage RO.
It is likely that a few relevant scenarios, a small subset of the
uncertainty set, play a significant role in the formulation. Therefore,
the key idea of the CCG procedure is to generate recourse decision
variables for the significant scenarios.

###### Definition 8.3.4 (CCG sub-problem).

Similar to the BD method, the CCG algorithm uses a master sub-problem
framework. Let assume an oracle can solve the dispatch problem for a
given first-stage @xmath

  -- -------- -- --------
     @xmath      (8.17)
  -- -------- -- --------

Then, the optimal solution @xmath can be derived to build constraints
and variables in the master problem.

###### Definition 8.3.5 (CCG master problem).

At iteration @xmath of the CCG master problem is

  -- -------- -------- -- --------
     @xmath   @xmath      (8.18)
              @xmath      (8.19)
              @xmath      (8.20)
              @xmath      (8.21)
              @xmath      (8.22)
  -- -------- -------- -- --------

which can compute an optimal first stage solution @xmath .

Similar to BD, the MP and SP provide the lower and upper bounds with
@xmath and @xmath , respectively, at iteration @xmath .

#### 8.4 Conclusions

This Chapter introduces the basics of linear programming and approaches
to handle uncertainty in the parameters with the stochastic programming
and robust approach. Depending on the application, each approach has its
pros and cons. When considering stochastic programming, the number of
scenarios needed to describe the most plausible outcomes of the
uncertain parameters may be huge, leading to large-scale optimization
problems that may become difficult to solve or intractable. In this
case, a robust approach provides an alternative and compact manner to
describe uncertain parameters. However, the robust counterpart
optimization problem may be challenging to solve and requires a
decomposition technique such as a Benders-dual cutting plane or column
and constraints generation algorithm that is not trivial to implement
numerically.

### Chapter 9 Coordination of the planner and controller {infobox}

Overview This Chapter presents a two-layer approach with a value
function to propagate information from operational planning to real-time
optimization. The value function-based approach shares some similarities
with the coordination scheme proposed in kumar2018stochastic , which is
based on stochastic dual dynamic programming. This study brings new
contributions:

1.  The approach is tested by accounting for forecasting errors and
    high-resolution data monitored on-site corresponding to a
    "real-life" case.

2.  The value function approach allows to deal with indeterminacy
    issues. When there are several optimal solutions to the upper-level
    problem, this is accounted for in the lower level part, and a bias
    term can be added to favor one type of behavior over another, e.g. ,
    charge early.

3.  This methodology is fully compatible with the energy markets as it
    can deal with imbalance, reserve, and dynamic selling/purchasing
    prices.

This study reports results on an industrial microgrid capable of
on/off-grid operation. Generation and consumption forecasts are based on
weather forecasts obtained with the MAR model (
fettweis2017reconstructions ) .

References: This chapter is an adapted version of the following
publication:
\bibentry dumas2021coordination.

  “ When making a decision of minor importance, I have always found it
  advantageous to consider all the pros and cons. In vital matters,
  however, such as the choice of a mate or a profession, the decision
  should come from the unconscious, from somewhere within ourselves. In
  the important decisions of personal life, we should be governed, I
  think, by the deep inner needs of our nature. ”

  — Sigmund Freud

The hierarchical microgrid control levels divide a global microgrid
control problem in time and space ( palizban2014microgrids ) . Control
levels range from distributed device level controllers that run at a
high frequency to centralized controllers optimizing market integration
that run much less frequently. For computation time reasons, centralized
controllers are often subdivided into two levels. Operational planning
controllers that optimize decisions over a time horizon of one or
several days but with a market period resolution, e.g. , 15 minutes.
Real-time optimization controllers that deal with actions within the
current market period. The coordination of these two levels is paramount
to achieving the safest and most profitable operational management of
microgrids. Microgrid control and management can be achieved in several
ways. Control techniques and the principles of energy-storage systems
are summarized in palizban2014microgrids . A classification of microgrid
control strategies into primary, secondary, and tertiary levels is done
in olivares2014trends . The two-level approach has been intensively
studied. A double-layer coordinated control approach, consisting of the
scheduling layer and the dispatch layer, is adopted in jiang2013energy .
The schedule layer provides an economical operation scheme, including
state and power of controllable units based on the look-ahead multi-step
optimization. In contrast, the dispatch layer follows the scheduling
layer by considering power flow and voltage limits. A two-stage dispatch
strategy for grid-connected systems is discussed in wu2014hierarchical ,
where the first stage deals with the day-ahead schedule, optimizing
capital and operational cost. At the same time, the lower level handles
the rescheduling of the units for few hours ahead with a time resolution
of 15 min. A two-stage control strategy for a PV BESS-ICE (Internal
Combustion Engine) microgrid is implemented in sachs2016two . Discrete
Dynamic Programming is used in the first layer, while the second layer
problem is posed as a Boundary Value Problem. An approach with a
high-level deterministic optimizer running at a slow timescale, 15 min,
coupled to a low-level stochastic controller running at a higher
frequency, 1 min, is studied in cominesi2017two . A two-layer predictive
energy management system for microgrids with hybrid energy storage
systems consisting of batteries and supercapacitors is considered in
ju2017two . This approach incorporates the degradation costs of the
hybrid energy storage systems. A practical Energy Management System for
isolated microgrid which considers the operational constraints of
Distributed Energy Resources, active-reactive power balance, unbalanced
system configuration, and loading, and voltage-dependent loads is
studied in solanki2018practical . A two-layer mixed-integer linear
programming predictive control strategy was implemented and tested in
simulation and experimentally in Polimeni2019 . Finally,
moretti2019assessing implemented a two-layer predictive management
strategy for an off-grid hybrid microgrid featuring controllable and
non-controllable generation units and a storage system.

It is organized as follows. Section 9.1 summarizes the notation. Section
9.2 formulates the problem in an abstract manner. Section 9.3 introduces
the novel two-level value function-based approach and the assumptions
made. Section 9.4 describes the numerical tests. Section 9.5 reports the
results. Conclusions are drawn in Section 9.6 . The methodology used for
forecasting is reported in Section 3.3 of Chapter 3 .

#### 9.1 Notation

##### Sets and indices

##### Parameters

##### Forecasted or computed variables

#### 9.2 Problem statement

A global microgrid control problem can be defined, for a given microgrid
design and configuration, as operating a microgrid safely and in an
economically efficient manner, by harvesting as much renewable energy as
possible, operating the grid efficiently, optimizing the service to the
demand side, and optimizing other side goals. We refine this definition
below and start by making a few assumptions.

##### 9.2.1 Assumptions

In this study, the control optimizes economic criteria, which are only
related to active power. All devices areconnected to the same electrical
bus, which can be connected or disconnected from a public grid
permanently or dynamically. Device-level controllers offer an interface
to communicate their operating point and constraints, e.g. , maximum
charge power as a function of the current state, and implement control
decisions to reach power set-points. Fast load-frequency control,
islanding management, as well as reactive power control are not in
scope. The microgrid is a price taker in energy and reserve markets.

##### 9.2.2 Formulation

Abstractly, a microgrid optimization problem can be formulated as
follows

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (9.1a)
     @xmath   @xmath      (9.1b)
              @xmath      (9.1c)
  -- -------- -------- -- --------

A controller has to return a set of actions @xmath at any time @xmath
over the life of the microgrid ( @xmath ). Actions should be taken as
frequently as possible to cope with the economic impact of the
variability of the demand and generation sides, but not too often to let
transients vanish, e.g. , every few seconds. The time delta between
action @xmath and the next action taken is denoted by @xmath , and is
not necessarily constant. Some of these actions are purely
market-related @xmath , while other actions are communicated as
set-points to the devices of the microgrid @xmath . The state @xmath of
the microgrid at time @xmath is thus also made of two parts. (1) @xmath
represents the state of the devices, such as a storage system or a
flexible load. (2) @xmath gathers information related to the current
market position, such as the nominated net position of the microgrid
over the next market periods. The cost function @xmath gathers all the
economic criteria considered. The transition function @xmath describes
the physical and net position evolution of the system. At time instants
@xmath , with @xmath the market period, some costs are incurred based on
the value of some state variables, which are then reset for the
following market period. This problem is challenging to solve since the
system’s evolution is uncertain, actions have long-term consequences,
and are both discrete and continuous. Furthermore, although functions
@xmath and @xmath are assumed time-invariant, they are generally
non-convex and parameterized with stochastic variables @xmath .

#### 9.3 Proposed method

In practice, solving the microgrid optimization problem above amounts,
at every time @xmath , to forecasting the stochastic variables @xmath ,
then solving the problem ¹ ¹ 1 Which is here expressed as a
deterministic problem for simplicity but should be treated as a
stochastic problem in practice.

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (9.2a)
     @xmath   @xmath      (9.2b)
              @xmath      (9.2c)
  -- -------- -------- -- --------

and applying @xmath (potentially changing @xmath at some specific
moments only). Forecasts are valid only for a relatively near future,
and optimizing over a long time would be incompatible with the real-time
operation. Thus, this problem is approximated by cropping the lookahead
horizon to @xmath . However, market decisions must be refreshed much
less frequently than set-points. We thus propose to further decompose
the problem in an operational planning problem (OPP) for @xmath that
computes market decisions

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (9.3a)
     @xmath   @xmath      (9.3b)
              @xmath      (9.3c)
  -- -------- -------- -- --------

and a real-time problem (RTP) that computes set-points for time @xmath

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (9.4a)
              @xmath      (9.4b)
              @xmath      (9.4c)
  -- -------- -------- -- --------

with @xmath . The function @xmath is the cost-to-go as a function of the
system’s state at the end of the ongoing market period. It regularizes
decisions of RTP to account for the longer-term evolution of the system.
We detail hereunder how we obtain @xmath . An overview of the approach
is depicted in Figure 9.2 .

##### 9.3.1 Computing the cost-to-go function

The function @xmath represents the optimal value of ( 9.3 ) as a
function of the initial state @xmath of this problem. If we make the
assumption that ( 9.3 ) is modeled as a linear program, the function
@xmath is thus convex and piecewise linear. Every evaluation of ( 9.3 )
with the additional constraint ² ² 2 The @xmath notation means that
@xmath is the dual variable of the constraint.

  -- -------- -- -------
     @xmath      (9.5)
  -- -------- -- -------

yields the value @xmath and a supporting inequality (a cut)

  -- -------- -- -------
     @xmath      (9.6)
  -- -------- -- -------

The algorithm to approximate @xmath works as follows:

1.  estimate the domain of @xmath , i.e. , the range of states reachable
    at time @xmath and the most probable state that will be reached
    @xmath ;

2.  evaluate @xmath and the associated @xmath ;

3.  repeat step 2 for other state values until all regions of @xmath are
    explored.

Note: that if the state is of dimension one and ( 9.3 ) is a linear
program, simplex basis validity information can be used to determine for
which part of the domain of @xmath the current cut is tight, else a
methodology such as proposed in bemporad2003greedy can be used.

##### 9.3.2 OPP formulation

The OPP objective function implemented for the case study is

  -- -------- -------- -- -------
     @xmath   @xmath      (9.7)
  -- -------- -------- -- -------

with Operational Planner (OP) the name of this planer. @xmath is
composed of 96 values with @xmath minutes and @xmath hours. @xmath
models the immediate costs and @xmath the delayed costs at @xmath .
@xmath takes into account different revenues and costs related to energy
flows: the costs of shed demand, steered and non steered generation, the
revenues from selling energy to the grid, the costs of purchasing energy
from the grid and the costs for using storage

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      (9.8)
  -- -------- -------- -- -------

@xmath is composed of the peak cost and symmetric reserve revenue

  -- -------- -------- -- -------
     @xmath   @xmath      (9.9)
  -- -------- -------- -- -------

@xmath is the peak difference between the previous maximum historic peak
@xmath and the current peak within the market period @xmath . @xmath is
the symmetric reserve provided to the grid within the current market
period @xmath .

##### 9.3.3 OP constraints

The first set of constraints defines bounds on state and action
variables, @xmath

  -- -------- -------- -- ---------
                          
     @xmath   @xmath      (9.10a)
     @xmath   @xmath      (9.10b)
     @xmath   @xmath      (9.10c)
     @xmath   @xmath      (9.10d)
  -- -------- -------- -- ---------

The energy flows are constrained, @xmath , by

  -- -------- -- ---------
                 
     @xmath      
     @xmath      (9.11a)
     @xmath      (9.11b)
     @xmath      (9.11c)
  -- -------- -- ---------

The dynamics of the state of charge are, @xmath

  -- -------- -- ---------
                 
     @xmath      (9.12a)
     @xmath      (9.12b)
     @xmath      (9.12c)
  -- -------- -- ---------

The set of constraints related to the peak power @xmath

  -- -------- -------- -- ---------
                          
     @xmath   @xmath      (9.13a)
     @xmath   @xmath      (9.13b)
     @xmath   @xmath      (9.13c)
  -- -------- -------- -- ---------

The last constraints define symmetric reserve @xmath

  -- -------- -- ---------
                 
     @xmath      (9.14a)
     @xmath      (9.14b)
     @xmath      (9.14c)
     @xmath      (9.14d)
     @xmath      (9.14e)
     @xmath      
     @xmath      (9.14f)
  -- -------- -- ---------

##### 9.3.4 RTP formulation

The RTP objective function implemented for the case study is

  -- -------- -------- -- --------
     @xmath   @xmath      (9.15)
  -- -------- -------- -- --------

with Real-Time Optimizer (RTO) the name of this controller. @xmath
models the immediate costs, @xmath the delayed costs and @xmath the
cost-to-go function of the state of the system at time @xmath within a
current market period. @xmath is the same as @xmath by replacing @xmath
by @xmath , @xmath by @xmath and considering only one period of time
@xmath . @xmath is composed of the peak cost and symmetric reserve
penalty costs

  -- -------- -------- -- --------
     @xmath   @xmath      (9.16)
  -- -------- -------- -- --------

@xmath is the peak difference between the previous maximum historic peak
@xmath and the current peak within the market period computed by RTO.
The difference with OP relies on its computation as at @xmath the market
period is not finished. Thus, the peak within this market period is made
of two parts. (1) The peak from the beginning of the market period to
@xmath . (2) The peak from the actions taken from @xmath to the end of
the market period. @xmath is the difference between the symmetric
reserve computed by OP and the current reserve within the market period
computed by RTO. @xmath is the reserve activation signal to activate the
tertiary symmetric reserve. It is set by the TSO, 0 if activated, else
1. The activation occurs at the beginning of the following market
period.

##### 9.3.5 RTO constraints

The set of constraints that defines the bounds on state and action
variables and the energy flows are the same as the OP ( 9.10 ) and (
9.11 ) by replacing @xmath by @xmath , @xmath by @xmath and considering
only one period of time @xmath . The following constraint describes the
dynamics of the state of charge @xmath and @xmath

  -- -------- -- --------
     @xmath      (9.17)
  -- -------- -- --------

The set of constraints related to the peak power @xmath

  -- -------- -------- -- ---------
                          
     @xmath   @xmath      (9.18a)
     @xmath   @xmath      (9.18b)
     @xmath   @xmath      (9.18c)
     @xmath   @xmath      (9.18d)
  -- -------- -------- -- ---------

with @xmath . The last set of constraints defining the symmetric reserve
are the same as the OP ( 9.14 ) by replacing @xmath by @xmath , @xmath
by @xmath and adding @xmath

  -- -------- -- ---------
                 
     @xmath      (9.19a)
     @xmath      (9.19b)
  -- -------- -- ---------

#### 9.4 Test description

Our case study is the MiRIS microgrid located at the John Cockerill
Group’s international headquarters in Seraing, Belgium ³ ³ 3
https://johncockerill.com/fr/energy/stockage-denergie/ . It is composed
of PV, several energy storage devices, and a non-sheddable load. The
load and PV data we use come from on-site monitoring. All data,
including the weather forecasts, are available on the Kaggle platform ⁴
⁴ 4 https://www.kaggle.com/jonathandumas/liege-microgrid-open-data . The
case study consists of comparing to a Rule-Based Controller (RBC) for
three configurations of the installed PV capacity, cf. Table 9.1 . The
RBC prioritizes the use of PV production for the supply of electrical
demand. If the microgrid is facing a long position, it charges the
battery. Furthermore, if this one is fully charged, it exports to the
main grid. If the microgrid is facing a short position, it prioritizes
using the battery to supply the demand. Moreover, if this one is fully
discharged, it imports from the main grid. This controller does not take
into account any future information, e.g. , PV, consumption forecasts,
energy prices, or market information such as the peak of the symmetric
reserve. Case 3 results from a sizing study that defined the optimal
device sizes given the PV and consumption data. The sizing methodology
used is described in dakir2019sizing .

Figure 9.3 shows the PV & consumption data over the simulation period:
from @xmath to @xmath . The selling price @xmath is constant, and the
purchasing price is composed of a day @xmath and night prices @xmath .
Day prices apply from 5 a.m. to 8 p.m. (UTC) during the weekdays, and
night prices apply from 8 p.m. to 5 a.m. during weekdays and the entire
weekend. The peak mechanism is taken into account with a constant peak
price @xmath and an initial maximum historic peak @xmath . Storage
systems are initially fully charged. The PV and consumption data have a
1-second resolution, meaning the RTO could compute its optimization
problem each five to ten seconds in operational mode. CPLEX 12.9 is used
to solve all the optimization problems on an Intel Core i7-8700 3.20
GHz-based computer with 12 threads and 32 GB of RAM. The average
computation time per optimization problem composed of the OP and RTO is
a few seconds. However, to maintain a reasonable simulation time, RTO is
called every minute. The dataset is composed of 28 days with an average
computation time of two hours to solve 1440 optimization problems per
day, with a one-minute resolution, leading to two days for the entire
dataset. The OP computes a quarterly planning corresponding to the
Belgian market period. The computation time of the RTO on a regular
computer is around a few seconds and the OP around twenty seconds. In
total, the simulation computation time is up to a few hours. The OP
computes quarterly planning based on PV and consumption twenty-four
ahead forecasts. The weather-based forecast methodology is described in
detail in Section 3.3 of Chapter 3 . Two "classic" deterministic
techniques are implemented, a Recurrent Neural Network (RNN) with the
Keras Python library ( chollet2015keras ) and a Gradient Boosting
Regression (GBR) with the Scikit-learn Python library ( scikit-learn ) .
These models use as input the weather forecasts provided by the
Laboratory of Climatology of the Liège University, based on the MAR
regional climate model ( fettweis2017reconstructions ) . It is an
atmosphere model designed for meteorological and climatic research, used
for a wide range of applications, from km-scale process studies to
continental-scale multi-decade simulations. To estimate the impact of
the PV and consumption forecast errors on the controllers, the
simulation is performed with the OP having access to the PV and
consumption future values ( @xmath ). Then, the simulation is performed
with the symmetric reserve mechanisms to cope with the forecast errors.
A constant symmetric reserve price @xmath for the OP and a penalty
reserve @xmath for the RTO are set to 20 (€/ kW).

#### 9.5 Numerical results

##### 9.5.1 No symmetric reserve

Table 9.2 provides the simulation results without taking into account
the symmetric reserve. The smaller the PV installed capacity, the higher
the peak and energy costs. The @xmath provides the minimal peak cost,
whereas the RBC provides the minimal energy cost on all cases. However,
@xmath achieves the minimal total cost, composed of the energy and peak
costs.

This simulation illustrates the impact of the forecasts on the behavior.
The RNN forecaster provides the best results, but the @xmath is still a
long way to manage the peak as @xmath due to the forecasting errors. The
peak cost strongly penalizes the benefits as it applies to the entire
year ahead once it has been reached.

In case 3, all the controllers except @xmath reached the maximum peak on
@xmath around 10:30 a.m. as shown on Figure 9.4 . Figure 9.3 shows a
sudden drop in the PV production around 10 a.m. that is not accurately
forecasted by the RNN and GBR forecasters as shown in Figure 3.2 . This
prediction leads to the non-accurate planning of OP. Thus, the RTO
cannot anticipate this drop and has to import energy to balance the
microgrid at the last minute. Figure 9.5 shows the controllers behavior
on @xmath where the peak is reached. In case 2, all controllers reached
the same peak as in case 3 except @xmath that reached a smaller one on
@xmath . The forecast’s accuracy explains this behavior as in case 3.
Finally, in case 1, each controller reached a different peak. The
smallest one is achieved by the @xmath , followed by the @xmath . These
cases show that the controller optimizes PV-storage usage and thus
requires less installed PV capacity for a given demand level. This
result was expected as the peak management is not achieved by the RBC
and becomes critical when the PV production is smaller than the
consumption. This simulation also demonstrates the forecast accuracy
impact on the behavior.

##### 9.5.2 Results with symmetric reserve

Table 9.3 provides the simulation results by taking into account the
symmetric reserve. Figure 9.6 depicts on case 3 the behavior differences
between @xmath without and with symmetric reserve. Figures 9.7 and 9.8
show the SOC and peaks costs evolution of case 2 & 1. The controller
tends to maintain a storage level that allows @xmath to better cope with
forecast errors. Indeed for case 3, there is no more peak reached by
@xmath , only 1 kW for case 2, and it has been almost divided by two for
case 1. However, this behavior tends to increase the energy cost if the
PV production is large compared to the consumption, such as in case 3.
Indeed, the controller will tend to store more energy in the battery
instead of exporting it. @xmath did not perform better with the
symmetric reserve. The symmetric reserve competes with the peak
management, and the @xmath tends not to discharge the battery entirely
even if it is required to avoid a peak. In case 2, the peak is reached
on @xmath around 08:00. The controller could have avoided it by totally
discharging the battery but did not maintain the reserve level. It is
the same behavior in case 1, where the peak could have been limited if
all the battery was discharged. There is an economic trade-off to manage
the peak and the reserve simultaneously. It depends on the valorization
or not on the market of the symmetric reserve. The reserve can also be
valorized internally to cope with non or complex forecastable events.
Such as a sudden drop in export or import limits due to loss of
equipment or grid congestion.

#### 9.6 Conclusions

A two-level value function-based approach is proposed as a solution
method for a multi-resolution microgrid optimization problem. The value
function computed by the operational planner based on PV and consumption
forecasts allows coping with the forecasting uncertainties. The
real-time controller solves an entire optimization problem, including
the future information propagated by the value function. This approach
is tested on the MiRIS microgrid case study with PV and consumption data
monitored on-site. The results demonstrate the efficiency of this method
to manage the peak in comparison with a Rule-Based Controller. This test
case is completely reproducible as all the data used are open, PV,
consumption monitored, and forecasted, including the weather forecasts.
The proposed method can be extended in three ways. First, a stochastic
formulation of the operational planning problem to cope with
probabilistic forecasts. Second, adding the balancing market mechanisms.
Finally, considering a community composed of several entities inside the
microgrid.

### Chapter 10 Capacity firming using a stochastic approach {infobox}

Overview This Chapter proposes a stochastic approach to address the
energy management of a grid-connected renewable generation plant coupled
with a battery energy storage device in the capacity firming market.
Both deterministic and stochastic approaches result in optimization
problems formulated as quadratic problems with linear constraints. The
considered case study is a real microgrid with PV production monitored
on-site.

References: This chapter is an adapted version of the following
publication:
\bibentry dumas2020stochastic.

  “ We are our choices. ”

  — Jean-Paul Sartre

The capacity firming framework is mainly designed for isolated markets,
such as the Overseas France islands. For instance, the French Energy
Regulatory Commission (CRE) publishes capacity firming tenders and
specifications. The system considered is a grid-connected renewable
energy power plant, e.g. , photovoltaic or wind-based, with a battery
energy storage system (BESS) for firming the renewable generation. At
the tendering stage, offers are selected on the electricity selling
price. Then, the successful tenderer builds its plant and sells the
electricity exported to the grid at the contracted selling price, but
according to a well-defined daily engagement and penalization scheme
specified in the tender specifications. The electricity injected in or
withdrawn from the grid must be nominated the day-ahead, and engagements
must satisfy ramping power constraints. The remuneration is calculated a
posteriori by multiplying the realized exports by the contracted selling
price minus a penalty. The deviations of the realized exports from the
engagements are penalized through a function specified in the tender. A
peak option can be activated in the contract for a significant selling
price increase during a short period defined a priori. Therefore, the
BESS must shift the renewable generation during peak hours to maximize
revenue and manage renewable energy uncertainty.

The problem of modeling a two-phase engagement/control with an approach
dealing with uncertainty in the context of the CRE capacity framework is
still an open issue. This framework has received less attention in the
literature than more traditional energy markets such as day-ahead and
intraday markets of European countries.

The optimal day-ahead bidding strategies of a plant composed of only a
production device have been addressed in, e.g. , pinson2007trading ;
bitar2012bringing ; giannitrapani2014bidding ; giannitrapani2015bidding
. The optimal offer turns out to be a suitable percentile of the PV/wind
power cumulative distribution function. Under the assumption of
time-invariant power generation statistics, the cumulative distribution
functions can be estimated from historical data of the power generated
by the plant. This assumption is not always justified, especially for PV
power generation. In giannitrapani2015bidding , the authors investigate
two approaches to properly take into account the effects of seasonal
variation and non-stationary nature of PV power generation in the
estimation of PV power statistics. However, incorporating energy storage
in the framework is still an open problem, and the literature provides
several approaches and methodologies to this end. An optimal power
management mechanism for a grid-connected PV system with storage is
implemented in riffonneau2011optimal using Dynamic Programming (DP) and
is compared with simple ruled-based management. The sizing and control
of an energy storage system to mitigate wind power uncertainty is
addressed by haessig2014dimensionnement ; haessig2013aging ;
haessig2015energy using stochastic dynamic programming (SDP). The
framework is similar to the CRE PV capacity firming tender with a wind
farm operator committed on a day-ahead basis to a production engagement.
Finally, three distinct optimization strategies, mixed-integer quadratic
programming, simulation-based genetic algorithm, and expert-based
heuristic, are empirically compared by n2019optimal in the CRE
framework.

This study addresses the energy management of a grid-connected PV plant
and BESS. This topic is studied within the capacity firming
specifications of the CRE, in line with the tender AO-CRE-ZNI 2019
published on @xmath , using the MiRIS microgrid case study. The capacity
firming problem can be decomposed into two steps. The first step
consists of computing the day-ahead nominations. The second step
consists of computing the renominations and the set-points in real-time
to minimize the energy and ramp power deviations from nominations. This
study focuses on the first step and proposes both a stochastic and a
deterministic formulation. The main goal of this study is to validate
the stochastic approach by using an ideal predictor providing unbiased
PV scenarios. Thus, the BESS efficiencies are perfect, and the
degradation is not taken into account for the sake of simplicity.
Different levels of prediction accuracy are evaluated. Then, the results
are compared with those of the deterministic formulation, assuming
perfect forecasts returned by an oracle. Both deterministic and
stochastic approaches result in optimization problems formulated as
quadratic problems with linear constraints. The considered case study is
a real microgrid with PV production monitored on-site.

The study is organized as follows. Section 10.1 provides the notation
that is also used for Chapters 11 and 12 . Section 10.2 describes the
capacity firming framework. Section 10.3 proposes the deterministic and
stochastic formulations of the nomination process. Section 10.4
introduces the MiRIS microgrid case study and presents the results.
Conclusions are drawn in Section 10.5 . Appendix 10.6 describes the
methodology to generate the set of unbiased PV scenarios.

#### 10.1 Notation

##### 10.1.1 Sets and indices

##### 10.1.2 Parameters

##### 10.1.3 Variables

For the sake of clarity the subscript @xmath is omitted.

##### Dual variables, and corresponding constraints

Dual variables of constraints are indicated with brackets @xmath .

#### 10.2 The Capacity Firming Framework

The capacity firming framework can be decomposed into a day-ahead
engagement process, Section 10.2.1 , and a real-time control process,
Section 10.2.2 . Each day is discretized in @xmath periods of duration
@xmath . In the sequel, the period duration is the same for day-ahead
engagement and the real-time control, @xmath is used as a period index,
and @xmath is the set of periods in a day.

##### 10.2.1 Day-ahead engagement

Each day, the operator of the renewable generation plant is asked to
provide the generation profile to be followed the next day to the grid
operator, based on renewable generation forecasts. More formally, a
planner computes on a day-ahead basis, before a deadline, a vector of
engagements composed of @xmath values @xmath . Figure 10.2 illustrates
the day-ahead nomination process. The grid operator accepts the
engagements if they satisfy the constraints

  -- -------- -------- -- ---------
                          
     @xmath   @xmath      (10.1a)
     @xmath   @xmath      (10.1b)
     @xmath   @xmath      (10.1c)
  -- -------- -------- -- ---------

with @xmath a ramping power constraint, a fraction of the total
installed capacity @xmath determined at the tendering stage and imposed
by the grid operator.

##### 10.2.2 Real-time control

Then, in real-time, a receding-horizon controller computes at each
period the generation level and the charge or discharge set-points from
@xmath to @xmath , based on forecasts of renewable generation and the
engagements. Only the set-points of the first period are applied to the
system. The remuneration is calculated ex-post based on the realized
power @xmath at the grid coupling point. For a given control period, the
net remuneration @xmath of the plant is the gross revenue @xmath minus a
penalty @xmath , with @xmath the contracted selling price set at the
tendering stage

  -- -------- -- --------
     @xmath      (10.2)
  -- -------- -- --------

The penalty function @xmath depends on the specifications of the tender.
For the sake of simplicity for the rest of this Chapter, @xmath is
assumed to be symmetric, convex, and quadratic piecewise-linear

  -- -------- -------- -- --------
     @xmath   @xmath      (10.3)
  -- -------- -------- -- --------

with @xmath , and @xmath is a slack price (€/ @xmath ).

#### 10.3 Problem formulation

The problem statement follows the abstract formulation defined in
Chapter 9 where a global microgrid control problem can be defined as
operating a microgrid safely and in an economically efficient manner. In
the capacity firming context, a two-stage approach is considered with a
planner and a controller. A quadratic formulation with linear
constraints models the CRE non-convex penalty. In this study, the
planner and controller optimize economic criteria, which are only
related to active power. The ancillary or grid services are not in the
scope of the capacity firming specifications. The BESS degradation is
not taken into account. The planner and controller horizons are cropped
to twenty-four hours.

Deterministic (D) and stochastic (S) formulations of the day-ahead
nomination problem are compared. The deterministic formulation is used
as a reference to validate the stochastic approach by considering
perfect knowledge of the future (D @xmath ). In this Chapter, both
approaches consider only exports to the grid ¹ ¹ 1 The imports from the
grid are allowed only under specific conditions into the contract. . The
optimization variables and the parameters are defined in Section 10.1 .

##### 10.3.1 Deterministic approach

The objective function @xmath to minimize is the opposite of the net
revenue

  -- -------- -------- -- --------
     @xmath   @xmath      (10.4)
  -- -------- -------- -- --------

The deterministic formulation is the following Mixed-Integer Quadratic
Program (MIQP)

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (10.5)
     @xmath   @xmath      
  -- -------- -------- -- --------

where @xmath and @xmath are the sets of feasible engagements @xmath and
dispatch solutions @xmath for a fixed engagement @xmath and renewable
generation point forecast @xmath . The optimization variables of (
10.3.1 ) are the engagement variables @xmath , the dispatch variables
@xmath (the net power at the grid connection point), @xmath (BESS
discharging power), @xmath (BESS charging power), @xmath (BESS state of
charge), @xmath (BESS binary variables), @xmath (renewable generation),
and @xmath (deviation variables) (cf. the notation Section 10.1 ). From
( 10.1 ), the engagement constraints are

  -- -------- -------- -- ---------
                          
     @xmath   @xmath      (10.6a)
     @xmath   @xmath      (10.6b)
     @xmath   @xmath      (10.6c)
     @xmath   @xmath      (10.6d)
  -- -------- -------- -- ---------

The ramping constraint on @xmath is deactivated to decouple consecutive
days of simulation. In reality, the updated value of the last engagement
of the previous day would be taken to satisfy the constraint. The set of
constraints that bound @xmath , @xmath , and @xmath variables are @xmath

  -- -------- -------- -------- -- ---------
                                   
     @xmath   @xmath   @xmath      (10.7a)
     @xmath   @xmath   @xmath      (10.7b)
     @xmath   @xmath   @xmath      (10.7c)
     @xmath   @xmath   @xmath      (10.7d)
  -- -------- -------- -------- -- ---------

where @xmath are binary variables that prevent the simultaneous charge
and discharge of the BESS. The power balance equation and the
constraints on the net power at the grid connection point are @xmath

  -- -------- -------- -------- -- ---------
                                   
     @xmath   @xmath   @xmath      (10.8a)
     @xmath   @xmath   @xmath      (10.8b)
     @xmath   @xmath   @xmath      (10.8c)
  -- -------- -------- -------- -- ---------

The dynamics of the BESS state of charge are

  -- -------- -------- -------- -- ---------
                                   
     @xmath   @xmath   @xmath      (10.9a)
     @xmath   @xmath   @xmath      (10.9b)
     @xmath   @xmath   @xmath      (10.9c)
  -- -------- -------- -------- -- ---------

where the parameters @xmath and @xmath are introduced to decouple
consecutive days of simulation. In reality, @xmath would be the updated
value of the last measured state of charge of the previous day. The
variables @xmath are defined @xmath to model the penalty

  -- -------- -------- -------- -- ----------
                                   
     @xmath   @xmath   @xmath      (10.10a)
     @xmath   @xmath   @xmath      (10.10b)
  -- -------- -------- -------- -- ----------

with @xmath . Finally, the PV generation is bounded @xmath by the point
forecast @xmath

  -- -------- -------- -------- -- ---------
     @xmath   @xmath   @xmath      (10.11)
  -- -------- -------- -------- -- ---------

##### 10.3.2 Deterministic approach with perfect forecasts

With perfect forecasts, ( 10.3.1 ) becomes

  -- -------- -------- -- ---------
     @xmath   @xmath      (10.12)
  -- -------- -------- -- ---------

with @xmath @xmath in ( 10.11 ).

##### 10.3.3 Stochastic approach

In the stochastic formulation, the objective is given by

  -- -------- -------- -- ---------
     @xmath   @xmath      (10.13)
  -- -------- -------- -- ---------

where the expectation is taken with respect to @xmath . Using a
scenario-based approach, ( 10.13 ) is approximated by

  -- -------- -------- -- ---------
     @xmath   @xmath      (10.14)
  -- -------- -------- -- ---------

with @xmath the probability of scenario @xmath , and @xmath . Then, the
problem to solve becomes

  -- -------- -------- -- ---------
     @xmath   @xmath      (10.15)
  -- -------- -------- -- ---------

All the optimization variables but @xmath are now defined @xmath .

##### 10.3.4 Evaluation methodology

The second step of capacity firming, i.e. , computing the set-points in
real-time, is required to assess the quality of the nomination process.
However, since this study focuses on the computation of day-ahead
engagements, we simulate the second step with an ideal real-time
controller ² ² 2 Using a real-time controller with intraday forecasts is
required to assess the planner-controller. However, this study focus
only on the nomination step. once the engagements are fixed. The
methodology to assess the engagements consists of solving

  -- -------- -------- -- ---------
     @xmath   @xmath      (10.16)
  -- -------- -------- -- ---------

s.t ( 10.7 )-( 10.11 ) with @xmath in ( 10.11 ) and given engagements
@xmath previously computed by the planner S. The optimization variables
of ( 10.16 ) are @xmath , @xmath , @xmath , @xmath , @xmath , @xmath ,
and @xmath . The optimal value of @xmath is compared with the optimal
value of @xmath in ( 10.12 ).

#### 10.4 MiRIS microgrid case study

The MiRIS ³ ³ 3 https://johncockerill.com/fr/energy/stockage-denergie/
microgrid case study, located at the John Cockerill Group’s
international headquarters in Seraing, Belgium, is composed of a PV
production plant, a BESS, and a load. For the need of this study, only
historical data of PV generation are required. The BESS capacity @xmath
is 1000 kWh, @xmath , and the total PV installed capacity @xmath is 2000
kWp. The market period duration @xmath is 15 minutes. The simulation
dataset @xmath is the month of February 2019. Figure 10.3 illustrates
the MiRIS PV production and Table 10.1 provides some key statistics.
Table 10.2 defines the indicators used in this section.

It is of paramount importance to notice that the results of this case
study are only valid for this dataset and cannot be extrapolated over an
entire year without caution. CPLEX ⁴ ⁴ 4
https://www.ibm.com/products/ilog-cplex-optimization-studio 12.9 is used
to solve all the optimization problems, on an Intel Core i7-8700 3.20
GHz based computer with 12 threads and 32 GB of RAM. Tables 10.3 and
10.4 provide the case study and BESS parameters.

##### 10.4.1 Results for unbiased PV scenarios with fixed variance

A set of unbiased PV scenarios is generated for several values of the
standard deviation @xmath of the prediction error. Table 10.5 shows the
considered values of @xmath , expressed as a fraction of the actual PV
generation. Moreover, Table 10.5 reports the cardinality of the
generated scenario sets. Table 10.6 compares the average computation
time per optimization problem between planners S and D @xmath . Note,
The optimization problem of planner S with @xmath has the same number of
variables and constraints as the planner D @xmath . The computation time
is compatible with a day-ahead process even with 100 scenarios, as it
takes on average 7 seconds to compute the nominations for the day-ahead.
Table 10.7 and Figure 10.4 provide the results of the ratio indicators,
respectively, for the planners D @xmath and S.

For all indicators, the results of both planners are almost equal with
the smaller value of @xmath and the highest value of @xmath , as
expected. On average, the curtailment of PV generation equals @xmath .
The maximum @xmath value is achieved with @xmath because the nominations
are more conservative when the variance increases, leading to a smaller
ratio. On average, 30% (27%) of the production, for planner D @xmath
(S), is stored in the BESS over the entire dataset. @xmath is equal to
17.9% (17.9% ⁵ ⁵ 5 The value is the same for the @xmath and @xmath
values considered. ) for the planner D @xmath (S), meaning the BESS
reached its maximum storage level 5 days out of the 28 days of the
dataset. In fact, during sunny days, the BESS is fully charged. A larger
BESS capacity should decrease the curtailment and improve the gross
revenue. Note: it is a winter month where the maximum generated PV power
reached only half of the installed PV capacity. During a summer month,
the maximum production should reach at least 80% of the total installed
capacity on sunny days. Thus, with a storage capacity of 1 MWh, the
curtailment is expected to be higher during sunny summer days.

Table 10.8 and Figure 10.5 provide the results of the revenue indicators
for the planners D @xmath and S, respectively. It should be noted that
in this case, @xmath .

The smallest value of the objective function is achieved by the planner
D @xmath . It is followed closely by the planner S, even for the highest
value of @xmath . This result demonstrates the validity of the approach
when exploiting an unbiased stochastic predictor.

In terms of net revenue, both planners achieved 93.7% of @xmath k€,
which results in a loss of 6.3%. Most of this loss is due to the
curtailment of PV generation. For both planners, the net revenue
increases with the generation.

For sunny days, the difference between the nominations and the exports
is higher than the tolerance just before the production occurs, between
5 and 8 am. Indeed, the planner tends to maximize the revenue by
maximizing the exports. However, the ramping power constraints ( 10.6 )
impose a maximum difference between two consecutive nominations. To
maximize the net revenue over the entire day, the planner computes
nominations that are not achievable at the beginning of the day to
maximize the exports during the day. This results in a penalty between 5
and 8 am.

##### 10.4.2 BESS capacity sensitivity analysis

The goal is to conduct a sensitivity analysis on the BESS capacity
@xmath to determine its marginal value and the optimal BESS size @xmath
for a given CAPEX @xmath . The efficiencies are still assumed to be
unitary. @xmath and @xmath are set to 0 kWh. Table 10.9 provides the
other BESS parameters for the five cases. The scenarios are generated
using @xmath , and @xmath .

A new indicator, expressed in k€, is defined to quantify the gain
provided by the BESS over fifteen years

  -- -------- -- ---------
     @xmath      (10.17)
  -- -------- -- ---------

It is a lower bound of the total gain as it relies on the results of a
winter month. A summer month should provide higher revenue. Table 10.10
provide the planner D @xmath indicators. The results demonstrate the
interest in using a BESS to optimize the bidding. The larger the BESS
is, the lower the curtailment is. Thus, the net revenue increases with
the BESS capacity. The maximum achievable revenue is reached with a
storage capacity of 2 MWh. However, the larger the BESS is, the smaller
@xmath increases. It means the marginal benefit decreases with the
increase of BESS capacity. A trade-off should be found between the BESS
capacity and its CAPEX. Figure 10.5(a) provides @xmath and its quadratic
interpolation in comparison with two BESS prices @xmath 0.1 and 0.228
k€/kWh. The value of the derivative @xmath provides the maximum CAPEX
that provides a profitable BESS. Then, the optimal storage capacity
@xmath for a given CAPEX is provided solving @xmath . For instance, with
a CAPEX of 0.1 k€/kWh, @xmath is approximately 350 kWh. Figure 10.5(b)
provides the values of @xmath with a quadratic interpolation.

Figure 10.7 provides the planner S revenue indicators. The results are
still almost identical for all indicators for the smallest value of
@xmath and very close with the highest one, as expected.

#### 10.5 Conclusions and perspectives

This Chapter addresses the energy management of a grid-connected PV
plant coupled with a BESS within the capacity firming framework. The
method is composed of two steps: computing the day-ahead nominations,
then computing the renominations and the set-points in real-time to
minimize the energy and ramp power deviations from nominations. This
study investigates the first step by comparing a stochastic and a
deterministic formulation. The main goal is to validate the stochastic
approach by using an ideal predictor providing unbiased PV scenarios.

The results of the stochastic planner are comparable with those of the
deterministic planner, even when the prediction error variance is
non-negligible. Finally, the BESS capacity sensitivity analysis results
demonstrate the advantage of using a BESS to optimize the bidding
day-ahead strategy. However, a trade-off must be found between the
marginal gain provided by the BESS and its investment and operational
costs.

Several extensions of this work are under investigation. The first is to
assess the planner’s behavior better using a full year of data. Then,
the next challenge is to use a more realistic methodology to generate PV
generation scenarios. Several scenario generation approaches could be
investigated, based on a point forecast model such as the PVUSA model (
dows1995pvusa ; bianchini2013model ; bianchini2020estimation ) ,
combined with Gaussian copula papaefthymiou2008using ;
pinson2012evaluating ; golestaneh2016generation . Alternatively, using
the deep generative models introduced in Chapter 6 . Another challenge
is to consider the non-convex penalty function specified by the CRE into
the objective. Finally, the last challenge is to investigate the second
step of the capacity firming problem, for instance, by adapting the
approach implemented in dumas2021coordination .

#### 10.6 Appendix: PV scenario generation

This Annex describes the methodology to generate the set of unbiased PV
scenarios. The goal is to define an ideal unbiased predictor with a
fixed variance over all lead times. In this section, let @xmath be the
current time index, @xmath be the lead time of the prediction, @xmath be
the maximum lead time of the prediction, @xmath be the true value of the
signal @xmath at time @xmath , and @xmath be the value of @xmath
predicted at time @xmath . The forecasts are computed at 4 pm
(nominations deadline) for the day-ahead. With a market period duration
of fifteen minutes, @xmath is equal to 128. The PV forecasts are needed
for lead times from @xmath (00:00 to 00:15 am) to @xmath (11:45 to 12:00
pm). Then, @xmath and @xmath are assumed to be related by

  -- -------- -------- -- ---------
     @xmath   @xmath      (10.18)
  -- -------- -------- -- ---------

The error term @xmath is generated by the moving-average model (
box2015time , Chapter 3)

  -- -------- -------- -- ----------
                          
     @xmath   @xmath      (10.19a)
     @xmath   @xmath      (10.19b)
  -- -------- -------- -- ----------

with @xmath scalar coefficients, @xmath independent and identically
distributed sequences of random variables from a normal distribution
@xmath . Thus, the variance of the error term is

  -- -------- -- ----------
                 
     @xmath      (10.20a)
     @xmath      (10.20b)
  -- -------- -- ----------

It is possible to simulate with this model an increase of the prediction
error variance with the lead time @xmath by choosing

  -- -------- -------- -- ---------
     @xmath   @xmath      (10.21)
  -- -------- -------- -- ---------

( 10.20a ) becomes, @xmath

  -- -------- -------- -- ---------
     @xmath   @xmath      (10.22)
  -- -------- -------- -- ---------

with @xmath defined @xmath by

  -- -------- -------- -- ---------
     @xmath   @xmath      (10.23)
  -- -------- -------- -- ---------

Then, with @xmath , it is possible to make the prediction error variance
independent of the lead time as it increases. Indeed:

  -- -------- -------- -- ---------
     @xmath   @xmath      (10.24)
  -- -------- -------- -- ---------

For instance, with @xmath and @xmath , for @xmath , @xmath that is
approximately 5.26. Thus, @xmath

  -- -------- -- ---------
     @xmath      (10.25)
  -- -------- -- ---------

Finally, the @xmath value to set a maximum @xmath with a high
probability of 0.997, corresponding to a three standard deviation
confidence interval from a normal distribution, is found by imposing
@xmath :

  -- -------- -------- -- ---------
     @xmath   @xmath      (10.26)
  -- -------- -------- -- ---------

with @xmath , @xmath .

### Chapter 11 Capacity firming sizing {infobox}

Overview This Chapter proposes an approach to size a grid-connected
renewable generation plant coupled with a battery energy storage device
in the capacity firming market. The main novelties in the CRE capacity
framework context are three-fold.

1.  First, a MIQP formulation is proposed to address the planning stage
    of the two-phase engagement control that is compatible with a
    scenario approach to approximate the Mixed-Integer Non-Linear
    Programming problem generated by the CRE non-convex penalty
    function. It is compared to the deterministic formulation using
    perfect knowledge of the future and PV point forecasts on empirical
    data from the PV production monitored on-site at the Liège
    University (ULiège), Belgium.

2.  Second, a transparent and easily reproducible Gaussian copula
    methodology is implemented to generate PV scenarios based on the
    parametric PVUSA model using a regional climate model.

3.  Finally, the sizing of the system is addressed by a grid search to
    approximate the optimal sizing for a given selling price using both
    the deterministic and stochastic approaches.

References: This chapter is an adapted version of the following
publication:
\bibentry dumas2020probabilistic.

  “ Each player must accept the cards life deals him or her: but once
  they are in hand, he or she alone must decide how to play the cards in
  order to win the game. ”

  — Voltaire

The sizing has only been investigated by haessig2014dimensionnement in a
similar context but with a wind farm. The main thread motivating the
contribution of this Chapter is to extend Chapter 10 to address the
sizing of a grid-connected PV plant and a BESS subject to grid
constraints in this context. However, the sizing problem is difficult
due to this two-phase engagement control with a day-ahead nomination and
an intraday control to minimize deviations from the planning. A single
sizing optimization problem would result in a non-linear bilevel
optimization problem with an upper level, the sizing part, and a lower
level, the two-phase engagement control. Thus, a grid search is
conducted to approximate the optimal sizing for a given selling price
using both the deterministic and stochastic approaches. In the two-phase
engagement control, the PV uncertainty is considered at the planning
stage by considering a stochastic approach that uses PV scenarios
generated by a Gaussian copula methodology. The planner determines the
engagement profile on a day-ahead basis given a selling price profile,
the PV scenarios, and the system’s current state, including the battery
state of charge. Then, the engagement plan is sent to the grid operator
and to the controller that computes every 15 minutes the production,
i.e. , injection or withdrawal, and set-points, e.g. , BESS charge or
discharge, until the end of the day. The optimization problems are
formulated as Mixed-Integer Quadratic Problems (MIQP) with linear
constraints.

The remainder of this Chapter is organized as follows. Section 11.1
defines the problem statement. Section 11.2 provides the PVUSA
parametric point forecast model, and the Gaussian Copula approach to
generate PV scenarios. Section 11.3 investigates the system sizing using
both the deterministic and stochastic MIQP approaches. Finally, Section
11.4 summarizes the main findings and highlights ideas for further work.
Note, the capacity firming process is described in Section 10.2 of
Chapter 10 .

#### 11.1 Problem statement

In this Chapter, the problem formulation is almost strictly identical to
Chapter 10 . The only difference lies in the definition ( 10.2 ) of the
penalty @xmath . The penalty defined in the CRE specifications of the
tender AO-CRE-ZNI 2019 published on @xmath is adopted to conduct the
sizing. The optimization variables and the parameters are defined in
Section 10.1 .

##### 11.1.1 Stochastic approach

A stochastic planner with a MIQP formulation and linear constraints is
implemented using a scenario-based approach. The planner computes on a
day-ahead basis the engagement plan @xmath to be sent to the grid. The
problem formulation is given by ( 10.15 ), where only short deviations
are considered with the penalty defined in the CRE specifications of the
tender AO-CRE-ZNI 2019 published on @xmath . In compact form, the
optimization problem is

  -- -------- -------- -- ---------
                          
     @xmath               (11.1a)
     @xmath   @xmath      (11.1b)
     @xmath   @xmath      (11.1c)
     @xmath   @xmath      (11.1d)
     @xmath   @xmath      (11.1e)
     @xmath   @xmath      (11.1f)
     @xmath   @xmath      (11.1g)
     @xmath   @xmath      (11.1h)
     @xmath   @xmath      (11.1i)
     @xmath   @xmath      (11.1j)
     @xmath   @xmath      (11.1k)
     @xmath   @xmath      (11.1l)
     @xmath   @xmath      (11.1m)
     @xmath   @xmath      (11.1n)
     @xmath   @xmath      (11.1o)
     @xmath   @xmath      (11.1p)
     @xmath   @xmath      (11.1q)
     @xmath   @xmath      (11.1r)
  -- -------- -------- -- ---------

The optimization variables are @xmath (engagement at the coupling
point), @xmath (net power at the coupling point), @xmath , @xmath
(underproduction), @xmath (PV generation), @xmath (BESS binary
variable), @xmath (BESS charging power), @xmath (BESS discharging
power), and @xmath (BESS state of charge) (cf. the notation Section 10.1
). The engagement constraints are ( 11.1b )-( 11.1e ), where the ramping
constraint on @xmath is deactivated to decouple consecutive days of
simulation. The CRE non-convex piecewise quadratic penalty function is
modeled by the constraints ( 11.1f ) @xmath , that defines the variables
@xmath to model the quadratic penalty for underproduction, and ( 11.1g )
@xmath forbidding overproduction that is non-optimal as curtailment is
allowed n2019optimal . The set of constraints that bound @xmath , @xmath
, @xmath , and @xmath variables are ( 11.1h )-( 11.1l ) @xmath where
@xmath are PV scenarios, and @xmath are binary variables that prevent
the BESS from charging and discharging simultaneously. The power balance
equation and the production constraints are ( 11.1m ) and ( 11.1n )-(
11.1o ) @xmath . The dynamics of the BESS state of charge is provided by
constraints ( 11.1p )-( 11.1r ) @xmath . Note, the parameters @xmath and
@xmath are introduced to decouple consecutive days of simulation.

##### 11.1.2 Deterministic approach

The deterministic (D) formulation of the planner is a specific case of
the stochastic formulation by considering only one scenario where @xmath
become @xmath , PV point forecasts. The deterministic formulation with
perfect forecasts ( @xmath ) is D with @xmath @xmath . For both the
deterministic planners D and @xmath , the optimization variables are
@xmath , @xmath , @xmath , @xmath , @xmath , @xmath , @xmath , and
@xmath .

##### 11.1.3 Oracle controller

The oracle controller is an ideal real-time controller that assumes
perfect knowledge of the future by using as inputs the engagement
profile to compute the set-points, maximize the revenues and minimize
the deviations from the engagements. The oracle controller is @xmath
where the engagements are parameters.

#### 11.2 Forecasting methodology

The Gaussian copula approach has been widely used to generate wind and
PV scenarios in power systems ( pinson2009probabilistic ;
golestaneh2016generation ) . However, to the best of our knowledge,
there is almost no guidance available on which copula family can
describe correlated variations in PV generation (
golestaneh2016generation ) , hence the Gaussian copula family is
selected instead of copulas like Archimedean or Elliptical.

##### 11.2.1 Gaussian copula-based PV scenarios

In this section, let @xmath be the current time index, @xmath be the
lead time of the prediction, @xmath a multivariate random variable,
@xmath , @xmath the marginal cumulative distribution functions, and
@xmath the correlation matrix. The goal is to generate samples of @xmath
. The Gaussian copula methodology consists of generating a trajectory
@xmath from the multivariate Normal distribution @xmath . Then, to
transform each entry @xmath through the standard normal cumulative
distribution function @xmath : @xmath , @xmath , and finally, to apply
to each entry @xmath the inverse marginal cumulative distribution
function of @xmath : @xmath , @xmath . In our case, @xmath is defined as
the error between the PV measurement @xmath and the PV point forecast
@xmath

  -- -------- -- --------
     @xmath      (11.2)
  -- -------- -- --------

@xmath and @xmath are estimated from the data, and following the
methodology described above, a PV scenario @xmath at time @xmath is
generated for time @xmath

  -- -------- -- --------
     @xmath      (11.3)
  -- -------- -- --------

The PV point forecasts @xmath are computed by using the PVUSA model
presented in the following Section.

##### 11.2.2 PV point forecast parametric model

A PV plant can be modeled using the well-known PVUSA parametric model (
dows1995pvusa ) , which expresses the instantaneous generated power as a
function of irradiance and air temperature

  -- -------- -------- -- --------
     @xmath   @xmath      (11.4)
  -- -------- -------- -- --------

where @xmath , @xmath and @xmath are the generated power, irradiance and
air temperature at time @xmath , respectively, and @xmath , @xmath ,
@xmath are the PVUSA model parameters. These parameters are estimated
following the algorithm of bianchini2013model that efficiently exploits
only the power generation measurements and the theoretical clear-sky
irradiance and is characterized by lightweight computational effort. The
same implementation of the algorithm is used with a sliding window of 12
hours. The parameters reached the steady-state values in 50 days on the
Uliège case study, described in the following Section, with

  -- -------- -------- -- --------
     @xmath   @xmath      (11.5)
  -- -------- -------- -- --------

The weather hindcasts from the MAR regional climate model (
fettweis2017reconstructions ) , provided by the Laboratory of
Climatology of the Liège University, are used as inputs of the
parametric PVUSA model. The ERA5 ¹ ¹ 1 The fifth-generation European
Centre for Medium-Range Weather Forecasts atmospheric reanalysis of the
global climate. reanalysis database forces the MAR regional climate to
produce weather hindcasts. Finally, the PV point forecasts are computed

  -- -------- -------- -- --------
     @xmath   @xmath      (11.6)
  -- -------- -------- -- --------

and use as inputs to generate PV scenarios following the Gaussian copula
approach.

##### 11.2.3 PV scenarios

The Uliège case study is composed of a PV generation plant with an
installed capacity of 466.4 kWp. The simulation dataset is composed of
August 2019 to December 2019, 151 days in total, with a total production
of 141.3 MWh. The Uliège PV generation is monitored on a minute basis
and is resampled to 15 minutes. The set of PV scenarios is generated
using the Gaussian copula approach based on the PVUSA point forecasts.
Figure 11.2 illustrates a set of 5 PV scenarios on four days of the
dataset.

The PV scenarios are used as input of a stochastic optimization problem.

#### 11.3 Sizing study

The goal is to determine the optimal sizing of the BESS and PV for a
given selling price and its related net revenue over the lifetime
project to bid at the tendering stage optimally.

##### 11.3.1 Problem statement and assumptions

The ratio @xmath , BESS maximum capacity over the PV installed capacity,
is introduced to model several BESS and PV system configurations. The
total exports, imports, deviation costs, number of charging and
discharging cycles for several @xmath and selling prices are computed.
Based on the BESS and PV CapEx (Capital Expenditure) I and OpEx
(Operational Expenditure) O&M, the LCOE (Levelized Cost Of Energy) is
calculated

  -- -- -------- -- --------
        @xmath      (11.7)
  -- -- -------- -- --------

with CRF the Capital Recovery Factor (or Annuity Factor), and E, W, C,
the annual export, withdrawal, and deviation costs, respectively. Then,
the net revenue over the lifetime project is defined as the annual gross
revenue R divided by the annual export to the grid minus the LCOE

  -- -------- -------- -- --------
     @xmath   @xmath      (11.8)
  -- -------- -------- -- --------

The higher the net revenue, the more profitable the system. Section
11.3.2 details the LCOE definition and the assumptions to establish (
11.7 ) and ( 11.8 ). Finally, the optimal sizing for a given selling
price is provided by

  -- -------- -------- -- --------
     @xmath   @xmath      (11.9)
  -- -------- -------- -- --------

with @xmath the sizing space, and the sizing approach is depicted in
Figure ( 11.3 ).

##### 11.3.2 Levelized cost of energy (LCOE)

The LCOE (€/ MWh) is "the unique break-even cost price where discounted
revenues, price times quantities, are equal to the discounted net
expenses" ² ² 2 Annex II Metrics & Methodology of the AR5 IPCC report. .
It means the LCOE is the lower bound of the selling price to be
profitable over the lifetime of the project and is defined as

  -- -------- -------- -- ---------
     @xmath   @xmath      (11.10)
  -- -------- -------- -- ---------

with @xmath (%) the discount rate, @xmath (years) the lifetime of the
project, and @xmath (MWh) the annual energy at year @xmath . The
lifetime expenses comprise investment costs I that are the upfront
investment (or CapEx) (€/kW for PV and €/kWh for BESS), operation and
maintenance cost (or OpEx) O&M (€/kW for PV and €/kWh for BESS), the
annual cost of the energy withdrawn from the grid W (€), and the annual
deviation cost penalty C (€)

  -- -- -------- -- ---------
        @xmath      (11.11)
  -- -- -------- -- ---------

We assume the annual cost of the energy @xmath ( @xmath ) exported
(withdrawn), the annual OpEx @xmath , and the annual deviation cost
@xmath are constant during the lifetime of the project: E, W, C and
@xmath . The investment costs I are the sum of all capital expenditures
needed to make the investment fully operational discounted to @xmath .
Thus, @xmath and @xmath . Finally, the system is operational at year
@xmath : @xmath , @xmath , @xmath , and @xmath . ( 11.11 ) becomes

  -- -- -------- -- ---------
        @xmath      (11.12)
  -- -- -------- -- ---------

that is re-written

  -- -- -------- -- ---------
        @xmath      (11.13)
  -- -- -------- -- ---------

with @xmath . The gross revenue is the energy exported to the grid that
is remunerated at the selling price. The annual gross revenue @xmath is
assumed to be constant over the project lifetime. Then, the LCOE can be
compared to R divided by the annual export @xmath to assess the
financial viability of the project by calculating ( 11.8 ).

##### 11.3.3 Case study description

The ULiège case study comprises a PV generation plant with an installed
capacity @xmath of 466.4 kW. The period from August 2019 to December
2019, 151 days in total, composes the dataset. The PV generation is
monitored on a minute basis and resampled to 15 minutes.

The simulation parameters of the planners and the oracle controller are
identical. The planning and controlling periods are @xmath minutes. The
peak hours are between 7 pm and 9 pm (UTC+0). The specifications of the
tender AO-CRE-ZNI 2019 published on @xmath define the ramping power
constraint on the engagements @xmath ( @xmath ) during off-peak (peak)
hours. The lower bound on the engagement is @xmath ( @xmath ) during
off-peak (peak) hours. The lower bound on the production is @xmath (
@xmath ) during off-peak (peak) hours. The upper bounds on the
engagement and production are @xmath . Finally, the engagement deadband
is @xmath of @xmath .

The Python Pyomo ³ ³ 3 www.http://www.pyomo.org/ 5.6.7 library is used
to implement the algorithms in Python 3.7. IBM ILOG CPLEX Optimization
Studio ⁴ ⁴ 4 https://www.ibm.com/products/ilog-cplex-optimization-studio
12.9 is used to solve all the mixed-integer quadratic optimization
problems. Numerical experiments are performed on an Intel Core i7-8700
3.20 GHz based computer with 12 threads and 32 GB of RAM running on
Ubuntu ⁵ ⁵ 5 https://ubuntu.com 18.04 LTS. The average computation time
per optimization problem of the S planner with @xmath scenarios is 3 (s)
for an optimization problem with 15 000 variables and 22 000
constraints.

##### 11.3.4 Sizing parameters

The BESS and PV CAPEX are 300 €/kWh and 700 €/kW, the BESS and PV OPEX
are 1% of the CAPEX, the project lifetime is 20 years, and the weighted
average cost of capital is 5%. The BESS lifetime in terms of complete
charging and discharging cycles is 3 000. The BESS is assumed to be
capable of fully charging or discharging in one hour @xmath , with
charging and discharging efficiencies @xmath . Each simulation day is
independent with a discharged battery at the first and last period to
its minimum capacity @xmath . The BESS minimum ( @xmath ) and maximum (
@xmath ) capacities are 10% and 90% of the total BESS storage capacity
@xmath . The sizing space is a grid composed of 56 values with @xmath
and @xmath .

##### 11.3.5 Sizing results

The @xmath , @xmath , and @xmath planners are used with the oracle
controller over the simulation dataset. E, W, C, and the number of
complete charging and discharging cycles are calculated by extrapolating
the 151 days to one year. Figure 11.4 provides the grid search sizing
results for the three planners. For a given selling price, the net is
maximal when @xmath 0.5. When @xmath increases, the BESS is more and
more used to withdraw and export during peak hours. It leads to an
increase in the number of charging/discharging cycles that implies an
increase of the number of BESS required during the project lifetime and
consequently an increase of the BESS CAPEX. As the BESS CAPEX mainly
drives the LCOE, @xmath is not capable of compensating the LCOE increase
resulting in a net decrease. The number of charging and discharging
cycles is approximately the same for both the @xmath and @xmath
planners, independently of the selling price, and rises from 4 700 with
@xmath 0.5 to 13 000 with @xmath 2.

The differences between @xmath , @xmath , and @xmath planners are minor.
@xmath tends to overestimate the net by underestimating the LCOE
(underestimating the deviation, BESS CAPEX, and withdrawal costs) and
overestimating @xmath . However, the minimal selling price to be
profitable with @xmath , is approximately 80 €/ MWh for all planners as
shown by Figure 11.4 . Then, the higher the selling price, the higher
the net. In the CRE specifications, the best tender is mainly selected
based on the selling price criterion. A trade-off should be reached
between the net and the selling price to be selected.

This sizing study seems to indicate that it is not very sensitive to the
control policy, i.e , deterministic with perfect knowledge,
deterministic with point forecasts, and stochastic with scenarios.
However, it may be dangerous not considering the uncertainty at the
sizing stage and could lead to overestimating the system performance and
underestimating the sizing. Indeed, the two-phase engagement control net
revenues are similar between planners explaining the minor differences
in terms of sizing. Two main limitations could explain this result.
First, large deviations (15-20%) from the engagement plan at the control
step occur rarely. Indeed, the oracle controller may compensate for
inadequate planning and limits the deviations. A more realistic
controller with point forecasts should be considered. Second, when such
deviations occur, they are usually within the tolerance where there is
no penalty. Furthermore, when the deviations are outside, the penalty is
relatively small in comparison with the gross revenue. A sensitivity
analysis of the numerical settings of the CRE specifications should be
performed.

#### 11.4 Conclusions and perspectives

The key idea of this Chapter is to propose a methodology to size the PV
and BESS in the context of the capacity firming framework. Indeed, the
two-phase engagement control cannot easily be modeled as a single sizing
optimization problem. Such an approach would result in a non-linear
bilevel optimization problem challenging to solve with an upper level,
the sizing part, and a lower level, the two-phase engagement control.
The two-phase engagement control is decomposed into two steps: computing
the day-ahead engagements, then recomputing the set-points in real-time
to minimize the deviations from the engagements. The CRE non-convex
penalty function is modeled by a threshold-quadratic penalty that is
compatible with a scenario approach. The stochastic formulation using a
scenario approach is compared to the deterministic formulation. The PV
scenarios are generated using a Gaussian copula methodology and PV point
forecasts computed with the PVUSA model. The minimal selling price to be
profitable, on this dataset, in the context of the capacity firming
framework is approximately 80 €/ MWh with a BESS having a maximal
capacity, fully charged or discharged in one hour, of half the total PV
installed power. The sizing study indicates that it is not very
sensitive to the control policy. The differences are minor between the
deterministic with perfect knowledge (or point forecasts) and stochastic
with scenarios strategies. However, further investigations are required
to implement a more realistic controller that uses intraday point
forecasts and conduct a sensitivity analysis on the simulation
parameters.

Several extensions are under investigation.

-   A PV generation methodology that is less dependent on the PV point
    forecasts and considers the PV power’s error dependency should be
    implemented.

-   PV scenarios clustering and reduction techniques could be considered
    to select relevant PV scenarios and improve the stochastic planner
    results.

-   A sizing formulation as a single optimization problem with the PV
    and BESS capacities as variables. It allows to compute the optimum
    directly and avoid doing a grid search. However, this formulation is
    not trivial due to the specific two-phase engagement control of the
    capacity firming framework.

-   Finally, the BESS aging process could be modeled in the sizing
    study. A dataset with at least a full year of data should be
    considered to consider the PV seasonality fully.

### Chapter 12 Capacity firming using a robust approach {infobox}

Overview This Chapter proposes a robust approach to address the energy
management of a grid-connected renewable generation plant coupled with a
battery energy storage device in the capacity firming market. It is an
extension of Chapter 10 by considering another optimization formulation
to handle the PV uncertainty. The main contributions are two-fold:

1.  The core contribution is applying the robust optimization framework
    to the capacity firming market in a tractable manner thanks to a
    Benders decomposition of the optimization problem and a warm start
    of the algorithm. In addition, a dynamic risk-averse parameters
    selection taking advantage of the quantile forecast distribution is
    proposed.

2.  The secondary contribution is the use of the Normalizing Flows,
    which is a new advanced forecasting technique, to provide the
    uncertainty estimation in the form of PV quantiles for the robust
    planner. To the best of our knowledge, it is the first study to use
    NFs in a power system application.

References: This chapter is an adapted version of the following
publication:
\bibentry dumas2021probabilistic.

  “ You have to start with the truth. The truth is the only way that we
  can get anywhere. Because any decision-making that is based upon lies
  or ignorance can’t lead to a good conclusion. ”

  — Julian Assange

There are several approaches to deal with renewable energy uncertainty.
One way is to consider a two-stage stochastic programming approach (
birge2011introduction ) . It has already been applied to the capacity
firming framework ( dumas2020probabilistic ; n2020controle ;
haessig2014dimensionnement ; parisio2016stochastic ) . The generation
uncertainty is captured by a set of scenarios modeling possible
realizations of the power output. However, this approach has three
drawbacks. First, the problem size and computational requirement
increase with the number of scenarios, and a large number of scenarios
are often required to ensure the good quality of the solution. Second,
the accuracy of the algorithm is sensitive to the scenario generation
technique. Finally, it may be challenging to identify an accurate
probability distribution of the uncertainty. Another option is to
consider robust optimization (RO) ben2009robust ; bertsimas2011theory ,
applied to unit commitment by bertsimas2012adaptive ; jiang2011robust ,
and in the capacity firming setting n2020controle . RO accounts for the
worst generation trajectory to hedge the power output uncertainty, where
the uncertainty model is deterministic and set-based. Indeed, the RO
approach puts the random problem parameters in a predetermined
uncertainty set containing the worst-case scenario. It has two main
advantages bertsimas2012adaptive : (1) it only requires moderate
information about the underlying uncertainty, such as the mean and the
range of the uncertain data; (2) it constructs an optimal solution that
immunizes against all realizations of the uncertain data within a
deterministic uncertainty set. Therefore, RO is consistent with the
risk-averse fashion way to operate power systems. However, the RO
version of a tractable optimization problem may not itself be tractable,
and some care must be taken in choosing the uncertainty set to ensure
that tractability is preserved.

Traditionally, a two-stage RO model is implemented for the unit
commitment problem in the presence of uncertainty. However, it is
challenging to compute and often NP-hard. Two classes of cutting plane
strategies have been developed to overcome the computational burden. The
Benders-dual cutting plane (BD) algorithms are the most used and seek to
derive exact solutions in the line of Benders’ decomposition
benders1962partitioning method. They decompose the overall problem into
a master problem involving the first-stage commitment decisions at the
outer level and a sub-problem associated with the second-stage dispatch
actions at the inner level. Then, they gradually construct the value
function of the first-stage decisions using dual solutions of the
second-stage decision problems bertsimas2012adaptive ; jiang2011robust .
In contrast, the column-and-constraint generation (CCG) procedure,
introduced by zhao2012robust ; zeng2013solving does not create
constraints using dual solutions of the second-stage decision problem.
Instead, it dynamically generates constraints with recourse decision
variables in the primal space for an identified scenario. The generated
variables and constraints in the CCG procedure are similar to those in
the deterministic equivalent of a two-stage stochastic programming
model. The BD and CCG algorithms have not been compared in the capacity
firming framework to the best of our knowledge.

This Chapter proposes a reliable and computationally tractable
probabilistic forecast-driven robust optimization strategy. It can use
either a BD or CGG algorithm in the capacity firming framework, depicted
in Figure 12.2 .

Our work goes several steps further than n2020controle . The main
contributions of this Chapter are three-fold:

1.  The core contribution is applying the robust optimization framework
    to the capacity firming market in a tractable manner by using a
    Benders decomposition. The non-linear robust optimization problem is
    solved both using the Benders-dual cutting plane and the
    column-and-constraint generation algorithms. To the best of our
    knowledge, it is the first time that a comparison of these
    algorithms is performed in the capacity firming framework. In
    addition, the convergence of the BD algorithm is improved with a
    warm-start procedure. It consists of building an initial set of cuts
    based on renewable generation trajectories assumed to be close to
    the worst-case scenario. The results of both the CCG and BD
    two-stage RO planners are compared to the deterministic planner
    using perfect knowledge of the future, the nominal point forecasts,
    i.e. , the baseline to outperform, and the quantiles (a conservative
    approach). The case study is the photovoltaic (PV) generation
    monitored on-site at the University of Liège (ULiège), Belgium.

2.  Second, a dynamic risk-averse parameters selection taking advantage
    of the quantile forecast distribution is investigated and compared
    to a strategy with fixed risk-averse parameters.

3.  Finally, the normalizing flows (NFs) is implemented. A new class of
    probabilistic generative models has gained increasing interest from
    the deep learning community in recent years. NFs are used to compute
    day-ahead quantiles of renewable generation for the robust planner.
    Then, an encoder-decoder architecture forecasting model
    bottieau2019very computes the intraday point forecasts for the
    controller. To the best of our knowledge, it is the first study to
    use NFs in a power system application.

In addition to these contributions, this study also provides open access
to the Python code ¹ ¹ 1
https://github.com/jonathandumas/capacity-firming-ro to help the
community to reproduce the experiments. The rest of this Chapter is
organized as follows. Section 12.1 provides the mathematical
formulations of the robust and deterministic planners. Section 12.2
develops the Benders-dual cutting plane algorithm. The case study and
computational results are presented in Section 12.3 . Finally, Section
12.4 concludes this study and draws some perspectives of future works.
Note, the capacity firming process is described in Section 10.2 of
Chapter 10 , and Section 4.5 in Chapter 4 introduces the forecasting
techniques and proposes a quality evaluation.

#### 12.1 Problem formulation

For the sake of simplicity in this Chapter, the penalty @xmath defined
in ( 10.2 ) is assumed to be symmetric, convex, and piecewise-linear

  -- -------- -------- -- --------
     @xmath   @xmath      (12.1)
  -- -------- -------- -- --------

with @xmath a penalty factor. Note, in this Chapter, both the import and
export are considered in contrast to Chapter 10 . A two-stage robust
optimization formulation is built to deal with the engagement for the
uncertain renewable generation that is modeled with an uncertainty set.
The deterministic and robust formulations of the planner are presented
in Sections 12.1.1 and 12.1.2 . The robust optimization problem with
recourse has the general form of a min-max-min optimization problem. The
uncertainty set is defined by quantiles forecasts and a budget of
uncertainty @xmath . Section 12.1.3 uses the dual of the inner problem
to formulate a min-max optimization problem. Finally, Section 12.1.4
presents the formulation of the controller. The optimization variables
and the parameters are defined in Section 10.1 .

##### 12.1.1 Deterministic planner formulation

The deterministic formulation of the planner is provided in Section 10.3
of Chapter 10 . The only difference lies in the definition of the
penalty @xmath that is assumed to be symmetric, convex, and
piecewise-linear instead of a quadratic penalty in ( 10.3.1 ).
Therefore, in this Chapter, the objective function @xmath of the
deterministic planner to minimize is

  -- -------- -------- -- --------
     @xmath   @xmath      (12.2)
  -- -------- -------- -- --------

The deterministic formulation is the following Mixed-Integer Linear
Program (MILP)

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (12.3)
     @xmath   @xmath      
  -- -------- -------- -- --------

where @xmath and @xmath are the sets of feasible engagements @xmath and
dispatch solutions @xmath , respectively. The optimization variables of
( 12.1.1 ) are the engagement variables @xmath , the dispatch variables
@xmath (the net power at the grid connection point), @xmath (BESS
discharging power), @xmath (BESS charging power), @xmath (BESS state of
charge), @xmath (BESS binary variables), @xmath (renewable generation),
and @xmath (deviation variables) (cf. the notation Section 10.1 ).

##### 12.1.2 Robust planner formulation

The uncertain renewable generation @xmath of ( 10.11 ) is assumed to be
within an interval @xmath that can be obtained based on the historical
data or an interval forecast composed of quantiles. In the following
@xmath is replaced by @xmath in ( 10.11 ). The proposed two-stage robust
formulation of the capacity firming problem consists of minimizing the
objective function over the worst renewable generation trajectory

  -- -------- -------- -- --------
     @xmath   @xmath      (12.4)
  -- -------- -------- -- --------

that is equivalent to

  -- -------- -------- -- --------
     @xmath   @xmath      (12.5)
  -- -------- -------- -- --------

The worst-case dispatch cost has a max-min form, where

  -- -------- --
     @xmath   
  -- -------- --

determines the economic dispatch cost for a fixed engagement and a
renewable generation trajectory, which is then maximized over the
uncertainty set @xmath .

In the capacity firming framework, where curtailment is allowed, the
uncertainty interval consists only in downward deviations @xmath , with
@xmath the PV 50% quantile forecast.

###### Demonstration 1.

Let consider @xmath , @xmath , @xmath , @xmath , @xmath , and @xmath .
Then, for a given engagement @xmath

  -- -- -------- -- --------
        @xmath      (12.6)
  -- -- -------- -- --------

The only difference between @xmath and @xmath is provided by ( 10.11 )
where @xmath and @xmath . By definition of the uncertainty sets @xmath
@xmath , and it is straightforward that @xmath . In addition, the
variables @xmath satisfy all the other constraints of @xmath @xmath .
Therefore, @xmath @xmath , and @xmath . Thus, for a given engagement
@xmath , @xmath , and @xmath

  -- -------- -- --------
     @xmath      (12.7)
  -- -------- -- --------

Finally, @xmath . It means the worst-case is in @xmath that corresponds
to @xmath .

In addition, the worst generation trajectories, in robust unit
commitment problems, are achieved when the uncertain renewable
generation @xmath reaches the lower or upper bounds of the uncertainty
set ( zhao2012robust , Proposition 2) . Thus, the uncertainty set at
@xmath is composed of two values and @xmath .

Following bertsimas2012adaptive ; jiang2011robust , to adjust the degree
of conservatism, a budget of uncertainty @xmath taking integer values
between 0 and 95 is employed to restrict the number of periods that
allow @xmath to be far away from its nominal value, i.e. , deviations
are substantial. Therefore, the uncertainty set of renewable generation
@xmath is defined as follows

  -- -------- -- --------
     @xmath      (12.8)
  -- -------- -- --------

where @xmath , with @xmath . When @xmath , the uncertainty set @xmath is
a singleton, corresponding to the nominal deterministic case. As @xmath
increases the size of @xmath enlarges. This means that a larger total
deviation from the expected renewable generation is considered, so that
the resulting robust solutions are more conservative and the system is
protected against a higher degree of uncertainty. When @xmath , @xmath
spans the entire hypercube defined by the intervals for each @xmath .

##### 12.1.3 Second-stage planner transformation

The robust formulation ( 12.5 ) consists of solving a min-max-min
problem, which cannot be solved directly by a commercial software such
as CPLEX or GUROBI. A scenario-based approach, e.g. , enumerating all
possible outcomes of @xmath that could lead to the worst-case scenario
for the problem, results in at least @xmath possible trajectories ² ² 2
There are @xmath possible trajectories where @xmath is within the
interval @xmath as @xmath and @xmath by using the binomial formula. .
Thus, to deal with the huge size of the problem a Benders type
decomposition algorithm is implemented.

Constraints ( 10.7a )-( 10.7b ) make the dispatch problem a MILP, for
which a dual formulation cannot be derived. In view of this, following
jiang2011robust , the constraints ( 10.7a )-( 10.7b ) are relaxed (the
convergence of the relaxed dispatch problem is discussed in Section
12.2.2 ). Then, by applying standard tools of duality theory in linear
programming, the constraints and the objective function of the dual of
the dispatch problem are derived. The dual of the feasible set @xmath ,
with ( 10.7a )-( 10.7b ) relaxed, provides the dual variables @xmath and
the following objective

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (12.9)
  -- -------- -------- -- --------

Then, the dual of the dispatch problem @xmath is

  -- -------- -------- -- ---------
     @xmath   @xmath      (12.10)
     @xmath   @xmath      
  -- -------- -------- -- ---------

with the set of constraints @xmath defined by

  -- -------- -------- -------- -------- -- ----------
                                            
     @xmath   @xmath   @xmath   @xmath      (12.11a)
     @xmath   @xmath   @xmath   @xmath      (12.11b)
     @xmath   @xmath   @xmath   @xmath      (12.11c)
     @xmath   @xmath            @xmath      (12.11d)
     @xmath   @xmath   @xmath   @xmath      (12.11e)
     @xmath   @xmath            @xmath      (12.11f)
     @xmath   @xmath   @xmath   @xmath      (12.11g)
     @xmath   @xmath            @xmath      (12.11h)
     @xmath   @xmath   @xmath   @xmath      (12.11i)
     @xmath   @xmath            @xmath      (12.11j)
     @xmath   @xmath   @xmath   @xmath      (12.11k)
  -- -------- -------- -------- -------- -- ----------

###### Definition 12.1.1 (Sub-Problem).

As a result, the worst-case dispatch problem @xmath is equivalent to the
following problem, which yields the sub-problem (SP) in the Benders-dual
cutting plane and column-and-constraint generation algorithms

  -- -------- -------- -- ---------
     @xmath   @xmath      (12.12)
  -- -------- -------- -- ---------

Overall, ( 12.5 ) becomes a min-max problem

  -- -------- -------- -- ---------
     @xmath   @xmath      (12.13)
  -- -------- -------- -- ---------

that can be solved using a Benders decomposition technique such as BD or
CCG, between a master problem, that is linear, and a sub-problem, that
is bilinear, since Indeed, @xmath has the following bilinear terms
@xmath . It is possible to linearize the products of the binary and
continuous variables @xmath of @xmath by using a standard integer
algebra trick ( savelli2018new ) with the following constraints @xmath

  -- -------- -------- -- ----------
                          
     @xmath   @xmath      (12.14a)
     @xmath   @xmath      (12.14b)
  -- -------- -------- -- ----------

where @xmath are the big-M’s values of @xmath and @xmath is an auxiliary
continuous variable. The definition of the uncertainty set ( 12.8 ) with
binary variables, based on ( zhao2012robust , Proposition 2) , is
essential to linearize @xmath .

##### 12.1.4 Controller formulation

last measured values, and renewable generation intraday point forecasts.
It computes at each period @xmath the set-points from @xmath to the last
period @xmath of the day. The formulation is the following MILP

  -- -------- -------- -- ---------
     @xmath   @xmath      (12.15)
  -- -------- -------- -- ---------

#### 12.2 Solution methodology

Following the methodology described by bertsimas2012adaptive ;
jiang2011robust , a two-level algorithm can be used to solve the min-max
( 12.5 ) problem with a Benders-dual cutting plane algorithm.

###### Definition 12.2.1 (Master Problem).

The following master problem (MP) is solved iteratively by adding new
constraints to cut off the infeasible or non-optimal solutions and is
defined at iteration @xmath by

  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- -------- -------- -- -- ----------
                                                                                                                                                                                                                                                                                                                                                       
                                                                                                                                                                                                                                                                                                                               @xmath   @xmath         (12.16a)
                                                                                                                                                                                                                                                                                                                                        @xmath         (12.16b)
                                                                                                                                                                                                                                                                                                                                        @xmath         (12.16c)
  where constraints ( 12.16b ) represent the optimality cuts, generated by retrieving the optimal values @xmath of the SP ( 12.12 ), while constraints ( 12.16c ) represent the feasibility cuts, generated by retrieving the extreme rays @xmath of ( 12.12 ), and @xmath is the optimal value of the second-stage problem.                           
  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- -------- -------- -- -- ----------

The MP can compute an optimal solution at iteration @xmath @xmath .
Note, that @xmath provides an upper bound and @xmath provides a lower
bound to the optimal value of ( 12.5 ). Therefore, by iteratively
introducing cutting planes ( 12.16b and ( 12.16c ) from the SP and
computing MP ( 12.16 ), lower and upper bounds will converge and an
optimal solution of ( 12.5 ) can be obtained.

##### 12.2.1 Convergence warm start

A four-dimension taxonomy of algorithmic enhancements and acceleration
strategies is proposed by rahmaniani2017benders : solution generation,
solution procedure, decomposition strategy, cut generation. The solution
generation is the method used to set trial values for the SP. The
quality of these solutions impacts the number of iterations, as the SP
uses them to generate cuts and bounds. The standard strategy is to solve
the MP without modification. However, heuristics can be used as a
warm-start strategy to generate an initial set of tight cuts to
strengthen the MP. A simple heuristic is proposed by lin2013exact to
generate feasible solutions and a set of good initial cuts.
Computational evidence demonstrated the efficiency of this approach in
terms of solution quality and time. Therefore, we designed the following
warm-start method to improve the Benders convergence by building an
initial set of cuts @xmath for the master problem ( 12.16 ). It consists
of sampling renewable generation trajectories that are assumed to be
close to the worst trajectory of @xmath . Let @xmath and @xmath be the
time periods corresponding to the first and last non null PV 50%
quantile forecast values. If @xmath , @xmath trajectories are sampled.
The @xmath sampled trajectory is built by setting the @xmath values of
the PV 50% quantile forecast to the @xmath lower bound for time periods
@xmath . An additional trajectory is built by setting the @xmath maximum
values of the PV 50% quantile forecast to @xmath lower bound. Then, for
each sampled trajectory @xmath , the MILP formulation ( 12.1.1 ) is used
to compute the related engagement plan @xmath . Finally, the cut @xmath
is built by solving ( 12.12 ) where the uncertainty set is a singleton
@xmath , and the engagement plan is @xmath to retrieve the optimal
values with ( 12.16b )

  -- -------- --
     @xmath   
  -- -------- --

##### 12.2.2 Algorithm convergence

First ³ ³ 3 The comments of this subsection apply to the BD and CCG
algorithms. , we make the relatively complete recourse assumption that
the SP is feasible for any engagement plan @xmath and generation
trajectory @xmath . This assumption is valid in the capacity firming
framework where curtailment is allowed. If the system faces
underproduction where @xmath is large, the generation is 0, and the BESS
discharged, penalties are applied. If it encounters overproduction where
@xmath is close to 0, the generation is large, and the BESS is charged,
the excess of generation is curtailed. In both cases, there is always a
feasible dispatch. Notice, when the relatively complete recourse
assumption does not hold, bertsimas2018scalable propose an extension of
the CCG algorithm.

Second, the convergence of the relaxed SP is checked at each iteration
of the algorithm by ensuring there is no simultaneous charge and
discharge. However, such a situation should not occur because, in case
of overproduction, the excess of generation can be curtailed.
Simultaneous charging and discharging could indeed be an equivalent
solution to dissipate the excess energy. That solution can be avoided in
practice by adding a minor penalty for using the storage system.
However, we never observed simultaneous charge and discharge over the
hundreds of simulations carried out. Thus, it is not required to
implement an extension of the BD or CCG algorithm that handles a linear
two-stage robust optimization model with a mixed-integer recourse
problem such as proposed by zhao2012exact .

Finally, the overall convergence of the algorithm toward the optimal
solution is checked. Indeed, depending on the big-M’s values, the
algorithm may converge by reducing the gap between the MP and SP.
However, it does not ensure an optimal solution. Therefore, once the
convergence between the MP and SP is reached at iteration @xmath , the
objective of the MP at @xmath is compared to the objective of the MILP
formulation ( 12.1.1 ) using the worst-case generation trajectory @xmath
as parameters. If the absolute gap @xmath is higher than a convergence
threshold @xmath , the convergence is not reached. Then, larger big-M’s
values are set, and the algorithm is restarted until convergence, or a
stopping criterion is reached.

##### 12.2.3 Benders-dual cutting plane algorithm

The Benders-dual cutting plane algorithm consists of solving ( 12.13 )
without constraints [( 10.7a )-( 10.7b )] following the procedure
previously described, and to obtain a day-ahead robust schedule @xmath
at the last iteration @xmath .

###### Definition 12.2.2 (Benders-dual cutting plane algorithm).

The initialization step consists of setting the initial big-M’s values
@xmath and @xmath , the time limit resolution of the sub-problem ( 12.12
) to 10 s, and the threshold convergence @xmath to 0.5 €. Let @xmath ,
@xmath , be the MP and SP objective values at iteration @xmath , the
lower and upper bounds, respectively, and @xmath the MILP objective
value using the worst renewable generation trajectory @xmath at
iteration @xmath .

Initialization.

Warm-start: build the initial set of cuts @xmath .

while @xmath and @xmath do

Initialize @xmath , solve the MP ( 12.16 ) and retrieve @xmath .

while the 10 last @xmath are not @xmath do

Solve the SP ( 12.12 ) with @xmath as parameters:

if the SP is unbounded then

Retrieve the extreme rays @xmath .

Add the @xmath -th feasibility cut: @xmath .

else

Retrieve the optimal values @xmath .

Add the @xmath -th optimality cut: @xmath .

Update the upper bound @xmath .

SP check: no simultaneous charge and discharge.

end if

Solve the MP ( 12.16 ): get the optimal values @xmath .

Update the lower bound @xmath and @xmath .

end while

@xmath : convergence between the SP and MP is reached. Check convergence
with MILP: get @xmath from @xmath and compute @xmath ( 12.1.1 ).

if @xmath then

if @xmath then

Update big-M’s values @xmath .

else

Update big-M’s values @xmath .

end if

Reset @xmath to 0 and restart algorithm with a new @xmath .

end if

end while

Retrieve the final @xmath engagement.

##### 12.2.4 Column and constraints generation algorithm

We implemented the column and constraints generation procedure proposed
by zhao2012robust ; zeng2013solving . The following master problem (
@xmath ) is solved at iteration @xmath

  -- -------- -------- -- ----------
                          
     @xmath   @xmath      (12.17a)
              @xmath      (12.17b)
              @xmath      (12.17c)
  -- -------- -------- -- ----------

where constraints ( 12.17b ) and ( 12.17c ) serve as optimality and
feasibility, respectively. @xmath are the new variables added to the
@xmath , and @xmath represent the worst PV trajectory computed by the SP
at iteration @xmath . Note: in our CCG implementation, we solve the SP
with the same approach as the SP of the BD algorithm.

###### Definition 12.2.3 (Column and constraints generation algorithm).

The initialization step is identical to BD, and the CCG algorithm
implemented is similar to BD. Note: there is a maximum of 50 iterations
between the SP and the @xmath before checking the convergence with the
MILP. If the criterion is not reached, the big-M’s values are increased.
Indeed, at each iteration @xmath the @xmath variables are added to the
@xmath . In our case, it represents approximately 1 000 new variables at
each iteration. With 50 iterations, the @xmath is a MILP with
approximately 50 000 variables which begins to be hard to solve within a
reasonable amount of time.

Initialization.

while @xmath and @xmath do

Initialize @xmath , solve the @xmath ( 12.17 ) and retrieve @xmath .

while the two last @xmath are not @xmath and @xmath do

Solve the SP ( 12.12 ) with @xmath as parameters:

Create variables @xmath in @xmath .

Retrieve @xmath from the SP.

Add the feasibility cut to the @xmath : @xmath .

if the SP is bounded then

Add the optimality cut: @xmath .

Update the upper bound: @xmath .

SP check: no simultaneous charge and discharge.

end if

Solve the @xmath ( 12.17 ): get the optimal values @xmath .

Update the lower bound: @xmath and @xmath .

end while

@xmath : convergence between the SP and MP is reached. Check convergence
with MILP: get @xmath from @xmath and compute @xmath ( 12.1.1 ).

if @xmath then

if @xmath then

Update big-M’s values @xmath .

else

Update big-M’s values @xmath .

end if

Reset @xmath to 0 and restart algorithm with a new @xmath .

end if

end while

Retrieve the final @xmath engagement.

#### 12.3 Case Study

The BD and CCG algorithms are compared on the ULiège case study. It
comprises a PV generation plant with an installed capacity @xmath 466.4
kWp. The PV generation is monitored on a minute basis, and the data are
resampled to 15 minutes. The dataset contains 350 days from August 2019
to November 2020, missing data during March 2020. The NFs approach is
compared to a widely used neural architecture, referred to as Long
Short-Term Memory (LSTM). In total, eight versions of the planner are
considered. Four RO versions: BD-LSTM, BD-NF, CCG-LSTM, and CCG-LSTM.
Four deterministic versions: the oracle that uses perfect knowledge of
the future, a benchmark that uses PV nominal point forecasts, and two
versions using NFs and LSTM PV quantiles. The set of PV quantiles is
@xmath . The controller uses PV intraday point forecasts and the
day-ahead engagements computed by the planners to compute the set-points
and the profits. They are normalized by the profit obtained with the
oracle planner and expressed in %.

Section 12.3.1 presents the numerical settings. Section 12.3.2 provides
the results of the sensitivity analysis for several risk-averse pairs
@xmath , with @xmath , and @xmath . Section 12.3.3 investigates a
dynamic risk-averse parameter selection. Section 12.3.4 presents the
improvement in terms of computation time provided by the initial set of
cuts. Finally, Section 12.3.5 compares the BD and CCG algorithms.

##### 12.3.1 Numerical settings

The testing set is composed of thirty days randomly selected from the
dataset. The simulation parameters of the planners and the controller
are identical. The planning and controlling periods duration are @xmath
minutes. The peak hours are set between 7 pm and 9 pm (UTC+0). The
ramping power constraint on the engagements are @xmath ( @xmath ) during
off-peak (peak) hours. The lower bounds on the engagement @xmath and the
net power @xmath are set to 0 kW. The upper bound on the engagement
@xmath and the net power @xmath are set to @xmath . Finally, the
engagement tolerance is @xmath , and the penalty factor @xmath . The
BESS minimum @xmath and maximum @xmath capacity are 0 kWh and 466.4 kWh,
respectively. It is assumed to be capable of fully charging or
discharging in one hour @xmath with charging and discharging
efficiencies @xmath %. Each simulation day is independent with a fully
discharged battery at the first and last period @xmath kWh. The Python
Gurobi library is used to implement the algorithms in Python 3.7, and
Gurobi ⁴ ⁴ 4 https://www.gurobi.com/ 9.0.2 to solve all the optimization
problems. Numerical experiments are performed on an Intel Core i7-8700
3.20 GHz based computer with 12 threads and 32 GB of RAM running on
Ubuntu 18.04 LTS.

Figures 12.2(a) and 12.2(b) illustrate the LSTM and NFs PV quantile
forecasts, observation, and nominal point forecasts on @xmath . Figures
12.2(c) and 12.2(d) provide the engagement plan (x) and the BESS state
of charge (s) computed with the RO planner, the deterministic planner
with the nominal point forecasts, and the perfect knowledge of the
future.

##### 12.3.2 Constant risk-averse parameters strategy

The risk-averse parameters of the RO approach @xmath are constant over
the dataset. One way to identify the optimal pair is to perform a
sensitivity analysis ( wang2015robust ) . Figure 12.4 provides the
normalized profits of the BD-RO, CCG-RO, and deterministic planners
using PV quantiles, left with LSTM and right with NFs, and nominal point
forecasts. The RO and deterministic planners outperform by a large
margin the baseline. The latter, the deterministic planner with nominal
point forecasts, cannot deal with PV uncertainty and achieved only 53.3
%. Then, the planners using NFs quantiles significantly outperform the
planners with LSTM quantiles. Overall, the CCG algorithm achieved better
results for almost all pairs of risk-averse parameters. The highest
profits achieved by the CCG-NF, BD-NF and NF-deterministic planners are
73.8 %, 72.6 % and 74.1 %, respectively, with the risk-averse parameters
@xmath , @xmath , and the quantile 30 %. It should be possible to
improve the RO results by tuning the risk-averse parameters @xmath .
However, these results emphasize the interest in considering a
deterministic planner with the relevant PV quantile as point forecasts,
which are easy to implement, fast to compute (a few seconds), and less
prone to convergence issues than the RO approach.

##### 12.3.3 Dynamic risk-averse parameters strategy

In this section, the risk-averse parameters @xmath of the RO approach
are dynamically set based on the day-ahead quantile forecasts
distribution, and @xmath is not necessarily equal to the same quantile
@xmath . The motivation of this strategy is to assume that the sharper
the quantile forecast distribution around the median is, the more
risk-averse the RO approach should be.

Two parameters are designed to this end: (1) the PV uncertainty set max
depth @xmath to control @xmath ; (2) the budget depth @xmath to control
@xmath . @xmath is a percentage of the distance between the median and
the 10% quantile @xmath , and @xmath is a percentage of the total
installed capacity @xmath . Then, two rules are designed to dynamically
set the risk-averse parameters @xmath for each day of the dataset. For a
given day, and the set of time periods where the PV median is non null,
the distances between the PV median and the PV quantiles 20, 30, and 40%
are computed: @xmath , @xmath , @xmath . @xmath is dynamically set at
each time period @xmath as follows

  -- -------- -- ---------
     @xmath      (12.18)
  -- -------- -- ---------

For a given day, the budget of uncertainty @xmath is dynamically set
based on the following rule

  -- -------- -- ---------
     @xmath      (12.19)
  -- -------- -- ---------

Figure 12.5 provides the normalized profits of the CCG-RO, BD-RO, and
deterministic planners for several pairs @xmath using both the LSTM and
NF quantiles. The planners achieved better results when using the NF
quantiles. Overall, the results are improved compared to fixed
risk-averse parameters for all the planners. The highest profits
achieved by the CCG-NF, BD-NF and NF-deterministic planners are 75.0 %,
72.6 % and 75.0 %, respectively, with @xmath , @xmath , and @xmath .

##### 12.3.4 BD convergence warm start improvement

Overall, the warm-start procedure of the BD algorithm improves the
convergence by reducing the number of iterations and allows to reach
more frequently optimal solution. In addition, it reduces the number of
times the big-M’s values need to be increased before reaching the final
convergence criterion with the MILP. It is illustrated by considering
the dynamic risk-averse parameters strategy with @xmath . Figure 12.6
illustrates the reduction of the total number of iteration @xmath
required to converge below the threshold @xmath , on a specific day of
the dataset. It is divided by 3.6 from 159 to 44. The computation time
is divided by 4.1 from 7.4 min to 1.8 min. Table 12.1 provides the
computation times (min) statistics over the entire dataset with and
without warm start. The averaged @xmath and total @xmath computation
times are drastically reduced when using the warm-start.

##### 12.3.5 BD and CCG comparison

Table 12.2 provides a comparison of the BD and CCG algorithms when using
NF quantiles for both the static and dynamic robust optimization
strategies. Overall, the CCG algorithm converges in 5-10 iterations
instead of 50-100 for BD. Therefore, the CCG computes the day-ahead
planning in approximately 10 seconds, ten times faster than BD. This
observation is consistent with zeng2013solving that demonstrated the CCG
algorithm converges faster than BD. Let @xmath be the number of extreme
points of the uncertainty set @xmath and @xmath of the space @xmath
defined by constraint ( 12.11 ). The BD algorithm computes an optimal
solution in @xmath iterations, and the CCG procedure in @xmath
iterations zeng2013solving . Note: the BD algorithm is still competitive
in an operational framework as it takes on average 1-2 minutes to
compute the day-ahead planning. However, we observed that the CCG does
not always converge to an optimal solution (see Section 12.2.2 ), which
never happened with the BD algorithm. Fortunately, these cases amount to
only a few % of the total instances. Overall, the CCG algorithm achieved
better results than the BD for almost all the risk-averse parameters.
Finally, in our opinion, both algorithms require the same amount of
knowledge to be implemented. Indeed, the only difference is the MP as
the SP are solved identically.

#### 12.4 Conclusion

The core contribution of this study is to address the two-phase
engagement/control problem in the context of capacity firming. A
secondary contribution is to use a recent deep learning technique,
Normalizing Flows, to compute PV quantiles. It is compared to a typical
neural architecture, referred to as Long Short-Term Memory. We developed
an integrated forecast-driven strategy modeled as a min-max-min robust
optimization problem with recourse that is solved using a Benders
decomposition procedure. Two main cutting plane algorithms used to
address the two-stage RO unit commitment problems are compared: the
Benders-dual cutting plane and the column-and-constraint generation
algorithms. The convergence is checked by ensuring a gap below a
threshold between the final objective and the corresponding
deterministic, objective value. A risk-averse parameter assessment
selects the optimal robust parameters and the optimal conservative
quantile for the deterministic planner. Both the NF-based and LSTM-based
planners outperformed the deterministic planner with nominal point PV
forecasts. The NF model outperforms the LSTM in forecast value as the
planner using the NF quantiles achieved higher profit than the planner
with LSTM quantiles. Finally, a dynamic risk-averse parameter selection
strategy is built by taking advantage of the PV quantile forecast
distribution and provides further improvements. The CCG procedure
converges ten times faster than the BD algorithm in this case study and
achieves better results. However, it does not always converge to an
optimal solution.

Overall, the RO approach for both the BD and CCG algorithms allows
finding a trade-off between conservative and risk-seeking policies by
selecting the optimal robust optimization parameters, leading to
improved economic benefits compared to the baseline. Therefore, offering
a probabilistic guarantee for the robust solution. However, the
deterministic planner with the relevant PV quantile achieved interesting
results. It emphasizes the interest to consider a well-calibrated
deterministic approach. Indeed, it is easy to implement, computationally
tractable for large-scale problems, and less prone to convergence
issues. Note: this approach can be used in any other case study. It only
requires a few months of data, renewable generation, and weather
forecasts to train the forecasting models to compute reliable forecasts
for the planner.

Several extensions are under investigation: (1) a stochastic formulation
of the planner with improved PV scenarios based on Gaussian copula
methodology or generated by a state-of-the-art deep learning technique
such as Normalizing Flows, Generative Adversarial Networks or
Variational AutoEncoders; (2) an improved dynamic risk-averse parameter
selection strategy based on a machine learning tool capable of
better-taking advantage of the PV quantiles distribution.

### Chapter 13 Energy retailer {infobox}

Overview This Chapter investigates the forecast value assessment of the
deep generative models studied in Chapter 6 . The reader is referred to
Chapter 6 for the context, primary contributions, the NFs, GANs, and
VAEs background, and the description of the Global Energy Forecasting
Competition 2014 (GEFcom 2014) case study.

References: This chapter is an adapted version of the following
publication:
\bibentry dumas2021nf.

  “ Be willing to make decisions. That’s the most important quality in a
  good leader. ”

  — George S. Patton

A model that yields lower errors in terms of forecast quality may not
always point to a more effective model for forecast practitioners (
hong2020energy ) . To this end, similarly to toubeau2018deep , the
forecast value is assessed by considering the day-ahead market
scheduling of electricity aggregators, such as energy retailers or
generation companies. The energy retailer aims to balance its portfolio
on an hourly basis to avoid financial penalties in case of imbalance by
exchanging the surplus or deficit of energy in the day-ahead electricity
market. The energy retailer may have a battery energy storage system
(BESS) to manage its portfolio and minimize imports from the main grid
when day-ahead prices are prohibitive.

Section 13.2 introduces the notations used in this Chapter. Section 13.2
presents the formulation of the energy retailer case study. Section 13.3
details empirical value results on the GEFcom 2014 dataset, and Section
13.4 summarizes the main findings and highlights ideas for further work.

#### 13.1 Notation

##### Sets and indices

##### Parameters

##### Variables

For the sake of clarity the subscript @xmath is omitted.

#### 13.2 Problem formulation

Let @xmath [MWh] be the net energy retailer position on the day-ahead
market during the @xmath -th hour of the day, modeled as a first stage
variable. Let @xmath [MWh] be the realized net energy retailer position
during the @xmath -th hour of the day, which is modeled as a second
stage variable due to the stochastic processes of the PV generation,
wind generation, and load. Let @xmath [€/ MWh] the clearing price in the
spot day-ahead market for the @xmath -th hour of the day, @xmath ex-post
settlement price for negative imbalance @xmath , and @xmath ex-post
settlement price for positive imbalance @xmath . The energy retailer is
assumed to be a price taker in the day-ahead market. It is motivated by
the individual energy retailer capacity being negligible relative to the
whole market. The forward settlement price @xmath is assumed to be fixed
and known. As imbalance prices tend to exhibit volatility and are
difficult to forecast, they are modeled as random variables, with
expectations denoted by @xmath and @xmath . They are assumed to be
independent random variables from the energy retailer portfolio.

##### 13.2.1 Stochastic planner

A stochastic planner with a linear programming formulation and linear
constraints is implemented using a scenario-based approach. The planner
computes the day-ahead bids @xmath that cannot be modified in the future
when the uncertainty is resolved. The second stage corresponds to the
dispatch decisions @xmath in scenario @xmath that aims at avoiding
portfolio imbalances modeled by a cost function @xmath . The
second-stage decisions are therefore scenario-dependent and can be
adjusted according to the realization of the stochastic parameters. The
stochastic planner objective to maximize is

  -- -------- -------- -- --------
     @xmath   @xmath      (13.1)
  -- -------- -------- -- --------

where the expectation is taken with respect to the random variables, the
PV generation, wind generation, and load. Using a scenario-based
approach, ( 13.1 ) is approximated by

  -- -------- -------- -- --------
     @xmath   @xmath      (13.2)
  -- -------- -------- -- --------

with @xmath the probability of scenario @xmath , and @xmath . The
mixed-integer linear programming (MILP) optimization problem to solve is

  -- -------- -------- -- ---------
                          
     @xmath               (13.3a)
     @xmath   @xmath      (13.3b)
     @xmath   @xmath      (13.3c)
  -- -------- -------- -- ---------

The optimization variables are @xmath , day-ahead bid of the net
position, @xmath , @xmath , retailer net position in scenario @xmath ,
@xmath , short deviation, @xmath , long deviation, @xmath , PV
generation, @xmath , wind generation, @xmath , battery energy storage
system (BESS) charging power, @xmath , BESS discharging power, @xmath ,
BESS state of charge, and @xmath a binary variable to prevent from
charging and discharging simultaneously. The imbalance penalty is
modeled by the constraints ( 13.4a )-( 13.4b ) @xmath , that define the
short and long deviations variables @xmath . The energy balance is
provided by ( 13.4c ) @xmath . The set of constraints that bound @xmath
and @xmath variables are ( 13.4d )-( 13.4e ) @xmath where @xmath and
@xmath are PV and wind generation scenarios. The load is assumed to be
non-flexible and is a parameter ( 13.4f ) @xmath where @xmath are load
scenarios. The BESS constraints are provided by ( 13.4g )-( 13.4j ), and
the BESS dynamics by ( 13.4k )-( 13.4m ) @xmath .

  -- -------- -------- -- ---------
                          
     @xmath   @xmath      (13.4a)
     @xmath   @xmath      (13.4b)
     @xmath   @xmath      (13.4c)
     @xmath   @xmath      (13.4d)
     @xmath   @xmath      (13.4e)
     @xmath   @xmath      (13.4f)
     @xmath   @xmath      (13.4g)
     @xmath   @xmath      (13.4h)
     @xmath   @xmath      (13.4i)
     @xmath   @xmath      (13.4j)
     @xmath   @xmath      (13.4k)
     @xmath   @xmath      (13.4l)
     @xmath   @xmath      (13.4m)
  -- -------- -------- -- ---------

Notice that if @xmath , the surplus quantity is remunerated with a
non-negative price. In practice, such a scenario could be avoided
provided that the energy retailer has curtailment capabilities, and
@xmath are strictly positive in our case study. The deterministic
formulation with perfect forecasts, the oracle (O), is a specific case
of the stochastic formulation by considering only one scenario where
@xmath , @xmath , and @xmath become the actual values of PV, wind, and
load @xmath . The optimization variables are @xmath , @xmath , @xmath ,
@xmath , @xmath , and @xmath , @xmath , @xmath , @xmath , and @xmath .

##### 13.2.2 Dispatching

Once the bids @xmath have been computed by the planner, the dispatching
consists of computing the second stage variables given observations of
the PV, wind power, and load. The dispatch formulation is a specific
case of the stochastic formulation with @xmath as parameter and by
considering only one scenario where @xmath , @xmath , and @xmath become
the actual values of PV, wind, and load @xmath . The optimization
variables are @xmath , @xmath , @xmath , @xmath , and @xmath , @xmath ,
@xmath , @xmath , and @xmath .

#### 13.3 Value results

The energy retailer portfolio comprises wind power, PV generation, load,
and a battery energy storage device. The 50 days of the testing set are
used and combined with the 30 possible PV and wind generation zones
(three PV zones and ten wind farms), resulting in 1 500 independent
simulated days. A two-step approach is employed to evaluate the forecast
value:

-   First, for each generative model and the 1 500 days simulated, the
    two-stage stochastic planner computes the day-ahead bids of the
    energy retailer portfolio using the PV, wind power, and load
    scenarios. After solving the optimization, the day-ahead decisions
    are recorded.

-   Then, a real-time dispatch is carried out using the PV, wind power,
    and load observations, with the day-ahead decisions as parameters.

This two-step methodology is applied to evaluate the three generative
models, namely the NF, GAN, and VAE.

Figure 13.2 illustrates an arbitrary random day of the testing set with
the first zone for both the PV and wind. @xmath [€/ MWh] is the
day-ahead prices on @xmath of the Belgian day-ahead market used for the
1 500 days simulated. The negative @xmath and positive @xmath imbalance
prices are set to @xmath , @xmath . The retailer aims to balance the net
power, the red curve in Figure 13.2 , by importing/exporting from/to the
main grid. Usually, the net is positive (negative) at noon (evening)
when the PV generation is maximal (minimal), and the load is minimal
(maximal). As the day-ahead spot price is often maximal during the
evening load peak, the retailer seeks to save power during the day by
charging the battery to decrease the import during the evening.
Therefore, the more accurate the PV, wind generation, and load scenarios
are, the better is the day-ahead planning.

The battery minimum @xmath and maximum @xmath capacities are 0 and 1,
respectively. It is assumed to be capable of fully (dis)charging in two
hours with @xmath , and the (dis)charging efficiencies are @xmath %.
Each simulation day is independent with a fully discharged battery at
the first and last period of each day @xmath . The 1 500 stochastic
optimization problems are solved with 50 PV, wind generation, and load
scenarios. The python Gurobi library is used to implement the algorithms
in Python 3.7, and Gurobi ¹ ¹ 1 https://www.gurobi.com/ 9.0.2 is used to
solve the optimization problems. Numerical experiments are performed on
an Intel Core i7-8700 3.20 GHz based computer with 12 threads and 32 GB
of RAM running on Ubuntu 18.04 LTS.

The net profit, that is, the profit minus penalty, is computed for the 1
500 days of the simulation and aggregated in the first row of Table 13.1
. The ranking of each model is computed for the 1 500 days. The
cumulative ranking is expressed in terms of percentage in Table 13.1 .
NF outperformed both the GAN and VAE with a total net profit of 107 k€.
There is still room for improvement as the oracle, which has perfect
future knowledge, achieved 300 k€. NF, ranked first 39.0% during the 1
500 simulation days and achieved the first and second ranks 69.6%.

Overall, in terms of forecast value, the NF outperforms the VAE and GAN.
However, this case study is "simple," and stochastic optimization relies
mainly on the quality of the average of the scenarios. Therefore, one
may consider taking advantage of the particularities of a specific
method by considering more advanced case studies. In particular, the
specificity of the NFs to provide direct access to the probability
density function may be of great interest in specific applications. It
is left for future investigations as more advanced case studies would
prevent a fair comparison between models.

##### 13.3.1 Results summary

Table 13.2 summarizes the main results of this study by comparing the
VAE, GAN, and NF implemented through easily comparable star ratings. The
rating for each criterion is determined using the following rules - 1
star: third rank, 2 stars: second rank, and 3 stars: first rank.
Specifically, training speed is assessed based on reported total
training times for each dataset: PV generation, wind power, and load;
sample speed is based on reported total generating times for each
dataset; quality is evaluated with the metrics considered; value is
based on the case study of the day-ahead bidding of the energy retailer;
the hyper-parameters search is assessed by the number of configurations
tested before reaching satisfactory and stable results over the
validation set; the hyper-parameters sensitivity is evaluated by the
impact on the quality metric of deviations from the optimal the
hyper-parameter values found during the hyper-parameter search; the
implementation-friendly criterion is appraised regarding the complexity
of the technique and the amount of knowledge required to implement it.

#### 13.4 Conclusions

This Chapter proposed a fair and thorough comparison of NFs with the
state-of-the-art deep learning generative models, GANs and VAEs, in
terms of value. The experiments are performed using the open data of the
Global Energy Forecasting Competition 2014. The generative models use
the conditional information to compute improved weather-based PV, wind
power, and load scenarios. This study demonstrated that NFs are capable
of challenging GANs and VAEs. They are overall more accurate both in
terms of quality (see Chapter 6 ) and value and can be used effectively
by non-expert deep learning practitioners. Note, Section 6.6 in Chapter
6 presents the NFs advantages over more traditional deep learning
approaches that should motivate their introduction into power system
applications.

#### 13.5 Appendix: Table 13.2 justifications

The VAE is the fastest to train, with a recorded computation time of 7
seconds on average per dataset. The training time of the GAN is
approximately three times longer, with an average computation time of 20
seconds per dataset. Finally, the NF is the slowest, with an average
training time of 4 minutes. This ranking is preserved with the VAE the
fastest concerning the sample speed, followed by the GAN and NF models.
The VAE and the GAN generate the samples over the testing sets, 5 000 in
total, in less than a second. However, the NF considered takes a few
minutes. In contrast, the affine autoregressive version of the NF is
much faster to train and generate samples. Note: even a training time of
a few hours is compatible with day-ahead planning applications. In
addition, once the model is trained, it is not necessarily required to
retrain it every day.

The quality and value assessments have already been discussed in
Sections 6.5 and 13.3 . Overall, the NF outperforms both the VAE and GAN
models.

Concerning the hyper-parameters search and sensibility, the NF tends to
be the most straightforward model to calibrate. Compared with the VAE
and GAN, we found relevant hyper-parameter values by testing only a few
combinations. In addition, the NF is robust to hyper-parameter
modifications. In contrast, the GAN is the most sensitive. Variations of
the hyper-parameters may result in very poor scenarios both in terms of
quality and shape. Even for a fixed set of hyper-parameters values, two
separate training may not converge towards the same results illustrating
the GAN training instabilities. The VAE is more accessible to train than
the GAN but is also sensitive to hyper-parameters values. However, it is
less evident than the GAN.

Finally, we discuss the implementation-friendly criterion of the models.
Note: this discussion is only valid for the models implemented in this
study. There exist various architectures of GANs, VAEs, and NFs with
simple and complex versions. In our opinion, the VAE is the effortless
model to implement as the encoder and decoder are both simple
feed-forward neural networks. The only difficulty lies in the
reparameterization trick that should be carefully addressed. The GAN is
a bit more difficult to deploy due to the gradient penalty to handle but
is similar to the VAE with both the discriminator and the generator that
are feed-forward neural networks. The NF is the most challenging model
to implement from scratch because the UMNN-MAF approach requires an
additional integrand network. An affine autoregressive NF is easier to
implement. However, it may be less capable of modeling the stochasticity
of the variable of interest. However, forecasting practitioners do not
necessarily have to implement generative models from scratch and can use
numerous existing Python libraries.

### Chapter 14 Part Ii conclusions

  “ Everything has been figured out, except how to live. ”

  — Jean-Paul Sartre

Part II presents several approaches to address the energy management of
a grid-connected microgrid. It proposes handling the renewable
generation uncertainty by considering deterministic, stochastic, and
robust approaches that rely on point forecasts, scenarios, and quantiles
forecasts. Several numerical experiments evaluate these approaches.

-   The MiRIS microgrid located at the John Cockerill Group’s
    international headquarters in Seraing, Belgium, assesses a value
    function-based approach to propagate information from operational
    planning to real-time optimization. This methodology relies on point
    forecasts of PV and electrical demand to conduct the deterministic
    optimization. The results demonstrate the efficiency of this method
    to manage the peak in comparison with a Rule-Based Controller.

-   The MiRIS case study is also employed in the capacity firming
    market. It allows studying a stochastic approach to address the
    energy management of a grid-connected renewable generation plant
    coupled with a battery energy storage device. The results of the
    stochastic planner are comparable with those of the deterministic
    planner, even when the prediction error variance is non-negligible.
    However, further investigations are required to use a more realistic
    methodology to generate PV scenarios and a realistic controller.

-   A sizing study is performed using the capacity firming framework in
    a different case: the ULiège case study. The results indicate that
    the approach is not very sensitive to the control policy. The
    difference between the deterministic with perfect knowledge (or
    point forecasts) and stochastic with scenarios is minor. However,
    further investigations are required to implement a more realistic
    controller that uses intraday point forecasts and conduct a
    sensitivity analysis on the simulation parameters.

-   A robust day-ahead planner is compared to its deterministic
    counterpart using the ULiège case study in the capacity firming
    framework. Overall, it allows finding a trade-off between
    conservative and risk-seeking policies by selecting the optimal
    robust optimization parameters, leading to improved economic
    benefits compared to the baseline.

-   Finally, a stochastic approach deals with the optimal bidding of an
    energy retailer portfolio on the day-ahead market to assess the
    value of a recent class of deep learning generative models, the
    normalizing flows. The numerical experiments use the open-access
    data of the Global Energy Forecasting Competition 2014. The results
    demonstrate that normalizing flows can challenge Variational
    AutoEncoders and Generative Adversarial Networks. They are overall
    more accurate both in terms of quality (see Chapter 6 ) and value
    and can be used effectively by non-expert deep learning
    practitioners.

### Chapter 15 General conclusions and perspectives {infobox}

Overview This chapter concludes the thesis and identifies four potential
research directions in both forecasting and planning for microgrids. (1)
Forecasting techniques of the future with the development of new machine
learning models that take advantage of the underlying physical process.
(2) Machine learning for optimization with models that simplify
optimization planning problems by learning a sub-optimal space. (3)
Modelling and simulation of energy systems by applying forecasting and
decomposition techniques investigated in this thesis into open-source
code. (4) Machine learning and psychology where models could identify
appropriate behaviors to reduce carbon footprint. Then, inform
individuals, and provide constructive opportunities by modeling
individual behavior.

  “ Words are loaded pistols. ”

  — Jean-Paul Sartre

#### 15.1 Summary

The IPCC report ’AR5 Climate Change 2014: Mitigation of Climate Change’
¹ ¹ 1 https://www.ipcc.ch/report/ar5/wg3/ , states that electrical
systems are responsible for about a quarter of human-caused greenhouse
gas emissions each year. According to the IPCC, the transition to a
carbon-free society goes through an inevitable increase in the renewable
generation’s share in the energy mix and a drastic decrease in the total
consumption of fossil fuels. However, a high share of renewables is
challenging for power systems that have been designed and sized for
dispatchable units. In addition, a major challenge is to implement these
changes across all countries and contexts, as electricity systems are
everywhere. Machine learning can contribute on all fronts by informing
the research, deployment, and operation of electricity system
technologies. High leverage contributions in power systems include (
rolnick2019tackling ) : accelerating the development of clean energy
technologies, improving demand and clean energy forecasts, improving
electricity system optimization and management, and enhancing system
monitoring. This thesis focuses on two leverages: (1) the supply and
demand forecast; (2) the electricity system optimization and management.

Since variable generation and electricity demand both fluctuate, they
must be forecast ahead of time to inform real-time electricity
scheduling and longer-term system planning. Better short-term forecasts
enable system operators to reduce reliance on polluting standby plants
and proactively manage increasing amounts of variable sources. Better
long-term forecasts help system operators and investors to decide where
and when to build variable plants. Forecasts need to become more
accurate, span multiple horizons in time and space, and better quantify
uncertainty to support these use cases. Therefore, probabilistic
forecasts have become a vital tool to equip decision-makers, hopefully
leading to better decisions in energy applications.

When balancing electricity systems, system operators use scheduling and
dispatch to determine how much power every controllable generator should
produce. This process is slow and complex, governed by NP-hard
optimization problems ( rolnick2019tackling ) such as unit commitment
and optimal power flow that must be coordinated across multiple time
scales, from sub-second to days ahead. Scheduling becomes even more
complex as electricity systems include more storage, variable
generators, and flexible demand. Indeed, operators manage even more
system components while simultaneously solving scheduling problems more
quickly to account for real-time variations in electricity production.
Thus, scheduling must improve significantly, allowing operators to rely
on variable sources to manage systems.

These two challenges raise the following two central research questions:

-   How to produce reliable probabilistic forecasts of renewable
    generation, consumption, and electricity prices?

-   How to make decisions with uncertainty using probabilistic forecasts
    to improve scheduling?

This thesis studies the day-ahead management of a microgrid system and
investigates both questions into two parts.

Part I investigates forecasting techniques and quality metrics required
to produce and evaluate point and probabilistic forecasts. Then, Part II
use them as input of decision-making models. Chapters 2 and 3 present
the forecasting basics. They introduce the different types of forecasts
to characterize the behavior of stochastic variables, such as renewable
generation or electrical consumption, and the assessment procedures. An
example of forecast quality evaluation is provided by employing standard
deep-learning models such as recurrent neural networks to compute PV and
electrical consumption point forecasts. The following Chapters of Part I
study the point forecasts, quantile forecasts, scenarios, and density
forecasts. Chapter 4 proposes to implement deep-learning models such as
the encoder-decoder architecture to produce PV quantile forecasts. Then,
a day-ahead robust optimization planner uses these forecasts in the form
of prediction intervals in the second part of this thesis. Chapter 5
presents a density forecast-based approach to compute probabilistic
forecasting of imbalance prices with a particular focus on the Belgian
case. Finally, Chapter 6 analyzes the scenarios of renewable generation
and electrical consumption of a new class of deep learning generative
models, the normalizing flows. Relevant metrics assess the forecast
quality. A thorough comparison is performed with the variational
autoencoders and generative adversarial networks models.

Part II presents several approaches and methodologies based on
optimization for decision-making under uncertainty. It employs the
forecasts computed in the first part of the thesis. Chapter 8 introduces
the different types of optimization strategies for decision making under
uncertainty. Chapter 9 presents a value function-based approach as a way
to propagate information from operational planning to real-time
optimization in a deterministic framework. Chapters 10 , 11 , and 12
consider a grid-connected renewable generation plant coupled with a
battery energy storage device in the capacity firming market. This
framework has been designed to promote renewable power generation
facilities in small non-interconnected grids. First, a stochastic
optimization strategy is adopted for the day-ahead planner. Second, a
sizing study of the system is conducted to evaluate the impact of the
planning strategy: stochastic or deterministic. Finally, a robust
day-ahead planner is considered to optimize the results regarding
risk-aversion of the decision-maker. Finally, Chapter 13 is the
extension of Chapter 6 . It presents the forecast value of the deep
learning generative models by considering the day-ahead market
scheduling of an energy retailer. It is an easily reproducible case
study designed to promote the normalizing flows in power system
applications.

#### 15.2 Future directions

We propose four primary research future directions.

##### 15.2.1 Forecasting techniques of the future

Nowadays, the renewable energy forecasting field is highly active and
dynamic. Hence, new forecasting methods will likely be proposed and used
in operational problems related to power system applications in the
coming years. The type of forecasts to be used as input to operational
problems will depend upon the problem’s nature and the formulation of
the corresponding optimization problem. Machine learning algorithms of
the future will need to incorporate domain-specific insights. For
instance, the study ( rolnick2019tackling ) defines exciting research
directions. We propose in the following several research directions.
First, it is well known that the weather fundamentally drives both
variable generation and electricity demand. Thus, machine learning
algorithms should draw from climate modeling, weather forecasting
innovations, and hybrid physics-plus-ML modeling techniques (
voyant2017machine ; das2018forecasting ) . Second, machine learning
models could be designed to directly optimize system goals (
donti2019task ; elmachtoub2021smart ) . An example is given by
donti2019task . They use a deep neural network to produce demand
forecasts that optimize electricity scheduling costs rather than
forecast accuracy. Third, better understanding the value of improved
forecasts is an interesting challenge. In this line, the benefits of
specific solar forecast improvements in a region of the United States
are described by martinez2016value . Finally, studying techniques that
take advantage of the power system characteristics, such as graphical
neural networks ( wehenkel2020graphical ; donon2019graph ) . They are
capable of learning the power network structure and could provide
successful contributions to hierarchical forecasting. Indeed,
probabilistic graphical models reduce to Bayesian networks with a
pre-defined topology and a learnable density at each node (
wehenkel2020graphical ) . From this new perspective, the graphical
normalizing flow provides a promising way to inject domain knowledge
into normalizing flows while preserving Bayesian networks’
interpretability and the representation capacity of normalizing flows.
In this line, a graphical model is used to detect faults in rooftop
solar panels ( iyengar2018solarclique ) . This paper proposes
Solar-Clique, a data-driven approach that can flag anomalies in power
generation with high accuracy.

##### 15.2.2 Machine learning for optimization

There is a broad consensus in the power system community that the
uncertain nature of renewable energy sources like wind and solar is
likely to induce significant changes in the paradigms of power systems
management ( morales2013integrating ) . The electricity market results
from traditional practices, such as unit commitment or economic
dispatch, designed given a generation mix mainly formed by dispatchable
plants. Therefore, they are now to be reexamined so that stochastic
producers can compete on equal terms. For instance, the capacity firming
market is a new design conceived to promote renewable generation in
isolated markets. The type of decision-making tools to deal with these
new market designs and systems, such as microgrids, depends upon the
nature of the problem itself. These tools have been studied intensively,
and new methods will likely be proposed and used in power system
applications. In particular, machine learning for optimization is
nowadays a hot topic. It learns partially or totally the sizing space to
provide a fast and efficient sizing tool. It also simplifies
optimization planning problems by learning a sub-optimal space. This
approach has already been investigated in a few applications. For
instance:

-   Machine learning can be used to approximate or simplify existing
    optimization problems ( bertsimas2021online ; zamzam2020learning ) ,
    and to find good starting points for optimization ( jamei2019meta )
    .

-   Dynamic scheduling ( essl2017machine ) and safe reinforcement
    learning can also be used to balance the electric grid in real-time.

-    misyris2020physics propose a framework for physics-informed neural
    networks in power system applications. In this line,
    fioretto2020predicting present a deep learning approach to the
    optimal power flows. The learning model exploits the information
    available in the similar states of the system and a dual Lagrangian
    method to satisfy the physical and engineering constraints present
    in the optimal power flows.

-    donon2019graph propose a neural network architecture that emulates
    the behavior of a physics solver that solves electricity
    differential equations to compute electricity flow in power grids.
    It uses proxies based on graph neural networks.

-    tsaousoglou2021managing consider an economic dispatch problem for a
    community of distributed energy resources, where energy management
    decisions are made online and under uncertainty. The economic
    dispatch problem is a multi-agent Markov Decision Process. The
    difficulties lie in the curse of dimensionality and in guaranteeing
    the satisfaction of constraints under uncertainty. A novel method
    that combines duality and deep learning tackles these challenges.

##### 15.2.3 Modelling and simulation of energy systems

As already previously stated, the transition towards more sustainable
fossil-free energy systems requires a high penetration of renewables,
such as wind and solar. These new energy resources and technologies will
lead to profound structural changes in energy systems, such as an
increasing need for storage and drastic electrification of the heating
and mobility sectors. Therefore, new flexible and open-source
optimization modeling tools are required to capture the growing
complexity of such future energy systems. To this end, in the past few
years, several open-source models for the strategic energy planning of
urban and regional energy systems have been developed. A list of such
models is depicted in ( limpens2019energyscope ) . We select and present
two ² ² 2 In both cases, machine learning investigation for optimization
may be an interesting research direction to simplify and improve the
models. of them where we think it may be relevant to implement and test
the forecasting techniques and scheduling strategies developed in this
thesis.

First, EnergyScope TD ( limpens2019energyscope ) is a novel open-source
model for the strategic energy planning of urban and regional energy
systems. EnergyScope TD optimizes an entire energy system’s investment
and operating strategy, including electricity, heating, and mobility.
Its hourly resolution, using typical days, makes the model suitable for
integrating intermittent renewables, and its concise mathematical
formulation and computational efficiency are appropriate for uncertainty
applications. A new research direction could be integrating
multi-criterion optimization to consider the carbon footprint, the land
use, the energy return on investment, and the cost could be an exciting
and challenging research direction ( muyldermans2021 ) . Solving such a
complex optimization problem may require decomposition techniques such
as the ones investigated in this thesis.

Second, E4CLIM ( tantet2019e4clim ) is open-source Python software
integrating flexibility needs from variable renewable energies in the
development of regional energy mixes. It aims at evaluating and
optimizing energy deployment strategies with higher shares of variable
renewable energies. It also assesses the impact of new technologies and
climate variability and allows conducting sensitivity studies. The
E4CLIM’s potential was already illustrated at the country scale with an
optimal recommissioning study of the 2015 Italian PV-wind mix. A new
research direction could be to adapt and develop E4CLIM at the
infra-regional scale and consider the electrical grid constraints. It
may imply solving complex optimization problems requiring
state-of-the-art decomposition techniques.

##### 15.2.4 Machine learning and psychology

This last research direction goes off the beaten tracks. In my opinion,
achieving sustainability goals requires as much the use of relevant
technology as psychology. Therefore, one of the main challenges is not
designing relevant technological tools but changing how we consume and
behave in our society. In other words, technology will not save us.
Nevertheless, it is the way we use technology that could help us to meet
the sustainability targets. Therefore, it requires research in
transdisciplinary fields to design collaborative solutions and guidances
towards this goal. Psychology is a comprehensive tool to study and
understand why there is a gap between rhetoric and actions. An increase
in psychological research on climate change has been conducted since the
