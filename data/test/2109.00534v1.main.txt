## 1 Introduction

Large-scale optimization plays an vital role in many modern
computational applications, and particularly in machine learning.
Because of the diversity of use cases, there is great value in
developing broadly applicable, general-purpose algorithms that can be
readily applied wherever they are needed, without relying on any unique
structure. For example, the stochastic gradient descent algorithm (SGD)
and its variants can be applied to a huge variety of optimization
objectives, and almost all of the recent accomplishments of machine
learning owe some of their success to SGD.

Optimization problems are growing increasingly large and therefore it is
often necessary to develop algorithms that leverage parallelism. In
machine learning, for example, it has become common to use models that
have millions or billions of trainable parameters, and the datasets used
for learning often include millions or billions of examples. Training
such models gives rise to enormous, very high-dimensional optimization
objectives which cannot be tackled on a single machine.

In this thesis, we are motivated by these massive optimization problems
and the need for new and better general-purpose, distributed
optimization algorithms to solve them. We will begin by formulating one
notion of an optimal algorithm in distributed optimization settings. We
will then consider a number of distributed optimization problems and ask
for optimal algorithms for them. Identifying optimal algorithms has
obvious benefits and the pursuit of new and better algorithms naturally
leads to better performance in downstream applications. At the same
time, failing to identify optimal algorithms highlights settings in
which there is room for improvement over the status quo and it motivates
further research to find better methods. Finally, even in settings where
we can identify an optimal algorithm, there is always an opportunity to
be “better than optimal” by figuring out additional structure in the
problem that can be exploited to yield better methods.

### 1.1 Machine Learning as an Optimization Problem

One of the most important applications of optimization is training
machine learning models, and because machine learning will serve as a
running example throughout, we will take a moment to conceptualize it as
fundamentally a stochastic optimization problem.

In supervised machine learning, the user specifies a model—a mapping
from parameters to a prediction function—and a loss function—an
evaluation metric measuring the accuracy of each prediction—and then
“trains the model” meaning they find a setting of the parameters that
“fits” the data in the sense of minimizing the loss function. Naturally,
for any given model and loss function, one could come up with a bespoke
training algorithm that finds a good setting of the parameters by
cleverly exploiting some special structure. However, the typical
approach is much more general: we set up training as a continuous
optimization problem “minimize the loss function over the parameters,”
and then we apply a general purpose optimization algorithm, often
stochastic gradient descent (SGD), to solve that optimization problem.

This generalized approach to training machine learning models by
reducing it to a generic continuous optimization problem has several
advantages. First, this method (in combination with tools like automatic
differentiation) allows users to easily change their model or loss
function without needing to design a whole new training algorithm. This
flexibility has allowed the machine learning community to rapidly switch
between different models and loss functions as we learn more about what
works for which problems. You can imagine that if we had a killer
training algorithm for 2-layer neural networks with the square loss,
specifically, then we would likely have been much slower to try deep
learning approaches which have led to many of the recent triumphs of
machine learning. Second, separating machine learning into orthogonal
modelling and optimization components allows all machine learners to
benefit from advances in optimization algorithms. In recent years,
numerous general-purpose optimization algorithms have been proposed
(e.g. Duchi et al., 2011 ; Kingma and Ba, 2014 ) , and these have proven
highly successful in a wide variety of applications.

In order for this scheme to work, optimization algorithms should be
general-purpose and applicable to as many different objectives as
possible. However, there is no optimization algorithm that can be
guaranteed to work on every function, and any algorithm needs to exploit
something about the objective in order to succeed. Therefore, an
important aspect of optimization research is identifying a small set of
properties that (1) can be exploited by an algorithm to efficiently
optimize the objective and (2) can be expected to hold for objectives of
interest. Even for broad classes of optimization objectives, there is a
lot of plausibly exploitable structure, and it is important to
distinguish the relevant from the irrelevant. Simultaneously, there is
also a degree to which we control the properties of the optimization
objectives, for example, in machine learning, our choices of model and
loss function give rise to the training objective. Therefore, if we
learned that some property A allows for very efficient optimization,
that would motivate designing models/loss functions which produce this
property.

### 1.2 Minimax Optimality in Optimization

One of the primary goals of optimization research is to seek out better
and more efficient optimization algorithms. Doing so of course requires
developing and analyzing new and more clever methods, but it is also
important to identify where there is room for improvement over the
status quo, and what that improvement would look like. Finding such
opportunities requires posing and answering questions of optimality—what
is the best we can do in a given situation, and do our current methods
perform that well? A significant amount of work is necessary to properly
formulate a useful notion of optimality, which we discuss in \prettyref
sec:formulating-the-complexity. At a high level, we do this by
specifying a family of possible optimization objectives and of possible
optimization algorithms and then we ask what the best of these
algorithms can guarantee for the hardest objective in the family. This
notion of “minimax complexity” indicates what is the best we can hope
for when trying to optimize those sorts of objectives using that type of
optimization algorithm.

Analyzing the minimax complexity has two parts: “upper bounds” and
“lower bounds.” Whenever we analyze an optimization algorithm and
guarantee that it achieves a certain level of accuracy for any objective
in the family, this puts an “upper bound” on the minimax error because,
of course, the best algorithm’s guarantee is no worse. On the other
hand, a lower bound is a proof that no optimization algorithm in the
family of algorithms being considered can guarantee better than a
certain level of accuracy for every objective in the family.

Matching upper bounds and lower bounds specify the minimax error, and
whichever optimization algorithm achieved the upper bound is optimal.
There are obvious benefits to identifying optimal methods, after all,
everyone wants to use the best possible algorithm. On the other hand,
there are frequently gaps between the best known upper bounds and lower
bounds on the minimax error and, in a certain sense, these gaps are more
exciting because they identify opportunities to design new methods that
improve over the current state of the art.

One of the most famous examples of a gap between upper and lower bounds
was for optimizing smooth, convex objectives using first-order
algorithms. For a very long time, the best known upper bound
corresponded to the guarantee of the Gradient Descent algorithm (which
dates all the way back to Cauchy in the 1840’s), which was known to
guarantee error of at most @xmath after @xmath iterations. On the other
hand, the best known lower bound showed that no first-order method could
guarantee error less than @xmath after @xmath iterations (Nemirovsky and
Yudin, 1983 ) ¹ ¹ 1 This lower bound was actually originally proven in
Russian in 1978—the 1983 citation is for the book’s English translation.
. This gap—between @xmath and @xmath —persisted for several years, and
at the time it was quite unclear what the minimax error would be. Many
efforts were made both to design better algorithms with guarantees
better than @xmath and to prove better lower bounds that showed that it
is impossible to do better than @xmath . It was not until several years
later that Nesterov’s famous Accelerated Gradient Descent algorithm was
proposed and shown to converge at the @xmath rate after all (Nesterov,
1983 ) . In this example, the existence of Nemirovsky and Yudin ’s
@xmath lower bound played an important role in driving optimization
research forward, despite the fact that the lower bound did not match
anything at the time.

It is important to properly interpret the meaning of a lower bound. It
says that no optimization algorithm in the considered class of
algorithms is able to provide a better guarantee for all objectives in
the considered family of objectives . This does not mean that continued
progress is futile, and that we should give up and settle for whatever
“optimal” algorithm we have. Instead, it means that additional progress
requires identifying additional, useful structures that algorithms can
exploit and modifying the classes of algorithms and objectives that we
consider accordingly. In this sense, studying minimax optimality and
proving lower bounds can be thought of as a task of modelling—out of the
many possible properties that an objective might have, which ones are
useful and exploitable, and which ones are not? What additional
properties would allow for better methods? Conveniently, lower bounds
typically identify a particular optimization objective that is hard to
optimize, and show us precisely why it is hard. Once we know the
pitfalls in a given setting, we can identify additional structure that
could be used to avoid them.

### 1.3 Distributed Optimization

The field of distributed optimization is marked by a huge diversity of
possible forms of parallelism. Parallel optimization algorithms can be
implemented on multi-core processors within a single computing device.
They can also arise in data center setting where many, very powerful
devices are arrayed in the same location. The parallel computers could
also be spread around the world, leading to high-latency communication
between them. These are just a few examples of the nearly unlimited
possible parallelism scenarios that one could face.

Given this variety and our interest in general-purpose algorithms, we
make efforts to study optimization methods in a way that is broadly
applicable to many different distributed optimization settings.
Accordingly, our framework for studying the complexity of distributed
optimization (see \prettyref subsec:algorithm-class-and-graph-framework)
is based around the structure of the parallelism—e.g. there are @xmath
parallel workers, or the machines communicate with each other every
@xmath iterations, or the parallel workers have access to distinct
datasets—rather than details of the setting—e.g. the machines have a
low-latency connection with each other, or each worker computes at
@xmath petaflops. This allows us to understand distributed optimization
in a greater variety of settings, and these general principles can often
also be applied to answer questions about specific settings.

Throughout, we will generally focus on understanding distributed
optimization in particular, fixed settings, for example, we might study
algorithms that use @xmath parallel workers which each compute @xmath
stochastic gradient estimates. In much of our analysis, we would treat
the quantities @xmath and @xmath as set in stone for several reasons.
First, if we have an algorithm that is optimal for any given @xmath and
@xmath , this naturally tells us the minimax error as a function of
@xmath and @xmath , and we can easily tell what would happen if they
were changed. Second, this allows us to better capture the tradeoffs
that are inherent in distributed optimization. In particular, the answer
to “would using more parallel workers improve my algorithm’s
performance?” is almost always “yes, obviously.” Similarly, running for
more iterations, using larger minibatches, and communicating more
frequently will always improve performance. However, the question in
distributed optimization is often how can we manage tradeoffs between
competing considerations. If I double the number of parallel workers,
can I halve the number of iterations—and therefore the total
runtime—without hurting performance? If communication between machines
takes @xmath ms and computing one stochastic gradient on each machine
takes @xmath ms, how large of a batchsize would get us to error @xmath
in the shortest amount of time? These are often the most important
questions in distributed optimization, and the answer generally depends
on the particulars of the situation. Finally, some aspects of the
parallel environment are outside of our control—for instance, my
department only has so many GPUs available—and it would not be so
helpful to know what the best number of machines is when that choice is
unavaiable. Nevertheless, again, this is largely a philosophical
question since our approach also allows for answering many of these
types of questions.

### 1.4 Overview of Results

In this thesis, we build a theory of minimax optimality for distributed
stochastic optimization and apply it to several parallel settings.

In \prettyref sec:formulating-the-complexity, we begin by describing an
extension of the classical oracle model (Nemirovsky and Yudin, 1983 ) to
the distributed setting, which allows us to rigorously pose questions of
optimality. The basic oracle model, which allows for proving tight and
informative lower bounds in the sequential (i.e. not distributed)
setting, is based on the idea of restricting the means through with an
algorithm interacts with the objective function, but not what the
algorithm is allowed to do with the information it learns about the
objective. This allows for strong lower bounds that apply to broad
classes of optimization algorithms and give deep insight into the
complexity of sequential optimization. However, we describe that the
classic oracle model is insufficient for distributed optimization, and
we describe in \prettyref subsec:algorithm-class-and-graph-framework an
extension of the model to the parallel setting, the “graph oracle
model.” The idea is to capture the distributed structure of an
optimization algorithm using a graph structure, where each vertex in the
graph corresponds to a single oracle access, and the edges describe the
dependencies between different queries. This approach is highly flexible
and allows us to formulate a notion of minimax oracle complexity for
many different distributed optimization settings using a single
framework.

In \prettyref sec:lower-bound-tools, we present several generic tools
for analyzing the minimax oracle complexity in the graph oracle model
which prove useful for our other results and are likely of interest more
broadly. First, many existing optimization lower bounds, even in the
sequential setting, apply only to fairly resrictive classes of
algorithms—typically only deterministic, “span-restricted,” or
“zero-respecting” algorithms. While these families contain many
algorithms of interest, they do not answer the question of whether we
might be able to do better using other methods. In \prettyref
subsec:high-level-lower-bound-approach, we sketch a general approach to
proving lower bounds that apply to much larger classes of optimization
algorithms, up to and including the class of all randomized algorithms
corresponding to a particular graph oracle setting. In \prettyref
subsec:generic-graph-oracle-lower-bound, we proceed to use this method
to prove a lower bound in the graph oracle model that applies to any
randomized distributed first-order method corresponding to any graph.
This lower bound only depends on two generic properties of the graph—the
number of vertices and its depth—and we apply it extensively in our
later results. Finally, in \prettyref subsec:reduction-section-all, we
describe a generic reduction that connects the complexity of optimizing
convex objectives with the complexity of optimizing strongly convex
objectives. In particular, we show that algorithms for convex
optimization, when applied to strongly convex objectives, can
automatically attain much faster rates of convergence without exploiting
the strong convexity in any explicit way.

In \prettyref sec:local-sgd, we study the theoretical properties of the
popular Local SGD algorithm. We begin in \prettyref
subsec:local-sgd-and-baselines by identifying three natural baseline
algorithms, corresponding to other variants of SGD that correspond to
the same graph oracle setting. The conventional wisdom says that Local
SGD should dominate these baselines, but little of the existing work
makes any direct comparison with these methods. In \prettyref
subsec:local-sgd-homogeneous, we study Local SGD in the “homogeneous”
setting, where each parallel worker has access to data from the same
distribution. We begin by showing that existing analysis of Local SGD
fails to show any improvement over the baseline algorithms, which raises
serious questions about the idea that Local SGD is uniformly better. We
proceed to show that in the special case of least squares problems,
Local SGD does indeed dominate the baselines; we prove a new guarantee
for Local SGD for general convex objectives that is sometimes better the
baselines but sometimes is not; and we conclude by showing that this was
no accident, and Local SGD really is worse than the baselines in some
regimes. In \prettyref subsec:local-sgd-heterogeneous, we turn to the
“heterogeneous” setting, where each parallel worker has access to data
from a different distribution, but where the goal is optimize the
average of the local objectives. We show that, as in the homogeneous
setting, the existing guarantees for Local SGD fail to improve over the
baselines. We also show that under the standard assumptions, Local SGD
might be able to improve over the baselines in a narrow regime, but will
generally perform much worse than a Minibatch SGD baseline. We conclude
by introducing a new assumption about the objective which allows for
Local SGD to improve over the baselines in certain regimes which we
identify.

In \prettyref sec:intermittent-communication-setting we study, more
broadly, the “intermittent communication setting,” a natural distributed
optimization setting that commonly arises in practice. The intermittent
communication setting corresponds to the case where @xmath parallel
workers collaborate to optimize an objective over the course of @xmath
rounds of communication, and in each round of communication, each
machine is able to compute @xmath stochastic gradients sequentially. In
\prettyref subsec:homogeneous-intermittent-minimax, we study the minimax
oracle complexity of optimization in the homogeneous intermittent
communication setting. For convex, strongly convex, and non-convex
objectives, we tighly characterize the minimax error and we identify
optimal algorithms that are given by the combination of two accelerated
SGD variants, a “minibatch” variant and a “single-machine” variant.
These results highlight an interesting dichotomy in the homogeneous
intermittent communication setting between exploiting the local
computation (captured by @xmath ) and exploiting the parallelism
(captured by @xmath ). In \prettyref
subsec:heterogeneous-convex-intermittent-minimax, we look to the
heterogeneous intermittent communication setting. Here, we also identify
the minimax error and optimal algorithms for convex and strongly convex
settings. This time, the optimal algorithm is just the minibatch
algorithm, which exploits the parallelism but not the local computation,
in contrast to the homogeneous case.

Finally, in \prettyref sec:breaking-the-lower-bounds, we revisit the
intermittent communication setting with the goal of “breaking” the lower
bounds presented in \prettyref sec:intermittent-communication-setting.
Specifically, we identify several additional properties of the objective
or oracle that allow for better methods whose guarantees are better than
the lower bounds would allow. In \prettyref
subsec:homogeneous-nearly-quadratic-intermittent-minimax, we show that
in the homogeneous intermittent communication setting, it is possible to
attain better error when the objective is “nearly-quadratic.” In
\prettyref subsec:bounded-heterogeneous-convex-intermittent-minimax, we
show that when the objective is only boundedly heterogeneous, meaning
the local objectives are not arbitrarily different, it is possible to
exploit this structure to outperform the optimal algorithm from
\prettyref subsec:heterogeneous-convex-intermittent-minimax. Finally, in
\prettyref subsec:non-convex-with-MSS, we show that when the oracle
satisfies a certain smoothness property, then it is possible to
circumvent the lower bound for homogeneous non-convex optimization
presented in \prettyref subsec:homogeneous-intermittent-minimax.

## 2 Formulating Distributed Stochastic Optimization

Throughout this thesis, we consider a stochastic optimization objective,
where the goal is to optimize

  -- -------- -- -----
     @xmath      (1)
  -- -------- -- -----

##### Machine Learning as Stochastic Optimization

The optimization problem ( 1 ) naturally captures many machine learning
problems. For example, supervised learning corresponds to taking @xmath
to be the loss of the predictor parametrized by @xmath on the sample
@xmath , then @xmath corresponds to the expected risk, and our goal is
to find parameters that minimize this risk ² ² 2 Unfortunately, standard
notation differs between the optimization and machine learning
literature. As is typical for optimization, we use “ @xmath ” to denote
the optimization variable of interest, and in machine learning other
notation—e.g. @xmath , @xmath , or @xmath —are more common, and “ @xmath
” is typically used for a feature representation of the data. . For
example, least squares regression from samples @xmath would correspond
to

  -- -------- -- -----
     @xmath      (2)
  -- -------- -- -----

In the context of machine learning, there are two ways of thinking about
the problem ( 1 ) and, in particular, the role of @xmath . The first is
a “sample average approximation” (SAA) viewpoint (Rubinstein and
Shapiro, 1990 ; Kleijnen and Rubinstein, 1996 ) , where we take @xmath
to be the empirical distribution over a training set of i.i.d. samples
from the distribution of interest, and solving ( 1 ) amounts to
empirical risk minimization, that is, finding parameters that minimize
the training loss. The second is a “stochastic approximation” (SA)
viewpoint (Robbins and Monro, 1951 ) , where @xmath is the population
distribution of interest, from which a collection of i.i.d. samples are
available. There are advantages and disadvantages to both perspectives.
In the SAA view, the objective @xmath has a special finite-sum structure
and the distribution @xmath is “known”, which opens up various
algorithmic possibilities that can allow for substantially faster
convergence to a minimizer. For example, variance reduction methods can
very efficiently exploit finite-sum structure (e.g. Johnson and Zhang,
2013 ) . On the other hand, solving ( 1 ) in the SAA sense says nothing
about how well the model would perform on unseen data, and a separate
argument is required to show generalization (e.g. via uniform
convergence or algorithmic stability). Conversely, in the SA setting,
solving ( 1 ) directly implies strong performance on unseen data, but SA
algorithms typically require fresh samples for each update which may
result in worse sample complexity.

##### Optimization and Learning

There are two, mostly orthogonal, sources of difficulty in solving the
stochastic optimization problem ( 1 ). First, there is the challenge of
optimizing the function @xmath , irrespective of the stochastic nature
of the problem. Indeed, even for algorithms that “know” the distribution
@xmath , it is far from trivial to find a minimizer of @xmath , and the
complexity of optimization using exact information about the objective
has been studied extensively. Simultaneously, there is an issue of
stochasticity—optimization algorithms need to optimize @xmath based on
noisy information, and there are fundamental statistical limits to what
can be learned about @xmath in this way. As a result of these two
challenges, the complexity of stochastic optimization typically involves
two pieces: an “optimization term” and a “statistical term,” which we
will highlight in our results.

Our goal is to understand the complexity of stochastic optimization for
different classes of optimization problems. However, significant care
must be taken to formalize this complexity in a useful way. In this
thesis, we define the complexity using three pieces: (1) a class of
objectives, (2) a class of oracles, and (3) a class of optimization
algorithms. These components together define an “optimization problem,”
for which we proceed to define and study the minimax complexity. We will
now discuss each of these pieces before defining our notion of
complexity.

### 2.1 The Function Class

To define an optimization problem, we first restrict our attention to a
set of objective functions satisfying certain properties. There are
innumerable conditions that we might impose of the objective—convexity,
smoothness, Lipschitzness, etc.—any combination of which may be
reasonable. However, we must make some assumptions in order to have any
hope of optimizing the function because it is possible to cast any
number of intractable or even uncomputable problems as (perhaps
extremely pathological) instances of ( 1 ).

Generally, we will try to consider broad function classes that make
minimal restrictions on the objective. It is a stronger statement when
an algorithm can guarantee good performance on a broader class of
functions; and algorithms that rely on less structure are more broadly
applicable. As described in \prettyref subsec:intro-ml-motivation, the
frequent changes to state-of-the-art machine learning models and loss
functions (which, together with the data distribution, determine @xmath
) means there is great value in general-purpose algorithms that can be
readily applied to new objectives. However, there is a balance to be
struck since there can also be value in imposing stronger restrictions
on the function class, which allows for specialized optimization
algorithms that exploit specific properties of the objective to achieve
stronger performance.

We will consider numerous function classes, most of which will be
defined as they become relevant. However, there are two function classes
to which we will return frequently: the class of smooth and convex
objectives and the class of smooth and strongly convex objectives. We
recall that a function @xmath is convex when

  -- -------- -- -----
     @xmath      (3)
  -- -------- -- -----

We say that @xmath is @xmath -strongly convex when @xmath is convex.
Finally, a function @xmath is @xmath -smooth if it is differentiable and
its gradient is @xmath -Lipschitz with respect to the L2 norm. For
convex functions, this is equivalent to the inequality

  -- -------- -- -----
     @xmath      (4)
  -- -------- -- -----

We will often consider the following classes of smooth objectives

  -- -------- -------- -- -----
     @xmath   @xmath      (5)
     @xmath   @xmath      (6)
  -- -------- -------- -- -----

We note that for both of these function classes, we impose restrictions
of @xmath , the population objective, only. To provide any meaningful
guarantee, it is necessary to bound in some way “how far away” the
minimizer might be. We follow the standard practice of measuring this
via the norm of the solution in the convex case, and the value of @xmath
in the strongly convex case.

##### Dimension-Free Complexity

We focus on a function classes where the dimension, @xmath , is not
explicitly bounded, and throughout this thesis @xmath should be thought
of as being “large.” Our notion of complexity will therefore be
dimension-free, capturing what it is possible to guarantee without
relying on the dimension being small in any way. Our complexity lower
bounds will hold only in sufficiently high dimensions (typically
polynomially-large in the other problem parameters), and our upper
bounds hold even in unbounded, even infinite, dimensions. Of course, it
is also important and interesting to study the complexity of
optimization in a dimension-dependent manner, which opens up the
possiblity of algorithms that can take advantage of a bound on the
dimension to ensure better performance. Nevertheless, our focus on the
dimension-free complexity is motivated by machine learning applications,
where the dimension, i.e. the parameter count, can easily run into the
millions or billions. In this context, dimension-dependent rates are
often weaker, and algorithms that depend on the dimension would
typically incur unreasonably high computational costs.

### 2.2 The Oracle

We study the complexity of optimization in the context of an oracle
model, which specifies through which means the algorithm interacts with
the optimization objective, and the oracle essentially specifies what
form the “input” to the optimization algorithm takes (Nemirovsky and
Yudin, 1983 ) . As an example, we mostly focus on optimization using a
stochastic first-order oracle, which an algorithm can query at a point
@xmath to receive a noisy estimate of the gradient @xmath . The classic
notion of oracle complexity essentially amounts to counting the number
of times that an algorithm needs to interact with the oracle before
reaching an approximate solution to ( 1 ).

Such oracle models have a long history in the study of optimization, and
they generally serve as a proxy for the computational complexity of
optimization. The number of oracle accesses generally serves as a good
proxy for the computational cost because the portions of the algorithm
corresponding to oracle accesses typically constitute the bulk of the
total computational cost. For instance, each iteration of stochastic
gradient descent involves computing one stochastic gradient, one
scalar-vector product, and one vector-vector addition, of which
computing the stochastic gradient will almost always be the most costly.
It is possible, in principle, to study the computational complexity of
optimization directly, but there are a number of challenges to doing so,
and across computer science it is notoriously difficult to formulate and
prove bounds on the computational complexity of almost anything, even
for much simpler problems than optimizing high-dimensional, real-valued
functions.

We focus on optimization using a stochastic first-order oracle which,
given a point @xmath , simply returns an unbiased, and bounded-variance
stochastic estimate of the gradient @xmath . There is, however, some
subtlety in the source of the stochasticity.

The first and most general type of stochastic first-order oracle, which
we will refer to as an “ independent-noise ” (I-N) oracle, simply
returns any random vector @xmath such that (1) @xmath , (2) @xmath , and
(3) @xmath is conditionally independent of both the state of the
algorithm and the previous interactions between the algorithm and the
oracle. For an I-N oracle, the stochasticity is almost completely
unconstrained: it can depend arbitrarily on the query @xmath , and
repeated queries at the same point @xmath can yield estimates of the
gradient with arbitrarily different distributions. The I-N oracle
imposes almost no structure on the stochastic gradients besides
unbiasedness and bounded variance, but it turns out that @xmath is often
sufficient for solving ( 1 ), and algorithms like stochastic gradient
descent require nothing more. Many of the results in this thesis will be
stated in terms of an independent-noise oracle.

We will refer to a second variant as a “ statistical learning ”
first-order oracle, @xmath . This oracle also returns an unbiased and
bounded variance estimate of the gradient, but with additional
structure. In particular, when queried at @xmath , the oracle returns
@xmath for an i.i.d. @xmath . Without imposing any assumptions on the
function @xmath , this structure is little different than the I-N oracle
described above. Nevertheless, @xmath opens the door to making
assumptions about the components @xmath and/or the distribution @xmath
which can be exploited by optimization algorithms. For example, in many
cases it is reasonable to assume that @xmath is convex and smooth in its
first argument for each @xmath , which introduces a potentially
non-trivial constraint on the stochastic gradients, see \prettyref
subsec:breaking-assumptions-on-components for further discussion.

Finally, we define an “ active statistical learning ” first-order
oracle, @xmath . This is the same as the statistical learning
first-order oracle above, but where the algorithm may either receive
@xmath for an i.i.d. @xmath or it may receive @xmath for some previously
seen @xmath of its choice. As an example, finite sum optimization
corresponds to the case where @xmath is the uniform distribution over
@xmath , and an active statistical learning oracle allows an
optimization algorithm to calculate the gradient of a chosen component.
We discuss active oracles further in \prettyref
subsec:repeated-accesses.

### 2.3 The Algorithm Class and Oracle Graph Framework

The final piece of an “optimization problem” is a class of algorithms
under consideration. A major advantage of oracle models as a concept is
that it allows us to analyze very broad families of algorithms.
Restricting how the algorithm gains information about the objective, but
not restricting what it can do with that information, allows for proving
strong lower bounds that can even apply to the class of all optimization
algorithms that only interact with the function through the oracle.

In the context of sequential optimization, it has been common
historically to consider a restricted class of algorithms for which each
oracle query must be in the linear span of previous oracle responses.
This class of span-restricted algorithms is conducive to proving lower
bounds, and numerous classic results on the complexity of convex
optimization study this family of optimization algorithms (e.g.
Nemirovsky and Yudin, 1983 ; Nesterov, 2004 ) . Indeed, this is a
natural family of algorithms which contains the vast majority of known
optimization methods including gradient descent, accelerated variants of
gradient descent, variance reduction methods, etc. and other methods
like coordinate descent belong to a recent generalization of the class
of span-restricted algorithms, termed “zero-respecting” algorithms
(Carmon et al., 2017a ) . Nevertheless, such results are still limited
and they do not preclude the possibility that algorithms might be able
to perform better by exploring points outside the span of the previously
seen oracle responses.

A related simplification is to consider the family of deterministic
algorithms that is not necessarily span-restricted or zero-respecting.
It turns out that this family of algorithms is essentially no more
powerful than the class of span-restricted or zero-respecting ones
because of their determinism. Specifically, it is possible to prove
nearly identical lower bounds by constructing a “resisting oracle” which
adversarially rotates the objective function so that any time the
algorithm deviates from the span of previous oracle responses, those
deviations happen only along invariant directions of the objective and
therefore reveal no useful information. It is possible to construct
resisting oracles for deterministic algorithms since the algorithm’s
every move can be anticipated from the outset.

For these reasons, there have been recent efforts to extend results on
the oracle complexity of optimization to broader families of algorithms,
up to and including the class of all randomized optimization algorithms
(e.g. Woodworth and Srebro, 2016 ; Carmon et al., 2017a ) . Indeed, in
this thesis, we will mostly focus on the complexity of optimization for
classes of randomized algorithms that are not necessarily
span-restricted or zero-respecting. We note that proving lower bounds
for such broad families of algorithms often requires considerably more
sophisticated proofs than are needed for deterministic span-restricted
or zero-respecting classes, as we discuss in \prettyref
subsec:high-level-lower-bound-approach.

Orthogonal to these issues of randomization versus determinism is the
question of how to formalize different types of distributed optimization
algorithms. There are myriad distributed optimization settings—from
parallelization across distant devices, to synchronous
single-instruction-multiple-data parallelism, to asynchronous parallel
processing—and capturing the complexity of optimization in any given
setting requires carefully delineating what exactly what the algorithm
is allowed to do.

A key challenge is capturing the difference between the following two
scenarios: (1) two machines query a stochastic gradient oracle @xmath
times each, and may communicate whatever and whenever they would like,
and (2) two machines query a stochastic gradient oracle @xmath times
each, but they may not communicate at all. While it is clear that the
first class of algorithms is more powerful, we note that the number of
oracle accesses does nothing to distinguish between these two scenarios,
which each involve @xmath stochastic gradient oracle queries. We observe
that the relevant distinction is the dependence structure between the
queries: in case (1), the second oracle query on the first machine might
depend both on the first query on the first machine and the first query
on the second machine, whereas in case (2) all of the queries on the
first machine are completely independent of all the queries on the
second machine.

We therefore introduce a “graph oracle framework” which captures the
nature of a distributed algorithm using a directed, acyclic graph and we
define families of distributed optimization algorithms in terms of the
associated graph. At a high level, each vertex in the graph corresponds
to a single oracle access, and the result of each oracle access is only
available in descendents of the corresponding vertex in the graph.

Let @xmath be a directed, acyclic graph with vertices @xmath and define

  -- -------- -- -----
     @xmath      (7)
  -- -------- -- -----

We associate a query rule @xmath and an oracle @xmath with each vertex
@xmath in the graph. The query rule at vertex @xmath is a mapping from
all of the available information about the function—i.e. the queries and
oracle responses in the ancestors of @xmath , which are in the set of
possible queries, @xmath , and set of possible answers, @xmath —in order
to choose a new query @xmath to submit to the oracle @xmath :

  -- -------- -- -----
     @xmath      (8)
  -- -------- -- -----

where @xmath is a string of independent, random bits available to the
algorithm, which allows us to capture randomized optimization algorithms
that nevertheless have deterministic query rules. Finally, the algorithm
has an output rule @xmath which takes all of the queries and oracle
responses and chooses the algorithm’s output:

  -- -------- -- -----
     @xmath      (9)
  -- -------- -- -----

In this way, for a given graph structure @xmath and set of associated
oracles @xmath , an optimization algorithm is specified by the query
rules @xmath and the output rule @xmath . We therefore define the family
@xmath as the set of all algorithms that can be implemented in this way.
We can, of course, consider subclasses of @xmath consisting of, for
example, only deterministic algorithms, or only span-restricted
algorithms. However, we will mostly avoid such restrictions, and many of
our lower bounds will apply to arbitrary randomized algorithms in @xmath
.

It will be helpful to consider several examples:

#### 2.3.1 Example: The Sequential Graph

The sequential graph @xmath , depicted in \prettyref
fig:sequential-graph, has @xmath vertices labelled @xmath with an edge
@xmath for each @xmath . This is the most basic non-trivial graph, and
it corresponds to the standard serial optimization setting. In
particular, for algorithms that correspond to the sequential graph, the
@xmath oracle access is allowed to depend on all of the first @xmath
oracle queries and oracle responses.

This graph specifies the structure of the oracle accesses allowed to the
algorithm, but to pose useful questions about the complexity of
optimization we also need to associate an oracle with each vertex in the
graph. Some natural examples include:

A Deterministic First-Order Oracle: When each vertex is associated with
a single deterministic gradient oracle access, @xmath , the family
@xmath contains all deterministic first-order serial optimization
algorithms that compute at most @xmath gradients. For instance, @xmath
steps of Gradient Descent corresponds to query rules

  -- -------- -- ------
     @xmath      (10)
  -- -------- -- ------

with output rule @xmath . In a similar way, other query rules can be
chosen that capture common algorithms like Accelerated Gradient Descent
(Nesterov, 1983 ) , Mirror Descent (Nemirovsky and Yudin, 1983 ) , and
many more.

A Stochastic First-Order Oracle: Each vertex could instead be associated
with a stochastic gradient oracle access, @xmath such that @xmath . In
this case, the family @xmath contains all stochastic first-order serial
optimization algorithms that compute at most @xmath stochastic
gradients. With appropriately defined query rules, this allows us to
capture a wide range of algorithms including stochastic gradient
descent, stochastic mirror descent, accelerated variants of stochastic
gradient descent, and more.

Finite Sum Optimization: When the optimization objective has finite sum
structure, i.e. @xmath , we can consider a component gradient oracle
@xmath . With appropriately defined query rules this can capture many of
the existing finite sum algorithms like SAG (Schmidt et al., 2017 ) ,
SAGA (Defazio et al., 2014 ) , SVRG (Johnson and Zhang, 2013 ) ,
accelerated variants of these, and more.

These are only a few examples and there are innumerable other oracles
that could be paired with the sequential graph to specify families of
serial optimization algorithms. Nevertheless, the graph oracle framework
is not actually necessary to understand the complexity of serial
optimization. Indeed, for all of the listed examples, the minimax oracle
complexity was already well understood before the graph oracle framework
was even proposed (Nemirovsky and Yudin, 1983 ; Woodworth and Srebro,
2016 ) . For this reason, we will focus on the complexity of
optimization in more interesting graphs.

#### 2.3.2 Example: The Layer Graph

The layer graph @xmath , shown in \prettyref fig:layer-graph, has @xmath
vertices labelled @xmath for @xmath and @xmath , with edges from @xmath
for all @xmath . This corresponds to simple synchronous parallelism
where algorithms can issue @xmath oracle queries in parallel. Such
algorithms are natural when using multi-core processors or when multiple
computing devices are available. As in the previous section, pairing the
layer graph @xmath with a set of oracles allows us to capture natural
families of distributed optimization algorithms, and to study the
minimax complexity of optimization for these families of algorithms. As
an example, we will discuss stochastic first-order algorithms with this
graph:

Stochastic First-Order Parallel Optimization The layer graph with
stochastic gradient oracles @xmath with @xmath specifies a family of
stochastic first-order parallel algorithms @xmath . A natural algorithm
in this family is minibatch stochastic gradient descent, which
corresponds to query rules

  -- -------- -- ------
     @xmath      (11)
  -- -------- -- ------

This family of algorithms also includes Accelerated Minibatch SGD
(Cotter et al., 2011 ; Lan, 2012 ) , Minibatch Stochastic Mirror
Descent, and many others. As with Minibatch SGD, any of the stochastic
first-order algorithms corresponding to the sequential graph can be
naturally extended to the layer graph via minibatching, reducing the
variance of the stochastic gradients and generally speeding convergence.

#### 2.3.3 Example: The Intermittent Communication Graph

The intermittent communication graph @xmath , see \prettyref
fig:intermittent-graph, has @xmath vertices labelled @xmath for @xmath ,
@xmath , and @xmath , with edges from @xmath and from @xmath for each
@xmath . In this way, the intermittent communication graph most
naturally corresponds to a setting in which @xmath devices work in
parallel but where communication between the devices is limited. In
contrast to the layer graph where oracle queries are issued in parallel
but all of the responses from time @xmath are available for all of the
queries at time @xmath , in the intermittent communication graph the
queries are broken up into @xmath “rounds of communication,” each of
which corresponds to @xmath queries on each machine. So, @xmath , and
only the oracle responses obtained on device @xmath and responses from
previous rounds ( @xmath ) are available to choose the queries on device
@xmath .

This natural distributed optimization setting will be our main focus in
\prettyref sec:intermittent-communication-setting. As with the
previously discussed examples, we can pair the intermittent
communication graph with many oracles in order to define families of
optimization algorithms and for the most part, we will focus on
stochastic first-order oracles.

The Homogeneous Setting: In this setting, each vertex is associated with
the same oracle, a stochastic gradient oracle @xmath such that @xmath .
This family of algorithms @xmath includes Minibatch SGD, Local SGD, and
many more, which we will discuss in \prettyref sec:local-sgd and
\prettyref sec:intermittent-communication-setting. In the context of
supervised machine learning, this could correspond to a situation where
each of the stochastic gradients is computed using an independent sample
from the data distribution.

The Heterogeneous Setting: However, unlike the previous examples, it is
often interesting to associate different vertices in the graph with
different oracles. In the heterogeneous setting, we suppose that the
objective has finite sum structure @xmath , and that the oracle queries
in vertices corresponding to the @xmath machine yield stochastic
gradient estimates for @xmath specifically. That is, @xmath such that
@xmath . This should be thought of as the @xmath machine having
stochastic gradient access to @xmath and the goal is for the @xmath
machines to achieve consensus by finding parameters @xmath that minimize
the average of the local objectives. In a supervised learning setting,
this corresponds to each machine computing stochastic gradients of the
local loss based on a separate samples on each machine. Heterogeneity
can arise, for example, when partitioning an i.i.d. training dataset
across the machines, which introduces some (probably “small”) amount of
heterogeneity to the stochastic gradients. Otherwise, when each machine
uses data from genuinely different sources, for example from users on
different continents, this can also introduce heterogeneity.

The Federated Setting: We could also consider a stylized version of
Federated Learning (Kairouz et al., 2019 ) that captures some, but not
all, of the interesting features of the setting. This version of
Federated Learning is similar to the heterogeneous setting, except that
the components of the objective are not tied to any particular parallel
worker. In particular, we suppose that the objective has the form @xmath
where @xmath is an arbitrary distribution (whose support need not be
finite or even countable). We then associate with each machine and each
round of communication a stochastic gradient oracle for @xmath for a
random @xmath , i.e. @xmath such that @xmath and @xmath .

The Federated setting captures optimization in the intermittent
communication setting when the stochastic gradients on each machine in
each round are allowed to be correlated. This can arise, for example,
when training a language model using data held on users’ cell phones. In
each round, @xmath of the available cell phones are randomly chosen and
used to compute @xmath stochastic gradients, these gradients are then
communicated back to a central coordinator and the process repeats.
Since each user will have different language patterns, the stochastic
gradients computed on each machine in each round will correspond to
somewhat different objectives.

### 2.4 The Minimax Complexity

The combination of a function class, a graph and oracles that define the
structure of an algorithm’s interaction with the objective, and the
associated class of optimization algorithms define “an optimization
problem.” We proceed to define the minimax oracle complexity of an
optimization problem, which asks what is the best guarantee that any
algorithm can provide for every function in the class? For a given
function class @xmath , oracle graph @xmath , assignment of oracles to
vertices @xmath , and family of algorithms @xmath , we define

  -- -------- -- ------
     @xmath      (12)
  -- -------- -- ------

Throughout this thesis, we will bound the quantity ( 12 ) for various
distributed optimization settings of interest.

There are other similar but distinct ways that we could have defined the
minimax complexity. One minor variation is to require a bound on the
suboptimality @xmath with constant or high probability. We note that
constant probability bounds are essentially equivalent to in-expectation
bounds up to constant factors and, indeed, many of our lower bounds are
shown to hold with constant probability. High probability bounds on the
suboptimality are often impossible using merely bounded-variance
stochastic oracles, and they typically require less standard assumptions
like subgaussianity of the oracle. Obtaining high probability bounds is
interesting and important, but here we focus on the more standard
setting of in-expectation bounds.

It is also common to see the definition of the minimax complexity turned
around—rather than asking what is the smallest achievable error with a
certain number of oracle queries, asking instead how many oracle queries
would be necessary to reach a given suboptimality @xmath . In the
context of sequential optimization (i.e. the sequential graph), it is
easy to see that these questions are two sides of the same coin: a bound
on ( 12 ) in terms of the number of queries, @xmath , can be solved for
@xmath to yield a bound on the number of queries needed to reach
accuracy @xmath as a function of @xmath .

However, for more complex distributed optimization settings like the
intermittent communication setting, there are multiple dimensions along
which the graph could vary ( @xmath , the number of machines, @xmath ,
the number of rounds of communication, and @xmath , the number of
queries per round), and it is therefore less obvious how to “invert” (
12 ) in a general-purpose way. For this reason, we prefer to think of
the graph as fixed and to ask about the minimax complexity with respect
to that graph specifically. Of course, by seeing how the minimax
complexity depends on various properties of this graph, we can also
answer questions about what sort of graph would allow us to reach a
particular accuracy @xmath .

##### Alternatives to the Graph Oracle Model

Besides the graph oracle model, there are other possible formulations of
minimax complexity for distributed optimization algorithms. One
alternative is a communication complexity approach (Tsitsiklis and Luo,
1987 ; Zhang et al., 2013b ; Garg et al., 2014 ; Braverman et al., 2016
) , where @xmath parallel workers have a local function—perhaps based on
a locally held dataset—and the goal is to compute the minimizer of the
average of the local functions. In the communication complexity
formulation, each worker has unlimited computational power and can
compute arbitrary information about its local objective (including,
e.g. its exact minimizer), but it is limited to transmit only a limited
number of bits to the other machines. This approach is necessarily
dimension-dependent because the number of bits needed just to represent
the solution scales with the dimension, and beyond this issue,
algorithms in this setting often explicitly rely on the dimension being
bounded. Consequently, this approach is not as well suited to our
settings. Another alternative allows the machines to communicate
real-valued vectors, but restricts the vectors that they are allowed to
compute and transmit. For instance, Arjevani and Shamir ( 2015 )
presents communication complexity lower bounds for algorithms that can
only compute vectors that lie in a certain subspace, which includes
e.g. linear combinations of gradients of their local function. Lee
et al. ( 2017 ) impose a similar restriction, but allow the data
defining the local functions to be allocated to the different machines
in a strategic manner.

Our framework applies to general stochastic optimization problems and
does not impose any restrictions on what computation the algorithm may
perform or what it can communicate. Rather, we restrict the means by
which the algorithm interacts with the objective and the structure of
that interaction. In this way, our lower bounds can apply to very broad
classes of algorithms, up to and including the family of all randomized
algorithms that correspond to a given graph, whereas previous arguments
are typically restricted to substantially smaller families of
algorithms.

## 3 Tools for Proving Lower Bounds

We will now introduce several tools that will be useful for analyzing
the minimax complexity of optimization.

First, we will introduce a conceptual approach to proving lower bounds
for arbitrary randomized algorithms. As mentioned in \prettyref
subsec:algorithm-class-and-graph-framework, optimization lower bounds
can be quite simple for classes of zero-respecting algorithms and often
require much more sophisticated constructions when dealing with broader
families of randomized algorithms. Nevertheless, we will describe a
minor modification to a lower bound construction which allows us to
argue that any randomized algorithm is nearly zero-respecting, which
facilitates proving lower bounds.

Second, we will prove a lower bound on the minimax complexity for
classes of algorithms based on very simple and generic properties of the
associated graph. These lower bounds are very general, and we argue that
they are tight in a certain sense. However, for some graphs, including
the intermittent communication graph, they are not tight and a more
specialized analysis is required, which we will perform in later
sections. Nevertheless, these basic lower bounds are broadly useful and
we will refer to them frequently.

Finally, we will describe a method of corresponding algorithmic
guarantees for convex objectives with better guarantees in the strongly
convex setting. It is well-known that algorithms for strongly convex
optimization can be applied to merely convex functions by adding a small
regularization term to the objective. We show a reduction that goes in
the opposite direction. Of course, since strongly convex objectives are
also convex, a convex optimization algorithm will obviously succeed when
applied to a strongly convex function. However, it is not at all obvious
that the algorithm would obtain better guarantees with strong convexity;
we show that it will indeed perform better and identify the better rate.

### 3.1 A Technique for Proving Lower Bounds for Randomized Algorithms

Before proceeding to the argument for randomized algorithms, it is
worthwhile to describe the basic proof of lower bounds for
zero-respecting algorithms. For simplicity, we focus on lower bounds for
smooth, convex objectives in @xmath , but a similar technique applies
more broadly. The classic lower bound for functions in @xmath with a
deterministic first-order oracle is based on the following hard instance
due to Nesterov ( 2004 ) :

  -- -------- -- ------
     @xmath      (13)
  -- -------- -- ------

The key property of this function @xmath is the “chain-like” nature of
its gradient. Specifically, it is easy to see from the gradient,

  -- -------- -- ------
     @xmath      (14)
  -- -------- -- ------

that if @xmath , then @xmath too. For this reason, a zero-respecting
algorithm—whose queries have non-zero coordinates only where previously
seen gradients had non-zero coordinates—can only increase the number of
non-zero coordinates in its iterates by one for each gradient it
computes. Therefore, any span-restricted or zero-respecting optimization
algorithm that makes @xmath first-order oracle queries will have an
output with @xmath . From here, the rest of the lower bound proof is
very simple; all that is necessary is to show that the suboptimality of
such a point @xmath is relatively large. For this particular function,
it can be shown that the suboptimality scales with @xmath , which gives
the classic and well-known lower bound for first-order optimization.
This technique of arguing that the algorithm’s output will lie in some
restricted subspace, and then showing that any vector in that subspace
will have high suboptimality is quite powerful, and will form the basis
for many of our results.

However, the above argument relied crucially on the algorithm being
zero-respecting. In particular, there is a non-zero-respecting algorithm
that immediately, exactly minimizes this objective without making even a
single gradient oracle query, which is the algorithm that just returns
@xmath . Of course, this algorithm only works for this specific
objective, but this just goes to show that proving lower bounds for
non-zero-respecting algorithms is considerably more difficult. Indeed,
such lower bounds cannot be proven using a single hard instance for this
reason. Moreover, even if a non-zero-respecting algorithm isn’t
“cheating” by immediately returning the minimizer of @xmath , we note
that the chain-like property of the gradient is extremely delicate. If
an algorithm simply queries the gradient oracle at any point plus almost
any miniscule perturbation, then the gradient would be dense, and the
algorithm could immediately “find” all of relevant coordinates.
Fortunately, there is a relatively straightforward fix for this issue,
which involves two pieces.

The first piece is the observation that, in high dimensions, a vector’s
inner product with a random unit vector will be very small with high
probability, specifically, on the order of @xmath . To capitalize on
this, we introduce a random rotation @xmath for large @xmath and take as
our hard instance @xmath . Now, optimizing this rotated function
requires obtaining a significant inner product with the columns of
@xmath , which is very unlikely to happen by merely “guessing.”

Nevertheless, despite the fact that a vector’s inner product with each
columns of @xmath is likely to be very small, it won’t be exactly zero,
so a single gradient oracle query can reveal a lot of information about
@xmath , which can break the lower bound. To address this, the second
idea is to “flatten out” the objective in such a way that the gradient
maintains the important “chain-like” property even when the query has a
slightly non-zero inner product with potentially all of the columns of
@xmath . To that end, we can modify the objective @xmath to be

  -- -------- -- ------
     @xmath      (15)
  -- -------- -- ------

where

  -- -------- -- ------
     @xmath      (16)
  -- -------- -- ------

for some small parameter @xmath (see \prettyref fig:flattened-out-psi).

Because @xmath for @xmath , the coordinates of @xmath are zero until the
corresponding coordinates of @xmath are substantially non-zero, to an
extent that would not happen by chance. Specifically, if @xmath is a
vector such that @xmath for @xmath , then the gradient

  -- -------- -- ------
     @xmath      (17)
  -- -------- -- ------

will be a linear combination of @xmath only, and very little information
about @xmath is leaked beyond the fact that their inner products with
@xmath are small.

Using this approach, it is generally possible to extend the classic
lower bound technique for span-restricted or zero-respecting algorithms
to the class of all randomized algorithms. However, the first
applications of the random rotation and flattening-out of the objective
involved very long and delicate proofs (Woodworth and Srebro, 2016 ,
2017 ; Carmon et al., 2017a ; Woodworth et al., 2018 ; Arjevani et al.,
2019 ) , which hinged on carefully controlling the statistical
dependencies between the yet “unknown” columns of @xmath and the
previous oracle interactions. Since then, the argument has gradually
been refined and simplified, culminating in the PhD thesis of Carmon (
2020 ) , who shows in a very general manner that any algorithm is almost
zero-respecting when optimizing functions like @xmath above, and
provides a concise and simple proof of this fact.

This idea forms the basis of many of the lower bounds that we will
prove. However, there is one additional technicality that requires some
attention. In particular, the intuition that the inner product of a
vector with a random unit vector is on the order @xmath depends on that
vector having bounded norm. Annoyingly, it is still the case that an
algorithm can achieve a high inner product with a random vector—even in
high dimensions—by simply querying the oracle with a vector that has a
huge norm. Although it seems intutitive that, generally speaking,
querying the oracle at a point with a super large norm—much larger than
the norm of the function’s minimizer—should not be an effective
strategy, this must be addressed by our lower bound proofs.

The easiest way to deal with this is to further modify the objective
such that its gradient at far away points is independent of @xmath , and
therefore querying there reveals no useable information. When proving
lower bounds for non-convex optimization, this can be easily
accomplished by introducing a soft projection to the objective so the
algorthm must optimize @xmath where @xmath (Carmon et al., 2017a ) .
This way, the algorithm’s queries are essentially bounded by @xmath ,
and the argument goes through. Unfortunately, when the objective is
required to be convex, this approach does not work as readily. In the
next section, we address the issue of bounding the queries by using a
slightly different construction than we have so far described.
Nevertheless, the proof follows the same idea: we introduce a random
rotation, and we make the objective insensitive to small inner products
with the columns of the rotation matrix.

### 3.2 A Generic Graph Oracle Lower Bound

In this section, we prove a lower bound in the graph oracle model for
any distributed, stochastic first-order optimization algorithm which
depends on the associated graph. Our generic lower bound for first-order
algorithms in the graph oracle model is based on a hard instance with
the following properties:

###### Lemma 1.

Let @xmath for @xmath be orthogonal so that @xmath , and let @xmath and
@xmath be given. Then there exists a function @xmath such that for any
@xmath with @xmath

  -- -------- --
     @xmath   
  -- -------- --

Furthermore, for each @xmath , @xmath ; if @xmath then @xmath regardless
of @xmath ; and if @xmath for all @xmath , then @xmath and it does not
depend on the columns @xmath .

The function @xmath is constructed as the Moreau envelope (Bauschke
et al., 2011 ) of a function with the form

  -- -------- -- ------
     @xmath      (18)
  -- -------- -- ------

for a small constant @xmath . This resembles a classic construction for
lower bounds for non-smooth objectives (Nemirovsky and Yudin, 1983 ) ,
and taking @xmath to be its Moreau envelope “smoothes it out” to also be
@xmath -smooth. Upon inspection, it is fairly clear that minimizing
@xmath requires finding a point @xmath whose inner product with each
column @xmath is substantially negative, on the order of @xmath . The
gradient of @xmath is related to the subgradients of @xmath , and when
@xmath is large, it is easy to see that @xmath with @xmath regardless of
@xmath . On the other hand, because of the terms @xmath , if @xmath is
very small, then @xmath will not be in the @xmath and therefore, @xmath
will play no role in the subgradients of @xmath . The proof is
straightforward but technical, and we defer the details to \prettyref
app:generic-graph-oracle-lower-bound.

As discussed above, the idea of the lower bound is that any algorithm
will be almost zero-respecting when optimizing @xmath for a uniformly
random orthogonal matrix @xmath when the dimension @xmath is
sufficiently large. Furthermore, the gradient of @xmath has the property
that for approximately zero-respecting queries, each query only reveals
a single new column of @xmath . Consequently, the number of columns that
can be learned by the algorithm is bounded by the depth of the graph,
i.e. the length of the longest directed path in the graph.

###### Theorem 1.

For any graph @xmath , let @xmath be an exact gradient oracle for each
@xmath . For any @xmath and @xmath and any dimension @xmath

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Let @xmath and for each @xmath , define @xmath to be the length of the
longest directed path in @xmath that ends at @xmath . Let @xmath be a
uniformly random orthogonal matrix and let @xmath be the objective
described in \prettyref lem:prox-construction. Finally, consider an
arbitrary algorithm in @xmath .

For @xmath , we define the following “good” events

  -- -------- -------- -- ------
     @xmath   @xmath      (19)
     @xmath   @xmath      (20)
  -- -------- -------- -- ------

which indicates that the query @xmath does not have a large inner
product with @xmath for @xmath greater than its depth, and similarly for
the algorithm’s output. We now proceed to lower bound @xmath conditioned
on an abritrary realization of the algorithm’s coins @xmath :

  -- -------- -------- -- ------
     @xmath   @xmath      (21)
              @xmath      (22)
  -- -------- -------- -- ------

Here, we rewrote the union as a disjoint union and applied the union
bound, following the clever approach of Diakonikolas and Guzmán ( 2019 )
. Writing it this way is helpful for the following reason: by \prettyref
lem:prox-construction, the event @xmath implies that @xmath is a
measurable function of @xmath and @xmath . Furthermore, for all @xmath ,
@xmath . We also recall that in the graph oracle model, for each vertex
@xmath , the query, @xmath , is generated according to a query rule
@xmath as

  -- -------- -- ------
     @xmath      (23)
  -- -------- -- ------

where @xmath are the random coins of the algorithm. Therefore, under the
event @xmath , there there exists a function @xmath such that

  -- -------- -- ------
     @xmath      (24)
  -- -------- -- ------

Therefore, we just need to bound

  -- -------- -------- -- ------
     @xmath   @xmath      
              @xmath      (25)
              @xmath      (26)
              @xmath      (27)
  -- -------- -------- -- ------

Finally, we note that since @xmath is a uniformly random orthogonal
matrix, @xmath is independent of @xmath and conditioned on @xmath ,
@xmath is uniformly distributed on the unit sphere in the @xmath
-dimensional subspace that is orthogonal to their span. Therefore, the
above probability corresponds to the inner product between a fixed
vector and a uniformly random unit vector being larger than @xmath . For
now, assume that the queries @xmath have norm bounded by @xmath . Then,
standard results about concentration on the sphere (Ball et al., 1997 )
proves that

  -- -------- -------- -- ------
     @xmath   @xmath      (28)
              @xmath      (29)
  -- -------- -------- -- ------

By the same argument,

  -- -------- -- ------
     @xmath      (30)
  -- -------- -- ------

From this and the fact that @xmath was arbitrary, we conclude that if
@xmath then

  -- -------- -- ------
     @xmath      (31)
  -- -------- -- ------

Above, we assumed that the algorithm’s queries have norm @xmath .
However, this assumption is without loss of generality because, by
\prettyref lem:prox-construction, @xmath for any query with @xmath .
Therefore, for any algorithm that makes queries with norm greater than
@xmath , there is another equally good algorithm that instead simply
assumes that the gradient for such queries would be equal to @xmath .

Finally, the event @xmath implies that for all @xmath , @xmath . We
therefore take @xmath and conclude from \prettyref lem:prox-construction
that under the event @xmath

  -- -------- -- ------
     @xmath      (32)
  -- -------- -- ------

This completes the proof. ∎

This lower bound is tight in the sense that it is the highest lower
bound that applies for all graphs with a given depth. Momentarily, we
will discuss several examples of graphs where we can identify algorithms
whose guarantees match this lower bound, and we will also discuss one
example where the lower bound does not match known upper bounds, which
therefore motivates further study.

It is also worth emphasizing that the lower bound \prettyref
thm:generic-graph-lower-bound applies even for an exact gradient oracle,
and does not rely at all upon making the problem difficult through
stochasticity. Before we proceed, we also provide a simple lower bound
on the “statistical term,” which corresponds to the
information-theoretic difficulty of optimizing on the basis of only
@xmath samples. Lower bounds very similar to this are well-known (see
e.g. Nemirovsky and Yudin, 1983 ) , but we include it to be
self-contained:

###### Lemma 2.

Let @xmath and an arbitrary graph @xmath be given, and let @xmath be a
stochastic gradient oracle with variance bounded by @xmath . Then in any
dimension

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Consider the following pair of objectives:

  -- -------- -------- -------- ------
     @xmath   @xmath            (33)
              @xmath   @xmath   
  -- -------- -------- -------- ------

with a stochatic gradient oracles

  -- -------- -------- -------- ------
     @xmath   @xmath            (34)
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- ------

First, we note that for any @xmath ,

  -- -------- -- ------
     @xmath      (35)
  -- -------- -- ------

and vice versa. Therefore, any algorithm that succeeds in optimizing
both @xmath and @xmath to accuracy better than @xmath with probability
at least @xmath needs to determine which of the two functions it is
optimizing with probability at least @xmath . However, by the Pinsker
inequality, the total variation distance between @xmath queries to
@xmath and @xmath is at most

  -- -------- -------- -- ------
     @xmath   @xmath      (36)
              @xmath      (37)
              @xmath      (38)
  -- -------- -------- -- ------

Therefore, if @xmath , no algorithm can optimize to accuracy better
@xmath with probability greater than @xmath .

Finally, we note that @xmath and @xmath are @xmath -smooth, and have
minimizers @xmath . Therefore, in the smooth convex case we take @xmath
and @xmath so that the objectives are @xmath -smooth and have solutions
of norm @xmath and with probability at least @xmath

  -- -------- -- ------
     @xmath      (39)
  -- -------- -- ------

Similarly, @xmath and @xmath are @xmath -smooth, @xmath -strongly
convex, and @xmath for @xmath . Therefore, we take @xmath and @xmath so
that the objectives are @xmath -smooth and @xmath -strongly convex and
so that @xmath for @xmath , and so that with probability at least @xmath

  -- -------- -- ------
     @xmath      (40)
  -- -------- -- ------

This completes the proof. ∎

#### 3.2.1 Example: The Sequential Graph

In the sequential graph, corresponding to @xmath sequential queries to
the oracle, the minimax complexity of stochastic first-order
optimization is already very well known and our lower bounds are
redundant (Nemirovsky and Yudin, 1983 ; Lan, 2012 ) . Neverthless, it is
the case that the pair of lower bounds \prettyref
thm:generic-graph-lower-bound and \prettyref
lem:statistical-term-lower-bound match the known minimax error, i.e.

  -- -------- -- ------
     @xmath      (41)
  -- -------- -- ------

Our lower bounds are matched by the guarantee of an accelerated variant
of SGD, AC-SA (Lan, 2012 ) , which establishes this as the minimax
optimal rate. Since we will be referring to AC-SA frequently, we will
also take this opportunity to describe the algorithm in more detail.

Initialize: @xmath

for @xmath do

@xmath

@xmath

@xmath

end for

Return: @xmath

Algorithm 1 AC-SA

The algorithm is inspired by Nesterov’s Accelerated Gradient Descent
algorithm, which converges at the optimal @xmath rate in the case of an
exact gradient oracle (Nesterov, 1983 ) . In the same way, AC-SA
maintains multiple iterates, which are updated using a carefully tuned
sequence of momentum parameters. When the momentum and stepsize
parameters are optimally tuned, Lan ( 2012 ) shows that for any @xmath ,

  -- -------- -- ------
     @xmath      (42)
  -- -------- -- ------

i.e. the optimal error. We note that by the @xmath -smoothness of @xmath
, it is always the case that @xmath , so the final term in the lower
bound can be attained for free.

#### 3.2.2 Example: The Layer Graph

In the layer graph, @xmath , with a stochastic gradient oracle,
corresponding to @xmath sequential batches of @xmath parallel queries to
the oracle, the depth is @xmath and @xmath . Therefore, by combining
\prettyref thm:generic-graph-lower-bound and \prettyref
lem:statistical-term-lower-bound, we have the lower bound

  -- -------- -- ------
     @xmath      (43)
  -- -------- -- ------

This lower bound is matched by the minibatch AC-SA algorithm—that is
@xmath iterations of AC-SA using minibatches of size @xmath , which
reduces the stochastic gradient variance by a factor of @xmath —so we
conclude that the lower bounds are also tight in this setting.

#### 3.2.3 Example: The Delay Graph

The delay graph @xmath , shown in \prettyref fig:delay-graph, has two
parameters: @xmath , the number of vertices, and a delay @xmath . The
vertices are labelled @xmath and @xmath . Therefore, the delay graph
might correspond to an asynchronous setting in which the algorithm
issues queries to an oracle, but does not receive a response for @xmath
time steps. The depth of the delay graph is @xmath , so the lower bound
from \prettyref thm:generic-graph-lower-bound and \prettyref
lem:statistical-term-lower-bound is

  -- -------- -- ------
     @xmath      (44)
  -- -------- -- ------

A natural algorithm for this setting is delayed-update SGD, which uses
updates @xmath . Early analysis of this algorithm only proved an
optimization term scaling with the poor scaling @xmath (Feyzmahdavian
et al., 2016 ) . In later work, Arjevani et al. ( 2020b ) improved the
optimization term to @xmath in the special case of quadratic objectives,
and most recently, Stich and Karimireddy ( 2019 ) showed that
delayed-update SGD guarantees

  -- -------- -- ------
     @xmath      (45)
  -- -------- -- ------

for any @xmath . However, to have any hope of matching the lower bound
would require accelerating the algorithm, and we are not aware of any
successful attempts to do this.

Nevertheless, the following simple approach turns out to be optimal:
first, we note that @xmath minibatch gradients of size @xmath can be
computed fully sequentially, and we can therefore implement minibatch
AC-SA (Lan, 2012 ) to guarantee

  -- -------- -- ------
     @xmath      (46)
  -- -------- -- ------

which matches the lower bound ( 44 ) ³ ³ 3 In case @xmath , we can
always return 0 to match the lower bound since @xmath . . We accomplish
this by splitting the @xmath steps into @xmath windows of length @xmath
; during the first window, we query the stochastic gradient oracle
@xmath times at the same point @xmath , then during the second window we
wait @xmath time steps until all of the @xmath queries are answered, we
take one AC-SA update, and we repeat this. This algorithm is somewhat
unnatural, since it wastes half of its allowed oracle queries waiting,
and it would be nice if a prettier algorithm like an accelerated variant
of delayed-update SGD could be shown to achieve this same rate.
Nevertheless, this algorithm is optimal and confirms that \prettyref
thm:generic-graph-lower-bound is also tight for the delay graph.

#### 3.2.4 A Gap: The Intermittent Communication Graph

The fact that \prettyref thm:generic-graph-lower-bound and \prettyref
lem:statistical-term-lower-bound are tight for the sequential, layer,
and delay graphs makes it tempting to hope that they might be tight for
all graphs. However, it turns out to be loose for the intermittent
communication graph, and much of the rest of this thesis will be
dedicated to closing that gap.

We recall from \prettyref subsec:intermittent-graph that the
intermittent communication graph, @xmath , corresponds to a setting in
which @xmath machines work in parallel over @xmath rounds of
communication with @xmath oracle queries per round of communication. The
number of vertices in the graph is @xmath and the depth is equal to
@xmath . The lower bounds \prettyref thm:generic-graph-lower-bound and
\prettyref lem:statistical-term-lower-bound indicate

  -- -- -- ------
           (47)
  -- -- -- ------

However, the depth of the graph already suggests a problem—since the
lower bound \prettyref thm:generic-graph-lower-bound depends only on the
depth @xmath , it does not distinguish between algorithms that
communicate once and make @xmath queries per machine ( @xmath , @xmath ,
and @xmath ) and algorithms that communicate @xmath times and make one
query per communication ( @xmath , @xmath , and @xmath ). It seems
intuitively obvious that the latter family of algorithms can perform
better, but the lower bound \prettyref thm:generic-graph-lower-bound
fails to show this.

Indeed, this shortcoming is not limited to our lower bound. The
intermittent communication setting has been widely studied for over a
decade, with many optimization algorithms proposed and analyzed
(Zinkevich et al., 2010 ; Cotter et al., 2011 ; Dekel et al., 2012 ;
Zhang et al., 2013a , c ; Shamir and Srebro, 2014 ) , and obtaining new
methods and improved analysis is still a very active area of research
(Stich and Karimireddy, 2019 ; Wang et al., 2017 ; Stich, 2018 ; Wang
and Joshi, 2018 ; Khaled et al., 2020 ; Haddadpour et al., 2019a ;
Woodworth et al., 2020b ) . However, despite these efforts, existing
results cannot rule out the possibility that the optimal rate for fixed
@xmath can be achieved using only a single round of communication (
@xmath ). Existing upper bounds do not beat the lower bound \prettyref
thm:generic-graph-lower-bound, and existing lower bounds do not
distinguish between @xmath and @xmath . The possibility that the optimal
rate is achievable with @xmath was originally suggested by Zhang et al.
( 2013c ) , and indeed Woodworth et al. ( 2020b ) proved that an
algorithm that communicates just once is optimal in the special case of
quadratic objectives. While it seems unlikely that a single round of
communication suffices in the general case, existing results cannot
answer this extremely basic question.

We will revisit this issue and establish matching upper and lower bounds
which establish the minimax complexity in the intermittent communication
setting in \prettyref sec:intermittent-communication-setting.

### 3.3 Reductions Between Convex and Strongly Convex Optimization

Algorithms for convex optimization are typically analyzed in two
distinct settings: the convex setting and the strongly convex setting.
Since strongly convex functions are also convex, any algorithm that
guarantees a particular rate of convergence for arbitrary convex
objectives immediately guarantees that same rate when applied to a
strongly convex objective. However, it is generally possible to attain a
much better convergence rate when the objective is strongly convex, and
it is not clear that an algorithm designed for merely convex objectives
would necessarily do this. For example, it is well known that gradient
descent guarantees convergence at a rate @xmath for smooth and convex
objectives, and it therefore guarantees the same @xmath rate for smooth
and strongly convex objectives too. However, it is also well known that
gradient descent actually converges at the much faster rate @xmath for
strongly convex objectives.

It is not clear if and when algorithmic guarantees in the convex setting
can be parlayed into better guarantees for the strongly convex case.
Indeed, it would not be surprising if convex guarantees did not, in and
of themselves, imply a better strongly convex guarantee, and if
guaranteeing faster rates required a direct analysis. Perhaps
surprisingly, we show that this is not the case.

We present a generic “reduction” from strongly convex optimization to
convex optimization which indicates that any algorithm for convex
optimization will, with very minor modification, achieve a faster
convergence when the objective has strongly convex structure. In fact,
our reduction implies faster convergence for a broader class of
objectives that increase sufficiently quickly away from their minima
(see \prettyref def:growth-condition below).

Our reduction shows, for example, that any algorithm that converges as
@xmath for convex objectives will also convergence as @xmath for
strongly convex objectives. In other words, there is nothing
particularly special about gradient descent that enables it to converge
linearly in the strongly convex setting. In this way, our reduction
directly connects convex guarantees to better strongly convex
guarantees, without requiring any special analysis of the algorithm.
This conversion appears to be optimal in the sense that algorithms with
optimal convergence rates in the convex setting yield optimal strongly
convex guarantees.

The reduction in the opposite direction—from convex optimization to
strongly convex optimization—is a standard and well-known part of the
optimization toolkit. This, combined with our result, identifies a
certain equivalence between convex and strongly convex optimization that
was not previously recognized. In many cases, it implies that algorithms
with a certain rate of convergence for convex objectives exist if and
only if algorithms with a corresponding, faster rate exist for strongly
convex objectives, and the relationship between these rates is given by
the pair of reductions.

##### Setting:

We consider optimization problems of the form

  -- -------- -- ------
     @xmath      (48)
  -- -------- -- ------

in two cases, one where @xmath is convex, and the other where @xmath
satisfies the following condition:

###### Definition 1.

For @xmath , we say that @xmath satisfies the @xmath -growth condition
(hereafter abbreviated @xmath -GC) if there exists a mapping @xmath such
that @xmath for all @xmath , and for all @xmath

  -- -------- --
     @xmath   
  -- -------- --

For the most part, we will be interested in Euclidean case where @xmath
. In this case, it is easy to see that the @xmath -GC is implied by
@xmath -strong convexity with respect to @xmath . However, the growth
condition can be substantially weaker, in particular, it does not
require that @xmath have a unique minimizer. Furthermore, the results
are general and extend to any function @xmath , including those that do
not correspond to a norm such as Bregman divergences.

We consider optimization algorithms, @xmath , equipped with a
convergence guarantee for convex objectives: that is, for a given class
of objectives @xmath —e.g. the set of all convex and smooth
functions—the algorithm guaratees that it will find an @xmath
-suboptimal solution @xmath in a certain amount of time. We are
deliberately vague about the precise meaning of “time” here; often, it
correspond to the number of iterations of the algorithm, but it could
also count the number of times the algorithm accesses a certain oracle,
or even the wall-clock time of an implementation of the algorithm.

The amount of time that an algorithm needs to optimize a convex
objective must depend in some way on the distance from the algorithm’s
“initialization” to the minimizer of the function. This can be measured
in different ways, but in the convex case we focus on the standard
setting where the algorithm is provided with a potentially random point
@xmath such that @xmath , and the convergence guarantee scales with
@xmath . When discussing convergence rates for @xmath -GC objectives, we
suppose the algorithm is provided with a point @xmath for which @xmath ,
and the rate will depend on @xmath .

For a given optimization algorithm, we therefore define two types of
convergence rates, @xmath and @xmath corresponding to the convex and
@xmath -GC settings, respectively:

###### Definition 2.

Let @xmath be a class of convex objectives, then @xmath is the time
needed by the algorithm @xmath to find a point @xmath with @xmath for
any objective @xmath , when provided with a point @xmath with @xmath .
Similarly, @xmath is the time needed by the algorithm @xmath to find a
point @xmath with @xmath for any @xmath that also satisfies the @xmath
-GC, when provided with a point @xmath such that @xmath .

#### 3.3.1 The Reduction from Convex to Strongly Convex Optimization

Before we present our result, it is helpful to present the well-known
existing reduction from convex optimization to strongly convex
optimization. Throughout this section, we focus specifically on @xmath
-GC objectives where @xmath is the squared Euclidean distance.

The simplest form of the reduction from convex to strongly convex
optimization begins with an algorithm for optimizing @xmath -GC
objectives and uses it to optimize convex objectives by applying it to
the @xmath -GC surrogate @xmath . Since for @xmath

  -- -------- -- ------
     @xmath      (49)
  -- -------- -- ------

when we choose @xmath , minimizing @xmath to accuracy @xmath also
implies minimizing @xmath to accuracy @xmath . This simple idea allows
us to apply the algorithm for @xmath -GC objectives to convex functions.
In some cases, this reduction can be suboptimal, and it is necessary to
use the more sophisticated approach of Allen-Zhu and Hazan ( 2016 ) ,
which involves solving a sequence of regularized problems with an
exponentially decreasing regularization parameter. For the purpose of
discussion, we will briefly present their reduction:

Given: @xmath s.t. @xmath

for @xmath do

Set @xmath

Set @xmath to be the output of @xmath on @xmath initialized with @xmath
after @xmath

end for

Return @xmath

Algorithm 2 @xmath

###### Theorem 2.

[c.f. Theorem 3.1 (Allen-Zhu and Hazan, 2016 ) ] Let @xmath satisfy that
for all @xmath , @xmath implies @xmath . For any algorithm @xmath and
@xmath , then @xmath defined as in \prettyref alg:elad-reduction
guarantees

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Let @xmath and for each @xmath let @xmath and @xmath . First, we note
that by the definition of @xmath , if @xmath for every @xmath , then

  -- -------- -- ------
     @xmath      (50)
  -- -------- -- ------

We now prove that @xmath for every @xmath . For any @xmath ,

  -- -------- -- ------
     @xmath      (51)
  -- -------- -- ------

Therefore, for each @xmath , @xmath . We now bound

  -- -------- -------- -- ------
     @xmath   @xmath      
              @xmath      (52)
              @xmath      (53)
              @xmath      (54)
  -- -------- -------- -- ------

For the first inequality, we used the optimality of @xmath and for the
third inequality we used ( 51 ).

As the base case, by the definition of @xmath , @xmath . Therefore, by
the definition of @xmath , we have @xmath . Furthermore, for any @xmath
, if @xmath then by ( 54 )

  -- -------- -- ------
     @xmath      (55)
  -- -------- -- ------

This proves the claim by induction. ∎

In this way, the algorithm for @xmath -GC objectives can be readily
applied to merely convex functions. Our reduction, which we present in
the next section is conceptually similar to @xmath but it is even
simpler.

#### 3.3.2 The Reduction from Strongly Convex to Convex Optimization

Given: @xmath s.t. @xmath

for @xmath do

Set @xmath to be the output of @xmath initialized with @xmath after
running for @xmath

end for

Return @xmath

Algorithm 3 @xmath

We start with an objective @xmath that satisfies the @xmath -growth
condition and an algorithm @xmath equipped with a guarantee for merely
convex objectives @xmath . Our reduction, \prettyref alg:my-reduction
simply applies @xmath to the objective a logarithmic number of times,
and we show that each application reduces the suboptimality by a factor
of @xmath for some @xmath . The key insight is just that decreasing the
suboptimality for an objective satisfying the growth condition also
implies reducing the distance to the minimizer. So,

  -- -------- -- ------
     @xmath      (56)
  -- -------- -- ------

Therefore, with each application of the algorithm, the distance to a
minimizer is smaller, meaning that the time needed for the next call to
@xmath is correspondingly smaller. The following theorem uses this idea
to generically upper bound @xmath in terms of @xmath :

###### Theorem 3.

For any algorithm @xmath and @xmath , @xmath as in \prettyref
alg:my-reduction guarantees

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

By the definition of @xmath , if @xmath at each iteration, then @xmath
for each @xmath , and @xmath . We now prove by induction that the
condition @xmath always holds.

As the base case, the @xmath -GC implies that

  -- -------- -- ------
     @xmath      (57)
  -- -------- -- ------

Now, suppose that for all @xmath , @xmath . Then, by the definition of
@xmath , we have @xmath so by the @xmath -GC

  -- -------- -- ------
     @xmath      (58)
  -- -------- -- ------

This completes the proof. ∎

This idea has been applied before in specific cases (see, e.g. Ghadimi
and Lan, 2013a ) , but not with this level of generality or simplicity.
To understand the utility of the theorem, it is helpful to consider some
examples.

##### Example: Gradient Descent for Smooth Objectives

Let @xmath be the set of all @xmath -smooth (w.r.t. the Euclidean norm
@xmath ), convex objectives, and let @xmath . It is well known that the
gradient descent algorithm, which we denote @xmath , requires

  -- -------- -- ------
     @xmath      (59)
  -- -------- -- ------

gradients to find an @xmath -suboptimal point, where @xmath is a
universal constant. Applying the reduction, \prettyref thm:my-reduction
implies that

  -- -------- -------- -- ------
     @xmath   @xmath      (60)
              @xmath      (61)
              @xmath      (62)
  -- -------- -------- -- ------

Therefore, our reduction recovers (up to constant factors) the @xmath
convergence rate of gradient descent for @xmath -strongly convex (or,
more broadly, @xmath -GC) objectives. We emphasize that this guarantee (
62 ) has nothing to do with gradient descent specifically—for any
algorithm @xmath with

  -- -------- -- ------
     @xmath      (63)
  -- -------- -- ------

the modified algorithm @xmath will enjoy the same linear convergence as
gradient descent, ( 62 ).

##### Example: Accelerated SGD for Smooth Objectives

For @xmath , the class of convex and @xmath -smooth (w.r.t. the
Euclidean norm @xmath ) objectives, Lan ( 2012 ) proposed an algorithm,
AC-SA which, for @xmath , requires

  -- -------- -- ------
     @xmath      (64)
  -- -------- -- ------

stochastic gradients with variance bounded by @xmath to find an @xmath
-suboptimal point, which is optimal. Our reduction says that this
algorithm needs

  -- -------- -------- -- ------
     @xmath               (65)
              @xmath      (66)
              @xmath      (67)
              @xmath      (68)
              @xmath      (69)
  -- -------- -------- -- ------

stochastic gradients with variance bounded by @xmath for a @xmath
-strongly convex objective, where @xmath is a universal constant. This
is, up to constant factors, the optimal rate for strongly convex
objectives.

In follow-up work to Lan ( 2012 ) , Ghadimi and Lan ( 2013a ) describe a
“multi-stage” variant of AC-SA which they show achieves the same rate (
69 ). This algorithm closely resembles @xmath with some small
differences, and their analysis matches at a high level the proof of
\prettyref thm:my-reduction. However, their analysis is considerably
more complicated, and we feel that there is significant value in our
generalized approach which, by reasoning at a higher level of
abstraction, results in a much simpler proof.

#### 3.3.3 A New Minibatch Accelerated SGD Algorithm

In the context of machine learning, stochastic optimization algorithms
are often employed in an “overparametrized” regime, where there exist
settings of the parameters (typically many of them) that achieve zero,
or near zero, loss on the population. An important observation made by
Cotter et al. ( 2011 ) , is that when the loss function is itself smooth
and non-negative, this introduces a useful bound on the variance of the
gradient of the instantaneous losses, which decreases as the parameters
approach a minimizer. Specifically, for @xmath with @xmath being @xmath
-smooth and non-negative for all @xmath , we have

  -- -------- -- ------
     @xmath      (70)
  -- -------- -- ------

Therefore, when @xmath and @xmath is small, then the stochastic gradient
variance is also small.

At the same time, given increases in the size of datasets and the
availability of parallel computing resources, many training procedures
utilize minibatch stochastic gradients which can easily be calculated in
parallel. Using minibatch stochastic gradients of size @xmath reduces
the variance of the updates by a factor of @xmath , which naturally
gives faster convergence for any given number of updates, @xmath .
However, given runtime and sample complexity costs to computing these
minibatch gradients, using larger minibatches typically necessitates
making fewer updates, and it is important to understand to what extent
trading off @xmath and @xmath affects the performance of an algorithm.

Let @xmath denote the set of objectives, @xmath for which @xmath and
@xmath for some @xmath -smooth, non-negative, and convex @xmath ; and
let @xmath . In their work, Cotter et al. ( 2011 ) propose an
accelerated SGD variant, AG, which uses minibatch gradients of the form
@xmath for i.i.d. @xmath to guarantee

  -- -------- -- ------
     @xmath      (71)
  -- -------- -- ------

As is discussed by Cotter et al. ( 2011 ) , this bound suggests that
this algorithm has two or three regimes of convergence depending on the
batchsize, @xmath , and the relative scale of @xmath , @xmath , @xmath ,
and @xmath (ignoring the logarithmic factor in the third term):

The first regime corresponds to convergence after @xmath iterations,
which happens when

  -- -------- -- ------
     @xmath      (72)
  -- -------- -- ------

When @xmath is this small, the time needed to reach accuracy @xmath
improves linearly with an increase the batchsize, so it would generally
be advantageous to take @xmath at least as large as the upper bound ( 72
) to exploit this. Convergence in this regime can be extremely fast when
@xmath , as is common in many machine learning applications.

The second regime requires @xmath iterations when

  -- -------- -- ------
     @xmath      (73)
  -- -------- -- ------

This intermediate regime shows that there are diminishing returns to
increasing the batchsize beyond a certain point. Once @xmath is
moderately large, increasing @xmath results in a sublinear reduction in
the number of iterations needed to reach accuracy @xmath versus a linear
reduction in the first case.

Finally, the third regime has convergence governed by the term @xmath ,
which occurs once the batchsize is sufficiently large:

  -- -------- -- ------
     @xmath      (74)
  -- -------- -- ------

Once the batchsize has passed this critical threshold, there is nothing
to be gained by increasing it further. Indeed, the rate @xmath is
optimal even for first-order methods with access to exact gradients of
the objective (Nemirovsky and Yudin, 1983 ) , so this represents a
setting where the batchsize is large enough that the noise in the
stochastic gradients becomes negligible.

This algorithm achieves fast convergence in the convex setting, but it
is not clear how well it would perform when the objective has more
favorable properties such as strong convexity. Indeed, Cotter et al. ’s
guarantee for the algorithm required many pages of analysis, and it is
definitely not trivial to directly extend their proof to the strongly
convex case. Luckily, to understand how much improvement is possible, we
can simply apply our reduction. By \prettyref thm:my-reduction, for
@xmath which also satisfies the @xmath -growth condition,

  -- -------- -------- -- ------
     @xmath   @xmath      
                          (75)
                          (76)
                          (77)
                          (78)
              @xmath      (79)
  -- -------- -------- -- ------

As in the other examples, under the @xmath -GC, convergence can be
substantially faster and can depend only logarithmically on the accuracy
parameter @xmath . We also note that while the @xmath -GC is implied by
@xmath -strong convexity with respect to the L2 norm, the growth
condition can apply more broadly. This is of particular importance in
the context of training overparametrized machine learning models, for
which there are generally many minimizers of the objective so strong
convexity will not hold. Despite the fact that the objective may be
constant along some directions around its minimizers, as long as it
grows sufficiently quickly along the other directions the @xmath -GC can
still hold. For example, for least squares regression in @xmath with
@xmath training examples, the sample covariance matrix is
rank-deficient, and thus the training loss cannot be strongly convex.
Nevertheless, the training loss will satisfy the @xmath -GC with @xmath
equal to the smallest non-zero eigenvalue of the sample covariance
matrix, and the modified algorithm can therefore minimize the training
loss very efficiently.

Examining the fast rate ( 79 ) and ignoring the @xmath term, we see that
as in the convex setting there are three regimes of convergence
depending on the batchsize:

In the small batchsize regime, the algorithm requires @xmath iterations
for

  -- -------- -- ------
     @xmath      (80)
  -- -------- -- ------

As in the convex case, for this regime, the number of iterations needed
to reach accuracy @xmath decreases linearly with an increase in the
batchsize, and the rate of convergence depends very favorably on minimal
value of the objective when @xmath .

In the intermediate batchsize regime, the algorithm requires @xmath
iterations when

  -- -------- -- ------
     @xmath      (81)
  -- -------- -- ------

Again, this shows that there are diminishing returns to increasing the
batchsize beyond a certain point, and the iteration complexity decreases
only sublinearly in this case. Nevertheless, this still represents rapid
convergence to an approximate minimizer of @xmath , depending only
logarithmically on the accuracy parameter.

Finally, in the large batchsize regime, the algorithm requires @xmath
iterations for

  -- -------- -- ------
     @xmath      (82)
  -- -------- -- ------

After this point, there is nothing to be gained by further increasing
the batchsize and this represents the optimal rate of convergence for
first-order algorithms, even when they have access to exact gradients of
the objective (Nemirovsky and Yudin, 1983 ) ⁴ ⁴ 4 The lower bound
applies to @xmath -strongly convex (w.r.t. L2) objectives, a subclass of
@xmath -GC objectives which are therefore covered by the same lower
bound. .

##### Comparison with Liu and Belkin (2018):

In recent work, Liu and Belkin ( 2018 ) proposed another stochastic
first-order algorithm, MaSS, for optimizing functions in @xmath in the
special case @xmath , which obtains qualitatively similar guarantees.
There are, however, several key differences between our modification of
Cotter et al. ’s algorithm and MaSS.

In the special case of least squares problems, where @xmath , and where
@xmath is the training loss, MaSS requires (Theorem 2 Liu and Belkin,
2018 )

  -- -------- -- ------
     @xmath      (83)
  -- -------- -- ------

iterations, where @xmath is the smallest non-zero eigenvalue of the
Hessian @xmath . This matches the guarantee ( 79 ) for the modification
of Cotter et al. ’s algorithm, @xmath . However, this result is limited
to least squares problems, to the case that @xmath , and only guarantees
finding an @xmath -suboptimal point with respect to the training loss,
whereas the guarantee ( 79 ) applies to the population loss.

In addition to the least squares result, Liu and Belkin also analyze the
MaSS algorithm for a more general class of @xmath -strongly convex
objectives that satisfy a certain smoothness property (see Theorem 3 Liu
and Belkin, 2018 ) . However, it requires @xmath iterations to reach an
@xmath -suboptimal point, i.e. an “unaccelerated” rate. Furthermore, it
does not show any benefit from minibatching like ( 79 ) does. Finally,
this result is limited to the case of (1) strongly convex objectives and
(2) @xmath , which implies very strong constraints on the objective. In
particular, a common source of strong convexity is training a linear
model with an L2 regularization penalty @xmath , in which case these
conditions are only satisfied in the trivial case where @xmath minimizes
the objective. In contrast, our guarantee ( 79 ) applies even when
@xmath , and also to the broader class of @xmath -GC objectives.

#### 3.3.4 Optimality of the Reductions

The combination of \prettyref thm:elad-reduction and \prettyref
thm:my-reduction suggests that there is a certain equivalence between
convex and strongly convex optimization. Our reduction shows that the
existence of an algorithm that converges like @xmath for convex
objectives implies the existence of an algorithm that converges like
@xmath for @xmath -strongly convex ones. Conversely, Allen-Zhu and Hazan
’s reduction in the other direction says that the existence of an
algorithm that converges as @xmath for @xmath -strongly convex
objectives implies the existence of an algorithm with rate @xmath in the
convex setting. Therefore, these are actually equivalent statements: a
@xmath algorithm for convex objectives exists if and only if an @xmath
algorithm exists for @xmath -strongly convex objectives.

Indeed, we argue that these reductions are optimal in the following
sense: if we take an algorithm @xmath for optimizing convex objectives
and use \prettyref alg:my-reduction to convert it into an algorithm for
strongly convex optimization, @xmath , and then we use \prettyref
alg:elad-reduction to convert it back into an algorithm for convex
optimization, @xmath then the guarantee generally degrades by just a
constant factor.

By \prettyref thm:my-reduction and \prettyref thm:elad-reduction, we
have

  -- -------- -------- -- ------
     @xmath   @xmath      
              @xmath      (84)
              @xmath      (85)
  -- -------- -------- -- ------

Therefore, in the typical case where @xmath for some @xmath , depends
polynomially on @xmath and @xmath , then

  -- -------- -------- -- ------
     @xmath   @xmath      
              @xmath      (86)
              @xmath      (87)
              @xmath      (88)
  -- -------- -------- -- ------

So, applying both reductions maintains the same @xmath and @xmath
dependence, and loses only a “small constant” factor assuming that
@xmath and @xmath are relatively small, as they typically are. This is
evidence that we should not expect there to be a significantly better
general purpose reduction in either direction.

#### 3.3.5 Proving Lower Bounds by Reduction

Here, we consider the implications of these reductions to lower bounds.
Specifically, \prettyref thm:elad-reduction shows that algorithms for
strongly convex optimization with a guarantee @xmath implies the
existence of an algorithm for convex optimization with a corresponding
guarantee @xmath . Taking the contrapositive of this statement, we
conclude that a lower bound that shows @xmath implies a certain lower
bound on @xmath . However, this lower bound does not apply for all
values of @xmath , @xmath , and @xmath . In particular, we have that for
any @xmath

  -- -------- -- ------
     @xmath      (89)
  -- -------- -- ------

Therefore, the lower bound only really applies to strongly convex
functions for parameters @xmath , @xmath , and @xmath with a very
particular relationship with each other. For instance, with @xmath ,
@xmath , etc. Nevertheless, this does tell us that an upper bound with a
particular functional form cannot exist. We now show how to apply this
idea to derive a lower bound in the strongly convex setting for the
graph oracle model:

###### Theorem 4.

For any graph @xmath , let @xmath be an exact gradient oracle for each
@xmath . Then for any @xmath and any dimension @xmath , there exists
@xmath and @xmath and a function @xmath in dimension @xmath such that
the output of any algorithm in @xmath will have suboptimality at least

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

We prove this by contradiction. Let @xmath be the subset of @xmath in
dimension @xmath , and suppose there were an algorithm @xmath with
guarantee

  -- -------- -- ------
     @xmath      (90)
  -- -------- -- ------

where here @xmath refers to the minimum graph depth @xmath needed to
guarantee that @xmath . We note that if such an algorithm existed, then
its guarantee would imply a guarantee in terms of @xmath that would
contradict the claim of the theorem, which can be seen by solving this
for @xmath .

Then, \prettyref thm:elad-reduction implies

  -- -------- -------- -- ------
     @xmath   @xmath      
              @xmath      (91)
              @xmath      (92)
              @xmath      (93)
              @xmath      (94)
  -- -------- -------- -- ------

However, for sufficiently small @xmath and @xmath , this would
contradict the lower bound \prettyref thm:generic-graph-lower-bound. We
conclude that there are constants @xmath and @xmath for which no
algorithm can provide the guarantee ( 90 ). ∎

While this lower bound is limited in that it only applies for some
values @xmath and @xmath , it is nevertheless suggestive of a more broad
lower bound. For essentially all of the strongly convex optimization
settings we are aware of, optimal algorithms provide an upper bound that
is a simple, continuous function of @xmath , @xmath , and @xmath , and
these upper bounds apply for any value of these parameters. Therefore,
although it is certainly possible that this lower bound does not apply
for some values of @xmath and @xmath , that appears quite unlikely.

#### 3.3.6 The Practicality of the Reduction

Our reduction \prettyref alg:my-reduction takes an algorithm for convex
optimization and lightly modifies it to create an algorithm for strongly
convex (or GC) objectives, but it is important to be clear about what
this modified algorithm actually looks like. Consider the case of
gradient descent with a constant stepsize @xmath . In this case, since
each of @xmath ’s updates have exactly the same form, and since the
@xmath call to @xmath in \prettyref alg:my-reduction picks up at the
final iterate of the @xmath run of @xmath , @xmath is actually exactly
the same as gradient descent with the same constant stepsize @xmath
—it’s just run for a different number of iterations.

In contrast, consider an algorithm like SGD. Importantly, in order to
achieve the rate @xmath using SGD, it is necessary to use a stepsize
that depends on @xmath and @xmath , and it is also necessary to return
an average of the SGD iterates. For this reason, the modified algorithm
@xmath , when viewed as a single unit, will appear somewhat strange. It
will still resemble SGD, but the stepsize schedule is separated into
several distinct phases and at the end of each phase, the next iterate
will be changed to an average of some of the previous iterates. This is,
of course, a perfectly valid first-order optimization algorithm, but it
is fairly strange, and from a practical perspective, it is quite
possible that @xmath would not perform as well as @xmath itself despite
its enhanced guarantee. Nevertheless, our analysis of @xmath proves that
some algorithm with that better guarantee exists, and suggests that a
“more natural” probably exists too. On the other hand, if a “more
natural” algorithm did not exist, that would also be extremely
interesting! For example, the only optimal algorithm for stochastic
first-order optimization in the strongly convex setting that we are
aware of is either @xmath or Ghadimi and Lan ( 2013a ) ’s essentially
identical method. Because of the reductions, both of these methods
resemble SGD with momentum, but the momentum and stepsize parameters are
somewhat crazy and non-monotonic.

## 4 Analysis of Local SGD

Local SGD is a very popular and natural algorithm for the intermittent
communication setting with a stochastic first-order oracle. The idea is
simple: during a round of communication, each of the @xmath machines
independently takes @xmath SGD steps and at the end of the round, their
@xmath iterates are averaged together to form the starting point for the
next round.

Local SGD has been widely used in a variety of applications, including
in the homogeneous, heterogeneous, and federated settings, and for
convex and non-convex objectives. However, while Local SGD is often
quite successful in these applications, its theoretical properties were
not well understood. Specifically, despite dozens of papers trying to
prove convergence guarantees for Local SGD, none show substantial
improvement over simple baseline algorithms that we will discuss
shortly. Despite its empirical success, this raises the question of
whether Local SGD truly has poorer theoretical performance, or if we
have just not figured out how to analyze it properly.

We will partially resolve these theoretical questions about Local SGD
for smooth and convex/strongly convex objectives in the homogeneous and
heterogeneous settings. In \prettyref subsec:local-sgd-and-baselines, we
will define Local SGD along with several baseline algorithms which will
serve as a useful point of reference in evaluating guarantees for Local
SGD. In \prettyref subsec:local-sgd-homogeneous, we analyze Local SGD in
the homogeneous setting, where each machine relies on data drawn from
the same distribution. For homogeneous objectives, we provide new upper
and lower bounds on the accuracy of Local SGD and show how these are the
first to significantly improve over the baselines in a certain regime.
In \prettyref subsec:local-sgd-heterogeneous, we move on to the
heterogeneous setting, where each machine has access to data drawn from
different distributions. Here, we also provide new and better upper and
lower bounds on the accuracy of Local SGD and we compare with the
baseline algorithms.

### 4.1 Local, Minibatch, and Single-Machine SGD

We will now define the Local SGD algorithm along with two natural
baselines and one unnatural baseline. Each of these algorithms belongs
to the class @xmath as defined in \prettyref
sec:formulating-the-complexity. We recall that the intermittent
communication graph captures a setting in which @xmath parallel machines
collaborate to optimize the objective over the course of @xmath rounds
of communication, and in each round of communication, each machine is
allowed @xmath queries to a stochastic gradient oracle.

In the homogeneous setting, the stochastic gradient oracle associated
with each vertex is the same— @xmath for all @xmath —meaning that each
oracle access on each machine provides an unbiased estimate of @xmath
with variance bounded by @xmath . Conversely, in the heterogeneous
setting, the stochastic gradient oracles are different depending on
which machine is making a query. Specifically, the objective has the
form @xmath , and each of the vertices corresponding to the @xmath
machine is associated with an oracle @xmath that provides an unbiased
estimate of @xmath with variance less than @xmath . Throughout this
section, we will use @xmath to denote the stochastic gradient oracle for
the @xmath query on the @xmath machine during the @xmath round of
communication.

We ask what is the best guarantee that can be provided for Local SGD in
the intermittent communication setting, for a particular and fixed
@xmath , @xmath , and @xmath . In order to make a fair comparison, it is
therefore important that the baselines algorithms also belong to the
same class of algorithms @xmath . After all, it would not be fair to
compare Local SGD against an algorithm which is allowed to use @xmath
parallel workers, or one that is only allowed to communicate @xmath
times.

Local SGD: We first define the Local SGD updates in \prettyref
alg:local-sgd. At any given time, Local SGD maintains @xmath
iterates—one for each machine—and we use @xmath to denote the @xmath
iterate during the @xmath round of communication on machine @xmath .
Between communications, each machine simply updates its iterate
according to a standard stochastic gradient step. At the end of each
round of communication, the local iterates are averaged and form the
starting point for the next round.

For each @xmath : @xmath

for @xmath do

for @xmath do

For each @xmath in parallel: @xmath

end for

Communicate and for each @xmath : @xmath

end for

Return: @xmath

Algorithm 4 Local SGD

Minibatch SGD: The first baseline algorithm is Minibatch SGD, meaning
@xmath steps of SGD using minibatches of size @xmath . While “Minibatch
SGD” could refer to many different algorithms (i.e. many different
combinations of a number of steps and a minibatch size), we focus on one
specific version that belongs to the family @xmath . In particular,
\prettyref alg:minibatch-sgd can be implemented in the intermittent
communication setting by having each of the @xmath machines compute
@xmath stochastic gradients, all at the same point. At the end of the
round of communication, all of these stochastic gradients can be
combined into a single large minibatch of size @xmath and a single SGD
step can be taken, meaning that @xmath SGD steps can be taken in total.

Initialize: @xmath

for @xmath do

for @xmath do

For each @xmath in parallel: compute @xmath

end for

@xmath

@xmath

end for

Return: @xmath

Algorithm 5 Minibatch SGD

This is a very simple algorithm, and it seems intuitive that Minibatch
SGD would be generally be worse than Local SGD. After all, Minibatch SGD
only takes @xmath SGD steps in total, @xmath times fewer than Local SGD
takes. Furthermore, Minibatch SGD only computes stochastic gradients in
@xmath locations versus Local SGD which computes stochastic gradients in
@xmath different locations so, in some sense, it seems that Local SGD
should obtain more diverse and informative gradients of the objective.

Single-Machine SGD: The second baseline algorithm is “Single-Machine
SGD,” which is just @xmath steps of SGD using minibatches of size @xmath
. \prettyref alg:single-machine-sgd is easily implemented in the
intermittent communication setting by simply running SGD on one machine,
and completely ignoring the remaining @xmath machines.

Initialize: @xmath

for @xmath do

for @xmath do

On machine @xmath : @xmath

On machines @xmath : do nothing.

end for

@xmath

end for

Return: @xmath

Algorithm 6 Single-Machine SGD

This algorithm seems a bit silly since it does not leverage the
available parallelism at all, and for this reason, it seems clearly
worse than Local SGD, which does in fact use all @xmath of the machines.
Indeed, this intuition is correct and we will see later that Local SGD
is never worse than Single-Machine SGD. Nevertheless, many previous
Local SGD analyses appear worse than the Single-Machine SGD guarantee so
this is an important baseline to keep in mind.

Furthermore, Minibatch and Single-Machine SGD are useful points of
reference because they constitute opposite poles of a spectrum. On the
one hand, Minibatch SGD fully exploits the available parallelism but it
hardly takes advantage of the availability of local computation on each
of the machines between communications. On the other hand,
Single-Machine SGD in some sense fully exploits the local computation
but does not take advantage of the parallelism. The idea of Local SGD is
to find a happy medium between these extremes—exploiting both the local
computation and the parallelism at the same time in order to achieve
better performance.

Thumb-Twiddling SGD: The final baseline algorithm is something of a
strawman, but one that is nevertheless useful as a point of comparison,
and it corresponds simply to @xmath steps of SGD using minibatches of
size @xmath . \prettyref alg:thumb-twiddling-sgd belongs to the family
of intermittent communication algorithms since it can be implemented by
each machine computing just a single stochastic gradient during each
round of communication and sitting and “twiddling its thumbs” rather
than computing any of the other @xmath stochastic gradients.

Initialize: @xmath

for @xmath do

for @xmath do

if @xmath then

For each @xmath in parallel: compute @xmath

else

Twiddle thumbs.

end if

end for

@xmath

@xmath

end for

Return: @xmath

Algorithm 7 Thumb-Twiddling SGD

If Single-Machine SGD is a bit silly, this algorithm is completely
ludicrous and it is obviously strictly worse than Minibatch SGD, which
is the same algorithm except with bigger minibatches (and therefore
lower-variance stochastic gradient steps). So, there is no reason to use
Thumb-Twiddling SGD in the intermittent communication setting but it
still serves as a useful—and surprisingly strong—baseline to compare
against Local SGD.

##### Accelerated Methods:

It is worth pointing out that none of these methods are actually minimax
optimal, and obtaining optimal algorithms requires acceleration. There
are accelerated variants of Minibatch and Single-Machine SGD (Lan, 2012
; Ghadimi and Lan, 2013a ) that achieve faster rates of convergence than
the “unaccelerated” methods we disscussed here, and Yuan and Ma ( 2020 )
recently proposed an accelerated variant of Local SGD and analyzed it in
the homogeneous setting. However, our goal here is to understand the
theoretical properties of the popular and natural algorithm Local SGD,
and to compare this to other reasonable baseline algorithms. As we will
show in \prettyref sec:intermittent-communication-setting, although
accelerated Local SGD may be better than unaccelerated Local SGD, it
cannot beat an analogous set of accelerated baselines, which are
optimal.

#### 4.1.1 An Alternative Viewpoint: Reducing Communication

Here, and in much of this thesis, we consider the intermittent graph to
be fixed—that is, we think of @xmath , @xmath , and @xmath as parameters
that are beyond our control, and we ask how well we can do with whatever
we are given. However, this viewpoint is dual to another, which asks how
we should set these parameters @xmath , @xmath , and @xmath in order to
achieve a given level of accuracy. We are particularly interested in how
small we can set @xmath —i.e. how little we can communicate—without
“paying for it” by suffering worse error, since communication is
typically expensive.

In this view, we can consider as a baseline the class of algorithms that
process @xmath stochastic gradients on each machine, but can communicate
at every step, i.e. @xmath times (this corresponds to the layer graph
\prettyref fig:layer-graph). In this setting, we can implement @xmath
steps of SGD using minibatches of @xmath samples, and we know that this
algorithm is essentially optimal up to acceleration (see \prettyref
subsec:layer-graph-minimax-complexity).

It then makes sense to ask whether we can achieve the same performance
as this baseline while communicating less frequently? Communicating only
@xmath times instead of @xmath while maintaining the total number of
gradients computed per machine brings us right back to the intermittent
communication setting we are considering, and all the methods discussed
above (Minibatch SGD, Local SGD, etc) reduce the communication. Checking
whether these algorithms’ guarantees for @xmath rounds matches the same
accuracy as the dense communication baseline is a starting point, but
the question is how small can we push @xmath (while keeping @xmath
fixed) before accuracy degrades. Better error guarantees (in terms of
@xmath and @xmath ) mean we can use a smaller @xmath with less
degradation, and the smallest @xmath with no asymptotic degradation can
be directly calculated from the error guarantee (see, e.g., discussion
in Cotter et al., 2011 ) .

### 4.2 Local SGD in the Homogeneous Setting

Let us now consider the theoretical performance of Local SGD in the
homogeneous setting, that is, when each machine in each round of
communication computes stochastic gradients from @xmath which give
unbiased estimates of @xmath . Over the years, dozens of papers have
attempted to prove convergence guarantees for Local SGD but all of them
fail to show improvement over the simple baselines of Minibatch and
Single-Machine SGD. In fact, they often fail to show an advantage over
Thumb-Twiddling SGD!

This really raises the question of whether the unfavorable comparison
between Local SGD’s guarantees versus the baselines’ is an artifact of
our analysis techniques, or if Local SGD really is worse. After
surveying some of the existing work, we will now show that the answer
depends on the details of the setting. First, we show that Local SGD
performs very well in the special case of quadratic objectives, and
dominates the baselines. In the case of general convex and smooth
objectives, we show a new upper bound which is always at least as good
as Single-Machine SGD and sometimes improves over Minibatch SGD.
Finally, we show a lower bound which indicates that Local SGD is indeed
sometimes strictly worse than Minibatch SGD, and even than
Thumb-Twiddling SGD.

#### 4.2.1 Prior Analysis of Local SGD in the Homogeneous Setting

There is a long history of analyses of Local SGD, going back almost 30
years to Mangasarian and Solodov ( 1994 ) , who first proposed the
algorithm and showed asymptotic convergence to stationary points of
arbitrary continuously differentiable objectives. Since then, numerous
papers have analyzed Local SGD in a number of settings (Khaled et al.,
2020 ; Stich, 2018 ; Stich and Karimireddy, 2019 ; Haddadpour et al.,
2019a ; Wang and Joshi, 2018 ; Dieuleveut and Patel, 2019 ; Zhou and
Cong, 2018a ; Yu et al., 2019 ; Wang et al., 2017 ; Haddadpour et al.,
2019b ; Zinkevich et al., 2010 ; Zhang et al., 2012 ; Li et al., 2014 ;
Rosenblatt and Nadler, 2016 ; Godichon-Baggioni and Saadane, 2017 ; Jain
et al., 2017 ) . The best existing guarantees for Local SGD specialized
to the convex, smooth, and homogeneous setting, due to Stich ( 2018 )
and Stich and Karimireddy ( 2019 ) , are included in \prettyref
tab:existing-local-sgd-analysis.

It is worth emphasizing that the very best of the existing analysis for
Local SGD are actually always strictly worse than Minibatch SGD. The
guarantees of Stich ( 2018 ) and Stich and Karimireddy ( 2019 ) do not
necessarily even improve over Thumb-Twiddling SGD due to the @xmath and
@xmath dependence in their first terms, versus the @xmath rate for
Minibatch SGD and Thumb-Twiddling SGD. Finally, while they can sometimes
improve over Single-Machine SGD, they can only do so when Single-Machine
SGD is already worse than Minibatch SGD.

While we focus on the function classes @xmath and @xmath , there are
many other analyses for Local SGD under different sets of assumptions,
or with a more detailed dependence on the noise in the stochastic
gradients. For instance, Stich and Karimireddy ( 2019 ); Haddadpour
et al. ( 2019a ) analyze local SGD assuming two notions of
not-quite-convexity; and Wang and Joshi ( 2018 ); Dieuleveut and Patel (
2019 ) derive guarantees under both multiplicative and additive bounds
on the noise. Dieuleveut and Patel ( 2019 ) analyze local SGD with the
additional assumption of a bounded third derivative, but even with this
assumption do not improve over Minibatch SGD. Numerous works study Local
SGD in the non-convex setting (Zhou and Cong, 2018a ; Yu et al., 2019 ;
Wang et al., 2017 ; Stich and Karimireddy, 2019 ; Haddadpour et al.,
2019b ) , and although their bounds would apply in our convex setting,
they are understandably worse than Minibatch SGD due to the much weaker
assumptions. There is also a large body of work studying the special
case @xmath , i.e. where the iterates are averaged just one time at the
end (Zinkevich et al., 2010 ; Zhang et al., 2012 ; Li et al., 2014 ;
Rosenblatt and Nadler, 2016 ; Godichon-Baggioni and Saadane, 2017 ; Jain
et al., 2017 ) . However, these analyses do not easily extend to
multiple rounds, and the @xmath constraint may harm performance (Shamir
et al., 2014 ) . Finally, there are many papers analyzing Local SGD in
the heterogeneous setting, which we will discuss in \prettyref
subsec:local-sgd-heterogeneous.

#### 4.2.2 Local SGD Analysis for Quadratic Homogeneous Objectives

To better understand the theoretical performance of Local SGD, we begin
by studying the special case of quadratic objectives, e.g. least squares
problems. Here, it turns out that Local SGD is always at least as good
as Minibatch SGD, and can be much better. More generally, we show that a
Local SGD analogue for a large family of serial first-order optimization
algorithms enjoys an error guarantee which depends only on the product
@xmath and not on @xmath or @xmath individually. Therefore, such
algorithms essentially perfectly parallelize, and a single round of
communication allows for equally good performance as many would.

We consider the following family of “linear update algorithms”:

###### Definition 3.

We say that a sequential, stochastic first-order optimization algorithm
is a linear update algorithm if, for fixed linear functions @xmath , the
algorithm generates its @xmath iterate according to

  -- -------- --
     @xmath   
  -- -------- --

This family captures many standard first-order methods including SGD,
which corresponds to the linear mappings @xmath and @xmath . Another
notable algorithm in this class is AC-SA (Lan, 2012 ) (see \prettyref
alg:ac-sa), an accelerated variant of SGD which also has linear updates.
Some important non-examples, however, are adaptive gradient methods like
AdaGrad (McMahan and Streeter, 2010 ; Duchi et al., 2011 ) —these
methods use stepsizes that depend on previous gradients which leads to
non-linear updates.

For a linear update algorithm @xmath , we will use Local- @xmath to
denote the Local SGD analogue with @xmath replacing SGD. That is, during
each round of communication, each machine independently executes @xmath
iterations of @xmath and then the @xmath resulting iterates are
averaged. For quadratic objectives, we show that this approach inherits
the guarantee of @xmath with the additional benefit of variance
reduction:

###### Theorem 5.

Let @xmath be a linear update algorithm which, when executed for @xmath
iterations on any quadratic @xmath , guarantees @xmath and for any
quadratic @xmath , guarantees @xmath . Then, Local- @xmath ’s averaged
final iterate @xmath will satisfy @xmath and @xmath in the convex and
strongly convex cases, respectively.

We prove this in \prettyref app:local-sgd-homogeneous-quadratics by
showing that the average iterate @xmath is updated according to @xmath
using minibatch stochatic gradients of size @xmath —even in the middle
of rounds of communication when @xmath is not explicitly computed. The
key property that we exploit is that the gradient of a quadratic
function is linear. Therefore, we write the updates on the average
iterate as

  -- -------- -- ------
     @xmath      (95)
  -- -------- -- ------

Then, by the linearity of @xmath and @xmath , we have

  -- -------- -- ------
     @xmath      (96)
  -- -------- -- ------

and we show that the variance is reduced to @xmath . Therefore, @xmath
’s guarantee applies but with the added benefit of smaller variance.

To rephrase \prettyref thm:linear-update-alg-quadratics, on quadratic
objectives, Local- @xmath is in some sense equivalent to @xmath
iterations of @xmath using minibatch stochastic gradients of size @xmath
. Furthermore, this guarantee depends only on the product @xmath , and
not on @xmath or @xmath individually. Thus, averaging the @xmath iterate
of @xmath independent executions of @xmath , sometimes called “one-shot
averaging,” enjoys the same error upper bound as @xmath iterations
@xmath using size- @xmath minibatches.

Nevertheless, it is important to highlight the boundaries of \prettyref
thm:linear-update-alg-quadratics. Firstly, @xmath ’s error guarantee
@xmath must not rely on any particular structure of the stochastic
gradients themselves, as this structure might not hold for the implicit
updates of Local- @xmath . Furthermore, even if some structure of the
stochastic gradients is maintained for Local- @xmath , the particular
iterates generated by Local- @xmath will generally vary with @xmath and
@xmath (even when @xmath is held constant). Thus, Theorem 5 does not
guarantee that Local- @xmath with two different values of @xmath and
@xmath would perform the same on any particular instance. We have merely
proven matching upper bounds on their worst-case performance.

We apply \prettyref thm:linear-update-alg-quadratics to yield error
upper bounds for Local SGD and Local AC-SA:

###### Corollary 1.

For any quadratic @xmath and quadratic @xmath , Local SGD guarantees

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

and Local AC-SA ⁵ ⁵ 5 The AC-SA algorithm must be slightly modified in
order to achieve linear convergence in the strongly convex setting. This
modification is provided by Ghadimi and Lan ( 2013a ) and is also
discussed in \prettyref subsec:the-reduction. guarantees

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

This follows immediately from the guarantees of SGD and AC-SA in the
sequential setting and \prettyref thm:linear-update-alg-quadratics. In
the convex case, comparing \prettyref
cor:acc-local-sgd-optimal-quadratics’s guarantee for Local SGD with the
bound for Minibatch SGD in \prettyref tab:existing-local-sgd-analysis,
we see that Local SGD’s bound is strictly better, due to the first term
scaling as @xmath versus @xmath . At the same time, the Local SGD
guarantee is also strictly better than Single-Machine SGD due to the
@xmath scaling of the statistical term versus just @xmath . We note that
Minibatch and Single-Machine SGD can also be accelerated (Cotter et al.,
2011 ; Lan, 2012 ) , which improves their optimization terms, however
these guarantees are still outmatched by Local AC-SA.

There is also reason to think that Local AC-SA might be minimax optimal
for quadratic objectives. The statistical terms @xmath and @xmath are
tight by \prettyref lem:statistical-term-lower-bound, which was proven
using quadratic hard instances. In addition, the optimization terms
match the lower bounds \prettyref thm:generic-graph-lower-bound and
\prettyref thm:generic-graph-oracle-strongly-convex. These lower bounds
were proven using non-quadratic hard instances, so it is possible that
the minimax error for quadratic objectives might be lower. However, the
generic graph lower bounds could have been proven using quadratic hard
instances if we restricted our attention to the class of
span-restricted/zero-respection/deterministic optimization algorithms.
Also, the work of Simchowitz ( 2018 ) shows that the Local AC-SA
guarantee is minimax optimal (up to a log factor) for quadratic
objectives and the class of all randomized algorithms, but only in the
special case @xmath . It is certainly plausible, and perhaps even
likely, that these results could be extended to show that Local AC-SA is
optimal for quadratics for all @xmath , although such lower bounds do
not yet exist.

#### 4.2.3 Local SGD Analysis for General Homogeneous Objectives

As we just saw, Local SGD is extremely effective for the special case of
quadratic objectives. In this section, we turn to general convex and
strongly convex objectives. Our main result is the following theorem:

###### Theorem 6.

For any @xmath and @xmath , Local SGD guarantees

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

This Theorem is proven in \prettyref
app:local-sgd-homogeneous-upper-bound. We use a similar approach as
Stich ( 2018 ) , who analyzes the behavior of the averaged iterate
@xmath , even when it is not explicitly computed. Stich shows, in
particular, that the averaged iterate almost evolves according to size-
@xmath -Minibatch SGD updates, up to a term proportional to the
dispersion of the individual machines’ iterates @xmath . Stich bounds
this with @xmath , but this bound is too pessimistic—in particular, it
holds even if the gradients are replaced by arbitrary vectors of norm
@xmath . In \prettyref
lem:local-sgd-homogeneous-upper-bound-distance-bound-between-local-iterates,
we improve this bound to @xmath which allows for our improved guarantee.
⁶ ⁶ 6 In recent work, Stich and Karimireddy ( 2019 ) present a new
analysis of Local SGD which, in the general convex case has the form
@xmath . As stated, this is strictly worse than Minibatch SGD. However,
we suspect that this bound should hold for any @xmath because,
intuitively, having more machines should not hurt you. If this is true,
then optimizing their bound over @xmath yields a similar result as
\prettyref thm:local-sgd-homogeneous-upper-bound. Our approach resembles
the concurrent work of Khaled et al. ( 2020 ) , however our analysis is
more refined and we optimize more carefully over the stepsize to get a
better rate.

##### Comparison with the Baselines

We now compare the upper bound from \prettyref
thm:local-sgd-homogeneous-upper-bound with the guarantees for Minibatch
and Single-Machine SGD. The second term in the @xmath s for Local SGD
match the convergence rate for Single-Machine SGD, so we conclude that
Local SGD is never worse. On the other hand, its relationship with
Minibatch SGD is more complicated.

For clarity, and in order to highlight the role of @xmath , @xmath , and
@xmath in the convergence rate, we will compare rates for general convex
objectives when @xmath , and we will also ignore numerical constants. In
this setting, the worst-case error of Minibatch SGD is (Nemirovsky and
Yudin, 1983 ) :

  -- -------- -- ------
     @xmath      (97)
  -- -------- -- ------

Our guarantee for Local SGD from \prettyref
thm:local-sgd-homogeneous-upper-bound reduces to:

  -- -------- -- ------
     @xmath      (98)
  -- -------- -- ------

These guarantees have matching statistical terms of @xmath , which
cannot be improved by any first-order algorithm (Nemirovsky and Yudin,
1983 ) . Therefore, in the regime where the statistical term dominates
both rates, i.e. @xmath and @xmath , both algorithms will have similar
worst-case performance. When we leave this noise-dominated regime, we
see that Local SGD’s guarantee @xmath is better than Minibatch SGD’s
@xmath when @xmath and is worse when @xmath . This makes sense
intuitively: Minibatch SGD takes advantage of very precise stochastic
gradient estimates, but pays for it by taking fewer gradient steps;
conversely, each Local SGD update is noisier, but Local SGD is able to
make @xmath times more updates.

We therefore establish that for general convex objectives in the large-
@xmath and large- @xmath regime, Local SGD will strictly outperform
Minibatch SGD. However, in the large- @xmath and small- @xmath regime,
our Local SGD guarantee does not show improvement over Minibatch SGD. We
are only comparing upper bounds, so it is not clear that Local SGD is in
fact worse, yet it raises the question of whether this is the best we
can hope for from Local SGD. Is Local SGD truly better than Minibatch
SGD in some regimes but worse in others? Or, should we believe the
intuitive argument suggesting that Local SGD is always at least as good
as Minibatch SGD in the same way that it is always better than
Single-Machine SGD?

#### 4.2.4 A Lower Bound for Local SGD for Homogeneous Objectives

We will now show that in a certain regime, Local SGD really is inferior
(in the worst-case) to Minibatch SGD, and even to Thumb-Twiddling SGD.
We show this by constructing a simple, smooth piecewise-quadratic
objective in three dimensions on which Local SGD performs poorly. We
define this hard instance as @xmath where

  -- -------- -- ------
     @xmath      (99)
  -- -------- -- ------

and where @xmath and @xmath .

###### Theorem 7.

For any @xmath , @xmath , and dimension at least 3, there exists @xmath
such that the final averaged iterate of Local SGD initialized at @xmath
with any fixed stepsize will have suboptimality at least

  -- -------- --
     @xmath   
  -- -------- --

Similarly, for any @xmath , there exists @xmath such that the final
averaged iterate of Local SGD initialized at @xmath with any fixed
stepsize will have suboptimality at least

  -- -------- --
     @xmath   
  -- -------- --

We defer a detailed proof of \prettyref
thm:local-sgd-homogeneous-lower-bound to \prettyref
app:local-sgd-homogeneous-lower-bound. Intuitively, it relies on the
fact that for non-quadratic functions, the SGD updates are no longer
linear as in \prettyref subsec:local-sgd-quadratic-upper-bound, and the
Local SGD dynamics introduce an additional bias term which does not
improve with @xmath , and scales poorly with @xmath . This phenomenon
does not seem to be unique to our construction, and should be expected
to exist for any “sufficiently non-quadratic” function.

The proof shows specifically that the suboptimality is large unless
@xmath , but Local SGD introduces a bias which causes @xmath to drift in
the negative direction by an amount proportional to the stepsize. On the
other hand, optimizing the first term of the objective requires the
stepsize to be relatively large. Combining these yields the first term
of the lower bound. The second term is classical and holds even for
first-order algorithms that compute @xmath stochastic gradients
sequentially (Nemirovsky and Yudin, 1983 ) .

In order to compare this lower bound with \prettyref
thm:local-sgd-homogeneous-upper-bound and with Minibatch SGD, we again
consider the general convex setting with @xmath . Then, the upper bound
reduces to @xmath . Comparing this to \prettyref
thm:local-sgd-homogeneous-lower-bound, we see that our upper bound is
tight up to a factor of @xmath in the optimization term. Furthermore,
comparing the lower bound to the worst-case error of Minibatch SGD ( 97
), we see that Local SGD is indeed worse than Minibatch SGD in the worst
case when @xmath . Our lower bound is unable to identify the exact
cross-over point, but it is some @xmath . For @xmath , Minibatch SGD is
better than Local SGD in the worst case, for @xmath , Local SGD is
better. Since the optimization terms of Minibatch SGD and
Thumb-Twiddling SGD are identical, this further indicates that Local SGD
can even be outperformed by Thumb-Twiddling SGD in the small @xmath and
large @xmath regime.

For \prettyref thm:local-sgd-homogeneous-lower-bound, we constructed a
hard instance that is convenient for analysis but is maybe somewhat
“artificial.” We also conducted an experiment showing that the same
qualitative picture holds also for a more “natural” logistic regression
task. In \prettyref fig:local-sgd-LR-experiments, we plot the
suboptimality of Local, Minibatch, and Thumb-Twiddling SGD iterates with
optimally tuned stepsizes and, as is predicted by \prettyref
thm:local-sgd-homogeneous-lower-bound, we see Local SGD performing worse
than Minibatch in the small @xmath regime, but improving relative to the
other algorithms as @xmath increases to @xmath and then @xmath , when
Local SGD is far superior to Minibatch. For each fixed @xmath ,
increasing @xmath causes Thumb-Twiddling SGD to improve relative to
Minibatch SGD, but does not have a significant effect on Local SGD,
which is consistent with the method introducing a bias that depends on
@xmath but not on @xmath . This highlights that the “problematic regime”
for Local SGD is one where there are few iterations per round.

### 4.3 Local SGD in the Heterogeneous Setting

We now move on to optimization in the intermittent communication setting
with heterogeneous data, where the objective has the form @xmath and
each of the @xmath machines has access to a stochastic gradient oracle
for its corresponding objective @xmath . Here, we focus on the problem
of finding a single consensus solution for @xmath , which achieves a low
value on all of the local objectives @xmath on average (Bertsekas and
Tsitsiklis, 1989 ; Boyd et al., 2011 ) . Because each machine only has
access to a single component of the objective, the heterogeneous is
substantially harder than the previously considered homogeneous case.

Like in the homogeneous setting, a number of recent papers have analyzed
the convergence properties of Local SGD in the heterogeneous data
setting (Wang and Joshi, 2018 ; Karimireddy et al., 2019 ; Khaled
et al., 2020 ; Koloskova et al., 2020 ) . Also as in the homogeneous
setting, we will show that none of these Local SGD guarantees show
improvement over the baseline of Minibatch SGD, even without
acceleration, and in many regimes their guarantees are much worse. This
again raises the question of whether this a weakness of the analysis or
the Local SGD method itself. Can the bounds be improved to show that
Local SGD is actually better than Minibatch SGD in certain regimes, or
is Local SGD always worse?

Recall that in the homogeneous setting, prior analysis had not been able
to show that Local SGD improves over Minibatch SGD yet the combination
of \prettyref thm:local-sgd-homogeneous-upper-bound and \prettyref
thm:local-sgd-homogeneous-lower-bound showed that Local SGD is better
than Minibatch SGD in some regimes and worse in others. Specifically,
when communication was relatively infrequent Local SGD improves over
Minibatch SGD.

How does this situation play out in the more challenging, and perhaps
more interesting, heterogeneous setting? In several discussions
following the publication of Woodworth et al. ( 2020b ) , people had
suggested that the more difficult heterogeneous setting is where we
should expect Local SGD to really shine, and that Minibatch SGD is too
naïve. So, how does heterogeneity affect Local SGD, Minibatch SGD, and
the comparison between them? Is Local SGD better than Minibatch SGD when
communication is rare as in the homogeneous case? Does the difficulty
introduced by heterogeneity perhaps necessitate the more sophisticated
Local SGD approach, as some have suggested?

In recent work, Karimireddy et al. ( 2019 ) showed heterogeneity can be
problematic for Local SGD, proving a lower bound that indicates some
degradation as degree of heterogeneity increases. As we will discuss,
this lower bound implies that Local SGD is strictly worse than Minibatch
SGD when the level of heterogeneity is very large, but it is not clear
whether or not Local SGD can improve over Minibatch SGD for slightly or
moderately heterogeneous objectives.

##### Boundedly Heterogeneous Objectives

In addition to the usual assumptions of smoothness and convexity/strong
convexity, we will introduce a new parameter that captures the extent to
which the local objectives disagree about the minimizer. In particular,
we will say that a heterogeneous objective @xmath is @xmath
-heterogeneous if, for some @xmath

  -- -------- -- -------
     @xmath      (100)
  -- -------- -- -------

Since @xmath , this can be thought of as measuring the variance of the
gradient at @xmath when selecting a random component. If the objective
is @xmath -heterogeneous, then all of the objectives share a minimizer,
but we note that it does not imply that all the local objectives are the
same, since ( 100 ) is only a statement about the gradients at @xmath .
This assumption of @xmath -heterogeneity is common in the Local SGD
literature (Karimireddy et al., 2019 ; Khaled et al., 2020 ; Koloskova
et al., 2020 ) and it is used to prove most of the convergence
guarantees for Local SGD that we are aware of. Nevertheless, in the
consensus optimization literature, it is common to analyze distributed
algorithms with no such bound on the heterogeneity, and it is certainly
still possible to minimize @xmath without this assumption (Boyd et al.,
2011 ; Nedic and Ozdaglar, 2009 ; Nedic et al., 2010 ; Ram et al., 2010
) .

We define the function class @xmath as the class of @xmath
-heterogeneous objectives of the form @xmath where @xmath is @xmath
-smooth for all @xmath , and @xmath . Similarly, we define @xmath as the
class of @xmath -heterogeneous objectives of the form @xmath where
@xmath is @xmath -smooth for all @xmath , and @xmath .

#### 4.3.1 Minibatch SGD in the Heterogeneous Setting

To begin, we analyze the worst-case error of our baseline Minibatch SGD
and Minibatch AC-SA (Lan, 2012 ; Ghadimi and Lan, 2013a ) (see
\prettyref alg:ac-sa) in the heterogeneous setting. A simple but
important observation is that the minibatch gradients used by these
algorithms are unbiased estimates of @xmath despite the heterogeneity of
the objective:

  -- -------- -- -------
     @xmath      (101)
  -- -------- -- -------

Since the minibatch stochastic gradients are unbiased estimates of
@xmath , we can simply appeal to the standard analysis for (accelerated)
SGD. To do so, we calculate the variance of these estimates:

  -- -------- -- -- -------
     @xmath         (102)
  -- -------- -- -- -------

So, the variance is always reduced by @xmath and, importantly, it is not
affected by the level of heterogeneity @xmath . Plugging this
calculation into the analysis of SGD and Accelerated SGD (Nemirovsky and
Yudin, 1983 ; Lan, 2012 ) yields:

###### Theorem 8.

For any @xmath and any @xmath , the output of Minibatch SGD guarantees

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

and Minibatch AC-SA guarantees

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

The theorem follows immediately from the observation about the
unbiaseness and variance of the stochastic gradients above, and
previously established convergence rates for SGD (Nemirovsky and Yudin,
1983 ; Stich, 2019 ) and AC-SA (Lan, 2012 ; Ghadimi and Lan, 2013a ) .
The most important feature of these guarantees is that they are
completely independent of @xmath because of the use of minibatch
stochastic gradients. In the following sections, we will see how Local
SGD compares.

#### 4.3.2 Prior Analysis of Local SGD in the Heterogeneous Setting

Recently, Khaled et al. ( 2020 ) and Koloskova et al. ( 2020 ) analyzed
Local SGD in the heterogeneous and convex setting, and Koloskova et al.
did also in the strongly convex setting. Their guarantees are summarized
in \prettyref tab:prior-local-sgd-analysis-heterogeneous-convex and
\prettyref tab:prior-local-sgd-analysis-heterogeneous-strongly-convex.
Also included are guarantees for SCAFFOLD ⁷ ⁷ 7 Karimireddy et al.
analyze SCAFFOLD in the Federated Learning setting where only a random
subset of @xmath of the machines are available in each round. Here, we
present the analysis as it applies to our setting where @xmath . , a
related method for heterogeneous distributed optimization (Karimireddy
et al., 2019 ) .

Upon inspection, Koloskova et al. ’s guarantee is slightly better than
Khaled et al. , but even this guarantee is (up to logarithmic terms) the
sum of the Minibatch SGD bound plus additional terms, and is thus always
worse in every regime. The question is whether this just reflects a
weakness of their analysis, or a true weakness of the Local SGD
algorithm?

Indeed, \prettyref thm:local-sgd-homogeneous-upper-bound shows that a
tighter upper bound for Local SGD is possible in the homogeneous case,
and this rate is better than Koloskova et al. ( 2020 ) ’s when @xmath ⁸
⁸ 8 Although, we remind the reader that @xmath does not imply that the
problem is homogeneous. . Can we generalize \prettyref
thm:local-sgd-homogeneous-upper-bound to the heterogeneous case and show
improvement over Minibatch SGD?

Optimistically, we might hope that Koloskova et al. ’s @xmath term, in
particular, could be improved. Unfortunately, it is already known that
some dependence on @xmath is necessary, as Karimireddy et al. ( 2019 )
shows a lower bound of @xmath in the strongly convex case, which
suggests a lower bound of @xmath in the convex case. But perhaps the
Koloskova et al. analysis can be improved to match this bound?

If the @xmath dependence suggested by Karimireddy et al. ’s lower bound
were possible, it would be lower-order than @xmath for @xmath , and we
would see no slow down until the level of heterogeneity is fairly large.
In particular, since the components @xmath are @xmath -smooth, adding
the assumption that the local objectives @xmath each have a minimizer of
norm at most @xmath would be enough to bound @xmath . In this case, the
dependence on the level of heterogeneity would be fairly mild, and could
be ignored under reasonable circumstances. On the other hand, if the
@xmath term from Koloskova et al. cannot be improved, then we see a
slowdown as soon as @xmath , which corresponds to a quite low level of
heterogeneity! So, what is the correct rate?

#### 4.3.3 Upper and Lower Bounds for Local SGD in the Heterogeneous
Setting

We now show that the poor dependence on @xmath from Koloskova et al. ’s
analysis cannot be improved. Consequently, for sufficiently
heterogeneous data, Local SGD is strictly worse than Minibatch SGD,
regardless of the frequency of communication, unless the level of
heterogeneity is very small.

###### Theorem 9.

For @xmath and any dimension at least 4, there exists an objective
@xmath and for any @xmath , there exists @xmath such that the final
averaged iterate of Local SGD initialized at zero and using any fixed
stepsize @xmath will have suboptimality at least

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

This is proven in \prettyref app:local-sgd-heterogeneous-lower-bound
using a similar approach as \prettyref
thm:local-sgd-homogeneous-lower-bound, and it is conceptually similar to
the lower bounds for heterogeneous objectives of Karimireddy et al. (
2019 ) . Koloskova et al. ( 2020 ) also prove a lower bound, but
specifically for 1-strongly convex objectives, which obscures the
important role of the strong convexity parameter.

In the convex case, this lower bound closely resembles the upper bound
of Koloskova et al. ( 2020 ) . Focusing on the case @xmath to emphasize
the role of @xmath , the only gaps are between (i) a term @xmath vs
@xmath —a gap which also exists in the homogeneous case (see \prettyref
thm:local-sgd-homogeneous-upper-bound and \prettyref
thm:local-sgd-homogeneous-lower-bound)—and (ii) another term @xmath vs
@xmath .

For @xmath , the lower bound shows that Local SGD has error at least
@xmath and thus performs strictly worse than Minibatch, regardless of
@xmath . This is quite surprising—Local SGD is often suggested as an
improvement over Minibatch SGD for the heterogeneous setting, yet we see
that even a small degree of heterogeneity can make it much worse.
Furthermore, increasing the duration of each round, @xmath , is often
thought of as more beneficial for Local SGD than Minibatch SGD, but the
lower bound indicates it does little to help Local SGD in the
heterogeneous setting.

Similarly, in the strongly convex case, the lower bound from \prettyref
thm:local-sgd-heterogeneous-lower-bound nearly matches the upper bound
of Koloskova et al. . Focusing on the case @xmath in order to emphasize
the role of @xmath , the only differences are between (i) a term @xmath
versus @xmath —a gap which also exists in the homogeneous case (see
\prettyref thm:local-sgd-homogeneous-upper-bound and \prettyref
thm:local-sgd-homogeneous-lower-bound)—and (ii) between @xmath and
@xmath . The latter gap is more substantial than the convex case, but
nevertheless indicates that the @xmath rate cannot be improved until the
number of rounds of communication is at least the condition number or
@xmath is very small.

Thus, \prettyref thm:local-sgd-heterogeneous-lower-bound indicates that
it is not possible to radically improve over the Koloskova et al.
analysis, and thus over Minibatch SGD for even moderate heterogeneity,
without stronger assumptions. In order to obtain an improvement over
Minibatch SGD in a heterogeneous setting, at least with very low
heterogeneity, we introduce a stronger version of the heterogeneity
measure @xmath which bounds the difference between the local objectives’
gradients everywhere, not just at @xmath :

  -- -------- -- -------
     @xmath      (103)
  -- -------- -- -------

This quantity precisely captures homogeneity since @xmath if and only if
@xmath (up to an irrelevant additive constant). In terms of @xmath , we
are able to analyze Local SGD and see a smooth transition from the
heterogeneous ( @xmath large) to homogeneous ( @xmath ) setting.

###### Theorem 10.

For any @xmath and any @xmath with the additional property that @xmath ,
Local SGD guarantees

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

We prove this in \prettyref app:local-sgd-heterogeneous-upper-bound.
This is the first analysis of Local SGD, or any other method for
heterogeneous distributed optimization, which shows any improvement over
Minibatch SGD in any heterogeneous regime. When @xmath , \prettyref
thm:local-sgd-heterogeneous-uppper-bound reduces to the homogeneous
analysis of Local SGD given by \prettyref
thm:local-sgd-homogeneous-upper-bound, which already showed that in that
case, we see improvement when @xmath . \prettyref
thm:local-sgd-heterogeneous-uppper-bound degrades smoothly when @xmath
increases, and shows improvement for Local SGD over Minibatch SGD also
when @xmath in the convex case, i.e. with low, yet positive,
heterogeneity. It is yet unclear whether this rate of convergence can be
ensured in terms of @xmath rather than @xmath .

##### Experimental evidence

Finally, while \prettyref thm:local-sgd-heterogeneous-lower-bound proves
that Local SGD is worse than Minibatch SGD unless @xmath is very small
in the worst case , one might hope that for “normal” heterogeneous
problems, Local SGD might perform better than its worst case error
suggests. However, a simple binary logistic regression experiment on
MNIST indicates that this behavior likely extends significantly beyond
the worst case. The results, depicted in \prettyref
fig:heterogeneous-local-sgd-experiments, show that Local SGD performs
worse than Minibatch SGD unless both @xmath is very small and @xmath is
large. Finally, we also observe that Minibatch SGD’s performance is
essentially unaffected by @xmath empirically as predicted by theory.

### 4.4 Conclusion

In the homogeneous setting we showed that: (1) Local SGD attains very
low error for quadratic objectives, and strictly dominates the baselines
of Single-Machine SGD and Minibatch SGD; (2) for general objectives,
Local SGD is always at least as good as Single-Machine SGD, and it is
strictly better than Minibatch SGD when communication is relatively
infrequent ( @xmath ); and (3) Local SGD is strictly worse than
Minibatch SGD when communication is relatively frequent ( @xmath ), and
it can even be worse than Thumb-Twiddling SGD.

In the heterogeneous setting, Local SGD compares relatively less
favorably against Minibatch SGD. For @xmath -heterogeneous functions, no
existing analysis shows any improvement over Minibatch SGD in any
regime, and our lower bound shows that no such improvement is possible
as soon as @xmath . On the other hand, when the heterogeneity is bounded
everywhere by @xmath , then we show that Local SGD can improve over
Minibatch SGD, at least when @xmath .

To better understand the relationship between Minibatch SGD and Local
SGD, and for thinking about how to improve over them, it is useful to
consider a unified algorithm that interpolates between them. This
involves taking SGD steps locally with one stepsize, and then when the
machines communicate, they take a step in the resulting direction with a
second, different stepsize. Such a dual-stepsize approach was already
presented and analyzed as FedAvg by Karimireddy et al. ( 2019 ) . We
will refer to these two stepsizes as “inner” and “outer” stepsizes,
respectively, and consider

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (104)
              @xmath   @xmath   @xmath   
  -- -------- -------- -------- -------- -------

Choosing @xmath , this is equivalent to Minibatch SGD with stepsize
@xmath , and choosing @xmath recovers Local SGD. Therefore, when the
stepsizes are chosen optimally, this algorithm is always at least as
good as both Minibatch and Local SGD. Therefore, using the inner-outer
algorithm ( 104 ) with optimal stepsizes guarantees for @xmath

  -- -------- -- -------
     @xmath      (105)
  -- -------- -- -------

where the first option in the @xmath is obtained by choosing @xmath and
the second by choosing @xmath . We can also get a similar minimum of the
Minibatch SGD and Local SGD rates in the strongly convex case and also
in the homogeneous setting.

## 5 The Intermittent Communication Setting

In the intermittent communication setting, @xmath parallel workers are
used to optimize a single objective over the course of @xmath rounds.
During each round, each machine sequentially and locally computes @xmath
independent unbiased stochastic gradients of the global objective, and
then all the machines communicate with each other. This captures, for
example, the natural setting where multiple parallel workers are
available, and computation on each worker is much faster than
communication between workers. It includes applications ranging from
optimization using multiple cores or GPUs, to using a cluster of
servers, to Federated Learning where workers are edge devices.

As a concrete example, Goyal et al. ( 2017 ) were able to train a large
ResNet-50 neural network on Imagenet in under an hour using SGD with
very large minibatches, which was implemented in exactly the
intermittent communication setting. Specifically, they used @xmath GPUs
in parallel, computed @xmath stochastic gradients per communication
between the GPUs, and performed roughly @xmath updates/communications.
This is, of course, just one example, but it is indicative of many
practical uses of distributed optimization for training large machine
learning models.

The theoretical properties of optimization algorithms in the
intermittent communication setting have been widely studied for over a
decade, with many approaches proposed and analyzed (Zinkevich et al.,
2010 ; Cotter et al., 2011 ; Dekel et al., 2012 ; Zhang et al., 2013a ,
c ; Shamir and Srebro, 2014 ) , and obtaining new methods and improved
analysis is still a very active area of research (Wang et al., 2017 ;
Stich, 2018 ; Wang and Joshi, 2018 ; Khaled et al., 2019 ; Haddadpour
et al., 2019a ; Woodworth et al., 2020b ) . However, despite these
efforts, we do not yet know which methods are optimal, what the minimax
complexity is, and what methodological or analytical improvements might
allow us to make further progress.

A key issue in the existing literature is that known lower bounds for
the intermittent communication setting depend only on the product @xmath
(i.e. the total number of gradients computed on each machine over the
course of optimization), and not on the number of rounds, @xmath , and
the number of gradients per round, @xmath , separately. Thus, existing
results cannot rule out the possibility that the optimal rate for fixed
@xmath can be achieved using only a single round of communication (
@xmath ), since they do not distinguish between methods that communicate
very frequently ( @xmath , @xmath ) and methods that communicate just
once ( @xmath , @xmath ). The possibility that the optimal rate is
achievable with @xmath was suggested by Zhang et al. ( 2013c ) , and
indeed we showed in \prettyref subsec:local-sgd-homogeneous that in the
special case of quadratic objectives, Local SGD and Local AC-SA perform
just as well with a single communication as they do with many rounds of
communication. While it seems unlikely that a single round of
communication suffices in the general case, none of our existing lower
bounds are able to answer this extremely basic question.

In \prettyref subsec:homogeneous-intermittent-minimax, we resolve (up to
a logarithmic factor) the minimax complexity of smooth, convex
stochastic optimization in the homogeneous intermittent communication
setting and we show that, generally speaking, a single round of
communication does not suffice to achieve the min-max optimal rate. In
\prettyref subsec:homogeneous-convex-intermittent-minimax, we prove
lower bounds on the optimal rate of convergence with matching upper
bounds for convex and strongly convex objectives, and in \prettyref
subsec:homogeneous-non-convex-intermittent-minimax, we prove matching
upper and lower bounds on the optimal rate for finding approximate
stationary points of non-convex objectives.

Interestingly, in all of these cases we show that the combination of two
extremely simple and naïve methods are optimal. In the convex setting,
the methods are based on an accelerated SGD variant AC-SA (Lan, 2012 ) .
Specifically, we show that the better of the following methods is
optimal: Minibatch AC-SA which executes @xmath steps of AC-SA using
minibatch gradients of size @xmath , and Single-Machine AC-SA which
executes @xmath steps of AC-SA on just one of the machines, completely
ignoring the other @xmath . Similarly, in the non-convex setting, the
better of Minibatch SGD and Single-Machine SGD are optimal.

These methods might appear suboptimal: the Minibatch methods only
perform one update per round of communication, and the Single-Machine
methods only use one of the available workers! This perceived
inefficiency has prompted many attempts at developing improved methods
which take multiple steps on each machine locally in parallel including,
in particular, numerous analyses of Local SGD (Zinkevich et al., 2010 ;
Dekel et al., 2012 ; Stich, 2018 ; Haddadpour et al., 2019a ; Khaled
et al., 2019 ; Woodworth et al., 2020b , a ) , which we already
discussed in \prettyref sec:local-sgd. Nevertheless, we establish that
one or the other is optimal in every regime, so more sophisticated
methods cannot yield improved guarantees for arbitrary smooth
objectives. Our results therefore highlight an apparent dichotomy
between exploiting the available parallelism but not the local
computation (Minibatch) and exploiting the local computation but not the
parallelism (Single-Machine).

In addition to the homogeneous setting, in \prettyref
subsec:heterogeneous-convex-intermittent-minimax, we also study the
heterogeneous setting. Here, we also prove matching upper and lower
bounds for convex and strongly convex objectives which establishes that
Minibatch AC-SA is minimax optimal.

Our lower bounds apply quite broadly, including to the settings covered
by the bulk of the existing work on stochastic first-order optimization
in the intermittent communication setting. However, like many lower
bounds, we should not interpret them to mean that progress is
impossible, and that we are stuck with naïve algorithms like Minibatch
SGD. Instead, these results indicate that we need to modify our
assumptions in order to develop better methods. In \prettyref
sec:breaking-the-lower-bounds we explore several additional assumptions
that allow—or might plausibly allow—for circumventing the lower bounds
in various ways. These include when the third derivative of the
objective is bounded (as in recent work by Yuan and Ma ( 2020 ) ), when
the objective has a certain statistical learning-like structure, or when
the algorithm has access to a more powerful oracle.

### 5.1 The Homogeneous Setting

We begin with the homogeneous setting, where all of the algorithm’s
queries are to the same stochastic gradient oracle which gives an
unbiased estimate of @xmath . More precisely, each vertex of the
intermittent communication graph corresponds to an oracle @xmath , which
is an “independent-noise” oracle (see \prettyref subsec:the-oracle)
which just gives an unbiased estimate of @xmath with variance bounded by
@xmath that is independent of all other oracle queries. In \prettyref
sec:breaking-the-lower-bounds, we will discuss other types of
first-order oracles that have additional structure.

The proofs of our lower bounds generally follow the approach outlined in
\prettyref subsec:high-level-lower-bound-approach. As was discussed
there, part of the argument hinges on the norm of the algorithm’s
queries being bounded so that the algorithm cannot “cheat” and get a
large inner product with the unknown columns of @xmath by simply
guessing a random vector with huge norm. In the proof of \prettyref
thm:generic-graph-lower-bound, we were able to avoid this issue by
constructing a function for which querying the gradient oracle at a
point with norm larger than @xmath gives essentially no information.
However, for the constructions used in this section this is more
difficult, and we will instead rely on an explicit bound on the norm of
the algorithm’s queries. Specifically, we define @xmath to be the class
of optimization algorithms in the intermittent communication setting
with stochastic gradient oracles @xmath for which all queries are
bounded in norm by @xmath . The bound @xmath is arbitrary in the sense
that our lower bounds apply for any @xmath in a sufficiently large
dimension of at least @xmath . However, our lower bounds do not apply to
algorithms which query the oracle at unboundedly large points, or which
query the oracle at points with norm that depend on the dimension. The
restriction that the norm of the algorithm’s queries is bounded can also
be removed if the algorithm is deterministic or
span-restricted/zero-respecting.

#### 5.1.1 Convex Objectives

We begin with our lower bound in the convex, smooth, and homogeneous
intermittent communication setting:

###### Theorem 11.

For any @xmath , there exists a function @xmath in any dimension

  -- -------- --
     @xmath   
  -- -------- --

such that the output of any algorithm in @xmath will have suboptimality
at least

  -- -------- --
     @xmath   
  -- -------- --

Proof Sketch The first two terms of this lower bound follow directly
from \prettyref thm:generic-graph-lower-bound and \prettyref
lem:statistical-term-lower-bound; the @xmath term corresponds to the
error when optimizing a function using a deterministic gradient oracle,
and the @xmath term is a very well-known statistical limit (Nemirovsky
and Yudin, 1983 ) . The distinguishing feature of our lower bound is the
third term, which depends differently on @xmath than on @xmath . For
quadratics, Local AC-SA attains the rate given by just the first two
terms, and actually does depend only on the product @xmath , as shown in
\prettyref cor:acc-local-sgd-optimal-quadratics. Consequently, proving
our lower bound necessitates going beyond quadratics. In contrast, all
or at least most of the lower bounds for sequential smooth convex
optimization apply even for quadratic objectives.

We start by describing the proof of the theorem for zero-respecting
algorithms, and we will discuss how it is extended to arbitrary
algorithms at the end. The proof uses the following non-quadratic hard
instance:

  -- -------- -- -------
     @xmath      (106)
  -- -------- -- -------

where @xmath is defined as

  -- -------- -- -------
     @xmath      (107)
  -- -------- -- -------

and where @xmath , @xmath , and @xmath are hyperparameters that are
chosen depending on @xmath so that @xmath satisfies the necessary
conditions. This construction closely resembles the classic lower bound
for deterministic first-order optimization of Nesterov ( 2004 ) , which
essentially uses @xmath . To describe our stochastic gradient oracle, we
will use @xmath , which denotes the highest index of a non-zero
coordinate of @xmath . We also define @xmath to be equal to the
objective with the @xmath term removed:

  -- -------- -- -------
     @xmath      (108)
  -- -------- -- -------

The stochastic gradient oracle for @xmath is then given by

  -- -------- -- -------
     @xmath      (109)
  -- -------- -- -------

This stochastic gradient oracle resembles the one used by Arjevani
et al. ( 2019 ) to prove lower bounds for non-convex optimization, and
its key property is that @xmath . Therefore, for zero-respecting
algorithms, each oracle access only allows the algorithm to increase its
progress with probability @xmath . The rest of the proof revolves around
bounding the total progress of the algorithm and showing that if @xmath
, then @xmath has high suboptimality.

Since each machine makes @xmath sequential queries and only makes
progress with probability @xmath , the total progress scales like @xmath
. By taking @xmath smaller, we decrease the amount of progress made by
the algorithm, and therefore increase the lower bound. Indeed, when
@xmath , the algorithm only increases its progress by about @xmath per
round, which gives rise to the key @xmath term in the lower bound.
However, we are constrained in how small we can take @xmath since our
stochastic gradient oracle has variance

  -- -------- -- -------
     @xmath      (110)
  -- -------- -- -------

This is where our choice of @xmath comes in. Specifically, we chose the
function @xmath to be convex and smooth so that @xmath is, but it is
also Lipschitz:

  -- -------- -- -------
     @xmath      (111)
  -- -------- -- -------

Notably, this Lipschitz bound on @xmath , which implies a bound on
@xmath , is the key non-quadratic property that allows for our lower
bound. Since @xmath is bounded, we are able to able to choose @xmath
without violating the variance constraint on the stochastic gradient
oracle. Carefully balancing @xmath completes the argument.

To extend this argument to randomized algorithms that may not be
zero-respecting, we follow the approach described in \prettyref
subsec:high-level-lower-bound-approach by introducing a random rotation
@xmath and “flattening out” the @xmath functions around the origin. We
defer the remaining details to \prettyref
app:intermittent-communication-homogeneous-convex-lower-bound.

To complement the lower bound \prettyref
thm:homogeneous-convex-lower-bound and to establish the minimax error
for the convex, smooth, and homogeneous intermittent communication
setting, we prove a nearly matching upper bound. This upper bound is
attained by either Minibatch AC-SA or Single-Machine AC-SA. We recall
from \prettyref subsec:initially-introduce-ac-sa-convex-rate that AC-SA
(see \prettyref alg:ac-sa) is an accelerated variant of SGD (Lan, 2012 )
.

The Minibatch AC-SA algorithm corresponds to taking @xmath steps of
AC-SA using minibatch stochastic gradients of size @xmath . This can be
implemented in the intermittent communication setting by having all
@xmath machines calculate @xmath stochastic gradients at the same point
during each round of communication. When the machine do communicate,
they can combine all @xmath of these gradients into one large minibatch
and compute a single AC-SA update.

The Single-Machine AC-SA algorithm corresponds to simply taking @xmath
steps of AC-SA using minibatch stochastic gradients of size @xmath .
This can be implemented in the intermittent communication setting by
simply implementing the algorithm on a single machine, and ignoring the
remaining @xmath workers altogether.

While these approaches may seem simple, the following theorem shows that
the better of the two is optimal:

###### Theorem 12.

For any @xmath , either Minibatch AC-SA or Single-Machine AC-SA
guarantees that for any @xmath

  -- -------- --
     @xmath   
  -- -------- --

A simple proof is given in \prettyref
app:intermittent-communication-homogeneous-convex-upper-bound, and
requires simply plugging the number of updates and bound on the variance
of the minibatch stochastic gradients into the existing guarantee for
AC-SA.

#### 5.1.2 Strongly Convex Objectives

We also show nearly matching upper and lower bounds in the strongly
convex setting:

###### Theorem 13.

For any @xmath , and dimension

  -- -------- --
     @xmath   
  -- -------- --

there exist @xmath and @xmath , and an objective @xmath in dimension
@xmath such that the output of any algorithm in @xmath will have
suboptimality at least

  -- -- --
        
  -- -- --

This is proven in \prettyref
app:intermittent-communication-homogeneous-strongly-convex-lower-bound
using the reduction between convex and strongly-convex objectives
described in \prettyref subsec:lower-bounds-by-reduction, which explains
the weaker statement. Nevertheless, as in the convex case, this lower
bound is matched by the better of Minibatch and Single-Machine AC-SA ⁹ ⁹
9 The AC-SA algorithm requires a slight modification in order to achieve
the optimal rate for strongly convex objectives, see \prettyref
subsec:the-reduction. .

###### Theorem 14.

For any @xmath , either Minibatch AC-SA or Single-Machine AC-SA
guarantees that for any @xmath

  -- -- --
        
  -- -- --

This is proven in \prettyref
app:intermittent-communication-homogeneous-strongly-convex-upper-bound
by simply plugging the number of steps of AC-SA and the variance of the
minibatch stochastic gradients into the existing guarantees for AC-SA.

#### 5.1.3 Non-Convex Objectives

We now consider the non-convex setting. Without assuming convexity, it
is generally intractable to ensure convergence to a global minimizer of
the objective (Nemirovsky and Yudin, 1983 ) . For this reason, it is
common to analyze algorithms in terms of their ability to find
approximate stationary points of the objective (Vavasis, 1993 ; Nocedal
and Wright, 2006 ; Nesterov and Polyak, 2006 ; Ghadimi and Lan, 2013b ;
Carmon et al., 2017b ; Lei et al., 2017 ; Fang et al., 2018 ; Zhou
et al., 2018 ; Fang et al., 2019 ) , i.e. a point @xmath such that

  -- -------- -- -------
     @xmath      (112)
  -- -------- -- -------

It is important to understand optimal algorithms for non-convex since
most modern machine learning applications, like training neural
networks, involve solving non-convex optimization. Furthermore, given
the large scale of these non-convex problems, it is often critical to
leverage parallelism in order to speed training.

In the homogeneous intermittent communication setting, we can pose the
same questions of minimax optimality for non-convex optimization as we
did for the convex case, with this new success criterion of finding
approximate stationary points replacing approximate minimization. In
order to state our results, we define @xmath to be the class of all
@xmath -smooth, possibly non-convex objectives such that @xmath . The
following theorem proves a lower bound on the optimal rate of
convergence to an approximate stationary point for any intermittent
communication algorithm:

###### Theorem 15.

For any @xmath , there exists a function @xmath in a sufficiently large
dimension @xmath such that for any algorithm in @xmath

  -- -------- --
     @xmath   
  -- -------- --

The construction is based on one used by Carmon et al. ( 2017a ) to show
a lower bound of @xmath for sequential non-convex optimization using an
exact gradient oracle. Beyond extending the argument to the intermittent
communication setting, we also augment the construction with a
stochastic gradient oracle like was used for the proof of \prettyref
thm:homogeneous-convex-lower-bound. The stochastic gradient oracle
“zeros out” the next relevant component of the gradient with probability
@xmath , which reduces the amount of progress made by the algorithm by a
factor of @xmath . Setting @xmath as small as possible without violating
the gradient variance constraint completes the arguement. We defer
additional details of the proof to \prettyref
app:intermittent-communication-homogeneous-non-convex-lower-bound.

In this case, we can also identify a pair of algorithms whose combined
guarantee matches the lower bound. Indeed, the same pattern holds in the
non-convex case and, again, the better of Minibatch SGD and
Single-Machine SGD is optimal. While it is perhaps unsurprising that the
convex and strongly convex settings have the same punchline, it was less
clear that these observations would apply in the non-convex setting, yet
the only difference is that for non-convex objectives, there is no need
for acceleration, and regular SGD is able to attain the optimal rate.

###### Theorem 16.

For any @xmath , either Minibatch SGD or Single-Machine SGD guarantees
that for any objective @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

This is proven in \prettyref
app:intermittent-communication-homogeneous-non-convex-upper-bound by
appealing to existing analysis for SGD for smooth non-convex objectives
Ghadimi and Lan ( 2013b ) .

#### 5.1.4 Conclusions

In this section, we identified the minimax error and optimal algorithms
in the homogeneous intermittent communication setting with convex,
strongly convex, and non-convex objectives up to logarithmic factors. In
doing so, we have highlighted several interesting characteristics of the
intermittent communication setting:

##### A Tradeoff Between Parallelism and Local Computation

In light of \prettyref thm:homogeneous-convex-upper-bound and the third
term in the lower bound \prettyref thm:homogeneous-convex-lower-bound
(and the analogous terms in the other settings), we see that algorithms
are offered the following dilemma: they may either attain the optimal
statistical rate @xmath for the convex setting but suffer an
optimization rate @xmath that does not improve with @xmath , or they may
attain the optimal optimization rate of @xmath but suffer a statistical
term @xmath that does not improve with @xmath . In this way, there is a
very real dichotomy between exploiting the availability of parallel
computation (e.g. using Minibatch AC-SA) an exploiting the availability
of sequential local computation (e.g. using Single-Machine AC-SA).
Importantly, under the conditions we study, it is impossible to exploit
both simultaneously, and one of the extremes of this spectrum is always
optimal. This is quite surprising, and for a long time we thought that
it should be possible to design an algorithm which gets the best of both
worlds.

##### Mixed Statistical and Optimization Terms

The structure of the optimal error in the convex and strongly convex
settings have a notably different structure than in many other
stochastic optimization settings. Typically in convex optimization, the
minimax error for stochastic first-order optimization is the sum of two
terms, an “optimization term”—which is equal to the minimax error when
using an exact first-order oracle—and a “statistical term”—which is
equal is the optimal error of any method using that number of samples,
e.g. of the empirical risk minimizing solution. This holds, for example,
in the sequential graph, the layer graph, and the delay graph (see
\prettyref subsec:sequential-graph-minimax-complexity, \prettyref
subsec:layer-graph-minimax-complexity, and \prettyref
subsec:delay-graph-minimax-complexity). For instance, the minimax error
for smooth, convex stochastic first-order optimization in the sequential
settings is

  -- -------- -- -------
     @xmath      (113)
  -- -------- -- -------

which is the sum of the deterministic first-order minimax error plus the
error of (regularized) ERM. In contrast, the optimal error in the
intermittent communication setting is different. In the convex case, it
is (ignoring log factors)

  -- -- -- -------
           (114)
  -- -- -- -------

The first term is the deterministic first-order minimax error (see
\prettyref thm:generic-graph-lower-bound) and the second term is the
statistical limit (see \prettyref lem:statistical-term-lower-bound), but
the third term is something different. In particular, it mixes
optimization and statistical terms in a different and interesting
manner.

This suggests that we should think about optimization in the sequential
setting as qualitatively different from optimization in the intermittent
communication setting. In the former case, optimal deterministic
first-order algorithms (e.g. accelerated gradient descent) are often,
perhaps with slight modifications, also optimal stochastic first-order
algorithms (e.g. AC-SA). Furthermore, algorithms can be understood via a
bias-variance decomposition—the bias is basically the algorithm’s
performance if the gradients were exact, and the variance is how much
the algorithm’s output is affected by the noisy gradients. In contrast,
in the intermittent communication setting, the interplay between
optimization and statistics appears to be more complex. This shows up
also in the proof of our lower bound, where the “progress” of the
algorithm—which is relevant only to the optimization term in the
sequential setting—is hindered by the noise in the stochastic gradient
oracle.

##### Computational Efficiency

The optimal algorithms in each setting—the better of Minibatch or
Single-Machine AC-SA/SGD—are computationally efficient and require no
significant overhead. Each machine only needs to store a constant number
of vectors, performs only a constant number of vector additions for each
stochastic gradient oracle access, and communicates just one vector per
round. Therefore, the total storage complexity is just @xmath per
machine, the sequential runtime complexity (excluding the oracle
computation) is @xmath , and the total communication complexity is at
most @xmath . In fact, the communication complexity is exactly @xmath
for the Single-Machine methods. Therefore, we should not expect a
substantially better algorithm from the standpoint of computational
efficiency either.

##### Aesthetics

The optimal algorithms are somewhat “ugly” because of the hard switch
between the Minibatch and Single-Machine approach. It would be nice, if
only aesthetically, to have an algorithm that more naturally transitions
between the Minibatch to the Single-Machine rate. Accelerated Local SGD
(Yuan and Ma, 2020 ) or something similar is a contender for such an
algorithm, although it is unclear whether or not this method can match
the optimal rate in all regimes. Local SGD methods can also be augmented
by using two stepsizes—a smaller, conservative stepsize for the local
updates between communications, and a larger, aggressive stepsize when
the local updates are aggregated—this two-stepsize approach allows for
interpolation between Minibatch-like and Single-Machine-like behavior,
and could be used to design a more “natural” optimal algorithm (see
\prettyref subsec:inner-outer).

### 5.2 The Heterogeneous Setting

We now consider the complexity of optimization in the heterogeneous
intermittent communication setting. The objective is the average of
@xmath components, one for each machine,

  -- -------- -- -------
     @xmath      (115)
  -- -------- -- -------

We focus on the goal of finding a single consensus solution that
achieves low value on all of the machines on average Bertsekas and
Tsitsiklis ( 1989 ); Boyd et al. ( 2011 ) . There are, of course, many
other sensible formulations, including “personalized” approaches where
separate minimizers are computed for each objective while still
leveraging relevant information from the others (Hanzely et al., 2021 )
. For this section, we consider the class @xmath consisting of all
objectives ( 115 ) where @xmath and @xmath where @xmath . Notably, we
make no assumptions about the individual components @xmath , and we only
require that their average is smooth, convex, etc.

In the heterogeneous intermittent communication setting, each machine
only has access to information about its corresponding objective. So,
the @xmath machine has access to a stochatic gradient oracle @xmath to
each machine, which provides an unbiased estimate of the @xmath
component @xmath , with variance bounded by @xmath . In the language of
the graph oracle model, we consider algorithms in @xmath where each
vertex @xmath is associated with the oracle @xmath . As in the
homogeneous setting, for technical reasons we also restrict our
attention to algorithms whose queries are bounded in norm by @xmath ,
although this restriction can be eliminated by instead considering
deterministic or span-restricted/zero-respecting algorithms.

#### 5.2.1 Convex Objectives

We begin with our lower bound for convex objectives:

###### Theorem 17.

For any @xmath and @xmath , there exists a quadratic objective @xmath in
any dimension

  -- -------- --
     @xmath   
  -- -------- --

such that the output of any algorithm in @xmath will have suboptimality
at least

  -- -------- --
     @xmath   
  -- -------- --

Proof Sketch To describe the idea of the proof, we will first focus our
attention on the class of span-restricted/zero-respecting algorithms.
The proof is based on the following two quadratic functions:

  -- -------- -------- -------- -------
     @xmath   @xmath            (116)
              @xmath   @xmath   
  -- -------- -------- -------- -------

These functions are identical to a construction from Arjevani and Shamir
( 2015 ) , who show similar lower bounds for a different formulation of
distributed optimization with an exact gradient oracle. These objectives
essentially partition the classic lower bound construction of Nesterov (
2004 ) across two functions. Importantly, if @xmath for all @xmath , and
@xmath is even, then

  -- -------- -- -------
     @xmath      (117)
  -- -------- -- -------

and the vice versa when @xmath is odd. In other words, querying the
gradient of @xmath only reveals an additional coordinate when the
current progress is even, and the gradient of @xmath only gives a new
coordinate when the current progress is odd. This means that making
progress requires alternately querying @xmath and @xmath , which can
only be done once per round of communication. This argument paired with
assigning @xmath to the first @xmath machines and @xmath to the
remaining @xmath machines essentially completes the proof for the
span-restricted/zero-respecting case. We note that this part of the
argument applies even when the algorithm has access to exact gradients
of the objective, as in the setting of Arjevani and Shamir .

To extend the argument to the case of general randomized algorithms, we
again introduce a random rotation matrix @xmath and consider the
functions @xmath and @xmath . If we followed the ideas in \prettyref
subsec:high-level-lower-bound-approach, we would, in addition, “flatten
out” the objective so that small inner products with the columns of
@xmath do not reveal any information about those columns in the
gradient. However, in this case we do something different in order to
preserve the quadratic nature of the objective. Specifically, we do this
“flattening out” only using the stochatic gradient oracle .
Specifically, we construct stochastic gradient oracles for @xmath and
@xmath such that the property ( 117 ) is maintained even when @xmath is
slightly non-zero for @xmath . We show that it is possible to construct
such stochastic gradient oracles without introducing too much variance,
and this allows us to prove the result even for quadratic objectives.
The remaining details of the proof can be found in \prettyref
app:heterogeneous-convex-lower-bound.

In the homogeneous intermittent communication setting, all of our lower
bounds were matched by the better of two algorithms: Minibatch AC-SA and
Single-Machine AC-SA. Notably, in the heterogeneous setting,
Single-Machine AC-SA is not a sensible algorithm because it will only
succeed in optimizing a single component of the objective, which does
not imply anything about optimizing @xmath . Of course, other approaches
like averaging the outputs of Single-Machine AC-SA run on each machine
individually, or an accelerated variant of Local SGD (Yuan and Ma, 2020
) are perfectly reasonable methods which could plausibly outperform
Minibatch AC-SA in certain regimes. However, we now show that actually
Minibatch AC-SA is minimax optimal in the heterogeneous case and no
other algorithm can improve over it in any regime

###### Theorem 18.

For any @xmath and any @xmath the output of Minibatch AC-SA will have
suboptimality at most

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

The result follows from the observation that was made in \prettyref
subsec:minibatch-sgd-in-heterogeneous-local-sgd: that the minibatch
stochastic gradients used by Minibatch AC-SA are actually unbiased
estimates of @xmath despite the heterogeneity of the problem, and their
variance is also reduced by a factor of @xmath . The result then follows
immediately from the guarantee for @xmath steps of AC-SA with stochastic
gradients of variance @xmath (Lan, 2012 ) . ∎

#### 5.2.2 Strongly Convex Objectives

Unsurprisingly, the picture is qualitatively very similar in the
strongly convex setting. We have an analogous lower bound

###### Theorem 19.

For any @xmath and @xmath , there exists an objective @xmath in any
dimension

  -- -------- --
     @xmath   
  -- -------- --

such that the output of any algorithm in @xmath will have suboptimality
at least

  -- -------- --
     @xmath   
  -- -------- --

This algorithm is proven in \prettyref
app:heterogeneous-convex-lower-bound with essentially the same argument
as was used for \prettyref thm:heterogeneous-convex-lower-bound. The
only difference is that @xmath was added to the hard instance to make
the function strongly convex. Similarly, we have a matching upper bound
from the guarantee of Minibatch AC-SA:

###### Theorem 20.

For any @xmath , and any @xmath , the output of Minibatch will have
suboptimality at most

  -- -------- --
     @xmath   
  -- -------- --

As for \prettyref thm:heterogeneous-convex-upper-bound, we observe that
the minibatch stochastic gradients are unbiased estimates of @xmath with
variance @xmath and plug this into the AC-SA guarantee (see \prettyref
subsec:the-reduction).

#### 5.2.3 Conclusions

These results tightly bound the minimax error for optimization in the
heterogeneous intermittent communication setting with convex and
strongly convex objectives, and establish that Minibatch AC-SA is an
optimal algorithm. By analogy, we speculate that Minibatch SGD would be
minimax optimal in the non-convex setting too. These results are
particularly interesting in comparison with the homogeneous intermittent
communication setting:

##### Sequential Local Computation Does Not Help

In the homogeneous setting, the optimal algorithm was the better of
Minibatch AC-SA and Single-Machine AC-SA. This demonstrated a tradeoff
between exploiting the parallelism (using Minibatch AC-SA) and
exploiting the available sequential local computation (Single-Machine
AC-SA). In contrast, in the heterogeneous regime, where the optimal
algorithm is just Minibatch AC-SA, we see that there is no such
tradeoff. In particular, it is actually impossible to exploit the
availability of sequential local computation at all! To see this, we
note that if each machine could only query its stochastic gradient
oracle once, but with variance @xmath , the Minibatch AC-SA guarantee
would be unchanged. In this way, the sequential nature of the @xmath
local stochastic gradient queries is useless beyond its ability to
reduce the variance.

##### Quadratic Structure Does Not Help

In a similar vein, in the homogeneous convex setting, the optimal rate
could be substantially different when the objective is quadratic versus
when it is an arbitrary convex function. Indeed, our lower bounds
\prettyref thm:homogeneous-convex-lower-bound and \prettyref
thm:homogeneous-strongly-convex-lower-bound were based on decidedly
non-quadratic constructions, and their proofs relied critically on their
non-quadratic nature. In contrast, the lower bounds \prettyref
thm:heterogeneous-convex-lower-bound and \prettyref
thm:heterogeneous-strongly-convex-lower-bound apply even for quadratic
objectives, so this additional structure does not help the algorithm at
all.

## 6 Better than Optimal: Breaking The Lower Bounds

Many of the results so far presented have been lower bounds on the
minimax error in various optimization settings. In a certain sense,
these are positive results since they are used to identify optimal
algorithms, but, they also have a negative interpretation, as they show
fundamental limits on optimization algorithms for these settings. For
the intermittent communication setting in particular, the optimal
algorithms—a combination of Minibatch AC-SA and Single-Machine AC-SA—are
disappointingly naïve, and their guarantees unable to simultaneously
exploit the availability of local computation and parallelism.

However, one of the most important uses of optimization lower bounds is
to identify how to break them. Rather than viewing them as impossibility
results that show when we should throw up our hands because we can do no
better than what we have, we should view them as a hint about which of
our assumptions should be modified or strengthened. One way to break a
lower bound—by designing an algorithm that achieves a better
guarantee—is to impose additional structure on the problem that an
algorithm might exploit. Lower bounds identify particular hard
objectives and show us why those objectives, specifically, are hard to
optimize, which in turn motivates new assumptions that (1) obviate the
need to deal with those particular hard functions and (2) allow for more
effective algorithms more broadly.

Of course, it is trivial to introduce new assumptions that make
optimization easy—for instance, we could assume that the function has a
minimizer @xmath , which would make optimization very easy indeed!—but
these assumptions are too strong. Therefore, it is important to identify
new assumptions that allow for the circumvention of the lower bounds by
making the problem “easier” while simultaneously applying to the
objectives that we are actually interested in optimizing. We should
therefore think of this moreso as modelling rather than assuming—we want
to distill the relevant properties of the objectives we are interested
in down to a short list that are amenable for analyzing algorithms. In
most of the results so far, we have relied on a very short list of
properties: smoothness, convexity, a bound on the gradient variance, and
a bound on @xmath . In this section, we will explore some possible
additions to this list that will allow us to break the lower bounds.

### 6.1 Intermittent Communication and Near-Quadratic Objectives

In the homogeneous intermittent communication setting, \prettyref
thm:homogeneous-convex-lower-bound and \prettyref
thm:homogeneous-strongly-convex-lower-bound show that the better of
Minibatch and Single-Machine AC-SA is optimal. However, we had already
seen from \prettyref cor:acc-local-sgd-optimal-quadratics that Local
AC-SA could achieve substantially lower error when the objective is
quadratic. Naturally, this suggests that if the objective were “nearly”
quadratic in some way, then Local AC-SA or some other similar method
should be able to defeat the lower bounds \prettyref
thm:homogeneous-convex-lower-bound and \prettyref
thm:homogeneous-strongly-convex-lower-bound. Indeed, the constructions
used to prove those lower bounds were not quadratic, and their proofs
relied crucially on this fact.

One means of quantifying “near quadratic” is to impose a bound on the
third derivative of the objective—a quadratic objective has a constant
Hessian, and therefore its third derivative is uniformly zero. We
therefore introduce the class of @xmath -second order smooth convex
objectives @xmath and strongly convex objectives @xmath , which contain
all twice-differentiable @xmath and @xmath for which the Hessian is
@xmath -Lipschitz. The assumption of a bounded third derivative is often
reasonable and, for example, it holds for training generalized linear
models with sufficiently smooth link functions, such as logistic
regression.

In recent work, Yuan and Ma ( 2020 ) proposed an accelerated Local SGD
variant FedAc and analyzed its error for objectives in @xmath and @xmath
. They showed that

###### Theorem 21 (c.f. Theorems C.1 and E.3 (Yuan and Ma, 2020)).

For any @xmath , and any @xmath and @xmath , FedAc , using a stochastic
gradient oracle with bounded @xmath moment @xmath guarantees (omitting
logarithmic factors)

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

The first two terms of both guarantees come close to matching the
guarantee of Local AC-SA for quadratic objectives, except with the
optimization terms scaling with @xmath and @xmath rather than @xmath and
@xmath as for Local AC-SA. The third term in each rate is reminiscent of
the (unaccelerated) Local SGD rates (see \prettyref
thm:local-sgd-homogeneous-upper-bound), but with a better dependence on
@xmath and @xmath . The final terms of these rates are where @xmath
comes in, and they vanish as @xmath .

It is perhaps not immediately obvious, but these guarantees can be
better than the lower bounds in certain parameter regimes. For example,
in the convex case, when @xmath and @xmath , the lower bound reduces to
(ignoring logarithmic factors)

  -- -------- -- -------
     @xmath      (118)
  -- -------- -- -------

On the other hand, the FedAc guarantee is

  -- -------- -- -------
     @xmath      (119)
  -- -------- -- -------

Therefore, whenever @xmath , the FedAc guarantee is strictly better than
the lower bound. This is just one example, but it illustrates that
improvement over the lower bounds is possible when the second-order
smoothness parameter is sufficiently small.

A relevent question is how tight Yuan and Ma ’s guarantee is, and
whether their algorithm might be optimal. It is actually clear that
their method is not optimal in all cases, because the guarantee does not
match Local AC-SA’s ( \prettyref cor:acc-local-sgd-optimal-quadratics)
for @xmath , i.e. when the objective is quadratic, but in other regimes
is it less clear. In order to understand to what extent a bounded third
derivative might help, we provide the following lower bound:

###### Theorem 22.

For any @xmath , there is an objective @xmath in any dimension

  -- -------- --
     @xmath   
  -- -------- --

such that the output of any algorithm in @xmath will have suboptimality
at least

  -- -------- --
     @xmath   
  -- -------- --

This lower bound is identical to \prettyref
thm:homogeneous-convex-lower-bound plus the addition of the final term
in the @xmath , and indeed we prove this using the same proof as for
\prettyref thm:homogeneous-convex-lower-bound. In fact, \prettyref
thm:homogeneous-convex-lower-bound is proven as a corollary to this by
taking @xmath sufficiently large that the corresponding term is
irrelevant. The details of the proof can be found in \prettyref
app:intermittent-communication-homogeneous-convex-lower-bound-third-order-smooth.
There is a significant gap between the lower bound and existing upper
bounds like FedAc , so this lower bound is not the final word, and there
is additional work to be done.

### 6.2 Boundedly-Heterogeneous Objectives

In the heterogeneous intermittent communication setting, we proved that
Minibatch AC-SA is an optimal algorithm. We also showed that it is
essentially impossible to leverage the @xmath sequential stochastic
gradients that each machine is allowed to compute in each round beyond
simply computing (non-adaptively) a minibatch stochastic gradient with
lower variance. However, we already saw in \prettyref
subsec:local-sgd-heterogeneous that when the heterogeneous objective is
not arbitrarily heterogeneous, then can be opportunities for
improvement.

To that end, we will now consider heterogeneous intermittent
communication optimization under a bounded-heterogeneity assumption. In
particular, we will say that a heterogeneous objective @xmath is @xmath
-heterogeneous if for some @xmath

  -- -------- -- -------
     @xmath      (120)
  -- -------- -- -------

and we will say that it is @xmath -uniformly heterogeneous if

  -- -------- -- -------
     @xmath      (121)
  -- -------- -- -------

We then define the function classes @xmath and @xmath as the class of
@xmath -heterogeneous and @xmath -uniformly heterogeneous objectives,
respectively, where @xmath is @xmath -smooth for all @xmath , and @xmath
.

We already saw in \prettyref thm:local-sgd-heterogeneous-uppper-bound
that Local SGD guarantees for any @xmath that

  -- -------- -- -------
     @xmath      (122)
  -- -------- -- -------

This algorithm is not accelerated and it is definitely not going to be
optimal. However, even this algorithm’s guarantee can improve over the
lower bound \prettyref thm:heterogeneous-convex-lower-bound in certain
regimes. For example, if @xmath , and @xmath , then the lower bound
reduces to

  -- -------- -- -------
     @xmath      (123)
  -- -------- -- -------

and Local SGD’s guarantee to

  -- -------- -- -------
     @xmath      (124)
  -- -------- -- -------

Therefore, when @xmath , then the upper bound breaks the lower bound. Of
course, this constraint on @xmath is very tight and it requires the
problem be very nearly homogeneous. Nevertheless, it indicates that
improvement is possible when the problem is not arbitrarily
heterogeneous. Indeed, this raises the possibility that the assumption
of @xmath -uniform heterogeneity or the weaker constraint of @xmath
-heterogeneity might be sufficient to develop better algorithms that
circumvent the pessimistic lower bound \prettyref
thm:heterogeneous-convex-lower-bound.

To test the limits of how far this could take us, we prove the following
lower bound for optimizing @xmath -heterogeneous objectives:

###### Theorem 23.

For any @xmath , there exists an objective @xmath in any dimension

  -- -------- --
     @xmath   
  -- -------- --

and an objective @xmath in any dimension

  -- -------- --
     @xmath   
  -- -------- --

such that the output of any algorithm in @xmath will have suboptimality
at least

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

This is proven in \prettyref app:heterogeneous-convex-lower-bound using
the same approach as for \prettyref thm:heterogeneous-convex-lower-bound
and \prettyref thm:heterogeneous-strongly-convex-lower-bound. In fact,
those theorems are proven as a corollary to this one by taking @xmath
large enough that the @xmath -dependent terms drop out.

We see from the lower bounds that once @xmath becomes sufficiently
small—smaller than @xmath in the convex case and smaller than @xmath in
the strongly convex case—there is potential for improvement over the
lower bounds \prettyref thm:heterogeneous-convex-lower-bound and
\prettyref thm:heterogeneous-strongly-convex-lower-bound. Of course, the
lower bounds \prettyref thm:homogeneous-convex-lower-bound and
\prettyref thm:homogeneous-strongly-convex-lower-bound also apply here
since homogeneous objectives are a special case of heterogeneous ones,
so the possibility of improvement has limits.

This result raises the question of how large or small we should expect
@xmath to be “typically.” The answer to this depends significantly on
how the heterogeneity arises. We will focus on three cases in the
context of machine learning training: different data sources,
underdetermined problems, and randomly partitioned data.

##### Different Data Sources

The first and probably most obvious way that heterogeneity can arise is
when each parallel worker is computing stochastic gradients using data
drawn from genuinely different sources. For instance, when an English
language model is being trained in parallel on servers in various
Anglophone countries. In this case, while the data sources are
presumably somewhat related (or else why try to minimize the average of
the local objectives?), there is no reason to think that it would be
particularly small. Nevertheless in the convex case, if we make the
assumption that the local objectives have minimizers @xmath with norm
@xmath then by the @xmath -Lipschitzness of @xmath , we can bound

  -- -------- -- -------
     @xmath      (125)
  -- -------- -- -------

Therefore, it is reasonable to expect @xmath in the convex case, which
is substantially larger than would allow for improvement over the
original lower bound \prettyref thm:heterogeneous-convex-lower-bound by
\prettyref thm:bounded-heterogeneous-convex-lower-bound.

##### Underdetermined Problems

If we consider just the task of minimizing the training loss over @xmath
samples, @xmath per machine, then the local objectives will naturally be
heterogeneous since they are based on different data. However, even if
each machine’s data comes from a completely different source, when the
problem is underdetermined—meaning that there are many solutions which
minimize the training loss—then @xmath because there will be at least
one minimizer that is shared amongst all of the local objectives.
However, when training machine learning models in this underdetermined
regime, it is typically necessary to introduce a regularizer, often an
L2 regularizer of the form @xmath , to allow for better generalization
performance, and the optimal regularization parameter typically scales
with @xmath . In this case, since @xmath minimizes the unregularized
local objectives, we would have

  -- -------- -- -------
     @xmath      (126)
  -- -------- -- -------

Therefore, we can expect @xmath in this regime, which would generally be
small enough to hope for some improvement over the original lower bound
\prettyref thm:heterogeneous-convex-lower-bound.

##### Randomly Partitioned Data

The final example of how heterogeneity might arise is when a large
training set, all from the same source, is randomly partitioned across
the @xmath machines, with @xmath samples per machine. Even when the
problem is not underdetermined as in the previous example, we can again
expect the level of heterogeneity to be small. In particular, for each
individual sample, the expectation of the gradient of the loss of that
sample at @xmath is zero, and @xmath is the average of @xmath
independent samples’ gradients. Therefore, when the sample gradients
have bounded variance @xmath , we would have

  -- -------- -- -------
     @xmath      (127)
  -- -------- -- -------

where the expectation is over the draw of the @xmath i.i.d. samples.
Therefore, the level of heterogeneity would be bounded by @xmath .
Whether or not this is small enough for improvement over the original
lower bound, of course, depends on @xmath and the number of samples per
machine, but it would certainly not require an unreasonably large number
of samples.

### 6.3 The Statistical Learning Setting: Assumptions on Components

Stochastic optimization commonly arises in the context of statistical
learning, where the goal is to minimize the expected loss with respect
to a model’s parameters. In this case, the objective can be written
@xmath , where @xmath represents data drawn i.i.d. from an unknown
distribution, and the “components” @xmath represent the loss of the
model parametrized by @xmath on the example @xmath .

For most of the results that have been presented so far, we only placed
restrictions on the objective @xmath itself, and on the first and second
moments of @xmath . However, in the statistical learning setting, it is
often natural to assume that the loss function @xmath itself satisfies
particular properties for each @xmath individually . For instance, for
many machine learning problems, the loss @xmath is convex and smooth and
furthermore, the most natural implementation of a gradient oracle is to
compute @xmath for an i.i.d. @xmath . This is a non-trivial restriction
on the stochastic gradient oracle, and it is conceivable that this
property could be leveraged to design and analyze methods that converges
faster than lower bounds like, for example, \prettyref
thm:homogeneous-convex-lower-bound would allow.

The specific stochastic gradient oracle ( 108 ) used to prove \prettyref
thm:homogeneous-convex-lower-bound, which zeroed out particular
coordinates of the gradient depending on the query point, cannot be
written as the gradient of a random smooth function. Similarly, the
gradient oracles used for several of the other lower bounds are also not
expressible as the gradient of a smooth function. In this sense, these
lower bound constructions are somewhat “unnatural.” However, we are not
aware of any analysis that meaningfully exploits the fact that the
gradient is given by @xmath for a smooth @xmath . There are numerous
papers that make this exact assumption: that @xmath and that the
stochastic gradients are given by @xmath for some smooth, convex @xmath
(e.g. Bottou et al., 2018 ; Nguyen et al., 2019 ; Koloskova et al., 2020
; Woodworth et al., 2020a ) . However, the purpose of this assumption is
just to bound quantities like @xmath or @xmath in terms of @xmath ,
i.e. the variance of the gradients at the optimum. It is, of course,
useful to provide guarantees in terms just of @xmath , but we point out
that the components do not necessarily have to be smooth to attain such
bounds. For example, the stochatic gradients satisfying @xmath is enough
to obtain guarantees in terms just of the variance at the minimizer, and
this is only a condition on the second moment of the gradient, not the
components per se. Furthermore, in all of our lower bound constructions,
the variance of the stochastic gradient oracles is bounded uniformly by
@xmath , so @xmath can always be replaced by @xmath in our theorems.

A very interesting question is what sorts of assumptions about the
components can be leveraged to obtain better rates in the various
settings we have considered, and under what conditions. Alternatively,
it would also be interesting to find situations where properties like
smooth components do not allow for any improvement. For example, perhaps
it is possible to prove the same result as \prettyref
thm:homogeneous-convex-lower-bound using a smooth gradient oracle?

### 6.4 The Statistical Learning Setting: Repeated Access to Components

In the statistical learning setting, it is also natural to consider
algorithms that can evaluate the gradient at multiple points for the
same datum @xmath . Specifically, allowing the algorithm access to a
pool of samples @xmath drawn i.i.d. from @xmath and to compute @xmath
for any chosen @xmath and @xmath opens up additional possibilities.
Indeed, Arjevani et al. ( 2019 ) showed that multiple—even just
two—accesses to each component enables substantially faster convergence
( @xmath vs. @xmath ) in sequential stochastic non-convex optimization.
Similar results have been shown for zeroth-order and bandit convex
optimization (Agarwal et al., 2010 ; Duchi et al., 2015 ; Shamir, 2017 ;
Nesterov and Spokoiny, 2017 ) , where accessing each component twice
allows for a quadratic improvement in the dimension-dependence.

In sequential smooth convex optimization, if @xmath has “finite-sum”
structure (i.e. @xmath is the uniform distribution on @xmath ), then
allowing the algorithm to pick a component and access it multiple times
opens the door to variance-reduction techniques like SVRG (Johnson and
Zhang, 2013 ) . These methods have updates of the form:

  -- -------- -- -------
     @xmath      (128)
  -- -------- -- -------

Computing this update therefore requires evaluating the gradient of
@xmath at two different points, which necessitates multiple accesses to
a chosen component. For finite sums, this stronger oracle access allows
faster rates compared with a single-access oracle (see discussion in,
e.g., Arjevani et al., 2020a ) .

Most relevantly, in the intermittent communication setting, distributed
variants of SVRG are able to improve over the lower bound in \prettyref
thm:homogeneous-convex-lower-bound (Wang et al., 2017 ; Lee et al., 2017
; Shamir, 2016 ; Woodworth et al., 2018 ) . Specifically, when the
components are @xmath -smooth and @xmath -Lipschitz, and when the
algorithm can make multiple stochastic gradient queries the same @xmath
, Woodworth et al. show that using distributed SVRG to optimize an
empirical objective composed of suitably many samples is able to achieve
convergence at the rate

  -- -------- -- -------
     @xmath      (129)
  -- -------- -- -------

While this guarantee (necessarily!) holds in a different setting than
\prettyref thm:homogeneous-convex-lower-bound, the Lipschitz bound
@xmath is generally analogous to the standard deviation of the
stochastic gradient variance, @xmath (indeed, @xmath is an upper bound
on @xmath ). With this in mind, this distributed SVRG algorithm can beat
the lower bound in \prettyref thm:homogeneous-convex-lower-bound when
@xmath , @xmath , and @xmath are sufficiently large.

### 6.5 Non-Convex Optimization with Mean Squared Smoothness

We will now revisit the homogeneous intermittent communication setting
with non-convex objectives. We recall that \prettyref
thm:homogeneous-non-convex-lower-bound proved a lower bound on how small
any intermittent communication algorithm can make the gradient of

  -- -------- -- -------
     @xmath      (130)
  -- -------- -- -------

Our proof, which followed the general scheme described in \prettyref
subsec:high-level-lower-bound-approach, involved constructing an
objective whose argument is rotated by some unknown matrix @xmath , and
showing that any algorithm that finds a point where the gradient is
small must essentially be able to identify all of the columns of @xmath
. To make this more difficult, a stochastic gradient oracle was
constructed such that the influence of the “yet-unknown” columns of
@xmath is erased from the gradient with probability @xmath , slowing
progress by a factor of @xmath .

However, the responses of this stochastic gradient oracle are very
sensitive to their input, because the columns of @xmath that were
determined to be “unknown” based on a query @xmath —specifically, those
columns for which @xmath —can change sharply with @xmath . Consequently,
the stochastic gradient oracle used in the proof of \prettyref
thm:homogeneous-non-convex-lower-bound was highly
non-smooth—discontinuous actually—as a function of @xmath . Of course,
this is allowed the context of “independent noise” oracles (see
\prettyref subsec:the-oracle), and a reasonable algorithm
(Minibatch/Single-Machine SGD) was able to match the lower bound, so
there is nothing wrong with this setting.

Nevertheless, in the statistical learning setting, we can identify a
setting in which it is possible to improve over the lower bound ( 130 )
by using an algorithm which exploits a certain smoothness property of
the stochastic gradients in addition to multiple queries for the same
@xmath . Specifically, we will consider the complexity of non-convex
optimization in the homogeneous intermittent communication setting under
the condition that the stochastic gradient oracle available to the
algorithm is smooth. To quantify this, we use the notion of “mean
squared smoothness” which has been previously considered in the
non-convex optimization literature (Fang et al., 2018 ; Lei et al., 2017
) .

###### Definition 4.

For @xmath equipped with a statistical learning first-order oracle which
returns @xmath for an i.i.d. @xmath , we say that the oracle @xmath is
@xmath -mean squared smooth (MSS) if for all @xmath

  -- -------- --
     @xmath   
  -- -------- --

We will use @xmath to denote an arbitrary @xmath -MSS statistical
learning first-order oracle for @xmath with variance bounded by @xmath ,
and we define @xmath to be the class of all @xmath -smooth, possibly
non-convex objectives with @xmath . We note that @xmath -MSS is implied
by @xmath being @xmath -smooth, but can apply more broadly. We also note
that by Jensen’s inequality, @xmath -MSS implies that @xmath is @xmath
-smooth.

We also consider algorithms that may access the stochastic gradient
oracle for the same @xmath multiple times. Specifically, algorithm has
access to an oracle @xmath which, when queried with a vector @xmath
returns @xmath for an i.i.d. @xmath , and when queried with @xmath for
any previously seen @xmath returns @xmath for the chosen @xmath .

In prior work, Fang et al. ( 2018 ) analyzed an algorithm, Spider ,
which uses @xmath sequential queries to an oracle @xmath to find an
approximate stationary point for any @xmath of norm

  -- -------- -- -------
     @xmath      (131)
  -- -------- -- -------

In the sequential seting, Arjevani et al. ( 2019 ) show that this rate
is essentially optimal and cannot be improved.

In the intermittent communication setting, as before, we consider two
variants of this algorithm: Minibatch Spider and Single-Machine Spider .
Minibatch Spider corresponds to @xmath steps of Spider using minibatches
of size @xmath , and Single-Machine Spider corresponds to @xmath steps
of Spider using minibatches of size just @xmath . Plugging the number of
steps and the variance reduction implied by minibatching, we can
guarantee using the better of these methods that

  -- -------- -- -------
     @xmath      (132)
  -- -------- -- -------

The first term corresponds in the @xmath to Single-Machine Spider ; the
second term corresponds to Minibatch Spider ; and the last term
corresponds to simply returning @xmath which, by the @xmath -smoothness
of @xmath , has gradient norm at most @xmath .

Comparing this to ( 130 ), we can see that these methods, which leverage
the mean squared smoothness of the stochastic gradient oracle, are
sometimes able to break the lower bound. Specifically, the upper bound (
132 ) avoids dependence on any terms that scale with @xmath and replace
them with potentially better @xmath or @xmath terms instead.

It is interesting to ask whether the combination of Minibatch and
Single-Machine Spider might be optimal in the mean squared smooth
intermittent communication setting, as Spider is in the sequential
setting. To try to answer this question, we prove the following lower
bound

###### Theorem 24.

For any @xmath , there exists a function @xmath in a sufficiently large
dimension @xmath such that for any algorithm in @xmath

  -- -------- --
     @xmath   
  -- -------- --

The proof of this lower bound is similar to the proof of \prettyref
thm:homogeneous-non-convex-lower-bound, and is also very similar to the
proof we used in the sequential setting to show the optimality of Spider
(Arjevani et al., 2019 ) . In the proof of \prettyref
thm:homogeneous-non-convex-lower-bound we used the stochastic gradient
oracle to zero out the next relevant direction that the gradient might
reveal using something like a non-smooth, discontinuous indicator
function, which led to that oracle being highly non-mean squared smooth.
This time, we instead use a smoothed out indicator function, which makes
the oracle mean squared smooth but, of course, it makes the lower bound
lower. Details of the proof can be found in \prettyref
app:intermittent-communication-homogeneous-non-convex-MSS-lower-bound.
While a very similar argument sufficed to prove a lower bound that
precisely matched the Spider guarantee in the sequential setting, there
are some gaps between the upper bound ( 132 ) and the lower bound
\prettyref thm:homogeneous-non-convex-MSS-lower-bound.

## 7 Conclusion

This thesis addresses a number of theoretical questions in distributed
stochastic optimization, with particular emphasis on understanding the
minimax oracle complexity of distributed optimization. Answers to these
theoretical questions are quite useful—they can be used to identify
optimal algorithms; to identify gaps in our understanding which can
prompt further study; and even when optimal algorithms are known, to
shed light on additional problem structure that can be introduced and
exploited to develop better, more specialized methods.

Nevertheless, there are limits to how much we can learn from pure theory
and from the concept of minimax oracle complexity in particular. In
fact, there are frequently mismatches between theoretical prescriptions
and practical observations. As an example, accelerated variants of
common optimization algorithms like Accelerated Gradient Descent,
Accelerated Stochastic Gradient Descent, Accelerated SVRG, etc. require
very carefully chosen momentum parameters in order for their convergence
guarantees to hold. However, any practitioner will tell you that you
should just set the momentum to some cross-validated constant value.

Another example that is perhaps more consequential is the case of Local
SGD in the intermittent communication setting. As discussed in
\prettyref sec:local-sgd, the theoretical guarantees for Local SGD are
not particularly impressive. In certain cases, Local SGD can fail to
improve over very simple and naive baselines, and in \prettyref
sec:intermittent-communication-setting we show that accelerated variants
of these baselines will always dominate Local SGD or any of its
accelerated variants. However , all sorts of people use Local SGD to
solve all sorts of optimization problems all the time, and it often
works very well and better than the available alternatives (Lin et al.,
2018 ; Zhang et al., 2016 ; Zhou and Cong, 2018b ) . This suggests that
there is more to Local SGD than just its worst case convergence
guarantees under the particular set of assumptions that we consider.

Moving forward, there are a number of interesting questions about the
relationship between theoretical and practical properties of
optimization algorithms. It is apparent that optimization algorithms are
very often deployed outside of the worst-case, how should we think about
studying and understanding their performance in the “average case”?
Proving theorems about optimization algorithms often requires choosing
stepsizes/momentum parameters/etc. very carefully, but how important is
this, really? Does the algorithm not work with simpler parameter
choices? When and why? Also, as we alluded to in \prettyref
sec:breaking-the-lower-bounds, the particular details of the assumptions
about the objective and oracle can have a substantial impact on the
minimax oracle complexity, and on which algorithms are or are not
optimal. This raises questions about which assumptions we should make
and empirical questions about which choices best correspond with
“typical” applications.
