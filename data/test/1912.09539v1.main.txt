## Chapter 1 Introduction

Service robots are appearing more and more in our daily life. They are
extremely useful because they can help elders or people with motor
impairments to achieve independence in everyday tasks like delivering
objects, e.g. serving a coffee / meal or cleaning tables (Ciocarlie
et al., 2014 ) . One of the primary challenges of service robotics is
the adaptation of robots to new tasks in changing environments, where
they interact with non-expert users. Elderly, injured, and disabled
people have consistently attributed a high priority to object
manipulation tasks (Jain and Kemp, 2010 ) . Object manipulation tasks
consist of two phases: the first is the perception of the object and the
second is the planning and execution of arm or body motions which grasp
the object and carry out the manipulation task. These two phases are
closely related: object perception provides information to update the
model of the environment, while planning uses this world model
information to generate sequences of arm movements and grasp actions for
the robot. Therefore object perception is one crucial component of a
service robot besides capabilities like manipulation or navigation.

In addition, assistive robots must perform the tasks in reasonable time.
It is also expected that the competence of the robot increases over
time, that is, robots must robustly adapt to new environments by being
capable of handling new objects. However, it is not feasible to assume
that one can pre-program all necessary object categories for assistive
robots. Instead, robots should learn autonomously from novel
experiences, supported in the feedback from human teachers. In order to
incrementally adapt to new environments, an autonomous assistive robot
must have the ability to process visual information and conduct learning
and recognition tasks in a concurrent and open-ended fashion.

To cope with these issues, we explore how robots could learn
incrementally from their own experiences as well as from interaction
with humans. This thesis is a product of efforts in this direction.

### 1.1 Motivation

Several state-of-the-art assistive robots use traditional object
category learning and recognition approaches (Leroux and Lebec, 2013 ;
Beetz et al., 2011 ) . These classical approaches are often designed for
static environments in which it is viable to separate the training
(off-line) and testing (on-line) phases. In these cases, the world model
is static, in the sense that the representation of the known categories
does not change after the training stage. Although such systems have
been shown to be useful in a variety of real world scenarios, they are
unable to adapt to dynamic environments (Jeong and Lee, 2012 ) .
Therefore, most robots lack the ability to learn new objects from past
experiences. To migrate a robot to a new environment one must often
completely re-generate the knowledge-base that it is running with.

In open-ended domains, it is not viable to hand-code all possible
behaviours and to anticipate all possible exceptions. One of the
challenging tasks in open-ended domains is object category learning and
recognition because of the very large number of objects present in
household environments and, moreover, due to the infinite variety in the
appearance of those objects. Given this, the appropriate strategy to
solve the problem is to make robots capable of learning on site, rather
than to exhaustively program them before deployment. One of the early
works on open-ended object category learning and recognition was carried
out having in mind applications in language acquisition. The authors
characterize their approach as follows:

-   “ The learning approach is open-ended in that there is no set of
    words and meanings defined in advance, and new words and meanings
    are acquired incrementally through interaction with a human
    instructor. ” ( Chauhan and Seabra Lopes ( 2011 ) ; see also
    Seabra Lopes and Chauhan ( 2007 ) ; Seabra Lopes and Chauhan ( 2008
    ) )

In other words, “open-ended” means that the robot does not know in
advance which object categories it will have to learn, which
observations will be available, and when they will be available to
support the learning.

Towards this goal, cognitive robotics looks at human cognition as a
source of inspiration for developing automatic perception capabilities
that will allow robots to, incrementally learn object categories from
the set of accumulated experiences and reason about how to perform
complex tasks. In particular, humans learn to recognize object
categories ceaselessly over time. This ability to refine knowledge from
the set of accumulated experiences facilitates the adaptation to new
environments. Inspired by such abilities, this thesis proposes different
approaches towards 3D object category learning and recognition in an
interactive and open-ended manner. This is necessary for service robots,
not only to perform tasks in a reasonable amount of time and in an
appropriate manner, but also to robustly adapt to new environments by
handling new objects. In particular, we propose a set of perception
capabilities that will allow robots to, incrementally learn object
categories from the set of accumulated experiences and reason about how
to perform complex tasks. To achieve these goals, it is critical to
detect, track and recognize objects in the environment as well as to
conceptualize experiences and learn novel object categories in an
open-ended manner, based on human-robot interaction.

This PhD project puts forward the development of a real-time system for
acquiring and recognizing object categories in open-ended manner based
on human-robot interaction. This work focuses on learning and
recognizing table-top objects, which can be manipulated by the robot. In
this context, many interesting and challenging issues regarding
interactive open-ended 3D visual object category learning and
recognition are addressed.

### 1.2 Objectives

The main objective of this research was to study, design and develop an
interactive open-ended 3D object category learning and recognition
system to be utilized in autonomous service robots. This kind of
perception system comprises a significant number of software modules,
which must be closely coupled in their structure and functionality. The
software modules of the proposed system are including Pre-Processing ,
Object Detection , Feature Extraction , Object Representation , Object
Conceptualization , Object Recognition , Perceptual Memory and
Human-Robot Interaction . The developed perception and perceptual
learning capabilities target objects in table-top scenes, e.g. in a
restaurant environment. Every module must be evaluated based on real or
simulated data individually. The following specific objectives will be
pursued:

-   Design and develop a real-time modular 3D object recognition system
    for acquiring and recognizing object categories in open-ended manner
    based on human-robot interaction.

-   Acquire a deep understanding about how to automatically detect,
    recognize and conceptualize objects in 3D unorganized scenes in
    open-ended manner .

-   Design and develop human-robot interaction capabilities for naming
    objects and scenes, and for providing corrective feedback for
    learning.

-   Acquire a deep understanding of point cloud processing and 3D shape
    descriptors. This objective led to proposing a new object descriptor
    named Global Orthographic Object Descriptor (GOOD) as described in
    Chapter 5 .

-   Investigate and use hierarchical object representation technique for
    facilitating object category learning and optimizing recognition as
    well as memory usage.

-   Consider instance-based and model-based learning mechanisms to
    incrementally learn object categories from the set of accumulated
    experiences.

-   Design and develop a “ Simulated Teacher ” to evaluate system
    performance in a systematic and comprehensive way in different
    scenarios, including multi-context scenarios.

### 1.3 Research Context

This work started in the framework of the European project, RACE:
Robustness by Autonomous Competence Enhancement (Rockel et al., 2013 ;
Hertzberg et al., 2014 ) . The overall aim of this project is to develop
an artificial cognitive system, embodied by a service robot, able to
build a high-level understanding of the world it acts in by storing and
exploiting appropriate memories of its experiences ¹ ¹ 1
http://project-race.eu/ . The RACE project assumed that versatility and
competence enhancement can be obtained by learning from experiences. The
project focused on acquiring and conceptualizing experiences about
objects (Oliveira et al., 2015b ) , scene layouts (Dubba et al., 2014 )
and activities (Mokhtari et al., 2017 ) as a means to enhance robot
competence over time thus achieving robustness. Stimuli for learning can
be collected, either autonomously by robots, or when they receive
appropriate feedback from users. The functional components of the RACE
architecture are represented by boxes in Fig. 1.1 .

Each component may contain one or more modules, which are implemented as
nodes (or nodelets) over the Robot Operating System (ROS) (Cousins
et al., 2010 ) . The Reasoning and Interpretation component includes a
temporal reasoner, a spatial reasoner and a description logics reasoner.
Perception contains several modules for symbolic proprioception and
exteroception, which generate occurrences. The Experience Management and
Conceptualization component pre-processes occurrences, extracts relevant
experiences, uses them to create new concepts and stores these in the
Memory component. The User Interface component receives instructions
from the user and relays them to the Planning component. Planning is
carried out using SHOP2, a Hierarchical Task Network planner (Nau
et al., 2003 ) . The produced plans are executed by the Plan Execution
Management component. It outputs a plan which is collected by the Plan
Execution and Management component. Finally, actions are conveyed to the
robot actuators.

In this work, we study the topic of 3D object category learning and
recognition in open-ended robotic domains and evaluate the proposed
object recognition system in the framework of the RACE project. In this
context, “open-ended” implies that visual information arrives
continuously, and learning and recognition are performed in an any time
basis. The open-ended object learning system must update its models
ceaselessly over time with limited computational resources. This type of
perception system must comprise a significant number of software
modules, which must be closely coupled in their structure and
functionality. These modules might handle large size data, which they
may need to store or transfer to other modules.

In this thesis, two different cognitive perception architectures were
explored, where the key difference between these architectures is the
learning and recognition approach used. We evaluate the proposed object
perception system in the framework of the RACE project. Later, the
proposed methodology was also evaluated on standard datasets and using a
robotic arm platform provided by the University of Aveiro.

### 1.4 Publications

The work presented in this thesis spawned a series of publications
presented at major conferences and top journals in the field. Below is
an excerpt from this list:

-    Journals

    1.   Kasaei, S. Hamidreza , et al. “Local-LDA: Open-Ended Learning
        of Latent Topics for 3D Object Recognition”. IEEE Transactions
        on Pattern Analysis and Machine Intelligence (TPAMI) (2019).

    2.   Kasaei, S. Hamidreza , et al. “Towards Lifelong Assistive
        Robotics: A Tight Coupling between Object Perception and
        Manipulation”. Neurocomputing Journal 291 (2018): 151-166.

    3.   Kasaei, S. Hamidreza , et al. “GOOD: A Global Orthographic
        Object Descriptor for 3D Object Recognition and Manipulation.”
        Pattern Recognition Letters 83 (2016): 312-320.

    4.  Oliveira Miguel, Luís Seabra Lopes, Gi Hyun Lim, Kasaei, S.
        Hamidreza , Ana Maria Tomé, “3D object perception and perceptual
        learning in the RACE project.” Robotics and Autonomous Systems
        75 (2016): 614-626.

    5.   Kasaei, S. Hamidreza , et al. “Interactive Open-Ended Learning
        for 3D Object Recognition: An Approach and Experiments." Journal
        of Intelligent & Robotic Systems 80, no. 3 (2015): 537-553.

    6.  Joachim Hertzberg, Jianwei Zhang, Liwei Zhang, Sebastian Rockel,
        Bernd Neumann, Jos Lehmann, Krishna S. R. Dubba, Anthony G.
        Cohn, Alessandro Saffiotti, Federico Pecora, Masoumeh Mansouri,
        Stefan Konecny, Martin Gunther, Sebastian Stock, Luis Seabra
        Lopes, Miguel Oliveira, Gi Hyun Li, Kasaei, S. Hamidreza , Vahid
        Mokhtari, Lothar Hotz, Wilfried Bohlken. “The RACE
        Project – Robustness by Autonomous Competence Enhancement.”
        Kunstliche Intelligenz 28, no. 4 (2014): 297-304.

-    Conferences

    1.   Kasaei, S. Hamidreza , et al. “Interactive Open-Ended Object,
        Affordance and Grasp Learning for Robotic Manipulation”. 2019
        IEEE/RSJ International Conference on Robotics and Automation
        (ICRA 2019).

    2.   Kasaei, S. Hamidreza , et al. “Coping with Context Change in
        Open-Ended Object Recognition without Explicit Context
        Information”. 2018 IEEE/RSJ International Conference on
        Intelligent Robots and Systems (IROS 2018).

    3.   Kasaei, S. Hamidreza , et al. “Perceiving, Learning, and
        Recognizing 3D Objects: An Approach to Cognitive Service
        Robots.” 2018 Thirty-Second AAAI Conference on Artificial
        Intelligence (AAAI 2018).

    4.   Kasaei, S. Hamidreza , et al. “Hierarchical Object
        Representation for Open-Ended Object Category Learning and
        Recognition.” 2016 Advances in Neural Information Processing
        Systems (NIPS 2016), pp. 1948-1956.

    5.   Kasaei, S. Hamidreza , et al. “An Orthographic Descriptor for
        3D Object Learning and Recognition”. 2016 IEEE/RSJ International
        Conference on Intelligent Robots and Systems (IROS 2016), pp.
        4158-4163.

    6.  Nima Shafii, Kasaei, S. Hamidreza , et al. “Learning to Grasp
        Familiar Objects using Object View Recognition and Template
        Matching”. 2016 IEEE/RSJ International Conference on Intelligent
        Robots and Systems (IROS 2016), pp. 2895-2900.

    7.   Kasaei, S. Hamidreza , et al. “Object Learning and Grasping
        Capabilities for Robotic Home Assistants”. RoboCup-2016: Robot
        Soccer World Cup, Lecture Notes in Computer Science, vol. 9776,
        Springer, 2016.

    8.   Kasaei, S. Hamidreza , et al. “Concurrent 3D Object Category
        Learning and Recognition based on Topic Modelling and Human
        Feedback”. 2016 IEEE International Conference on Autonomous
        Robot Systems and Competitions (ICARSC 2016), pp. 329-334.

    9.  Shafii, Nima,  S. Hamidreza Kasaei , et al. “A Learning Approach
        for Robotic Grasp Selection in Open-Ended Domains”. 2016 IEEE
        International Conference on Autonomous Robot Systems and
        Competitions (ICARSC 2016), pp. 112-117.

    10.  Kasaei, S. Hamidreza , et al. “An Adaptive Object Perception
        System based on Environment Exploration and Bayesian Learning”.
        2015 IEEE International Conference on Autonomous Robot Systems
        and Competitions (ICARSC 2015), pp. 221-226.

    11. Oliveira Miguel, Luís Seabra Lopes, Gi Hyun Lim,  S. Hamidreza
        Kasaei , Angel D. Sappa, Ana Maria Tomé, “Concurrent Learning of
        Visual Codebooks and Object Categories in Open-ended Domains”.
        2015 IEEE/RSJ International Conference on Intelligent Robots and
        Systems (IROS 2015), pp. 2488-2495.

    12. Lim, Gi Hyun, Miguel Oliveira,  S. Hamidreza Kasaei , Luís
        Seabra Lopes, "Hierarchical Nearest Neighbor Graphs for Building
        Perceptual Hierarchies", 2015 International Conference on Neural
        Information Processing (ICONIP 2015). Springer, pp. 646-655.

    13.  Kasaei, S. Hamidreza , et al. “An Interactive Open-Ended
        Learning Approach for 3D Object Recognition”, 2014 IEEE
        International Conference on Autonomous Robot Systems and
        Competitions (ICARSC 2014), pp. 47-52.

    14. Oliveira Miguel, Gi Hyun Lim, Luís Seabra Lopes,  S. Hamidreza
        Kasaei , Ana Maria Tomé, Aneesh Chauhan, “A Perceptual Memory
        System for Grounding Semantic Representation in Intelligent
        Service Robot”. 2014 IEEE/RSJ International Conference on
        Intelligent Robots and Systems (IROS 2014), pp. 2216-2223.

    15. Lim, Gi Hyun, Miguel Oliveira, Vahid Mokhtari,  S. Hamidreza
        Kasaei , Luís Seabra Lopes, Ana Maria Tomé, Aneesh
        Chauhan,“Interactive Teaching and Experience Extraction for
        Learning about Objects and Robot Activities”. 2014 The 23rd IEEE
        International Symposium on Robot and Human Interactive
        Communication (RO-MAN 2014), pp. 153-160.

    16. Krishna S.R. Dubba, Miguel Oliveira, Gi Hyun Lim, S.Hamidreza
        Kasaei , Luís Seabra Lopes, Ana Maria Tomé, Anthony G. Cohn and
        David C. Hogg, “Grounding Language in Perception for Scene
        Conceptualization in Autonomous Robots”, Proceedings of
        Artificial Intelligence spring symposium on qualitative
        representations for robots, (AAAI 2014), AI Access Foundation,
        pp. 26-33.

    17.  Kasaei, S. Hamidreza , et al. “On-Line Evaluation of Open-Ended
        Object Recognition System”, Proceedings of the 20 @xmath
        Portuguese Conference on Pattern Recognition (RecPad2014),
        Covilha, Portugal, 2014.

-    Workshops

    1.  Sock Juil, Kasaei, S. Hamidreza , et al. “Multi View 6D Object
        Pose Estimation and Camera Motion Planning using RGBD Images”.
        2017 IEEE International Conference on Computer Vision Workshop
        (ICCV). 2017.

    2.   Kasaei, S. Hamidreza , et al. “An Object Perception Framework
        for Open-Ended Object Conceptualization from Experiences”. In
        NIPS, workshop of Continual Learning and Deep Networks,
        Barcelona, Spain, 2016.

    3.   Kasaei, S. Hamidreza , et al. “An Instance-Based Approach to 3D
        Object Recognition in Open-Ended Robotic Domains”. In Robotics:
        Science and Systems (RSS), workshop of RGB-D: Advanced Reasoning
        with Depth Cameras, Berlin, Germany, 2013.

### 1.5 Thesis Outline

This thesis is structured in nine chapters. The first being this
introduction. In Chapter 2 , we focus on the development of a 3D object
perception and perceptual learning architectures designed for complex
artificial cognitive agents. Chapter 3 is dedicated to the gathering
object experiences in both supervised and unsupervised manner. In
particular, we propose automatic perception capabilities that will allow
robots to automatically detect multiple objects in a crowded scene.
Chapter 4 is devoted to object representations. We present a new object
descriptor named Global Orthographic Object Descriptor (GOOD), designed
to be robust, descriptive and efficient to compute and use. Furthermore,
we propose an extension of Latent Dirichlet Allocation to learn
structural semantic features (i.e., topics) from low-level feature
co-occurrences for each object category independently. Although the
model we developed has been intended to be used for object category
learning and recognition, it is a novel probabilistic model that can be
used in the fields of computer vision and machine learning.

In Chapter 5 , we approach object category learning and recognition from
a long-term perspective and with emphasis on open-endedness, i.e., not
assuming a pre-defined set of categories. Chapter 6 is dedicated to
classical evaluation of all the proposed representation, learning and
recognition approaches. To examine the performance of the proposed
approaches, several sets of 10-fold cross validation experiments were
carried out. Chapter 7 begins by a discussion about an open-ended
evaluation protocols. A novel protocol for evaluating open-ended
learning approaches in multi-context scenarios is then proposed.
Afterwards, we report and discuss the results of the open-ended
experiments that were carried out in both classic single context and
multi-context settings. Profiling and demonstration of the developed
system is the topic of Chapter 8 . Finally, in Chapter 9 , the
conclusions are presented and future research directions are discussed.

## Chapter 2 Architecture of Object Perception and Perceptual Learning
System

This chapter proposes two cognitive architectures designed to create a
proper coupling between perception and action for service robots. This
is necessary for service robots, not only to perform manipulation tasks
in a reasonable amount of time and in an appropriate manner, but also to
robustly adapt to new environments by handling new objects. In
particular, these cognitive architectures provide perception
capabilities that will allow robots to, incrementally learn object
categories from the set of accumulated experiences and reason about how
to perform complex tasks. To achieve these goals, it is critical to
detect, track and recognize objects in the environment as well as to
conceptualize experiences and learn novel object categories in an
open-ended manner, based on human-robot interaction. Therefore, the
following key aspects will have to be taken into consideration:

-    Perception is used to perceive the world. An agent may sense the
    world through different modalities. The perception system provides
    important information that the robot has to use for interacting with
    users and environments. For instance, a robot needs to know which
    kinds of objects exist in a scene and where they are, to interact
    with users and environment.

-    Memory is used to store content about the agent’s beliefs, goals,
    and knowledge; Learning is closely related to memory in human
    cognition. In the cognitive science literature, the existence of
    multiple memory systems is widely accepted. Most recent literature
    converges on five major memory systems (Tulving, 2005 , 1991 ) :
    procedural memory , for sensory-motor skills; perceptual
    representation memory , mainly for the identification of objects;
    working memory , to support basic cognitive activity; semantic
    memory , mainly for spatial and relational information; and episodic
    memory , for specific past happenings, enabling “mental time
    travel”.

-    Interaction and Communication is one of the effective way for an
    agent to obtain knowledge from a human user/teacher. Therefore, a
    communication interface that facilitates language transfer from a
    human user to the robotic agent is another important aspect that a
    cognitive architecture should support (Langley et al., 2009 ) .
    Interaction capabilities are mainly developed to enable human users
    to teach new object categories and instruct the robot to perform
    complex tasks.

-    Learning mechanisms that allow incremental and open-ended learning.
    A cognitive robot must update its models over time with limited
    computational resources. Moreover, open-ended systems should involve
    experience management to prevent the accumulation of examples.
    Otherwise, the memory consumption and the required time to both
    update the models and recognize new objects would increase
    exponentially.

-    Recognition and Categorization is essential for a robotic agent to
    be considered intelligent (Langley et al., 2009 ) . It must have
    capabilities to make decisions and select among alternatives.
    Recognition is closely related to categorization, which involves the
    assignment of objects to known concepts or categories. An ideal
    cognitive architecture should incorporate some way to improve its
    decisions through learning.

-    Planning and Execution : a cognitive architecture must be able to
    generate plans and solve problems to achieve adaptability in novel
    situations. Furthermore, it must be able to execute skills and
    actions in the environment. In some frameworks, this happens in a
    completely reactive manner, with the agent selecting one or more
    primitive actions on each decision cycle, executing them, and
    repeating the process on the next cycle. This approach is associated
    with closed-loop strategies for execution, since the agent can also
    sense the environment on each time step (Langley et al., 2009 ) .

This kind of system must comprise a significant number of software
modules, which must be closely coupled in their structure and
functionality (Jeong and Lee, 2012 ) . Three main design options address
the key computational issues involved in processing and storing
perception data. First, a lightweight, NoSQL database, is used to
implement the perceptual memory. Second, a threadbased approach with
zero copy transport of messages is used in implementing the modules.
Finally, a multiplexing scheme, for the processing of the different
objects in the scene, enables parallelization. This way, the system is
capable of real time object detection, tracking and recognition. The
developed perception and perceptual learning capabilities target objects
in table-top scenes, e.g. in a restaurant environment. These
capabilities are fully integrated in both cognitive architectures and
are running on the PR2 robot used by the RACE project (Hertzberg et al.,
2014 ) , as depicted in Fig. 2.1 (left) , and on a robotic-arm platform
provided by the University of Aveiro as shown in Fig. 2.1 (right) . It
is worth mentioning that two colleagues have contributed to the part of
the work in this thesis. Miguel Oliveira contributed to the development
of the gesture recognizer module, an extended version of which became
part of this thesis. Gi Hyun Lim contributed to the development of
software for the memory system. Most material in this chapter has
already been published in (Hertzberg et al., 2014 ; Hamidreza Kasaei
et al., 2014 ; Oliveira et al., 2015b ; Kasaei et al., 2016e ; Shafii
et al., 2016 ; Kasaei et al., 2018b ) .

The remaining part of this chapter is organized as follows. In the next
section, related works are discussed. Memory and cognitive architecture
issues are discussed in Section 2.2 , leading to the choice of a dual
memory approach and to the development of a perceptual memory system.
Two 3D object perception architectures and their computational issues
are then discussed. Finally, in Section 2.6 , the summary is presented.

### 2.1 Related Work

Although an exhaustive survey of service robotics as well as object
perception techniques is beyond the scope of this chapter,
representative works will be reviewed in this section.

As robots are expected to increasingly interact and collaborate closely
with humans, robotics researchers need to look at human cognition as a
source of inspiration. Learning is closely related to memory in human
cognition. Biological findings about memory and learning have served as
inspiration for the development of computational models and
applications. Wood et al. ( 2011 ) present a thorough review and
discussion on memory systems in animals as well as artificial agents,
having in mind further developments in artificial intelligence and
cognitive science.

Over the past decade, several researches have been conducted to develop
cognitive architectures for autonomous robots. Most of the
state-of-the-art cognitive architectures like SOAR (Laird, 2012 ) ,
DIARC (Scheutz et al., 2013 ) and ACT-R (Anderson et al., 1997 ) use
classical object category learning and recognition approaches (i.e.,
offline training and online testing are two separated phases), where
open-ended object category learning is generally ignored (Leroux and
Lebec, 2013 ) . Therefore, they work well for specific tasks where there
are limited and predictable numbers of objects and fail at any other
assignment. Unlike our approach, the perceptual knowledge of these
cognitive architectures are static, in the sense that the representation
of the known categories does not change after the training stage.
Therefore, these robots are unable to adapt to dynamic environments
(Jeong and Lee, 2012 ) (Laird et al., 2012 ) . This leads to several
shortcomings such as the inability to detect/recognize new or unknown
categories. To cope with these issues, several cognitive robotics groups
have started to explore how robots could learn incrementally from their
own experiences as well as from interaction with humans.

In the ARMEN project, Leroux et al. proposed a mobile assistive robotics
approach providing advanced functions to help maintaining elderly or
disabled people at home (Leroux and Lebec, 2013 ) . Similar to our
system, this project involves object manipulation, knowledge
representation and object recognition. The authors also developed an
interface to facilitate the communication between the user and the
robot. Jain et al. presented an assistive mobile manipulator named EL-E
that can autonomously pick objects from a flat surface and deliver them
to the users (Jain and Kemp, 2010 ) . They used a multi-step control
policy that is not suitable to achieve real time performance. In our
approach, we can achieve real-time performance through the use of ROS
nodelets and multiplexing mechanisms. Furthermore, in (Jain and Kemp,
2010 ) , the user provides the location of the object to be grasped to
the robot by briefly illuminating a location with a laser pointer. In
our system, objects are detected and recognized autonomously. Therefore
it is enough for the user to specify the category of the object to be
picked up.

In another work (Srinivasa et al., 2008 ) , a multi-robot assistive
system, consisting of a Segway mobile robot with a tray and a stationary
Barrett WAM robotic arm, was developed. The Segway robot navigates
through the environment and collects empty mugs from people. Then, it
delivers the mugs to a predefined position near the Barrett arm.
Afterwards, the arm detects and manipulates the mugs from the tray and
loads them into a dishwasher rack. This work is similar to ours in that
it integrates perception and motion planning for pick and place
operations. However there are some differences: their vision system is
designed for detecting a single object type (mugs), while our perception
system not only tracks the pose of different types of objects but also
recognizes their categories. Furthermore, because there is a single
object type (i. e. mug), they computed the set of grasp points off-line.
In our approach, grasping must handle a variety of objects never seen
before.

In the RACE project (Robustness by Autonomous Competence Enhancement), a
PR2 robot demonstrated effective capabilities in a restaurant scenario
including the ability to serve a coffee, set a table for a meal and
clear a table (Hertzberg et al., 2014 ) (Rockel and et al., 2013 ) . The
aim of RACE was to develop a cognitive system, embodied by a service
robot, which enabled the robot to build a high-level understanding of
the world by storing and exploiting appropriate memories of its
experiences. Other examples of assistive robot platforms that have
demonstrated coupling perception and action include TUM Rosie robot
(Beetz et al., 2011 ) , HERB (Srinivasa et al., 2010 ) and ARMAR-III
(Vahrenkamp et al., 2010 ) .

Willow Garage developed the Object Recognition Kitchen ( ORK ) ¹ ¹ 1
http://wg-perception.github.io/object_recognition_core , a 3D object
recognition system built on top of the Ecto framework ² ² 2
http://plasmodic.github.io/ecto/ . Ecto organizes computation as a
directed acyclic graph, which implies important limitations in the
architecture of the perception system. Moreover, in ORK,
training/learning and detection/recognition are two separate stages.
Such approach is not suitable for developing open-ended learning agents.
In contrast, our system allows for concurrent or interleaved learning
and recognition, and real-time performance is achieved through nodelets
and multiplexing.

### 2.2 The Dual Memory System Approach of RACE

Arguably, robots that interact closely with non-expert users should be:
animate , meaning that they react appropriately to different events,
based on a tight coupling of perception and action; adaptive , to cope
with changing users, tasks and environments, which requires reasoning
and learning capabilities; and accessible , that is, they should be easy
to command and instruct, and they should also be able to explain their
beliefs, motivations and intentions (Seabra Lopes and Connell, 2001 ) .

In an abstract architecture for intelligent robots, as shown in Fig. 2.2
( left ), a Perception component processes all momentary information
coming from sensors, including sensors that capture the actions and
utterances of the user. A Reasoning component updates the world model
and determines plans to achieve goals. An Action component reactively
dispatches and monitors the execution of actions, taking into account
the current plans and goals. Action processing ranges from low-level
control to high-level execution management. Finally, a Learning
component, which typically runs in the background , analyzes the trace
of foreground activities recorded in a Memory component and extracts and
conceptualizes possibly interesting experiences . The resulting
conceptualizations are stored back in memory. Each component in such
abstract architecture decomposes into a set of software modules,
possibly distributed across multiple computers.

The reasoning component manipulates primarily semantic representations
of the current world state, goals and plans, that is, representations
that are symbolic and relational in nature. In RACE, where case studies
were carried out in a restaurant environment, semantic representations
describe tables, chairs, table-top objects, guests, the robot, etc., the
categories of these objects, the relations between them, and the actions
and events that change these relations. The action component includes
multiple modules that control the robot actuators based on a tight
coupling with perception. In addition, the action component carries out
high-level execution management, which consists of reactively
dispatching and monitoring the execution of actions, taking into account
the current plans and goals. Like reasoning, execution management
primarily manipulates semantic representations. The semantic information
flowing between reasoning, execution management and memory is typically
of small size, and its processing tends to be slow (Rockel et al., 2013
) .

One of the challenges in a project like RACE was to combine and store
semantic and perceptual representations. Standard SQL databases do not
cope well neither with semantic data nor with perception data, as both
tend to be partially unstructured and/or of variable size. This suggests
that modern NoSQL databases (Sahib, 2013 ) should be used. Semantic data
represents the world in terms of instances, categories and relations
between them. A semantic representation of the state of the world can be
simply a set of subject-predicate-object triples . A special kind of
database, the triplestore , which shares some features with both SQL and
graph databases, is especially optimized to store information in the
form of a set of triples. Triplestores are clearly one of the database
types to take into account when developing memory systems for robots. An
RDF triplestore was in fact the choice for the initial memory component
in the RACE architecture (Rockel et al., 2013 ) . The contents of this
memory system, which is used as blackboard for all processes, is
semantic in nature. It keeps track of the evolution of both the internal
state of the robot and the events observed in the environment.

Access to the triplestore is granted via a Robot Operating System ( ROS
) node that provides database query and write services for all other
nodes (an interface node). Information exchange is performed using
either publisher / subscriber or client / server mechanisms. ROS
communications are a robust framework (Zaman et al., 2013 ) . However,
when the size of the messages is large (e.g., when passing 3D point
clouds), the communication between processes is slow. In the case of
perception related data, its large size implies large ROS messages to be
passed between the database interface node and the other nodes. This is
a major constraint, especially considering that, unlike semantic data,
perceptual data flows continuously at the sensor output frequency. Using
a database interface node creates a bottleneck for accessing the
database, since it handles access requests in a first in, first out
basis.

Moreover, although triplestores are well suited for storing semantic
information, they can hardly be considered suited for storing perception
data. In fact, the perception modules will primarily process numeric
information organized in structures like vectors and matrices, possibly
grouped in sets. For instance the raw perception data about an object,
after detection, can be a 3D point cloud, which is a set of points
described by their 3D coordinates and possibly RGB information. Based on
the point cloud, shape features can be extracted, and the object can be
represented by a set of local shape features, where each of them can be
a 2D shape histogram. To ensure timely reaction to events in the
environment, perception modules run continuously at the frame rate of
the used sensors. Although raw data tends to be massive
(high-dimensional), the perception modules must run fast, and whatever
memory support they use, must also be lightweight.

In the context of RACE, to accommodate semantic and perceptual
information in the same database, the only option would be to replace
the triplestore with a more generic kind of database. However, we would
loose the special features of triplestores, which are optimized for
storing triples. In alternative, two different databases can be used,
one for semantic information, and the other for perceptual information.
The second alternative, which seems more promissing, allows to use
databases that are well suited for the kinds of data that each will
store. In RACE, we converged to the second option (Hertzberg et al.,
2014 ; Dubba et al., 2014 ; Oliveira et al., 2015b ) .

Figure 2.2 ( right ) shows an abstract architecture diagram in which we
make explicit the dual memory approach. In what concerns reasoning, we
make explicit both interpretation and planning capabilities. One of the
most basic interpretation capabilities is anchoring, i.e., connecting
object symbols used in the semantic memory to the perception of those
objects that is recorded in the perceptual memory. Interpretation also
includes computing spatial relations between objects to keep an updated
relational model of the scene around the robot. In turn, this scene
model can be taken into account for anchoring.

The perceptual memory contains, not only object perception data, but
also object category knowledge, in the form of perceptual categories
that enable to recognize instances of those categories. These perceptual
categories are learned in an open-ended fashion with user mediation
(Chauhan et al., 2013 ; Lim et al., 2014b ) . The perceptual learning
component primarily uses data from perceptual memory (e.g. shape
features of objects) as well as from the semantic memory (e.g. teaching
instructions from the user). In RACE, the implementation of the
perceptual memory was carried out using a flexible and scalable NoSQL
database which operates in memory (see the next section for details).

It is worth emphasizing that, although our design choices were guided
primarily by engineering criteria, we converged to a solution that is
biologically and cognitively plausible. In fact, as previously pointed
out, human memory is not a single monolithic system, but rather a
combination of several memory subsystems specialized for storing
different types of information and supporting different functionalities
(Tulving, 2005 , 1991 ) . In particular, our perceptual memory resembles
the so-called Perceptual Representation Memory System , used in human
cognition for enhancing the identification of objects as structured
physical-perceptual entities, a process referred to as perceptual
priming (Tulving, 1991 ) . Another key distinction in cognitive science
is between processes that are fast, automatic and unconscious, and
processes that are slow, deliberative and conscious (Evans, 2008 ) . Our
dual semantic/perceptual memory approach is also in line with these
findings.

In the following sections, we focus on extending the perceptual
capabilities of the initial RACE architecture (Hertzberg et al., 2014 )
. In particular, two different system architectures are presented as
depicted in Fig. 2.3 and Fig. 2.5 . Both architectures are reusable
frameworks and developed over ROS (Quigley et al., 2009 ) . Each
software module is organized into a ROS package and will typically
correspond to a node or a nodelet ³ ³ 3 http://wiki.ros.org/nodelet at
runtime. Information exchange is performed using standard ROS mechanisms
(i.e., either publish / subscribe or server / client). Therefore, any
new module can be easily added to the system. Each of these
architectures will be discussed in detail in the following sections.

### 2.3 RACE Perception System

The overall RACE perception and perceptual learning system is depicted
in Fig. 2.3 . From a global perspective, the RACE perception system is
composed of six functional components: Object Detection , Multiplexed
Object Perception , User Interface , Reasoning and Interpretation ,
Memory and Conceptualization . The implementation of the Perceptual
Memory was carried out using LevelDB , a lightweight, flexible and
scalable NoSQL database developed by Google ⁴ ⁴ 4
https://code.google.com/p/leveldb/ . LevelDB is a key-value storage
database that provides an ordered mapping from string keys to string
values. In addition, LevelDB operates in memory and is copied to the
file system asynchronously. This significantly improves its access
speed.

In the RACE system, an RGB-D sensor is used for the perception of both
the user and the table-top scene. The starting point for the perception
of the table-top scene is Table-Top Segmentation (TTS) ⁵ ⁵ 5
http://wiki.ros.org/tabletop_object_detector , which uses a hierarchical
clustering procedure to isolate (partial) point clouds of the objects.
The Object Detection (OD) module periodically requests the current list
of objects from TTS . Then, OD will check if any of those objects is
already being tracked. To do this, OD matches the point clouds of all
objects on the table with the bounding boxes of all objects currently
being tracked. The percentages of points of the tabletop objects that
lie inside the bounding boxes of the tracked objects are computed. A
large percentage indicates that the tracked object and the segmented
object are the same. Point clouds that cannot be matched with any of the
tracked bounding boxes are assumed to represent new objects just added
to the scene. OD will assign a new identifier ( track-id ) to each newly
detected object. Also for each new object, OD will launch an object
perception pipeline which contains three modules: Object Tracking ,
Feature Extraction and Object Recognition . Figure 2.4 shows a situation
where two objects are segmented and tracked, i.e., they have bounding
boxes around them.

Object Tracking (OT) is responsible for keeping track of the target
object over time while it remains visible. Tracking is an essential base
for anchoring. On initialization, OT receives the point cloud of the
detected object and computes a bounding box for that point cloud, the
centre of which defines the pose of the object. A particle filter
approach is then used ⁶ ⁶ 6
http://www.willowgarage.com/blog/2012/01/17/tracking-3d-objects-point-cloud-library
to predict the next probable pose of the object. In each cycle, OT sends
out the tracked pose of the object both to OD and to the Interpretation
component. At a lower rate, OT sends the point cloud of the object
(i.e., containing the points inside the predicted bounding box) to
Feature Extraction.

The Feature Extraction (FE) module computes and stores object
representations in the perceptual memory. Objects are represented by
sets of local shape features computed in certain keypoints. In addition
to storing object representations in the perceptual memory, FE also
sends them to Object Recognition (OR) . The perceptual categories
learned so far and stored in the perceptual memory are used by OR to
predict the category of the target object. OR is a low frequency module,
which runs at 1 Hz. Accordingly, FE receives object point clouds from OT
and sends the extracted representations for recognition at the same
frequency. Thus, only OT itself uses object point clouds at the frame
rate of the sensor (30 Hz).

Object recognition results are written to the perceptual memory, where
the Interpretation component can fetch them to support symbol anchoring.
Anchoring involves keeping track of objects even when they cannot be
visually tracked. Suppose that an object with track-id =7 disappears
from the visible the scene. Then, after some time, an object becomes
visible in the same location and starts being tracked as track-id =8. In
such case, high-level interpretation may infer that both identifiers
refer to the same object, so it will associate both to the same object
symbol in the semantic memory. The current implementation is capable of
anchoring symbols that refer to objects only while these remain visible.
However, further work in the context of RACE project has been done to
enable anchoring object symbols when the visual tracking is lost, but
this functionality is out of the scope of this thesis, and will not be
further discussed.

In the RACE perception architecture, pointing gesture recognition and
verbal teaching of object categories support open-ended category
learning. Verbal input is currently provided through interactive markers
in RVIZ , a 3D visualization tool for ROS. The Skeleton Tracker (ST)
module tracks the user skeleton pose over time based on RGB-D data ⁷ ⁷ 7
http://wiki.ros.org/openni_tracker . The skeleton pose information is
passed to the Gesture Recognition (GR) module, which computes a pointing
direction. Currently, the pointing direction is assumed to be the
direction of the right forearm (see an example in Fig. 2.4 ). The
pointing direction is then passed to the Interpretation component. Upon
receiving verbal input, the Interpretation component checks if the
received pointing direction intersects the bounding box of any of the
objects currently on the table according to the world state recorded in
the semantic memory. If that is the case, then a teaching instruction is
recorded in the semantic memory, stating that the target object was
taught to belong to the given category. Teaching instructions trigger
perceptual learning to create and/or update object categories.

### 2.4 Addressing Computational Issues

In contrast with the reasoning processes supported by the semantic
memory, the processes developed around the perceptual memory must run
fast to cope with the continuous stream of massive sensor data. As
pointed out, one of the reasons for using LevelDB to implement the
memory systems is the fact that it operates in RAM. There is, however,
the limitation that simultaneous access to LevelDB is only possible by
threads within the same process. To comply with this constraint while
keeping ROS as the framework for the newly developed modules, we use ROS
nodelets ⁸ ⁸ 8 http://wiki.ros.org/nodelet . Nodelets, which run as
threads of a single process, were designed to provide a way of
concurrently running different modules with zero copy transport between
publisher and subscriber calls (as an example, see Munaro et al. ( 2013
) ). The motivation for ROS nodelets comes from systems with high
throughput data flows as is common in perception systems. It is not
surprising, therefore, that the developers of Point Cloud Library (PCL)
and ROS nodelets are the same. In our system, in addition to handling
high throughput data flows, nodelets come handy to implement modules
that need to simultaneously access the perceptual memory (LevelDB).

Another way of optimizing perception is to parallelize computations. In
our system, instead of tracking all objects in a single tracking module,
there is a tracker for each object. Similar strategy is used for feature
extraction and object recognition. In other words, object perception is
designed to be multiplexed. Every time a new object is detected, a
corresponding instance of the object perception pipeline (see Fig. 2.3
and Fig. 2.5 ) is launched. Thus there are as many object perception
pipelines as the number of currently tracked objects, and each pipeline
targets a specific object. Since the modules in an object perception
pipeline run as independent nodes/nodelets, they can be distributed to
different CPU cores, thus improving the overall computational efficiency
of perception. Note that the three modules in the object perception
pipeline are traditionally amongst the heaviest in terms of
computational requirements. The parallelization is aimed at the hotspot
or bottleneck of the computation flow and takes full advantage of modern
multi-core machines. In fact, experiments with a non-multiplexed version
of this architecture show that it cannot run in real-time.

We can easily configure the perception and perceptual learning modules
to be launched with different runtime configurations, that is, using ROS
nodelets only, ROS nodes only, or a combination of both. By default, the
object perception pipelines, the perceptual learning module and the
perceptual memory run as a set of nodelets of a single process. When
debugging is necessary, we use a configuration where all modules run as
nodes. In this configuration, the modules access the perceptual memory
using ROS services provided by a database interface.

### 2.5 Coupling Object Perception with Manipulation

Figure 2.5 provides an illustration of a cognitive architecture designed
to create a tight coupling between perception and manipulation for
assistive robots (Kasaei et al., 2016e ) . This architecture is an
evolution of the RACE perception architecture described in the section
2.3 (Hamidreza Kasaei et al., 2014 ; Oliveira et al., 2015b ) . Although
both architectures support an interactive open-ended learning for 3D
object category recognition, their complexity and performance differ
depending on the characteristics of modules and the methods used for
learning and classification. In the RACE architecture, we mainly use a
local 3D shape descriptor for object representation and an
instance-based object category learning, while in this architecture, we
employ more advanced object representation techniques such as
bag-of-words (BoW) and Latent Dirichlet Allocation (LDA), to represent
objects in uniform and compact format which is suitable for both
instance-based and model-based object category learning. Moreover, in
this architecture a proper coupling between object perception and
manipulation is provided. As mentioned before, object manipulation tasks
consist of two phases: the first is the perception of the object and the
second is the planning and execution of arm or body motions which grasp
the object and carry out the manipulation task. These two phases are
closely related: object perception provides information to update the
model of the environment, while planning uses this world model
information to generate sequences of arm movements and grasp actions for
the robot (Kasaei et al., 2018b ) .

This cognitive architecture includes two memory systems, namely the
Working Memory and the Perceptual Memory . Both memory systems have been
implemented using LevelDB. The Working Memory is used for temporarily
storing information as well as for communication among different
modules. It keeps track of the evolution of both the internal state of
the robot and the events observed in the environment (i.e., world
model). The object features, dictionary of visual words, object
representation data and object category models are stored into
Perceptual Memory . The goal of Grasp Planning is to extract a grasp
pose (i.e., a gripper pose relative to the object) either from above or
from the side of the object, using global characteristics of the object.
The Execution Manager works based on a Finite-State-Machine (FSM)
paradigm. It retrieves the task plan and the world model information
from Working Memory and computes the next action (i.e., a primitive
operator) based on the current context. Then, it dispatches the action
to the robot platform as well as records success or failure information
in the Working Memory .

Whenever the robot captures a scene, the first step is preprocessing
which includes three filtering procedures, namely distance filtering, a
filter to remove the robot’s body from sensor data, and a downsampling
filter for reducing the size of the data. Object Detection , responsible
for detecting objects in the scene, launches a new perception pipeline
for each detected object. Each pipeline includes Object Tracking ,
Feature Extraction , Object Representation and Object Recognition
modules. The Object Tracking module estimates the current pose of the
object based on a particle filter, which uses shape and color data
(Oliveira et al., 2014b ) . The Feature Extraction module extracts
features of the current object view and stores them in the Perceptual
Memory . Based on the extracted features and on a visual dictionary, the
Object Representation module describes objects as histograms of visual
words/topics and stores them into the Perceptual Memory . User
Interaction is essential for supervised experience gathering.
Interaction capabilities are developed to enable human users to teach
new object categories and instruct the robot to perform complex tasks
(Lim et al., 2014a ) .

The proposed architecture, as shown in Fig. 2.5 , includes two
perceptual learning modules. One of them, the Dictionary Builder , is
concerned with building a dictionary of visual words for object
representation. The dictionary plays a prominent role because it is used
for category learning as well as recognition. The second learning module
is the Object Conceptualizer . Whenever the instructor provides a
category label for an object, the Conceptualizer retrieves the
probabilistic models of the current object categories as well as the
representation of the labeled object in order to improve an existing
object category model or to create a new category model. In recognition
situations, a probabilistic classification rule is used to assign a
category label to the detected object. The system is run in two stages.
The first stage is dedicated to environment exploration, which will be
further discussed in the next chapter. In this stage, unsupervised
object discovery is carried out in the environment while the robot
operates. The robot seeks to segment the world into “object” and
“non-object”. Afterwards, a pool of shape features is created by
computing local shape features for the extracted objects. The pool of
features is then clustered by the Dictionary Builder leading to a set of
visual words (dictionary). Only the modules directly involved in object
discovery and dictionary building are active in this stage. The second
stage corresponds to the normal operation of the robot, with object
category learning, recognition, planning and execution. In the following
chapters, the characteristics of each module are explained in detail.

In this framework, task planning is triggered when a user instructs the
robot to achieve a task (e.x. clear_table ). This is handled by the User
Interaction module. Figure 2.6 shows a schematic representation of the
planning and execution framework. The current state of the system,
including world model information, global characteristics of the object
of interest (i.e., overall shape, main axis, center of bounding box) and
robot pose is retrieved from the working memory. Then, a task plan would
be generated. A plan is a sequence of primitive operators to be
performed to achieve the given goal. It should be noted that Task
Planning is not in the scope of this thesis. Previously, we showed how
to conceptualize successfully executed task plans and how to use these
conceptualized experiences for task planning (Mokhtari et al., 2016 ) .
In the present work, a predefined task plan is used. In order to be
executed, a task plan must be complemented with end-effector poses. A
pose is represented as a tuple @xmath , specified relative to the base
reference frame of the robot.

The Grasp Planning module receives the task plan and chooses a grasp
point either from above or from the side as well as a pre-grasp pose
using the world model information and global characteristics of the
object. In the current setup, the pre-grasp pose is placed at a fixed
distance ( @xmath ) behind or above the center of bounding box of the
object. The intuition behind this assumption is that many domestic
objects are graspable by aligning grippers with the principal axes of
the object (Ciocarlie et al., 2014 ) (Stückler et al., 2013 ) .
Afterwards, the Execution Manager retrieves the plan and grasp
information from the Working Memory . The Execution Manager uses a Fine
State Machine to reactively execute the plan. The actions are dispatched
to the Robot Capabilities module. Inverse kinematics and safe
controller, integrated from the JACO arm driver, are used to transform a
given end-effector pose goal into joint-space goals. It should be noted
that, a discussion about the grasp approaches is out of the scope of
this thesis. For interested readers, we provide a detailed discussion
and evaluation about grasping capabilities in (Kasaei et al., 2016e ) .
In another work, we proposed an advanced grasping approach to learn how
to grasp familiar objects using interactive object view labeling and
kinesthetic grasp teaching (Shafii et al., 2016 ) .

Whenever the object is grasped, the height of the robot’s end-effector
relative to the robot’s base is recorded into Working Memory and it is
used as the desired height for placing the grasped object. The Execution
Manager computes a new trajectory to navigate the robot’s end-effector
to the placing area and sends out the action. After executing each
action, the current state of the robot is updated in the Working Memory
. Since world model information is updated by different modules (i.e.,
Object Detection , Execution Manager and etc.), the Execution Manager
can abort execution when an unpredictable situation happens along
expected execution path such as new obstacles move into the planned path
of the robot arm. It should be noted that an orientation constraint on
the end-effector is used to grasp and move an object parallel to the
support plane. In addition, objects outside of the arm’s workspace are
not considered. Figure 2.7 illustrates the result of a constrained pick
and place plan executed on the robot.

### 2.6 Summary

In this chapter, we have presented two system architectures designed to
enhance a proper perception for a complex artificial cognitive agent
working in a restaurant scenario. In particular, we provide a thorough
description of the developed system, starting with motivations,
cognitive considerations and then architecture design. Both
architectures integrate detection, tracking, learning and recognition of
tabletop objects. Interaction capabilities were also developed to enable
a human user to take the role of instructor and teach new object
categories. Thus, the system learns in an incremental and open-ended way
from user-mediated experiences. In RACE architecture, based on the
analysis of memory requirements for storing both semantic and perceptual
data, a dual memory approach, comprising a semantic memory and a
perceptual memory, was adopted. The second architecture enables robots
to adapt to different environments and reason out how to behave in
response to the request of a complex task such as clear_table . We have
also tried to make the proposed architectures easy to integrate on other
robotic systems. In the next chapter, we will discuss the process of
gathering object experiences in both supervised and unsupervised manner.

## Chapter 3 Object Perception and Experience Extraction

An autonomous robot typically uses a perception system to perceive the
world. The perception system provides a set of important information
that the robot has to use for interacting with users and environments. A
robot needs to know which kinds of objects exist in a scene and where
they are. Therefore, both object detection and recognition are important
and difficult tasks due to the dynamic nature of the world. Some robots
mainly use a RGB camera to perceive the world. RGB data is not suitable
for acquiring complete information of the world due to the fact of
encoding 3D world by 2D images (Philipona et al., 2003 ) . Moreover,
environmental changes such as light, shadows and reflections complicate
2D detection approaches. To cope with these limitations, several
constraints are usually considered (e.g. uniform colored backgrounds or
less clutter). These kinds of constraints obviously reduce the
applicability of the entire system to work robustly in a real world
environment.

Following the recent release of inexpensive 3D sensing devices such as
Microsoft Kinect ¹ ¹ 1 http://www.xbox.com/en-US/kinect and ASUS Xtion ²
² 2 http://www.asus.com/Multimedia/Xtion_PRO/ , which record RGB and
depth information, 3D object detection has become a widespread research
topic. We believe involving spatial depth information facilitates the
detection of objects in domestic environments. In 3D space, objects are
more likely to be correctly detected due to their spatial and geometric
properties. Nevertheless RGB-D sensors have some drawbacks such as the
inability of detecting transparent objects and distorted sensor readings
of reflective and dark surfaces.

Real 3D scenes generally consist of several objects present in a scene.
It is a challenging task to robustly detect multiple objects in a
domestic environment due to severe occlusions and clutter. The
environment can be highly crowded and cluttered . Clutter is seen when
points that do not belong to the target object are included in the
segmentation. Occlusion and self-occlusion can lead to only a part of an
object being visible in certain views. Moreover, some captured data can
be inaccurate or missing due to sensor noise .

Throughout this chapter, we try to impose as few constraints as possible
for the object detection. We assume objects are situated on a planar
surface, as this is the common pose of objects in domestic environments
and transparent objects like glasses are not considered; but we do not
consider any other assumptions about the object appearance. In this
chapter, we will propose automatic perception capabilities that will
allow robots to, ( i ) automatically detect multiple objects in a
crowded scene; ( ii ) incrementally accumulate object experiences in
both supervised and unsupervised manner; and ( iii ) Next-Best-View
(NBV) prediction algorithm to predict the next best camera pose for
object detection by rendering virtual scenes based on current object
hypotheses.

All the work presented in this chapter has appeared in conference and
journal papers (Hertzberg et al., 2014 ; Hamidreza Kasaei et al., 2014 ;
Oliveira et al., 2015b ; Kasaei et al., 2016e , 2018b ; Sock et al.,
2017 ; Kasaei et al., 2018c ) . The remainder of the chapter is
organized as follows: the first section is dedicated to the related
work. In section 3.2 , the pre-processing of raw sensory data is
explained. Section 3.3 describes in detail the object detection
methodologies. Unsupervised and supervised experience gathering are
explained in sections 3.4 and section 3.5 respectively. An approach for
online object model construction will be discussed in section 3.6
followed by the Next Best View prediction approach. Summary is then
presented in section 3.8 .

### 3.1 Related Work

Many approaches have been developed for gathering online object
experiences based on both supervised and unsupervised algorithms
(Kirstein et al., 2012a ; Oliveira et al., 2015b ) . In supervised
methods, an object experience is collected when an instructor provides a
category label for an object. These methods provide an opportunity to
collect interactively object experiences (e.g., visual observations) for
learning. Kirstein et al. ( 2012a ) proposed a lifelong learning
approach for interactive learning of multiple categories based on vector
quantization and an user interface. Similar to our work, Chauhan and
Seabra Lopes ( 2011 ) approached the problem of object experience
gathering and category learning with a focus on open-ended learning and
human-robot interaction. They used RGB data whereas we used depth data.
Therefore, their object detection and representation approaches are
completely different from our approach. Collet et al. ( 2014 ) proposed
a graph-based approach for lifelong robotic object discovery. Similar to
our approach, they used a set of constraints to explore the environment
and to detect object candidates from raw RGB-D data streams. In
contrast, their system does not interactively acquire more data to learn
and recognize the object. Steels and Kaplan ( 2000 ) used the notion of
“language game” to develop a social learning framework through which a
robot can learn its first words. A teacher points to objects and
provides their names. The robot uses color histograms and an
instance-based learning method to learn word meanings. The mediator can
also ask questions and provide feedback on the robot’s answers. They
conclude that social interaction must be used to help the learner focus
on what needs to be learned in the context of communication. We take
inspiration from this work, since our system also employs the three core
instructions teach , ask , correct . Lopes and Chauhan ( 2007 ) also
developed a vocabulary acquisition and category learning system that
integrates the user as instructor. The user can provide the names of
objects through pointing and verbal teaching actions. The user can also
ask questions about the categories of objects under shared attention
and, if appropriate, provide corrective feedback. In this work the
teaching was limited to object names.

Object detection and pose estimation are also crucial for robotics
applications and recently attracted attention of the research community
(Sock et al., 2017 ; Tejani et al., 2014 ) . Many researchers
participated in public challenges such as Amazon picking challenge ³ ³ 3
https://www.amazonrobotics.com to solve multiple objects detection and
pose estimation in a realistic scenario. This shows that object
detection research is moving towards more realistic robotics
environment. Traditional approaches have mostly been well studied in
known object scenarios. For instance, Doumanoglou et al. ( 2016 ) used
sparse autoencoder to represent each patch and classified using random
forest. Tejani et al. ( 2014 ) used LINEMOD (Hinterstoisser et al., 2011
) to represent each patch and also used random forest and Hough voting
to generate hypotheses. In these approaches, the robot is expected to
encounter the same objects it was trained on.

A more realistic and challenging setting is when the objects that the
robot will encounter were not learned in advance. Towards this goal,
Mason et al. ( 2014 ) proposed a robotic system for unsupervised object
and class discovery, in which object candidates are first discovered
from several scenes, and then grouped into classes in an unsupervised
fashion. Similar to our work, this approach first segments the world
into “object” and “non-object” components, and then performs data
association between the objects. Kang et al. ( 2011 ) presented an
unsupervised object discovery approach based on combining a hierarchical
over-segmentation with visual information. Kang et al. ( 2011 ) and
Karpathy et al. ( 2013 ) proposed methods for discovering object models
from 3D point clouds of indoor environments. Similar to our approach,
the focus of these works is on identifying portions of a scene that
could correspond to objects (i.e., scene segmentation) for the purposes
of object recognition, semantic understanding or robotic manipulation.

A robot operating in human environments may frequently encounter with a
pile of objects such as a pile of toys in the living room, tidying up a
messy dinning table, and multiple unused objects stacked in a box in the
garage. Object detection and object pose estimation in such environments
are challenging tasks due to severe occlusions and clutter. Several
strategies, including active exploration (Doumanoglou et al., 2016 ;
Mauro et al., 2014 ) and interaction with piles (Katz et al., 2013 ,
2014 ) have been proposed to overcome pile segmentation issues.

In active perception scenarios, whenever the observer fails to recover
the poses of objects from the current view point, the observer will
estimate the next view position and capture a new scene image from that
position to improve the knowledge of the environment. This will reduce
the object detection and pose estimation uncertainty. Towards this end,
Mauro et al. ( 2014 ) proposed a unified framework for content-aware
next best view selection based on several quality features such as
density, uncertainty, 2D and 3D saliency. Using these features, they
computed a view importance factor for a given scene. Unlike this
approach, we first segment a given scene into object hypotheses. Then,
the next best view is predicted based on the properties of those object
hypotheses. In another work, Biasotti et al. ( 2013 ) approached the
problem of defining the representative views for a single 3D object
based on visual complexity. They proposed a new method for measuring the
viewpoint complexity based on entropy. Their approach revealed that it
is possible to retrieve and to cluster similar viewpoints. Doumanoglou
et al. ( 2016 ) uses class entropy of samples stored in the leaf nodes
of Hough forest to estimate the Next-Best-View which could reduce the
uncertainty of the class of detected objects.

Some researchers have recently adopted deep learning algorithms for next
best view prediction in active object perception (Wu et al., 2015 ;
Johns et al., 2016 ) . For instance, Wu et al. ( 2015 ) proposed a deep
network namely 3D ShapeNets to represent a geometric shape as a
probabilistic distribution of binary variables on a 3D voxel grid. As
they pointed out, training a deep network for next best view prediction
requires a large collection of 3D objects to provide accurate
representations and typically involves long training times. Moreover,
unlike our approach, these kinds of approaches are mainly suitable for
isolated objects and become brittle and unreliable in crowded scenarios.

Object manipulation is also useful for a robot to discover and segment
objects in cluttered environments (Katz et al., 2013 , 2014 ) . Van Hoof
et al. ( 2014 ) presented a part-based probabilistic approach for
interactive segmentation. They tried to minimize human intervention in
the sense that the robot learns from the effects of its actions, rather
than human-given labels. In another work, Gupta and Sukhatme ( 2012 )
explored manipulation-aided perception and grasping in the context of
sorting small objects on a tabletop. They presented a pipeline that
combines perception and manipulation to accurately sort the bricks by
color and size. This topic is out of the scope of this thesis and an
interesting topic for further research.

### 3.2 Pre-Processing

Processing massive point clouds is one of the main challenges of 3D
perception systems. In dense 3D point cloud data, considering all points
is computationally too expensive, and real-time processing is not
possible. The key idea for fast processing of massive point clouds is to
use mechanisms for removing unnecessary or irrelevant data. To
accomplish this, we use two separate filters that discard vast
quantities of unnecessary 3D points. The first filter uses a cubic
volume to define the region of interest. Experimental results provided
by Khoshelham ( 2011 ) have shown that the reliable and useful data are
located in a cubic area with almost two meters length on each side. In
our current setup, we use a table which is approximately one meter away
from the camera / robot. Using this information, we define the size of
the cubic volume to include a typical table in front of the robot. We
also filter out undefined or unrepresentable (NaN) points. Figure 3.1
shows an example of this process. In Fig. 3.1 (left) , the complete
point cloud is shown, along with the cube. This filter enables a
significant reduction of the number of points.

The second filter reduces the spatial resolution of the point cloud,
since our approach does not require the full resolution of the sensor.
To do this, the point cloud is down sampled using a voxelized grid
approach ⁴ ⁴ 4
http://pointclouds.org/documentation/tutorials/voxel_grid.php . The
advantage of this, apart from the fact that the number of points is
further reduced, is that the spatial distribution of 3D points becomes
uniform. In Fig. 3.1 (right) , the filtered point cloud is displayed.
Furthermore, as shown in Fig. 3.2 , the points corresponding to the body
of the robot are filtered out from the original point cloud by
retrieving the knowledge of the positions of the arm joints relative to
the camera pose from the working memory.

### 3.3 Object Detection and Tracking

After pre-processing, the next step is to find objects in the scene. Our
approach assumes that objects are placed on top of a planar surface
(e.g., a table) in order to be detected. The planar surface is detected
by finding the dominant plane in the point cloud. This is done using the
RANSAC algorithm (Fischler and Bolles, 1981 ) . The algorithm starts by
generating plane hypotheses based on three unique non-collinear points.
For each plane hypothesis, distances from all points in the point cloud
to the plane are computed. The plane hypotheses are then scored based on
counting the number of points whose distance to the plane falls below a
user-specified threshold, @xmath . The RANSAC algorithm is repeated for
a certain number of iterations, @xmath . In the current implementation,
@xmath and @xmath . An example of the proposed table detector algorithm
is illustrated in Fig. 3.2 (left) . With the table detected, it is now
possible to extract the points which lie directly above it. The
mechanism we use to do this is called extraction of polygonal prisms ⁵ ⁵
5
http://docs.pointclouds.org/1.0.0/classpcl_1_1_extract_polygonal_prism_data.html
. After this, we have a point cloud where all the objects that are on
top of the table are included. Therefore, the extracted points are
assumed to potentially belong to objects. The obtained point cloud is
then segmented into individual clusters using the Euclidean Cluster
Extraction algorithm ⁶ ⁶ 6
http://www.pointclouds.org/documentation/tutorials/cluster_extraction.php
. Each small group of points will be treated as an object candidate. An
example of the object detector module is shown in Fig. 3.2 . (right) ,
where four objects are on top of the table. In Fig. 3.2 (right) , the
segmented object point clouds are shown. Note that the point clouds of
each object have different colors, meaning that they have been correctly
segmented. The object detection uses a size constraint, @xmath , to
detect objects which can be manipulated by the robot.

In case of multiple objects touching each other (e.g., in a pile of
objects or a messy dinning table), the Euclidean cluster extraction
algorithm is not enough to appropriately detect object candidates and
further processing is required (see Fig. 3.3 ). It is a challenging task
to robustly detect multiple objects stacked in a pile or in a box, which
are often found in a domestic environment. In our current setup, a
hierarchical clustering procedure is presented to segment the extracted
point cloud using geometric, surface normal data and color. A region of
the given point cloud is considered as an object candidate whenever
points inside the region are continuous in both the orientation of
surface normals and the depth values. The depth continuity between every
point and its neighbors is computed. If the distance between points is
lower than a threshold, then the two points belong to the same region. A
region growing segmentation algorithm ⁷ ⁷ 7
http://pointclouds.org/documentation/tutorials/region_growing_segmentation.php
is also applied on medium-size hypotheses. The purpose of this algorithm
is to merge the points that are close enough in terms of the smoothness
and color constraints.

Each cluster of points will be treated as an object candidate namely
@xmath , where @xmath . It should be noted that the number of clusters,
@xmath , is not pre-defined and varies for different viewpoints. Fig.
3.4 illustrates the results of the segmentation process in four
different pile scenarios.

As discussed in the previous chapter, the Object Detection module
assigns a new TrackID to each newly detected object and launches an
object perception pipeline for the object. Finally, the object detection
pushes the segmented object candidate into the respective pipeline for
subsequent processing steps. The Object Tracking module is responsible
for keeping track of the target object over time while it remains
visible. The object tracker works based on a particle filter (Schulz
et al., 2001 ; Hertzberg et al., 2014 ) which uses geometric information
as well as color and surface normal data to predict the next probable
pose of the object. It receives the point cloud of the detected object
and computes an oriented bounding box aligned with the point cloud’s
principal axes. The center of the bounding box is considered as the
position of the object. The module sends out the tracked object
information to the Feature Extraction module. The detail of object’s
reference frame construction and feature extraction will be presented in
the next chapter. We qualitatively report the performance of object
detection on different scene datasets including Washington RGBD Scenes
Dataset v2 (Lai et al., 2014 ) and Imperial College London Bin-Picking
Datasets (Doumanoglou et al., 2016 ) . Washington RGB-D Scenes Dataset
v2 dataset consists of @xmath scenes containing a subset of the objects
in the RGB-D Object Dataset. As depicted in Fig. 3.5 , all objects have
been correctly detected in all scenes.

### 3.4 Unsupervised Experience Gathering

Gathering object experiences by exploration has the advantage of not
requiring any human annotation of individual objects. Non goal-directed
exploration provides chances to discover new objects. In general, object
exploration is a challenging task because of the dynamic nature of the
world and ill-definition of the objects (Collet et al., 2014 ) .

Since a system of boolean equations can represent any expression or any
algorithm, it is particularly well suited for encoding the world and
object candidates. Similar to Collet’s work (Collet et al., 2014 ) , we
use boolean algebra. A set of boolean constraints, C, was then defined
based on which boolean expressions, @xmath , were established to encode
object candidates for the process of constructing a pool of object
candidates as well as for interactive object category learning and
recognition (see Table 3.1 ). The definition of “object” in the stage of
unsupervised experience gathering (i.e., exploration stage) is more
general than in the normal operation stage (see equations 3.1 and 3.2 ).
In both cases, we assume that interesting objects are on horizontal
planar surfaces (i.e., tables) and the robot seeks to detect tabletop
objects (i.e., @xmath ). Due to memory size concerns, a representation
of an object should only contain distinctive views. Key object views are
selected by the Object Tracking module. A view which is different from
the current view may appear after the object is moved (i.e., the pose of
the object relative to the sensor changes). An object view is selected
as a key view (i.e., @xmath ) whenever the tracking of an object is
initialized ( @xmath ), or when it becomes static again after being
moved and the user’s hands are far away from the object (Lim et al.,
2014a ) . Therefore, the @xmath constraint is used to optimize memory
usage and computation while keeping potentially relevant and distinctive
information. Moreover, @xmath and @xmath are used to filter out object
candidates which are part of the instructor’s body or robot’s body.
Accordingly, the resulting object candidates are less noisy and include
only data corresponding to the environment:

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

The object detection expression uses a size constraint, @xmath , to
detect objects which can be manipulated by the robot. Moreover, a @xmath
constraint is considered to filter out the segmented point clouds that
are too close to the edge of the table due to the safety concern.

For unsupervised experience gathering, every cluster that satisfies the
exploration expression, @xmath , is selected. The output of object
exploration is a pool of object candidates that can be used for the
process of constructing the dictionary of visual words. The subject of
visual word dictionary construction will be discussed in detail in the
next chapter. In the context of the RACE project (Hertzberg et al., 2014
) , the University of Osnabruck provided us with a rosbag collected by
one of their robots while exploring an office environment. The
exploration stage was run on this rosbag. A video of this exploration is
available at: http://youtu.be/MwX3J6aoAX0 .

### 3.5 Supervised Experience Gathering

Human-robot interaction is essential for supervised experience
gathering, i.e., for humans to teach robots how to perform different
tasks. Particularly, open-ended object category learning will be faster
and more robust if it is able to learn new categories using the feedback
of human users. In this section, a user interface for supervised
experience gathering is presented. The interface is used not only for
teaching new object categories in situations where the robot encounters
with new objects but also for providing corrective feedback in case
there is a misclassification (Chauhan et al., 2013 ; Lim et al., 2014a )
. Moreover, interaction capabilities were developed to enable human
users to instruct the robot to perform complex tasks such as clear_table
and serve_a_meal (out of the scope of this chapter).

The User Interaction module provides a graphical menu to facilitate the
collection of supervised object experiences. Two alternative
interactions mechanisms are supported: gesture recognition or the usage
of a graphical menu interface. In the first case the instructor points
to an object and then selects the desired label from a menu that appears
in front of the table in the 3D visualization of the scene. The Skeleton
Tracker (ST) module tracks the user skeleton pose over time based on
RGB-D data ⁸ ⁸ 8 http://wiki.ros.org/openni_tracker . The skeleton pose
information is passed to the Gesture Recognition (GR) module, which
computes a pointing direction. Currently, the pointing direction is
assumed to be the direction of the right forearm. To produce a valid
object category label, the pointing gesture must co-occur with the
object labeling via the menu. The User Interaction module contains a
global state of the scene. Upon receiving category label input, it
checks if the received pointing direction intersects the bounding box of
any of the objects and decides if an object is being pointed at by the
instructor. If that is the case, teaching instructions trigger
perceptual learning to create and / or update object categories.

An example of object labelling is depicted in Fig. 3.7 . The instructor
puts a ‘Vase’ on the table. Tracking is initialized with TrackID 1. The
gray bonding box signals the pose of the object as estimated by the
tracker. TrackID 1 is classified as ‘Unknown’ because vases are not yet
known to the system; the instructor points at the object. The system
detects the pointing gesture and the corresponding menu is activated.
The instructor labels the object as ‘Vase’ , which activates the Object
Conceptualizer (category learning) module.

As an alternative, the instructor can select the category label for an
object based on its TrackID . This capability is currently provided
through interactive markers in RVIZ, a 3D visualization tool for ROS. In
addition, the User Interaction module provides a task menu that is used
to instruct the robot to perform a task or to abort the current task.
Further details on the supervised object experience gathering developed
for the RACE project are available in (Lim et al., 2014a ) .

### 3.6 Online Object Model Construction

In this section, we develop an approach to autonomously construct models
of unknown objects. This capability is necessary for cognitive robots,
since it will allow robots to actively investigate their environments
and learn about objects in an unsupervised and incremental way. Online
construction of full surface models of objects is not only useful for
improving object recognition performance by collecting several views,
but also can be used for manipulation purposes.

As shown in Fig. 3.8 , our approach enables a robot to move around an
object and build an increasingly complete 3D model of the object by
extracting object’s points from different perspectives and aligning them
together by considering the tracked object pose and robot pose as well
as geometrical and visual information. In such scenario, tracking the
target object is necessary since many objects in everyday environments
exhibit rotational symmetries or lack a distinctive geometry for
matching. As stated by Krainin et al. ( 2010 ) , without pose
information, Iterative Closest Point (ICP) based approaches are unable
to recover the proper transformations because of the ambiguity in
surface matching. Towards this goal, we employ a Kalman filter that uses
depth and visual information for keeping track of robot motion and the
target object while it remains visible over time. Afterwards, the
extracted object views are united using an incremental ICP ⁹ ⁹ 9
http://pointclouds.org/documentation/tutorials/pairwise_incremental_registration.php#pairwise-incremental-registration
approach (Pomerleau et al., 2013 ) that incorporates both tracking and
appearance information.

It should be noted that this approach cannot provide information about
regions of the target object that were not visible throughout the
exploration procedure. Moreover, since the robot localization is out of
the scope of this work, we use noisy ground truth information and showed
that this approach can compensate the noise of robot motion and generate
proper models of household objects. A video showing the robot exploring
an environment for constructing a full model of an Amita juice box is
available at: https://youtu.be/CuBS2L2q5NU

### 3.7 Next-Best-View Prediction

The ability to predict the Next-Best-View (NBV) point is important for
mobile robots performing tasks in everyday environments. In active
scenarios, whenever the robot fails to detect or manipulate objects from
the current view point, it is able to predict the next best view
position, goes there and captures a new scene to improve the knowledge
of the environment. This may increase the object detection and
manipulation performance (see Fig. 3.9 ). Towards this end, we proposed
an entropy-based NBV prediction algorithm by rendering the scene using
the current object hypotheses ¹⁰ ¹⁰ 10 This work was done in
collaboration with Imperial Computer Vision and Learning Lab at Imperial
College London (Sock et al., 2017 ) .

In this step, the robot captures a point cloud of the scene and computes
a list of object hypotheses containing both objects’ 6D pose and
recognized label (i.e., object recognition and pose estimation will be
discussed in the next chapters). The inputs to the NBV prediction module
are:

-   the constructed 3D models of the objects;

-   the point cloud of the scene;

-   a set of 6D object hypotheses @xmath ;

-   and a possible set of viewing poses, @xmath where each viewing poses
    represents the camera rotation and translation in 3D space.

The given scene is first segmented and the obtained clusters are then
used to compute viewpoint entropy for the given scene. There are various
methods for computing the viewpoint entropy. In general, the number of
visible voxels or points is used as an indicator of area for entropy
computation. This measure is not good enough since it only considers the
coverage objective. We note the following: on one hand, observing a
large portion of an object (a big cluster) helps the system to recognize
the object better; on the other hand, wider scene coverage (visibility
objective) causes the system to detect more object candidates.
Therefore, we propose a new formulation for viewpoint entropy
calculation that takes into account both the visibility (i.e., given by
the number of visible points in the scene) and saliency (i.e., given by
the size of each cluster; since observing a large portion of an object
can potentially reduce the object recognition and pose estimation
uncertainty) objectives. The viewpoint entropy of a given scene is
computed as follows:

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

where, @xmath is the number of clusters, @xmath is the area of the
@xmath cluster and @xmath is the total area of the given scene. Before
actually moving the camera, we aim to predict the NBV from the camera
pose list, . For this purpose, first, we have to predict what can be
observed from each pose in by taking a “ virtual point cloud ”. Towards
this goal, based on the given set of 6D object hypothesises, the full
model of the objects are first added to the current scene (see Fig. 3.10
b ). Afterwards, for each possible camera pose, a virtual point cloud is
rendered based on depth buffering and orthogonal projection methods (see
Fig. 3.10 c and d ). Then, the viewpoint entropy is calculated for each
rendered view as before.

In general, choosing the view with the minimum view-entropy as the next
camera position has two problems. Firstly, in a real system, the cost of
moving the camera too far could also be considered. Secondly, view
entropy estimation becomes less reliable if the virtual (rendering) view
is far from the current position. To alleviate this issue, we apply
weights to the view entropy value calculated for each view candidate by
Gaussian distribution.

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

where @xmath is a smoothness parameter which restrict the movement of
the camera, @xmath is the weight applied to view entropy for @xmath ,
@xmath is the current camera pose, @xmath is the view entropy of the
view @xmath and @xmath is the weighted view entropy of the view @xmath .
Although a set of viewpoints which are close to each other may have good
attributes, obtaining a sequence of similar viewpoints would not help to
detect new objects which are visible from different viewpoints. To
encourage exploratory behaviour, the following equation is introduced
where viewpoints with higher entropy have a higher chance of being
chosen :

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

#### 3.7.1 Evaluation of Next-Best-View Prediction

We test the proposed NBV prediction on the bin-picking dataset
(Doumanoglou et al., 2016 ) , which is one of the few datasets that
contain multiple objects in highly crowded scenes. The two scenarios of
the dataset are depicted in Fig. 3.9 and Fig. 3.11 . The coffee-cup
scenario contains @xmath different views of the scene with 15 cups in a
pile. The Juice box scenario contains @xmath juice boxes and also has
@xmath views. Ground truth view point entropy (i.e., highlighted by the
red lines in Fig. 3.12 ) is calculated based on the proposed viewpoint
entropy and ground truth object positions provided by the dataset. In
this evaluation, an object pose estimator based on sparse auto-encoder
(Doumanoglou et al., 2016 ) is first used to generate multiple object
hypotheses. The proposed method for rendering virtual point cloud is
then used. Afterwards, an entropy for the rendered viewpoints is
calculated.

Top-ten views, in terms of predicting view point entropy, are selected
based on the square error to the ground truth. Boxplot for the selected
views in both scenarios are depicted in Fig. 3.12 , which displays the
range of variation. The NBV algorithm works well in both scenarios since
the standard deviation (SD) of the view entropy is small (i.e., SD for
the coffee-cup was @xmath and for the juice-box was @xmath ) and the
mean is near to the ground truth in both scenarios. The mean squared
error (MSE) was @xmath and @xmath for the coffee-cup and the juice-box
scenarios respectively. Note that the coffee cup scenario is much more
complex as it has more objects and many of them are occluded in
different views. In contrast, objects in the juice box scenario are
visible in most of the views.

For additional details and comparison with furthest view and random view
selecting strategies, we refer the reader to our previous works on 6D
object pose estimation (Sock et al., 2017 ) . It is worth to mention
that furthest view strategy, as the name implies, selects the furthest
possible view from the current camera position. Random selection and
furthest possible view are two popular choices in the literature for
comparison.

#### 3.7.2 Correlation between Visibility and Viewpoint Entropy

To verify the correlation between object detection performance and the
proposed viewpoint entropy, a synthetic dataset is built for the
following reasons: ( i ) more dense and even sampling of camera
viewpoint can be obtained; ( ii ) perfect knowledge on object ground
truth, camera pose and calibration parameters are known. Towards this
end, @xmath object models are randomly thrown into a virtual box using
MuJoCo physics engine (Todorov et al., 2012 ) . As depicted in Fig. 3.13
( right ), RGBD scenes are rendered at @xmath evenly sampled viewpoints
around the upper hemisphere. The dataset is publicly available at
https://goo.gl/BSr2mU

For each object, the ratio of visible pixels to the total number of
pixels if the object were not occluded is calculated and these values of
every objects in a scene are averaged to quantify the average visibility
score for each viewpoint. Object detector proposed by Doumanoglou et al.
( 2016 ) is used to obtain the F1-score for each viewpoint and the
proposed view entropy is also used to calculate viewpoint entropy.
Results are plotted in Fig. 3.14 . The view indices are ordered in
descending average visibility score and the graph shows the F1 score and
viewpoint entropy decreases along with the visibility of the viewpoint.
The viewpoint entropy and F1 score are positively correlated, with a
correlation coefficient of @xmath for the dataset.

### 3.8 Summary

In this chapter, several approaches for supervised and unsupervised
gathering of object experiences have been introduced. In particular, we
first proposed perception capabilities that will allow robots to
automatically detect multiple objects in a crowded scene. Furthermore,
we discussed how to collect object experiences interactively and
incrementally in both supervised and unsupervised manner. We also
introduced view entropy formulation, which can be used to predict the
Next-Best-View in an environment where robot movement is costly and the
scene is complex. This approach was tested on several datasets as shown
in Fig. 3.2 and Fig. 3.4 . Results showed that 3D information is very
helpful for object detection purposes. We have also shown that the
proposed NBV prediction approach works well in different scenarios. In
the next chapter, we present several approaches for 3D object
representation.

## Chapter 4 Object Representation

One of the primary goals in service robotics is to develop perception
capabilities that will allow robots to robustly interact with the
environment by manipulating objects. For this purpose, a robot must
reliably recognize the object. Furthermore, in order to interact with
human users, this process of object recognition cannot take more than a
fraction of a second.

Although many object recognition methods for both 2D and 3D data have
been proposed (Andreopoulos and Tsotsos, 2013 ) , recognizing 3D objects
in the presence of noise and variable point cloud resolution is still a
challenging task. However, 3D data contains more information about the
spatial positioning of objects, which in turn eases the process of
object segmentation. Moreover, depth data is more robust than RGB data
to the effects of illumination and shadows (Regazzoni et al., 2014 ) .
Therefore, 3D data can be employed to describe the surface of the
objects based on geometric properties and 2D data can also be used to
distinguish objects that have same geometric properties with different
texture (e.x. a Coke can from a Diet Coke can).

As described in Chapter 2 , a 3D object recognition system is composed
of several software modules such as Object Detection , Object
Representation , Object Recognition and Perceptual Memory . Object
Detection is responsible for detecting all objects in a scene. Object
representation is concerned with the calculation of a set of features
for the detected object, which are sent to the Object Recognition . The
category of an object is recognized by comparing its description against
the descriptions of known objects of different categories (stored in the
Perceptual Memory ). Among these modules, Object Representation plays a
prominent role because the output of this module is used for learning as
well as recognition. Therefore, the Object Representation module must
provide reliable information in real-time to enable the robot to
physically interact with the objects in its environment. Moreover, the
representation of an object should contain enough information enabling
to recognize the same or similar objects seen from different
perspectives.

Existing 3D object representation approaches are based on either
hand-crafted descriptions or machine learning approaches. Global
descriptors encode the entire 3D object, while local descriptors
represent a small area around a specific keypoint in the object.
Generally, global descriptors are increasingly used in the context of 3D
object recognition, object manipulation, as well as geometric
categorization (Aldoma et al., 2012a ) . These must be efficient in
terms of computation time as well as memory, to facilitate real-time
performance. For example, Ensemble of Shape Functions (ESF) (Wohlkinger
and Vincze, 2011 ) , Global Fast Point Feature Histogram (GFPFH) (Rusu
et al., 2009b ) , Viewpoint Feature Histogram (VFH) (Rusu et al., 2010 )
and Global Radius-based Surface Descriptor (GRSD) (Marton et al., 2010 )
, are global descriptors.

Object representation based on local features tends to handle occlusion
and clutter better when compared to global features. Examples in this
category include Signature of Histograms of Orientations (SHOT) (Tombari
et al., 2010 ) , Spin Images (SI) (Johnson and Hebert, 1999 ) , Fast
Point Feature Histogram (FPFH) (Rusu et al., 2009a ) and Hierarchical
Kernel Descriptors (Bo et al., 2011 ) . However, comparing 3D object
views based on their local features tends to be computationally
expensive (Aldoma et al., 2012a ) . To address this problem, machine
learning approaches such as Bag of Words (BoW) (Csurka et al., 2004 )
and Latent Dirichlet Allocation (LDA) (Blei et al., 2003 ) can be used
for representing objects in a uniform, highly compact and distinctive
way.

In open-ended learning, the learning agent extracts training instances
from its online experiences. Thus, training instances become gradually
available over time, rather than being completely available at the
beginning of the learning process. Classical object recognition
approaches do not perform well in these scenarios because they are often
designed for static environments, i.e., training (offline) and testing
(online) are two separated phases. If limited training data is used,
this might lead to non-discriminative object representations and, as a
consequence, to poor object recognition performance. Therefore, building
a discriminative object representation is a challenging step to improve
object recognition performance. Most of the recent approaches use
hand-crafted features. These approaches may be not the best option for
open-ended domains and require to be designed separately for each
application. Topic modeling (Blei et al., 2003 ) is suitable for
open-ended learning because it provides, not only short object
descriptions (i.e., optimizing memory), but also enables efficient
processing of large collections. It is a type of generative latent
structure model that represents the underlying structure of data as
topics.

In this chapter, we first present a new global 3D shape descriptor named
GOOD (i.e., Global Orthographic Object Descriptor). GOOD provides an
appropriate trade-off between descriptiveness, computation time and
memory usage. The descriptor is designed to be scale and pose invariant,
informative and stable, with the objective of supporting accurate 3D
object recognition. Afterwards, a set of object representation
approaches based on local shape features is presented and discussed. The
main motivation of this part is to explore and compute object
representations at different level of abstraction (see Fig. 4.4 ). We
propose a novel object representation approach for describing the shape
of an object. In particular, we propose an extension of Latent Dirichlet
Allocation (LDA) to learn structural semantic features (i.e., topics)
from low-level feature co-occurrences for each category independently.
Topics in each category are discovered in an unsupervised fashion and
are updated in an open-ended manner using new object views.

Throughout this chapter we assume that an object has already been
segmented from the point cloud of the scene, and we focus on detailing
the object representation approaches. All object representation
approaches are computed directly from a segmented 3D point cloud and
require neither triangulation of the object’s points nor surface
meshing. The contributions presented in this chapter are the following:
( i ) a novel global object descriptor computed using a local reference
frame, that provides a good trade-off between descriptiveness,
computation time and memory usage; ( ii ) design a new sign
disambiguation method to compute a unique and unambiguous complete local
reference frame, from the eigenvectors obtained through Principal
Component Analysis of the segmented point cloud of the object and ( iii
) propose an extension of Latent Dirichlet Allocation to learn topics
for each category incrementally and independently. The work presented in
this chapter spawned a series of publications presented at major
conferences (Kasaei et al., 2016d , f ; Oliveira et al., 2015a , 2014b )
and top journals in the field (Kasaei et al., 2016g , 2018b , 2015c ;
Oliveira et al., 2015b ) .

The remainder of this chapter is organized as follows. In section 4.1 ,
we discuss related works. Section 4.2 describes the GOOD object
descriptor. The methodology of object representation based on local
features is presented in section 4.3 . Finally, summary is presented and
future research is discussed.

### 4.1 Related Work

Three-dimensional shape description has been under investigation for a
long time in various research fields, such as pattern recognition,
computer graphics and robotics. Although an exhaustive survey of 3D
shape descriptors is beyond the scope of this chapter, we will review
the main efforts.

#### 4.1.1 Hand-Crafted Descriptors

As previously mentioned, some descriptors use a Reference Frame (RF) to
compute a pose invariant description. Therefore, this property can be
used to categorize 3D shape descriptors into three categories including
( i ) shape descriptors without a common reference; ( ii ) shape
descriptors computed relative to a reference axis; ( iii ) shape
descriptors computed relative to a RF. Most of the shape descriptors of
the first category use certain statistic features or geometric
properties of the points on the surface like depth value, curvature and
surface normal to generate a description. For instance, the Shape
Distributions descriptor (Osada et al., 2002 ) represents an object as a
shape distribution sampled from a shape function measuring global
geometric properties of the object. The Extended Gaussian Images (EGI)
descriptor (Horn, 1984 ) is based on the distribution of surface normals
on the Gaussian sphere. Descriptiveness of the EGI depends on the shape
of the object and it is not suitable for non-convex object. Wohlkinger
and Vincze ( 2011 ) introduced a global shape descriptor called Ensemble
of Shape Function (ESF) that does not require the use of normals to
describe the object. The characteristic properties of the object are
represented using an ensemble of ten 64-bin histograms of angle, point
distance, and area shape functions. The descriptiveness of the above
shape descriptors is limited because the 3D spatial information either
is not taken into account or is discarded during the description
process.

In contrast, the shape descriptors in the second and third category
encode the spatial information of the objects’ points using a Reference
Frame (RF). Chen and Bhanu ( 2007 ) proposed a Local Surface Patch (LSP)
descriptor that encodes the shape of objects by accumulating points in
particular bins along the two dimensions that are the shape index value
and the cosine of the angle between the surface normals. Point Feature
Histogram (PFH) (Rusu et al., 2008 ) can be used as a local or global
shape descriptor. PFH represents the relative orientation of normals, as
well as distances, between point pairs. For each point @xmath ,
k-neighbourhood points are selected based on a sphere centred at @xmath
with radius @xmath . Afterwards, a surface normal for each point is
estimated. Subsequently, four features are calculated for every pair of
points using their surface normals, positions and angular variations. In
a later work (Rusu et al., 2009a ) , in order to improve the robustness
of PFH in case of point density variations, the distance between point
pairs is excluded from the histogram of PFH. The computation complexity
of a PFH is @xmath , where @xmath is the number of points in the point
cloud. Fast Point Feature Histogram (FPFH) (Rusu et al., 2009a ) is an
extension of PFH. FPFH estimates the sets of values only between every
point and its k nearest neighbours. This is different from PFH, where
all pairs of points in the support region are considered. Therefore, the
computational complexity is reduced to @xmath . FPFH is a scale and pose
invariant descriptor which is not suitable for grasping. Viewpoint
Feature Histogram (VFH) (Rusu et al., 2010 ) is another extension of PFH
that encodes both geometry and viewpoint information, allowing
simultaneous recognition of the object and its pose. VFH computes the
same angular features as PFH. Additionally, it computes other statistics
between the central viewpoint direction and the normals estimated at
each point. The VFH shape descriptor produces a single histogram that
encodes the geometry of the whole object and its viewpoint. Because of
the global nature of VFH, the computational complexity of VFH is @xmath
.

Invariance to the pose of an object is a critical property of object
descriptors too. A number of 3D shape descriptors achieve pose
invariance using either a reference axis only or a complete object
reference frame. For example, Spin-Images (Johnson and Hebert, 1999 )
use the surface normal in a vertex as a reference axis. The Spin-Image
(Johnson and Hebert, 1999 ) encodes the surrounding shape in a keypoint
by projecting the surface points to the tangent plane of the keypoint.
Each projected point is represented by a pair @xmath , where @xmath is
the distance to the surface normal, i.e., the radius, and @xmath is the
perpendicular distance from the point to the tangent plane. A histogram
is formed by counting the occurrences of different discretized distance
pairs. Spin-images have been successfully used in many applications, but
one limitation of this descriptor is that it is not scale invariant.
Dinh and Kropac ( 2006 ) proposed multi-resolution pyramids of spin
images to improve the discrimination of the original spin image and
speed up the matching process. Some variants of the spin image shape
descriptor were also presented, such as Tri-Spin-Image descriptor
(TriSI) (Guo et al., 2013 ) and color spin image (Pasqualotto et al.,
2013 ) .

Similar to the spin-image, the 3D Shape Context (3DSC) uses the surface
normal at a given point as its RF (Frome et al., 2004 ) . The 3DSC
descriptor is calculated by counting the weighted number of points
falling into each bin of a spheric grid centred on the given point and
its north pole oriented with the surface normal. The spheric grid is
constructed based on dividing the support area into bins by
logarithmically spaced boundaries along the radial dimension and equally
spaced boundaries in the azimuth and elevation dimensions. Whenever only
an axis is used as a reference frame, there is an uncertainty in the
rotation around the axis that should be handled for generating a robust
and repeatable description. In order to eliminate this issue, several
descriptors (e.g. 3D Shape Context) proposed to compute multiple
descriptions for different possible rotations of the object. Since this
kind of solutions greatly increase the computational cost in terms of
both execution time as well as memory usage, they are not satisfactory
solutions.

Zhong ( 2009 ) proposed a shape descriptor named Intrinsic Shape
Signatures (ISS) based on a LRF computed from the eigenvectors of the
scatter matrix of the point cloud of the object and describing the point
distribution in the spherical angular space. Similar to Zhong’s work,
Mian et al. ( 2010 ) introduced a RF computed with eigenvectors of the
covariance matrix of the object’s points. In both cases, although the
eigenvectors define the principal directions of the data, their sign is
not defined unambiguously. Therefore, different descriptors can be
generated for the object. As highlighted before, they are neither
computationally efficient nor repeatable. Pang and Neumann ( 2015 )
proposed a multi-view 3D object recognition approach in which each
object is projected into 46 projection planes uniformly distributed
around a sphere. In contrast we just compute three principal
orthographic projections. Their object representation is clearly not
efficient for real time application like robotics. In order to achieve
true rotation invariance, Tombari et al. ( 2010 ) proposed a 3D shape
descriptor named Signature of Histograms of OrienTations (SHOT). They
first apply a sign disambiguation technique to the eigenvectors of the
scatter matrix of the object and constructed a unique and unambiguous
RF. The object’s points are then aligned with the RF. Similar to 3DSC,
an approach based on spherical coordinate is used to generate a SHOT
description for the given object. 3D object descriptors that use the
spherical coordinates system suffer from the singularity issue at the
poles, because bins at the poles are significantly smaller than bins
around the equator.

Aldoma et al. ( 2012a ) reviewed properties, advantages and
disadvantages of several state-of-the-art 3D shape descriptors available
from the Point Cloud Library (PCL) to develop 3D object recognition and
pose estimation systems. They also proposed two pipelines for object
recognition systems using local and global 3D shape descriptors from
PCL. These descriptors have been popular among the robotic community. A
summary of the mentioned descriptors is reported in Table 4.1 .

The GOOD descriptor differs from all of the global descriptors listed in
Table 4.1 as it is simultaneously unique, unambiguous, and robust to
noise and varying low-level point cloud density. Besides, our approach
can be used not only for object recognition but also for object
manipulation. It is worth mentioning that GOOD has been recently
integrated into the PCL and will appear in the next release (i.e., PCL
1.9.x).

#### 4.1.2 Machine Learning Approaches for Object Representation

Comparing 3D objects based on their representative sets of local
features is computationally expansive. To address this limitation,
several machine learning approaches have been introduced for object
representation in which objects are described as a histogram of local
shape features. In particular, Bag-of-Words (BoW) and topic modeling
(Blei et al., 2003 ) techniques are suitable for open-ended learning
because, these approaches provide not only short object descriptions
(i.e., optimizing memory) but also enable efficient processing of large
collections. Csurka et al. ( 2004 ) extended the BoW methods from
Natural Language Processing (NLP) domain to the object recognition
domain. In text-classification, BoW is a methodology where documents are
represented as a histogram of words disregarding grammar. Yeh et al. (
2009 ) integrated the bag-of-words methodology to propose an efficient
method for concurrent object localization and recognition. In (Islam
et al., 2011 ) , an object classification approach was proposed, in
which object representation was based on SIFT, SURF and color
histograms. All these features were compacted into a histogram of visual
words for optimizing the recognition process, as well as memory usage.
In this case, the authors used a naive Bayes classifier in the
recognition stage. Kasaei et al. ( 2015b ) proposed an approach for
object representation in which objects are described by histograms of
features.

The codebooks used in the BoW models are usually constructed offline by
feeding a sample set of features (the codebook training set) to a
clustering algorithm, e.g., K-means or hierarchical K-means. Because
clustering is done offline, the representativeness of this training set,
i.e., how well the features in the set describe the space of features
that the robot will find when operating online, becomes critical to the
performance of the system. This is most noticeable in open-ended domains
because the categories to be learned are not known a priori and may
yield feature patterns which were not included in the codebook training
set. In other words, the learning agent extracts training instances from
its online experiences in open-ended learning. Thus, training instances
become gradually available over time, rather than being completely
available at the beginning of the learning process. Classical object
recognition approaches do not perform well in these scenarios because
they are often designed for static environments, i.e., training
(offline) and testing (online) are two separated phases. If limited
training data is used, this might lead to non-discriminative object
representations and, as a consequence, to poor object recognition
performance. A long-term process of building and updating a
discriminative object representation is an important contribution to
improving object recognition performance.

On the other hand, the approaches that represent objects based on sets
of local features or histogram of visual words completely discard
structural information, i.e., information related to the co-occurrence
of local object features. To handle these limitations, several kinds of
research have been proposed to concurrently learn both the object
category models and the underlying representation used to encode those
category models. Riesenhuber and Poggio ( 1999 ) proposed a hierarchical
approach for object recognition consistent with physiological data, in
which objects are modelled in a hierarchy of increasingly sophisticated
representations. Sivic et al. ( 2005 ) proposed an approach to discover
objects in images using Probabilistic Latent Semantic Indexing (pLSI)
modelling (Hofmann, 1999 ) . Blei et al. ( 2003 ) argued that the pLSI
is incomplete in that it provides no probabilistic model at the level of
documents. They extended the pLSI model calling the approach Latent
Dirichlet Allocation (LDA). It is a type of generative latent structure
model that represents the underlying structure of data as topics.
Similar to pLSI and LDA, we discover topics in an unsupervised fashion.
Unlike our approach, pLSI and LDA do not incorporate class information.

Several works have been presented to incorporate a class label in the
generative model. Mcauliffe and Blei ( 2008 ) extended LDA and proposed
Supervised LDA (sLDA). The sLDA was first used for supervised text
prediction. Later, Wang et al. ( 2009 ) extended sLDA to classification
problems. Another popular extension of LDA is the classLDA (cLDA)
(Fei-Fei and Perona, 2005 ) . Similar to our approach, the only
supervision used by sLDA and cLDA is the category label of each training
object. However, there are two main differences. First, the learned
topics in sLDA and cLDA are shared among all categories, while we
propose to learn specific topics per category. Second, the sLDA and cLDA
approaches follow a standard train-and-test procedure (i.e., set of
classes, train and test data are available in advance), our approach can
incrementally update topics using new observations and the set of
classes is continuously growing. There are some topic-supervised
approaches e.g. Labeled LDA (Ramage et al., 2009 ) and semiLDA (Wang
et al., 2007 ) that consider class labels for topics. On the one hand,
these approaches need tens of hours of manual annotation. On the other
hand, it is hard for a human to provide a specific category label for a
3D local shape feature.

There are some LDA approaches that support incremental learning of
object categories. The difference between incremental and open-ended
learning is that the set of classes is predefined in incremental
learning, while in open-ended learning the set of classes is
continuously growing. Banerjee and Basu ( 2007 ) proposed online LDA
(o-LDA) that is a simple modification of batch collapsed Gibbs sampler.
The o-LDA first applies the batch Gibbs sampler to the full dataset and
then samples new topics for each newly observed word using information
observed so far. Canini et al. ( 2009 ) extended o-LDA and proposed
incremental Gibbs sampling for LDA (here referred to as I-LDA). The
I-LDA does not need a batch initialization phase like o-LDA. In o-LDA
and I-LDA, the set of categories is fixed, while in our approach the set
of categories is growing. Moreover, o-LDA and I-LDA are used to discover
topics shared among all categories, while our approach discovers
specific topics per category. Gao et al. ( 2014 ) and Zhang et al. (
2016 ) proposed approaches for fine-grained image categorization by
learning category-specific dictionaries and a shared dictionary for all
the categories. The works of Gao et al. ( 2014 ) and Zhang et al. ( 2016
) are similar to ours in that they learned specific representations for
each category. However, unlike our approach, the representations of
known categories do not change after the training stage.

Some researchers have recently adopted deep learning algorithms for 3D
object representation, learning and recognition (Li et al., 2016 ; Su
et al., 2015 ; Wu et al., 2015 ) . These works use a collection of
images rendered from different view points to learn a shape
representation that aggregates information from input views and provides
a compact shape descriptor. As it was pointed by Wu et al. ( 2015 ) ,
training a deep artificial neural network for 3D object representation
requires a large collection of 3D objects to provide accurate
representations and typically involves long training times.

### 4.2 GOOD: A Global Orthographic Object Descriptor

In this section, a new global 3D shape descriptor named GOOD (i.e.,
Global Orthographic Object Descriptor) is introduced. GOOD provides an
appropriate trade-off between descriptiveness, computation time and
memory usage. The descriptor is designed to be scale- and
pose-invariant, informative and stable, for ensuring highly accurate 3D
object recognition. A novel sign disambiguation method is proposed to
compute a unique and repeatable reference frame from the eigenvectors
obtained through Principal Component Analysis of the point cloud of the
object. Using this reference frame, three principal projections, namely
@xmath , @xmath and @xmath , are created based on orthographical
projection. The space of each projection is partitioned into bins and
the number of points falling into each bin is counted. From this, three
distribution matrices are obtained for the projected views. Two
statistical features, namely entropy and variance are then calculated
for each distribution matrix. The distribution matrices are finally
concatenated using the entropy and variance features to decide the order
of concatenation.

#### 4.2.1 Global Object Reference Frame

A global object descriptor should be computed in a reference frame that
is invariant to translations and rotations and robust to noise. We call
it global object reference frame to distinguish it from reference frame
used for computing local features. Since the repeatability of the global
object frame directly affects the descriptiveness of the object
representation (Mian et al., 2010 ) , it should be as repeatable and
robust as possible to improve the performance of object recognition. In
this subsection, we propose a method to compute this RF. For this
purpose, the three principal axes of the target object are firstly
determined based on Principal Component Analysis (PCA). Given a point
cloud of an object that contains @xmath points, @xmath , the geometric
center of the object is defined as:

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

where each @xmath is a three dimensional point in the object’s point
cloud. The normalized covariance matrix, C , of the object is
constructed:

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

Then, eigenvalue decomposition is performed on C :

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

where @xmath contains the three eigenvectors, @xmath is a diagonal
matrix of the corresponding eigenvalues and @xmath . Since the
covariance matrix is symmetric positive, its eigenvalues are positive
and the eigenvectors are orthogonal.

Eigenvectors define directions which are not unique, i.e., not
repeatable across different PCA trials. This is known as the sign
ambiguity problem, for which there is no mathematical solution (Bro
et al., 2008 ) . Since there are two possible directions for each
eigenvector, a total of eight reference frames can be created from the
same set of eigenvectors. A mechanism is needed to transform this
reference frame into a unique object reference frame, which will be the
same across multiple trials.

We start with a provisional reference frame, in which the first two
axes, @xmath and @xmath , are defined by the eigenvectors @xmath and
@xmath , respectively. However, regarding the third axis, @xmath ,
instead of defining it based on @xmath , we define it based on the cross
product @xmath . This way, because the result of the cross product
follows the right-hand rule, the number of alternatives is reduced to
four. It is now enough to disambiguate the directions of the @xmath and
@xmath axes. So either the directions of @xmath and @xmath are both
changed or both remain unchanged.

To complete the disambiguation, the object’s point cloud, , is
transformed to be placed in the provisional reference frame. Then, the
number of points that have positive @xmath , @xmath , and the number of
points that have negative @xmath , @xmath , are computed:

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

where @xmath denotes the number of points of the argument and @xmath is
a threshold (e.g. @xmath ) that is used to deal with the special case
when a point is close to the @xmath plane, and therefore can change from
negative to positive @xmath in different trials. Afterwards, the
variable @xmath is defined as:

  -- -------- -- -------
     @xmath      (4.5)
  -- -------- -- -------

A similar indication, @xmath , is computed for the @xmath axis. Finally,
the sign of the axes is determined as:

  -- -------- -- -------
     @xmath      (4.6)
  -- -------- -- -------

where @xmath can be either @xmath or @xmath . In case of @xmath , the
directions of @xmath and @xmath must be changed, otherwise not.
Therefore, the final RF @xmath will be defined by @xmath . An
illustrative example of the sign disambiguation procedure is provided in
Fig. 4.1 .

#### 4.2.2 Computing the Object Descriptor

This section describes the computation of the proposed object
descriptor, GOOD, in the obtained RF centered in the geometric center of
the object. The descripor consists of a concatenation of the
orthographic projections of the object on the three orthogonal planes,
@xmath , @xmath and @xmath . Each projection is described by a
distribution matrix. To ensure correct comparison between different
object shapes, the number of bins in the distribution matrices must be
the same and the bins should be of equal size. Therefore, each
distribution matrix must be computed from a square area in the
projection plane centered on the object’s center, and this square area
must have the same dimensions for the three projections. The side length
of these square areas, @xmath , is determined by the largest edge length
of a tight-fitting axis-aligned bounding box (AABB) of the object. The
dimensions of the AABB are obtained by computing the minimum and maximum
coordinate values along each axis. With this setup, the number of bins,
@xmath , is the only parameter that must be specified to compute GOOD.
For each projection, the @xmath projection area is divided into @xmath
square bins. Finally, a distribution matrix @xmath is obtained by
counting the number of points falling into each bin.

For each projected point @xmath , where @xmath is the perpendicular
distance to the horizontal axis and @xmath is the perpendicular distance
to the vertical axis, a row, @xmath , and a column, @xmath , are
associated as follows:

  -- -- -- -------
           (4.7)
  -- -- -- -------

  -- -- -- -------
           (4.8)
  -- -- -- -------

where @xmath is a very small value used to deal with the special cases
when a point is projected onto the upper bound of the projection area,
and @xmath returns the largest integer not greater than @xmath . Note
that the projected view is shifted to right and top by @xmath (i.e.,
@xmath and @xmath ). Furthermore, to achieve invariance to point cloud
density, is normalized such that the sum of all bins is equal to 1.0
(see Fig. 4.2 ). The matrix is called distribution matrix because it
represents the 2D spatial distribution of the object’s points. According
to standard practice, this matrix is converted to a vector @xmath . The
three projection vectors will be concatenated producing a histogram
vector of dimension @xmath which is the final object descriptor, GOOD
(i.e., @xmath ). Statistical features are used to decide the order in
which the projection vectors will be concatenated.

For the first projection in the descriptor, the one with largest area is
preferred. The number of points is not a good indicator of area because
all points of the object are represented in the three projections. The
number of occupied bins (the ones with a mass greater than 0) could be
used as a measure of area. However, this measure tends to be brittle
when the boundary of the object is close to boundaries between bins.
Therefore, in this work the entropy of the projection is used. Entropy,
a measure from Information Theory (Cover and Thomas, 2012 ) , nicely
takes into account both the number of occupied bins and their density.
In this work, the entropy of a projection is computed as follows:

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

where @xmath is the mass in bin @xmath . The logarithm is taken in base
2 and @xmath . The projection with highest entropy is the one that will
appear in the first @xmath positions of the descriptor.

The next step is to select, from the remaining two projections, which
one should appear in the second part of the descriptor (positions @xmath
to @xmath ). It is common that these two projections have similar areas,
and therefore similar entropies, leading to instability of the decision
if it is made based on entropy. Therefore, instead of entropy, we use
variance to make this decision. Since the projection matrices are
probability mass functions (pmf), the variance is defined as follows:

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

where @xmath is the expected value (i.e., a weighted average of the
possible values of @xmath , corresponding to the geometric center of the
projection), which is computed as follows:

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

The variance measure, @xmath , is used to measure the spread or
variability of the spatial distribution of the object’s points in the
projection vector. A small variance indicates that the projected points
tend to be very close to each other and to the mean of the vector, i.e.,
the distribution is small and compact. A high variance indicates that
the data points in the projection vector are very spread out from the
mean.

An illustrative example of the proposed shape descriptor is depicted in
Fig. 4.2 . In this example, after determining the global object
reference frame, a mug is projected onto the three orthogonal planes.
Based on the entropy criterion, the @xmath projection (Fig. 4.2 b) is
selected to appear in the first part of the descriptor. Based on the
variance criterion, the @xmath projection (Fig. 4.2 c) is selected to
appear in the second part of the descriptor. The remaining projection,
@xmath (Fig. 4.2 d), appears in the last part of the descriptor.

#### 4.2.3 Relevance for Object Manipulation

In order to grasp an object, it is necessary to know the true dimensions
of different parts of the object. Such information is not adequately
represented in most shape descriptors (e.g. Viewpoint Feature Histogram
(Rusu et al., 2010 ) ). Because GOOD is composed of three orthogonal
projections, it is especially rich in terms of information suited for
manipulation tasks. We previously showed how to use orthographic
projections for object manipulation purposes (Kasaei et al., 2016e ) .

In Fig. 4.3 , again we consider the projections of a mug. Here, we adopt
a multi-view orthographic projection layout in which there is a central
or front view, a top view and a side view. The central view is the one
selected based on the entropy criterion and appearing in the first part
of the descriptor. The top view contains the projection in the
orthogonal plane formed by the horizontal axis of the central projection
and the third axis. The side view contains the projection in the
orthogonal plane formed by the vertical axis of the central projection
and the third axis. The figure shows that projections can be further
processed for object manipulation purposes. In the top view, the grey
symbols @xmath , @xmath , @xmath and @xmath represent how the projection
can be further processed and some features for manipulation are
extracted, namely inner radius ( @xmath ), thickness ( @xmath ), handle
length ( @xmath ) and handle thickness ( @xmath ).

### 4.3 Representations based on Local Features

Global object representations are widely used in shape retrieval and
object recognition techniques, but these features are sensitive to
clutter and occlusion. To cope with this limitations, local features can
be used (Mian et al., 2006 ; Bariya et al., 2012 ) . The main motivation
of this section is to explore object representations at different levels
of abstraction. We present three object representation methods based on
local features. In the first approach, an object is represented by a set
of local features. The second approach represents an object based on the
Bag of Visual Words technique, i.e., as a histogram of visual words
(classes of local features). Finally, we propose an extension of Latent
Dirichlet Allocation (LDA) to learn structural semantic features (i.e.,
topics) from low-level feature co-occurrences for each category
incrementally and independently.

All the approaches are designed to be used by a service robot working in
a domestic environment. Fig. 4.4 shows a PR2 robot looking at some
objects on the table. Tabletop objects (identified by different bounding
boxes) are tracked and processed sequentially through five layers. For
instance, to describe an object view using the feature layer, a
spin-image shape descriptor (Johnson and Hebert, 1999 ) is used to
represent the local shapes of the object in different key points. The
object therefore is represented by a set of spin-images. When using the
Bag-of-Words (BoW) layer, the given object view is described by a
histogram of local shape features, as defined in Bag-of-Words models. In
the topic layer, each topic is defined as a discrete distribution over
visual words. Depending on how the system is configured, object views
are represented in the object view layer as sets of local features,
histograms of visual words, or histograms of LDA topics. Therefore, the
content of the object view layer depends on the selected object
representation approach. Finally, the category model is updated by
incorporating the obtained object representation (category layer). In
following subsections, we present and discuss each approach in detail.

#### 4.3.1 Object Views Represented by Sets of Spin-images

In this subsection, we adopt an approach to object representation in
which objects are described by sets of local shape features called
spin-images (Johnson and Hebert, 1999 ) . The reason why we use spin
images rather than other 3D feature descriptors is that the spin-image
is pose invariant, and therefore suitable for 3D perception in
autonomous robots. Another advantage of the spin-image is that only a
repeatable normal - rather than a full reference frame - is required to
compute the local descriptor.

The process of local feature extraction consists of two main phases:
extraction of keypoints and computation of spin images. For efficiency
reasons, the number of keypoints in an object should be much smaller
than the total number of points. For keypoint extraction, first, a
voxelized grid approach is used to obtain a smaller set of points. The
nearest neighbor point to each voxel center is selected as a keypoint.
Afterwards, the spin-image descriptor is used to encode the surrounding
shape in each keypoint using the original point cloud. Figure 4.5 ( a
and b ) shows an example of the keypoint extraction process. In the
second stage, spin-image descriptors are computed for each keypoint in
order to describe the shape surrounding the keypoint. A spin-image is a
local shape histogram obtained by projecting the 3D surface points onto
the tangent plane of the keypoint. The normal vector of the tangent
plane is called surface normal. Then, each point is represented by a
pair ( @xmath , @xmath ), where @xmath is the distance to the surface
normal of the keypoint, i.e., the radius, and @xmath is the
perpendicular distance from the point to the tangent plane:

  -- -------- -- --------
     @xmath      (4.12)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (4.13)
  -- -------- -- --------

where n is the surface normal for keypoint p . Every spin-image bin
counts, for a given neighborhood of points around the keypoint, the
number of neighbors that fall in a given range of @xmath and @xmath .
The procedure is illustrated in Fig. 4.5 ( c ) and ( d ).

To compute a spin-image, the following parameters must be specified ¹ ¹
1 http://pointclouds.org :

-   Image width ( @xmath ): defines the resolution of the spin-image,
    which will be @xmath bins in the radius dimension and @xmath bins in
    the distance dimension.

-   Support length ( @xmath ): determines the amount of space swept out
    by a spin-image, which will have a radius of @xmath and a height of
    @xmath .

-   Support angle ( @xmath ): maximum angle between the surface normal
    at the keypoint and the surface normal in other points to be
    included as neighbors.

Finally, each object view, , is described by a set of spin-images,

  -- -------- -- --------
     @xmath      (4.14)
  -- -------- -- --------

where @xmath is the number of key-points. In the next chapter, we will
describe how these features are used both for conceptualizing and
recognizing object categories. Examples of keypoints, detected by the
proposed approach, on a set of object views of the Washington RGB-D
dataset (Lai et al., 2011a ) are shown in Fig. 4.6 .

#### 4.3.2 Object Views Represented by Histograms of Visual of Words

As we discussed earlier, comparing 3D objects by their local features
tends to be computationally expensive. To address this problem, a BoW
approach can be adopted for object representation, i.e., objects are
described by histograms of local shape features. This approach requires
a dictionary of visual words. Usually, this dictionary is created
off-line through clustering of a given training set. In open-ended
learning scenarios, there is not a set of training data available at the
beginning of the learning process. To cope with this limitation, we look
at human cognition, in particular at the fact that human babies explore
their environment in a playful (arbitrary) way (Smith and Gasser, 2005 )
. Therefore, we propose that the robot freely explores several scenes
and collects several object experiences. Towards this goal, unsupervised
object discovery, as proposed in chapter 3 , section 3.4 , is carried
out in the environment while the robot operates. Only the modules
directly involved in object discovery and dictionary building are active
at this stage. The robot seeks to segment the world into ‘‘object’’ and
‘‘non-object’’. The output of object exploration is a pool of object
candidates. It should be noted that, to balance computational efficiency
and robustness, a downsampling filter is applied to obtain a smaller set
of points distributed over the surface of the object. Subsequently, to
construct a pool of features, spin-images are computed for the selected
points extracted from the pool of object candidates. We use a PCL
function to compute spin-images ² ² 2 In this work, we computed around
32000 spin-images from the point cloud of the 194 objects. . Finally,
the dictionary is constructed by clustering the features using the
k-means algorithm (Hartigan and Wong, 1979 ) . The centers of @xmath
computed clusters are defined as the visual words, @xmath ( @xmath ).
Figure 4.7 shows a dictionary containing 50 words. In the
implementation, we tested different dictionary sizes.

When using the BoW approach, object views (instances) are described in
the object view layer by histograms of frequencies of visual words. As
depicted in Fig. 4.4 , the input to this layer is the set of spin-image
features of an object candidate, , which is computed as described in the
previous section. Afterwards, by searching for the nearest neighbor in
the dictionary, each spin image is assigned to a visual word. Therefore,
the given object is first represented as a set of visual words, @xmath ,
where each entry represents one of the @xmath words of the dictionary,
and @xmath is the number of features of the given object. The given
object is then represented as a histogram of occurrences of visual
words:

  -- -------- -- --------
     @xmath      (4.15)
  -- -------- -- --------

where the @xmath element of is the count of the number spin-images
assigned to a visual word, @xmath , and @xmath is the size of the
dictionary. The obtained histogram is dispatched to the Object
Recognition module or to the Object Conceptualization module.
Furthermore, in order to build a more compact and complex object
representation, the set of visual words of the object, i.e., @xmath , is
given as input to the topic layer .

#### 4.3.3 Object Views Represented by Histograms of LDA Topics

In this section, we propose two variant of Latent Dirichlet Allocation
(LDA) (Blei et al., 2003 ) . In both LDA-based approaches, each object
is represented as a finite mixture over a set of latent variables (i.e.,
topics), which are expected to summarize the word-order (semantic)
information. One of these approaches develops local (per category)
topics (Kasaei et al., 2016f ) whereas the other develops global topics,
shared among all categories1, as in standard LDA (Blei et al., 2003 ) .

##### Local LDA

In the topic layer, a statistical model is used to get structural
semantic features from low-level feature co-occurrences. The basic idea
is to represent the objects as random mixtures over topics (i.e., latent
variables), where each topic is characterized by a distribution over
visual words (i.e., observed variables). Both random distributions are
defined using multinomial distributions and their parameters are
characterized by a Dirichlet distribution. It must be pointed out that
we are using shape features rather than semantic properties to encode
the statistical structure of object categories (Kim et al., 2009 ) . It
is easier to explain the details using an example. We start by selecting
a category label, for example Mug . To represent a new instance of Mug,
a distribution over Mug topics is drawn that will specify which topics
should be selected for generating each visual word of the object.
According to this distribution, a particular topic is selected out of
the mixture of possible topics of the Mug category for generating each
visual word in the object. For instance, a Mug usually has a handle, and
a “handle” topic refers to some visual words that frequently occur
together in handles. The process of drawing both the topic and visual
word is repeated several times to choose a set of visual words that
would construct a Mug . We use statistical inference techniques for
inverting this process to automatically find out a set of topics for
each category from a collection of instances. In other words, we try to
learn a model for each category (a set of latent variables) that
explains how each object obtains its visual words.

Towards this end, the obtained representation from the BoW layer is
presented as input to the topic layer . The LDA model contains
parameters at three levels: including category-level parameters (i.e.,
@xmath ), which are sampled once in the process of generating a category
of objects; object-level variables (i.e., @xmath ), which are sampled
once per object, and word-level variables (i.e., @xmath and @xmath ),
which are sampled every time a feature is extracted. The variables
@xmath , @xmath and are latent variables that should be inferred. Assume
everything is observed and a category label is selected for each object;
i.e., each object belongs to one category. The joint distribution of all
hidden and observed variables for a category is defined as follows:

  -- -------- -- --------
     @xmath      (4.16)
  -- -------- -- --------

where @xmath and @xmath are Dirichlet prior hyper-parameters that affect
the sparsity of distributions. Although, @xmath and @xmath could be
vector-valued, we assume symmetric Dirichlet priors, with @xmath and
@xmath each having a single value. @xmath is the number of topics,
@xmath is the number of known objects in the category @xmath and @xmath
is the number of words in object @xmath . Each @xmath represents an
instance of category @xmath in topic-space as a histogram (i.e., topic
layer ), represents an object as a vector of visual words, @xmath ,
where each entry represents the index of one of the @xmath words of the
dictionary (i.e., BoW layer ). Next, the object should be described as a
histogram of topics, @xmath . It should be noticed that there is a topic
for each word. Therefore, the object is first described as a set of
topics @xmath , where each element of represents the index of one of the
@xmath topics and then the object is represented as histogram of topics.
@xmath is a @xmath matrix, which represents word-probability matrix for
each topic, where @xmath is the size of the dictionary and @xmath ;
thus, the posterior distribution of the latent variables given the
observed data is computed as follows:

  -- -------- -- --------
     @xmath      (4.17)
  -- -------- -- --------

As Blei ( 2012 ) showed, the denominator of the equation 4.17 is
intractable and cannot be computed exactly due to a large number of
possible instantiations of the topic structure. Griffiths and Steyvers (
2004 ) proposed an analytical solution based on collapsed Gibbs sampling
(Porteous et al., 2008 ) that can be used. Since @xmath and @xmath can
be derived from @xmath , they are integrated out from the sampling
procedure. In this work, for each category, an incremental LDA model is
created. Whenever a new training instance is presented, the collapsed
Gibbs sampling is employed to update the parameters of the model. The
collapsed Gibbs sampler is used to estimate the probability of topic
@xmath being assigned to a word @xmath , given all other topics assigned
to all other words:

  -- -------- -- --------
     @xmath      (4.18)
  -- -------- -- --------

where @xmath shows the relative counters of category @xmath ; @xmath
means all hidden variables expect @xmath and @xmath and @xmath is the
number of times topic @xmath from category @xmath is assigned to some
visual word in object @xmath and @xmath shows the number of times visual
word @xmath is assigned to topic @xmath . In addition, the denominator
of the @xmath is omitted because it does not depend on @xmath . After
the iterative sampling procedure, the multinomial parameter sets @xmath
(i.e., object-topic matrix) and @xmath (i.e., topic-word matrix) can be
estimated using the following equations:

  -- -------- -- --------
     @xmath      (4.19)
  -- -------- -- --------

where @xmath is the number of times a word assigned to topic @xmath in
category @xmath and @xmath is the number of words in the object @xmath .

What happens next depends only on the current state of the system and
not on the sequence of previous states. Therefore, whenever a new
training instance, @xmath , is taught by an instructor (i.e., supervised
learning), the collapsed Gibbs sampler is employed to first estimate a
topic for each word of the given instance and then update the parameters
of the model including @xmath and @xmath incrementally (i.e.,
unsupervised learning). Figure 4.8 represents the 25 sample topics
learned from the Washington RGB-D object dataset (Lai et al., 2011a ) .
@xmath is the object-topic matrix, where each row represents a training
instance in topic-space as a histogram:

  -- -------- -- --------
     @xmath      (4.20)
  -- -------- -- --------

Examples of object-topic representations for eight categories of the
Restaurant Object Dataset (Kasaei et al., 2015a ) are depicted in Fig.
4.9 . The proposed Local LDA procedure is summarized in Algorithm 4.1.

To assess the dissimilarity between a target object view and stored
instances of a certain category @xmath , the target object should be
described based on the learned topics of the category @xmath as a
histogram (Eq. 4.19 ). To this end, the target object and a temporal
copy of @xmath and @xmath counters are first loaded into the Gibbs
sampler. Then, a topic for each word of the target object is estimated.
Finally, the target object is represented as a topic frequency
histogram.

##### Lda

We also implement a modified version of the standard smoothed LDA
approach (Blei et al., 2003 ) . The basic idea is to represent the
objects as random mixtures over topics (i.e., latent variables shared
among all categories), where each topic is characterized by a
distribution over visual words (i.e., observed variables). Both random
distributions are defined using multinomial distributions and their
parameters are characterized by a Dirichlet distribution. As Blei ( 2012
) showed, the posterior distributions of the latent variables given the
observed data cannot be computed exactly due to the large number of
possible instantiations of the topic structure. Griffiths and Steyvers (
2004 ) proposed an analytical solution based on collapsed Gibbs sampling
(Porteous et al., 2008 ) that can be used to solve the inference
problem. Similar to the Local LDA approach, an object is first
represented as a set of visual words, @xmath , where each entry
represents one of the @xmath words of the dictionary. Next, the object
should be described as a set of topics @xmath , where each element of
represents the index of one of the @xmath topics. Towards this goal, the
probability of topic @xmath being assigned to a word , given all other
topics assigned to all other words, i.e., @xmath , is estimated by the
collapsed Gibbs sampler (Porteous et al., 2008 ) . After the iterative
sampling procedure, the multinomial parameter sets @xmath (i.e.,
object-topic matrix) and @xmath (i.e., topic-word matrix) can be
estimated using the following equations:

  -- -------- -- --------
     @xmath      (4.21)
  -- -------- -- --------

where @xmath is the number of times topic @xmath was assigned to some
visual word in object @xmath and @xmath is the number of words in the
object @xmath ; @xmath is the number of topics and @xmath shows the
number of times visual word @xmath was assigned to topic @xmath ; @xmath
is the number of times a word was assigned to topic @xmath and @xmath
and @xmath are Dirichlet prior hyper-parameters that affect the sparsity
of distributions. @xmath is a @xmath matrix, which represents
word-probability matrix for each topic, where @xmath is the size of
dictionary, @xmath is the number of topics and @xmath .

In this approach, whenever a teacher adds a new training instance to a
category, the collapsed Gibbs sampler is employed to first estimate a
topic for each word of the given instance and then update the @xmath and
@xmath parameters incrementally (i.e., unsupervised learning). Similar
to the Local LDA, @xmath is the object-topic matrix, where each row
represents a training instance in topic-space as a histogram. The obtain
histogram can be used as a training instance in instance-based learning
or can be used to create or update the probabilistic object category
models based on Bayesian learning. To be classified, a target object
view, , should be first represented as a topic distribution @xmath (Eq.
4.20 ). Towards this goal, the target object and a temporal copy of
@xmath and @xmath are loaded into the Gibbs sampler and then a topic for
each word of the target object is estimated. Next, the object is
represented as a fixed-size histogram in the topic-space. Then, the
dissimilarity between the target object and learned categories can be
estimated.

### 4.4 Summary

This chapter presented a set of object representation approaches for 3D
object category learning and recognition. In particular, we presented a
new global object descriptor named GOOD (i.e., Global Orthographic
Object Descriptor) that provides a good trade-off between
descriptiveness, computation time and memory usage, allowing concurrent
object recognition and pose estimation. For an object view, GOOD
provides a unique and repeatable local reference frame and three
principal orthographic projections. GOOD is then calculated with the
discretization of the three orthographic projections computed on that
reference frame and concatenates them to form a single description for
the given object. We release the source code of the GOOD descriptor, to
the benefit of the research community, in Point Cloud Library ³ ³ 3
http://pointclouds.org/ (PCL version 1.9) and our Github repository ⁴ ⁴
4 https://github.com/SeyedHamidreza/GOOD_descriptor .

Furthermore, we presented a set of object representation approaches
based on local features to enhance a 3D object category learning and
recognition. In this part, we mainly focused on representing objects
based on local shape features, BoW and topic-based representations to
construct powerful object descriptions. In particular, we propose an
extension of Latent Dirichlet Allocation to learn structural semantic
features (i.e., topics) from low-level feature co-occurrences for each
category independently. In this approach, for optimizing the recognition
process and memory usage, each object view was described as a random
mixture over a set of latent topics, and each topic was defined as a
discrete distribution over visual words. Topics in each category are
discovered in an unsupervised fashion and are updated incrementally
using new object views. In the next chapter, we will propose and discuss
a set of open-ended object category learning and recognition approaches.

## Chapter 5 Open-Ended Object Category Learning and Recognition

Nowadays robots utilize 3D computer vision algorithms to perform complex
tasks such as object recognition and manipulation. Although, many
problems have already been understood and solved successfully, many
issues still remain. Open-ended object recognition is one of these
issues waiting for many improvements.

In particular, most robots lack the ability to learn new objects from
past experiences. To migrate a robot to a new environment one must often
completely re-generate the knowledge base that it is running with. Since
in open-ended domains the set of categories to be learned is not
predefined, it is not feasible to assume that one can pre-program all
necessary object categories for assistive robots. Instead, robots should
learn autonomously from novel experiences, supported in the feedback
from human teachers. This way, it is expected that the competence of the
robot increases over time.

Several state-of-the-art assistive robots use traditional object
category learning and recognition approaches (Leroux and Lebec, 2013 ;
Beetz et al., 2011 ; Vahrenkamp et al., 2010 ) . These classical
approaches are often designed for static environments in which it is
viable to separate the training (off-line) and testing (on-line) phases.
In these cases, the world model is static, in the sense that the
representation of the known categories does not change after the
training stage. Therefore, these robots are unable to adapt to dynamic
environments (Jeong and Lee, 2012 ) . This leads to several shortcomings
such as the inability to detect/recognize previously unseen categories.
To cope with these issues, several cognitive robotics groups have
started to explore how robots could learn incrementally from their own
experiences as well as from interaction with humans (Smith and Gasser,
2005 ; Chauhan and Seabra Lopes, 2011 ; He and Chen, 2008 ) . In order
to incrementally adapt to new environments, an autonomous assistive
robot must have the ability to process visual information and conduct
learning and recognition tasks in a concurrent and interleaved fashion.

We approach object category learning and recognition from a long-term
perspective and with emphasis on open-endedness, i.e., not assuming a
pre-defined set of categories. Learning methods used in most of the
classical object recognition systems are not designed for open-ended
domains, since those methods do not support an incremental update of the
robot’s knowledge based on new experiences. In contrast, open-ended
learning approaches can, not only incrementally update the acquired
knowledge (category models), but also extend the set of categories over
time, which is suitable for real-world scenarios. For example, if the
robot does not know how a ‘Mug’ looks like, it may ask the user to show
one. Such situation provides an opportunity to collect training
instances from actual experiences of the robot and the system can
incrementally update its knowledge rather than retraining from scratch
when a new instance is added or a new category is introduced.

In this chapter, the subject of online 3D object category learning and
recognition in open-ended robotic domains is investigated. The
characteristics of open-ended learning and the possible role of
human-robot interaction in that context are analyzed. Two main
open-ended learning approaches are investigated, namely instance-based
learning and model-based learning. Instance-based learning is explored
with both variable size representations (sets of local features) and
fixed size representations (GOOD, histograms of local features and
histograms of LDA topics). The notion of intra-category distance is
proposed as a reference to decide if an object belongs to one of the
known categories or if it belongs to an unknown category. For
model-based learning, the Bayesian learning approach is explored with
fixed size representations

Parts of the work presented herein have been published in various
international workshops (Kasaei et al., 2013 , 2016c ) , conferences
(Hamidreza Kasaei et al., 2014 ; Oliveira et al., 2014b ; Lim et al.,
2014a ; Dubba et al., 2014 ; Kasaei et al., 2015b , 2016b ) as well as
journal articles (Kasaei et al., 2015c ; Oliveira et al., 2015b ;
Hertzberg et al., 2014 ; Kasaei et al., 2018b ) . The remainder of this
chapter is organized as follows. In section 5.1 , we discuss related
works. Afterwards, the characteristics of open-ended learning are
presented in section 5.2 . The methodology of instance-based learning
approach is described in section 5.4 . Model-based object category
learning and recognition are then explained in section 5.5 . Finally,
conclusions are presented and future research is discussed.

### 5.1 Related Work

Object category learning and recognition has become an active subject in
several research communities such as computer vision and robotics
because of its potential applications. Over the past five decades,
different object category learning and recognition approaches have been
proposed (Andreopoulos and Tsotsos, 2013 ) .

Collet Romea et al. ( 2009 ) described an object recognition and pose
estimation system based on one-step learning. In this case, the system
is decomposed into an off-line training stage and an on-line recognition
stage. In the training stage, for every object, a set of images are
captured from different viewpoints. Then, SIFT features are extracted
for each image and stored in a database. During the recognition stage,
SIFT features are computed for the current view and matched against the
training models. The authors use a Best Bin First algorithm (Beis and
Lowe, 1997 ) for matching. Kootstra et al. ( 2008 ) proposed an active
perception system for recognizing objects that are placed in cluttered
and uncontrolled environments. They used a mobile robot that explores
the objects by circling around them and capturing data. They also used
SIFT descriptors for learning and recognition tasks. Liu et al. ( 2006 )
developed a system based on the bag-of-words technique to optimize
memory usage and the recognition process. The authors investigated the
problem of efficient partial 3D shape retrieval. First, a Monte-Carlo
method to select interest points is proposed, and then, the spin-image
descriptors are used to encode the geometry around the interest points.
In the recognition stage, they proposed to use a dissimilarity measure
based on the asymmetric Kullback-Leibler divergence.

Willow Garage started a project named Object Recognition Kitchen (ORK) ¹
¹ 1 http://wg_perception.github.io/object_recognition_core/ , a 3D
object recognition system built on top of the Ecto framework ² ² 2
http://plasmodic.github.io/ecto/ . ORK was designed to run
simultaneously several traditional object recognition techniques, so
that these can be combined for example using a voting scheme. Ecto is a
C++ / Python computation graph framework, which can organize the
computation modules in a directed acyclic graph. In ORK, the training
and recognition are not simultaneous.

In all the systems described above, training and testing are separate
processes, i.e., they do not occur simultaneously. However, in
open-ended applications, data is continuously available and the target
object categories are not known in advance. In these cases, traditional
object recognition approaches are not well suited, because those systems
are limited to using off-line data for training and are therefore unable
to adapt to new objects and new environments.

There are some approaches which support incremental learning of object
categories. In these approaches, the set of classes is predefined and
the models of known object categories are enhanced (e.g., augmented,
improved) over time. He and Chen ( 2008 ) proposed an incremental
multiple-object recognition and localization (IMORL) framework using a
multilayer perceptron (MLP) structure as the base learning model. The
authors claimed that the proposed framework can incrementally learn from
experiences and use such knowledge for object recognition. Yeh and
Darrell ( 2008 ) developed novel methods for efficient incremental
learning of SVM-based visual category classifiers, and showed that,
using their framework, it is possible to adapt the classifiers
incrementally. Martinez Torres et al. ( 2010 ) described a fast and
scalable perception system for object recognition and pose estimation.
The authors employed the RANSAC and Levenberg Marquardt algorithms to
segment objects and represented them based on SIFT descriptors.

In the last decade, various research groups have made substantial
progress towards the development of learning approaches which support
life-long object category learning (Oliveira et al., 2016 ; Kasaei
et al., 2016a , 2015b ) . These methods assume that the set of
categories to be learned is not known in advance. For instance,
zero-shot, low-shot, and open-ended learning approaches have recently
received significant attention from the machine learning and computer
vision communities (Akata et al., 2014 ; Hariharan and Girshick, 2017 ;
Kasaei et al., 2016a ) . Zero-shot learning approaches aim to recognize
instances of object categories that have not been included during the
training phase. This type of learning approaches, in addition to visual
features, use textual descriptions of object categories to train
classifiers. In other words, they first project object categories into a
semantic space learned using large text corpora, and then predict an
initial model for new categories by image synthesis methods (Akata
et al., 2014 ) . Although, this type of approaches could be used to
reduce the number of images required to train a model by incorporating
textual information, they are out of the scope of this thesis.

Low-shot learning usually consists of two phases including
representation learning and few-shot learning for classification
purposes . These approaches mainly assume a large set of training data
is available in advance for the representation learning phase and focus
on transfer learning and classification tasks. The representation
learning phase is commonly based on training a Deep Convolutional
Networks (ConvNet). These approaches train a linear classifier in the
low-shot learning phase (Hariharan and Girshick, 2017 ) . The learned
representation is usually frozen and not changed during the test phase.
Such approaches require many training instances, and their learning rate
is slow and typically involves long training times. From an applied
perspective, collecting massive amounts of labeled data is costly in
many domains (e.g. robotics).

In contrast, open-ended learning allows for concurrent or interleaved
learning and recognition. In open-ended learning, the learning agent
extracts training instances from its on-line experiences. Thus, training
instances become gradually available over time, rather than being
partially or completely available at the beginning of the learning
process (Kasaei et al., 2016a ) . Interactive open-ended object category
learning and recognition is a key capability in assistive and service
robots. This means that a robot should be capable of continuously
learning new objects in order to perform different tasks. This type of
object category learning and recognition approaches is extremely useful
in assistive and service robots since the end user expects that the
competence of the robot increases over time.

Steels and Kaplan ( 2002 ) use the notion of “language game” to develop
a social learning framework through which an AIBO robot can learn its
first words. A teacher points to objects and provides their names. The
robot uses color histograms and an instance-based learning method to
learn word meanings. The teacher can also ask questions and provide
feedback on the robot’s answers. The authors show, with concrete robotic
experiments, that unsupervised category formation may produce categories
that are completely unrelated to the categories that are needed for
grounding the words of the used language. They therefore conclude that
social interaction must be used to help the learner focus on what needs
to be learned in the context of communication.

Seabra Lopes and Chauhan ( 2008 ) developed a category learning
architecture that included a metacognitive processing component.
Multiple object representations and multiple classifiers combinations
were used. Classifier combinations are based on majority voting and the
Dempster-Shafer evidence theory. All learning computations are carried
out during the normal execution of the agent, which allows continuous
monitoring of the performance of the different classifiers. The measured
classification successes of the individual classifiers support an
attentional selection mechanism, through which classifier combinations
are dynamically reconfigured and a specific classifier is chosen to
predict the category of a new object. In another work, the same authors
approached the problem of object experience gathering and category
learning with a focus on open-ended learning and human-robot interaction
(Chauhan and Seabra Lopes, 2011 ) . Moreover, they considered forgetting
rules to optimize memory usage. In this approach, the user can provide
the names of objects through pointing and verbal teaching actions. The
user can also ask questions about the categories of objects under shared
attention and, if appropriate, provide corrective feedback. The authors
assume that a long-term category learning process in an artificial agent
will eventually reach a breakpoint, i.e., an internal state of the agent
in which new categories can no longer be discriminated. They showed a
system that starts with an empty vocabulary and can incrementally
acquire object categories through the interaction with a human user. In
both works, they used RGB data whereas we used depth data. Moreover,
their object detection, learning and recognition approaches are
completely different from our approach.

Iwahashi et al. ( 2010 ) approach their concept learning problem in an
incremental and online manner. Their robotic platform consists of a
robotic-arm with multi-fingered gripper (with tactile sensors), a
stereo-vision camera and an infra-red sensor. They have developed an
online learning approach (LCore) based on a Bayesian framework that
allows incremental category learning. The experimental setup involves a
human-user describing objects and/or actions in an environment visually
shared with the robotic agent. Robot’s participation can be passive
(e.g. human user moves the object and describes the action/object) or
active (e.g. human user asks the robot to move an object). During active
participation, in case the robot makes a mistake, the user verbally
corrects the robot. This interaction is carried out until the robot
performs the correct action. In a passive interaction or a correct
active interaction, the robot stores object features (colour, shape,
size and tactile information), object movement configurations and
phoneme strings (of each word) identified in a spoken utterance. LCore,
using new and previously stored knowledge, calculates joint
probabilities over co-occurring speech, visual (shape, size and color
descriptors) and tactile information. This process leads to grounding
names of concrete categories (e.g. object names) and names of other
perceptual characteristics (e.g. words referring to color, shape or
size). Additionally, their agent is also able to learn motion concepts
(modeled by HMMs) and is able to ground some action words (e.g.
move-over, place-on).

Kirstein et al. ( 2012b ) proposed an approach for interactive learning
of multiple categories based on vector quantization and a graphical user
interface. The instructor could give the names of objects using the
graphical user interface. Using only 2D images, they showed the system
could successfully learn 5 color categories and 10 shape categories
observed in 56 objects. Collet et al. ( 2014 ) proposed a graph-based
approach for lifelong robotic object discovery. Similar to our approach,
they used a set of constraints to explore the environment and to detect
object candidates from raw RGB-D data streams. In contrast, their system
does not interactively acquire more data to learn and recognize the
object. Skočaj et al. ( 2016 ) describe a system with similar goals.
Fäulhammer et al. ( 2017 ) presente a system which allows a mobile robot
to autonomously detect, model and recognize objects in everyday
environments.

Currently, a popular approach in object recognition is deep learning. It
is now clear that if an application has a pre-defined fixed set of
object categories and thousands of examples per category, an effective
way to build an object recognition system is to train a deep
Convolutional Neural Network (CNN). However, there are several
limitations to use CNN in open-ended domains. CNNs are incremental by
nature but not open-ended, since the inclusion of novel categories
enforces a restructuring in the topology of the network. Moreover, CNN
usually need a lot of training data and cannot converge to a proper
solution using a small set of training data.

### 5.2 Characteristics of Open-Ended Learning

One of the primary challenges in service robotics is to allow robots to
adapt to open-ended dynamic environments where they need to interact
with non-expert users. For robots acting in these domains, it is not
viable to hand-code all possible behaviours and to anticipate all
possible exceptions. Instead, robots must be supported by life-long
learning and adaptation capabilities. Towards this goal, as proposed by
Seabra Lopes and Wang ( 2002 ) , the learning system of the robots
should have five basic characteristics:

-    Supervised  - to include the human instructor in the learning
    process. This is an effective way for a robot to obtain knowledge
    from a human user/teacher.

-    On-line  - meaning that the learning procedure takes place while
    the robot is running.

-    Opportunistic  - apart from learning from a batch of labelled
    training data at predefined times or according to a predefined
    training schedule, the robot must be prepared to accept a new
    example when it is observed or becomes available.

-    Incremental  - it is able to adjust the learned model of a certain
    category when a new instance is taught.

-    Concurrent  - it is able to handle multiple learning problems at
    the same time (e.g., learn object category model as well as
    learning/improving underlying object representation such as topics).

As surveyed in the previous section, learning approaches used in
classical object category recognition approaches, do not satisfy some of
these requirements. For instance, most of these approaches support
incremental learning, but are not designed for scaling up to larger sets
of categories and do not support open-ended learning. It is worth
mentioning that the target set of categories is predefined in standard
incremental learning, while in open-ended learning the target set of
categories is not known in advance and is continuously growing. In an
open-ended scenario, an instructor may teach the robot a set of new
instances from previously seen as well as new categories at any
arbitrary time. Given the open-ended nature of category learning,
different learning methods, presented in this thesis, are designed to
support open-ended learning of an arbitrary set of categories. In such
approaches, the introduction of new categories can significantly
interfere with the existing category descriptions. To cope with this
issue, memory management mechanisms, including salience and forgetting,
can be considered. Moreover, as more categories are learned, the
classification accuracy first decreases (performance degradation phase),
and then starts going up again as more categories are introduced
(recovery phase). This is expected since the number of categories known
by the system makes the classification task more difficult. However, as
the number of learned categories increases, also the number of instances
per category increases, which augments the category models and therefore
improves performance of the system. Eventually the learning agent
reaches to a breakpoint where s/he is no longer able to learn more
categories (Chauhan, 2014 ) . Seabra Lopes and Chauhan ( 2007 ) states
these features of an open-ended learning process:

-    Evolution: The classification performance should be improved as the
    number of examples per category increases while no new categories
    are introduced. Moreover, the prediction accuracy should not
    fluctuate at every incremental learning step.

-    Recovery: With the increase in the number of categories, it is
    expected that the prediction accuracy decreases. The time spent in
    system evolution until correcting and adjusting all current
    categories defines recovery. In other words, if the performance
    drops at a certain learning step, the agent should be able to
    recover to the previous best performance. Recovery is based on
    classification errors and corresponding corrections as new instances
    become available.

-    Breakpoint: The agent is no longer able to recover and learn more
    categories.

### 5.3 Human-Robot Interaction

In the present work, a human user teaches the robot the names of the
objects present in their visually shared environment using the developed
interface for Human-Robot Interaction (HRI) (i.e., described in section
3.5 ). Category names are then grounded by the robot in sensor-based
object representations, leading to a vocabulary shared with its teacher.
The HRI interface provides a set of actions that the instructor can use
for interacting with the robot. For object category learning and
recognition purposes, the primary action is the teach action which allow
a user to provide the category labels of objects, present in a shared
scene, to the robot. Another important action is the correct action. In
particular, whenever the agent could not classify an object correctly,
the user has the facility to give correction. Therefore, at the most
basic level of interaction, the interface allows the user to perform
four main actions:

-    Select: point to the desired object using either the object’s
    TrackID or pointing to the object with the arm.

-    Teach: provide the category label of the selected object.

-    Ask: inquire the category label of the selected object, which the
    agent will predict based on previously learned knowledge.

-    Correct: if the agent could not recognize a given object correctly,
    the user can teach the correct category.

The robot responds to the teacher actions by either running the relevant
learning functionalities (i.e., in the cases of teach and correct
actions) or performing classification task (i.e., in response to an ask
action). In particular, the robot’s possible responses include:

-    Learning : teach and correct actions trigger perceptual learning to
    create new or modify existing category models, and create
    association between the category label and its corresponding
    category description.

-    Recognition : in response to the ask action, the results of object
    recognition are visually available to the user (e.g. user can see
    which object was recognized correctly) and divergence from expected
    response leads the user to provide corrective feedback.

In the following subsections two object category learning and
recognition techniques are presented to support open-ended learning of
an arbitrary set of categories.

### 5.4 Instance-Based Learning

As discussed in the previous chapter, either a layered data processing
system based on local features or a global object descriptor can be used
for object representation purposes. The layer-based data processing
system builds an increasingly complex object representation for the
given object view. It consists of five layers, including feature layer ,
BoW layer , topic layer , object view layer and category layer (see
Section 4.3 ). In the feature layer each object view, @xmath , is
described by a set of spin-images. It is worth mentioning that the
number of keypoints depends on the size of the object and as a
consequence, we have a variable size representation for each object (see
Fig. 4.6 ). In the BoW layer , the given object view is represented as a
histogram of occurrences of visual words. In order to build more compact
and complex object representation in the topic layer , the given object
is represented as a histogram of topics. Object category learning can
occur at all layers.

As proposed in Section 4.2 , an object can also be represented based on
global descriptors. Similar to the BoW and LDA-based approaches, global
object descriptors produce a fixed size representation for a given
object.

#### 5.4.1 General Aspects

The instance-based approach considers category learning as a process of
learning about the instances of the category (see Fig. 5.1 ). In other
words, a category is represented by a set of views of instances of the
category (Daelemans and Van den Bosch, 2005 ) :

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

where @xmath are the constituent views. As discussed in Section 5.3 ,
the user identifies the need for teaching a new category or for
providing corrective feedbacks when s/he notices classification errors
in robot’s responses. As described in chapter 3 , the object perception
system stores key views of the tracked objects. Therefore, when the user
teaches or provides corrective feedback about the category of a target
object, the learning module will store one or more views of that object.
In an instance-based learning framework, the stored object views are the
instances. In particular, new instances are stored in the Perceptual
Memory in the following situations:

-   When the teacher for the first time teaches a certain category,
    through a Teach or a Correct action, an instance-based
    representation of this new category is created and initialized with
    the set of key views of the target object collected since object
    tracking started:

      -- -------- -- -------
         @xmath      (5.2)
      -- -------- -- -------

    where @xmath is the number of stored key object views for the first
    teaching action.

-   In later teaching actions, the target object views are added to the
    instance-based representation of the category:

      -- -------- -- -------
         @xmath      (5.3)
      -- -------- -- -------

    where @xmath is the number of stored key object views for the @xmath
    -th teaching action.

Whenever a new object view is added to a category, the Object
Conceptualizer module retrieves the current model of the category as
well as the representation of the new object views, and creates a new,
or updates the existing category. To assess the dissimilarity between a
target object and stored instances, a recognition mechanism in the form
of a nearest neighbour classifier is usually used. Therefore, each
instance-based object learning and recognition approach can be seen as a
combination of a particular object representation , similarity measure
and classification rule . One advantage that instance-based learning has
over other methods of machine learning is its ability to adapt its model
to previously unseen data. The disadvantage of this approach is that the
computational complexity can grow with the number of training instances.
The computational complexity of classifying a single new instance is
@xmath , where @xmath is number of instances stored in memory. Salience
and forgetting mechanisms can be used to bound the memory usage (Seabra
Lopes and Chauhan, 2008 ) . These mechanisms are also useful for
reducing the risk of overfitting to noise in the training set. Another
advantage of the instance-based approach is that it facilitates
incremental learning in an open-ended fashion.

In the following sub-sections, we present and discuss instance-based
object category learning and recognition approaches for both variable
size object representations (i.e., set of spin-images) and fixed size
object representations such as BoW, LDA and GOOD representations.

#### 5.4.2 The “Unknown Category” Problem

Most of the work in this thesis is focused on maximizing scalability,
i.e., the capacity to scale to larger and larger sets of categories
while minimizing the losses in other performance metrics. Another
problem that is typical of open-ended learning in real-world scenarios
is what we call here the “unknown category” problem.

In classical learning and classification approaches, there is a set of
target categories to be learned and the target objects to be recognized
are assumed to belong to one of the target categories. In contrast, in
open-ended scenarios, the learning agent must be prepared to handle
situations where the target object does not belong to any of the known
categories.

To realize that the target object belongs to an unknown category, the
agent must reason in terms of distances to the categories taking also
into account the size or spread of the categories. High intra-class
variation means that objects from the same category can have very
different shapes. For instance, the Cup category contains many types of
cups with different shapes (see Fig. 5.2 ). Even the views of the same
object can vary significantly depending on the viewpoint. Therefore,
when judging if a given object belongs to a given category, it is not
enough to compute the nearest-neighbor distance or the centroid
distance. Such distances are only meaningful when compared to the spread
of the category. In this work, to address this problem, a measure of
category spread called the Intra-Category Distance ( @xmath ), is
introduced:

  -- -- -- -------
           (5.4)
  -- -- -- -------

where @xmath is the total number of instances (i.e., object views) in
the category, and are two different known instances of and @xmath is a
measure of distance between two object views. This way, we represent an
object category by ( i ) a set of known instances and ( ii ) the
corresponding Intra-Category Distance. Whenever new instances are added
to a category, the object conceptualizer retrieves the instances of the
object category from memory and the set of features describing the new
instances, and then re-computes ICD and stores the new ICD value.

The “unknown category” problem is relatively easy to handle when
compared to the scalability problem mentioned earlier. Therefore, most
of the learning approaches were formulated having in mind scenarios in
which, for simplicity, the “unknown category” problem is assumed not to
exist. The “unknown category” problem was addressed (based on ICD) only
in instance-based learning with a variable size representation (sets of
spin images, in the next subsection).

#### 5.4.3 Variable Size Representations: Sets of Local Features

This section presents the instance-based learning approach developed by
us for the object perception and perceptual learning system of the RACE
project (Kasaei et al., 2015c ; Oliveira et al., 2015b ) . It is based
on representing objects as sets of spin images and uses ICD to normalize
distances and to decide when an object belongs to an unknown category.

##### Basic Distance Metric

Since in this approach the size of an object representation depends on
the size of the object (number of points, number of keypoints), we
cannot use common distance metrics such as Euclidean distance.
Therefore, we propose the following function to estimate the distance or
dissimilarity between two object views (i.e., two sets of spin-images):

  -- -------- -- -------
     @xmath      (5.5)
  -- -------- -- -------

where @xmath , and @xmath , are the spin-images of U and V respectively,
@xmath is the number of spin-images in U and @xmath is the Euclidean
distance. It should be noted that D(·) is not symmetric (i.e., @xmath ).

#### 5.4.4 Category Initialization and Update

As discussed above, the teach and correct actions from the user trigger
the object conceptualizer module to create a new category or modify an
existing category. If the integrated system is used, the instance-based
is updated as described in section 5.4.1 . ICD is initialized when at
least three views are available. For experiment based on datasets, the
following schema is used. Whenever a new object category is taught,
three consecutive object views are stored to initialize the category and
to compute the relevant ICD:

  -- -------- -- -------
     @xmath      (5.6)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (5.7)
  -- -------- -- -------

If the user provides corrective feedback, or teaches a new instance of a
known category, the agent retrieves the current object category from
memory, as well as a set of features describing the labeled object, and
updates the category by adding the new instance to the set of instances
of the category and re-computing the ICD:

  -- -------- -- -------
     @xmath      (5.8)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (5.9)
  -- -------- -- -------

Such interactive teaching mechanism provides an opportunity to collect a
training instance (an experience) for learning in an open-ended manner.
In this approach, the set of object categories to be learned is not
known in advance. The training instances are extracted from actual
experiences of a robot, and thus become gradually available, rather than
being available at the beginning of the learning process. Finally, it
updates the category model by storing the ICD and representation of the
new instance. For the object recognition purpose, we consider two
approaches. In the following subsections, the details of each approach
are presented.

##### Normalized Object-Category Distance: Approach I

In this approach, the proposed unsymmetrical distance function (Eq. 5.5
) is used to estimate the dissimilarity between a target object view and
a stored instance of a category in memory. In order to define
Object-Category Distance, @xmath , the minimum distance between the
target object, and all the instances of a certain category, , is
considered as the @xmath :

  -- -------- -- --------
     @xmath      (5.10)
  -- -------- -- --------

In object recognition, since some categories are more spread than
others, normalizing @xmath by @xmath will help to prevent
misclassification. Therefore, the Normalized Object-Category Distance of
the target object view to the category , @xmath , is computed as
follows:

  -- -------- -- --------
     @xmath      (5.11)
  -- -------- -- --------

##### Normalized Object-Category Distance: Approach II

In this approach, we consider computing the @xmath based on the average
distance between the target object, and all the instances of a certain
category, :

  -- -- -- --------
           (5.12)
  -- -- -- --------

In this approach, the Normalized Object-Category Distance, @xmath ,
between the target object and stored instances of category is computed
by:

  -- -------- -- --------
     @xmath      (5.13)
  -- -------- -- --------

where @xmath is the average of the intra-category distances of all
categories:

  -- -------- -- --------
     @xmath      (5.14)
  -- -------- -- --------

where @xmath is the number of categories.

##### Classification rule

Finally, the target object is classified based on the minimum normalized
distance to the known categories:

  -- -------- -- --------
     @xmath      (5.15)
  -- -------- -- --------

This classification rule can be easily modified to predict an “ unknown
” category when the target object is too far from all known categories.
This can be done by considering:

  -- -------- -- --------
     @xmath      (5.16)
  -- -------- -- --------

where the Classification Threshold (CT) is used to decide when the
target object is too far from the known categories. Two examples of the
instance-based 3D object recognition are shown in Fig. 5.3

#### 5.4.5 Fixed Size Representations: GOOD, BoW, LDA

Fixed size representations (i.e., histograms) are frequently used in
various object classification tasks to represent rich information in
local/global regions of objects. Global object descriptors such as GOOD
and machine-learning techniques based on BoW and topic modelling produce
a fixed size representation for a given object. Such representations are
usually compact, rotation-invariant, translation-invariant and
normalized. These properties make these representation methods suitable
for performing object recognition task.

In this approach, a purely memory-based learning approach is adopted, in
which a category is represented by a set of representations of instances
of the category. Similar to the instance-based approaches, teaching and
correction by the user lead the agent to add a new instance to the
taught category. In this approach a category is initialized using one
instance (see Eq. 5.2 ) and whenever the agent receives a corrective
feedback, it adds the object to the set of instances of the category
(see Eq. 5.3 ).

To classify a previously unseen instance, we compute the similarity of
the target instance with all the previously seen instances, which have
been stored in memory. Finally, the target object is classified based on
the minimum distance to the known instances. In this approach, the
choice of similarity metric has a great impact on the recognition
performance. The similarity between two object views can be computed by
different standard distance functions. We refer the reader to a
comprehensive survey on distance/similarity measures provided by Cha (
2007 ) . After performing several cross-validation experiments with
different representations, we conclude two type of distance functions
are suitable to estimate the similarity between two instances. One
function properly works with GOOD and BoW representations, and the other
one shows good results with LDA-based representations. Both functions
are in the form of a bin-to-bin distance function which compares
corresponding bins in two object representation vectors.

Mathematically, let P and Q @xmath be two vectors with the same size.
One of the common forms of bin-to-bin distance function is the
Minkowski-Form, also known as @xmath norm:

  -- -------- -- --------
     @xmath      (5.17)
  -- -------- -- --------

where @xmath is the size of representation and @xmath returns the
absolute value. The most widely used distances from this family are
@xmath and @xmath forms. The @xmath is also known as the Manhattan
distance and the @xmath distance is also known as the Euclidean
distance. Throughout this thesis, we use the @xmath norm (i.e.,
Euclidean distance) with the BoW and GOOD representations.

Since LDA-based approaches represent a given object in the form of
probability distribution, we use a specific type of Kullback–Leibler
(KL) divergence to estimate the similarity of two instances. The KL
divergence is defined as:

  -- -------- -- --------
     @xmath      (5.18)
  -- -------- -- --------

This form of KL is non-symmetric (i.e., @xmath ). One of the limitations
of KL is that when @xmath , it equals infinity and when @xmath , it is
undefined. To overcome KL limitations, the Jensen-Shannon (JS)
divergence was suggested (Pele and Werman, 2009 ) :

  -- -------- -- --------
     @xmath      (5.19)
  -- -------- -- --------

where @xmath . One of the specific distance functions that can be
derived from @xmath using Taylor extension, is the chi-squared distance
(Yang et al., 2015 ; Pele, 2011 ) . The function is known in the
computer vision community as @xmath distance:

  -- -------- -- --------
     @xmath      (5.20)
  -- -------- -- --------

Pele and Werman ( 2008 ) and Pele ( 2011 ) showed that the practical
results of @xmath and JS are almost identical. Since the computation of
@xmath is more efficient than the JS, we use @xmath function to estimate
the similarity of two LDA representations.

### 5.5 Model-Based Learning

The model-based learning approaches are often contrasted with
instance-based approaches. The instance-based learning considers
category learning as a process of learning about the instances of the
category while the model-based learning is a process of learning a
parametric/non-parametric model from a set of instances of a category.
In other words, each category is represented by a single model. The
instance-based approach proposes that a new object is compared to all
previously seen instances, while the model-based approach proposes that
a target object is compared to the model of categories. Therefore, in
the case of recognition response, the model-based approach is faster
than the instance-based approach. In contrast, instance-based learning
approach can recognize objects using small number of experiences, while
model-based approaches need more experiences to achieve a good
classification results. Therefore, training is very fast in instance
based approach while they require more time in recognition phase.
Another disadvantage of instance-based learning approach is that they
need a large amount of memory to store the instances.

In this section, we propose an open-ended 3D object category learning
approach, which considers category learning as a process of computing a
probabilistic model for each object category using the Naive Bayes
approach. Similar to the instance-based approach, teaching and
correction by the user lead the agent to create a new category or to
update an existing category. In the classification phase, the likelihood
between a target object and all object category models is first
estimated. Then, the target object is classified based on the maximum
likelihood. There are two reasons why Bayesian learning is useful for
open-ended learning. One of them is the computational efficiency of the
Naive Bayes approach. In fact, this model can be easily updated when new
information is available, rather than retrained from scratch. Second,
memory usage in instance-based open-ended systems is continuously
growing since these systems are constantly storing new object views
(instances). Therefore, these systems must resort to experience
management methodologies to discard some instances and thus prevent the
accumulation of a too large set of experiences (Wilson and Martinez,
2000 ; Seabra Lopes and Chauhan, 2008 ) . In Bayesian learning, new
experiences are used to update category models and then the experiences
are forgotten immediately. The category model encodes the information
collected so far. Therefore, this approach consumes a much smaller
amount of memory when compared to any instance-based approach. This
learning approach can be used with all the proposed fixed size object
representations, i.e., BoW, LDA and GOOD. In the following sub-section,
we present how to acquire and refine object category models based on a
naive Bayes learning approach.

#### 5.5.1 Category Representation

Let us assume an object is represented by a n-dimensional vector @xmath
. In this approach, an object category, @xmath , is represented as a
tuple:

  -- -------- -- --------
     @xmath      (5.21)
  -- -------- -- --------

where @xmath is the number of seen instances in category @xmath , @xmath
is a vector of bin accumulators for category @xmath . In particular,
@xmath is the accumulation of @xmath bin of all instances of category
@xmath and the length of is equal to the length of . @xmath is the prior
probability of category @xmath and each element of @xmath shows the
probability of word/topic occurring in class @xmath or in the case of
GOOD descriptor, the @xmath shows the probability of a point falling
into the bin @xmath . In this work, we consider the probability of each
bin independently, regardless of any possible correlations with the
other bins (Naive Bayes approach). The @xmath is equivalent to the joint
probability model:

  -- -------- -- --------
     @xmath      (5.22)
  -- -------- -- --------

The joint model can be rewritten using conditional independence
assumptions:

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- -- --------
     @xmath      (5.23)
  -- -------- -- --------

where @xmath is the size of the object representation and @xmath is the
probability of the bin @xmath in an object of category @xmath .

#### 5.5.2 Category Initialization and Update

Similar to the instance-based approach, the teach and correct actions by
the user lead the robot to create a new category or to modify an
existing category. In particular, whenever the user explicitly teaches a
new category, the category is initialized using the key views of the
target object. For simplicity, the process is formalized below assuming
that each teaching action provides a single object view. It would be
straightforward to extend this formalization to a certain number of key
views of the target object ³ ³ 3 In the simulated teacher experiments,
the teach action always sends three views. .

The new instance, represented as a histogram @xmath , is added to the
taught category @xmath . Category initialization involves updating the
total number of instances of all known categories, @xmath , and
initializing category specific parameters, namely the number of
instances of the category, @xmath , and the bin accumulators, @xmath :

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- -- --------
     @xmath      (5.24)
  -- -------- -- --------

  -- -------- --
     @xmath   
  -- -------- --

If the user provides corrective feedback for a known category, @xmath ,
the category model is updated using that particular instance:

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- -- --------
     @xmath      (5.25)
  -- -------- -- --------

  -- -------- --
     @xmath   
  -- -------- --

Upon each teaching action, the probabilities are updated, namely the the
prior probabilities of all existing categories:

  -- -------- -- --------
     @xmath      (5.26)
  -- -------- -- --------

where @xmath is the number of known categories up to now and the
probability of each bin, @xmath , in the category @xmath , @xmath is
updated as follows:

  -- -------- -- --------
     @xmath      (5.27)
  -- -------- -- --------

Note that the probabilities are estimated with Laplace smoothing, by
adding one to every counter, in order to prevent @xmath .

#### 5.5.3 Classification Rule

To classify a given object O , which is represented as a n-dimensional
histogram, @xmath , the posterior probability for each object category
is approximated using the Bayes theorem as:

  -- -------- -- --------
     @xmath      (5.28)
  -- -------- -- --------

Because the denominator does not depend on @xmath , equation 5.28 is
re-expressed based on equation 5.23 and multinomial distribution
assumption:

  -- -------- -- --------
     @xmath      (5.29)
  -- -------- -- --------

In addition, to avoid underflow problems, the logarithm of the
likelihood is computed:

  -- -------- -- --------
     @xmath      (5.30)
  -- -------- -- --------

The category of the target object is the one with highest likelihood:

  -- -------- -- --------
     @xmath      (5.31)
  -- -------- -- --------

### 5.6 Summary

In this chapter, we first discussed the characteristics of open-ended
learning approaches. Afterwards, instance-based learning and model-based
learning approaches for grounding 3D object categories were presented in
details. In this work, “ open-ended ” implies that the set of object
categories to be learned is not known in advance. The training instances
are extracted from on-line experiences of a robot, and thus become
gradually available over time, rather than being completely available at
the beginning of the learning process.

In particular, we presented and discussed a set of instance-based
learning approaches for both variable size object representations (i.e.,
a set of spin-images) and fixed size object representations (i.e., BoW,
LDA and GOOD). In these approaches, learning a category involves storing
its representative object representations and the classifiers are
derived from a combination of a specific instance representation,
similarity measure and a classification rule. In the case of model-based
object category learning, a Naive Bayes learning approach was used. In
this approach, each object was first represented as a fixed-size
histogram (i.e., BoW, LDA and GOOD). Then, a probabilistic object
category model was computed for each category. For recognition, a
classification rule was used to assign a category label to the given
object.

Each approach has a set of parameters that must be tuned to provide an
appropriate balance between recognition performance, memory usage and
computation time. We address this point in the next chapter by
evaluating the performance of each approach with different
configurations using k-fold cross-validation method. This type of
evaluation provides a straightforward base for comparing different
approaches among themselves and possibly with other approaches described
in the literature.

## Chapter 6 Classical Evaluations

In this chapter, we present an evaluation of the multi-class learning
and classification approaches presented in the previous chapter for
object recognition. There are two main approaches to multi-class
classification: in single-label classification, each test object is
assigned to one and only one category; and in multi-label
classification, a test object can be assigned to one or more categories.
In our case, both instance-based and model-based approaches are designed
to perform single-label classification by taking advantage of the
relation between object representation, similarity measure and
classification rule. Each method has a set of parameters, in which its
performance strongly depends on the suitability of the value chosen for
each parameter. Therefore, in this chapter, we focus on classical
evaluation of the proposed approaches for different parameter
configurations to determine the best configurations.

To examine the performance of each approach with different
configurations, the @xmath -fold cross-validation procedure was used.
This is one of the most widely used methods for estimating the
generalization performance of a learning algorithm. In this case, the
@xmath folds are randomly created by dividing the dataset into @xmath
equal sized subsets, where each subset contains examples from all the
categories. In each iteration, a single fold is used for testing, and
the remaining folds are used as training data. For @xmath -fold
cross-validation, we set @xmath to 10, as is generally recommended in
literature. This type of evaluation is not only useful for parameter
tuning, but it also provides straightforward results for comparing the
different approaches among themselves and possibly with other approaches
described in the literature.

These evaluations are conducted using the datasets described in Section
6.1 . Evaluation metrics are discussed in Section 6.2 . A set of
experiments was carried out to evaluate the performance of the proposed
instance-based object category learning and recognition approaches. The
obtained results are reported and discussed in Section 6.3 . Then, the
proposed model-based approaches are evaluated in Section 6.4 . In
Section 6.5 , we evaluate in detail the proposed Global Orthographic
Object Descriptor (GOOD) concerning descriptiveness , scalability ,
robustness and efficiency characteristics. It is worth mentioning that
all tests were performed with an i7, 2.40GHz processor, and 16GB RAM.
Finally, summary and future research are presented and discussed.

### 6.1 Datasets

The experimental evaluation of different approaches, reported in this
chapter, was carried out on two object images datasets, including the
Restaurant Object Dataset (Kasaei et al., 2015a ) and the Washington
RGB-D Object Dataset (Lai et al., 2011b ) . In this chapter, we mainly
use the restaurant object dataset for evaluating the performances of the
proposed approaches because it has a small number of classes with
significant intra-class variation, which is suitable for performing
extensive sets of experiments. The Washington RGB-D object dataset is
used for evaluating the GOOD descriptor with respect to several
characteristics. The following subsections describe these datasets in
more detail.

#### 6.1.1 Restaurant Object Dataset

The Restaurant Object Dataset ¹ ¹ 1 Restaurant Object Dataset is
available online at : https://goo.gl/64IXx9 was created in the framework
of the RACE project. It contains 341 views of instances of 10 categories
from different perspectives ( Bottle , Bowl , Flask , Fork , Knife , Mug
, Plate , Spoon , Teapot , and Vase ) and 30 views of false or unknown
objects (e.g. points that belong to the instructor’s hands) (Kasaei
et al., 2015a ) . Each object view contains exactly one object. These
object views were extracted from 100 views of table-top scenes by
running the proposed object detection module and storing the segmented
point clouds. In our setting, a Kinect camera is placed about one meter
away from the table. This is the minimum distance required for the
Kinect camera to return reliable depth data. The images were recorded
with the camera mounted on a tripot at approximately @xmath height and
@xmath degrees relative to the table (Fig. 6.1 left ). All segmented
point clouds, i.e., the object views, were hand-annotated with the
respective category labels. Figure 6.2 displays one sample point cloud
per category and can give an idea of the type of objects in this
dataset.

#### 6.1.2 Washington RGB-D Object Dataset

Several experiments were also carried out on one of the largest
available RGB-D object datasets namely the Washington RGB-D Object
Dataset (Lai et al., 2011b ) . This dataset is highly popular in the
computer vision and robotics communities for evaluating 3D object
recognition methods. The Washington RGB-D Object Dataset is a large
scale dataset with respect to the number of images. It consists of
250,000 views of 300 common household objects taken from multiple views.
The objects are organized into 51 categories arranged using WordNet
(Miller, 1995 ) hypernym-hyponym relationships (similar to ImageNet
(Deng et al., 2009 ) ). This dataset was recorded using a Kinect style
3D camera that records synchronized and aligned @xmath RGB and depth
images. Each object was placed on a turntable and video sequences were
captured for one whole rotation. We have excluded the Ball and Binder
categories because of high shape similarity to the Apple and Notebook
categories respectively. Since we are using only depth information, it
is impossible to distinguish these categories only based on shape
features. Therefore, we worked with the remaining 49 categories. Figure
6.3 shows some example objects from the dataset (Lai et al., 2011b ) .

### 6.2 Evaluation Metrics

In machine learning literature, the main metrics for evaluating
supervised multi-class learning algorithms are accuracy , micro and
macro precision (i.e., indices @xmath and @xmath respectively) and micro
and macro recall . In multi-class classification, each test instance
gets assigned to one of the @xmath classes leading to an @xmath
confusion matrix, @xmath . All the mentioned evaluation metrics can be
computed from the confusion matrix as:

-   accuracy is computed as the fraction of correct predictions to total
    number of predictions:

      -- -------- -- -------
         @xmath      (6.1)
      -- -------- -- -------

-   @xmath is the ratio of number of correct predictions to total number
    of predictions.

      -- -------- -- -------
         @xmath      (6.2)
      -- -------- -- -------

-   @xmath is the average precision for @xmath categories. The precision
    of each category is computed as the ratio of number of correct
    predictions to total number of instances that are predicted to
    belong to that category.

      -- -------- -- -------
         @xmath      (6.3)
      -- -------- -- -------

-   @xmath : is the ratio of the number of correct predictions to the
    total number of instances of the known categories.

      -- -------- -- -------
         @xmath      (6.4)
      -- -------- -- -------

-   @xmath is the average recall across the N categories.

      -- -------- -- -------
         @xmath      (6.5)
      -- -------- -- -------

where @xmath is the total number of predictions, @xmath and @xmath are
true positive and false positive respectively. False negative and true
negative are shown by @xmath and @xmath respectively. It is worth
mentioning that the difference between @xmath and @xmath (also valid for
@xmath and @xmath ) is that @xmath gives equal weight to each category,
whereas @xmath gives equal weight to each prediction. Furthermore, there
are many other common metrics in machine learning literature such as
F-measure, kappa statistic, etc (Powers, 2011 ) . The focus of this
chapter is on evaluating the performance of the proposed approaches. The
choice of evaluation metric is based on its suitability. In multi-class
learning and recognition, if an approach can not detect unknown objects,
then @xmath , @xmath and @xmath will lead to the same results.
Therefore, we use accuracy as the primary evaluation measure in our
experimental setup in this chapter.

### 6.3 Instance-Based Learning

The instance-based learning approach is a baseline approach to evaluate
object representations for the purpose of object recognition.
Experiments in this section mainly reflect how the different object
representations support object recognition. Each approach has a set of
parameters that must be tuned to provide a good balance between
recognition performance, memory usage and computation time. As mentioned
earlier, the restaurant object dataset has a small number of classes
with significant intra-class variation. Therefore, we use this dataset
to examine the accuracy of each approach using different parameter
configurations. Based on the obtained results, we finally select a
default value for each parameter of all the approaches.

#### 6.3.1 Sets of Local Features

For the instance-based approaches with variable size representations,
i.e., Approach I and Approach II, as discussed in section 5.4.3 , two
sets of @xmath experiments were performed for different values of the
three system parameters namely the voxel size (VS), which is related to
the number of keypoints extracted from each object view, the image width
(IW) and the support length (SL) of spin images. Results are presented
in Table 6.1 . The accuracy value presented for each parameter value is
an average computed over the experiments carried out for all
combinations of the other parameters values. One important observation
is that, by increasing the SL, the classification performance also
increases. This is due to the fact that the support length parameter
determines the amount of space swept out by a spin-image, which will
have a radius of SL and height of @xmath SL. Therefore, selecting a
large value for the SL parameter causes the spin image to behave like a
global descriptor. To prove this observation, we ran a set of @xmath
experiments for each approach. Results are summarized in Table 6.2 . The
parameters that obtained the best average accuracy were selected as the
default system parameters. For Approach I, the default system
configurations are the following: VS = @xmath , IW = @xmath and SL =
@xmath . The accuracy of the proposed system with the default
configuration was @xmath percent. For Approach II, the following
parameters obtained the best average accuracy and were set as the
default system parameters: VS = @xmath , IW = @xmath and SL = @xmath .
The accuracy of the proposed system with this configuration was @xmath .
Results show that the overall performance of the recognition system is
promising. Spin images are capable of collecting distinctive traits of
the local surface patches of each object. For an entire experiment, the
average computation time for the first instance-based approach (
Approach I) was around @xmath seconds while the second approach (
Approach II) in average took around @xmath seconds. It can be concluded
that both approaches require significant computational resources. This
set of experiments shows the pros and cons of considering a large value
for the SL parameter. We conclude that a larger SL adds more information
and causes the object representation takes more time to be computed.
Since we mainly focus on robotics applications, especially open-ended
learning, the computation time is an important factor. Therefore, we
prefer to choose a value for the support length parameter in the range
of 0.02m to 0.05m in the following approaches.

#### 6.3.2 Bag of Words

The descriptiveness of the BoW layer was evaluated with varying
dictionary size. All the parameters must be well selected to provide a
good balance between recognition performance, memory usage and
computation time. Towards this end, a set of 120 experiments has been
performed. Experiments were repeated for different values of four
parameters of the system, namely the voxel size (VS), the image width
(IW) and support length (SL) of spin images and the dictionary size
(DS). Results are summarized in Table 6.3 and Fig. 6.4 . Based on the
obtained results, the default system configuration is set as follow: VS
= @xmath , IW = @xmath , SL = @xmath and DS = @xmath . The accuracy of
this configuration was @xmath percent.

#### 6.3.3 Standard LDA

In the topic layer, we have four new parameters in addition to those of
the Bag of Words layer: Number of Topics (NT), the Dirichlet prior
hyper-parameters @xmath and @xmath and Number of Iterations for Gibbs
sampling. In this thesis, we assumed symmetric Dirichlet prior for both
@xmath and @xmath parameters. Therefore, a high @xmath value means that
each object is likely to contain a mixture of most of the topics, and
not a single specific topic. Likewise, a low @xmath value means that a
topic may contain a mixture of just a few of the words. We have ran
several experiments and concluded that @xmath and @xmath should be set
to @xmath and @xmath respectively. Moreover, the number of Gibbs
sampling iterations was set to @xmath . Experiments must be performed
for different values of the remaining five parameters. In this section,
the standard LDA approach is evaluated. This is the baseline to which
the proposed Local LDA approach must be compared (see the next
subsection). A set of 360 experiments was performed with standard LDA
for different values of the considered parameters.

The obtained results are summarized in Table 6.4 which shows the average
accuracies obtained with different configurations. Similar to the
previous approaches, the parameter configuration that obtained the best
average accuracy and leads to better efficiency was selected as the
default configuration: VS @xmath , IW @xmath and SL @xmath , DS @xmath
and K @xmath . The performance of the topic layer with LDA (i.e., shared
topics among all categories) was carried out using this configuration
and the accuracy of this configuration was @xmath .

#### 6.3.4 Local LDA

A similar set of 360 experiments was performed with the proposed Local
LDA approach for the same values of the five considered parameters. A
summary of the experiments is reported in Table 6.5 . The parameter
configuration that obtained the best average accuracy was selected as
the default configuration: VS @xmath , IW @xmath and SL @xmath , DS
@xmath and K @xmath . The accuracy of this configuration was @xmath .
This configuration displays a good balance between recognition
performance and memory usage.

#### 6.3.5 Good

A set of experiments was performed to evaluate the performance of
instance-based learning using the GOOD descriptor. As described in
chapter 4 , GOOD has single a parameter namely the number of bins ,
@xmath . We performed five experiments for different values of this
parameter: @xmath . Results are summarized in Fig. 6.5 . In these
experiments, the best result was obtained with @xmath bins. The accuracy
of the proposed system with this configuration was @xmath percent. The
experiment time for this approach on average was around @xmath seconds.

By comparing the results obtained with the different instance-based
approaches using the default configurations, several points can be
concluded. One important observation is that the overall performance,
concerning classification accuracy and computation time, obtained with
GOOD descriptor is clearly better than the best performances obtained
with the local-feature based approaches. Experimental results also show
that, among approaches based on local-feature, the overall performance
of the recognition system based on topic modelling is promising and the
proposed Local LDA representation is capable of providing a distinctive
representation for the objects. Moreover, it was observed that the
discriminative power of the Local LDA representation was better than the
other local-feature based approaches. In addition, independent topics
for each category provide better representation than shared topics for
all categories. Furthermore, it has been observed that the
discriminative power of shared topics depends on the order of
introduction of categories (Ando et al., 2013 ) . The accuracy of object
recognition based on variable size representation (i.e., sets of
spin-images) was not as good as the other approaches. The BoW and LDA
(shared topics) representations obtained an acceptable performance. The
local topic representation provided a good balance between memory usage
and descriptiveness. The variable size representations were the less
compact representations.

### 6.4 Model-Based Learning

The performance of the model-based object category learning approach was
evaluated using BoW, LDA (standard and local) and GOOD representations.
The variable-size representation approaches are not considered since
most of the machine learning methods, including the proposed model-based
learning approach, take fixed-length vectors as input. In the case of
BoW and Local LDA, three parameters (IW, SL and DS) have different
effects on both memory usage and recognition accuracy. Dictionary size
(DS) determines the number of words in the dictionary. For LDA
approaches, an additional parameter, NT is the number of topics. We
tried to find a good value for these parameters to obtain a good balance
between recognition performance, memory usage, and processing speed. In
the case of GOOD, the number of bins , is the parameter that has to be
set optimally. We describe in detail each set of experiments in the
following subsections.

#### 6.4.1 Bag of Words

A set of 120 experiments was carried out to evaluate the proposed
model-based learning approach with BoW representation for different
values of the four BoW parameters. The obtained results are summarized
in Table 6.6 . The object recognition performance for each system
configuration is depicted in figure 6.6 where the system parameters are
represented as a tuple (VS, IW, SL, DS).

The parameters that obtained the best average accuracy were selected as
the default system parameters: VS = 0.01, IW = 4, SL = 0.05 and DS = 90.
The accuracy of the proposed system with this configuration was @xmath .

One important observation is that the accuracy of the proposed
model-based object recognition approach using BoW is similar to the
instance-based approach with BoW (see section 6.3.2 ). Although, the
instance-based approach achieves a good description power, it increases
computation time, memory usage and sensitivity to outliers. In
instance-based approaches, the number of instances directly affects
computation time and memory usage. Therefore, we calculate the average
time required to complete all the 120 experiments for the proposed
instance-based and model-based approaches using BoW. The average
computation time of the model-based approach is around @xmath less than
the instance-based approach with BoW representation. In particular, the
average computation time for the instance based approach was around
@xmath seconds while the model-based approach on average took @xmath
seconds. It is worth mentioning that the difference of computation time
was significant for such a small dataset and could grow exponentially by
increasing the number of instances and categories.

#### 6.4.2 Standard LDA

Another round of experiments was carried out for different values of
five parameters of the LDA approach. Similar to the previous
experiments, we tried to find a good value for these parameters to
obtain a good balance between recognition performance, memory usage, and
processing speed. A summary of all experiments is reported in Table 6.7
. The parameters that obtained the best average accuracy was selected as
the default configuration: VS @xmath , IW @xmath and SL @xmath , DS
@xmath , K @xmath . The accuracy of the proposed system in topic layer
with the default configuration was @xmath percent. It can be noticed
that the model-based learning approach with LDA representation performs
better than the instance-based approach with LDA representation. We
believe that this difference can be explained by the fact that the
former approach is not sensitive to the outliers.

The computation time for a complete experiment of model-based learning
with LDA representation (including both learning and recognition phases)
on average was @xmath seconds, which is @xmath times less than the
instance-based learning with the same object representation. In terms of
computation time, it was observed that among the object representation
approaches based on local-feature, the BoW with model-based learning
approach achieved the best performance, which is around @xmath and
@xmath times less than model-based approach with LDA and Local LDA
representations respectively. The underlying reason is that there is a
Gibbs sampling procedure in the LDA approaches which takes time to
accurately represent the desired distribution.

#### 6.4.3 Local LDA

Another round of experiments was performed to evaluate the system using
the proposed Local LDA representation and model-based object category
learning and recognition. In this case, @xmath experiments were carried
out. In these experiments, the parameters that got the best average
accuracy were: VS @xmath , IW @xmath and SL @xmath , DS @xmath , K
@xmath . This is adapted as the default configuration. The accuracy of
the system with this configuration was @xmath percent. Table 6.8
provides a detailed summary of the obtained results.

Similar to the previous evaluation, by comparing the obtained results of
instance-based and the model-based approaches, the value of model based
learning approaches is highlighted. The average computation time of the
model-based approach with Local LDA was around @xmath times less than
the instance-based approach with Local LDA. Specificity, for all
experiments, the average computation time for the instance-based
approach was @xmath seconds while model-based approach on average took
@xmath seconds. It was observed that the proposed local topic modelling
is capable to provide distinctive object representation for recognizing
different type of objects.

#### 6.4.4 Good

A set of experiments was carried out to evaluate the proposed
model-based learning approach representing object views with the GOOD
descriptor. In particular, we performed five 10-fold cross-validation
experiments with different values for the number of bins parameter:
@xmath and @xmath . Results are plotted in Fig. 6.7 . Although a large
number of bins may provide more details about the point distribution, it
increases computation time and memory usage. Therefore, since the
difference in accuracy is relatively small, @xmath is set to @xmath bins
by default. This is a good balance between recognition performance,
memory usage, and processing speed (see additional details in section
6.5 ). The accuracy of the proposed system with this configuration was
@xmath percent. It is interesting that instance-based learning with GOOD
achieved better accuracy than the model-based approach. We believe that
the reason lies in the learning procedure. In the model-based approach,
a probabilistic model for a given category is created by combining
information from all the instances of the category. This procedure works
fine with all local feature-based object representations. In the case of
a global object representation, i.e., GOOD, the presence of some
instances with diverging appearances, i.e., totally different from the
other instances, will lead to create a poor model for the given
category. The instance-based approach with GOOD does not suffer from
this drawback and therefore, showed a better performance.

Although model-based object category learning and recognition with BoW
and LDA representations are the two most compact approaches in this
evaluation, their computation time and descriptiveness are not as good
as the GOOD representation. Overall, instance-based learning with GOOD
achieves the best recognition performance, which is 2 percentage points
(p.p.) better than GOOD with model-based learning, 4 p.p. better than
Local LDA and 7 p.p. and 8 p.p. better than LDA and BoW representations
respectively. The BoW and Local LDA led to experiment times 2.2 to 3.5
times higher than the experiment time obtained with GOOD. The underlying
reason is that GOOD works directly on 3D point cloud and requires
neither computation of local features nor a sampling procedure.
According to the evaluations, this approach is competent for robotic
applications with strict limits on the computation time requirement. A
summary of all evaluations is reported in Table 6.9 . In the next
section, we provide a detailed analysis of the GOOD descriptor.

### 6.5 GOOD Descriptor

Several additional experiments were carried out to evaluate the
performance of the proposed GOOD descriptor concerning descriptiveness ,
scalability , robustness and efficiency . In these experiments, we
mainly use the Washington RGB-D Object Dataset (Lai et al., 2011b ) ,
one of the largest publicly available datasets for object recognition.
For some experiments, the Restaurant Object Dataset (Kasaei et al.,
2015a ) is also used (for datasets, see Section 6.1 ).

In all experiments, the instance-based learning approach is used (see
section 5.4.5 ). GOOD was compared with four state-of-the-art object
descriptors that are available in the Point-Cloud Library ² ² 2
http://pointclouds.org/ , namely VFH (Rusu et al., 2010 ) , ESF
(Wohlkinger and Vincze, 2011 ) , GFPFH (Rusu et al., 2009b ) , from PCL
1.7, and GRSD (Marton et al., 2010 ) , from PCL 1.8. For all selected
descriptors, the default parameters in the respective PCL
implementations were used (Aldoma et al., 2012b ) .

#### 6.5.1 Descriptiveness

As mentioned above, GOOD has a parameter called number of bins that has
effect on descriptiveness, efficiency and robustness. Therefore, it must
be well selected to provide a good balance between recognition
performance, memory usage and computation time. The descriptiveness of
the proposed descriptor with respect to varying number of bins was
evaluated using the Washington RGB-D Object Dataset. For each value of
the number of bins, 10-fold cross validation experiments were performed.

Results are presented in Fig. 6.8 ( left ) and Table 6.10 . In these
experiments, the configurations that obtained the best accuracy figures
were 15 and 25 bins. Although, a large number of bins provides more
details about the point distribution, it increases computation time,
memory usage and sensitivity to noise. Therefore, since the difference
to other configurations is not very large, we prefer to use the first
configuration, i.e., @xmath bins which displays a good balance between
recognition performance, memory usage, and processing speed. The
accuracy of the proposed system with this configuration was 92 percent.
It shows that the overall performance of the recognition system is
distinctive. Unless otherwise noted, the remaining results are computed
using this configuration.

#### 6.5.2 Scalability

A set of experiments was carried out to evaluate the performance of the
proposed descriptor on the Washington RGB-D Object Dataset, concerning
its scalability with respect to varying number of categories. Results
are depicted in Fig. 6.8 ( center ) and ( right ). One important
observation is that the accuracy decreases in all approaches as more
categories are covered (Fig. 6.8 ( center )). This is expected since a
higher number of categories tends to make the classification task more
difficult. Moreover, it can be concluded from Table 6.11 that when the
number of object categories increases (i.e., more than 35 categories),
VFH and GOOD (15 bins) descriptors achieve the best accuracy and stable
performance regarding varying numbers of categories.

It is clear from Fig. 6.8 ( right ) that the experiment time of our
approach is significantly smaller than VFH, GRSD and GFPFH. Although
GOOD, VFH and ESF descriptors obtain an acceptable scalability regarding
varying numbers of categories, the scalability of GRSD and GFPFH is very
low and their performance drops aggressively when the number of
categories increases. Although EFS descriptor achieves better
performance than our approach with 5 bins (i.e., GOOD 5bins), the length
of EFS (i.e., an inverse indicator of compactness) is around 8.5 times
more than our descriptor (see Table 6.13 ). For a number of known
categories greater than 35, the difference in accuracy between ESF and
GOOD with 5 bins, is equal or less than 1%, whereas GOOD 15 bins is
clearly better than the others.

#### 6.5.3 Robustness

The robustness of the proposed object descriptor with respect to
different levels of Gaussian noise and varying point cloud resolutions
was evaluated and compared with other global object descriptors. These
experiments were run on the mentioned Restaurant Object Dataset. This
dataset is suitable for such evaluations since it is a small dataset and
the objects are extracted from cluttered scenes. Furthermore, the
Restaurant Object Dataset contains some occluded or truncated objects,
which improves the generalization power of the relevant learnt models.

##### Gaussian Noise

Ten levels of Gaussian noise with standard deviations from 1 to 10mm
were added to the test data. For a given test object, Gaussian noise is
independently added to the X, Y and Z-axes. As an example, a Vase object
with three levels of standard deviation of Gaussian noise ( @xmath ) is
depicted in Fig. 6.9 . The robustness results under different levels of
noise are presented in Table 6.12 and Fig. 6.10 ( left ). An important
observation can be made from Fig. 6.10 and Fig. 6.8 . Although, GOOD,
ESF and VFH achieved a really good performance on noise free data, GOOD
outperformed ESF, GFPFH and GRSD descriptors by a large margin under all
levels of Gaussian noise. While the performance of VFH was similar to
our approach under a low-level noise (i.e., @xmath ), our shape
descriptor outperformed all descriptors under high levels of noise.

It can be concluded from this observation that GOOD is robust to noise
due to using a stable, unique and unambiguous object reference frame. In
contrast, since VFH and GFPFH rely on surface normals to calculate their
shape descriptions, they are highly sensitive to noise. GRSD employs
radial relationships to describe the geometry of points at each voxel
cell and ESF uses distances and angles between randomly sampled points
to generate a shape description; therefore, GRSD and ESF are also
sensitive to noise and their performances decrease rapidly when the
standard deviation of the Gaussian noise increases. In addition, GOOD
uses three distribution matrices that are constructed based on
orthographic projection, therefore less affected by noise (i.e., in each
projection one dimension is discarded).

##### Varying Point Cloud Density

Two sets of experiments were performed to examine the robustness of the
proposed descriptor with respect to varying point cloud density. In the
first set of experiments, the original density of training objects has
been used and the point cloud density of test objects was reduced
(downsampling) using a voxelized grid approach ³ ³ 3
http://pointclouds.org/documentation/tutorials/voxel_grid.php . The
methodology of this kind of downsampling commences with a voxelization
of the surface points. This is initiated with a root volume element
(voxel) and the eight children voxels in which each internal node has
exactly eight children nodes. These are recursively subdivided until all
voxels contain at most one point or the minimum voxel size is reached
(i.e., the cloud is divided in multiple voxels with the desired
resolution). Afterwards all the points that fall into the same voxel
will be downsampled with their centroid. In this evaluation, each test
object is downsampled using five different voxel sizes: @xmath and
@xmath mm. An example of a Flask object with four levels of downsampling
is depicted in Fig. 6.11 . The robustness results regarding varying
point cloud density in test data are presented in Fig. 6.10 ( center ).

From experiments of reducing density of test data (i.e., Fig 6.10 ( left
)), it was found that our approach is more robust than the other
descriptors concerning low-level downsampling (i.e., @xmath ) and works
slightly better than the other in high-level downsampling resolution
(i.e., @xmath ). In contrast, the performance of VFH, ESF and GRSD were
better than GOOD in mid-level downsampling resolution (i.e., @xmath ).
The performance of GFPFH was very low under all levels of point cloud
resolution. Besides, it can be concluded from Fig. 6.10 ( right ) that
when the level of down-sampling decreases, VFH, ESF and GRSD descriptors
achieve better performance than GOOD and GFPFH descriptors.

#### 6.5.4 Efficiency

In this subsection, two evaluations regarding memory footprint (i.e.,
the amount of main memory that a program uses or references while
running) and computation time are provided and discussed.

##### Memory Footprint

The length of a descriptor has influence on memory usage and computation
time in object recognition (see Fig. 6.8 ). The length of all
descriptors used in this evaluation is given in Table 6.13 . Although
GFPFH and GRSD are the two most compact shape descriptors (see Table
6.13 ), their computation time and descriptive power are not good as
depicted in figures 6.8 and 6.12 . Our approach is the third most
compact descriptor and provides good balance between computation time
and descriptiveness with 75 floats. Although, VFH and ESF descriptors
achieve a good description power, their lengths are around 4.10 and 8.50
times larger than GOOD with 5 bins and 19 and 40 times larger than GFPFH
respectively. ESF is the less compact descriptor compared to all the
other descriptors.

##### Computation Time

Several experiments were performed to measure computation time for all
descriptors used in this evaluation. Since the number of object’s points
directly affects the computation time, first, we randomly select 20
objects from the Washington RGB-D dataset. We then calculate the average
time required to generate a description for the 20 selected objects.
Figure 6.12 compares the average computation time of the selected object
descriptors. Several observations can be made. GOOD is the most time
efficient descriptor. In contrast, GFPFH is the most computationally
expensive descriptor. ESF, VFH and GRSD achieve a medium performance in
terms of computation time. GOOD is around 10 times faster than ESF and
44, 50 and 254 times faster than VFH, GRSD and GFPFH descriptors. The
underlying reason is that GOOD works directly on 3D point clouds and
requires neither triangulation of the object’s points nor surface
meshing. According to this evaluation and the memory footprint
evaluation (i.e., subsection 6.5.4 ), our approach is especially well
suited for robotic applications with strict limits on the memory
footprint and computation time requirements.

### 6.6 Summary

In this chapter, we first focused on the classical evaluation of
instance-based (Section 6.3 ) and model-based (Section 6.4 ) object
category learning and recognition approaches. The reported results
indicate that the overall classification performance obtained with the
proposed instance-based learning approach using GOOD feature is better
than the best performances achieved with the other approaches. The
underlying reason was that GOOD feature encodes the object globally,
while the other representations encode the entire object based on a set
of local features. Furthermore, it was observed that the model-based
learning approach with the GOOD feature provided the best computation
time performance. Among the local feature-based approaches, Local LDA
achieved the best results. It is due to this point that Local LDA
transforms objects from bag-of-words space into a local semantic space
and used distribution over distribution representation for providing
dominant representation.

After evaluation of all the proposed learning and recognition
approaches, a set of experiments was carried out to assess the
performance of the GOOD descriptor and compare its performance with
other state-of-art descriptors. Experimental results show that the
overall classification performance obtained with GOOD is comparable to
the best performances obtained with the state-of-the-art Global object
descriptors. GOOD outperformed the selected state-of-the-art descriptors
(i.e., VFH, ESF, GRSD and GFPFH descriptors), achieving appropriate
descriptiveness and significant robustness to Gaussian noise. GOOD was
robust to varying low-level point cloud density too. The accuracy of
VFH, ESF and GRSD was better than GOOD in the case of varying medium and
high point cloud density. In addition, GOOD obtained the best
computation time performance.

The off-line evaluation methodologies (e.g k-fold cross validation,
etc.) are not well suited to evaluate open-ended learning systems,
because they do not abide to the simultaneous nature of learning and
recognition and also those methodologies imply that the set of
categories must be predefined. We address this issue in the next chapter
by proposing an approach for evaluating open-ended object category
learning and recognition approaches in open-ended and multi-context
scenarios.

## Chapter 7 Open-Ended Evaluations

One of the primary goals in computer vision is to develop reliable
capabilities that will allow robots to work in an unconstrained
environment by recognizing a large number of object categories. It is
still a challenging problem because of ill-definition of objects, large
variations in object appearance and concept drift. To deploy a robot in
a human-centric environment, it is important that the robot is able to
continuously acquire and update object categories while working in the
environment. Therefore, autonomous robots must have the ability to
continuously execute learning and recognition in a concurrent and
interleaved fashion. In an unstructured environment, an agent must
process observations that become gradually available over time, and form
hypotheses about the environment. If the agent works in a single-context
environment, or if the agent can receive/extract explicit information
about the current context, one may consider to pre-program the agent to
use this information for memory management and for adapting to the
environment. In the first part of this chapter, in order to evaluate and
compare the proposed object category learning and recognition approaches
in an open-ended manner, the teaching protocol proposed by Seabra Lopes
and Chauhan ( 2007 ) is used. An extensive set of experiments was
carried out to evaluate each approach.

The second part of this chapter is dedicated to evaluating how different
approaches cope with the effects of context switch. In a real-world
environment, the context may change implicitly and it is not feasible to
assume that one can pre-program all the contexts required by an agent.
Therefore, the agent must have the ability to continuously execute
learning and recognition in a concurrent and interleaved fashion even
when changes of context occur without explicit cueing. For instance, an
intelligent robot working in a human-centric environment needs to learn
and remember many different object categories. This is a challenging
task since in such environments context may change implicitly and some
of the object categories may disappear for some time. As a baseline, the
robot must demonstrate a capacity for open-ended learning: that is, the
ability to learn new object categories sequentially without forgetting
the previously learned object categories. In other words, whenever an
agent migrates to a novel context, some new object categories should be
learned to represent the environment. To achieve adaptability, the agent
can either preserve and update the current category models, learning
additional categories as needed, or discard the categories learned so
far and learn new category models from scratch. In unstructured
environments, since the learning agent may need to go back to a past
context, discarding the category models learned in previous contexts is
not a rational choice (see Fig. 7.1 ).

Having this in mind, we propose an approach for evaluating the
adaptability of different open-ended object category learning and
recognition methods to context change; A new teaching protocol,
supporting context change, was designed and used for experimental
evaluation. A full round of experiments was carried out to assess and
compare the proposed methods in depth from the point of view of
adaptability to context change. In this chapter, all evaluations are
conducted using the Washington RGB-D dataset described in the previous
chapter (see Section 6.1 ).

This chapter presents three main contributions: ( i ) Open-ended
evaluation of the proposed object category learning and recognition
approaches; ( ii ) An approach for evaluating the adaptability of
open-ended object category learning and recognition systems to context
change; ( iii ) Evaluation of the adaptability of the proposed object
category learning and recognition approaches to context change. Parts of
the work presented herein have been published in the IROS conference
(Kasaei et al., 2018a ) . The remainder of this chapter is organized as
follows. In section 7.1 , we discuss related works. Open-ended
evaluation of selected approaches is presented in Section 7.2 . An
evaluation of adaptability to context change is the topic of Section 7.3
. Finally, summary is presented and future research is discussed.

### 7.1 Related Work

In recent years, the role of open-ended learning in robotics has been a
topic of considerable interest (Collet et al., 2014 ; Oliveira et al.,
2015b ) . In the last decade, various research groups have made
substantial progress towards the development of learning approaches
which support online, incremental and open-ended category learning
(Oliveira et al., 2016 ; Kasaei et al., 2016a ; Celikkanat et al., 2016
; Kasaei et al., 2015b ) . Although all the proposed methods have been
shown to make progress over the previous one, it is challenging to
quantify this progress without a concerted evaluation protocol.
Therefore, learning in online and open-ended scenarios calls for new
evaluation procedures. Although classical evaluation procedures
(holdout, cross-validation, leave-one-out) provide unbiased estimates of
the learning performance, they are not suitable for the cases where the
problem domain changes over time (Seabra Lopes and Chauhan, 2007 ; Gama
et al., 2009 ) . Accordingly, a well-defined protocol can facilitate the
comparison of different approaches as well as the assessment of future
improvements.

#### 7.1.1 Teaching Protocols for Open-Ended Evaluation

The well established evaluation methodologies follow the classical
train-and-test procedure, i.e., two separate stages, training followed
by testing. Training is accomplished offline, and once it is complete
the testing is performed. These methodologies are not well suited to
evaluate open-ended learning systems, because they do not abide to the
simultaneous nature of learning and recognition and because the number
of categories must be predefined.

Teaching protocols for open-ended evaluation of a learning algorithm
determines which examples are used for training the algorithm, and which
are used to test the algorithm. Seabra Lopes and Chauhan proposed a
teaching protocol to evaluate the ability of an agent to incrementally
acquire visual object categories in an open-ended setting (Seabra Lopes
and Chauhan, 2007 ; Chauhan and Seabra Lopes, 2011 ) . This protocol,
which can be followed by a human teacher or by a simulated teacher, is
based on a Test-then-Train scheme. It is an elaborate and exhaustive
evaluation procedure, where, for every new category introduced to the
agent, the average accuracy of the system is calculated by performing
classification with all known categories. Towards this end, a teacher
repeatedly presents instances of known categories to the agent, checks
the agent’s predictions and provides corrective feedback in case of
misclassification. This way, the system is trained, and at the same time
the recognition performance of the system is continuously estimated.
More specifically, the teacher interacts with the learning agent using
three basic actions (see sections 3.5 and 5.3 for additional details):

-    Teach : used for introducing a new object category;

-    Ask : used to ask the system what is the category of a given object
    view;

-    Correct : used for providing corrective feedback in case of
    misclassification.

Algorithm 7.1 describes the teaching protocol. This protocol has been
used in several recent works (Oliveira et al., 2016 ; Kasaei et al.,
2016a ) .

1: Introduce @xmath

2: @xmath

3: repeat

4: @xmath @xmath ready for the next category

5: Introduce @xmath

6: @xmath

7: @xmath

8: repeat @xmath question / correction iteration

9: Present a previously unseen instance of @xmath

10: Ask the category of this instance

11: If needed, provide correct feedback

12: c @xmath c+1 if else @xmath

13: if @xmath then @xmath sliding window

14: @xmath

15: @xmath accuracy in last @xmath question/correction iterations

16: until ( ( @xmath > @xmath and @xmath >= @xmath ) @xmath accuracy
threshold crossed

17: or (user sees no improvement in accuracy) ) @xmath breakpoint
reached

18: until ( user sees no improvement in accuracy ) @xmath breakpoint
reached

Teaching protocol for performance evaluation

The teacher continuously estimates the recognition performance
(accuracy) of the agent using a sliding window of size @xmath
iterations, where @xmath is the number of categories that have already
been introduced. If @xmath , the number of iterations since the last
time a new category was introduced, is less than @xmath , all results
are used. In case accuracy exceeds a given classification threshold (
@xmath ), the teacher introduces a new object category.

In another work, this protocol has been modified to cope with scenarios
in which the learning agent is able to acquire large sets of categories
(hundreds or more) (Chauhan and Seabra Lopes, 2015 ) . The major
differences are two fold: in the original protocol, after introducing a
new category, all known categories are tested once, while Chauhan and
Seabra Lopes ( 2015 ) proposed that it is enough to test a randomly
generated subset of all known categories for introducing a new category;
the second difference, with respect to the original protocol, is that,
in the original protocol, categories are tested in the sequence in which
they were introduced to the agent, while in the modified protocol,
categories are tested in a random sequence, which steers to prevent
storing more instances for the categories introduced earlier.

In the field of data-stream learning, i.e., sensor networks, social
networks, financial data etc., the data usually became available through
the time and can significantly change over time. Given that the number
of categories is usually predefined in data-stream learning, the
evaluation procedure follows the standard train and test approach.
Therefore, the evaluation of these algorithms faces the same issues as
that for evaluating open-ended learning algorithms. To handle this
issue, some online evaluation approaches have been proposed, such as
Prequential evaluation (Gama et al., 2009 , 2013 ) and MOA (Bifet
et al., 2010 ) . These approaches also follow a Test-then-Train scheme.
However, they do not take into account the impact of learning new
categories and the capability of learning algorithms to scale up to
larger sets of categories. The existing online evaluation approaches do
not take into account the possibility of context change. In this
chapter, the mentioned teaching protocol (Seabra Lopes and Chauhan, 2007
; Chauhan and Seabra Lopes, 2011 ) is modified to evaluate the learning
agent in scenarios of context change.

#### 7.1.2 Metrics for Open-Ended Evaluation

After deciding which teaching protocol is suitable for evaluating an
open-ended learning system, one of the unique concerns is how to build a
picture of performance over time. Some authors consider the classical
measures versus training time. In classical scenarios with a fixed set
of categories, such evaluations show a gradual increase of the classical
measures and a convergence to a stable value. The performance evaluation
of an open-ended learning system cannot be limited to the classical
evaluation metrics (see Section 6.2 ). In addition to classical measures
such as accuracy, precision and recall, an open-ended learning approach
should be evaluated by the other measures like “ number of learned
categories ” and “ number of teaching iterations ”.

In the continuation of their earlier works (Seabra Lopes and Chauhan,
2007 ; Chauhan and Seabra Lopes, 2011 ) , Chauhan and Seabra Lopes (
2015 ) proposed additional criteria and measures to evaluate the overall
learning performance of the agent during an open-ended experiment. In
particular, they suggested the following measures to characterize the
quality and coverage of the learned knowledge after an experiment has
finished:

1.  Global accuracy : This is given as the percentage of correct
    predictions made during a complete teaching protocol experiment,
    i.e., in an experiment that reached the breakpoint.

2.  Average protocol accuracy : The average of all protocol accuracy
    values, i.e., computed over all the question/correction iterations
    in a complete teaching protocol experiment;

3.  Number of learned categories in a complete teaching protocol
    experiment.

In applications where the learning agent can recognize an object as
belonging to an “ unknown ” category, the accuracy measure can be
replaced by some other recognition success measure, e.g. F-measure.
Additionally, they considered the following measures to characterize the
learning process in terms of memory and time:

1.  Number of question/correction iterations during the experiment;

2.  Average number of stored instances per category in a complete
    teaching protocol experiment (this measure is applicable to
    instance-based learning approaches only).

Oliveira et al. ( 2015a ) proposed to organize these evaluation metrics
in three groups:

1.  How much does it learn? This is measured as the number of categories
    learned in a complete teaching protocol experiment;

2.  How well does it learn? They mentioned that classical evaluation
    metrics such as accuracy, precision and recall are well suited for
    this metric. They used global accuracy measured as defined above.

3.  How fast does it learn? This is measured as the number
    question/correction iterations in a teaching protocol experiment up
    to a certain number of learned categories.

#### 7.1.3 Context Change in Open-Ended Learning

Humans can adapt to different environments dynamically by watching and
learning. Learning is closely related to memory in human cognition. Yeh
and Barsalou ( 2006 ) demonstrated in a series of experiments that human
subjects perform better at a variety of cognitive tasks when taking
context into account. Without considering contextual information, all
possibilities in a classification space must be explored, which scales
poorly with large-scale data.

Several cognitive experiments have been performed showing that animals
also retain knowledge of past contexts (Sissons and Miller, 2009 ; Rosas
and Callejas-Aguilera, 2006 ) . Cognitive scientists demonstrated that
humans use contextual information to handle large-scale object
recognition tasks faster and more accurately (Oliva and Torralba, 2007 )
.

Some authors make a useful distinction between internal and external
contexts (Kokinov, 1997 ; Snidaro et al., 2015 ) . External context is
the state of the physical and social environment while internal context
is the current mental (memory) state of the agent. Kokinov’s dynamic
theory of context (Kokinov, 1995 , 1997 ) assumes that the internal
context influences perception, memory, and reasoning processes. He
suggests that the internal context is formed by the interaction between
at least three processes: building new representations based on
perception of the environment; accessing memory traces therefore
reactivating and possibly modifying old representations; and
constructing new representations based on reasoning.

Kokinov ( 1995 , 1997 ) introduced several distinctions between various
concepts, e.g. internal versus external context, implicit versus
explicit context, and proposed a dynamic approach to context modeling.
External context is related to the social and situational dimensions of
contexts such as location, time, light and co-location. Internal context
can be understood as a mental (memory) state of the agent. Researchers
in both cognitive science (Rosas et al., 2013 ) and computer vision
communities (Galleguillos and Belongie, 2010 ; Oliva and Torralba, 2007
) have mainly studied the effects of external context and very rarely
the internal context (Snidaro et al., 2015 ) . Recently, several
computer vision approaches have shown that information about the
external context improves the efficiency of the perceptual tasks such as
object detection (Mottaghi et al., 2014 ) and semantic segmentation
(Mottaghi et al., 2014 ; Shelhamer et al., 2016 ) .

In robotics, the notion of context has grown in prominence over the last
decade. Several researchers considered the role of context in object
recognition. They used explicit context information, in training and/or
in recognition, with different methods for representing the context in
terms of relationship among objects in a scene (Galleguillos and
Belongie, 2010 ; Mottaghi et al., 2014 ; Ruiz-Sarmiento et al., 2015b ,
a ) . Moreover, they showed that a statistical summary of the scene
(i.e., global scene representation) provides a rich source of
information for contextual inference. For instance, Mottaghi et al. (
2014 ) investigated the role of context for object detection and
semantic segmentation. They proposed a new deformable part-based model,
which exploits both local and global context for object segmentation in
unstructured environments and showed that this contextual reasoning was
useful to detect objects at all scales. Nigam and Riek ( 2015 ) proposed
a social context perception approach for mobile robots. They considered
different aspects of the external context, including social environment,
physical location, audio (i.e., varied levels of noise) and the time of
day, to recognize dining, studying and lobby (waiting) contexts.

Such approaches are at some point dependent on an explicit context cue
and may fail when the environment undergoes a change in context without
explicit cueing. This is an important limitation since no matter how
extensive the training data, an agent might always be confronted with
unknown objects in new contexts when operating in everyday environments.
Therefore, the agent should be able to deal with implicit context
changes in an incremental and open-ended manner. The ability of
different learning techniques to cope with context change in the absence
of explicit cueing is usually not evaluated. Furthermore, the existing
online evaluation approaches do not take into account the possibility of
context change. That is precisely one of the focuses of this chapter. In
particular, the teaching protocol of (Seabra Lopes and Chauhan, 2007 ;
Chauhan and Seabra Lopes, 2011 ) is modified to evaluate the learning
agent in scenarios of context change. The idea is to emulate the
interactions of a learning and recognition system with the surrounding
environment over long periods of time and evaluate how different
approaches cope with the effects of context switch. For this purpose, a
simulated teacher was developed to follow the modified teaching protocol
and autonomously interact with the developed learning agent. After
teaching a certain number of categories, the simulated teacher changes
the context and continues teaching and testing the agent in the new
context.

### 7.2 Open-Ended Evaluation of Selected Approaches

This section presents the experimental setup and the results obtained
for the main approaches proposed and explored in this thesis. For each
approach, the default parameters, as reported in the previous chapter,
were used.

#### 7.2.1 Simulated Teacher

Off-line evaluation methodologies do not comply with the simultaneous
nature of learning and recognition in autonomous agents. Moreover, they
assume that the set of categories is predefined. Therefore, the
mentioned teaching protocol (Seabra Lopes and Chauhan, 2007 ; Chauhan
and Seabra Lopes, 2011 ) is adopted in this evaluation (Algorithm 7.1;
see section 7.1.1 ). The idea is to emulate the interactions of a
learning agent with the surrounding environment over long periods of
time. The protocol can be followed by a human teacher. However,
replacing a human teacher with a simulated one makes it possible to
conduct systematic, consistent and reproducible experiments for
different approaches. It allows the possibility to perform multiple
experiments and explore different experimental conditions in a fraction
of time a human would take to carry out the same task.

For this purpose, a simulated teacher, connected to a large database of
labelled object views was developed. The overall system architecture is
depicted in Fig. 7.2 . In this chapter, the Washington RGB-D dataset is
used (i.e., see Section 6.1 ). The idea is that the simulated teacher
repeatedly picks unseen object views from the currently known categories
and presents them to the agent for testing. Inside the learning agent,
the object view is recorded in the Perceptual Memory if it is marked as
a training sample (i.e., whenever the teacher uses teach or correct
instructions), otherwise it is dispatched to the Object Recognition
module.

For introducing a new category, the simulated teacher presents three
randomly selected object views. Since the order in which categories are
introduced in a teaching protocol experiment may have an effect on the
performance of the system, 10 experiments were carried out for each
approach and, in each experiment, categories were introduced in random
sequences. These sequences were the same for all approaches.

In case the agent cannot reach the classification threshold after a
certain number of iterations (i.e., 100 iterations), the simulated
teacher can infer that the agent is no longer able to learn more
categories and therefore, terminates the experiment. It is possible that
an approach learns all existing categories before reaching the
breakpoint. In such case, it is no longer possible to continue the
protocol and the evaluation process is halted. In the reported results,
this is shown by the stopping condition, “ lack of data ”.

#### 7.2.2 Evaluation Metrics

When an experiment is carried out, learning performance is evaluated
using several measures, including: ( i ) the number of learned
categories (NLC) at the end of the experiment, an indicator of how much
the system was capable of learning ; ( ii ) the number of question /
correction iterations (QCI) required to learn those categories and the
average number of stored instances per category (AIC), indicators of
time and memory resources required for learning ; ( iii ) Global
Classification Accuracy (GCA), computed using all predictions in a
complete experiment, and the Average Protocol Accuracy (APA), i.e.,
average of all accuracy values successively computed during the
experiment to control the application of the teaching protocol. GCA and
APA are indicators of how well the system learns .

#### 7.2.3 Evaluation of Instance-Based Approaches

The first round of experiments was performed to evaluate the proposed
learning approaches concerning their scalability with respect to the
number of learned categories.

The left column in Fig. 7.3 provides a detailed summary of the obtained
results. By comparing all approaches, it is visible that the agent
learned (on average) more categories using GOOD than with other
approaches. The GOOD approach was able to learn about 39 categories, on
average, while the other approaches learned less than 30 categories. In
particular, GOOD learned around 11 categories more than Local LDA and
19, 17 and 25 categories more than Approach II, BoW and standard LDA
approaches, respectively. Based on the obtained results, it can be
concluded that the agent with the global feature does lead to a better
incremental and open-ended performance when compared with the
performances of the agent using local-features. Similar results have
been reported previously on classical evaluation using instance-based
approaches where GOOD led to better category descriptions when compared
with the other approaches. It is worth to mention that among the local
feature-based approaches, Local LDA very clearly outperforms the other
three approaches. Although, Approach II and BoW approaches stored fewer
instances per category (AIC), on average, than Local LDA, their
discriminative power is lower and their performance dropped quickly as
the number of categories increased (see the right column of Fig. 7.3 ).
It was also observed that the discriminative power of shared topics
depends a lot on the order of introduction of categories.

The center column of Fig. 7.3 illustrates how fast the learning occurred
in each of the experiments. It shows the number of question/correction
iterations required to learn a certain number of categories. From Fig.
7.3 , we see that on overage the longest experiments were observed with
GOOD and the shortest ones were observed with standard LDA. In the case
of standard LDA, the agent on average learned @xmath categories using
@xmath question/correction iterations. GOOD on average continued for
@xmath question/correction iterations and the agent was able to learn
@xmath categories. By comparing all the experiments based on GOOD, it is
visible that in the seventh experiment, the number of iterations
required to learn 41 object categories was greater than other
experiments. The maximum number of categories that the agent could learn
with GOOD was 45 categories (i.e., Exp. #9). This experiment took 1540
question/correction iterations for the agent to acquire these
categories. It can be observed that both evaluation measures (i.e., GCA
and APA) of this experiment are also higher than the other experiments.

The right column of the Fig. 7.3 shows the global classification
accuracy obtained by the selected approaches as a function of the number
of learned categories. One important observation is that accuracy
decreases in all approaches as more categories are introduced. This is
expected since a higher number of categories known by the system tends
to make the classification task more difficult. BoW and Local LDA
achieved the best accuracies with stable performance. One important
observation is that BoW achieved better APA than GOOD and Local LDA
approaches. This is expected since BoW learned fewer categories, and it
is easier to get better APA in fewer categories. By comparing all
experiments, it is visible that the agent learned more categories when
using GOOD while in the case of other metrics, including accuracies (but
this is inversely related to the number of learned categories), memory
usage and computation time, its performance is not as good as Local LDA.
The Local LDA provides an appropriate balance between all critical
parameters.

#### 7.2.4 Evaluation of Model-Based Approaches

In these experiments, the performance and scalability of the proposed
model-based approaches (i.e., naive Bayes with different fixed-size
representations), with respect to an increasing number of categories
were evaluated. Results are presented in Fig. 7.4 . One important
observation is that the agent learned all @xmath categories using GOOD
and Local LDA in all experiments and all experiments concluded
prematurely due to the “ Lack of data ”, i.e., no more categories
available in the dataset (indicating the potential for learning many
more categories). The agent with BoW obtained an acceptable scalability
(i.e., the agent on average learned @xmath categories) while the
scalability of standard LDA was very low (i.e., on average learned
@xmath categories). It should be noted that 8 out of 10 BoW experiments
were finished because no more categories were available to be learned (“
lack of data ”).

The left column in Fig. 7.4 provides a detailed summary of the obtained
results. The center column of Fig. 7.4 shows the number of learned
categories as a function of the number of protocol iterations. This
gives a measure of how fast the learning occurred in each of the
experiments. Moreover, it shows the number of question / correction
iterations required to learn a certain number of categories. It can be
concluded that the agent with GOOD learned all categories faster than
with Local LDA. The agent with BoW and LDA achieved the third and forth
places respectively.

The right column of the Fig. 7.4 shows the global classification
accuracy obtained by the proposed approaches as a function of the number
of learned categories. By comparing all approaches, it is visible that
the agent with GOOD achieved the best accuracy (i.e., @xmath ) with
stable performance and outperformed the other approaches by a large
margin (i.e., 4% or more). The agent with Local LDA also showed a
promising performance and provide a good balance among all parameters.
Although, BoW and standard LDA approaches on average achieved similar
accuracies, the discriminative power of standard LDA is lower than BoW
and its performance dropped quickly as the number of categories
increased (see the right column of Fig. 7.4 ( a and b )).

The average protocol accuracy of the agent with GOOD feature is also
considerably higher than the other approaches (i.e., more than @xmath ).
It should be noted that these results should be seen in the light of the
number of categories learned. For example, BoW and LDA seem to indicate
similar average protocol accuracy (APA), however, standard LDA on
average reached the breakpoint after the introduction of the 31st
category whereas BoW learned around all (49) categories in six
experiments and between 43 and 46 categories in the remaining four
experiments.

### 7.3 Evaluation of Adaptability to Context Change

In this section, a methodology is proposed and used for evaluation
learning agents in open-ended learning scenarios with context change.
This includes a new teaching protocol and an adaptability measure.

#### 7.3.1 Open-Ended Evaluation with Context Change

In order to evaluate the adaptability to context change, we modified the
standard teaching protocol described above to include a change of
context. A simulated teacher is developed to follow the teaching
protocol and autonomously interact with the system using teach , ask and
correct actions. The main idea is to emulate the interactions of an
agent with the environment over long periods of time in two different
contexts. Towards this end, the object categories in the database and
respective object views are randomly assigned to two different contexts,
A and B. Then, the teacher starts presenting categories from context A.
The simulated teacher repeatedly picks object views of the currently
known categories from the context A and presents them to the system for
checking whether the system can recognize them. If not, the simulated
teacher provides corrective feedback. Whenever the agent learned @xmath
categories from context A, the simulated teacher changes to the other
context, B, and interacts (i.e., teach, ask and correct) with the
learning agent using the object categories of context B. The complete
process is summarized in Algorithm 7.2.

In this way, the agent begins with zero knowledge and the training
instances become gradually available according to the teaching protocol.
After context change, the learning agent will recognize object views
taking into account all acquired category models from both contexts.
Note that, although, a learning agent can have internal mechanisms
specifically designed for keeping track of context (explicit context
information, either inferred by the learning agent or obtained from an
external source), no such mechanism was developed in this work.
Therefore, what was evaluated was the potential of the different
representations and recognition rules for implicitly coping with context
change.

1: Input: @xmath

2: context @xmath

3: Introduce @xmath from context

4: @xmath

5: repeat

6: if context @xmath and @xmath then @xmath context switch

7: context @xmath

8: @xmath @xmath ready for the next category

9: Introduce @xmath from context

10: @xmath

11: @xmath

12: repeat @xmath question / correction iteration

13: if ( inContext ( @xmath , context )) then

14: Present a previously unseen instance of @xmath

15: Ask the category of this instance

16: If needed, provide correct feedback

17: c @xmath c+1 if else @xmath

18: if ( @xmath ) then @xmath sliding window

19: @xmath

20: @xmath accuracy in last @xmath question/correction iterations

21: until ( ( @xmath > @xmath and @xmath ) or @xmath accuracy threshold
crossed

22: (user sees no improvement) ) @xmath breakpoint reached

23: until ( user sees no improvement in accuracy ) @xmath breakpoint
reached

Teaching protocol with context change

#### 7.3.2 Measuring the Adaptability to Context Change

A new adaptability metric is proposed here in order to compare the
adaptability of different approaches to context change. This metric is
intended to be orthogonal to other metrics, namely accuracy or number
learned categories. Adaptability is a measure of relative performance of
the learning agent in the second context, B, when compared with the
performance of the same agent in the first context, A. To carry out this
comparison in a controlled way, we define, for each learning approach,
the size (i.e., number of categories) of the first context based on the
average number of learned categories of that learning approach in a set
of standard teaching protocol experiments i.e., without context change
(see subsection 7.2 ). To ensure that all categories in context A are
learned, enabling the agent to move to context B, the number of
categories of context A is set to be on average 0.75 ALC, where ALC is
the average number of learned categories in a round of experiments
without context change. In order to converge to that average context
size, the context transition point, @xmath , is generated randomly in
the interval @xmath . for each context change experiment. This way, for
each learning approach, the context change happens at the same point
(i.e., around 0.75 ALC) with respect to the full capacity of that
approach as captured by ALC. This enables an evaluation of adaptability
orthogonal to the evaluation of learning capacity. With this setup,
adaptability is measured by:

  -- -------- -- -------
     @xmath      (7.1)
  -- -------- -- -------

where @xmath and @xmath are the average numbers of categories learned in
the first and second contexts respectively. It should be noted that, if
the experiment is finished due to “ lack of data ”, it is not possible
to measure adaptability. In order for the adaptability to be comparable
across different experiments, it is essential that all experiments end
with the same terminating condition (breakpoint).

#### 7.3.3 Instance-Based Approaches

The results obtained with the instance-based approaches are presented in
Fig. 7.5 and Fig. 7.6 . Several observations can be made based on Fig.
7.5 . The agent with GOOD and Local LDA representations show a good
balance among all evaluation criteria. In particular, the best
performance, in terms of total number of learned categories in both
contexts, was obtained with GOOD, closely followed by the local LDA
approach. Approach II achieved the best GCA and APA accuracies. This
point could be explained by the fact that a higher number of categories
learned by the other approaches tends to make the classification task
more difficult. It is worth to mention that the agent was never able to
learn all the existing categories with instance-based approaches.

In terms of number of learned categories in the second context, the
agent learned (on average) more categories using Local LDA than with
other approaches. The underlying reason is that Local LDA transforms
objects from bag-of-words space into a more complex space and used
distribution over distribution representation for providing powerful
representation. BoW, GOOD and Approach II achieved the second, third and
forth places respectively. The worst one was LDA since the agent needs
more data and time to reduce the effects of the topics learned in the
first context (left column of Fig. 7.5 ).

In terms of average number of stored instances per category (AIC),
although Approach II on average stored smallest number of instance per
category, its discriminative power is not good. BoW, GOOD and local LDA
achieved the second, third and forth places respectively while they
provided a good balance between discriminative power and number of
question/correction iterations (QCI). LDA was the worst approach in this
comparison. The results of adaptability evaluation of all approaches are
reported in Fig. 7.6 . By comparing all experiments, it is visible that
only when the agent used BoW approach, it could learn more categories in
the second context than in the first context. Therefore, the most
adaptable approach was BoW (adaptability of 1.01), followed by Local LDA
(adaptability of 0.86). The Approach II and standard LDA achieved the
third and forth places respectively. The agent with GOOD was the less
adaptable approach.

#### 7.3.4 Model-Based Approaches

Another set of experiments in a multi context scenario was carried out
to evaluate how different model-based object recognition approaches cope
with the effects of the context change (see Fig. 7.7 ).

The left column in Fig. 7.7 provides a detailed summary of the obtained
results. Concerning ALC, in all experiments, the agent learned all the
existing categories in the dataset using GOOD and Local-LDA. Therefore,
experiments finished due to “ lack of data ”, and not by reaching the
breakpoint. Therefore, more categories have been learned with GOOD and
Local LDA if they were available in the dataset (see Fig. 7.7 ( c and d
)). BoW achieved the third place. We observed that in 8 out of 10
experiments with BoW, all the categories in the dataset were learned by
the agent. Standard LDA achieved the fourth place. The agent in none of
the tests could learn all categories in the second context.

By comparing all experiments, it is clear that the number of iterations
required to learn all object categories with GOOD was smaller than other
approaches. The agent with GOOD, on average, learned all categories
after @xmath iterations. Local LDA achieved the second place. It is also
clear from Fig. 7.7 that the GOOD achieved the best accuracies (i.e.,
APA and GCA) with stable performance and clearly outperforms the other
approaches.

As depicted in the center column of Fig. 7.7 , the best performance, in
terms of number of learned categories and the number of iterations
required to learn a certain number of categories, was obtained with the
GOOD, closely followed by the Local LDA approach. The agent with BoW
also showed good results. The worst one was standard LDA since the agent
needs more data and time to reduce the effects of the topics learned in
the first context.

The right column of Fig. 7.7 shows the global classification accuracy
(i.e., since the beginning of the experiment) as a function of the
number of learned categories. It is clear from Fig. 7.7 (right) that
GOOD achieved the best accuracies with stable performance and clearly
outperforms the other approaches. Local LDA achieved the second places
also with stable performance. The worst one was standard LDA, since it
uses shared topics among all categories and the agent needs more data
and time to reduce the effects of the topics learned in the first
context.

Since the proposed adaptability evaluation presupposes that experiments
reach the breakpoint, it was not possible to compute the adaptability of
GOOD, Local LDA and BoW approaches. In fact, the agent with GOOD and
Local LDA successfully learned all categories in both contexts and all
experiments concluded prematurely due to the “ Lack of data ”.
Similarly, the agent with BoW, in 8 out of 10 experiments could learn
all categories in both contexts. The adaptability of the standard LDA
approach was 0.85, much better than the adaptability of instance-base
learning with the same representation (adaptability of 0.58).

#### 7.3.5 Demonstration

A real demonstration was carried out using Imperial College domestic
environment dataset (Doumanoglou et al., 2016 ) . For this
demonstration, an instance-based learning approach with BoW has been
integrated into the object perception system. The system initially had
no prior knowledge. In the first context, an instructor teaches two
object categories including oreo and amita to the system, and the system
conceptualizes those categories. The system is then tested by the other
scenes captured from different viewpoints. The system could recognize
all objects properly (see Fig. 7.1 top-left ). Then, the system is moved
to the second context. In this context, the system gains knowledge about
lipton and softkings categories. Similar to the first context, the
knowledge of system is validated using several scenes (see Fig. 7.1
bottom-left ). Later, we moved the system to a new context, which is
much more crowded and complex than the previous contexts. In this
context, eight instances of the four categories exist. The robot could
recognize all objects correctly by using knowledge from previous
contexts (see Fig. 7.1 right ). This evaluation illustrates the process
of acquiring object categories in an open-ended fashion from multi
contexts. Moreover, it shows that disrupting or erasing the category
models learned from the previous contexts is not a rational choice. A
video of this demonstration is online at: https://youtu.be/l6q6fI5H6zY .

### 7.4 Summary

In this chapter, we defined an online evaluation approach to assess the
performance of open-ended object recognition approaches regarding their
ability to cope with the effects of the context change in multi-context
scenarios. Two learning approaches (instance-based and model-based) with
several object representations were evaluated using the proposed
methodology. Two sets of experiments were carried out to assess the
performance of all the proposed 3D object category learning and
recognition approaches in single context and two-context scenarios.
Experimental results proved that all the approaches can incrementally
learn new object categories.

In case of instance-based approaches, the overall number of categories
learned with GOOD is clearly better than the best performances obtained
with the other approaches. The agent with Local LDA also demonstrated an
appropriate balance among all criteria. The underlying reason was that
Local LDA used distribution over distribution representation for
providing powerful representations. Furthermore, adaptability of all
approaches was evaluated. BoW was the most adaptable approach
immediately followed by the Local LDA. Therefore, the agent could adapt
better to a new context when it uses BoW or Local LDA approaches. It was
observed that BoW and Local LDA provided a good balance between
discriminative power and number of question/correction iterations (QCI).
The discriminative power of Approach II and standard LDA were clearly
not good for such context-sensitive environments.

In case of model-based approaches, experimental results show that the
overall classification performance obtained with GOOD is comparable to
the best performances obtained with the other approaches. Moreover, it
was observed that the agent could learn all categories in both contexts
when it uses GOOD and Local LDA approaches. Similar to the
instance-based experiments, it was observed that GOOD and Local LDA
provided a good balance between discriminative power and number of
question/correction iterations (QCI). The discriminative power of
standard LDA was not good when compared with the other approaches.
Furthermore, adaptability of all approaches was evaluated. Since the
proposed adaptability evaluation presupposes that experiments reach the
breakpoint, it was not possible to compute the adaptability of GOOD, BoW
and Local LDA. Therefore, the best adaptability was achieved by standard
LDA.

For future work we would like to investigate the possibility of
improving performance by considering both external and internal
contexts, since there are several evidences of a relation between the
external and internal contexts (Kokinov, 1997 ) (Qian et al., 2012 ) .
It would also be important to do experiments with a larger dataset in
order to avoid the “lack-of-data” termination condition in open-ended
evaluations.

## Chapter 8 System Demonstration and Profiling

Throughout this thesis, a set of interactive open-ended learning
approaches for grounding 3D object categories has been presented,
enabling robots to adapt to different environments and reason out how to
behave in response to the request of a complex task. In this chapter,
three types of experiments were carried out to evaluate the proposed
approaches. First, based on a session where users manipulate objects on
a table and interact with the developed perception and perceptual
learning system, we carry out a profiling analysis of the main modules
of the system. We also used the recorded session to demonstrate all the
characteristics of the proposed GOOD descriptor. Second, two scenarios
namely clear_table and serve_a_meal have been designed to show all
functionalities of the object recognition and grasping. These
demonstrations show that the proposed approaches have been successfully
tested on a PR2 robot and a JACO arm, showing the importance of having a
tight coupling between perception and manipulation. Finally, two
demonstrations using Washington RGB-D Scenes Dataset v2 and Imperial
College Domestic Environment Dataset (Doumanoglou et al., 2016 ) have
been performed. These demonstrations showed that the system is capable
of using prior knowledge to recognize new objects in the scene and learn
about new object categories in an open-ended fashion. All tests were
performed with an i7, 2.40GHz processor and 16GB RAM.

### 8.1 Open-Ended 3D Object Category Learning Scenarios

To show all the functionalities of the system, a session has been
recorded, where several users interacted with the system. During this
session, users presented objects to the system and provided the
respective category labels. All raw data from the RGB-D sensor was
recorded in a rosbag, which was then used to test different
configurations of our system. Three demonstrations were performed using
the recorded rosbag. In all demonstrations, we have assumed that the set
of object categories to be learned is not known in advance and the
training instances are extracted from actual experiences of a robot
rather than being available at the beginning of the learning process. In
the first and third demonstrations, when the system started, the set of
categories known to the system was empty while in the second
demonstration, the system initially had prior knowledge about two
categories and there is no information about the other categories. In
these demonstrations, we have used instance-based object category
learning and recognition as discussed in chapter 6 , section 5.4 .

#### 8.1.1 The object perception system in an interactive session

A 3.5 minutes session has been recorded, where several users interacted
with the object perception system. During this session, users present
objects to the system and provide their respective category labels. The
system detects pointing gestures of the user and detects and tracks the
presented objects. Figure 8.1 presents some snapshots of this session.
Table 8.1 presents a summary of the main events. In more detail, the
session progressed as follows:

1.  The system works in scenario where a table is in front of the robot
    and there is no knowledge about any category. The graphical menu in
    front of the table is the interactive menu that enables teaching new
    object categories. The instructor puts a Mug on the table. Tracking
    is initialized with track ID 1 (TID1). The gray bonding box signals
    the pose of the object as estimated by the tracker. TID1 is
    classified as Unknown because mugs are not yet known to the system;

2.  Instructor labels TID1 as a Mug . The system conceptualizes the
    category;

3.  The Mug is correctly classified. The instructor places a Vase on the
    table. Tracking is initialized with TID2. The Vase is unknown to the
    system; this frame shows that the system is able to detect and track
    multiple objects in the scene. Moreover, it demonstrates that both
    the tracking and recognition work when the user is holding the
    objects;

4.  The instructor labels TID2 as a Vase. This labeling is done using a
    different interaction modality: the instructor points at track ID 2,
    and labels this object as Vase ;

5.  The Vase is properly recognized. An additional Mug is placed at the
    center of the table. Tracking is initialized with TID3. This
    particular Mug had not been previously seen, but the system can
    correctly recognize it, because the Mug category was previously
    taught. This shows that the system is capable of using prior
    knowledge to recognize new objects in the scene;

6.  Another instructor arrives; once he sits on front of the robot, he
    will be considered as the system’s instructor. This frame shows
    instructor detection and tracking;

7.  The instructor removes all objects from the scene; no objects will
    be visible;

8.  A Plate enters the scene. It is detected and assigned to TID4.
    Because there is no prior knowledge about plates, TID4 is classified
    as Unknown. TID4 is labeled as a Plate and the system conceptualized
    the category.

This sequence shows that the proposed architecture is capable of
detecting new objects, tracking and recognizing those object in various
positions. Moreover, it shows capability of human-robot interaction
based on a graphical interface and pointing gesture recognition.

All raw data from the RGB-D sensor was recorded in a rosbag, which was
then used to test different configurations of our system. Three
demonstrations were performed using the recorded rosbag.

#### 8.1.2 Profiling

In the first demonstration, objects are represented by sets of
spin-images. The instance-based learning approach (section 5.4 ) is
adopted. In addition to the basic object and user perception already
included in the recorded rosbag, the system now conceptualizes and
recognizes object categories. A video of this demonstration is available
at: https://youtu.be/XvnF2JMfhvc

Based on this demonstration, different aspects of the performance of the
system were profiled. As discussed in chapter 2 , section 2.4 , nodelets
can significantly improve the efficiency since they support zero copy
transport and they enable simultaneous access to LevelDB. Figure 8.2
compares the processing time of the object perception modules. The
tracker modules (Fig. 8.2 ( a ) nodes and ( d ) nodelets) tend to
display a stable processing time shortly after their initialization.
This is explained by the fact that the size of the input data is more or
less stable over time. In this case, nodelets are more efficient when
compared to nodes: for example for pipelines 1, 2 and 3 in the 100 to
150 time interval, nodes display an average processing time of 45
miliseconds, compared to 25 miliseconds in the case of nodelets. Since
the trackers do not access the database, the main factor contributing to
the increase in efficiency is the zero copy transport.

The messages that are received (sensor point cloud) and sent (partial
object point cloud) by the trackers are of large size, which explains
why zero copy transport enables such a significant improvement. The
feature extraction modules (Fig. 8.2 ( b ) nodes and ( e ) nodelets)
show a different behaviour. These modules periodically compute the
spin-image representation from the partial object point cloud. At some
points, the point cloud is signaled to belong to a key view, which will
trigger the writting of that representation to the perceptual memory.

The curves show these points in time with a rapid increase in processing
time. Nodelets also display these peaks, but because access to the
database is much faster, the peaks are smaller, as is the average
processing time. The object recognition modules (Fig. 8.2 ( c ) nodes
and ( f ) nodelets) receive a representation of the current object view
from the feature extraction, and compare it against the representations
of all known category views. Thus, they are continuously reading the
database in the search for an update to the known categories. As a
result, the larger the size of the database, the slower the reading of
the complete set of categories. However, in the case of nodelets, this
deterioration is minor when compared with nodes, since accessing the
database is much more efficient.

Figure 8.3 ( left ) shows the memory usage of the system. Notice that at
the end of the experiment the memory size would be above 1 MB if all
object point clouds extracted by the trackers would be stored (roughly 5
Kb / sec.). In a continously running system, this rate of data
acumulation would be hard to handle, and would not bring any real
benefit. The total size of the point clouds of all the selected key
views is much smaller (one order of magnitude in this experiment). The
data actually acumulated in memory (shape representations based on
spin-images) is even smaller.

Figure 8.3 ( right ) shows the evolution of object recognition
performance throughout the experiment. When the first Mug (T1) is placed
on the table the system recognizes it as Unknown. After some time, the
instructor labels T1 as a Mug and the system starts displaying a
precision of 1.0. However, the recall score is under 0.2, because the
system classified T1 as Unknown several times before the user labelled
the object. After the labelling, the recall starts improving
continuously. The instructor then places a Vase (T2) on the table.
Because the category Vase has not been taught yet, the performance goes
down. After labelling T2 as Vase, performance starts going up again.
When a second Mug (T3) enters the scene, the system can correctly
recognize it and the scores continue to increase. Then, a Plate (T4)
enters the scene, causing recall to drop. Successively, the Plate is
taught, a Bottle is placed on the table and then taught, and eventually
performance starts going up again. This illustrates the process of
acquiring categories in an open-ended fashion with user mediation.

#### 8.1.3 Local LDA

In this demonstration, we configured the system to use the Local LDA
representation proposed in Chapter 4 . Initially, the system only had
prior knowledge about the Vase and Dish categories, learned from batch
data (i.e. set of observations with ground truth labels), and there is
no information about other categories (i.e. Mug , Bottle , Spoon ).
Throughout this session, the system must be able to recognize instances
of learned categories and incrementally learn new object categories.
Figure 8.4 illustrates the behaviour of the system:

1.  The instructor puts object TID6 (a Mug ) on the table. It is
    classified as Unknown because mugs are not known to the system;
    Instructor labels TID6 as a Mug . The system conceptualizes Mug and
    TID6 is correctly recognized. The instructor places a Vase on the
    table. The system has learned the Vase category from batch data,
    therefore, the Vase is properly recognized (Fig. 8.4 ( left )).

2.  Later, another Mug is placed on the table. This particular Mug had
    not been previously seen, but the system can recognize it, because
    the Mug category was previously taught (Fig. 8.4 ( right )).

This demonstration shows that the system is capable of using prior
knowledge to recognize new objects in the scene and learn about new
object categories in an open-ended fashion. A video of this
demonstration is available at: https://youtu.be/J0QOc_Ifde4 .

#### 8.1.4 Good

To show all the described functionalities and properties of the GOOD
descriptor, another demonstration was performed using the recorded
rosbag. For this purpose, GOOD has been integrated in the RACE object
perception system presented in Chapter 2 (see Fig. 2.3 ) (Kasaei et al.,
2015a ; Oliveira et al., 2015b , 2014a ) . It should be noted that a
constraint has been set on the Z axis that the initial direction of
@xmath axis of objects’ LRF should be similar to the @xmath axis
direction of the table LRF. There are no learned categories in memory at
the beginning of the demonstration. It was observed that the proposed
object descriptor is capable to provide distinctive global feature for
recognizing different types of objects. It also estimates poses of
objects and build orthographic projections for object manipulation
purposes (see Fig. 8.5 ). A video of this demonstration is available in:
https://youtu.be/iEq9TAaY9u8 .

### 8.2 Assistive Robotic Scenarios: Coupling Perception and
Manipulation

Elderly, injured, and disabled people have consistently attributed a
high priority to object manipulation tasks (Jain and Kemp, 2010 ) .
Object manipulation tasks consist of two phases: the first is the
perception of the object and the second is the planning and execution of
arm or body motions for grasping the object and carrying out the
manipulation task. These two phases are closely related: object
perception provides information to update the model of the environment,
while planning uses this world model information to generate sequences
of arm movements and grasp actions for the robot. In addition, assistive
robots must perform the tasks in reasonable time. It is also expected
that the competence of the robot increases over time, that is, robots
must robustly adapt to new environments by being capable of handling new
objects.

To show the strength of the proposed perception system, two qualitative
analysis of the coupling between perception and manipulation for service
robots are shown and analysed in this section. In this case, two
demonstrations are described, where users manipulate objects on a table
and interact with the system to instruct the robot to perform a
“clear_table” and “serve a meal” task as well as teach object categories
to the robot. In both demonstrations, a naive Bayes learning approach
with a Bag-of-Words object representation are used to acquire and refine
object category models. Moreover, a JACO robotic arm manufactured by
KINOVA, as depicted in Fig. 8.7 , is used. It has six degrees of freedom
and a three fingers gripper. Since the JACO arm can carry up to 1.5kg ¹
¹ 1 http://www.kinovarobotics.com , it is ideal for manipulating
everyday objects. Moreover, infinite rotation around the wrist joints
allows for flexible and effective interaction in a domestic environment.
It should be noted that grasping itself is not in the scope of this
thesis. Previously, we showed how to grasp household objects in
different situations (Kasaei et al., 2016e ; Shafii et al., 2016 ) .

#### 8.2.1 “Clear_Table” Scenario

In this demonstration, the system works in a scenario where a table is
in front of the robot and a user interacts with the system. Note that,
at the start of the experiment, the set of categories known to the
system is empty. During the session, a user presents objects to the
system and provides the respective category labels. The user then
instructs the robot to perform a clear_table task (i.e. puts the table
back into a clear state). To achieve this task, the robot must be able
to detect and recognize different objects and transport all objects
except standard table items (e.g. table sign, flower, etc.) to
predefined areas. While there are active objects on the table, the robot
retrieves the world model information from the Working Memory including
label and position of all active objects. The robot then selects the
object which is closer to the arm’s base and clears it from the table
(see Fig. 2.7 left ). In case the system predicts a category that is not
the true object category, both a false negative (true object category
not detected) and a false positive (predicted object category not
correct) are accounted for. Figure 8.6 ( right ) shows the evolution of
object recognition performance throughout the experiment. First, the
system recognizes all table-top objects as Unknown . After some time,
the instructor labels T1 as a Vase and the system starts displaying a
recall of 1.0. However, the precision starts to decrease, because the
categories Bottle , Coffee Jug and Plastic Cup have not been taught yet,
the performance goes down. After labelling the objects, the precision
starts improving continuously. As it is shown in the Fig. 8.6 ( right ),
whenever the robot grasps an object (i.e. iterations 155, 280, 332), the
shape of the object is partially changed, misclassification might happen
and the performance goes down. The grasped object is then transported to
the placing area and the tracking of the object is lost (i.e. iterations
181, 302, 375). Afterwards, the performance starts going up again. A
video of this session is available at: https://youtu.be/cTK10iNyYXg .

We also provide another demonstration for the clear_table task. A video
of this demonstration can be found at: https://youtu.be/LZtI-s95uTk

#### 8.2.2 “Serve_A_Meal” Scenario

Similar to the previous demonstration, a user presents objects to the
system and provides the respective category labels throughout the
session. The user then instructs the robot to perform a serve a meal
task (i.e. puts different restaurant objects on the table in front of
the user). The setup of our robotic system and the system reference
frames for the serve a meal scenario are shown in Fig. 8.7 . To achieve
this task, the robot must be able to detect and recognize different
objects and transport the objects to the predefined areas and completely
serve a meal. For this purpose, the robot retrieves the world model
information from the Working Memory , including label and position of
all active objects. The robot then chooses the object that is nearest to
the arm base frame and serves it to the user. A video of this
demonstration is available at: https://youtu.be/GtXBiejdccw .

These small demonstrations show that the developed system is capable of
detecting new objects, tracking and recognizing them, as well as
manipulating objects in various positions. Moreover, it shows how
human-robot interaction is currently supported.

### 8.3 Demonstrations using Scenes Datasets

Two demonstrations have been performed using the Washington RGB-D Scenes
Dataset v2 (Lai et al., 2014 ) and the Imperial College Domestic
Environment Dataset (Doumanoglou et al., 2016 ) . In particular, the
first demonstration was conducted using Washington RGB-D Scenes Dataset
(Lai et al., 2014 ) to show the strength of the proposed Local LDA
representation. In the second demonstration, a set of tests was executed
to measure the accuracy of the proposed GOOD descriptor on the Imperial
College Domestic Environment Dataset (Doumanoglou et al., 2016 ) . In
both demonstrations, an instance-based learning approach is used, i.e.,
object categories are represented by sets of known instances. Similarly,
a simple baseline recognition mechanism in the form of an Euclidean
nearest neighbor classifier is used. These demonstrations show that the
proposed system supports classical learning from a batch of train
labeled data and open-ended learning from actual experiences of a robot.

#### 8.3.1 Demonstration I

A real demonstration was carried out using the Washington RGB-D Scenes
Dataset v2 (Lai et al., 2014 ) . This dataset consists of 14 scenes
containing a subset of the objects in the RGB-D Object Dataset,
including bowls , caps , mugs , soda , cans and cereal boxes . For this
demonstration, an instance-based learning approach with the Local LDA
has been integrated into the object perception system presented in
chapter 2 . It is worth mentioning that in Local LDA each object view
was described as a random mixture over a set of latent topics, and each
topic was defined as a discrete distribution over visual words. The
system initially had no prior knowledge. The four first objects are
introduced to the system using the first scene and the system
conceptualizes those categories. The system is then tested using the
second scene of the dataset and it can recognize all objects except
cereal boxes, because this category was not previously taught. The
instructor provided corrective feedback and the system conceptualized
the cereal boxes category. Afterwards, all objects are classified
correctly in all 12 remaining scenes. This evaluation illustrates the
process of acquiring categories in an open-ended fashion. Results are
depicted in Fig. 8.8 . A video of this demonstration is online at:
https://youtu.be/pe29DYNolBE .

#### 8.3.2 Demonstration II

Imperial College dataset is related to domestic environments, where
everyday objects are placed on a kitchen table (Doumanoglou et al., 2016
) . It consists of variety of different scenes with a set of table top
objects including amita , colgate , lipton , elite , oreo , softkings ,
mug , shampoo and salt-shaker . This dataset contains @xmath different
sets of table-top scenes from two different heights which has @xmath
scenes in total (see Fig. 8.9 ). Each set consists of several views of
table-top scenes to cover 360 degrees view around the table. The objects
were extracted from the table-top scenes by running the proposed object
detection. This is an especially suitable dataset to evaluate the system
since the object dataset was collected under various clutter conditions
and distances. The objects were extracted from the scenes by running the
object segmentation proposed in chapter 3 . All detected objects were
manually labelled by the author. To examine the performance of the
proposed approach, a 10-fold cross-validation has been used. The
accuracy of the object recognition system was @xmath for the extracted
objects. A set of results is visualized on Fig. 8.10 .

### 8.4 Summary

In this chapter, a set of evaluations and demonstrations has been
performed to show all the functionalities of the system for object
recognition as well as its relevance for grasping. These demonstrations
showed that the system can incrementally learn new object categories and
perform manipulation tasks in reasonable time and appropriate manner.
Our approach to object perception has been successfully tested on a JACO
arm, showing the importance of having a tight coupling between
perception and manipulation. We also report on two demonstrations of the
system using Washington RGB-D Scenes Dataset v2 and Imperial College
Dataset (Doumanoglou et al., 2016 ) . These demonstrations showed that
the system is capable of using prior knowledge to recognize new objects
in the scene and learn about new object categories in an open-ended
fashion.

## Chapter 9 Conclusions and Future Research Directions

One of the primary challenges of service robotics is the adaptation of
robots to new tasks in changing environments, where they interact with
non-expert users. This challenge requires support from complex
perception routines which can learn and recognize object categories in
open-ended manner based on human-robot interaction. In this thesis, we
assumed that versatility and competence enhancement can be obtained by
learning from experiences. This thesis focused on acquiring and
conceptualizing experiences about objects as a means to enhance robot
competence over time thus achieving robustness. Towards this end, two
perception architectures were explored that can be used by robotic
agents for long-term and open-ended category learning. The problem of
scaling-up to larger number of categories and adapting to new contexts
were addressed using these architectures and the corresponding learning
approaches. Moreover, some realistic scenarios were designed, where a
human or a simulated instructor taught the robot the names of the
objects present in their shared visual environment. This work has been
integrated in a larger effort in the framework of the European project
RACE (Robustness by Autonomous Competence Enhancement (Hertzberg et al.,
2014 ) ), in which robot performance improves with accumulated
experiences and conceptualizations.

### 9.1 Contributions

The contributions of this thesis consist in multiple theoretical
formulations and practical solutions to the problem of open-ended 3D
object category learning and recognition. They are listed as follows:

-   The first contribution is focused on the development of a 3D object
    perception and perceptual learning system designed for a complex
    artificial cognitive agent working in a restaurant scenario. This
    system, developed in collaboration with other researchers in IEETA
    and within the scope of the European project RACE, integrates
    detection, tracking, learning and recognition of tabletop objects.
    Interaction capabilities were also developed to enable a human user
    to take the role of instructor and teach new object categories.
    Thus, the system learns in an incremental and open-ended way from
    user-mediated experiences. Thorough details of the agent’s complete
    architecture have been presented in chapter 2 .

-   The second contribution is related to the gathering object
    experiences in both supervised and unsupervised manner. In general,
    gathering object experiences is a challenging task because of the
    dynamic nature of the world and ill-definition of the objects. In
    this work, a system of boolean equations was used for encoding the
    world and object candidates. In particular, we proposed automatic
    perception capabilities that will allow robots to automatically
    detect multiple objects in a crowded scene. The relevant aspects
    have been described in chapter 3 .

-   Other contributions, presented in chapter 4 , are concerned with
    object representation. Object representation is one of the most
    challenging tasks in robotics because it must provide reliable
    information in real-time to enable the robot to physically interact
    with the objects in its environment. We have tackled the problem of
    object representation, by proposing a novel global object descriptor
    named Global Orthographic Object Descriptor (GOOD). It has been
    designed to be robust, descriptive and efficient to compute and use.
    The overall classification performance obtained with GOOD was
    comparable to the best performances obtained with the
    state-of-the-art descriptors. Concerning memory and computation
    time, GOOD clearly outperformed the other descriptors. Therefore,
    GOOD is especially suited for real-time applications. The estimated
    object’s pose is precise enough for real-time object manipulation
    tasks.

    We also proposed an extension of Latent Dirichlet Allocation to
    learn structural semantic features (i.e., topics) from low-level
    feature co-occurrences for each object category independently.
    Although the model we developed has been intended to be used for
    object category learning and recognition, it is a novel
    probabilistic model that can be used in the fields of computer
    vision and machine learning.

-   The problem of open-ended learning for 3D object category
    recognition has been tackled in chapter 5 . We approached object
    category learning and recognition from a long-term perspective and
    with emphasis on open-endedness, i.e. not assuming a pre-defined set
    of categories. The major contributions are the following: ( i )
    defining new distance functions for estimating dissimilarity between
    sets of local shape features that can be used in instance-based
    learning approaches; ( ii ) proposing a learning approach to
    incrementally learn probabilistic models of object categories to
    achieve adaptability.

-   The last contribution of this dissertation is concerned with the
    evaluation of open-ended object category learning and recognition
    approaches in multi-context scenarios. Off-line evaluation
    approaches such as cross-validation do not comply with the
    simultaneous nature of learning and recognition autonomous agents.
    An adaptability measure and a teaching protocol, supporting context
    change, were therefore designed and used for open-ended experimental
    evaluation. This contribution has been presented and discussed in
    chapter 7 .

All the algorithms and concepts presented in this thesis have been
implemented and tested on data acquired in realistic restaurant
environments. A set of experimental results was also carried out on two
different robotic platforms, including the PR2 and the manipulation
platform at the university of Aveiro (see Figure 2.1 ).

### 9.2 Future Research Directions

Despite the promising results presented in this thesis, there are a list
of open issues that still remain to be tackled for future research:

-   Though we proved the usefulness of 3D geometry in the context of
    learning and recognizing object categories, this thesis has not
    addressed the fusion of color information with geometry at all.
    Color and texture are two important features for particular
    applications which geometry alone cannot solve. For example, color
    and texture information can also be used to distinguish objects that
    have the same geometric properties with different texture (e.x. a
    Coke can from a Diet Coke can).

-   In this thesis, we already tackled the problem of environment
    exploration and visual word dictionary construction but mostly in
    terms of building the dictionary in advance by feeding a sample set
    of features of extracted objects to a clustering algorithm e.g.,
    K-means. Since the proposed architectures receive a continuous
    stream of 3D data, we would like to consider data stream clustering
    methods to update the visual word dictionary as another direction of
    future work. We are already taking steps towards addressing this
    point (Oliveira et al., 2015a ) .

-   Although the work in this thesis mainly focused on 3D object
    category learning, it would be interesting and relevant to extend
    the proposed learning architecture to other domains, including grasp
    learning for object manipulation. We are already taking steps
    towards addressing this point and some interesting results have
    already been published (Kasaei et al., 2016e ; Shafii et al., 2016 )
    .

-   Currently, a popular approach in computer vision is deep learning.
    However, there are several limitations to use Deep Neural Networks
    (DNN) in open-ended domains. Deep networks are incremental by nature
    but not open-ended, since the inclusion of novel categories enforces
    a restructuring in the topology of the network. Overcoming such
    limitations is also one of the possible directions of continuation
    of the work in thesis. The relevant keywords for this research topic
    are Zero-shot learning , Low-shot learning and Overcoming
    catastrophic forgetting in neural networks .

-   In the continuation of this work, we will also investigate the
    possibility of improving memory management by considering salience
    and forgetting mechanisms.

-   I believe this framework has great potential for further
    developments. During the development of the proposed architecture, I
    attended several conferences to discuss my research with many
    experts. The feedbacks I received broadened my view and encouraged
    me to further develop the system. The functionalities of the
    developed system make it unique in the robotic community. Currently,
    it provides several functionalities that will allow robots to: ( i )
    detect objects in highly crowded scenes, ( ii ) incrementally learn
    object categories from the set of accumulated experiences, ( iii )
    construct the full model of an unknown object in an on-line manner,
    ( iv ) infer how to grasp objects in different situations, ( v )
    predict the next-best-view to improve object detection and
    manipulation.
