## Chapter 1 Introduction

This thesis deals with the subject of classical groups. Specifically, we
deal with the Gaussian elimination for some similitude groups, and
conjugacy classes of centralizers for certain classical groups. We give
a concrete algorithm for symplectic and split orthogonal similitude
groups analogous to the usual row and column operations to solve the
word problem. Also, we give structure of centralizers and classes of
centralizers in unitary groups to complete the story for classical
groups, at least as far as the topics we deal with are concerned.

### What is the Gaussian elimination?

Gaussian elimination is a very old technique in Mathematics. It appeared
in print as chapter eight in a Chinese mathematical text called, “The
nine chapters of the mathematical art”. It is believed, a part of that
book was written as early as @xmath BCE. For a historical perspective on
Gaussian elimination, we refer to a nice work by Grcar [ Gc ] .

In computational group theory, one is always looking for algorithms that
solve the word problem (for definition see p. 4, section 1.4 [ Ob ] ).
Algorithms for word problem are useful in other programs in
computational group theory, namely, the group recognition program and
the membership problems. Extensive work on these programs are being done
by several people Leedham-Green and O’Brien [ LO ] , and Guralnick
et.al. [ GKKL ] . Thus, one of the main objectives of this thesis is to
give an algorithm, on similar lines as the row-column operations for
general linear groups, to solve the word problem for similitude groups.
In this thesis, we work with Chevalley generators [ Ca1 ] . Chevalley
generators for the special linear group @xmath are elementary
transvections, which are used to do the Gaussian elimination for @xmath
. The similitude groups are thought of as an analog of what @xmath is
for @xmath . So, for the Gaussian elimination of symplectic and split
orthogonal similitude groups, we use the Chevalley generators.

These Chevalley generators for classical groups are well-known for a
very long time. However, its use in row-column operations in symplectic
and split orthogonal similitude groups is new. We develop row-column
operations, very similar to the Gaussian elimination algorithm for
general linear groups. We call our algorithms Gaussian elimination in
symplectic and split orthogonal similitude groups respectively.

In a nutshell, Gaussian elimination is nothing but a series of row and
column operations. For details see Chapter 6 . The algorithms that we
develop in this thesis work for a split bilinear form @xmath (see (4) in
Example 2.2.22 ). First, we define elementary matrices (see Section 3.2
), which give elementary operations (see 6.2 ) for similitude groups. We
prove the following result:

###### Theorem 1.0.1 (Theorem 6.3.11).

Every element of the symplectic similitude group @xmath or split
orthogonal similitude group @xmath (here @xmath or @xmath ), can be
written as a product of elementary matrices and a diagonal matrix.
Furthermore, the diagonal matrix is of the following form:

1.   In @xmath , @xmath , where @xmath .

2.   In @xmath , @xmath , where @xmath .

3.   In @xmath , @xmath , where @xmath and @xmath .

### What is the spinor norm and why study them?

Let @xmath be a field with @xmath . The spinor norm is a group
homomorphism @xmath defined by @xmath , where @xmath using
Cartan-Dieudonne theorem (see Section 2.2.3 ) and @xmath is the
quadratic form associated to the bilinear form @xmath . In the
connection to group recognition project, Scott H. Murray and Colva M.
Roney-Dougal [ MR ] studied spinor norm earlier. The definition of the
spinor norm is not friendly to compute. Hahn, Wall, and Zassenhaus [ Ha
, Wa1 , Za ] developed a theory to compute the spinor norm. In this
thesis, we will give an efficient algorithm to compute the spinor norm
using Gaussian elimination algorithm. From Gaussian elimination
algorithm, one can compute the spinor norm easily. Since the commutator
subgroup of the orthogonal group is the kernel of the spinor norm
restricted to the special orthogonal group, so the following theorem
also gives a membership test for the commutator subgroup in the
orthogonal group. We prove the following result:

###### Theorem 1.0.2 (Theorem 7.1.2).

Let @xmath (here @xmath or @xmath ). Suppose Gaussian elimination
reduces @xmath to @xmath , where @xmath . Then the spinor norm @xmath .

### What are the @xmath-classes and why study them?

Let @xmath be a group. The elements @xmath and @xmath are said to be
@xmath -equivalent denoted as @xmath if their centralizers in @xmath are
conjugate, i.e., @xmath for some @xmath , where @xmath denotes
centralizer of @xmath in @xmath . Clearly @xmath is an equivalence
relation on @xmath . The equivalence classes with respect to this
relation are called @xmath -classes . It is easy to see that if two
elements of a group @xmath are conjugate then their centralizers are
conjugate, thus they are also @xmath -equivalent. However, in general,
the converse is not true. In fact, a group may have infinitely many
conjugacy classes but finitely many @xmath -classes (see Example 8.2.7
). In this thesis, we explore the @xmath -classes for classical groups.
In [ St2 ] , R. Steinberg proved the following:

###### Theorem 1.0.3 (Steinberg).

Let @xmath be a reductive algebraic group defined over an algebraically
closed field @xmath of good characteristic, then the number of @xmath
-classes in @xmath is finite.

###### Question 1.0.4.

What can we say about the finiteness of @xmath -classes for algebraic
group @xmath defined over an arbitrary field @xmath ?

To study this we assume that the field @xmath satisfies the following
property:

###### Definition 1.0.5 (Property FE).

A perfect field @xmath of @xmath has the property FE if @xmath has only
finitely many field extensions of any fixed finite degree.

Examples of such fields are, algebraically closed fields (for example,
@xmath ), real numbers @xmath , local fields (for example, @xmath ), and
finite fields @xmath . From now on we assume that @xmath has property FE
unless stated otherwise. In [ Si ] , A. Singh studied @xmath -classes
for real compact groups of type @xmath . Ravi S. Kulkarni proved the
following (see Theorem 7.4 [ Ku ] ):

###### Theorem 1.0.6 (Kulkarni).

Let @xmath be an @xmath -dimensional vector space over a field @xmath
with the property FE, then the number of @xmath -classes in @xmath is
finite.

K. Gongopadhyay and Ravi S. Kulkarni proved the following (Theorem 1.1 [
GK ] ):

###### Theorem 1.0.7 (Gongopadhyay-Kulkarni).

Let @xmath be an @xmath -dimensional vector space over a field @xmath
with the property FE, equipped with a non-degenerate symmetric or
skew-symmetric bilinear form @xmath . Then, there are only finitely many
@xmath -classes in orthogonal groups @xmath and symplectic groups @xmath
.

This result generalizes Steinberg’s result mentioned above (Theorem
1.0.3 ). In this thesis, we extend this result to the unitary groups. We
prove the following result:

###### Theorem 1.0.8 (Theorem 8.2.4).

Let @xmath be a perfect field of @xmath with a non-trivial Galois
automorphism of order @xmath . Let @xmath be a finite dimensional vector
space over @xmath with a non-degenerate hermitian form @xmath . Suppose
the fixed field @xmath has the property FE, then the number of @xmath
-classes in the unitary group @xmath is finite.

The FE property of the field is necessary for the above theorem. For
example, the field of rationals @xmath does not have property FE. We
show that the above theorem is no longer true over @xmath (see Example
8.2.6 ).

If we look at character table of @xmath (for example see [ B ] and [ Pr
] ), we notice conjugacy classes and irreducible characters bunched
together (see p. 404 in [ Gr ] ). One observes a similar pattern in the
work of Srinivasan [ Sr ] for @xmath . In [ Gr ] , Green studied the
complex representations of @xmath where he introduced the function
@xmath for the ‘types of characters/classes’ (towards the end of section
1 on page 407-408) which is same as the number of @xmath -classes in
@xmath .

In Deligne-Lusztig theory, where one studies representation theory of
finite groups of Lie type, @xmath -classes of semisimple elements play
an important role. In [ Ca2 ] Carter and in [ Hu2 ] Humphreys defined
genus of an algebraic group @xmath defined over @xmath . Two semisimple
elements have same genus if they are @xmath -equivalent in @xmath . Thus
understanding @xmath -classes for finite groups of Lie type, especially
semisimple @xmath -classes, and their counting is of importance in
representation theory (see [ Fl , FG , Ca2 , DM ] ). A. Bose, in [ Bo ]
, calculated the genus number for simply connected simple algebraic
groups over an algebraically closed field, and compact simple Lie
groups. In this thesis we prove the following:

###### Theorem 1.0.9 (Theorem 9.2.6).

The number of @xmath -classes in @xmath is same as the number of @xmath
-classes in @xmath if @xmath . Thus, the number of @xmath -classes for
either group can be read off by looking at the coefficients of the
function @xmath , where @xmath and @xmath is the number of partitions of
@xmath .

Along the way, we also prove some counting results (see for example,
Proposition 9.1.1 , Proposition 9.1.2 , Theorem 9.2.3 ).

A chapter wise description: A conscious effort is made to make this
thesis self-contained and reader-friendly. The results in Chapters
@xmath to @xmath are all well-known. They are preliminary in nature, and
almost all basic results are recalled in the first four chapters, which
are used in this thesis. After covering the preliminaries in the first
four chapters, we report on author’s research work in the next four
chapters. Finally, in the last chapter, we give some further research
problems. That pretty much summarizes the thesis giving glimpses into
the main results proved in the various chapters.

## Chapter 2 Classical Groups

This chapter is the most basic and at the same time most essential part
of this thesis. In this chapter, we will discuss the groups that are
popularly known as the classical groups, as they were named by Hermann
Weyl. Let @xmath be a field. Let @xmath be an @xmath -dimensional vector
space over @xmath . We denote the set of all invertible linear
transformations of @xmath by @xmath . The set @xmath is a group under
the multiplication defined by the composition of maps. Let us fix a
basis @xmath of @xmath . Then we can identify @xmath with @xmath , the
set of all @xmath invertible matrices. This group is called the general
linear group . All further groups discussed are subgroups of @xmath .
The special linear group @xmath . In Weyl’s words, “each group stands in
its own right and does not deserve to be looked upon merely as a
subgroup of something else, be it even Her All-embracing Majesty @xmath
”. The exposition in this chapter is mostly based on the book by Larry
C. Grove [ Gv ] . In Section 2.1 we describe reductive algebraic groups.
Section 2.2 covers the basic definitions and some very basic properties
of classical groups, especially for symplectic and orthogonal groups.
Also in this section, we introduce the notion of the spinor norm. In the
last section, we describe the unitary groups and some important
examples, which will be useful later in this thesis.

### 2.1 Reductive Algebraic Groups

There are several excellent references for this topic, Borel [ Br ] ,
Springer [ Sp ] and Humphreys [ Hu1 ] , to name a few. We fix a perfect
field @xmath ( @xmath ) for this section, and @xmath denotes the
algebraic closure of @xmath . An algebraic group @xmath defined over
@xmath is a group as well as an affine variety over @xmath such that the
maps @xmath , and @xmath given by @xmath , and @xmath are morphisms of
varieties. An algebraic group @xmath is defined over @xmath , if the
polynomials defining the underlying affine variety @xmath are defined
over @xmath , with the maps @xmath and @xmath defined over @xmath , and
the identity element @xmath is a @xmath -rational point of @xmath . We
denote the @xmath -rational points of @xmath by @xmath . Any algebraic
group @xmath is a closed subgroup of @xmath for some @xmath . Hence
algebraic groups are called linear algebraic groups .

An element in @xmath is called semisimple (respectively, unipotent ) if
it is diagonalizable over @xmath (respectively, if all its eigenvalues
are equal to @xmath ). We have @xmath . An element @xmath is said to be
semisimple (respectively, unipotent ) if the image of @xmath , under the
above inclusion, is semisimple (respectively, unipotent) in @xmath . An
algebraic group @xmath is said to be unipotent if all its elements are
unipotent. The radical of an algebraic group @xmath over @xmath is
defined to be the largest closed, connected, solvable, normal subgroup
of @xmath , denoted by @xmath . We call @xmath to be a semisimple
algebraic group if @xmath . The unipotent radical of @xmath is defined
to be the largest, closed, connected, unipotent, normal subgroup of
@xmath and denoted by @xmath . We call a connected group @xmath to be
reductive if @xmath . For example, the group @xmath is a reductive
group, whereas @xmath is a semisimple group. A semisimple algebraic
group is always a reductive group. In next section, we see more examples
of algebraic groups, namely, classical groups.

#### 2.1.1 Jordan decomposition

Recall that an element @xmath can be written as @xmath , in a unique
way, where @xmath is semisimple, and @xmath is unipotent. This
decomposition is called the Jordan decomposition for invertible
matrices. We have the following Jordan decomposition in linear algebraic
groups. We need the following (Theorem 2.4.8 [ Sp ] ),

###### Theorem 2.1.1 (Jordan decomposition).

Let @xmath be a linear algebraic group defined over a perfect field
@xmath and let @xmath . Then there exist unique elements @xmath such
that @xmath . Furthermore, if @xmath is a homomorphism of linear
algebraic groups, then @xmath and @xmath .

The elements @xmath and @xmath are called the semisimple part and the
unipotent part of @xmath respectively.

### 2.2 Symplectic and Orthogonal Groups

In this section, we follow Larry C. Grove [ Gv ] , and define two
important classes of groups, which preserve certain bilinear form. Let
@xmath be a field of @xmath . Let @xmath be an @xmath -dimensional
vector space over @xmath .

###### Definition 2.2.1.

A bilinear form on @xmath is a function @xmath satisfying

1.  @xmath

2.  @xmath

3.  @xmath

for all @xmath and all @xmath .

If @xmath is a bilinear form on @xmath and @xmath is a basis for @xmath
, set @xmath for all @xmath . Then @xmath is called the matrix of @xmath
relative to @xmath . If @xmath , write @xmath , and @xmath , so that
@xmath and @xmath are represented by column vectors @xmath and @xmath .
Then @xmath for all @xmath , where @xmath are the column vectors with
the entries being the components of @xmath with respect to the given
basis @xmath of @xmath . If @xmath is another basis for @xmath , write
@xmath , where @xmath , for all @xmath . Then @xmath , which is the
@xmath -entry of @xmath , where @xmath , is the change of basis matrix.
We say two @xmath matrices @xmath are congruent if @xmath , for some
@xmath . So @xmath . Define @xmath . Then @xmath is a subgroup of @xmath
.

###### Notation 2.2.2.

A vector space @xmath having a bilinear form @xmath will be denoted by
@xmath .

###### Definition 2.2.3.

Define the discriminant of @xmath to be

  -- -------- --
     @xmath   
  -- -------- --

Observe that the discriminant @xmath , is independent of the choice of
basis.

###### Definition 2.2.4.

The bilinear form @xmath is said to be non-degenerate if @xmath .

###### Definition 2.2.5.

A subspace @xmath of @xmath is said to be non-degenerate if @xmath ,
where @xmath .

Unless otherwise specified, we assume from now on that @xmath is a
non-degenerate bilinear form.

###### Definition 2.2.6.

Two bilinear forms @xmath and @xmath are said to be equivalent , denoted
by @xmath , if there exists a vector space isomorphism @xmath such that
@xmath for all @xmath .

###### Remark 2.2.7.

We call the above @xmath an isometry with respect to @xmath and @xmath .

#### 2.2.1 Symplectic groups

###### Definition 2.2.8.

A bilinear form @xmath is said to be skew-symmetric or alternating if
@xmath for all @xmath .
Alternatively, this definition is equivalent to @xmath for all @xmath .
In matrix terminology, the bilinear form @xmath is skew-symmetric if and
only if any representing matrix @xmath is skew-symmetric, i.e., @xmath .

For the remainder of this section @xmath will denote a non-degenerate
alternating bilinear form.

###### Definition 2.2.9.

A pair @xmath of vectors is said to be a hyperbolic pair if @xmath and
@xmath .

The restriction of @xmath to the subspace generated by @xmath has
representing matrix @xmath relative to @xmath .

###### Proposition 2.2.10 (Theorem 2.10 [Gv]).

If @xmath is a non-degenerate alternating bilinear form on @xmath , then
there exists a basis @xmath of @xmath relative to which the representing
matrix has the following form @xmath , where @xmath is a hyperbolic pair
for all @xmath .

###### Definition 2.2.11 (Symplectic group).

The symplectic group is denoted by @xmath .

In matrix terminology, the symplectic group is defined as:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath .

###### Definition 2.2.12 (Symplectic similitude group).

The symplectic similitude group with respect to the matrix @xmath as in
Definition 2.2.11 , is defined by @xmath , where @xmath , is a group
homomorphism with @xmath ; @xmath is called a similitude character .

#### 2.2.2 Orthogonal groups

###### Definition 2.2.13.

A bilinear form @xmath is said to be symmetric if @xmath for all @xmath
. In matrix terminology, the bilinear form @xmath is symmetric if and
only if any representing matrix @xmath is symmetric, i.e., @xmath .

###### Definition 2.2.14.

If @xmath is a symmetric bilinear form on @xmath , then @xmath defined
by @xmath , is called a quadratic form associated to @xmath .

Thus @xmath for all @xmath . So the bilinear form @xmath is completely
determined by the quadratic form @xmath and vice-versa.

For the remainder of this section @xmath will denote a non-degenerate
symmetric bilinear form.

###### Definition 2.2.15 (Orthogonal group).

The orthogonal group is defined by

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

In matrix terminology, the orthogonal group is defined as:

  -- -------- --
     @xmath   
  -- -------- --

###### Remark 2.2.16.

Equivalent forms give conjugate groups in @xmath , i.e., if @xmath for
some @xmath then @xmath .

###### Definition 2.2.17.

A vector @xmath is called isotropic if @xmath , and anisotropic if
@xmath . A vector space @xmath is called isotropic if @xmath for some
@xmath and @xmath is called totally isotropic if @xmath for all @xmath .

###### Definition 2.2.18.

The dimension of a maximal totally isotropic subspace of a quadratic
space is called the Witt index .

Let @xmath be any non-zero anisotropic vector, and define a linear
transformation @xmath via

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath . Then @xmath . We call @xmath is the reflection in the
hyperplane orthogonal to @xmath . The following theorem is well-known,
that the orthogonal group is generated by reflections. We have (see
Theorem 6.6 [ Gv ] ):

###### Theorem 2.2.19 (E. Cartan-Dieudonné).

If @xmath is an @xmath -dimensional vector space, equipped with a
non-degenerate symmetric bilinear form @xmath , then every element of
@xmath is a product of at most @xmath reflections.

###### Definition 2.2.20 (Orthogonal similitude group).

The orthogonal similitude group with respect to an invertible symmetric
matrix @xmath is defined by @xmath , where @xmath , is a group
homomorphism with @xmath ; @xmath is called a similitude character .

###### Lemma 2.2.21.

Let @xmath have the property FE, then @xmath is finite.

###### Proof.

If possible suppose that @xmath is infinite, then there are infinitely
many @xmath not in @xmath . So @xmath as field. Hence there are
infinitely many field extentions of degree @xmath , which contradicts
the fact that @xmath has the property FE. ∎

###### Example 2.2.22.

1.   Let @xmath be an @xmath -dimensional vector space over @xmath
    equipped with a non-degenerate symmetric bilinear form @xmath . It
    is known that any two non-degenerate symmetric bilinear forms on
    @xmath are equivalent, i.e., there is a basis for @xmath relative to
    which @xmath . So the corresponding orthogonal group is denoted by

      -- -------- --
         @xmath   
      -- -------- --

2.   Let @xmath be an @xmath -dimensional vector space over @xmath
    equipped with a non-degenerate symmetric bilinear form @xmath . In
    this situation, non-degenerate symmetric bilinear forms are
    classified by their signature, i.e., there is a basis for @xmath
    relative to which @xmath . So the corresponding orthogonal groups
    are denoted by

      -- -------- --
         @xmath   
      -- -------- --

    where @xmath .

3.   Let @xmath be an @xmath -dimensional vector space over @xmath
    equipped with a non-degenerate symmetric bilinear form @xmath . Then
    there is a basis for @xmath relative to which @xmath , where @xmath
    . Thus, up to equivalence, there are two such forms corresponding to
    a square and non-square elements of @xmath . So the corresponding
    orthogonal groups are denoted by

      -- -------- --
         @xmath   
      -- -------- --

4.   Let @xmath be an @xmath -dimensional vector space over @xmath . Up
    to equivalence, there is a unique non-degenerate symmetric bilinear
    form @xmath of maximal Witt index over @xmath . This is called the
    split form . More explicitly we can fix a basis @xmath for even
    dimension, and @xmath for odd dimension, so that the matrix of
    @xmath is as follows:

      -- -------- --
         @xmath   
      -- -------- --

    The orthogonal group corresponding to this form is called a split
    orthogonal group . In this thesis, we will work with only the split
    orthogonal groups, and this group will be denoted by @xmath .

#### 2.2.3 Spinor norm

For @xmath with @xmath , we defined the reflection @xmath by @xmath
along @xmath , which is an element of the orthogonal group. We know from
Theorem 2.2.19 that every element of the orthogonal group @xmath can be
written as a product of at most @xmath reflections. Let @xmath then
@xmath ( @xmath ), where @xmath for all @xmath . We are now in a
position to define the spinor norm. To show that this is well-defined
(see p. 75, Proposition 9.1 in [ Gv ] ) map, we need Clifford algebra
theory.

###### Definition 2.2.23.

The spinor norm is a group homomorphism @xmath defined by @xmath , where
@xmath .

Thus for a reflection, we have @xmath . However, for computational
purposes, this definition is difficult to use. In Chapter 4 , we will
define the spinor norm using Wall’s theory, and we will give an
efficient algorithm in Chapter 7 to compute the spinor norm.

### 2.3 Unitary Groups

For the material covered here, we refer to the books [ Kn ] and [ Gv ] .
Let @xmath be a commutative ring with @xmath . An involution on @xmath
is an automorphism @xmath of @xmath of order @xmath . Thus:

@xmath ,

for all @xmath . Set @xmath . Let @xmath be a free @xmath -module of
rank @xmath . In this section, we discuss the unitary groups which are
also one of the classical groups. The General Linear Group @xmath is a
group of all @xmath -linear isomorphism of the module @xmath over @xmath
. In matrix terminology it consists of all @xmath invertible matrices
and denoted as @xmath .

###### Definition 2.3.1.

A sesquilinear form on @xmath , with respect to @xmath , is a function
@xmath satisfying

1.  @xmath

2.  @xmath

3.  @xmath

for all @xmath and all @xmath .

If @xmath is a sesquilinear form on @xmath and @xmath is a free basis
for @xmath , set @xmath for all @xmath . Then @xmath is called the
matrix of @xmath relative to @xmath . If @xmath , write @xmath , and
@xmath , so that @xmath and @xmath are represented by column vectors
@xmath and @xmath . Then @xmath for all @xmath , where @xmath are the
column vectors with the entries being the components of @xmath with
respect to the given basis @xmath of @xmath . If @xmath is another free
basis for @xmath , write @xmath , where @xmath , for all @xmath . Then
@xmath , which is the @xmath -entry of @xmath , where @xmath . We say
two @xmath matrices @xmath are congruent if @xmath , for some @xmath .
So @xmath . Define @xmath is a subgroup of @xmath .

###### Notation 2.3.2.

A free module @xmath having a sesquilinear form @xmath will be denoted
by @xmath .

###### Definition 2.3.3.

Define the discriminant of @xmath to be

  -- -- --
        
  -- -- --

Note that @xmath is independent of the choice of basis. The sesquilinear
form @xmath is said to be non-degenerate if @xmath .

Another way to look at the sesquilinear form is the following. Denote
the dual of @xmath by @xmath . The form @xmath induces a map @xmath for
all @xmath , which is @xmath -linear. Conversely, an @xmath -linear
homomorphism @xmath defines a sesquilinear form @xmath for all @xmath .
We call @xmath the adjoint of @xmath . Since @xmath and @xmath , a
sesquilinear form is determined by its adjoint and vice-versa. If @xmath
is an @xmath -module isomorphism between @xmath and @xmath then @xmath
is non-degenerate. The above two definitions for non-degeneracy are
equivalent. Let @xmath and @xmath be two sesquilinear forms on @xmath
and @xmath respectively. Two forms are said to be equivalent , denoted
by @xmath , if there exists a @xmath -module isomorphism @xmath such
that @xmath for all @xmath . We call @xmath an isometry with respect to
@xmath and @xmath .

###### Definition 2.3.4.

A sesquilinear form @xmath is said to be hermitian if @xmath for all
@xmath . In matrix terminology, the sesquilinear form @xmath is
hermitian if and only if any representing matrix @xmath is hermitian,
i.e., @xmath .

###### Definition 2.3.5.

A sesquilinear form @xmath is said to be skew-hermitian if @xmath for
all @xmath . In matrix terminology, the sesquilinear form @xmath is
skew-hermitian if and only if any representing matrix @xmath is
skew-hermitian, i.e., @xmath .

###### Remark 2.3.6.

If @xmath is a skew-hermitian form, then @xmath is a hermitian form for
some @xmath with @xmath . So the corresponding isometry group will be
same whether we consider hermitian or skew-hermitian form.

For the remainder of this section @xmath will denote a non-degenerate
hermitian form.

###### Definition 2.3.7 (Unitary group).

The unitary group is defined as follows: @xmath .

In matrix terminology, the unitary group is defined as:

  -- -------- --
     @xmath   
  -- -------- --

Most of the time we will consider unitary groups over fields.

###### Definition 2.3.8 (Unitary similitude group).

The unitary similitude group with respect to an invertible hermitian
matrix @xmath is defined by @xmath , where @xmath , is a group
homomorphism with @xmath ; @xmath is called a similitude character .

###### Example 2.3.9.

1.   Let @xmath be an @xmath -dimensional vector space over @xmath with
    @xmath . In this situation, hermitian forms are classified by
    signature and given by @xmath . So the corresponding unitary groups
    are denoted by

      -- -------- --
         @xmath   
      -- -------- --

    where @xmath , where @xmath is the usual complex conjugate and
    @xmath .

2.   Let @xmath be an @xmath -dimensional vector space over a finite
    field @xmath with @xmath . It is known that any two hermitian forms
    on @xmath are equivalent and thus we may choose @xmath . So the
    corresponding unitary group is, unique up to conjugation, and is
    denoted by

      -- -------- --
         @xmath   
      -- -------- --

    where @xmath .

3.   Let @xmath be a free module over @xmath of rank @xmath with @xmath
    . Then @xmath . Then the unitary group defined over @xmath is

      -- -------- --
         @xmath   
      -- -------- --

    where @xmath and @xmath , and @xmath . In particular, if @xmath then
    @xmath .

4.   Let @xmath be an @xmath -dimensional vector space over @xmath with
    an involution @xmath . Up to equivalence, there is a unique
    non-degenerate hermitian form @xmath of maximal Witt index over
    @xmath . This is called the split form . More explicitly we can fix
    a basis @xmath for even dimension, and @xmath for odd dimension, so
    that the matrix of @xmath is as follows:

      -- -------- --
         @xmath   
      -- -------- --

    The unitary group corresponding to this form is called a split
    unitary group . In Section 6.4 of Chapter 6 we will work with only
    the split unitary groups, and this group will be denoted by @xmath ,
    where @xmath is the fixed field.

## Chapter 3 Chevalley Groups

This is another basic chapter of this thesis. In the present chapter, we
will take another approach to define the split classical groups. For the
Gaussian elimination, which we will develop in Chapter 6 , we need an
analog of elementary matrices. These matrices are described in Section
3.2, which come from the theory of Chevalley groups (of adjoint type).
In this theory, one begins with a complex simple Lie algebra @xmath , a
field @xmath , and get a group @xmath (see Section 3.1). The theory was
developed by Chevalley [ Ch ] himself, and further generalized by Robert
Steinberg [ St1 ] . In our computations, we often imitate the notation
from Carter [ Ca1 ] .

### 3.1 Construction of Chevalley Groups (adjoint type)

Let @xmath be a complex simple Lie algebra. Since any two Cartan
subalgebras of @xmath are conjugate, we fix a Cartan subalgebra @xmath .
Then there is the adjoint representation of @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

given by @xmath . Since @xmath is Abelian, @xmath is a commuting family
of semisimple linear transformations of @xmath . Hence @xmath is
simultaneously diagonalizable. Thus we have (see p.35 [ Ca1 ] ):

###### Theorem 3.1.1 (Cartan decomposition).

With this notation, we have,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath are root spaces and @xmath is a root system with respect to
@xmath .

We call this decomposition the Cartan decomposition of @xmath with
respect to @xmath . The classification of finite dimensional complex
simple Lie algebras gives four infinite families @xmath and @xmath
called classical types , and five exceptional types @xmath and @xmath .
Chevalley proved that, there exists a basis of @xmath such that all the
structure constants, which define @xmath as a Lie algebra, are integers.
The following (Theorem 4.2.1 [ Ca1 ] ) is a key theorem to define
Chevalley groups.

###### Theorem 3.1.2 (Chevalley basis theorem).

Let @xmath be a simple Lie algebra over @xmath , @xmath be a Cartan
subalgebra, and

  -- -------- --
     @xmath   
  -- -------- --

be a Cartan decomposition of @xmath . Let @xmath be the co-root
corresponding to the root @xmath . Then, for each root @xmath , an
element @xmath can be chosen in @xmath such that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where @xmath is the greatest integer for which @xmath .

The elements @xmath form a basis for @xmath , called a Chevalley basis .
The basis elements multiply together as follows:

  -- -------- -- ---------
     @xmath      (3.1.1)
  -- -------- -- ---------

where @xmath are Cartan integers and @xmath , a simple root system fixed
for @xmath .

The structure constants of the algebra with respect to a Chevalley basis
are all integers.

The map @xmath is a nilpotent linear map on @xmath . Let @xmath , then
@xmath is also nilpotent. Thus @xmath is an automorphism of @xmath . We
denote by @xmath the subset of @xmath of all @xmath -linear combinations
of the Chevalley basis elements of @xmath . By Equation ( 3.1.1 ), a Lie
bracket can be defined for @xmath . Thus @xmath is a Lie algebra over
@xmath . Now let @xmath be any field. We define @xmath . Then @xmath is
a Lie algebra over @xmath via the Lie multiplication

  -- -------- --
     @xmath   
  -- -------- --

where @xmath are Chevalley basis elements of @xmath , and @xmath denote
the identity element of @xmath .

Now everything makes sense over an arbitrary field @xmath . So we are in
a position to define the Chevalley groups of adjoint type. The Chevalley
group of type @xmath over the field @xmath , denoted by @xmath , is
defined to be the subgroup of automorphisms of the Lie algebra @xmath
generated by @xmath for all @xmath . In fact, the group @xmath over
@xmath is determined up to isomorphism by the simple Lie algebra @xmath
over @xmath and the field @xmath .

Observe that (see Lemma 4.5.1, p.65 [ Ca1 ] ), when @xmath is a linear
Lie algebra,

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath , for all @xmath , and for all @xmath . We shall abuse
the notation slightly and denote the matrix of the linear map by @xmath
itself. Define @xmath . We call the @xmath , elementary matrix . Let
@xmath be the group of matrices generated by the elements @xmath for all
@xmath and all @xmath . Thus there is a homomorphism from @xmath onto
@xmath such that

  -- -------- --
     @xmath   
  -- -------- --

whose kernel is the center of @xmath . Hence @xmath . We work with
@xmath instead of the Chevalley group @xmath . In (see Section 11.2 [
Ca1 ] ), the classical Lie algebras and their Chevalley basis are
described explicitly. Usually, row-column operations are defined by pre
and post multiplication by certain elementary matrices. We are going to
define the elementary matrices for symplectic and orthogonal groups, and
more generally, for symplectic and orthogonal similitude groups.

###### Example 3.1.3 (Cartan decomposition and Chevalley basis of
@xmath).

Let us consider the Lie algebra of type @xmath :

  -- -------- --
     @xmath   
  -- -------- --

where @xmath . We can write elements of @xmath in block form. Let @xmath
, where @xmath are @xmath matrices. We use the condition that @xmath
satisfies @xmath , then we get @xmath and @xmath . The set of diagonal
matrices in @xmath is a Cartan subalgebra @xmath of @xmath . The
elements of @xmath have form @xmath . We index the rows and columns by
@xmath and @xmath . The elements @xmath , form a basis of @xmath . Then
by Theorem 3.1.1 , we have,

  -- -------- --
     @xmath   
  -- -------- --

where

  -- -------- -- ---------
     @xmath      (3.1.2)
  -- -------- -- ---------

The above decomposition is the Cartan decomposition of the Lie algebra
@xmath , and a Chevalley basis for this Lie algebra is

  -- -------- --
     @xmath   
  -- -------- --

where @xmath as in Equation ( 3.1.2 ). Observe that the above mentioned
Chevalley basis is not unique, in fact, any integral multiple of it is
again a Chevalley basis. Now @xmath ’s are nilpotent endomorphisms of
@xmath with @xmath . So @xmath , elementary matrix, is an automorphism
of @xmath . Similarly, we can do this, for orthogonal Lie algebras. For
details see [ Ca1 ] . In next section, we define these matrices
explicitly.

### 3.2 Elementary Matrices

First of all, let us describe the elementary matrices for symplectic and
split orthogonal similitude groups. The genesis of these elementary
matrices lies in the Chevalley basis theorem (Theorem 3.1.2 ). In what
follows, the scalar @xmath varies over the field @xmath , @xmath or
@xmath , and @xmath . We define @xmath as the @xmath matrix with @xmath
in the @xmath position, and zero everywhere else. We simply use @xmath
to denote @xmath . We often use the well-known matrix identity @xmath ,
where @xmath is the Kronecker delta. For more details on elementary
matrices see [ Ca1 ] .

###### Example 3.2.1.

Elementary matrices (or elementary transvections) in @xmath are @xmath ,
where @xmath .

#### 3.2.1 Elementary matrices for @xmath

We index rows and columns by @xmath . The elementary matrices are as
follows:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath            
     @xmath   @xmath            
  -- -------- -------- -------- --

and in matrix format they look as follows:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

#### 3.2.2 Elementary matrices for @xmath

We index rows and columns by @xmath . The elementary matrices are as
follows:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

and in matrix format they look as follows:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

#### 3.2.3 Elementary matrices for @xmath

We index rows and columns by @xmath . The elementary matrices are as
follows:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

and in matrix format they look as follows:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Here @xmath is the row vector with @xmath at @xmath place and zero
elsewhere.

In [ Re ] , Ree proved that the above defined elementary matrices
generate the symplectic group @xmath and the commutator subgroups of the
orthogonal groups @xmath and @xmath respectively. We will give an
algorithmic proof of this fact via our Gaussian elimination algorithm
(see Theorem 6.3.11 ).

## Chapter 4 Conjugacy Classes of an Isometry

G. E. Wall [ Wa2 ] , and Springer-Steinberg [ SS ] classified isometries
with respect to a symmetric, skew-symmetric and hermitian forms up to
conjugacy. They associated certain forms to an isometry. In this
chapter, we define those forms associated with an element in the
isometry group. In Section 4.1 we describe the Wall’s form, which is
associated with an orthogonal element, which will be used in Chapter 7
to compute the spinor norm. In Section 4.2, we describe the other form
associated to an element of the unitary group, which will be used in
Chapter 8 to prove the finiteness of @xmath -classes in unitary group.
The material in this chapter is based on the work of Wall and
Springer-Steinberg, and is presented here for the sake of completeness.

### 4.1 Wall’s Form

In [ Wa2 ] , Wall classified conjugacy classes in classical groups by
associating a bilinear form and thus reducing the problem of conjugacy
to the equivalence of bilinear forms. Let @xmath and define the residual
space of @xmath by @xmath , where @xmath denotes the identity linear map
on @xmath . Observe that @xmath is @xmath -stable.

###### Definition 4.1.1.

An element @xmath is said to be regular if the residual space @xmath is
non-degenerate.

###### Example 4.1.2.

The reflection @xmath is an example of a regular element, as @xmath ,
which is non-degenerate.

If @xmath then we have,

  -- -------- -- ---------
     @xmath      (4.1.1)
  -- -------- -- ---------

for all @xmath . This defines a map

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath , where @xmath for some @xmath . Thus to @xmath we
associate @xmath , called Wall’s form . We have (see p.6 [ Wa2 ] ):

###### Proposition 4.1.3.

The map @xmath is a well-defined non-degenerate bilinear form on @xmath
, and @xmath is an isometry on @xmath with respect to @xmath .
Furthermore, we have

1.  @xmath ,

2.  @xmath

for all @xmath .

###### Proof.

Let @xmath and @xmath be in @xmath for some @xmath . Then we have

  -- -------- -------- -------- --
     @xmath   @xmath            
              @xmath   @xmath   
              @xmath            
              @xmath   @xmath   
  -- -------- -------- -------- --

So the map @xmath is well-defined. Let @xmath , and if @xmath for all
@xmath , then @xmath for all @xmath , which implies @xmath , as @xmath
is nondegenerate. Hence @xmath is nondegenerate. It follows immediately
that @xmath is a bilinear form on @xmath , as @xmath is so. Now @xmath .
Hence @xmath is an isometry on @xmath with respect to the new form
@xmath . Furthermore, let @xmath then @xmath and @xmath for some @xmath
. We have

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Hence @xmath , which proves (1). Now we have

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Therefore @xmath , proving (2). Hence the Proposition. ∎

We have seen that the Wall’s form @xmath is always non-degenerate, but
need not be symmetric. Here we give a criterion for the Wall’s form
@xmath to be symmetric. We have (see p.116 [ Ha ] ):

###### Proposition 4.1.4.

The Wall’s form @xmath is symmetric if and only if @xmath .

###### Proof.

Suppose @xmath is symmetric, then @xmath for all @xmath . Then by part 2
of the Proposition 4.1.3 , we have @xmath . So @xmath for all @xmath ,
which implies that @xmath for all @xmath , where @xmath . Hence @xmath .

Conversely, suppose that @xmath , then we have

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath            
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath            
              @xmath            
  -- -------- -------- -------- --

Therefore the form @xmath is symmetric. ∎

Wall developed this to classify the conjugacy class of @xmath . We have
(see Theorem 1.3.1 [ Wa2 ] ):

###### Proposition 4.1.5.

Let @xmath . Then @xmath is conjugate to @xmath in @xmath if and only if
@xmath .

Now the residual space @xmath is equipped with two bilinear forms:

1.  The Wall’s form @xmath , for this we use the notation @xmath .

2.  Restriction of the usual form @xmath on @xmath is, denoted by @xmath
    .

#### 4.1.1 Spinor norm using Wall’s theory

We will now define the spinor norm using Wall’s theory, which will be
useful for our purpose.

###### Definition 4.1.6.

The spinor norm is a group homomorphism @xmath defined by @xmath , where
@xmath is defined as above.

Let @xmath be a reflection in @xmath . Then the residual space is @xmath
, therefore @xmath . Hence @xmath , which is same as the spinor norm
computed in Section 2.2.3 . The following Proposition and its Corollary
are due to A. J. Hahn [ Ha ] . We include the proof for the sake of
completeness.

###### Proposition 4.1.7.

Let @xmath be regular with residual space @xmath . Then @xmath .

###### Proof.

If @xmath , then @xmath , since @xmath , for any @xmath and @xmath .
Hence, in this case the result follows immediately. Suppose now @xmath .
Since @xmath is regular, @xmath , and @xmath . So @xmath . Hence @xmath
. Therefore @xmath for all @xmath . Fix any basis for @xmath , say
@xmath . Let @xmath be the matrices corresponding to the forms @xmath
and @xmath respectively, and let @xmath be the matrix corresponding to
the linear transformation @xmath with respect to the above mentioned
basis. Then using @xmath , we get @xmath . Then @xmath . Therefore
@xmath . Hence @xmath . ∎

###### Corollary 4.1.8.

Let @xmath be unipotent. Then @xmath .

###### Proof.

The fixed space of @xmath is @xmath , since @xmath is not an eigenvalue
of @xmath . Hence, its residual space @xmath , which is non-degenerate.
Hence @xmath is regular. As @xmath is unipotent, there is a basis for
@xmath such that the matrix of @xmath is upper triangular with diagonal
entries equal to @xmath . Therefore by Proposition 4.1.7 we have, @xmath
. Again @xmath has no fixed points (except 0), hence @xmath is regular
with residual space @xmath . Therefore @xmath . Hence @xmath . ∎

### 4.2 Springer-Steinberg Form

Let us fix some notation and terminology. Let @xmath be a perfect field
of @xmath with an involution @xmath such that the fixed field of @xmath
is @xmath . Let @xmath be a vector space over @xmath , equipped with a
non-degenerate hermitian form @xmath . Let @xmath with minimal
polynomial @xmath . We define a @xmath -algebra @xmath . Clearly, @xmath
is an @xmath -module, denoted by @xmath . The @xmath -module structure
on @xmath determines @xmath -conjugacy class of @xmath (see [ As ] and [
SS ] for more details). To determine conjugacy classes of @xmath within
@xmath , Springer and Steinberg defined a hermitian form @xmath on
@xmath , called Springer-Steinberg form , denoted by @xmath (see 2.6 in
[ SS ] Chapter IV). Since @xmath is self-U-reciprocal (see 5.1 for
definition), there exists a unique involution @xmath on @xmath such that
@xmath and @xmath is an extension of @xmath on scalars. Thus @xmath is
an algebra with involution. They prove that there exists a @xmath
-linear function @xmath such that the symmetric bilinear form @xmath
given by @xmath is non-degenerate with @xmath for all @xmath . It
follows that there exists a hermitian form @xmath on @xmath -module
@xmath (with respect to @xmath ) satisfies @xmath for all @xmath , and
@xmath (see p. 254, 2.5 [ SS ] ). Let @xmath , then the following
commutative diagrams clarify what we are talking about so far, which
will also be useful in the following proposition (Proposition 4.2.2 ).

[]

###### Definition 4.2.1.

Let @xmath and @xmath be two hermitian spaces over @xmath and @xmath
respectively, where @xmath and @xmath are isomorphic modules over @xmath
and let @xmath be an isomorphism. Then we say @xmath and @xmath are
equivalent , denoted as @xmath , if there exists a @xmath -isomorphism
@xmath such that

1.  @xmath and

2.  @xmath

for all @xmath and all @xmath .

We need the following (see p. 255, 2.7 and 2.8 [ SS ] Chapter IV):

###### Proposition 4.2.2.

With the notation as above, let @xmath and @xmath . Then,

1.   the elements @xmath and @xmath are conjugate in @xmath if and only
    if @xmath and @xmath are equivalent.

2.   The centralizer of @xmath in @xmath is @xmath .

###### Proof.

1.  Suppose @xmath and @xmath are conjugate in @xmath . Then there
    exists a @xmath such that @xmath . Then @xmath is a @xmath
    -isomorphism. Here also @xmath is a @xmath -isomorphism such that
    @xmath . Now for @xmath , @xmath . It then follows that @xmath for
    all @xmath and for all @xmath . Let @xmath , then @xmath . Therefore
    @xmath . Hence @xmath .

    Conversely, suppose that @xmath and @xmath are equivalent. Then
    there exists a @xmath -isomorphism @xmath such that @xmath for all
    @xmath , where @xmath is a @xmath -isomorphism such that @xmath and
    @xmath for all @xmath and @xmath . For @xmath , @xmath , then @xmath
    , i.e., @xmath . Now, look at @xmath for all @xmath , then @xmath is
    an isometry. Hence @xmath and @xmath are conjugate in @xmath .

2.  Enough to show an isometry @xmath is in @xmath if and only if @xmath
    preserves @xmath . Let @xmath such that @xmath . Then we get @xmath
    for all @xmath (here we replace @xmath by @xmath and @xmath by
    identity in part (1)). Conversely, suppose @xmath preserves @xmath ,
    then @xmath . So @xmath is an isometry. Also as @xmath for all
    @xmath , then @xmath . Therefore @xmath . Hence @xmath .

∎

We can decompose @xmath , where @xmath are indecomposable subalgebras of
@xmath with respect to @xmath , i.e., @xmath are not direct sums of
non-trivial @xmath -stable subalgebras (see section 2.2 Chapter IV of [
SS ] ). The restriction of @xmath to @xmath is an involution on @xmath
denoted by @xmath . Clearly, @xmath ’s, are of one of the following
forms according to the decomposition of @xmath (see Equation ( 5.1.1 )):

-   @xmath , where @xmath is an irreducible self-U-reciprocal
    polynomial.

-   @xmath , where @xmath is irreducible and not self-U-reciprocal.

In the second case, the two components @xmath and @xmath are isomorphic
local rings (For @xmath , let @xmath be a root of @xmath then @xmath is
a root of @xmath . Therefore @xmath . Let @xmath be an isomorphism
sending @xmath . Now for @xmath , define @xmath via @xmath , which is a
ring homomorphism. Similarly, we can define a map other way. Hence the
isomorphism). The restriction of @xmath is given by @xmath via the
isomorphism. Using Wall’s approximation theorem (Corollary 4.2.4 ) it’s
easy to see that all hermitian forms over such rings are equivalent.
Thus to determine equivalence of @xmath , we need to look at modules
over rings of the first kind.

#### 4.2.1 Wall’s approximation theorem

We recall a theorem of Wall (see Theorem 2.2.1 [ Wa2 ] ), which will be
useful for further analysis. Also, see Asai (Proposition 2.5 [ As ] )
for more details. Let @xmath be a commutative ring with @xmath , @xmath
be its Jacobson radical, and @xmath be an involution on @xmath . Let
@xmath be a non-degenerate hermitian space of rank @xmath over @xmath .
We define @xmath a module over @xmath . Now @xmath induces a hermitian
form @xmath on @xmath with respect to the involution @xmath of @xmath
induced by @xmath . Then we have (Theorem 2.2.1 [ Wa2 ] ):

###### Theorem 4.2.3 (Wall’s approximation theorem).

With the notation as above,

1.   any non-degenerate hermitian form over @xmath is induced by some
    non-degenerate hermitian form over @xmath .

2.   Let @xmath and @xmath be non-degenerate hermitian spaces over
    @xmath , and correspondingly, @xmath and @xmath be non-degenerate
    hermitian spaces over @xmath . Then @xmath is equivalent to @xmath
    if and only if @xmath is equivalent to @xmath .

For our purpose, we need the following (see also p. 256 [ SS ] ),

###### Corollary 4.2.4.

Let @xmath be a module over @xmath , and @xmath and @xmath be two
non-degenerate hermitian forms on @xmath with respect to the involution
on @xmath given by @xmath . Then @xmath and @xmath are equivalent.

###### Proof.

We use Wall’s approximation theorem (Theorem 4.2.3 ). Here the Jacobson
radical of @xmath is @xmath . Then @xmath , where @xmath is a finite
extension of @xmath (thus separable). Now we have hermitian forms @xmath
defined by @xmath for all @xmath . Thus it is enough to show that @xmath
is equivalent to @xmath on @xmath -module @xmath . The norm map @xmath
is @xmath (for definition of the norm map see p. 3 [ Kn ] ). Clearly,
this norm map is surjective. Thus @xmath is trivial. Hence the hermitian
form is unique up to equivalence in this case (see p. 87, Theorem 10.2 [
Gv ] ). ∎

## Chapter 5 Conjugacy Classes and @xmath-classes

The results in this chapter are part of [ BS ] . To study the @xmath
-classes, it is important to understand the conjugacy classes because
@xmath -classes are union of conjugacy classes. The problem of
classifying conjugacy classes in classical groups has been studied by
many mathematicians, and there is a known substantial amount of results.
See, for example, Asai, Macdonald, Milnor, Springer-Steinberg, Wall,
Williamson [ As , Ma , Mi , SS , Wa2 , Wi ] . When the field is finite,
Wall [ Wa2 ] gave an explicit description of all the conjugacy classes
in the unitary, symplectic and orthogonal groups, and also the order of
centralizers. For some recent accounts in this direction, especially
with the applications in mind, see Thiem-Vinroot [ TV ] , and Burness
and Giudici [ BG ] etc. The conjugacy classes in @xmath are given by the
canonical form theory, and with the unitary group being its subgroup,
one needs to begin there. We begin with recalling the notation involved
in the description of conjugacy classes and @xmath -classes. In Section
5.1 we define certain kinds of polynomials, which will be used in
Section 5.2 to decompose the space with respect to a unitary linear
transformation. This decomposition may be thought of as a reduction
step, which will be used in Chapter 8 to prove one of the main theorems
of this thesis. In Section 5.3 we describe @xmath -classes in orthogonal
and symplectic groups (for more details see [ GK ] ).

### 5.1 Self-@xmath-reciprocal Polynomials

Let @xmath be a field with an involution given by @xmath for all @xmath
. Let @xmath . We extend the involution on @xmath to that of @xmath by
@xmath . Let @xmath be a polynomial with @xmath . The corresponding
@xmath -reciprocal polynomial of @xmath is defined by

  -- -------- --
     @xmath   
  -- -------- --

A monic polynomial @xmath with a non-zero constant term is said to be
self- @xmath -reciprocal if @xmath . In terms of roots, it means that
for a self-U-reciprocal polynomial, whenever @xmath is a root, @xmath is
also a root with the same multiplicity. Note that @xmath , and if @xmath
then @xmath provided @xmath is a monic polynomial. Also, @xmath is
irreducible if and only if @xmath is irreducible. In the case of @xmath
, the polynomial @xmath is self-U-reciprocal if and only if @xmath . A
slightly more general polynomial, called self-dual polynomial will be
defined in Section 5.3. Over a finite field, we have the following
result due to Ennola (Lemma 2 [ En ] ):

###### Proposition 5.1.1.

Let @xmath be a monic, irreducible, self-U-reciprocal polynomial over a
finite field @xmath . Then the degree of @xmath is odd.

###### Proof.

Here the involution of @xmath is given by @xmath . Let @xmath . Let
@xmath be a root of @xmath in its splitting field @xmath over @xmath .
Let @xmath be the Frobenius automorphism of @xmath given by @xmath .
Then @xmath . Since @xmath is self-U-reciprocal, so if @xmath is a root
of @xmath , then @xmath is also a root of @xmath with the same
multiplicity. Therefore there is an automorphism @xmath of @xmath over
@xmath such that @xmath . Then @xmath , so @xmath since @xmath . Now
@xmath , so @xmath for some @xmath . Therefore @xmath , so @xmath .
Hence @xmath is a divisor of @xmath , so @xmath is odd. ∎

###### Lemma 5.1.2.

Let @xmath , and suppose @xmath is the minimal polynomial of @xmath .
Then the minimal polynomial of @xmath is @xmath .

###### Proof.

Since @xmath , and @xmath , then @xmath . Thus we conclude that @xmath
is the minimal polynomial of @xmath . ∎

###### Remark 5.1.3.

If @xmath , then @xmath . So @xmath , which is conjugate to @xmath , as
@xmath is conjugate to its transpose in @xmath . Hence the minimal
polynomials of @xmath and @xmath are same, i.e., @xmath .

If @xmath then its minimal polynomial @xmath is monic with a non zero
constant term, and is self-U-reciprocal. We can write it as follows:

  -- -------- -- ---------
     @xmath      (5.1.1)
  -- -------- -- ---------

where @xmath and @xmath are irreducible, and @xmath is self- @xmath
-reciprocal but @xmath is not self-U-reciprocal for all @xmath .

### 5.2 Space Decomposition with Respect to a Unitary Transformation

Let @xmath , and @xmath satisfying @xmath . Then,

###### Lemma 5.2.1.

For any @xmath , we have @xmath .

###### Proof.

Let @xmath , then @xmath . Observe that @xmath for all @xmath . Now
@xmath for all @xmath . Hence @xmath . ∎

###### Lemma 5.2.2.

The subspaces @xmath and @xmath are mutually orthogonal.

###### Proof.

Let @xmath and @xmath , therefore @xmath and @xmath for some @xmath .
Now @xmath . Hence @xmath . ∎

Let @xmath with minimal polynomial @xmath . Write @xmath as in Equation
( 5.1.1 ), where @xmath or @xmath . Then,

###### Proposition 5.2.3.

The direct sum decomposition @xmath is a decomposition into
non-degenerate mutually orthogonal @xmath -invariant subspaces.

###### Proof.

Let @xmath for some @xmath , then @xmath . Since @xmath are pairwise
relatively prime, then @xmath for some @xmath . So @xmath , therefore
@xmath . Hence the sum @xmath is a direct sum. Clearly, these subspaces
are @xmath -invariant. Observe that @xmath , and @xmath , since @xmath
for all @xmath . By Lemma 5.2.2 , we have @xmath . So we get @xmath for
all @xmath . Hence in the sum @xmath , the subspaces are mutually
orthogonal. Also mutual orthogonality implies that the restriction of
the form on each subspaces are non-degenerate. ∎

This decomposition helps us reduce the questions about conjugacy classes
and @xmath -classes of a unitary transformation to the unitary
transformations with minimal polynomial of one of the following two
kinds:

  Type 1.  

    @xmath , where @xmath is monic irreducible self-U-reciprocal
    polynomial with a non-zero constant term,

  Type 2.  

    @xmath , where @xmath is monic, irreducible, not self-U-reciprocal
    with a non-zero constant term.

Thus Proposition 5.2.3 gives us a primary decomposition of @xmath into
@xmath -invariant @xmath non-degenerate subspaces

  -- -------- -- ---------
     @xmath      (5.2.1)
  -- -------- -- ---------

where @xmath corresponds to the polynomials of Type 1, and @xmath
corresponds to the polynomials of Type 2, where @xmath and @xmath .
Denote the restriction of @xmath to each @xmath by @xmath . Then the
minimal polynomial of @xmath is one of the two types. It turns out that
the centralizer of @xmath in @xmath is

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a hermitian form obtained by restricting @xmath to
@xmath . Thus the conjugacy class and the @xmath -class of @xmath is
determined by the restriction of @xmath to each of the primary
subspaces. Hence it is enough to determine the conjugacy class and the
@xmath -class of @xmath , which has the minimal polynomial of one of the
types in 5.2 .

### 5.3 @xmath-classes in Orthogonal and Symplectic Groups

Let @xmath be an @xmath -dimensional vector space over @xmath with the
property FE, equipped with a non-degenerate symmetric or skew-symmetric
bilinear form @xmath . The @xmath -classes of orthogonal groups @xmath
and symplectic groups @xmath have been discussed by Gongopadhyay and
Ravi S. Kulkarni in [ GK ] (see Theorem 1.0.7 ). We will be very brief
in this section to parametrize the @xmath -classes in orthogonal and
symplectic groups. Let @xmath be a polynomial in @xmath of degree @xmath
such that @xmath and @xmath are not its roots. The corresponding dual
polynomial of @xmath is defined by

  -- -------- --
     @xmath   
  -- -------- --

A monic polynomial @xmath with @xmath are not its roots is said to be
self-dual if @xmath . In terms of roots, it means that for a self-dual
polynomial, whenever @xmath is a root, @xmath is also a root with the
same multiplicity. Suppose @xmath or @xmath with the minimal polynomial
@xmath . Thus an irreducible factor says @xmath , of the minimal
polynomial, can be one of the following three types:

-   @xmath or @xmath .

-   @xmath is self-dual.

-   @xmath is not self-dual. In this case, there is an irreducible
    factor @xmath will occur in the minimal polynomial.

If @xmath or @xmath then its minimal polynomial @xmath is monic with a
non-zero constant term, and is self-dual. We can write it as follows

  -- -------- -- ---------
     @xmath      (5.3.1)
  -- -------- -- ---------

where @xmath and @xmath are irreducible, and @xmath is self-dual but
@xmath is not self-dual for all @xmath . Thus Proposition 5.2.3 gives us
a primary decomposition of @xmath into @xmath -invariant @xmath
non-degenerate subspaces

  -- -------- -- ---------
     @xmath      (5.3.2)
  -- -------- -- ---------

where @xmath , and @xmath corresponds to the self-dual polynomials, and
@xmath corresponds to the not self-dual polynomials, where @xmath and
@xmath . Denote the restriction of @xmath to each @xmath by @xmath so
@xmath . Then the minimal polynomial of @xmath is one of the three
types. It turns out that the centralizer of @xmath in @xmath or @xmath
is

  -- -------- --
     @xmath   
  -- -------- --

Thus the @xmath -class of @xmath is determined by the restriction of
@xmath to each of the primary subspaces. Then it has been proved that
there are only finitely many @xmath -classes of semisimple and unipotent
elements in orthogonal and symplectic groups respectively. Thus using
Jordan decomposition (Theorem 2.1.1 ), there are only finitely many
@xmath -classes in orthogonal groups @xmath and symplectic groups @xmath
. For more details see p. 257 in [ GK ] .

## Chapter 6 Gaussian Elimination

The results in this chapter are part of [ BMS ] . We improved the
results on the symplectic and split orthogonal similitude groups. This
chapter is one of the main chapters of this thesis. For instance, in
Chapter 7 we use Gaussian elimination to compute the spinor norm as well
as similitude characters. In dealing with constructive group recognition
project, one needs to solve the word problem in some generating set.
Thus, the main objective of this chapter is to develop a similar
algorithm for symplectic and split orthogonal similitude groups to solve
the word problem. In Section 6.1 we describe the classical Gaussian
elimination algorithm for general linear groups. In Section 6.2 we
define elementary operations for similitude groups, and describe the
Gaussian elimination in similitude groups in Section 6.3. In Section 6.4
we record a result [ MS ] on the Gaussian elimination in the split
unitary groups.

### 6.1 Gaussian Elimination in General Linear Groups

As we know, in the general linear group @xmath , the word problem has an
efficient solution in elementary transvections (or elementary matrices)
- the Gaussian elimination. One observes that the effect of multiplying
by elementary transvections on a matrix from left or right is either a
row or column operation respectively. We have the following classical
Gaussian elimination algorithm for @xmath :

###### Theorem 6.1.1.

Every element @xmath can be written as a product of elementary matrices
and a diagonal matrix, the diagonal matrix is of the form @xmath if
@xmath ; else @xmath .

Using the above Theorem 6.1.1 one can solve the word problem in @xmath ,
which can be stated as follows:

###### Corollary 6.1.2.

Every element of the special linear group @xmath can be written as a
product of elementary transvections (or elementary matrices).

Let @xmath be a subgroup of upper triangular matrices and @xmath be the
subgroup of permutation matrices in @xmath respectively. In this case
@xmath , symmetric group on @xmath letters. Then we have the following
(see p.108 [ Ca1 ] ):

###### Theorem 6.1.3 (Bruhat decomposition).

With the notation as above,

  -- -------- --
     @xmath   
  -- -------- --

So the above Theorem 6.1.3 says that any element @xmath can be written
as @xmath for some @xmath , and @xmath (which is unique). Therefore
@xmath . Thus, any invertible matrix can be transformed into a
permutation matrix by a series of row and column operations.

### 6.2 Elementary Operations

Elementary operations can be thought of as usual row-column operations
for matrices. We already described the elementary matrices in Section
3.2 for the symplectic and split orthogonal similitude groups. Then
multiplications of those elementary matrices on the left and right to an
element of the similitude groups, for example, symplectic and split
orthogonal similitude groups, are elementary operations, which we are
going to describe below case by case. The Gaussian elimination algorithm
is slightly different for matrices of even and odd size. We first
describe it for matrices of even size and then for matrices of the odd
size.

#### 6.2.1 Elementary operations for @xmath

Let @xmath be a @xmath matrix written in block form of size @xmath .
Then the row and column operations are as follows:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

#### 6.2.2 Elementary operations for @xmath

Let @xmath be a @xmath matrix written in block form of size @xmath .
Then the row and column operations are as follows:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

#### 6.2.3 Elementary operations for @xmath

Let @xmath be a @xmath matrix, where @xmath are @xmath matrices, and
@xmath and @xmath are @xmath matrices, and @xmath and @xmath are @xmath
matrices. Let @xmath . Then the row and column operations are as
follows:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

For @xmath we only write the equations which we need later.

-   Let the matrix @xmath have @xmath .

      -- -------- --
         @xmath   
      -- -------- --

-   Let the matrix @xmath have @xmath .

      -- -------- --
         @xmath   
      -- -------- --

      -- -------- --
         @xmath   
      -- -------- --

    where @xmath

### 6.3 Gaussian Elimination in Symplectic and Orthogonal Similitude
Groups

#### 6.3.1 Some useful lemmas

To justify the steps of the Gaussian elimination algorithm we need
several lemmas. So this subsection is devoted to prove these lemmas.

###### Lemma 6.3.1.

Let @xmath be of size @xmath with the number of @xmath s equal to @xmath
. Let @xmath be a matrix of size @xmath such that @xmath is symmetric
(resp. skew-symmetric) then @xmath is of the form @xmath , where @xmath
is an @xmath symmetric (resp. skew-symmetric), and @xmath (resp. @xmath
). Furthermore, if @xmath then @xmath is symmetric (resp.
skew-symmetric).

###### Proof.

First, observe that the matrix @xmath . Since the matrix @xmath is
symmetric (resp. skew-symmetric), then @xmath is symmetric (resp.
skew-symmetric), and @xmath (resp. @xmath ). Also if @xmath then @xmath
is symmetric (resp. skew-symmetric). ∎

###### Corollary 6.3.2.

Let @xmath be either in @xmath or @xmath .

1.   If @xmath is a diagonal matrix @xmath , then the matrix @xmath is
    of the form @xmath , where @xmath is an @xmath symmetric if @xmath ,
    and @xmath is skew-symmetric with @xmath if @xmath .

2.   If @xmath is a diagonal matrix @xmath , then the matrix @xmath is
    of the form @xmath , where @xmath is an @xmath symmetric matrix if
    @xmath , and is skew-symmetric if @xmath .

###### Proof.

We use the condition that @xmath satisfies @xmath , and @xmath is
symmetric (using @xmath , as @xmath is diagonal), when @xmath , and
@xmath is skew-symmetric, when @xmath . Then Lemma 6.3.1 gives the
required form for @xmath . ∎

###### Corollary 6.3.3.

Let @xmath or @xmath , where @xmath , then the matrix @xmath is of the
form @xmath , where @xmath is a symmetric matrix of size @xmath if
@xmath , and skew-symmetric with @xmath if @xmath .

###### Proof.

We use the condition that @xmath satisfies @xmath and @xmath to get
@xmath is symmetric if @xmath , and skew-symmetric if @xmath . Again
Lemma 6.3.1 gives the required form for @xmath . ∎

###### Lemma 6.3.4.

Let @xmath . Then,

1.  @xmath if and only if @xmath and @xmath , and

2.  @xmath if and only if @xmath and @xmath .

###### Proof.

1.  Let @xmath then @xmath satisfies @xmath . Then this implies @xmath
    and @xmath .

    Conversely, if @xmath satisfies the given condition then clearly
    @xmath .

2.  This follows by similar computation.

∎

###### Lemma 6.3.5.

Let @xmath be of size @xmath , where @xmath and @xmath be a matrix such
that @xmath is symmetric (resp. skew-symmetric). Then @xmath , where
each @xmath is of the form @xmath for some @xmath or of the form @xmath
for some @xmath (resp. each @xmath is of the form @xmath for some @xmath
).

###### Proof.

Since the matrix @xmath is symmetric (resp. skew-symmetric), then the
matrix @xmath is of the form @xmath , where @xmath is symmetric (resp.
skew-symmetric), @xmath (resp. @xmath ) and @xmath is a row of size
@xmath . Clearly, @xmath is a sum of the matrices of the form @xmath . ∎

###### Lemma 6.3.6.

For @xmath ,

1.   The element @xmath is a product of elementary matrices.

2.   The element @xmath is a product of elementary matrices.

3.   The element @xmath is a product of elementary matrices.

###### Proof.

1.  We have @xmath .

2.  We produce these elements inductively. First we get @xmath , and
    @xmath . Set @xmath . Then compute @xmath . So inductively we get
    @xmath is a product of elementary matrices.

3.  We have @xmath .

∎

###### Lemma 6.3.7.

The element @xmath is a product of elementary matrices.

###### Proof.

First we compute

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Then compute

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

which is the required element. ∎

###### Lemma 6.3.8.

Let @xmath . Then,

1.   If @xmath and @xmath , then @xmath is of the form @xmath with
    @xmath skew-symmetric.

2.   If @xmath , and @xmath with its first @xmath entries @xmath , then
    @xmath is of the form @xmath with @xmath skew-symmetric.

###### Proof.

We use the equation @xmath , and get @xmath . In the first case, @xmath
is skew-symmetric (using @xmath and @xmath ). Then Lemma 6.3.1 and
Corollary 6.3.2 give the required form for @xmath . In the second case,
we note that @xmath has top-left and top-right blocks @xmath , and get
the required form for @xmath . ∎

###### Lemma 6.3.9.

Let @xmath , then @xmath , and @xmath .

###### Proof.

We compute @xmath , and get @xmath and @xmath . Hence @xmath , and
@xmath . ∎

###### Lemma 6.3.10.

Let @xmath , with @xmath an invertible diagonal matrix. Then @xmath if
and only if @xmath and @xmath , where @xmath is similitude of @xmath .

###### Proof.

Let @xmath then we have @xmath . So we get @xmath and @xmath .

Conversely, if @xmath satisfies the given condition, then @xmath . ∎

#### 6.3.2 Gaussian elimination for @xmath and @xmath

The algorithm is as follows:
Step @xmath :

1.   Input : A matrix @xmath or @xmath .

2.   Output : The matrix @xmath is one of the following kind:

    1.  The matrix @xmath is a diagonal matrix @xmath with @xmath , and
        @xmath , where @xmath is symmetric, when @xmath , and
        skew-symmetric, when @xmath , and is of size @xmath .
        Furthermore, @xmath , when @xmath , and @xmath , when @xmath .

    2.  The matrix @xmath is a diagonal matrix @xmath , and @xmath ,
        where @xmath is an @xmath symmetric, when @xmath and
        skew-symmetric, when @xmath .

3.   Justification : Observe the effect of ER @xmath and EC @xmath on
    the block @xmath . This amounts to the classical Gaussian
    elimination (see Theorem 6.1.1 ) on a @xmath matrix @xmath . Thus we
    can reduce @xmath to a diagonal matrix, and Corollary 6.3.2 makes
    sure that @xmath has the required form.

Step @xmath :

1.   Input : matrix @xmath .

2.   Output : matrix @xmath .

3.   Justification : Observe the effect of ER @xmath . It changes @xmath
    by @xmath . Using Lemma 6.3.5 we can make the matrix @xmath the zero
    matrix in the first case, and @xmath the zero matrix in the second
    case. Further, in the second case, we make use of Lemma 6.3.6 to
    interchange the rows, so that we get a zero matrix in place of
    @xmath . If required, use ER @xmath and EC @xmath to make @xmath a
    diagonal matrix. Lemma 6.3.4 ensures that @xmath becomes @xmath .

Step @xmath :

1.   Input : matrix @xmath .

2.   Output : matrix @xmath .

3.   Justification : Using Corollary 6.3.3 we see that the matrix @xmath
    has a certain form. We can use ER @xmath to make the matrix @xmath a
    zero matrix because of Lemma 6.3.5 .

The algorithm terminates here for @xmath . However for @xmath there is
one more step.
Step @xmath :

1.   Input : matrix @xmath .

2.   Output : matrix @xmath , where @xmath .

3.   Justification : Using Lemma 6.3.7 .

#### 6.3.3 Gaussian elimination for @xmath

The algorithm is as follows:
Step @xmath :

1.   Input : A matrix @xmath .

2.   Output : The matrix @xmath is one of the following kind:

    1.  The matrix @xmath is a diagonal matrix @xmath with @xmath .

    2.  The matrix @xmath is a diagonal matrix @xmath .

3.   Justification : Using ER @xmath and EC @xmath we do the classical
    Gaussian elimination (see Theorem 6.1.1 ) on a @xmath matrix @xmath
    .

Step @xmath :

1.   Input : matrix @xmath .

2.   Output : matrix @xmath is one of the following kind:

    1.  The matrix @xmath is @xmath with @xmath , and @xmath , where
        @xmath is skew-symmetric of size @xmath .

    2.  The matrix @xmath is @xmath ; @xmath have first @xmath entries
        @xmath , and @xmath , where @xmath is an @xmath skew-symmetric
        matrix.

3.   Justification : Once we have @xmath in diagonal form, we use ER
    @xmath and EC @xmath to change @xmath and @xmath to the required
    form. Then Lemma 6.3.8 makes sure that @xmath has the required form.

Step @xmath :

1.   Input : matrix @xmath .

2.   Output :

    1.  matrix @xmath .

    2.  matrix @xmath ; @xmath have first @xmath entries @xmath , and
        @xmath .

3.   Justification : Observe the effect of ER @xmath , and Lemma 6.3.5
    ensures the required form.

Step @xmath :

1.   Input : matrix @xmath

2.   Output : matrix @xmath with @xmath , and @xmath .

3.   Justification : In the first case, Lemma 6.3.10 ensures the
    required form. In the second case, we interchange @xmath with @xmath
    for @xmath . This will make @xmath . Then, if needed, we use ER
    @xmath and EC @xmath on @xmath to make it diagonal. Then Lemma 6.3.9
    ensures that @xmath has full rank. Further, we can use ER @xmath and
    EC @xmath to make @xmath . Lemma 6.3.10 gives the required form.

Step @xmath :

1.   Input : matrix @xmath .

2.   Output : matrix @xmath with @xmath .

3.   Justification : Lemma 6.3.10 ensures that @xmath is of a certain
    kind. We can use ER @xmath to make @xmath .

Thus the main result of this chapter is the following theorem:

###### Theorem 6.3.11.

Every element of symplectic similitude group @xmath or split orthogonal
similitude group @xmath (here @xmath or @xmath ), can be written as a
product of elementary matrices and a diagonal matrix. Furthermore, the
diagonal matrix is of the following form:

1.   In @xmath , @xmath , where @xmath .

2.   In @xmath , @xmath , where @xmath .

3.   In @xmath , @xmath , where @xmath and @xmath .

###### Proof.

This follows from the above algorithms 6.3.2 and 6.3.3 . ∎

This gives us following:

###### Corollary 6.3.12.

Every element @xmath (here @xmath or @xmath ) can be written as a
product of elementary matrices and a diagonal matrix. Furthermore, the
diagonal matrix is @xmath .

###### Proof.

As @xmath so @xmath . In the odd dimensional orthogonal group, @xmath .
In this situation, if needed we use Lemma 6.3.6 to make the first
diagonal entry @xmath . Hence this follows from Theorem 6.3.11 . ∎

###### Corollary 6.3.13.

Every element of the symplectic group @xmath can be written as a product
of elementary matrices.

###### Proof.

This follows from Theorem 6.3.11 , as @xmath . ∎

###### Remark 6.3.14.

Corollary 6.3.12 and Corollary 6.3.13 solve the word problem in
orthogonal groups @xmath and symplectic groups @xmath .

### 6.4 Gaussian Elimination in Unitary Groups

A similar algorithm has been developed in [ MS ] . One can define
elementary matrices and elementary operations for split unitary groups,
similar to that of symplectic and split orthogonal groups. Using those
elementary matrices and elementary operations, Mahalanobis and Singh
solved the word problem in split unitary groups. They proved (Theorem A
[ MS ] ):

###### Theorem 6.4.1.

Every element of the split unitary group @xmath (here @xmath or @xmath )
can be written as a product of elementary matrices and a diagonal
matrix. Furthermore, the diagonal matrix is of the following form:

1.   In @xmath , @xmath , where @xmath .

2.   In @xmath , @xmath , where @xmath with @xmath .

## Chapter 7 Computing Spinor Norm and Similitude

This chapter reports the work done in [ BMS ] . In this chapter, we show
how we can use Gaussian elimination developed in Chapter 6 to compute
the spinor norm for split orthogonal groups. Also in this chapter, we
compute similitude character for split groups using the Gaussian
elimination algorithm. In this chapter, we make use of Wall’s theory
developed in Chapter 4 .

### 7.1

To compute the spinor norm, we will use the following lemma.

###### Lemma 7.1.1.

With the notation as earlier for the group @xmath (here @xmath or @xmath
), we have,

1.  @xmath .

2.  @xmath .

3.  @xmath .

###### Proof.

1.  This follows from Corollary 4.1.8 , since the given elements are all
    unipotent.

2.  Observe that @xmath is a reflection along @xmath , and @xmath ,
    hence @xmath .

3.  First observe that @xmath . Since @xmath is a hyperbolic pair (see
    in Section 2.2) then @xmath , and @xmath . Hence

      -- -------- --
         @xmath   
      -- -------- --

∎

The main result is the following:

###### Theorem 7.1.2 (Spinor norm).

Let @xmath (here @xmath or @xmath ). Suppose Gaussian elimination
reduces @xmath to @xmath , where @xmath . Then the spinor norm @xmath .

###### Proof.

Let @xmath . We write @xmath as a product of elementary matrices and a
diagonal matrix of the form @xmath , following Corollary 6.3.12 . Again
from Lemma 7.1.1 , we get the spinor norm for the elementary matrices
and the diagonal matrix. Hence @xmath . ∎

###### Remark 7.1.3.

The Gaussian elimination algorithm also gives us how to compute the
similitude character @xmath of the symplectic and split orthogonal
similitude groups (see Theorem 6.3.11 ).

## Chapter 8 Finiteness of @xmath-classes

The results in this chapter are part of [ BS ] . This chapter is devoted
to the study of @xmath -classes in unitary groups. A unitary group is an
algebraic group defined over @xmath . Since we are working with perfect
fields, an element @xmath has a Jordan decomposition, @xmath , where
@xmath is semisimple and @xmath is unipotent (see Theorem 2.1.1 ).
Further one can use this to compute the centralizer @xmath . So the
Jordan decomposition helps us reduce the study of conjugacy and
computation of the centralizer of an element to the study of that of its
semisimple and unipotent parts. In Section 8.1 we study the @xmath
-classes for unipotent elements. In Section 8.2 we explore the @xmath
-classes for semisimple elements, and then we prove our main theorem,
which states that the number of @xmath -classes in any unitary group is
finite if @xmath has the property FE. The preliminaries for this chapter
have been discussed in Chapters 2 , 4 and 5 .

### 8.1 Unipotent @xmath-classes

We look at a special case when the minimal polynomial is @xmath , where
@xmath is an irreducible, self- @xmath -reciprocal polynomial. This
includes unipotent elements. The rational canonical form theory gives a
decomposition of

  -- -------- --
     @xmath   
  -- -------- --

with @xmath , and each @xmath is a free module over the @xmath -algebra
@xmath (see 2.14 Chapter IV [ SS ] ). Thus,

###### Proposition 8.1.1.

Let @xmath and @xmath be in @xmath . Suppose the minimal polynomial of
both @xmath and @xmath are equal, and it equals @xmath , where @xmath is
irreducible self- @xmath -reciprocal. Then @xmath and @xmath are
conjugate in @xmath if and only if

1.   the elementary divisors @xmath of @xmath and @xmath are same for
    @xmath , and

2.   the sequence of hermitian spaces, @xmath corresponding to @xmath ,
    and @xmath corresponding to @xmath are equivalent. Here @xmath and
    @xmath take values in the cyclic @xmath -algebra @xmath .

Moreover, the centralizer of @xmath , in this case, is the direct
product @xmath .

###### Proof.

Suppose @xmath and @xmath are conjugate in @xmath . Since they are
conjugate they have the same set of elementary divisors which proves
(1), and (2) follows from Proposition 4.2.2 .

Conversely, the elementary divisors of @xmath and @xmath determine the
orthogonal decomposition of @xmath as follows:

  -- -------- -------- -- ---------
     @xmath   @xmath      (8.1.1)
     @xmath   @xmath      (8.1.2)
  -- -------- -------- -- ---------

where @xmath , and for each @xmath , @xmath and @xmath are free as
@xmath and @xmath -module respectively. Since @xmath and @xmath are
isomorphic as @xmath -modules. We may write @xmath . Also by (2) we have
@xmath for all @xmath . So by Proposition 4.2.2 , we get @xmath is
conjugate to @xmath by @xmath , then @xmath conjugates @xmath and @xmath
.

Moreover, we have already seen that @xmath . And by Proposition 4.2.2 ,
we have @xmath for all @xmath . Hence @xmath . ∎

This gives us following:

###### Corollary 8.1.2.

Let @xmath have the property FE. Then,

1.   the number of conjugacy classes of unipotent elements in @xmath is
    finite.

2.   The number of @xmath -classes of unipotent elements in @xmath is
    finite.

###### Proof.

1.  In view of Proposition 8.1.1 , let the minimal polynomial be @xmath
    . Thus, we have @xmath . Then the conjugacy classes correspond to a
    sequence @xmath , and hermitian spaces @xmath up to equivalence. Now
    @xmath . Then, by the Wall’s approximation theorem (Theorem 4.2.3 ),
    the number of non-equivalent hermitian forms @xmath is exactly equal
    to the number of non-equivalent hermitian forms @xmath . Now @xmath
    has the property FE, so @xmath is finite (see Lemma 2.2.21 ). Then
    there are only finitely many non-equivalent quadratic forms over
    @xmath (see p. 32, Corollary 4.3 in [ Gv ] ). Hence, we know (see p.
    267, Theorem [ Ja ] ) that there are only finitely many
    non-equivalent hermitian forms over @xmath . Thus @xmath has only
    finitely many choices for each @xmath . Hence the result.

2.  Two elements are conjugate implies that they are also @xmath
    -conjugate. Hence it follows from the previous part.

∎

### 8.2 Semisimple @xmath-classes

Let @xmath be a semisimple element. First, we begin with a basic case.

###### Lemma 8.2.1.

Let @xmath be a semisimple element such that its minimal polynomial is
either @xmath , which is irreducible, self- @xmath -reciprocal of degree
@xmath , or @xmath , where @xmath is irreducible not self-U-reciprocal.
Let @xmath in the first case and @xmath in the second case. Then the
@xmath -class of @xmath is determined by the following:

1.   the algebra @xmath over @xmath , and

2.   the equivalence class of the @xmath -valued hermitian form @xmath
    on @xmath .

###### Proof.

Suppose @xmath are in the same @xmath -class, then @xmath for some
@xmath . We may replace @xmath by its conjugate @xmath , so we get
@xmath , thus @xmath . Hence @xmath is equivalent to @xmath . So, in
particular, @xmath and @xmath are isomorphic as @xmath -algebras. The
converse follows from Proposition 4.2.2 . ∎

Now for the general case, let @xmath be a semisimple element with
minimal polynomial

  -- -------- --
     @xmath   
  -- -------- --

where the @xmath are self- @xmath -reciprocal polynomials of degree
@xmath , and @xmath not self- @xmath -reciprocal of degree @xmath . Let
the characteristic polynomial of @xmath be

  -- -------- --
     @xmath   
  -- -------- --

Let us write the primary decomposition of @xmath with respect to @xmath
into @xmath -invariant subspaces as

  -- -------- -- ---------
     @xmath      (8.2.1)
  -- -------- -- ---------

Denote @xmath and @xmath , the field extensions of @xmath of degree
@xmath and @xmath respectively.

###### Theorem 8.2.2.

With notation as above, let @xmath be a semisimple element. Then the
@xmath -class of @xmath is determined by the following:

1.   a finite sequence of integers @xmath each @xmath and @xmath .

2.   Finite field extensions @xmath of @xmath of degree @xmath for
    @xmath and @xmath of @xmath of degree @xmath , for @xmath , and

3.   equivalence classes of @xmath -valued hermitian forms @xmath of
    rank @xmath , and @xmath -valued hermitian forms @xmath of rank
    @xmath .

Further with this notation, @xmath .

###### Proof.

Follows from Lemma 8.2.1 and Proposition 4.2.2 . Also, we use the fact
that unitary group is a form of general linear group. ∎

This gives us following:

###### Corollary 8.2.3.

Let @xmath have the property FE. Then the number of semisimple @xmath
-classes in @xmath is finite.

###### Proof.

This follows if we show that there are only finitely many hermitian
forms over @xmath , up to equivalence of any degree @xmath . We use
Jacobson’s theorem (see p. 267 Theorem in [ Ja ] ) that equivalence of
hermitian forms @xmath over @xmath is given by equivalence of
corresponding quadratic forms @xmath over @xmath . However, because of
the FE property of @xmath it turns out that @xmath is finite (see Lemma
2.2.21 ), and hence there are only finitely many quadratic forms of
degree @xmath over @xmath (see p. 32, Corollary 4.3 [ Gv ] ). This
proves the required result. ∎

The main result of this chapter is the following theorem:

###### Theorem 8.2.4.

Let @xmath be a perfect field of @xmath with a non-trivial Galois
automorphism of order @xmath . Let @xmath be a finite dimensional vector
space over @xmath with a non-degenerate hermitian form @xmath . Suppose
the fixed field @xmath has the property FE, then the number of @xmath
-classes in the unitary group @xmath is finite.

###### Proof.

It follows from Corollary 8.2.3 that the number of conjugacy classes of
centralizers of semisimple elements is finite. Hence, up to conjugacy,
there are finitely many possibilities for @xmath for @xmath semisimple
in @xmath . Let @xmath , then it has a Jordan decomposition @xmath .
Recall @xmath , and @xmath (see p. 230, Remarks 3.16 in [ SS ] and also
see in 11.12 [ Br ] ). Now @xmath is a product of certain unitary groups
and general linear groups possibly over a finite extension of @xmath
(Initially, the hermitian forms were over local ring @xmath . In fact,
@xmath . Using Wall’s approximation Theorem 4.2.3 , we can go to it’s
quotient by Jacobson radical, which is a finite extension of @xmath ).
Corollary 8.1.2 applied on the group @xmath implies that, up to
conjugacy, @xmath has finitely many possibilities in @xmath . Hence, up
to conjugacy, @xmath has finitely many possibilities in @xmath .
Therefore the number of @xmath -classes in @xmath is finite. ∎

###### Remark 8.2.5.

The FE property of the field @xmath is necessary for the above theorem.
For example, the field of rational numbers @xmath does not have the FE
property. We show by an example that the above theorem is no longer true
over @xmath .

###### Example 8.2.6.

Over field @xmath , there could be infinitely many non-conjugate maximal
tori in @xmath . Since a maximal torus is centralizer of a regular
semisimple element in it, we get an example of infinitely many @xmath
-classes. For the sake of clarity let us write down this concretely when
@xmath .

The group @xmath has infinitely many semisimple @xmath -classes. For, if
we take @xmath any degree @xmath irreducible polynomial, then the
centralizer of the companion matrix @xmath is isomorphic to @xmath ,
where @xmath , a field extension. Thus non-isomorphic degree two field
extensions (hence can not be conjugate) give rise to distinct @xmath
-classes (these are maximal tori in @xmath ).

Consider @xmath (a quadratic extension) with the usual involution @xmath
. We embed @xmath in @xmath with respect to the hermitian form @xmath
given by

  -- -------- --
     @xmath   
  -- -------- --

This embedding describes maximal tori in @xmath starting from that of
@xmath . Yet again, non-isomorphic degree @xmath field extensions would
give rise to distinct @xmath -classes. In turn, this gives infinitely
many @xmath -classes (of semisimple elements) in @xmath .

###### Example 8.2.7.

For @xmath , consider a unipotent element @xmath in @xmath . Then @xmath
. Then, @xmath is conjugate to @xmath in @xmath if and only if @xmath .
Let @xmath be a (perfect or non-perfect) field with @xmath infinite.
Then this would give an example, where we have infinitely many conjugacy
classes of unipotents but still, they are in a single @xmath -class.

## Chapter 9 Counting @xmath-classes

This chapter reports the work done in [ BS ] . In this chapter, we
investigate the @xmath -classes for classical groups. Without further
ado, we shall now go into computing @xmath -classes for @xmath and
@xmath for different @xmath . In Section 9.1 we compute the number of
@xmath -classes and their generating functions for general linear
groups, and in Section 9.2 we compute the same for unitary groups. The
main theorem proved here is that the number of @xmath -classes in @xmath
is same as the number of @xmath -classes in @xmath , when @xmath
(Theorem 9.2.6 ).

### 9.1 @xmath-classes in General Linear Groups

Let @xmath be a positive integer with a partition @xmath , denoted by
@xmath , i.e., @xmath , and @xmath denote the number of partitions of
@xmath . Let @xmath be the generating function for the partitions of
integers so @xmath . Let @xmath denotes the number of @xmath -classes in
@xmath . Define @xmath be the generating function for the @xmath
-classes in @xmath . If @xmath is an algebraically closed field then we
will suppress @xmath , and simply denote them as @xmath and @xmath
respectively.

###### Proposition 9.1.1.

Let @xmath be an algebraically closed field. Then,

1.   the number of @xmath -classes of semisimple elements in @xmath is
    @xmath , which is same as the number of @xmath -classes of unipotent
    elements.

2.   The number of @xmath -classes in @xmath is

      -- -------- --
         @xmath   
      -- -------- --

    and the generating function is

      -- -------- --
         @xmath   
      -- -------- --

###### Proof.

Since @xmath is an algebraically closed field then for each element
@xmath has a unique Jordan form. Suppose it has @xmath -distinct
eigenvalues @xmath . In each Jordan block corresponding to @xmath ’s,
the entries in superdiagonal can be filled with zeros and ones. These
possibilities will determine the number of @xmath -classes. These can be
said using the following argument. We know that @xmath . Therefore the
coefficient of @xmath in @xmath is @xmath . So for a fixed partition
@xmath of @xmath , the number of @xmath -classes is @xmath . Therefore
the total number of @xmath -classes in @xmath is @xmath . ∎

###### Proposition 9.1.2.

Let @xmath . Then,

1.  @xmath .

2.  @xmath .

3.   If @xmath then @xmath .

###### Proof.

1.  Here @xmath can be replaced by any algebraically closed field. Since
    an algebraically closed field has no extension at all, @xmath .

2.  Now @xmath has two extensions, one is @xmath itself of degree @xmath
    , and @xmath of degree @xmath . We are looking at rational canonical
    form for each element @xmath . Then over @xmath (i.e., @xmath degree
    @xmath extension of @xmath ), @xmath -classes are given by the
    generating function @xmath (see Proposition 9.1.1 ). Clearly the
    contributions to @xmath coming from @xmath is @xmath . There will be
    more @xmath -classes apart from these, which will be coming from the
    generating function @xmath (over @xmath itself). Hence @xmath .

3.  For finite field @xmath , for each degree extension @xmath , there
    is a unique field of that degree, namely @xmath . So the
    contributions to @xmath coming from @xmath are @xmath . Hence @xmath
    , and this product is well-defined because @xmath has the property
    FE.

∎

To compare these numbers we make a table for small ranks. The last row
of this table is there in the work of Green (see p.408 in [ Gr ] ).

  @xmath   z(1)   z(2)   z(3)   z(4)   z(5)   z(6)   z(7)   z(8)   z(9)   z(10)
  -------- ------ ------ ------ ------ ------ ------ ------ ------ ------ -------
  @xmath   1      3      6      14     27     58     111    223    424    817
  @xmath   1      4      7      20     36     87     162    355    666    1367
  @xmath   1      4      8      22     42     103    199    441    859    1784

### 9.2 @xmath-classes in Unitary Groups

The genus number of compact Lie groups has been computed in [ Bo ] . In
this situation we have a vector space @xmath over @xmath of dimension
@xmath . From now on the field is @xmath up until the start of Section
9.2.3. The hermitian forms are classified by the signature, and the
corresponding groups are denoted by @xmath , where @xmath and @xmath
(see (1) of Example 2.3.9 ).

#### 9.2.1 @xmath-classes in @xmath

We record the result (see Theorem 3.1 [ Bo ] ) here as follows:

###### Proposition 9.2.1.

The number of @xmath -classes in @xmath is @xmath .

###### Proof.

The group @xmath is a compact Lie group. So every element is semisimple.
Let @xmath , then @xmath is conjugate to @xmath , where @xmath ’s are
distinct complex numbers such that @xmath and @xmath . Hence

  -- -------- --
     @xmath   
  -- -------- --

So, up to conjugacy, @xmath is determined by the partitions of @xmath
(here order of the @xmath is not important). Hence the number of @xmath
-classes in @xmath is @xmath . ∎

#### 9.2.2 @xmath-classes in @xmath

The @xmath -classes of @xmath have been discussed by Cao and
Gongopadhyay in [ CG ] . In fact, they classified how the centralizers
will look like (see p. 3319, Corollary 1.2 [ CG ] ). So what is new here
is the enumeration. Here we present the number of @xmath -classes in
this group using the parametrization described there. Recall that the
hermitian matrix used there is @xmath , and the unitary group is @xmath
.

Another way to look at it is the following ball model: Let @xmath be a
vector space of dimension @xmath over @xmath , i.e., @xmath equipped
with the hermitian form of signature @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath are column vectors in @xmath . Define

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

Let @xmath be the complex projective space, i.e., @xmath , where @xmath
if there exists @xmath such that @xmath . Here @xmath is equipped with
the quotient topology, and the quotient map is @xmath . The @xmath
-dimensional complex hyperbolic space is defined to be @xmath . The
boundary @xmath in @xmath is @xmath . The isometry group @xmath of the
hermitian space @xmath acts as the isometries of @xmath . The actual
group of isometries of @xmath is @xmath , where @xmath is the center.
Thus an isometry @xmath of @xmath lifts to a unitary transformation
@xmath . The fixed points of @xmath correspond to eigenvectors of @xmath
. However, for convenience, we will mostly deal with the linear group
@xmath rather than the projective group @xmath . In the following, we
shall often forget the lift and use the same symbol for an isometry as
well as its lifts.

Now by Brouwer’s fixed point theorem, it follows that every isometry
@xmath has a fixed point on the closure @xmath . An isometry @xmath is
called elliptic if it has a fixed point on @xmath . It is called
parabolic if it is not elliptic and has exactly one fixed point on the
boundary @xmath , and is called hyperbolic if it is not elliptic and has
exactly two fixed points on the boundary @xmath .

Thus the elements of this group are classified as either elliptic,
hyperbolic or parabolic depending on their fixed points. Using
conjugation classification [ CGb ] we know that if an element @xmath is
elliptic or hyperbolic, then they are always semisimple. But a parabolic
element need not be semisimple. However it has a Jordan decomposition
@xmath , where @xmath is elliptic, hence semisimple, and @xmath is
unipotent. In particular if a parabolic isometry is unipotent, then it
has minimal polynomial @xmath or @xmath and is called vertical
translation or non-vertical translation respectively.

###### Definition 9.2.2.

An eigenvalue @xmath (counted with multiplicities) of an element @xmath
is called null, positive or negative if the corresponding @xmath
-eigenvectors belong to @xmath or @xmath respectively.

Accordingly, a similarity class of eigenvalues @xmath is null , positive
or negative according to its representative @xmath is null, positive or
negative respectively.

###### Theorem 9.2.3.

1.   The number of @xmath -classes of hyperbolic elements in @xmath is
    @xmath .

2.   The number of @xmath -classes of elliptic elements in @xmath is

      -- -------- --
         @xmath   
      -- -------- --

3.   The number of @xmath -classes of parabolic elements in @xmath is
    @xmath ( @xmath ).

###### Proof.

1.  Now, suppose @xmath is hyperbolic. Then @xmath has an orthogonal
    decomposition @xmath , where @xmath , and @xmath is the eigenspace
    of @xmath corresponding to the similarity class of positive
    eigenvalue @xmath with @xmath (see p. 3324 in [ CG ] and reference
    there for definition). The subspace @xmath is the two-dimensional
    @xmath -invariant subspace spanned by the corresponding similarity
    class of null-eigenvalues @xmath for @xmath , respectively. Then
    @xmath . Here @xmath , i.e., @xmath . Thus, the number of @xmath
    -classes of hyperbolic elements is @xmath .

2.  Let @xmath be an elliptic element. Then @xmath has a negative class
    of eigenvalue say @xmath . Let @xmath , which is @xmath . It follows
    from the conjugacy classification that all the eigenvalues have norm
    @xmath and there is a negative eigenvalue. All other eigenvalues are
    of the positive type. Then @xmath . Suppose @xmath , then @xmath .
    Now since @xmath is of negative type, so @xmath . Here @xmath ,
    therefore @xmath . This gives that the number of @xmath -classes of
    elliptic elements is @xmath .

3.  Let @xmath be parabolic. First, let @xmath be unipotent. If the
    minimal polynomial of @xmath is @xmath (i.e., @xmath is a vertical
    translation), then @xmath . If the minimal polynomial of @xmath is
    @xmath (i.e., @xmath is non-vertical translation), then @xmath .
    Hence there are exactly two @xmath -classes of unipotents, one
    corresponds to the vertical translation and the other to the
    non-vertical translation. Now assume that @xmath is not unipotent.
    Suppose that the similarity class of a null-eigenvalue is @xmath .
    Then @xmath has a @xmath -invariant orthogonal decomposition @xmath
    , where @xmath is a @xmath -indecomposable subspace of @xmath ,
    which is either @xmath or @xmath (see p. 956 [ Go ] ). Then @xmath .
    For each choice of @xmath , there is exactly one choice for the
    @xmath -classes of @xmath in @xmath , i.e., @xmath or @xmath . Note
    that @xmath can be embedded into @xmath . Hence it suffices to find
    out the number of @xmath -classes of @xmath in @xmath . For each
    choice of @xmath , there are exactly one choice for the @xmath
    -classes of @xmath in @xmath . Hence the total number of @xmath
    -classes of non-unipotent parabolic is @xmath . Therefore the total
    number of @xmath -classes of parabolic transformations is @xmath (
    @xmath ).

∎

#### 9.2.3 @xmath-classes in @xmath

Now we will focus on unitary groups over finite field @xmath with @xmath
given by @xmath and @xmath . It is well-known that over a finite field
there is a unique non-degenerate hermitian form up to equivalence. We
denote the unitary group by @xmath . The groups @xmath and @xmath , both
are subgroups of @xmath . We want to count the number of @xmath
-classes, and write its generating function. In view of Ennola duality,
the representation theory of both these groups are closely related. Thus
it is always useful to compare any computation for @xmath with that of
@xmath .

###### Corollary 9.2.4.

With the same notation as in Theorem 8.2.2 . Let @xmath be a semisimple
element. Then the @xmath -class of @xmath is determined by a finite
sequence of integers @xmath each @xmath and @xmath .

###### Proof.

We know that, for finite field @xmath there is a unique field of each
degree extension @xmath , namely @xmath . Also hermitian form is unique,
up to equivalence, over a finite field (see p. 88, Corollary 10.4 in [
Gv ] ). Hence the result follows from Theorem 8.2.2 , when we specify
@xmath a finite field. ∎

###### Lemma 9.2.5.

1.   The number of @xmath -classes of unipotent elements in @xmath is
    @xmath , which is the number of @xmath -classes of unipotent
    elements in @xmath .

2.   The number of @xmath -classes of semisimple elements in @xmath is
    same as the number of @xmath -classes of semisimple elements in
    @xmath if @xmath .

###### Proof.

1.  Let @xmath be a unipotent element in @xmath written in Jordan block
    form (see Chapter 3 in [ BG ] for more details). Wall proved the
    following membership test (see Case(A) on page 34 of [ Wa2 ] ): Let
    @xmath then @xmath is conjugate to @xmath in @xmath if and only if
    @xmath is conjugate to an element of @xmath . Since unipotents are
    conjugate to their own inverse in @xmath , this implies @xmath is
    conjugate to @xmath in @xmath . Hence @xmath is conjugate to an
    element of @xmath . Wall also proved that two elements of @xmath are
    conjugate in @xmath if and only if they are conjugate in @xmath (see
    also 6.1 [ Ma ] ). Thus, up to conjugacy, there is a one-one
    correspondence of unipotent elements between @xmath and @xmath .
    This gives that the number of unipotent conjugacy classes in @xmath
    is @xmath , and it is same as that of @xmath . Now, we note that
    (see Lemma 3.3.8 [ BG ] ) @xmath , where

      -- -------- --
         @xmath   
      -- -------- --

    Clearly, the centralizers are distinct and hence can not be
    conjugate. Thus the number of unipotent @xmath -classes in @xmath is
    @xmath .

2.  For semisimple elements, we use Theorem 8.2.2 . Over a finite field
    (when @xmath ), we get that semisimple @xmath -classes in @xmath are
    characterized by simply @xmath , where @xmath is odd (being a degree
    of a monic, irreducible, self-U-reciprocal polynomial, see
    Proposition 5.1.1 ) and @xmath is even. This corresponds to the
    number of ways @xmath can be written as @xmath , which is same as
    the number of semisimple @xmath -classes in @xmath .

∎

The main result of this chapter is the following:

###### Theorem 9.2.6.

The number of @xmath -classes in @xmath is same as the number of @xmath
-classes in @xmath if @xmath . Thus, the number of @xmath -classes for
either group can be read off by looking at the coefficients of the
function @xmath , where @xmath and @xmath is the number of partitions of
@xmath .

###### Proof.

Recall that if @xmath is the Jordan decomposition of @xmath then @xmath
, and the structure of @xmath in Theorem 8.2.2 implies that

  -- -------- --
     @xmath   
  -- -------- --

where the sum runs over semisimple @xmath -classes. Hence the number of
@xmath -classes in @xmath is the same as the number of @xmath -classes
in @xmath . ∎

###### Remark 9.2.7.

However, the above Theorem 9.2.6 need not be true when @xmath . For
sufficiently large @xmath there will be @xmath -classes of every type
but for small values of @xmath certain types may not be available. For
example, if @xmath , then there are no matrices in @xmath of type @xmath
, where @xmath and are distinct.

###### Example 9.2.8.

Over a finite field @xmath , if @xmath is not large enough we may not
have as many finite extensions available as required in part 2 of
Theorem 8.2.2 . Thus we expect less number of @xmath -classes. We use
GAP [ GAP ] to calculate the number of @xmath -classes for small order
and present our findings below:

  @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath
  -------- -------- -------- -------- -------- -------- --------
  @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath
  @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath

  @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath
  -------- -------- -------- -------- -------- -------- --------
  @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath
  @xmath   @xmath   @xmath   @xmath   @xmath   @xmath   @xmath

  @xmath   @xmath   @xmath   @xmath   @xmath   @xmath
  -------- -------- -------- -------- -------- --------
  @xmath   @xmath   @xmath   @xmath   @xmath   @xmath
  @xmath   @xmath   @xmath   @xmath   @xmath   @xmath

Thus we demonstrate the following:

1.   When @xmath the number of @xmath -classes in @xmath and @xmath are
    not given by the formula in Theorem 9.2.6 .

2.   When @xmath the number of @xmath -classes in @xmath and @xmath need
    not be equal.

## Chapter 10 Future Plans

The groups we study here are fundamental objects in algebraic groups.
Given wide interest and applications in group theory, it is interesting
to compute centralizers and @xmath -classes in algebraic groups.

### 10.1 Further Questions

We would like to continue our study for other groups, especially for
exceptional groups. So the precise problem would be the following:

###### Problem 10.1.1.

Is the number of @xmath -classes finite for the exceptional groups of
type @xmath defined over @xmath with the property FE?

R. Steinberg proved the result all at once for reductive algebraic
groups over an algebraically closed field. So one can ask the following:

###### Problem 10.1.2.

Is the number of @xmath -classes finite for a reductive algebraic group
defined over @xmath with the property FE?

This problem is hard but will be quite interesting. I believe the answer
to these questions is positive. We have some ideas and preliminary
results on this. Another natural question would be; what is the number
of @xmath -classes for a certain group @xmath ? We would like to address
this question over finite fields @xmath . A more concrete question I
would like to address in future is the following:

###### Problem 10.1.3.

What are the number of @xmath -classes in @xmath and @xmath ?

###### Problem 10.1.4.

How does it reflect on the representation theory of these groups?

We have seen that the Bruhat decomposition (Theorem 6.1.3 ) for general
linear groups @xmath has a nice connection to the classical Gaussian
elimination algorithm. So one would expect the same kind of
decomposition for other groups, namely, similitude groups using our
Gaussian elimination algorithms developed in Section 6.3.2 and 6.3.3 .
So the precise problem would be the following:

###### Problem 10.1.5.

Do the Bruhat decomposition for the symplectic and orthogonal groups
using our algorithms.

More generally,

###### Problem 10.1.6.

Do the Bruhat decomposition for the symplectic and orthogonal similitude
groups using our algorithms.
