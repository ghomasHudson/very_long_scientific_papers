## Chapter 1 Introduction

“Life is short, and the Art long; the occasion fleeting; experience
fallacious, and judgment difficult. The physician must not only be
prepared to do what is right himself, but also to make the patient, the
attendants, and externals cooperate.”

Hippocrates [460-375 BC]

### 1.1 Aims and Motivations

Convolutional Neural Networks (CNNs) have revolutionized Medical Image
Analysis by extracting valuable insights for better clinical examination
and medical intervention; the CNNs occasionally outperformed even expert
physicians in diagnostic accuracy when large-scale annotated datasets
were available [ 64 , 114 ] . However, obtaining such massive datasets
often involves the following intrinsic challenges [ 88 , 41 ] : ( i ) it
is costly and laborious to collect medical images, such as Magnetic
Resonance (MR) and Computed Tomography (CT) images, especially for rare
disease; ( ii ) it is time-consuming and observer-dependent, even for
expert physicians, to annotate them due to the low
pathological-to-healthy ratio. To tackle these issues, researchers have
mainly focused on extracting as much information as possible from the
available limited data [ 20 , 147 ] . Instead, Generative Adversarial
Networks (GANs) [ 39 ] can generate realistic but completely new samples
via many-to-many mappings, and thus effectively cover the real image
distribution; they showed great promise in Data Augmentation (DA) using
natural images, such as 21% performance improvement in eye-gaze
estimation [ 135 ] .

Interpolation refers to new data point construction within a
discretely-sampled data distribution. In terms of the interpolation, the
GAN-based image augmentation is reliable on the medical images because
medical modalites (e.g., X-ray, CT, MRI) can display the human body’s
strong anatomical consistency at fixed position while clearly reflecting
inter-subject variability [ 58 , 16 ] —this is different from the
natural images, where various objects can appear at any position;
accordingly, to tackle large inter-subject, inter-pathology, and
cross-modality variability [ 88 , 41 ] , we propose to use
noise-to-image GANs (e.g., random noise samples to diverse pathological
images) for ( i ) medical DA and ( ii ) physician training [ 45 ] . The
noise-to-image GAN training is much more difficult than training
image-to-image GANs (e.g., a benign image to a malignant one); but, it
can perform more global regularization (i.e., adding constraints when
fitting a loss function on a training set to prevent overfitting) and
increase image diversity for further performance boost.

Regarding the DA, the GAN-generated images can improve Computer-Aided
Diagnosis (CAD) based on supervised learning [ 34 ] . For the physician
training, the GANs can display novel desired pathological images and
help train medical trainees despite infrastructural and legal
constraints [ 31 ] . However, we cannot directly use conventional GANs
for realistic/diverse high-resolution medical image augmentation.
Moreover, we have to find effective loss functions and training schemes
for each of those applications [ 48 ] ; the diversity matters more for
the DA to sufficiently fill the real image distribution whereas the
realism matters more for the physician training not to confuse the
medical students and radiology trainees.

So, how can we perform clinically relevant GAN-based DA/physician
training using only limited annotated training images? Always in
collaboration with physicians, for improving 2D classification, we
combine the noise-to-image [ 49 , 50 ] (i.e., Progressive Growing of
GANs, PGGANs [ 72 ] ) and image-to-image GANs (i.e., Multimodal
UNsupervised Image-to-image Translation, MUNIT [ 61 ] ); the two-step
GAN can generate and refine realistic/diverse original-sized @xmath
brain MR images with/without tumors separately. Nevertheless, further DA
applications require pathology localization for detection (i.e.,
identifying target pathology positions in medical images) and advanced
physician training needs atypical image generation, respectively. To
meet both clinical demands, we propose novel 2D/3D bounding box-based
GANs conditioned on pathology position/size/appearance; the bounding
box-based detection requires much less physicians’ annotation effort
than rigorous segmentation.

Extrapolation refers to new data point estimation beyond a
discretely-sampled data distribution. While it is not mutually-exclusive
with the interpolation and both rely on a model’s restoring force, it is
more subject to uncertainty and thus a risk of meaningless data
generation. In terms of the extrapolation, the pathology-aware GANs
(i.e., the conditional GANs controlling pathology, such as tumors and
nodules, based on position/size/appearance) are promising because common
and/or desired medical priors can play a key role in the
conditioning—theoretically, infinite conditioning instances, external to
the training data, exist and enforcing such constraints have an
extrapolation effect via model reduction [ 140 ] ; inevitable errors,
not limited between two data points, caused by the model reduction
forces a generator to synthesize images that the generator has never
synthesized before.

For improving 2D detection, we propose Conditional PGGANs (CPGGANs) that
incorporates highly-rough bounding box conditions incrementally into the
noise-to-image GAN (i.e., the PGGANs) to place realistic/diverse brain
metastases at desired positions/sizes on @xmath MR images [ 47 ] . As
its pathology-aware conditioning, we use 2D tumor position/size on MR
images. Since lesions vary in 3D position/appearance, for improving 3D
detection, we propose 3D Multi-Conditional GAN (MCGAN) that translates
noise boxes into realistic/diverse @xmath lung nodules placed naturally
at desired position/size/attenuation on CT scans [ 46 ] ; inputting the
noise box with the surrounding tissues has the effect of combining the
noise-to-image and image-to-image GANs. As its pathology-aware
conditioning, we use 3D nodule position/size/attenuation on CT scans.

Lastly, we confirm our pathology-aware GANs’ clinical relevance for
diagnosis as a clinical decision support system and non-expert physician
training tool via two discussions: ( i ) Conducting a questionnaire
survey about our GAN projects for 9 physicians; ( ii ) Holding a
workshop about how to develop medical Artificial Intelligence (AI)
fitting into a clinical environment in five years for 7 professionals
with various AI and/or Healthcare background.

Contributions. Our main contributions are as follows:

-    Noise-to-Image GAN Applications: We propose clinically-valuable
    novel noise-to-image GAN applications, medical DA and physician
    training, focusing on their ability to generate realistic and
    diverse images.

-    Pathology-Aware GANs: For required extrapolation, always in
    collaboration with physicians, we propose novel 2D/3D GANs
    controlling pathology (i.e., tumors and nodules) on most major
    modalities (i.e., brain MRI and lung CT).

-    Clinical Validation: After detailed discussions with many
    physicians and professionals with various AI and/or Healthcare
    background, we confirm our pathology-aware GANs’ clinical relevance
    as a ( i ) clinical decision support system and ( ii ) non-expert
    physician training tool.

### 1.2 Thesis Overview

This Ph.D. thesis aims to present the clinical relevance of our novel
pathology-aware GAN applications, medical DA and physician training,
always in collaboration with physicians.

The thesis is organized as follows (Fig. 1.1 ). Chapter 2 covers the
background of Medical Image Analysis and Deep Learning, as well as
methods to address data paucity to bridge them. Chapter 3 describes
related work on the GAN-based medical DA and physician training, which
emerged after our proposal to use noise-to-image GANs for those
applications in Chapter 4 . Chapter 5 presents a two-step GAN for 2D
classification that combines both noise-to-image and image-to-image
GANs. Chapter 6 proposes CPGGANs for 2D detection that incorporates
highly-rough bounding box conditions incrementally into the
noise-to-image GAN. Finally, we propose 3D MCGAN for 3D detection that
translates noise boxes into desired pathology in Chapter 7 . Chapter 8
discusses both our pathology-aware GANs’ clinical relevance via a
questionnaire survey and how to develop medical AI fitting into a
clinical environment in five years via a workshop. Lastly, Chapter 9
provides the conclusive remarks and future directions for further
GAN-based extrapolation.

## Chapter 2 Background

This chapter introduces basic concepts in Medical Image Analysis and
Deep Learning. Afterwards, we describe methods to address data paucity
because they play the greatest role in bridging the Medical Image
Analysis and Deep Learning.

### 2.1 Medical Image Analysis

Medical Image Analysis refers to the process of increasing clinical
examination/medical intervention efficiency, based on several imaging
modalities and digital image processing techniques [ 81 , 122 ] ; to
effectively visualize the human body’s anatomical and physiological
features, it covers various modalities including X-ray, CT, MRI,
positron emission tomography, endoscopy, optical coherence tomography,
pathology, ultrasound imaging, and fundus imaging. Its tasks are mainly
classified into three groups: ( i ) Early detection/diagnosis/prognosis
of disease often based on pathology
classification/detection/segmentation and survival prediction [ 158 , 82
] ; ( ii ) Clinical workflow enhancement often based on body part
segmentation, inter-modality registration, 3D reconstruction, flow
measurement, and surgery simulation [ 18 , 160 ] ; ( iii ) Clinically
impossible image analysis, such as radiogenomics that identifies the
correlation between cancer imaging features and gene expression [ 165 ]
.

Among the various modalities, this thesis focuses on the most common 3D
modalities for non-invasive diagnosis, CT and MRI. To get a detailed
picture inside the body, the CT merges multiple X-rays at different
angles using computational tomographic reconstruction [ 58 , 37 ] .
Since X-ray intensity is associated with the mass attenuation
coefficient, higher-density tissues show higher attenuation and vice
versa . Accordingly, each voxel possesses its attenuation value
following the Hounsfield scale from @xmath to @xmath (e.g., Hounsfield
units @xmath for air, 0 for water, and @xmath for dense bone). The CT
can provide a outstanding contrast within soft-tissue, bone, and lung
while the soft-tissue contrast is poor—accordingly, it is especially
performed for comprehensive lung assessment.

The MRI uses magnetization properties of atomic nuclei [ 16 , 98 ] .
Since different tissues show various relaxation processes when the
nuclei return to their resting alignment, the tissues’ proton density
maps serve as both anatomical and functional images. Since the tissues
possess two different relaxation times, T1 (i.e., longitudinal
relaxation time) and T2 (i.e., transverse relaxation time), as MRI
sequences, we can obtain both T1-weighted (T1) and T2-weighted (T2)
images. Moreover, using very long repetition time and time to echo, we
can obtain FLuid Attenuation Inversion Recovery (FLAIR) images. The MRI
can provide a superior soft-tissue contrast to the CT—accordingly, it is
especially performed for comprehensive brain assessment.

### 2.2 Deep Learning

Deep Learning is a kind of Machine Learning algorithms, based on
Artificial Neural Networks [ 84 ] . The Deep Neural Networks consist of
many linearly connected non-linear units whose parameters are optimized
by gradient descent [ 12 ] ; accordingly, their multiple layers can
gradually grasp more-detailed features as training progresses (i.e.,
learning which features to place is automatic). Thanks to the good
generalization ability, under large-scale data, the Deep Learning
significantly outperforms classical Machine Learning algorithms relying
on feature engineering. A visual cortex includes arrangements of simple
and complex cells activated by a receptive field (i.e., subregions of a
visual field); inspired by this biological structure [ 62 ] , CNNs adopt
a mathematical operation called convolution to achieve translation
invariance [ 127 ] . Since the CNNs are excellent at image/video
recognition, their diverse medical applications include pathology
classification/detection/segmentation and survival prediction [ 158 , 82
] .

Variational AutoEncoders (VAEs) often suffer from blurred samples
despite easier training, due to the imperfect reconstruction using a
single objective function [ 75 ] ; meanwhile, GANs have revolutionized
image generation in terms of realism and diversity [ 164 ] , including
denoising [ 151 ] and MRI-to-CT translation [ 30 ] , based on a
two-player objective function using two CNNs [ 39 ] : a generator @xmath
tries to generate realistic images to fool a discriminator @xmath while
maintaining diversity; @xmath attempts to distinguish between the real
and synthetic images. However, difficult GAN training from the
two-player objective function accompanies artifacts and mode collapse [
42 ] , when generating high-resolution images (e.g., @xmath pixels) [
113 ] ; to tackle this, multi-stage noise-to-image GANs have been
proposed: AttnGAN generated images from text using attention-based
multi-stage refinement [ 154 ] ; PGGANs generated realistic images using
low-to-high resolution multi-stage training [ 72 ] .

Contrarily, to obtain images with desired texture and shape, some
researchers have proposed image-to-image GANs: MUNIT translated images
using both GANs and VAEs [ 61 ] ; Simulated and unsupervised learning
GAN (SimGAN) translated images for DA using the self-regularization term
and local adversarial loss [ 135 ] ; Isola et al. proposed Pix2Pix GAN
to produce robust images using paired training samples [ 67 ] . Others
have proposed conditional GANs: Reed et al. proposed bounding box-based
conditional GAN to control generated images’ local properties [ 117 ] ;
Park et al. proposed multi-conditional GAN to refine base images based
on texts describing desired position [ 107 ] .

In Healthcare, medical images have generated the largest volume of data
and this trend will no doubt increase due to equipment improvement [ 63
, 156 ] . Accordingly, as the Deep Learning dominates Computer Vision,
Medical Image Analysis is not an exception; their combination can
analyze the large-scale medical images and extract valuable insights for
better clinical examination and medical intervention. However, the
biggest challenge to bridge them lies in the difficulty of obtaining
desired pathological images, especially for rare disease [ 88 , 41 ] .
Moreover, it is time-consuming and observer-dependent, even for expert
physicians, to annotate them.

### 2.3 Methods to Address Data Paucity

So, how can we tackle the data paucity? We can either attempt to ( a )
overcome the lack of generalization or ( b ) overcome difficulties in
optimization. The most straightforward and effective way to address the
generalization is DA [ 109 , 134 ] ; because the best model when given
data is uncertain, we commonly increase training set size. Human
perception is invariant to size, shape, brightness, and color [ 55 ] .
Accordingly, we recognize the same objects while their such features
change, and thus intentionally changing the features is plausible to
obtain more data. Such classical DA include ( i ) x/y/z-axis flipping
and rotating, ( ii ) zooming and scaling, ( iii ) cropping, ( iv )
translating, ( v ) elastic deformation, ( vi ) adding Gaussian noise
(i.e, the distortion of high frequency features), and ( vii ) brightness
and contrast fluctuation.

Recent DA techniques focus on regularization: Mixup [ 163 ] and
Between-class learning [ 142 ] mixed two images during training, such as
a dog image and a cat one, for regularization; Cutout randomly masked
out square regions during training for regularization [ 28 ] ; CutMix
combined the Mixup and Cutout [ 161 ] . As a recent impressive DA
approach, AutoAugment automatically searched for improved DA policies [
27 ] . Moreover, similarly to the Mixup among all images within the same
class, GAN-based DA can fill the uncovered real image distribution by
generating realistic and diverse images via many-to-many mapping [ 6 ] .

Along with the DA, researchers proposed many other techniques to improve
the generalization: semi-supervised learning can considerably increase
accuracy under limited labeled data by using pseudo labels for unlabeled
data [ 20 ] ; unsupervised anomaly detection allows to detect
out-of-distribution images from normal ones, such as disease, without
any labeled data [ 51 ] ; regularization techniques, such as dropout [
139 ] , Lasso [ 108 ] , and elastic net [ 166 ] , are commonly used for
reducing overfitting; similarly, ensembling multiple models via bagging
[ 15 ] and boosting [ 33 ] can effectively increase the robustness;
Lastly, in Medical Image Analysis, we can fuse multiple image modalities
and/or sequences, such as MRI @xmath CT [ 57 ] and T1 MRI @xmath T2 MRI
[ 22 ] .

Moreover, many techniques exist for overcoming the difficulties in
optimization: transfer learning can achieve better parameter
initialization [ 133 ] ; problem reduction, such as inputting 2D/3D
image patches instead of a whole image, can eliminate unnecessary
parameters [ 162 ] ; learning methods with less data, such as zero-shot
learning [ 119 ] , one-shot learning [ 147 ] , and neural Turing machine
[ 40 ] , are also promising; meta-learning promotes a versatile model
applicable to various tasks without requiring multiple training from
scratch [ 32 ] .

## Chapter 3 Investigated Contexts and Applications

In terms of interpolation, GAN-based medical image augmentation is
reliable because medical modalities (e.g., X-ray, CT, MRI) can display
the human body’s strong anatomical consistency at fixed position while
clearly reflecting inter-subject variability [ 58 , 16 ] —this is
different from natural images, where various objects can appear at any
position. Accordingly, we proposed to use noise-to-image GANs for ( i )
medical DA and ( ii ) physician training [ 45 ] in Chapter 4 . Since
then, research towards such clinically valuable applications has shown
great promise. This chapter covers such related research works except
our own works [ 49 , 50 , 47 , 46 ] included in Chapters 5-7 . Involving
9 physicians, we discuss in detail the clinical relevance of the
GAN-based medical DA and physician training [ 52 ] in Chapter 8 .

### 3.1 GAN-based Medical DA

Because the lack of annotated pathological images is the greatest
challenge in CAD [ 88 , 41 ] , to handle various types of
small/fragmented datasets from multiple scanners, researchers have
actively conducted GAN-based DA studies especially in Medical Image
Analysis. For better classification, some researchers adopted
image-to-image GANs similarly to their conventional medical
applications, such as denoising [ 151 ] and MRI-to-CT translation [ 30 ]
: Wu et al. translated @xmath normal mammograms into lesion ones [ 152 ]
, Gupta et al. translated @xmath normal leg X-ray images into bone
lesion ones [ 44 ] , and Malygina et al. translated @xmath / @xmath
normal chest X-ray images into pneumonia/pleural-thickening ones [ 92 ]
. Meanwhile, others adopted the noise-to-image GANs as we proposed, to
increase image diversity for further performance boost—the diversity
matters more for the DA to sufficiently fill the real image
distribution: Frid-Adar et al. augmented @xmath liver lesion CT images [
34 ] , Madani et al. augmented @xmath chest X-ray images with
cardiovascular abnormality [ 91 ] , and Konidaris et al. augmented
@xmath brain MR images with Alzheimer’s disease [ 77 ] .

To facilitate pathology detection and segmentation, researchers
conditioned the image-to-image GANs, not the noise-to-image GANs like
our work in Chapter 6 , with pathology features (e.g., position, size,
and appearance) and generated realistic/diverse pathology at desired
positions in medical images. In terms of extrapolation, the
pathology-aware GANs are promising because common and/or desired medical
priors can play a key role in the conditioning—theoretically, infinite
conditioning instances, external to the training data, exist and
enforcing such constraints have an extrapolation effect via model
reduction [ 140 ] . To the best of our knowledge, only Kanayama et al.
tackled bounding box-based pathology detection using the image-to-image
GAN [ 71 ] ; they translated normal endoscopic images with various image
sizes ( @xmath on average) into gastric cancer ones by inputting both a
benign image and a black image (i.e., pixel value: 0) with a specific
lesion Region Of Interest (ROI) at desired position. Without
conditioning the noise-to-image GAN with nodule position, Gao et al.
generated @xmath 3D nodule subvolumes only applicable to their
subvolume-based detector using binary classification [ 35 ] .

Since 3D imaging is spreading in radiology (e.g., CT and MRI), most
GAN-based DA works for segmentation exploited 3D conditional
image-to-image GANs. However, 3D medical image generation is more
challenging than 2D one due to expensive computational cost and strong
anatomical consistency; so, instead of generating a whole image
including pathology, researchers only focused on a malignant Voxel Of
Interest (VOI): Shin et al. translated @xmath normal brain MR images
into tumor ones by inputting both a benign image and a
tumor-conditioning image [ 132 ] , similarly to the Kanayama et al. ’s
work [ 71 ] ; Jin et al. generated @xmath CT images of lung nodules
including the surrounding tissues by only inputting a VOI centered at a
lung nodule, but with a central sphere region erased [ 70 ] . Recently,
instead of generating realistic images and training classifiers on them
separately, Chaitanya et al. directly optimized segmentation results on
cardiac MR images [ 18 ] ; however, it segmented body parts, instead of
pathology. Since effective GAN-based medical DA generally requires much
engineering effort, we also published a tutorial journal paper [ 48 ]
about tricks to boost classification/detection/segmentation performance
using the GANs, based on our experience and related work.

### 3.2 GAN-based Physician Training

While medical students and radiology trainees must view thousands of
images to become competent [ 149 ] , accessing such abundant medical
images is often challenging due to infrastructural and legal constraints
[ 24 ] . Because pathology-aware GANs can generate novel medical images
with desired abnormalities (e.g., position, size, and appearance)—while
maintaining enough realism not to confuse the medical trainees—GAN-based
physician training concept is drawing attention: Chuquicusma et al.
appreciated the GAN potential to train radiologists for educational
purpose after successfully generating @xmath CT images of lung nodules
that even deceived experts [ 25 ] ; thanks to their anonymization
ability, Shin et al. proposed to share pathology-aware GAN-generated
images outside institutions after achieving considerable tumor
segmentation results with only synthetic @xmath MR images for training [
132 ] ; more importantly, Finlayson et al. from Harvard Medical School
are currently validating a class-conditional GANs’ radiology educational
efficacy after succeeding in learning features that distinguish
fractures from non-fractures on @xmath pelvic X-ray images [ 31 ] .

## Chapter 4 GAN-based Medical Image Generation

### 4.1 Prologue to First Project

#### 4.1.1 Project Publication

-    GAN-based Synthetic Brain MR Image Generation .  C. Han , H.
    Hayashi, L. Rundo, R. Araki, Y. Furukawa, W. Shimoda, S.
    Muramatsu, G. Mauri, H. Nakayama, In IEEE International Symposium on
    Biomedical Imaging (ISBI) , pp. 734–738, April 2018.

#### 4.1.2 Context

Prior to this work, it remained challenging to generate realistic and
diverse medical images using noise-to-image GANs, not image-to-image
GANs [ 151 ] , due to their unstable training. GAN architectures
well-suited for medical images were unclear. Yi et al. published results
on the noise-to-image GAN-based brain MR image generation, proposing its
potential for medical DA and physician training while our paper was
under submission [ 17 ] ; however, they only generated single-sequence
low-resolution @xmath brain MR images without tumors.

#### 4.1.3 Contributions

This project’s main contribution is to propose to use recently developed
Wasserstein Generative Adversarial Network (WGAN) [ 7 ] for medical DA
and physician training—the medical GAN applications are reliable in
terms of interpolation because medical modalities can display the human
body’s strong anatomical consistency at fixed position while clearly
reflecting inter-subject variability. We also demonstrate the
noise-to-image GAN’s such potential by generating multi-sequence
realistic/diverse @xmath whole brain tumor MR images [ 7 ] ; then, we
confirm the superb realism via Visual Turing Test by a physician.

#### 4.1.4 Recent Developments

Since proposing the GAN applications, we have successfully applied the
noise-to-image GANs to improve 2D tumor classification/detection on
@xmath brain MR images [ 49 , 50 , 47 ] as described in Chapters 5 and 6
. For better 3D tumor segmentation, Shin et al. have translated @xmath
normal brain MR images into tumor ones using the image-to-image GAN [
132 ] . Finlayson et al. have generated @xmath pelvic
fracture/non-fracture X-ray images using a class-conditional
noise-to-image GAN, also introducing ongoing work on validating such
GANs’ radiology educational efficacy [ 31 ] . Kwon et al. have generated
realistic/diverse 3D brain MR images using the noise-to-image GAN [ 80 ]
.

### 4.2 Motivation

Along with classic methods [ 126 ] , CNNs have recently revolutionized
medical image analysis [ 131 ] , including brain MRI segmentation [ 53 ]
. However, CNN training demands extensive medical data that are
laborious to obtain [ 115 ] . To overcome this issue, DA techniques via
reconstructing original images are common for better performance, such
as geometry and intensity transformations [ 120 , 100 ] .

However, those reconstructed images intrinsically resemble the original
ones, leading to limited performance improvement in terms of
generalization abilities; thus, generating realistic (similar to the
real image distribution) but completely new images is essential. In this
context, GAN-based DA has excellently performed in general computer
vision tasks. It attributes to GAN’s good generalization ability from
matching the noise-generated distribution to the real one with a sharp
value function. Especially, Shrivastava et al. (SimGAN) outperformed the
state-of-the-art with a relative 21% improvement in eye-gaze estimation
[ 135 ] .

So, how can we generate realistic medical images completely different
from the original samples? Our aim is to generate synthetic
multi-sequence brain MR images using GANs, which is essential in medical
imaging to increase diagnostic reliability, such as via DA in CAD as
well as physician training (Fig. 4.1 ) [ 112 ] . However, this is
extremely challenging—MR images are characterized by low contrast,
strong visual consistency in brain anatomy, and intra-sequence
variability. Our novel GAN-based approach for medical DA adopts Deep
Convolutional Generative Adversarial Network (DCGAN) [ 113 ] and WGAN [
7 ] to generate realistic images, and an expert physician validates them
via Visual Turing Test [ 128 ] .

Research Questions. We mainly address two questions:

-    GAN Selection: Which GAN architecture is well-suited for realistic
    medical image generation?

-    Medical Image Processing: How can we handle MR images with specific
    intra-sequence variability?

Contributions. Our main contributions are as follows:

-    MR Image Generation: This research shows that WGAN can generate
    realistic multi-sequence brain MR images, possibly leading to
    valuable clinical applications: DA and physician training.

-    Medical Image Generation: This research provides how to exploit
    medical images with intrinsic intra-sequence variability towards
    GAN-based DA for medical imaging.

### 4.3 Materials and Methods

Towards clinical applications utilizing realistic brain MR images, we
generate synthetic brain MR images from the original samples using GANs.
Here, we compare the most used two GANs, DCGAN and WGAN, to find a
well-suited GAN between them for medical image generation—it must avoid
mode collapse and generate realistic MR images with high resolution.

#### 4.3.1 BRATS 2016 Dataset

This project exploits a dataset of multi-sequence brain MR images to
train GANs with sufficient data and resolution, which was originally
produced for the Multimodal Brain Tumor Image Segmentation Benchmark
(BRATS) Challenge [ 99 ] . In particular, the BRATS 2016 training
dataset contains 220 High-Grade Glioma (HGG) and 54 Low-Grade Glioma
(LGG) cases, with T1-weighted (T1), contrast enhanced T1-weighted (T1c),
T2-weighted, and FLAIR sequences—they were skull stripped and resampled
to isotropic @xmath resolution with @xmath voxels; among the different
sectional planes, we use sagittal multi-sequence scans of the HGG
patients to show that our GANs can generate a complete view of the whole
brain anatomy (allowing for visual consistency among the different brain
lobes), including also severe tumors for clinical purpose.

#### 4.3.2 DCGAN/WGAN-based Image Generation

Pre-processing We select the slices from @xmath to @xmath among the
whole @xmath slices to omit initial/final slices, since they convey a
negligible amount of useful information and could affect the training.
The images are resized to both @xmath / @xmath pixels from @xmath for
better GAN training (DCGAN architecture results in stable training on 64
@xmath 64 pixels [ 113 ] , and so @xmath is reasonably a
high-resolution). Fig. 4.2 shows example real MR images used for
training; each sequence contains 15,400 images with 220 patients @xmath
70 slices (61,600 in total).

MR Image Generation DCGAN and WGAN generate six types of images as
follows:

-   T1 sequence ( @xmath ) from the real T1;

-   T1c sequence ( @xmath ) from the real T1c;

-   T2 sequence ( @xmath ) from the real T2;

-   FLAIR sequence ( @xmath ) from the real FLAIR;

-   Concat sequence ( @xmath ) from concatenating the real T1, T1c, T2,
    and FLAIR (i.e., feeding the model with samples from all the MRI
    sequences);

-   Concat sequence ( @xmath ) from concatenating the real T1, T1c, T2,
    and FLAIR.

Concat sequence refers to a new ensemble sequence for an alternative DA,
containing features of all four sequences. We also generate @xmath
Concat images to compare the generation performance in terms of image
resolution.

DCGAN [ 113 ] is a standard GAN [ 39 ] with a convolutional architecture
for unsupervised learning; this generative model uses up-convolutions
interleaved with Rectified Lineaer Unit (ReLU) non-linearity and batch
normalization.
Let @xmath be a generating distribution over data @xmath . The generator
@xmath is a mapping to data space that takes a prior on input noise
variables @xmath , where @xmath is a neural network with parameters
@xmath . Similarly, the discriminator @xmath is a neural network with
parameters @xmath that takes either real data or synthetic data and
outputs a single scalar probability that @xmath came from the real data.
The discriminator @xmath maximizes the probability of classifying both
training examples and samples from @xmath correctly while the generator
@xmath minimizes the likelihood; it is formulated as a minimax
two-player game with value function @xmath :

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

This can be reformulated as the minimization of the Jensen-Shannon (JS)
divergence between the distribution @xmath and another distribution
@xmath derived from @xmath and @xmath .

DCGAN Implementation Details We use the same DCGAN architecture [ 113 ]
with no @xmath in the generator, ELU as the discriminator, all filters
of size @xmath , and a half channel size for DCGAN training. A batch
size of @xmath and Adam optimizer with @xmath learning rate were
implemented.

WGAN [ 7 ] is an alternative to traditional GAN training, as the JS
divergence is limited, such as when it is discontinuous; this novel GAN
achieves stable learning with less mode collapse by replacing it to the
Earth Mover (EM) distance (i.e., the Wasserstein-1 metrics):

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

where @xmath is the set of all joint distributions @xmath whose
marginals are @xmath and @xmath , respectively. In other words, @xmath
implies how much mass must be transported from one distribution to
another. This distance intuitively indicates the cost of the optimal
transport plan.

WGAN Implementation Details We use the same DCGAN architecture [ 113 ]
for WGAN training. A batch size of 64 and Root Mean Square Propagation
(RMSprop) optimizer with @xmath learning rate were implemented.

#### 4.3.3 Clinical Validation via Visual Turing Test

To quantitatively evaluate how realistic the synthetic images are, an
expert physician was asked to constantly classify a random selection of
@xmath real/ @xmath synthetic MR images as real or synthetic shown in
random order for each GAN/sequence, without previous training stages
revealing which is real/synthetic; Concat images were classified
together with real T1, T1c, T2, and FLAIR images in equal proportion.
The so-called Visual Turing Test [ 128 ] uses binary questions to probe
a human ability to identify attributes and relationships in images. For
these motivations, it is commonly used to evaluate GAN-generated images,
such as for SimGAN [ 135 ] . This applies also to medical images in
clinical environments [ 25 ] , wherein physicians’ expertise is
critical.

### 4.4 Results

This section shows how DCGAN and WGAN generate synthetic brain MR
images. The results include instances of synthetic images and their
quantitative evaluation of the realism by an expert physician.

#### 4.4.1 MR Images Generated by DCGAN/WGAN

DCGAN Fig. 4.3 illustrates examples of synthetic images by DCGAN. The
images look similar to the real samples. Concat images combine
appearances and patterns from all the four sequences used in training.
Since DCGAN’s value function could be unstable, it often generates
hyper-intense T1-like images analogous to mode collapse for 64 @xmath 64
Concat images, while sharing the same hyper-parameters with 128 @xmath
128.

WGAN Fig. 4.4 shows the example output of WGAN in each sequence.
Remarkably outperforming DCGAN, WGAN successfully captures the
sequence-specific texture and tumor appearance while maintaining the
realism of the original brain MR images. As expected, @xmath Concat
images tend to have more messy and unrealistic artifacts than @xmath
Concat ones, especially around boundaries of the brain, due to the
introduction of unexpected intensity patterns.

#### 4.4.2 Visual Turing Test Results

Table 4.1 shows the confusion matrix concerning the Visual Turing Test.
Even the expert physician found classifying real and synthetic images
challenging, especially in lower resolution due to their less detailed
appearances unfamiliar in clinical routine, even for highly
hyper-intense @xmath Concat images by DCGAN; distinguishing Concat
images was easier compared to the case of T1, T1c, T2, and FLAIR images
because the physician often felt odd from the artificial sequence. WGAN
succeeded to deceive the physician significantly better than DCGAN for
all the MRI sequences except FLAIR images ( @xmath to @xmath ).

### 4.5 Conclusion

Our preliminary results show that GANs, especially WGAN, can generate
@xmath realistic multi-sequence brain MR images that even an expert
physician is unable to accurately distinguish from the real, leading to
valuable clinical applications, such as DA and physician training. This
attributes to WGAN’s good generalization ability with a sharp value
function. In this context, DCGAN might be unsuitable due to both
inferior realism and mode collapse in terms of intensity. We only use
slices of interest in training to obtain desired MR images and generate
both original/Concat sequence images for DA in medical imaging.

This study confirms the synthetic image quality by the human expert
evaluation, but a more objective computational evaluation for GANs
should also follow, such as Classifier Two-Sample Tests (C2ST) [ 90 ] ,
which assesses whether two samples are drawn from the same distribution.
Currently this work uses sagittal MR images alone, so we plan to
generate coronal and transverse images. As this research uniformly
selects middle slices in pre-processing, better data generation demands
developing a classifier to only select brain MRI slices with/without
tumors.

Towards DA, whereas realistic images give more insights on
geometry/intensity transformations in classification, more realistic
images do not always assure better DA, so we have to find suitable image
resolutions and sequences; that is why we generate both high-resolution
images and Concat images, yet they looked more unrealistic for the
physician. For physician training, generating desired realistic tumors
by adding conditioning requires exploring latent spaces of GANs
extensively.

Overall, our novel GAN-based realistic brain MR image generation
approach sheds light on diagnostic and prognostic medical applications;
future studies on these applications are needed to confirm our
encouraging results.

## Chapter 5 GAN-based Medical Image Augmentation for 2D Classification

### 5.1 Prologue to Second Project

#### 5.1.1 Project Publications

-    Infinite Brain MR Images: PGGAN-based Data Augmentation for Tumor
    Detection .  C. Han , L. Rundo, R. Araki, Y. Furukawa, G. Mauri, H.
    Nakayama, H. Hayashi, In A. Esposito, M. Faundez-Zanuy, F. C.
    Morabito, E. Pasero (eds.) Neural Approaches to Dynamics of Signal
    Exchanges, Springer, pp. 291–303, September 2019.

-    Combining Noise-to-Image and Image-to-Image GANs: Brain MR Image
    Augmentation for Tumor Detection .  C. Han , L. Rundo, R. Araki, Y.
    Nagano, Y. Furukawa, G. Mauri, H. Nakayama, H. Hayashi, IEEE Access
    , pp. 156966–156977, October 2019.

#### 5.1.2 Context

At the time we wrote the former paper, high-resolution (e.g., 256 @xmath
256) medical image generation using noise-to-image GANs had been
challenging [ 91 ] while most CNN architectures adopt around 256 @xmath
256 input sizes (e.g., InceptionResNetV2 [ 141 ] : @xmath , ResNet-50 [
54 ] : @xmath ). Moreover, prior to the latter paper, analysis had been
immature on GAN-generated additional training images for better
CNN-based classification.

#### 5.1.3 Contributions

This project’s core contribution is to firstly combine noise-to-image
and image-to-image GANs for improved 2D classification. The former paper
adopts a noise-to-image GAN called PGGANs to generate realistic/diverse
original-sized 256 @xmath 256 whole brain MR images with/without tumors
separately; additionally, the latter paper exploits an image-to-image
GAN called MUNIT to further refine the synthetic images’ texture and
shape similarly to real ones. By so doing, our two-step GAN-based DA
boosts sensitivity 93.7% to 97.5% in tumor/non-tumor classification.
Moreover, we firstly analyze how medical GAN-based DA is associated with
pre-training on ImageNet and discarding weird-looking synthetic images
to humans to achieve high sensitivity. A physician classifies a few
synthetic images as real in Visual Turing Test despite the high
resolution.

#### 5.1.4 Recent Developments

Since the former paper’s acceptance (the book chapter’s publication
process took more than a year), to improve 2D classification, Konidaris
et al. generated @xmath brain MR images with Alzheimer’s disease using
the noise-to-image GAN [ 77 ] . No more recent developments to report
exist for the latter paper because it is very recent.

### 5.2 Motivation

CNNs are playing a key role in Medical Image Analysis, updating the
state-of-the-art in many tasks [ 53 , 124 , 73 ] when large-scale
annotated training data are available. However, preparing such massive
medical data is demanding; thus, for better diagnosis, researchers
generally adopt classic DA techniques, such as geometric/intensity
transformations of original images [ 120 , 100 ] . Those augmented
images, however, intrinsically have a similar distribution to the
original ones, resulting in limited performance improvement. In this
sense, GAN-based DA can considerably increase the performance [ 39 ] ;
since the generated images are realistic but completely novel samples,
they can relieve the sampling biases and fill the real image
distribution uncovered by the original dataset [ 159 ] .

The main problem in CAD lies in small/fragmented medical imaging
datasets from multiple scanners; thus, researchers have improved
classification by augmenting images with noise-to-image GANs [ 45 ] or
image-to-image GANs [ 152 ] . However, no research has achieved further
performance boost by combining noise-to-image and image-to-image GANs.

So, how can we maximize the DA effect under limited training images
using the GAN combinations? To generate and refine brain MR images
with/without tumors separately (Fig. 5.1 ), we propose a two-step
GAN-based DA approach: ( i ) PGGANs [ 72 ] , low-to-high resolution
noise-to-image GAN, first generates realistic/diverse @xmath images—the
PGGANs helps DA since most CNN architectures adopt around @xmath input
sizes (e.g., InceptionResNetV2 [ 141 ] : @xmath , ResNet-50 [ 54 ] :
@xmath ); ( ii ) MUNIT [ 61 ] that combines GANs/VAEs [ 75 ] or SimGAN [
135 ] that uses a DA-focused GAN loss , further refines the texture and
shape of the PGGAN-generated images to fit them into the real image
distribution. Since training a single sophisticated GAN system is
already difficult, instead of end-to-end training, we adopt a two-step
approach for performance boost via an ensemble generation process from
those state-of-the-art GANs’ different algorithms.

We thoroughly investigate CNN-based tumor classification results, also
considering the influence of pre-training on ImageNet [ 127 ] and
discarding weird-looking GAN-generated images. Moreover, we evaluate the
synthetic images’ appearance via Visual Turing Test [ 128 ] by an expert
physician, and visualize the data distribution of real/synthetic images
via t-Distributed Stochastic Neighbor Embedding (t-SNE) [ 144 ] . When
combined with classic DA, our two-step GAN-based DA approach
significantly outperforms the classic DA alone, boosting sensitivity
@xmath to @xmath .

Research Questions. We mainly address two questions:

-    GAN Selection: Which GAN architectures are well-suited for
    realistic/diverse medical image generation?

-    Medical DA: How to use GAN-generated images as additional training
    data for better CNN-based diagnosis?

Contributions. Our main contributions are as follows:

-    Whole Image Generation: This research shows that PGGANs can
    generate realistic/diverse @xmath whole medical images —not only
    small pathological sub-areas—and MUNIT can further refine their
    texture and shape similarly to real ones .

-    Two-step GAN-based DA: This novel two-step approach, combining for
    the first time noise-to-image and image-to-image GANs, significantly
    boosts tumor classification sensitivity .

-    Misdiagnosis Prevention: This study firstly analyzes how medical
    GAN-based DA is associated with pre-training on ImageNet and
    discarding weird-looking synthetic images to achieve high
    sensitivity with small and fragmented datasets.

### 5.3 Materials and Methods

#### 5.3.1 BRATS 2016 Dataset

We use a dataset of @xmath T1c brain axial MR images of @xmath HGG cases
from BRATS 2016 [ 99 ] . T1c is the most common sequence in tumor
classification thanks to its high-contrast [ 76 ] .

#### 5.3.2 PGGAN-based Image Generation

Pre-processing For better GAN/ResNet-50 training, we select the slices
from @xmath to @xmath among the whole @xmath slices to omit
initial/final slices, which convey negligible useful information; also,
since tumor/non-tumor annotation in the BRATS 2016 dataset, based on 3D
volumes, is highly incorrect/ambiguous on 2D slices, we exclude ( @xmath
) tumor images tagged as non-tumor, ( @xmath ) non-tumor images tagged
as tumor, ( @xmath ) borderline images with unclear tumor/non-tumor
appearance, and ( @xmath ) images with missing brain parts due to the
skull-stripping procedure. For tumor classification, we divide the whole
dataset ( @xmath patients) into:

-   Training set
    ( @xmath patients/ @xmath tumor/ @xmath non-tumor images);

-   Validation set
    ( @xmath patients/ @xmath tumor/ @xmath non-tumor images);

-   Test set
    ( @xmath patients/ @xmath tumor/ @xmath non-tumor images).

During the GAN training, we only use the training set to be fair; for
better PGGAN training, the training set images are zero-padded to reach
a power of @xmath : @xmath pixels from @xmath . Fig. 5.2 shows example
real MR images.

PGGANs [ 72 ] is a GAN training method that progressively grows a
generator and discriminator: starting from low resolution, new layers
model details as training progresses. This study adopts the PGGANs to
synthesize realistic/diverse @xmath brain MR images (Fig. 5.3 ); we
train and generate tumor/non-tumor images separately.

PGGAN Implementation Details The PGGAN architecture adopts the
Wasserstein loss with Gradient Penalty (WGAN-GP) [ 42 ] :

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

where @xmath denotes the expected value, the discriminator @xmath (i.e.,
the set of @xmath -Lipschitz functions) , @xmath is the data
distribution defined by the true data sample @xmath , and @xmath is the
model distribution defined by the generated sample @xmath ( @xmath is
the input noise @xmath to the generator sampled from a Gaussian
distribution) . A gradient penalty is added for the random sample @xmath
, where @xmath is the gradient operator towards the generated samples
and @xmath is the gradient penalty coefficient.

We train the model (Table LABEL:tab5_1 ) for @xmath epochs with a batch
size of @xmath and @xmath learning rate for the Adam optimizer (the
exponential decay rates @xmath ) [ 74 ] . All experiments use @xmath
with @xmath critic iteration per generator iteration. During training,
we apply random cropping in @xmath - @xmath pixels as DA.

#### 5.3.3 MUNIT/SimGAN-based Image Refinement

Refinement Using resized @xmath images for ResNet-50, we further refine
the texture and shape of PGGAN-generated tumor/non-tumor images
separately to fit them into the real image distribution using MUNIT [ 61
] or SimGAN [ 135 ] . SimGAN remarkably improved eye gaze estimation
results after refining non-GAN-based synthetic images from the UnityEyes
simulator @xmath image-to-image translation; thus, we also expect such
performance improvement after refining synthetic images from a
noise-to-image GAN (i.e., PGGANs) @xmath an image-to-image GAN (i.e.,
MUNIT /SimGAN) with considerably different GAN algorithms.

We randomly select @xmath real/ @xmath PGGAN-generated tumor images for
tumor image training, and we perform the same for non-tumor image
training. To find suitable refining steps for each architecture, we pick
the MUNIT /SimGAN models with the highest accuracy on tumor
classification validation, when pre-trained and combined with classic
DA, among @xmath / @xmath / @xmath steps, respectively.

MUNIT [ 61 ] is an image-to-image GAN based on both
auto-encoding/translation; it extends UNIT [ 89 ] to increase the
generated images’ realism/diversity via a stochastic model representing
continuous output distributions.

MUNIT Implementation Details The MUNIT architecture adopts the following
loss:

  -- -------- -- -------- -------- -------
     @xmath      @xmath            (5.2)
                 @xmath   @xmath   
  -- -------- -- -------- -------- -------

where @xmath denotes the loss function. Using the multiple encoders
@xmath / @xmath , generators @xmath / @xmath , discriminators @xmath /
@xmath , cycle-consistencies CC @xmath /CC @xmath , and domain-invariant
perceptions VGG @xmath /VGG @xmath [ 137 ] , this framework jointly
solves learning problems of the VAE @xmath /VAE @xmath and GAN @xmath
/GAN @xmath for the image reconstruction streams, image translation
streams, cycle-consistency reconstruction streams, and domain-invariant
perception streams. Since we do not need the style loss for our
experiments, instead of the MUNIT loss, we use the UNIT loss with the
perceptual loss for the MUNIT architecture (as in the UNIT authors’
GitHub repository). The MUNIT architecture adopts the following loss:

  -- -------- -- -------
     @xmath      
     @xmath      (5.3)
  -- -------- -- -------

We train the model (Table LABEL:tab5_2 ) for @xmath steps with a batch
size of 1 and @xmath learning rate for the Adam optimizer ( @xmath ) [
74 ] . The learning rate is reduced by half every @xmath steps. We use
the following MUNIT weights: the adversarial loss weight @xmath ; the
image reconstruction loss weight @xmath ; the Kullback-Leibler (KL)
divergence loss weight for reconstruction @xmath ; the cycle consistency
loss weight @xmath ; the KL divergence loss weight for cycle consistency
@xmath ; the domain-invariant perceptual loss weight @xmath ; the Least
Squares GAN objective function for the discriminators [ 94 ] . During
training, we apply horizontal flipping as DA.

SimGAN [ 135 ] is an image-to-image GAN designed for DA that adopts the
self-regularization term/local adversarial loss; it updates a
discriminator with a history of refined images.

SimGAN Implementation Details The SimGAN architecture (i.e., a refiner)
uses the following loss:

  -- -- -- -------
           (5.4)
  -- -- -- -------

where @xmath denotes the loss function, @xmath is the function
parameters, @xmath is the @xmath PGGAN-generated training image, and
@xmath is the set of the real images @xmath . The first part @xmath adds
realism to the synthetic images using a discriminator , while the second
part @xmath preserves the tumor/non-tumor features.

We train the model (Table LABEL:tab5_3 ) for @xmath steps with a batch
size of 10 and @xmath learning rate for the Stochastic Gradient Descent
(SGD) optimizer [ 14 ] without momentum . The learning rate is reduced
by half at 15,000 steps. We train the refiner first with just the
self-regularization loss with @xmath for @xmath steps; then, for each
update of the discriminator, we update the refiner @xmath times. During
training, we apply horizontal flipping as DA.

#### 5.3.4 ResNet-50-based Tumor Classification

Pre-processing As ResNet-50’s input size is @xmath pixels, we resize the
whole real images from @xmath and whole PGGAN-generated images from
@xmath .

ResNet-50 [ 54 ] is a @xmath -layer residual learning-based CNN. We
adopt it to conduct tumor/non-tumor binary classification on MR images
due to its outstanding performance in image classification tasks [ 13 ]
, including binary classification [ 158 ] . Chang et al. [ 19 ] also
used a similar @xmath -layer residual convolutional network for the
binary classification of brain tumors (i.e., determining the Isocitrate
Dehydrogenase status in LGG/HGG).

DA Setups To confirm the effect of PGGAN-based DA and its refinement
using MUNIT/SimGAN, we compare the following @xmath DA setups under
sufficient images both with/without ImageNet [ 127 ] pre-training (i.e.,
20 DA setups):

1.  @xmath real images;

2.  + @xmath k classic DA;

3.  + @xmath k classic DA;

4.  + @xmath k PGGAN-based DA;

5.  + @xmath k PGGAN-based DA w/o clustering/discarding;

6.  + @xmath k classic DA & @xmath k PGGAN-based DA;

7.  + @xmath k MUNIT -refined DA;

8.  + @xmath k classic DA & @xmath k MUNIT -refined DA;

9.  + @xmath k SimGAN-refined DA;

10. + @xmath k classic DA & @xmath k SimGAN-refined DA.

Due to the risk of overlooking the tumor diagnosis, higher sensitivity
matters much more than higher specificity [ 95 ] ; thus, we aim to
achieve higher sensitivity, using the additional synthetic training
images. We perform McNemar’s test on paired tumor classification results
[ 97 ] to confirm our two-step GAN-based DA’s statistically-significant
sensitivity improvement; since this statistical analysis involves
multiple comparison tests, we adjust their @xmath -values using the
Holm-Bonferroni method [ 56 ] .

Whereas medical imaging researchers widely use the ImageNet
initialization despite different textures of natural/medical images,
recent study found that such ImageNet-trained CNNs are biased towards
recognizing texture rather than shape [ 36 ] ; thus, we aim to
investigate how the medical GAN-based DA affects classification
performance with/without the pre-training. As the classic DA, we adopt a
random combination of horizontal/vertical flipping, rotation up to
@xmath degrees, width/height shift up to @xmath , shearing up to @xmath
, zooming up to @xmath , and constant filling of points outside the
input boundaries (Fig. 5.4 ). For the PGGAN-based DA and its refinement,
we only use success cases after discarding weird-looking synthetic
images (Fig. 5.5 ); DenseNet-169 [ 65 ] extracts image features and
k-means++ [ 9 ] clusters the features into @xmath groups, and then we
manually discard each cluster containing similar weird-looking images.
To verify its effect, we also conduct a PGGAN-based DA experiment
without the discarding step. Additionally, to confirm the effect of
changing training data set sizes, we compare classification results with
pre-training on @xmath / @xmath / @xmath / @xmath real images vs real
images + @xmath k classic DA vs real images + @xmath k classic DA &
@xmath k PGGAN-based DA (i.e., @xmath setups).

ResNet-50 Implementation Details The ResNet-50 architecture adopts the
binary cross-entropy loss for binary classification both with/without
ImageNet pre-training. As shown in Table LABEL:tab5_4 , for robust
training, before the final sigmoid layer, we introduce a @xmath dropout
[ 139 ] , linear dense, and batch normalization [ 66 ] layers—training
with GAN-based DA tends to be unstable especially without the batch
normalization layer. We use a batch size of @xmath , @xmath learning
rate for the SGD optimizer [ 14 ] with @xmath momentum, and early
stopping of @xmath epochs. The learning rate was multiplied by @xmath
every @xmath epochs for the training from scratch and by @xmath every
@xmath epochs for the ImageNet pre-training.

#### 5.3.5 Clinical Validation via Visual Turing Test

To quantify the ( i ) realism of @xmath synthetic images by PGGANs,
MUNIT, and SimGAN against real images respectively (i.e., 3 setups) and
( ii ) clearness of their tumor/non-tumor features, we supply, in random
order, to an expert physician a random selection of:

-   @xmath real tumor images;

-   @xmath real non-tumor images;

-   @xmath synthetic tumor images;

-   @xmath synthetic non-tumor images.

Then, the physician is asked to classify them as both ( i )
real/synthetic and ( ii ) tumor/non-tumor, without previously knowing
which is real/synthetic and tumor/non-tumor.

#### 5.3.6 Visualization via t-SNE

To visualize distributions of geometrically-transformed and each
GAN-based @xmath images by PGGANs, MUNIT , and SimGAN against real
images respectively (i.e., 4 setups), we adopt t-SNE [ 144 ] on a random
selection of:

-   @xmath real tumor images;

-   @xmath real non-tumor images;

-   @xmath geometrically-transformed or each GAN-based tumor images;

-   @xmath geometrically-transformed or each GAN-based non-tumor images.

We select only @xmath images per each category for better visualization.
The t-SNE method reduces the dimensionality to represent
high-dimensional data into a lower-dimensional (2D/3D) space; it
non-linearly balances between the input data’s local and global aspects
using perplexity.

T-SNE Implementation Details The t-SNE uses a perplexity of @xmath for
@xmath iterations to visually represent a 2D space. We input the images
after normalizing pixel values to @xmath . For point locations of the
real images, we compress all the images simultaneously and plot each
setup (i.e., the geometrically-transformed or each GAN-based images
against the real ones) separately; we maintain their locations by
projecting all the data onto the same subspace.

### 5.4 Results

This section shows how PGGANs generates synthetic brain MR images and
how MUNIT and SimGAN refine them. The results include instances of
synthetic images, their quantitative evaluation by a physician, their
t-SNE visualization, and their influence on tumor classification.

#### 5.4.1 MR Images Generated by PGGANs

Fig. 5.5 illustrates examples of synthetic MR images by PGGANs. We
visually confirm that, for about @xmath of cases, it successfully
captures the T1c-specific texture and tumor appearance, while
maintaining the realism of the original brain MR images; but, for the
rest @xmath , the generated images lack clear tumor/non-tumor features
or contain unrealistic features (i.e., hyper-intensity, gray contours,
and odd artifacts).

#### 5.4.2 MR Images Refined by MUNIT/SimGAN

MUNIT and SimGAN differently refine PGGAN-generated images—they render
the texture and contours while maintaining the overall shape (Fig. 5.6
). Non-tumor images change more remarkably than tumor images for both
MUNIT and SimGAN; it probably derives from unsupervised image
translation’s loss for consistency to avoid image collapse, resulting in
conservative change for more complicated images.

#### 5.4.3 Tumor Classification Results

Table LABEL:tab5_5 shows the brain tumor classification results
with/without DA while Table LABEL:tab5_6 indicates their pairwise
comparison ( @xmath -values between our two-step GAN-based DA setups and
the other DA setups) using McNemar’s test. ImageNet pre-training
generally outperforms training from scratch despite different image
domains (i.e., natural images to medical images). As expected, classic
DA remarkably improves classification, while no clear difference exists
between the @xmath / @xmath classic DA under sufficient
geometrically-transformed training images. When pre-trained, each
GAN-based DA (i.e., PGGANs/ MUNIT /SimGAN) alone helps classification
due to the robustness from GAN-generated images; but, without
pre-training, it harms classification due to the biased initialization
from the GAN-overwhelming data distribution. Similarly, without
pre-training, PGGAN-based DA without clustering/discarding causes poor
classification due to the synthetic images with severe artifacts, unlike
the PGGAN-based DA’s comparable results with/without the discarding step
when pre-trained.

When combined with the classic DA, each GAN-based DA remarkably
outperforms the GAN-based DA or classic DA alone in terms of sensitivity
since they are mutually-complementary: the former learns the non-linear
manifold of the real images to generate novel local tumor features
(since we train tumor/non-tumor images separately) strongly associated
with sensitivity; the latter learns the geometrically-transformed
manifold of the real images to cover global features and provide the
robustness on training for most cases. We confirm that test samples,
originally-misclassified but correctly classified after DA, are
obviously different for the GAN-based DA and classic DA; here, both
image-to-image GAN-based DA, especially MUNIT , produce remarkably
higher sensitivity than the PGGAN-based DA after refinement. Specificity
is higher than sensitivity for every DA setup with pre-training,
probably due to the training data imbalance; but interestingly, without
pre-training, sensitivity is higher than specificity for both
image-to-image GAN-based DA since our tumor classification-oriented
two-step GAN-based DA can fill the real tumor image distribution
uncovered by the original dataset under no ImageNet initialization.
Accordingly, when combined with the classic DA, the MUNIT -based DA
based on both GANs/VAEs achieves the highest sensitivity @xmath against
the best performing classic DA’s @xmath , allowing to significantly
alleviate the risk of overlooking the tumor diagnosis; in terms of
sensitivity, it outperforms all the other DA setups, including two-step
DA setups, with statistical significance.

Figure 5.7 shows that the PGGAN-based DA, even without further
refinement, could moderately increase both accuracy/sensitivity on top
of the classic DA in tumor classification; it achieves considerably high
sensitivity with only @xmath / @xmath of the real training images.
However, it should be noted that the MUNIT-based DA could outperform the
PGGAN-based DA in return for more computational power.

#### 5.4.4 Visual Turing Test Results

Table 5.7 indicates the confusion matrix for the Visual Turing Test. The
expert physician classifies a few PGGAN-generated images as real ,
thanks to their realism, despite high resolution (i.e., @xmath pixels);
meanwhile, the expert classifies less GAN-refined images as real due to
slight artifacts induced during refinement. The synthetic images
successfully capture tumor/non-tumor features; unlike the non-tumor
images, the expert recognizes a considerable number of the mild/modest
tumor images as non-tumor for both real/synthetic cases. It derives from
clinical tumor diagnosis relying on a full 3D volume, instead of a
single 2D slice.

#### 5.4.5 T-SNE Results

As Fig. 5.8 represents, the real tumor/non-tumor image distributions
largely overlap while the non-tumor images distribute wider. The
geometrically-transformed tumor/non-tumor image distributions also often
overlap, and both images distribute wider than the real ones. All
GAN-based synthetic images by PGGANs, MUNIT, and SimGAN distribute
widely, while their tumor/non-tumor images overlap much less than the
geometrically-transformed ones (i.e., a high discrimination ability
associated with sensitivity improvement) ; the MUNIT -refined images
show better tumor/non-tumor discrimination and a more similar
distribution to the real ones than the PGGAN-based and SimGAN-based
images. This trend derives from the MUNIT ’s loss function adopting both
GANs/VAEs that further fits the PGGAN-generated images into the real
image distribution by refining their texture and shape; contrarily, this
refinement could also induce slight human-recognizable but DA-irrelevant
artifacts . Overall, the GAN-based images, especially the MUNIT -refined
images, fill the distribution uncovered by the real or
geometrically-transformed ones with less tumor/non-tumor overlap; this
demonstrates the superiority of combining classic DA and GAN-based DA .

### 5.5 Conclusion

Visual Turing Test and t-SNE results show that PGGANs, multi-stage
noise-to-image GAN, can generate realistic/diverse @xmath brain MR
images with/without tumors separately. Unlike classic DA that
geometrically covers global features and provides the robustness on
training for most cases, the GAN-generated images can non-linearly cover
local tumor features with much less tumor/non-tumor overlap; thus,
combining them can significantly boost tumor classification sensitivity
—especially after refining them with MUNIT or SimGAN, image-to-image
GANs; thanks to an ensemble generation process from those GANs’
different algorithms, the texture/shape- refined images can replace
missing data points of the training set with less tumor/non-tumor
overlap, and thus handle the data imbalance by regularizing the model
(i.e., improved generalization). Notably, MUNIT remarkably outperforms
SimGAN in terms of sensitivity , probably due to the effect of combining
both GANs/VAEs.

Regarding better medical GAN-based DA, ImageNet pre-training generally
improves classification despite different textures of natural/medical
images; but, without pre-training, the GAN-refined images may help
achieve better sensitivity, allowing to alleviate the risk of
overlooking the tumor diagnosis —this attributes to our tumor
classification-oriented two-step GAN-based DA’s high discrimination
ability to fill the real tumor image distribution under no ImageNet
initialization. GAN-generated images typically include odd artifacts;
however, only without pre-training, discarding them boosts DA
performance.

Overall, by minimizing the number of annotated images required for
medical imaging tasks, the two-step GAN-based DA can shed light not only
on classification, but also on object detection [ 47 ] and segmentation
[ 132 ] . Moreover, other potential medical applications exist: ( i ) A
data anonymization tool to share patients’ data outside their
institution for training without losing classification performance [ 132
] ; ( ii ) A physician training tool to show random pathological images
for medical students/radiology trainees despite infrastructural/legal
constraints [ 31 ] . As future work, we plan to define a new end-to-end
GAN loss function that explicitly optimizes the classification results,
instead of optimizing visual realism while maintaining diversity by
combining the state-of-the-art noise-to-image and image-to-image GANs;
towards this, we might extend a preliminary work on a three-player GAN
for classification [ 146 ] to generate only hard-to-classify samples to
improve classification; we could also ( i ) explicitly model deformation
fields/intensity transformations and ( ii ) leverage unlabeled data
during the generative process [ 18 ] to effectively fill the real image
distribution.

## Chapter 6 GAN-based Medical Image Augmentation for 2D Detection

### 6.1 Prologue to Third Project

#### 6.1.1 Project Publication

-    Learning More with Less: Conditional PGGAN-based Data Augmentation
    for Brain Metastases Detection Using Highly-Rough Annotation on MR
    Images .  C. Han , K. Murao, T. Noguchi, Y. Kawata, F. Uchiyama, L.
    Rundo, H. Nakayama, S. Satoh, In ACM International Conference on
    Information and Knowledge Management (CIKM), Beijing, China, pp.
    119–127, November 2019.

#### 6.1.2 Context

Further DA applications require pathology localization for detection and
advanced physician training needs atypical image generation,
respectively. To meet both clinical demands, developing pathology-aware
GANs (i.e., GANs conditioned on pathology position and appearance) is
the best solution—the pathology-aware GANs are promising in terms of
extrapolation because common and/or desired medical priors can play a
key role in the conditioning [ 140 ] . However, prior to this work,
researchers had focused only on improving segmentation, instead of
bounding box-based detection, while the detection requires much less
physicians’ annotation effort [ 132 , 70 ] . Moreover, they had relied
on image-to-image GANs, instead of noise-to-image GANs, which sacrifices
image diversity due to an input benign image.

#### 6.1.3 Contributions

This project’s fundamental contribution is to propose a novel
pathology-aware noise-to-image GAN called CPGGANs for improved 2D
bounding box-based detection; it incorporates highly-rough bounding box
conditions incrementally into the noise-to-image GAN (i.e., PGGANs) to
place realistic/diverse brain metastases at desired positions/sizes on
@xmath MR images. By so doing, our CPGGAN-based DA boosts sensitivity
83% to 91% with Intersection over Union (IoU) threshold @xmath in tumor
detection with clinically acceptable additional False Positives (FPs).
Moreover, we find that GAN training on additional normal images could
increase synthetic images’ realism, including pathology, but decrease DA
performance.

#### 6.1.4 Recent Developments

Almost simultaneously, Kanayama et al. also tackle bounding box-based
pathology detection using the image-to-image GAN, instead of the
noise-to-image GAN [ 71 ] ; they translated normal endoscopic images
with various image sizes ( @xmath on average) into gastric cancer ones
by inputting both a benign image and a black image (i.e., pixel value:
0) with a specific lesion ROI at desired position.

### 6.2 Motivation

Accurate CAD with high sensitivity can alleviate the risk of overlooking
the diagnosis in a clinical environment. Specifically, CNNs have
revolutionized medical imaging, such as diabetic eye disease diagnosis [
43 ] , mainly thanks to large-scale annotated training data. However,
obtaining such annotated medical big data is demanding; thus, better
diagnosis requires intensive DA techniques, such as geometric/intensity
transformations of original images [ 120 , 100 ] . Yet, those augmented
images intrinsically have a similar distribution to the original ones,
leading to limited performance improvement; in this context, GAN [ 39 ]
-based DA can boost the performance by filling the real image
distribution uncovered by the original dataset, since it generates
realistic but completely new samples showing good generalization
ability; GANs achieved outstanding performance in computer vision,
including @xmath performance improvement in eye-gaze estimation [ 135 ]
.

Also in medical imaging, where the primary problem lies in small and
fragmented imaging datasets from various scanners [ 124 ] , GAN-based DA
performs effectively: researchers improved classification by
augmentation with noise-to-image GANs [ 34 ] and segmentation with
image-to-image GANs [ 132 , 70 ] . Such applications include @xmath
brain MR image generation for tumor/non-tumor classification [ 49 ] .
Nevertheless, unlike bounding box-based object detection, simple
classification cannot locate disease areas and rigorous segmentation
requires physicians’ expensive annotation.

So, how can we achieve high sensitivity in diagnosis using GANs with
minimum annotation cost, based on highly-rough and inconsistent bounding
boxes? We aim to generate GAN-based realistic and diverse @xmath brain
MR images with brain metastases at desired positions/sizes for accurate
CNN-based tumor detection (Fig. 6.1 ); this is clinically valuable for
better diagnosis, prognosis, and treatment, since brain metastases are
the most common intra-cranial tumors, getting prevalent as oncological
therapies improve cancer patients’ survival [ 10 ] . Conventional GANs
cannot generate realistic @xmath whole brain MR images conditioned on
tumor positions/sizes under limited training data/highly-rough
annotation [ 49 ] ; since noise-to-image GANs cannot directly be
conditioned on an image describing desired objects, we have to use
image-to-image GANs (e.g., input both conditioning image/random noise
samples or the conditioning image alone with dropout noises [ 139 ] on a
generator [ 67 ] )—it results in unrealistic high-resolution MR images
with odd artifacts due to the limited training data/rough annotation,
tumor variations, and strong consistency in brain anatomy, unless we
also input a benign image sacrificing image diversity.

Such a high-resolution whole image generation approach, not involving
ROIs alone, however, could facilitate detection because it provides more
image details and most CNN architectures adopt around @xmath input
pixels. Therefore, as a conditional noise-to-image GAN not relying on an
input benign image, we propose CPGGANs, incorporating highly-rough
bounding box conditions incrementally into PGGANs [ 72 ] to naturally
place tumors of random shape at desired positions/sizes on MR images.
Moreover, we evaluate the generated images’ realism via Visual Turing
Test [ 128 ] by three expert physicians, and visualize the data
distribution via t-SNE algorithm [ 144 ] . Using the synthetic images,
our novel CPGGAN-based DA boosts @xmath sensitivity in diagnosis with
clinically acceptable additional FPs. Surprisingly, we confirm that
further realistic tumor appearance, judged by the physicians, does not
contribute to detection performance.

Research Questions. We mainly address two questions:

-    PGGAN Conditioning: How can we modify PGGANs to naturally place
    objects of random shape, unlike rigorous segmentation, at desired
    positions/sizes based on highly-rough bounding box masks?

-    Medical DA: How can we balance the number of real and additional
    synthetic training data to achieve the best detection performance?

Contributions. Our main contributions are as follows:

-    Conditional Image Generation: As the first bounding box-based
    @xmath whole pathological image generation approach, CPGGANs can
    generate realistic/diverse images with objects naturally at desired
    positions/sizes; the generated images can play a vital role in
    clinical oncology applications, such as DA, data anonymization, and
    physician training.

-    Misdiagnosis Prevention: This study allows us to achieve high
    sensitivity in automatic CAD using small/fragmented medical imaging
    datasets with minimum annotation efforts based on
    highly-rough/inconsistent bounding boxes.

-    Brain Metastases Detection: This first bounding box-based brain
    metastases detection method successfully detects tumors with
    CPGGAN-based DA.

### 6.3 Materials and Methods

#### 6.3.1 Brain Metastases Dataset

As a new dataset for the first bounding box-based brain metastases
detection, this project uses a dataset of T1c brain axial MR images,
collected by the authors (National Center for Global Health and
Medicine, Tokyo, Japan) and currently not publicly available for ethical
restrictions; for robust clinical applications, it contains @xmath brain
metastatic cancer cases from multiple MRI scanners—those images differ
in contrast, magnetic field strength (i.e., @xmath T, @xmath T), and
matrix size (i.e., @xmath , @xmath , @xmath , @xmath pixels). We also
use additional brain MR images from @xmath normal subjects only for
CPGGAN training, not in tumor detection, to confirm the effect of
combining the normal and pathological images for training.

#### 6.3.2 CPGGAN-based Image Generation

Data Preparation For tumor detection, our whole brain metastases dataset
( @xmath patients) is divided into: ( i ) a training set ( @xmath
patients); ( ii ) a validation set ( @xmath patients); ( iii ) a test
set ( @xmath patients); only the training set is used for GAN training
to be fair. Our experimental dataset consists of:

-   Training set ( @xmath images/ @xmath bounding boxes);

-   Validation set ( @xmath images/ @xmath bounding boxes);

-   Test set ( @xmath images/ @xmath bounding boxes).

Our training set is relatively small/fragmented for CNN-based
applications, considering that the same patient’s tumor slices could
convey very similar information. To confirm the effect of realism and
diversity—provided by combining PGGANs and bounding box conditioning—on
tumor detection, we compare the following GANs: ( i ) CPGGANs trained
only with the brain metastases images; ( ii ) CPGGANs trained also with
additional @xmath brain images from @xmath normal subjects; ( iii )
Image-to-image GAN trained only with the brain metastases images. After
skull-stripping on all images with various resolution, remaining brain
parts are cropped and resized to @xmath pixels (i.e., a power of @xmath
for better GAN training). As Fig. 6.2 shows, we lazily annotate tumors
with highly-rough and inconsistent bounding boxes to minimize expert
physicians’ labor.

CPGGANs is a novel conditional noise-to-image training method for GANs,
incorporating highly-rough bounding box conditions incrementally into
PGGANs [ 72 ] , unlike conditional image-to-image GANs requiring
rigorous segmentation masks [ 11 ] . The original PGGANs exploits a
progressively growing generator and discriminator: starting from
low-resolution, newly-added layers model fine-grained details as
training progresses. As Fig. 6.3 shows, we further condition the
generator and discriminator to generate realistic and diverse @xmath
brain MR images with tumors of random shape at desired positions/sizes
using only bounding boxes without an input benign image under limited
training data/highly-rough annotation. Our modifications to the original
PGGANs are as follows:

-   Conditioning image: prepare a @xmath black image (i.e., pixel value:
    @xmath ) with white bounding boxes (i.e., pixel value: @xmath )
    describing tumor positions/sizes for attention;

-   Generator input: resize the conditioning image to the previous
    generator’s output resolution/channel size and concatenate them
    (noise samples generate the first @xmath images);

-   Discriminator input: concatenate the conditioning image with a real
    or synthetic image.

CPGGAN Implementation Details We use the CPGGAN architecture with the
WGAN-GP loss [ 42 ] :

  -- -------- -- -------
     @xmath      (6.1)
  -- -------- -- -------

where the discriminator @xmath belongs to the set of @xmath -Lipschitz
functions, @xmath is the data distribution by the true data sample
@xmath , and @xmath is the model distribution by the synthetic sample
@xmath generated from the conditioning image uniform noise samples in
@xmath . The last term is gradient penalty for the random sample @xmath
.

Training lasts for @xmath steps with a batch size of @xmath and @xmath
learning rate for the Adam optimizer [ 74 ] . We flip the
discriminator’s real/synthetic labels once in three times for
robustness. During testing, as tumor attention images, we use the
annotation of training images with a random combination of
horizontal/vertical flipping, width/height shift up to @xmath , and
zooming up to @xmath ; these CPGGAN-generated images are used as
additional training images for tumor detection.

Image-to-image GAN is a conventional conditional GAN that generates
brain MR images with tumors, concatenating a @xmath conditioning image
with noise samples for a generator input and concatenating the
conditioning image with a real/synthetic image for a discriminator
input, respectively. It uses a U-Net-like [ 120 ] generator with @xmath
convolutional/deconvolutional layers in encoders/decoders respectively
with skip connections, along with a discriminator with @xmath decoders.
We apply batch normalization [ 66 ] to both convolution with LeakyReLU
and deconvolution with ReLU. It follows the same implementation details
as for the CPGGANs.

#### 6.3.3 YOLOv3-based Brain Metastases Detection

You Only Look Once v3 (YOLOv3) [ 116 ] is a fast/accurate CNN-based
object detector: unlike conventional classifier-based detectors, it
divides the image into regions and predicts bounding boxes/probabilities
for each region. We adopt YOLOv3 to detect brain metastases since its
high efficiency can play a clinical role in real-time tumor alert;
moreover, it shows very comparable results with @xmath network
resolution against other state-of-the-art detectors, such as Faster RCNN
[ 118 ] .

To confirm the effect of GAN-based DA, the following detection results
are compared: ( i ) @xmath real images without DA, ( ii ), ( iii ), ( iv
) with @xmath / @xmath / @xmath CPGGAN-based DA, ( v ), ( vi ), ( vii )
with @xmath / @xmath / @xmath CPGGAN-based DA, trained with additional
normal brain images, ( viii ), ( ix ), ( x ) with @xmath / @xmath /
@xmath image-to-image GAN-based DA. Due to the risk of overlooking the
diagnosis @xmath medical imaging, higher sensitivity matters more than
less FPs; thus, we aim to achieve higher sensitivity with a clinically
acceptable number of FPs, adding the additional synthetic training
images. Since our annotation is highly-rough, we calculate
sensitivity/FPs per slice with both IoU threshold 0.5 and 0.25.

YOLOv3 Implementation Details We use the YOLOv3 architecture with
Darknet-53 as a backbone classifier and sum squared error between the
predictions/ground truth as a loss:

  -- -------- -- -------
     @xmath      
     @xmath      
     @xmath      
     @xmath      (6.2)
  -- -------- -- -------

where @xmath are the centroid location of an anchor box, @xmath are the
width/height of the anchor, @xmath is the (i.e., confidence score of
whether an object exists), and @xmath is the classification loss. Let
@xmath and @xmath be the size of a feature map and the number of anchor
boxes, respectively. @xmath is @xmath when an object exists in cell
@xmath and otherwise @xmath .

During training, we use a batch size of @xmath and @xmath learning rate
for the Adam optimizer. The network resolution is set to @xmath pixels
during training and @xmath pixels during validation/testing respectively
to detect small tumors better. We recalculate the anchors at each DA
setup. As classic DA, geometric/intensity transformations are also
applied to both real/synthetic images during training to achieve the
best performance. For testing, we pick the model with the best
sensitivity on validation with detection threshold 0.1%/IoU threshold
0.5 between @xmath - @xmath steps to avoid severe FPs while achieving
high sensitivity.

#### 6.3.4 Clinical Validation via Visual Turing Test

To quantitatively evaluate how realistic the CPGGAN-based synthetic
images are, we supply, in random order, to three expert physicians a
random selection of @xmath real and @xmath synthetic brain metastases
images. They take four tests in ascending order: ( i ), ( ii ) test 1,
2: real vs CPGGAN-generated resized @xmath tumor bounding boxes, trained
without/with additional normal brain images; ( iii ), ( iv ) test 3, 4:
real vs CPGGAN-generated @xmath MR images, trained without/with
additional normal brain images.

Then, the physicians constantly classify them as real/synthetic, if
needed, zooming/rotating them, without previous training stages
revealing which is real/synthetic.

#### 6.3.5 Visualization via t-SNE

To visually analyze the distribution of real/synthetic images, we use
t-SNE [ 144 ] on a random selection of:

-   @xmath real tumor images;

-   @xmath CPGGAN-generated tumor images;

-   @xmath CPGGAN-generated tumor images, trained with additional normal
    brain images.

We normalize the input images to @xmath .

t-SNE Implementation Details We use t-SNE with a perplexity of @xmath
for @xmath iterations to get a 2D representation.

### 6.4 Results

This section shows how CPGGANs and image-to-image GAN generate brain MR
images. The results include instances of synthetic images and their
influence on tumor detection, along with CPGGAN-generated images’
evaluation via Visual Turing Test and t-SNE.

#### 6.4.1 MR Images Generated by CPGGANs

Fig. 6.4 illustrates example GAN-generated images. CPGGANs successfully
captures the T1c-specific texture and tumor appearance at desired
positions/sizes. Since we use highly-rough bounding boxes, the synthetic
tumor shape largely varies within the boxes. When trained with
additional normal brain images, it clearly maintains the realism of the
original images with less odd artifacts, including tumor bounding boxes,
which the additional images do not include. However, as expected,
image-to-image GAN, without progressive growing, generates clearly
unrealistic images without an input benign image due to the limited
training data/rough annotation.

#### 6.4.2 Brain Metastases Detection Results

Table 6.1 shows the tumor detection results with/without GAN-based DA.
As expected, the sensitivity remarkably increases with the additional
synthetic training data while FPs per slice also increase. Adding more
synthetic images generally leads to a higher amount of FPs, also
detecting blood vessels that are small/hyper-intense on T1c MR images,
very similarly to the enhanced tumor regions (i.e., the contrast agent
perfuses throughout the blood vessels). However, surprisingly, adding
only @xmath CPGGAN-generated images achieves the best sensitivity
improvement by @xmath with IoU threshold @xmath and by @xmath with IoU
threshold @xmath , probably due to the real/synthetic training image
balance—the improved training robustness achieves sensitivity @xmath
with moderate IoU threshold @xmath despite our highly-rough bounding box
annotation.

Fig. 6.5 also visually indicates that it can alleviate the risk of
overlooking the tumor diagnosis with clinically acceptable FPs; in the
clinical routine, the bounding boxes, highly-overlapping around tumors,
only require a physician’s single check by switching on/off transparent
alpha-blended annotation on MR images. It should be noted that we cannot
increase FPs to achieve such high sensitivity without CPGGAN-based DA.
Moreover, our results reveal that further realism—associated with the
additional normal brain images during training—does not contribute to
detection performance, possibly as the training focuses less on tumor
generation. Image-to-image GAN-based DA just moderately facilitates
detection with less additional FPs, probably because the synthetic
images have a distribution far from the real ones and thus their
influence on detection is limited during testing.

#### 6.4.3 Visual Turing Test Results

Table 6.2 shows the confusion matrix for the Visual Turing Test. The
expert physicians easily recognize @xmath synthetic images due to the
lack of training data. However, when CPGGANs is trained with additional
normal brain images, the experts classify a considerable number of
synthetic tumor bounding boxes as real; it implies that the additional
normal images remarkably facilitate the realism of both healthy and
pathological brain parts while they do not include abnormality; thus,
CPGGANs might perform as a tool to train medical students and radiology
trainees when enough medical images are unavailable, such as
abnormalities at rare positions/sizes. Such GAN applications are
clinically prospective [ 31 ] , considering the expert physicians’
positive comments about the tumor realism.

#### 6.4.4 T-SNE Results

As presented in Fig. 6.6 , synthetic tumor bounding boxes have a
moderately similar distribution to real ones, but they also fill the
real image distribution uncovered by the original dataset, implying
their effective DA performance; especially, the CPGGAN-generated images
trained without normal brain images distribute wider than the
center-concentrating images trained with the normal brain images.
Meanwhile, real/synthetic whole brain images clearly distribute
differently, due to the real MR images’ strong anatomical consistency
(Fig. 6.7 ). Considering the achieved high DA performance, the tumor
(i.e., ROI) realism/diversity matter more than the whole image
realism/diversity, since YOLOv3 look at an image patch instead of a
whole image, similarly to most other CNN-based object detectors.

### 6.5 Conclusion

Without relying on an input benign image, our CPGGANs can generate
realistic and diverse @xmath MR images with brain metastases of random
shape, unlike rigorous segmentation, naturally at desired
positions/sizes, and achieve high sensitivity in tumor detection—even
with small/fragmented training data from multiple MRI scanners and lazy
annotation using highly-rough bounding boxes; in the context of
intelligent data wrangling, this attributes to the CPGGANs’ good
generalization ability to incrementally synthesize conditional whole
images with the real image distribution unfilled by the original
dataset, improving the training robustness.

We confirm that the realism and diversity of the generated images,
judged by three expert physicians @xmath Visual Turing Test, do not
imply better detection performance; as the t-SNE results show, the
CPGGAN-generated images, trained with additional non-tumor normal
images, lack diversity probably because the training less focuses on
tumors. Moreover, we notice that adding over-sufficient synthetic images
leads to more FPs, but not always higher sensitivity, possibly due to
the training data imbalance between real and synthetic images; as the
t-SNE results reveal, the CPGGAN-generated tumor bonding boxes have a
moderately similar—mutually complementary—distribution to the real ones;
thus, GAN-overwhelming training images may decrease the necessary
influence of the real samples and harm training, rather than providing
robustness. Lastly, image-to-image GAN-based DA just moderately
facilitates detection with less additional FPs, probably due to the lack
of realism. However, further investigations are needed to maximize the
effect of the CPGGAN-based medical image augmentation.

For example, we could verify the effect of further realism in return for
less diversity by combining @xmath loss with the WGAN-GP loss for GAN
training. We can also combine those CPGGAN-generated images, trained
without/with additional brain images, similarly to ensemble learning [
29 ] . Lastly, we plan to define a new GAN loss function that directly
optimizes the detection results, instead of realism, similarly to the
three-player GAN for optimizing classification results [ 146 ] .

Overall, minimizing expert physicians’ annotation efforts, our novel
CPGGAN-based DA approach sheds light on diagnostic and prognostic
medical applications, not limited to brain metastases detection; future
studies, especially on 3D bounding box detection with highly-rough
annotation, are required to extend our promising results. Along with the
DA, the CPGGANs has other potential clinical applications in oncology: (
i ) A data anonymization tool to share patients’ data outside their
institution for training while preserving detection performance. Such a
GAN-based application is reported in Shin et al. [ 132 ] ; ( ii ) A
physician training tool to display random synthetic medical images with
abnormalities at both common and rare positions/sizes, by training
CPGGANs on highly unbalanced medical datasets (i.e., limited
pathological and abundant normal samples, respectively). It can help
train medical students and radiology trainees despite infrastructural
and legal constraints [ 31 ] .

## Chapter 7 GAN-based Medical Image Augmentation for 3D Detection

### 7.1 Prologue to Fourth Project

#### 7.1.1 Project Publication

-    Synthesizing Diverse Lung Nodules Wherever Massively: 3D
    Multi-Conditional GAN-based CT Image Augmentation for Object
    Detection .  C. Han , Y. Kitamura, A. Kudo, A. Ichinose, L.
    Rundo, Y. Furukawa, K. Umemoto, H. Nakayama, Y. Li, In International
    Conference on 3D Vision (3DV), Québec City, Canada, pp. 729–737,
    September 2019.

#### 7.1.2 Context

Prior to this work, no researchers had tackled 3D GANs for general
bounding box-based detection whereas 3D Medical Image Analysis can
improve diagnosis by capturing anatomical and functional information.
Jin et al. had used an image-to-image GAN to generate @xmath CT images
of lung nodules including the surrounding tissues by inputting a VOI
centered at a lung nodule, but with a central sphere region erased [ 70
] ; however, they had targeted annotation-expensive segmentation,
instead of the detection, also translating both nodules/surroundings via
expensive computation. Without conditioning a noise-to-image GAN with
nodule position, Gao et al. had generated @xmath 3D nodule subvolumes
only applicable to their subvolume-based detector using binary
classification [ 35 ] . Unfortunately, no research had focused on
multiple GAN conditions for more versatile 3D GANs while lesions vary in
position, size, and attenuation.

#### 7.1.3 Contributions

This project’s primary contribution is to propose a novel 3D
pathology-aware multi-conditional GAN called 3D MCGAN for improved 3D
bounding box-based detection in general; it translates noise boxes into
realistic/diverse @xmath lung nodules placed naturally at desired
position, size, and attenuation on CT scans—inputting the noise box with
the surrounding tissues has the effect of combining the noise-to-image
and image-to-image GANs. The @xmath nodule-only generation, not
translating the @xmath surroundings, can decrease computational cost. By
so doing, our 3D MCGAN-based DA boosts sensitivity in nodule detection
under any nodule size/attenuation at fixed FP rates. Moreover, we find
that GAN training with @xmath loss could increase synthetic images’
realism, but decrease DA performance. Using proper augmentation ratio
(i.e., @xmath ) could improve the DA performance. Considering the
outstanding realism confirmed by physicians, it could perform as a
physician training tool to display realistic medical images with desired
abnormalities (i.e., position, size, and attenuation).

#### 7.1.4 Recent Developments

According to their arXiv paper, Xu et al. have generated
realistic/diverse @xmath CT images of lung nodules combining the
image-to-image GAN with gene expression profiles [ 155 ] .

### 7.2 Motivation

Accurate CAD, thanks to recent CNNs, can alleviate the risk of
overlooking the diagnosis in a clinical environment. Such great success
of CNNs, including diabetic eye disease diagnosis [ 43 ] , primarily
derives from large-scale annotated training data to sufficiently cover
the real data distribution. However, obtaining and annotating such
diverse pathological images are laborious tasks; thus, the massive
generation of proper synthetic training images matters for reliable
diagnosis. Researchers usually use classical DA techniques, such as
geometric/intensity transformations [ 120 , 100 ] . However, those
one-to-one translated images have intrinsically similar appearance and
cannot sufficiently cover the real image distribution, causing limited
performance improvement; in this regard, thanks to their good
generalization ability, GANs [ 39 ] can generate realistic but
completely new samples using many-to-many mappings for further
performance improvement; GANs showed excellent DA performance in
computer vision, including @xmath performance improvement in eye-gaze
estimation [ 135 ] .

This GAN-based DA trend especially applies to medical imaging, where the
biggest problem lies in small and fragmented datasets from various
scanners. For performance boost in various 2D medical imaging tasks,
some researchers used noise-to-image GANs for classification [ 34 , 50 ,
49 ] ; others used image-to-image GANs for object detection [ 47 ] and
segmentation [ 11 ] . However, although 3D imaging is spreading in
radiology (e.g., CT and MRI), such 3D medical GAN-based DA approaches
are limited, and mostly focus on segmentation [ 132 , 70 ] —3D medical
image generation is more challenging than 2D one due to expensive
computational cost and strong anatomical consistency. Accordingly, no 3D
conditional GAN-based DA approach exists for general bounding box-based
3D object detection, while it can locate disease areas with physicians’
minimum annotation cost, unlike rigorous 3D segmentation. Moreover,
since lesions vary in position/size/attenuation, further GAN-based DA
performance requires multiple conditions.

So, how can GAN generate realistic/diverse 3D nodules placed naturally
on lung CT with multiple conditions to boost sensitivity in any 3D
object detector? For accurate 3D CNN-based nodule detection (Fig. 7.1 ),
we propose 3D MCGAN to generate @xmath nodules—such nodule detection is
clinically valuable for the early diagnosis/treatment of lung cancer,
the deadliest cancer [ 136 ] . Since nodules vary in
position/size/attenuation, to improve CNN’s robustness, we adopt two
discriminators with different loss functions for conditioning: the
context discriminator learns to classify real vs synthetic
nodule/surrounding pairs with noise box-centered surroundings; the
nodule discriminator attempts to classify real vs synthetic nodules with
size and attenuation conditions. We also evaluate the synthetic images’
realism via Visual Turing Test [ 128 ] by two expert physicians, and
visualize the data distribution via t-SNE [ 144 ] . The 3D
MCGAN-generated additional training images can achieve higher
sensitivity under any nodule size/attenuation at fixed FP rates. Lastly,
this study suggests training GANs without @xmath loss and using proper
augmentation ratio (i.e., @xmath ) for better medical GAN-based DA
performance.

Research Questions. We mainly address two questions:

-    3D Multiple GAN Conditioning: How can we condition 3D GANs to
    naturally place objects of random shape, unlike rigorous
    segmentation, at desired position/size/attenuation based on bounding
    box masks?

-    Synthetic Images for DA: How can we set the number of
    real/synthetic training data and GAN loss functions to achieve the
    best detection performance?

Contributions. Our main contributions are as follows:

-    3D Multi-conditional Image Generation: This first multi-conditional
    pathological image generation approach shows that 3D MCGAN can
    generate realistic and diverse nodules placed on lung CT at desired
    position/size/attenuation, which even expert physicians cannot
    distinguish from real ones.

-    Misdiagnosis Prevention: This first GAN-based DA method available
    for any 3D object detector allows to boost sensitivity at fixed FP
    rates in CAD with limited medical images/annotation.

-    Medical GAN-based DA: This study implies that training GANs without
    @xmath loss and using proper augmentation ratio (i.e., @xmath ) may
    boost CNN-based detection performance with higher sensitivity and
    less FPs in medical imaging.

### 7.3 Materials and Methods

#### 7.3.1 3D MCGAN-based Image Generation

Data Preparation This study exploits the Lung Image Database Consortium
image collection (LIDC) dataset [ 8 ] containing @xmath chest CT scans
with lung nodules. Since the American College of Radiology recommends
lung nodule evaluation using thin-slice CT scans [ 130 ] , we only use
scans with the slice thickness @xmath mm and @xmath mm @xmath in-plane
pixel spacing @xmath mm. Then, we interpolate the slice thickness to
@xmath mm and exclude scans with slice number @xmath .

To explicitly provide MCGAN with meaningful nodule appearance
information and thus boost DA performance, the authors further annotate
those nodules by size and attenuation for GAN training with multiple
conditions: small (slice thickness @xmath mm); medium ( @xmath mm @xmath
slice thickness @xmath mm); large (slice thickness @xmath @xmath mm);
solid; part-solid; Ground-Glass Nodule (GGN). Afterwards, the remaining
dataset ( @xmath scans) is divided into: ( i ) a training set ( @xmath
scans/ @xmath nodules); ( ii ) a validation set ( @xmath scans/ @xmath
nodules); ( iii ) a test set ( @xmath scans/ @xmath nodules); only the
training set is used for MCGAN training to be methodologically sound.
The training set contains more average nodules since we exclude patients
with too many nodules for the validation/test sets; we arrange a
clinical environment-like situation, where we could find more healthy
patients than highly diseased ones to conduct anomaly detection.

3D MCGAN is a novel GAN training method for DA, generating realistic but
new nodules at desired position/size/attenuation, naturally blending
with surrounding tissues (Fig. 7.2 ). We crop/resize various nodules to
@xmath voxels and replace them with noise boxes from a uniform
distribution between @xmath , while maintaining their @xmath
surroundings as VOIs—using those noise boxes, instead of boxes filled
with the same voxel values, improves the training robustness; then, we
concatenate the VOIs with @xmath size/attenuation conditions tiled to
@xmath voxels (e.g., if the size is small, each voxel of the small
condition is filled with @xmath , while the medium/large condition
voxels are filled with @xmath to consider the effect of scaling factor).
So, our generator uses the @xmath inputs to generate desired nodules in
the noise box regions. The 3D U-Net [ 26 ] -like generator adopts @xmath
convolutional layers in encoders and @xmath deconvolutional layers in
decoders respectively with skip connections to effectively capture both
nodule/context information.

We adopt two Pix2Pix GAN [ 67 ] -like discriminators with different loss
functions: the context discriminator learns to classify real vs
synthetic nodule/surrounding pairs with noise box-centered surroundings
using Least Squares loss (LSGANs) [ 94 ] ; the nodule discriminator
attempts to classify real vs synthetic nodules with size/attenuation
conditions using WGAN-GP [ 42 ] . The LSGANs in the context
discriminator forces the model to learn surrounding tissue background by
reacting more sensitively to every pixel in images than regular GANs.
The WGAN-GP in the nodule discriminator allows the model to generate
realistic/diverse nodules without focusing too much on details.
Empirically, we confirm that such multiple discriminators with the
mutually complementary loss functions, along with size/attenuation
conditioning, help generate realistic/diverse nodules naturally placed
at desired positions on CT scans; similar results are also reported by
this work [ 106 ] for 2D pedestrian detection without label
conditioning. We apply dropout to inject randomness and balance the
generator/discriminators. Batch normalization is applied to both
convolution (using LeakyReLU) and deconvolution (using ReLU).

Most GAN-based DA approaches use reconstruction @xmath loss [ 35 ] to
generate realistic images, even modifying it for further realism [ 70 ]
. However, no one has ever validated whether it really helps DA—it
assures synthetic images resembling the original ones, sacrificing
diversity; thus, to confirm its influence during classifier training, we
compare our MCGAN objective without/with it, respectively:

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (7.1)
                       @xmath   @xmath   
     @xmath   @xmath   @xmath            (7.2)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

We set 100 as a weight for the @xmath loss, since empirically it works
well for reducing visual artifacts introduced by the GAN loss and most
GAN works adopt the weight [ 67 , 106 ] .

3D MCGAN Implementation Details Training lasts for @xmath steps with a
batch size of @xmath and @xmath learning rate for the Adam optimizer. We
use horizontal/vertical flipping as DA and flip real/synthetic labels
once in three times for robustness. During testing, we augment nodules
with the same size/attenuation conditions by applying a random
combination to real nodules of width/height/depth shift up to @xmath and
zooming up to @xmath for better DA. As post-processing, we blend
bounding boxes’ @xmath nearest surfaces from all the boundaries by
averaging the values of @xmath nearest voxels/itself for @xmath
iterations. We resample the resulting nodules to their original
resolution and map back onto the original CT scans to prepare additional
training data.

#### 7.3.2 3D Faster RCNN-based Lung Nodule Detection

3D Faster RCNN is a 3D version of Faster RCNN [ 118 ] using multi-task
loss with a @xmath -layer Region Proposal Network of 3D
convolutional/batch normalization/ReLU layers. To confirm the effect of
MCGAN-based DA, we compare the following detection results trained on (
i ) @xmath real images without GAN-based DA, ( ii ), ( iii ), ( iv )
with @xmath / @xmath / @xmath MCGAN-based DA (i.e., @xmath / @xmath /
@xmath additional synthetic training images) , ( v ), ( vi ), ( vii )
with @xmath / @xmath / @xmath MCGAN-based DA trained with @xmath loss.
During training, we shuffle the real/synthetic image order. We evaluate
the detection performance as follows: ( i ) Free Receiver Operation
Characteristic (FROC) analysis, sensitivity as a function of FPs per
scan; ( ii ) Competition Performance Metric (CPM) score [ 104 ] ,
average sensitivity at seven pre-defined FP rates: 1/8, 1/4, 1/2, 1, 2,
4, and 8 FPs per scan—this quantifies if a CAD system can identify a
significant percentage of nodules with both very few FPs and moderate
FPs.

3D Faster RCNN Implementation Details During training, we use a batch
size of @xmath and @xmath learning rate ( @xmath after @xmath steps) for
the SGD optimizer with momentum. The input volume size to the network is
set to @xmath voxels. As classical DA, a random combination of
width/height/depth shift up to @xmath and zooming up to @xmath are also
applied to both real/synthetic images to achieve the best performance.
For testing, we pick the model with the highest sensitivity on
validation between @xmath - @xmath steps under IoU threshold @xmath
/detection threshold @xmath to avoid severe FPs.

#### 7.3.3 Clinical Validation via Visual Turing Test

To quantitatively evaluate the realism of MCGAN-generated images, we
supply, in random order, to two expert physicians a random selection of
@xmath real and @xmath synthetic lung nodule images with all of 2D
axial/coronal/sagittal views at the center. They take four
classification tests in ascending order: Test1, 2: real vs
MCGAN-generated @xmath nodules, trained without/with @xmath loss; Test3,
4: real vs MCGAN-generated @xmath nodules with surroundings without/with
@xmath loss.

#### 7.3.4 Visualization via t-SNE

To visually analyze the distribution of real/synthetic images, we use
t-SNE [ 144 ] on a random selection of @xmath real, @xmath synthetic,
and @xmath @xmath loss-added synthetic nodule images, with a perplexity
of @xmath for @xmath iterations to get a 2D representation. We normalize
the input images to @xmath .

### 7.4 Results

#### 7.4.1 Lung Nodules Generated by 3D MCGAN

We generate realistic nodules in noise box regions at various
position/size/attenuation, naturally blending with surrounding tissues
including vessels, soft tissues, and thoracic walls (Fig. 7.3 ).
Especially, when trained without @xmath loss, those synthetic nodules
look clearly more different from the original real ones, including
slight shading difference.

#### 7.4.2 Lung Nodule Detection Results

Table 7.1 and Fig. 7.4 show that it is easier to detect nodules with
larger size/lower attenuation due to their clear appearance. 3D
MCGAN-based DA with less augmentation ratio consistently increases
sensitivity at fixed FP rates—especially, training with @xmath
MCGAN-based DA without @xmath loss outperforms training only with real
images under any size/attenuation in terms of CPM, achieving average CPM
improvement by 0.032. It especially boosts nodule detection performance
with larger size and lower attenuation. Fig. 7.5 visually reveals its
ability to alleviate the risk of overlooking the nodule diagnosis with
clinically acceptable FPs (i.e., the highly-overlapping bounding boxes
around nodules only require a physician’s single check by switching
on/off transparent alpha-blended annotation on CT scans). Surprisingly,
adding more synthetic images tends to decrease sensitivity, due to the
real/synthetic training image balance. Moreover, further nodule realism
introduced by @xmath loss rather decreases sensitivity as @xmath loss
sacrifices diversity in return for the realism.

#### 7.4.3 Visual Turing Test Results

As Table 7.2 shows, expert physicians fail to classify real vs
MCGAN-generated nodules without surrounding tissues—even regarding the
synthetic nodules trained without @xmath loss more realistic than the
real ones. Contrarily, they relatively recognize the synthetic nodules
with surroundings due to slight shading difference between the
nodules/surroundings, especially when trained without the reconstruction
@xmath loss. Considering the synthetic images’ realism, CPGGANs might
perform as a tool to train medical students and radiology trainees when
enough medical images are unavailable, such as abnormalities at rare
position/size/attenuation. Such GAN applications are clinically
promising [ 31 ] .

#### 7.4.4 T-SNE Results

Implying their effective DA performance, synthetic nodules have a
similar distribution to real ones, but concentrated in left inner areas
with less real ones especially when trained without @xmath loss (Fig.
7.6 )–using only GAN loss during training can avoid overwhelming
influence from the real image samples, resulting in a moderately similar
distribution; thus, those synthetic images can partially fill the real
image distribution uncovered by the original dataset.

### 7.5 Conclusion

Our bounding box-based 3D MCGAN can generate diverse CT-realistic
nodules at desired position/size/attenuation, naturally blending with
surrounding tissues—those synthetic training data boost sensitivity
under any size/attenuation at fixed FP rates in 3D CNN-based nodule
detection. This attributes to the MCGAN’s good generalization ability
coming from multiple discriminators with mutually complementary loss
functions, along with informative size/attenuation conditioning; they
allow to cover the real image distribution unfilled by the original
dataset, improving the training robustness.

Surprisingly, we find that adding over-sufficient synthetic images
produces worse results due to the real/synthetic image balance; as t-SNE
results show, the synthetic images only partially cover the real image
distribution, and thus GAN-overwhelming training images rather harm
training. Moreover, we notice that GAN training without @xmath loss
obtains better DA performance thanks to increased diversity providing
robustness; also, expert physicians confirm their sufficient realism
without @xmath loss.

Overall, our 3D MCGAN could help minimize expert physicians’
time-consuming annotation tasks and overcome the general medical data
paucity, not limited to lung CT nodules. As future work, we will
investigate the MCGAN-based DA results without size/attenuation
conditioning to confirm their influence on DA performance. Moreover, we
will compare our DA results with other non-GAN-based recent DA
approaches, such as Mixup [ 163 ] and Cutout [ 28 ] . For further
performance boost, we plan to directly optimize the detection results
for MCGANs, instead of realism, similarly to the three-player GAN for
classification [ 146 ] . Lastly, we will investigate how our MCGAN can
perform as a physician training tool to display random realistic medical
images with desired abnormalities (i.e., position/size/attenuation
conditions) to help train medical students and radiology trainees
despite infrastructural and legal constraints [ 31 ] .

## Chapter 8 Discussions on Developing Clinically Relevant AI-Powered
Diagnosis Systems

### 8.1 Prologue to First Project

#### 8.1.1 Project Publication

-    Bridging the gap between AI and healthcare sides: towards
    developing clinically relevant AI-powered diagnosis systems .  C.
    Han , L. Rundo, K. Murao, T. Nemoto, H. Nakayama, In IFIP
    International Conference on Artificial Intelligence Applications and
    Innovations (AIAI) , pp. 320–333, June 2020.

### 8.2 Feedback from Physicians

#### 8.2.1 Methods for Questionnaire Evaluation

To confirm the clinical relevance for diagnosis of our proposed
pathology-aware GAN methods for DA and physician training respectively,
we conduct a questionnaire survey for @xmath Japanese physicians who
interpret MR and CT images in daily practice. The experimental settings
are the following:

-    Subjects: @xmath physicians (i.e., a radiologist, a psychiatrist,
    and a physiatrist) committed to (at least one of) our
    pathology-aware GAN projects and @xmath project non-related
    radiologists without much AI background.

-    Experiments: Physicians are asked to answer the following
    questionnaire within 2 weeks from December 6th, 2019 after reading
    10 summary slides written in Japanese ¹ ¹ 1 Available via Dropbox:
    https://www.dropbox.com/sh/bacowc3ilz1p1r3/AABNS9SyjArHq8BntgaODLb2a?dl=0
    about general Medical Image Analysis and our pathology-aware GAN
    projects along with example synthesized images. We conduct both
    qualitative (i.e., free comments) and quantitative (i.e., five-point
    Likert scale [ 5 ] ) evaluation: Likert scale 1 @xmath very
    negative, 2 @xmath negative, 3 @xmath neutral, 4 @xmath positive, 5
    @xmath very positive.

-    Question 1: Are you keen to exploit medical AI in general when it
    achieves accurate and reliable performance in the near future?
    (five-point Likert scale) Please tell us your expectations, wishes,
    and worries (free comments).

-    Question 2: What do you think about using GAN-generated images for
    DA? (five-point Likert scale) Please tell us your expectations,
    wishes, and worries (free comments).

-    Question 3: What do you think about using GAN-generated images for
    physician training? (five-point Likert scale) Please tell us your
    expectations, wishes, and worries (free comments).

-    Question 4: Any comments or suggestions about our projects towards
    developing clinically relevant AI-powered systems based on your
    daily diagnosis experience?

#### 8.2.2 Results

We show the questions and Japanese physicians’ corresponding answers.

Question 1: Are you keen to exploit medical AI in general when it
achieves accurate and reliable performance in the near future?

-    Likert scale Project-related physicians: 5 5 5 (average: 5)
    Project non-related radiologists: 5 5 3 4 5 5 (average: 4.5)

-    Free comments (one comment for each physician)

-   As radiologists, we need AI-based diagnosis during image
    interpretation as soon as possible.

-   It is common to conduct further medical examinations when
    identifying disease is difficult from CT/MR images; thus, if
    AI-based diagnosis outperforms that of physicians, such clinical
    decision support systems could prevent unnecessary examinations.
    Moreover, recently lung cancer misdiagnosis occurred in Japan, but
    AI technologies may prevent such death caused by misdiagnosis.

-   The lack of diagnosticians is very evident in Healthcare, so AI has
    great potential to support us. It may be already applicable without
    severe problems for typical disease cases.

-   I am looking forward to its practical applications, especially at
    low or zero price.

-   I would like to use AI-based diagnosis as a kind of data, but it is
    yet uncertain how much I trust AI.

-   I am wondering whether such systems will become popular due to
    practical problems such as introduction cost.

-   The definition of accurate and reliable is unclear. Since a
    physician’s annotation is always subjective, we cannot claim that
    AI-based diagnosis is really correct even if AI diagnoses similarly
    to the specific physician. Because I do not believe other
    physicians’ diagnosis, but my own eyes, I would use AI just to
    identify abnormal candidates.

As expected, the project-related physicians are AI-enthusiastic while
the project non-related radiologists are also generally very positive
about the medical AI. Many of them appeal the necessity of AI-based
diagnosis for more reliable diagnosis because of the lack of physicians.
Meanwhile, other physicians worry about its cost and reliability. We may
be able to persuade them by showing expected profitability (e.g.,
currently CT scanners have an earning rate 16% and CT scans require 2-20
minutes for interpretation in Japan); similarly, we can explain how
experts annotate medical images and AI diagnoses disease based on them
(e.g., multiple physicians, not a single one, can annotate the images
via discussion).

Question 2: What do you think about using GAN-generated images for DA?

-    Likert scale Project-related physicians: 5 5 4 (average: 4.7)
    Project non-related radiologists: 4 5 4 4 4 4 (average: 4.2)

-    Free comments (one comment for each physician)

-   Achieved accuracy improvement shows its superiority in identifying
    diverse disease.

-   It would be effective, especially as rare disease training data.

-   I am looking forward to the future with advanced GAN technology.

-   It significantly improves detection sensitivity; but I am also
    curious about its influence on other metrics, such as specificity.

-   If Deep Learning could be more effective, we should introduce it;
    but anonymization would be important for privacy preservation.

-   Achieved accuracy improvement shows its superiority in identifying
    diverse disease.

-   It would be effective to train AI on data-limited disease, but which
    means that AI is inferior to humans.

-   It would be helpful if such DA improves accuracy and reliability.
    Since I am not familiar with AI and a generator/classifier’s failure
    judgment mechanisms, I am uncertain whether it will really increase
    reliability though.

As expected, the project-related physicians are very positive about the
GAN-based DA while the project non-related radiologists are also
positive. Many of them are satisfied with its achieved
accuracy/sensitivity improvement when available annotated images are
limited. However, similarly to their opinions on general Medical Image
Analysis, some physicians question its reliability.

Question 3: What do you think about using GAN-generated images for
physician training?

-    Likert scale Project-related physicians: 3 4 3 (average: 3.3)
    Project non-related radiologists: 3 5 2 3 2 3 (average: 3)

-    Free comments (one comment for each physician)

-   In future medical care, physicians should actively introduce and
    learn new technology; in this sense, GAN technology should be
    actively used for physician training in rare diseases.

-   It could be useful for medical student training, which aims for 85%
    accuracy by covering typical cases. But expert physician training
    aims for over 85% accuracy by comparing typical/atypical cases and
    acquiring new understanding—real atypical images are essential.

-   In physician training, we use radiological images after definite
    diagnosis, such as pathological examination—but, we actually lack
    rare disease cases. Since the GAN-generated images’ realism
    fluctuates based on image augmentation schemes and available
    training images, further realistic image generation of the rare
    cases would help the physician training.

-   It depends on how to construct the system.

-   Which specific usage is assumed for such physician training?

-   I cannot state an opinion before actually using the system, but I
    strongly recognize the importance of looking at real images.

-   I do not exactly understand in which situation such physician
    training is used, but eventually training with realistic images
    would be also helpful. However, if real images are available, using
    them would be better.

We generally receive neutral feedback because we do not provide a
concrete physician training tool, but instead general pathology-aware
generation ideas with example synthesized images—thus, some physicians
are positive, and some are not. A physician provides a key idea about a
pathology-coverage rate for medical student/expert physician training,
respectively; for extensive physician training with GAN-generated
atypical images, along with pathology-aware GAN-based extrapolation,
further GAN-based extrapolation would be valuable.

Question 4: Any comments or suggestions about our pathology-aware GAN
projects towards developing clinically relevant AI-powered systems based
on your daily diagnosis experience?

-   This approach will change the way physicians work. I have high
    expectations for AI-based diagnosis, so I hope it to overcome the
    legal barrier.

-   For now, please show small abnormal findings, such as nodules and
    ground glass opacities—it would halve radiologists’ efforts. Then,
    we could develop accurate diagnosis step by step.

-   Showing abnormal findings with their shapes/sizes/disease names
    would increase diagnosis accuracy. But I also would like to know how
    diagnosticians’ roles change after all.

-   I hope that this approach will lead to physicians’ work reduction in
    the future.

-   Please develop reliable AI systems by increasing accuracy with the
    GAN-based image augmentation.

-   GANs can generate typical images, but not atypical images; this
    would be the next challenge.

-   AI can alert physicians to detect typical cases, and thus decrease
    interpretation time; however, it may lead to the diagnosticians’
    easy diagnosis without much consideration. Especially in Japan, we
    currently often conduct unnecessary diagnostic tests, so the
    diagnosticians should be more responsible of their own duties after
    introducing AI.

Most physicians look excited about our pathology-aware GAN-based image
augmentation projects and express their clinically relevant requests.
The next steps lie in performing further GAN-based extrapolation,
developing clinician-friendly systems with new practice guidelines, and
overcoming legal/financial constraints.

### 8.3 AI and Healthcare Workshop

#### 8.3.1 Methods for Workshop Evaluation

Convolutional Neural Networks (CNNs) have achieved accurate and reliable
Computer-Aided Diagnosis (CAD), occasionally outperforming expert
physicians [ 64 , 153 , 96 ] . However, such research results cannot be
easily applied to a clinical environment: AI and Healthcare sides have a
huge gap around technology, funding, and people, such as clinical
significance/interpretation, data acquisition, commercial purpose, and
anxiety about AI. Aiming to identify/bridge the gap between AI and
Healthcare sides in Japan towards develop medical AI fitting into a
clinical environment in five years, we hold a workshop for @xmath
Japanese professionals with various AI and/or Healthcare background. The
experimental settings are the following:

-    Subjects: @xmath Medical Imaging experts (i.e., a Medical Imaging
    researcher and a medical AI startup entrepreneur), @xmath physicians
    (i.e., a radiologist and a psychiatrist), and @xmath generalists
    between Healthcare and Informatics (i.e., a nurse and researcher in
    medical information standardization, a general practitioner and
    researcher in medical communication, and a medical technology
    manufacturer’s owner and researcher in health disparities)

-    Experiments: As its program shows (Table 8.1 ), during the
    workshop, we conduct 2 activities: ( Learning ) Know the overview of
    Medical Image Analysis, including state-of-the-art research,
    well-known challenges/solutions, and the summary of our
    pathology-aware GAN projects; ( Thinking ) Find the intrinsic gap
    and its solutions between AI researchers and Healthcare workers
    after sharing their common and different thinking/working styles.
    Supported by GCL program, this workshop was held on March 17th, 2019
    at Nakayama Future Factory, Open Studio, The University of Tokyo,
    Tokyo, Japan.

#### 8.3.2 Results

We show the summary of clinically-relevant findings from this Japanese
workshop.

##### Gap Between AI and Healthcare Sides

Gap 1: AI, including Deep Learning, does not provide clear decision
criteria, does it make physicians reluctant to use it in a clinical
environment, especially for diagnosis?

-    Healthcare side : We rather expect applications other than
    diagnosis. If we use AI for diagnosis, instead of replacing
    physicians, we suppose a reliable second opinion , such as alert to
    avoid misdiagnosis, based on various clinical data not limited to
    images—every single diagnostician is anxious about their diagnosis.
    AI only provides minimum explanation, such as a heatmap showing
    attention, which makes persuading not only the physicians but also
    patients difficult; so, the physicians’ intervention is essential
    for intuitive explanation. Methodological safety and feeling safe
    are different. In this sense, pursuing explainable AI generally
    decreases AI’s diagnostic accuracy [ 3 ] , so physicians should
    still serve as mediators by engaging in high-level conversation or
    interaction with patients. Moreover, according to the medical law in
    most countries including Japan, only doctors can make the final
    decision. The first autonomous AI-based diagnosis without a
    physician was cleared by the Food and Drug Administration in the US
    in 2018 [ 2 ] , but such a case is exceptional.

-    AI side : Compared with other systems or physicians, Deep
    Learning’s explanation is not particularly poor, so we require too
    severe standards for AI; the word AI is excessively promoting
    anxiety and perfection. If we could thoroughly verify the
    reliability of its diagnosis against physicians by exploring
    uncertainty measures [ 102 ] , such intuitive explanation would be
    optional.

Gap 2: Are there any benefits to actually introducing medical AI?

-    Healthcare side : After all, even if AI can achieve high accuracy
    and convenient operation, hospitals would not introduce it without
    any commercial benefits. Moreover, small clinics, where physicians
    are desperately needed, often do not have CT or MRI scanners [ 69 ]
    .

-    AI side : The commercial deployment of medical AI is strongly tied
    to diagnostic accuracy [ 148 ] ; so, if it can achieve significantly
    outstanding accuracy at various tasks in the near future, patients
    would not visit hospitals/clinics without AI. Accordingly,
    introducing medical AI would become profitable in five years.

Gap 3: Is medical AI’s diagnostic accuracy reliable?

-    Healthcare side : To evaluate AI’s diagnostic performance, we
    should consider many metrics, such as sensitivity and specificity.
    Moreover, its generalization ability for medical data highly relies
    on inter-scanner/inter-individual variability [ 105 ] . How can we
    evaluate whether it is suitable as a clinically applicable system?

-    AI side : Generally, alleviating the risk of overlooking the
    diagnosis is the most important, so sensitivity matters more than
    specificity unless their balance is highly disturbed. Recently, such
    research on medical AI that is robust to different datasets is
    active [ 124 ] .

##### How to Develop Medical AI Fitting into a Clinical Environment in
Five Years

Why: Clinical significance/interpretation

-    Challenges : We need to clarify which clinical situations actually
    require AI introduction. Moreover, AI’s early diagnosis might not be
    always beneficial for patients.

-    Solutions : Due to nearly endless disease types and frequent
    misdiagnosis coming from physicians’ fatigue, we should use it as
    alert to avoid misdiagnosis [ 145 ] (e.g., reliable second opinion),
    instead of replacing physicians. It should help prevent oversight in
    diagnostic tests not only with CT and MRI, but also with blood data,
    chest X-ray, and mammography before taking CT and MRI [ 86 ] . It
    could be also applied to segmentation for radiation therapy [ 4 ] ,
    neurosurgery navigation [ 1 ] , and pressure ulcers’ echo
    evaluation. Along with improving the diagnosis, it would also make
    the physicians’ workflow easier, such as by denoising [ 157 ] .
    Patients should decide whether they accept AI-based diagnosis under
    informed consent.

How: Data acquisition

-    Challenges : Ethical screening in Japan is exceptionally strict, so
    acquiring and sharing large-scale medical data/annotation are
    challenging—it also applies to Europe due to General Data Protection
    Regulation [ 123 ] . Considering the speed of technological advances
    in AI, adopting it for medical devices is difficult in Japan, unlike
    in medical AI-ready countries, such as the US, where the ethical
    screening is relatively loose in return for the responsibility of
    monitoring system stability. Moreover, whenever diagnostic criteria
    changes, we need further reviews and software modifications; for
    example, the Tumor-lymph Node-Metastasis (TNM) classification [ 138
    ] criteria changed for oropharyngeal cancer in 2018 and for lung
    cancer in 2017, respectively. Diagnostic equipment/target changes
    also require large-scale data/annotation acquisition again.

-    Solutions : For Japan to keep pace, the ethical screening should be
    adequate to the other leading countries. Currently, overseas
    research and clinical trials are proceeding much faster, so it seems
    better to collaborate with overseas companies than to do it in Japan
    alone. Moreover, complete medical checkup, which is extremely
    costly, is unique in East Asia, so Japan could be superior in
    individuals’ multiple medical data—Japan is the only country, where
    most workers 40 or older are required to have medical checkups once
    a year independent of their health conditions by the Industrial
    Safety and Health Act [ 103 ] . To handle changes in diagnostic
    criteria/equipment and overcome dataset/task dependency, it is
    necessary to establish a common database creation workflow [ 93 ] by
    regularly entering electronic medical records into the database. For
    reducing data acquisition/annotation cost, AI techniques, such as
    GAN-based DA [ 46 ] and domain adaptation [ 38 ] , would be
    effective.

How: Commercial deployment

-    Challenges : Hospitals currently do not have commercial benefits to
    actually introduce medical AI.

-    Solutions : For example, it would be possible to build AI-powered
    hospitals [ 21 ] operated with less staff. Medical manufacturers
    could also standardize data format [ 83 ] , such as for X-ray, and
    provide some AI services. Many IT giants like Google are now working
    on medical AI to collect massive biomedical data [ 101 ] , so they
    could help rural areas and developing countries, where physician
    shortage is severe [ 69 ] , at relatively low cost.

How: Safety and feeling safe

-    Challenges : Considering multiple metrics, such as sensitivity and
    specificity [ 121 ] , and dataset/task dependency [ 59 ] , accuracy
    could be unreliable, so ensuring safety is challenging. Moreover,
    reassuring physicians and patients is important to actually use AI
    in a clinical environment [ 78 ] .

-    Solutions : We should integrate various clinical data, such as
    blood test biomarkers and multiomics, with images [ 86 ] . Moreover,
    developing bias-robust technology is important since confounding
    factors are inevitable [ 85 ] . To prevent oversight, prioritizing
    sensitivity over specificity is essential while maintaining a
    balance [ 68 ] . We should also devise education for medical AI
    users, such as result interpretation, to reassure patients [ 150 ] .

## Chapter 9 Conclusion

### 9.1 Final Remarks

Inspired by their excellent ability to generate realistic and diverse
images, we propose to use noise-to-image GANs for ( i ) Medical DA and (
ii ) physician training [ 45 ] . Through information conversion, such
applications can relieve the lack of pathological data and their
annotation; this is uniquely and intrinsically important in Medical
Image Analysis, as CNN generalization becomes unstable on unseen data
due to large inter-subject, inter-pathology, and cross-modality
variability [ 124 , 125 , 111 ] . Towards clinically relevant
implementation for the DA and physician training, we find effective loss
functions and training schemes for each of them [ 49 , 50 ] —the
diversity matters more for the DA to sufficiently fill the real image
distribution whereas the realism matters more for the physician training
not to confuse medical students and radiology trainees.

Specifically, our results imply that GAN training without @xmath loss,
using proper augmentation ratio (i.e., @xmath ), and further refining
synthetic images’ texture/shape could improve the DA performance,
whereas discarding weird-looking synthetic images to humans is
unnecessary; for example, adding over-sufficient GAN-generated training
images leads to more FPs in detection, but not always higher
sensitivity, due to the real/synthetic training data balance (both of
their distributions are biased, but differently). Regarding the
physician training, GAN training with @xmath loss, GAN training on
additional normal images, and post-processing, such as by Poisson image
editing [ 110 ] , could improve the synthetic images’ realism; for
instance, the GAN training on normal images along with pathological
ones, remarkably facilitates the realism of both healthy and
pathological parts while they do not include abnormality.

Because such excellent realism and diversity can be achieved by
GAN-based interpolation and extrapolation, we propose novel 2D/3D
pathology-aware GANs for bounding box-based pathology detection [ 47 ,
46 ] : ( Interpolation ) The GAN-based medical image augmentation is
reliable because medical modalities (e.g., X-ray, CT, MRI) can display
the human body’s strong anatomical consistency at fixed position while
clearly reflecting inter-subject variability [ 58 , 16 ] —this is
different from natural images, where various objects can appear at any
position; ( Extrapolation ) The pathology-aware GANs are promising
because common and/or desired medical priors can play a key role in the
conditioning—theoretically, infinite conditioning instances, external to
the training data, exist and enforcing such constraints have an
extrapolation effect via model reduction [ 140 ] .

After conducting a questionnaire survey about our GAN projects for 9
physicians and holding a workshop about how to develop medical AI
fitting into a clinical environment for 7 professionals with various AI
and/or Healthcare background, we confirm our pathology-aware GANs’
clinical relevance for diagnosis: ( DA ) They could be integrated into a
clinical decision support system; since CT has a much higher earning
rate and longer interpretation time than MRI (16% to 3% and 2-20 minutes
to 1 minute in Japan), alerting abnormal findings on CT, such as
nodules/ground glass opacities, would halve radiologists’ efforts and
increase hospitals’ financial outcomes; ( Physician training ) They
could perform as a non-expert physician training tool; when the normal
training images are sufficiently available, we can stably generate
typical pathological images useful for medical student training, thanks
to the excellent interpolation; but it is still challenging to generate
atypical images needed for expert physician training. Whereas our
pathology-aware bounding box conditioning largely improves extrapolation
ability, better DA and physician training would require further
GAN-based extrapolation.

### 9.2 Future Work

We believe that the next steps towards GAN-based extrapolation and thus
atypical pathological image generation lie in ( i ) generation by parts
with coordinate conditions [ 87 ] , ( ii ) generation with both image
and gene expression conditions [ 155 ] , and ( iii ) transfer learning
among different body parts and disease types [ 23 ] . Due to biological
constraints, human interaction is restricted to part of the surrounding
environment. Accordingly, we must reason spatial relationships across
the surrounding parts to piece them together. Similarly, since machine
performance also depends on computational constraints, it is plausible
for a generator to generate partial images using the corresponding
spatial coordinate conditions—meanwhile, a discriminator attempts to
judge realism across the assembled patches by global coherence, local
appearance, and edge-crossing continuity. This approach allowed
COnditional COordinate GAN (COCO-GAN) to generate state-of-the-art
realistic and seamless full images [ 87 ] . Since human anatomy has a
much stronger local consistency than various object relationships in
natural images, reasoning the body’s spatial relationships, like the
COCO-GAN, would perform effective extrapolation both for medical DA and
physician training.

We can also condition the GANs both on the image features and gene
expression profiles to non-invasively identify molecular properties of
disease. By so doing, Xu et al. succeeded to produce @xmath realistic
synthetic CT images of lung nodules [ 155 ] . If the gene expression
data are available, such condition fusing could be helpful for the
medical DA and physician training.

Such information conversion, not limited to the GAN conditioning, should
locate in the core of future Medical Image Analysis to overcome the data
paucity. Whereas the transfer learning from large-scale natural
image/video datasets for CNNs is already common in Medical Image
Analysis, such pre-trained models cannot extract general human
anatomical features. Accordingly, pre-training on large-scale 3D medical
volumes for CNNs, such as CT and MRI, significantly outperformed the
pre-training on natural videos or training from scratch for
classification and segmentation [ 23 ] both by accuracy and training
convergence speed. Similarly, transfer learning from mammography for the
CNNs also significantly improved mass detection on digital breast
tomosynthesis slices [ 129 ] . Such transfer learning across different
body parts and disease types for the GANs would also largely improve
their extrapolation ability.
