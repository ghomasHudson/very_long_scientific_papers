## Chapter 1 Introduction

This thesis explores effective ways towards fully empowering the
semantic feature space to mitigate the domain shift problem, and hence
obtain a more robust visual-semantic mapping function for zero-shot
learning. This is an important research problem with a wide range of
applications in machine learning and multimedia areas. In this chapter,
we first give a brief overview of the research problems in Section 1.1 .
Then we highlight the main contributions of this thesis in Section 1.2 .
Finally, Section 1.3 outlines the thesis organization.

### 1.1 Overview

In the past few years, with the increasing development of deep learning
techniques, many machine learning tasks and techniques have been
proposed and consistently achieved state-of-the-art performance. Among
them, most of the tasks can be grouped into supervised learning
problems, such as image processing [ 58 , 91 , 132 , 126 , 38 ] , face
verification [ 44 , 23 , 101 , 20 , 59 ] , multimedia retrieval [ 89 ,
52 ] , medical imaging [ 95 , 113 , 33 , 80 ] , time-series forecasting
[ 24 , 69 , 70 ] and so on. These tasks usually require a large amount
of labeled examples to train the model and then to make inferences on
testing examples. Generally, in most cases, the larger the quantity of
data, the better the model performance. Taking ImageNet [ 22 ] as an
example, which consists of 21,841 classes and 14,197,122 images in
total, its emergence has brought unprecedented opportunities to the
machine learning and multimedia areas. Many tasks trained on large
datasets, e.g., ImageNet, and related tasks, e.g., pre-trained models on
ImageNet, continue to make progress in contrast to many other areas.
These tasks have achieved superior performance and even surpassed humans
in some scenarios [ 41 ] . Despite the great power of supervised
learning, it relies too much on a large amount of labeled data examples,
which makes it difficult for models to be generalized to unfamiliar or
even unseen classes. Transfer Learning [ 83 , 103 , 66 , 49 ] has
partially solved this problem. It pre-trains a model on a source domain
dataset of a similar task and then transfers the whole or part of the
trained model to the target domain task and fine-tunes with target data.
For example, it is easier for a learner who has already learned English
to learn French because many internal similarities and overlaps exist
between these two languages. Human beings have an excellent ability to
generalize learned knowledge to explore the unknown. It has been proven
that humans can recognize over 30,000 object classes and many more
subclasses [ 12 ] , e.g., breeds of birds and combinations of attributes
and objects. Moreover, human beings are also very good at recognizing
object classes without previously seeing them before. For example, if a
learner who has never seen a panda is taught that the panda is a
bear-like animal that has black and white fur, then he or she will be
able to easily recognize a panda when seeing a real example. In machine
learning, this process is considered as the problem of Zero-shot
Learning [ 61 ] . The settings of zero-shot learning can be regarded as
an extreme case of transfer learning: the model is trained to imitate
human ability in recognizing examples of unseen classes that are not
shown during training stage [ 28 , 62 , 96 , 111 , 15 , 131 , 16 , 57 ,
134 , 35 ] . In conventional supervised learning, the training and
testing examples belong to the same class-set, which means that the
learned model has already seen some examples of all the classes it
encounters during testing. In contrast, the zero-shot learning only
trains the model on seen class examples, and the learned model is
expected to infer novel unseen class examples. Thus, as shown in Figure
1.1 , the essential difference between the zero-shot learning and
conventional supervised learning is that the training and testing
class-sets of zero-shot learning are disjoint from each other. As a
result, the zero-shot learning can be regarded as a complement to the
conventional supervised learning, and gaps the scenarios where
collecting and labeling a large amount of examples for all classes is
impossible in real-world applications. As such, the zero-shot learning
has received increasing attention in recent years [ 28 , 62 , 96 , 111 ,
15 , 131 , 16 , 57 , 134 , 35 ] .

To recognize zero-shot classes (i.e., novel class examples not shown
during training) in the target domain, one has to utilize the knowledge
learned from source domain. Unfortunately, in the zero-shot setting,
since there are no examples available during training phase on the
target domain, it may be difficult for existing methods to do the domain
adaptation. Thus, The key idea to achieve the zero-shot recognition is
to discover and exploit the knowledge of how an unseen class can be
related to seen classes. In zero-shot learning, this is typically
achieved by taking utilization of labeled seen class examples and
certain Common Knowledge that can be shared and transferred between seen
and unseen classes. This common knowledge is per-class, semantic and
high-level features for both the seen and unseen classes [ 61 ] , which
enables easy and fast implementation and inference to zero-shot
learning. Among them, the Semantic Attributes and Semantic Word Vectors
become the most popular ones in recent years. Taking the semantic
attributes as an example. The attributes are meaningful high-level
descriptions of examples, such as their shapes, colors, components,
texture, etc. Intuitively, the cat is more closely related to the tiger
than to the snake. In the semantic feature space, this intuition also
holds: the similar classes should have similar patterns. To this end, in
zero-shot learning, each class can be represented by a semantic feature
vector. This particular pattern is called semantic prototype and each
class is endowed with a unique prototype. Figure 1.2 demonstrates a
simple case of semantic attributes (binary attributes). For example, the
animal that has black and brown fur, lives in/near the water, and eats
fish can be recognized as “otter”.

Thus, to handle the zero-shot recognition, one can construct a link from
the original feature space, e.g., visual feature space of image
examples, to the semantic feature space, to establish the cross-modal
retrieval.

Under the most commonly adopted visual-semantic mapping paradigm, the
zero-shot learning task can be formalized as follows. Given a set of
labeled seen class examples @xmath , where @xmath is seen class example,
i.e., image, with class label @xmath belonging to @xmath seen classes
@xmath . The goal is to construct a model for a set of unseen classes
@xmath ( @xmath ) from which no example is available during training. In
the inference phase, given a testing unseen class example @xmath , the
model is expected to predict its class label @xmath . To this end, some
common knowledge, e.g., the semantic features, denoted as @xmath , are
needed to bridge the gaps between the seen and unseen classes.
Therefore, the labeled seen class examples @xmath can be further
specified as @xmath . Each seen class @xmath is endowed with a semantic
prototype @xmath , and similarly, each unseen class @xmath is also
endowed with a semantic prototype @xmath . Thus, for each seen class
example we have its semantic features @xmath , while for testing unseen
class example @xmath , we need to predict its semantic features @xmath
and set the class label by searching the most closely related semantic
prototype within @xmath . In summary, given @xmath , the training can be
described as:

  -- -------- -- -------
     @xmath      (1.1)
  -- -------- -- -------

where @xmath being the loss function and @xmath being the regularization
term (if needed). The @xmath is a mapping function with parameter @xmath
maps from the visual feature space to semantic feature space. The @xmath
is a feature extractor, e.g., a pre-trained CNNs, to obtain the visual
features of @xmath . For the inference, given a testing example, e.g.,
@xmath , the recognition can be described as:

  -- -------- -- -------
     @xmath      (1.2)
     @xmath      (1.3)
  -- -------- -- -------

where @xmath is a similarity metric and @xmath searches the most closely
related prototype and set the class corresponding with this prototype to
@xmath . Specifically, Eq. 1.2 is used for conventional zero-shot
learning which the similarity search is only on unseen classes, and Eq.
1.3 is used for generalized zero-shot learning which the search can also
generalize to novel examples from seen classes.

In view of the above formalization that the zero-shot learning is indeed
a cross-modal mapping problem, i.e., training and reusing the mapping
function between the visual and semantic feature spaces, and in view of
the absence of unseen classes during training, the Domain Shift problem
[ 29 ] may easily occur. The domain shift problem refers to the
phenomenon that when mapping an unseen class example from the visual to
semantic feature spaces, the obtained semantic features may easily shift
away from its real prototype. The domain shift problem is essentially
caused by the nature of zero-shot learning: the training (seen) and
testing (unseen) classes are mutually disjoint from each other. Thus,
the mapping function learned from source domain (seen classes) may not
well-adopted to the target domain (unseen classes). Moreover, since the
visual and semantic feature spaces are generally independent and may
exist in entirely different manifolds, the trained visual-semantic
mapping function can hardly reach a stable convergence. Therefore, when
the learned mapping function is transferred and reused by unseen
classes, it may further exacerbate the shift phenomenon.

Being an open issue in zero-shot learning, the domain shift problem has
been hindering the performance of zero-shot learning models for a long
time. Moreover, based on our observation, some key building blocks of
zero-shot learning, e.g., the semantic feature space itself and its
correlation with surrounding elements such as the visual feature space,
does not seem to receive comparable attention. Therefore, in this
thesis, we aim at exploring effective ways to fully empower the semantic
feature space towards better performance on mitigation of the domain
shift problem, which is rarely studied by previous work. The research of
this thesis comprises three parts, and basically follows a bottom-up
progressivism. In the first part, we consider to adaptively adjust the
semantic feature space to enhance the model’s ability to adapt and
generalize to unseen classes. In the second part, we consider to expand
the semantic features and to conduct an alignment of manifold structures
between the visual and semantic feature spaces. This approach is more
conservative compared to the first method for not directly adjusting the
previous semantic features. In the third part, we take a further step by
considering the correlation between the visual and semantic feature
spaces in a more fine-grained perspective. This approach makes use of
the graph techniques to better exploit the visual-semantic correlation
and the local substructure information in examples.

### 1.2 Thesis Contributions

We briefly summarize our contributions below.

-    Rectification on Class Prototype and Global Distribution .
    To deal with the domain shift and hubness problems, we propose a
    novel zero-shot recognition framework to adaptively adjust to
    rectify the semantic feature space. This adjustment is based on both
    the prototypes and the global distribution of data during each loop
    of training phase. To the best of our knowledge, our work is the
    first to consider such adaptive adjustment of semantic feature space
    in zero-shot learning. Moreover, we also propose to combine the
    adjustment jointly with a cycle mapping, which first maps the
    examples from visual to semantic feature space and then vice versa,
    guaranteeing the mapping function obtains more robust results to
    further mitigate the domain shift problem. We formulate our solution
    to a more efficient framework which can significantly boosts the
    training. Experimental results on several benchmark datasets
    demonstrate the significant performance improvement of our method
    compared with other existing methods on both recognition accuracy
    and training efficiency.

-    Manifold Structure Alignment by Semantic Feature Expansion .
    To address the domain shift problem, we propose a novel model to
    implicitly align the manifold structures between the visual and
    semantic feature spaces by expanding the semantic feature space.
    Specifically, we build upon an autoencoder-based model that takes
    the visual features as input to generate/expand several auxiliary
    semantic features for each prototype. We then combine these
    auxiliary and predefined semantic features to discover better
    adaptability for the semantic feature space. This adaptability is
    mainly achieved by aligning the manifold structure of the combined
    semantic feature space, to an embedded manifold structure extracted
    from the original visual feature space of data. To simultaneously
    supervise the expansion and alignment phases, we propose to combine
    the reconstruction term with an alignment term within the
    autoencoder-based model. Our model is the first attempt to align
    both feature spaces by expanding semantic features and derives two
    benefits: first, we expand some auxiliary features that enhance the
    semantic feature space; second and more importantly, we implicitly
    align the manifold structures between the visual and semantic
    feature spaces, which resulting in a more robust well-matched
    visual-semantic mapping function and can better mitigates the domain
    shift problem. Extensive experiments show remarkable performance
    improvement and verifies the effectiveness of our method.

-    Zero-shot Learning as a Graph Recognition .
    To take a further step on mitigating the domain shift problem, our
    interest is to focus on the fine-grained perspective. We propose a
    fine-grained zero-shot learning framework based on the example-level
    graph. Specifically, we decompose an image example into several
    parts and use a graph-based model to measure and utilize certain
    relations between these parts. Taking advantage of recently
    developed graph neural networks (GNNs), we formulate the zero-shot
    learning to a graph-to-semantic mapping problem which converts the
    zero-shot recognition to a graph recognition task. Our method can
    better exploit part-semantic correlation and local substructure
    information in examples, and makes the obtained visual-semantic
    mapping function more robust and accurate. Experimental results
    demonstrate that the proposed method can mitigate the domain shift
    problem and achieve competitive performance against other
    representative methods.

### 1.3 Thesis Organization

The rest of this thesis consists of five chapters and organized as
follows.

-    Chapter 2
    This chapter reviews the background knowledge and some related works
    of this thesis. First, we briefly introduce the semantic feature
    space which is critical for zero-shot learning in section 2.1 ,
    including the categories of semantics, pros and cons. Next, we
    explain and compare the learning frameworks and datasets of the
    zero-shot learning in section 2.2 . Last, we introduce the domain
    shift problem which is the open issue and main challenge in
    zero-shot learning.

-    Chapter 3
    This chapter presents our study on the adaptive adjustment and
    rectification of semantic feature space for zero-shot learning,
    which can helps to mitigate the domain shift and hubness problems.
    In particular, section 3.2 introduces some related work. In section
    3.3 , we explain our proposed method regarding the adjustment on
    class prototypes and global distribution, and the formulation to a
    more efficient unified training framework. The experiments including
    the results and analysis are addressed in section 3.4 .

-    Chapter 4
    This chapter presents our study on the alignment of manifold
    structures between the visual and semantic feature spaces, which is
    typically achieved by expanding semantic features. In particular, we
    introduce some related work in section 4.2 . Then section 4.3
    explains our proposed method including the semantic feature
    expansion, manifold extraction and alignment, etc. The experimental
    results and analysis are addressed in section 4.4 .

-    Chapter 5
    This chapter presents our study on the conversion of zero-shot
    learning to the graph recognition task, which is a fine-grained
    zero-shot learning framework based on the example-level graph. In
    particular, section 5.2 introduces some related work. In section 5.3
    , we explain our proposed method including the parts decomposition,
    example graph construction and example graph recognition. In section
    5.4 , we introduce the experimental results and analysis.

-    Chapter 6
    This chapter summarizes this thesis and provides some potential
    future research directions.

## Chapter 2 Background Review

This chapter reviews some background knowledge and related work. We
first introduce the semantic feature space which is the key building
block of zero-shot learning in Section 2.1 . Then we introduce the
learning framework, models and datasets of zero-shot learning in Section
2.2 . Last, we explain the domain shift problem, which is the main
challenge in Section 2.3 .

### 2.1 Semantic Feature Space

The semantic feature space is critical for zero-shot learning. Since the
unseen class examples are not available during training, it may be
difficult for existing methods to do the domain adaptation to transfer
learned knowledge from seen to unseen classes. Thus, some common
knowledge is required to discover and model how an unseen class can be
related to seen classes. In zero-shot learning, this common knowledge is
considered as the semantic feature space which is shared by both seen
and unseen classes [ 61 ] . The corresponding semantics (a.k.a.,
semantic features) are generally per-class high-level features
describing a particular class. In zero-shot learning, each class is
endowed with a unique semantic feature representation which is called
semantic prototype.

The semantic features work as a bridge between seen and unseen classes.
By sharing a common feature space, each class can be easily implemented
and connected to other classes if exist. For example, as shown in Figure
2.1 , the “zebra” can be recognized as an animal with a “horse”-like
body, “tiger”-like stripes, and “panda”-like black and white color
appearance. Because the semantic features are global high-level
descriptions which are semantically meaningful to each class, thus,
intuitively, for any descriptions the cat is more closely related to the
tiger than to the snake. This intuition should be also held in the
semantic feature space, which means that under certain metrics, e.g.,
cosine similarity, the cat prototype should be more similar to the tiger
prototype than to the snake prototype, which resulting similar classes
reside nearby in the semantic feature space.

#### 2.1.1 Categories of Semantics

The semantic features can be roughly grouped into two mainstreams based
on either supervised or unsupervised settings. For the supervised
setting, the semantic attributes are the most popular semantics and are
adopted in various zero-shot learning models [ 27 , 61 , 84 , 107 ] .
The attributes are usually generated/defined and verified by human
experts, which is some kind of laborious task. For the unsupervised
setting, the semantic word vectors are usually learned from some
large-scale unannotated linguistic knowledge corpus, e.g., from
Wikipedia, news, etc. Such kind of word embeddings are widely used in
natural language processing problems and can be efficiently extended to
zero-shot learning. Among them, word2vec [ 73 , 74 ] , FastText [ 50 ]
and GloVe [ 85 ] vectors are most frequently used [ 114 , 129 , 10 , 37
] . Now, we elaborate on different categories of semantic features as
follows.

Attribute: In the attribute space, a list of human understandable
characteristics describing various properties of the classes are defined
as attributes. The attributes are meaningful high-level common
descriptions of classes, where each attribute is usually a word or a
phrase corresponding to one particular property of these classes. For
example, in an animal image recognition problem, the attributes can be
defined as various properties of animals such as their body colors
(e.g., “black”, “white”, “brown”, etc.), their habitats (e.g., “water”,
“forest”, “desert”, etc.), their components (e.g., “stripes”, “spots”,
“lumps”, etc.) and so on. These attributes are then used to form the
semantic feature space which is shared and can transfer learned
knowledge between the seen and unseen classes. For the semantic
prototype of each class, the values of each dimension are determined by
whether the class has a corresponding attribute. Taking the most simple
animal recognition problem as the example, suppose there are 6
attributes, i.e., “black”, “white”, “brown”, “stripes”, “water”, and
“eats fish”, describing each class. In this attribute space, the “otter”
can be recognized as one kind of animal who has black and brown fur,
lives in/near the water, and eats fish. Thus, the “otter” prototype can
be presented by a semantic attribute vector as @xmath , where each
dimensional binary value “0/1” indicates whether the corresponding
attribute exists or not. The binary attribute space is the most simple
case of semantic feature space. In general, the attribute can also be
real numbers indicating the degree or confidence of a class having a
corresponding attribute, which is the continuous attribute space .
Moreover, there also exist the relative attribute space which can
measure the relative degree or confidence of having an attribute among
different classes. Given an input example image, the traditional
recognition models directly classify the label or name of the example,
without paying any attention to various attributes. To solve this
problem, several investigations have been made to predict the class
attribute before classifying the labels or names [ 27 , 61 , 107 , 84 ]
. By using the attributes, we can develop more advanced models to
describe, compare, and classify examples easily in a human
understandable format. Moreover, the attributes can also help to
classify novel or unseen class by the shared attribute space where the
certain combinations of known attributes may become an unseen class [ 62
, 115 ] .

Word2vec: The word2vec is a continuous-valued high-dimensional vector
representation of a linguistic word of a vocabulary [ 73 , 74 ] . They
are learned from billions of words. Moreover, the vocabulary collated in
this manner contains millions of words itself. Word vector is first
investigated in the area of natural language processing (NLP). Two basic
architectures named continuous bag-of-words (CBOW) and continuous
skip-gram are proposed to generate word2vec vectors [ 73 ] . Both
architectures contain a two-layer neural network with non-linear
activation. Within an input sentence, CBOW defines a word as the current
word. Considering the words adjacent to our word of interest in a
sentence as input to the network, the method aims to predict the current
word. In contrast, the skip-gram method aims at predicting the previous
and future word based on the input of the current word. In this way, the
word2vec vectors are learned by considering the similarity of the word
with the context of the descriptions. Therefore, algebraic operations
within word2vec vectors can also demonstrate linguistic regularities (as
demonstrated in Figure 2.2 ). For example, if we subtract the vector
“Man” from the vector “king”, and add the vector ”woman”, the resulting
vector should be close to the vector “queen”:

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

By using the word2vec as the semantic feature space, the above
advantages can be easily extended to zero-shot learning by mapping
examples from the visual to the word2vec embedding space, and be
recognized by searching the mostly closely related prototypical vector.

FastText: The FastText [ 50 ] is an extended version of Word2Vec. It is
an open-sourced library from Facebook containing pre-trained models of
word vectors of 294 languages. The key difference from word2vec is that
FastText breaks a word into several character n-grams or sub-words. So
that the vector for a word is made of the sum of this character n-grams.
For example, the word vector “apple” can be the sum of the vectors of
the n-grams: “ap”, “app”, “appl”, “apple”, “apple”, “ppl”, “pple”,
“pple”, “ple”, “ple” and “le”. In contrast to the word2vec which learns
vectors only for complete words found in the training corpus. FastText
aims to learn vectors for the n-grams that are found within each word,
as well as each complete word. At each training step in FastText, the
mean of the target word vector and its component n-gram vectors are used
for training. The adjustment that is calculated from the error is then
used uniformly to update each of the vectors that were combined to form
the target. The above process brings about more calculation resource
while the resulting vectors have been shown to be more accurate than
word2vec by different metrics.

GloVe: The GloVe [ 85 ] is an another important type of word
representation. It is generated using a log-bilinear model trained on
the word to word co-occurrence statistics of a given corpus. The
training tries to match the dot product of any given pair of the
vocabulary word vectors to the logarithm co-occurrence probability of
that pair. GloVe vectors are especially useful for the word analogy
task.

#### 2.1.2 Semantic Pros and Cons

To generate the semantic features of any class, we can either use the
supervised or unsupervised settings. The supervised features are the
attributes that are usually generated/defined and verified manually by
human experts [ 107 , 62 ] . Thus, the attributes are more capable of
describing each class with less noise and obtains a more accurate
semantic prototype for object classes. Being widely used in various
zero-shot learning methods, however, since the attributes require
considerable human efforts to acquire annotations, the process is
usually costly. As a workaround, the unsupervised features, i.e., word
vectors, can be generated automatically from a large corpus of
unannotated text, e.g., Wikipedia, news, etc., or the hierarchical
relationship of classes in WordNet corpus [ 75 ] . Some widely used
examples of such semantic features are word2vec [ 73 , 74 ] , FastText [
50 ] and GloVe [ 85 ] . Since these vectors are generated in an
unsupervised manner, there may exist more noisy components compared to
the attributes. Despite the defect, the word vectors are more flexible
and can provide more scalability compared to the manually acquired
attributes. We visualize the semantic attributes and semantic word
vectors based on CUB-200-2011 dataset [ 107 ] in Figure 2.4 and Figure
2.4 , respectively. It can be observed that these semantic prototypes
are clustered better in attribute space than word vector space. In this
thesis, our works are based on both attributes and word vectors to
implement our methods. Specifically, we mainly use the attributes for
small and medium scale experiments, and use the word vectors for large
scale experiment.

### 2.2 Zero-shot Learning

As stated, one can construct a mapping function between the visual and
semantic feature spaces to establish the cross-modal retrieval for
zero-shot leaning task. The key is to map examples and classes into the
same latent space and to perform the similarity search (e.g., nearest
neighbor search). Based on the mapping directions, the main learning
frameworks of zero-shot learning can be grouped into three ways: Forward
Mapping, Reverse Mapping, and Intermediate Mapping (Figure 2.5 ). We now
elaborate different learning frameworks including the above three
typical ones and others, and introduce the benchmark datasets of
zero-shot learning.

#### 2.2.1 Learning Framework

Forward Mapping: The forward mapping is the most widely used mapping in
zero-shot learning. As shown in Figure 2.5 .(a), it refers to finding a
mapping function that maps from the visual feature space to the semantic
feature space. SOC [ 82 ] maps the visual features to the semantic
feature space and then searches the nearest class embedding vector.
Akata et al. [ 2 ] proposed to view attribute-based image classification
as a label-embedding problem, and introduce a function which measures
the compatibility between an image and a label embedding. SJE [ 4 ]
optimizes the structural SVM loss to learn the bilinear compatibility,
while ESZSL [ 92 ] utilizes the square loss to learn the bilinear
compatibility and adds a regularization term to the objective with
respect to the Frobenius norm. ALE [ 3 ] trains a bilinear compatibility
function between the semantic attribute space and the visual space by
ranking loss. Similarly, DeViSE [ 28 ] also trains a linear mapping
function between visual and semantic feature space by an efficient
ranking loss formulation. Bucher et al. [ 15 ] embedded the visual
features into the attribute space. Recently, SAE [ 57 ] uses a semantic
autoencoder to regularize zero-shot recognition. Xian et al. [ 114 ]
extended the bilinear compatibility model of SJE [ 4 ] to be a piecewise
linear model by learning multiple linear mappings with the selection
being a latent variable. Socher et al. [ 97 ] used a deep learning model
that contains two hidden layers to learn a nonlinear mapping from the
visual feature space to the semantic word vector space [ 74 ] . Unlike
other works which build their embedding on top of fixed image features,
Ba et al. [ 63 ] trained a deep CNNs and learning a visual to semantic
embedding. SP-AEN [ 19 ] introduces an independent visual-to-semantic
mapping that disentangles the semantic space into two subspaces and an
adversarial-style discriminator between them to prevent the semantic
loss and improve the zero-shot recognition.

Reverse Mapping: In contrast to forward mapping, some investigations
reported the hubness phenomenon in zero-shot learning that mapping the
high-dimensional visual features to the low-dimensional semantic feature
space could reduce the variance of features, and the results may become
more clustered [ 25 , 96 , 127 ] . Thus, some researchers propose to
reversely map from the semantic to visual feature spaces (Figure 2.5
.(b)). Dinu et al. [ 25 ] proposed to take the proximity distribution of
potential neighbours across many mapped vectors into account to correct
the hubness. Shigeto et al. [ 96 ] analyzed the mechanism behind the
emergence of hubness and proved that mapping labels into the visual
feature space is desirable to suppress the emergence of hubs in the
subsequent nearest neighbor search step. Ba et al. [ 8 ] and Zhang et
al. [ 127 ] both proposed to train a deep neural network to map the
semantic features to the visual feature space. Changpinyo et al. [ 17 ]
proposed a simple model based on a support vector regressor to map the
semantic features to the visual feature space and performed
nearest-neighbor algorithms. However, since each class prototype or
label has various corresponding visual features in different examples,
the conventional reverse mapping may still problematic. Thus, some
recent works proposed to generate some examples or visual features based
on the semantic features of classes, to increase the diversity of
examples especially for unseen classes. For example, CVAE-ZSL [ 77 ]
proposes to generate various visual examples from given semantic
attribute by conditional variational autoencoders (VAEs), and uses the
generated examples for classification of the unseen classes. GFZSL [ 104
] models each class-conditional distribution as a Gaussian and learns a
regression that maps a class embedding into the latent space. SE-GZSL [
60 ] also adopts the VAEs architecture that consisting of a
probabilistic encoder and a probabilistic conditional decoder to
generate novel visual examples from seen/unseen classes. f-CLSWGAN [ 116
] proposes a novel generative adversarial network (GAN)-based model to
generate CNNs features rather than real examples conditioned on
class-level semantic features, which can offer a shortcut from a
semantic descriptor of a class to a class-conditional feature
distribution.

Intermediate Mapping: The intermediate mapping refers to finding an
intermediary feature space that both the visual and semantic features
are mapped to [ 68 ] (Figure 2.5 .(c)). The intermediate mapping can
also be considered as the metric learning which learns to compare or
evaluate the relatedness of a pair of visual and semantic features. SSE
[ 130 ] utilizes the mixture of seen class parts as the intermediate
feature space; then, the examples belonging to the same class should
have similar mixture patterns. Zhang et al. [ 131 ] proposed to map the
visual features and semantic features to two separate intermediate
spaces. Additionally, some researchers also proposed several hybrid
models to jointly embed several kinds of textual features and visual
features to ground attributes [ 31 , 1 , 67 , 16 ] . Lu et al. [ 68 ]
proposed to linearly combine the base classifiers trained in a
discriminative learning framework to construct the classifier of unseen
classes. RelationNet [ 99 ] proposes to take a pair of visual and
semantic features as input and learns a deep metric network which
calculates their similarity. DCN [ 65 ] proposes to map the visual
features of image examples and the semantic prototypes of classes into
one common embedding space, thus the compatibility of seen classes to
both the source and target classes can be maximized.

Others: Expect for the above three typical learning frameworks, there
also exist several other methods including either earlier and recently
proposed ones. Some previous works of ZSL usually take utilization of
the attributes in a two-phase model to predict the label of an example
belonging to one from unseen classes [ 53 , 79 , 47 , 62 , 5 ] . In the
first stage, the model predicts the attribute feature of an example.
Then, in the second stage, the class label is predicted by searching the
class that attains the most similar set of attributes. For example, DAP
[ 62 ] proposes to first estimates the posterior of all attributes of
the input visual example by learning probabilistic classifiers on
attributes, and then computes the classes posteriors and infers the
class labels by MAP estimate. CAAP [ 5 ] proposes to first learn a
probabilistic classifier for each attribute and use the random forest to
estimate the class posteriors. IAP [ 62 ] chooses to first predicts the
class posterior of seen classes and then to uses the probability of
every class to compute the attribute posteriors of examples.
Additionally, the two-stage approach can also be extended to the
scenario where attributes are not available. For example, based on IAP [
62 ] , CONSE [ 79 ] chooses to first predicts the posteriors of seen
classes, then it maps the visual features into the Word2vec [ 74 ]
feature space. Recently, some graph-based methods have been proposed to
handle to zero-shot learning. For example, Wang et al. [ 110 ] proposes
to use graph neural networks to present and propagate information among
each class. DGP [ 51 ] further extends it to a dense graph propagation
module to jointly consider the distant nodes. So that the model can make
use of the hierarchical structural information of graphs. Meanwhile,
some recent methods such as @xmath [ 48 ] , Zhu et al. [ 135 ] and Chen
et al. [ 18 ] , which make use of attention mechanism on local regions
or learn dictionaries through joint training with examples, attributes
and labels are proposed to focus on a more fine-grained perspective of
zero-shot learning.

In our thesis, we mainly contribute to the forward mapping, graph and
fine-grained based methods. Specifically, the methods proposed in
Chapter 3 and Chapter 4 mainly consider the forward mapping in zero-shot
learning, while the method proposed in Chapter 5 mainly consider graph
and fine-grained based model.

#### 2.2.2 Datasets

There are five widely used benchmark datasets for zero-shot learning
including Animals with Attributes ¹ ¹ 1 http://cvml.ist.ac.at/AwA/ (AWA)
[ 62 ] , CUB-200-2011 Birds ² ² 2
http://www.vision.caltech.edu/visipedia/CUB-200-2011.html (CUB) [ 107 ]
, aPascal&Yahoo ³ ³ 3 http://vision.cs.uiuc.edu/attributes/ (aPa&Y) [ 27
] , SUN Attribute ⁴ ⁴ 4 http://cs.brown.edu/ gmpatter/sunattributes.html
(SUN) [ 84 ] , and ILSVRC2012 ⁵ ⁵ 5
http://image-net.org/challenges/LSVRC/2012/index / ILSVRC2010 ⁶ ⁶ 6
http://image-net.org/challenges/LSVRC/2010/index (ImageNet) [ 93 ] .
Among them, the first four are small and medium scale datasets, and
ImageNet is a large-scale dataset. The basic description of these
datasets is listed in Table 2.1 .

Animals with Attributes: The AWA [ 62 ] is an animal example collection
from public sources such as Flickr. It consists of 30,475 images of 50
animal classes, of which 40 of them are seen classes and the remaining
10 animals are unseen classes. In the dataset, each class is represented
by an 85-dimensional numeric attribute feature vector as the prototype.
By using the shared attributes, it is possible to transfer information
between different classes. For the zero-shot learning setting, the 40
seen classes including 24,295 images are used for training, and the
remaining 10 unseen classes associating with 6,180 images are used for
testing.

CUB-200-2011 Birds: The CUB [ 107 ] is a bird example collection from
Flickr image search and then further filtered by showing each image
example to various users of Mechanical Turk [ 112 ] . All image examples
in CUB are annotated with attribute labels, bounding boxes and part
locations. The dataset consists of 11,788 images of 200 bird species,
from which 150 of them are seen classes and the remaining 50 species are
unseen classes. In the dataset, each class is represented by a
312-dimensional semantic attribute feature vector as the prototype. The
attributes are generally visual in nature, with most pertaining to a
pattern, shape, or color of a particular part. For the zero-shot
learning setting, 8,855 images within 150 seen classes are used for
training, and the remaining 2,933 images within 50 unseen classes are
used for testing.

aPascal&Yahoo: The aPa&Y [ 27 ] is a natural object example collection
consists of 15,339 images. The dataset is formed by two subsets
including the Pascal VOC 2008 dataset and the Yahoo dataset. Among them,
the Pascal VOC 2008 dataset contains 20 classes of 12,695 natural object
image examples such as people, cat, bicycle, bus, sofa, etc. Each class
is presented by a 64-dimensional attribute feature vector as the
prototype. The Yahoo dataset is a supplementary to the Pascal VOC 2008
dataset which consists of 2,644 natural object image examples from 12
additional classes by the Yahoo image search, such as monkey, bag,
carriage, mug, etc. The annotations of the Yahoo dataset follow the same
standard as the Pascal VOC 2008 dataset, each class prototype is
presented as a 64-dimensional attribute feature vector. For the
zero-shot learning setting, the Pascal VOC 2008 dataset is used as seen
classes for training, and the Yahoo dataset is used as unseen classes
for testing.

SUN Attribute: The SUN [ 84 ] is a scene image example collection
consists of 717 classes, such as river, ice cave, railroad, forest, etc.
In this dataset, each class is presented by a 102-dimensional attribute
feature vector as the prototype. These discriminative attributes are
obtained by crowd-sourced human studies and cover various properties
such as surface, materials, lighting, functions/affordances, and spatial
envelope, etc. The SUN dataset contains total 14,340 image examples and
has two commonly used standard splits including 707/10 and 645/72. In
our zero-shot learning experimental settings, we consider the latter
645/72 split from which 645 classes are used as seen classes for
training, and the remaining 72 classes are used as unseen classes for
testing.

ILSVRC2012 / ILSVRC2010: The ImageNet [ 93 ] is a large scale benchmark
dataset for zero-shot learning which covers a wide range of objects from
Flickr and other search engines. The dataset consists of total 1,360
classes from two parts including ILSVRC2012 and ILSVRC2010. For the
zero-shot learning setting, 1,000 classes containing @xmath image
examples from the ILSVRC2012 are used as seen classes for training, and
the remaining 360 classes containing @xmath image examples from
ILSVRC2010 which are not included in ILSVRC2012 are used as the unseen
classes for testing. The prototype of each class is presented by a
1,000-dimensional semantic word vector trained by a skip-gram text model
on a corpus of 4.6M Wikipedia documents.

### 2.3 Domain Shift Problem

The domain shift problem [ 29 ] is an open issue in zero-shot learning
which refers to the phenomenon that when mapping an unseen class example
from the visual to semantic feature spaces, the obtained semantic
features may easily shift away from its real prototype. As shown in
Figure 2.6 , in the semantic feature space, both the “Zebra” and “Pig”
are endowed with the same attribute property “hasTail”. However, their
visual “Tail” appearances are considerably different from each other in
the visual feature space. Thus, if we transfer the mapping function
learned from the “Zebra” to infer the “Pig” examples, the obtain
semantic features of these “Pigs” may not reside or even far away from
the real “Pig” prototype. The domain shift problem is essentially caused
by the nature of zero-shot learning: the training (seen) and testing
(unseen) classes are mutually disjoint from each other. Moreover, the
visual and semantic feature spaces are generally independent.
Specifically, the visual features represented by high-dimensional
vectors are usually not semantically meaningful, and the semantic
features represented by attributes or word vectors are not visually
meaningful as well. Thus, these two feature spaces may exist in entirely
different manifolds, and makes it difficult to obtain a well-matched
mapping function between them. This visual-semantic gap can further
exacerbate the domain shift problem and weakens the performance of
zero-shot recognition models.

Recently, several investigations have been made to mitigate the domain
shift problem including inductive learning-based methods, which enforce
additional constraints from the training data [ 31 , 16 ] , and
transductive learning-based methods, which assume that the unseen class
examples or their visual features (unlabeled) are also available during
training [ 29 , 64 , 98 ] . It should be noted that the model
performance of the transductive paradigm is generally better than that
of the inductive paradigm, because of the utilization of extra
information from unseen classes during training can naturally mitigate
the domain shift problem. However, the transductive paradigm does not
fully comply with the zero-shot setting: no unseen class examples are
available during training. With the popularity of generative adversarial
networks (GANs), some generative-based methods have also been proposed
recently. GANZrl [ 102 ] applied GANs to synthesize examples with
specified semantics to cover a higher diversity of seen classes. In
contrast, GAZSL [ 134 ] leverages GANs to imagine unseen classes from
text descriptions. Despite the progress made, however, since most
existing methods inherently lack enough adaptability to the
visual-semantic correlation when constructing the mapping function, the
domain shift problem is still an open issue hindering the further
development of zero-shot learning. In this thesis, we focus on fully
empowering the semantic feature space, one of the key building block, to
explore effective ways towards better performance on mitigation of the
domain shift problem.

## Chapter 3 Rectification on Class Prototype and Global Distribution

In most recent years, zero-shot learning (ZSL) has gained increasing
attention in multimedia and machine learning areas. It aims at
recognizing unseen class examples with knowledge transferred from seen
classes. This is typically achieved by exploiting a semantic feature
space, i.e., semantic attributes or word vectors, as a bridge to
transfer knowledge between seen and unseen classes. However, due to the
absence of unseen class examples during training, the conventional ZSL
models easily suffers from domain shift and hubness problems. In this
chapter, we propose a novel ZSL model that can handle these two issues
well by adaptively adjusting and rectifying semantic feature space.
Specifically, this adjustment is conducted by jointly considering both
the class prototypes and the global distribution of data. To the best of
our knowledge, our work is the first to consider the adaptive adjustment
of semantic feature space in ZSL. Moreover, we also formulate the
adjustment process to a more efficient training framework by combining
it with a cycle mapping, which significantly boosts the training. By
using the proposed method, we could mitigate the domain shift and
hubness problems and obtain more generalized results to unseen classes.
Experimental results on several widely used benchmark datasets show the
remarkable performance improvement of our method compared with other
representative methods.

### 3.1 Introduction

Zero-shot learning (ZSL) imitates human ability in recognizing novel
unseen classes. ZSL is achieved by exploiting labeled seen class
examples and a semantic feature space, e.g., attribute or word vector
space, which is shared and can be transferred between seen and unseen
classes [ 62 , 88 ] . In ZSL, the common practice is to map an unseen
class example from its original feature space, e.g., visual feature
space, to the semantic feature space by a mapping function learned on
seen classes. Then with such obtained semantic feature vector, we search
its most closely related semantic prototype whose corresponding class is
set to this example. Specifically, this relatedness can be measured by a
certain similarity metrics, e.g., cosine similarity, between two
semantic prototypes. Thus, we can use some simple algorithms such as the
K-Nearest-Neighbour (KNN) to search the class prototypes.

However, this mapping function is trained solely on seen classes that
concerns the mapping from the visual to semantic feature spaces with
only seen class examples. Although the semantic feature space is shared
by both seen and unseen classes, the training and testing classes are
intuitively different. Due to the absence of unseen classes during
training, ZSL easily suffers from the domain shift problem [ 29 ] ,
which means that the mapping function learned from source domain (seen
classes) may not well-adopted to the target domain (unseen classes) when
transferring and reusing it to infer unseen class examples. Moreover,
during the KNN search, a small number of semantic prototypes may easily
become the most closely related semantic prototypes to most testing
unseen class examples and become the hubs [ 96 ] . The hubness commonly
exists in most similarity or distance based algorithms, while the causes
are still under investigation [ 87 ] . The domain shift and hubness
problems hinder the performance of ZSL models and become the open
challenges. Several investigations have been made to mitigate the domain
shift problem including inductive learning-based methods, which enforce
additional constraints from the training data [ 31 , 16 ] , and
transductive learning-based methods, which assume that the unseen class
examples or their visual features (unlabeled) are also available during
training [ 29 , 64 , 98 ] . Despite the efforts made, most existing
methods still have some drawbacks that need further investigation. For
example, some key building blocks in ZSL, e.g., the semantic feature
space itself and the inherent data distribution, do not seem to receive
comparable attention. Conventional ZSL methods usually treat the
semantic feature space as unchangeable features and keep each class
prototype fixed during training. However, based on our observation, the
mapped unseen class examples are usually quite concentrated in the
semantic feature space. Worse still, some class prototypes are also too
closely distributed. These deficiencies affect the model’s ability to
adapt and generalize to unseen classes. As we know, the process of human
beings understanding things is constantly improving. Similarly, we argue
the unchangeable semantic feature space as another inducement for the
domain shift and hubness problems.

To address these problems, we propose a novel ZSL model combined with a
cycle mapping to adaptively adjust the semantic feature space (Figure
3.1 ). Specially, this adjustment is conducted on both the semantic
prototypes and the global distribution of data, focus on decreasing the
intra-class variance and enlarging the inter-class diversity. This
adaptively adjustment has advantages in two aspects: first, in the
statistical point of view, the semantic features become more
discriminative and powerful. Second, in the geometrical point of view,
the obtained semantic features could be more spatially separated and has
more diversity. Thus, the semantic feature space can be better shared
and transferred between seen and unseen classes.

Moreover, we further combine the above adjustment with a cycle mapping,
e.g., an encoder-decoder structure, to formulate our solution to a more
efficient training framework. Our model first maps examples from visual
to semantic feature spaces, and then from semantic to visual feature
spaces vice versa. This cycle mapping makes the visual-semantic mapping
function more faithful and robust, which can not only obtain the
semantic features, but can also retain and embed more information from
the visual features. Last, we construct the whole training process to a
unified framework and formulate it to a generalized Lyapunov equation
that significantly boosts the training efficiency. Experimental results
on several benchmark datasets demonstrate the effectiveness of our
method. Our contributions can be summarized as follows:

-   The first model proposed to adaptively adjust the semantic feature
    space for zero-shot learning.

-   We combine the adjustment with a cycle mapping to further obtain the
    robust mapping function and boosts the training efficiency.

-   Our method can better handle the domain shift and hubness problems.

### 3.2 Related Work

#### 3.2.1 Existing Works

There are several recently proposed works have partially addressed the
domain shift problem. TMV-HLP [ 29 ] proposes the transductive
multi-view ZSL which assumes that the unseen class examples (unlabeled)
are also available during training. DeViSE [ 28 ] trains a linear
mapping function between visual and semantic feature spaces by an
effective ranking loss formulation. ESZSL [ 92 ] applie the square loss
to learn the bilinear compatibility and adds regularization to the
objective with respect to Frobenius norm. SSE [ 130 ] proposes to use
the mixture of seen class parts as the intermediate feature space. AMP [
15 ] embeds the visual features into the attribute space. RRZSL [ 96 ]
argues that using semantic space as shared latent space may reduce the
variance of features and causes the domain shift, and thus proposes to
map the semantic features into visual feature space for zero-shot
recognition. SynC @xmath [ 16 ] and CLN+KRR [ 67 ] propose to jointly
embed several kinds of textual features and visual features to ground
attributes. Similarly, JLSE [ 131 ] proposes a a joint discriminative
learning framework based on dictionary learning to jointly learn model
parameters for both domains. SAE [ 57 ] proposes to use a linear
semantic autoencoder to regularize the zero-shot learning and makes the
model generalize better to unseen classes. MFMR [ 119 ] utilizes the
sophisticated technique of matrix tri-factorization with manifold
regularizers to enhance the mapping function between the visual and
semantic feature spaces. Differently, RELATION NET [ 99 ] learns a
metric network that takes paired visual and semantic features as inputs
and calculates their similarities. Chen et al. [ 18 ] proposed learn
dictionaries through joint training with examples, attributes and labels
to achieve the zero-shot recognition. With the popularity of generative
adversarial networks (GANs), CAPD-ZSL [ 104 ] proposes a simple
generative framework for learning to predict previously unseen classes,
based on estimating class-attribute-gated class-conditional
distributions. GANZrl [ 102 ] proposes to apply GANs to synthesize
examples with specified semantics to cover a higher diversity of seen
classes. Instead, GAZSL [ 134 ] applies GANs to imagine unseen classes
from text descriptions. Recently, SGAL [ 122 ] uses the variational
autoencoder with class-specific multi-modal prior to learn the
conditional distribution of seen and unseen classes.

Despite the progress made, most of these methods ignore one of the key
building blocks in ZSL, i.e., the semantic feature space, which hinders
further mitigation of the domain shift problem. Worse still, since the
causes for hubness problem are still under investigation in other areas,
most of these ZSL methods can hardly handle the hubness problem.

#### 3.2.2 Autoencoder

The basic autoencoder was first introduced in 1986 and recently became
popular and widely used in various applications in machine learning and
data mining areas. The autoencoder is an encoder-decoder structural
networks which can automatically learn a latent feature representation
of data. Firstly, the encoder is given an input data example @xmath ,
then the encoder maps the input example to a latent feature space in
which we obtain the latent feature representation @xmath of @xmath as:

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

where @xmath is the weight of encoder and @xmath is a bias. Then the
latent feature representation @xmath is mapped back to its original
feature space by the decoder, and be further reconstructed as @xmath :

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

where @xmath is the weight of decoder and @xmath is a bias. The error
between the original input data example @xmath and the reconstructed
@xmath can be calculated as:

  -- -------- -------- -------- -------
     @xmath   @xmath            (3.3)
                       @xmath   
  -- -------- -------- -------- -------

The autoencoder forces the latent feature representation to retain the
most powerful information of input data. The optimization target of the
autoencoder is to minimize the error or loss function with respect to
the input and output to achieve a better reconstruction ability and at
the same time obtain a powerful latent feature representation of data.
In recent years, some variants extended from the vanilla autoencoder
have been proposed. These variants usually associate with some
regularizers to achieve different objectives. Some representative
methods include Denoising AE [ 105 ] , Sparse AE [ 118 ] , Graph AE [
123 ] , Winner-take-all AE [ 72 ] , Similarity-aware AE [ 21 ] , etc. In
our method, we apply the autoencoder to form the cycle mapping and
combine with the adaptive adjustment of semantic feature space to
construct our unified framework.

### 3.3 Methodology

In this section, we elaborate on the design of our proposed method. We
first introduce the cycle mapping which is mainly implemented by an
autoencoder structural network. Then we introduce the adaptive
adjustment of the semantic feature space regarding the class prototypes
and the global data distribution. Last, we construct a unified framework
of the whole training process and further formulate it to a more
efficient solution.

#### 3.3.1 Cycle Mapping

There usually consists of two phases of ZSL: mapping and searching.
Taking the visual to semantic mapping as an example, the model first
maps the visual features of an unseen class example to the semantic
feature space. Then with the obtained semantic feature vector, the model
searches the most closely related semantic prototype and sets the class
corresponding with this prototype to the testing example. The
recognition can be described as:

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

where @xmath is a similarity metric that predicts the class @xmath of
the testing example @xmath . @xmath being the unseen class prototypes,
and @xmath being the mapping function with the trainable weight @xmath
which can map the visual features @xmath to the semantic feature space.
@xmath is a CNNs feature extractor which is usually trained by large
scale dataset. Specifically, the mapping function @xmath , i.e., as
demonstrated in the upper part of Figure 3.2 , is trained on labeled
seen class examples as:

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

where @xmath is the number of total seen class examples, @xmath is the
@xmath -th seen class example, and @xmath is the corresponding semantic
prototype. Similarly, for the semantic to visual mapping, i.e., as
demonstrated in the lower part of Figure 3.2 , we can also reversely map
an example from its semantic features (e.g., corresponding semantic
prototype) to the visual feature space as:

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

This reverse mapping is indeed to find a template visual feature
representation for each class that minimizes the variance among examples
within this class.

However, it should be noted that whether it is visual to semantic
mapping, i.e., @xmath , or semantic to visual mapping, i.e., @xmath ,
they are all independent training processes. Taking @xmath as an
example, we denote the whole mapped space as @xmath -dimensional feature
space as @xmath . In such a space, only a compact sub-space, i.e., we
denote as @xmath , can really represent the semantic feature space of
data. In practice, it is not difficult to train such a mapping function
approaching @xmath , and due to the supervision, i.e., semantic class
prototypes, the obtained semantic feature space of data are certainly
can be used for recognition, regardless of whether identical to @xmath
or not. In our method, we propose to couple these two mappings, namely
cycle mapping, for joint training (Figure 3.2 ). Specifically, the cycle
mapping can be implemented by an encoder-decoder structure as:

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

Thus, given seen class examples, the training can be described as:

  -- -------- -------- -------- -------
     @xmath   @xmath            (3.8)
                       @xmath   
  -- -------- -------- -------- -------

where the cycle mapping @xmath first maps the visual features of
training examples to semantic feature space with @xmath , and then
reconstructs them by reversely mapping them back to visual feature space
with @xmath . The constraint @xmath is applied to learn the exact
mapping function between the visual and semantic feature spaces. By
using the cycle mapping structure, the obtained semantic feature space
can not only correctly describe the semantic features of examples, but
also retain as much information as it could from the visual feature
space by the reconstruction process. In our method, one may also
understand that the decoder part indeed provides an additional
regularization in an end-to-end manner and forces the mapped results
must be able to reconstruct the original inputs, which can make the
mapping more accurate.

#### 3.3.2 Adaptive Adjustment

Following the cycle mapping, we propose to adaptively adjust the
semantic feature space. Specifically, during each training epoch, we
jointly adjust the class prototypes and the global data distribution.
For the adjustment of class prototypes, we mainly consider the current
centroid and overall distribution of examples from each class in the
semantic feature space. This prototype adjustment is partly inspired by
PSO [ 7 , 26 ] , which simulates a kind of behavior performed by a group
of animals such as wild gooses for their adaptation of changing the
flight positions. For the adjustment of global data distribution, we
propose a regularization term to decrease the intra-class variance of
examples from each class, and enlarge the the inter-class diversity at
the same time. These adjustments are jointly conducted during the
training process and involved with the cycle mapping, to form a unified
framework, which will be explained in Section 3.3.3 .

##### 3.3.2.1 Seen Class Prototype

To adjust seen class prototypes, we focus on the current centroid and
overall distribution of examples from each class in the semantic feature
space as:

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

where @xmath and @xmath are the updated and current prototype of the
@xmath -th seen class, respectively. @xmath is the example and @xmath is
the total number of examples belonging to this class. @xmath is the
visual to semantic mapping that calculates the semantic features of
@xmath . @xmath and @xmath are two hyper-parameters used to control the
balance of these two terms. We can consider the above adjustment in the
following reason. First, for each class, the class prototype is usually
not the centroid of examples in the semantic feature space, which is
mainly caused by the mismatch between the diversity of various examples
and the identity of unique semantic attributes or word vectors. However,
this unchangeable semantic feature space hinders the improvement of
models to adapt to more unseen classes. Thus, it is necessary to do
adaptation, e.g., adjust the semantic prototypes. Second, since the
semantic attributes or word vectors are obtained from expertise or
learned knowledge which cannot be adjusted drastically. Therefore,
during each training epoch, the prototype is forced to move a small step
(controlled by @xmath ) towards the centroid @xmath of examples in the
semantic feature space (Figure 3.3 .(a)).

##### 3.3.2.2 Unseen Class Prototype

Compared to the adjustment of seen class prototypes whose visual
examples are available during training, we cannot adjust that
straightforward for unseen class prototypes. However, since the seen and
unseen classes share the common semantic feature space, e.g., attribute
space or word vector space, which can be transferred among classes, we
thus can have another strategy to deal with the unseen class prototypes
by associating with seen class prototypes. More specifically, for each
unseen class prototype, we focus on its nearest seen class prototype
neighbors and adjust the current unseen class prototype partially based
on these neighbors (Figure 3.3 .(b)). The adjustment can be described
as:

  -- -------- -- --------
     @xmath      (3.10)
  -- -------- -- --------

where @xmath and @xmath are the updated and current prototype of the
@xmath -th unseen class, respectively. @xmath ( @xmath ) are the @xmath
nearest seen class prototype neighbors of @xmath . The @xmath is a
similarity metric, e.g., cosine similarity, between two vectors. @xmath
and @xmath are two hyper-parameters used to control the balance of these
two terms. In the above adjustment, we calculate the similarity between
each pair of the unseen class prototype and its selected @xmath nearest
seen class prototype neighbors to obtain an updating score for each
neighbor as @xmath , which can represent the relative importance of each
neighbor contributing to the adjustment of an unseen class prototype.

##### 3.3.2.3 Global Distribution

The adjustment of class prototypes can rectify the mismatch between the
diversity of various examples and the identity of unique semantic
attributes or word vectors, which can help to mitigate the domain shift
problem. However, the global domain examples are usually not evenly
distributed. For example, some intra-class examples are too sparse from
each other, and some inter-class examples may have some overlaps in the
semantic feature space. This property exacerbates the hubness problem
and can also hinder the further mitigation of the domain shift problem.
To address these issues, we propose a regularization term to consider
both the identity within a class and the diversity among different
classes for the adjustment of global data distribution. The
regularization term can be described as:

  -- -------- -- --------
     @xmath      (3.11)
  -- -------- -- --------

where @xmath is the number of seen classes and @xmath is the number of
examples belonging to @xmath -th seen class @xmath . @xmath is the
semantic centroid of the @xmath -th seen class, which can be calculated
by averaging the mapped semantic feature vectors of examples belonging
to @xmath as @xmath . By using this regularization, we can force
intra-class examples to be more concentrated, and at the same time
mitigate the overlaps of inter-class examples in the semantic feature
space (Figure 3.3 .(c)).

#### 3.3.3 Unified Framework

In this section, we formulate our proposed method to a unified framework
and introduce our algorithm in detail. Specifically, our method contains
two components for the zero-shot recognition including the cycle mapping
process which mainly act as the base trainer, and the adaptation process
which is responsible for adjusting the semantic feature space. Our
method can be optimized alternately with Eqs. 3.8 @xmath 3.11 . We first
optimize Eq. 3.8 to obtain the initial weight of mapping function. Then
Eq. 3.9 and Eq. 3.10 are applied to perform the adaptive adjustment for
the semantic feature space by considering the class prototypes. Last,
Eq. 3.8 and Eq. 3.11 are jointly optimized to obtain an updated weight
of mapping function and adaptively adjust the global distribution at the
same time. These steps are performed iteratively to reach an optimum.

In general, our objective to be minimized in the combination of Eq. 3.8
and Eq. 3.11 can be described as:

  -- -- -------- -------- --------
        @xmath            (3.12)
                 @xmath   
  -- -- -------- -------- --------

where the first term is the cycle mapping, which is implemented by an
autoencoder structural network. @xmath is the number training examples
and @xmath is the number of seen classes. The current semantic centroid
of seen class @xmath is denoted as @xmath . A hyper-parameter @xmath is
used to balance these two terms. For simplicity, we rewrite the
objective to matrix form as:

  -- -- -- -------- --------
                    (3.13)
           @xmath   
  -- -- -- -------- --------

where @xmath is the matrix of visual features whose elements are
obtained by @xmath . @xmath and @xmath are the weight of the
visual-semantic mapping @xmath and semantic-visual mapping @xmath ,
respectively. @xmath and @xmath are class prototypes and semantic
centroids. Both of them are duplicated and re-organized to the shape of
@xmath , so that each semantic feature row in matrix @xmath has a
corresponding semantic prototype and centroid row in the matrix of
@xmath and @xmath , respectively. The objective of Eq. 3.13 contains two
parameter matrixes @xmath and @xmath , which are usually somehow
redundant and considerably increases the training cost. To further
optimize the model, we apply the tied weights [ 14 ] to half the
parameters to be optimized in our objective. The tied weights are proven
to be more efficient and can obtain similar results for the
encoder-decoder structural network. Hence, @xmath and @xmath can be
simplified to tied weights as @xmath and @xmath . In this step, we can
also substitute @xmath with @xmath for the cycle mapping term, our
objective can be rewritten as:

  -- -- -------- -------- --------
        @xmath            (3.14)
                 @xmath   
  -- -- -------- -------- --------

To solve this problem, we consider two operations. First, we consider to
relax the hard constraint @xmath in Eq. 3.14 to make our solution more
efficient:

  -- -------- -- --------
     @xmath      (3.15)
  -- -------- -- --------

where @xmath is also hyper-parameter controls the balance. By further
considering the trace properties of matrix, i.e., @xmath and @xmath , we
can rewritten our objective as:

  -- -- -- --------
           (3.16)
  -- -- -- --------

To solve it, we take a derivative of Eq. 3.16 with respect to @xmath .
Here, for the convenience of calculation, we divide the objective by 2
which does not affect the solution of the parameters:

  -- -------- -------- -------- --------
     @xmath   @xmath            (3.17)
                       @xmath   
                       @xmath   
                       @xmath   
                       @xmath   
  -- -------- -------- -------- --------

We set the result to zero, i.e., @xmath and obtain the following form:

  -- -------- -- --------
     @xmath      (3.18)
  -- -------- -- --------

We denote @xmath , @xmath , and @xmath . Then the above equation can be
rewritten as:

  -- -------- -- --------
     @xmath      (3.19)
  -- -------- -- --------

which is the standard form of generalized Lyapunov equation and can be
solved efficiently [ 86 , 11 ] . The unified framework of our method is
illustrated in Figure 3.1 . The training process is summarized in
Algorithm 1 . As to the inference process, since our method adopts the
autoencoder structure, so we can either use the visual-semantic mapping
or semantic-visual mapping to recognize unseen class examples. The
inference processes are summarized in Algorithm 2 and Algorithm 3 ,
respectively.

1: Seen class examples @xmath , seen class prototypes @xmath , and
unseen class prototypes @xmath .

2: Mapping weight @xmath , updated @xmath , and updated @xmath

3: Initialization: optimize Eq. 3.8 to obtain initial mapping weight
@xmath .

4: repeat 3-5:

5: Adjust seen class prototypes with Eq. 3.9 ;

6: Adjust unseen class prototypes with Eq. 3.10 ;

7: Update the model, optimize Eq. 3.19 to obtain updated mapping weight
@xmath ;

8: until : find an optimum.

9: return @xmath , @xmath , @xmath .

Algorithm 1 Training process of our method

1: Mapping weight @xmath , unseen class examples @xmath , unseen class
prototypes @xmath , and unseen class labels set @xmath .

2: Predicted class labels of @xmath : @xmath

3: Inference:

4: Map @xmath to semantic feature space with @xmath , obtain semantic
features @xmath ;

5: Search the nearest neighbor(s) of @xmath , targeting on @xmath ;

6: Set class labels @xmath to @xmath from @xmath with nearest
neighbor(s) results;

7: return @xmath

Algorithm 2 Inference for ZSL on visual-semantic mapping @xmath

1: Mapping weight @xmath , unseen class examples @xmath , unseen class
prototypes @xmath , and unseen class labels set @xmath .

2: Predicted class labels of @xmath : @xmath

3: Inference:

4: Map @xmath to visual feature space with @xmath , obtain predicted
template visual features for each class @xmath ;

5: Search the nearest neighbor(s) of @xmath , targeting on @xmath ;

6: Set class labels @xmath to @xmath from @xmath with nearest
neighbor(s) results;

7: return @xmath

Algorithm 3 Inference for ZSL on semantic-visual mapping @xmath

### 3.4 Experiments

In this section, we demonstrate the experiments of our proposed method.
We first briefly introduce the evaluated datasets and metrics. Then, we
introduce some experimental settings related to zero-shot leaning. Last,
we introduce the results compared with some existing representative
methods and some analysis of our method.

#### 3.4.1 Datasets and Metrics

##### 3.4.1.1 Datasets

Our model is evaluated on four widely used benchmark datasets of
zero-shot learning including Animals with Attributes ¹ ¹ 1
http://cvml.ist.ac.at/AwA/ (AWA) [ 62 ] , CUB-200-2011 Birds ² ² 2
http://www.vision.caltech.edu/visipedia/CUB-200-2011.html (CUB) [ 107 ]
, aPascal&Yahoo ³ ³ 3 http://vision.cs.uiuc.edu/attributes/ (aPa&Y) [ 27
] and ILSVRC2012 ⁴ ⁴ 4 http://image-net.org/challenges/LSVRC/2012/index
/ ILSVRC2010 ⁵ ⁵ 5 http://image-net.org/challenges/LSVRC/2010/index
(ImageNet) [ 93 ] . AWA dataset consists of 30,475 images of 50 animal
classes (40/10 seen and unseen classes) with six pre-extracted features
for each image. The animal classes are aligned with Osherson’s classical
class/attribute matrix [ 81 , 54 ] providing 85-dimensional attribute
features for each class. 24,295 images within 40 seen classes are used
for training, and the remaining 6,180 images within 10 unseen classes
are used for testing. CUB dataset consists of 11,788 images of 200 bird
species, each of them roughly covers 30 training images and 30 testing
images. Among them, 150 are seen classes and the remaining 50 are unseen
classes. In this dataset, 8,855 images within 150 seen classes are used
for training, and the remaining 2,933 images within 50 unseen classes
are used for testing. Each image example is endowed with 312-dimensional
semantic features. aPa&Y dataset consists of 15,339 images and each of
them is endowed with 64-dimensional attribute features. Among them,
12,695 images within 20 classes are used as the seen classes, and the
remaining 2,644 images within 12 classes are used as the unseen classes.
ImageNet dataset consists of 1,000 seen classes from ILSVRC2012 and 360
unseen classes from ILSVRC2010. In the dataset, @xmath images within
1000 seen classes are used for training, and the remaining @xmath images
within 360 unseen classes are used for testing. Each image example has
1,000-dimensional semantic features. Among them, AWA, CUB and aPa&Y are
small and medium datasets, and ImageNet is a large scale dataset.

##### 3.4.1.2 Metrics

As the common practice in zero-shot learning, we use hit@k accuracy [ 28
, 79 ] to evaluate the model performance. Hit@k accuracy is a widely
used metric in zero-shot learning which refers to predict the top-k
possible labels of the testing example, the model classifies the example
correctly if and only if the ground truth is within these top-k labels.
The hit@k accuracy can be described as:

  -- -------- -- --------
     @xmath      (3.20)
  -- -------- -- --------

where @xmath is an indicator function takes the value “1” if the
argument is true, and “0” otherwise. @xmath is the operation which
determines the class label of example. Similar with most methods, we
choose hit@1 for AWA, CUB and aPa&Y, which is the normal accuracy, and
choose hit@5 for ImageNet to fit a larger scale. In our model, the
cosine similarity is adopted for the nearest neighbor search. The
hyper-parameters @xmath / @xmath are set to 0.75/0.25, and @xmath /
@xmath are set to 0.8/0.2, respectively by grid-search [ 43 ] .

#### 3.4.2 Experimental Setups

##### 3.4.2.1 Feature Description

As to the visual feature space, we choose to use the GoogleNet features
[ 100 ] consistent with most existing methods. All image examples are
extracted by a trained GoogleNet. Hence, each image example is presented
by a 1024-dimensional vector as the visual features. As to the semantic
feature space, we use the semantic attributes for AWA, CUB and aPa&Y,
and use the semantic word vector for ImageNet. Because we do not
directly adopt the deep convolutional neural networks in our model as a
building block, so our proposed method is a non-deep model. And by using
the efficient solver, our training process is more efficient despite the
alternate optimization process.

##### 3.4.2.2 Non-transductive Learning

Several zero-shot learning methods adopt the transductive setting to
their models [ 29 , 109 ] , which refers to assume that the unseen
examples (unlabeled) are also available during the training process.
While in our proposed method, we strictly comply with the zero-shot
setting that the training of our mapping function relies solely on seen
class examples.

#### 3.4.3 Results and Analysis

This section demonstrates the experimental results in detail. Our model
is compared with several competitors including DeViSE [ 28 ] , DAP [ 62
] , MTMDL [ 120 ] , ESZSL [ 92 ] , SSE [ 130 ] , RRZSL [ 96 ] , Ba et
al. [ 8 ] , AMP [ 15 ] , JLSE [ 131 ] , SynC @xmath [ 16 ] , MLZSC [ 15
] , SS-voc [ 30 ] , SAE [ 57 ] , CVAE-ZSL [ 76 ] , CLN+KRR [ 67 ] , MFMR
[ 119 ] , RELATION NET [ 99 ] , CAPD-ZSL [ 88 ] , Chen et al. [ 18 ] ,
and SGAL [ 122 ] . The selection standard for these competitors is based
on following criteria: 1) all of these competitors are published in the
most recent years; 2) they cover a wide range of models; 3) all of these
competitors are under the same settings, i.e., datasets, evaluation
criteria, etc.; and 4) they clearly represent the state-of-the-art.

##### 3.4.3.1 Results on AWA

The comparison results on AWA dataset is shown in Table 3.1 . We compare
our model with 16 representative methods on hit@1 accuracy. It can be
observed that our model outperforms all competitors with great
advantages in both visual-semantic and semantic-visual mappings as 88.8%
and 88.6%, respectively. The average accuracy of our model reaches 88.7%
which produces the state-of-the-art performance.

##### 3.4.3.2 Results on CUB

The comparison results on AWA dataset is shown in Table 3.2 . We compare
our model with 14 representative methods on hit@1 accuracy. We can
observe that our model also obtains state-of-the-art performance with
great advantages among these competitors in both visual-semantic and
semantic-visual mappings. The average accuracy of our model reaches
64.3%, and the results of visual-semantic mapping is 0.8% higher than
semantic-visual mapping.

##### 3.4.3.3 Results on aPa&Y

The comparison results on aPa&Y dataset is shown in Table 3.3 . We
compare our proposed model with 7 representative methods. It can be
observed from the results that, although the advantages of our model are
not significantly greater than other competitors, we can also obtain the
best results among them. The average accuracy of our model reaches
56.4%. Different from AWA and CUB, the results of semantic-visual
mapping is slightly higher, i.e., 0.3%, than visual-semantic mapping.

##### 3.4.3.4 Results on ImageNet

The comparison results on ImageNet dataset is shown in Table 3.4 . Our
model is compared with 7 representative methods on hit@5 accuracy. We
can observe from the results that our model can obtain competitive
results against these competitors. The average accuracy of our model
reaches 27.3%. Similar with aPa&Y, the results of semantic-visual
mapping is also slightly higher, i.e., 0.3%, than visual-semantic
mapping.

#### 3.4.4 Further Analysis

We introduce some further analysis regarding the experimental results.
First, we explore the impact of parameter @xmath on the model
recognition accuracy, which refers to the k nearest seen class prototype
neighbors in the adjustment for the unseen class prototypes (Eq. 3.10 ).
To deal with it, we choose to evaluate on AWA dataset for a smaller
k-search, and on CUB for a larger k-search. The results are demonstrated
in Figure 3.4 and 3.5 , respectively.

We can observe that for AWA dataset, the preferred range is @xmath and
the optimal @xmath is around 12. For CUB, the preferred range is @xmath
and the optimal @xmath is around 16. Second, we evaluate the running
time of our proposed method. Because the inference process of zero-shot
learning is usually not costly compared to the training process. Thus,
we choose to consider the AWA training time of our proposed method
against three representative methods including SSE [ 130 ] , AMP [ 15 ]
and ESZSL [ 92 ] , and the results are shown in Figure 3.6 . From the
results, we can observe that our method has the fastest training speed
among these competitors, which the speed is 336 @xmath , 216 @xmath and
4.1 @xmath faster than SSE, AMP and ESZSL, respectively.

### 3.5 Remarks

In this chapter, we proposed a novel model based on a unified learning
framework for zero-shot learning. Our model can adaptively adjust to
rectify the semantic feature space by considering both the class
prototype and the global distribution of data. Moreover, we further
formulate our model combined with a cycle mapping process to a much more
efficient framework that significantly boosts the training process. By
using the proposed method, we could mitigate the domain shift and
hubness problems and make the model better adapted to recognize unseen
classes. Experimental results on several widely used benchmark datasets
verified the effectiveness of our model.

## Chapter 4 Manifold Structure Alignment by Semantic Feature Expansion

To mitigate the domain shift problem, our previous work in Chapter 2
proposes to adaptively adjust to rectify the semantic feature space
regarding the class prototypes and global distribution. The adjustment
improves the zero-shot learning (ZSL) models in two aspects. First, the
adjustment of class prototypes helps to rectify the mismatch between the
diversity of various visual examples and the identity of unique semantic
features, e.g., attributes or word vectors, which makes the
visual-semantic mapping more robust and accurate. Second, the adjustment
of global distribution helps to decrease the intra-class variance and to
enlarge the inter-class diversity in the semantic feature space, which
can further mitigate the domain shift problem. However, there may have a
possible weakness of this method on directly adjusting the previous
semantic features, which we could call it as a hard adjustment.

In this chapter, we propose a novel model called AMS-SFE, to adjust the
semantic feature space. It considers to align the manifold structures
between the visual and semantic feature spaces, by semantic feature
expansion. Specifically, we build upon an autoencoder-based model to
expand the semantic features from the visual inputs. Additionally, the
expansion is jointly guided by an embedded manifold extracted from the
visual feature space of the data. Compared to the previous work which
conducts a hard adjustment, the alignment process is more conservative
for not directly adjusting the previous semantic features. Thus, we
could call it as a soft adjustment. Extensive experiments show
significant performance improvement, which verifies the effectiveness of
our model.

### 4.1 Introduction

As a common practice in ZSL, an unseen class example is first mapped
from the original input feature space, i.e., the visual feature space,
to the semantic feature space by a mapping function trained on seen
classes. Then, with such obtained semantic features, we search the most
closely related prototype whose corresponding class is set to this
example. Specifically, this relatedness can be measured by metrics such
as the similarity or distance between the semantic features and
prototypes. Thus, some simple algorithms, such as nearest-neighbor (NN),
can be applied to search the class prototypes. However, due to the
absence of unseen classes when training the mapping function, the domain
shift problem [ 29 ] easily occurs. This is mainly because the visual
and semantic feature spaces are mutually independent. More specifically,
visual features represented by high-dimensional vectors are usually not
semantically meaningful, and the semantic features are not visually
meaningful as well. Therefore, it is challenging to obtain a
well-matched mapping between the visual and semantic feature spaces.

To address the above issues, we propose a novel model to align the
manifold structures between the visual and semantic feature spaces, as
shown in Figure 4.1 . Specifically, we train an autoencoder-based model
that takes the visual features as input to generate @xmath -dimensional
auxiliary features for each prototype in the semantic feature space,
except for the pre-defined @xmath -dimensional features. Additionally,
we combine these auxiliary semantic features with the pre-defined
features to discover better adaptability for the semantic feature space.
This adaptability is mainly achieved by aligning the manifold structures
between the combined semantic feature space ( @xmath ) to an embedded
@xmath -dimensional manifold extracted from the original visual feature
space. The expansion and alignment phases are conducted simultaneously
by joint supervision from both the reconstruction and alignment terms
within the autoencoder-based model. Our model is the first attempt to
align these two feature spaces by expanding semantic features and
derives benefits in two aspects. First, we can enhance the
representation capability of semantic feature space, so it can better
adapt to unseen classes. Second and more importantly, we can implicitly
align the manifold structures between the visual and semantic feature
spaces, so the domain shift problem can be better mitigated. Our
contributions are three-fold:

-   We are the first to consider the expansion of semantic feature space
    for zero-shot learning and align between the visual and semantic
    feature spaces.

-   Our model obtains a well-matched visual-semantic projection that can
    mitigate the domain shift problem.

-   Our model outperforms various existing representative methods with
    significant improvements and shows its effectiveness.

The rest of this paper is organized as follows. Section 4.2 introduces
the related work. Then, in Section 4.3 , we present our proposed method.
Section 4.4 discusses the experiment, and the remarks is addressed in
Section 5.5 .

### 4.2 Related Work

#### 4.2.1 Existing Works

The domain shift problem was first identified and studied by Fu et al. [
29 ] . It refers to the phenomenon that when projecting unseen class
examples from the visual feature space to the semantic feature space,
the obtained results may shift away from the real results (prototypes).
The domain shift problem is essentially caused by the nature of ZSL that
the training (seen) and testing (unseen) classes are mutually disjoint.
Recently, several researchers have investigated how to mitigate the
domain shift problem, including inductive learning -based methods, which
enforce additional constraints from the training data [ 31 , 16 ] , and
transductive learning -based methods, which assume that the unseen class
examples (unlabeled) are also available during training [ 29 , 64 , 98 ]
. It should be noted that the model performance of the transductive
setting is generally better than that of the inductive setting because
of the utilization of extra information from unseen classes during
training, thus naturally avoiding the domain shift problem. However,
transductive learning does not fully comply with the zero-shot setting
in which no examples from an unseen class are available. Recently, some
other methods have also proposed to address the domain shift problem.
DeViSE [ 28 ] trains a linear mapping function between visual and
semantic feature spaces by an effective ranking loss formulation. ESZSL
[ 92 ] applie the square loss to learn the bilinear compatibility and
adds regularization to the objective with respect to Frobenius norm. SSE
[ 130 ] proposes to use the mixture of seen class parts as the
intermediate feature space. AMP [ 15 ] embeds the visual features into
the attribute space. RRZSL [ 96 ] argues that using semantic space as
shared latent space may reduce the variance of features and causes the
domain shift, and thus proposes to map the semantic features into visual
feature space for zero-shot recognition. SynC @xmath [ 16 ] and CLN+KRR
[ 67 ] propose to jointly embed several kinds of textual features and
visual features to ground attributes. Similarly, JLSE [ 131 ] proposes a
a joint discriminative learning framework based on dictionary learning
to jointly learn model parameters for both domains. SAE [ 57 ] proposes
to use a linear semantic autoencoder to regularize the zero-shot
learning and makes the model generalize better to unseen classes. MFMR [
119 ] utilizes the sophisticated technique of matrix tri-factorization
with manifold regularizers to enhance the mapping function between the
visual and semantic feature spaces. Guo et al. [ 36 ] proposed to
adaptively adjust the semantic feature space to better train the
visual-semantic mapping. Differently, RELATION NET [ 99 ] learns a
metric network that takes paired visual and semantic features as inputs
and calculates their similarities. Chen et al. [ 18 ] proposed learn
dictionaries through joint training with examples, attributes and labels
to achieve the zero-shot recognition. With the popularity of generative
adversarial networks (GANs), CAPD-ZSL [ 104 ] proposes a simple
generative framework for learning to predict previously unseen classes,
based on estimating class-attribute-gated class-conditional
distributions. GANZrl [ 102 ] proposes to apply GANs to synthesize
examples with specified semantics to cover a higher diversity of seen
classes. Instead, GAZSL [ 134 ] applies GANs to imagine unseen classes
from text descriptions. Recently, SGAL [ 122 ] uses the variational
autoencoder with class-specific multi-modal prior to learn the
conditional distribution of seen and unseen classes.

Although several works have already achieved some progress, the domain
shift problem is still an open issue. In our model, the expansion phase
is also a generative task but focuses on the semantic feature level. We
adopt an autoencoder-based model that is lighter and easier to implement
yet effective. Moreover, we strictly comply with the zero-shot setting
and isolate all unseen class examples from the training process.

#### 4.2.2 Manifold Learning

The manifold is a concept from mathematics that refers to a topological
space that locally resembles Euclidean space near each point. Manifold
learning is based on the idea that there exists a lower-dimensional
manifold embedded in a high-dimensional space [ 108 ] . Recently, some
manifold learning-based ZSL models have been proposed. Fu et al.
introduces the semantic manifold distance to redefine the distance
metric in the semantic feature space using an absorbing Markov chain
process. MFMR [ 119 ] leverages the sophisticated technique of matrix
trifactorization with manifold regularizers to enhance the projection
between the visual and semantic spaces. In our model, we consider to
obtain an embedded manifold in a lower-dimensional space of data in the
original visual feature space. This embedded manifold is expected to
retain the geometrical and distribution constraints of the visual
feature space. Such manifold information is further used to guide the
alignment of manifold structures between the visual and semantic feature
spaces.

### 4.3 Methodology

In this section, we first introduce our proposed method and formulation
in detail. More specifically, an autoencoder-based network is first
applied to generate and expand some auxiliary semantic features except
for the predefined ones. Then, we extract a lower-dimensional embedded
manifold from the original visual feature space, which properly retains
its geometrical and distribution constraints. By using the obtained
embedded manifold, we then construct an additional regularization term
to guide the alignment of manifold structures between the visual and
semantic feature spaces. Finally, the prototype updating strategy and
the recognition process of our model are addressed.

#### 4.3.1 Semantic Feature Expansion

To align the manifold structures between the visual and semantic feature
spaces, the first step from the bottom up is to expand the semantic
features. Specifically, we keep the pre-defined @xmath -dimensional
semantic features, i.e., @xmath , fixed and expand extra @xmath
-dimensional auxiliary semantic features, i.e., @xmath , to enlarge the
target semantic feature space as @xmath . Practically, there are several
techniques such as the generative adversarial network (GAN) [ 32 ] , and
the autoencoder (AE) [ 9 ] can achieve this target on the basis that the
expanded auxiliary semantic features are faithful to the original input
features, i.e., visual features. Compared with the GAN, which is good at
synthesizing data distribution, e.g., more realistic images in
example-level [ 125 ] , the AE is much lighter and easier to train [ 105
, 123 , 21 , 36 ] . Moreover, in our model, the expanded auxiliary
features are expected to be more per-class semantically high-level in
contrast to example-level features. Considering the generation target,
the training cost and the model complexity, the AE is a better choice
for our purpose. The standard AE consists of two components. The encoder
maps the visual features @xmath to a latent feature space, in which the
latent features @xmath are normally a high-level and compact
representation of the visual features. The decoder then maps the latent
features back to the visual feature space and reconstructs the original
visual features as @xmath . The AE loss measuring the reconstruction can
be described as:

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

We minimize the objective of Eq. 4.1 to guarantee the learned latent
features @xmath retain the most powerful information of the input @xmath
. It should be noted that the SAE [ 57 ] first implemented the AE for
the zero-shot learning task, which applies the semantic autoencoder to
obtain the mapping function between the visual and semantic feature
spaces. However, in our model, the AE is mainly implemented in the
expansion process to obtain extra auxiliary semantic features.

The standard AE performs well in our model despite the latent feature
space being point-wise sensitive, which means that the margins of each
class within the latent feature space are discrete and the data in the
latent feature space are unevenly distributed. In other words, some
areas in the latent feature space do not represent any data. Although
the prototypes of each class are also inherently discrete, a smooth and
continuous latent feature space can intuitively better represent the
margins among classes and makes the projection between the visual and
latent feature spaces more robust. To this end, we further apply the
variational autoencoder (VAE) [ 56 ] to formulate the expansion process
in our model.

Different from the standard AE, the encoder of the VAE predicts the mean
feature vector @xmath and the variance matrix @xmath , such that the
distribution of latent features @xmath can be approximated by @xmath ,
i.e., @xmath , from which a latent feature @xmath is sampled and further
decoded to reconstruct the original visual features as @xmath . The key
difference between the AE and VAE is the embedding methods of the inputs
in the latent feature space. The AE learns a compressed data
representation that is normally more discrete, while the VAE attempts to
learn the parameters of a probability distribution representing the
data, which makes the learned latent features smoother and more
continuous [ 56 , 13 ] . It should be noted that the sampling operation
from @xmath is usually non-differentiable which makes the
backpropagation impossible. As suggested by the reparameterization trick
[ 56 ] , sampling from @xmath is equivalent to sampling @xmath and
setting @xmath . Thus, @xmath can be regarded as an input of the network
and makes the sampling operation differentiable. Moreover, we need an
additional constraint other than the reconstruction loss, i.e., @xmath ,
to guide the training of VAE. It should be noted that the additional
constraint is expected to force the latent feature distribution to be
similar to a prior, so the objective of the VAE can be further specified
as:

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

where the first term is the conventional reconstruction loss, which
forces the latent feature space to be faithful and restorable to the
original visual feature space, and the second term is the unpacked
Kullback-Leibler divergence between the latent feature and the chosen
prior @xmath , e.g., a multivariate standard Gaussian distribution,
which further forces the margins of each class to be smooth and
continuous and makes the visual-semantic mapping more robust.

#### 4.3.2 Manifold Extraction and Alignment

##### 4.3.2.1 Extraction

Before exploiting these auxiliary semantic features, we need to extract
a lower-dimensional embedded manifold ( @xmath ) of the visual feature
space ( @xmath ) to utilize the structure information. To this end, we
first find and define the center of each seen class in the visual
feature space as @xmath , where @xmath are @xmath class labels and
@xmath is the center, e.g., the mean value, of all examples belonging to
class @xmath . We compose a matrix @xmath to record the distance of each
center pair from @xmath in the original visual feature space as:

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

where @xmath is calculated as @xmath . Then, our target is to search for
a lower-dimensional embedded manifold ( @xmath ) that can be modeled by
a @xmath -dimensional embedded feature representation, i.e., denoted as
@xmath , where each @xmath is the embedded representation of the class
center @xmath . The embedded representation is expected to retain the
geometrical and structural information of the visual feature space of
the data. To obtain @xmath , a natural and straightforward approach is
that the distance matrix @xmath can also restrain the embedded
representation @xmath , which means that the distance of each point pair
of @xmath also has the same distance matrix @xmath in the corresponding
@xmath -dimensional feature space. To solve this problem, we denote the
inner product of @xmath as @xmath , so that @xmath and we obtain:

  -- -------- -------- -- -------
     @xmath   @xmath      (4.4)
  -- -------- -------- -- -------

We set @xmath to simplify the problem so that the summation of each
row/column of @xmath equals zero. The zero-centered setting can reduce
computations while retaining the data’s geometrical and structural
information. Then, we can easily obtain:

  -- -------- -- -------
     @xmath      (4.5)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (4.6)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

where @xmath is the trace of the matrix, i.e., @xmath . We denote:

  -- -------- -- -------
     @xmath      (4.8)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

From Eq. 4.4 , we can easily obtain:

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

From Eq. 4.7 and Eq. 4.10 , we can obtain:

  -- -------- -- --------
     @xmath      (4.12)
  -- -------- -- --------

From Eq. 5.6 and Eq. 4.8 , and from Eq. 4.5 and Eq. 4.9 , respectively,
we can obtain:

  -- -------- -- --------
     @xmath      (4.13)
  -- -------- -- --------

Combine Eqs. 4.11 @xmath 4.13 , we can obtain the inner product matrix
@xmath by the distance matrix @xmath as:

  -- -------- -- --------
     @xmath      (4.14)
  -- -------- -- --------

By applying eigenvalue decomposition with @xmath , we can easily obtain
the @xmath -dimensional representation @xmath , which models the
geometrical and structural information of the expected @xmath
-dimensional embedded manifold.

##### 4.3.2.2 Structure Alignment

With the obtained @xmath , we now can align the manifold structures
between the visual and semantic feature spaces. Specifically, we measure
the similarity of the combined semantic feature representation @xmath
(predefined @xmath combined with expanded @xmath ) and the embedded
representation @xmath by cosine distance, in which the output similarity
between two vectors is bounded in @xmath and is magnitude free as well.
It should be noted that in our model, the alignment is jointly completed
with the semantic feature expansion. To achieve this, we construct a
regularization term to further guide the autoencoder-based network as:

  -- -------- -- --------
     @xmath      (4.15)
  -- -------- -- --------

where @xmath is the combined semantic feature representation of the
@xmath -th seen class example @xmath . @xmath is the class label and
@xmath is the @xmath -th class label among @xmath classes. @xmath is an
indicator function that takes a value of “1” if its argument is true,
and “0” otherwise. Last, combined with Eq. 5.2 , the unified objective
can be described as:

  -- -------- -------- -------- --------
     @xmath   @xmath            (4.16)
                       @xmath   
  -- -------- -------- -------- --------

where @xmath acts as a base term that mainly guides the reconstruction
of the input visual examples. @xmath is an alignment term that provides
additional guidance to learning latent vectors and forces the manifold
structure of the combined semantic feature space to approximate the
structure of the embedded manifold extracted from the visual feature
space. @xmath and @xmath are two hyper-parameters that control the
balance between these two terms.

#### 4.3.3 Prototype Update

After the expansion phase, we need to update the prototypes for each
class. We have different strategies regarding the seen and unseen
classes. First, for each seen class. Because we obtained the trained
autoencoder and all the seen class examples are available, we can simply
compute the center, i.e., the mean value, of all latent vectors @xmath
belonging to the same class and combine the center with the predefined
prototype for each seen class as:

  -- -------- -- --------
     @xmath      (4.17)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (4.18)
  -- -------- -- --------

where @xmath is the expanded semantic features obtained by the encoder,
@xmath is the number of examples belonging to this specific seen class,
@xmath and @xmath are predefined and expanded prototypes, respectively,
and @xmath denotes the operation that concatenates two vectors. Second,
for each unseen class, as no example is available during the whole
training phase, we cannot apply the trained autoencoder to update the
prototypes directly. Instead, we use another strategy by considering the
local linearity among prototypes. Specifically, for each predefined
unseen class prototype, we first obtain its @xmath nearest neighbors
from predefined seen class prototypes. Then, we estimate each predefined
unseen class prototype by a linear combination of its corresponding
@xmath neighbors as:

  -- -------- -------- -------- --------
     @xmath   @xmath            (4.19)
                       @xmath   
  -- -------- -------- -------- --------

where @xmath is the predefined prototype of the unseen class, @xmath are
its @xmath nearest neighbors from predefined seen class prototypes, and
@xmath are the estimation parameters. Eq. 4.19 is a simple linear
programming, and we can easily obtain the estimation parameters by
solving a minimization problem as:

  -- -------- -- --------
     @xmath      (4.20)
  -- -------- -- --------

With the obtained estimation parameter @xmath , we can update the
prototype for each unseen class as:

  -- -------- -------- -------- --------
     @xmath   @xmath            (4.21)
                       @xmath   
  -- -------- -------- -------- --------

  -- -------- -- --------
     @xmath      (4.22)
  -- -------- -- --------

where @xmath is the updated prototype for the unseen class and @xmath
are the corresponding @xmath expanded prototypes of its @xmath seen
class neighbors.

#### 4.3.4 Recognition

In our model, similar to some methods, we also adopt the simple
autoencoder training framework [ 57 ] to learn the mapping function
between the visual and semantic feature spaces. The encoder @xmath first
maps an example from the visual feature space to the semantic feature
space to reach its prototype. Then, the decoder @xmath reversely maps it
back to the visual feature space and reconstructs the example. The
latent vectors of the autoencoder are forced to be the prototypes of
each class. These two steps guarantee the robustness of our learned
projection. Our model mainly focuses on the alignment of manifold
structures by semantic feature expansion, so we do not apply any
additional techniques to the mapping function training phase. A simple
linear autoencoder with just one hidden layer is trained to obtain the
visual-semantic mapping. Specifically, the encoder @xmath can be
regarded as the forward mapping, and the decoder can be regarded as the
reverse mapping. In the testing phase, taking the forward mapping as an
example, we can map an unseen class example @xmath to the semantic
feature space and obtains its semantic feature representation as @xmath
. As to the recognition, we simply search its most closely related
prototype and set the class corresponding with it to the testing example
as:

  -- -------- -- --------
     @xmath      (4.23)
  -- -------- -- --------

where @xmath is the prototype of the @xmath -th unseen class, @xmath is
a similarity metrics, e.g., cosine similarity, and @xmath returns the
class label of @xmath .

### 4.4 Experiments

#### 4.4.1 Datasets and Metrics

In our experiments, five widely used benchmark datasets are selected for
the evaluation, including Animals with Attributes ¹ ¹ 1
http://cvml.ist.ac.at/AwA/ (AWA) [ 62 ] , CUB-200-2011 Birds ² ² 2
http://www.vision.caltech.edu/visipedia/CUB-200-2011.html (CUB) [ 107 ]
, aPascal&Yahoo ³ ³ 3 http://vision.cs.uiuc.edu/attributes/ (aPa&Y) [ 27
] , SUN Attribute ⁴ ⁴ 4 http://cs.brown.edu/ gmpatter/sunattributes.html
(SUN) [ 84 ] , and ILSVRC2012 ⁵ ⁵ 5
http://image-net.org/challenges/LSVRC/2012/index / ILSVRC2010 ⁶ ⁶ 6
http://image-net.org/challenges/LSVRC/2010/index (ImageNet) [ 93 ] . The
first four are small and medium scale datasets, and ImageNet is a
large-scale dataset. The AWA [ 62 ] consists of 30,475 images of 50
animal classes. Among them, 40 classes covering 24,295 images are used
as seen classes for training, and 10 classes covering 6,180 images are
used as unseen classes for testing. Each class is represented by an
85-dimensional numeric attribute vector as the prototype. The CUB [ 107
] consists of 11,788 images of 200 bird species. Among them, 150 species
covering 8,855 images are used as seen classes for training, and 50
classes covering 2,933 images are used as unseen classes for testing.
Each of their prototypes is represented by a 312-dimensional semantic
attribute vector. The aPa&Y [ 27 ] consists of 15,339 images from the
Pascal VOC 2008 and Yahoo. Among them, 20 classes covering 12,695 images
are used as seen classes for training, and 12 classes covering 2,644
images are used as unseen classes for testing. Each of the prototypes is
represented by a 64-dimensional semantic attribute vector. The SUN [ 84
] consists of 14,340 scene images with two splits 707/10 and 645/72. In
our model, we only consider the latter split, which contains more unseen
classes. A total of 645 seen classes are used for training, and the
remaining 72 unseen classes are used for testing. Each of the prototypes
is represented by a 102-dimensional semantic attribute vector. The
ImageNet [ 93 ] consists of total 1,360 classes. Among them, 1,000
classes from ILSVRC2012 covering @xmath images are used as seen classes
for training, and 360 classes from ILSVRC2010 covering @xmath images are
used as unseen classes for testing. Each of their prototypes is
represented by a 1,000-dimensional word vector.

In our experiments, the hit@k accuracy [ 28 , 79 ] is used to evaluate
the model performance. The hit@k accuracy is the standard evaluation
metrics in ZSL which can be defined as:

  -- -------- -- --------
     @xmath      (4.24)
  -- -------- -- --------

where @xmath is an indicator function takes the value “1” if the
argument is true, and “0” otherwise. @xmath is the operation which
determines the class label of example. Similar with most methods, we
choose hit@1 for AWA, CUB, aPa&Y, and SUN, which is the normal accuracy,
and choose hit@5 for ImageNet to fit a larger scale.

#### 4.4.2 Experimental Setups

In our experiments, to consider all competitors, we also use GoogleNet [
100 ] to extract the visual features for image examples, from which each
image is presented by a 1024-dimensional visual vector. Regarding the
semantic features, we use the semantic attributes for AWA, CUB, aPa&Y,
and SUN, and use the semantic word vectors for ImageNet. In our model,
the autoencoder-based network for expansion and alignment has five
layers. Specifically, the encoder and decoder parts contain two layers
with (1024,256) and (256,1024) neurons, respectively. The central hidden
layer represents the expected latent feature space, which can be
adjusted to the dimension of semantic features we expand.

Theoretically, within a reasonable expansion range, we can expect that
the more auxiliary semantic features we expand, the better the model
performance will be. This is because with more auxiliary semantic
features, more information can be utilized when training and inferring
for the recognition. It will also be easier to align the manifold
structures between the visual and semantic feature spaces; thus, the
projection between these two feature spaces can be better trained and
further mitigate the domain shift problem. Taking the AWA as an example,
we conduct a comparison experiment to evaluate the effects of expanded
dimensions of the auxiliary semantic features. The comparison results
are shown in Figs. 4.3 and 4.4 . From Fig. 4.3 , we can observe that
within a reasonable expansion range, e.g., within an expansion rate of
100% ( @xmath ), as the expanded dimensions increase, the alignment
error gradually decreases. In addition, we can also observe that even
when the expansion rate reaches 200% and 300%, the trend of alignment
error reduction remains. As to the prediction accuracy, it can be seen
from Fig. 4.4 that as the expanded dimensions increase, the accuracy
also gradually increases and eventually becomes comparatively stable
with larger expansion rates. Based on the above analysis, in our
experiments, without loss of generality, we empirically apply a
relatively medium expansion rate, i.e., @xmath , for all datasets except
ImageNet. We expand 65, 138, 26, and 58 auxiliary semantic features for
AWA, CUB, aPa&Y, and SUN to fit the total semantic features as nice
round numbers for 150, 450, 90, and 160. Regarding the ImageNet, because
the dimensions of the visual features and predefined semantic features
are 1,024 and 1,000, respectively, which results in expandable auxiliary
semantic features ranging from 0 to 24 for a better projection. Thus, we
fairly expand 12 auxiliary semantic features for ImageNet. In our model,
the dimensional comparison of predefined and expanded semantic features
for these five datasets is shown in Table 4.1 . As mentioned above, the
neurons of the central hidden layer are adjusted to 65, 138, 26, 58, and
12 for AWA, CUB, aPa&Y, SUN, and ImageNet, respectively.

For the hyper-parameters @xmath and @xmath in Eq. 4.16 , we conduct a
grid-search experiment [ 43 , 39 ] to evaluate their sensibilities.
Based on the definition in Eq. 4.15 , the alignment loss is normally
considerably smaller than the reconstruction loss. Thus, a larger
penalty, i.e., @xmath , should be applied to the alignment term in Eq.
4.16 to accelerate the convergence of alignment loss [ 55 ] . In our
grid-search experiment, we set the @xmath and @xmath ranges within
@xmath and @xmath , respectively. The grid-search results are shown in
Figure. 4.5 @xmath 4.7 , from which we can observe that the optimal
@xmath selections are 9/77 for the reconstruction loss, 10/110 for the
alignment loss, and 9/77 for the total loss. Therefore, we choose to set
the hyper-parameters @xmath and @xmath as 9 and 77, respectively. The
hyperparameter @xmath is used to update the class prototypes of unseen
classes. Based on our previous work [ 34 ] which adopted the same
strategy by linearly associating with seen class prototypes, @xmath is
not a critical parameter when ranged on a reasonable scale, e.g.,
several neighbors to tens of neighbors. Therefore, without loss of
generality, we empirically choose to consider 8 nearest neighbors in our
model. As to the recognition phase, a simple linear autoencoder with
just one hidden layer is trained to obtain the visual-semantic
projection, i.e., the neurons of input/output are equal to the visual
features, and the neurons of the central hidden layer are equal to the
semantic features. The cosine similarity is applied to search the
related prototype of the testing examples.

#### 4.4.3 Results and Analysis

##### 4.4.3.1 Competitors

This section demonstrates the experimental results in detail. Our model
is compared with several competitors including DeViSE [ 28 ] , DAP [ 62
] , MTMDL [ 120 ] , ESZSL [ 92 ] , SSE [ 130 ] , RRZSL [ 96 ] , Ba et
al. [ 8 ] , AMP [ 15 ] , JLSE [ 131 ] , SynC @xmath [ 16 ] , MLZSC [ 15
] , SS-voc [ 30 ] , SAE [ 57 ] , CLN+KRR [ 67 ] , MFMR [ 119 ] ,
RELATION NET [ 99 ] , CAPD-ZSL [ 88 ] , LSE [ 124 ] , Chen et al. [ 18 ]
, and SGAL [ 122 ] . The selection standard for these competitors is
based on following criteria: 1) all of these competitors are published
in the most recent years; 2) they cover a wide range of models; 3) all
of these competitors are under the same settings, i.e., datasets,
evaluation criteria, etc.; and 4) they clearly represent the
state-of-the-art. Moreover, as mentioned in previous, our model and all
selected competitors strictly comply with the non-transductive zero-shot
setting that the training only relies on seen class examples, while the
unseen class examples are only available during the testing phase.

##### 4.4.3.2 Comparison Results

The comparison results with the selected state-of-the-art competitors
are shown in Table 4.2 . It can be seen from the results that our model
outperforms all competitors with great advantages in four datasets,
including AWA, CUB, aPa&Y, and SUN. The prediction accuracy of our model
achieves 90.9%, 70.1%, 59.7%, and 92.9%, respectively. In ImageNet, our
model obtains suboptimal performance among all competitors. Our
prediction accuracy is 26.3%, which is slightly weaker than LSE [ 124 ]
and similar to SAE [ 57 ] . It should be noted that these similar
results in ImageNet of SAE and our model may be caused by the following
two reasons. First, as mentioned in Sections 4.3.4 and 4.4.2 , our model
also adopts a simple autoencoder training framework to obtain the
projection between the visual and semantic feature spaces, which is
similar to some existing methods such as SAE. Second, it can be seen in
Table 4.1 that there are only 12 expanded semantic features for
ImageNet, which also makes the total semantic features similar to the
predefined features. From the dataset description in Section 4.4.1 , we
can know that the dimension of the predefined semantic features of
ImageNet is 1,000. However, the dimension of the visual features is
1,024. Based on the analysis in Section 4.4.2 , we only expand 12
auxiliary semantic features for ImageNet, which are far fewer than the
predefined features. This limitation makes it difficult to perform the
alignment between the visual and semantic feature spaces for ImageNet.
Thus, the improvement regarding ImageNet is not that significant. In
contrast, we have enough space to expand the auxiliary semantic features
for AWA, CUB, aPa&Y, and SUN, so the alignment can be better
approximated for them. In our model, the dimensions of the expanded
auxiliary semantic features for these , datasets are 65, 138, 26, 58,
and 12, respectively.

From the comparison results, we can also observe that the performance of
our VAE-based model is slightly better than the AE-based model for CUB,
aPa&Y, SUN, and ImageNet, from which the most significant improvement is
from 67.8% to 70.1% for CUB. One possible reason is that CUB is also a
good benchmark dataset for fine-grained image recognition tasks, except
for ZSL, which consists of multiple bird species, and the transition
between each class is relatively smoother and more continuous.
Therefore, it is more likely to maintain these features in the semantic
feature space, which helps the prediction by using the VAE.

#### 4.4.4 Further Analysis

##### 4.4.4.1 Mapping Robustness

We conduct the evaluation of projection robustness on AWA and CUB. AWA
consists of 10 unseen classes, and CUB consists of 50 unseen classes.
Our model is compared with the overall strongest competitor SAE [ 57 ]
to verify the mapping robustness. A mapping function that maps from the
visual to the semantic feature spaces is trained on seen class examples
with our model. Then, we apply the mapping function to all unseen class
examples to obtain their semantic features and visualize them in a 2D
map by t-SNE [ 71 ] . The visualization results are shown in Figure. 4.8
. The left column is the results for AWA, and the right column is the
results for CUB. In AWA, we observe that by using our model, only a
small portion of these unseen class examples are mis-mapped in the
semantic feature space. Moreover, due to the alignment of manifold
structures between the visual and semantic feature spaces, these
mis-mapped examples are less shifted from their class prototypes, which
means that our model can also obtain better results for Hit@k accuracy
when k varies and converges faster to the best performance. The
comparison is shown in Table 4.3 and Figure. 4.9 . In CUB, the class
number is much larger than that of AWA, so the visualization results
seem more complicated and intertwined in the 2D map, but we still
observe that our model can obtain more continuous and smoother semantic
features in a wider visual field.

##### 4.4.4.2 Fine-Grained Accuracy

To further evaluate the predictive power of our model, we record and
count the prediction results for each unseen class example and analyze
the per-class performance. This evaluation is conducted on AWA and CUB,
and we also compare our model with the overall strongest competitor SAE
[ 57 ] . The results are presented by the confusion matrixes in Figure.
4.10 . The upper part is the results for AWA, and the lower part is the
results for CUB. In each confusion matrix, the diagonal position
indicates the classification accuracy for each class. The column
position indicates the ground truth, and the row position denotes the
predicted results. It can be seen from the results that our model can
obtain higher accuracy, along with a more balanced and robust prediction
for each unseen class for both AWA and CUB.

##### 4.4.4.3 Ablation Comparison

We conduct an ablation experiment to further evaluate the effectiveness
of our model. The performance is compared on all five benchmark
datasets, including AWA, CUB, aPa&Y, SUN, and ImageNet, with our
AE-based model on three scenarios: (1) only predefined semantic features
are used; (2) only expanded auxiliary semantic features are used; and
(3) both predefined and expanded auxiliary semantic features are used.
The comparison results are shown in Table 4.4 and Figure. 4.11 . It can
be seen from the results that our model greatly improves the performance
of zero-shot learning by performing the alignment with the expanded
auxiliary semantic features. Nevertheless, it should also be noted that,
due to the very few (i.e., 12) expanded auxiliary semantic features for
ImageNet as analyzed in Section 4.4.2 , the model obtains similar
performance as 26.1% for both scenarios (1) and (3), where only the
predefined semantic features and both semantic features are used,
respectively. Thus, the impact of expanded semantic features on ImageNet
is limited in our model, and the visual and semantic feature spaces
cannot be better aligned. We may further investigate this problem in our
future work.

### 4.5 Remarks

In this chapter, we propose a novel model called AMS-SFE for zero-shot
learning. Our model aligns the manifold structures between the visual
and semantic feature spaces by jointly conducting semantic feature
expansion. Our model can better mitigate the domain shift problem and
obtain a more robust and generalized visual-semantic projection. In the
future, we have two research routes to further improve zero-shot
learning. The first route investigates a more efficient and generalized
method to further empower the semantic feature space. The second route
goes from the coarse-grained model to the fine-grained model to better
model the subtle differences among different classes.

## Chapter 5 Zero-shot Learning as a Graph Recognition

To mitigate the domain shift problem, our previous works focus on two
forms of adjustments on the semantic feature space to make the trained
mapping function between the visual and semantic feature spaces more
robust and accurate. Chapter 3 proposes to adaptively adjust to rectify
the semantic feature space regarding the class prototypes and global
distribution, which can be considered as a hard adjustment. While
chapter 4 proposes to align the manifold structures between the visual
and semantic feature spaces by semantic feature expansion, which is more
conservative for not directly adjusting previous semantic features and
can be regarded as a soft adjustment. Like most existing works, these
methods consider constructing mapping functions in a coarse-grained
manner, i.e., features are extracted from the whole examples.

In this chapter, unlike our previous and most existing works, our
interest is to focus on the fine-grained perspective based on
example-level graph. Specifically, we decompose an image example into
several parts and use a graph-based model to measure and utilize certain
relations between these parts. Taking advantage of recently developed
graph neural networks, we formulate the zero shot learning (ZSL) problem
to a graph-to-semantic mapping problem, which can better exploit the
part-semantic correlation and local substructure information in
examples. Experimental results demonstrate that the proposed method can
mitigate the domain shift problem and achieve competitive performance
against other representative methods.

### 5.1 Introduction

In recent years, deep learning as one of the state-of-the-art techniques
in current advanced machine learning, has attracted tremendous research
interests in the domains of computer vision, natural language
processing, speech analysis, and so on. The success of deep learning
relies heavily on the emergence of large-scale annotated datasets such
as ImageNet in the domain of computer vision. However, conventional deep
learning techniques are still not capable of handling the issue of
scalability that collecting and labeling examples for rare or
fine-grained classes are difficult. Thus, the conventional deep learning
techniques perform poorly in scenarios where only limited or even no
training data are available. In the real-world application, these
limitations naturally exist because novel classes of examples arise
frequently, yet to obtain their annotations is expensive and
time-consuming. Zero-shot learning (ZSL), which aims to imitate human
ability in recognizing unseen classes under such restrictions, has
received increasing attention in the most recent years [ 61 , 28 , 96 ,
57 , 37 ] . ZSL takes utilization of seen classes with labeled examples
and auxiliary knowledge that can be shared and transferred between seen
and unseen classes to solve the problem. This knowledge, e.g., semantic
descriptions that exist in a high dimensional semantic feature space,
can represent meaningful high-level information about examples of
different classes. Intuitively, the cat is semantically closer to the
tiger than to the snake. This intuition also holds and becomes the
fundamental basis of ZSL in the the semantic feature space, in which
each class is embedded and endowed with a prototype. In ZSL, the common
practice is first to map an unseen class example from its original
feature space, e.g., visual feature space, to semantic feature space by
a mapping function trained on seen classes. With such semantic features,
we then search the most closely related (measured by similarity or any
other metrics) prototype whose corresponding class is set to this
example. Specifically, the ZLS can be further restricted to the
conventional ZSL (CZSL) and generalized ZSL (GZSL), where the former
only considers the unseen classes during inference while the latter can
also generalize well to novel examples from seen classes.

In ZSL, one non-negligible fundamental issue is that the visual and
semantic feature spaces are generally independent and may exist in
entirely different manifolds. This issue brings about the ubiquitous
domain shift problem when generalizing the obtained knowledge, e.g., the
trained mapping function, from seen to unseen classes. Recently, several
methods have been proposed to mitigate the domain shift problem. For
example, some methods consider directly doing the alignment between
these two feature spaces when constructing the mapping function [ 130 ,
94 , 37 ] . Differently, some methods try to synthesize unseen class
examples/features based on generative adversarial networks (GANs) or
variational autoencoders (VAEs) and involve them in the training process
[ 60 , 116 , 45 ] . Some other methods apply the encoder-decoder
structure to maintain the robustness of mapping function [ 57 , 124 ] .
However, most existing methods are trained on whole feature
representations while ignored the fine-grained and structural
information of examples from different classes. This limitation still
hinders the domain shift problem being solved and makes it difficult to
fully utilize the information of examples.

To address these problems, we propose a fine-grained ZSL framework based
on the example-level graph (Figure 5.1 ). Our method first decomposes an
example to several parts, e.g., by key-point localization and cropping,
and then constructs an example graph to present it. Specifically, we use
the nodes to present each part of the example and use the linking edges
to present whether a relation exists between two parts. Next, we build
upon the currently popular graph neural networks (GNNs) to extract and
fuse the example graph, and further formulate the original ZSL to a
graph-to-semantic mapping problem which indeed converts the ZSL to a
graph recognition task and soundly becomes our main contribution of this
paper. Our method can benefit the ZSL in at least two folds. On the one
hand, because the semantic descriptions are usually connected more to
local features of examples rather than the global ones. Thus, the
fine-grained decomposition can make the mapping more robust and
conserves the part-semantic correlation. On the other hand, by using the
GNNs we can fully extract the local substructure information of example
graphs and fuse them to better feature representations. The above
benefits ultimately resulting in a better mapping between the visual and
semantic feature spaces, which can greatly mitigate the domain shift
problem.

The rest of this paper is organized as follows. Section 5.2 introduces
the related work. Then, in Section 5.3 , we present our proposed method.
Section 5.4 discusses the experiments and the remarks are addressed in
Section 5.5 .

### 5.2 Related Work

#### 5.2.1 Existing Works

ZSL can be roughly divided into three categories according to the
mapping direction that adopted as visual to semantic mapping, semantic
to visual mapping, and intermedia mapping (metric learning). Most
existing methods map the visual features to semantic feature space
spanned by class descriptions and then perform nearest neighbor search [
61 , 28 , 130 , 92 , 4 , 57 , 67 , 124 , 94 , 35 , 37 , 135 ] . For
example, DeViSE [ 28 ] trains a linear mapping function between visual
and semantic feature spaces by an effective ranking loss formulation.
ESZSL [ 92 ] applie the square loss to learn the bilinear compatibility
and adds regularization to the objective with respect to Frobenius norm.
SSE [ 130 ] proposes to use the mixture of seen class parts as the
intermediate feature space. SynC @xmath [ 16 ] and CLN+KRR [ 67 ]
propose to jointly embed several kinds of textual features and visual
features to ground attributes. SAE [ 57 ] proposes to use a linear
semantic autoencoder to regularize the zero-shot learning and makes the
model generalize better to unseen classes. In contrast, some works
propose to map the semantic features into visual feature space and point
out that using semantic space as shared latent space may reduce the
variance of features which makes results more clustered as hubs [ 96 ,
127 ] . Different from the directly mappings, some works propose to
learn a metric network or compatibility measurement that takes paired
visual and semantic features/classifiers as inputs and calculates their
similarities [ 99 , 110 , 51 ] . For example, RELATION NET [ 99 ] learns
a metric network that takes paired visual and semantic features as
inputs and calculates their similarities. Recently, some generative
models have emerged. For example, f-CLSWGAN [ 116 ] proposes to
synthesize several example features for unseen classes by using a
Wasserstein GAN with a classification loss. SE-GZSL [ 60 ] uses a
variational autoencoder based architecture to generate novel examples
from seen/unseen classes conditional on respective class attributes.
DASCN [ 78 ] proposes two GAN networks, namely the primal GAN and dual
GAN in a unified framework, for generalized zero-shot recognition.
Schonfeld et al. [ 94 ] applied the variational autoencoder to do the
cross-alignment of the visual and semantic feature spaces. SGAL [ 122 ]
uses the variational autoencoder with class-specific multi-modal prior
to learn the conditional distribution of seen and unseen classes.
AMS-SFE [ 35 , 37 ] proposes to expand the semantic feature space and
implicitly to align the visual and semantic feature spaces.

Compared with above existing methods, our work is more likely related to
some fine-grained ZSL methods such as @xmath [ 48 ] , Zhu et al. [ 135 ]
and Chen et al. [ 18 ] , which make use of attention mechanism on local
regions or learn dictionaries through joint training with examples,
attributes and labels, and some graph-oriented ZSL methods such as Wang
et al. [ 110 ] and DGP [ 51 ] which perform graph techniques to explore
the class-level dependencies. Our method differs from above fine-grained
and graph-oriented ZSL methods in that it decomposes an example to
several parts and present it as a graph structure, which considers the
example-level graph rather than class-level graph and converts the ZSL
to graph-to-semantic mapping problem. It should be noted that our method
is not in the same category of graph-oriented ZSL methods such as Wang
et al. [ 110 ] and DGP [ 51 ] , and cannot be directly compared. Our
method focuses on fine-grained stream.

#### 5.2.2 Graph Recognition with GNNs

The GNNs are currently popular graph techniques in deep learning and
attracts increasing attention from machine learning and data mining
communities. We can train the GNNs in either a supervised or
unsupervised manner to handle multiple tasks such as node
classification, edge prediction, graph embedding, and graph
classification. Specifically, the graph-level classification aims at
classifying an entire graph [ 128 , 121 , 133 ] . Our method makes use
of the graph-level classification to converts the ZSL to a
graph-to-semantic mapping problem, where we replace the class labels to
class semantic descriptions and indeed form a graph-level regression
task.

### 5.3 Methodology

#### 5.3.1 Problem Definition

We start by formalizing the ZSL task and then introduce our proposed
method based on this formalization. Given a set of labeled seen class
examples @xmath , where @xmath is seen class example, i.e., image, with
class label @xmath belonging to @xmath seen classes @xmath . The goal is
to construct a model for a set of unseen classes @xmath ( @xmath ) from
which no example is available during training. In the inference phase,
given a testing unseen class example @xmath , the model is expected to
predict its class label @xmath . To this end, some auxiliary knowledge,
e.g., the semantic descriptions, denoted as @xmath , are needed to
bridge the gaps between the seen and unseen classes. Therefore, the
labeled seen class examples @xmath can be further specified as @xmath .
Each seen class @xmath is endowed with a semantic prototype @xmath , and
similarly, each unseen class @xmath is also endowed with a semantic
prototype @xmath . Thus, for each seen class example we have its
semantic features @xmath , while for testing unseen class example @xmath
, we need to predict its semantic features @xmath and set the class
label by searching the most closely related semantic prototype within
@xmath . In summary, given @xmath , the training can be described as:

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

where @xmath being the loss function and @xmath being the regularization
term. The @xmath is a mapping function with parameter @xmath maps from
the visual feature space to semantic feature space. The @xmath is a
feature extractor, e.g., a pre-trained CNNs, to obtain the visual
features of @xmath . For the inference, given a testing example, e.g.,
@xmath , the recognition can be described as:

  -- -------- -- -------
     @xmath      (5.2)
     @xmath      (5.3)
  -- -------- -- -------

where @xmath is a similarity metric and @xmath searches the most closely
related prototype and set the class corresponding with this prototype to
@xmath . Specifically, Eq. 5.2 is used for conventional ZSL which the
similarity search is only on unseen classes, and Eq. 5.3 is used for
generalized ZSL which the search can also generalize to novel examples
from seen classes.

#### 5.3.2 Graph Neural Networks

In our method, the GNNs are utilized to receive the example graph and
outputs the corresponding class semantic descriptions of the example.
Our GNNs framework mainly contains three consecutive parts as graph
convolutional layers, graph pooling layers and regression layers. The
graph convolution layers are responsible for exacting high-level part
representations and local substructure information of examples. The
graph pooling layers downsample and fuse the high-level features which
coarsens them to more representative graphical features. Last, the
regression layers are convolutional and dense layers outputs and links
to the semantic descriptions of the example. We introduce the graph
convolution layers here and then address our graph pooling and
regression layers in Section 5.3.3 .

Given an example graph as @xmath , where @xmath is the adjacency matrix
of example graph in which the element @xmath denotes there exists an
edge between two nodes and @xmath otherwise. @xmath is the node feature
matrix represents the example graph has @xmath nodes and each node is
constituted as @xmath -dimensional features. Our graph convolution
layers can be described as:

  -- -------- -- -------
     @xmath      (5.4)
  -- -------- -- -------

where @xmath in which @xmath is the identity matrix denotes a self-loop
in each node of an example graph. @xmath is the diagonal degree matrix
of nodes. @xmath is a trainable parameter matrix of graph convolution
which receives @xmath -dimensional features and outputs @xmath
-dimensional ones. The @xmath is a nonlinear activation function, e.g.,
@xmath .

This graph convolution can be decoupled into four operations. A linear
feature transformation is first performed by @xmath which maps the node
feature representation from @xmath to @xmath channels in the next graph
convolution layers. Secondly, @xmath propagates node information to
neighboring nodes as well as the node itself. The @xmath normalizes each
row in obtained node feature matrix @xmath after the graph convolution.
The last nonlinear activation function @xmath performs point-wise
activation and outputs the graph convolution results. The graph
convolution can aggregate node features in local neighborhoods to
extract local substructure information. To further extract the deep
high-level multi-scale features, we stack multiple above graph
convolution layers as:

  -- -------- -- -------
     @xmath      (5.5)
  -- -------- -- -------

where the @xmath -th graph convolution layer’s output @xmath is mapped
by the next layer’s parameter matrix @xmath and outputs the new @xmath .
By stacking multiple layers allows us to form a deep and hierarchical
graph neural network that is powerful and capable of achieving
high-level part features interaction and fusion. Assuming that we have
stacked @xmath graph convolution layers, then the obtain @xmath can be
further received by subsequential graph pooling and regression layers to
output semantic descriptions, which the process can be supervised by
training seen class semantic prototypes.

#### 5.3.3 Zero-shot Learning as a Graph Recognition

Given labeled seen class examples @xmath , we have three steps to
convert the ZSL to a graph-to-semantic mapping problem: 1) parts
decomposition, 2) graph construction, and 3) example graph recognition.
The parts decomposition obtains several parts of an example. These parts
are then presented by a graph structure in which each node is an example
part and each edge stands for a certain relation between two
corresponding parts. With these presented graphs of labeled seen class
examples, we feed them to our GNNs and sequentially pass through the
graph convolutional layers, graph pooling layers and regression layers
to output semantic descriptions.

##### 5.3.3.1 Parts Decomposition

Key-point localization is usually applied to predict a set of semantic
key-points for objects [ 46 , 40 ] . For example, a bird can have
several standard key-points reflecting its appearance and subtle
characteristics. Taking the CUB-200-2011 Birds dataset [ 106 ] as an
example, a bird image can be detected with 15 key-points from which each
key-point is located at a specific part, e.g., forehead, beak, leg,
tail, etc. These key-points can be used to align birds and reveal their
subtle differences which helps to recognize different bird species. This
recognition problem usually handles the scenario where objects have
small inter-class variations such as fine-grained categorization. The
deep learning based key-point localization networks usually follow two
paradigms for prediction. The first is to directly regress discrete
key-point coordinates, and the second is to uses a @xmath -dimensional
probability distribution heat map of the object image to localize the
key-points. In our method, we follow PAIRS [ 40 ] which falls into the
second paradigm, to construct the parts decomposition module. A
pre-trained network, e.g., ResNet-34 [ 42 ] with the classification
layer removed, is used as the encoder. Then, by stacking three blocks
consisting of one upsampling (bilinear interpolation) layer, one
convolution layer, one batch normalization layer and one ReLU layer
each, and a final convolution layer and upsampling layer to output the
key-points location tensor.

The parts decomposition module can localize several key-points of an
example (Figure 5.2 .a), and we can then use a cropping operation to
decompose the example (e.g., with width @xmath and height @xmath ) to
several parts based on these key-points as:

  -- -------- -- -------
     @xmath      (5.6)
  -- -------- -- -------

where @xmath is the 2-d coordinate of one key-point. @xmath and @xmath
are width and height of the cropped part. @xmath is the 2-d coordinate
of the top left corner of the cropped part where @xmath and @xmath are
potentially set to @xmath and @xmath respectively, or 0, to retain the
cropped part within the example size range. These parts are then can be
regarded as nodes of an example graph, and each of them is further be
extracted by a CNNs to obtain its visual features as the node features.

##### 5.3.3.2 Example Graph Construction

Given the nodes, we need to determine their neighboring nodes and set an
edge if two nodes can be regarded as neighbors. It should be noted that
the neighboring nodes are not necessarily spatially adjacent, and the
spatial neighbors are also not necessarily neighboring nodes of an
example graph. To determine the edges, we assume that every 2-nodes pair
is first imposed and established with a relation, and then to verify
whether this relation can be satisfied under a certain measurement. In
our method, we propose to first establish a relation between two nodes
(parts) which is related to the nodes propagation of a graph. In Figure
5.2 .b, we assume two nodes Part-i and Part-j can propagate with each
other. @xmath and @xmath are their node features and the propagation is
achieved by @xmath , @xmath . Regardless of the orders and weights, we
can simply use the mean features of these two nodes, i.e., @xmath , to
present whether the propagation is positive or not. This is similar to
two nodes performing graph convolution with a same weight of @xmath and
containing self-loop. To verify the positivity, we train a classifier
based on whole feature representations of our training examples, and to
test on each nodes (parts) pair to compute the classification confidence
score on nodes @xmath , @xmath and their propagated @xmath ,
respectively. If both @xmath and @xmath hold, we can say the relation is
satisfied. Here the @xmath is a small constant that controls the
threshold. Taking the CUB-200-2011 Birds ¹ ¹ 1
http://www.vision.caltech.edu/visipedia/CUB-200-2011.html as an example,
we obtain 15 parts of each training example and compute the
classification confidence score on each 2-parts pair. The results are
shown in Figure 5.2 .c which the filled contour plot reflects the
confidence of each 2-parts pair under such a relation. The results on
the diagonal represent the confidence of the 15 parts themselves and
each intersection of a point on the horizontal axis, e.g., @xmath , and
a point on the vertical axis, e.g., @xmath , represents the confidence
of pair Part-i and Part-j. By choosing a threshold @xmath , we can
further construct the adjacency matrix of example graph where a smaller
threshold results more edges and a larger one controls the number of
edges in a reasonable scale. The adjacency matrix is illustrated in
Figure 5.2 .d. It should be noted that in our method, we focus on the
fine-grained ZSL which considers the example-level graph. For
simplicity, we assume each example, e.g., birds, has the same part
relations and thus the example graph also has the same adjacency matrix.

##### 5.3.3.3 Example Graph Recognition

Given labeled seen class examples @xmath , we can present them by
example graphs as @xmath , where @xmath and @xmath are the adjacency
matrix and node feature matrix, respectively, in which we stack @xmath
graph convolution layers to obtain @xmath outputs as:

  -- -------- -------- -- --------
     @xmath   @xmath      (5.7)
     @xmath   @xmath      (5.8)
              @xmath      (5.9)
     @xmath   @xmath      (5.10)
  -- -------- -------- -- --------

where each obtained @xmath is the propagated feature matrix of @xmath
-th layer. Each row represents a node and each column represents a
feature channel. Specifically, @xmath allows every single node (part) to
propagate information to its neighboring nodes (parts) and aggregate
their information along with self-ones. Next, we add a layer to
horizontally concatenate these outputs as:

  -- -------- -- --------
     @xmath      (5.11)
  -- -------- -- --------

where @xmath is the concatenated feature matrix in which each row can be
regarded as the global feature representation of a node encoding the
high-level and multi-scale local substructure information. For
simplicity, we denote @xmath and apply the column-wise average pooling
to @xmath and obtain a @xmath -dimensional feature vector as @xmath ,
which downsamples and fuses high-level features. As to the regression
layers, we simply apply the fully-connected dense multi-layers to @xmath
and links to its class semantic prototypes. Thus, given @xmath , the
training of ZSL can be formalized as:

  -- -------- -- --------
     @xmath      (5.12)
  -- -------- -- --------

where @xmath is the loss function, @xmath and @xmath are trainable
network parameters for GNNs and fully-connected dense regression layers,
respectively. @xmath and @xmath being the regularization terms. For the
inference, given a testing example, e.g., @xmath , the recognition can
be described as:

  -- -------- -- --------
     @xmath      (5.13)
     @xmath      (5.14)
  -- -------- -- --------

where @xmath is a similarity metric, i.e., we select cosine similarity,
and @xmath searches the most closely related prototype and set the class
corresponding with this prototype to the test example. Eq. 5.13 is used
for the conventional ZSL and Eq. 5.14 is used for the generalized ZSL.

### 5.4 Experiments

#### 5.4.1 Datasets and Metrics

To demonstrate the effectiveness of our model, the dataset needs to
satisfy some criteria: 1) focuses on the fine-grained domain; 2)
annotations on key-points or parts are required to train and obtain
parts of examples; and 3) is the benchmark dataset of ZSL. Considering
the above criteria on all five benchmark datasets of ZSL, i.e.,
CUB-200-2011 Birds, Animals with Attributes, aPascal&Yahoo, SUN
Attribute, and ILSVRC2012/ILSVRC2010, only the CUB-200-2011 Birds [ 106
] can satisfy the needs. The CUB-200-2011 Birds ² ² 2
http://www.vision.caltech.edu/visipedia/CUB-200-2011.html (CUB) dataset
consists of 11,788 images of 200 bird species. Each image is annotated
with part location, bounding box, and attribute labels. For ZSL, 150
species containing 8,855 images act as seen classes for training, and
the other 50 species containing 2,933 images are unseen classes for
testing. Each of their prototypes is represented by a 312-dimensional
semantic attribute descriptions present meaningful class-level
information.

In our experiments, we compare our method on two different ZSL settings
including 1) conventional ZSL and 2) generalized ZSL . For the
conventional ZSL setting, all test examples only belong to unseen
classes, i.e., only search the class prototypes on seen classes @xmath
(Eq. 5.13 ). For the generalized ZSL setting [ 117 ] , the search can
also generalize to novel examples from seen classes, i.e., search the
class prototypes on both seen and unseen classes @xmath (Eq. 5.14 ). The
hit@k accuracy [ 28 , 79 ] is used to evaluate the model performance.
The hit@k accuracy is the standard evaluation metrics in ZSL which can
be defined as:

  -- -------- -- --------
     @xmath      (5.15)
  -- -------- -- --------

where @xmath is an indicator function takes the value “1” if the
argument is true, and “0” otherwise. @xmath is the operation which
determines the class label of example. In our experiments, hit@1
accuracy is used for CUB.

#### 5.4.2 Experimental Setups

Our method is implemented by Pytorch and trained with one Nvidia GTX
1080 Ti GPU. The GNNs consist of four graph convolution layers with
hidden channel size 512. A concatenation layer is used to horizontally
concatenate outputs of different layers, and a column-wise average
pooling is further applied to downsample and fuse the high-level
features. The regression layers are simple fully-connected dense
multi-layers and output 312-dimensional features and supervised by class
semantic prototypes. The mean absolute error (MAE) is selected as the
loss function @xmath in Eq. (9). We have the GNNs 5000 runs on the
training set. As to the parts decomposition, we follow all settings of
PAIRS [ 40 ] to construct the key-point localization network. A
pre-trained ResNet-34 [ 42 ] with the classification layer removed is
acted as an encoder. Three blocks consisting of one upsampling (bilinear
interpolation) layer, one convolution layer, one batch normalization
layer, and one ReLU layer each, and a final convolution and upsampling
layers are stacked to decode the key-points location. The cropped part
size @xmath and @xmath are set to 56 in Eq. 5.6 . In the experiments, we
also report the result when directly use the key-point annotations to
construct the example graph, which the two results are denoted as
detection based (Ours (D)) model and annotation based (Ours (A)) model,
respectively. When constructing the example graph, we control the
threshold to empirically retain around 50 edges (52 in CUB-200-2011
Birds verified on training examples). We use GoogleNet [ 100 ] to
extract the visual features (1024-d) for nodes (parts).

#### 5.4.3 Results and Analysis

#### 5.4.4 Conventional ZSL Results

To demonstrate the effectiveness of our proposed method, we first
compare it with existing representative methods in the conventional ZSL
setting. We selected 16 competitors based on the following criteria: 1)
published in the most recent years; 2) cover a wide range of models; 3)
they clearly represent the state-of-the-art; and 4) all of them are
under the standard splits [ 117 ] . We compute and report the multi-way
classification accuracy as in previous works. The comparison results
with the selected representative competitors are shown in Table 5.1 . It
can be seen from the results that our method outperforms all competitors
with great advantages. The prediction accuracy of our method achieves
76.1% and 78.4% for detection and annotation based models, respectively.
Moreover, we can also observe that the performance of fine-grained based
methods is generally better than the average result of all competitors.
Specifically, comparing with @xmath [ 48 ] , Chen et al. [ 18 ] and Zhu
et al. [ 135 ] which also fall into the fine-grained ZSL models, our
method improves the prediction accuracy by a large margin as 3.1%, 20.1%
and 7.9%, respectively, which fully demonstrate the effectiveness of our
method.

#### 5.4.5 Generalized ZSL Results

In Table 5.2 , we compare our method with 15 competitors on generalized
ZSL setting [ 117 ] . In addition to the selection criteria of 1), 2)
and 3) of conventional ZSL, all selected competitors are also required
to comply with the disjoint assumption stated by [ 117 ] . For the
generalized ZSL, we compute and report the average per-class prediction
accuracy on test images from unseen classes (U) and seen classes (S),
respectively, and report the Harmonic Mean calculated by @xmath , which
can quantify the aggregate performance across both seen and unseen
classes. It can be seen from the results that, although most of these
competitors cannot retain the same level performance on both seen and
unseen classes, our method can achieve the best balanced prediction
accuracy. For example, ESZSL [ 92 ] , SYNC [ 16 ] and SAE [ 57 ] have a
very large margin, i.e., 51.2%, 59.4% and 50.1%, between the accuracy of
seen and unseen classes. In contrast, our method can obtain both
comparative results on unseen classes and seen classes as 42.8% and
69.7%, and thus resulting the best result of Harmonic Mean as 53.0%. Our
method outperforms all competitors for the most balanced prediction
accuracy which makes it better fits a more realistic application
scenario.

#### 5.4.6 Mapping Robustness

We further conduct the evaluation of mapping robustness on our method.
Given the trained model, we map the unseen class examples from the
visual to semantic feature space. With these obtained semantic features
of examples, we apply t-SNE to visualize them in a 2D map. We show the
visualization results on SAE [ 57 ] , AMS-SFE [ 37 ] and our method
under the conventional ZSL setting in Figure 5.3 .a, Figure 5.3 .b and
Figure 5.3 .c, respectively. It can be seen from our method that only a
small portion of unseen class examples are shifted in the semantic
feature space. Moreover, the obtained semantic features are also more
continuous and aggregated. These merits demonstrate that our method can
significantly mitigate the domain shift problem.

### 5.5 Remarks

This chapter proposes a novel fine-grained ZSL framework based on
example-level graph, to address the challenging domain shift problem.
Our method decomposes examples into several parts to be presented as a
graph, in which nodes and edges are different parts and certain
relations between parts. Taking advantages from recently developed GNNs,
we formulate the ZSL to a graph-to-semantic mapping problem which can
better exploit part-semantic correlation and local substructure
information in examples. Experimental results verified the effectiveness
of our method.

## Chapter 6 Conclusion and Future Work

### 6.1 Conclusion

This thesis explores three efficient ways to mitigate the domain shift
problem and hence obtain a more robust visual-semantic mapping function
for zero-shot learning (ZSL). Our solutions focus on fully empowering
the semantic feature space, which is the key building block of ZSL.
First, we propose to adaptively adjust to rectify the semantic feature
space regarding the class prototypes and global distribution. The
adjustment improves the ZSL models in two aspects. First, the adjustment
of class prototypes helps to rectify the mismatch between the diversity
of various visual examples and the identity of unique semantic features,
e.g., attributes or word vectors, which makes the visual-semantic
mapping more robust and accurate. Second, the adjustment of global
distribution helps to decrease the intra-class variance and to enlarge
the inter-class diversity in the semantic feature space, which can
further mitigate the domain shift problem. Second, we propose to align
the manifold structures between the visual and semantic feature spaces,
by semantic feature expansion. We build upon an autoencoder-based model
to expand the semantic features from the visual inputs. Additionally,
the expansion is jointly guided by an embedded manifold extracted from
the visual feature space. Compared to the previous work which conducts a
hard adjustment, the alignment process is more conservative for not
directly adjusting the previous semantic features. Thus, we could call
it as a soft adjustment. This alignment can derive two benefits: 1) we
expand some auxiliary features that can enhance the semantic feature
space; 2) more importantly, we implicitly align the manifold structures
between the visual and semantic feature spaces. Thus, the mapping
function can be better trained and mitigate the domain shift problem.
Last, we propose to further explore the correlation between the visual
and semantic feature spaces in a more fine-grained perspective. Unlike
several existing works, we decompose an image example into several parts
and use an example-level graph-based model to measure and utilize
certain relations between these parts. Taking advantage of recently
developed graph neural networks, we further formulate the ZSL to a
graph-to-semantic mapping problem, which can better exploit the visual
and semantic correlation and the local substructure information in
examples. By converting the ZSL to the graph recognition task, we can
better utilize the correlation between two feature spaces, thus can
mitigate the domain shift problem.

### 6.2 Future Work

In the future, we mainly have three research directions. First, we will
continue investigating the empowerment of semantic feature space for
ZSL. This direction aims to enhance the semantic feature space, thus to
mitigate the domain shift problem in natural and obtain the more robust
visual-semantic mapping. Second, we will go from the coarse-grained
method to the fine-grained method to better model the subtle differences
among classes in ZSL. Last, we will investigate and conduct more work on
the graph-based method to solve the ZSL problem. This direction makes
use of the powerful representation ability of the graph techniques,
e.g., graph neural networks, to model complex correlation in real world
application.
