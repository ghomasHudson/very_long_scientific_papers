##### Contents

-    Declaration
-    Acknowledgements
-    Summary
-    1 Introduction
    -    1.1 Atmospheric turbulence and light propagation
        -    1.1.1 Basic turbulence modelling
        -    1.1.2 Image propagation
    -    1.2 Characterising wavefront aberrations and image quality
    -    1.3 Adaptive optics
        -    1.3.1 Field of view and isoplanatic angle
        -    1.3.2 Sky coverage
        -    1.3.3 Observation wavelengths
    -    1.4 Lucky imaging
        -    1.4.1 A very brief history
        -    1.4.2 Pros and cons
        -    1.4.3 Current status of lucky imaging techniques
    -    1.5 New hardware: The quad-CCD LuckyCam
    -    1.6 Statement of thesis
    -    1.7 Chapter summaries
-    2 Calibration of electron multiplying CCDs
    -    2.1 The physics of electron multiplying charge-coupled devices
    -    2.2 Basics of CCD calibration
    -    2.3 Motivation and comparison with previous work on EMCCD
        calibration
    -    2.4 Probability distribution models for EMCCDs
        -    2.4.1 Clock induced charge
    -    2.5 Histogram analysis algorithms
        -    2.5.1 Bias determination at low pixel counts for the
            purpose of single frame bias estimation
        -    2.5.2 Electron multiplication gain
    -    2.6 Calibrating the Cambridge e2v CCD201
        -    2.6.1 Spatial gradient in bias pedestal
        -    2.6.2 Bias pedestal drift
        -    2.6.3 Internally generated signal levels
        -    2.6.4 Clock induced charge levels
        -    2.6.5 Flat fielding and gain uniformity
        -    2.6.6 Bad pixels and pixel weighting
    -    2.7 Results
-    3 Lucky imaging of faint sources: Thresholding techniques
    -    3.1 Summary and comparison with literature
    -    3.2 Signal to noise ratio for conventional and electron
        multiplying CCDs
    -    3.3 Thresholded signal to noise equation
    -    3.4 Choosing the best detector mode for an observation
    -    3.5 Threshold optimization
    -    3.6 Combining thresholded and linear data
    -    3.7 Results of applying thresholding techniques to real data
-    4 Optimising and predicting the image formation process
    -    4.1 Background — Sampling theory and image combination
    -    4.2 Background: Speckle patterns and speckle imaging
    -    4.3 Frame registration overview
    -    4.4 Registration methods
        -    4.4.1 Implementation details
    -    4.5 Simulation methods used for testing
    -    4.6 Testing registration algorithms
        -    4.6.1 Results from comparison via simulation
        -    4.6.2 Comparison using real data
    -    4.7 Image formation processes
        -    4.7.1 Instantaneous Strehl ratio probability distribution
            function
        -    4.7.2 Strehl estimation error
        -    4.7.3 Frame registration positional error
    -    4.8 Image formation model
-    5 Scientific applications of lucky imaging
    -    5.1 Binarity of planetary transit hosts
        -    5.1.1 Planetary transit surveys and the need for follow-up
            observations
        -    5.1.2 General techniques for detecting faint secondary
            sources
        -    5.1.3 Primary star PSF modelling and subtraction
        -    5.1.4 Applying a matched filter
        -    5.1.5 Automated companion candidate detection
        -    5.1.6 Companion candidate analysis
        -    5.1.7 Results
    -    5.2 High temporal resolution photometry with EMCCDs
        -    5.2.1 Cygnus X-1
        -    5.2.2 Estimating a lower bound to signal variance
        -    5.2.3 Fast photometry data reduction techniques
        -    5.2.4 Results
        -    5.2.5 Future work
    -    5.3 General high resolution imaging in the visible
        -    5.3.1 Faint limits
        -    5.3.2 High resolution across a wide field of view
    -    5.4 Science with lucky imaging-enhanced adaptive optics:
        Probing the binary star distribution in globular clusters
        -    5.4.1 Globular clusters
        -    5.4.2 Metric: star separations detected vs. random
            positioning model
        -    5.4.3 Probing the binary distribution of M13 with LAMP
-    6 Modelling of lucky imaging systems
    -    6.1 End-to-end Monte Carlo simulation of atmospheric effects
        and optical systems
        -    6.1.1 Atmospheric phase screens
        -    6.1.2 End-to-end Monte Carlo simulation packages
    -    6.2 Simulating photon shot noise and the EMCCD response
    -    6.3 Modelling lucky imaging at the Nordic Optical Telescope
    -    6.4 Modelling hybrid adaptive optics systems
        -    6.4.1 Strehl ratio statistics behind adaptive optics
        -    6.4.2 Preliminary investigations through simulation
        -    6.4.3 Future work
-    7 Data reduction
    -    7.1 The lucky imaging pipeline
        -    7.1.1 Observation reduction preparation tool
        -    7.1.2 First pass — frame calibration and registration
        -    7.1.3 Second pass — frame thresholding and recombination
    -    7.2 Technical aspects
        -    7.2.1 Data storage considerations
        -    7.2.2 Notes on pipeline implementation
    -    7.3 Astrometric Calibration
        -    7.3.1 Creating an astrometric calibration catalogue from
            HST archive data
        -    7.3.2 Calibration procedure
-    8 Conclusion

###### List of Figures

-    1.1 The von Karman Power Spectrum
-    1.2 Zernike modes
-    1.3 AO Schematic
-    1.4 The cone effect of LGS-AO
-    1.5 LuckyCam 2009 on the NOT
-    1.6 LuckyCam 2009 camera schematic
-    1.7 LuckyCam 2009 internals
-    2.1 EMCCD serial register PDF curves.
-    2.2 Calibration Frames Histogram
-    2.3 CCD201 Schematic
-    2.4 EMCCD model PDF components
-    2.5 Testing EM gain estimation algorithms
-    2.6 Calibrating the bias pedestal gradient
-    2.7 Variation in horizontal bias gradient
-    2.8 Bias drift
-    2.9 Average of bias frames showing internally generated signal
-    2.10 Fitting the CICIR component
-    2.11 EM gain variation
-    2.12 Improvement in image quality due to calibration
-    3.1 Comparison of SNR for different CCD observing modes
-    3.2 SNR comparisons
-    3.3 SNR improvement with thresholding
-    3.4 SNR test image
-    4.1 Effective PSF
-    4.2 Sampling rates
-    4.3 Drizzle schematic
-    4.4 Simulated PSF examples
-    4.5 Typical range in simulated results
-    4.6 Comparison of interpolation methods
-    4.7 Effect of normalising CC reference
-    4.8 Improvement in Strehl ratio in real data through use of
    normalised CC reference
-    4.9 Strehl ratio histogram
-    4.10 Scatter plots of Strehl ratio estimates
-    4.11 Correlation values between estimated and true Strehl ratio
-    4.12 Mean Strehl ratio of selected 10%
-    4.13 Frame selection probability functions
-    4.14 Sample plots of x position estimation error
-    4.15 Variation in position estimation error standard deviation with
    Strehl ratio
-    4.16 Comparison between analytically predicted and simulated
    positional estimation error
-    5.1 Illustration of transits and occultations
-    5.2 PSF residuals cross section
-    5.3 PSF residuals
-    5.4 Images from the analysis procedure.
-    5.5 Binary detections
-    5.6 HAT-P-6 residual images
-    5.7 Binary detection limits
-    5.8 Cygnus X-1
-    5.9 Cygnus X-1 photometry methods
-    5.10 Frames of data rejected by the Ultracam pipeline
-    5.11 Atmospheric effects on fast photometry
-    5.12 Standard deviation in sub-samples of the fast photometry
-    5.13 M13 FWHM analysis targets
-    5.14 FWHM across a wide field of view
-    5.15 Comparison of LAMP and HST data
-    5.16 LAMP footprint
-    5.17 GC Binary separation distribution
-    5.18 Source extraction in the LAMP M13 data
-    5.19 Observed and simulated source separation histograms
-    6.1 Aliasing of non-periodic low frequencies
-    6.2 Subharmonic sampling points in the Fourier domain
-    6.3 Improvement in structure function with subharmonics
-    6.4 Improvement in Zernike polynomial statistics with subharmonics
-    6.5 The Devil’s Staircase
-    6.6 Dye in a Jet
-    6.7 Temperature Fluctuation
-    6.8 Inadequate low spatial frequencies
-    6.9 Simulation of photon shot noise and detector response
-    6.10 Turbulence profile at the ORM
-    6.11 Atmospheric profiles at the El Teide Observatory
-    6.12 CN2 profiles at other locations
-    6.13 Instantaneous Strehl probability distributions behind AO
    systems
-    6.14 Instantaneous Strehl probability distributions after removal
    of Zernike modes
-    6.15 Strehl ratios obtained with lucky imaging adaptive optics
-    6.16 FWHMs obtained with lucky imaging adaptive optics
-    7.1 Data reduction preparation.
-    7.2 Astrometric catalogue created using S-Extractor and archival
    HST data
-    7.3 A drizzled mosaic of M13, created using the astrometrically
    calibrated inter-CCD spacings

## Chapter 1 Introduction

In this chapter I briefly review the aspects of ground based astronomy
needed to give context to the rest of this thesis. The effects of
atmospheric turbulence are described and key mathematical notations set
out. After defining basic terminology, the corrective techniques of
adaptive optics and lucky imaging are introduced. Finally, I state my
thesis proposition and summarise the following chapters.

### 1.1 Atmospheric turbulence and light propagation

Looking up at the night sky, it is easy to observe the distorting
effects of the atmosphere, even on a cloudless night — the stars
twinkle. This phenomenon of varying intensity in the stellar light,
scintillation, is one of a wider range of atmospheric optical effects
known generally as “seeing.” Since seeing and the atmospheric turbulence
that causes it play a key role in the techniques discussed here I will
give a brief introduction to the models used in its description. Many of
the intricacies are omitted for brevity; the reader is referred to, for
example, Roddier ( 1981 ) for a concise review of the relevant
derivations.

#### 1.1.1 Basic turbulence modelling

The seeing effects of turbulent flow within the atmosphere are indirect.
Optical distortions are not due to the turbulent motion of the
atmosphere itself, but rather due to variations in refractive index
caused by temperature differences. There is an ever changing temperature
gradient in the atmosphere, and seeing occurs when small pockets of air
become mixed into a layer with a different temperature, causing a
significant change in refractive index on small spatial scales. The
temperature of a small volume of air equalises with its surroundings
slowly compared to the mixing timescale, so we can think of the
temperature as a ‘passive advective scalar’ carried by the turbulent
velocity field in much the same way as dye can be seen to follow a
turbulent liquid flow (buoyancy effects being negligible).

To characterise this process mathematically we first invoke the
Kolmogorov (1941) scaling law for homogeneous three dimensional
turbulence. The basic assumption is that energy only enters the
turbulent system at the largest scales and only dissipates at the
smallest due to viscosity; for example in a teacup the largest or
‘outer’ scale might be the stroke length of a stirring spoon. We further
assume that between these large and small scales, in what is known as
the ‘inertial range,’ kinetic energy is conserved. ¹ ¹ 1 Note that the
assumptions of Kolmogorov’s 1941 theory do not account for energy stored
as or converted into pressure fluctuations, which are sometimes
significant. The energy propagates down to the inner scale through a
series of subdividing vortices in what is known as a Richardson cascade
(after the fluid dynamicist Lewis Fry Richardson). This allows a swift
dimensional analysis arriving at the power law for the spectral energy
density in three dimensions:

  -- -------- -- -------
     @xmath      (1.1)
  -- -------- -- -------

where @xmath is the spectral energy density, and @xmath is the
wavenumber corresponding to a spatial scale @xmath , with the relation
@xmath . This is the ubiquitous ‘Kolmogorov power spectrum,’ which has
been verified for the velocity field in a wide range of turbulent
fluids, at scales in the inertial range (Figure 1.1 illustrates the drop
off at the inner and outer scales). Kolmogorov also introduced the use
of the structure function as a tool for studying turbulence. Random
variables such as velocity are often not stationary, that is to say the
underlying mean value of the random process may be slowly varying. We
use the structure function, denoted @xmath and defined as

  -- -------- -- -------
     @xmath      (1.2)
  -- -------- -- -------

i.e. the mean square difference between two locations, to look at the
variations on different scales. The Kolmogorov structure function for
the velocity field in three dimensional homogeneous turbulence is

  -- -------- -- -------
     @xmath      (1.3)
  -- -------- -- -------

where @xmath is a parameter depending on the energy of the turbulence.

Obukhov ( 1949 ) and Corrsin ( 1951 ) independently suggested that a
passive advective scalar should have the same structure function and
power spectrum as the velocity field. This assumption is somewhat flawed
(see Section 6.1.1 ), but the structure function largely satisfies the
standard Kolmogorov power spectrum, and this has proven to be an
adequate hypothesis for much modelling of astronomical seeing.

#### 1.1.2 Image propagation

The next step is to consider the interaction of the star light with this
turbulence. Taking the classical view of light as oscillations in a
complex scalar field we can consider incident monochromatic light with
representation

  -- -------- -- -------
     @xmath      (1.4)
  -- -------- -- -------

which experiences perturbations due to the varying refractive index of
the turbulence and emerges as

  -- -------- -- -------
     @xmath      (1.5)
  -- -------- -- -------

for any instantaneous observation. Here @xmath is the fractional change
in the amplitude, A, due to atmosphere, and @xmath is the wavefront
perturbation, which evolve as the turbulent atmospheric temperature
structure changes.

Since we are considering ‘clear air turbulence’ ² ² 2 i.e. without
clouds partially absorbing the light, or any other change in atmospheric
transmission. the varying refractive index of the atmosphere only
directly affects the phase of the light; amplitude variations are a
secondary effect caused by the diffraction of light as it propagates
between the turbulent layer and the observer. For the purposes of
modelling a good astronomical site at longer wavelengths it is usually
appropriate to neglect diffraction and scintillation effects. This is
called the geometric propagation model, as it effectively models bundles
of rays ³ ³ 3 Vectors representing paths of light propagation. whose
wavefronts are advanced or retarded by the turbulent phase perturbations
collectively referred to as the ‘phase screen.’

Tatarski ( 1961 ) showed that for an atmospheric refractive index @xmath
, which has turbulence induced variations with structure function

  -- -------- -- -------
     @xmath      (1.6)
  -- -------- -- -------

the structure of the atmospheric phase screen @xmath takes the form

  -- -------- -- -------
     @xmath      (1.7)
  -- -------- -- -------

where @xmath is the structure function parameter, varying with height
@xmath in accordance with different intensities of turbulent variation
in refractive index. This integral can be rewritten in terms of a single
parameter; the Fried coherence length @xmath (Fried 1965 ) :

  -- -------- -- -------
     @xmath      (1.8)
  -- -------- -- -------

which describes how the mean square phase difference between two points
scales with the distance between them. The Fried coherence length
represents the diameter of a telescope which will be almost unaffected
by the seeing ⁴ ⁴ 4 Specifically, the Fried coherence length defines the
aperture of a telescope with the same optical transfer function as the
atmospheric effects. See Roddier ( 1981 ) for more detail. , and varies
with the wavelength of observation as @xmath . As the ratio between
telescope diameter and Fried length increases the point spread function
that would ideally be an Airy disc splits up into multiple bright spots
known as speckles, created by interference between light from regions
across the telescope aperture within which the phase differs by around 1
radian, sometimes referred to as ‘coherent cells.’ This ratio, @xmath ,
is a useful dimensionless number when comparing the severity of
atmospheric turbulence for different telescope sizes, observing
wavelengths, and prevailing atmospheric conditions. This phase
disturbance scaling law has been verified by observation over a range of
scales, with deviations as the outer scale of the turbulence is
approached. This characterisation of the phase screens can be used as a
model to simulate the effects of atmospheric turbulence upon
astronomical telescopes, as covered in chapter 6 .

### 1.2 Characterising wavefront aberrations and image quality

Since terms describing wavefront perturbations and image quality are
often of use when discussing astronomy in the presence of atmospheric
turbulence, it is worth introducing them for the unfamiliar reader
before proceeding.

##### Zernike modes

In the same way that sine and cosine functions can be used to represent
a signal composed of many different frequencies via a Fourier transform,
a surface representing the phase of a non-planar wavefront entering a
telescope pupil can be broken down into orthogonal components. A
suitable set of functions for performing such a deconstruction are the
Zernike modes (Noll 1976 ) . The low order Zernike modes correspond to
the lowest frequency changes across the wavefront, and due to the
Kolmogorov power spectrum these large scale changes always have the
greatest amplitude (on average). Therefore when performing modal
correction — i.e. trying to correct for each Zernike mode in turn — the
largest improvements come from correcting the low-order terms first.
Hence the terms ‘low order’ and ‘high order’ adaptive optics. The first
Zernike term is piston, important for comparing phase across two
interferometry apertures but irrelevant for a single aperture. The next
two terms are tip-tilt, which together create an overall slope of the
wavefront - this slope causes the point spread function (PSF) to move
about in the focal plane. Higher order terms cause all manner of
distortions, well known to manufacturers of optics and telescopes.
Figure 1.2 illustrates the wavefront shape for the astigmatism and
defocus modes.

##### Point Spread Function

When describing the image quality of an optical system, the point spread
function is usually the main focus of discussion. This is the light
intensity distribution as measured in the focal plane, for an unresolved
source (i.e one well approximated by an infinitesimal ‘point’ source of
light). Note that there are subtleties in describing the PSF, for
example one may choose to include or neglect the effects of pixellation
and sampling. These distinctions are covered briefly in chapter 4 .

##### Strehl ratio and full width at half maximum

While Zernike modes describe phase perturbations directly, they do not
describe the effect of atmospheric perturbations upon the image quality,
and indeed the PSF. There are two ubiquitous measures for these effects.

The first is the Strehl ratio, defined as the ratio between the peak
intensity of the observed PSF, and that which would be measured if no
phase perturbations were present and the optical system were performing
perfectly. To determine this ratio we require the total source flux, in
order to estimate the ideal peak intensity. Accurate measurements of
Strehl ratio therefore require both accurate photometry and a well
sampled image so that the peak intensity may be accurately estimated.
The ideal peak intensity is often well approximated by using an Airy
disc model for the ideal image, although this may not be the case if
significant support structures are present or a segmented primary mirror
with large inter-segment spacing is used.

The second is full width at half maximum (FWHM). As the name suggests,
this enumerates the angular width of the PSF at half the peak intensity.
The implicit assumption is that the PSF is axisymmetric; while often not
exactly true this is usually a reasonable approximation to the half
maximum radius.

Many other measures such as full width at half enclosed flux (FWHEF) and
encircled energy (EE) at a given radius exist, but Strehl ratio and FWHM
are the most commonly used.

##### Seeing width

Another commonplace term in ground based astronomy is seeing width. This
is simply the FWHM of a long exposure image obtained via conventional
imaging at a large telescope. The atmospheric effects upon the PSF make
this effectively independent of the telescope aperture for @xmath of,
say, 10 or greater. The seeing width @xmath is dependent upon observing
wavelength through the relation (Tokovinin 2002 ) :

  -- -------- -- -------
     @xmath      (1.9)
  -- -------- -- -------

For use as a general metric of seeing conditions @xmath is usually
quoted for a reference wavelength of 500nm.

##### Wavelength dependence

It is worth mentioning that metrics of image quality which refer to
characteristics of the PSF, such as Strehl ratio, FWHM, seeing width,
and isoplanatic angle (see below) are all strongly dependent upon the
observation wavelength. This is due to two factors. Firstly, propagation
path differences caused by atmospheric turbulence or imperfect optics
equate to different fractions of different wavelengths. For example, a
250nm optical path difference may be quite detrimental at 400nm (blue
light) but will have little effect at 2200nm (‘K band’ infrared).
Secondly, the ideal PSF also varies accordingly, such that longer
wavelengths will have a wider, shallower ideal PSF. The resulting
combined effect is usually a much improved Strehl ratio when observing
at longer wavelengths.

A corollary is the fact that since @xmath , the seeing width actually
decreases as the observation wavelength is shifted to longer wavelengths
according to the equation:

  -- -------- -- --------
     @xmath      (1.10)
  -- -------- -- --------

for a given severity of atmospheric turbulence, when in the seeing
limited regime (i.e. @xmath ).

### 1.3 Adaptive optics

The most widespread method of attempting to overcome atmospheric
turbulence effects in astronomy is adaptive optics. A brief recap of
some key concepts and issues in this significant subfield of optical
engineering is appropriate here, as it lends context to the discussion
of lucky imaging to come.

A full introduction to adaptive optics (AO) is beyond the scope of this
document, and the reader is referred to the excellent books of Tyson (
2000 ) for an overview and Hardy ( 1998 ) for a full review. Figure 1.3
serves as an illustration of the general premise. In short, a portion of
the incoming light is directed via a beamsplitter to some form of
wavefront sensor. From the sensor, data is passed to a control system
which estimates the perturbations to the wavefront. The control system
then calculates appropriate commands to send to the corrective
component, a deformable mirror. By using the deformable mirror to alter
the path lengths for different sections of the wavefront a correction to
the phase perturbations can be applied. The deformable mirror is placed
before the beamsplitter in the optical train, creating a closed loop of
feedback and correction. The aim is to perfectly counteract the ever
changing atmospheric phase perturbations, resulting in a planar
wavefront and thus achieving the full resolution and light concentration
potential of the telescope.

Note that the wavefront sensor requires a bright source in order to
sense the phase perturbations with good signal to noise ratio, in a time
span shorter than the typical evolutionary timescale of the atmospheric
turbulence. The easiest way to achieve this is to observe a single
bright star, termed the “guide star.” In practice, sensing and
correcting the atmospheric phase perturbations with the required
accuracy and speed is a challenging feat of engineering, and even with
idealised systems there are fundamental limits to the performance.

#### 1.3.1 Field of view and isoplanatic angle

Since the phase perturbations of star light are accrued during
propagation through the atmosphere, they depend on exactly which section
of atmosphere the light wave passed through. As a result, correcting for
phase perturbations for one source will generally only provide partial
correction for sources separated by a significant angular width. For
widely separated sources the phase perturbations may even be near
independent, such that applying corrections for one increases the phase
variance of the other.

In order to quantify this effect with a single number we use the
isoplanatic angle. This is defined as the angular separation between the
guide star and a secondary source at which Strehl ratio is decreased by
a factor of @xmath . As with Strehl ratio, the isoplanatic angle is
strongly dependent upon the wavelength of observation. At shorter
wavelengths, the path differences observed in light from sources of a
given angular separation becomes apparent at much smaller angles. Hence
observations at shorter wavelengths entail a smaller isoplanatic angle.

Typically the isoplanatic angle of a standard AO system is small
compared with the field of view — for example, measurements at the
Paranal observatory range from around 2.5 – 4 arcseconds (Sarazin et al.
2002 ) at 500nm. Even supposing total sky coverage (see below) this is a
serious limitation, particularly for surveys or large extended objects.
There are two developments of adaptive optics which seek to enlarge the
isoplanatic patch, multi-conjugate adaptive optics (MCAO) and
multi-object adaptive optics (MOAO). Both techniques use multiple guide
stars to probe the atmosphere in different directions and so
tomographically reconstruct a 3D model of the turbulence structure.
Multi-conjugate systems then use 2 or more deformable mirrors conjugated
to the optical heights of the turbulent layers, to correct for each
layer in turn. In this way the phase correction is arranged to be
different for different parts of the focal plane — so each target within
the corrected field of view has the phase disturbance of its own
propagation path corrected for, enlarging the effective isoplanatic
patch.

There is currently one MCAO demonstrator on sky, with the acronym MAD,
at the ESO Very Large Telescope facility (VLT). Gullieuszik et al. (
2008 ) report J band imaging with a mean FWHM of 0.15 arcseconds across
a 45 by 45 arcsecond region, when the estimated seeing FWHM at J band
was 0.52 arcseconds. This is an impressive accomplishment, and suggests
that imaging of a Hubble space telescope resolution and field of view is
now becoming available from the ground in the J and K bands. However,
the sky coverage issues described below still apply. Target acquisition
is also quite slow, with a typical loop closing period of 20-40 minutes
on MAD. The system relies on favourable turbulence conditions for best
performance, where most of the @xmath profile is confined to only 2
layers, one at ground level and the other at 8.5km. Future MCAO systems
could have more conjugated mirrors and variable conjugation heights, but
this reduces light throughput and increases complexity.

Multi-object adaptive optics is a proposed technique mainly applicable
to integral field unit spectroscopy (see for example Assémat et al. 2007
) . The idea is to use laser tomography and many deformable mirrors to
correct the phase disturbances separately for each spectrography target
in the field. Since it is impractical to perform wavefront sensing for
all the targets this requires open loop wavefront correction. Open loop
correction presents a significant technical challenge, because it
requires phase corrections to be made accurately without any direct
measurements of the current state of the deformable mirrors — so their
positioning must be correct first time. However it is not so reliant as
MCAO on the turbulence being highly confined to few layers.

The widest field of view likely to be achieved is with ground layer
adaptive optics (GLAO). This attempts to correct only for the layer of
turbulence closest to the telescope, which may often contribute half or
more of the integrated @xmath profile depth. This gives a wide
isoplanatic angle because the propagation paths for different targets
are close at this height. GLAO is mainly aimed at survey and
spectrography programs, as simulations suggest that typical FWHM
improvement is a comparatively modest 0.1 arcsecond better than seeing
width, but applies over a field of up to 49 by 49 arcminutes (For a low
order AO system on an 8m telescope, Andersen et al. 2006 ) . As the
number of guide stars increases the potential improvement under bad
seeing conditions becomes larger. Such a system has been proposed for
the Gemini observatory.

#### 1.3.2 Sky coverage

One of the major obstacles to widespread use of adaptive optics has been
sky coverage. The requirement for a bright enough guide star in the
vicinity of the target to be observed significantly limits the regions
of sky to which the technique can be applied. This has led to the
development of artificially generated “laser guide stars” (LGS). For
easy differentiation, a bright star used for guiding is generally
referred to as a “natural guide star” (NGS).

The current generation of adaptive optics systems have limiting natural
guide star magnitudes in the R band varying from 4th to 14th magnitude,
with curvature wavefront sensors providing lower order wavefront
information but working with sources up to 4 magnitudes fainter than
Shack-Hartmann sensors (Racine 2006 ) . A generous isoplanatic patch
size of 40 arcseconds (in K band, say) then implies sky coverage of
about 5%.

The AO community has sought to overcome this severe target constraint by
developing laser guide stars that can be pointed anywhere in the sky;
these come in two varieties, Rayleigh scattering and Sodium excitation
lasers. Rayleigh back scattering laser guide stars are a maturing
technology in terms of reliability and cost, to the level that a
low-cost robotic LGS-AO system is being developed for 1.5m class
telescopes Law ( 2008 ) . However, there are several problems associated
with Rayleigh guide stars, as reviewed in Devaney ( 2007 ) :

-   No tip-tilt determination: Tip-tilt information cannot be recovered
    from an LGS because the refraction of the beam on the way out
    cancels with the refraction on the way back. The usual solution is
    to observe an NGS simultaneously. Since the NGS is only being used
    to recover lowest order information it has less severe requirements
    than a full AO guide star — for example at the VLT this tip-tilt
    star must be brighter than 17th magnitude in V band and within about
    60 arcseconds of the target (Davies et al. 2008 ) . This requires an
    additional beamsplitter in the optics, and is an intrinsic problem
    with all laser guide stars.

-   Focal anisoplanatism or cone effect: The typical Rayleigh scattering
    height of 8-12 km presents problems for high order AO. The less than
    infinite propagation height means that the laser wavefronts are
    slightly curved, introducing focal anisoplanatism, and the highest
    layers of atmospheric turbulence are not probed (see Figure 1.4 ).
    This problem increases in magnitude with telescope size.

-   Spot elongation: Away from the axis of propagation the elongation of
    the laser scatter column can be seen, and this extension of the
    source makes it harder to use with a wavefront sensor. Again this
    problem gets worse as telescope size increases.

-   No fly zone: High power laser guide stars are required to employ
    aircraft spotters and switch off the beam if there is any chance of
    an encounter.

Due to the cone effect and spot elongation it becomes impractical to use
a Rayleigh scattering laser guide star as telescope size increases.
Sodium lasers excite the layer of atmospheric sodium at around 95km, and
so ameliorate the cone effect and to a lesser extent the spot
elongation. However, they come with their own problems --- sodium
frequencies are currently only widely available through use of dye
lasers, which are inefficient and unreliable compared to solid state.
For example, operation of the Keck system requires an extra observing
assistant and a laser technician. Solid state sodium lasers are in
development however, so wider application may not be many years away.
The larger size of sodium lasers usually prohibits them from being
placed on-axis of the main telescope, and so the beams have to be piped
by fibre optics, introducing another technical challenge. Finally, even
if the equipment works as intended the sodium layer varies in height and
density, ⁵ ⁵ 5 The sodium layer owes its existence to meteor showers,
resulting from the sodium atoms released as they burn up in the
atmosphere. Hence the abundance varies according to the seasonal meteor
density. so that if conditions are bad it may only produce a faint
fluorescence. In conclusion, sodium lasers are certainly an improvement,
but not a panacea.

#### 1.3.3 Observation wavelengths

The current generation of adaptive optics only work well at longer
wavelengths where the atmospheric phase perturbations are a smaller
fraction of the observing wavelength, or equivalently, @xmath ratios are
lower. Typically observations are made in the infra-red J,H and K bands,
with Strehl ratios ranging from @xmath 0.4–0.7 in K and @xmath 0.2–0.3
in the shorter J band (Racine 2006 ) . As a result, 10 metre class AO
assisted telescopes such as Keck and the VLT typically achieve imaging
resolution with a FWHM of order @xmath 0.1 arcseconds — on a par with,
but not exceeding, the resolution of the Hubble Space Telescope (of
course this data is complementary, since it provides information from a
different wavelength regime). It is worth noting that tests of next
generation adaptive optics systems are promising (e.g. Esposito et al.
2010 ) and the situation may improve once the new systems come online,
but certainly these figures are likely to remain valid for most
observatories in the near future.

To summarise, adaptive optics is now a relatively successful tool for
high resolution ground based observing with infra-red wavelengths, but
under significantly restricted conditions. The engineering complexities
presented mean that AO systems are expensive, and require highly skilled
personnel in order to operate and maintain them. This will remain the
case as the systems evolve and increase in complexity in order to
overcome some of the limitations of more basic systems.

### 1.4 Lucky imaging

#### 1.4.1 A very brief history

The origin of lucky imaging is credited to Fried ( 1978 ) , who gave one
of the first quantitative analyses of the subject in his paper
“Probability of getting a lucky short-exposure image through turbulence”
— giving rise to the technique’s name. It is of historical interest to
note the earlier work of R.E. Hufnagel (whom Fried cites in his own
work). Hufnagel proposed similar methods and derived the result that
lucky imaging achieves the best results when the ratio @xmath (Hufnagel
1966 ) , but was not widely published. ⁶ ⁶ 6 More information on
Hufnagel’s work may be found at:
http://www.ast.cam.ac.uk/research/instrumentation.surveys.and.projects/lucky.imaging/references
However, the technique was not feasible with the detectors available at
the time, and so its development into an observing tool has taken over
20 years.

The basic concept of lucky imaging is simple. The atmospheric seeing is
random; as such there are moments in time when the seeing is
significantly better or worse than average. The coherence time
associated with the seeing is on the order of tens of milliseconds, so
to pick out the very best moments requires a camera that can record
frames on a similar timescale.

With the continued development of CCDs throughout the 1980s and 1990s
the technical challenges started to become tractable; although detectors
were still either sensitive, but too slow to freeze the atmospheric
changes (e.g. Nieto et al. 1987 ) , or fast but noisy — a good example
of this is Dantowitz et al. ( 2000 ) , in which the authors produced
high resolution images of Mercury (a very bright source, often brighter
than 0th magnitude) on the Palomar 60 inch telescope at 60 frames per
second with digital video equipment. Baldwin et al. ( 2001 ) used a
specialised CCD at frame rates of up to 185 frames per second to prove
the concept of lucky imaging with stellar sources on the 2.5m Nordic
optical telescope, but were still limited to 6th magnitude stars by
readout noise. Development of the electron multiplying CCD cameras
described in chapter 2 finally combined the desired properties of fast
frame rates and low read noise, enabling application of lucky imaging to
a wide range of targets.

Another technological advance key to lucky imaging is the ever
increasing performance of computer systems, both in terms of processing
power and data throughput, since lucky imaging involves a fairly
sizeable amount of data processing. After data acquisition, each frame
is cleaned of detector artefacts and analysed in order to assess the
severity of the atmospheric turbulence at that moment (detailed in
chapter 4 ). The best frames are then aligned to correct the effects of
atmospheric tip-tilt shifting the image in the focal plane, and co-added
via a Drizzle algorithm (Fruchter and Hook 2002 ) . As more frames are
combined, signal to noise ratios slowly improve and faint stars become
visible above the background noise.

#### 1.4.2 Pros and cons

The key limitation of lucky imaging is that the the technique is only
feasible when telescope aperture size and atmospheric observing
conditions result in @xmath ratios of around 7 or less, as detailed in
table 1.1 . For higher ratios the chance of obtaining good frames
diminishes rapidly, following the formula (Fried 1978 ) :

  -- -------- -- --------
     @xmath      (1.11)
  -- -------- -- --------

The observation wavelengths are restricted by the availability of
sensitive high speed detectors, the sensitivity of current electron
multiplying CCDs declining as the wavelength moves from the visible into
the infra red. For typical seeing at good astronomical sites this limits
the technique to application on telescopes of 3m diameter at most,
resulting in images with FWHM of order @xmath 0.1 arcseconds. However,
lucky imaging fills the niche of high resolution imaging in the visible
wavelengths where adaptive optics is ineffective, the only other option
being to obtain space based observations.

Future developments in Mercury-Cadmium-Tellurium infrared detectors
(Rogalski 2005 , Rothman et al. 2009 ) may change this situation,
enabling lucky imaging in the infrared as a cheap alternative to
adaptive optics on medium sized telescopes. However, a proper
feasibility study considering current technology is beyond the scope of
this thesis.

Compared to adaptive optics, lucky imaging is a relatively low cost tool
for high resolution imaging. While an EMCCD camera may cost a few tens
of thousands of dollars, the price of a full adaptive optics system may
number in the millions. Operation could also be made relatively simple
and reliable, since no moving parts are involved.

Another advantage is that zero target acquisition time (after telescope
slew) means lucky imaging can make efficient use of observing time, for
example several hundred binary targets were observed over a few nights
with a queue observing routine during a February 2008 observing run (Law
et al. 2008 ) .

Finally, isoplanatic angles appear to be larger than for adaptive
optics, typically 30 arcseconds in average to good seeing. The faint
guide star limit is also considerably better than for adaptive optics,
with previous limiting estimates of approximately 16th magnitude in SDSS
i’ band Tubbs ( 2003 ), Law et al. ( 2006 ) (more on this in chapter 4
). The combination of these factors results in much better sky coverage
when compared to natural guide star AO.

#### 1.4.3 Current status of lucky imaging techniques

##### Conventional lucky imaging

Standard lucky imaging is now a proven technique for providing
diffraction limited imaging at telescope diameter to Fried coherence
length ratios of up to @xmath , for example in the SDSS i’ band on the
2.5m Nordic Optical Telescope (Law et al. 2005 , Lodieu et al. 2009 ) .
Electron multiplying CCDs capable of high frame rates are becoming more
widely available, and other teams have successfully implemented their
own routine usage lucky imaging systems which are often employed for
projects surveying binarity (Hormuth et al. 2008a , Oscoz et al. 2008 )
. Early EMCCD detectors were only available in small pixel formats, but
larger devices are now available with a format of @xmath , increasing
the field of view. Noise artefacts present in earlier cameras are now
much less troublesome.

##### Lucky imaging enhanced adaptive optics

In July 2007 the Cambridge lucky camera was tested behind the ‘PALMAO’
adaptive optics system on the 200 inch Hale telescope at Palomar
observatory (Law et al. 2008 ) . PALMAO usually operates in K band,
giving Strehl ratios of up to 0.5 (Troy et al. 2000 ) . Using the lucky
camera, observations were made at wavelengths of 500nm, 710nm, and
950nm. At 710nm lucky selection improved the FWHM from 75 to 40
milliarcseconds, and improved the Strehl ratio from 0.06 to 0.15,
demonstrating that lucky imaging can greatly improve the imaging
performance of an adaptive optics system at shorter wavelengths.
Investigation of the decorrelation time for the PALMAO data set suggests
the lucky camera may need to run at or above the operating frequency of
the AO system to achieve best performance. There is some discussion of
the future potential of such systems in chapter 5 , and methods for
modelling and predicting their performance are covered in chapter 6 .

### 1.5 New hardware: The quad-CCD LuckyCam

A large portion of this dissertation is based upon data taken with a new
design of lucky imaging camera, recorded during a July 2009 observing
run at the Nordic Optical Telescope, at the Observatorio del Roque de
los Muchachos, La Palma (illustrated in Figure 1.5 ). The 8 night
observing run was undertaken by Craig Mackay, David King and myself,
with remote software support from Frank Suess.

One of the aims of the trip was to thoroughly test the new camera, which
utilises 4 synchronised EMCCDs running in parallel. Each EMCCD is a
@xmath pixels format, resulting in a total mosaic of @xmath pixels, with
a frame rate of @xmath Hz. The main reason for adding this complexity is
to achieve a wider field of view whilst retaining good sampling of the
image, although there are secondary benefits such as improved dynamic
range and enabling the use of multiple filters simultaneously (by using
different readout configurations and different filters for each
detector, respectively). The layout of the camera for the July 2009 run
is presented in figures 1.6 and 1.5 , while the detector properties are
detailed in table 2.1 . The data acquisition and analysis requirements
for the camera are considerable, and the software and techniques I
developed to deal with the data are covered in chapter 7 .

### 1.6 Statement of thesis

The proposition of this thesis is twofold; firstly that lucky imaging
hardware and data reduction techniques are now sufficiently mature that
it has become a viable tool for a wide range of astronomical science
applications that benefit from high resolution imaging at visible
wavelengths. Secondly, coupled with adaptive optics technology it has
the potential for extremely high resolution imaging that will open up
entirely new areas of investigation.

Lucky imaging as a generally applicable tool for astronomy is desirable,
because it provides information that is difficult or impossible to
obtain with other ground based systems (table 1.2 ), and will soon be
unobtainable from space, too. The Hubble Space Telescope, now producing
excellent images with the latest set of electronics, will not receive
any further servicing missions now the shuttle fleet has retired, and
will almost certainly be decommissioned in the next 5-10 years. The
James Webb Space Telescope, if it flies, will produce images of inferior
resolution in the visible. Adaptive optics technology is advancing, but
correction in the visible is still poor even with the very latest
cutting-edge systems, and only works over a small field of view. Lucky
imaging provides a cheap, viable alternative for obtaining high
resolution images in the visible to complement the high resolution
infra-red imaging that is becoming more widespread.

When I began my PhD project, lucky imaging was a proven concept, but
limited in scope of application. The strenuous requirements for high
frame rates necessitated relatively small fields of view, and
challenging detector noise characteristics and non-uniformities made
imaging of very faint targets difficult. As a result the technique had
been successfully applied to close binaries of similar magnitude, but
not much else.

A large portion of my research has focused upon overcoming these issues,
and demonstrating the potential applications made possible as a result.
The pace of hardware development has been rapid, and the lucky imaging
camera deployed in 2009 ran with a mosaic field of view of approximately
@xmath pixels, resulting in a factor of 16 increase in critically
sampled field of view, and correspondingly in data rates, compared to
the @xmath pixel squared system described in Law ( 2007 ) . The larger
field of view greatly improves both the number of observable lucky
imaging targets, and the efficiency as a potential survey tool. The
calibration and reduction algorithms have also been substantially
developed and refined, such that when coupled with reduction techniques
developed specifically for EMCCDs, imaging of very faint targets is now
a proven possibility. When coupled with adaptive optics, the
unprecedented resolution makes possible entirely new scientific
investigations.

### 1.7 Chapter summaries

The rest of this thesis is structured as follows:

Chapter 2 details the models and calibration techniques required to get
the best results from electron multiplying CCDs. Chapter 3 then uses
those models to examine thresholding schemes to improve signal to noise
ratio at low light levels. Implementation and application of the
thresholding schemes to real data is covered.

Chapter 4 reviews some basic elements of sampling and image processing
theory, and presents improvements to the lucky imaging frame
registration procedure, which enables more effective use of faint guide
stars. An analytical model to predict the Strehl ratio in reduced lucky
images is proposed.

Chapter 5 covers the scientific applications of lucky imaging. An image
analysis procedure for stellar binarity surveys is presented, along with
new faint companion detections to planetary transit host stars, which
significantly expand the range of contrast ratios in binary stars
resolved with lucky imaging. A case is made for lucky imaging as a
generally applicable high resolution imaging tool that may often provide
an adequate substitute for Hubble Space Telescope data. Finally, a
possible application of lucky-imaging-enhanced adaptive optics is
discussed.

Chapter 6 reviews the theory of Monte Carlo simulations for modelling
adaptive optics and lucky imaging systems. Preliminary performance
investigations of a hybrid lucky imaging adaptive optics system are
presented.

Finally, chapter 7 describes the software and techniques I developed
during my PhD for dealing with the large datasets and novel data
reduction problems that lucky imaging presents.

## Chapter 2 Calibration of electron multiplying CCDs

“The only uniform CCD is a dead CCD” (Mackay 1986 ) . Due to
imperfections in manufacturing causing slightly different pixel areas,
readout electronics varying their characteristics during a frame read,
and so on, all CCDs have at least slight non-uniformities in their pixel
response to incident light. To avoid introducing errors in scientific
data it is important to calibrate and correct for these. The electron
multiplying CCDs used in the Cambridge lucky imaging camera require some
specialized calibration routines. Some elements of these calibration
procedures were introduced in (Law 2007 ; see Section 2.3 below for
specifics) , however considerable effort has gone into developing more
accurate and comprehensive methods, so I present these in some detail.

As an aside, where possible I shall adopt the same notations in this
chapter as those of Tulloch and Dhillon ( 2011 ) , in the hopes of
achieving some amount of uniformity across the literature.

### 2.1 The physics of electron multiplying charge-coupled devices

Conventional charge couple devices or CCDs are widely used as detectors
for astronomy due to their linear response characteristics, high quantum
efficiency, wide dynamic range, and ever increasing pixel array sizes.
The reader is referred to Howell ( 2000 ) and Tulloch and Dhillon ( 2011
) for relevant reviews, but it will be useful to recall the fundamentals
of CCD and EMCCD operation here.

Photons incident upon a pixel element of a CCD may excite electrons from
the silicon substrate if the photon energy is of the same order as the
electron excitation band gap. The freed electrons (known as
photo-electrons due to their origin) are then held in place by a
potential well due to the voltage at electrodes within each pixel known
as gates. When the exposure time has elapsed, the accumulated charge
packets of electrons are shifted across the CCD by cycling the voltage
values of neighbouring pixel gates through multiple phases ¹ ¹ 1
Specifically, conventional CCDs use 3 voltage phases. EMCCDs use 2 or 4
phases during parallel charge transfer, and 3 phases during serial
transfer through the readout register. (a process known as clocking).
Eventually all electron packets are passed through a component called
the ‘charge amplifier’, which converts the charge into a voltage.
Finally the voltage is passed to an analogue-to-digital unit (ADU) for
digitization, and the resulting integer value is recorded.

The conversion of the photo-electrons to voltage by the charge amplifier
introduces additive noise (“readout noise”), such that the inferred
pixel values will be spread in a Gaussian distribution about the true
value representing the photo-electrons captured. As a result, even at
slow readout speeds (tens of kHz pixel rates) typically a CCD
experiences readout noise of RMS equivalent to a few photo-electrons.
Lucky imaging requires frame rates of 20Hz or higher, resulting in pixel
readout rates greater than 20MHz for a large CCD. At these rates a
conventional CCD may experience read noise of tens or hundreds of
electrons (e.g. Tubbs ( 2003 ) cites readout noise of 50-60 electrons at
5.5MHz pixel rates). This level of noise makes it impossible to perform
lucky imaging on all but the brightest stars with any useful level of
signal to noise when utilizing conventional CCDs.

Fortunately, electron multiplying CCDs (EMCCDs, also known as
low-light-level or LLLCCDs) (Jerram et al. 2001 , Mackay et al. 2001 )
offer a solution. In an EMCCD an additional multi-stage “serial
register” is inserted into the circuit prior to the readout components.
This is a series of gates similar to those used for the pixel grid of a
conventional CCD, but with one of the three phases operating at much
higher voltage than would usually be applied, typically around 40v
compared with the 12v applied for conventional parallel transfer (Mackay
et al. 2010 ) . The large electric fields generated accelerate the
electrons sufficiently to give a significant probability of impact
ionisation at each stage — effectively releasing an extra electron and
so multiplying the charge in a stochastic manner. The probability of
ionisation at each stage is small, but using many stages in series it is
possible to achieve a high degree of gain in the average signal level,
albeit in a stochastic manner. The mean gain may be altered by making
small adjustments to the voltage, and is usually set to many times the
average readout noise variation. As a result, the previously
insurmountable read noise now equates to a mere tenth or twentieth of a
photo-electron signal, with electron multiplication factors of one or
two thousand.

### 2.2 Basics of CCD calibration

When calibrating conventional CCDs it is customary to take a handful of
calibration frames, some with shutters closed to calibrate internal
detector effects (detailed below), and some ‘flat fields’ with uniform
illumination to calibrate the detector response to light. These are then
averaged and subtracted to infer the bias pedestal, dark current, and
flat field (pixel light response). All these quantities may vary
somewhat from pixel to pixel, so they are recorded as pixel maps and
used for data reduction as follows (Howell 2000 ) :

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

Here the bias map records variation in the ‘bias pedestal’, an offset
introduced by the readout electronics which has the effect of ensuring
all raw pixel values are positive (and hence digitizable). The dark
current map corrects for internally generated signal, usually dominated
by thermal emissions within the CCD, which would otherwise be mistaken
for signal from photo-electrons. Bias and dark current maps are
calibrated by taking frames with different exposure lengths. The bias
calibration frames are recorded with short exposure times to minimize
dark current contributions. These short exposures can be analysed in
conjunction with dark current calibration frames of different exposure
times, to estimate the pixel value contributions from bias pedestal and
dark current.

The flat field is a normalising pixel map which corrects for different
levels of sensitivity to incident light - these may be due to intrinsic
variations in pixel sensitivity, and also extrinsic factors such as
partial obscuration by dust in the camera lens.

Note that there is some ambiguity in the nomenclature with regards to
calibration frames. It is common practice to take at least a handful of
each sort of calibration frame, and then combine them (usually via their
median pixel values) to create averaged frames for use in data
reduction. However, the raw and median frames are often referred to
interchangeably for brevity (the median of the bias frames simply being
called the ‘bias frame,’ for example). In this thesis I shall take care
to refer to the raw exposures as e.g. ‘bias frames,’ and the final
version for use in data reduction as the ‘bias map.’

Even for a conventional CCD, these calibrations are only valid for a
certain length of time. Varying operating temperatures may alter the
bias characteristics or thermal emission levels, camera lenses get
cleaned and then slowly accrue dust again. Typically such variations
occur at a significant level on timescales of hours or longer.

With EMCCDs, things are a bit more complex. Not only are there extra
variables which must be accounted for, some of these effects alter from
observation to observation, and potentially even from frame to frame.
Since the Cambridge group EMCCD camera is custom built, distinguishing
and calibrating the various different effects is a task that must be
carefully undertaken with each new camera configuration.

Both spatial and temporal variations of the CCD response must be
corrected for as far as possible on a frame by frame basis — i.e. prior
to any further data reduction — for several reasons. First, it enables
accurate determination of the gain since it stabilises the bias
pedestal, resulting in a cleaner histogram from which to estimate the EM
gain (cf. Section 2.5.2 ). Second, if pixel thresholding photon-counting
techniques are to be applied (cf. chapter 3 ), then the data must be
debiased before thresholding, otherwise a uniform threshold level will
result in non-uniform fractions of photon-events passing the threshold
across the detector. Third, the ‘drizzle’ algorithm (Figure 4.3 ) by
definition combines values from different pixels, by default assigning
them equal weights and resulting in some loss of information and
correlated noise - it is therefore desirable to ensure that the input
data truly are as uniform as possible before drizzling. Finally, the
lucky imaging data has potential for analysis of high-speed variation in
sources as explored in Section 5.2 , which obviously requires that
detector corrections are applied to every frame.

The rest of this section details the various phenomena which must be
considered for accurate EMCCD operation, and methodology to calibrate
and correct for any non-uniformities. Unless stated otherwise, all
results refer to full frame readout of the Cambridge EMCCD detector
(1072 by 1040 pixel e2v CCD201) used in the summer 2009 observing run at
a frame rate of @xmath 21Hz, i.e. 26MHz pixel rate.

### 2.3 Motivation and comparison with previous work on EMCCD
calibration

Law ( 2007 ) introduced some methods for calibration and reduction of
lucky imaging data taken with EMCCDs, addressing the need for
re-calibration of the column-to-column bias pattern with each run, and a
method for estimating the electron multiplication gain by fitting a
straight line to the pixel histogram. While reasonably effective, the
accuracy of the algorithms used was not investigated.

Further work in this area was needed for a number of reasons. Firstly,
the most recent camera uses a different model of EMCCD, with slightly
different noise and uniformity characteristics, which took some work to
determine and correct. Secondly, lucky imaging observations have
previously been largely focused upon binary detection. Only relative
photometry was performed, usually in a small region of the detector. The
larger format EMCCDs and the multi-EMCCD mosaic camera now in use allow
application of lucky imaging to a wider range of targets, but require
more precise calibration of detector noise and non-uniformities for
accurate relative photometry across the CCD. Furthermore, inter-CCD
relative photometry requires accurate determination of the EM gain for
each CCD. Finally, for faint objects we can further increase photometric
accuracy using pixel value thresholding techniques, but determination of
the optimal threshold level requires a detailed knowledge of the CCD
noise characteristics (see chapter 3 ).

In summary, to achieve better accuracy in the reduced images, I have:

-   Refined the bias determination algorithms, and tailored them to the
    current CCD model (Section 2.6.1 ).

-   Introduced calibration steps for internally generated signal
    (Section 2.6.3 ).

-   Tested and refined the EM gain calibration algorithms, and developed
    algorithms to further calibrate detector effects such as EM register
    clock induced charge (sections 2.6.3 , 2.5.2 ).

### 2.4 Probability distribution models for EMCCDs

Conventional CCDs are typically calibrated using a handful of frames. In
the case of the lucky imaging the stochastic signal multiplication of
the EMCCDs introduces an extra source of variance, and the fast frame
rate produces many frames in a short time, so it makes sense to analyse
many thousands of frames. This enables us to sensibly make use not only
of the average or median pixel values, but to go further and model the
pixel value distributions, varying model parameters to fit the full
histograms of pixel values obtained. First, we need a model.

A simple model of the EMCCD consists of two stages, the first
representing the stochastic EM gain process.

Before proceeding, we must deal with a notation issue. Tulloch and
Dhillon ( 2011 ) introduce the notation of using @xmath to denote the
“avalanche multiplication gain” or “EM gain,” the dimensionless average
multiplication factor between electrons input to the serial register,
and electrons output. They use the subscript to distinguish this from
the conventional “ system gain,” denoted @xmath , as measured with
conventional CCDs, where system gain refers to the number of
photoelectrons represented by 1 ADU, and so has units of photoelectrons
/ ADU. They also introduce the term @xmath to denote the actual ratio
between electrons at the input to the readout electronics, and ADU
output. The same convention is followed here.

For relatively high gain and low photon flux, a histogram of the number
of electrons output from the serial multiplication register, x, for n
photo-electrons input, over many samples, is well modelled by the
probability distribution (PDF):

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

which has a mean of @xmath and a variance of @xmath (Matsuo et al. 1985
, Basden et al. 2003 ) . At high photon flux levels the register output
probability distribution function is better modelled by a Gaussian of
the same mean and standard deviation. Figure 2.1 illustrates the
resulting PDF curves.

For a single photo-electron input, the EM register output distribution
simplifies to an exponential decay curve. At low light levels where the
chance of 2 or more photo-electrons being captured by a single pixel is
negligible, we may consider two distribution components, representing
pixels which have captured 0 or 1 photo-electrons. The second stage of
this model is simply addition of Gaussian readout noise. Under the
assumption that no photo-electrons captured results in zero charge at
readout, then we may model the EMCCD output distribution as follows. If
we denote the serial register output probability distribution for a
single photo-electron input as @xmath with:

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

then we would expect a histogram of pixel values to approximately
conform to the following distribution where @xmath is the Gaussian
distribution, @xmath is the standard deviation of the readout values,
@xmath is the bias pedestal, and @xmath represents convolution:

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

—that is, a sum of a Gaussian readout noise distribution representing
pixels which have captured no photo-electrons, and an exponential
distribution due to single photo-electron events, likewise convolved
with the Gaussian readout noise.

#### 2.4.1 Clock induced charge

A histogram obtained from real EMCCD data without incident light, but
with a small signal level due to internally generated signal (described
in Section 2.6.3 ), is displayed in Figure 2.2 . For most of the pixel
range the data is well fitted by the model of equation 2.4 , the
exception being around the knee of the curve where the data lies above
the model. This excess of low pixel values is due to an effect not
included in our simple model — clock induced charge. Clock induced
charge (CIC) is an important source of noise, consisting of internally
generated electrons released by clock transitions during the CCD readout
process.

We may consider CIC in two categories. I shall refer to stray electrons
freed during the transfer of charge across the CCD prior to the EM
register (see Figure 2.3 ) as “transfer CIC.” Transfer CIC events are
indistinguishable from photo-electron events, since they originate
before the EM gain register and so experience the same stochastic gain
process. The signal due to transfer CIC should be calibrated using bias
frames; and subtracted from on-sky data if a true estimate of the sky
background is required. CIC generated during transfer through the EM
register will on average experience a lower gain, since it passes
through fewer amplification stages. I shall refer to these events as
CICIR (CIC in register). These account for the poorly fitted region of
histogram in Figure 2.2 . We may model the CICIR distribution by
assuming an equal probability of a CIC event occurring at any stage in
the EM register. Then the distribution will be a superposition of many
different exponential decay curves, each representing single input
electrons passing through different numbers of amplification stages, and
therefore undergoing a different mean avalanche gain. The mean signal
contribution from one of these events will be a factor of @xmath smaller
than that of a photo-electron (Tulloch and Dhillon 2011 ) . Figure 2.4
illustrates the relative contribution of the CICIR component needed to
accurately model the EMCCD output data.

### 2.5 Histogram analysis algorithms

#### 2.5.1 Bias determination at low pixel counts for the purpose of
single frame bias estimation

When reducing EMCCD data, we often need a way to estimate the bias
pedestal from a fairly small pixel events sample, perhaps a thousand
recorded pixels. ² ² 2 For example, when using a faintly illuminated
region of an on-sky dataset to track bias pedestal drift. We desire such
an algorithm to be both extremely robust to noisy data, and also
computationally efficient since it must be run at least once per frame
of data to deal with bias drift (described in Section 2.6.2 ). A naive
approach is to simply use the mode of the histogram, however when
applied to raw data composed of integer ADU values this only estimates
the bias offset to the nearest integer, and is quite susceptible to
noise in the histogram, which may cause the bias estimate to “jump
around,” varying rapidly and unrealistically. Law ( 2007 ) employed the
tenth percentile as a metric to track bias drift which does not suffer
from this noise induced variation, however not only is the sort
computation required moderately time consuming, it is a biased estimator
at best, consistently underestimating the bias pedestal with an offset
that varies depending on the distribution of the readout noise.

After some experimentation with fitting routines, I settled on a
thresholded centroid algorithm as an efficient, robust and accurate
estimator of bias pedestal for tracking bias drift. The weighted mean of
all histogram bin central values with bin counts above 80% of the
maximum count is used, and this proves very effective.

#### 2.5.2 Electron multiplication gain

The Cambridge lucky imaging camera incorporates controls for on-the-fly
adjustment of the EM serial register voltage, and hence control over the
EM gain. This is useful as it allows the user to make adjustments based
on the current field of view, avoiding saturation of brighter sources
whilst maintaining the best signal to noise allowed by the dynamic range
for faint objects imaged on the same detector.

One key advantage of using a synchronised multi-CCD mosaic camera is
that the gain may be controlled independently across the CCDs. Careful
positioning of the mosaic field of view often makes it possible to
observe a bright guide star on one CCD at low EM gain, while observing
faint sources nearby at much higher EM gain with a second CCD —
effectively giving extremely wide dynamic range.

The cost of this flexibility is that the gain becomes an extra variable
to be carefully calibrated. If we wish to perform inter-CCD relative
photometry across a mosaic field of view, then the gain setting of each
individual CCD must be accurately measured so that we may normalise the
images prior to comparison.

As described in (Law 2007 ; pp. 43-46) , the EM gain is most easily
calibrated using a histogram of pixel events from a low flux area of a
field of view. The alternative method is to switch the EM gain off for
comparison using calibrator stars, but this is time consuming and
largely unnecessary. Law used a fit to the straight line region (on a
log plot) of the exponential decay curve of the serial register output
histogram for a single input photo-electron (fig. 2.1 ), with the
fitting region determined either manually or based on goodness-of-fit.

To determine the EM gain and other detector characteristics from the
pixel histogram, I developed a two stage, fully-automated fitting
routine. In the first stage, the bias pedestal, EM gain, and light level
are estimated by fitting models to the histogram sections highlighted in
Figure 2.2 (the light level is estimated by comparing the relative
integrated areas under the Gaussian readout hump and the tail of
photo-electron events). This initial fit takes less than 0.01 seconds to
compute. Optionally, a fit is then made to the whole histogram using a
fully featured distribution model with parameters corresponding to bias
pedestal, readout noise, EM gain, light level, and serial register CIC
event frequency. The full fit takes around 1.5 seconds to compute.

I undertook some investigations into the algorithm performance, to
verify that the estimation algorithm did not introduce any systematic
errors, and also determine how the accuracy improved as the histogram
converges to the underlying distribution with increasing number of
recorded pixel-events.

I tested the code against simulated data generated with the Monte-Carlo
methods described in Section 6.2 . The results are plotted in Figure 2.5
. While the code produced consistent results, initially a systematic
over-estimation of the EM gain was evident, especially at higher light
levels. This corresponds to pixel events where 2 photo-electrons are
captured, which raises the number of pixel events in the tail of the
histogram. Adding an extra component to the model which accounts for the
2 photon pixel events largely corrects this error. Alternatively, if the
high speed single stage fitting procedure is preferred, it appears that
the gain estimate can be corrected to give reasonably good accuracy
simply by computing the relative frequency of 1 and 2 photon pixel
events — for example, if the light level is 0.2 photons per pixel per
frame, then the EM gain will be overestimated by a factor of:

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

The simulations suggest at least @xmath recorded pixels are required to
give a reasonable estimate of the gain, with larger samples giving
better accuracy, though this is partially dependent on the light level.
This is of interest for reasonably crowded fields where much of the
frame has light levels too high for EM gain estimation, suggesting that
if a background region of 100 by 100 pixels may be selected, then 1000
or more frames of data will give a reasonable estimate.

### 2.6 Calibrating the Cambridge e2v CCD201

While the previous sections describe general techniques applicable to
EMCCD data, the rest of this chapter details the specific
characteristics of the detectors used in the July 2009 observing run at
the Nordic Optical Telescope, and the techniques used to calibrate these
detectors as accurately as possible. A summary of the detector
properties is provided in table 2.1 .

#### 2.6.1 Spatial gradient in bias pedestal

Conventionally, the bias pedestal in CCD images is estimated using a
handful of bias frames. However, due to the high sensitivity of the
EMCCD to low levels of internally generated signal such as CIC, the bias
pedestal must instead be estimated using the histogram analysis
techniques described in Section 2.5.1 . Whilst the bias estimation
algorithm is insensitive to modest illumination levels, the best
estimate will be made from a series of exposures taken with the camera
shutter closed.

Creating a full two dimensional bias map from a series of bias frames
requires recording a histogram for every pixel, for several thousand
frames. I implemented routines to perform this analysis. Unfortunately,
the bias map varies from run to run, most likely varying as the EM gain
is adjusted (Figure 2.7 ). As a result, a full 2d bias calibration can
only be achieved by recording bias frames for every EM gain setting to
be used. Alternatively, the variation across the bias map may be largely
broken up into horizontal and vertical components (Figure 2.6 ). While
the horizontal (column to column) gradient varies with EM gain setting,
the vertical (row to row) component appears stable. This allows us to
gather column histograms for each on-sky observation, which in most
fields will largely contain pixels at low light levels, meaning we can
determine the horizontal bias gradient component for each observation,
then add it to the vertical component calibrated using a single set of
closed shutter bias frames. ³ ³ 3 Note: when observing say, globular
cluster fields, saturation of the column histograms by photon-events can
and does become an issue. Signal from bright stars will dominate over
bias pedestal variation, so if accurate photometry and PSF fitting is
required then the observer should revert to taking closed shutter bias
frames between observations periodically. It may be possible to use
pre-set levels of EM gain and build up a library of bias maps, if the
column to column bias pattern is not strongly dependent on other factors
such as temperature.

#### 2.6.2 Bias pedestal drift

The bias pedestal also varies temporally, i.e. on a frame by frame basis
— generally this is known as bias drift. The drift range is typically of
the same order as the readout noise (fig. 2.8 ), and so it is desirable
to correct for this. Fortunately the spatial gradient appears stable on
these timescales, with drift affecting the bias pedestal uniformly
across the frame. Therefore it is possible to correct for the bias drift
by analysing pixel histograms gathered from each frame, as long as a
faintly illuminated region may be selected.

#### 2.6.3 Internally generated signal levels

When processing bias frames, a significant and non-uniform signal is
evident after subtraction of the (histogram calibrated) bias map,
illustrated in Figure 2.9 . There are several possible sources of this
residual signal:

-   Dark current, electrons randomly generated due to thermal emissions

-   Clock induced charge as described in Section 2.4.1 .

-   Light leakage through the camera baffles.

-   Amplifier glow as described in Tulloch ( 2004 ) .

Amplifier glow seems unlikely, as the resultant signal would be
localised in one corner of the detector, near to the amplifier component
location. Uniform light leakage would not be expected at the levels
observed, leaving dark current and CIC effects.

While CICIR levels may be estimated by histogram fitting, the only way
to distinguish transfer CIC contributions from dark current is to obtain
closed shutter calibration frames with different exposure times — dark
current signal levels should increase with exposure time, while CIC
levels remain constant. Hence the relative contributions may be
inferred. Closed shutter datasets of differing exposure time were not
available for calibrating the summer 2009 data. Interestingly, the
internally generated signal level increases towards the bottom of the
frame, suggesting some common causation factor along with the rise in
bias pedestal. At any rate, the source of the internal signal is
unimportant with regards to general data reduction as the resulting data
reduction process is the same whatever the source — we simply wish to
subtract any signal not due to photons from the sky.

#### 2.6.4 Clock induced charge levels

Transfer CIC is indistinguishable from photo-electron signal, and so is
calibrated as part of the internally generated signal (Section 2.6.3 ).
However, using the multi-component model illustrated in Figure 2.4 ,
CICIR levels can be estimated. This is important not only for
characterising and fine tuning EMCCDs, but also when determining
thresholds for low light levels (cf. chapter 3 ). Figure 2.10
illustrates just such a fit to real data.

#### 2.6.5 Flat fielding and gain uniformity

Careful flat fielding of CCDs is essential for accurate photometry. Law
( 2007 ) reported variations of up to 10% in EM gain across frames from
the 2005 camera. To work around this variation the recommended method
was to perform calibration with histograms from the same columns as any
photometry target objects. To determine whether photometry across many
objects in wide fields of view is feasible I investigated if this
variation was still present in the 2009 camera.

Investigation of inter-pixel gain variations is difficult, since the
estimates of EM gain are inaccurate when using small pixel samples (see
Figure 2.5 ). Inspection of the mean internal signal levels in closed
shutter calibration frames after subtraction of the bias map suggested
there may be some horizontal (column to column) gradient in the EM gain.
I was able to implement a per-column estimation of EM gain levels by
analysing the pixel histograms. Figure 2.11 shows an example plot of
estimated EM gain across the detector. The gain appears appears uniform
to around the 5% level over most of the detector (excepting some spikes
due to bad columns which are rejected in reduced images), although there
was greater deviation in columns 1 through 200. As a result, reasonably
accurate photometry can only be extracted from the region right of
column 200 in the 2009 datasets. If such variation remains in future
detector configurations it should be possible to incorporate a
normalizing ‘gain map’ to correct for such effects, but calibration
requires long series of closed shutter (or very weakly illuminated)
calibration frames. Such long period calibration datasets were not
available for the 2009 data.

Flat fielding for other non-uniformities such as relative pixel area and
lens obscuration is a relatively simple process of taking twilight sky
flats, as with a conventional CCD, and provision for such flat fields is
made in the reduction pipeline. The variations in both EM gain and pixel
sensitivity can be disentangled in two ways. Firstly, the EM gain can be
switched off and the conventional readout mode utilised when taking flat
fields, to calibrate the pixel sensitivity alone. Alternatively the EM
gain can be estimated first from sky flats using histogram analysis
methods, then taken into account when estimating pixel sensitivity.

#### 2.6.6 Bad pixels and pixel weighting

It is common for CCDs to exhibit a few ‘bad pixels,’ which consistently
produce high or low values without much response to actual light levels.
In the 2009 camera, each CCD contained at least a few columns of such
pixels, easily identifiable by eye (e.g. the bright column in Figure 2.9
). The locations of such columns are stored in a configuration file for
the pipeline, which then zeroes the pixels in average images, and also
zeroes their allocated weight for the drizzle algorithm (see chapter 7
).

### 2.7 Results

Figure 2.12 illustrates the improvement in background uniformity due to
accurate calibration. Bias pedestal variation typically has a range of
levels equivalent to a signal level of 0.1 photo-electrons, evident in
2.12(a) . Dark current affects the bottom of the frame worst, where the
additional signal reaches 0.2 photo-electrons, evident in 2.12(b) . Bad
pixel columns are also noticeable (their pixel values have been zeroed).
Figure 2.12(c) shows a fully calibrated, drizzled image. The RMS
background variation over most of the image is of the order 0.005
photo-electrons, though some slight non-uniformities are still present
near the edges at the level of 0.02 photo-electrons.

## Chapter 3 Lucky imaging of faint sources: Thresholding techniques

The stochastic nature of the electron multiplication gain in EMCCDs
introduces an extra source of variance, so that compared to the ideal
case of purely photon shot noise an EMCCD at high light levels will
experience signal variation larger by a factor of @xmath - this is
termed the “excess noise factor” (Basden et al. 2003 ) . At low light
levels (where the mean photon flux per pixel per frame is much less than
1) we can be reasonably confident that any given pixel only receives 0
or 1 photo-electrons per exposure. In this case we can exploit the fact
that, at high electron multiplication gain, most photon events result in
ADU values many times the readout noise RMS above the background, and
reduce the excess noise factor by employing a pixel thresholding scheme.

The concepts behind this thresholding approach rely heavily upon the
models developed in chapter 2 , likewise the details of implementation
rely upon careful calibration, and so the chapters are closely linked.
Where the text would otherwise be unclear, I shall refer to EMCCD
observations which do not employ thresholding as ‘linear mode’
observations.

### 3.1 Summary and comparison with literature

Basden et al. ( 2003 ) and Lantz et al. ( 2008 ) have explored the
possibility of applying thresholding schemes across a range of light
levels via either multiple thresholds or Bayesian estimators. However
there are a number of practical issues with these schemes.

Firstly, the scheme of Basden et al. does not account for CIC effects,
which would need to be taken into account and thresholds changed
accordingly if a similar scheme were to be applied. The scheme of Lantz
et al. ( 2008 ) requires the mean light level on every pixel to be
constant so that an optimum Bayesian estimator can be chosen. In lucky
imaging this is not the case as the atmospheric tip-tilt shifts the
sources around the CCD from frame to frame. The suggested work-around is
thresholding on frames which have already been recentred using the guide
star. This is not entirely infeasible but still runs into the problems
of guide star registration error and anisoplanatic effects — so it seems
likely that the technique could only be practically applied close to a
bright guide star, and would require substantial computational effort.

Accordingly, I have focused upon developing and implementing photon
counting methods which employ a single threshold to process data at low
light levels. Thresholding is at any rate most needed at low light
levels where the detector noise would otherwise dominate over the photon
shot and stochastic multiplication noise.

Photon counting is very simple in concept — if a pixel has a value above
a threshold it is recorded as a 1, below the threshold it is recorded as
0 — but obtaining the best possible signal to noise and maintaining a
linear signal response requires some care. This first aim boils down to
choosing the optimum threshold level, given a set of known detector
characteristics and a particular light level.

The question of optimum threshold choice for photon counting was
partially treated in section 2 of Lantz et al. ( 2008 ) , but in that
paper the authors focused on optimizing the exposure time, and hence
light level, alongside the threshold to achieve a minimum “misfit,” i.e.
to ensure absolute photon count estimations were as close as possible to
their true values.

A more general approach is to assume that true photon counts can be
estimated from the thresholded values, and focus on achieving optimum
signal to noise ratio (SNR) in the thresholded estimate. To do this we
require a model for estimating the SNR, which is non-trivial given the
stochastic detector behaviour.

I presented a basic SNR equation for photon-counting with EMCCDS at low
light levels in Staley and Mackay ( 2010 ) . This result was
independently reproduced in Tulloch and Dhillon ( 2011 ) , but with more
care taken over correction for signal loss due to photon coincidence (2
or more photons in one pixel), including calculation of the correction
factor required to correct for coincidence losses. In the interests of
consistency I shall adopt the notations of Tulloch and Dhillon herein.

In lucky imaging data there is always a range of light levels in a field
of view, and a truly optimal thresholding scheme would require a
variation of threshold for each pixel, depending upon the light level of
that pixel. However, as previously mentioned, atmospheric effects make
it difficult to estimate the mean light level on any given pixel a
priori; additionally multiple thresholds would require a complex data
reduction process. As a simple and practical alternative I explore the
effectiveness of applying a single threshold over a range of light
levels.

Finally I present results of applying the thresholding techniques to
real lucky imaging data. To my knowledge results from application of the
techniques to real data have not been published elsewhere (except in
Staley and Mackay 2010 ) .

### 3.2 Signal to noise ratio for conventional and electron multiplying
CCDs

Given an ideal detector, the only noise would be the photon-shot noise
of the Poisson distribution, and so the SNR of a single pixel in an
ideal detector is simply:

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

where M is the mean signal in photo-electrons per pixel from the target,
and K is the background sky flux, in the same units.

Of course, detectors are imperfect. If we take into account the various
sources of noise described in chapter 2 , then in a conventional CCD
with read noise of standard deviation @xmath , dark current of mean
signal @xmath , transfer CIC ¹ ¹ 1 See Section 2.4.1 for an explanation
of transfer CIC and CICIR. of mean signal @xmath and CICIR of mean
signal @xmath ² ² 2 We denote the frequency with which CICIR events
occur as @xmath , but since they undergo a lower mean gain than
photo-electron events their signal contribution is @xmath . See Section
2.4.1 for details. then the SNR equation of a conventional CCD becomes
the rather unwieldy:

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

Typically, in a conventional CCD run at slow frame rates the CIC
contributions are negligible compared to the other factors. With a dark
sky and a well cooled CCD, the limiting noise contribution becomes the
readout noise, @xmath , which is typically of the order of a few
photo-electrons and will increase when run at high frame rates. From
this we can see that at signal levels any lower than 10 photo-electrons
per pixel per frame the readout noise will dominate over the signal.

For an EMCCD, the output for a fixed input of @xmath photo-electrons
(i.e. without photon-shot noise) to the electron multiplication serial
register obeys the distribution described in Section 2.4 , with mean
@xmath and variance @xmath . This stochastic gain variance is added in
quadrature to the shot noise, producing a factor of 2 in the SNR
denominator. If we assume the dark current @xmath is negligible (it is
at any rate indistinguishable from transfer CIC in effect), then the SNR
equation for EMCCD observations taken in linear mode with readout noise
@xmath becomes, after dividing through by @xmath :

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

From this equation we can deduce a couple of points. First, at high
light levels the photon count dominates over the other terms and
performance is a factor of @xmath worse than a conventional CCD. Second,
at low light levels the dominant noise term will depend largely on the
relative magnitudes of the readout noise and the CIC event frequencies.
However, as is clear in Figure 2.4 , the readout, CICIR and signal
components have considerably different probability distributions, and so
employing a thresholding scheme helps to distinguish between them.

### 3.3 Thresholded signal to noise equation

Since thresholding reduces the digitized signal to a binary one, we may
assume that the frequency with which pixel values cross the threshold is
determined by a Poisson distribution, with the mean frequency determined
by the fraction of the original pixel value distributions which lie
above the threshold. If we denote the CICIR event frequency @xmath and
the threshold ‘pass fractions’ — that is, the fraction of the component
PDFs (c.f. Figure 2.4 ) which lie above a given threshold — as @xmath ,
@xmath , and @xmath for the photo-electron, CICIR and readout
distributions respectively, then the thresholded signal to noise
equation becomes:

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

Generally the optimum threshold is several times the readout noise
standard deviation, and so @xmath . Neglecting this term, we may also
account for the loss of signal induced by coincidence losses (Tulloch
and Dhillon 2011 ) , which is of order @xmath ; the equation then
becomes:

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

### 3.4 Choosing the best detector mode for an observation

Having derived SNR equations for data from conventional CCDs, linear
mode EMCCDs, and thresholded EMCCDs, we may consider their relative
merits across a range of light levels. A comparative plot is shown in
Figure 3.1 . We may summarise this comparison by dividing it into three
regimes. At high light levels (greater than @xmath photo-electrons per
pixel per frame) the excess noise factor introduced by the stochastic
electron multiplication gain means that conventional mode CCD
observations still give best signal to noise. At light levels between
about 1 and 10 photo-electrons per pixel per frame, linear mode EMCCD
observations win out due to the effectively suppressed read noise, while
photon-counting is impossible due to coincidence losses. At still lower
light levels photon-counting using thresholding methods gives the best
result. In particular, at light levels of around 0.1 photo-electrons per
pixel per frame, photon-counting EMCCD observations are getting close to
achieving the maximum theoretically possible SNR (ignoring quantum
efficiency effects), but are currently limited by noise from clock
induced charge. See Tulloch and Dhillon ( 2011 ) for further detail.

### 3.5 Threshold optimization

Considering equation 3.5 , it is clear that the thresholded signal to
noise ratio will be partially determined by the choice of threshold,
which in turn determines the ‘pass fractions’ of the various
contributing signals. I now consider the matter of choosing an optimal
threshold to maximize SNR.

Given a chosen threshold level, the pass fractions for the
photo-electron event pixel distribution and readout noise distributions
may be estimated with simple analytical expressions; however, the pass
fraction for the CICIR is calculated numerically from the rather complex
distribution model. In practice the photo-electron pass fraction is
estimated numerically too, since this makes it trivial to account for
the contribution from pixels which receive 2 photo-electrons. Once this
is achieved, it is simple to implement a numerical optimization routine.

Conceptually, the trade off is between higher noise from readout and
CICIR at low threshold levels, and higher relative shot noise levels
when the signal pass fraction is small at high threshold levels. This
trade off evidently depends upon the light level. However, since we
employ thresholding techniques to improve the light level estimate
accuracy, we do not expect to have an accurate estimate a priori. As
such, it is interesting to investigate the effect of using a fixed
threshold across a range of light levels.

Fortunately, as long as the threshold is chosen to exclude almost all
readout noise and the highest levels of CICIR, the SNR equation is
fairly insensitive to further variation of the threshold. As a result, a
fixed threshold may be used effectively across a reasonably wide range
of illumination levels, as illustrated in Figure 3.2 . Figure 3.3
displays the corresponding improvement predicted in SNR at low light
levels when using a fixed threshold.

### 3.6 Combining thresholded and linear data

To linearise the photometry of thresholded images, and allow for direct
comparison with linear mode images, we must correct for the effects of
the thresholding process.

To calculate the correction factor for loss of multiple photo-electron
pixel events, we consider the mean number of pixels with photo-electron
counts of 1 or greater, @xmath , for a mean illumination level of @xmath
photo-electrons. From the Poisson distribution we have

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

therefore

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

We must also consider the reduction in signal level due to the threshold
level, and the contribution of CICIR events, which will be different for
thresholded and linear reductions. If we consider pixel values of @xmath
and @xmath in the reduced linear and thresholded images respectively,
then the signal estimates may then be calculated as:

  -- -------- -- -------
     @xmath      (3.8)
  -- -------- -- -------

and

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

Once these corrections have been applied, it is possible to combine
linear mode and thresholded images created from the same dataset. The
linear mode image may be used to estimate the light level of any given
pixel, and any pixels with light levels below a chosen threshold, e.g.
0.25 photons per frame, are replaced with the pixel values from the
thresholded image. Such an approach achieves high dynamic range coupled
with the SNR improvements of thresholding, but requires extra care when
undertaking further analysis. Since the noise level estimates will be
different for thresholded and linear images, if they are combined a
separate “pixel flag” image must be maintained so that any analysis
algorithms can employ the correct noise estimator on a per pixel basis.

### 3.7 Results of applying thresholding techniques to real data

To assess the impact of applying photon counting techniques to real
data, I undertook an analysis of a sample dataset — 1 hour of short
exposures of the radio galaxy 3C405. These data were taken under
excellent seeing conditions of @xmath 0.4 arcseconds seeing FWHM, during
the July 2009 Nordic Optical Telescope observing run detailed in
sections 1.5 and 2.6 . The guide star is about 20 arcseconds from the
centre of the field of view and was imaged on a different detector in
the mosaic field of view to 3C405. This meant it was possible to use a
high EM gain setting for observing 3C405, without it resulting in
detector saturation. FWHM at this radius from the guide star was
estimated at @xmath 0.25 arcseconds. ³ ³ 3 See Section 5.3.2 for details
of how the FWHM varies across the field. This observation was chosen as
it is an unusually long timespan dataset, and as a result the faint
limit is largely due to detector noise.

The dataset was reduced in the usual manner and final images were
produced using a 50 percent selection of the dataset, applying both the
standard linear mode reduction and photon counting thresholding
technique. I then compared the SNR for both a resolved region of
extended emission in the galaxy profile (solid line box in Figure 3.4 )
and for 3 faint point sources (solid-line circles in the figure). For
the region of extended emission we estimated background level and
variance using a remote region which was judged to only contain sky
pixels (dashed box in the figure), while for the point sources we used
an annulus around the photometric aperture (dashed circles denote outer
radius, inner radius was set at photometric aperture boundary). The
signal to noise ratio was then estimated using the formula:

  -- -------- -- --------
     @xmath      (3.10)
  -- -------- -- --------

where @xmath and @xmath are the mean value and variance of the relevant
background pixels, @xmath is the sum flux of the pixels in the
photometric aperture, and @xmath is the number of pixels in the
photometric aperture.

Numbering the stars 1 through 3 from left to right, the results are:

  Region          Analogue SNR   Photon Counting SNR   SNR Increase
  --------------- -------------- --------------------- --------------
  Galaxy region   239            391                   63.5%
  Star 1          4.55           4.28                  -6%
  Star 2          2.44           3.46                  42%
  Star 3          5.12           6.24                  22%

The decrease in SNR for star 1 is due to a relatively increased variance
in the annulus used to determine the local background — while not
immediately apparent in the image, inspection at high contrast shows
that star 1 is in a region of relatively steep decline of the faint
wings of the galaxy. As such the background variability is largely due
to real signal variation which would need to be modelled and subtracted.
Stars 2 and 3 also suffer from this, but to a lesser extent such that it
is not the dominant source of background variation. This is, of course,
a quick and crude comparative test - more optimal SNR ratios could be
achieved through PSF fitting. We note that the peak pixel in the region
of star 2 has a photon-flux per frame of only 0.005 above the sky flux
level - i.e one excess photon in 200 short exposures, on average.

Thresholding techniques are clearly useful at these low light levels,
and have potential to further the limits of lucky imaging, especially as
the detectors continue to improve and produce lower CIC levels.

## Chapter 4 Optimising and predicting the image formation process

In this chapter I consider the extrinsic variation in the short
exposures recorded for lucky imaging — variations in the point spread
function (PSF) caused by the fluctuations in atmospheric seeing
conditions due to the turbulent processes described in Section 1.1 — and
how we may deal with them. To place my work in a meaningful context I
first recall some salient points of sampling theory, and describe the
drizzle algorithm used to combine multiple exposures. I then summarise
the characteristics of the PSF in short exposures obtained at ground
based telescopes. With this grounding in the image formation processes,
the frame analysis and registration algorithms are compared and optimal
methods determined. Finally, error budgets are explored with the dual
aims of predicting image quality and enabling multiple guide star
frame-registration techniques.

### 4.1 Background — Sampling theory and image combination

When designing a camera for astronomy, pixel size is a crucial
parameter, due to the competing requirements of wide field of view, high
signal-to-noise ratio, and good sampling of the PSF. The relevant pixel
measurement in this discussion is subtended angular width, the width of
the section of sky from which light is focused upon one pixel.

A pixel of large angular width gathers more photons and so has a better
signal level. However, the Nyquist-Shannon theory (Shannon 1998 ) tells
us that to properly encode a signal we require a sampling rate of twice
the maximum frequency present. In the case of a plane wavefront entering
perfect telescope optics, the corresponding critical pixel angular width
is often taken to be @xmath , where @xmath is the wavelength and D the
telescope primary aperture diameter. However, there is a further
complexity in that the Nyquist-Shannon theory refers to instantaneous or
point sampling, whereas a pixel integrates a signal over a finite area.
This is most easily analysed using the formalism of an “effective PSF”
(Anderson and King 2000 ) ; the PSF resulting from the convolution of
the actual light intensity distribution (called the instrumental PSF)
with the pixel response function. We can then apply point sampling
theory to this effective PSF. This formalism highlights the fact that to
obtain the highest instrumental resolution we actually need sampling of
the PSF with pixels of angular width smaller than @xmath (fig. 4.1 ),
although realistically this would be impractical for most cases.

To obtain a higher sampling rate of a signal, we can do one of two
things — sample at smaller intervals, or if the signal is stable we may
sample multiple times at different phase offsets (fig. 4.2 ). In optical
astronomy these correspond to pixels subtending a smaller angle on sky,
or multiple pointings at sub-pixel angular offsets. The optimal choice
of pixel size will depend upon many factors such as desired angular
width of the field of view, readout noise and other detector
characteristics, data transfer rates, dynamic range and saturation
levels, and so on. As a result, it is often useful to observe a field of
view with multiple pointings.

If multiple pointings are used, then the individual exposures recorded
are usually combined to produce an average image, in order to reduce
data storage requirements, ease data analysis procedures, and produce
datasets that are easily inspected visually. We desire an image
combination algorithm which minimises further convolutions with the
pixel response function and resulting loss of resolution, while at the
same time providing methods for bad pixel data rejection. The drizzle
algorithm (Fruchter and Hook 2002 ) provides parameters that may be
varied towards one or the other of these aims, as illustrated in Figure
4.3 .

### 4.2 Background: Speckle patterns and speckle imaging

We can calculate the theoretical (instrumental) PSF for a telescope of
circular or annular aperture with relative ease. It is described by the
Airy function, known as the Airy disc, and has an angular width
proportional to the ratio of the observed wavelength and the aperture
size, @xmath . However, as all astronomers know, the PSF observed using
a conventional long exposure camera behind a medium or large
ground-based telescope (of say, a metre plus aperture size) is very
different to the Airy disc. Instead of a compact, sharply peaked profile
we observe a wide, shallow PSF known as the seeing disc, well modelled
using the Moffat function (Trujillo et al. 2001 ) .

The increased width of the seeing PSF causes loss of information. If we
maintain a pixel size suited to sampling of an Airy disc then the source
flux will be spread across many pixels and so the signal-to-noise ratio
will be much lower, due to readout noise. If we increase the angular
pixel width then we lose resolution due to convolution with the pixel
response function. Even in the ideal case of a well sampled PSF at high
signal level, the wider PSF blurs away much of the information present
at high spatial frequencies. This introduces degeneracy in signal from
multiple close sources, so that for example a binary or triple star
system may not be distinguished.

The reason for this loss of resolution is perturbations in the phase of
the incoming starlight, caused by the turbulent atmospheric processes
described in Section 1.1 . Due to the power law scaling of these
effects, the phase disturbance across a small aperture is too small to
cause significant deviation of the PSF from the Airy model. As the
telescope aperture size increases, for a fixed observation wavelength
the phase perturbations increase according to the power law until, at
phase disturbances of around 1 radian, the PSF becomes noticeably
distorted. For larger apertures still, the aperture phase may be
considered in terms of multiple coherent regions, within which the phase
difference is around 1 radian. The size of such a coherent region is
equivalent to the Fried coherence length, denoted @xmath (Fried 1965 ) ,
and is dependent upon the wavelength being observed according to the
formula:

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

where @xmath is the wavenumber, @xmath is the angle of the telescope
from the zenith, and @xmath is the “structure constant,” denoting the
strength of the turbulence optical effects at each altitude (Hardy 1998
) . It is worth noting that @xmath is dependent upon wavelength, varying
as @xmath . This is a useful quantity, since we may discuss severity of
turbulence for any given site and telescope in terms of the
dimensionless ratio @xmath , where @xmath is the diameter of the
telescope primary aperture. At values of @xmath the multiple coherent
phase regions typically result in an instantaneous PSF of multiple local
maxima, which we may consider as a superposition of distorted copies of
the Airy disc — these are known as “speckles” or collectively as speckle
patterns. The speckle patterns evolve as the atmospheric phase
disturbances change, and their long-term average light intensity
distribution is the seeing disc PSF observed in long exposure images
(see Figure 4.4 for an illustration). Note that the compact nature of
the individual speckles represents high frequency spatial resolution,
lost during the long exposure averaging process as the speckle patterns
evolve and shift position in the focal plane.

It has been recognised for some time that it is possible to retrieve
some of the high frequency spatial information present in speckle
patterns, through speckle interferometry methods (Labeyrie 1970 ) .
Exposures are recorded on a timescale short enough to give a reasonable
approximation to the instantaneous PSF (or colloquially “to freeze the
seeing”), and then analysed post-exposure to retrieve information
present at high resolution, e.g. parameters of binary star systems.

Lucky imaging is a different approach to analysis of similar datasets.
By selectively combining the short exposures we may exploit the
stochastic nature of the turbulence (as noted in Fried 1978 ) ,
combining data from moments when the RMS phase disturbance across the
telescope aperture is smaller than average. Ideally we select short
exposures in which the PSF closely resembles an Airy disc. However, even
under poorer seeing conditions, or when using all recorded short
exposures, the resolution is improved compared to the seeing disc — the
data reduction process preserves some of the high frequency spatial
resolution lost in a long exposure.

### 4.3 Frame registration overview

The speed at which the atmospheric phase disturbances and corresponding
speckle patterns evolve can be characterised using a defined
‘atmospheric coherence time’ (Scaddan and Walker 1978 , Lopez 1992 ) .
The relevant formulation for speckle imaging, which is likely most
relevant to lucky imaging, is (Roddier et al. 1982 , Aime et al. 1986 )
:

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

where @xmath is the mean velocity dispersion for the turbulent layers,
and is typically of the order 10ms (Lopez and Sarazin 1993 ) . To
preserve the high spatial resolution information present in the
instantaneous PSF, the length of the short exposures must be of the same
order as this atmospheric coherence time. As a result, to gain useful
information about the instantaneous PSF we must observe a reasonably
bright source, in order that a sufficient number of photons are detected
from that source within every atmospheric coherence time. The difference
between a naive and an optimal analysis algorithm applied to the short
exposure data will determine the faint limit, i.e. the faintest source
which the technique may be usefully applied to, and also how close we
get to the diffraction resolution potentially obtainable under
favourable observing conditions. Clearly, this is a crucial aspect of
the data analysis to get right.

For lucky imaging, frame registration and estimation of image quality
may be equated to the problem of locating and estimating the peak value
of the short exposure PSF. By aligning the images upon the point of
brightest intensity (i.e. the brightest speckle), these add coherently
to produce a narrow core, while secondary speckles add incoherently,
converging to an axisymmetric, wider PSF component of lower intensity,
or ‘halo,’ over many exposures. In the seeing regime of @xmath or less,
a small but significant portion of frames are reasonably close to the
diffraction-limited instrumental PSF (see table 1.1 ). If signal levels
are high enough then potentially the diffraction-limited frames may even
be aligned with sub-pixel precision, such that the atmospheric motion is
exploited in the manner of multiple pointings to improve PSF sampling.

Since the guide star registration is such a critical step in determining
the lucky imaging system performance, I decided to investigate the
registration algorithms to see if they could be improved further,
particularly with the aim of extending the guide-star faint limit.

### 4.4 Registration methods

The simplest method of estimating the PSF peak is simply to use the
brightest pixel, but this is a flawed method — even when considering
pixellation effects in the absence of noise, it only locates the PSF
peak to the nearest pixel, and results in underestimation of the PSF
peak value depending on sub-pixel location. Tubbs ( 2003 ) implemented a
Fourier resampling and filtering algorithm to overcome these problems,
but this is computationally expensive and introduces artefacts. Law (
2007 ) implemented a combination of bicubic resampling and a
cross-correlation algorithm, using the core of an Airy PSF as a
reference model. If we consider the short exposures closely resembling
an Airy PSF, cross correlation with an Airy reference is a reasonable
approximation to the maximum likelihood estimator of position, due to
the well known result that cross correlation is equivalent to the
maximum likelihood position estimator for a pixellated signal in the
presence of uniform noise (see e.g. the appendix of Gratadour et al. (
2005 ) for a derivation).

We can do even better, by dropping the false assumption of uniform
noise. As detailed in chapter 3 , the largest source of signal variance
at all but the lowest light levels is photon shot noise, increased by a
factor of @xmath due to the stochastic electron multiplication gain.
Hence, a more optimal reference image can be obtained by weighting using
this simplified noise estimate proportional to @xmath , where N is the
mean light level in photo-electrons at each pixel — or equivalently by
replacing each pixel value of the reference by its square root (referred
to hereafter as ‘normalising the reference’). Note that uniform scaling
of the reference has no effect on the estimation of best matching
position.

If sub pixel accuracy is desired, then we must also perform some kind of
interpolation. A simple and efficient method is to calculate the
cross-correlation at the original pixel scale, and then fit a parabola
in the X and Y directions about the maximum correlation value to
determine the correlation peak, as suggested in Poyneer ( 2003 ) .
However, this method is sub-optimal for a critically-sampled or
under-sampled image, since the signal is only ever compared to a
reference generated at a particular sub-pixel offset, artificially
lowering the cross correlation value in some instances. In a high
signal-to-noise regime (i.e. when using a bright guide star) it
therefore makes sense to either attempt cross-correlation with a
reference of varied sub-pixel shift (Gratadour et al. 2005 ) , or to
interpolate the signal data prior to cross-correlation at each
interpolated point.

#### 4.4.1 Implementation details

Since lucky imaging deals with many short exposures per second, and we
may wish to perform frame registration using multiple stars, it is
desirable that any frame registration routine be implemented so as to
achieve the best computational speed possible. To calculate the
cross-correlation we have a choice of computational methods. We can
either calculate the convolution at each pixel index @xmath in the
spatial domain:

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

where @xmath are defined over a small range of negative and positive
offsets about the central pixel of the reference image; or we may
multiply their discrete Fourier transforms (DFT) pixel-wise and then
transform back:

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

Both methods have pros and cons.

In computational terms, spatial convolution is very expensive for large
references, scaling as @xmath where @xmath , @xmath are the pixel widths
of the reference and signal images respectively, and so Fourier
convolution ( @xmath ) is favoured for ‘scene’ cross-correlation wherein
a large reference image is used (see e.g. Poyneer 2003 ) . However, for
the purposes of lucky imaging I favour spatial convolution, for the
following reasons:

-   For a critically sampled Airy PSF the reference will always be
    small, since beyond a radius of a few pixels the signal becomes
    negligible compared to noise. Therefore, typically @xmath .

-   It allows for calculating the cross-correlation value only at input
    pixels above a certain threshold, e.g. 50% of the input maximum
    pixel value. If the cross-correlation region only covers the guide
    star and background sky without further bright sources, the number
    of calculations is drastically reduced.

-   Spatial convolution enables the masking of bad pixels, useful if the
    guide star happens to be observed near to bad detector columns (cf.
    Section 2.6.6 ).

-   Even if the PSF is oversampled, photon and detector noise will
    introduce signal variation at the single pixel level. Therefore, the
    image data must be embedded in an array of twice the size before
    performing the DFT to prevent aliasing effects due to this
    high-frequency pixel noise, which further increases DFT
    computational cost (Thomas et al. 2006 ) .

I implemented spatial cross-correlation routines for use in the pipeline
described in Section 7.1 . The routines accept a reference image class
which keeps track of subtleties such as the nominal sub-pixel position
of the reference central location. I also implemented subroutines to
generate a reference image using any axisymmetric function defined by
the user. Alternatively the reference image may be externally generated
and loaded into the pipeline program. The reference image may be
generated either at the pixel scale of the original data, or at finer
pixel scales for cross-correlation with data that has been interpolated.
I implemented a bicubic resampling algorithm via convolution with a
smoothing kernel (Park and Schowengerdt 1983 ) , as previously used by
Law ( 2007 ) , the algorithm has been implemented anew and optimized for
high computational speed. The interpolation algorithm of Park and
Schowengerdt has also been found to be effective by Diolaiti et al. (
2000 ) . Optionally, parabolic fitting of the cross-correlation maxima
may also be applied.

### 4.5 Simulation methods used for testing

In order to test various frame registration methods under controlled
conditions I used the packages of code described in chapter 6 to perform
end-to-end Monte Carlo simulation of a lucky imaging system
representative of the system used at the Nordic Optical Telescope in
summer 2009, with a telescope primary aperture diameter of 2.5m and
central obscuration diameter of 0.5m. The code was used to produce
time-evolving short exposure images, sampled at four times the Nyquist
rate and at 0.05 second intervals. A seeing width of 0.5 arcseconds (as
observed at 500nm, which is the standard reference wavelength for
measurements of seeing) was chosen as representative of good, but not
exceptional, observing conditions for La Palma in the summer months. The
atmospheric turbulence layers were simulated according to the model
detailed in table 6.1 . An observation wavelength of 770nm was
simulated, representative of the central wavelength of the SDSS i’ band
filter.

The generated focal-plane images were normalised such that the peak
pixel value of each short exposure represented the Strehl ratio of that
frame. After using the peak pixel to obtain a precise estimate of the
Strehl and PSF peak location in each frame (hereafter referred to as the
“true” values), the images were rebinned to represent pixel angular
widths corresponding to those used in the real lucky imaging
observations, specifically pixel scales of @xmath and @xmath
milliarcseconds per pixel. These correspond to Nyquist sampling at the
observation wavelength, and under-sampling by a factor of 3,
respectively.

Next, the stochastic processes of photon arrival and detector response
were simulated to produce realistic images. Various source photon flux
levels were simulated. Transfer CIC events were simulated at a signal
level equivalent to 0.05 photo-electrons per pixel per frame and CICIR
events were simulated at an occurrence rate of 0.04 per pixel per frame,
representative of calibrated levels from the real detector (cf. Section
2.6.3 ). The background sky flux level was assumed to be negligible in
comparison (equivalent to dark time observing).

The realistic datasets were then processed by the lucky imaging pipeline
described in chapter 7 . While reduction steps involving calibration
frames are not applied to simulated data, all other aspects of the
pipeline reduction process are identical to real data.

### 4.6 Testing registration algorithms

#### 4.6.1 Results from comparison via simulation

The chain of simulated data generation and processing steps was
controlled via scripts written in Python, to allow for feasible
variation, processing and analysis of many different parameters. ten
atmospheric simulations of 1800 frames each (90 seconds worth of data)
were generated. For each simulated detector pixel width and source light
level, these datasets were processed with 5 different Monte-Carlo
realisations to obtain a good sampling of the photon and detector noise
processes. The simulated exposures were then processed using the
pipeline, and the output image qualities estimated using the image
analysis libraries I developed for the pipeline described in chapter 7 .
A typical distribution of the simulated results is plotted in figure 4.5
. The simulated Strehl ratios for any given parameter set can be seen to
have reasonably small standard deviation and range compared to
differences between parameter sets, and appear to cluster around a
single mode. Hence error bars are suppressed in later plots for clarity.

Initial investigations with simulated data tested the relative
performance of different cross-correlation algorithm implementations.
Extensive investigation of the various parameter combinations were
undertaken. The results may be summarised as follows:

##### Interpolation method

The simulations indicate that, so long as one of the described
interpolation methods is employed — e.g. interpolate data and then
cross-correlate at interpolated positions, or cross-correlate data and
then fit a parabola about the maximum — the differences between them are
small (Figure 4.6 ). At high and intermediate flux levels the difference
in resulting Strehl ratio is negligible. This null result is of some
use, since the interpolation process and cross-correlation at the
resulting higher pixel resolution incur a significant penalty in
computational speed. On workstations with less powerful CPUs which are
not performance limited by the data input/output, switching to parabola
fitting may result in a useful speed boost. Interestingly, at the lowest
light levels (source flux of 50 photo-electrons per frame) the
interpolation process does provide a small but non-negligible benefit
(relative increase of 10% in Strehl ratio, although in absolute terms
this is only @xmath at most), presumably due to the smoothing effect of
the interpolation process.

##### Cross-correlation reference

The most interesting results came from variation of the
cross-correlation reference. Normalising the reference to account for a
non-uniform noise distribution as described in Section 4.4 significantly
improves performance at low light levels, as seen in Figure 4.7 .

#### 4.6.2 Comparison using real data

The trends observed in simulated results were verified using real data.
Various fields were reduced using the standard and normalised
cross-correlation references, and for faint guide stars use of the
normalised reference does indeed result in improved image quality.
Figure 4.8 illustrates one particularly nice example, where a Strehl
ratio improvement of around 70% is observed.

### 4.7 Image formation processes

In this chapter thus far, I have only considered the Strehl ratio in
reduced images as a metric of the frame registration and image quality
estimation process. This is often the single most useful number when
summarising image quality, and gives an easy point of comparison for
different frame registration algorithms. However, having settled upon an
optimum registration algorithm, it is of interest to explore the
detailed error budgets applicable under different observing conditions.
If we can characterise and predict the errors for any particular set of
observing conditions then we may predict the image quality of the lucky
imaging results, and adjust our observing strategy accordingly. If this
could be done reliably it would be invaluable in both designing and
undertaking large scale lucky imaging observing campaigns.

First however, we require a model of the image formation process. If we
neglect the direct effects of pixellation, then the image formation
process for lucky imaging can be broken down as follows. First, I
consider the average PSF resulting from perfect alignment of many
instantaneous PSF speckle patterns upon their position of maximum
intensity (brightest speckle). The resultant Strehl ratio will simply be
the mean of the Strehl ratios present in each instantaneous PSF.
Fortunately, a model for the instantaneous Strehl ratio probability
distribution function exists. Second, I consider the Strehl
misestimation, since this affects our frame selection process. Finally I
consider the deleterious effect of frame registration positional error.

#### 4.7.1 Instantaneous Strehl ratio probability distribution function

The instantaneous Strehl ratio probability distribution function (PDF)
is equivalent (after multiplication by some constant normalisation
factor) to the PDF of the peak intensity in the focal plane, which has
been investigated under the term “speckle statistics.” Fitzgerald and
Graham ( 2006 ) give an excellent summary of the speckle statistics PDF
originally derived in Goodman ( 1975 ) in the context of laser speckles,
and later applied in the context of astronomical adaptive optics by
Canales and Cagigal ( 1999 ) . The intensity PDF in the context of
uncorrected or partially-corrected seeing is well modelled by a modified
Rician (MR) distribution:

  -- -------- -- -------
     @xmath      (4.5)
  -- -------- -- -------

where @xmath is the intensity, and @xmath , @xmath represent the
coherent and speckle wavefront amplitude components respectively. It is
possible to derive values for the parameters of the function purely from
knowledge of the atmospheric seeing conditions and the number of Zernike
modes corrected by any adaptive optics present, however this assumes
perfect correction of the Zernike modes (Canales and Cagigal 1999 ) .
Since correction at higher Zernike modes in particular tends to be far
from perfect, the actual intensity PDF is often better described by a
semi-empirical distribution obtained by fitting the parameters @xmath ,
@xmath to observed data. Fitzgerald and Graham present just such an
experimental verification of the model PDF for short exposures obtained
on-sky with partially corrected adaptive optics.

Gladysz et al. ( 2010b ) point out that the modified Rician model is not
valid in the PSF core of high order, high Strehl-ratio adaptive optics
systems, as made evident by the fact that it has positive skew for all
values of the parameters, whereas the intensity distribution in a high
Strehl system has negative skew. However, it is an effective model of
both conventional lucky imaging and hybrid lucky imaging adaptive optics
systems — in high Strehl ratio regimes lucky imaging produces rapidly
diminishing returns and so is unlikely to be applied to such systems.

Note that when using the modified Rician intensity PDF to estimate the
performance of conventional lucky imaging, it is appropriate to assume a
phase correction of the first 3 Zernike modes (piston and tip-tilt),
since we are assuming perfect re-centring upon the brightest speckle.
Figure 4.9 illustrates the Strehl ratio histogram observed in the
simulated data.

#### 4.7.2 Strehl estimation error

With a model for the instantaneous Strehl PDF, we may begin to estimate
the performance of lucky imaging under favourable conditions, by
considering the mean Strehl ratio attained by selecting a certain
portion of frames with the highest Strehl ratios, e.g. the mean Strehl
ratio of the top 10% of the PDF. However, applying this in practice
requires a reliable estimate of the Strehl ratio in each short exposure,
at least via a proxy measure such as the cross-correlation maxima.
Errors in the estimation process will lower the Strehl ratio in the
final image if frame selection is applied. A reliable estimator of
Strehl ratio in short exposure images is also of interest when testing
models of the atmospheric turbulence and the speckle statistics that
result, since we may begin to investigate the short-timescale properties
of the models and real data.

An investigation of Strehl estimation errors in real data is deferred to
future work. For the time being I present results from the simulations
described in Section 4.5 as an illustration of the typical errors that
might be expected. Figures 4.10 and 4.11 give some idea of the scatter
in estimated versus true Strehl ratio, while Figure 4.12 plots the
resultant mean Strehl ratios resulting from the 10% selection cut-offs
based on the cross-correlation quality estimates at various source
photon fluxes.

We may characterise the effect of Strehl estimation error upon the image
formation process by defining a selection probability function, which I
shall denote @xmath , where @xmath is the true Strehl ratio in a short
exposure. This represents the probability that a short exposure with
true Strehl ratio @xmath will be selected and drizzled to produce the
final reduced image. For example, we may wish to construct a reduced
image using the best 10% of the short exposures — i.e. using all images
where @xmath , @xmath denoting the 90th percentile short exposure Strehl
ratio. Then the ideal selection function is simply a step function:

  -- -------- -- -------
     @xmath      (4.6)
  -- -------- -- -------

However, due to Strehl estimation error the transition from a
probability of 0 to 1 is much broader for the selection function
resulting from quality estimation with real data. As can be seen in
Figure 4.13 , at very low source signal levels the frame selection is
not very much better than a random selection.

#### 4.7.3 Frame registration positional error

Once we have determined the Strehl ratio of the short exposures which
will be used to produce the reduced image, all that’s left to be
modelled is the alignment and combination process. Neglecting the direct
effects of pixellation, we may consider the reduced image resulting from
registration, alignment and summing of short exposures.

Designating the focal-plane co-ordinates with the two-vector @xmath ,
and the intensity distribution for each short exposure (speckle pattern)
as @xmath , with peak intensity at @xmath then we may consider the sum
image resulting from perfect alignment of the short exposures, which I
shall call the ‘zero-error’ sum PSF with light intensity in the focal
plane described by the function @xmath :

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

The effects of pixellation, photon shot noise and detector noise will
cause error in the frame registration process, represented by the
two-vector @xmath , such that the estimated position of peak intensity
is

  -- -------- -- -------
     @xmath      (4.8)
  -- -------- -- -------

resulting in the mean image with registration error described by the
function

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

Now, if we consider the two-dimensional error probability distribution
function @xmath , such that

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

then over many thousands of frames, the summing of images with error
offsets @xmath determined by the PDF @xmath is equivalent to convolution
by the error distribution, and so we get the result:

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

where @xmath represents spatial convolution.

If @xmath may be reasonably approximated as a Gaussian distribution,
then knowledge of the standard deviation @xmath leads to an extremely
simple, if somewhat crude, analytical estimate of the Strehl ratio in
the final reduced image. We expect the core of the zero-error sum PSF to
be essentially an Airy disc core, of full width at half maximum (FWHM)
@xmath . This may be approximated by a Gaussian of the same FWHM, with
parameter @xmath . ¹ ¹ 1 (2.3548 is approximately the ratio between the
FWHM of a Gaussian and its parameter @xmath .) The PSF resulting from
convolution with the error distribution may then be approximated by a
Gaussian of parameter

  -- -------- -- --------
     @xmath      (4.12)
  -- -------- -- --------

with corresponding reduction in Strehl ratio, which I shall denote
@xmath :

  -- -------- -- --------
     @xmath      (4.13)
  -- -------- -- --------

This obviously neglects contributions from the halo of the PSF, which
will mitigate the effect somewhat at low Strehl ratios.

The error distribution is complex, depending upon many parameters, not
least of which is the Strehl ratio of any given short exposure. Ideally
we require a formula to estimate the typical error without undertaking
time-consuming Monte-Carlo simulations. I attempted a breakdown of the
error variance into various contributing factors, along the lines of the
analyses presented in Thomas et al. ( 2006 ) , in the hopes of producing
a useful approximation. As explained below, the assumptions used are
somewhat flawed and the simulations show the estimates to be poor, but I
present the mathematical treatment nonetheless as a starting point for
further investigations.

In order to apply analytic estimates, I first estimated the flux present
in the bright speckle against which the reference image is
cross-correlated. Assuming that the brightest speckle is reasonably
similar in shape to an Airy disc, we can make the approximation that the
proportion of total source flux concentrated in the brightest speckle is
proportional to the Strehl ratio of the short exposure image. Then, the
proportion of flux within the first minima of an Airy disc is around 80%
of the total Airy disc flux (the exact proportion depends on the
aperture obscuration ratio, i.e. the ratio between the diameters of the
inner and outer radii of the telescope’s primary aperture annulus).
Denoting the mean flux level @xmath , we then have

  -- -------- -- --------
     @xmath      (4.14)
  -- -------- -- --------

where @xmath denotes the short exposure Strehl ratio. Then, for a pixel
width corresponding to Nyquist sampling of the Airy disc, we may
estimate the cross-correlation positional error variance in the
x-direction due to shot noise after detection by the EMCCD as (Thomas
et al. 2006 )

  -- -------- -- --------
     @xmath      (4.15)
  -- -------- -- --------

where an additional factor of 2 has been introduced to account for the
additional variance due to stochastic signal multiplication in the EMCCD
multiplication register (cf. Section 2.4 ), which is equivalent to a
photon flux reduced by a factor of @xmath . (Note I have chosen the
x-axis without loss of generality as a convenient co-ordinate system for
representing positional error along any given axis.)

We must then consider error due to light in the halo, since this will
dominate over readout noise if the typical pixel illumination due to
halo flux is greater than around 0.1 photo-electrons per frame. Thomas
et al. give an estimation of cross-correlation error variance due to a
uniform readout noise. We may utilise this to give a crude approximation
of error-variance due to halo flux, though I note that the approximation
is a poor one at low light levels as the halo flux noise will of course
be Poissonian and not Gaussian in nature. With this in mind, we may
derive a first order approximation to the halo intensity in the vicinity
of the PSF core by approximating it as a Gaussian with FWHM equivalent
to the seeing width, which I denote here as @xmath , measured in
multiples of pixel width. For a sum halo flux

  -- -------- -- --------
     @xmath      (4.16)
  -- -------- -- --------

we then get a halo light level per pixel per frame, @xmath , in the
vicinity of the core of

  -- -------- -- --------
     @xmath      (4.17)
  -- -------- -- --------

which we may then insert into the appendix result of Thomas et al.
(equation C10) to estimate the positional error variance in the
X-direction due to background flux from the halo:

  -- -------- -- --------
     @xmath      (4.18)
  -- -------- -- --------

where an extra factor of 2 has been inserted again to account for added
variance due to stochastic multiplication, and @xmath is the FWHM of the
reference auto-correlation — @xmath has an angular width of @xmath in
this case.

Figure 4.15 plots the standard deviation of the error distance in the
X-axis, against true Strehl of the short exposures, for various
simulated source signal levels. Figure 4.16 compares this with the
analytical error variance estimates derived above, which clearly
underestimate the simulated error. This discrepancy is to be expected if
we take into consideration the fact that, unlike readout noise, halo
noise is not in fact independent between pixels, but highly spatially
correlated (hence the observed speckle patterns). Unfortunately, to the
best of my knowledge an analytical formula for estimation of
registration error in the presence of correlated background noise is not
available in the literature. Furthermore, even when I take into account
an empirical estimate of the position error variance due to speckle, the
combined error estimate is still lower than the simulated error for much
of the range of instantaneous Strehl ratios. It is interesting to note
that the disparity becomes particularly clear around the instantaneous
Strehl ratio of 0.3, which is about the level where secondary speckles
become clearly noticeable in short exposures.

### 4.8 Image formation model

Having considering the processes affecting the formation of the reduced
image, we may now consider a model that draws them together. I propose
the following formalism for a model predicting Strehl ratio in reduced
lucky images. I propose that the Strehl ratio in the final, reduced
image, @xmath may be estimated as:

  -- -------- -- --------
     @xmath      (4.19)
  -- -------- -- --------

where @xmath is the Strehl ratio in a short exposure, @xmath is the
reduction in Strehl ratio due to position estimation error, @xmath is
the short exposure selection probability function and @xmath is the
modified Rician distribution of instantaneous Strehl ratios (this
assumes the short exposure time is less than the atmospheric coherence
time). Validity of the model largely depends upon whether the effects of
positional estimation error may be well approximated in this fashion.
Verification of the model is deferred to future work.

## Chapter 5 Scientific applications of lucky imaging

To date, the applications of lucky imaging have been limited in scope.
The choice of useful observation targets has been restricted by the
narrow field of view entailed by good sampling with the older @xmath
pixel EMCCD detectors, and the difficulties of accurate detector
calibration required to observe faint and extended sources. These
factors, together with the generally developmental nature of the
technique, have meant that conservative choice of observing targets was
essential in order to achieve scientific results. As such, productive
observing programs have largely revolved around high-resolution stellar
binarity surveys, typically looking at very-low-mass stellar
populations, for example Law et al. ( 2005 ; 2008 ), Lodieu et al. (
2009 ), Bergfors et al. ( 2010 ) . Such programs benefit from the short
target acquisition time overheads associated with lucky imaging, when
compared to adaptive optics systems, for comparable spatial resolution.

I begin this chapter with further work on stellar binarity, this time in
the context of planetary transit host candidates. This is a relevant
topic for current research, given the current plethora of transit
surveys. Well quantified, preferably high contrast, companion detection
limits are a necessity, as explained in Section 5.1.1 .

I present analysis algorithms and application of a matched filter
companion detection technique, which produces quantified detection
limits with a challenging dataset, and results in several companion
detections at fainter contrasts than previously achieved with lucky
imaging.

I then explore some of the possibilities for more general application of
lucky imaging techniques, given the improvements in field of view and
detector calibration achieved with the summer 2009 Cambridge LuckyCam
observing run. The possibility of high temporal-resolution photometry
with EMCCDs is explored in a preliminary fashion, and recommendations
for further investigations made. A case is made for lucky imaging as a
viable alternative to Hubble space telescope observations for high
resolution imaging with an excellent faint limit. Furthermore, the
possibilities of lucky imaging coupled with adaptive optics systems are
considered, specifically in the context of probing binarity
distributions at previously unobtainable levels of resolution in the
crowded central regions of globular clusters.

### 5.1 Binarity of planetary transit hosts

#### 5.1.1 Planetary transit surveys and the need for follow-up
observations

Since the discovery of the first planet around a Sun-like star in the
mid-nineties (Marcy and Butler 1995 ) , exoplanet science has grown to
become a major sub-field of astronomy, and was listed as a priority in
the most recent Astronomy and Astrophysics decadal survey (Committee
et al. 2010 ) . While radial-velocity surveys have produced the majority
of confirmed exoplanets to date, transit surveys have an important role
to play. The a priori chance of discovering any particular exoplanet
system through transit detection is much smaller than for the
radial-velocity method, due to two reasons: first, transit surveys are
sensitive only to those planetary systems which are by chance aligned
such that their orbit crosses our line of sight (see Figure 5.1 ), and
second, we must be lucky enough to observe the system during transit.
However, while radial velocity surveys require large amounts of time on
large telescopes to perform spectroscopic measurements of host star
candidates, transit surveys can monitor many thousands of stars
simultaneously through use of instruments with wide fields of view.
Consequently, in recent years ground-based transit surveys have detected
exoplanets in numbers rivalling the radial-velocity method (Winn 2010 )
, and the Kepler space mission has detected over 2000 planetary-host
candidates since its launch in 2009 Borucki et al. ( 2010 ), Batalha
et al. ( 2012 ) . Transits also give us information that is
complementary to the mass estimates obtained via radial-velocity
measurements — for transiting systems we may derive the planetary
radius, orbital inclination, and even atmospheric composition (though
the latter does of course require spectroscopic follow-up).

Unfortunately, since identification of planetary candidates is based
primarily on detecting shallow transits wherein the detected flux drops
by a few percent, we often observe similar phenomena which contaminate
the candidate set. The two primary contaminants are binary star systems
with grazing eclipses, and binary systems close to an unresolved third
star, both of which result in a shallow drop in the observed lightcurve
(Charbonneau et al. 2004 ) . As a result some ground-based surveys have
a false/real candidate ratio of ten to one, and so candidate
verification becomes a major problem (Winn 2010 ) . To conclusively
confirm or reject a planetary-host candidate found via transit surveys
requires spectroscopic follow up, but making the required observations
requires a considerable amount of allocated time on a large telescope.
Techniques which can detect a subset of the false-positives and so
winnow down the remaining candidates are thus highly desirable. Lucky
imaging can perform admirably in this role by providing a low-cost,
highly efficient observing tool to produce excellent constraints on the
presence, or lack, of close secondary stellar sources in the vicinity of
planetary-transit candidates.

As part of the 2009 observing run, we observed a number of planetary
host candidates identified by transit detection programs such as WASP
(Pollacco et al. 2006 ) and HAT (Bakos et al. 2004 ) with the aim of
determining their binarity. Placing constraints on the possible binary
companions of planetary hosts is important for verifying the
significance of transit data, as an undetected secondary star may add
variability to a photometric signal that can be misinterpreted as a
planetary transit. The binary fraction is also an interesting statistic,
giving insight into phenomena that govern planetary system formation
such as Kozai migration (Takeda et al. 2009 ) .

Observations were made during the period 18th-22nd July 2009 at the 2.56
metre Nordic Optical Telescope on La Palma with the Cambridge LuckyCam
visitor instrument. Seeing ranged from @xmath to @xmath as measured by
the differential image motion monitor (DIMM). All observations were made
in SDSS i’ band, using a plate scale of 32.4 milliarcseconds per pixel,
providing good sampling of the point spread function (PSF). The camera
frame rate was 21 frames per second using full chip readout ( @xmath
pixels ). Table 5.1 lists the observations made.

The data were reduced using the LuckyCam pipeline as described in
Section 7.1 .

#### 5.1.2 General techniques for detecting faint secondary sources

The technique most widely applied when attempting to identify faint or
crowded point sources in astronomical images is that of PSF fitting and
subtraction. A step crucial to this process is the choice and evaluation
of PSF model, which may be derived semi-analytically, empirically, or by
some combined analytical model fit with empirical corrections — see
Dolphin ( 2000 ), Diolaiti et al. ( 2000 ), Stetson ( 1987 ) for
examples.

Ideally, for complex PSFs such as those produced by adaptive optics
systems, a fully empirical PSF model is created by analysing a number of
bright, isolated calibrator star images, close to the time of
observation of the binary candidate to ensure similar atmospheric and
instrumental conditions. However, unless the field of view containing
the binary candidate happens to also contain suitable bright calibrator
stars simultaneously this is very costly with regard to observing time,
as it requires separate calibration observations for every target
observation. Also, it may not be practical to use separate calibration
observations if the system parameters are evolving on timescales similar
to observation timespans, for example when quasi-static speckles
(Gladysz et al. 2008 ) are present due to imperfect correction in an
adaptive optics system. In contrast, conventional long exposure
observations produce a PSF which may be expected to conform to a Moffat
profile (Trujillo et al. 2001 ) . As such it is largely parametrised by
a single number, the seeing width, which is relatively easy to extract
and use as a model parameter, although obviously the data is inherently
lower resolution.

The case of lucky imaging is somewhere between the two described above.
The data contains information at higher resolution than conventional
long exposures, and we should be able to calibrate the PSF with relative
ease compared to the complexities of adaptive optics observations. We
expect the PSF in the reduced images to be axisymmetric, however the
radial profile is non-trivial as the PSF consists of a narrow core
surrounded by a wide halo (see cross-section in figure 5.2 ). At any
rate, for this dataset, a fully empirical PSF model was not an option,
due to small non-axisymmetric components that changed between targets,
as can be seen from the residual images in Figure 5.3 .

One approach I considered was fitting an axisymmetric multi-component
analytic model representing the core and the halo components. Just such
a multi-component fitting routine is described in chapter 5 of Law (
2007 ) , with a “sliding box” method used for faint companion detection,
whereby a local estimate of noise levels is used to isolate particularly
bright regions as candidates for PSF fitting. Unfortunately the code was
not available for testing and comparison. Such a model requires 4 or
more parameters, and preliminary work suggested that attempting to fit
such a model using a single image which evidently has unmodelled
residuals would be unreliable. Instead, I settled on the simple and
robust semi-empirical method described below, which also provides a
pixel variance estimate as a function of radius from the primary star.

#### 5.1.3 Primary star PSF modelling and subtraction

To create a PSF model for subtraction of the primary star flux, I wrote
a program to create an axisymmetric, semi-empirical model. For this data
set, all the candidate exoplanet host stars are bright, and so may be
used to calibrate the PSF model with reasonable accuracy. The PSF model
may then later be used for fitting companion star candidates at any
sub-pixel position. First, the central location of the PSF was
determined to sub-pixel precision by fitting a Gaussian profile to 9
pixels around the peak pixel. The pixel values around this nominal
centre were then collected into bins by radius (I refer to these as
“annulus bins”) to get a median value and standard deviation at
approximately one pixel width radius intervals. A continuous radial
profile can then be produced — the values of the Gaussian core fit are
used at radii within 1.5 pixels, and the annulus bin median values are
interpolated to provide model values at larger radii.

Figures 5.2 and 5.3 display results from fitting and then subtracting
the primary star PSF in this manner. Unfortunately the images were
affected by some degree of aberration which was not obvious during the
observing run. As a result, subtraction of the axisymmetric model
produces much larger residuals than would be expected simply due to shot
and detector noise. As Figure 5.3 shows the non-axisymmetric PSF
components vary from target to target, and so a full empirical
calibration to further remove these residuals from the raw data is not
possible — without a stable PSF, any empirical corrections might
attribute a faint companion source to PSF aberration effects. The source
of the non-axisymmetric PSF components is unclear — it may have been due
to problems with the atmospheric dispersion corrector optics, or
imperfect focussing. Unfortunately the problems did not become apparent
until the PSF analysis was undertaken, after the observing run was
completed. If such problems cannot be rectified in future observing
runs, calibrator stars may be observed to improve the PSF modelling and
subtraction process.

#### 5.1.4 Applying a matched filter

To enable automated detection of companion stars, and ameliorate the
variance introduced by the non-axisymmetric PSF components, I applied a
matched filter to the residual images, using the cross-correlation
routines described in chapter 4 . This was done by using the PSF model
to generate a small reference image for cross-correlation with the
residuals image. Before cross-correlation, the reference image pixel
values are rescaled, such that the mean pixel value of the reference is
zero. This has the effect that convolving the reference with a uniform
or slowly varying background section of an image will produce pixel
values that are close to zero, while copies of the PSF will produce high
pixel values (this is essentially the same algorithm employed by the
DAOphot FIND routine (Stetson 1987 ) ). Obviously, this algorithm relies
on the assumption that the PSF of companion stars will be a good match
to the modelled guide star PSF. The assumption is valid for this dataset
for the following reasons. Firstly, the search region of interest is
within the typical isoplanatic patch for lucky imaging (Tubbs et al.
2002 ) . Secondly the guide stars are all relatively bright and
therefore should not suffer from the central bright pixel effect that
becomes an issue when guiding at low photon rates due to the frame
registration process locking on to pixels which are bright simply due to
shot noise (Christou 1991 , Law 2007 ) .

One parameter to be determined is the size of the reference image to use
in the filtering process. This partly depends on the proximity to the
primary star — at larger radii where the primary star residuals are
small and detector noise dominates, a wider reference image is more
effective. Close to the primary star a narrower reference image is best.
After thorough experimentation I settled on a 15x15 pixel reference at
large radii, and an 11x11 pixel reference for the region close to the
primary star. I also experimented with using different selection cut-off
levels for the initial lucky imaging reduction process, but it appears
that for the relatively short observations in this dataset the best
signal-to-noise is obtained from analysing images created using 100% of
the short exposures, despite the slight increase of FWHM and decrease in
Strehl ratio.

Figure 5.4 illustrates the results of the filtering process. The
procedure is effective in reducing the noise levels and partially
suppressing the non-axisymmetric features.

#### 5.1.5 Automated companion candidate detection

Finally, the filtered images were analysed for companion candidates at a
user specified threshold level. At a threshold of 5 sigma the companion
candidates are almost entirely automatically identified in agreement
with visual inspection, while at 4 sigma a few false candidates appear
along with fainter real companions. For this dataset it was practical to
visually confirm or reject all detections at the 4 sigma level. Close to
the primary star, the noise estimates are based on the standard
deviation in annulus bins (reanalysed using the filtered image), while
far from the primary a simple background standard deviation noise
estimate is used (this is obtained by calculating the standard deviation
of pixels in a user defined background region). The switch-over radius
is chosen as the radius at which the standard deviation in the annulus
bins equals the background noise estimate. Typically this is at a radius
of around 90 pixels ( @xmath 3 arcseconds).

Certain criteria must be satisfied for a pixel to be marked as a
companion candidate:

-   First, the pixel value must be above a user specified multiple of
    the noise estimate (i.e., above the sigma threshold). The
    signal-to-noise ratio (SNR) is stored for reference.

-   Second, the pixel must be a local maximum - to prevent small noise
    spikes becoming candidates this is assessed by computing whether the
    pixel has a higher value than all the pixels in a surrounding 9x9
    pixel box.

-   Third, the region about the pixel must also have non-negligible
    signal-to-noise in the unfiltered residuals image. The flux is
    measured in a small aperture about the candidate pixel in the
    unfiltered image (measurement in the filtered image would
    underestimate the flux), and the aperture SNR calculated using
    equation 5.1 . If the unfiltered aperture SNR is below 0.5 the
    candidate is rejected.

-   Finally, an extra caveat is applied to filter out false detections
    caused by a detector artefact — charge transfer inefficiency, which
    causes a faint trail of raised pixel values to the right of very
    bright pixels. All candidates that are within one row of the primary
    star centre, and to the right, are automatically rejected as
    companion candidates, but saved for further visual inspection.

#### 5.1.6 Companion candidate analysis

After detection in the filtered image, analysis of companions is
performed on the unfiltered residuals image. Each companion candidate is
fitted with a Gaussian to determine the central location, then flux is
measured in a circular aperture of diameter 6 times the primary target
star FWHM (this is measured from the reduced image, and is much smaller
than the seeing disc FWHM). The flux of the primary star is measured in
a similar manner, and the magnitude difference calculated from the
ratio. The SNR of the companion in the unfiltered image is calculated
taking into account background and photon shot noise using the formula:

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

where @xmath and @xmath are the mean value and variance of the relevant
background pixels, @xmath is the sum flux of the pixels in the
photometric aperture, and @xmath is the number of pixels in the
photometric aperture.

The algorithm was implemented in C++ using the image processing routines
described in chapter 7 . The program is run twice, once to initially
identify candidates, and then a second time with the confirmed or
rejected candidates input so that any companions close to the primary
target star may be masked during creation of the PSF model.

#### 5.1.7 Results

Companion detections are detailed in table 5.2 . Targets were often
observed slightly off-centre on the CCD detector to achieve better
positioning of the mosaic field of view, in order to visually confirm
the target being observed using the positions of nearby stars. The
target observed closest to the CCD edge has an unbroken observation area
of radius 6.5 arcseconds. As a result, to give a uniform dataset we only
list detections within 6.5 arcseconds in table 5.2 .

8 new companions are detected. These detections significantly expand the
range of binary contrast ratios for which lucky imaging detections
exist, as clearly shown in Figure 5.5 .

Detection limits at a SNR threshold of 4 @xmath are plotted in Figure
5.7 . As one would expect, the limits improve quite rapidly as the
seeing improves, and also improve gradually as more exposures are
gathered. Beyond these general trends it is difficult to draw any more
quantitative conclusions from this relatively heterogeneous dataset — as
the plots show the limits are somewhat noisy, as one would expect from
exposure times on the order of 5 minutes. However, what is clear is that
even with slightly aberrated images good binary detection limits can be
achieved on short timescales with lucky imaging, and excellent detection
limits are achievable in good seeing conditions.

### 5.2 High temporal resolution photometry with EMCCDs

Observations at high temporal resolution are rare in optical astronomy,
leaving potential for a range of investigations if such data were
available. Notably, the ‘Ultracam’ group have undertaken a very
successful science program with a low noise, high speed camera based on
frame transfer CCD technology (see for example Dhillon et al. 2007 ) .

Law et al. ( 2006 ) gave a proof of concept demonstration that lucky
imaging EMCCD cameras could be used for observation of rapidly varying
optical sources. Observations of the Crab pulsar were taken with a 100Hz
frame rate, clearly displaying the 33 millisecond variation pattern. The
larger format cameras available for the 2009 observing run provide
further opportunities to exploit the high temporal resolution of lucky
imaging data. The CCDs can run in a 1000x200 pixel format with a frame
rate of approximately 100Hz. The wider format now available (relative to
the previous detector size of @xmath pixels) gives a better capability
for positioning multiple bright sources in the field of view, enabling
relative photometry at these high frame rates. Potentially, the very low
effective read noise levels of EMCCD cameras may improve the faint limit
of such techniques. However, there is a trade-off at intermediate signal
levels due to increased noise from the stochastic multiplication
process.

A small number of observations were recorded during the 2009 observing
run with two aims. Firstly, detecting high speed optical variability in
the x-ray source Cygnus X-1 as has been previously detected in other
sources (Durant et al. 2011 ) , and secondly providing some data on the
accuracy of the high-speed photometry achievable with LuckyCam.

#### 5.2.1 Cygnus X-1

Cygnus X-1 is the best known example of a high-mass X-ray binary,
consisting of a black hole of mass @xmath in a tight orbit about a giant
star of mass @xmath (Orosz et al. 2011 ) . X-ray binaries are an
intensely studied class of objects with a rich set of phenomena arising
from the interacting extremes of strong gravity and dense matter (see
e.g. Remillard and McClintock 2006 ) . Insight into these systems has
been gained primarily by studying the highly variable emission across a
range of wavelengths. The emission results from accreting matter in
clumpy stellar winds releasing gravitational potential energy during
infall (see e.g. Oskinova et al. 2012 ; and references therein) . Flux
variation from these sources can be extremely rapid — the dynamical
timescales resulting from accretion onto a compact object such as a
black hole or neutron star are on the order of milliseconds (van der
Klis 2000 ) , and observations of X-ray binaries have recorded
variations over a wide range of timescales, right down to the predicted
millisecond regime (Westphal et al. 1968 , Motch et al. 1982 , Meekins
et al. 1984 ) . However, while rapid variation has been detected in both
X-ray and optical, simultaneous observations in both bands with
high-temporal resolution are rare (Durant et al. 2011 ) . The July 2009
observing run gave us a chance to test LuckyCam to see if it would be
suitable for performing such observations.

As part of the 2009 observing run, observations of Cygnus X-1 were
recorded in the 1024x200 pixel frame-size mode. Around 700 000 short
exposures were recorded over two observations, over a total observing
time of around 2 hours. The lens resulting in a pixel size of 95
milliarcseconds was employed, since high angular resolution is not a
priority here, and larger pixel widths result in better signal levels.
Figure 5.8 depicts the field of view observed for the CCD containing the
target.

#### 5.2.2 Estimating a lower bound to signal variance

When considering the likely accuracy of high temporal resolution
photometry, we may derive a first estimate of the expected signal
variance due to photon shot noise and detector noise. Obviously this
neglects variance in the photometry due to PSF variations induced by
atmospheric turbulence, and thus forms a lower bound. Recalling equation
3.3 , we may first consider what are likely to be dominant sources of
variance for this bright target. Total signal from Cygnus X-1 was around
4000 photo-electrons per frame. Seeing width was estimated at around 15
pixels (1.42 arcseconds). While every short exposure will of course have
a different PSF, we may proceed to analyse the per pixel signal-to-noise
ratio equation by simply assuming a uniform illumination within an
aperture of the same diameter as the seeing FWHM, such that the mean
signal per pixel is

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

To avoid detector saturation, Cygnus X-1 was observed at an EM gain
(c.f. section 2.4 ) @xmath ADU / photo-electron. Readout noise had an
estimated standard deviation of 12.2 ADU, equivalent to 1.15
photo-electrons signal level. Background signal, including sky flux and
CIC signal, was around 0.3 photo-electrons per pixel per frame. Clearly
then, for such a bright target the major contributor to signal variance
will be the shot noise, as might be expected. Including the additional
variance factor due to stochastic multiplication, the per frame SNR
estimate is then simply

  -- -------- -- -------
     @xmath      (5.3)
  -- -------- -- -------

or equivalently, for the Cygnus X-1 data we expect a minimum standard
deviation in the per-frame source signal of around 0.02 times the mean
source signal, due to photon-shot noise and detector effects alone.

Of course, the real signal variance will unavoidably be larger.
Atmospheric effects cause variation in the PSF so that some of the
source flux may fall outside the photometric aperture, the aperture may
be misplaced due to a position estimation error, and there may be
intrinsic variation in atmospheric transparency. Minimising this
additional variation requires judicious choice and application of data
reduction techniques.

In hindsight, the observations of Cygnus X-1 would have achieved a
signal-to-noise ratio better by a factor of @xmath , if the detectors
had been switched to the conventional readout, rather than utilising the
electron multiplication register. This is obvious when considering the
‘three regimes’ as outlined in Section 3.4 , but unfortunately was not
considered during the observing run. ¹ ¹ 1 In defence of the LuckyCam
observing team, the observing run was fairly intense, with 24 hours to
assemble the camera shortly after arrival, and residual software
glitches requiring some last minute bug fixing. Future instrumentation
scientists pay heed and plan your observations carefully!

#### 5.2.3 Fast photometry data reduction techniques

I developed two methods for reducing the Cygnus X-1 dataset. The first
was to make some alterations to the lucky imaging pipeline described in
Section 7.1 , to implement a simple aperture photometry algorithm.
First, the user selects the target sources by selecting regions using a
simple average image. For each source, a circular aperture is defined
about the local maximum pixel. The brightest source is then utilised as
a guide star to adjust the aperture positions in each frame — a
thresholded centroiding algorithm is used to locate the weighted centre
of the source, and the position compared to the position measured in the
average frame. The apertures are then shifted accordingly. This approach
is robust, since the centroiding algorithm is very reliable, and fast,
as the pipeline multi-threading capabilities may be employed. A
reasonable processing speed is useful since an hour’s worth of data is
around 150 gigabytes when uncompressed. However, simple aperture
photometry has disadvantages. Over the course of an hour’s observation
we expect the PSF width to vary significantly, and as a result the
proportion of flux within the defined aperture will vary (unless we
define a very large aperture, which will give unnecessarily poor
signal-to-noise for most frames).

With these considerations in mind I employed the Ultracam data reduction
program as described in Dhillon et al. ( 2007 ) . This involved some
modification of source code to allow conversion from lucky imaging FITS
format to the Ultracam ‘.ucm’ format, and a somewhat laborious process
of decompressing all the individual frames before storing to disk.
However, it enabled use of the well developed Ultracam pipeline feature
set, which includes optimal photometry (photometry using annular
apertures weighted according to SNR, see Naylor 1998 ) , and a choice of
position estimation and frame by frame PSF width estimation via fitting
of a Moffat or Gaussian profile.

Experimentation with the Ultracam pipeline found that for this dataset
simple aperture photometry, using a fixed radius aperture, was in fact
the method which produced least variance in the recorded photometric
signal, as illustrated by the histograms in Figure 5.9 . With a bright
source such as Cygnus X-1 the benefits of sophisticated photometric
techniques are outweighed by the variation introduced by aperture
adjustment with a varying PSF, and a fixed, large aperture size is more
reliable. Indeed, some of the frames were assigned error codes by the
Ultracam pipeline and rejected, since no fit could be made to the PSF to
determine position. These generally occur when the PSF clearly has
multiple bright speckles, as depicted in Figure 5.10 , which presumably
cause the fitting routine to not converge. ² ² 2 From conversations with
Vik Dhillon, I have since learned this is usually preventable with
appropriate adjustment of the Ultracam pipeline centroiding settings.

#### 5.2.4 Results

Regarding optical variability in Cygnus X-1, a preliminary analysis of
the extracted photometric time series was undertaken by Poshak Gandi
(who has some experience in this area, see Durant et al. 2011 ) . Power
spectrum analysis revealed no indication of an intrinsic signal. Some
coherent variations at slightly longer timescales was present in the
data due to the atmospheric variability, which was considerable — drops
in photometry of up to 30% were encountered, as illustrated in Figure
5.11 . Regarding the general feasibility of high temporal resolution
with EMCCDs, the poor atmospheric conditions again rather limit any
conclusions that may be drawn from the data. However, I did consider the
variation in the data during intervals shorter than the full observing
period, since we expect at least short periods of relative atmospheric
stability. Figure 5.12 depicts the histograms of standard deviations in
sub-samples of the data, created by simply binning the full dataset into
sections of 2000 frames, corresponding to around 20 seconds of
observations per sample. Normalised aperture counts derived by dividing
the raw counts by the median aperture count were used for the analysis.
Note that the histogram peaks at a standard deviation in the normalised
counts of around 0.02, which is the value predicted by the simple
estimate of variance due to photon shot noise and detector noise in
equation 5.3 . Similarly, the number of sample standard deviations for
the second aperture peak at around 0.04 — this doubling of standard
deviation is to be expected if photon shot noise is the dominant factor,
since the source of aperture 2 has a flux level around 1/4 of Cygnus
X-1. This gives some supporting evidence that when atmospheric
conditions are stable, the noise levels in high temporal resolution
photometry performed with EMCCDs conform to the noise level predicted
using simple signal-to-noise considerations — at least in the context of
bright sources.

Of course, there will always be some degree of atmospheric variation,
which is why we hope to employ relative photometry. Figure 5.12(c)
displays a similar histogram of sample standard deviations, created
using a time-series of the ratio between the normalised photometric
counts for apertures 1 and 2. Much of the large range variation due to
atmospheric effects is removed as a result.

#### 5.2.5 Future work

This preliminary study gives some evidence that EMCCDs are a viable
detector for high temporal-resolution photometry, but perhaps more
importantly it has raised a number of issues for further consideration
when attempting future observations.

First and foremost is the matter of observation timing. When collating
multi-wavelength data from multiple observatories, an accurate time
stamp to accompany the data is crucial. For the summer 2009 LuckyCam
set-up, absolute time-stamping of short exposures was a secondary
consideration, since the primary concern was to ensure that the multiple
EMCCDs in the camera mosaic were simply synchronised. As such, absolute
timings relied upon an uncalibrated PC clock stamp, with an undetermined
error due to delay in the data acquisition pipeline. As such, an
absolute timing accuracy of around 0.5 seconds is probably the best that
could be expected. If future observing campaigns are to focus on high
temporal resolution work, then an added time-stamping facility, possibly
along the lines of the GPS module described in Hormuth ( 2008 ) , will
be a key consideration.

Second is the consideration of data reduction software. If large numbers
of observations are to be recorded it may be worthwhile to implement the
per-frame photometric algorithms using the multi-threaded lucky imaging
pipeline as a basis, or to adapt the Ultracam pipeline to a
multi-threaded form.

Finally, care must be taken to ensure observations are made with the
detector in the appropriate mode (conventional or
electron-multiplication readout), depending upon which of the ‘three
regimes’ the observation falls under (sec. 3.4 ). On a more interesting
note we may also consider faint targets, where a bright companion may be
used for photometric comparison and guiding of the photometric aperture
position, while photon-thresholding techniques are employed to enhance
the signal-to-noise ratio of the faint variable source. The mosaic
nature of the camera will enable this sort of observation, since a high
EM gain may be set on one EMCCD and conventional readout employed on
another, so that both the bright and faint sources are observed with an
optimal detector set-up. If the investigator were so inclined, a
numerical study of signal-to-noise ratio in such observations could be
undertaken using the simulation tools I describe in chapter 6 .

### 5.3 General high resolution imaging in the visible

With a wide field of view and a sensitive detector, lucky imaging
techniques should now be applicable to many more targets, allowing a
wide variety of science applications. In this section I give some
supporting evidence for the claims that lucky imaging can be applied as
a general observing tool.

#### 5.3.1 Faint limits

The faint limit of lucky imaging observations has not previously been
explored. If the detector readout noise can be controlled to
sufficiently low levels, then lucky imaging should have very good faint
limits, compared to other ground-based optical astronomy techniques. The
narrower PSF and associated increase in encircled energy at small radii
give the technique a significant advantage when considering the case
where sky background flux is the dominant noise source. Since the
observations are at visible wavelengths, thermal background is less of a
problem compared to adaptive optics observations in the infrared. A
mosaic field with independently variable electron multiplication gain
gives practically unlimited dynamic range, since a bright guide star can
be observed on one CCD at low or zero gain without saturation, while a
faint target is observed on a nearby sub-field at very high gain. When
thresholding techniques are employed the detector noise can be lowered
even further.

The best example of faint source detection is the observation of 3C405
discussed in Section 3.7 . A source of estimated magnitude @xmath was
observed with signal-to-noise ratio (SNR) of 6 in approximately 1 hour
of observation. For comparison, I generated some SNR estimates using the
signal-to-noise calculator ³ ³ 3
http://www.not.iac.es/observing/forms/signal/v2.2/index.php for ALFOSC,
the Andalucia Faint Object Spectrograph and Camera, which is a permanent
instrument on the NOT. Modelling a set of twelve 5 minute exposures with
seeing width of 0.4 arcseconds (matched to our observed seeing) under
dark time observing conditions, the SNR for 1 hour of observing time on
a 22.5 magnitude source in the I band is estimated at around 50. Note
that this represents a pixel sampling of 0.19 arcseconds per pixel,
rather than the 0.032 arcseconds sampling used for the lucky imaging
observations. Since our observation is largely limited by detector
noise, increasing the pixel scale to match could result in a 34 fold
reduction in our noise levels. Observing with a pixel size of 0.1
arcseconds would have likewise reduced detector noise levels by around a
factor of 9, giving SNR of around 50 while still benefiting from
significant resolution improvements over seeing- limited observations
(considering a reduction process involving 100% selection of the short
exposures, though this is effectively tip-tilt correction rather than
lucky imaging). The CCD configuration used for our observation
experienced a significant level of noise due to clock induced charge
(c.f. Section 2.4.1 ) and it seems very likely that further tuning of
the camera electronics will reduce noise levels significantly.

#### 5.3.2 High resolution across a wide field of view

Previous work has suggested that lucky imaging observations have a wide
isoplanatic angle, typically in the region of 15–30 arcseconds (Tubbs
2003 , Law 2007 ) . The availability of a wide, well sampled field of
view offers the opportunity to verify this estimate and extend
investigations to wider angles.

However, accurate measurements of Strehl ratio across a wide field of
view are hard to determine. Ideally a crowded field should be utilised,
to give a good number of sources, but accurate photometry in such fields
is difficult — since the lucky imaging PSF varies slightly across the
field, conventional PSF fitting routines will give poor estimates of
photometry, and hence poor Strehl ratio estimates. However, globular
cluster data does allow for measurement of the full width at half
maximum (FWHM) across a wide angle, since this does not rely on accurate
characterisation of the faint halo away from the PSF core. To my
knowledge investigation of this particular pair of variables (FWHM vs.
axis offset) has not been previously published; instead only plots of
Strehl vs. axis offset are available in the literature (Tubbs 2003 , Law
2007 ) .

Figure 5.13 displays a pair of drizzled images from different CCDs,
produced using a 10% selection drawn from 6000 short exposures (around 5
minutes total observing time) of the M13 globular cluster. Pixel angular
width was 32.5 milliarcseconds, resulting in good sampling of the PSF.
Seeing width was estimated at 0.49 arcseconds at the observation
wavelength, which was SDSS i’ band. The boxes overlaid in the image mark
manually-selected stars chosen for FWHM analysis. The criteria were that
the star should be the brightest source in the local region of the
image, without nearby companions of comparable brightness that would
significantly affect FWHM measurements.

Figure 5.14 plots the FWHM measurements resulting from analysis of
stellar PSFs over 2 CCDs of the full mosaic, for images resulting from
drizzling 10% and 100% frame selections. The FWHM measurements were
obtained using custom analysis routines written using the lucky imaging
libraries (chapter 7 ). The plots clearly display an improvement in FWHM
by more than a factor of 2 even beyond radii of 30 arcseconds, when a
10% frame selection is employed.

### 5.4 Science with lucky imaging-enhanced adaptive optics: Probing the
binary star distribution in globular clusters

As discussed in chapter 6 , lucky imaging techniques may be employed
behind adaptive optics systems, enabling high-Strehl imaging at shorter
wavelengths, and hence at higher resolutions, than previously
achievable. This has already been tried using the (soon to be upgraded)
adaptive optics system on the 5 metre Hale telescope at the Mount
Palomar observatory (Law et al. 2009 ) , resulting in the highest
resolution images obtained using direct imaging, as illustrated in
Figure 5.15 . Figure 5.16 shows a footprint of the fields of view for
the camera used in the tests at Mount Palomar, overlaid on an image from
the Hubble Space Telescope for comparison of field sizes. Use of the
technique with the latest multi-EMCCD mosaic camera opens up the
intriguing possibility of very high resolution imaging across a field of
view several arcminutes across, as opposed to tens of arcseconds.

For a Nyquist sampled PSF in i’ band on the Hale telescope, the current
configuration would correspond to a field angular width of around @xmath
arcseconds, although a redesign is in development for a @xmath arcsecond
mosaic configuration. One set of attractive targets for such an
instrument would be globular clusters, since they provide a multitude of
bright sources which may be used for guiding, and present fields of view
with stellar densities impossible to resolve with current imaging
techniques.

#### 5.4.1 Globular clusters

Globular clusters are some of the oldest surviving structures in the
universe (Krauss and Chaboyer 2003 ) , providing a rich source of
information about the early galactic conditions under which they first
formed, and the evolutionary processes that have determined their
behaviour since. Their densely populated inner regions give rise to
relatively large numbers of exotic stellar objects such as blue
stragglers (Bailyn 1995 , Knigge et al. 2009 ) , millisecond pulsars
(Davies and Hansen 1998 , Ivanova et al. 2005 ) , and X-ray binaries
(Hut et al. 1991 ) . Imaging at greatly increased resolution might allow
more accurate observations of many targets in these dense regions, and
one could imagine very-high resolution multi-colour photometry surveys
as an extremely interesting project. On a wider scale, we might gain
insight into the dynamical history of the globular cluster as a whole by
examining the binary fraction, and it is this avenue of investigation we
consider in detail here.

In the central, densely populated regions of globular clusters, the
binary fraction is key to the dynamical evolution and lifetime of the
globular cluster (Hut et al. 1992 ) . In a process analogous to a single
star burning nuclear fuel to support the core, globular clusters undergo
a continual process of kinetic energy transfer to delay the onset of
core collapse. Binary systems provide the fuel, transferring energy to
the globular system through close proximity gravitational interactions.
These interactions result in dissolution of wide binaries and tighter
binary coupling of close pairs. By probing the binary distribution in
these regions we gain an insight into these dynamical processes, which
produce many interesting objects such as blue stragglers and cataclysmic
variables.

In contrast, beyond the half mass radius the binary fraction is thought
to remain fairly stable throughout the lifetime of the globular cluster
(Hurley et al. 2007 ) . Determining this primordial binary distribution
gives a probe of the earliest star formation processes.

Presently, a number of globular cluster populations have been
investigated for evidence of the binary fraction. The traditional method
of testing for binarity is to take spectroscopic observations and look
for radial velocity perturbations, but taking such an approach for a
globular cluster would be infeasible due to the observing time required.
Another technique is undertake monitoring observations which look for
source dimming due to binary transits, but these will only observe
binaries whose orbit is viewed edge on, and again requires a large
amount of observing time. The investigative method of choice for
globular clusters is photometric colour measurement. On a colour
magnitude diagram, a binary system of equal mass components appears
twice as bright as a single star of the same spectral type, and so
binary fraction can be inferred. However, this technique is inherently
insensitive to binaries of unequal mass. Also, with current facilities
it is only applicable to the closest globular clusters, and open
clusters, where typical angular separation between sources is sufficient
such that crowding does not prevent individual photometric measurements.
The resulting estimates of binary frequency vary considerably.

The high spatial resolution provided by hybrid lucky imaging adaptive
optics systems at visible wavelengths makes it possible to begin probing
the binary distribution of close globular clusters through direct binary
detection, i.e. resolving the separate components of some binaries which
would otherwise remain unresolved. We should be able to make binarity
estimates based on single epoch data using the spatial distributions of
sources. Detailed Monte Carlo simulations of individual clusters are now
available (e.g. Heggie and Giersz ( 2008 ), Giersz and Heggie ( 2009 ) )
which provide precise observational hypotheses which may be explored
with such data. For example, it should be possible to probe the globular
cluster Messier 4, at a distance of 1.72kpc (Richer et al. 2004 ) , down
to a spatial resolution of around 60 astronomical units (AU), assuming a
PSF FWHM of 35 milliarcseconds as obtainable on a 5m class telescope.
Observations in i’ band on the VLT could reduce the resolvable distance
to around 40AU. At these scales we should be able to place observational
constraints upon the binary populations predicted by simulation (see
Figure 5.17 ). I performed a preliminary investigation to assess the
feasibility of such a study.

#### 5.4.2 Metric: star separations detected vs. random positioning
model

Ideally, high resolution globular cluster observations should be tested
against full Monte-Carlo simulations, but this requires simulating and
recording parameters which have not been focused upon in the past, such
as typical ‘nearest neighbour’ distances and other clustering metrics,
for a projection of the star positions onto a plane representing an
observational image. For this preliminary investigation I resorted to
using a simple random positioning model as a point of comparison to the
observational data. By comparing the spatial distribution of the
detected sources to distribution expected from random positioning, we
should be able to look for over-densities at typical binary separation
distances.

In order to analyse the data I first wrote a simple Monte-Carlo
simulation to estimate the spatial distributions, assuming random
positioning. ⁴ ⁴ 4 While globular clusters are clearly denser in their
central regions, and so a random positioning simulation would obviously
not be comparable, the field of view analysed covers only a small
sub-region of the globular cluster. Given a detected number of sources
and a particular detector layout, the simulation produces many thousands
of source position datasets, so that an average distribution of, for
example, nearest neighbour distances may be built up for comparison with
the real data. To account for the fact that real data has a minimum
distance between resolvable sources, any sources randomly positioned
within this radius of a pre-existing simulated source are rejected, and
the random positioning algorithm runs again to determine a new location.

I then needed some metrics for comparing the simulated datasets with the
observations. The first of these was a set of simple algorithms for
producing “Nth nearest neighbour” distances, given a set of co-ordinates
representing source detections. Written in C++, these run quickly for a
few hundred sources. Histograms of neighbour distances can then be
compared. I also experimented with algorithms for estimating local
over-densities, but quickly came to the conclusion that the datasets
analysed are too small for such tests to have much significance, with
local estimates being easily influenced by artificial elements such as a
bright saturated star.

#### 5.4.3 Probing the binary distribution of M13 with LAMP

Only one pre-existing dataset of lucky adaptive optics observations was
available for analysis, the ‘Lucky at Mount Palomar’ (LAMP) run from
2007 (Law et al. 2009 ) . Figure 5.18 gives an illustration of the
source extraction and analysis. Many of the nearest globular clusters
which would offer the smallest resolvable spatial scales have
declinations well below the equator (Harris 1996 ; 2010 ) , and so were
not viable targets for this observing run. However, observations were
made of NGC6205 / M13, which has an estimated distance of 30.4 kpc from
the sun. The smallest angular scale we can hope to resolve with the LAMP
data corresponds to around 1000AU, and so if the binary distribution is
dominated by close pairs we would expect few if any resolved binary
detections. Despite this, analysis of the data is useful since it
provides a practical test of applying source extraction techniques to
crowded fields observed in this manner.

The M13 observations were analysed using Starfinder (Diolaiti et al.
2000 ) . Of the currently available source extraction packages this is
probably the best suited to lucky+AO observations, since it uses a
number of user-identified sources to build a fully empirical model of
the PSF.

Figure 5.16 shows the fields of view observed with LAMP at 2 pixel
scales. The best source extraction was achieved with the smallest pixel
scale, however the @xmath pixel squared detector size does not allow for
much sky coverage at this resolution. As a result, the number of
detected sources is relatively small, around 250. Figure 5.19 shows the
histograms resulting from analysis of this dataset. The observed data
seem to conform fairly well to a random distribution, as expected at the
large separation distances resolved here. There is an intriguing excess
in the observed data for the number of sources detected at closest
separations, but with small number statistics this may be a simple
statistical anomaly. Sources were de-blended right down to separations
of 3 pixels, which proves the data can provide source separation right
down to the resolving limit.

## Chapter 6 Modelling of lucky imaging systems

Whether fine tuning data reduction techniques or designing an entirely
new instrument, models and simulated data are often a useful tool.
Provided that the real phenomena are adequately modelled, simulated data
can be used to verify that reduction algorithms give accurate results,
provide a range of datasets with carefully controlled parameters to
investigate the interplay of external factors, and offer insight into
potential performance under entirely new instrumental configurations.

Both lucky imaging and electron multiplying CCD observations are
relatively new techniques in astronomy, and as such there are no
standard simulation packages. During my PhD I developed a body of
software which may be used to simulate many aspects of lucky imaging.
Specifically, I developed two packages; one aimed at simulating
atmospheric effects and optical propagation, and one focusing upon
simulating the stochastic processes of photon arrival and detector
effects, as detailed below.

A large portion of this chapter is spent giving a brief review of the
models, data generation techniques and parameter choices required for
such simulations. A good understanding of all of the above is vital if
one wishes to produce computationally efficient, accurate simulations,
and to my knowledge no review materials of this particular niche area
exist in the literature ( Roddier ( 1981 ) and Hardy ( 1998 ) provide
excellent reviews of the models and subject matter, but do not give any
information on simulation techniques). I also highlight recent advances
both in modelling and sensing atmospheric turbulence, which make
possible more comprehensive and better informed simulations, tailored to
specific observing sites.

Once developed, the simulations were used for verification and
improvement of various aspects of the data reduction process, as
detailed in chapters 2 and 4 . I also began developing simulations to
look into possible configurations of hybrid lucky imaging adaptive
optics systems, and preliminary results are presented at the end of this
chapter.

### 6.1 End-to-end Monte Carlo simulation of atmospheric effects and
optical systems

A number of methods and tools have been developed in order to simulate
and predict the behaviour of ground based astronomical systems, usually
aimed at adaptive optics or occasionally interferometry. These may be
grouped into three main categories (Le Louarn 2010 ) :

-   Error budgets and analytic estimators: For well understood and well
    characterised sources of error or image degradation effects, there
    is sometimes a neat formula that estimates the average effect or
    outcome. For example, given a typical wavefront phase RMS error we
    may use the Marechal approximation to estimate the Strehl ratio
    (Ross 2009 ) :

      -- -------- -- -------
         @xmath      (6.1)
      -- -------- -- -------

    where @xmath is the phase RMS error.

    Where error sources do not have analytic estimations, simulations or
    large empirical datasets may be used to generate an error budget
    table, so that the effect may then be incorporated into
    calculations. Where available, these methods often provide the
    quickest and easiest route to an answer, but they may neglect
    correlation between error sources, and are obviously unavailable
    when examining poorly understood phenomena. The simplicity may also
    be a drawback, if additional details are required that may not be
    obtained from these methods — for example we may wish to analyse
    some metric of the PSF other than the Strehl ratio.

-   Semi-analytical Fourier space methods: If the investigator requires
    a detailed estimate of the long exposure PSF, numerical simulations
    may be obtained using Fourier space methods (see e.g. Jolissaint
    et al. 2006 ) . These methods are based upon calculations involving
    the ensemble average power spectrum of the atmospheric turbulence.
    By calculating the average filtering effect of any given system
    component upon the power spectrum, a long exposure PSF may be
    obtained. However, these methods are not well suited to examining
    the behaviour of the system over short timescales, and some
    phenomena such as the laser guide star ‘cone effect’ described in
    Section 1.3.2 are hard to model in this fashion.

-   End-to-end Monte Carlo methods: These produce the most accurate and
    comprehensive simulations, but are also orders of magnitude slower
    than the semi-analytical methods. Accurate simulation of lucky
    imaging requires use of end-to-end Monte Carlo (or simply MC, for
    brevity) simulations, since the method is very much dependent on
    short term variations (although I have attempted to determine
    approximate analytical estimators for the reduced image Strehl
    ratio, as covered in chapter 4 ). It is these methods upon which
    this chapter focuses.

#### 6.1.1 Atmospheric phase screens

##### Basic generation algorithms and application

Recalling the model of optical propagation through atmospheric
turbulence described in Section 1.1 , we can reproduce the relevant
effects upon the wavefront by defining a “phase screen.” This is a
two-dimensional scalar field (or equivalently, a three-dimensional
surface) where the value at each position in the x-y plane represents
the optical path difference incurred by the atmospheric variations in
refractive index. For a given observational wavelength this may be
converted into fractions of the wavelength, so that each value
represents the phase perturbation in radians.

We may generate an array of values to represent discrete samples drawn
from such a phase screen at regular spacings. We expect the values to
conform to some random distribution, resulting from the chaotic
turbulent mixing. In order to successfully model the real phenomena, we
must ensure this distribution of the random values conforms to the
structure function derived from the Kolmogorov turbulence model by
Tatarski:

  -- -------- -- -------
     @xmath      (6.2)
  -- -------- -- -------

or equivalently, that the power spectrum of the random values conforms
to the Kolmogorov @xmath power law.

In truth, generation of truly random values is a complex issue, since we
expect computers to behave in a deterministic manner, but we may employ
cunning generation algorithms to generate pseudo-random numbers which
appear random to the casual observer, and suffice for the purposes of
simulation.

To generate an array of pseudo-random numbers distributed according to
the desired power spectrum, we make use of the discrete Fourier
transform. If we assume a Gaussian distribution to the phase
disturbances, an array of complex pseudo-random numbers representing
phase and intensity at different wavenumbers can be drawn from the
Gaussian normal distribution and then modulated by the relative
amplitude for the appropriate part of the power spectrum. Taking the
discrete Fourier transform then produces a phase screen array which
satisfies the Tatarski structure function (for most length scales at
least — to ensure conformity at the largest scales extra care must be
taken, as described below).

The final stage is to produce an image of a star as seen through the
turbulence. Simply applying an annular mask to the wave amplitude values
produces sections of the phase screen that represent the phase of light
entering the telescope pupil. Taking the Fourier transform of this
subregion then simulates Fraunhofer diffraction to produce a focal plane
image. To model time evolution, the phase screen can be shifted past the
mask at an appropriate speed to simulate the turbulent layer being blown
past the telescope. This is known as the “frozen flow” model, since it
assumes that the turbulent timescales which produce intrinsic changes in
the turbulent layer are much slower than the ‘crossing time’, i.e. the
time it takes for a single point in the turbulent layer to be blown past
the telescope aperture. This simplifying assumption is common in
atmospheric modelling for astronomy, and there is some experimental
evidence it is valid — see Poyneer et al. ( 2009 ) .

The simulation as described thus far uses an over-simplified model, but
does produce many of the qualitatively observed atmospheric effects such
as speckle patterns and image motion due to tip-tilt. The biggest
failing point is that only one turbulent layer is simulated, directly
above the telescope (i.e. at ground level). Observations uniformly
suggest atmospheric turbulence is usually concentrated in two or more
turbulent layers (for example, Caccia et al. 1987 ), but in the past
single layer simulations have been used for simplicity and to reduce the
necessary computational power. By varying the @xmath parameter a single
strongly turbulent layer represented by a phase screen of large phase
perturbations can approximate two layers of weaker turbulence; but
although the long term seeing PSF will be much the same, the high speed
behaviour of short exposure speckles is markedly different. For a single
frozen layer the speckles track uniformly across the focal plane in the
direction of the phase screen’s associated wind velocity, whereas a
superposition of two or more layers causes more of a ‘random-walk’
behaviour (cf. Roddier et al. 1982 ) .

With the basic model set out, we may consider the further refinements
and subtleties that are required to produce datasets of adequate
realism. Note that the subject of the turbulence intensity vertical
distribution (i.e. turbulent layer heights) is rather dependent upon
choice of observing site, and so discussion of this aspect is delayed
until Section 6.3 .

##### Outer Scales and subharmonic methods

Kolmogorov’s @xmath power law only applies over an intermediate inertial
range, well removed from either the outer or inner scales. The von
Karman spectrum shown in Figure 1.1 is the ubiquitous model used in
astronomy to accommodate the deviations from the Kolmogorov power
spectrum beyond the inner and outer scales.

Naturally, the question arises of exactly what those inner and outer
scales are, and when they are significant in astronomy. The inner scale
generally goes unmentioned in astronomical literature. Roddier ( 1981 )
asserts in passing that the inner scale for the velocity field, @xmath
ranges from a few millimetres near the ground to 1cm near the
tropopause, and that the scale for the temperature fluctuations will be
of the same order. The universal assumption is that fluctuations at or
below the inner scale are generally negligible for modelling wave
propagation since the minor random phase fluctuations tend to cancel.

In contrast, the outer scale is significant because it places an upper
limit on the wavefront difference across a large aperture. If the outer
scale is larger than the aperture in question there could potentially be
a continuous underlying tilt causing a large image shift in the focal
plane, whereas a smaller outer scale should result in decorrelation and
hence no tilt above a certain range except in rapidly shrinking
statistical tails. This information allows instrumentation designers to
estimate the maximum stroke required for adaptive optics deformable
mirrors, interferometer fringe trackers, etc.

In the context of lucky imaging, outer scale is mainly significant when
considering targets off-axis, i.e. at a significant angular separation
from the guide star. While the tip-tilt motion of the guide star is
hopefully corrected by the re-alignment part of the data reduction
process, the outer scale will still determine at what angular separation
the atmospheric effects totally decorrelate and the tip-tilt correction
becomes invalid.

It seems likely that the outer scales of the atmospheric turbulence that
causes atmospheric seeing vary to some extent with weather, location,
and altitude. However, there has been a sustained effort to constrain
the numbers, and estimates seem to have now converged upon an outer
scale for the phase fluctuations ranging from 10– 30 metres. For a
recent discussion of the best models for outer scale, see Maire et al. (
2008 ) . It is important to note that this is the relevant number for
designing astronomical instruments, but there are a variety of more
general outer scales for atmospheric turbulence relating to the vertical
and horizontal scales of boundary layers and their velocity fields, etc.
For the simulations of this thesis an outer scale parameter of 30 metres
was used.

Having chosen an outer scale, care must be taken to ensure that the
corresponding structure function is accurately represented by the
simulated data. The method described above utilises normalisation of the
random data using the desired power spectrum in Fourier space, followed
by a discrete Fourier transform to get the spatial phase screen.
However, if we consider the inverse process, it is clear that a
Fourier-space representation will not accurately represent the lowest
frequency components at long spatial scales (see Figure 6.1 ).
Fortunately there are methods for dealing with this problem. If we wish
to simulate a finite outer scale, we may simply ensure that the phase
screens generated are always significantly larger than the outer scale,
at the cost of increasing the computational requirements (especially
memory). However, if we wish to test the Kolmogorov spectrum with
infinite outer scale, or simply avoid excessive array sizes, then an
alternative method is available.

Lane et al. ( 1992 ) proposed the ‘subharmonics’ method of phase screen
correction, later refined by Johansson and Gavel ( 1994 ) . First, a
phase screen is generated using the standard Fourier transform method
described above. Then additional random samples are generated and
normalised by the power spectrum positions on a subgrid in Fourier space
representing low frequencies not sampled in the standard phase screen
(Figure 6.2 ). For each sample the corresponding waveforms in real space
are calculated and the values added, pixel by pixel, to the standard
phase screen. While fairly computationally intensive, this requires only
the memory allocated to the standard phase screen, and is still faster
than generating an oversized phase screen. Figures 6.3 and 6.4 display
the resulting improvement in phase screen statistics when employing the
generalized subharmonic methods implemented in the Arroyo simulation
package (described in Section 6.1.2 ). The subharmonics clearly produce
a more accurate simulation of the tip-tilt Zernike modes, which result
in image motion in the focal plane.

A second consideration with regard to generating phase screens with the
correct statistics is that of correctly representing the smaller scales.
This is simply a case of computing enough samples to give a sufficiently
small spatial sampling interval. To my knowledge there is no guidance on
this matter in the literature, but Francois Rigaut (author of the ‘YAO’
AO simulation package, see Section 6.1.2 ) recommends a sampling rate
producing at least two and preferably three sample intervals per @xmath
. ¹ ¹ 1 Source: http://frigaut.github.com/yao/manual.html This seems
sensible considering that phase disturbances on scales smaller than this
are of order 1 radian or less, and are therefore likely to have
negligible effect.

##### Intermittency

When performing simulations where the focus is upon phenomena at short
timescales, it is worth noting that verifying the statistical averages
of the power law tells us nothing about short timescale evolution within
the turbulence. Long exposure images smear out the rapidly varying
speckle statistics. Balloon measurements should be sensitive to very
rapid variations, but are almost universally focused on gathering large
scale information about the structure and distribution of turbulence
(for example Azouit and Vernin 2005 ) . One of the first major
criticisms of Kolmogorov’s 1941 universal theory of homogeneous
turbulence was an observation by Landau, that at the outer scale
turbulence is often observed to be intermittent.

Intermittency literally refers to an ‘on / off’ nature - for example a
usually quiet signal with occasional short bouts of noise. Sometimes
this fleeting activity leaves the base signal unchanged, but it may be
integral to the nature of a function — an abstract example of this is
the ‘Cantor’s function’ or ‘Devil’s Staircase’ (Figure 6.5 ) which
increases from 0 to 1 over the unit interval, but is constant almost
everywhere (except on the Cantor set, see e.g. Thomson 2008 ) . In
physical terms, an observable that when measured produces an
intermittent signal usually has some kind of structure, as illustrated
in figure 6.7 . In turbulence this is somewhat contrary to the
homogeneous, isotropic background to Kolmogorov’s energy cascade
hypothesis. Nonetheless, it is observed that at the outer scale the
energetic motion injected into a turbulent system is often intermittent.

The classic example of this is a boundary layer - particularly relevant
since the turbulent laminae where seeing arises can be thought of as a
boundary layer between background fluids of different ambient
temperatures (Coulman et al. 1995 ) . Patches of concentrated vorticity
can be seen to release from the boundary resulting in a highly
non-uniform velocity field. There is also some experimental evidence to
suggest that this intermittency persists into the inertial range, which
is to say it is not dispersed by the turbulent mixing. However there
have been no conclusive results on accurately characterising this
intermittency in the inertial range of the velocity field, and there
seems to be uncertainty over the significance.

However, passive scalars are now generally recognised to be less
isotropic and homogeneous than the velocity field (Sreenivasan 1991 ) ,
particularly in shear layers. Intermittency produces significant
deviations from the standard Gaussian Kolmogorov distribution in these
observables, for example temperature fluctuations (Figure 6.7 ). There
are a few models for this fine structure of passive scalar fields, but
none are singled out as the preferred approach. Shraiman and Siggia (
2000 ) give a review of current work on scalar turbulence.

The extent to which intermittency effects can be observed at scales of
interest to astronomy has been shown in simulations to have direct
implications for both lucky imaging and adaptive optics, by Tubbs ( 2006
) , who proposes an adaptable generalised model of intermittency. In
short, for a given mean level of seeing lucky imaging benefits from
intermittency because it isolates the moments of good seeing and
discards the poor ones. Similarly if intermittent effects are observed
frequently then adaptive optics systems could benefit from continuous
monitoring and re-optimization of the system operation rates to achieve
best correction at all times. If intermittency effects are extreme then
it poses real problems for standard adaptive optics systems because the
wavefront correction is performed in a feedback loop, with the potential
for sudden large changes to upset the tracking and require repetition of
the (often lengthy) target acquisition process known as “closing the
loop.”

Observational evidence for or against intermittency effects is sparse.
It is well known that seeing can vary greatly on timescales of hours and
minutes (for example, see Tokovinin et al. 2003 ) but without high
temporal resolution measurements it is hard to determine whether these
variations are due to large scale intermittency or simply a rapid but
progressive change in the mean value of @xmath . Baldwin et al. ( 2008 )
investigates this distinction, with the tentative conclusion that
intermittency may vary with atmospheric conditions — rapid step changes
are observed in some data sets, while others are better characterised as
Gaussian fluctuations about an evolving mean. In the absence of hard
evidence I utilised the conventional model without intermittency effects
for the simulations described in this thesis, but the subject merits
further investigation since many expensive adaptive optics projects
depend upon simulated data for design and optimization.

##### Rolling phase screen generation

Using the discrete Fourier transform phase-screen generation methods
described above, long period simulations require large amounts of memory
for allocating the phase screen arrays. This places a practical limit on
the maximum simulation time, and also models a fixed mean intensity of
turbulence for the duration of the simulation (i.e. fixed @xmath ).
However, algorithms and code for producing dynamically updated phase
screens have recently been developed, as reported in Assemat et al. (
2006 ) . In short, the method uses an analysis of the covariance
functions corresponding to the prescribed power spectrum model, in order
to derive special matrix operators. These matrices may then be applied
to a phase screen array and a vector of newly generated Gaussian
pseudo-random values, to extend the phase screen along one edge.
Considering a phase screen being blown past an aperture, one may then
continually delete values from one side of the array while appending new
values to the other side, producing a simulation of any desired timespan
without large memory requirements. There is an additional advantage
since the atmospheric parameters may be varied as the simulation
progresses. A high degree of numerical precision is required to prevent
divergent results due to round-off errors (Basden 2011 ) , but the
technique has been successfully employed for some simulations, e.g.
Basden et al. ( 2007 ) .

##### Wavefront propagation methods

If multiple atmospheric phase screens at various heights are modelled,
the wavefront must be propagated between them. The most commonly
employed model for wavefront propagation is the so called “geometric
propagation” method, where a uniform phase offset is added to the
wavefront phase according to the distance travelled. While this is
appropriate over relatively short distances or for wavefronts with only
small phase perturbations (relative to a plane wave), if the phase
perturbations are a significant proportion of a wavelength then
diffraction effects may become significant, and a more sophisticated
method may be required for accurate results. Examples include detailed
simulations of defocused wavefronts, e.g. Guyon ( 2010 ) , or wavefronts
focused through small lenslets, e.g. Goodwin et al. ( 2007 ) . For a
full discussion of propagation algorithms the reader is referred to
Papalexandris and Redding ( 2000 ) .

#### 6.1.2 End-to-end Monte Carlo simulation packages

Of the Monte Carlo simulation packages, ‘CAOS’ (Carbillet et al. 2005 ;
IDL based ) , and ‘YAO’ (van Dam et al. 2010 ; based on a free IDL-like
language called ‘Yorick’) , are notable examples which are freely
available. Other expansive end-to-end simulation packages exist, such as
the “Durham extremely large telescope adaptive optics simulation
platform” (Basden et al. 2007 ) and the “Octopus” simulation code
developed within the European Southern Observatories (Montilla et al.
2010 ) , but these are generally utilised in-house and are not aimed at
‘casual’ users. Lucky imaging places some fairly strenuous requirements
on the simulations package. It must be capable of a full time-evolving
Monte-Carlo simulation, as opposed to an analytic model, in order to
investigate the effects of atmospheric coherence times, etc. It must be
able to provide a long period atmospheric phase screen, in order to
prevent unrealistic repetition of point spread function behaviour.
Finally, there is ongoing work to design a curvature sensor for use with
a lucky imaging adaptive optics system. While this work is not discussed
further here, the model relies upon Fresnel diffractive propagation
effects, which are not modelled in many simulations, despite the fact
that including these propagation effects produces significantly
different results when compared to simple geometric propagation in some
contexts (see Goodwin et al. 2007 ) .

For these reasons, and due to my familiarity with the C++ programming
language, I chose to develop simulations using an open source C++
package called ‘Arroyo’ (Britton 2004 ) . The package is well featured,
but somewhat inaccessible to new users due to paucity of documentation
and use of some rather esoteric programming idioms. During my use of the
package I have made a number of alterations that I believe go some way
to addressing these practical issues. I also modified sections of the
code to allow for use of very large phase screens with pixel indices
that may extend beyond the address capacity of a standard 32 bit integer
variable, which is potentially of use on machines with RAM capacity of
over 4 gigabytes.

I also checked various aspects of the Arroyo simulated data to ensure
that the simulated results were valid. While undertaking these tests I
discovered that under certain conditions the low spatial frequencies are
insufficiently reproduced in one direction even when subharmonic
techniques are employed, due to use of long, thin ‘ribbon’ phase
screens. This was rectified by modifying the code to enforce a minimum
phase screen pixel width in any direction equivalent to 50m, at the cost
of a slight increase in computational memory requirements (Fig. 6.8 ).

I note that in theory, it should be relatively simple to integrate
rolling phase screen functionality into the Arroyo package, which would
allow for simulations of very long timespan and dynamically evolving
atmospheric conditions. This potentially very useful enhancement is
deferred to future work.

### 6.2 Simulating photon shot noise and the EMCCD response

For lucky imaging, the detector is crucial to the behaviour of the
entire system. The detector characteristics not only determine the
signal to noise of faint objects, but also influences the guide star
registration process, and hence the resolution obtained. This, and the
fact that I needed to develop and verify a range of detector analysis
routines, meant that careful modelling of the detector was a necessity.
Creating tools that would allow Monte-Carlo simulations required some
work with pseudo-random number generation.

Photon shot noise and detector readout noise are easily modelled with
widely available pseudo-random number generators conforming to
Poissonian and Gaussian distributions. For conventional CCDs these often
suffice. However, the electron multiplying CCD has an unusual ADU output
probability distribution as detailed in Section 2.4 . To generate random
numbers accordingly, the distribution first needs to be calculated, and
then random numbers drawn from it. An early version of my code used a
crude implementation of a “discrete sequential search” algorithm -
essentially a random number is drawn from a uniform random number
distribution, and the EMCCD distribution values are iterated through
until the matching cumulative probability is reached. This has painfully
slow performance for large simulations, in practice. Further
investigation suggested the UNURAN package (Hormann and Leydold 2000 ) ,
which provides “black box” tools for sampling pseudo-random numbers from
any user generated distribution, and is also employed in the ROOT (Brun
and Rademakers 1997 ) modelling package at CERN. Utilising the UNURAN
package provides excellent performance, and the current revision of the
code typically simulates @xmath 7 million pixel events per second on a
single CPU core. Figure 6.9 illustrates the stages of the detector
simulation process.

### 6.3 Modelling lucky imaging at the Nordic Optical Telescope

As astronomical observing techniques and equipment have progressed,
methods of characterising observing sites and observing conditions have
developed alongside. Since the late 1980’s, differential image motion
monitors (DIMM’s, Pedersen et al. 1988 ) have enabled automatic
monitoring of the seeing width, allowing observers to test prospective
observatory sites, gain a uniform record of seeing conditions over long
periods and optimise observation scheduling.

As adaptive optics systems have become more widespread, more in depth
knowledge of atmospheric conditions has become necessary in order to
predict and optimise system behaviour. Real time correction techniques
must be able to keep up with the changing atmospheric conditions, and so
atmospheric coherence times have become important. More recently the
vertical structure of atmospheric turbulence has come to the fore, since
it is key to the performance of layer orientated adaptive optics
techniques such as multi-conjugate AO (Campbell et al. 2010 ; and
references therein) and ground layer AO (Tokovinin 2004 , Andersen
et al. 2006 ) . Over the past decade a number of techniques have emerged
to provide full vertical profiling, including ‘multi-aperture
scintillation sensors’ (MASS, Tokovinin and Kornilov 2007 ) , ‘slope
detection and ranging’ (SLODAR, Wilson 2002 ) , and generalized
scintillation detection and ranging (G-SCIDAR, Fuchs et al. 1998 ) , but
the data reduction methodologies are only now maturing, and only a
handful of sites have been characterised using these new techniques over
a significant length of time (e.g. Avila et al. 2006 , Egner et al. 2007
, García-Lorenzo and Fuensalida 2011 ) . As one would expect, the long
term (over many nights) average turbulence profiles observed vary from
site to site and between seasons, although commonly detected features at
sites of good seeing include a strong ground layer component, one or two
peaks in the 3 to 5 km altitude region, and then another peak in the
troposphere region of 8-15 km altitude — see figures 6.10 , 6.11 and
6.12 .

Fortunately, García-Lorenzo and Fuensalida ( 2011 ) provide profiles for
the Observatorio del Roque de los Muchachos. Their median high
resolution turbulence profile is based largely on data taken between the
months of June and August between 2004 and 2009 (i.e. under summer
conditions). In theory, this provides an excellent match to the
conditions under which the lucky imaging campaign of summer 2009 was
undertaken, although the reality will of course depend upon the
particular turbulence profiles of those nights — indeed the observed
seeing widths of the lucky imaging campaign range significantly about
the seeing width of the median profile model. With the aim of producing
simulated data broadly comparable to that of the observing campaign, I
chose to use this data to inform my choice of atmospheric model.

When experimental data on the atmospheric turbulence profile is
available, one must carefully interpret the data, and decide how to
proceed in modelling the turbulence profile computationally. There is
experimental evidence from balloon based observations (Vernin and
Munoz-Tunon 1994 , Azouit and Vernin 2005 ) to support the use of thin
layer approximations for regions of turbulence. The balloon data suggest
there are often a few dominant layers, with many more adding smaller
contributions to the total turbulence integral. When choosing how many
layers to include in the simulation, a trade-off must be made between
accurately modelling a detailed profile with many contributing layers,
and the corresponding increase in computational requirements. For
example, simulations of multi-conjugate adaptive optics require a large
number of layers to be simulated, since too few would produce
artificially high estimates of the image quality attainable, with phase
perturbations from the few simulated turbulence layers all being
perfectly corrected. Ground layer adaptive optics, which only aims to
correct at one conjugation altitude, is often simulated with fewer
layers (Andersen et al. 2006 ) , since the most important property is
the allocation of turbulent strength between low-altitude and
high-altitude layers. Crucially, for short timescale techniques
including lucky imaging, at least two layers should be simulated if
there is any evidence for significant variance in the wind velocities,
since the relative velocity of the wind layers along with the Fried
coherence length determine the atmospheric coherence time.

For my simulations, computational requirements were a limiting factor,
since long timespan simulations were desired. As a result I chose a 3
layer model providing a crude approximation to the median
high-resolution atmospheric profile measured at the Observatorio del
Roque de los Muchachos, as described in (García-Lorenzo and Fuensalida
2011 ; Figure 6.10 ) . The layer properties are listed in table 6.1 .

Once the turbulent layers to model have been chosen, a realistic wind
velocity profile is required. The combination of layer velocities
determines the rate at which the instantaneous PSF evolves. This is
typically measured by monitoring the autocorrelation properties of light
intensity variation in the focal plane, as described in Scaddan and
Walker ( 1978 ), Fitzgerald and Graham ( 2006 ) . To the best of my
knowledge a full vertical profile of wind velocities is not available
for the Observatorio del Roque de los Muchachos, however García-Lorenzo
et al. ( 2009 ) provide a profile for the nearby El Teide observatory
(fig. 6.11 ). The wind velocities chosen for my simulated model are
based upon these data.

### 6.4 Modelling hybrid adaptive optics systems

One target of investigation with the simulations I developed is hybrid
adaptive optics systems. While there has already been a proof of concept
demonstration (Law et al. 2009 ) , there are plenty of questions worth
exploring, relating to expected performance over wider fields of view,
use of different adaptive optics systems, and application to telescopes
of different sizes. In this section I give a brief summary of the
motivations for development of future hybrid systems, in terms of system
performance — a science case is discussed in Section 5.4 . I then give
preliminary results that illustrate some of the performance gains which
might be expected.

#### 6.4.1 Strehl ratio statistics behind adaptive optics

As discussed in chapter 4 , the instantaneous Strehl ratio as recorded
in a very short exposure varies along with fluctuations in the seeing
strength and (where relevant) errors in the adaptive optics corrections.
For systems with no correction or partial AO, the histogram of these
instantaneous Strehl ratios has a positive skew, i.e. a long tail of
samples when the Strehl ratio is significantly better than average.
Clearly by performing frame selection upon such a sample the mean Strehl
ratio of the selected subsample may be much improved compared to the
mean Strehl ratio of the full histogram.

For high levels of adaptive optics correction, on-axis (i.e. near to the
guide star), if a long exposure Strehl ratio of greater than say, 0.5 is
achieved, this situation begins to change. The standard deviation of the
instantaneous Strehl ratios is smaller, and the distribution becomes
negatively skewed, as depicted in Figure 6.13 . As a result, any
benefits to be gained from frame selection become diminished, and may be
outweighed by the signal to noise ratio considerations of rejecting
frames. For this reason, use of high speed imaging behind high Strehl
systems for on-axis targets is only of use for specialized applications
such as distinguishing faint companions from halo noise, e.g. Gladysz
et al. ( 2008 ) .

However, current adaptive optics systems still struggle to produce long
exposures of moderate Strehl ratio at visible wavelengths. Since this is
the wavelength region where EMCCDs are most sensitive, it is an obvious
target for application of lucky imaging techniques. Furthermore, for
conventional single conjugate AO systems (as opposed to multi-conjugate
AO, see Section 1.3.1 ), the Strehl ratio always declines as the target
separation from the guide star increases. Application of lucky imaging
techniques to off-axis targets may significantly expand the effective
isoplanatic patch size. Finally, for laser guide star systems, lucky
imaging may offer greatly improved sky coverage. Laser guide stars do
not provide tip-tilt information, and so a natural guide star, typically
of magnitude @xmath in V band, is still required to provide this
information (further details in Section 1.3.2 ). The results of lucky
imaging with guide stars of magnitude @xmath in i’ band on the 2. 5m
Nordic Optical Telescope suggest that, with large aperture telescopes
and partial correction from the AO system, guiding with much fainter
natural guide stars may be possible.

#### 6.4.2 Preliminary investigations through simulation

An instrument concept that has recently been proposed by the Cambridge
lucky-imaging group is that of a hybrid lucky-imaging enhanced adaptive
optics system for use on the 4.2 metre William Herschel Telescope, at
the Observatorio del Roque de los Muchachos. I used the simulation tools
to perform a preliminary analysis of what sort of performance might be
expected from such an instrument, given different levels of adaptive
optics correction.

For the analysis, I generated ten independent simulations of 1000 frames
each, simulated at 20 frames per second. The wind model of table 6.1 was
used, with a seeing width of 0.5 arcseconds at 500nm. An observation
wavelength of 770nm was simulated, corresponding to the SDSS i’ band
filter. This gives a @xmath ratio of around 12, which is nearly double
the optimum for conventional lucky imaging — hence conventional lucky
imaging would perform very poorly. The adaptive optics correction was
simulated in an idealised fashion, implemented as a simple calculation
of Zernike polynomials from the guide-star phase screen, up to a user
specified number of Zernike modes. The combined Zernike polynomials were
then subtracted from the phase screens for simulated targets across a
field of view of 60 arcseconds. Three levels of correction were applied,
0 Zernike modes (regular seeing), 10 Zernike modes (all modes up to
order 3), and 28 Zernike modes (all modes up to order 6). Detector and
photon noise were not simulated, corresponding to performing the lucky
guiding process upon bright stars of good signal to noise ratio.

Histograms of the instantaneous Strehl ratios resulting from the
simulations are displayed in Figure 6.14 , and clearly show how the
off-axis Strehl distribution remains negatively skewed even for when
moderate Strehl ratios are achieved on-axis. Figures 6.15 and 6.16 plot
the image quality metrics obtained across the field of view, for AO and
lucky imaging enhanced AO. The simulations clearly suggest that, at
least under favourable conditions, lucky imaging techniques considerably
enhance AO performance. The most intriguing plot is that depicting FWHM
across the field. Using a lucky imaging stabilization of the adaptive
optics frames, this is very much enhanced across the field of view. It
should be pointed out that this result comes from simulating a local
lucky imaging correction at each radius from the AO guide star — that
is, assuming a bright guide star could be utilised and the data reduced
accordingly, for each point plotted. Crucially though, the AO corrected
speckle patterns produce a diffraction limited PSF when re-aligned, even
for a 4m class telescope.

#### 6.4.3 Future work

A full investigation of lucky-imaging enhanced adaptive optics requires
a full simulation of adaptive optics system components such as wavefront
sensing and deformable mirrors, with detector and photon noise. Models
for some of these components are implemented in Arroyo, but others
require work. The simulations are also somewhat time consuming to
compute. I have been in touch with Alistair Basden of the Durham AO
simulation team, and procured a sample dataset of simulated post-AO
short exposures, which may then be processed with the detector
simulation package described above, and the standard lucky imaging
pipeline. I hope to develop a full collaborative investigation in
future, although I note that continued development of Arroyo remains
valuable for small studies and development of novel instrument models
and data reduction algorithms.

## Chapter 7 Data reduction

This chapter describes the software I developed to reduce lucky imaging
data. A general overview of the data reduction process is given, and
some technical aspects of the software are highlighted. Finally, I
describe the combinations of pre-existing astronomy software I have
utilised to accomplish further tasks such as astrometric calibration.

### 7.1 The lucky imaging pipeline

The reduction of any particular dataset is currently a 2 stage process.
In a first pass, calibration data is taken, the frames are cleaned of
artefacts, cosmic rays are identified and guide star frames are
registered. In the second pass this information is used to create
drizzled ( Fruchter and Hook ( 2002 ) , also see Section 4.1 ) science
images at different selection levels. The drizzle implementation
maintains two versions of the final image in parallel — one thresholded
image, one treated in the normal linear manner — so both of these
reduced outputs are produced in a single run. One C++ program deals with
each of these stages, with frame registration data output into a
formatted text file by the first.

To preserve disk space and prevent data loss due to human error, the
data is treated in an entirely read-only manner, to the extent that a
data drive may be mounted in read-only mode. Input and output
directories are designated in configuration files, allowing for easy
re-reduction if the user wishes to try a reduction process with
different parameters.

#### 7.1.1 Observation reduction preparation tool

Before either of the main data reduction programs may be utilised,
various configuration files must be written, and target regions
selected. I created a pair of data-reduction preparation tools in order
to ease the creation of these files, create output directory structures
mirroring the input directories, and produce scripts which will automate
reduction of many targets consecutively.

The run preparation tools are written in the Python programming
language. Python allows easy alterations to the source code without
recompilation, and enables use of many supporting packages, for example
giving easy access to spreadsheet data, and interactivity with the DS9
image display tool for use with FITS format images (Joye et al. 2011 ) .

The first stage of preparing for data reduction is to build a simple
database containing information about where each dataset is located on
the input drive, which pixel scale and wavelength was used for that
observation, where the CCD calibration files are stored, and so on. I
refer to this as “localizing the observations spreadsheet.” This process
relies upon a careful naming convention, where each dataset folder is
prefixed with an observation number, e.g. 7010 referring to the 10th
observation on night 7 of an observing run (although more complex naming
conventions could easily be encoded). The localization script takes as
an input a spreadsheet with rows containing the id number and relevant
information on filters, etc, for every observation. This may easily be
recorded at the time of observation using any spreadsheet package, or
created afterwards from observing logs. The script scans a specified
top-level directory for subdirectories prefixed with observation ID
numbers, which are cross-referenced with the observing log spreadsheet.
An extended spreadsheet is then output which has columns encoding the
dataset locations, and specifies a “camera configuration file” for every
observation, which is determined using a simple decision tree based on
the filter, lens and so on.

By automating the tedious process of dataset file location and parameter
entry, the rest of the data reduction preparation proceeds much more
easily. The user may specify observations for reduction simply by
listing their observation ID numbers in a text file. Once a top level
output directory has been specified, a second preparation script is
employed for the purposes of snapshot creation and region selection.

The first run of this second preparation script simply engages a
cut-down version of the main pipeline program to produce ‘quick look
snapshots,’ average frames created by cleaning and averaging the first
one hundred frames of each observation, which takes a few seconds per
observation.

The second run of the region selection script is more complex. For each
observation in turn, an instance of the image display tool DS9 is
initiated with the snapshot from each CCD of the camera mosaic loaded
into a tiled image, with a preselected colour scheme and scaling (Figure
7.1 ). The user is prompted to enter which CCDs of the mosaic require
data reduction (possibly all of them). Each selected CCD snapshot is
then redisplayed so that the user may select a guide star region and
background region, if there are suitable regions on the CCD. This
automation of file location, display and user prompting greatly speeds
the reduction preparation process. Finally, a script is output which
will consecutively call the main pipeline programs to reduce all the
chosen datasets.

#### 7.1.2 First pass — frame calibration and registration

The first of the two main pipeline programs performs clean-up and
registration of the short exposure images, and is run once for each of
the datasets recorded from the multiple CCDs of the camera mosaic.

The first action taken by the program is to attempt to load bias
calibration frames. Separate frames are stored to record the vertical
and horizontal gradients in the bias, for the reasons covered in Section
2.6.1 . If a horizontal gradient frame does not exist, it is generated
by taking a column-by-column histogram of the first few thousand frames.
An “internally generated signal” (see Section 2.6.3 ) frame is also
loaded if one exists. Next, if a guide star is present on the CCD to be
reduced, a cross correlation reference image is generated. The main
reduction procedure then commences.

The various steps of the reduction process are implemented as
independent “filters” which may be switched on or off depending upon
reduction parameters. The filters interface with a multi-threading
library (see Section 7.2.2 ) and may be parallel (in which case multiple
instances proceed concurrently) or serial (in which case only one
instance of the filter is ever in use). The significant filters are
ordered as follows:

-   Frame buffering: To ensure that data input-output speeds are
    maximised, I implemented a buffer filter. This does nothing except
    load the compressed data from disk into memory, leaving no “dead
    time” before the next data load request. This filter is serial,
    since the disk input speed is fastest when the data is accessed in a
    serial fashion.

-   Decompression: This filter extracts the compressed data to an array
    of floating point numbers.

-   Frame crop: Optionally, the raw data may be cropped to remove bad
    rows and columns at the edges.

-   Bias drift tracking / uniform de-bias: If a sky background region
    has been defined, a histogram of every frame is taken from this
    region and used to estimate the bias pedestal of that frame. This
    value is stored (so it may be used again in the drizzling process),
    and then subtracted across the frame by the “uniform debits filter.

-   Bias gradient correction: Uses the pre-calibrated bias gradient
    frame to flatten the bias pedestal variations across the image.

-   Frame summation. Records a sum of the debiased frames.

-   Histogram recording: If a background region is designated, this
    builds a debiased frame from that region over the entire dataset.
    This “cleaned” histogram may then be used to estimate the bias
    pedestal with increased precision, estimate the electron
    multiplication gain level, and calibrate other detector
    characteristics.

-   Cosmic ray detection and analysis filters: These are a series of
    filters which provide consecutively more stringent, and more time
    consuming, tests to determine whether a bright pixel is simply due
    to a bright stellar source, or a cosmic ray. The first filter simply
    uses a user-defined threshold to determine cosmic ray candidates.
    The second filter examines the candidate pixel to see if it is a
    sharp, local maxima or part of a bright region. The third detection
    filter compares this region of the frame to the same region in the
    previous frame, to see if the signal has increased dramatically or
    is roughly constant. Optionally, all candidate and confirmed cosmic
    ray pixel events may be output to file as 20x20 pixel images about
    the candidate pixels, which the user may visually inspect.

-   Cross correlation filter: Performs the frame registration procedures
    described in chapter 4 . This is one of the most computationally
    intensive filters, but since it is implemented in a parallel fashion
    plenty of processing power may be applied (on a multi-core PC).

Finally, the list of recorded frame information is output as a formatted
text file, encoding guide star locations and cross-correlation maxima,
bias drift levels, any confirmed cosmic ray events, and the file
location of each frame. A debiased and internally generated
signal-subtracted average frame is also output to file.

#### 7.1.3 Second pass — frame thresholding and recombination

The second main pipeline program performs frame recombination, through
use of the drizzle algorithm. This program needs to be able to reduce
synchronised frames from the multiple CCDs, in order to produce a mosaic
output from a single bright guide star located on any of the CCDs, which
complicates matters somewhat.

The initialization procedures are similar to the registration program,
with the addition of a frame list collation stage. A minor bug in the
data acquisition program (since rectified) occasionally caused the loss
of individual frames of data from the observation, and cosmic rays in
the vicinity of the guide star may cause rejection of frames, so the
program must be able to handle missing data. To solve this problem an
intersection set of the frames which are present across all CCDs is
determined. This section of the program also performs a
cross-correlation analysis of the file timestamps, to ensure that the
frame timestamps are in fact synchronised (again, minor bugs affected a
few datasets from the 2009 observing run).

Once the frame lists have been collated and validated, the frames are
ranked by their cross-correlation maxima, which is used as an estimator
of frame quality. If a background region was designated for creation of
a dataset histogram, fitting procedures covered in Section 2.5.2 are
applied to estimate the electron multiplication gain setting. The main
procedure then begins. The initial steps are much the same as for the
frame registration program, except that instead of loading all frames
sequentially, only frames meeting the user defined selection
requirements are loaded. Once the frames have been debiased, the
following filters are applied:

-   Normalisation: If an estimate of the electron multiplication gain is
    available, the frames are normalised so that the pixel values are in
    units of photo-electrons per frame.

-   Thresholding: Optionally, if the frames are normalised, a
    thresholded copy of the data is created by applying a photon
    counting threshold across the image.

-   Dark signal subtraction: The pre-calibrated, normalised internal
    signal frame is used to subtract internally generated signal from
    the frames.

-   Drizzle: Now fully calibrated, the frames can at last be realigned
    and combined according to the drizzle algorithm. Two output images
    are maintained, one containing weighted sums, while the other is a
    weight map. Pixels in the region of cosmic ray events or bad columns
    have zero weights assigned. This is the most computationally
    intensive filter in this stage of the pipeline. To improve
    distribution of the processing load over multiple CPU cores, if data
    from multiple CCDs are being reduced, then a pair of drizzle output
    images is maintained for each CCD. The drizzle output frames are
    carefully pre-aligned to the output mosaic pixels, so that creation
    of the output mosaic from the final images is a simple process of
    concatenating the images.

Finally, the drizzled image pairs of weighted sums and weight maps are
used to output reduced images representing the weighted average of the
input pixel values. If a thresholded image was produced, an image with
good signal to noise across a wide range of light levels may be created
by combining it with the linear image as described in Section 3.6 .

If multiple frame selection cut-off levels are specified, the main
procedure runs repeatedly until all the desired reduced images have been
generated.

### 7.2 Technical aspects

#### 7.2.1 Data storage considerations

EMCCDs by design are well suited to imaging at high frame rates and low
light levels, where a conventional CCD would have a readout noise level
far above the signal. The increasing pixel array sizes and readout rates
are enabling application to an ever greater range of problems. As a
result it is feasible and sometimes desirable to build an EMCCD imaging
system with extremely high data acquisition requirements. As an example,
the 4 CCD camera used by the Cambridge Lucky imaging group in summer
2009 produced @xmath megapixels of data per exposure at a frame rate of
21 Hz, or 188 MB/sec if the data is stored naively at 16 bits per pixel.
As a result, the step up in computational resources required for both
storage and processing from conventional imaging is a large one.
Sustained data write rates of 100MB/sec are about the limit of what can
be achieved with current off-the-shelf computing hardware (circa
2009-2011), which leaves the user with 3 choices: high-end data storage
solutions, real-time processing, or compression.

Fortunately, in the case of lucky imaging the data generally lends
itself easily to compression, enough that data storage and offline data
processing was still possible for the 2009 mosaic camera. The pixel data
output from our camera is represented by 14 bits per pixel, representing
integer values from 0 to 16384. For a typical lucky imaging astronomical
exposure the majority of these pixels do not represent photo-electron
events, i.e. they have a value drawn from a Gaussian distribution as
described in Section 2.4 . With typical readout noise, all but a
negligible proportion of these pixels are within say, 255 data numbers
of the mean and as such only require 8 bits to encode.

Law ( 2007 ) introduced a simple compression system whereby these “dark”
pixels are represented by one byte, and pixels outside the offset range
-128 to 127 were represented by 3 bytes — 1 key byte followed by 2 data
bytes. The current version of the Cambridge lucky group’s data
acquisition software takes this concept one stage further; varying
storage sizes at the bit, rather than byte, level. For each exposure a
histogram is generated and analysed. Depending upon the value range of
the dark pixels, the majority of the pixels in an image will then be
represented by a smaller number of bits, typically 5 or 6 bits, while
pixels with high values (usually due to a photo-electron event) are
represented by a key number of the same bit length as a “dark” pixel,
followed by the full 14 data bits.

The pixel representations are then combined into a conventional stream
of 8-bit bytes using bitwise operations. Higher compression rates could
be achieved by using the Rice algorithm as implemented in the CFITSIO
library (Pence et al. 1999 ) , but any algorithm must be low enough in
complexity to keep up with the data rate while allowing sufficient
remaining CPU time to deal with all other aspects of the data
acquisition process. This simple algorithm is relatively easy to
implement and optimise, and serves well in practice, though future
systems may require further work on this aspect.

#### 7.2.2 Notes on pipeline implementation

The particular needs of a lucky imaging data reduction pipeline — high
performance, custom decompression, large but fairly homogeneous data
sets — resulted in the decision to write a pipeline from scratch, rather
than attempt scripting via IRAF, IDL or some other high level language.
The code base has been written from the ground up in C++ with the goals
of being both object orientated and easily separable into coherent
modules, which allows for rapid development, testing and debugging
(indeed each section of the library on which the pipeline is built
includes a full suite of ‘unit tests’ (Hamill 2004 ) , which may be
invoked after compilation of the code to ensure that updates do not
break any functionality. The code base is now quite well featured,
including reusable modules for drizzling, interpolation, convolution,
data calibration, PSF characterisation, etc, and will hopefully be
useful for further astronomical image data reduction. I have summarised
a few key features of the implementation below, but the interested
reader is referred to further notes and the source code, available at
the author’s homepage ¹ ¹ 1 www.ast.cam.ac.uk/ @xmath ts337 .

It became apparent early in the development that a lot of code
complexity arose due to the complicated interplay of different
co-ordinate systems and indexes used for the different purposes of pixel
indexing, describing sub-pixel locations, describing mosaic locations,
and for dealing with pixels at different scales (e.g. when creating an
interpolated or drizzled image). To alleviate confusion, different C++
classes ² ² 2 For an introduction to C++, especially classes and
templates, I recommend Koenig and Moo ( 2000 ) . are used to represent
each type of co-ordinate system — thereby causing code to throw errors
at compile time if the wrong co-ordinate type is used for a particular
purpose. However, since the co-ordinate systems are all functionally the
same, they are actually implemented using a single, template class.

The class used to represent an image carries information on its outline
position and pixel scale relative to some global co-ordinate system,
thereby allowing easy conversion between different frames of reference.
This works rather nicely in practice, creating code that is shorter,
easier to read and maintain, and no less efficient if coded carefully.
In fact, the addition of a pixel iteration class ensures pixel values
are always accessed in the same order they are stored in memory, to
improve caching and hence increase performance when writing code which
loops over pixel arrays.

These features, together with template classes representing image arrays
and many subroutines go some way towards creating a high performance,
compact, shorthand language that may be used to solve data reduction
problems quickly and neatly, in pure C++ with few dependencies. While
C++ certainly has drawbacks, it does match this sort of challenge well.
The interested reader is referred to Heinzl ( 2007 ) for further
discussion on this topic.

In terms of overall efficiency, the code has been both optimised and
multi-threaded. Optimisation was undertaken using the C++ profiling tool
set Valgrind (Nethercote and Seward 2007 ) to identify bottlenecks and
memory cache misses (and ensure no memory leaks).

The different stages of the data reduction process have been
multi-threaded using the open source Intel “Thread Building Blocks”
package (Reinders 2007 ) . This includes a set of features for pipeline
functionality which map exceedingly well to the demands of processing
lucky imaging data, allowing for simultaneous data access and processing
across multiple threads with a relatively simple interface. As a result,
on our ‘off-the-shelf’ reduction workstation (running with 2 quad core
Intel Xeon 5530 processors) the pipeline will process data as fast as it
can be read from the disk, typically @xmath 80MB/sec of compressed data
- it is therefore capable of reducing the data nearly as fast as the
current camera can produce it.

Since the data reduction process is limited by data access rates, and
the datasets fill a considerable amount of disk space, it makes sense to
keep the data in compressed form. The data is most compressible in the
original raw integer format, i.e. before subtraction of floating point
bias frame pixel values. Therefore, keeping it compressed means
reapplying the calibration process whenever the frame is loaded, for
example in the second pass of the reduction process when the reduced
image is being created. While this requires slightly more processor
time, the routines are well optimized and data throughput rates are
still the limiting factor.

### 7.3 Astrometric Calibration

With any astronomical instrument, astrometric calibration is essential
to determine the pixel scale and sky orientation of the images recorded.
One simple, often used method of astrometric calibration is simply to
observe a “calibrator binary,” a pair of well separated bright stars
with accurately known parameters. Calibrating a mosaic camera is more
complex. If any astrometric measurements are to be made across different
sub-fields of the mosaic, then the offset between the CCD sub-fields
must also be carefully measured.

In order to cross-calibrate the 4 CCDs of the Cambridge lucky imaging
camera, we observed several crowded fields of globular cluster regions.
These regions give many stars across all 4 CCDs which may be used as
calibration points.

In theory it is possible to calibrate the relative pixel sizes and pixel
offsets between camera sub-fields simply by observing the same field of
stars at different offsets — certain pairs of stars are picked out,
their parameters are measured across one sub-field, then the mosaic
field of view is shifted and the pair parameters are measured again. One
example of this is the Hubble Space Telescope calibration program
described in Casertano and Wiggs ( 2001 ) . However such a task is
challenging, requiring many observations and careful analysis.

A much simpler alternative is to use a pre-existing catalogue of star
positions, if one of sufficient accuracy exists, that provides reference
points for stars across the entire field of view. Then the subfields may
be calibrated with respect to their on-sky properties, and the inter-CCD
offsets can be inferred.

When looking for a way to calibrate our globular cluster fields it
became apparent that no high-resolution catalogues existed. While the
fields are full of stars that are above, for example, the USNO-B
catalogue faint limit, the fields were too crowded and saturated to
provide source resolution in the data used to create such catalogues.
The 2MASS catalogue (Skrutskie et al. 2006 ) was the only source of
reference points available. While I was able to use 2MASS data to
produce approximate calibrations the density of catalogue entries is
relatively low compared to the number of stars visible in a lucky
imaging observation of a globular cluster field — typically ten or so
catalogue entries on a field of a hundred visible stars. To make matters
worse, it was often clear that a single entry in the 2MASS catalogue was
really a mid-point between 2 stars unresolved in the 2MASS data. Initial
calibrations suggested discrepancies between calibrations as large as
half an arcsecond, which is significant when each sub field is only 30
arcseconds across.

#### 7.3.1 Creating an astrometric calibration catalogue from HST
archive data

In order to obtain a high resolution astrometric catalogue, I turned to
archival data from the Hubble Space Telescope Wide Field / Planetary
Camera 2. (HST WFPC2). There is an ongoing effort to catalogue this
archival data (Casertano et al. 2010 ) , but the default source
extraction parameters are not optimized for crowded fields, and it was
necessary to create the source catalogues myself.

##### HSTphot

The specialized tool of choice for performing source extraction on HST
WFPC2 data is HSTphot (Dolphin 2000 ) . HSTphot uses a semi-analytical
pre-determined PSF model and detailed information on the WFPC2 chipset
to analyse raw archival data. As a result HSTphot can perform source
extraction with excellent faint limits, good source de-blending, and
astrometric accuracies down to around 1/100th of a pixel. After
extensive experimentation I was able to produce source catalogues with
excellent depth from the WFPC2 data. However, the program is not without
its drawbacks, requiring a rather laborious series of data reduction
steps to run, and lacking routines for converting pixel locations into
sky co-ordinates. I was able to overcome these issues somewhat by
writing auxiliary tools in python, but the final obstacle was that of
saturated sources. By default HSTphot entirely rejects sources which
have saturated pixels at the core. This often means that the stars which
may be located with best accuracy in the lucky imaging data are not
represented in the HSTphot catalogue, which makes it unsuitable for
observations taken in poor seeing or over a short time.

##### S-Extractor

I eventually settled upon the well known S-Extractor (Bertin and Arnouts
1996 ) as my tool of choice for creating WFPC2 catalogue data. Initial
trials suggested S-Extractor would not perform well with crowded WFPC2
data, but further investigation of the parameters resulted in decent
results. Since the algorithm used by S-Extractor does not model the PSF,
the catalogues produced are not as photometrically deep or
astrometrically precise as HSTphot, but it can be used to give locations
of sufficient bright and intermediate sources with sub-pixel accuracy
that the astrometric calibrations should be reasonably accurate. It is
also simpler to run, using pre-reduced ‘science’ images rather than the
archival raw-data. One problem was that the ‘science image’ mosaics are
embedded in a larger blank frame with low signal regions around the
edges due to the drizzling process. As a result, S-Extractor often
produces false sources in these edge regions. I wrote a Python script to
examine the source catalogues produced and compare them with the
‘weights’ image which accompanies the science image, rejecting any
sources that lie on a region of low weight and therefore low signal to
noise. Sources at low signal level in the main image region were also
rejected. Figure 7.2 depicts catalogue positions after rejection of bad
sources.

#### 7.3.2 Calibration procedure

Having created a reference catalogue, I required a method of matching
the images to known positions.

The first step was to extract source locations from the images. To do
this I wrote a short program to look for local maxima which are above a
user specified threshold in order to identify bright star locations,
then fit a Gaussian profile to 9 pixels about each bright pixel to
determine a sub-pixel position and interpolated peak pixel value.

Second, I required a fitting routine to match the image catalogues to
the reference catalogue. There is at least one publicly available tool
for this task, “Astromatic” (Bertin 2010 ) , but it is focused on large
surveys and only provides calibration against pre-existing standard
catalogues. I resorted to implementing my own fitting routine, which
varied pixel scale, central pixel sky location, and orientation. Each
field is first calibrated approximately by eye, then the image catalogue
positions are repeatedly converted to sky positions and compared with
the reference catalogue while the parameters are varied by the fitting
algorithm. While undoubtedly a crude solution compared to specialized
tools such as Astromatic, this served well enough and seems to produce
good fits. Figure 7.3 displays the final drizzled mosaic produced using
the astrometric calibrations.

Multiple astrometric calibrations using observations of different
targets, and from different observing nights, revealed discrepancies in
the calibrated inter-CCD spacings of up to a quarter of an arcsecond.
However, since the sample size is small and the calibration procedure is
not yet extensively tested, it is impossible to be sure whether the
discrepancies are due to real movement in the instrument, or simply
calibration errors.

## Chapter 8 Conclusion

In conclusion, the specialised hardware and software required for lucky
imaging techniques are now sufficiently mature that it should start
becoming less of a specialist technique, and more of a mainstream
observing tool, either where ‘better than seeing’ observations are
required, or under favourable conditions, for diffraction-limited
imaging at medium class telescopes.

In this dissertation I have demonstrated that, through careful
calibration and use of data thresholding techniques, the faint limit
achieved in lucky imaging is comparable to conventional imaging if a
moderate pixel angular width is employed (Section 5.3 ). This situation
will improve as development of electron multiplying CCDs continues and
levels of noise due to clock-induced charge get lower — since 2009 the
Cambridge group have achieved just this using updated
clock-signal-generation electronics. The concentration of stellar light
compared to conventional imaging should result in much better faint
limits under observing conditions with high sky background levels, such
as during a full moon. Previous estimations of high image quality over a
large field of view have been reconfirmed using the mosaic CCD camera,
with a factor of 2 improvement in full width at half maximum
demonstrated even at a radius of 30 arcseconds from the guide star. More
sophisticated data reduction techniques may widen this patch size even
further.

I believe we are already seeing a wider interest in lucky imaging. It is
telling that while I was only aware of the Cambridge lucky imaging group
when I began my PhD, there are now at least 2 other active
instrumentation groups performing binarity surveys with lucky imaging
techniques (Hormuth et al. 2008b , Oscoz et al. 2008 ) . No doubt this
is in part due to a growing recognition of the potential of electron
multiplying CCDs (Daigle et al. 2010 , Tulloch and Dhillon 2011 ) , but
also there is a growing awareness and understanding of how ground based
astronomical imaging systems behave on short timescales. The
astronomical community is now beginning to consider how we might exploit
that knowledge to design adaptive optics systems employing exposure
selection shutters controlled by wavefront sensors, or fast
tip-tilt-mirror image stabilisation (see, e.g. Keremedjiev and
Eikenberry 2011 , Gladysz et al. 2010a , Males et al. 2010 ; among many
others) .

Definitively staking out the observational programs that will benefit
from lucky imaging seems to me to be a pressing project. I hope that the
analytical and computational models I have developed will be of some use
in this. With mosaic EMCCD cameras providing wide fields of view and
excellent faint limits, at cheap cost and without moving parts,
automated high resolution survey programs seem like a natural target.
Detecting and measuring gravitational microlensing events in crowded
fields is one example of a science driver to which the technique may be
well suited.

While I have developed what are hopefully robust and reusable software
components, this is likely to be an ever evolving aspect of the lucky
imaging technique, as it is for most of astronomy. Larger mosaic
cameras, and more complex image reduction algorithms will likely require
judicious use of the latest computational tools such as general purpose
graphical processing units, to ensure that data may be processed in a
timely fashion.

Finally, probably the most exciting aspect of lucky imaging is the
potential of hybrid adaptive optics systems. The experimental set-up
described in Law et al. ( 2009 ) achieved the highest resolution
astronomical images ever obtained with direct imaging, ¹ ¹ 1 As opposed
to aperture synthesis methods. with full width at half maximum
measurements as small as 42 milliarcseconds. Application of lucky
imaging techniques behind the latest generation of adaptive optics
systems will push the resolution limit even further, and should enable a
much improved effective isoplanatic patch size (cf. Section 6.4.2 ).
Combined with laser guide stars or novel curvature wavefront sensors
which may utilise fainter natural guide stars (Guyon 2010 ) , the sky
coverage of adaptive optics systems may be greatly improved.

The Hubble space telescope revealed a wealth of new science by improving
angular resolution in astronomy. Who knows what we will find lurking at
the next resolution limit.