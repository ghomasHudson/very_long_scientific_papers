### Chapter 1 Introduction

After the cosmic microwave background radiation, the study of the
formation and evolution of the structures on the large scales of our
Universe forms one of the pillars of modern cosmology. These structures
can be mapped by galaxy surveys, and cosmological observables derived
from these surveys such as the galaxy two-point correlation function or
its Fourier transform the power spectrum are central to the field, and
are used to contrast predictions of cosmological models to observations
( Pope et al. , 2004 ; Tegmark et al. , 2006 ; Percival et al. , 2010 )
. Despite essential successes in the last two decades with the emergence
an observationally very successful concordance cosmology, this
description of our Universe, the @xmath CDM model, is still very
mysterious, with only about 5% of the energy budget of the Universe
being made of the matter which we have daily experience of ( Komatsu
et al. , 2011 ) . The real nature of the two dominating components, the
dark energy and the dark matter, remains unclear to this day and is the
heart of a large scientific effort. Both the dark matter density field
as well as the impact of dark energy on the geometry of the Universe can
now in principle both be observed with the help of weak lensing (
Bartelmann and Schneider , 2001 ; Schneider et al. , 2006 ) . It is thus
believed that large galaxy surveys able to reach for the lensing signal
are going to play an increasingly important role towards these
fundamental issues in cosmology ( Albrecht et al. , 2006 ) .
Of course, in order to assess some set of observables as valuable for
cosmology, and to design an experiment towards its extraction, it is
essential to understand both our capabilities to extract it, as well as
the robustness and pertinence of the predictions of our model. These
aspects, often of statistical nature, are the very backbones of this
thesis. Our main aim in the present research was to try and quantify
these aspects in several situations relevant to cosmology, focussing on
the dark matter density field, or its weighted projection along the line
of sight, the weak lensing convergence field, contributing in this way
to our understanding of the information content of galaxy surveys. We
review in this introductory chapter the known tools that we have built
upon as well as the class of observables we have focused on, putting
thus our work in context.

#### 1.1 Stochasticity in cosmological observables

All major predictions and measurements that are used to test our
understanding of our cosmological model are meaningful only in a
statistical sense. Indeed, our inability to observe initial conditions,
which we may tentatively evolve, as well as the complexity of some of
the physical processes involved render in general a statistical
description unavoidable. For this reason, a key element that determine
to an often decisive extent what observable will be of interest for the
purpose of the analyst is the probability density for the realisation of
the fields from which the observables are derived. Typically a CMB
temperature map, or a galaxy density field, from which one measures for
instance the two-point correlation function. This element of
stochasticity is sometimes referred to as cosmic variance , a
denomination that we adopt in the following. One must generically
include other sort of stochasticity on top of the cosmic variance, that
we refer to as noise , for instance due to the specificities of the
instrumentation, filling another gap between model predictions and
actual data outputs.
We need to introduce some notation :
We always write a probability density with @xmath , at times adding a
subscript indicating to which random variable it refers to for clarity.
In the case of cosmological fields, these probability densities are
generically high dimensional, describing the joint occurrence of fields
values at different points. Typically, when the random variable are the
values @xmath of a a field @xmath at points @xmath , then @xmath is a
function of @xmath variables, a @xmath -point probability density
function. The position label @xmath itself can have various meanings in
diverse cosmologically relevant situation. It can have for instance
dimension @xmath (Lyman- @xmath forest), @xmath (weak lensing
tomography, projected density fields, CMB), or @xmath (redshift
surveys).
The joint density for the realisation of the field @xmath at all points
can be written conveniently as the functional @xmath . Expectation
values of observables @xmath are given formally as

  -- -------- -- -------
     @xmath      (1.1)
  -- -------- -- -------

an infinite dimensional integral. It should be kept in mind that such
probability densities @xmath are however not always very well defined
and intrinsically difficult to handle, except in some cases. Expectation
values 1.1 can nevertheless be understood as the limit of a finite
dimensional, well defined average

  -- -------- -- -------
     @xmath      (1.2)
  -- -------- -- -------

over a finite sample of the field, with large @xmath . In a harmless
abuse of terminology we may identify at times in this work such finite
samples of the field with the field itself, especially when dealing with
@xmath -body simulations, that have of course only a finite number of
spatial resolution elements.

###### Homogeneity, isotropy, ergodicity

Cosmic variance in the sense defined above is the stochasticity of the
data due to the fact that we observe one particular realisation of a
random field, namely that of our own Universe (or of the observed part
of the Universe, in which case one can also refer to a component of
sample variance). It is a fundamental limitation in the sense that this
variability can never be beaten down, as this would ultimately require
the observation of several universes governed by the same density
functions, which is a mathematical construct useless to our purposes.
Within this framework, one relies on several assumptions, namely that of
statistical homogeneity, isotropy and ergodicity. The first two express
the absence of preferred locations and directions in the Universe.
Mathematically speaking, all density functions are required to be
invariant under spatial translations and rotations,

  -- -------- -- -------
     @xmath      (1.3)
  -- -------- -- -------

for any translation vector @xmath and rotation matrix @xmath . These two
important assumptions can be tested and are confronted to observations.
Of course, homogeneity and isotropy do not apply to fields in redshift
space coordinates. The third, ergodicity, states that we can reinterpret
the ensemble averages in equation 1.1 to be spatial averages. We expect
this assumption to be correct as long as the spatial averages can be
made over sufficiently large volumes, or using widely separated samples,
assuming that correlations at large distances decays quickly enough to
zero. Under these conditions, so called ergodic theorems can indeed be
proven. However, this assumption cannot be fundamentally tested and we
have no choice but to take it as granted in order to obtain useful
results out of this mathematical approach.
Very often of primary interest are the zero mean, dimensionless
fluctuations @xmath of @xmath , defined as

  -- -------- -- -------
     @xmath      (1.4)
  -- -------- -- -------

where @xmath is the mean of the field, independent of position @xmath by
homogeneity.

#### 1.2 Fisher information for cosmology : a first look

Inference on model parameters might appear extremely simple in
principle. For a set of model parameters @xmath of interest, and the
observed field @xmath , probability theory tells us that we must update
our knowledge of @xmath with the simple rule,

  -- -------- -- -------
     @xmath      (1.5)
  -- -------- -- -------

In this equation, the density @xmath describes our prior state of
knowledge on @xmath , and @xmath , viewed as function of the parameter
is called the likelihood. On the lefthand side, @xmath is called the
posterior. Of course, the simplicity of this formula should not hide the
very high complexity of its implementation for typical cosmological
instances. In particular, the likelihoods @xmath are in general only
poorly known, and the very high dimensionality of this object requires
the compression of @xmath to some smaller subset of observables, whose
statistics are set by the likelihood, all carrying some of the amount of
the information that the likelihood carried originally. It is thus
clearly of the uttermost interest to be able to quantify more precisely
this information, both that of the original likelihood as well as that
of the different observables. This is where Fisher information comes
into play.
It seems fair to say that the use of Fisher information in cosmology
begins, though indirectly, with Jungman et al. ( 1996a , b ) , two works
in the context of CMB experiments aiming at measuring the temperature
fluctuation spectrum @xmath . In these works, it is argued that the
posterior for the parameters will be approximately Gaussian. Let us
consider for simplicity the case of a single parameter of interest,
@xmath , as the discussion or an arbitrary number of parameters holds
essentially unchanged. With the true value, or best fit value of the
parameter defined as @xmath , the posterior is assumed to have the form

  -- -------- -- -------
     @xmath      (1.6)
  -- -------- -- -------

with the number @xmath is defined as

  -- -------- -- -------
     @xmath      (1.7)
  -- -------- -- -------

In this equation, @xmath is the variance of the estimates of @xmath ,
including cosmic variance, incomplete sky coverage and detector noise.
Under the assumption ( 1.6 ), it is clear that @xmath is the variance
@xmath of the parameter. Very interestingly, from its definition ( 1.7 )
we see that this variance can be evaluated prior obtaining data, if a
reasonable fiducial point @xmath can be chosen and the model predictions
of the spectrum are given. Provided the assumptions made there are
correct, this is making the approach of Jungman et al. ( 1996a , b )
quite powerful, providing us with a rather great understanding of the
capabilities of the experiment.
It is worthwhile spending a bit more thoughts on @xmath defined in ( 1.7
). It is a special case of an expression that weights the derivatives of
some set of observables @xmath according to their covariance matrix
@xmath ,

  -- -------- -- -------
     @xmath      (1.8)
  -- -------- -- -------

We recover ( 1.7 ) by setting the observables to be the spectrum, and
the covariance matrix to be diagonal, as required for a perfectly
Gaussian CMB map. The number ( 1.8 ) has an array of fundamental
properties, none of them being difficult to show :

-   It is a non negative number, that becomes larger for a smaller
    covariance matrix or a larger impact of @xmath on the observable,
    and vice versa.

-   @xmath corresponding to independent observables (i.e. with no
    covariance) is simply the sum of their respective @xmath .

-   Adding an observable @xmath to a set @xmath can only increase @xmath
    , and not decrease it.

-   @xmath is identical to the expected curvature of a least squares fit
    to the observables with the given covariance matrix ¹ ¹ 1 It should
    be noted that this identification to a curvature in a least square
    fitting procedure holds only if the covariance matrix is treated as
    parameter independent. , provided @xmath is the parameter value that
    gives the least squared residuals.

These properties are very consistent with what we would expect from a
measure of information on the parameter @xmath and are making the number
( 1.8 ) a promising candidate for such a measure. However, it is clearly
not the end of the story, since it depends only on the chosen set of
observables and their covariance matrix, but neglects all other aspects
of the probability density @xmath . The link with Fisher information, a
well known tool in statistics, was then exposed and extended to other
areas of cosmology in works such as Tegmark et al. ( 1997 ); Tegmark (
1997 ) . It was noted that the number ( 1.7 ) is identical to

  -- -------- -- -------
     @xmath      (1.9)
  -- -------- -- -------

where @xmath is a Gaussian likelihood for the noisy CMB temperature
fluctuation field. The connection between equation ( 1.9 ) and
covariances on parameters was also used in an astrophysical context
earlier in Amendola ( 1996 ) . Equation 1.9 is the Fisher information in
@xmath on @xmath , a most sensible measure of information on parameters,
whose properties and link to ( 1.8 ) we will have the occasion to
discuss extensively. For several parameters, it becomes the Fisher
information matrix

  -- -------- -- --------
     @xmath      (1.10)
  -- -------- -- --------

Since then, such Fisher information matrices for Gaussian variables and
the assumption ( 1.6 ) have been used routinely in cosmology in order to
assess the capabilities of some future experiments.
Two comments are in order at this point :
First, the definition ( 1.10 ) of the Fisher information matrix is the
most common in cosmology. However, in this thesis, we will rather use
the alternative

  -- -------- -- --------
     @xmath      (1.11)
  -- -------- -- --------

as the definition of the information matrix. These two definitions can
be shown to be equivalent for any probability density function using the
fact that probability densities are normalised to unity, @xmath . While
( 1.10 ) conveniently presents the information matrix as a curvature
matrix, it will become clear in chapter 3 that ( 1.11 ), making a
reference to the score function @xmath , is in fact much more
fundamental for our purposes. This form generalises more easily to non
normalised density functions as well.
Second, as discussed above, Fisher information is often interpreted in
cosmology as an approximation to the parameter posterior, approximated
as a Gaussian with covariance matrix @xmath . The Gausisan
approximation, as well as the identification of the Fisher information
matrix with the inverse covariance matrix are of course only assumptions
that can fail, at times severely. This is especially true when
marginalising within this approach over poorly constrained parameters,
whose distribution often cannot be approximated by a Gaussian shape (see
for example Wolz et al. ( 2012 ) ), giving rise to results that are
difficult to interpret. In this thesis the focus is on the more orthodox
interpretation of the Fisher information matrix as a very meaningful and
well defined measure of information, and not as an approximation to a
posterior. In particular, we are not going to inverse the Fisher matrix
or marginalise over a set of parameters, except in some instances making
connections to results in the literature.

#### 1.3 @xmath-point functions

A very common class of observables, at the heart of this thesis, are the
@xmath -point functions, that we review briefly in this section. They
are very convenient at least for two reasons. First, for Gaussian fields
the mean and two-point function do contain the entire information in the
field : in the language of orthodox statistics, they form a set of
sufficient statistics. Second, the measurement of a (connected)
three-point or higher order point function directly tests for non
Gaussianity of the field.

##### 1.3.1 @xmath-point functions from the density : characteristic
functional

In cosmology, the @xmath -point function @xmath is defined as the
connected part of the @xmath -point moment of the fluctuation field.
These are most easily defined using the generating function technology,
ubiquitous in any field theory. Consider first an arbitrary @xmath
-point moment of the field,

  -- -------- -- --------
     @xmath      (1.12)
  -- -------- -- --------

We can write it, at least formally, as a derivative of the generating
functional @xmath , or characteristic functional , essentially the
Fourier transform of the density :

  -- -- -- --------
           (1.13)
  -- -- -- --------

with

  -- -------- -- --------
     @xmath      (1.14)
  -- -------- -- --------

In other words, the @xmath -point moments can be considered as the
successive terms in an expansion of the generating functional in a power
series in @xmath . Note that there are cases, such as for instance the
lognormal field, where the generating functional cannot be written as a
power series, even for @xmath very close to zero. In this case, the
series should be considered as a formal power series regardless of
convergence. The connected @xmath -point correlation functions are then
defined as the successive terms in the formal expansion of @xmath :

  -- -------- -- --------
     @xmath      (1.15)
  -- -------- -- --------

If all arguments @xmath to @xmath are identical, these connected point
functions become the familiar cumulants of the one dimensional density
function @xmath .
The connected point functions are convenient since they are additive for
uncorrelated fields. Indeed, if two fields are uncorrelated, then one
finds directly from its very definition ( 1.14 ) that the characteristic
function @xmath of the joint density is the product of the
characteristic functions of each of the densities. Taking the logarithm
and using the definition ( 1.15 ) shows that the connected functions
just add up. From these relations ( 1.13 ) and ( 1.15 ) one can infer
recursion relations for the connected point functions, as well as
convenient diagrammatic representations, Feynman diagrams alike, where
connected point functions are represented by connected graphs (
Bernardeau et al. , 2002 ; Szapudi , 2005 , e.g.) .
It holds that the very first connected point functions are identical to
the first moments of the delta field,

  -- -------- -- --------
     @xmath      (1.16)
  -- -------- -- --------

but this is not the case anymore for higher @xmath .
From homogeneity and isotropy, these functions are invariant under
translations and rotations. In particular the two-point function is a
function of a single argument,

  -- -------- -- --------
     @xmath      (1.17)
  -- -------- -- --------

Translation invariance allows conveniently the use of a description in
terms of harmonics. In Cartesian space, with Fourier transform

  -- -------- -- --------
     @xmath      (1.18)
  -- -------- -- --------

we have for any statistically homogeneous field the simple relation

  -- -------- -- --------
     @xmath      (1.19)
  -- -------- -- --------

where @xmath is the Dirac @xmath function and @xmath , the power
spectrum, is the Fourier transform of the two-point function

  -- -------- -- --------
     @xmath      (1.20)
  -- -------- -- --------

Is the field further statistically isotropic, the spectrum is only a
function of the modulus @xmath of the wavenumber. Similarly, one can
define higher order spectra, the polyspectra, prominently the bispectrum
for @xmath and trispectrum for @xmath through the Fourier transforms of
the connected @xmath -point functions, or equivalently the expectation
of products of the Fourier modes of the field.
In this thesis, the distinction between connected and disconnected point
functions, or the use of polyspectra rather than the @xmath -point
functions, are of no fundamental relevance, as they provide equivalent
descriptions of the same source of information. We will not make a
difference between a connected or disconnected point function. We regard
a generic @xmath -point moment

  -- -------- -- --------
     @xmath      (1.21)
  -- -------- -- --------

as a @xmath -point function.
The prime example of a homogeneous isotropic random fluctuation field is
of course the Gaussian field. Gaussian fields are very convenient for
many reasons. They are stable under any linear transformations, such as
smoothing, and also under convolutions. The celebrated central limit
theorem states that sums of a large number of independent variables tend
to have a Gaussian distribution under fairly generic conditions.
Besides, they also arise as fields of maximum information entropy for a
given two-point function. The Gaussian field is defined through

  -- -------- -- --------
     @xmath      (1.22)
  -- -------- -- --------

or, in Fourier space,

  -- -------- -- --------
     @xmath      (1.23)
  -- -------- -- --------

The second representation shows that the Fourier modes of such a field
are independent complex Gaussian variables with the correlations as
given in ( 1.19 ). All finite @xmath -dimensional joint densities are
@xmath -dimensional multivariate Gaussian distributions.
The characteristic functional can be evaluated in closed form. It is a
standard result called the Gaussian integral.

  -- -------- -- --------
     @xmath      (1.24)
  -- -------- -- --------

It follows that @xmath is a polynomial second order in @xmath . It is
then immediate that the connected point functions of the Gaussian field
vanish for @xmath , since the derivatives of that order do vanish.

##### 1.3.2 The density from @xmath-point functions : determinacy of the
moment problem

A key to several results of this thesis is the so-called moment problem
and its determinacy . These are respectively the problem of finding a
density given the hierarchy of @xmath -point moments, and the question
of whether a solution is unique or not. While not part of the usual
cosmological literature, this topic is a well known area of research of
mathematics, in particular for one dimensional densities ( Akhiezer ,
1965 ; Simon , 1997 ) . For such one dimensional densities examples of
different distributions with the same moment series have been known for
more than a century ( Stieltjes , 1894 ; Heyde , 1963 ) .
It is not uncommonly argued in the cosmological literature that the
relation between the moments, the characteristic functional and the
density function can be inverted, suggesting that the density is always
uniquely set by the @xmath -point moments ( Fry , 1985 ; Mo et al. ,
2010 , e.g.) . It is important to keep in mind that this holds only when
the characteristic functional can be written as a convergent power
series in the moments in a region around @xmath . As already mentioned
this is not always true, in which case the characteristic functional
cannot be expressed in terms of @xmath -point moments. However, it is
true that the mapping between the characteristic functional and the
density is one to one. The indeterminacy of the moment problem was
touched upon in a cosmological context in Coles and Jones ( 1991 ) ,
though it did not attract much attention in the cosmological literature
since then.
Obviously, an indeterminate moment problem is relevant for our purposes
as it implies that the entire @xmath -point function hierarchy contains
less information than the density itself. In that case, the entire
@xmath -point hierarchy is an inefficient set of observables. Namely, it
is impossible to reconstruct uniquely the density from the hierarchy.
The implications for cosmological parameter inference are discussed in
several chapters of this thesis, notably in chapter 6 , where to the
best of our knowledge first explicit examples of densities of any
dimensionality with identical @xmath -point moments at all orders are
presented.
The Gaussian field is an example of a density that can be uniquely
recovered from the @xmath -point moment hierarchy. On the other hand,
the lognormal field, first introduced later in 1.4 is an example where
this is not possible.

##### 1.3.3 @xmath-point functions from discrete populations, poisson
samples

In galaxy or weak lensing surveys, the fields that are observed are
rather discrete than continuous. Namely positions of galaxies are
recorded, and additional information such as the distortion of galaxy
images can be effectively measured only on these positions. Discreteness
adds some complexity. We are not able to predict galaxy positions in the
sky, and this discrete field may not trace in an obvious manner the
underlying, interesting field, typically the dark matter density field
or its projection. Rather, they are only tracers that can be biased in
several ways, and the measured @xmath -point functions need not always
be representative of that of the underlying field.
There is no unique manner to create a point process from a continuous
random field, but for density fields the infinitesimal Poisson model is
rather natural and gives a direct interpretation of the point functions.
Within this prescription, one divides the total volume in infinitesimal
cells and simply set the probability for a point in a cell to be
proportional to the value of the continuous field @xmath at that point,
and this independently from cells to cells. In that case, given a total
number of @xmath objects in a total volume @xmath , the probability
density to find these at @xmath conditional on @xmath is by definition

  -- -------- -- --------
     @xmath      (1.25)
  -- -------- -- --------

Note that the normalisation @xmath must require ergodicity, in order to
be able to identify @xmath with @xmath . This condition implies that for
any @xmath the following link between @xmath -point functions must hold

  -- -------- -- --------
     @xmath      (1.26)
  -- -------- -- --------

which is a very non trivial condition on a density function, requiring
in fact the zeroth mode of the field, @xmath to be actually no random
variable but a usual number. For a Gaussian field, this is equivalent to
require the condition on the spectrum @xmath and thus presents no
difficulty for any volume @xmath . On the other hand, as we will discuss
in chapter 4 , a lognormal field never has this property fulfilled
exactly in a finite volume, since its power at zero must be strictly
positive.
We find the probability density @xmath , unconditional to @xmath , to
find these objects at these positions by marginalising over the unseen
underlying field @xmath ,

  -- -------- -- --------
     @xmath      (1.27)
  -- -------- -- --------

From ( 1.27 ) we find a direct interpretation of the two-point function.
From the rule of probability theory we find that the probability density
of observing an object at @xmath given that there is one at @xmath is
given by

  -- -------- -- --------
     @xmath      (1.28)
  -- -------- -- --------

Thus for such processes the connected two-point function describes
directly the clustering of the points, by enhancing or reducing this
conditional probability to find particles separated by some distance (
Peebles , 1980 ; Bernardeau et al. , 2002 , e.g.) .

#### 1.4 Gaussian and non-Gaussian matter density fields

In cosmology, the Gaussian field ( 1.22 ) is fundamental. It is used
routinely in order to describe the statistics of the small fluctuations
present in the early Universe that we observe in the CMB radiation, or
more importantly for us that of the density we observe on the largest
scales. While it is possible to treat this working hypothesis as an
ad-hoc assumption adopted for convenience or lack of a better
prescription, it has now some theoretical support as well, in that the
simplest model of inflation predict initial conditions that are
extremely close to Gaussian ( Liddle and Lyth , 2000 ) . It is
fortunately possible to see where this comes about without entering any
details : in such models a nearly free scalar field is responsible for
the rapid expansion of the Universe, and its fluctuations give rise to
the primordial deviations from homogeneity. Since the action of a free
field is quadratic in the field, and since the action plays the same
role in a quantum field theory as @xmath in a statistical field theory,
we see that it corresponds to a Gaussian field.
Nevertheless, it should be noted that in the particular case of the
matter density or fluctuation field relevant for galaxy surveys, the
assumption of Gaussianity is in fact flawed from the very beginning.
This is because the matter density is positive, while the Gaussian
assumption assigns non-zero probability density to negative values. As
long as the variance is small, this is however not an essential
shortcoming of the model.
Of course, there are many situations where non-Gaussian statistics play
a major role, even in the noise-free fields. For instance, signatures
from non-Gaussianities in the primordial fluctuations can be used to try
and constrain more sophisticated inflationary models. This is often
dubbed as primordial non-Gaussianity. In this thesis we are going to
deal with the non-Gaussianity sourced from the nonlinear evolution of
the density field. We already mentioned that Gaussian statistics cannot
provide a perfect description of a density field, and this is even more
true as nonlinear evolution take place. This is illustrated in figure
1.2 , from A. Pillepich ( Pillepich et al. , 2010 ) . The four panels on
the left show the evolution of the matter density field in a @xmath
-body simulation, from redshift 50 to redshift 0 downwards. The inbox
inside each panel shows the one-point probability density @xmath of the
matter fluctuation field as the dark line, together with that of the
previous panel in grey. As the fluctuations grows, one observes the
field to develop large tails in the overdense regions, and a cutoff in
the underdense regions. The right panels are the same simulations where
an amount of primordial non Gaussianity of the local type roughly 10
times larger than current observational constraints ( Komatsu et al. ,
2011 ) was added, represented as the blue line in the inboxes. It is
obvious that the non Gaussianity induced by the formation of structures
is both very strong and completely dominant over the primordial
non-Gaussianity in the late Universe.

More to the point in the case of the density field is the assumption of
a lognormal field ( Coles and Jones , 1991 ) , where essentially the
logarithm @xmath is set to be a Gaussian field. Since @xmath is very
close to @xmath for small fluctuations, these two fields are
indistinguishable for any practical purposes as long as the variance is
small. The lognormal is however always positive definite, correcting for
the defect of the Gaussian prescription. In the nonlinear regime, it
also shows large tails and a cutoff in the underdense regions,
reproducing the qualitative features the one-point distribution of the
fluctuation field remarkably well given the simplicity of the
prescription.
Let us illustrate these aspects in figure 1.1 with the help of
perturbation theory. As the dotted line is shown the prediction of
second order perturbation theory for the distribution function,
calculated with the same methods as Taylor and Watts ( 2000 ) , assuming
Gaussian initial conditions for @xmath , at a variance of @xmath . We
see that the assumption of Gaussian initial conditions gives rise to a
nonsensical non zero probability density for negative matter density. On
the other hand, as expected, it predicts a large tail in the overdense
regions. The dashed line shows the probability density obtained from the
same type of perturbative calculations for @xmath , and the solid line
the lognormal distribution. It is rather remarkable how both the tails
and the underdense regions of the lognormal are reproduced with these
two perturbative calculations. The agreement between the lognormal
prescription and the matter fluctuations measured in the @xmath -body
simulations is also seen to be very good, see Taylor and Watts ( 2000 )
.
We can see from the apparition of a long tail and a sharp cutoff in the
density fluctuations that the statistics of the nonlinear regime are not
going to obey the same rules as that of the linear Gaussian field. For
instance, it is worth mentioning at this point that the traditional
observable the spectrum of the field, that contains the entire
information in the field in the Gaussian regime, seems to lose most of
its advantages leaving the nonlinear regime. In particular, heavy
correlations appear between the Fourier modes, and little information
can apparently be extracted from the spectrum on these scales ( Rimes
and Hamilton , 2005 ; Neyrinck et al. , 2006 ; Lee and Pen , 2008 ) .
The study of the information within the lognormal field will form a
large part of this thesis, chapter 6 .

#### 1.5 Structure of the thesis

The thesis is built out of two parts. The first part, that includes the
first and second chapter, describes and builds upon the mathematical
tools that are used in later chapters. The second part, chapter 3 to
chapter 7, contains the cosmological research properly speaking. Each
chapter begins with a more detailed description of its content, as well
as the references to our corresponding publications when appropriate.
In chapter 2, we discuss two measures of information, Fisher’s
information matrix and Shannon’s information entropy, and the duality
between them. We comment extensively on the information inequality,
fundamental for much of this thesis. We discuss the information content
of maximal entropy distributions, and identify these distributions as
those for which the information inequality is an equality.
In chapter 3 we deal with the information content of @xmath -point
moments for a given density function. We present how to decompose the
Fisher information matrix in uncorrelated components, associated to the
@xmath -point moments of a given order, with the help of orthogonal
polynomials. The properties of this expansion are discussed. In
particular, it is shown that the hierarchy of @xmath -point moments does
not necessarily carry the entire Fisher information content of the
distribution for indeterminate moment problems, while the entire
information is recovered for determinate moment problems. We also
present several models for which we could obtain this expansion
explicitly at all orders.
Chapter 4 uses the tools introduced in chapter 2 in the context of weak
lensing. We show in this chapter how the information from different
probes of the weak lensing convergence field, the magnification, shears
and flexion fields, do combine in a very simple way. We then evaluate
their information content using current values for the dispersion
parameters and discuss the benefits of their combination.
Chapter 5 is a little note on the use of Gaussian distributions for the
statistics of estimators of second order statistics in cosmology. We
show that we can use these information-theoretic concepts to clarify
some issues in the literature and the signification of the parameter
dependence of the covariance matrices for two-point correlation
functions or power spectra.
Chapter 6 discusses the information content of @xmath -point moments in
the lognormal density field. It is the largest chapter in this thesis,
and contains our main results as well. After discussing some fundamental
limitations of the lognormal field to describe the matter density field
of the @xmath CDM universe, we present families of different fields all
having the same hierarchy of @xmath -point moments than the lognormal,
and discuss the implications for parameter inference. The expansion of
the Fisher information matrix introduced in chapter 2 is then performed
exactly at all orders in two simplified but tractable situations. This
then allows us to make successful connections with @xmath -body
simulation results on the extraction of power spectra
Finally, in chapter 7 we evaluate the information content of the moment
hierarchy of the one-point distribution of the weak lensing convergence
field, demonstrating that the nonlinearities generically lead to
distributions that are very poorly described by their moments. On the
other hand, it is shown that simple mappings are able to correct for
this deficiency.

## Part I Quantifying information

### Chapter 2 Shannon entropy and Fisher information

The primary aim of this first chapter is to introduce in a rather
detailed and comprehensive manner the tools that form the building
blocks of this thesis, which are Fisher’s matrix valued measure of
information, as well as Shannon’s measure of entropy. As such, unlike
the subsequent chapters, it does not contain exclusively original
material. Namely, the sections 2.1 and 2.2 , while important parts of
our publication Carron et al. ( 2011 ) can be considered to some extent
a review of our perspective on the properties of Fisher information and
Shannon entropy that are then built upon in the later parts of this
thesis.
This chapter is built as follows :
In section 2.1 , we introduce and discuss the main properties of the
Fisher information, with an emphasis on its information theoretic
properties. Essential to most of this thesis is the information
inequality, equation ( 2.21 ), and its consequences. In section 2.2 , we
discuss maximal entropy distributions associated to a prescribed set of
observables, and that these distributions are precisely those for which
the information inequality is an equality. We find with equation ( 2.44
) a measure of information content that depends only on the constraints
put on the data and the physical model, written in terms of the
curvature of Shannon’s entropy surface. We recover the Fisher
information matrices for Gaussian fields of common use in cosmology as
the special case of fields with prescribed two-point functions.
The text in these two sections is based to a large extent on the first
part of Carron et al. ( 2011 ) , with the exception of appendix 2.3.1 .

#### 2.1 Fisher information and the information inequality

We first review here a few simple points of interest that justify the
interpretation of the Fisher matrix as a measure of the information
content of an experiment. Let us begin by considering the case of a
single measurement @xmath , with different possible outcomes, or
realisations, @xmath , and our model has a single parameter @xmath . We
also assume that we have knowledge, prior to the given experiment, of
the probability density function @xmath , which depends on our parameter
@xmath , that gives the probability of observing particular realisations
for each value of the model parameter. The Fisher information, @xmath ,
in @xmath on @xmath , is a non-negative scalar in this one parameter
case. It is defined in a fully general way as a sum over all
realisations of the data ( Fisher , 1925 ) :

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

Three simple but important properties of Fisher information are worth
highlighting at this point.

-   The first is that @xmath is positive definite, and it vanishes if
    and only if the parameter @xmath does not impact the data, i.e. if
    the derivative of @xmath with respect to @xmath is zero for every
    realisation @xmath .

-   The second point is that it is invariant to invertible manipulations
    of the observed data. This can be seen by considering an invertible
    change of variable @xmath , which, due to the rules of probability
    theory can be expressed as

      -- -------- -- -------
         @xmath      (2.2)
      -- -------- -- -------

    Thus

      -- -------- -- -------
         @xmath      (2.3)
      -- -------- -- -------

    leading to the simple equivalence that @xmath . On the other hand,
    information may be lost when the transformation is not unique in
    both directions. For instance, if the data is combined to produce a
    new variable that could arise from different sets of data points.
    This is only the statement that manipulations of the data leads, at
    best, only to conservation of the information.

-   The third point is that information from independent experiments add
    together. Indeed, if two experiments with data @xmath and @xmath are
    independent, then the joint probability density factorises,

      -- -------- -- -------
         @xmath      (2.4)
      -- -------- -- -------

    and it is easy to show that the joint information in the
    observations decouples,

      -- -------- -- -------
         @xmath      (2.5)
      -- -------- -- -------

These three properties satisfy what we might intuitively expect from a
mathematical implementation of an abstract concept such as information.
However, we can ask the reverse question and try to find an alternative
measure of information that may be better suited, for a particular
purpose, than the Fisher information. In appendix 2.3.1 , we discuss to
what extent the information measure in ( 2.1 ) is in fact uniquely set
by the these requirements above.
These properties are making the Fisher information a meaningful measure
of information. This is independent of its interpretation as providing
error bars on parameters. It further implies that once a physical model
is specified with a given set of parameters, a given experiment has a
definite information content that can only decrease with data
processing.

##### 2.1.1 The case of a single observable

To quantify the last point above, and in order to get an understanding
of the structure of the information in a data set, we first discuss a
simple situation, common in cosmology, where the extraction of the model
parameter @xmath from the data goes through the intermediate step of
estimating a particular observable, @xmath , from the data, @xmath ,
with the help of which @xmath will be inferred. A typical example could
be, from the temperature map of the CMB ( @xmath ), the measurement of
the power spectra of the fluctuations ( @xmath ), from which a
cosmological parameter ( @xmath ) is extracted. The observable @xmath is
measured from @xmath with the help of an estimator, that we call @xmath
, and that we will take as unbiased. This means that its mean value, as
would be obtained for instance if many realizations of the data were
available, converges to the actual value that we want to compare with
the model prediction,

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

A measure for its deviations from sample to sample, or the uncertainty
in the actual measurement, is then given by the variance of @xmath ,
defined as

  -- -- -- -------
           (2.7)
  -- -- -- -------

In such a situation, a major role is played by the so-called Cramér-Rao
inequality ( Rao ( 1973 ) ), that links the Fisher information content
of the data to the variance of the estimator, stating that

  -- -------- -- -------
     @xmath      (2.8)
  -- -------- -- -------

This equation holds for any such estimator @xmath and any model
parameter @xmath . Two different interpretations of this equation are
possible:

The first bounds the variance of @xmath by the inverse of the Fisher
information. To see this, we consider the special case of the model
parameter @xmath being @xmath itself. Although we are making in general
a conceptual distinction between the observable @xmath and the model
parameter @xmath , nothing requires us from doing so. Since @xmath is
now equal to @xmath , the derivative on the right hand side becomes
unity, and one obtains

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

The variance of any unbiased estimator @xmath of @xmath is therefore
bounded by the inverse of the amount of information @xmath the data
possess on @xmath . If @xmath is known it gives a useful lower limit on
the error bars that the analysis of the data can put on this observable.
We emphasise at this point that this bound only holds in the case of
unbiased estimators. There are cosmologically relevant situations where
biased estimators can go beyond this level, and thus perform better
according to the minimal squared error criterion than any unbiased one.
The second reading of the Cramér-Rao inequality, closer in spirit to the
present thesis, is to look at how information is lost by constructing
the observable @xmath , and discarding the rest of the data set. For
this, we rewrite trivially equation ( 2.8 ) as

  -- -------- -- --------
     @xmath      (2.10)
  -- -------- -- --------

The expression on the right hand side is the ratio of the sensitivity of
the observable to the model parameter @xmath , to the accuracy with
which the observable can be extracted from the data, @xmath . One of the
conceivable approaches in order to estimate the true value of the
parameter @xmath , is to perform a @xmath fit to the measured value of
@xmath . It is simple to show that this ratio, evaluated at the best fit
value, is in fact proportional to the expected value of the curvature of
@xmath at this value. Since the curvature of the @xmath surface
describes how fast the value of the @xmath is increasing when moving
away from the best fit value, its inverse may be interpreted as an
approximation to the error estimate that the analysis with the help of
@xmath will put on @xmath .
Thus, equation ( 2.10 ) shows that by only considering @xmath and not
the full data set, we may have lost information on @xmath , a loss given
by the difference between the left and right hand side of that equation.
While the latter may be interpreted as the information on @xmath
contained in the part of the data represented by @xmath , we may have
lost trace of any other source of information.
It should be noted that while we have just chosen to interpret the right
hand side of ( 2.10 ) as the information in @xmath , this is a slight
abuse of terminology. More rigorously, the Fisher information in @xmath
is not the right hand side of ( 2.10 ) but the Fisher information of its
density function

  -- -------- -- --------
     @xmath      (2.11)
  -- -------- -- --------

but it will always be clear in this thesis from the context which one is
meant. Anticipating the nomenclature of chapter 3 , the right hand side
of ( 2.10 ) is actually the information in the mean of @xmath . From
equation ( 2.10 ) we infer

  -- -------- -- --------
     @xmath      (2.12)
  -- -------- -- --------

##### 2.1.2 The general case

These considerations on the Cramér-Rao bound can be easily generalised
to the case of many parameters and many estimators of as many
observables. Still dealing with a measurement @xmath with outcomes
@xmath , we want to estimate a set of parameters

  -- -------- -- --------
     @xmath      (2.13)
  -- -------- -- --------

with the help of some vector of observables,

  -- -------- -- --------
     @xmath      (2.14)
  -- -------- -- --------

that are extracted from @xmath with the help of an array of unbiased
estimators,

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

In this multidimensional setting, all the three scalar quantities that
played a role in our discussion in section 2.1.1 , i.e. the variance of
the estimator, the derivative of the observable with respect to the
parameter, and the Fisher information, are now matrices.
The Fisher information @xmath in @xmath on the parameters @xmath is
defined as the square matrix

  -- -------- -- --------
     @xmath      (2.16)
  -- -------- -- --------

While the diagonal elements @xmath are the information scalars @xmath in
equation ( 2.1 ), the off diagonal ones describe correlated information.
The Fisher information matrix still carries the three properties we
discussed in section 2.1 .
The variance of the estimator in equation ( 2.7 ) now becomes the
covariance matrix @xmath of the estimators @xmath , defined as

  -- -------- -- --------
     @xmath      (2.17)
  -- -------- -- --------

Finally, the derivative of the observable with respect to the parameter,
in the right hand side of ( 2.8 ), becomes a matrix @xmath , in general
rectangular, defined as

  -- -------- -- --------
     @xmath      (2.18)
  -- -------- -- --------

where @xmath runs over all elements of the set @xmath of model
parameters. Again, the Cramér-Rao inequality provides a useful link
between these three matrices, and again there are two approaches to that
equation : first, as usually presented in the literature ( Rao , 1973 )
, in the form of a lower bound to the covariance matrix of the
estimators,

  -- -------- -- --------
     @xmath      (2.19)
  -- -------- -- --------

The inequality between two symmetric matrices @xmath having the meaning
that the matrix @xmath is positive definite. ¹ ¹ 1 A symmetric matrix
@xmath is called positive definite when for any vector @xmath holds that
@xmath . Further we say for such matrices that @xmath is larger than
@xmath , or @xmath , whenever @xmath . A concrete implication for our
purposes is e.g. that the diagonal entries of the left hand side of (
2.19 ) or ( 2.20 ), which are the individual variances of each estimator
@xmath , are greater than those of the right hand side. For many more
properties of positive definite matrices, see for instance ( Bhatia ,
2007 ) . If, as above, we consider the special case of identifying the
parameters with the observables themselves, the matrix @xmath is the
identity matrix, and so we obtain that the covariance of the vector of
the estimators is bounded by the inverse of the amount of Fisher
information that there is on the observables in the data,

  -- -------- -- --------
     @xmath      (2.20)
  -- -------- -- --------

Second, we can turn this lower bound on the covariance to a lower bound
on the amount of information in the data set as well. By rearranging
equation ( 2.19 ), we obtain the multidimensional analogue of equation (
2.10 ), the information inequality, which describes the loss of
information that occurs when the data is reduced to a set of estimators,

  -- -- -- --------
           (2.21)
  -- -- -- --------

This information inequality is a central piece to much of this thesis. A
proof can be found in the appendix. A maybe simpler proof follows also
for instance from the discussion in the appendix of chapter 3 .
Instead of giving a useful lower bound to the covariance of the
estimator as in the Cramér-Rao inequality, equation ( 2.19 ), the
information inequality makes clear how information is in general lost
when reducing the data to any particular set of estimators. The right
hand side may be seen, as before, as the expected curvature of a @xmath
fit to the estimates produced by the estimators @xmath , when evaluated
at the best fit value, with all correlations fully and consistently
taken into account. Note that as before the right hand side of the
information inequality is not the Fisher information content of the
joint probability density function of the estimators, but only that of
their means.
In section 2.2 , we discuss how Jaynes’ Maximal Entropy Principle allow
us to understand the total information content of a data set, once a
model is specified, in very similar terms.

##### 2.1.3 Resolving the density function: Fisher information density

Due to its generality, the information inequality ( 2.21 ) is very
powerful. We now have a deeper look at a special case that sheds some
light on the definition of the Fisher information matrix ( 2.16 ), and
that we will use in part II .
Assuming that the variable @xmath is continuous and one dimensional,
pick a set of points @xmath with separation @xmath covering some range
@xmath of the variable, such that in the limit of a large number of
points we can write

  -- -------- -- --------
     @xmath      (2.22)
  -- -------- -- --------

Generalisation to discrete variables or multidimensional cases will be
obvious.
Define then a set of estimator @xmath as follows

  -- -------- -- --------
     @xmath      (2.23)
  -- -------- -- --------

These estimators simply build an histogram of the variable over @xmath .
In other words, our set of estimators are defined such the entire
density function is resolved over @xmath .
We want to evaluate the information inequality for this set of
estimators. We have

  -- -------- -- --------
     @xmath      (2.24)
  -- -------- -- --------

with covariance matrix

  -- -------- -- --------
     @xmath      (2.25)
  -- -------- -- --------

Define as @xmath the probability that a realisation of the variable does
not belong to @xmath :

  -- -------- -- --------
     @xmath      (2.26)
  -- -------- -- --------

It is then easily seen that the inverse covariance matrix is

  -- -------- -- --------
     @xmath      (2.27)
  -- -------- -- --------

It follows that the right hand side of the information inequality
becomes

  -- -- -- --------
           (2.28)
  -- -- -- --------

This is nothing else than

  -- -------- -- --------
     @xmath      (2.29)
  -- -------- -- --------

If @xmath covers the full range of the variable, then the first term is
precisely the total Fisher information matrix, and the second vanishes,
since @xmath . It is clear from ( 2.29 ) that we can interpret

  -- -------- -- --------
     @xmath      (2.30)
  -- -------- -- --------

as a Fisher information density, representing information from
observations of the variables around @xmath . The additional term in (
2.29 ) involving @xmath originates from the fact the the density is
normalised to unity : observations of the density over the range @xmath
provides some information on the density in the complement to @xmath ,

  -- -------- -- --------
     @xmath      (2.31)
  -- -------- -- --------

However, it is not possible to resolve the individual contributions of
@xmath to @xmath for each @xmath on the complement of @xmath , and thus
the derivatives act in this case outside of the integrals, unlike the
first term in ( 2.29 ).

#### 2.2 Jaynes Maximal Entropy Principle

In cosmology, the knowledge of the probability distribution of the data
as function of the parameters, @xmath , which is compulsory in order to
evaluate its Fisher information content, is usually very limited. In a
galaxy survey, a data outcome @xmath would be typically the full set of
angular positions of the galaxies, together with some redshift
estimation if available, to which we may add any other kind of
information, such as luminosities, shapes, etc. Our ignorance of both
initial conditions and of many relevant physical processes does not
allow us to predict either galaxy positions in the sky, or all
interconnections with all this additional information. Our predictions
of the shape of @xmath is thus limited to some statistical properties,
that are sensitive to the model parameters @xmath , such as the mean
density over some large volume, or certain types of correlation
functions.
In fact, even if it were possible to devise some procedure in order to
get the exact form of @xmath , it may eventually turn out to be useless,
or even undesirable, to do so. The incredibly large number of degrees of
freedom of such a function is very likely to overwhelm the analyst with
a mass of irrelevant details, which may have no relevant significance on
their own, or improve the analysis in any meaningful way.
These arguments call for a kind a thermodynamical approach, which would
try and capture those aspects of the data which are relevant to our
purposes, reducing the number of degrees of freedom in a drastic way.
Such an approach already exists in the field of probability theory (
Jaynes , 1957 ) . It is based on Shannon’s concept of entropy of a
probability distribution ( Shannon , 1948 ) and did shed new light on
the connection between probability theory and statistical mechanics.
As we have just argued, our predictive knowledge of @xmath is limited to
some statistical properties. Let us formalise this mathematically, in a
similar way as in section 2.1.2 . Astrophysical theory gives us a set of
constraints on the shape of @xmath , in the form of averages of some
functions @xmath ,

  -- -------- -- --------
     @xmath      (2.32)
  -- -------- -- --------

where @xmath enters through the angle brackets. As an example, suppose
the data outcome @xmath is a map of the matter density field as a
function of position. In this case, one of these constraints @xmath
could be the mean of the field or its power spectrum, as given by some
cosmological model.
The role of this array @xmath is to represent faithfully the physical
understanding we have of @xmath , according to the model, as a function
of the model parameters @xmath . In the ideal case, some way can be
devised to extract each one of these quantities @xmath from the data and
to confront them to theory. The set of observables @xmath , that we used
in section 2.1.2 , would be a subset of these predictions @xmath , and
we henceforth refer to @xmath as the ’constraints’.
Although @xmath must satisfy the constraints ( 2.32 ), there may still
be a very large number of different distributions compatible with these.
However, a very special status among these distributions has the one
which maximises the value of Shannon’s entropy ² ² 2 Formally, for
continuous distributions the reference to another distribution is needed
to render S invariant with respect to invertible transformations,
leading to the concept of the entropy of @xmath relative to another
distribution @xmath , @xmath , also called Kullback-Leibler divergence.
The quantity defined in the text is more precisely the entropy of @xmath
relative to a uniform probability density function. For an recent
account on this, close in spirit to this work, see Caticha ( 2008 ) . ,
defined as

  -- -------- -- --------
     @xmath      (2.33)
  -- -------- -- --------

First introduced by Shannon ( Shannon , 1948 ) as a measure of the
uncertainty in a distribution on the actual outcome, Shannon’s entropy
is now the cornerstone of information theory. Jaynes’ Maximal Entropy
Principle states that the @xmath for which this measure @xmath is
maximal is the one that best deals with our insufficient knowledge of
the distribution, and should be therefore preferred. We refer the reader
to Jaynes’ work ( Jaynes , 1983 ; Jaynes and Bretthorst , 2003 ) and to
Caticha ( 2008 ) for detailed discussions of the role of entropy in
probability theory and for the conceptual basis of maximal entropy
methods. Astronomical applications related to some extent to Jaynes’s
ideas include image reconstruction from noisy data, (see e.g. Skilling
and Bryan ( 1984 ); Starck and Pantin ( 1996 ); Maisinger et al. ( 2004
) and references therein) , mass profiles reconstruction from shear
estimates ( Bridle et al. , 1998 ; Marshall et al. , 2002 ) , as well as
model comparison when very few data is available ( Zunckel and Trotta ,
2007 ) . We will see that for our purposes as well it provides us a
powerful tool, and that the Maximal Entropy Principle is the ideal
complement to Fisher information, fitting very well within our
discussions in section 2.1 on the information inequality.
Intuitively, the entropy @xmath of @xmath tells us how sharply
constrained the possible outcomes @xmath are, and Jaynes’ Maximal
Entropy Principle selects the @xmath which is as wide as possible, but
at the same time consistent with the constraints ( 2.32 ) that we put on
it. The actual maximal value attained by the entropy @xmath , among all
the possible distributions which satisfy ( 2.32 ), is a function of the
constraints @xmath , which we denote by

  -- -------- -- --------
     @xmath      (2.34)
  -- -------- -- --------

Of course it is a function of the model parameters @xmath as well, since
they enter the constraints. As we will see, the shape of that surface as
a function of @xmath , and thus implicitly as a function of @xmath , is
the key point in understanding the Fisher information content of the
data. In the following, in order to keep the notation simple, we will
omit the dependency on @xmath of most of our expressions, though it will
always be implicit.
The problem of finding the distribution @xmath that maximises the
entropy ( 2.33 ), while satisfying the set of constraints ( 2.32 ), is
an optimization exercise. We can quote the end result ( Jaynes , 1983 ,
chap. 11) , ( Caticha , 2008 , chap. 4) :
The probability density function @xmath , when it exists, has the
following exponential form,

  -- -------- -- --------
     @xmath      (2.35)
  -- -------- -- --------

in which to each constraint @xmath is associated a conjugate quantity
@xmath , that arises formally as a Lagrange multiplier in this
optimization problem with constraints. The conjugate variables @xmath ’s
are also called ’potentials’, terminology that we will adopt in the
following. We will see below in equation ( 2.39 ) that the potentials
have a clear interpretation, in the sense that the each potential @xmath
quantifies how sensitive is the entropy function @xmath in ( 2.34 ) to
its associated constraint @xmath . The quantity @xmath , that plays the
role of the normalisation factor, is called the partition function.
Since equation ( 2.35 ) must integrate to unity, the explicit form of
the partition function is

  -- -------- -- --------
     @xmath      (2.36)
  -- -------- -- --------

The actual values of the potentials are set by the constraints ( 2.32 ).
They reduce namely, in terms of the partition function, to a system of
equations to solve for the potentials,

  -- -------- -- --------
     @xmath      (2.37)
  -- -------- -- --------

The partition function @xmath is closely related to the entropy @xmath
of @xmath . It is simple to show that the following relation holds,

  -- -------- -- --------
     @xmath      (2.38)
  -- -------- -- --------

and the values of the potentials can be explicitly written as function
of the entropy, in a relation mirroring equation ( 2.37 ),

  -- -------- -- --------
     @xmath      (2.39)
  -- -------- -- --------

Given the nomenclature, it is of no surprise that a deep analogy between
this formalism and statistical physics does exist. Just as the entropy,
or partition function, of a physical system determines the physics of
the system, the statistical properties of these maximal entropy
distributions follow from the functional form of the Shannon entropy or
its partition function as a function of the constraints. For instance,
the covariance matrix of the constraints is given by

  -- -- -- --------
           (2.40)
  -- -- -- --------

In statistical physics the constraints can be the mean energy, the
volume or the mean particle number, with potentials being the
temperature, the pressure and the chemical potential. We refer to Jaynes
( 1957 ) for the connection to the physical concept of entropy in
thermodynamics and statistical physics.

##### 2.2.1 Information in maximal entropy distributions

With our choice of probabilities @xmath given by equation ( 2.35 ), the
amount of Fisher information on the parameters @xmath of the model can
be evaluated in a straightforward way. The dependence on the model goes
through the constraints, or, equivalently, through their associated
potentials. It holds therefore that

  -- -------- -- --------
     @xmath      (2.41)
  -- -------- -- --------

where the second line follows from the first after application of the
chain rule and equation ( 2.37 ). Using the covariance matrix of the
constraints given in ( 2.40 ), the Fisher information matrix, defined in
( 2.16 ), can then be written as a double sum over the potentials,

  -- -------- -- --------
     @xmath      (2.42)
  -- -------- -- --------

There are several ways to rewrite this expression as a function of the
constraints and/or their potentials. First, it can be written as a
single sum by using equation ( 2.37 ) as

  -- -------- -- --------
     @xmath      (2.43)
  -- -------- -- --------

Alternatively, since we will be more interested in using the constraints
as the main variables, and not the potentials, we can show, using
equation ( 2.39 ), that it also takes the form ³ ³ 3 We note that this
result is valid only for maximal entropy distributions and is not
equivalent to the second derivative of the entropy with respect to the
parameters themselves. However it is formally identical to the
corresponding expression for the information content of distributions
within the exponential family ( Jennrich and Moore , 1975 ) , or ( van
den Bos , 2007 , chapter 4) , once the curvature of the entropy surface
is identified with the generalized inverse of the covariance matrix.

  -- -------- -- --------
     @xmath      (2.44)
  -- -------- -- --------

We will use both of these last expressions in chapter 4 of this thesis.
Equation ( 2.44 ) presents the total amount of information on the model
parameters @xmath in the data @xmath , when the model predicts the set
of constraints @xmath . The amount of information is in the form of a
sum of the information contained in each constraint, with correlations
taken into account, as in the right hand side in equation ( 2.21 ). In
particular, it is a property of the maximal entropy distributions, that
if the constraints @xmath are not redundant, then it follows that the
curvature matrix of the entropy surface @xmath is invertible and is the
inverse of the covariance matrix @xmath between the observables. To see
this explicitly, consider the derivative of equation ( 2.37 ) with
respect to the potentials,

  -- -------- -- --------
     @xmath      (2.45)
  -- -------- -- --------

The inverse of the matrix on the left hand side, if it can be inverted,
is @xmath , which can be obtained taking the derivative of equation (
2.39 ), with the result

  -- -------- -- --------
     @xmath      (2.46)
  -- -------- -- --------

We have thus obtained in equation ( 2.44 ), combining Jaynes’ Maximal
Entropy Principle together with Fisher’s information, the exact
expression of the information inequality ( 2.21 ) for our full set of
constraints, but with an equality sign.
We see that the choice of maximal entropy probabilities is fair, in the
sense that all the Fisher information comes from what was forced upon
the probability density function, i.e. the constraints. No additional
Fisher information is added when these probabilities are chosen. In
fact, as shown in the appendix this requirement alone is enough to
single out the maximal entropy distributions, as being precisely those
for which the information inequality is an equality. This can be
understood in terms of sufficient statistics and goes back to Pitman and
Wishart ( 1936 ) and Kopman ( 1936 ) . For a discussion in the language
of the exponential family of distribution see Zografos and Ferentinos (
1994 ) .
In the special case that the model parameters are the constraints
themselves, we have

  -- -------- -- --------
     @xmath      (2.47)
  -- -------- -- --------

which means that the Fisher information on the model predictions
contained in the expected future data is directly given by the
sensitivity of their corresponding potential. Also, the application of
the Cramér-Rao inequality, in the form given in equation ( 2.20 ), to
any set of unbiased estimators of @xmath , shows that the best joint,
unbiased, reconstruction of @xmath is given by the inverse curvature of
the entropy surface @xmath , which is, as we have shown, @xmath .
We emphasise at this point that although the amount of information is
seen to be identical to the Fisher information in a Gaussian
distribution of the observables with the above correlations, nowhere in
our approach do we assume Gaussian properties. The distribution of the
constraints @xmath themselves is set by the maximal entropy distribution
of the data.

##### 2.2.2 Redundant observables

We have just seen that in the case of independent constraints, the
entropy of @xmath provides through equation ( 2.44 ) both the joint
information content of the data, as well as the inverse covariance
matrix between the observables. However, if the constraints put on the
distribution are redundant, the covariance matrix is not invertible, and
the curvature of the entropy surface cannot be inverted either. We show
however that in these cases, our equations for the Fisher information
content ( 2.42 , 2.43 , 2.44 ) are still fully consistent, dealing
automatically with redundant information to provide the correct answer.

An example of redundant information occurs trivially if one of the
functions @xmath can be written in terms of the others. For instance,
for galaxy survey data, the specification of the galaxy power spectrum
as an constraint, together with the mean number of galaxy pairs as
function of distance, and/or the two-points correlation function, which
are three equivalent descriptions of the same statistical property of
the data. Although the number of observables @xmath , and thus the
number of potentials, describing the maximal entropy distribution
greatly increases by doing so, it is clear that we should expect the
Fisher matrix to be unchanged, by adding such superfluous pieces of
information. A small calculation shows that the potentials adjust
themselves so that it is actually the case, meaning that this type of
redundant information is automatically discarded within this approach.
Therefore, we need not worry about the independency of the constraints
when evaluating the information content of the data, which will prove
convenient in some cases.

There is another, more relevant type of redundant information, that
allow us to understand better the role of the potentials. Consider that
we have some set of constraints @xmath , and that we obtain the
corresponding @xmath that maximises the entropy. This @xmath could then
be used to predict the value @xmath of the average some other function
@xmath , that is not contained in our set of predictions,

  -- -------- -- --------
     @xmath      (2.48)
  -- -------- -- --------

For instance, the maximal entropy distribution built with constraints on
the first @xmath moments of @xmath , will predict some particular value
for the @xmath -th moment, @xmath , that the model was unable to predict
by itself.
Suppose now some new theoretical work provides the shape of @xmath as a
function of the model parameters. This new constraint can thus now be
added to the previous set, and a new, updated @xmath is obtained by
maximising the entropy. There are two possibilities at this point :

-   It may occur that the value of @xmath as provided by the model is
    identical to the prediction by the maximal entropy distribution that
    was built without that constraint. Since the new constraint was
    automatically satisfied, the maximal entropy distribution satisfying
    the full set of @xmath constraints must be equal to the one
    satisfying the original set. From the equality of the two
    distributions, which are both of the form ( 2.35 ), it follows that
    the additional constraint must have vanishing associated potential,

      -- -------- -- --------
         @xmath      (2.49)
      -- -------- -- --------

    while the other potentials are pairwise identical. It follows
    immediately that the total information, as seen from equation ( 2.43
    ) is unaffected, and no information on the model parameters was
    gained by this additional prediction. A cosmological example would
    be to enforce on the distribution of some field, together with the
    two-points correlation function, fully disconnected higher order
    correlation functions. It is well known that the maximal entropy
    distribution with constrained two-points correlation function has a
    Gaussian shape, and that Gaussian distributions have disconnected
    points function at any order. No information is thus provided by
    these field moments of higher order in this case.
    This argument shows that, for a given set of original constraints
    and associated maximal entropy distribution, any function @xmath ,
    which was not contained in this set, with average @xmath , can be
    seen as being set to zero potential. Such @xmath ’s therefore do not
    contribute to the information.

-   More interesting is, of course, the case where this additional
    constraint differs from the predictions obtained from the original
    set @xmath . Suppose that there is a mismatch @xmath between the
    predictions of the maximal entropy distribution and the model. In
    this case, when updating @xmath to include this constraint, the
    potentials are changed by this new information, a change given to
    first order by

      -- -------- -- --------
         @xmath      (2.50)
      -- -------- -- --------

    and the amount of Fisher information changes accordingly. It is
    interesting to note that the entropy itself is invariant at this
    order. From equation ( 2.39 ) we have namely

      -- -------- -- --------
         @xmath      (2.51)
      -- -------- -- --------

    since the new constraint was originally at zero potential. The
    entropy is, therefore, stationary not only with respect to changes
    in the probability distribution function, but also with respect to
    the predictions its associated maximal entropy distribution makes on
    any other quantities.

Of course, although the formulae of this section are valid for any
model, it requires numerical work in order to get the partition function
and/or the entropy surface in a general situation.

##### 2.2.3 The entropy and Fisher information content of Gaussian
homogeneous fields

We now obtain the Shannon entropy of a family of fields when only the
two-point correlation function is the relevant constraint, that we will
use later in this thesis. It is easily obtained by a straightforward
generalisation of the finite dimensional multivariate case, where the
means and covariance matrix of the variables are known. It is well known
( Shannon , 1948 ) that the maximal entropy distribution is in this case
the multivariate Gaussian distribution. Denoting the constraints on
@xmath with the matrix @xmath and vector @xmath

  -- -- -- --------
           (2.52)
  -- -- -- --------

the associated potentials are given explicitly by the relations

  -- -------- -- --------
     @xmath      (2.53)
  -- -------- -- --------

where the matrix @xmath is the covariance matrix

  -- -------- -- --------
     @xmath      (2.54)
  -- -------- -- --------

The Shannon entropy is given by, up to some irrelevant additive
constant,

  -- -------- -- --------
     @xmath      (2.55)
  -- -------- -- --------

The fact that about half of the constraints are redundant, due to the
symmetry of the @xmath and @xmath matrices, is reflected by the fact
that the corresponding inverse correlation matrix in equation ( 2.44 ),

  -- -------- -- --------
     @xmath      (2.56)
  -- -------- -- --------

is not invertible as such if we considers all entries of the matrix
@xmath as constraints. Of course, this is not the case anymore if only
the independent entries of @xmath form the constraints.
Using the handy formalism of functional calculus, we can
straightforwardly extend the above relations to systems with infinite
degrees of freedom, i.e. fields, where means as well as the two-point
correlation functions are constrained. A realisation of the variable
@xmath is now a field, or a family of fields @xmath , taking values on
some @xmath -dimensional space. The expressions above in the
multivariate case all stays valid, with the understanding that
operations such as matrix multiplications have to be taken with respect
to the discrete indices as well as the continuous ones.
With the two-point correlation function and means

  -- -- -- --------
           (2.57)
  -- -- -- --------

we still have, up to an unimportant constant,

  -- -------- -- --------
     @xmath      (2.58)
  -- -------- -- --------

In n-dimensional Euclidean space, within a box of volume @xmath for a
family of homogeneous fields, it is simplest to work with the spectral
matrices. These are defined as

  -- -------- -- --------
     @xmath      (2.59)
  -- -------- -- --------

where the Fourier transforms of the fields are defined through

  -- -------- -- --------
     @xmath      (2.60)
  -- -------- -- --------

It is well known that these matrices provide an equivalent description
of the correlations, since the they form Fourier pairs with the
correlation functions

  -- -------- -- --------
     @xmath      (2.61)
  -- -------- -- --------

In this case, the entropy in equation ( 2.58 ) reduces, again discarding
irrelevant constants, to an uncorrelated sum over the modes,

  -- -------- -- --------
     @xmath      (2.62)
  -- -------- -- --------

which is the straightforward mutlidimensional version of ( Taylor and
Watts , 2001 , eq. 39) . Comparison with equation ( 2.55 ) shows the
well-known fact that the modes can be seen as Gaussian, uncorrelated and
complex variables with correlation matrices proportional to @xmath . All
modes have zero mean, except for the zero-mode, which, as seen from its
definition, is proportional to the mean of the field itself.
Accordingly, taking the appropriate derivatives, the potentials @xmath
associated to @xmath read

  -- -------- -- --------
     @xmath      (2.63)
  -- -------- -- --------

and those associated to the means @xmath ,

  -- -------- -- --------
     @xmath      (2.64)
  -- -------- -- --------

Note that although the spectral matrices are, in general, complex, they
are hermitian, so that the determinants are real. The amount of Fisher
information in the family of fields is easily obtained with the help of
equation ( 2.43 ) , with the familiar result

  -- -- -- --------
           (2.65)
  -- -- -- --------

with @xmath being the connected part of the spectral matrices,

  -- -------- -- --------
     @xmath      (2.66)
  -- -------- -- --------

These expressions are of course also valid for isotropic fields on the
sphere. With a decomposition in spherical harmonics, the sum runs over
the multipoles.
The Fisher matrices in common use in weak lensing or clustering can thus
all be seen as special cases of this approach, namely equation ( 2.65 ),
when knowledge of the statistical properties of the future data does not
go beyond the two-point statistics. Indeed, in the case that the model
does not predict the means, and knowing that for discrete fields the
spectral matrices, equation ( 2.59 ), carry a noise term due to the
finite number of galaxies, or, in the case of weak lensing, also due to
the intrinsic ellipticities of galaxies, the amount of information in (
2.65 ) is essentially identical to the standard expressions used to
predict the accuracy with which parameters will be extracted from
power-spectra analysis.
Of course, the maximal entropy approach, which tries to capture the
relevant properties of @xmath through a sophisticated guess, gives no
guaranties that its predictions are actually correct. Nevertheless, as
discussed in section 2.2.2 , it provides a systematic approach with
which to update the probability density function in case of improved
knowledge of the relevant physics.

#### 2.3 Appendix

In 2.3.1 , we look at what possible ’information densities’ do in fact
satisfy those conditions that we would like any measure of information
about the true value of a model parameter to possess. We then provide in
2.3.2 a unified derivation of the Cramér-Rao and information inequality
in the multidimensional case (following a similar argumentation than in
Rao ( 1973 ) ) and then show its relation to maximal entropy
distributions.

##### 2.3.1 Measures of information on a parameter

Denoting with @xmath the probability density function of some variable,
we look for functionals @xmath

  -- -------- -- --------
     @xmath      (2.67)
  -- -------- -- --------

such that our candidate of the measure of information on @xmath is given
by

  -- -------- -- --------
     @xmath      (2.68)
  -- -------- -- --------

and has the following properties :
C1: We would want our functional to be a regular function of two
arguments, one being the value of the probability function at @xmath
itself, and the second the value of its derivative with respect to the
same @xmath ,

  -- -------- -- --------
     @xmath      (2.69)
  -- -------- -- --------

While the main reason for this very strong requirement is simplicity, it
appears nonetheless reasonable to us. This condition simply reflects the
fact that the values of @xmath in regions far away from the true value
of @xmath should not provide much information, or that the information
is provided by the probabilities and the linear impact of @xmath around
the true value.
C2: The choice of coordinates must not carry information, i.e. we want

  -- -------- -- --------
     @xmath      (2.70)
  -- -------- -- --------

whenever @xmath is an invertible function. While this is automatic for
discrete probabilities, this is not the case for continuous
distributions.
C3: The information on @xmath from independent experiments should add

  -- -------- -- --------
     @xmath      (2.71)
  -- -------- -- --------

The last two are the key requirements for the interpretation of @xmath
as information. In combination with the first requirement, it leads to
the following result: up to a multiplicative constant, there is a unique
positive definite density satisfying these conditions. it reads

  -- -------- -- --------
     @xmath      (2.72)
  -- -------- -- --------

which is easily seen to be precisely the Fisher information measure, as
defined in equation ( 2.1 ).
From now on, in order to simplify the notation, we write the two
arguments @xmath and @xmath of @xmath as @xmath and @xmath . If we drop
the positive definite condition, the general solution is

  -- -------- -- --------
     @xmath      (2.73)
  -- -------- -- --------

for arbitrary constants @xmath . This result follows from the claim that
the most general smooth function satisfying @xmath and @xmath should be
of the form

  -- -------- -- --------
     @xmath      (2.74)
  -- -------- -- --------

for a set of arbitrary constants @xmath . The two last terms do not meet
@xmath and are thus to be discarded, while the first one is the only
positive definite among the three others. In order to see why this
holds, we first note, as can be checked by direct calculation, that in
this form @xmath fulfill the requirements @xmath and @xmath . We then
use extensively @xmath for particular instances of variables with
different distributions and derivatives, sketching the proof that these
criteria do not allow other functional forms.
We first notice that, since any probability density function must be
normalised to unity, for any value of the model parameter, the following
relations must hold for any variable @xmath ,

  -- -------- -- --------
     @xmath      (2.75)
  -- -------- -- --------

as well as

  -- -------- -- --------
     @xmath      (2.76)
  -- -------- -- --------

We now consider, beside an arbitrary distribution @xmath , the
distribution of some independent variable @xmath , given by @xmath , and
denote the derivatives with respect to @xmath associated to @xmath and
@xmath , as @xmath and @xmath ,

  -- -------- -- --------
     @xmath      (2.77)
  -- -------- -- --------

where the dependency on @xmath is omitted. The joint probability density
function of the independent variables @xmath and @xmath is given by the
product of @xmath with @xmath , with associated derivative

  -- -------- -- --------
     @xmath      (2.78)
  -- -------- -- --------

Condition @xmath states in these terms explicitly that for any such
@xmath such that ( 2.75 ) holds, and @xmath such that ( 2.76 ) holds,
the following relation, describing the additivity of information, must
be true,

  -- -------- -- --------
     @xmath      (2.79)
  -- -------- -- --------

With the help of this relation we can constrain the functional form of
@xmath .
Let us first pick a uniform probability density for the variable @xmath
,

  -- -------- -- --------
     @xmath      (2.80)
  -- -------- -- --------

It must hold, using @xmath , that

  -- -------- -- --------
     @xmath      (2.81)
  -- -------- -- --------

for any allowed @xmath . It can be shown, for instance by performing
variations with respect to @xmath , that any solution to this integral
equation must obey the relation

  -- -------- -- --------
     @xmath      (2.82)
  -- -------- -- --------

for some function @xmath , that satisfies

  -- -------- -- --------
     @xmath      (2.83)
  -- -------- -- --------

This must still hold as above for any @xmath and associated derivative
function @xmath , satisfying ( 2.75 ) and ( 2.76 ) respectively. Taking
a variation with respect to @xmath shows that the only solutions for
@xmath of this equation are

  -- -------- -- --------
     @xmath      (2.84)
  -- -------- -- --------

for some arbitrary constants @xmath and @xmath . Therefore, we have
constrained the full function @xmath to be of the form

  -- -------- -- --------
     @xmath      (2.85)
  -- -------- -- --------

for some unknown functions @xmath and @xmath . The first two terms are
full solutions of @xmath . With very similar methods, the two other
terms can be reduced to

  -- -------- -- --------
     @xmath      (2.86)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (2.87)
  -- -------- -- --------

In this form, all terms in equation ( 2.85 ) are consistent with @xmath
and we have thus proved our claim ( 2.74 ).

##### 2.3.2 Cramér-Rao bound and maximal entropy distributions

We denote the vector of model parameters of dimension @xmath with

  -- -------- -- --------
     @xmath      (2.88)
  -- -------- -- --------

and a vector of functions of dimension @xmath the estimators

  -- -------- -- --------
     @xmath      (2.89)
  -- -------- -- --------

with expectation values @xmath . In the following, we rely on Gram
matrices, whose elements are defined by scalar products. Namely, for a
set of vectors @xmath , the Gram matrix @xmath generated by this set of
vectors is defined as

  -- -------- -- --------
     @xmath      (2.90)
  -- -------- -- --------

Gram matrices are positive definite and have the same rank as the set of
vectors that generate them. Especially, if the vectors are linearly
independent, the Gram matrix is strictly positive definite and
invertible.
We adopt a vectorial notation for functions, writing scalar products
between vectors as

  -- -------- -- --------
     @xmath      (2.91)
  -- -------- -- --------

with @xmath being the probability density function of the variable
@xmath of interest. In this notation, both the Fisher information matrix
and covariance matrix are seen to be Gram matrices. We have namely that
the Fisher information matrix reads

  -- -------- -- --------
     @xmath      (2.92)
  -- -------- -- --------

while the covariance matrix of the estimators is

  -- -------- -- --------
     @xmath      (2.93)
  -- -------- -- --------

For simplicity and since it is sufficently generic for our purpose, we
will assume that both sets of vectors @xmath and @xmath are lineary
independent, so that both matrices can be inverted. Note that we also
have

  -- -------- -- --------
     @xmath      (2.94)
  -- -------- -- --------

The Gram matrix @xmath of dimension @xmath generated by the set of
vectors @xmath takes the form

  -- -------- -- --------
     @xmath      (2.95)
  -- -------- -- --------

and is also positive definite due to its very definition. It is
congruent to the matrix

  -- -------- -- --------
     @xmath      (2.96)
  -- -------- -- --------

with

  -- -------- -- --------
     @xmath      (2.97)
  -- -------- -- --------

Since two congruent matrices have the same number of positive, zero and
negative eigenvalues respectively and since both @xmath and @xmath are
positive, we can conclude that

  -- -------- -- --------
     @xmath      (2.98)
  -- -------- -- --------

which is the Cramér-Rao inequality. The lower bound on the amount of
information is seen from the fact that for any matrix written in block
form holds

  -- -------- -- --------
     @xmath      (2.99)
  -- -------- -- --------

and using the same congruence argument leads to the lower bound on
information

  -- -------- -- ---------
     @xmath      (2.100)
  -- -------- -- ---------

Assume now that we have a probability density function such that this
inequality is in fact an equality, i.e.

  -- -------- -- ---------
     @xmath      (2.101)
  -- -------- -- ---------

By the above argument, the Gram matrix generated by

  -- -------- -- ---------
     @xmath      (2.102)
  -- -------- -- ---------

is congruent to the matrix

  -- -------- -- ---------
     @xmath      (2.103)
  -- -------- -- ---------

and has rank @xmath . By assumption, the covariance matrix is
invertible, such that the set @xmath alone has rank @xmath . It implies
that each of the @xmath vector can be written as linear combination of
the @xmath vectors,

  -- -------- -- ---------
     @xmath      (2.104)
  -- -------- -- ---------

or, more explicitly,

  -- -------- -- ---------
     @xmath      (2.105)
  -- -------- -- ---------

where the key point is that the coefficients @xmath are independent of
@xmath . Integrating this equation, we obtain

  -- -------- -- ---------
     @xmath      (2.106)
  -- -------- -- ---------

for some functions @xmath and @xmath of the model parameters only, and a
function @xmath of @xmath only. We obtain thus

  -- -------- -- ---------
     @xmath      (2.107)
  -- -------- -- ---------

This is precisely the distribution that we obtain by maximising the
entropy relative to @xmath , while satisfying the constraints

  -- -------- -- ---------
     @xmath      (2.108)
  -- -------- -- ---------

Taking @xmath as the uniform distribution makes it identical with the
formula in equation ( 2.35 ).

### Chapter 3 Information within N-point moments

In this chapter we demonstrate how to decompose the Fisher information
matrix into components unambigously associated to independent
information from @xmath -point moments of each order. The general
approach to decompose the Fisher information matrix in uncorrelated
components according to an orthogonal system was briefly discussed in a
statistical journal in theorem 3.1 in Jarrett ( 1984 ) . It seems
however that this procedure was not given further attention. In this
chapter, similar ideas are taken further, dealing mostly with the system
of moments, where the associated orthogonal system are orthogonal
polynomials.
We start in section 3.1 with one dimensional variables. We will in a
first step define for a probability distribution @xmath , coefficients
@xmath which unambiguously represent the independent information content
of the moment of order @xmath on @xmath . These coefficients can then be
used to reconstruct the Fisher information matrix order by order. The
straight forward generalisation to any number of variables is performed
in 3.2 , or to any hierarchical system other than the @xmath -point
moments in the appendix. Properties of this expansion under the presence
of noise are discussed in 3.2.2 .
In section 3.3 , we then present this exact decomposition for a few
determinate probability density functions. We give in closed form these
coefficients for several common classes of distributions, spanning a
wide range of different situations. We solve these decomposition for the
normal, the gamma and the beta families of distributions, as well as for
an extended Poisson model.
We use this decomposition in 3.3.1 to approach the indeterminate moment
problem. Namely, the Fisher information content of a moments series can
be less than the information content of the probability density function
it originates from, if that function cannot be uniquely recovered from
the moment series. Therefore, this loss of information is also a useful
measure of the indeterminacy of the moment problem. After dealing with
the lognormal distribution, whose expansion we will present in greater
details in chapter 6 , we treat numerically the Weibull distribution and
strectched exponential. For each of those we compare the information
lost to the moment hierarchy to Stoyanov’s dissimilarity index of
associated Stieltjes classes ( Stoyanov , 2004 ) .

#### 3.1 One variable

Our starting point is the information inequality of chapter 2 : for any
set of unbiased estimators @xmath aimed at extracting the vector of
observables @xmath , the following inequality between positive definite
matrices holds,

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

where @xmath is the covariance matrix of the estimators,

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

and @xmath is the matrix of derivatives,

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

When the vector @xmath are the moments themselves, @xmath for @xmath ,
it is possible to rewrite the information inequality in a more
insightful form. Consider a set of polynomials @xmath , @xmath the
degree of the polynomial, orthogonal with respect to @xmath ,

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

where @xmath is some strictly positive number, equal to unity if the
polynomials are orthonormal. Since the normalisation of many common
families of orthogonal polynomials is not unity, we will at the expense
of extra notation keep track of the terms @xmath in the following. Of
course, the polynomials

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

are then orthonormal. We refer to Freud ( 1971 ) or Szegö ( 2003 ) for
the theory of orthogonal polynomials. The polynomial @xmath is always
set by the first @xmath moments, and is unique up to an overall sign,
which we set by requiring the coefficient of @xmath in @xmath (the
leading coefficient) to be positive. A simple way to build formally the
orthonormal polynomials with this sign convention is for instance to
apply the Gram-Schmidt orthonormalisation process to the set @xmath with
respect to the scalar product @xmath .
The key point for our purposes is to realise that the inverse covariance
matrix between the moments can be written ¹ ¹ 1 A proof can be found in
a slightly more general setting later in section 3.4.2 .

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

This identity allows us to express the right hand side of the
information inequality in the form of a sum of uncorrelated pieces :
using ( 3.6 ), it holds

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

where

  -- -------- -- -------
     @xmath      (3.8)
  -- -------- -- -------

The matrix @xmath is therefore the part of the Fisher information in the
@xmath th moment that was not contained in the moments of lower order.
These coefficients have a straightforward interpretation. They can be
namely written as

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

which we will take as the definition of @xmath . This can be seen from
expanding @xmath in terms of the matrix @xmath , and noting that @xmath
, recovering ( 3.8 ). In other words, @xmath is nothing else than the
component of the corresponding function @xmath (the score function)
parallel to the orthonormal polynomial of order @xmath .
It follows immediately from equation ( 3.9 ) that the Fisher information
content of the moments depends on how well the score functions can be
approximated through polynomials. With increasing @xmath , one expects
the score function to be better and better reproduced by the series

  -- -------- -- --------
     @xmath      (3.10)
  -- -------- -- --------

We see that the following inequality between positive matrices holds

  -- -- -- --------
           (3.11)
  -- -- -- --------

In particular, for any parameter @xmath holds that the residual to the
best approximation of the score function with polynomials is given by

  -- -------- -- --------
     @xmath      (3.12)
  -- -------- -- --------

where @xmath is the Fisher information on @xmath . The bits of Fisher
information that are absent from the set of moments @xmath to @xmath are
thus precisely the mean squared error of the fit of the score function
through polynomials throughout the range of @xmath .
We define for further reference the matrices

  -- -------- -- --------
     @xmath      (3.13)
  -- -------- -- --------

as well as

  -- -------- -- --------
     @xmath      (3.14)
  -- -------- -- --------

These are the matrices representing the independent information content
of the @xmath th moment and of the first @xmath moments respectively. By
construction holds

  -- -------- -- --------
     @xmath      (3.15)
  -- -------- -- --------

We stress that these matrices are strictly speaking associated to a
moment series rather than a density function. These matrices are namely
identical for different densities having the same moment series.
We see that if the score function is itself a polynomial, of degree
@xmath , for each values of the parameters, then only the first @xmath
of these coefficients are possibly non-zero. A finite number of moments
do catch all the Fisher information content of @xmath in this case. In
fact, the reverse statement is also true. This can be understood in the
framework of orthodox statistics : in this case @xmath is proportional
to the exponential of a polynomial with parameter dependent
coefficients, where a finite number of sufficient statistics exist
Pitman and Wishart ( 1936 ); Kopman ( 1936 ) . In the light of chapter 2
, these distributions are precisely those that maximise Shannon entropy
for fixed values of the first @xmath moments. We thus have

  -- -------- -- --------
     @xmath      (3.16)
  -- -------- -- --------

The ubiquitous example of this family being of course the Gaussian
distribution, for which @xmath .
More generally, a sufficient criterium for the moment hierarchy to
possess the same amount of Fisher information as the density function
itself, is that the moment problem associated to the moment series is
determinate , that is to say that the density can be uniquely recovered
from the moment series. In this case, indeed, by a well known theorem
due to M. Riesz, ( Riesz , 1923 ) , the orthogonal polynomials
associated to the density form a complete set of basis functions for
square integrable functions. Therefore, the square residual in equation
( 3.12 ) must go to zero for @xmath . We have in this case

  -- -------- -- --------
     @xmath      (3.17)
  -- -------- -- --------

Finally, for moment indeterminate density functions, for which different
density functions exist with the same moment series, we have in general
an inequality (again, an inequality between positive matrices, not
matrix element to matrix element)

  -- -------- -- --------
     @xmath      (3.18)
  -- -------- -- --------

The amplitude of the mismatch can vary very substantially from case to
case. In chapter 6 , we study extensively the lognormal field, for which
the inequality is always a strict inequality, but is a very strong
function of the variance of the field.

##### 3.1.1 The Christoffel-Darboux kernel as information on the density
itself

To conclude this section, it is interesting to understand the
interpretation of the orthogonal polynomials themselves, in this
information theoretic framework. To this aim, consider that the model
parameter of interests are the values of the density function
themselves, allowing thus complete freedom. In the following, we do not
require the density to be normalised to unity for convenience. Take for
simplicity discrete values @xmath , with associated probability density
@xmath and the set of model parameters being precisely the density, i.e.
@xmath . Straight calculation leads to the following expression for the
Fisher information matrix elements on the parameters @xmath , @xmath ,

  -- -------- -- --------
     @xmath      (3.19)
  -- -------- -- --------

On the other hand, we have @xmath , leading to

  -- -------- -- --------
     @xmath      (3.20)
  -- -------- -- --------

Therefore, the @xmath th orthogonal polynomial @xmath is the information
content of the @xmath th moment on the density function @xmath itself.
The matrices @xmath become the celebrated Christoffel-Darboux kernel
@xmath ( Simon , 2008 ) ,

  -- -------- -- --------
     @xmath      (3.21)
  -- -------- -- --------

We have from these relations that the information escaping the first
@xmath moments is given by

  -- -------- -- --------
     @xmath      (3.22)
  -- -------- -- --------

In accordance with our discussions above, this right hand side of ( 3.22
) is known to tend to zero as @xmath precisely for determinate moment
problem. This is can be seen from the fact that in this case, from Riesz
theorem, the reproducing property holds

  -- -------- -- --------
     @xmath      (3.23)
  -- -------- -- --------

for any function @xmath that is square summable with respect to @xmath .
This implies

  -- -------- -- --------
     @xmath      (3.24)
  -- -------- -- --------

for any such function and therefore

  -- -------- -- --------
     @xmath      (3.25)
  -- -------- -- --------

#### 3.2 Several variables

The general theory on the statistical power of moments exposed in
section 3.1 extends is a straightforward way to density functions of any
number of variables and @xmath -point moments. We first need a little
bit of notation. For a @xmath -dimensional variable @xmath taking values
@xmath , a multiindex

  -- -------- -- --------
     @xmath      (3.26)
  -- -------- -- --------

is a @xmath -dimensional vector of non negative integers. The order of
the multiindex is defined as

  -- -------- -- --------
     @xmath      (3.27)
  -- -------- -- --------

For a given order @xmath , there are @xmath different such multiindices
@xmath . We define further the notation @xmath as

  -- -------- -- --------
     @xmath      (3.28)
  -- -------- -- --------

With this notation in place, a moment of order @xmath is given by

  -- -------- -- --------
     @xmath      (3.29)
  -- -------- -- --------

and the covariance matrix between the moments is

  -- -------- -- --------
     @xmath      (3.30)
  -- -------- -- --------

In this notation, the decomposition of the information in independent
bits of order @xmath proceeds by strict analogy with the one dimensional
case. We refer to Dunkl and Xu ( 2001 ) for the general theory of
orthogonal polynomials in several variables. A main difference being
that at a fixed order @xmath there are not one but @xmath independent
orthogonal polynomials. This number is the same as the number of the
above multiindices of that order @xmath . Each multiindex defines namely
an independent monomial @xmath of that order. These polynomials are not
defined in an unique way. The orthogonality of the polynomials of same
order is not essential for our purposes, but requiring the following
condition is enough,

  -- -------- -- --------
     @xmath      (3.31)
  -- -------- -- --------

for some positive matrices @xmath , which replace the normalisation
@xmath in 3.1 . The component of the score function @xmath parallel to
the polynomial @xmath is

  -- -------- -- --------
     @xmath      (3.32)
  -- -------- -- --------

and the expansion of the score function in terms of these polynomials
reads

  -- -------- -- --------
     @xmath      (3.33)
  -- -------- -- --------

It will converge to the score function for @xmath if the set of
polynomials is complete, whereas it may not if not. We note that there
is some freedom in the definition ( 3.31 ). This freedom is that of the
choice of a basis in the vector space of polynomials of order @xmath
orthogonal to all polynomials of lower order. For this reason, @xmath
depends on the particular basis. However, the expansion ( 3.33 ) does
not, and so will not the information matrices at fixed order.
Writing now the orthogonal polynomials in terms of a triangular
transition matrix

  -- -------- -- --------
     @xmath      (3.34)
  -- -------- -- --------

we see that the information matrix of order @xmath ( 3.13 ) becomes

  -- -------- -- --------
     @xmath      (3.35)
  -- -------- -- --------

The strict analog of equation ( 3.14 ) holds for each @xmath ,

  -- -------- -- --------
     @xmath      (3.36)
  -- -------- -- --------

recovering the right hand side of the information inequality for all
moments of order up to @xmath . Just as before, the missing piece
between @xmath and @xmath is the least squared residual the
approximation of the score function through polynomials of order up to
@xmath .
The matrices @xmath and @xmath are easily seen to be invariant under
mappings @xmath , where @xmath is an invertible square matrix of size
@xmath and @xmath a @xmath dimensional vector, provided both are
parameter independent.
As a simple illustration, for the multivariate Gaussian distribution
with mean vector @xmath and covariance matrix @xmath we have

  -- -------- -- --------
     @xmath      (3.37)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (3.38)
  -- -------- -- --------

summing up to the total information, @xmath .

##### 3.2.1 Independent variables and uncorrelated fiducial

In general, it is a rather difficult problem to obtain explicit
expressions for the orthogonal polynomials or the matrices @xmath and
@xmath , especially in the case of several variables. Using exact though
formal expressions as starting point requires the evaluation of
determinants of moment matrices, and cases are rare when this is
tractable. For independent variables @xmath , however, a canonical
choice of @xmath can be written down in terms of the ones of associated
to one dimensional problem. More specifically, if the variables are
independent, then

  -- -------- -- --------
     @xmath      (3.39)
  -- -------- -- --------

where @xmath denotes the one dimensional probability density function of
the @xmath th variable. Define then the polynomial in @xmath variables
of order @xmath through

  -- -------- -- --------
     @xmath      (3.40)
  -- -------- -- --------

where @xmath is the orthonormal polynomial in one variable of order
@xmath with respect to @xmath . It is not difficult to see that for any
multiindices @xmath and @xmath the average over @xmath factorizes in
averages with respect to each variable @xmath . We have namely

  -- -------- -- --------
     @xmath      (3.41)
  -- -------- -- --------

The so defined polynomials are therefore orthogonal with matrices @xmath
being unit matrices. From equation ( 3.39 ) follows that the function
@xmath is the sum of the functions @xmath of the individual variables.
Therefore, using the above polynomials defined in ( 3.40 ), it is not
difficult to see that all the coefficients @xmath that couples different
variables vanishes, i.e. @xmath if @xmath has two or more non zero
indices. The information content of order @xmath becomes then simply the
sum of the information of order @xmath within each variable, as
expected. This is an manifestation of the additivity of Fisher
information for independent variables, which is thus seen to hold order
by order.
Note that polynomials also take this product form ( 3.40 ) if as above
the probability density function factorizes at the fiducial values of
the model parameters, but with the difference that there is no splitting
of the derivative functions @xmath as a uncorrelated sum. In this case,
the fiducial model is uncorrelated, but model parameters create
correlations away from their fiducial values. @xmath -point moments at
non zero lag may carry genuine information in this case. In mathematical
terms, all @xmath can possibly be non zero, depending on how the score
functions couple the different variables.

##### 3.2.2 On the impact of noise

Very often the observed random variable is the sum of two random
variables,

  -- -------- -- --------
     @xmath      (3.42)
  -- -------- -- --------

where @xmath is a @xmath -dimensional variable carrying the information
on the parameters @xmath , and @xmath is some additive noise. We set

  -- -------- -- --------
     @xmath      (3.43)
  -- -------- -- --------

to be the probability density function for @xmath , and

  -- -------- -- --------
     @xmath      (3.44)
  -- -------- -- --------

being that for @xmath , which is parameter independent. From the rules
of probability theory, we have

  -- -------- -- --------
     @xmath      (3.45)
  -- -------- -- --------

In this section, we prove the following, maybe rather intuitive fact,
valid for any functional form of @xmath and of @xmath , any model
parameters and any dimensionality @xmath :
For any @xmath , the following relation between positive matrices holds

  -- -------- -- --------
     @xmath      (3.46)
  -- -------- -- --------

On the other hand, this relation does not hold for the matrices @xmath .
For instance, a non Gaussian noise @xmath on a Gaussian signal @xmath
will create in general third and higher order terms in the score
functions. However, their amplitude is constrained by ( 3.46 ).
We prove this inequality here only for one dimensional variables, @xmath
. The general proof consists in replacing indices such as @xmath with
multindices @xmath and so on.
Our proof is based on the following basic fact concerning blockwise
positive matrices (for a proof of this particular fact, see ( Bhatia ,
2007 , e.g.) : Whenever @xmath and @xmath are strictly positive
matrices, then

  -- -------- -- --------
     @xmath      (3.47)
  -- -------- -- --------

We proceed as follows : in general we can write the moments of @xmath as
a linear combination of those of @xmath

  -- -- -- --------
           (3.48)
  -- -- -- --------

Note that @xmath is a triangular matrix with non vanishing diagonal
elements, and is therefore invertible. Writing @xmath , it holds
therefore

  -- -------- -- --------
     @xmath      (3.49)
  -- -------- -- --------

where @xmath , respectively @xmath is the moment matrix @xmath of @xmath
, respectively @xmath . The claim ( 3.46 ) is proven provided

  -- -------- -- --------
     @xmath      (3.50)
  -- -------- -- --------

which is equivalent to

  -- -------- -- --------
     @xmath      (3.51)
  -- -------- -- --------

We now prove this last relation.
Consider the following vector

  -- -------- -- --------
     @xmath      (3.52)
  -- -------- -- --------

and the matrix @xmath defined with the help of this vector,

  -- -------- -- --------
     @xmath      (3.53)
  -- -------- -- --------

Such a matrix of scalar products is called a Gram matrix. @xmath has by
definition the block form

  -- -------- -- --------
     @xmath      (3.54)
  -- -------- -- --------

It is well known that any Gram matrix is positive : from the definition
of @xmath , we have that @xmath for any vector @xmath . Using the above
fact ( 3.47 ) we have

  -- -------- -- --------
     @xmath      (3.55)
  -- -------- -- --------

Equivalently

  -- -------- -- --------
     @xmath      (3.56)
  -- -------- -- --------

Taking the inverse gives

  -- -------- -- --------
     @xmath      (3.57)
  -- -------- -- --------

which concludes the proof.

##### 3.2.3 Poissonian discreteness effects

The same relations hold for another source of noise relevant in
cosmological surveys, i.e. discreteness effects due to a finite number
of tracers of the underlying fields. A common parametrisation is the
Poisson model, where the observed number of tracers in a cell is given
by a Poisson variable with intensity the value of the underlying
continuous field @xmath at that point. Explicitly, with @xmath the
number of tracers in @xmath cells, we set their joint probability to be

  -- -------- -- --------
     @xmath      (3.58)
  -- -------- -- --------

This model has the peculiar property of transforming the moments of
@xmath to factorial moments : the falling factorial in @xmath variables
² ² 2 see ( 3.95 ) for the falling factorial in one dimension becomes

  -- -------- -- --------
     @xmath      (3.59)
  -- -------- -- --------

and we have indeed the known curious relation

  -- -- -- --------
           (3.60)
  -- -- -- --------

Using this relation, it is easy to prove with the same methods than
above that

  -- -------- -- --------
     @xmath      (3.61)
  -- -------- -- --------

for any @xmath . This does not appear to be necessarily the case for
more generic functional form of @xmath , since there are no obvious
relations between the first @xmath moments of @xmath and those of @xmath
.

#### 3.3 Some exactly solvable models for moment determinate densities

We derive in this section the exact analytical expressions for the
information coefficients at all orders of well known families of moment
determinate probability density functions. We will deal with the normal
distribution, the beta and gamma families as well as an extended Poisson
model. For all these instances, the matrices @xmath converges to @xmath
as @xmath .

###### Normal distribution

The normal distribution provides us with a very simple illustration of
the approach. Its probability density function is

  -- -------- -- --------
     @xmath      (3.62)
  -- -------- -- --------

Its Fisher information matrix is well known,

  -- -------- -- --------
     @xmath      (3.63)
  -- -------- -- --------

The information coefficients take the form

  -- -------- -- --------
     @xmath      (3.64)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (3.65)
  -- -------- -- --------

We recover already the full matrix with the first two moments, @xmath .
The matrix is diagonal because there is no order @xmath for which both
coefficients @xmath and @xmath do not vanish.

###### Derivation :

It is easily seen that the score function @xmath is a polynomial of
first order in @xmath . This implies immediately that only @xmath is
non-zero. For any probability density function and parameter @xmath , it
holds from ( 3.8 ) that

  -- -------- -- --------
     @xmath      (3.66)
  -- -------- -- --------

which proves ( 3.64 ). Very similarly, the score function associated to
@xmath is a polynomial of second order, such that only the first two
coefficients can possibly be non-zero. However, @xmath vanishes since
@xmath does not impact the mean. Finally, @xmath can be gained by noting
that the polynomials orthogonal to the normal distribution are the
Hermite polynomials ( Szegö , 2003 ) ,

  -- -------- -- --------
     @xmath      (3.67)
  -- -------- -- --------

with normalisation @xmath . From @xmath and equation ( 3.8 ) follows
@xmath and @xmath

###### Beta distribution

The beta distribution is defined as

  -- -------- -- --------
     @xmath      (3.68)
  -- -------- -- --------

where @xmath is the beta integral, which will also enter the following
section on the gamma distribution. It has the well known representation
in terms of the gamma function @xmath , @xmath . It plays a fundamental
role in order statistics.
The full Fisher information matrix can be conveniently expressed in
terms of the second derivative of the logarithm of the gamma function,
called the trigamma function @xmath ( Abramowitz and Stegun , 1970 , p.
258-260) ,

  -- -------- -- --------
     @xmath      (3.69)
  -- -------- -- --------

Our derivation of the information coefficients associated to @xmath and
@xmath , based on the explicit expressions of the orthogonal
polynomials, in this case the Jacobi polynomials, is rather lengthy. We
defer it to the appendix.
The result is

  -- -------- -- --------
     @xmath      (3.70)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (3.71)
  -- -------- -- --------

where @xmath is the rising factorial. For the symmetric case @xmath ,
these expressions simplify considerably. We have namely

  -- -------- -- --------
     @xmath      (3.72)
  -- -------- -- --------

Since @xmath and @xmath , a short calculation shows that we recover
indeed, summing these coefficients, the full matrix given in ( 3.69 )

  -- -------- -- --------
     @xmath      (3.73)
  -- -------- -- --------

###### Gamma distribution

The gamma distribution is a two parameter family,

  -- -------- -- --------
     @xmath      (3.74)
  -- -------- -- --------

where @xmath is the gamma function. Special cases include the
exponential distribution ( @xmath ), or the chi squared distribution
with @xmath degrees of freedom ( @xmath ). The calculation of the Fisher
information matrix associated to @xmath and @xmath is not difficult.
Again, the trigamma function shows up,

  -- -------- -- --------
     @xmath      (3.75)
  -- -------- -- --------

The information coefficients are evaluated below, with the result

  -- -------- -- --------
     @xmath      (3.76)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (3.77)
  -- -------- -- --------

As a consistency check, we see that since @xmath , we recover trivially
the @xmath and @xmath elements of the matrix ( 3.75 ). Its @xmath
element implies that the following identity must hold,

  -- -------- -- --------
     @xmath      (3.78)
  -- -------- -- --------

which reduces for @xmath to Euler’s famous formula @xmath .

###### Derivation :

The polynomials associated to that distribution are the generalized
Laguerre polynomials @xmath ( Abramowitz and Stegun , 1970 , p.775) .
More precisely, we have

  -- -------- -- --------
     @xmath      (3.79)
  -- -------- -- --------

For this reason, the polynomials orthogonal to the gamma distribution
are

  -- -------- -- --------
     @xmath      (3.80)
  -- -------- -- --------

with normalisation

  -- -------- -- --------
     @xmath      (3.81)
  -- -------- -- --------

They have the explicit matrix elements @xmath ³ ³ 3 We added the factor
@xmath to the conventions of Abramowitz and Stegun ( 1970 ) , in
accordance with our own conventions of having a positive leading
coefficient @xmath . ,

  -- -------- -- --------
     @xmath      (3.82)
  -- -------- -- --------

The moments of the gamma function are given by

  -- -------- -- --------
     @xmath      (3.83)
  -- -------- -- --------

The Fisher information on @xmath is the simplest. It holds namely that
the score function associated to @xmath is a polynomial first order in
@xmath , and therefore that only @xmath is non-zero. It follows

  -- -------- -- --------
     @xmath      (3.84)
  -- -------- -- --------

The calculation of @xmath requires a little bit more work, but we can
make use of previous results we derived when dealing with the beta
family. The derivatives of the moments with respect to @xmath are given
by

  -- -------- -- --------
     @xmath      (3.85)
  -- -------- -- --------

Using the representation ( 3.8 ), together with

  -- -------- -- --------
     @xmath      (3.86)
  -- -------- -- --------

one obtains the following expression

  -- -------- -- --------
     @xmath      (3.87)
  -- -------- -- --------

The sum is precisely the function @xmath defined in ( 3.142 ), evaluated
a @xmath and @xmath , and that we proved in ( 3.150 ) to be a
representation of the beta function,

  -- -------- -- --------
     @xmath      (3.88)
  -- -------- -- --------

We conclude therefore

  -- -------- -- --------
     @xmath      (3.89)
  -- -------- -- --------

###### Poisson model

Consider the probability density function

  -- -------- -- --------
     @xmath      (3.90)
  -- -------- -- --------

with @xmath the Dirac delta function. This is the usual Poisson law,
together with some amplitude @xmath that is left free. If @xmath is set
to unity, we recover the Poisson law. This is a peculiar situation in
terms of Fisher information. Unlike the previous situations, the
parameter @xmath impacts indeed the range of @xmath . The Fisher
information on @xmath is not well defined, formally infinite.
Even though the Fisher information matrix is not well defined, the
information coefficients as given in equations ( 3.8 ) or ( 3.9 ) are
still meaningful, and so are their interpretation ( 3.7 ) in terms of
the expected curvature of a @xmath fit. The result is

  -- -------- -- --------
     @xmath      (3.91)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (3.92)
  -- -------- -- --------

Note that it is compatible with the fact that the Fisher information on
@xmath is infinite. For any @xmath the sum

  -- -------- -- --------
     @xmath      (3.93)
  -- -------- -- --------

is divergent. The turnover of the coefficients occurs around @xmath
where they start to increase. Atypically, the higher order the moment
the most interesting it becomes for inference on @xmath in this model.

###### Derivation :

Consider the Charlier polynomials ( Abramowitz and Stegun , 1970 ,
pp.788) , defined by

  -- -------- -- --------
     @xmath      (3.94)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (3.95)
  -- -------- -- --------

is the falling factorial. These are the polynomials orthogonal with
respect to the Poisson distribution with intensity @xmath . Clearly, the
polynomials

  -- -------- -- --------
     @xmath      (3.96)
  -- -------- -- --------

are then the polynomials orthogonal to @xmath . The normalization is

  -- -------- -- --------
     @xmath      (3.97)
  -- -------- -- --------

On the other hand, the moments are given by @xmath , where @xmath is the
@xmath th moment of the Poisson distribution. For this reason, it holds
that @xmath . This equation, together with ( 3.96 ) implies that the
information coefficients on @xmath are independent of the actual value
of @xmath , since @xmath and @xmath . So are the coefficients @xmath .
We can therefore from now on safely chose @xmath as the fiducial value,
and work with the usual Poisson distribution and associated Charlier
polynomials.
The coefficients associated to @xmath are the simplest to obtain. It is
not difficult to see that the score function associated to @xmath of the
Poisson distribution is a polynomial of first order. We obtain @xmath
and @xmath . Turning to @xmath , we need to evaluate

  -- -------- -- --------
     @xmath      (3.98)
  -- -------- -- --------

The derivation we propose uses extensively the technique of generating
functions ( Wilf , 1994 ) , and is a variation on the following theme :
consider the known expression for the factorial moments of the Poisson
distribution,

  -- -------- -- --------
     @xmath      (3.99)
  -- -------- -- --------

Even though it may appear mysterious at first sight, a simple way to
prove this identity is to notice

  -- -------- -- ---------
     @xmath      (3.100)
  -- -------- -- ---------

where @xmath is called the probability generating function, and to try
and evaluate the right hand side. This is indeed very convenient, since
the probability generating function of the poisson law takes a very
simple form,

  -- -------- -- ---------
     @xmath      (3.101)
  -- -------- -- ---------

The derivatives are trivial to evaluate, proving ( 3.99 ). We will need
the following result, that comes out directly from an essentially
identical argument,

  -- -------- -- ---------
     @xmath      (3.102)
  -- -------- -- ---------

where @xmath is some number.
We now turn to the evaluation of

  -- -------- -- ---------
     @xmath      (3.103)
  -- -------- -- ---------

This sum can be written as the first derivative with respect to @xmath
evaluated at @xmath of the following expression

  -- -------- -- ---------
     @xmath      (3.104)
  -- -------- -- ---------

as can be seen directly from an expansion of the Charlier polynomials
@xmath in terms of the coefficients @xmath . Using equation ( 3.94 ) and
our result ( 3.102 ), we obtain

  -- -- -- ---------
           (3.105)
  -- -- -- ---------

The derivative with respect to @xmath is easily performed :

  -- -------- -- ---------
     @xmath      (3.106)
  -- -------- -- ---------

Also, using Leibniz rule for derivatives of products, we can write

  -- -------- -- ---------
     @xmath      (3.107)
  -- -------- -- ---------

The factor of @xmath cancels in equation ( 3.105 ), so that

  -- -------- -- ---------
     @xmath      (3.108)
  -- -------- -- ---------

From the properties of the binomial coefficients follows that the sum
over @xmath is just @xmath . Therefore,

  -- -------- -- ---------
     @xmath      (3.109)
  -- -------- -- ---------

The derivatives are easily computed :

  -- -------- -- ---------
     @xmath      (3.110)
  -- -------- -- ---------

with the final result given in equation ( 3.92 ).

##### 3.3.1 Moment indeterminate densities

We now turn to indeterminate distributions, i.e. those that cannot be
uniquely recovered from their moments. In this case, the Fisher
information content of the moments will generally not converge to the
total amount. In this respect, the limit

  -- -------- -- ---------
     @xmath      (3.111)
  -- -------- -- ---------

a number in @xmath , may be thought of as an indirect measure of the
amount of indeterminacy of the moment problem. A value of unity means
that the moments do carry the same information on the parameter @xmath
as the probability density function, while for values close to zero a
large amount of information on @xmath is not present in the hierarchy.
The quantity @xmath has via the information inequality the
interpretation of the ratio of the expected constraints on @xmath by
extraction of the full set of moments to the best constraints achievable
with the help of unbiased estimators of @xmath . When more than one
parameter are of interest, this definition is not really satisfactory
anymore, since it is tied to @xmath exclusively. One way to get around
this is by defining a ratio of determinants,

  -- -------- -- ---------
     @xmath      (3.112)
  -- -------- -- ---------

where @xmath was defined in equation ( 3.14 ). Clearly, @xmath is the
cumulative efficiency to catch the information. Since the matrices in
the numerator and denominator do transform in the same way under any
smooth reparametrisation of the parameters, or coordinates, of the
family, this ratio is left unchanged under such transformations. Thus,
@xmath and @xmath are well defined quantities associated to that family
of distributions, independently of the chosen coordinates.
Naively, one might expect this lost information to reflect in some way
the freedom there is in the choice of a distribution with the very same
series of moments. We will therefore compare our results to a measure of
this freedom, @xmath , that was proposed recently ( Stoyanov , 2004 ) as
the index of dissimilarity of a Stieltjes class. A Stieltjes class is a
family of probability density functions of the form

  -- -------- -- ---------
     @xmath      (3.113)
  -- -------- -- ---------

all having the same moment series as the central probability density
function @xmath . The index @xmath , defined as

  -- -------- -- ---------
     @xmath      (3.114)
  -- -------- -- ---------

in @xmath is the maximal distance between two members of the class,
being zero for determinate moment problems, where @xmath must
identically vanish.
We will consider three types of probability density functions, the
lognormal, which is indeterminate for any values of its parameter space,
as well as the Weibull distribution and the stretched exponential, which
are indeterminate solely for parts of their parameter range. For these
last two distributions we could not find in all cases exact analytical
expressions for the coefficients @xmath . We generated them therefore
numerically, using a fine discretization procedure to obtain the
orthogonal polynomials, following the exposition in Gautschi ( 2004 ) .

###### Lognormal distribution

The lognormal distribution has the following functional form,

  -- -------- -- ---------
     @xmath      (3.115)
  -- -------- -- ---------

The parameters @xmath and @xmath are the mean and the variance of @xmath
which is normally distributed. They are related to the mean @xmath and
variance @xmath of @xmath by the relations

  -- -------- -- ---------
     @xmath      (3.116)
  -- -------- -- ---------

The only relevant parameter for our purposes is the reduced variance
@xmath , in terms of which our results can be expressed. The information
coefficients associated to the parameters @xmath and @xmath we will
derive in details in chapter 6 , (see also Carron ( 2011 ) ), with the
help of @xmath -series. These are given by

  -- -------- -- ---------
     @xmath      (3.117)
  -- -------- -- ---------

In these equations,

  -- -------- -- ---------
     @xmath      (3.118)
  -- -------- -- ---------

and

  -- -------- -- ---------
     @xmath      (3.119)
  -- -------- -- ---------

is the q-Pochammer symbol Andrews et al. ( 1999 ) . According to the
chain rule of derivation, we obtain @xmath and @xmath . On the other
hand, since @xmath is normally distributed, the total Fisher information
matrix @xmath associated to @xmath and @xmath is diagonal with @xmath
and @xmath as diagonal elements. It follows that in these coordinates
@xmath .
The Stieltjes class we consider is set by

  -- -------- -- ---------
     @xmath      (3.120)
  -- -------- -- ---------

Others are given in Heyde ( 1963 ) but are equivalent to a uninteresting
rescaling of @xmath . After an obvious variable substitution, the
dissimilarity index becomes

  -- -------- -- ---------
     @xmath      (3.121)
  -- -------- -- ---------

In figure 3.1 , we show the cumulative efficiency @xmath , for @xmath
and @xmath for the lognormal family, evaluated with the exact
expressions given in ( 3.117 ), solid lines from bottom to top, together
with the dissimilarity index @xmath , shown as the dashed line,
evaluated by numerical integration of equation ( 3.121 ). It is not
difficult to see from the above expressions that the coefficients @xmath
decay exponentially for large @xmath , such that the convergence is
rather quick over the full range. We observe a very sharp transition in
@xmath as soon as the reduced variance approaches unity, from a regime
where the entire information content is within the second moment, as for
the normal distribution (and thus where the indeterminacy of the moment
problem is irrelevant for parameter estimation), to the opposite regime
where all moments completely fail to capture the information. This
cutoff is discussed at length in chapter 6 . On the other hand. the
index @xmath is seen to be remarkably constant over the range shown,
equal to its limiting values for @xmath , which can be evaluated from (
3.121 ) to be @xmath .

###### Weibull distribution

We consider now the Weibull distribution with shape parameter @xmath and
scale parameter @xmath ,

  -- -------- -- ---------
     @xmath      (3.122)
  -- -------- -- ---------

The variable @xmath can be seen as a power of an exponentially
distributed variable with intensity unity, @xmath , for @xmath . It is
known that the moment problem associated to the moments of @xmath is
determinate for @xmath and indeterminate for @xmath ( Stoyanov , 1987 ,
section 11.3 e.g.) . A Stieltjes class in the latter regime is provided
in the same reference, which after some algebraic manipulations reduces
to

  -- -------- -- ---------
     @xmath      (3.123)
  -- -------- -- ---------

The index @xmath becomes

  -- -------- -- ---------
     @xmath      (3.124)
  -- -------- -- ---------

It is interesting to note that this integral can be performed
analytically, for instance with the help of partial integration, with
the result

  -- -------- -- ---------
     @xmath      (3.125)
  -- -------- -- ---------

We evaluated @xmath and the associated @xmath numerically, using the
convenient coordinates @xmath and @xmath . It is not difficult to see
that @xmath is independent of @xmath . The full information matrix for
these parameters can be evaluated analytically, with the result @xmath .
The results for @xmath are shown in figure 3.2 , that we present as for
the lognormal as function of the reduced variance

  -- -------- -- ---------
     @xmath      (3.126)
  -- -------- -- ---------

The dashed vertical line corresponding to @xmath separates the two
regimes where, on the left, @xmath is unity and @xmath zero since the
moment problem is determinate, and on the right, where the moment
problem is indeterminate. The scale @xmath corresponds to @xmath .
While the decay of @xmath is also very sharp in the indeterminate
regime, very slow convergence of @xmath is seen to occur in the large
reduced variance regime unlike for the lognormal distribution. This also
in the region @xmath , which corresponds to the phase where the Weibull
distribution goes from a unimodal to a monotonically decreasing
distribution, but @xmath still is unity, since the moment problem still
is determinate. For instance, for @xmath , the following exact result
can be gained with the same methods as exposed in this section,

  -- -------- -- ---------
     @xmath      (3.127)
  -- -------- -- ---------

###### Stretched exponential function

Finally, we treat the case of the stretched exponential,

  -- -------- -- ---------
     @xmath      (3.128)
  -- -------- -- ---------

Just as for the Weibull distribution, the moment problem associated to
the moments of @xmath is determinate for @xmath and indeterminate for
@xmath ( Stoyanov , 1987 , section 11.4) A Stieltjes class is given by

  -- -------- -- ---------
     @xmath      (3.129)
  -- -------- -- ---------

where @xmath is as above given by @xmath .

Numerical evaluation of @xmath , and of the dissimilarity index is shown
in fig 3.3 . As for the Weibull distribution, these results are
independent of the scale parameter @xmath . We can conclude that in none
of the situations we investigated is @xmath a good tracer of the
importance of the indeterminacy of the moment problem for parameter
inference.

#### 3.4 Appendix

##### 3.4.1 Derivation for the beta family

In the following we will need the first two derivatives of the logarithm
of the gamma function. These are called the digamma @xmath and trigamma
@xmath functions respectively ( Abramowitz and Stegun , 1970 , p.
258-260) ,

  -- -------- -- ---------
     @xmath      (3.130)
  -- -------- -- ---------

The Fisher information matrix can be gained by noting that by
differentiation under the integral sign, we have

  -- -------- -- ---------
     @xmath      (3.131)
  -- -------- -- ---------

Using the representation of the beta integral in terms of the Gamma
function, we conclude that

  -- -------- -- ---------
     @xmath      (3.132)
  -- -------- -- ---------

In order to obtain the information coefficients @xmath and @xmath , it
is more convenient to start with @xmath . @xmath will then be gained
effortlessly by looking at the symmetry of the problem.
From the definition of the beta integral, the moments of the beta
distribution are given by

  -- -------- -- ---------
     @xmath      (3.133)
  -- -------- -- ---------

The derivatives of the moments with respect to @xmath are given by

  -- -------- -- ---------
     @xmath      (3.134)
  -- -------- -- ---------

The orthogonal polynomials are the Jacobi polynomials @xmath (
Abramowitz and Stegun , 1970 , page 774) . Instead of the parameters
@xmath used in Abramowitz and Stegun ( 1970 ) , we stick to @xmath and
@xmath , which are more appropriate for our purposes ⁴ ⁴ 4 We have in
the notation of Abramowitz and Stegun ( 1970 ) @xmath . These
polynomials are proportional to the Jacobi polynomials @xmath ( Szegö ,
2003 , chap. IV) , orthogonal on the interval @xmath . Their matrix
elements of @xmath are given explicitly by

  -- -- -- ---------
           (3.135)
  -- -- -- ---------

We first note the following relation,

  -- -------- -- ---------
     @xmath      (3.136)
  -- -------- -- ---------

where

  -- -------- -- ---------
     @xmath      (3.137)
  -- -------- -- ---------

is the rising factorial. Since @xmath is given by

  -- -------- -- ---------
     @xmath      (3.138)
  -- -------- -- ---------

the evaluation of the following sum is necessary,

  -- -------- -- ---------
     @xmath      (3.139)
  -- -------- -- ---------

The following paragraphs are dedicated to the lengthy but
straightforward proof of the following result,

  -- -------- -- ---------
     @xmath      (3.140)
  -- -------- -- ---------

The proof consists of a number of steps.

###### Step 1 :

Using an always useful trick, we turn the rising factorial in the sum
into a power and differentiate : from

  -- -------- -- ---------
     @xmath      (3.141)
  -- -------- -- ---------

follows

  -- -------- -- ---------
     @xmath      (3.142)
  -- -------- -- ---------

We then perform the derivatives using the Leibniz rule of derivation.
Since

  -- -------- -- ---------
     @xmath      (3.143)
  -- -------- -- ---------

one obtains

  -- -------- -- ---------
     @xmath      (3.144)
  -- -------- -- ---------

###### Step 2

To obtain @xmath , we first construct an integral representation of the
function @xmath . From the relation ( Andrews et al. , 1999 , theorem
1.2.7)

  -- -------- -- ---------
     @xmath      (3.145)
  -- -------- -- ---------

of the digamma function, we note that

  -- -------- -- ---------
     @xmath      (3.146)
  -- -------- -- ---------

Writing

  -- -------- -- ---------
     @xmath      (3.147)
  -- -------- -- ---------

the sum over @xmath becomes a geometric series. We obtain therefore,

  -- -------- -- ---------
     @xmath      (3.148)
  -- -------- -- ---------

We can now perform the sum over @xmath in @xmath . We have indeed

  -- -------- -- ---------
     @xmath      (3.149)
  -- -------- -- ---------

It follows immediately

  -- -------- -- ---------
     @xmath      (3.150)
  -- -------- -- ---------

We only need derivatives of that function evaluated at @xmath . The
order of each derivative is @xmath . For this reason the term @xmath in
this expression actually plays no role. Each such derivative is thus a
beta integral :

  -- -------- -- ---------
     @xmath      (3.151)
  -- -------- -- ---------

###### Step 3 :

To go further, we use the following property of the beta integral,

  -- -------- -- ---------
     @xmath      (3.152)
  -- -------- -- ---------

which can be seen from its representation in terms of the gamma
function, or from its integral representation ( Andrews et al. , 1999 ,
page 5) . Two applications of this rule leads to

  -- -------- -- ---------
     @xmath      (3.153)
  -- -------- -- ---------

By combining this relation with ( 3.151 ), and using

  -- -------- -- ---------
     @xmath      (3.154)
  -- -------- -- ---------

we have shown

  -- -------- -- ---------
     @xmath      (3.155)
  -- -------- -- ---------

This form is very convenient, since many terms now cancel in equation (
3.144 ). We obtain

  -- -------- -- ---------
     @xmath      (3.156)
  -- -------- -- ---------

The sum over @xmath would vanish would it run up to @xmath . Its value
is therefore minus the @xmath term, which is unity. It follows

  -- -------- -- ---------
     @xmath      (3.157)
  -- -------- -- ---------

which was to be proved.
Getting @xmath requires now only to keep track of the normalization. The
normalization of the Jacobi polynomials @xmath Abramowitz and Stegun (
1970 ) is

  -- -------- -- ---------
     @xmath      (3.158)
  -- -------- -- ---------

(Note that there is a additional factor of @xmath with respect to
Abramowitz and Stegun ( 1970 ) since there the measure is not normalized
to unit integral). We have from equations ( 3.134 ), ( 3.136 ), ( 3.138
) and ( 3.140 ) together with some algebra

  -- -------- -- ---------
     @xmath      (3.159)
  -- -------- -- ---------

Tedious calculations are not needed to get @xmath , but symmetry
considerations are enough. The Jacobi polynomials obeys the symmetry
relation,

  -- -------- -- ---------
     @xmath      (3.160)
  -- -------- -- ---------

and therefore

  -- -------- -- ---------
     @xmath      (3.161)
  -- -------- -- ---------

It is then not difficult ⁵ ⁵ 5 for instance from the representation (
3.9 ) to see that @xmath is proportional to @xmath with @xmath and
@xmath exchanged. We conclude

  -- -------- -- ---------
     @xmath      (3.162)
  -- -------- -- ---------

##### 3.4.2 General hierarchical systems, recursion relations.

This chapter was focussed on the hierarchy of @xmath -point moments,
with associated orthogonal system the orthogonal polynomials. One of
course expects the approach of this chapter to extend in some way to any
system of observables. It is the aim of this section to discuss briefly
the case of other hierarchical systems.
Given a density function @xmath , where @xmath , consider a set of
functions, (a hierarchy @xmath ),

  -- -------- -- ---------
     @xmath      (3.163)
  -- -------- -- ---------

which can be finite or infinite. The restrictions are @xmath , the
functions to be linearly independent with respect to the density @xmath
, as well as @xmath to be well defined.
The density provides us with a scalar product,

  -- -------- -- ---------
     @xmath      (3.164)
  -- -------- -- ---------

Consider now the orthonormal system built from @xmath , following the
Gram-Schmidt orthogonalisation procedure. Explicitly, they can be built
recursively from

  -- -------- -- ---------
     @xmath      (3.165)
  -- -------- -- ---------

By construction, we have indeed

  -- -------- -- ---------
     @xmath      (3.166)
  -- -------- -- ---------

@xmath is a polynomial in the hierarchy members, in the sense that

  -- -------- -- ---------
     @xmath      (3.167)
  -- -------- -- ---------

for some matrix elements @xmath , with @xmath , and @xmath .
Very useful is the following identity between matrices of size @xmath :

  -- -------- -- ---------
     @xmath      (3.168)
  -- -------- -- ---------

###### Derivation

Consider the expansion of @xmath in terms of @xmath :

  -- -------- -- ---------
     @xmath      (3.169)
  -- -------- -- ---------

Such an expansion is always possible since the set of @xmath and @xmath
span the same space by construction. It follows @xmath . Expanding in
this relation @xmath with the help of the matrix @xmath , we obtain

  -- -------- -- ---------
     @xmath      (3.170)
  -- -------- -- ---------

Relation ( 3.168 ) follows.
We are writing now the moments of the hierarchy as @xmath . The inverse
of the covariance matrix @xmath between the members of @xmath is given
by

  -- -------- -- ---------
     @xmath      (3.171)
  -- -------- -- ---------

This is indeed the @xmath lower right block of @xmath .
From these considerations follows : Define the (positive) Fisher
information matrix within the first @xmath hierarchy members, @xmath as

  -- -------- -- ---------
     @xmath      (3.172)
  -- -------- -- ---------

If the density is a probability density, then @xmath , and therefore

  -- -------- -- ---------
     @xmath      (3.173)
  -- -------- -- ---------

Writing @xmath as @xmath we can expand the matrix as

  -- -------- -- ---------
     @xmath      (3.174)
  -- -------- -- ---------

The coefficients @xmath is therefore the information on @xmath in @xmath
that was not contained already in the previous members of the hierarchy.
The bits of information absent from the hierarchy are given by

  -- -------- -- ---------
     @xmath      (3.175)
  -- -------- -- ---------

This matrix on the right hand side of that relation is positive (it is a
Gram matrix), and thus this relation states that the information within
the hierarchy is always less than the total information. From this
inequality follows thus the information inequality for any set of
probes. Convergence to the total information occurs when

  -- -------- -- ---------
     @xmath      (3.176)
  -- -------- -- ---------

in the mean square error sense with respect to @xmath . This implies
that the hierarchy is efficient precisely when @xmath is sparse in
@xmath . In particular, maximal entropy distributions with the
prescribed moments of @xmath have a finite non zero numbers of
coefficients. Note that @xmath is the best approximation of @xmath with
the given hierarchy up to @xmath according to this mean square
criterium.

###### Recursion relations

Remember the triangular matrix @xmath of size @xmath defined above,
@xmath . The following relations are easily seen to hold,

  -- -------- -- ---------
     @xmath      (3.177)
  -- -------- -- ---------

It follows that the matrix @xmath is nothing else than the Cholesky
decomposition of the moment matrix. Multiplying ( 3.165 ) with the score
function and integrating, one obtains

  -- -------- -- ---------
     @xmath      (3.178)
  -- -------- -- ---------

This can provide a way to evaluate numerically these coefficient, after
a Cholesky decomposition of the moment matrix. However, it is necessary
to keep in mind that large moment matrices are infamously known for
being generically badly conditioned ( Tyrtyshnikov , 1994 ) .

## Part II Applications for cosmology

### Chapter 4 On the combination of shears, magnification and flexion
fields

At the heart of weak lensing as a cosmological probe lies the
convergence field, the weighted projection along the line of sight of of
the density fluctuation field, with weights sensitive to the geometry of
the Universe. In this chapter we use the duality of the Shannon entropy
and Fisher information introduced in chapter 2 to discuss quantitatively
the combination of different probes of the convergence field, both for
mass reconstruction as well as for cosmological purposes. The probes we
include are the galaxy shears, the magnification as well as the flexion
fields, i.e. all modifications to the galaxy images up to second order.
Section 4.2 describes the observables and the approach, and section 4.3
the results.
We find that flexion alone outperforms the well established shears on
the arcsecond scale, making flexion well suited for mass reconstruction
on small scales. At the same time, it complements powerfully the shears
on the scale of the arcminute. We find size information to carry some
modest, scale independent amount of information. Besides, the results of
this chapter show how standard cosmological Fisher matrix analysis based
on Gaussian statistics can incorporate these other probes in the most
simple way. From ( 4.24 ) follows namely that the inclusion of all the
two-point correlations of these additional weak lensing probes can be
accounted for by adapting the noise term.
The text in this section follows the second part of Carron et al. ( 2011
) .

#### 4.1 Introduction

Gravitational lensing, which can be used to measure the distribution of
mass along the line of sight, has been recognized as powerful probe of
the dark components of the Universe ( Schneider et al. , 1992 ;
Bartelmann and Schneider , 2001 ; Refregier , 2003 ; Munshi et al. ,
2006 ; Schneider et al. , 2006 ) since it is sensitive to both the
geometry of the Universe, and to the growth of structure. Weak lensing
data is typically used in two ways. The first, which is deployed for
cosmological parameter fitting, relies on measuring the correlated
distortions in galaxy images ( Albrecht et al. , 2006 ) . The second
approach uses each galaxy to make a noisy measurement of the lensing
signal at that position. These point estimates are then used to
reconstruct the dark matter density distribution (e.g. Kaiser and
Squires , 1993 ; Seitz and Schneider , 2001 ) . Most of the measurements
of weak lensing to date have focused on the shearing that galaxy images
experience. However, gravitational lensing causes a number of other
distortions of galaxy images. These include change in size, which is
related to the magnification, and higher order image distortions known
as the flexion ( Bacon et al. , 2006 ) . A number of techniques have
been developed for measuring these higher order images distortions, such
as HOLICS ( Okura et al. , 2007 ) and shapelets methods ( Massey et al.
, 2007 ) . Since all of the image distortions originate from the same
source, the lensing potential field, the information content of any two
lensing measurements must be degenerate. At the same time, since each
method has different systematics and specific noise properties,
combining multiple measurement may bring substantial benefits. Some
recent works have looked at the impact of combining shear and flexion
measurements for mass reconstruction ( Er et al. , 2010 ; Pires and
Amara , 2010 ; Velander et al. , 2010 ) , as well as the benefits for
breaking multiplicative bias of including galaxy size measurements (
Vallinotto et al. , 2010 ) .
We will focus on tracers of galaxy image distortions up to second order
in the distortions. These include, to first order, the change in
apparent size of the galaxies, due to the magnification source by the
convergence field, the two components of the shear, and, to second
order, the four components of the two flexion fields. We will limit
ourselves to the case where the noise contaminating each probe can be
effectively treated as independent of the model parameters. The common
point of the probes cited above is that they all trace, in some noisy
way, the same central field on a discrete set of points, represented by
the positions of the galaxies on which the tracers are measured. The
framework we presented in chapter 2 is ideal to deal with the special
structure of the situation. We will first show how we can understand the
total information content of such degenerate probes of a central field
in a general situation, and then make quantitative evaluations at the
two-point level.

#### 4.2 Linear tracers

The situation that we consider here is one where a broad number of
observables are linked in some way to a central field. We will limit
ourselves to the case where the noise contaminating each probe can be
effectively treated as independent of the model parameters. Imagine one
plans to perform different measurements, of observables that are all
linked in some way or another to the same central field, which is the
actually interesting quantity from the point of view of the analyst. The
most straightforward example of such a situation, occurs when the two
point correlations of the central field, say the the power spectrum of
the convergence in weak lensing, is predicted from theory, while we try
to extract it looking at the correlations of the two ellipticity
components of galaxies. In this case, predictive power of the power
spectrum of the convergence turns into predictive power of (parts of)
the three correlation functions there are between the two ellipticity
components.
The predictive power of some observable @xmath of a central field (for
instance its power spectrum at some mode) translates into an array of
constraints @xmath in the noisy probes, that we could try and extract
and confront to theory :

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

for some functions @xmath .
For the purpose of this work, the case of functions linear with respect
to @xmath is generic enough, i.e. we will consider that

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

The entropy @xmath of the data is a function of the @xmath constraints
@xmath . It is however fundamentally a function of @xmath since it does
enter all of these observables. It is therefore very natural to
associate a potential @xmath to @xmath , although it is not itself a
constraint on the probability density function. In analogy with

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

we define

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

with the result, given by application of the chain rule, of

  -- -------- -- -------
     @xmath      (4.5)
  -- -------- -- -------

On the other hand, the impact of a model parameter on each observables
can be similarly written in terms of the central observable @xmath ,

  -- -------- -- -------
     @xmath      (4.6)
  -- -------- -- -------

It follows directly from these relations ( 4.5 ) and ( 4.6 ), and the
linearity of @xmath , that the joint information in the full set of
constraints @xmath , given in equation ( 2.43 ) as a sum over all @xmath
constraints, reduces to a formally identical expression with the only
difference that only @xmath enters :

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

which can also be written in the form analog to ( 2.44 ),

  -- -------- -- -------
     @xmath      (4.8)
  -- -------- -- -------

This last equation shows that all the effect of combining this set of
constraints have been absorbed into the second total derivative of the
entropy. This second total derivative is the total amount of information
there is on the central quantity @xmath in the data. Indeed, taking as a
special case of model parameter to the central quantity itself, i.e.

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

one obtains now that the full amount of information in @xmath on @xmath
is

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

A simple application of the Cramer Rao inequality presented in equation
( 2.10 ) shows that this effective variance @xmath is the lower bound to
an unbiased reconstruction of the central observable from the noisy
probes.
These considerations on the effect of probe combination in the case of a
single central field observable @xmath generalize easily to the case
where there are many, @xmath . In this case, each central field quantity
leads to an array of constraints in the form of equation ( 4.1 ), it is
simple to show that the amount of Fisher information can again be
written in terms of the information associated to the central field,
with an effective covariance matrix between the @xmath . The result is

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

All the effects of probe combination are thus encompassed in an
effective covariance matrix @xmath of the central field observables,

  -- -------- -- --------
     @xmath      (4.12)
  -- -------- -- --------

Again, an application of the Cramer Rao inequality, in the
multi-dimensional case, shows that this effective covariance matrix is
the best achievable unbiased joint reconstruction of @xmath .
We now explore further the case of linear probes of homogeneous Gaussian
fields, which is cosmologically relevant and can be solved analytically
to full extent. We will focus on zero mean fields, for which according
to our previous section the entropy can be written in terms of the
spectral matrices, up to a constant,

  -- -------- -- --------
     @xmath      (4.13)
  -- -------- -- --------

##### 4.2.1 Linear tracers at the two-point level

The standard instance of a linear tracer @xmath of some central field
@xmath in weak lensing is provided by a relation in Fourier space of the
form

  -- -------- -- --------
     @xmath      (4.14)
  -- -------- -- --------

for some noise term @xmath , uncorrelated with @xmath , and coefficient
@xmath . Typically, if one observes a tracer of the derivative of the
field @xmath , then the vector @xmath would be proportional to @xmath .
We are ignoring here any observational effect, such as incomplete sky
coverage, that would require corrections to this relation. It is clear
from this relation that the spectral matrices of this family take a
special form of equation ( 4.1 ): defining the spectrum of the @xmath
field by @xmath , we obtain by putting this relation ( 4.14 ) into (
2.59 ), that the spectral matrices can be written at each mode in the
form

  -- -------- -- --------
     @xmath      (4.15)
  -- -------- -- --------

where @xmath is the hermitian conjugate of @xmath . The matrix @xmath is
the spectrum of the noise components @xmath ,

  -- -------- -- --------
     @xmath      (4.16)
  -- -------- -- --------

Our subsequent results hold for any family of tracers that obey this
relation. While the special case in ( 4.14 ) enter this category, this
need not be the only instance. All the weak lensing observables we deal
with in this work will satisfy equation ( 4.15 ).
Both the n-dimensional vector @xmath and the noise matrix @xmath can
depend on the wave vector @xmath , but they are independent from the
model parameters. The matrix @xmath of dimension @xmath is the noise
component of the spectra of the fields, typically built from two parts.
The first is due to the discrete nature of the fields, since such data
consist in quantities measured where galaxies sits, and the second to
the intrinsic dispersion of the measured values.

##### 4.2.2 Joint entropy and information content

Information on the model parameters enters through @xmath only. To
evaluate the full information content, we need only evaluate eq. ( 4.13
) with the spectral matrix given in ( 4.15 ), keeping in mind the result
from last section, that we need only the total derivative with respect
to @xmath . In other words, any additive terms in the expression of the
entropy that are independent of @xmath can be discarded.
This determinant can be evaluated immediately. Defining for each mode
the real positive number @xmath through

  -- -------- -- --------
     @xmath      (4.17)
  -- -------- -- --------

which can be seen as an effective noise term, a simple ¹ ¹ 1 We have
namely for any invertible matrix @xmath and vectors @xmath the matrix
determinant lemma,

@xmath (4.18)

calculation shows that the joint entropy ( 4.13 ) is equivalent to the
following, where the @xmath dimensional determinant has disappeared,

  -- -------- -- --------
     @xmath      (4.19)
  -- -------- -- --------

Comparison with equation ( 4.13 ) shows that we have with this equation
( 4.19 ) the entropy of the field @xmath itself, where all the effects
of the joint observation of this @xmath fields have been absorbed into
the effective noise term @xmath , that contaminates its spectrum. It
means that the full combined information in the @xmath probes of the
field @xmath is equivalent to the information in @xmath , observed with
spectral noise @xmath .
Our result ( 4.10 ) applied to ( 4.19 ) puts bounds on reconstruction of
the field @xmath out of the observed samples, which can be at best
reconstructed with a contaminating noise term of @xmath in its spectrum,
whose best unbiased reconstruction is given by

  -- -------- -- --------
     @xmath      (4.20)
  -- -------- -- --------

Since the effect of combining these probes at a single mode is only to
change the model independent noise term, the parameter correlations and
degenaracies as approximated by the Fisher information matrix stay
unchanged, whatever the number of such probes is. We have namely from (
4.19 ) that at a given mode @xmath , the Fisher information matrix reads

  -- -------- -- --------
     @xmath      (4.21)
  -- -------- -- --------

with

  -- -- -- --------
           (4.22)
  -- -- -- --------

From the point of view of the Fisher information, it makes formally no
difference to extract the full set of @xmath independent elements of
each spectral matrices, or reconstruct the field @xmath and extract its
spectrum. They carry indeed the same amount of Fisher information.
These results still hold when other fields are present in the analysis,
which are correlated with the field @xmath . To make this statement
rigorous, consider in the analysis on top of our @xmath samples of the
form ( 4.14 ) of @xmath , another homogeneous field @xmath , with
spectrum @xmath , and cross spectrum to @xmath given by @xmath The full
spectral matrices are in this case

  -- -------- -- --------
     @xmath      (4.23)
  -- -------- -- --------

Again, the determinant of this matrix can be reduced to a determinant of
lower dimension, leading to the equivalent entropy

  -- -------- -- --------
     @xmath      (4.24)
  -- -------- -- --------

It shows that the the full set of @xmath fields can be reduced without
loss to two fields, @xmath and @xmath , with the effective noise @xmath
contaminating the spectrum of @xmath .
Note that the derivation of our results do not refer to any hypothetical
estimators, but came naturally out of the expression of the entropy.

##### 4.2.3 Weak lensing probes

We now seek a quantitative evaluation of the full joint information
content of the weak lensing probes in galaxy surveys, up to second order
in the image distortions of galaxies. The data @xmath consists of a set
of fields, which are discrete point fields, which take values where
galaxies sits. We work in the two-dimensional flat sky limit, using the
more standard notation @xmath for the wave vector, and decompose it in
modulus and polar coordinate as

  -- -------- -- --------
     @xmath      (4.25)
  -- -------- -- --------

We will throughout assume that the intrinsic values of each probe are
pairwise uncorrelated, as commonly done. Also, we will assume that the
set of points on which the relevant quantities are measured show low
enough clustering so that corrections to the spectra due to intrinsic
clustering can be neglected. This is however not a limitation of our
approach, since corrections to the above assumptions, such as the
introduction of some level of intrinsic alignment, can be accommodated
for by introducing appropriate terms in the noise matrices @xmath in (
4.17 ). As a central field to which all our point fields relates, we
take for convenience the isotropic convergence field @xmath , with
spectrum

  -- -------- -- --------
     @xmath      (4.26)
  -- -------- -- --------

In the case of pairwise uncorrelated intrinsic values that we are
following, we see easily from ( 4.17 ) that by combining any number of
such probes the effective noise is reduced at a given mode according to

  -- -------- -- --------
     @xmath      (4.27)
  -- -------- -- --------

We therefore only need to evaluate the effective noise for each probe
separately, while their combination follows ( 4.27 ). To this aim, the
evaluation of the spectral matrices ( 4.15 ), giving us @xmath , is
necessary. The calculations for this are presented in appendix 4.4 and
we use the final results in this section.

###### First order, distortion matrix

To first order, the distortion induced by weak lensing on a galaxy image
is described by the distortion matrix that contains the shear, @xmath ,
and convergence, @xmath , which come from the second derivatives of the
lensing potential field @xmath , (e.g. Schneider et al. , 2006 )

  -- -------- -- --------
     @xmath      (4.28)
  -- -------- -- --------

The shear components read

  -- -------- -- --------
     @xmath      (4.29)
  -- -------- -- --------

and we assume they are measured from the apparent ellipticities of the
galaxies, with identical intrinsic dispersion @xmath . Denoting with
@xmath the number density of galaxies for which ellipticity measurements
are available, the effective noise is

  -- -------- -- --------
     @xmath      (4.30)
  -- -------- -- --------

The information content of the two observed ellipticity fields is thus
exactly the same as the one of the convergence field, with a mode
independent noise term as above.
To reach for the @xmath component of the distortion matrix, we imagine
we have measurements of their angular size @xmath , with intrinsic
dispersion @xmath . The intrinsic sizes of the galaxies @xmath gets
transformed through weak lensing according to

  -- -------- -- --------
     @xmath      (4.31)
  -- -------- -- --------

The coefficient @xmath , is equal to unity in pure weak lensing theory,
but we allow it to take other values, since in a realistic situation,
other effects such as magnification bias effectively enter this
coefficient (see e.g. Vallinotto et al. ( 2010 ) ). Under our assumption
that the correlation of the fluctuations in intrinsic sizes can
themselves be neglected, the effective noise reduces to

  -- -------- -- --------
     @xmath      (4.32)
  -- -------- -- --------

This combination of @xmath with the dispersion parameters @xmath and
@xmath becomes the only relevant parameter in our case, and not the
value of each of them.

###### Second order, flexion

To second order, the distortions caused by lensing on the galaxies
images are given by third order derivatives of the lensing potential.
These are conveniently described by the spin 1 and spin 3 flexion
components @xmath and @xmath , which in the notation of ( Schneider and
Er , 2008 ) read

  -- -------- -- --------
     @xmath      (4.33)
  -- -------- -- --------

and are extracted from measurements with intrinsic dispersion @xmath and
@xmath . The effective noise is this time mode-dependent,

  -- -------- -- --------
     @xmath      (4.34)
  -- -------- -- --------

#### 4.3 Results

Figure 4.1 shows the ratio of the effective noise to the noise present
considering the shear fields only, assuming the same number densities of
galaxies for each probe, and the values for the intrinsic dispersion
stated in table 4.1 . The conversion multipole @xmath (upper x-axis) to
angular scale @xmath (lower x-axis) follows @xmath . We have adopted for
the size dispersion parameters the numbers from ( Vallinotto et al. ,
2010 ) , who evaluated this number for the DES survey conditions ( The
Dark Energy Survey Collaboration , 2005 ) . We refer to the discussion
in ( Pires and Amara , 2010 ) for our choice of flexion dispersion
parameters. The curves on this figure are ratios independent of the
galaxy number density. They are redshift independent as well, only to
the extent that the dispersion in intrinsic values can be treated as
such. We can draw two main conclusions from figure 4.1 . First, flexion
information beings to play role only at the smallest scales, on the
arcsecond scales, where it takes over and becomes the most interesting
probe. On the scale of @xmath amin, it can bring substantial improvement
over shear only analysis, but only in combination with the shears, and
not on its own. This is in good agreement with the comparative analysis
of the power of the flexion @xmath field and shears fields for mass
reconstruction done in Pires and Amara ( 2010 ) , restricted to direct
inversion methods. Second, the inclusion of size of galaxies into the
analysis provides a density independent, scale independent, improvement
factor of

  -- -------- -- --------
     @xmath      (4.35)
  -- -------- -- --------

which is close to a @xmath improvement for the quoted numbers. Of
course, the precise value depends on the dispersion parameters of the
population considered.

For the purpose of measuring cosmological parameters rather than mass
reconstruction, more interesting are the actual values of the Fisher
information matrices. Since with any combination of such probes, these
matrices are proportional to each other at a single mode, it makes sense
to define the efficiency parameter of the probe @xmath through

  -- -------- -- --------
     @xmath      (4.36)
  -- -------- -- --------

which is a measure of what fraction of the information contained in the
convergence field is effectively captured by that probe. The information
in the convergence field is, at a given mode @xmath , counting the
multiplicity of the mode,

  -- -------- -- --------
     @xmath      (4.37)
  -- -------- -- --------

and we have indeed that the total Fisher information in the observed
fields is

  -- -------- -- --------
     @xmath      (4.38)
  -- -------- -- --------

Therefore, according to the interpretation of the Fisher matrix
approximating the expected constraints on the model parameters, the
factor @xmath is precisely equal to the factor of degradation in the
constraints one would be able to put on any a parameter, with respect to
the case of perfect knowledge of the convergence field at this mode. It
is not the purpose of this work to perform a very detailed study on the
behavior of the efficiency parameter for some specific survey and the
subsequent statistical gain, but its qualitative behavior is easy to
see. This parameter is essentially unity in the high signal to noise
regime, while it is the inverse effective noise whenever the intrinsic
dispersion dominates the observed spectrum. Since information on
cosmological parameters is beaten down by cosmic variance in the former
case, the latter dominates the constraints. We can therefore expect from
our above discussion the size information to tighten by a few percent
constraints on any cosmological parameter. On the other hand, while
flexion becomes ideal for mass reconstruction purposes on small scales,
it will be able to help inference on cosmological parameters only if the
challenge of very accurate theoretical predictions on the convergence
power spectrum for multipoles substantially larger than @xmath will be
met.
To make these expectations more concrete, we evaluated the improvement
in information on cosmological parameters performing a lensing Fisher
matrix calculation for a wide, EUCLID-like survey, in a tomographic
setting. For a data vector consisting of @xmath probes of the
convergence field @xmath in each redshift bin @xmath , it is simple to
see following our previous argument, that the Fisher information reduces
to

  -- -------- -- --------
     @xmath      (4.39)
  -- -------- -- --------

where the @xmath matrix is given by

  -- -------- -- --------
     @xmath      (4.40)
  -- -------- -- --------

with @xmath given by ( 4.17 ). The only difference between standard
implementations of Fisher matrices for lensing, such as the lensing part
of Hu and Jain ( 2004 ) , being thus the form of the noise component. we
evaluated these matrices respectively for

  -- -------- -- --------
     @xmath      (4.41)
  -- -------- -- --------

which is the precise form of the Fisher matrix for shear analysis, for

  -- -------- -- --------
     @xmath      (4.42)
  -- -------- -- --------

which account for size information, and

  -- -------- -- --------
     @xmath      (4.43)
  -- -------- -- --------

which accounts for the flexion fields as well. We note that in terms of
observables, these small modifications incorporate in its entirety the
full set of all possible correlations between the fields considered. The
values of the dispersion parameters involved in these formulae are the
same as in table ( 4.1 ). Our fiducial model is a flat @xmath CDM
universe, with parameters @xmath , @xmath , @xmath , power spectrum
parameters @xmath , and Chevallier- Polarski-Linder parametrisation (
Chevallier and Polarski , 2001 ; Linder , 2003 ) of the dark energy
equation of state implemented as @xmath . The distribution of galaxies
as function of redshift needed both for the calculation of the spectra
and to obtain the galaxy densities in each bin was generated using the
cosmological package iCosmo ( Refregier et al. , 2008 ) , in a way
described in ( Amara and Réfrégier , 2007 ) . We adopted EUCLID-like
parameters of @xmath redshift bins, a median redshift of @xmath , a
galaxy angular density of @xmath , and photometric redshift errors of
@xmath .
In figure 4.2 , we show the improvement in the dark energy Figure of
Merit (FOM), defined as the square root of the determinant of the
submatrix @xmath of the Fisher matrix inverse @xmath ( @xmath and @xmath
running over the set of eight parameters as described above), as
function of the maximal angular mode @xmath considered, while @xmath
being always taken to be 10. In perfect agreement with our discussion
above, including size information (solid line) increases the FOM
steadily until it saturates at a @xmath improvement when constraints on
the dark energy parameters are dominated by the low signal to noise
regime. Also, flexion becomes only useful in the deep non-linear regime,
where however theoretical understanding of the shape of the spectra
still leaves a lot to be desired.

These results are found to be very insensitive to the survey parameters,
for a fixed @xmath . There are also only weakly model parameter
independent, as illustrated in table 4.2 , which shows the corresponding
improvement in Fisher constraints,

  -- -------- -- --------
     @xmath      (4.44)
  -- -------- -- --------

at the saturation scale @xmath . These results are also essentially
unchanged using either standard implementations of the halo model (
Cooray and Sheth , 2002 , for a review) or the the HALOFIT ( Smith
et al. , 2003 ) non linear power spectrum.

#### 4.4 Appendix

The data consists in a set of numbers, at each position where a galaxy
sit and a measurement was done. We use the handy notation in terms of
Dirac delta function,

  -- -------- -- --------
     @xmath      (4.45)
  -- -------- -- --------

where the sum runs over the positions @xmath for which @xmath is
measured. To obtain the spectral matrices, we need the Fourier transform
of the field, which reads in our case

  -- -------- -- --------
     @xmath      (4.46)
  -- -------- -- --------

In this work, we assume that the set of points shows negligible
clustering, so that the probability density function for the joint
occurrence of a particular set of galaxy positions is uniform.
We decompose in the following the wave vector @xmath on the flat sky in
terms of its modulus and polar angle as

  -- -------- -- --------
     @xmath      (4.47)
  -- -------- -- --------

###### Ellipticities

When the two ellipticity components are measured, we have two such
fields @xmath at our disposal. For instance, the field describing the
first component becomes

  -- -------- -- --------
     @xmath      (4.48)
  -- -------- -- --------

We assume that the measured ellipticities trace the shear fields, in the
sense that the measured components are built out of the shear at that
position plus some value unrelated to it,

  -- -------- -- --------
     @xmath      (4.49)
  -- -------- -- --------

The vector @xmath relating the spectral matrices of the ellipticities
and the convergence is then obtained by plugging ( 4.48 ) with the above
relations ( 4.49 ) in its definition ( 4.15 ), and using the relation
between shears and convergence in equation ( 4.28 ). The result is

  -- -------- -- --------
     @xmath      (4.50)
  -- -------- -- --------

where @xmath is the number density of galaxies for which ellipticity
measurements are available. Under our assumptions of uncorrelated
intrinsic ellipticities, with dispersions of equal magnitude @xmath for
the two components, the noise matrix @xmath becomes

  -- -------- -- --------
     @xmath      (4.51)
  -- -------- -- --------

The effective noise, given in equation ( 4.17 ) is readily computed

  -- -------- -- --------
     @xmath      (4.52)
  -- -------- -- --------

###### Sizes

As noted in the main text, the apparent sizes of galaxies are modified
by lensing, in the following way,

  -- -------- -- --------
     @xmath      (4.53)
  -- -------- -- --------

for some coefficient @xmath which is unity in pure weak lensing theory.
Denoting the number of galaxies for which sizes measurements are
available by @xmath , and the mean intrinsic size of the sample by
@xmath , the spectrum of the size field reduces, under the assumption of
uncorrelated intrinsic sizes, to

  -- -------- -- --------
     @xmath      (4.54)
  -- -------- -- --------

The vector @xmath and matrix @xmath are now numbers, that are read out
from the above equation, to be

  -- -------- -- --------
     @xmath      (4.55)
  -- -------- -- --------

leading to the effective noise

  -- -------- -- --------
     @xmath      (4.56)
  -- -------- -- --------

###### Second order, flexion

Denoting with @xmath and @xmath the number of galaxies for which @xmath
and @xmath are measured, the vectors linking the flexion to convergence
are

  -- -------- -- --------
     @xmath      (4.57)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (4.58)
  -- -------- -- --------

Using again the assumption of uncorrelated intrinsic components, we have
the four dimensional diagonal noise matrix

  -- -------- -- --------
     @xmath      (4.59)
  -- -------- -- --------

leading to the effective noise, this time mode-dependent,

  -- -------- -- --------
     @xmath      (4.60)
  -- -------- -- --------

### Chapter 5 On the use of Gaussian likelihoods for power spectra
estimators

The text of this chapter follows very closely that of Carron ( 2012a ) .
In this note we revisit the Fisher information content of cosmological
power spectra of Gaussian fields, when based on the assumption of a
multivariate Gaussian likelihood for estimators, in order to comment on
that assumption. We discuss that despite the fact that the assumption of
a Gaussian likelihood is motivated by the central limit theorem, it
leads if used consistently to a Fisher information content that violates
the Cramér-Rao inequality, due to the presence of independent
information from the parameter dependent covariance matrix. At any fixed
multipole, this artificial term is shown to become dominant in the limit
of a large number of correlated fields. While the distribution of the
estimators does indeed tend to a Gaussian with a large number of modes,
it is shown, however, that its Fisher information content does not, in
the sense that their covariance matrix never carries independent
information content. The reason why the information content of the
spectra is correctly described by the usual formula (i.e. without the
covariance term) in this estimator perspective is precisely the fact the
the estimators have a chi-squared like distribution, and not a Gaussian
distribution. The assumption of a Gaussian estimators likelihood is thus
from the point of view of the information neither necessary nor really
adequate, and we warn against the use of Gaussian likelihoods with
parameter dependent covariance matrices for parameter inference from
such spectra.

#### 5.1 Introduction

Starting from the second half of the nineties ( Jungman et al. , 1996a ,
b ; Tegmark , 1997 ; Tegmark et al. , 1997 ) , the calculation of Fisher
information matrices in order to understand quantitatively the
constraining power of an experiment has become ubiquitous in cosmology,
with its fundamental aspects now covered in cosmological textbooks (
Dodelson , 2003 ; Durrer , 2008 , section 11 and 6 respectively, e.g.) ,
or ( Heavens , 2009 ) . This is especially true for experiments aimed at
measuring power spectra of close to Gaussian fields, since in this case
very handy analytical expressions can be obtained that can be applied in
a variety of major cosmological subfields, such as for instance the CMB,
galaxy clustering, weak lensing as well as their combination.
Nevertheless, even applied to Gaussian variables, Fisher information
matrices are not totally exempt of subtleties. In this note, we revisit
the two different possible perspectives on the Fisher information
content of such spectra. One starting point is often the assumption of
Gaussian errors. We point out that this assumption of a multivariate
Gaussian likelihood for power spectra estimators is not fully consistent
for the purpose of understanding their information content, due to a
term violating the Cramér-Rao inequality, that we show is not
necessarily small. Too much information is therefore assigned to the
spectra under this assumption. We show that we can understand why this
term is artificial precisely from the non Gaussian properties of the
estimators, and discuss the reasons why the usual formula, i.e. without
this term, or setting the covariance matrix to be parameter independent,
still gives the correct amount of information.
The note is built as follows : In section 5.2 the two common approaches
to the information content of spectra are discussed in details in the
case of a single field. We clarify to what extent and why one is
actually flawed, which is the source our comments on the use of Gaussian
likelihoods for spectra. In section 5.3 we then turn to a correlated
family of fields, where the violation of the Cramér-Rao inequality is
shown to become substantial. We summarize and conclude in section 5.4 .
We recall first the specific form of the Fisher information matrix,
defined for a probability density function @xmath as @xmath model
parameters of interest, in the particular case of a multivariate
Gaussian distribution with mean vector @xmath and covariance matrix
@xmath , ( Vogeley and Szalay , 1996 ; Tegmark et al. , 1997 )

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

Remember that the Fisher information matrix has all the properties a
meaningful measure of information on parameters must have, most
importantly for us here the fact that any transformation of the data can
only decrease its Fisher information matrix.

#### 5.2 One field, gamma distribution

Consider a zero mean isotropic homogeneous Gaussian random field, in
euclidean space or on the sphere. It is well known that the Gaussianity
of the field is equivalent to the fact that the Fourier or spherical
harmonic coefficients are independent complex Gaussian variables, only
constrained by the reality condition. Equivalently, the real and
imaginary parts of those coefficients form independent real Gaussian
variables. Such fields are entirely described by their spectrum, and so
the extraction of the spectrum from the data with the help of an
estimator is a fairly natural way to proceed for inference on parameters
of interest. We place ourselves on the sphere, adopting the spherical
harmonic notation for convenience. With the set of @xmath the harmonic
coefficients, the model parameter dependent spectrum @xmath is defined
as

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

Standard, unbiased quadratic estimators can be written as a sum over the
number of Gaussian modes available, as

  -- -------- -- -------
     @xmath      (5.3)
  -- -------- -- -------

We do not consider any source of observational noise, incomplete
coverage or any other such issue, which are irrelevant for the points of
our discussion. At this point, there are two ways to approach the
problem of evaluating its information content in the cosmological
literature. The first, - let us call this approach the ’field
perspective’ -, first calculates the information content of the field
itself (equal to that of the set of @xmath ’s), and then interprets this
information as being the one of the spectrum. In this case, the
information in the field is given by formula ( 5.1 ), with zero mean
vector and diagonal covariance matrix @xmath .

  -- -------- -- -------
     @xmath      (5.4)
  -- -------- -- -------

where the factor @xmath accounts for the number of independent Gaussian
variables at a given multipole @xmath . The sum is in practice
restricted to the multipole range that will actually be measured to
obtain the information in the spectrum to be extracted. A very small
sample of works in this approach are Tegmark et al. ( 1997 ) ; Hu and
Jain ( 2004 ) ; Bernstein ( 2009 ) . This approach is arguably
conceptually appealing, as it deals with the information content of the
field itself, and does not require the definition of estimators nor the
calculation of their covariance. However, for the same reasons, it is
only indirectly connected to data analysis as it is not yet specified
precisely how this information content is to be extracted.
In the second approach - that we call the ’estimator perspective’ - is
defined first an estimator @xmath for each @xmath to be extracted,
within some @xmath and @xmath (maybe with some bandwidth that we ignore
here), and its covariance matrix @xmath is calculated. Then it is argued
that due to the central limit theorem, the distribution of the estimator
will be approximately Gaussian. In the case of spectra of Gaussian
fields, this is very well founded, at least for small scales modes,
since ( 5.3 ) is a large sum of well behaved identically distributed
independent variables. Then, under this assumption of Gaussianity, their
information content is given by equation ( 5.1 ) with mean vector this
time the set of @xmath ’s itself and (model parameter dependent)
covariance matrix @xmath ,

  -- -------- -- -------
     @xmath      (5.5)
  -- -------- -- -------

It is well known that for the estimator ( 5.3 ) we have @xmath . The
Fisher information matrix, in the estimator perspective, reduces thus to

  -- -------- -- -------
     @xmath      (5.6)
  -- -------- -- -------

Clearly, the first term in the estimator perspective corresponds to that
of the field perspective. However, the second term, coming from the
derivative of the covariance matrix, is new. That term is not enhanced
by a @xmath factor, and is therefore very subdominant at high @xmath .
It is either usually neglected, or the covariance matrix of the
estimators is inconsistently taken to be parameter independent, and in
these cases the two approaches give the same results. Some expositions
using explicitly this perspective include ( Tegmark , 1997 ; Seo and
Eisenstein , 2003 , 2007 ) , where the additional term is neglected, or
the approach in ( Dodelson , 2003 , section 11.4.3) , where the
covariance matrix is treated as parameter independent. A work where this
term plays a direct role is ( Eifler et al. , 2009 ) , where the authors
specifically study the impact of parameter dependent covariance matrices
for parameter estimation using such Gaussian likelihoods.
Beyond the question of the quantitative relevance of this additional
term, its very appearance is however very disturbing. Under this
arguably reasonable Gaussian assumption, our estimator ( 5.3 ) is found
to carry more information than the full field, even on the smallest
scales. This obviously violates the most fundamental property of Fisher
information, i.e. that information can only be at best conserved when
transforming the data (in this case reducing the field to its spectrum),
a fact essentially equivalent to the celebrated Cramér-Rao inequality (
Tegmark et al. , 1997 ) . Something must clearly have gone wrong in the
assumption of a Gaussian likelihood for our spectra.
To understand what has happened, it is worth tracking the exact
distribution and information content of the estimator ( 5.3 ). Since
they are independent at different @xmath , we can work at a fixed @xmath
, and the total information content of these estimators will simply be
the sum over @xmath of the information of the estimator at fixed @xmath
. Under our assumptions, the estimator is a sum of squares of @xmath
independent Gaussian variables, and its probability density function can
be obtained with no difficulty. The exact distribution is the gamma
probability density function with shape parameter @xmath and location
parameter @xmath as follows

  -- -------- -- -------
     @xmath      (5.7)
  -- -------- -- -------

with

  -- -------- -- -------
     @xmath      (5.8)
  -- -------- -- -------

and where @xmath is the gamma function. It is well known that the gamma
distribution does indeed tend towards the Gaussian distribution for
large @xmath , with mean @xmath and variance @xmath , as expected.
However, its Fisher information content does not tend to that of the
Gaussian. In our case, since only @xmath is parameter dependent, we have
that the Fisher information in the estimator density function ( 5.7 ) is

  -- -------- -- -------
     @xmath      (5.9)
  -- -------- -- -------

Since @xmath , and @xmath , we obtain with straightforward algebra

  -- -------- -- --------
     @xmath      (5.10)
  -- -------- -- --------

Summing over @xmath , we recover the first term of ( 5.6 ), but not the
second. We have recovered the field perspective result ( 5.4 ) at any
@xmath without the Gaussian assumption but with the exact distribution.
It turns out that even though the variance of the gamma distribution is
parameter dependent, it does not in fact contribute to the information.
This can be seen as the following. Consider the information in the mean
only of the estimator. From the Cramér-Rao inequality this must be less
than the total information,

  -- -------- -- --------
     @xmath      (5.11)
  -- -------- -- --------

Plugging in the values for the mean and variance leads in fact to the
result that the inequality is an equality, so that the mean of the
estimator captures all of its information.
In summary, the Gaussian approximation assumes the mean and the variance
of the estimator are uncorrelated, such that both contributes to the
information, while for the exact gamma, they are degenerate in such a
way that the variance does not carry independent information. Another
way to see this, that we will use below when the exact form of the
distribution will be less convenient, comes from the fact that @xmath is
a first order polynomial in @xmath . It can be shown namely that the
first @xmath moments capture all the information precisely when @xmath
is a polynomial of order @xmath ( Carron , 2011 ) .

#### 5.3 Several fields, Wishart distribution

It is instructive to see how these considerations generalize to a
situation of a family of @xmath jointly zero mean Gaussian correlated
fields, where the analysis proceeds through the extraction of the
spectra and cross spectra. In this case, the @xmath of the above
discussion becomes a @xmath (possibly complex) Hermitian matrix

  -- -------- -- --------
     @xmath      (5.12)
  -- -------- -- --------

From the hermiticity property there are only @xmath non redundant
spectra. Adequate estimators are defined by a straightforward
generalization of equation ( 5.3 ),

  -- -------- -- --------
     @xmath      (5.13)
  -- -------- -- --------

While the estimators are still independent for different @xmath ’s, the
different components at a given @xmath are not. The information content
of the set of @xmath in the field perspective is still given by formula
( 5.1 ) for zero mean Gaussian variables. Explicitly, at a given @xmath
,

  -- -------- -- --------
     @xmath      (5.14)
  -- -------- -- --------

In the estimator perspective, assuming the estimators @xmath are jointly
Gaussian, we have instead

  -- -------- -- --------
     @xmath      (5.15)
  -- -------- -- --------

where the covariance matrix is

  -- -------- -- --------
     @xmath      (5.16)
  -- -------- -- --------

While it may not be immediately obvious this time, it has been noted (
Hu and Jain , 2004 , e.g.) that the first term in ( 5.15 ) is rigorously
equivalent to the expression from the field perspective ( 5.14 ). The
estimator perspective under the assumption of a multivariate Gaussian
distribution for @xmath thus still violates the Cramér-Rao inequality
due the presence of the second term. Since since this term is not
enhanced by a factor of @xmath we expect it to be subdominant again.
However, it is less true this time than in the one dimensional setting :
using the explicit form of the inverse covariance matrix,

  -- -- -- --------
           (5.17)
  -- -- -- --------

one can derive with some lengthy but straightforward algebra the
following expression for the violating term,

  -- -------- -- --------
     @xmath      (5.18)
  -- -------- -- --------

for any number @xmath of fields. If @xmath , we recover indeed ( 5.6 ).
While the term is still subdominant at high @xmath , the situation is
yet a bit less comfortable. The number of fields is not necessary very
small in cosmologically relevant situations, such as tomographic joint
shear and galaxy densities analysis in redshift slices, to which one may
also add magnification, flexion fields, etc. Writing schematically
@xmath , where @xmath is the number of bins and @xmath the number of
fields per bin, we have e.g. @xmath for the galaxy density and the two
shear fields, @xmath including magnification, @xmath adding
hypothetically the four flexion fields, and so on. Comparing ( 5.14 )
and ( 5.18 ), and neglecting the second term in ( 5.18 ), we have that
at

  -- -------- -- --------
     @xmath      (5.19)
  -- -------- -- --------

the Cramér-Rao violating term is actually still the dominant one. Note
that this is still optimistic. Due to the product of two traces in the
second term in ( 5.18 ), one can expect roughly the same scaling with
@xmath as the first term. Thus, the correct @xmath in ( 5.19 ) may
generically be closer to

  -- -------- -- --------
     @xmath      (5.20)
  -- -------- -- --------

From the discussion in section 5.2 , we can easily guess what went
wrong. Consider the information content of the means of the estimators
exclusively. This is given for any probability density function by
weighting the derivatives of the means with the inverse covariance
matrix, and is thus equal to the correct, first term in ( 5.15 ). Since
already the means of the estimators do exhaust the information in the
field, we can therefore already conclude that the total information
content of the estimators must be equal to that of their means, and in
particular that the covariance does not contribute to the information.
As before, the second term in the estimator perspective is an artifact
of the Gaussian assumption. It is interesting though to derive as above
more explicitly why only the means carry information, from the shape of
the joint probability density of the estimators. The remainder of this
section sketches how this can be simply performed, leading to equation (
5.26 ).
We restrict ourselves now for the sake of notation to the case of two
fields, @xmath , but the following argumentation holds for any @xmath .
The exact joint distribution for the three estimators @xmath , is given
from the rules of probability theory as

  -- -------- -- --------
     @xmath      (5.21)
  -- -------- -- --------

where @xmath is the Dirac delta function. The average is over the joint
probability density for the two sets of harmonic coefficients @xmath and
@xmath . Define the vector

  -- -------- -- --------
     @xmath      (5.22)
  -- -------- -- --------

Since the @xmath are zero mean Gaussian variables with correlations as
given in ( 5.12 ), this probability density function is given by

  -- -------- -- --------
     @xmath      (5.23)
  -- -------- -- --------

with

  -- -------- -- --------
     @xmath      (5.24)
  -- -------- -- --------

where @xmath is the unit matrix of size @xmath . @xmath is the
normalization of the density for @xmath , that does depend on the model
parameters through the determinant of the @xmath matrix. The inverse
matrix @xmath has the same block structure, with entries being those of
@xmath . In the following we are not really interested in keeping track
of the exact value of the components of this matrix, but only that they
are dependent on the model parameters. With the understanding that
@xmath , we have thus, due to the sparse structure of the @xmath matrix
and the Dirac delta functions in ( 5.21 ),

  -- -------- -- --------
     @xmath      (5.25)
  -- -------- -- --------

Due to the presence of the Dirac delta functions, we can thus take the
exponential ( 5.23 ) out of the integral in ( 5.21 ). Writing explicitly
the dependency of the different terms on @xmath and the model
parameters, we obtain the following form

  -- -------- -- --------
     @xmath      (5.26)
  -- -------- -- --------

which generalizes the gamma distribution, equation ( 5.7 ), in this
multidimensional case. The factor @xmath is what is left from the
integral ( 5.21 ) when the density for the set of @xmath is taken out,
i.e. the volume of the space spanned by the @xmath ’s that satisfies the
constraints set by the Dirac delta function. It is thus a factor that
depends on @xmath but importantly for us not on the model parameters ¹ ¹
1 The prefactors in ( 5.26 ) can be obtained in closed form, leading to
the Wishart density function. See ( Hamimeche and Lewis , 2008 , e.g.) .
The point of the representation ( 5.26 ) is that it is immediate that
@xmath is a polynomial first order in the components of @xmath . Second
order terms, corresponding to information within the covariance matrix
never appear, however close to a Gaussian the exact density function
might be. It follows that the total Fisher information matrix is always
equal to that of the mean, even if we did not derive the exact shape of
the distribution.

#### 5.4 Summary and conclusions

We discussed two common perspectives (the ’field’ and ’estimator’
perspectives) on the Fisher information content of cosmological power
spectra, and why in the estimator perspective the assumption of a
Gaussian likelihood of the spectra estimators violates the Cramér-Rao
inequality, assigning the estimators more information than there is in
the full underlying fields. Under the assumption of Gaussianity of the
estimators, their means and covariance matrix are artificially rendered
uncorrelated, creating an additional piece of information in their
covariance, that we showed was inexistent by calculating the exact
information content of the estimators true probability density function.
We showed that this violating term can become dominant in the limit of a
large number of fields. Using Gaussian likelihoods consistently, i.e.
with parameter dependent covariance matrices, as argued for example in (
Eifler et al. , 2009 ) , assigns therefore far too much information to
the spectra in this regime, and should thus be avoided.
In the estimator perspective of the derivation of the Fisher information
matrix, this term is usually neglected. This note clarifies why it
should not be present in the very first place, and how the agreement
between the field and estimator perspective can thus arguably be seen as
an happy cancellation of two inconsistencies. It is interesting to note
that the reason why we still find the exact result in the estimator
perspective without this wrong piece is that this expression is also the
exact Fisher information content of the exact, for low @xmath strongly
non Gaussian, distribution of the estimators, the central limit theorem
playing actually no role.
The other lesson we can take from this work is that in general, when in
doubt about the joint distribution of a set of estimators, a safe choice
of information content is always that of their means exclusively, which
requires only the knowledge of their covariance. Provided the covariance
matrix is correctly chosen, one is indeed sure for any probability
density function from the properties of Fisher information to make a
conservative evaluation, that does not rely on any further assumptions
on its shape. Thus, leaving apart the question of the very accuracy of
the approximation itself, using a Gaussian likelihood with parameter
independent covariance matrix, having the entire information in the
means, while not entirely consistent remains a safe prescription in the
sense that a conservative information content is always assigned to the
estimators.

### Chapter 6 @xmath-point functions in lognormal density fields

In this chapter we discuss extensively the information content of the
lognormal field as a model for the matter fluctuation field in
cosmology. It is built out of published as well as yet unpublished
elements. The text in sections 6.1 and 6.4 follows closely that of
Carron ( 2011 ) , and that of sections 6.1.1 , 6.1.2 , 6.3 and 6.5 that
of Carron and Neyrinck ( 2012 ) . On the other hand, sections 6.2 , 6.6
and 6.7 present unpublished material.

#### 6.1 Introduction

The cosmological matter density field is becoming more and more directly
accessible to observations with the help of weak lensing ( Schneider
et al. , 1992 ; Bartelmann and Schneider , 2001 ; Refregier , 2003 ;
Munshi et al. , 2006 ) . Its statistical properties are the key element
in trying to optimize future large galaxy surveys aimed at answering
actual fundamental cosmological questions, such as the nature of the
dark components of the universe ( Caldwell and Kamionkowski , 2009 ;
Frieman et al. , 2008 ) . To this aim, Fisher’s measure of information
on parameters ( Fisher , 1925 ; Rao , 1973 ; van den Bos , 2007 ) has
naturally become of standard use in cosmology. It provides indeed an
handy framework, in which it is possible to evaluate in a quantitative
manner the statistical power of some experiment configuration aimed at
some observable ( Tegmark et al. , 1997 ; Tegmark , 1997 ; Hu and
Tegmark , 1999 ; Hu and Jain , 2004 ; Amara and Réfrégier , 2007 ;
Parkinson et al. , 2007 ; Albrecht et al. , 2006 ; Bernstein , 2009 ,
e.g.) . Such studies are in the vast majority of cases limited to
Gaussian probability density functions, or perturbations therefrom, and
deal mostly with the prominent members of the correlation function
hierarchy ( Peebles , 1980 ) , or equivalently their Fourier transforms
the polyspectra, such as the matter power spectrum.
The approach via the correlation function hierarchy is very sensible in
the nearly linear regime for at least two reasons. First, in principle,
the correlations are the very elements that cosmological perturbation
theory is able to predict in a systematic manner (see Bernardeau et al.
