##### Different Approaches to Node Repair

The conventional node repair of the ubiquitous Reed-Solomon (RS) code is
inefficient in that in an @xmath RS code having block length @xmath and
dimension @xmath , the repair bandwidth equals @xmath times the amount
of data stored in the replacement node and the repair degree equals
@xmath , both of which are excessively large. In response, coding
theorists have come up with different approaches to handle the problem
of node repair. Two new classes of codes have sprung up, termed as
regenerating (RG) codes and locally recoverable (LR) codes respectively
that provide erasure codes which minimize respectively the repair
bandwidth and repair degree. A third class termed as locally
regenerating (LRG) codes , combines the desirable features of both RG
and LR codes and offers both small repair bandwidth as well as a small
value of repair degree.

In a different direction, coding theorists have taken a second, closer
look at the RS code and have devised efficient approaches to node repair
in RS code. Yet another direction is that adopted by liquid storage
codes which employ a lazy strategy approach to node repair to achieve a
fundamental bound on information capacity.

Fig. 1 provides a classification of the various classes of codes that
have been developed by coding theorists to address the problem of node
repair. The boxes outlined in red in Fig. 1 , correspond to the classes
of codes towards which this thesis has made significant contributions.
The primary contributions are identified by a box that is outlined in
red which is fully written in upper-case lettering.

###### Contributions of the Thesis

As noted above, this thesis makes contributions that advance the theory
of both RG and LR codes. We begin with LR codes, since the bulk of the
contributions of the thesis relate to this class of codes (see Fig. 2 ).
In the following, a local code refers to a punctured code of an @xmath
code of block length @xmath and dimension strictly less than block
length where @xmath .

###### Contributions to LR Codes

LR codes are designed with the objective of reducing the repair degree
and accomplish this by making sure that the overall erasure code has
several local codes in such a way that any single erased code symbol can
be repaired by containing at most @xmath other code symbols. The
parameter @xmath is termed the locality parameter of the LR code. Our
contributions in the direction of an LR code are aimed at LR codes that
are capable of handling multiple erasures efficiently. Improved bounds
on both the rate @xmath of an LR code as well as its minimum Hamming
distance @xmath are provided. We provide improved bounds under a
constraint on the size @xmath of the code-symbol alphabet and also
provide improved bounds without any constraint on the size @xmath of the
code-symol alphabet.

###### LR Codes for Multiple Erasures

The initial focus in the theory of LR codes was the design of codes that
can recover from the erasure of a single erased symbol efficiently.
Given that constructions that match the bounds on performance metrics on
LR codes are now available in the literature, the attention of the
academic community has since shifted in the direction of the design of
LR codes to handle multiple erasures . An LR code is said to recover
multiple erasures if it can recover from multiple simulataneus erasures
by accessing a small number @xmath of unerased code symbols. A strong
motivation for developing the theory of LR codes which can handle
multiple-erasure comes from the notion of availability because a sub
class of LR codes called @xmath -availability codes has the ability to
recover from @xmath simultaneous erasures and also has the interesting
property called availability which is explained in the following. In a
data center, there could be storage units that hold popular data for
which there could be several simultaneous competing demands. In such
situations, termed in the industry as a degraded read , the single-node
repair capability of an erasure code is called upon to recreate data
that is unavailable on account of multiple, competing demands for its
data. This calls for an ability to recreate multiple copies, say @xmath
, of the data belonging to the unavailable node. To reduce latency,
these multiple recreations must be drawn from disjoint sets of code
symbols. This property called availability is achieved by @xmath
-availability codes. An LR code constructed in such a way that for each
code symbol @xmath there is a set of @xmath local codes such that any
two out of the @xmath local codes have only this code symbol @xmath in
common, is termed as a @xmath -availability code .

###### Contributions to Availability Codes

The contributions of the thesis in the direction of @xmath -availability
codes include improved upper bounds on the minimum distance @xmath of
this class of codes, both with and without a constraint on the size
@xmath of the code-symbol alphabet. An improved upper bound on code rate
@xmath is also provided for a subclass of @xmath -availability codes,
termed as codes with strict availability . Among the class of @xmath
-availability codes, codes with strict availability typically have high
rate. A complete characterization of optimal tradeoff between rate and
fractional minimum distance for a special class of @xmath -availability
codes is also provided.

###### Contributions to LR Codes with Sequential Recovery

Since a @xmath -availability code also has the ability to recover from
@xmath simultaneous erasures. This leads naturally to the study of other
LR codes that can recover from multiple, simultaneous erasures. There
are several approaches to handling multiple erasures. We restrict
ourselves to a subclass of LR codes which can recover from multiple
erasures where we use atmost @xmath symbols for recovering an erased
symbol. Naturally the most general approach in this subclass of LR codes
is one in which the LR code recovers from a set of @xmath erasures by
repairing them one by one in a sequential fashion, drawing at each stage
from at most @xmath other code symbols. Such codes are termed as LR
codes with sequential recovery and quite naturally, have the largest
possible rate of any LR code that can recover from multiple erasures in
the subclass of LR codes we are considering. A major contribution of the
thesis is the derivation of a new tight upper bound on the rate of an LR
code with sequential recovery. While the upper bound on rate for the
cases of @xmath was previously known, the upper bound on rate for @xmath
is a contribution of this thesis. This upper bound on rate proves a
conjecture on the maximum possible rate of LR codes with sequential
recovery that had previously appeared in the literature, and is shown to
be tight by providing construction of codes with rate equal to the upper
bound for every @xmath and every @xmath .

Other contributions in the direction of codes with sequential recovery,
include identifying instances of codes arising from a special sub-class
of @xmath -regular graphs known as Moore graphs, that are optimal not
only in terms of code rate, but also in terms of having the smallest
block length possible. Unfortunately, Moore graph exists only for a
restricted set of values of @xmath and girth (length of cycle with least
number of edges in the graph). This thesis also provides a
characterization of codes with sequential recovery with rate equal to
our upper bound for the case @xmath as well as an improved lower bound
on block length for the case @xmath .

###### Contributions to RG Codes

An RG code derives its ability to minimize the repair bandwidth while
handling erasures from the fact that these codes are built over a vector
symbol alphabet for example over @xmath for a finite field @xmath and
some @xmath . The necessary value of the size @xmath of this vector
alphabet, also termed as the sub-packetization level of an RG code,
tends to grow very large as the rate of the RG code approaches @xmath ,
corresponding to a storage overhead which also approaches @xmath . In
practice, there is greatest interest in high-rate RG codes and hence
there is interest in knowing the minimum possible value of @xmath . An
optimal-access RG code is an RG code in which during node repair, the
number of scalar code symbols accessed at a helper node (i.e., for
example the number of symbols over @xmath accessed in a vector code
symbol from @xmath ) equals the number of symbols passed on by the
helper node to the replacement node. A node repair satisfying this
property is called repair by help-by-transfer. This has the practical
importance that no computation is needed at a helper node. The number of
helper nodes contacted during a node repair is usually denoted by @xmath
in the context of RG codes. A sub-class of optimal access RG codes
called optimal-access Minimum Storage RG (MSR) codes refers to optimal
access RG codes which are also vector MDS codes. In this thesis, we
provide a tight lower bound on the sub-packetization level of
optimal-access MSR codes. We do the same for Maximum Distance Separable
(MDS) codes over a vector alphabet, which are designed to do repair by
help-by-transfer for repairing any node belonging to a restricted subset
of nodes, with minimum possible repair bandwidth. We refer to these
codes as optimal access MDS codes. In both cases, we point to the
literature on sub-packetization level of existing RG codes to establish
that the bounds on sub-packetization level derived in this thesis are
tight. See Fig. 3 for a summary of contributions of the thesis to RG
codes. The equations used in the derivation of our lower bound on
sub-packetization level @xmath in the case of optimal access MDS codes,
also provides information on the structure of such codes with
sub-packetization level equal to our lower bound. The suggested
structure is present in a known construction of a high-rate,
optimal-access MSR code having least possible sub-packetization level.

###### Contributions to Maximal Recoverable (MR) Codes

Returning to the topic of LR codes, we note that an LR code is
constrained by a set of parity checks that give the code the ability to
recover from the erasure of any given code symbol by connecting to at
most @xmath other code symbols. We call this set of parity checks as
local parity checks. The code with only local parity checks imposed on
it typically result in a code of dimension @xmath that is larger than
the dimension of the desired code. Thus one has the option of adding
additional parity checks to bring the dimension down to @xmath .
Naturally, these additional parity checks are added so as to give the
code the ability to recover from additional erasure patterns as well,
for example recover from any pattern of @xmath erasures, without any
constraint on the number of helper nodes contacted during the recovery
from this larger number of erasures. MR codes are the subclass of LR
codes which given local parity checks and the desired overall dimension
@xmath , have the ability to recover from all possible erasure patterns
which are not precluded by the local parity checks. It is an interesting
and challenging open problem to construct MR codes having code symbol
alphabet of small size. Our contributions in this area are constructions
of MR codes over finite fields of small size.

There are several other contributions in the thesis, that are not
described here for lack of space.

\advisers

P. Vijay Kumar \submitdate June 2018 \dept Electrical Communication
Engineering \enggfaculty \iisclogotrue \tablespagetrue {dedication}

Dedicated to

[2em] My Mother, Father, Brother,

My Brother’s wife and Little Samyukhtha.

If numbers aren’t beautiful, I don’t know what is.

###### Acknowledgements. I would like to thank my mother, father,
brother for being kind to me during tough times. I would like to thank
my advisor Prof. P. Vijay Kumar for helping me to continue my PhD during
a rough patch of time. I would like to thank all my well wishers who
helped me technically or non-technically duing my tenure as a PhD
student at IISc. I would like to thank my collabotators Prashanth,
Ganesh and Myna with whom i had the pleasure of working with. I enjoyed
the technical discussions with them. I would like to thank all my course
instructors (in alphabetical order): Prof. Chiranjib Bhattacharyya,
Prof. Navin Kashyap, Prof. Pooja Singla, Prof. Pranesachar, Prof.
Shivani Agarwal, Prof. Sundar Rajan, Prof. Sunil Chandran, Prof.
Thanngavelu, Prof. Venkatachala. I would like to thank all the
professors and students and all people who helped me directly or
indirectly. The thought process i am going through now while doing
research is a combination of my efforts and the thought process of my
advisor. I picked up some of the thought process from my advisor P.
Vijay Kumar like trying to break down any idea into its simplest form
possible for which i am grateful. I also would like to thank my advisor
in heping me writing this thesis partly. My writing skills and
presentation skills greatly improved (although still not great) because
of the teachings of my advisor. I have no friends in IISc. So my tenure
as a PhD student was an extremely tough one. I would like to thank my
family for taking the trouble to shift to Bangalore and stay by my side.
Whatever little intelligence i have is attributed to all the above. I
also would like to thank my labmates Bhagyashree and Vinayak. I shared
many heated conversations with Vinayak which in hindsight was an
enjoyable one. I also would like to thank Shashank, Manuj, Mahesh, Anoop
Thomas, Nikhil, Gautham Shenoy, Birenjith, Myna for sharing a trip to
ISIT with me. I would like to thank Anoop Thomas for always talking in
an encouraging tone. I would like to thank Shashank, Vinayak and Avinash
for sharing little tehnical conversations with me after attending talks
at IISc. I would like to thank Lakshmi Narasimhan with whom i shared
many conversations during my PhD. I also would like to thank Samrat for
sharing some tehnical conversations with me after the math classes. I
also would like to thank students who attended graph theory course with
me as it was a sparse class and i shared many technical conversations
with them. I wish to thank all the people at IISc who treated me kindly.
Finally i would like to thank Anantha, Mahesh, Aswin who are going to
share a trip to ISIT with me this month. If someone’s name is left out
in the above, it is not intentional. \publications Conference

1.  S. B. Balaji and P. V. Kumar, ”A tight lower bound on the
    sub-packetization level of optimal-access MSR and MDS codes,” CoRR,
    (Accepted at ISIT 2018), vol. abs/1710.05876 , 2017.

2.  M. Vajha, S. B. Balaji, and P. V. Kumar, ”Explicit MSR Codes with
    Optimal Access, Optimal Sub-Packetization and Small Field Size for
    @xmath ,” CoRR (Accepted at ISIT 2018), vol. abs/1804.00598 , 2018.

3.  S. B. Balaji, G. R. Kini, and P. V. Kumar, ”A Rate-Optimal
    Construction of Codes with Sequential Recovery with Low Block
    Length,” CoRR, (Accepted at NCC 2018), vol. abs/1801.06794 , 2018.

4.  S. B. Balaji and P. V. Kumar, ”Bounds on the rate and minimum
    distance of codes with availability,” in IEEE International
    Symposium on Information Theory (ISIT), June 2017 , pp. 3155-3159.

5.  S. B. Balaji, G. R. Kini, and P. V. Kumar, ”A tight rate bound and a
    matching construction for locally recoverable codes with sequential
    recovery from any number of multiple erasures,” in IEEE
    International Symposium on Information Theory (ISIT), June 2017 ,
    pp. 1778-1782.

6.  S. B. Balaji, K. P. Prasanth, and P. V. Kumar, ”Binary codes with
    locality for multiple erasures having short block length,” in IEEE
    International Symposium on Information Theory (ISIT), July 2016 ,
    pp. 655-659.

7.  S. B. Balaji and P. V. Kumar, ”On partial maximally-recoverable and
    maximally-recoverable codes,” in IEEE International Symposium on
    Information Theory (ISIT), Hong Kong, 2015 , pp. 1881-1885.

\notations

  -------- ---------------------------------------------------------------------------------------------------------------------------------------
  @xmath   The set @xmath
  @xmath   The cartesian product of @xmath , @xmath times i.e., @xmath
  @xmath   The set of integers
  @xmath   The set of natural numbers @xmath
  @xmath   Complement of the set @xmath
  @xmath   The Finite field with @xmath elements
  @xmath   A linear block code
  @xmath   The dimension of the code @xmath
  @xmath   Block length of a code
  @xmath   Dimension of a code
  @xmath   Minimum distance of a code
  @xmath   Minimum distance of a code or parameter of a regenerating code
  @xmath   Locality parameter of a Locally Recoverable code
  @xmath   Parameters of a linear block code
  @xmath   Parameters of a linear block code
  @xmath   Parameters of a nonlinear code
  @xmath   Parameters of a @xmath Sequential-recovery LR code or an availability code with locality parameter @xmath for @xmath erasure recovery
  @xmath   Parameters of an @xmath LR code with locality parameter @xmath and minimum distance @xmath .
  -------- ---------------------------------------------------------------------------------------------------------------------------------------

  -------- -------------------------------------------------------------------
  @xmath   Support of the vector @xmath
  @xmath   Support of the subcode @xmath i.e., the set @xmath
  @xmath   The dual code of the code @xmath
  @xmath   A punctured code obtained by puncturing the code @xmath on @xmath
  @xmath   A parity check matrix of a code
  @xmath   A code word of a code
  @xmath   @xmath th code symbol of a code word
  @xmath   @xmath th Generalized Hamming Weight of a code
  @xmath   Vertex set of a graph @xmath
  @xmath   Parameters of a regenerating code
  @xmath   
  @xmath   Parameters of a Maximal Recoverable code
  @xmath   Row space of the matrix @xmath
  @xmath   The set @xmath for a vector space @xmath and matrix @xmath
  @xmath   The Transpose of the matrix @xmath
  -------- -------------------------------------------------------------------

\abbreviations

  -------- ----------------------------------------------
  LR       Locally Recoverable
  IS       Information Symbol
  AS       All Symbol
  S-LR     Sequential-recovery LR
  SA       Strict Availability
  MDS      Maximum Distance Separable
  RG       Regenerating
  MSR      Minimum Storage Regenerating
  PMR      Partial Maximal Recoverable
  MR       Maximal Recoverable
  @xmath   Greatest Common Divisor of @xmath and @xmath
  -------- ----------------------------------------------

\pdfbookmark

[section]Contentstoc \makecontents

## Chapter \thechapter Introduction

### 1 The Distributed Storage Setting

In a distributed storage system, data pertaining to a single file is
spatially distributed across nodes or storage units (see Fig. 4 ). Each
node stores a large amounts of data running into the terabytes or more.
A node could be in need of repair for several reasons including (i)
failure of the node, (ii) the node is undergoing maintenance or (iii)
the node is busy serving other demands on its data. For simplicity, we
will refer to any one of these events causing non-availability of a
node, as node failure . It is assumed throughout, that node failures
take place independently.

In [ 2 ] and [ 5 ] , the authors study the Facebook warehouse cluster
and analyze the frequency of node failures as well as the resultant
network traffic relating to node repair. It was observed in [ 2 ] that a
median of @xmath nodes are unavailable per day and that a median of
@xmath TB of cross-rack traffic is generated as a result of node
unavailability (see Fig. 5 ).

Thus there is significant practical interest in the design of
erasure-coding techniques that offer both low overhead and which can
also be repaired efficiently. This is particularly the case, given the
large amounts of data running into the tens or @xmath s of petabytes,
that are stored in modern-day data centers (see Fig. 6 ).

### 2 Different Approaches to Node Repair

The flowchart in Fig. 7 , provides a detailed overview of the different
approaches by coding theorists to efficiently handle the problem of node
repair and the numerous subclasses of codes that they have given rise
to. In the description below, we explain the organization presented in
the flowchart. The boxes outline in red in the flowchart are the topics
to which this thesis has made contributions. These topics are revisited
in detail in subsequent section of the chapter.

###### Drawbacks of Conventional Repair

The conventional repair of an @xmath Reed-Solomon (RS) code where @xmath
denotes the block length of the code and @xmath the dimension is
inefficient in that the repair of a single node, calls for contacting
@xmath other (helper) nodes and downloading @xmath times the amount of
data stored in the failed node. This is inefficient in @xmath respects.
Firstly the amount of data download needed to repair a failed node,
termed the repair bandwidth , is @xmath times the amount stored in the
replacement node. Secondly, to repair a failed node, one needs to
contact @xmath helper nodes. The number of helper nodes contacted is
termed the repair degree . Thus in the case of the @xmath RS code
employed in Facebook, the repair degree is @xmath and the repair
bandwidth is @xmath times the amount of data that is stored in the
replacement node which is clearly inefficient.

###### The Different Approaches to Efficient Node Repair

Coding theorists have responded to this need by coming up with two new
classes of codes, namely ReGenerating (RG) [ 6 , 7 ] .and Locally
Recoverable (LR) codes [ 8 ] . The focus in an RG code is on minimizing
the repair bandwidth while LR codes seek to minimize the repair degree.
In a different direction, coding theorists have also re-examined the
problem of node repair in RS codes and have come up [ 9 ] with new and
more efficient repair techniques. An alternative information-theoretic
approach which permits lazy repair , i.e., which does not require a
failed node to be immediately restored, can be found on [ 10 ] .

###### Different Classes of RG Codes

Regenerating codes are subject to a tradeoff termed as the
storage-repair bandwidth (S-RB) tradeoff , between the storage overhead
@xmath of the code and the normalized repair bandwidth (repair bandwidth
normalized by the file size). This tradeoff is derived by using
principles of network coding. Any code operating on the tradeoff is
optimal with respect to file size. At the two extreme ends of the
tradeoff are codes termed as minimum storage regenerating codes (MSR)
and minimum bandwidth regenerating (MBR) codes. MSR codes are of
particular interest as these codes are Maximum Distance Separable (MDS),
meaning that they offer the least amount of storage overhead for a given
level of reliability and also offer the potential of low storage
overhead. We will refer to codes corresponding to interior points of the
S-RB tradeoff as interior-point RG codes. It turns out the precise
tradeoff in the interior is unknown, thus it is an open problem to
determine the true tradeoff as well as provide constructions that are
optimal with respect to this tradeoff. Details pertaining to the S-RB
tradeoff can be found in [ 11 , 12 , 13 , 14 ] .

###### Variations on the Theme of RG Codes

The theory of regenerating codes has been extended in several other
directions. Secure RG codes (see [ 15 ] ) are RG codes which offer some
degree of protection against a passive or active eavesdropper.
Fractional Repair (FR) codes (see [ 16 ] ) are codes which give up on
some requirements of an RG code and in exchange provide the convenience
of being able to repair a failed node simply by transferring data
(without need for computation at either end) between helper and
replacement node. Cooperative RG codes (see [ 17 , 18 , 19 ] ) are RG
codes which consider the simultaneous repair of several failed nodes and
show that there is an advantage to be gained by repairing the failed
nodes collectively as opposed to in a one-by-one fashion.

###### MDS codes with Efficient Repair

There has also been interest in designing other classes of Maximum
Distance Separable (MDS) codes that can be repaired efficiently. Under
the Piggyback Framework (see [ 20 ] ), it is shown how one can take a
collection of MDS codewords and couple the contents of the different
layers so as to reduce the repair bandwidth per codeword. RG codes are
codes over a vector alphabet @xmath and the parameter @xmath is referred
to as the sub-packetization level of the code. It turns out that in an
RG code, as the storage overhead gets closer to @xmath , the
sub-packetization level @xmath , rises very quickly. @xmath -MSR codes
(see [ 21 ] ) are codes which for a multiplicative factor ( @xmath )
increase in repair bandwidth over that required by an MSR code, are able
to keep the sub-packetizatin to a very small level.

###### Locally Recoverable Codes

Locally recoverable codes (see [ 22 , 23 , 24 , 8 , 25 ] ) are codes
that seek to lower the repair degree. This is accomplished by
constructing the erasure codes in such a manner that each code symbol is
protected by a single-parity-check (spc) code of smaller blocklength,
embedded within the code. Each such spc code is termed as a local code .
Node repair is accomplished by calling upon the short blocklength code,
thereby reducing the repair degree. The coding scheme used in the
Windows Azure is an example of an LR code. The early focus on the topic
of LR codes was on the single-erasure case. Within the class of
single-erasure LR codes, is the subclass of Maximum Recoverable (MR)
codes . An MR code is capable of recovering from any erasure pattern
that is not precluded by the locality constraints imposed on the code.

###### LR Codes for Multiple-Erasures

More recent work in the literature has been directed towards the repair
of multiple erasures. Several approaches have been put forward for
multiple-erasure recovery. The approach via @xmath codes (see [ 26 , 27
] ), is simply to replace the spc local codes with codes that have
larger minimum distance. Hierarchical codes are codes which offer
different tiers of locality. The local codes of smallest block length
offer protection against single erasures. Those with the next higher
level of blocklength, offer protection against a larger number of
erasures and so on.

###### Codes with Sequential and Parallel Recovery

The class of codes for handling multiple erasures using local codes,
that are most efficient in terms of storage overhead, are the class of
codes with sequential recovery (for details on sequential recovery,
please see [ 28 , 29 , 30 , 31 , 32 ] ). As the name suggests, in this
class of codes, for any given pattern of @xmath erasures, there is an
order under which recovery from these @xmath erasures is possible by
contacting atmost @xmath code symbols for the recovery of each erasure.
Parallel Recovery places a more stringent constraint, namely that one
should be able to recover from any pattern of @xmath erasures in
parallel.

###### Availability Codes

Availability codes (see [ 33 , 34 , 4 , 35 ] ) require the presence of
@xmath disjoint repair groups with each repair group contains atmost
@xmath code symbols that are capable of repairing a single erased
symbol. The name availability stems from the fact that this property
allows the recreation of a single erased symbol in @xmath different
ways, each calling upon a disjoint set of helper nodes. This allows the
@xmath simultaneous demands for the content of a single node to be met,
hence the name availability code. In the class of codes with cooperative
recovery (see [ 36 ] ), the focus is on the recovery of multiple
erasures at the same time, while keeping the average number of helper
nodes contacted per erased symbol, to a small value.

###### Locally Regenerating (LRG) Codes

Locally regenerating codes (see [ 37 ] ) are codes in which each local
code is itself an RG code. Thus this class of codes incorporates into a
single code, the desirable features of both RG and LR codes, namely both
low repair bandwidth and low repair degree.

###### Efficient Repair of RS Codes

In a different direction, researchers have come up with alternative
means of repairing RS codes ( [ 38 , 9 ] ). These approaches view an RS
code over an alphabet @xmath , @xmath as a vector code over the subfield
@xmath having sub-packetization level @xmath and use this perspective,
to provide alternative, improved approaches to the repair of an RS code.

###### Liquid Storage Codes

These codes are constructed in line with an information-theoretic
approach which permits lazy repair , i.e., which does not require a
failed node to be immediately restored can be found on [ 10 ] .

### 3 Literature Survey

#### 3.1 Locally Recoverable (LR) codes for Single Erasure

In [ 22 ] , the authors consider designing codes such that the code
designed and codes of short block length derived from the code designed
through puncturing operations all have good minimum distance. The
requirement of such codes comes from the problem of coding for memory
where sometimes you want to read or write only parts of memory. These
punctured codes are what would today be regarded as local codes. The
authors derive an upper bound on minimum distance of such codes under
the constraint that the code symbols in a local code and code symbols in
another local code form disjoint sets and provide a simple
parity-splitting construction that achieves the upper bound. Note that
this upper bound on minimum distance is without any constraint on field
size and achieved for some restricted set of parameters by parity
splitting construction which has field size of @xmath . In [ 23 ] , the
authors note that when a single code symbol is erased in an MDS code,
@xmath code symbols need to be contacted to recover the erased code
symbol where @xmath is the dimension of the MDS code. This led them to
design codes called Pyramid Codes which are very simply derived from the
systematic generator matrix of an MDS code and which reduce the number
of code symbols that is needed to be contacted to recover an erased code
symbol. In [ 24 ] , the authors recognize the requirement of recovering
a set of erased code symbols by contacting a small set of remaining code
symbols and provide a code construction for the requirement based on the
use of linearized polynomials. In [ 8 ] , the authors introduce the
class of LR codes in full generality, and present an upper bound on
minimum distance @xmath without any constraint on field size. This paper
along with the paper [ 39 ] (sharing a common subset of authors) which
presented the practical application of LR codes in Windows Azure
storage, are to a large extent, responsible for drawing the attention of
coding theorists to this class of codes. The extension to the non-linear
case appears in [ 25 ] , [ 40 ] respectively. All of these papers were
primarily concerned with local recoverability in the case of a single
erasure i.e., recovering an erased code symbol by contacting a small set
of code symbols. More recent research has focused on the
multiple-erasure case and multiple erasures are treated in subsequent
chapters of this thesis. For a detailed survery on alphabet size
dependent bounds for LR codes and constructions of LR codes with small
alphabet size, please refer to Chapter Erasure Codes for Distributed
Storage: Tight Bounds and Matching Constructions . A tabular listing of
some constructions of Maximal Recoverbale or partial-MDS codes appears
in Table 5 in Chapter Erasure Codes for Distributed Storage: Tight
Bounds and Matching Constructions .

#### 3.2 Codes with Sequential Recovery

The sequential approach to recovery from erasures, introduced by Prakash
et al. [ 28 ] is one of several approaches to local recovery from
multiple erasures as discussed in Chapter Erasure Codes for Distributed
Storage: Tight Bounds and Matching Constructions , Section 10 . As
indicated in Fig. 15 , Codes with Parallel Recovery and Availability
Codes can be regarded as sub-classes of Codes with Sequential Recovery
(S-LR codes). Among the class of codes which contact at most @xmath
other code symbols for recovery from each of the @xmath erasures, codes
employing this approach (see [ 28 , 36 , 3 , 41 , 29 , 42 , 30 , 31 , 32
] ) have improved rate simply because sequential recovery imposes the
least stringent constraint on the LR code.

###### Two Erasures

Codes with sequential recovery (S-LR code) from two erasures ( @xmath )
are considered in [ 28 ] (see also [ 3 ] ) where a tight upper bound on
the rate and a matching construction achieving the upper bound on rate
is provided. A lower bound on block length and a construction achieving
the lower bound on block length is provided in [ 3 ] .

###### Three Erasures

Codes with sequential recovery from three erasures ( @xmath ) can be
found discussed in [ 3 , 30 ] . A lower bound on block length as well as
a construction achieving the lower bound on block length appears in [ 3
] .

###### More Than @xmath Erasures

A general construction of S-LR codes for any @xmath appears in [ 41 , 30
] . Based on the tight upper bound on code rate presented in Chapter
Erasure Codes for Distributed Storage: Tight Bounds and Matching
Constructions , it can be seen that the constructions provided in [ 41 ,
30 ] do not achieve the maximum possible rate of an S-LR code. In [ 36 ]
, the authors provide a construction of S-LR codes for any @xmath with
rate @xmath . Again, the upper bound on rate presented in Chapter
Erasure Codes for Distributed Storage: Tight Bounds and Matching
Constructions shows that @xmath is not the maximum possible rate of an
S-LR code. In Chapter Erasure Codes for Distributed Storage: Tight
Bounds and Matching Constructions , we observe that the rate of the
construction given in [ 36 ] is actually @xmath which equals the upper
bound on rate derived here only for two cases: case (i) for @xmath and
case (ii) for @xmath and @xmath exactly corresponding to those cases
where a Moore graph of degree @xmath and girth @xmath exist. In all
other cases, the construction given in [ 36 ] does not achieve the
maximum possible rate of an S-LR code.

#### 3.3 Codes with Availability

The problem of designing codes with availability in the context of LR
codes was introduced in [ 33 ] . High rate constructions for
availability codes appeared in [ 34 ] , [ 43 ] , [ 44 ] , [ 45 ] .
Constructions of availability codes with large minimum distance appeared
in [ 46 ] , [ 4 , 47 , 48 ] , [ 49 ] . For more details on constructions
of availability codes please see Chapter Erasure Codes for Distributed
Storage: Tight Bounds and Matching Constructions . Upper bounds on
minimum distance and rate of an availabiltiy code appeared in [ 33 ] , [
4 ] , [ 43 ] , [ 48 ] , [ 50 ] . For exact expressions for upper bounds
on minimum distance and rate which appeared in literature please refer
to Chapter Erasure Codes for Distributed Storage: Tight Bounds and
Matching Constructions .

#### 3.4 Regenerating codes

In the following, we focus only on sub-packetization level @xmath of
regenerating codes as this thesis is focussed only on this aspect. An
open problem in the literature on regenerating codes is that of
determining the smallest value of sub-packetization level @xmath of an
optimal-access (equivalently, help-by-transfer) MSR code, given the
parameters @xmath . This question is addressed in [ 51 ] , where a lower
bound on @xmath is given for the case of a regenerating code that is MDS
and where only the systematic nodes are repaired in a help-by-transfer
fashion with minimum repair bandwidth. In the literature these codes are
often referred to as optimal access MSR codes with systematic node
repair. The authors of [ 51 ] establish that:

  -- -------- --
     @xmath   
  -- -------- --

in the case of an optimal access MSR code with systematic node repair.
In a slightly different direction, lower bounds are established in [ 52
] on the value of @xmath in a general MSR code that does not necessarily
possess the help-by-transfer repair property. In [ 52 ] it is
established that:

  -- -------- --
     @xmath   
  -- -------- --

while more recently, in [ 53 ] the authors prove that:

  -- -------- --
     @xmath   
  -- -------- --

A brief survey of regenerating codes and in particular MSR codes appear
in Chapter Erasure Codes for Distributed Storage: Tight Bounds and
Matching Constructions .

### 4 Codes in Practice

The explosion in amount of storage required and the high cost of
building and maintaining a data center, has led the storage industry to
replace the widely-prevalent replication of data with erasure codes,
primarily the RS code (see Fig. 8 ). For example, the new release Hadoop
3.0 of the Hadoop Distributed File System (HDFS), incorporates HDFS-EC
(for HDFS- Erasure Coding) makes provision for employing RS codes in an
HDFS system.

However, the use of traditional erasure codes results in a repair
overhead, measured in terms of additional repair traffic resulting in
larger repair times and the tying up of nodes in non productive,
node-repair-related activities. This motivated the academic and
industrial-research community to explore approaches to erasure code
construction which were more efficient in terms of node repair and many
of these approaches were discussed in the preceding section. An
excellent example of research in this direction is the development of
the theory of LR codes and their immediate deployment in data storage in
the form of the Windows Azure system.

###### LR Codes in Windows Azure:

In [ 39 ] , the authors compare performance-evaluation results of an
@xmath LR code with that of @xmath RS code in Azure production cluster
and demonstrates the repair savings of LR code. Subsequently the authors
implemented an @xmath LR code in Windows Azure Storage and showed that
this code has repair degree comparable to that of an @xmath RS code, but
has storage overhead @xmath versus @xmath in the case of the RS code
(see Fig. 9 , and Fig. 10 ). This @xmath LR code is currently is use now
and has reportedly resulted in the savings of millions of dollars for
Microsoft [ 54 ] .

A second poular distributed storage system is Ceph and Ceph currently
has an LR code plug-in [ 55 ] . Some other examples of work directed
towards practical applications are described below. Most of this work is
work carried out by an academic group and presented at a major storage
industry conference and involves performance evaluation through
emulation of the codes in a real-world setting.

1.  In [ 5 ] , the authors implement HDFS-Xorbas. This system employs LR
    codes in place of RS codes in HDFS-RAID. The experimental evaluation
    of Xorbas was carried out in Amazon EC2 and a cluster in Facebook
    and the repair performance of @xmath LR code was compared against a
    @xmath RS code.

2.  A method, termed as piggybacking, of layering several RS codewords
    and then coupling code symbols across layers is shown in [ 20 ] , to
    yield a code over a vector alphabet, that has reduced repair
    bandwidth, without giving up on the MDS property of an RS code. A
    practical implementation of this is implemented in the Hitchhiker
    erasure-coded system [ 56 ] . Hitchhiker was implemented in HDFS and
    its performance was evaluated on a data-warehouse cluster at
    Facebook.

3.  The HDFS implementation of a class of codes known as HashTag codes
    is discussed in [ 57 ] (see also [ 58 ] ). These are codes designed
    to efficiently repair systematic nodes and have a lower
    sub-packetization level in comparison to an RG code at the expense
    of a larger repair bandwidth.

4.  The NCCloud [ 59 ] is an early work that dealt with the practical
    performance evaluation of regenerating codes and employs a class of
    MSR code known as functional-MSR code having @xmath parities.

5.  In [ 60 ] , the performance of an MBR code known as the pentagon
    code as well as an LRG code known as the heptagon local code are
    studied and their performance compared against double and triple
    replication. These code possess inherent double replication of
    symbols as part of the construction.

6.  The product matrix (PM) code construction technique yields a general
    construction of MSR and MBR codes. The PM MSR codes have storage
    overhead that is approximately lower bounded by a factor of @xmath .
    The performance evaluation of an optimal-access version of a rate
    @xmath PM code, built on top of Amazon EC2 instances, is presented
    in [ 61 ] .

7.  A high-rate MSR code known as the Butterfly code is implemented and
    evaluated in both Ceph and HDFS in [ 62 ] . This code is a
    simplified version of the MSR codes with two parities introduced in
    [ 63 ] .

8.  In [ 64 ] , the authors evaluate the performance in a Ceph
    environment, of an MSR code known as the Clay code, and which
    corresponds to the Ye-Barg code in [ 65 ] , (and independently
    rediscovered after in [ 66 ] ). The code is implemented in [ 64 ] ,
    from the coupled-layer perspective present in [ 66 ] . This code is
    simultaneously optimal in terms of storage overhead and repair
    bandwidth (as it is an MSR code), and also has the optimal-access
    (OA) property and the smallest possible sub-packetization level of
    an OA MSR code. The experimental performance of the Clay code is
    shown to be match its theoretical performance.

### 5 Contributions and Organization of the Thesis

The highlighted boxes appearing in the flow chart in Fig. 11 represent
topics with respect to which this thesis has made a contribution.

We now proceed to describe chapter wise, our contributions corresponding
to topics in the highlighted boxes. An overview of the contributions
appears in Fig. 12 .

###### Chapter 2: Locally Recoverable Codes: Alphabet-Size Dependent
Bounds for Single Erasures

This chapter begins with an overview of LR codes. Following this, new
alphabet-size dependent bounds on both minimum distance and dimension of
an LR code that are tighter than existing bounds in the literature, are
presented.

###### Chapter 3: Tight Bounds on the Rate of LR Codes with Sequential
Recovery

This chapter deals with codes for sequential recovery and contains the
principal result of the thesis, namely, a tight upper bound on the rate
of a code with sequential recovery for all possible values of the number
@xmath of erasures guaranteed to be recovered with locality parameter
@xmath . Matching constructions are provided in the chapter following,
Chapter Erasure Codes for Distributed Storage: Tight Bounds and Matching
Constructions . A characterization of codes achieving the upper bound on
code rate for the case of @xmath erasures is also provided here. The
bound on maximum possible code rate assumes that there is no constraint
(i.e., upper bound) on the block length of the code or equivalently, on
the code dimension. A lower bound on the block length of codes with
sequential recovery from three erasures is also given here. Also given
are constructions of codes with sequential recovery for @xmath having
least possible block length for a given dimension @xmath and locality
parameter @xmath . An upper bound on dimension for the case of @xmath
for a given dual dimension and locality parameter @xmath and
constructions achieving it are also provided.

###### Chapter 4: Matching (Optimal) Constructions of Sequential LR
Codes

In this chapter, we construct codes which achieve the upper bound on
rate of codes with sequential recovery derived in Chapter Erasure Codes
for Distributed Storage: Tight Bounds and Matching Constructions for all
possible values of the number @xmath of erasures guaranteed to be
recovered with locality parameter @xmath . We deduce the general
structure of parity check matrix of a code achieving our upper bound on
rate. Based on this, we show achievability of the upper bound on code
rate via an explicit construction. We then present codes which achieve
the upper bound on rate having least possible block length for some
specific set of parameters.

###### Chapter 5: Bounds on the Parameters of Codes with Availability

This chapter deals with codes with availability. Upper bounds are
presented on the minimum distance of a code with availability, both for
the case when the alphabet size is constrained and when there is no
constraint. These bounds are tighter than the existing bounds in
literature. We next introduce a class of codes, termed codes with strict
availability which are subclass of the codes with availability. The
best-known availability codes in terms of rate belong to this category.
We present upper bounds on the rate of codes with strict availability
that are tighter than existing upper bounds on the rate of codes with
availability. We present exact expression for maximum possible
fractional minimum distance for a given rate for a special class of
availability codes as @xmath where each code in this special class is a
subcode or subspace of direct product of @xmath copies of an
availability code with parameters @xmath for some @xmath . We also
present a lower bound on block length codes with strict avalability and
characterize the codes with strict availability achieving the lower
bound on block length.

###### Chapter 6: Tight Bounds on the Sub-Packetization Level of MSR and
Vector-MDS Codes

This chapter contains our results on the topic of RG codes. Here, we
derive lower bounds on the sub-packetization level an of a subclass of
MSR codes known as optimal-access MSR codes. We also bound the
sub-packetization level of optimal-access MDS codes with optimal repair
for (say) a fixed number @xmath of nodes. The bounds derived here are
tight as there are constructions in the literature that achieve the
bounds derived here. The bounds derived here conversely show that the
constructions that have previously appeared in the literature are
optimal with respect to sub-packetization level. We also show that the
bound derived here sheds light on the structure of an optimal-access MSR
or MDS code.

###### Chapter 7: Partial Maximal and Maximal Recoverable Codes

The final chapter deals with the subclass of LR codes known as Maximal
Recoverable (MR) codes. In this chapter we provide constructions of MR
codes having smaller field size than the constructions existing in the
literature. In particular we modify an existing construction which will
result in an MR code with field size of @xmath for some specific set of
parameters. We also modify (puncture) an existing construction for
@xmath to form an MR code which results in reduced field size in
comparison with the field size of constructions appearing in the
literature. We also introduce in the chapter, a class of codes termed as
Partial Maximal Recoverable (PMR) codes. We provide constructions of PMR
codes having small field size. Since a PMR code is in particular an LR
code, this also yields a low-field-size construction of LR codes.

## Chapter \thechapter Locally Recoverable Codes: Alphabet-Size
Dependent Bounds for Single Erasures

This chapter deals with locally recoverable (LR) codes, also known in
the literature as codes with locality. Contributions of the thesis in
this area include new best-known alphabet-size-dependent bounds on both
minimum distance @xmath and dimension @xmath for LR codes for @xmath .
For @xmath , our bound on dimension is the tightest known bound for
@xmath . We begin with some background including a fundamental bound on
@xmath (Section 6.1 ) and a description of two of the better-known and
general constructions for this class of codes (Section 6.2 ). More
recent research has focused on deriving bounds on code dimension and
minimum distance, that take into account the size @xmath of the
underlying finite field @xmath over which the codes are constructed. We
next provide a summary of existing field-size-dependent bounds on
dimension and minimum distance (Section 7 ). Our alphabet-size-dependent
bound (Section 7.1 ) on minimum distance and dimension makes use of an
upper bound on the Generalized Hamming Weights (GHW) (equivalently,
Minimum Support Weights (MSW)) derived in [ 28 ] . This bound is in
terms of a recursively-defined sequence of integers which we refer to
here as the Minimum Support Weight Sequence (MSWS). Our bound also makes
use of the notion of shortening of a code . Following a presentation of
our results, we then summarize existing alphabet-size-dependent
constructions (Section 7.2 ). The chapter ends with a summary of the
contributions of the thesis on the topic of LR codes for single
erasures. Contributions to the case of LR codes for multiple erasures
are contained in subsequent chapters. In this chapter, we will restrict
ourselves to only linear codes for most of the discussion.

### 6 Locally Recoverable Codes for Single Erasures

In [ 22 ] , the authors consider designing codes such that the code
designed and codes of short block length derived from the code designed
through puncturing operations all have good minimum distance. The
requirement of such codes comes from the problem of coding for memory
where sometimes you want to read or write only parts of memory. These
punctured codes are what would today be regarded as local codes. The
authors derive an upper bound on minimum distance of such codes under
the constraint that the code symbols in a local code and code symbols in
another local code form disjoint sets and provide a simple
parity-splitting construction that achieves the upper bound. Note that
this upper bound on minimum distance is without any constraint on field
size and achieved for some restricted set of parameters by parity
splitting construction which has field size of @xmath . In [ 23 ] , the
authors note that when a single code symbol is erased in an MDS code,
@xmath code symbols need to be contacted to recover the erased code
symbol where @xmath is the dimension of the MDS code. This led them to
design codes called Pyramid Codes which are very simply derived from the
systematic generator matrix of an MDS code and which reduce the number
of code symbols that is needed to be contacted to recover an erased code
symbol. In [ 24 ] , the authors recognize the requirement of recovering
a set of erased code symbols by contacting a small set of remaining code
symbols and provide a code construction for the requirement based on the
use of linearized polynomials. In [ 8 ] , the authors introduce the
class of LR codes in full generality, and present an upper bound on
minimum distance @xmath without any constraint on field size. This paper
along with the paper [ 39 ] (sharing a common subset of authors) which
presented the practical application of LR codes in Windows Azure
storage, are to a large extent, responsible for drawing the attention of
coding theorists to this class of codes. The extension to the non-linear
case appears in [ 25 ] , [ 40 ] respectively. All of these papers were
primarily concerned with local recoverability in the case of a single
erasure i.e., recovering an erased code symbol by contacting a small set
of code symbols. More recent research has focused on the
multiple-erasure case and multiple erasures are treated in subsequent
chapters of this thesis. Throughout this chapter:

1.  a codeword in an @xmath linear code will be represented by @xmath
    where @xmath denotes the @xmath th code symbol.

2.  all codes discussed are linear codes and we will use the term
    nonlinear explicitly when referring to a nonlinear code.

3.  we say a code achieves a bound (an inequality), iff it has
    parameters such that the bound is satisfied with equality.

4.  The notation @xmath or @xmath , refers to the minimum distance of a
    code under discussion.

Let @xmath be an @xmath code over a finite field @xmath . Let @xmath be
a @xmath generator matrix for @xmath having columns @xmath , i.e.,
@xmath . An information set @xmath is any subset of @xmath of size
@xmath satisfying: @xmath .

###### Definition 1.

An @xmath code @xmath over a finite field @xmath is said to be an LR
code with information-symbol (IS) locality over @xmath if there is an
information set @xmath such that for every @xmath , there exists a
subset @xmath , with @xmath , such that @xmath and there is a codeword
in the dual code @xmath with support exactly equal to @xmath . @xmath is
said to be an LR code with all-symbol (AS) locality over @xmath if for
every @xmath , there exists a subset @xmath with @xmath , such that
@xmath and there is a codeword in the dual code @xmath with support
exactly equal to @xmath . Clearly, an LR code with AS locality is also
an LR code with IS locality. The parameter @xmath appearing above is
termed the locality parameter.

Throughout this thesis, when we say LR code, it refers to an LR code
with all-symbol (AS) locality. When we discuss LR code with
information-symbol (IS) locality, we will state it explicitly. Note that
the presence of a codeword in the dual code with support set @xmath
implies that if the code symbol @xmath is erased then it can be
recovered from the code symbols in the set @xmath . Recovery from
erasures is termed as repair . The repair is local , since @xmath and
typically, @xmath is significantly smaller than the block length @xmath
of the code. It is easy to see that every linear code can trivially be
regarded as an LR code with locality parameter @xmath . The term a local
code of an LR code @xmath refers to the code @xmath for some @xmath
where @xmath for a set @xmath with @xmath , @xmath ..

#### 6.1 The @xmath Bound

A major result in the theory of LR codes is the minimum distance bound
given in ( 1 ) which was derived for linear codes in [ 8 ] . An
analogous bound for nonlinear codes can be found in [ 25 ] , [ 40 ] .

###### Theorem 6.1.

[ 8 ] Let @xmath be an @xmath LR code with IS locality over @xmath with
locality parameter @xmath and minimum distance @xmath . Then

  -- -------- -------- -------- -- -----
     @xmath   @xmath   @xmath      (1)
  -- -------- -------- -------- -- -----

On specializing to the case when @xmath , ( 1 ) yields the Singleton
bound and for this reason, the bound in ( 1 ) is referred to as the
Singleton bound for an LR code. Note that since ( 1 ) is derived for an
LR code with IS locality, it also applicable to an LR code with AS
locality.

#### 6.2 Constructions of LR Codes

In the following, we will describe two constructions of LR codes having
field size of @xmath and achieving the bound ( 1 ). The first
construction called the Pyramid Code construction [ 23 ] , allows us to
construct for any given parameter set @xmath , an LR code with IS
locality achieving the bound in ( 1 ). The second construction which
appeared in [ 46 ] , gives constructions for LR codes with AS locality,
achieving the bound ( 1 ) for any @xmath under the constraint that
@xmath .

##### 6.2.1 Pyramid Code Construction

The pyramid code construction technique [ 23 ] , allows us to construct
for any given parameter set @xmath an LR code with IS locality achieving
the bound in ( 1 ). We sketch the construction for the case @xmath . The
general case @xmath , @xmath or when @xmath , follows along similar
lines. The construction begins with the systematic generator matrix
@xmath of an @xmath scalar MDS code @xmath having block length @xmath .
It then reorganizes the sub-matrices of @xmath to create the generator
matrix @xmath of the pyramid code as shown in the following:

  -- -- -------- -------- --
        @xmath   @xmath   
  -- -- -------- -------- --

where @xmath . It is not hard to show that the @xmath code @xmath
generated by @xmath is an LR code with IS locality and that @xmath . It
follows that

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

and the code @xmath thus achieves the Singleton bound in ( 1 ).

##### 6.2.2 The Tamo-Barg Construction

The construction below by Tamo and Barg [ 46 ] , provides a construction
for LR codes with AS locality achieving the bound ( 1 ) for any @xmath
with @xmath . We will refer to this construction as the Tamo-Barg (T-B)
construction. Let @xmath be a finite field of size @xmath , let @xmath ,
@xmath , with @xmath and @xmath . Set @xmath . Let @xmath and let @xmath
, @xmath , such that @xmath represent a partitioning of @xmath . Let
@xmath be a ‘good’ polynomial, by which is meant, a polynomial over
@xmath that is constant on each @xmath i.e., @xmath for some @xmath and
degree of @xmath is @xmath . Let

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

where the @xmath are the message symbols and where the second term is
vacuous for @xmath , i.e., when @xmath . Consider the code @xmath of
block length @xmath and dimension @xmath where the codeword c of length
@xmath corresponding to a given @xmath message symbols @xmath is
obtained by evaluating @xmath at each of the @xmath elements in @xmath
after substituting the given values of @xmath message symbols in the
expression for @xmath . It can be shown that @xmath is an LR code with
AS locality with locality parameter @xmath and achieves the @xmath bound
in ( 1 ). The @xmath -th local code corresponds to evaluations of @xmath
at elements of @xmath (also see Fig 13 ). An example of how good
polynomials may be constructed is given below, corresponding to the
annihilator polynomial of a multiplicative subgroup @xmath of @xmath .
{example} Let @xmath be a chain of cyclic subgroups, where @xmath so
that @xmath @xmath @xmath @xmath @xmath . Let @xmath . Let @xmath be the
@xmath multiplicative cosets of @xmath in @xmath , with @xmath being the
multiplicative identity so that @xmath . It follows that

  -- -------- --
     @xmath   
  -- -------- --

so that @xmath is constant on all the cosets of @xmath in @xmath and may
be selected as the good polynomial @xmath i.e., @xmath is one possible
choice of good polynomial based on multiplicative group @xmath . Further
examples may be found in [ 46 , 67 , 68 ] . For constructions meeting
the Singleton bound with field size of @xmath and greater flexibility in
selecting the value of @xmath , please see [ 69 ] .

### 7 Alphabet-Size Dependent Bounds

This section contains the contributions of the thesis on the topic of LR
codes for the case of single erasures. These include the best-known
alphabet-size-dependent bounds on both minimum distance @xmath and
dimension @xmath for LR codes for @xmath . For @xmath , our bound on
dimension is the tightest known bound for @xmath . The bound in equation
( 1 ) as well as the bounds for non-linear and vector codes derived in [
25 , 70 ] hold regardless of the size @xmath of the underlying finite
field. The theorem below which appeared in [ 71 ] takes the size @xmath
of the code symbol alphabet into account and provides a tighter upper
bound on the dimension of an LR code for a given @xmath that is valid
even for nonlinear codes where @xmath is the minimum distance of the
code. The ‘dimension’ of a nonlinear code @xmath over an alphabet @xmath
of size @xmath is defined to be the quantity @xmath .

###### Theorem 7.1.

[ 71 ] Let @xmath be an @xmath LR code with AS locality and locality
parameter @xmath over an alphabet @xmath of size @xmath . Then the
dimension @xmath of the code must satisfy:

  -- -------- -- -----
     @xmath      (2)
  -- -------- -- -----

where @xmath denotes the largest possible dimension of a code (no
locality necessary) over @xmath having block length @xmath and minimum
distance @xmath .

###### Proof.

(Sketch of proof) The bound holds for linear as well as nonlinear codes.
In the linear case, with @xmath , the derivation proceeds as follows.
Let @xmath be a @xmath generator matrix of the locally recoverable code
@xmath . Then it can be shown that for any integer @xmath , there exists
an index set @xmath such that @xmath and @xmath where @xmath refers to
the set of columns of @xmath indexed by @xmath . This implies that
@xmath has a generator matrix of the form (after permutation of
columns):

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

In turn, this implies that the rowspace of @xmath defines an @xmath code
over @xmath , if @xmath . It follows that @xmath and the result follows.
Note that the row space of @xmath corresponds to a shortening @xmath of
@xmath with respect to the coordinates @xmath . The proof in the general
case is a (nontrivial) extension to the nonlinear setting. ∎

We next look at a bound on dimension of binary LR codes for a given
@xmath that appeared in [ 72 ] . We remark that there is an additional
bound on dimension given in [ 72 ] for the case when the local codes are
disjoint i.e., the case when the support sets @xmath , are pairwise
disjoint. However, here we only provide the bound on dimension given in
[ 72 ] , which applies in full generality, and without the assumption of
disjoint local codes.

###### Theorem 7.2.

[ 72 ] For any @xmath linear code @xmath that is an LR code with AS
locality with locality parameter @xmath over @xmath with @xmath and
@xmath , we must have:

  -- -------- -- -----
     @xmath      (3)
  -- -------- -- -----

The above bound is obtained by applying a Hamming-bound-type argument to
an LR code with AS locality. In [ 43 ] , the authors provide a bound on
the minimum distance of @xmath LR codes with IS locality ¹ ¹ 1 The bound
has an extension to codes with availability as well, see Chapter Erasure
Codes for Distributed Storage: Tight Bounds and Matching Constructions
for the definition of availability. , (the bound thus applies to LR
codes with AS locality as well) that depends on the size @xmath of the
underlying finite field @xmath :

###### Theorem 7.3.

For any @xmath linear code @xmath that is an LR code with IS locality
with locality parameter @xmath over @xmath :

  -- -------- -- -----
     @xmath      (4)
  -- -------- -- -----

where @xmath is the maximum possible minimum distance of a classical
(i.e., no locality necessary) @xmath block code over @xmath .

We next introduce the notion of Generalized Hamming Weights (also known
as Minimum Support Weights) which will be used to derive a new bound on
the minimum distance and dimension of an LR code with AS locality, that
takes into account the size @xmath of the underlying finite field @xmath
. The bound makes use of the technique of code shortening and the GHWs
of a code provide valuable information about shortened codes.

##### 7.0.1 GHW and the Minimum Support Weight Sequence

We will first define the Generalized Hamming Weights of a code,
introduced in [ 73 ] , and also known as Minimum Support Weights (MSW)
(see [ 74 ] ) of a code. In this thesis we will use the term Minimum
Support Weight (MSW).

###### Definition 2.

The @xmath th Minimum Support Weight (MSW) @xmath (equivalently, the
@xmath th Generalized Hamming Weight) of an @xmath code @xmath is the
cardinality of the minimum support of an @xmath -dimensional subcode of
C, i.e.,

  -- -------- -- -----
     @xmath      (5)
  -- -------- -- -----

where the notation @xmath denotes a subcode @xmath of @xmath and where
@xmath (called the support of the code @xmath ).

Although the MSW definition applies to any code, the interest in this
thesis, is on its application to a restricted class of codes that we
introduce here.

###### Definition 3 (Canonical Dual Code).

By a canonical dual code, we will mean an @xmath linear code @xmath
satisfying the following: @xmath contains a set @xmath of @xmath
linearly independent codewords of Hamming weight @xmath , such that the
sets @xmath , @xmath cover @xmath , i.e.,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

As it turns out, the dual code of an LR code with AS locality (and as we
shall see in subsequent chapters, dual of codes with sequential recovery
and dual of codes with availability) is an example of a canonical dual
code and this is the reason for our interest in the MSWs of this class
of codes.

###### Theorem 7.4.

[ 28 ] Let @xmath be a canonical dual code with parameters @xmath and
support sets @xmath as defined in Definition 3 . Let @xmath denote the
@xmath th @xmath MSW of @xmath . Let @xmath be the minimum possible
value of cardinality of the union of any @xmath distinct support sets
@xmath , @xmath , @xmath i.e.,

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath . Let the integers @xmath be recursively defined as follows:

  -- -------- -------- -------- -- -----
     @xmath   @xmath   @xmath      (6)
     @xmath   @xmath   @xmath      (7)
  -- -------- -------- -------- -- -----

Then

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

Note that in [ 28 ] , the Theorem 7.4 is proved for the case @xmath ,
@xmath , @xmath but we observe from the proof of Theorem 7.4 (proof for
@xmath ) given in [ 28 ] that the Theorem 7.4 is also true for any
@xmath , @xmath , @xmath . We will refer to the sequence @xmath
appearing in the theorem above as the Minimum Support Weight (MSW)
Sequence associated to parameter set @xmath . In the subsection below,
we derive new alphabet-size dependent bounds on minimum distance and
dimension, that are expressed in terms of the MSW sequence.

#### 7.1 New Alphabet-Size Dependent Bound Based on MSW

In this subsection, we present field-size dependent bounds on the
minimum distance and dimension of an LR code @xmath with AS locality
with @xmath as locality parameter. The bounds are derived in terms of
the MSW sequence associated with the dual @xmath of @xmath . The basic
idea is to shorten the LR code to a code with @xmath ( @xmath th term of
MSW sequence) code symbols set to zero for some @xmath . Theorem 7.4
provides a lower bound on the dimension of this shortened code.
Classical bounds on the parameters of this shortened code are shown to
yield bounds on the parameters of the parent LR code.

###### Theorem 7.5.

Let @xmath be an @xmath LR code with AS locality with locality parameter
@xmath over a field @xmath with minimum distance @xmath . Let @xmath be
the maximum possible minimum distance of an @xmath LR code with AS
locality with locality parameter @xmath over a field @xmath . Then:

  -- -------- -- -----
     @xmath      (8)
     @xmath      (9)
  -- -------- -- -----

where

1.  @xmath ,

2.  @xmath , @xmath

3.  @xmath ,

4.  @xmath is the maximum possible minimum distance of a classical
    (i.e., no locality necessary) @xmath block code over @xmath and

5.  @xmath is the largest possible dimension of a code (i.e., no
    locality necessary) over @xmath having block length @xmath and
    minimum distance @xmath .

###### Proof.

Since @xmath is an LR code with AS locality with locality parameter
@xmath , we have that @xmath is an @xmath canonical dual code with
locality parameter @xmath and @xmath . We explain the reason for the
inequality in the symbol @xmath . Since @xmath is an LR code AS
locality, for every code symbol @xmath , there is a codeword in @xmath
of weight @xmath whose support contains @xmath . So take a codeword in
@xmath of weight @xmath whose support set @xmath contains @xmath . Next,
choose @xmath and take a codeword in @xmath of weight @xmath whose
support set @xmath contains @xmath . Repeat this process. At the @xmath
step, choose @xmath and take a codeword in @xmath of weight @xmath whose
support set @xmath contains @xmath . Note that the set of @xmath
codewords corresponding to support sets @xmath form a set of @xmath
linearly independent codewords in @xmath and this process process can be
repeated until @xmath for some @xmath . Since @xmath , we have that
@xmath which implies @xmath as @xmath . We now set @xmath . Hence from
Theorem 7.4 , @xmath , @xmath . For simplicity, let us write @xmath ,
@xmath . Next, fix @xmath with @xmath . Let @xmath be the support of an
@xmath dimensional subspace or subcode of @xmath with the support having
cardinality exactly @xmath in @xmath . Add @xmath arbitrary extra
indices to @xmath and let the resulting set be @xmath . Hence @xmath and
@xmath . Now shorten the code @xmath in the co-ordinates indexed by
@xmath i.e., take @xmath where @xmath is the compliment of @xmath and
@xmath for a set @xmath with @xmath , @xmath . The resulting code @xmath
has block length @xmath , dimension @xmath and minimum distance @xmath
(if @xmath ) and the resulting code @xmath is also an LR code with AS
locality with locality parameter @xmath . Hence:

  -- -------- --
     @xmath   
  -- -------- --

The proof of ( 9 ) follows from the fact that @xmath and Hence :

  -- -------- -- ------
     @xmath      (10)
  -- -------- -- ------

∎

An example comparison of the upper bounds on dimension of linear LR
codes given in ( 2 ), ( 3 ) and ( 9 ) (our bound) is presented in Table
1 . The word bound in the following refers to an upper bound.

Since @xmath , it can be seen that the bound ( 8 ) is tighter than the
bound ( 4 ) when applied to an LR code with AS locality. For the same
reason, the bound ( 9 ) is tighter than bound ( 2 ). For @xmath , it is
mentioned in [ 72 ] , that the bound ( 3 ) is looser than the bound in (
2 ) for @xmath . Since our bound ( 9 ) presented here is tighter than
the bound appearing in ( 2 ), we conclude that for @xmath , @xmath , our
bound ( 9 ) is tighter than the bound in ( 3 ). Hence our bounds ( 8 ),(
9 ) are the tightest known bounds on minimum distance and dimension for
@xmath . For @xmath , our bound ( 9 ) is the tightest known bound on
dimension for @xmath . We here note that the bounds ( 8 ), ( 9 ) apply
even if we replace @xmath with any other upper bound on @xmath MSW. The
bounds derived here are general in this sense. For the sake of
completeness, in the following we give a survey of existing small
alphabet size constructions which are optimal w.r.t bounds appearing in
the literature.

###### Remark 1.

Let @xmath and let @xmath be a positive integer. Then if @xmath and
@xmath , it is trivial to observe that:

  -- -------- --
     @xmath   
  -- -------- --

Trivially, if @xmath is a monotonic function of @xmath , then assuming
continuity of @xmath , we get @xmath . This is because if @xmath is a
monotonic function of @xmath then the number of linearly independent
codewords of weight @xmath in the dual code needed to satisfy the
conditions necessary for an LR code is a negligible fraction of @xmath
as @xmath increases. Hence the region of interest in locality is when
@xmath is a constant or when @xmath is a small number.

#### 7.2 Small-Alphabet Constructions

##### 7.2.1 Construction of Binary Codes

Constructions for binary codes that achieve the bound on dimension given
in ( 2 ) for binary codes, appear in [ 75 , 45 , 76 ] . While [ 76 ] and
[ 75 ] provide constructions for @xmath and @xmath respectively, the
constructions in [ 45 ] handle the case of larger minimum distance but
have locality parameter restricted to @xmath . In [ 43 ] , the authors
give optimal binary constructions with information and all symbol
locality with @xmath . The construction is optimal w.r.t the bound ( 4
). Constructions achieving the bound on dimension appearing in [ 72 ]
and the further tightened bound for disjoint repair groups given in [ 77
] for binary codes, appear respectively, in [ 72 , 77 ] . These
constructions are for the case @xmath . In [ 76 ] , the authors present
a characterization of binary LR codes that achieve the Singleton bound (
1 ). In [ 78 ] , the authors present constructions of binary codes
meeting the Singleton bound. These codes are a subclass of the codes
characterized in [ 76 ] for the case @xmath .

##### 7.2.2 Constructions with Small, Non-Binary Alphabet

In [ 79 ] , the authors characterize ternary LR codes achieving the
Singleton bound ( 1 ). In [ 76 , 78 , 80 ] , the authors provide
constructions for codes over a field of size @xmath that achieve the
Singleton bound in ( 1 ) for @xmath . Some codes from algebraic geometry
achieving the Singleton bound ( 1 ) for restricted parameter sets are
presented in [ 81 ] .

##### 7.2.3 Construction of Cyclic LR Codes

Cyclic LR codes can be constructed by carefully selecting the generator
polynomial @xmath of the cyclic code. We illustrate a key idea behind
the construction of a cyclic LR code by means of an example.

###### Example 1.

Let @xmath be a primitive element of @xmath satisfying @xmath . Let
@xmath be a cyclic @xmath code having generator polynomial @xmath .
Since the consecutive powers @xmath of @xmath are zeros of @xmath , it
follows that @xmath by the BCH bound. Suppose we desire to ensure that a
code @xmath having generator polynomial @xmath has @xmath and in
addition, is locally recoverable with parameter @xmath , then we do the
following. Set @xmath . Let @xmath and @xmath . It follows that @xmath .
Summing over @xmath we obtain:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

It follows that the symbols @xmath of @xmath form a local code as they
satisfy the constraint of an overall parity-check. Since the code @xmath
is cyclic the same holds for the code symbols @xmath , for @xmath . Thus
through this selection of generator polynomial @xmath , we have obtained
a code that has both locality and @xmath . The zeros of @xmath are
illustrated in Fig. 14 . The code @xmath has parameters @xmath and
@xmath . Note that the price we pay for introduction of locality is a
loss in code dimension, equal to the degree of the polynomial @xmath .
Thus an efficient code will choose the zeros of @xmath for maximum
overlap.

The above idea of constructing cyclic LR code was introduced in [ 82 ]
and extended in [ 83 , 84 , 85 , 86 ] . In [ 87 ] , the use of locality
for reducing the complexity of decoding a cyclic code is explored. The
same paper also makes a connection with earlier work [ 88 ] that can be
interpreted in terms of locality of a cyclic code. In [ 82 ] a
construction of binary cyclic LR codes for @xmath an @xmath achieving a
bound derived within the same paper for binary codes is provided. In [
85 ] , the authors give constructions of optimal binary, ternary codes
meeting the Singleton bound ( 1 ) for @xmath and @xmath as well as a
construction of a binary code meeting the bound given in [ 72 ] for
@xmath based on concatenating cyclic codes. A discussion on the locality
of classical binary cyclic codes as well as of codes derived from them
through simple operations such as shortening, can be found in [ 89 , 43
] . The principal idea here is that any cyclic code has locality @xmath
where @xmath is the minimum distance of the dual code @xmath . In [ 84 ]
, the authors construct optimal cyclic codes under the constraint that
the local code is either a Simplex code or else, a Reed-Muller code. In
[ 83 ] , the authors provide a construction of cyclic codes with field
size @xmath achieving the Singleton bound ( 1 ) and also study the
locality of subfield subcodes as well as their duals, the trace codes.
In [ 86 ] , constructions of cyclic LR codes with @xmath for any @xmath
and flexible @xmath are provided.

### 8 Summary

This chapter dealt with LR codes for the single-erasure case and
presented the requisite background as well as the contributions of the
thesis in this direction. The thesis contributions on LR codes for
single erasure case correspond to new alphabet-size dependent upper
bounds on the minimum distance @xmath and dimension @xmath of a linear
LR code. Thus the upper bounds apply to the case of LR codes over a
finite field @xmath of fixed size @xmath . A key ingredient in the upper
bounds derived here are the bounds on the Generalized Hamming Weights
(GHW) derived in [ 28 ] . Evidence was presented showing our upper bound
on dimension to be tighter in comparison with existing upper bounds in
the literature.

## Chapter \thechapter LR Codes with Sequential Recovery

### 9 Introduction

The focus of the present chapter is on LR codes for multiple erasures.
We begin by providing motivation for studying the multiple-erasure case
(Section 9.1 ). As there are several approaches towards handling
multiple erasures in the literature, we next provide a broad
classification of LR codes for multiple erasures (Section 10 ). The
principal contributions of this thesis relate to a particular approach
towards the recovery from multiple erasures, termed as sequential
recovery. Section 11 introduces LR codes with sequential recovery and
surveys the known literature on the topic. This is followed by an
overview of the contributions of this thesis on the topic of LR codes
with sequential recovery in Section 12 . Sections 13 and 14 respectively
present in detail, the results obtained in this thesis, relating to the
case of @xmath and @xmath erasures respectively. Section 15 presents a
principal result of this thesis which involves establishing a tight
upper bound on the rate of an LR code with sequential recovery for
@xmath erasures along with a matching construction. The upper bound
derived also proves a conjecture that had previously appeared in the
literature. The final section, Section 16 , summarizes the contents of
the chapter. Throughout this chapter, we use the term weight to denote
the Hamming weight.

#### 9.1 Motivation for Studying Multiple-Erasure LR Codes

Given that the key problems on the topic of LR codes for the single
erasure case have been settled, the academic community has turned its
attention towards LR codes for multiple erasures.

###### Availability

A strong motivation for studying the multiple-erasure case, comes from
the notion of availability . A storage unit could end up storing data
that is in extremely high demand at a certain time instant. In such
situations, regarded by the storage industry as degraded reads , the
storage industry will look to create, on-the-fly replicas of the storage
unit’s data. If a code symbol can be recreated in @xmath different ways
by calling upon @xmath pairwise disjoint sets @xmath of helper nodes,
then one could recreate @xmath copies of the data-in-demand in parallel.
But a code which can, for any code symbol recreate in this fashion
@xmath simultaneous copies of a code symbol, also has the ability to
correct @xmath erasures simultaneously. This follows because any pattern
of @xmath erasures can affect at most @xmath of the helper node sets
@xmath and thus there is still a helper node set remaining that can
repair the erased symbol. The problem of designing codes with
availability in the context of locality was introduced in [ 33 ] and a
high rate construction for availability codes appeared in [ 34 ] . For a
survey on constructions of availability codes please see Chapter Erasure
Codes for Distributed Storage: Tight Bounds and Matching Constructions .
Upper bounds on minimum distance and rate of an availabiltiy codes
appeared in [ 4 ] , [ 43 ] , [ 35 ] , [ 48 ] .

###### Other reasons

Other reasons for being interested in the multiple erasure setting
include (a) the increasing trend towards replacing expensive servers
with low-cost commodity servers that can result in simultaneous node
failures and (b) the temporary unavailability of a helper node to assist
in the repair of a failed node.

### 10 Classification of LR Codes for Multiple Erasures

An overview of the different classes of LR codes that are capable of
recovering from multiple erasures proposed in the literature is
presented here. All approaches to recovery from multiple erasures place
a constraint @xmath on the number of unerased symbols that are used to
recover from a particular erased symbol. The value of @xmath is usually
small in comparison with the block length @xmath of the code and for
this reason, one speaks of the recovery as being local. All the codes
defined in this section are over a finite field @xmath . A codeword in
an @xmath code will be represented by @xmath . In this chapter, we will
restrict ourselves to only linear codes.

##### 10.0.1 Sequential-Recovery LR Codes

An @xmath sequential-recovery LR code (abbreviated as S-LR code) is an
@xmath linear code @xmath having the following property: Given a
collection of @xmath erased code symbols, there is an ordering @xmath of
these @xmath erased symbols such that for each index @xmath , there
exists a subset @xmath satisfying (i) @xmath , (ii) @xmath , and (iii)

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (11)
  -- -------- -------- -------- -- ------

It follows from the definition that an @xmath S-LR code can recover from
the erasure of @xmath code symbols @xmath , for @xmath by using ( 11 )
to recover the symbols @xmath , in succession.

##### 10.0.2 Parallel-Recovery LR Codes

If in the definition of the S-LR code, we replace the condition (ii) in
( 11 ) by the more stringent requirement:

  -- -------- -- ------
     @xmath      (12)
  -- -------- -- ------

then the LR code will be referred to as a parallel recovery LR code,
abbreviated as P-LR code. Clearly the class of P-LR codes is a subclass
of S-LR codes. From a practical point of view, P-LR codes are preferred
since as the name suggests, the erased symbols can be recovered in
parallel. However, this will in general, come at the expense of storage
overhead. We note that under parallel recovery, depending upon the
specific code, this may require the same helper (i.e., non-erased) code
symbol to participate in the recovery of more than one erased symbol
@xmath .

##### 10.0.3 Availability Codes

An @xmath availability LR code (see [ 33 , 34 , 4 , 35 ] ), is an @xmath
linear code having the property that in the event of a single but
arbitrary erased code symbol @xmath , there exist @xmath recovery sets
@xmath which are pairwise disjoint and of size @xmath with @xmath , such
that for each @xmath , @xmath can be expressed in the form:

  -- -------- --
     @xmath   
  -- -------- --

An @xmath availability code is also an @xmath P-LR code. This follows
because the presence of at most @xmath erasures implies, that there will
be at least one recovery set for each erased code symbol all of whose
symbols remain unerased.

##### 10.0.4 @xmath Codes

An @xmath linear code @xmath is said to have AS @xmath locality (see [
26 , 27 ] ), if for each co-ordinate @xmath , there exists a subset
@xmath , with @xmath , with

  -- -------- -- ------
     @xmath      (13)
  -- -------- -- ------

Recovery from @xmath erasures can also be accomplished by using the
codes with @xmath locality, if one ensures that the code has @xmath .
However in this case, repair is local only in those cases where the
erasure pattern is such that the number of erasures @xmath within the
local code @xmath satisfies @xmath . Thus one may regard @xmath codes as
offering probabilistic guarantees of local recovery in the presence of
@xmath erasures in exchange for a potential increase in code rate. Of
course, one could always employ an @xmath with local MDS codes (i.e.,
the code @xmath is MDS) and @xmath , but this would result in a
significant rate penalty.

##### 10.0.5 Cooperative Recovery

A cooperative recovery @xmath LR (C-LR) code (see [ 36 ] ) is an @xmath
linear code such that if a subset @xmath , @xmath of code symbols are
erased then there exists a subset @xmath of @xmath other code symbols
(i.e., @xmath for any @xmath ) such that for all @xmath :

  -- -------- --
     @xmath   
  -- -------- --

Clearly an @xmath C-LR code is also an @xmath P-LR code, but the @xmath
in the case of a cooperative LR code will tend to be significantly
larger. One may regard cooperative LR codes as codes that seek to
minimize the number of unerased symbols contacted per erased symbol on
average, rather than insist that each code symbol be repaired by
contacting @xmath other code symbols.

### 11 Codes with Sequential Recovery

###### Definition 4.

An @xmath sequential-recovery LR code (abbreviated as S-LR code) over a
finite field @xmath is an @xmath linear code @xmath over the finite
field @xmath having the following property: Given a collection of @xmath
erased code symbols, there is an ordering @xmath of these @xmath erased
symbols such that for each index @xmath , there exists a subset @xmath
satisfying (i) @xmath , (ii) @xmath , and (iii)

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (14)
  -- -------- -------- -------- -- ------

It follows from the definition that an @xmath S-LR code can recover from
the erasure of @xmath code symbols @xmath , for @xmath by using ( 14 )
to recover the symbols @xmath , in succession.

#### 11.1 An Overview of the Literature

The sequential approach to recovery from erasures, introduced by Prakash
et al. [ 28 ] is one of several approaches to local recovery from
multiple erasures as discussed in Section 10 . As indicated in Fig. 15 ,
Codes with Parallel Recovery and Availability Codes can be regarded as
sub-classes of codes with Sequential Recovery. Among the class of codes
which contact at most @xmath other code symbols for recovery from each
of the @xmath erasures, codes employing this approach (see [ 28 , 36 , 3
, 41 , 29 , 42 , 30 , 31 , 32 ] ) have improved rate simply because
sequential recovery imposes the least stringent constraint on the LR
code.

###### Two Erasures

Codes with sequential recovery (S-LR code) from two erasures ( @xmath )
are considered in [ 28 ] (see also [ 3 ] ) where a tight upper bound on
the rate and a matching construction achieving the upper bound on rate
is provided. A lower bound on block length and a construction achieving
the lower bound on block length is provided in [ 3 ] .

###### Three Erasures

Codes with sequential recovery from three erasures ( @xmath ) can be
found discussed in [ 3 , 30 ] . A lower bound on block length as well as
a construction achieving the lower bound on block length appears in [ 3
] .

###### More Than @xmath Erasures

A general construction of S-LR codes for any @xmath appears in [ 41 , 30
] . Based on the tight upper bound on code rate presented in this
chapter, it can be seen that the constructions provided in [ 41 , 30 ]
do not achieve the maximum possible rate of an S-LR code. In [ 36 ] ,
the authors provide a construction of S-LR codes for any @xmath with
rate @xmath . Again, the upper bound on rate presented here shows that
@xmath is not the maximum possible rate of an S-LR code. In the next
chapter, we observe that the rate of the construction given in [ 36 ] is
actually @xmath which equals the upper bound on rate derived here only
for two cases: case (i) for @xmath and case (ii) for @xmath and @xmath
exactly corresponding to those cases where a Moore graph of degree
@xmath and girth @xmath exist. In all other cases, the construction
given in [ 36 ] does not achieve the maximum possible rate of an S-LR
code. In the subsections below, we examine in greater detail, the
results in the literature pertaining to the cases @xmath and general
@xmath respectively.

#### 11.2 Prior Work: @xmath Erasures

All the results presented below are for the case of @xmath erasures. The
upper bound on code rate appearing below, can be found in [ 28 ] .

###### Theorem 11.1.

[ 28 ] [Bound on Code Rate] Let @xmath be an @xmath S-LR code over
@xmath . Then

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (15)
  -- -------- -------- -------- -- ------

S-LR codes with @xmath which achieve the bound ( 15 ) are said to be
rate-optimal . This upper bound on code rate can be rephrased as a lower
bound on the block length of the code.

###### Theorem 11.2.

[ 28 ] The block length @xmath of an @xmath S-LR code @xmath over @xmath
must satisfy:

  -- -------- -- ------
     @xmath      (16)
  -- -------- -- ------

An S-LR code with @xmath achieving the lower bound ( 16 ) on block
length is said to be block-length optimal . In [ 3 ] , the authors
provide a construction of binary S-LR codes with @xmath achieving the
lower bound on block length ( 16 ) for any @xmath such that @xmath . A
rate-optimal construction based on Turan graphs of S-LR codes and
appearing in [ 28 ] is presented below.

###### Construction 11.3.

[ 28 ] Let @xmath be a positive integer. Let @xmath , with @xmath and
@xmath . Consider a graph @xmath with @xmath vertices. We partition the
vertices into @xmath partitions, each partition containing @xmath
vertices. Next, we place precisely one edge between any two vertices
belonging to two distinct partitions. The resulting graph is known as a
Turán graph on @xmath vertices with @xmath vertex partitions. The number
of edges in this graph is @xmath and each vertex is connected to exactly
@xmath other vertices. Let the vertices be labelled from @xmath to
@xmath and the edges be labelled from @xmath to @xmath in some random
order. Now let each edge represent a distinct information bit and each
node represent a parity bit storing the parity or binary sum of the
information bits represented by edges incident on it. The code thus
described corresponds to an @xmath S-LR code over @xmath and hence a
rate-optimal code for @xmath . We will refer to these codes as
Turán-graph codes or as codes based on Turán graphs.

{example}

[Turán Graph] An example Turán graph with @xmath , @xmath , @xmath is
shown in Figure 16 .

{example}

By definition of a Turán Graph, it can be seen that any complete graph
is also a Turán Graph.

###### Remark 2.

With respect to the example above, with a complete graph as the example
Turán graph, it can be verified that the code obtained by applying
Construction 11.3 to complete graph is also an instance of a code with
availability for @xmath . This is because the sum of all the parity bits
represented by nodes of the complete graph is 0. This parity check gives
the second recovery set for all the parity bits represented by nodes of
the complete graph making the code an availability code for @xmath .
Since the rate of an availability code cannot exceed that of an S-LR
code, it follows that the resultant code is rate optimal as an
availability code for @xmath .

###### Remark 3 (MSW Optimality of Turán-graph codes).

It can be shown (see [ 28 ] ) that the MSW sequence @xmath given in
Theorem 7.4 of Chapter Erasure Codes for Distributed Storage: Tight
Bounds and Matching Constructions with @xmath provides an upper bound on
the MSWs (or equivalently the GHWs) of the dual of an @xmath S-LR code.
It was shown in [ 28 ] that Turán-graph codes achieve this upper bound
on MSW. Thus the dual code of an @xmath Turán-graph code has the largest
possible MSWs compared to the dual of any @xmath S-LR code.

#### 11.3 Prior Work: @xmath Erasures

###### Results for @xmath

The lower bound on block length for S-LR codes for the case of @xmath
given in [ 3 ] is presented below.

###### Theorem 11.4.

[ 3 ] Let @xmath denote an @xmath S-LR code over a finite field @xmath .
Then the block length @xmath must satisfy:

  -- -------- -- ------
     @xmath      (17)
  -- -------- -- ------

The constructions of S-LR codes for @xmath given in [ 3 ] include the
product code (product of two @xmath single parity check codes) which
achieves the maximum possible rate based on the bound ( 17 ). However,
the product code cannot provide codes for all values of @xmath . To
address this, additional constructions were provided in [ 3 ] for S-LR
codes with @xmath achieving the lower bound on block length ( 17 ) for
almost all @xmath such that @xmath (for the precise conditions on @xmath
please see [ 3 ] ).

###### Results for General @xmath

The following conjecture on the maximum achievable rate of an @xmath
S-LR code appeared in [ 30 ] .

###### Conjecture 11.5.

[ 30 ] [Conjecture] Let @xmath denote an @xmath S-LR code over a finite
field @xmath . Let @xmath . Then:

  -- -------- --
     @xmath   
  -- -------- --

As will be seen, the tight upper bound on rate of an @xmath S-LR code
derived in Section 15 of the present chapter proves the above conjecture
by identifying the precise value of the coefficients @xmath appearing in
the conjecture. Chapter Erasure Codes for Distributed Storage: Tight
Bounds and Matching Constructions provides constructions of S-LR codes
achieving the upper bound on rate derived in Section 15 for any @xmath
with @xmath .

### 12 Contributions of the Thesis to S-LR codes

This chapter contains one of the principal contributions of this thesis,
namely the derivation of a tight upper bound (Theorem 15.1 ) on the rate
of an @xmath S-LR code. Proof that this bound is tight (achievable)
follows from the matching constructions of S-LR codes provided in the
following chapter, Chapter Erasure Codes for Distributed Storage: Tight
Bounds and Matching Constructions . There are several other results as
well and these are summarized in Fig. 17 . These include:

1.  Case of S-LR codes with @xmath :

    1.  A rate-optimal construction of S-LR codes achieving the upper
        bound on rate given in Theorem 11.1 .

    2.  A block-length-optimal construction of S-LR codes which achieves
        the lower bound on block length given in Theorem 11.2 . This
        construction extends the range of parameters ( @xmath ) of block
        length optimal constructions compared to block length optimal
        constructions given in [ 28 , 3 ] .

    3.  A characterization of rate-optimal S-LR codes for @xmath .

    4.   An upper bound on the dimension of an S-LR code over @xmath for
        the case when one is given the dimension @xmath of the subspace
        of the dual code spanned by codewords of weight @xmath . It may
        not always be possible to achieve the lower bound on block
        length given in Theorem 11.2 . In such situations, this upper
        bound on code dimension can prove useful. We also provide here a
        construction achieving this upper bound on code dimension for a
        family of values of @xmath and locality parameter @xmath .

2.  Case of S-LR codes with @xmath :

    1.  A lower bound on block length of an S-LR code over @xmath for a
        given @xmath for @xmath .This lower bound is shown to be tighter
        than the previously known lower bound on block length given in
        Theorem 11.4 . This is followed by pointing out that the
        construction of a short block length S-LR code given in [ 29 ]
        that generalizes an instance of the Turán graph based
        construction 11.3 has block length very close to our lower bound
        . We give two specific examples of S-LR codes which achieve our
        lower bound on block length but does not achieve the lower bound
        on block length given in Theorem 11.4 .

3.  Case of S-LR codes for general @xmath :

    1.  An upper bound on rate of an S-LR code for any @xmath such that
        @xmath . This upper bound on rate is achievable and
        constructions achieving it appear in Chapter Erasure Codes for
        Distributed Storage: Tight Bounds and Matching Constructions .
        This upper bound on rate also proves the conjecture 11.5 given
        in [ 30 ] .

### 13 Contributions to S-LR codes with @xmath

We have already described the contributions of this thesis to S-LR codes
for @xmath . We give the detailed results in the following.

#### 13.1 Rate and Block-Length-Optimal Constructions

We begin by describing a generic, graph-based construction of an S-LR
code for @xmath . Special cases of this construction will yield rate and
block-length-optimal constructions.

###### Construction 13.1 (Generic Graph-based Construction).

Let @xmath be a graph having vertex set @xmath and edge set @xmath . Let
each edge in the graph represent an information symbol in the code and
each node or vertex, represent a parity symbol which is equal to some
linear combination of the information symbols represented by the edges
incident on that node, where the linear combination is taken over a
finite field @xmath with each coefficient in the linear combination
being non-zero. Each codeword in the resultant systematic code @xmath is
comprised of the information symbols represented by the edges in @xmath
and the parity symbols represented by the nodes in @xmath . The
dimension of this code is clearly equal to @xmath and the block length
@xmath . Since the parity symbol represented by a node @xmath is a
non-zero linear combination of precisely @xmath information symbols, the
corresponding parity check involves exactly @xmath code symbols. Thus
the code has locality @xmath . It is straightforward to see that the
code can sequentially recover from @xmath erasures (S-LR code with
@xmath ). Thus the code given by this construction is an @xmath S-LR
code over @xmath .

We next describe a construction that is a specialization of Construction
13.1 , in which all nodes with possibly a single exception, have the
same degree. This construction yields block-length-optimal codes.

###### Construction 13.2 (Near-Regular Graph Construction).

Let @xmath be positive integers. Let @xmath . Let @xmath be a graph on a
set of @xmath nodes with ‘ @xmath ’ nodes having degree @xmath and for
the case when @xmath , the remaining node having degree @xmath . The
condition for existence of such a near-regular graph @xmath is given in
Remark 4 below. Apply the generic graph-based-construction described in
Construction 13.1 to the graph @xmath by replacing @xmath in
Construction 13.1 with @xmath and setting @xmath . It follows that the
code thus obtained by applying the Construction 13.1 to the graph @xmath
with @xmath is an @xmath S-LR code over @xmath .

Block length optimality: Construction 13.2 yields S-LR codes with @xmath
which achieve the lower bound on block length given in ( 16 ). Hence
Construction 13.2 yields block-length-optimal codes. Construction 13.2
yields rate-optimal codes for @xmath when @xmath . This is described
below.

###### Construction 13.3 (Regular-Graph Construction).

In Construction 13.2 , when @xmath , i.e., when the graph @xmath is a
regular graph, the resultant code is a binary S-LR code with @xmath that
is rate-optimal. we will refer these codes as Regular-Graph Codes or as
the Codes based on Regular-Graphs .

Rate optimality: Construction 13.3 yields S-LR codes with @xmath which
achieve the upper bound on rate given in ( 15 ). Hence Construction 13.3
provides rate-optimal codes.

###### Proof.

Follows since the code has rate @xmath . ∎

###### Remark 4 (Existence Conditions for Near-Regular Graphs).

The parameter sets @xmath for which near-regular graphs @xmath of the
form described in Constructions 13.2 and 13.3 exist can be determined
from the Erdös-Gallai theorem [ 90 ] . The conditions are

  -- -------- -- ------
     @xmath      (18)
  -- -------- -- ------

As noted earlier, in [ 3 ] , the authors provide constructions of binary
S-LR codes for @xmath achieving the lower bound on block length ( 16 )
for any @xmath such that @xmath . Thus the Constructions 13.2 extends
the range of @xmath for which the lower bound on block length ( 16 ) is
achieved since Constructions 13.2 gives binary S-LR codes for @xmath
achieving the lower bound on block length for any @xmath such that
@xmath . For the case @xmath , Construction 13.3 extends the range of
constructions of rate-optimal codes for @xmath compared to construction
11.3 as construction 11.3 requires that @xmath with @xmath whereas
construction 13.3 needs only that @xmath .

{example}

An example @xmath S-LR code based on a regular graph and which is hence
rate optimal, is shown in Fig. 18 . In the example, a codeword takes on
the form

  -- -------- --
     @xmath   
  -- -------- --

#### 13.2 Characterizing Rate-Optimal S-LR Codes for @xmath

We characterize below rate-optimal S-LR codes for @xmath . We begin with
Theorem 13.4 below, which describes a general form of the parity check
matrix of a rate-optimal code for @xmath . This theorem will be used in
our characterization.

###### Theorem 13.4.

Let @xmath denote an @xmath S-LR code over a finite field @xmath . Let
@xmath be rate-optimal for @xmath . Let

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (19)
  -- -------- -------- -------- -- ------

Then

1.   It must be that @xmath , @xmath , @xmath @xmath and

2.   Let @xmath be a basis of @xmath such that @xmath , @xmath . The
    parity-check matrix @xmath of @xmath with @xmath as its rows, is
    upto a permutation of columns and scaling of rows, of the form:

      -- -------- -- ------
         @xmath      (20)
      -- -------- -- ------

    where @xmath denotes the @xmath identity matrix and @xmath is an
    @xmath matrix with each column having Hamming weight @xmath and each
    row having Hamming weight @xmath .

3.   There exists a parity-check matrix @xmath (upto a permutation of
    columns) of @xmath of the form given in ( 20 ).

The proof of Theorem 13.4 above follows as a consequence of the proof of
the Theorem 11.1 given in [ 28 ] . This is because, as can be seen from
the proof of the Theorem 11.1 given in [ 28 ] , that for achieving the
upper bound on rate given in Theorem 11.1 , one needs a parity check
matrix where each row has Hamming weight @xmath , where there are @xmath
columns of Hamming weight one and the remaining @xmath columns having
weight two. In the following, we exploit this result to characterize
rate-optimal codes for @xmath .

###### Theorem 13.5.

Let @xmath denote an @xmath S-LR code over a finite field @xmath . Let
@xmath be rate-optimal for @xmath . Then @xmath (after possible
permutation of code symbols or co-ordinates) must have one of the forms
below:

1.  -- -------- --
         @xmath   
      -- -------- --

2.  -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
      -- -------- -------- -------- --

3.  -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
      -- -------- -------- -------- --

where the phrase ‘regular-graph code over @xmath ’ refers to the code
obtained by applying the generic graph based construction over @xmath
described in Construction 13.1 to a regular graph @xmath of degree
@xmath by replacing @xmath in Construction 13.1 with @xmath .

###### Proof.

For proof please refer to the Appendix Erasure Codes for Distributed
Storage: Tight Bounds and Matching Constructions . ∎

#### 13.3 Upper Bound on Dimension of an S-LR Code for @xmath

Let @xmath be an @xmath S-LR code. Let @xmath . It can be seen that the
lower bound on block length ( 16 ) is achieved iff @xmath . This implies
@xmath . Hence if @xmath the lower bound ( 16 ) is not achievable. We
will now see that for some values of @xmath , @xmath for binary codes.
We derive an upper bound on dimension @xmath of @xmath for a given
@xmath that is tighter than ( 16 ) for small values of @xmath . The idea
is simply to place as many distinct vectors as possible in the columns
of the parity check matrix with @xmath rows under the constraint that
the total number of ones in all the columns put together is @xmath .

###### Theorem 13.6.

Let @xmath be an @xmath S-LR code over @xmath . Let

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

and let @xmath . Then

  -- -------- -- ------
     @xmath      (21)
  -- -------- -- ------

###### Proof.

For proof please refer to the Appendix Erasure Codes for Distributed
Storage: Tight Bounds and Matching Constructions . ∎

Tightness of the above bound : Let @xmath . Let @xmath for @xmath :
substituting the value of @xmath given above in the RHS (Right Hand
Side) of the bound ( 21 ) after removing the minimization in the RHS of
( 21 ) over @xmath and setting @xmath :

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (22)
  -- -------- -------- -------- -- ------

Using similar algebraic manipulations, it can be shown that in bound (
21 ) the minimum of the RHS for @xmath for @xmath is attained when
@xmath . We skip this proof. By simple algebraic mainpulations it can be
shown that ( 22 ), implies @xmath , for @xmath for @xmath , for large
values of @xmath , for a fixed @xmath and hence our bound is tighter
than the bound ( 16 ). We now show that the upper bound ( 22 ) is
actually achievable.

#### 13.4 Dimension-Optimal Constructions for Given @xmath:

We saw from ( 22 ) that when @xmath for @xmath :

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (23)
  -- -------- -------- -------- -- ------

In this section, we give a construction for achieving the bound ( 23 )
on @xmath when gcd( @xmath , @xmath )= @xmath and @xmath . The
construction is carried out in a simple manner as described below:

###### Construction 13.7.

Let @xmath . Let @xmath , for @xmath . Let gcd( @xmath , @xmath )=
@xmath and @xmath . Let @xmath . For @xmath ,let @xmath be an @xmath
matrix such that the set of columns of @xmath is equal to the set of all
possible distinct @xmath binary vectors of Hamming weight @xmath . Let
@xmath denote the set of all possible distinct @xmath binary vectors of
Hamming weight @xmath . Next, define a relation @xmath in set @xmath as
follows. For @xmath , @xmath , if @xmath can be obtained by cyclically
shifting the co-ordinates of @xmath . It can be seen that this relation
is an equivalence relation. Let @xmath be the equivalence classes, each
containing exactly @xmath vectors as gcd( @xmath , @xmath )= @xmath .
Then the desired code is the code having parity-check matrix:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the @xmath matrix with the set of columns of @xmath
equal to the set of @xmath vectors in @xmath and @xmath with @xmath .
Note that the weight of each row of @xmath is exactly @xmath and since
all the columns of @xmath are distinct, the code is an S-LR code with
@xmath . Note that the code with parity-check matrix @xmath defined as
above achieves the bound ( 23 ). Hence the code is a @xmath S-LR code
over @xmath and has maximum possible dimension for the given @xmath .

###### Construction 13.8.

Let @xmath . Let @xmath , for @xmath . Let gcd( @xmath , @xmath )=
@xmath and @xmath . For @xmath , let @xmath be an @xmath matrix such
that the set of columns of @xmath is equal to the set of all possible
distinct @xmath binary vectors of Hamming weight @xmath . Let @xmath be
the node-edge incidence matrix of a @xmath -regular @xmath - uniform
simple hypergraph with exactly @xmath nodes. Now our final desired code
is defined as the code with parity-check matrix:

  -- -------- --
     @xmath   
  -- -------- --

Note that the weight of each row of @xmath is exactly @xmath and since
all the columns of @xmath are distinct, the code is an S-LR code for
@xmath . Note that the code with parity-check matrix @xmath defined as
above achieves the bound ( 23 ). Hence the code is a @xmath S-LR code
over @xmath and has maximum possible dimension for the given @xmath .

### 14 Contributions to S-LR codes with @xmath

In this section, we adopt a slightly different perspective and we
consider lower bounding the block length of S-LR codes with given @xmath
and @xmath for binary codes. We first present a lower bound on the block
length of binary S-LR codes with given @xmath and @xmath for @xmath .
This lower bound is shown to be tighter than the previously known lower
bound on block length given in Theorem 11.4 . This is followed by
pointing out that the construction of a short block length S-LR code
with @xmath given in [ 29 ] that generalizes an instance of the Turán
graph based construction 11.3 has block length very close to our lower
bound.

##### 14.0.1 Lower Bound on Block Length

As noted earlier in Theorem 11.4 (due to [ 3 ] ):

  -- -------- -- ------
     @xmath      (24)
  -- -------- -- ------

Constructions of S-LR codes with @xmath achieving the above bound ( 24 )
were provided for almost all @xmath such that @xmath (almost the regime
( @xmath ). The maximum possible rate for @xmath is achieved by the
product code (product of 2 @xmath single parity check codes). Here
again, we present a new lower bound on block length of binary S-LR codes
with @xmath . Simulation shows that this new lower bound is tighter than
equation ( 24 ) for the regime @xmath . We also provide a few sporadic
examples where our new lower bound turns out to be tight.

###### Lemma 14.1.

Let @xmath denote a binary code whose minimum distance @xmath . Let us
assume in addition, that @xmath possesses an @xmath parity-check matrix
@xmath over @xmath such that each column of @xmath has Hamming weight
equal to @xmath . The matrix @xmath need not be full rank. Then, @xmath
satisfies:

  -- -------- -- ------
     @xmath      (25)
  -- -------- -- ------

###### Proof.

For proof please refer to the Appendix Erasure Codes for Distributed
Storage: Tight Bounds and Matching Constructions ∎

###### Theorem 14.2.

Let @xmath denote an @xmath S-LR code over @xmath . The block length
@xmath of the code @xmath satisfies the lower bound:

  -- -------- -- ------
     @xmath      (26)
  -- -------- -- ------

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

and @xmath denotes the set of natural numbers.

###### Proof.

For proof please refer to the Appendix Erasure Codes for Distributed
Storage: Tight Bounds and Matching Constructions . ∎

Fig. 19 shows a comparison between the two lower bounds ( 24 ) and ( 26
) on the block-length @xmath of a binary @xmath S-LR code for the case
when @xmath . Simulation results show that the new lower bound in ( 26 )
is tighter than the previously known lower bound in ( 24 ) for @xmath ,
@xmath .

Two example S-LR codes with @xmath which achieve the lower bound in ( 26
) are provided below. The construction in Examples 14.0.1 is based on
the proof of Theorem 14.2 whereas the construction in Example 14.0.1 ,
is based on the hyper graph-based construction which appeared in [ 29 ]
. In [ 29 ] , authors showed that the construction of S-LR codes with
@xmath based on hyper graph given in [ 29 ] , has block length differing
from our lower bound on block length ( 26 ) by atmost 2 for an infinite
set of parameters @xmath . This shows that the lower bound presented
here is tight in a significant number of cases. Table 2 compares the
block-lengths of the codes in the two examples with the lower bounds on
block-length appearing in ( 24 ) (due to [ 3 ] Song et al) and ( 26 )
(the new lower bound presented here).

{example}

@xmath : ( 24 ) gives @xmath for @xmath . The lower bound ( 26 ) derived
here gives @xmath for @xmath . It can be seen that the binary code
associated with the parity-check matrix given below in ( 27 ) is a
binary S-LR code with @xmath and based on ( 26 ) has the least possible
block length @xmath for @xmath .

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (27)
  -- -------- -------- -------- -- ------

{example}

@xmath : the lower bound ( 26 ) derived here in gives @xmath whereas (
24 ) gives @xmath . It can be seen that the binary code associated with
the parity-check matrix given below in ( 28 ) is a binary S-LR code with
@xmath and based on ( 26 ) has the least possible block length @xmath
for @xmath .

  -- -- -- ------
           (28)
  -- -- -- ------

### 15 Contributions to S-LR codes with any value of @xmath

Contributions include a tight upper bound on rate of S-LR codes and a
construction achieving the upper bound for any value of @xmath with
@xmath . The construction achieving the upper bound is presented in the
chapter following, Chapter Erasure Codes for Distributed Storage: Tight
Bounds and Matching Constructions . Only the upper bound is presented in
this chapter.

#### 15.1 A Tight Upper Bound on Rate of S-LR codes

In this section we provide an upper bound on rate of an @xmath S-LR code
for any @xmath and any @xmath . The cases of even @xmath and odd @xmath
are considered separately. The proof proceeds by deducing the structure
of parity-check matrix of an S-LR code. Constructions achieving our
upper bound on rate for any @xmath with @xmath are provided in the next
chapter, Chapter Erasure Codes for Distributed Storage: Tight Bounds and
Matching Constructions which establishes that the upper bound on rate
derived here gives the exact expression for maximum possible rate of an
S-LR code with parameters @xmath with @xmath . The matching
constructions also make use of the structure of parity-check matrix
deduced here. The upper bound on rate presented here also proves the
conjecture by Song et al given in 11.5 .

###### Theorem 15.1.

Rate Bound : Let @xmath denote an @xmath S-LR code over a finite field
@xmath . Let @xmath . Then

  -- -------- -------- -- -- ------
     @xmath   @xmath         (29)
     @xmath   @xmath         (30)
  -- -------- -------- -- -- ------

where @xmath .

###### Proof.

We begin by setting @xmath where @xmath denotes the Hamming weight of
the vector @xmath . Let @xmath be the dimension of @xmath . Let @xmath
be a basis of @xmath such that @xmath , @xmath . For all @xmath , We
represent @xmath as a @xmath vector where @xmath th component of the
@xmath vector is the @xmath th codesymbol of the codeword @xmath ,
@xmath . Let @xmath . It follows that @xmath is a parity check matrix of
an @xmath S-LR code as its row space contains every codeword of Hamming
weight at most @xmath which is present in @xmath . Also,

  -- -------- --
     @xmath   
  -- -------- --

The idea behind the next few arguments in the proof is the following.
S-LR codes with high rate will tend to have a larger value of @xmath for
a fixed @xmath . On the other hand, the Hamming weight of the matrix
@xmath (i.e., the number of non-zero entries in the matrix) is bounded
above by @xmath . It follows that to make @xmath large, one would like
the columns of @xmath to have as small a weight as possible. It is
therefore quite natural to start building @xmath by picking many columns
of weight @xmath , then columns of weight @xmath and so on. As one
proceeds by following this approach, it turns out that the matrix @xmath
is forced to have a certain sparse, block-diagonal, staircase form and
an understanding of this structure is used to derive the upper bound on
code rate.
The cases of @xmath being an even integer and an odd integer are
considered separately. We form linear inequalities which arise from the
structure of the matrix @xmath and derive the required upper bounds on
the rate from these linear inequalities. See Appendix Erasure Codes for
Distributed Storage: Tight Bounds and Matching Constructions for
detailed proofs for both the cases. ∎

###### Conditions for equality in (29): @xmath even case:

Note that for achieving the upper bound on rate given in ( 29 ), an S-LR
code must have a parity check matrix @xmath (upto a permutation of
columns) of the form given in ( 52 ) with parameters such that the
inequalities given in ( 54 ),( 55 ),( 56 ),( 58 ) (in Appendix) become
equalities with @xmath and @xmath must be an empty matrix i.e., no
columns of weight @xmath (because once all these inequalities become
equalities, the sub matrix of @xmath obtained by restricting @xmath to
the columns with weights 1,2 will have each row of weight exactly @xmath
and hence no non-zero entry can occur outside the columns having weights
1,2 for achieving the upper bound on rate). Hence it can be seen that an
@xmath S-LR code achieving the upper bound on rate ( 29 ) must have a
parity check matrix (upto a permutation of columns) of the form given in
( 31 ).

  -- -------- -- ------
     @xmath      (31)
  -- -------- -- ------

###### Conditions for equality in (30): @xmath odd case:

Note that for achieving the upper bound on rate given in ( 30 ), an S-LR
code must have a parity check matrix @xmath (upto a permutation of
columns) of the form given in ( 76 ) with parameters such that the
inequalities given in ( 78 ),( 79 ),( 80 ),( 82 ),( 83 ) (in Appendix)
become equalities with @xmath and @xmath must be an empty matrix i.e.,
no columns of weight @xmath (because once all these inequalities become
equalities, the sub matrix of @xmath obtained by restricting @xmath to
the columns with weights 1,2 will have each row of weight exactly @xmath
and hence no non-zero entry can occur outside the columns having weights
1,2 for achieving the upper bound on rate). Note that for achieving the
upper bound on rate, @xmath must also be an empty matrix. This is
because inequality ( 82 ) must become an equality which implies that
@xmath is a matrix with each row of weight @xmath and we also saw that
@xmath . Hence @xmath must be a zero matrix which implies @xmath is an
empty matrix. Hence it can be seen that an @xmath S-LR code achieving
the upper bound on rate ( 30 ) must have a parity check matrix (upto a
permutation of columns) of the form given in ( 32 ).

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (32)
  -- -------- -------- -------- -- ------

It may be noted here that our upper bound on rate given in Theorem 15.1
, for the special cases of @xmath , matches with the upper bound given
in [ 28 ] , [ 30 ] respectively. For @xmath , our upper bound on rate
given in Theorem 15.1 is new and hence a new contribution to S-LR codes.
In the remainder of the thesis, S-LR codes with parameters @xmath
achieving the upper bounds ( 29 ) or ( 30 ) depending on @xmath will be
referred to as “rate-optimal codes”.

###### Remark 5.

We now make a remark on blocklength of the rate-optimal codes. It can be
seen that, for even @xmath , with upper bound on rate given in ( 29 )
being achieved with equality and for @xmath to be an integer with @xmath
, @xmath needs to be an integer multiple of @xmath . Similarly for odd
@xmath , for @xmath , to be an integer and with upper bound on rate
given in ( 30 ) being achieved with equality, @xmath needs to be an
integer multiple of @xmath .

Note that the upper bound on rate of an S-LR code derived here is
applicable in a much larger setting. The upper bound on rate derived in
Theorem 15.1 , also yields an upper bound on the rate of a code with
minimum distance @xmath and has a parity check matrix @xmath with each
row of weight @xmath . This is true as at no point in the derivation of
Theorem 15.1 , did we use the sequential-recovery property apart from
the property that the minimum distance @xmath for a code with parity
check matrix @xmath . The problem of deriving an upper bound on rate in
this general setting without the sequential-recovery property is
addressed in the literature in [ 91 , 92 , 93 , 94 ] . But these papers
are interested in deriving an upper bound on the rate of a code with
parity check matrix with each row of weight @xmath with a relative
minimum distance @xmath . But here in this work, we are not interested
in having a non-zero relative minimum distance. It is enough if the
minimum distance @xmath irrespective of the relative minimum distance.
Hence we obtain a different bound in Theorem 15.1 whereas the
expressions for upper bound on rate in [ 91 , 92 , 93 , 94 ] are in
terms of the expression for binary entropy. Apart from the above, we
prove in next chapter that the upper bound on rate in Theorem 15.1 is
achievable and hence is tight whereas the tightness of the bounds in [
91 , 92 , 93 , 94 ] is unknown.

### 16 Summary

This chapter dealt with LR codes for the multiple erasures case.
Following an overview of this topic, the chapter focused on the case of
codes with sequential recovery from multiple erasures (S-LR codes) as
some of the principal contributions of the thesis relate to S-LR codes.
These results include (a) new rate-optimal and block-length-optimal
constructions of S-LR codes for the @xmath case, (b) a characterization
of rate-optimal S-LR codes for @xmath , (c) a new upper bound on
dimension for S-LR codes and constructions of S-LR codes achieving this
bound for the @xmath case, (d) a lower bound on the block length of S-LR
codes for @xmath case, which is tighter than the existing bounds in the
literature for @xmath , @xmath . We then present our main result,
namely, an upper bound on the rate of an S-LR code for any @xmath with
@xmath . Constructions of binary codes achieving the upper bound on rate
derived here for @xmath are deferred to the next chapter.

## Chapter \thechapter Proof of Theorem 13.5

###### Proof.

Throughout the proof: the term code symbols of @xmath with indices in a
set @xmath refers to code symbols @xmath . Whenever we say a row of
@xmath denoted by @xmath , @xmath is used to refer to either the vector
formed by the row of the matrix @xmath or the index of the row in matrix
@xmath depending on context. The term columns @xmath of @xmath refers to
the @xmath column, @xmath column,…, @xmath column of the matrix @xmath .
From the description of general form of parity check matrix of
rate-optimal code given in Theorem 13.4 , equation ( 20 ), there exists
(after possible permutation of code symbols of @xmath ) a parity-check
matrix @xmath for the rate-optimal code @xmath of the form (Note that
this is true over any finite field @xmath ):

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denote the @xmath identity matrix and @xmath is a @xmath
matrix with each columns having weight @xmath and each row having weight
@xmath . Consider @xmath rows of @xmath denoted by @xmath and @xmath .
Let @xmath . Since all columns in @xmath has Hamming weight exactly
@xmath , the columns @xmath of @xmath will have non zero entries in rows
@xmath and @xmath only. Let @xmath denote the @xmath sub matrix obtained
by restricting @xmath to the rows @xmath and @xmath and the columns
@xmath . In order to recover from any instance of @xmath erasures in the
code symbols of @xmath with indices in @xmath , any two columns of
@xmath must be linearly independent. Thus the @xmath sub matrix @xmath
is a generator matrix of an @xmath code of block length @xmath and
dimension 2. This also says that any vector, obtained by a non-zero
linear combination of the two rows of @xmath will have a Hamming weight
at least @xmath .
Let us consider two extreme cases:
Case 1: @xmath : In this case, the @xmath code symbols of @xmath with
indices in @xmath form an @xmath MDS code. The sub matrix of @xmath
obtained by restricting @xmath to the columns @xmath and all the rows
apart from the rows @xmath is a zero matrix.
Case 2: @xmath : @xmath .
If these are the only two cases that can occur for any pair of rows of
@xmath denoted by say @xmath and @xmath i.e., @xmath for any 2 distinct
rows of @xmath denoted by @xmath , then the set of code symbols can be
partitioned into two sets, one set of symbols forming a regular-graph
code over @xmath and the other set of symbols forming direct product of
@xmath MDS codes, with no linear constraint involving code symbols from
both the sets i.e., the code @xmath will be a direct product @xmath
(after possible permutation of code symbols of @xmath ) where @xmath is
a regular-graph code over @xmath and @xmath is a direct product of
@xmath MDS codes. Hence the code @xmath has the form stated in the
theorem. Now, we will prove that for any 2 distinct rows of @xmath
denoted by @xmath , @xmath is not possible, where @xmath . Wolog assume
that @xmath for the pair of rows @xmath and @xmath i.e., @xmath . As
denoted before, @xmath are some two elements of @xmath for @xmath .
Assume that the code symbols of @xmath with indices in @xmath are
erased. In order for @xmath to be an S-LR code with @xmath , a linear
combination of @xmath and @xmath and some of the remaining rows of
@xmath must result in a vector @xmath with the following properties.

1.  Hamming weight of @xmath is less than or equal to @xmath .

2.  @xmath has the value zero in the @xmath coordinate and has a non
    zero value in the @xmath coordinate, or vice versa.

Assume that a linear combination of @xmath rows of @xmath denoted by
@xmath results in @xmath where each coefficient in the linear
combination is non-zero. Let @xmath , @xmath . Clearly, @xmath . If
@xmath , we have already shown that the @xmath sub matrix of @xmath ,
obtained by restricting @xmath to the rows @xmath and @xmath and the
columns in @xmath is a generator matrix of an MDS code of block length
@xmath and dimension 2 and any non-zero linear combination of the two
rows of the sub matrix gives a vector of Hamming weight at least @xmath
. Thus the Hamming weight of @xmath is at least @xmath , where the first
term @xmath comes from the identity part of @xmath (i.e., columns @xmath
to @xmath of @xmath ) and the term @xmath comes from the single weight
columns in the sub matrix @xmath of @xmath obtained by restricting
@xmath to the rows @xmath and the columns @xmath to @xmath of @xmath .
Let @xmath . Since the Hamming weight of @xmath , must be upper bounded
by @xmath ,

  -- -------- -------- -- ------
     @xmath   @xmath      
     @xmath   @xmath      
     @xmath   @xmath      (33)
  -- -------- -------- -- ------

Also, by counting the non zero entries in @xmath column wise and row
wise

  -- -------- -------- -- ------
     @xmath   @xmath      (34)
     @xmath   @xmath      (35)
  -- -------- -------- -- ------

Substituting ( 34 ) and ( 35 ) in ( 33 ) gives:

  -- -------- -- ------
     @xmath      
     @xmath      (36)
  -- -------- -- ------

Now assuming @xmath , ( 36 ) gives @xmath . Hence assuming @xmath , we
get

  -- -------- --
     @xmath   
  -- -------- --

Hence we get @xmath , when @xmath . But when @xmath , the first @xmath
co-ordinates of @xmath will have atleast @xmath non-zero values (because
the columns @xmath to @xmath of @xmath form an identity matrix), making
the Hamming weight of @xmath strictly greater than @xmath as @xmath must
also have a non zero @xmath or @xmath coordinate as @xmath . This is a
contradiction as Hamming weight of @xmath is @xmath . Hence, if @xmath ,
@xmath is not possible. Now, assume @xmath i.e., a linear combination of
@xmath and @xmath should give @xmath . As seen before, the Hamming
weight of a linear combination of @xmath and @xmath is at least @xmath
(weight @xmath comes from the coordinates indexed by @xmath , and weight
@xmath comes from the remaining coordinates indexed by @xmath ). Since
weight of @xmath is @xmath , We need,

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

which is not possible as @xmath . Hence @xmath is also not possible.
Hence putting together we have @xmath not possible for @xmath , but we
need to linearly combine @xmath rows to get @xmath . Hence @xmath is not
possible. ∎

## Chapter \thechapter Proof of Theorem 13.6

###### Proof.

Let @xmath be a basis of @xmath with @xmath , where @xmath denotes the
Hamming weight of @xmath , @xmath . Let

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

Let @xmath . Let @xmath denote the total number of columns of @xmath
having Hamming weight @xmath for @xmath . Then by simple counting of
non-zero entries of @xmath row wise and column wise, we get:

  -- -------- -- ------
     @xmath      
     @xmath      (37)
  -- -------- -- ------

Since @xmath is a parity-check matrix of an @xmath S-LR code over @xmath
, all the columns of @xmath must be distinct. Hence @xmath for @xmath .
Also we know that @xmath . Hence substituting @xmath and @xmath in ( 37
):

  -- -------- -- ------
     @xmath      
     @xmath      (38)
  -- -------- -- ------

( 38 ) gives an upper bound on @xmath which is applicable for every
@xmath . Taking minium over @xmath gives:

  -- -------- -- ------
     @xmath      (39)
  -- -------- -- ------

∎

## Chapter \thechapter Proof of Lemma 14.1

###### Proof.

Each column of the @xmath binary matrix @xmath has Hamming weight equal
to @xmath . As a first step, do the following. Permute the rows of
@xmath such that the @xmath non-zero coordinates of the first column
appear in the first two rows. Let the resulting matrix be @xmath . Let
the Hamming weights of first two rows of @xmath be @xmath and @xmath .
Now, keeping the first column fixed, permute the columns of @xmath such
that the @xmath non zero coordinates of the first row appear in the
first @xmath columns and the @xmath non-zero coordinates (except the
first coordinate) of the second row appear in columns @xmath through
@xmath . Note that except the first column, there is no column with non
zero entries in both of the first @xmath rows. This follows from the
fact that @xmath . If a column has non-zero elements in the first row
and @xmath row, @xmath , then the remaining columns cannot have non-zero
entries in the second row and the @xmath row. This is due to the fact
that these two columns (one column with non-zero entry in the first row
and @xmath row and the other column with non-zero entry in the second
row and @xmath row) along with the first column of @xmath forms a set of
@xmath linearly dependent vectors, which would imply @xmath . Hence, if
a column has non-zero elements in the first row and @xmath row, then a
column with non-zero entries in the second and @xmath row cannot occur
and if a column (apart from the first column) has a one in the first
row, there are only @xmath possible coordinates where it can have the
second non-zero entry. Similarly if a column (apart from the first
column) has a one in the second row, there are only @xmath possible
coordinates where it can have the second non-zero entry. Hence, we have

  -- -------- -- ------
     @xmath      (40)
  -- -------- -- ------

Remove the first @xmath columns of @xmath and then remove the first
@xmath rows of @xmath . Let the resulting matrix be @xmath . In @xmath ,
since none of columns other then the first @xmath columns of @xmath has
ones in any of the first 2 rows, the new matrix @xmath is also a
parity-check matrix of a code with @xmath and each column of weight
@xmath with @xmath rows. As a second step, repeat the same arguments as
first step on @xmath . Let the resulting matrix after permutation of
rows and columns of @xmath in the second step be @xmath . Let @xmath and
@xmath denote the Hamming-weights of the row one and row two of @xmath
respectively. Using similar arguments as in the case of ( 40 ), we get

  -- -------- --
     @xmath   
  -- -------- --

Now remove the first @xmath columns of @xmath and then remove the first
two rows of @xmath as before and let the resulting matrix be @xmath .
Now as a third step, repeat the same arguments on @xmath . Repeat the
procedure until all columns are exhausted i.e., all the columns are
removed in the process. Let @xmath denote the total number of steps in
the process (we are removing atleast one column in each step. So the
process will terminate.). As we progress, we are removing @xmath columns
in the first step and @xmath columns in the second step and so on.
Hence, the number of columns of @xmath is given by:

  -- -------- --
     @xmath   
  -- -------- --

In the @xmath step we get:

  -- -------- -- ------
     @xmath      (41)
  -- -------- -- ------

Since @xmath rows are removed in each step, the number of steps is upper
bounded as follows:

  -- -------- -- ------
     @xmath      (42)
  -- -------- -- ------

From ( 41 ) and ( 42 ), we have

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

Hence,

  -- -------- -------- --
     @xmath   @xmath   
                       
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

∎

## Chapter \thechapter Proof of Theorem 14.2

###### Proof.

Let,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

Let @xmath be a basis of @xmath with @xmath , @xmath . Let

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

Let @xmath . Extend the basis @xmath of @xmath to a basis of @xmath and
form a @xmath parity-check matrix @xmath of @xmath , with this basis of
@xmath as its rows. Hence

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

where the rows of @xmath are precisely those vectors added to the basis
of @xmath while extending it to a basis of @xmath .

  -- -------- -- ------
     @xmath      (43)
  -- -------- -- ------

@xmath is a parity-check matrix of an @xmath S-LR code as the row space
of @xmath contains all the codewords of @xmath which has weight @xmath .
We will consider the @xmath S-LR code defined by the @xmath parity-check
matrix @xmath and derive a lower bound on @xmath as a function of @xmath
and @xmath . Using ( 43 ) and the derived lower bound on @xmath , we
obtain a lower bound on @xmath . Let @xmath be the number of columns of
@xmath having weights @xmath and @xmath respectively. Then by counting
non zero entries of @xmath row wise and column wise, we get:

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (44)
  -- -------- -------- -------- -- ------

Permute the columns and rows of @xmath such that:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

where @xmath is a @xmath diagonal matrix with non zero diagonal entries
and the set of first @xmath columns are the set of all columns of @xmath
having weight one. Note that in the first @xmath columns having weight
one there can’t be 2 columns with non zero entry at exactly the same row
as this would imply @xmath (where @xmath is the minimum distance of the
code defined by the parity-check matrix @xmath ). Now the @xmath columns
having weight @xmath are to the right of @xmath . In these @xmath
columns, we cannot have a column with two non zero entries in the first
@xmath rows, as this would imply @xmath , which is a contradiction since
the code defined by parity-check matrix @xmath is also an @xmath S-LR
code which requires @xmath . Let @xmath denote number of columns of
weight @xmath with exactly one non zero entry in the first @xmath rows
and @xmath denote the number of columns of weight @xmath with both non
zero entries in the last @xmath rows.

  -- -------- -------- -- ------
     @xmath   @xmath      (45)
     @xmath   @xmath      (46)
     @xmath   @xmath      (47)
  -- -------- -------- -- ------

where @xmath is the maximum number of columns in a parity-check matrix
(of a code with minimum distance @xmath ) with @xmath rows and each
column having a Hamming weight 2. Here, @xmath because the sub matrix of
@xmath , obtained by restricting @xmath to these @xmath columns and the
last @xmath rows is a parity-check matrix (of a code with minimum
distance @xmath ) with @xmath rows and each column having a Hamming
weight of 2. Restricting to binary codes, it is straightforward to see
that, @xmath . Using Lemma 14.1 , this can be tightened to:

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

Hence from ( 45 ),

  -- -------- -------- -- ------
     @xmath   @xmath      (48)
     @xmath   @xmath      (49)
  -- -------- -------- -- ------

Inequalities ( 50 ) and ( 51 ) given below are obtained by substituting
the above upper bounds ( 48 ) and ( 49 ) on @xmath in ( 44 ).

  -- -------- -------- -- ------
     @xmath   @xmath      (50)
     @xmath   @xmath      
              @xmath      (51)
  -- -------- -------- -- ------

( 50 ) (On using @xmath ) leads to:

  -- -------- --
     @xmath   
  -- -------- --

which along with the fact @xmath and the fact that the derivative of
@xmath at the negative root of @xmath is @xmath , shows that @xmath is
atleast the positive root of @xmath . Hence,

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

which when added with @xmath gives a required lower bound on the
block-length @xmath . ( 51 ) (On using @xmath ) leads to:

  -- -------- --
     @xmath   
  -- -------- --

which along with the fact @xmath and the fact that the derivative of
@xmath at the negative root of @xmath is @xmath , shows that @xmath is
atleast the positive root of @xmath . Hence,

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

Hence using @xmath , @xmath and @xmath we get

  -- -------- --
     @xmath   
  -- -------- --

Using @xmath , we get

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denote the set of natural numbers. ∎

## Chapter \thechapter Proof of theorem 15.1

###### Proof.

Throughout this proof: we denote any set of the form @xmath by @xmath
for any @xmath . The term @xmath -weight column of @xmath refers to a
column of the matrix @xmath with Hamming weight @xmath . The term sum of
row weights of a matrix @xmath , refers to the sum of weight of each row
of the matrix @xmath . Similar definition applies for the term sum of
column weights. For an @xmath matrix @xmath and for some @xmath , @xmath
with @xmath , @xmath and @xmath , @xmath we denote by @xmath , the
@xmath submatrix of @xmath with @xmath th entry of @xmath equal to
@xmath th entry of @xmath . The term empty matrix refers to a @xmath or
@xmath or @xmath matrix, for some @xmath . If a matrix is not an empty
matrix, it will be termed as a non-empty matrix.

## Appendix A case i: @xmath an even integer

Recall that @xmath where @xmath are @xmath linearly independent
codewords with @xmath , @xmath . It can be seen that the matrix @xmath
after permutation of rows and columns can be written in the form given
in ( 52 ).

  -- -------- -- ------
     @xmath      (52)
  -- -------- -- ------

where

1.  Rows are labeled by the integers @xmath with top most row of @xmath
    labelled @xmath and next row labeled @xmath and so on. Similarly
    columns are labeled by the integers @xmath with leftmost column of
    @xmath labeled @xmath and next column labeled @xmath and so on,

2.  @xmath is a @xmath matrix for @xmath , @xmath is a @xmath matrix for
    @xmath for some @xmath . Note that any of @xmath , @xmath is allowed
    to take the value 0 for any @xmath ,

3.  @xmath is a matrix with each column having weight 1 and each row
    having weight at least 1. The first @xmath columns of @xmath
    contains the columns of @xmath . The set of first @xmath columns of
    @xmath is equal to the set of all those columns of @xmath which has
    weight @xmath ,

4.  @xmath , @xmath , @xmath are such that for @xmath , each column of
    @xmath has weight 2, each column of @xmath has weight at least 1 and
    each row of @xmath has weight at least 1 and each column of @xmath
    has weight at most 1,

5.  @xmath is a matrix with each column having weight 2. The set of
    columns of the matrix @xmath is equal to the set of all those
    columns of @xmath which has weight @xmath .

Let @xmath . We set @xmath , if @xmath is an empty matrix. We now
redefine @xmath based on the value of @xmath . We set @xmath , @xmath to
be empty matrices and set @xmath , @xmath . Let @xmath such that @xmath
is the set of labels of all the 2-weight columns of @xmath apart from
those 2-weight columns of @xmath containing the columns of @xmath . Let
@xmath . If @xmath then @xmath is defined by ( 52 ). If @xmath , we
redefine @xmath . If @xmath , the matrix @xmath can be written in the
form given in ( 53 ) and hence defined by ( 53 ).

  -- -------- -- ------
     @xmath      (53)
  -- -------- -- ------

Irrespective of the value of @xmath , let the number of columns in
@xmath be denoted as @xmath . If @xmath is an empty matrix then we can
clearly set @xmath . The entire derivation of upper bound on rate is
correct and all the inequalities in the derivation will hold with @xmath
, @xmath , @xmath . Let @xmath . Let @xmath . If @xmath , then the proof
of the following Lemma A.1 (since the proof of Lemma A.1 proceeds by
induction starting with the proof of the lemma for @xmath first and then
proceeding to @xmath and then @xmath and so on and hence we prove the
lemma for @xmath first and then proceed to @xmath and since @xmath must
be non-empty matrices @xmath ) will imply that each column of @xmath has
weight 1 which will imply that @xmath cannot be an empty matrix. Hence
the case @xmath , a non-empty matrix and @xmath , an empty matrix cannot
occur. Although we have to prove the following Lemma A.1 for @xmath ,
@xmath , @xmath , we assume all @xmath @xmath , @xmath to be non-empty
matrices and prove the lemma. Since the proof of the lemma is by
induction, the induction can be made to stop after proving the lemma for
@xmath (induction starts by proving the lemma for @xmath as mentioned
before) and the proof is unaffected by it.

###### Lemma A.1.

For @xmath , @xmath is a matrix with each column having weight 1. For
@xmath , @xmath is a matrix with each row having weight 1 and each
column having weight 1.

###### Proof.

Let @xmath be the minimum distance of the code with parity check matrix
@xmath . We use the fact that @xmath to prove the lemma.
It is enough to show that:

-   For @xmath , @xmath is a matrix with each column having weight 1.

-   For @xmath , @xmath is a matrix with each row having weight 1.

This is because the property that @xmath is a matrix with each column
having weight 1 combined with the fact that each column of @xmath has
weight 2 implies that @xmath is a matrix with each column having weight
1 and @xmath by definition is a matrix with each column having weight 1.
Let us denote the column of the matrix @xmath with label @xmath by
@xmath , @xmath . Let us show the lemma by induction as follows:
Induction Hypothesis:

-   We induct on a variable denoted by @xmath .

-   Property @xmath : any @xmath vector having weight at most 2 with
    support contained in @xmath can be written as some linear
    combination of vectors @xmath for some @xmath and for some @xmath .

-   Let us assume as induction hypothesis that the property @xmath is
    true and the Lemma A.1 is true for @xmath , @xmath .

Initial step @xmath and @xmath :

-   We show that each row of @xmath has Hamming weight exactly 1.
    Suppose there exists a row of @xmath with Hamming weight more than
    1; let the support set of the row be @xmath . Then the columns
    @xmath of @xmath can be linearly combined to give a zero column.
    This contradicts the fact that @xmath and @xmath is even. Hence, all
    rows of @xmath have Hamming weight exactly 1.

-   If @xmath , then the lemma is already proved. So let @xmath .

-   We show that each column of @xmath has Hamming weight exactly 1.
    Suppose @xmath column of @xmath @xmath has Hamming weight 2; let the
    support of the column be @xmath in @xmath . Then the column @xmath
    in @xmath along with the 2 column vectors of @xmath say @xmath where
    @xmath where @xmath has exactly one non-zero entry in @xmath
    co-ordinate and @xmath has exactly one non-zero entry @xmath
    co-ordinate, can be linearly combined to give a zero column again
    leading to a contradiction on minimum distance. Such columns with
    one column having only one non-zero entry exactly in @xmath
    co-ordinate and another column having only one non-zero entry
    exactly in @xmath co-ordinate with column labels in @xmath exist due
    to the 1-weight columns in the matrix @xmath .

-   The above argument also shows that any @xmath vector having Hamming
    weight at most 2 with support contained in @xmath can be written as
    some linear combination of at most 2 column vectors of @xmath say
    @xmath for some @xmath ( @xmath ). Hence Property @xmath is true.

-   We now show that each row of @xmath has Hamming weight exactly 1.
    Suppose @xmath row of @xmath has Hamming weight more than 1; let the
    support set of the row be @xmath in @xmath . Now there is some
    linear combination of columns @xmath and @xmath in @xmath that gives
    a zero in @xmath coordinate and thus this linear combination has
    support contained in @xmath with Hamming weight at most 2. Now
    applying Property @xmath on this linear combination implies that
    there is a non-empty set of at most @xmath linearly dependent
    columns in @xmath leading to a contradiction on minimum distance.

-   Now we show that Property @xmath is true: We have to prove that any
    @xmath vector with Hamming weight at most @xmath with support
    contained in @xmath can be written as linear combination of at most
    @xmath column vectors of @xmath say @xmath for some @xmath and
    @xmath . This can be easily seen using arguments similar to ones
    presented before. Let an @xmath vector have non-zero entries exactly
    in coordinates @xmath or @xmath . Then this vector can be linearly
    combined with at most 2 column vectors in @xmath say @xmath where
    @xmath ( @xmath ) (2 columns @xmath with first and second column
    having a non-zero entry in co-ordinates @xmath respectively or a
    column @xmath with a non-zero entry in @xmath co-ordinate. These
    columns @xmath or @xmath exist due to @xmath . Note that @xmath is a
    matrix with each row and column having weight exactly 1.) to form a
    @xmath vector with Hamming weight at most 2 with support contained
    in @xmath which in turn can be written as linear combination of at
    most @xmath column vectors in @xmath say @xmath for some @xmath (
    @xmath or @xmath ) by property @xmath . Hence the given @xmath
    vector is written as linear combination of at most @xmath column
    vectors in @xmath say @xmath for some @xmath and @xmath .

Induction step :

-   Let us assume by induction hypothesis that Property @xmath is true
    and the Lemma A.1 is true for @xmath , @xmath for some @xmath and
    prove the induction hypothesis for @xmath . For @xmath , the initial
    step of induction completes the proof of the Lemma A.1 . Hence
    assume @xmath .

-   Now we show that each column of @xmath has Hamming weight exactly 1:
    suppose @xmath column of @xmath for some @xmath has Hamming weight
    2; let the support of the column be @xmath in @xmath . It is clear
    that the corresponding column vector @xmath in @xmath is a vector
    with support contained in @xmath and Hamming weight 2. Now applying
    Property @xmath on this column vector @xmath implies that there is a
    non-empty set of at most @xmath columns in @xmath which are linearly
    dependent; hence contradicts the minimum distance as @xmath . Hence
    each column of @xmath has Hamming weight exactly 1.

-   Now we show that each row of @xmath has Hamming weight exactly 1:
    suppose @xmath row of @xmath has Hamming weight more than 1; let the
    support set of the row be @xmath in @xmath . Now some linear
    combination of columns @xmath and @xmath in @xmath will make the
    resulting vector have a @xmath in @xmath coordinate and the
    resulting vector also has Hamming weight at most 2 with support
    contained in @xmath and hence applying Property @xmath on this
    resulting vector implies that there is a non-empty set of at most
    @xmath columns in @xmath which are linearly dependent; hence
    contradicts the minimum distance as @xmath ; thus proving that each
    row of @xmath has Hamming weight exactly 1.

-   Now we show that Property @xmath is true: We have to prove that any
    @xmath vector with Hamming weight at most @xmath with support
    contained in @xmath can be written as linear combination of at most
    @xmath column vectors of @xmath say @xmath for some @xmath and
    @xmath . This can be easily seen using arguments similar to ones
    presented before. Let an @xmath vector have non-zero entries in
    coordinates @xmath or @xmath . Then this vector can be linearly
    combined with at most 2 column vectors in @xmath say @xmath where
    @xmath with @xmath or @xmath , (2 columns @xmath with first column
    and second column having a non-zero entry in co-ordinates @xmath
    respectively or a column @xmath with a non-zero entry in @xmath
    co-ordinate. These columns @xmath or @xmath exist due to @xmath .
    Note that @xmath is a matrix with each row and column having weight
    exactly @xmath .) to form a @xmath vector with Hamming weight at
    most 2 with support contained in @xmath which in turn can be written
    as linear combination of at most @xmath column vectors in @xmath say
    @xmath for some @xmath and @xmath by property @xmath . Hence the
    given @xmath vector is written as linear combination of at most
    @xmath column vectors in @xmath say @xmath for some @xmath and
    @xmath .

∎

By Lemma A.1 , after permutation of columns of @xmath (in ( 52 ) or ( 53
) depending on @xmath ) within the columns labeled by the set @xmath for
@xmath , the matrix @xmath can be assumed to be a diagonal matrix with
non-zero entries along the diagonal and hence @xmath , @xmath . Since
the sum of the column weights of @xmath must equal the sum of the row
weights and since each row of @xmath for @xmath can have weight atmost
@xmath and not @xmath due to 1-weight rows in @xmath , and since for
@xmath , @xmath is an empty matrix and we have set @xmath , we obtain:

  -- -------- -------- -------- -- ------
     @xmath                        
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (54)
  -- -------- -------- -------- -- ------

For some @xmath ,

  -- -------- -- ------
     @xmath      (55)
  -- -------- -- ------

By equating sum of row weights of @xmath , with sum of column weights of
@xmath , we obtain:

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (56)
  -- -------- -------- -------- -- ------

Note that if @xmath is an empty matrix then also the inequality ( 56 )
is true as we would have set @xmath . If @xmath and @xmath a non-empty
matrix then the number of rows in @xmath is @xmath with each column of
@xmath having weight 2, hence the inequality ( 56 ) is still true.
Substituting ( 55 ) in ( 56 ) we get:

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (57)
  -- -------- -------- -------- -- ------

By equating sum of row weights of @xmath , with sum of column weights of
@xmath , we obtain:

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (58)
  -- -------- -------- -------- -- ------

If @xmath then @xmath , @xmath . If @xmath is an empty matrix then
@xmath . Hence the inequality ( 58 ) is true irrespective of whether
@xmath or @xmath (even if @xmath is an empty matrix).

  -- -------- -------- -------- -- ------
                                   
     @xmath   @xmath   @xmath      (59)
  -- -------- -------- -------- -- ------

Our basic inequalities are ( 54 ),( 55 ),( 56 ),( 58 ). We manipulate
these 4 inequalities to derive the bound on rate. Substituting ( 55 ) in
( 59 ) we get:

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (60)
  -- -------- -------- -------- -- ------

Substituting ( 57 ) in ( 60 ), we get:

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      
     @xmath   @xmath               (61)
  -- -------- -------- -------- -- ------

From ( 55 ), for any @xmath :

  -- -------- -- ------
     @xmath      (62)
  -- -------- -- ------

Subtituting ( 54 ) for @xmath in ( 62 ), we get:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (63)
  -- -------- -------- -------- -- ------

Let,

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (64)
     @xmath   @xmath   @xmath      (65)
  -- -------- -------- -------- -- ------

Let us prove the following inequality by induction for @xmath ,

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (66)
  -- -------- -------- -------- -- ------

( 66 ) is true for @xmath by ( 61 ). Hence ( 66 ) is proved for @xmath
and the range of @xmath is vacuous for @xmath . Hence assume @xmath .
Hence let us assume ( 66 ) is true for @xmath such that @xmath and prove
it for @xmath . Substituting ( 63 ) for @xmath in ( 66 ), we get:

  -- -------- -------- -------- -------- ------
     @xmath   @xmath   @xmath            
              @xmath   @xmath            
     @xmath   @xmath   @xmath            (68)
                                @xmath   
  -- -------- -------- -------- -------- ------

Substituing ( 65 ) in ( 68 ), we obtain

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (69)
  -- -------- -------- -------- -- ------

Hence ( 66 ) is proved for any @xmath for @xmath . Hence writing ( 66 )
for @xmath for @xmath , we obtain:

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (70)
  -- -------- -------- -------- -- ------

It can be seen that @xmath for @xmath has a product form as:

  -- -------- -- ------
     @xmath      (71)
  -- -------- -- ------

Hence for @xmath , @xmath :

  -- -------- --
     @xmath   
  -- -------- --

Hence we can substitute ( 63 ) for @xmath in ( 70 ) :

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      
              @xmath   @xmath      
              @xmath               (73)
  -- -------- -------- -------- -- ------

Substituting ( 65 ) in ( 73 ), we obtain:

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (74)
  -- -------- -------- -------- -- ------

Using ( 71 ), we obtain:

  -- -------- --
     @xmath   
  -- -------- --

Hence ( 74 ) implies:

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (75)
  -- -------- -------- -------- -- ------

( 75 ) after some algebraic manipulations gives the required upper bound
on @xmath and hence gives the required upper bound on @xmath as stated
in the theorem. Note that although the derivation is valid for @xmath ,
@xmath , the final bound given in the theorem is correct and tight for
@xmath . The upper bound on rate for @xmath can be derived specifically
by substituting @xmath in ( 61 ) and noting that @xmath .

## Appendix B case ii: @xmath an odd integer

Again it can be seen that the matrix @xmath after permutation of rows
and columns can be written in the form given in ( 76 ).

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (76)
  -- -------- -------- -------- -- ------

where

1.  Rows are labeled by the integers @xmath with top most row of @xmath
    labelled @xmath and next row labeled @xmath and so on. Similarly
    columns are labeled by the integers @xmath with leftmost column of
    @xmath labeled @xmath and next column labeled @xmath and so on,

2.  @xmath is a @xmath matrix for @xmath , @xmath is a @xmath matrix for
    @xmath for some @xmath . Note that any of @xmath , @xmath is allowed
    to take the value 0 for any @xmath ,

3.  @xmath is a matrix with each column having weight 1 and each row
    having weight at least 1. The first @xmath columns of @xmath
    contains the columns of @xmath . The set of first @xmath columns of
    @xmath is equal to the set of all those columns of @xmath which has
    weight @xmath ,

4.  @xmath , @xmath , @xmath are such that for @xmath , each column of
    @xmath has weight 2, each column of @xmath has weight at least 1 and
    each row of @xmath has weight at least 1 and each column of @xmath
    has weight at most 1,

5.  @xmath is a matrix with each column having weight 2. The set of
    columns of the matrix @xmath is equal to the set of all those
    columns of @xmath which has weight @xmath .

Let @xmath . We set @xmath , if @xmath is an empty matrix. We now
redefine @xmath based on the value of @xmath . We set @xmath , @xmath to
be empty matrices and set @xmath , @xmath . Let @xmath such that @xmath
is the set of labels of all the 2-weight columns of @xmath apart from
those 2-weight columns of @xmath containing the columns of @xmath . Let
@xmath . If @xmath then @xmath is defined by ( 76 ). If @xmath , we
redefine @xmath . If @xmath , the matrix @xmath can be written in the
form given in ( 77 ) and hence defined by ( 77 ).

  -- -------- -- ------
     @xmath      (77)
  -- -------- -- ------

Irrespective of the value of @xmath , let the number of columns in
@xmath be denoted as @xmath . If @xmath is an empty matrix then we can
clearly set @xmath . The entire derivation of upper bound on rate is
correct and all the inequalities in the derivation will hold with @xmath
, @xmath , @xmath . Let @xmath . Let @xmath . If @xmath , then the proof
of the following Lemma B.1 (since the proof of Lemma B.1 proceeds by
induction starting with the proof of the lemma for @xmath first and then
proceeding to @xmath and then @xmath and so on and hence we prove the
lemma for @xmath first and then proceed to @xmath and since @xmath must
be non-empty matrices @xmath ) will imply that each column of @xmath has
weight 1 which will imply that @xmath cannot be an empty matrix. Hence
the case @xmath , a non-empty matrix and @xmath , an empty matrix cannot
occur. Although we have to prove the following Lemma B.1 for @xmath ,
@xmath , @xmath , we assume all @xmath @xmath , @xmath to be non-empty
matrices and prove the lemma. Since the proof of the lemma is by
induction, the induction can be made to stop after proving the lemma for
@xmath (induction starts by proving the lemma for @xmath as mentioned
before) and the proof is unaffected by it.

###### Lemma B.1.

For @xmath , @xmath is a matrix with each column having weight 1. For
@xmath , @xmath is a matrix with each row and each column having weight
1. @xmath is a matrix with each column having weight 1.

###### Proof.

Proof is exactly similar to the proof of Lemma A.1 and proceeds by
induction. So we skip the proof. ∎

By Lemma B.1 , after permutation of columns of @xmath (in ( 76 ) or ( 77
) depending on @xmath ) within the columns labeled by the set @xmath for
@xmath , the matrix @xmath can be assumed to be a diagonal matrix with
non-zero entries along the diagonal and hence @xmath , for @xmath .
Since the sum of the column weights of @xmath must equal the sum of the
row weights and since each row of @xmath for @xmath can have weight
atmost @xmath and not @xmath due to weight one rows in @xmath , and
since for @xmath , @xmath is an empty matrix and we have set @xmath , we
obtain:

  -- -------- -------- -------- -- ------
     @xmath                        
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (78)
  -- -------- -------- -------- -- ------

For some @xmath ,

  -- -------- -- ------
     @xmath      (79)
  -- -------- -- ------

By equating sum of row weights of @xmath , with sum of column weights of
@xmath , we obtain:

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (80)
  -- -------- -------- -------- -- ------

If @xmath then the number of rows in @xmath is @xmath with each column
of @xmath having weight 2 and @xmath (and @xmath if @xmath is also an
empty matrix), hence the inequality ( 80 ) is true. If @xmath and @xmath
is an empty matrix then also the inequality ( 80 ) is true as we would
have set @xmath . Substituting ( 79 ) in ( 80 ):

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (81)
  -- -------- -------- -------- -- ------

By equating sum of row weights of @xmath , with sum of column weights of
@xmath , we obtain (Note that if @xmath is an empty matrix then also the
following inequality is true as we would have set @xmath ):

  -- -------- -- ------
     @xmath      (82)
  -- -------- -- ------

By equating sum of row weights of @xmath , with sum of column weights of
@xmath , we obtain

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (83)
  -- -------- -------- -------- -- ------

If @xmath then @xmath @xmath . If @xmath is an empty matrix then @xmath
. Hence the inequality ( 83 ) is true irrespective of whether @xmath or
@xmath (even if @xmath is an empty matrix).

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (84)
  -- -------- -------- -------- -- ------

Our basic inequalities are ( 78 ),( 79 ),( 80 ),( 82 ),( 83 ). We
manipulate these 5 inequalities to derive the bound on rate.
Substituting ( 81 ) in ( 84 ):

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (85)
  -- -------- -------- -------- -- ------

For @xmath , ( 85 ) becomes:

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (86)
  -- -------- -------- -------- -- ------

Substituting ( 82 ) in ( 86 ):

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (87)
  -- -------- -------- -------- -- ------

Substituting ( 79 ) in ( 87 ):

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (88)
  -- -------- -------- -------- -- ------

( 88 ) implies,

  -- -------- -- ------
     @xmath      (89)
  -- -------- -- ------

( 89 ) proves the bound ( 30 ) for @xmath . Hence from now on we assume
@xmath .
For @xmath , ( 85 ) implies:

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (90)
  -- -------- -------- -------- -- ------

Substituting ( 78 ) in ( 90 ) and since @xmath :

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (91)
  -- -------- -------- -------- -- ------

Rewriting ( 79 ):

  -- -------- -- ------
     @xmath      (92)
  -- -------- -- ------

Substituting ( 82 ),( 78 ) in ( 92 ):

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath                        
     @xmath                        (93)
  -- -------- -------- -------- -- ------

Substituting ( 93 ) in ( 91 ):

  -- -------- -------- -------- -- ------
     @xmath   @xmath               
     @xmath   @xmath   @xmath      (94)
  -- -------- -------- -------- -- ------

( 94 ) after some algebraic manipulations gives the required upper bound
on @xmath and hence gives the required upper bound on @xmath as stated
in the theorem.
∎

## Chapter \thechapter Optimal Constructions of Sequential LR Codes

In this chapter, a construction of rate-optimal S-LR codes is presented
for any @xmath with @xmath . The construction presented here will
establish the tightness of the upper bound on rate derived in Chapter
Erasure Codes for Distributed Storage: Tight Bounds and Matching
Constructions . The starting point of the construction is the staircase
form of the parity-check (p-c) matrix @xmath of a rate-optimal S-LR code
derived in Chapter Erasure Codes for Distributed Storage: Tight Bounds
and Matching Constructions , equations ( 31 ), ( 32 ). It will be shown
that this forces the code to possess a certain, simple, graphical
representation @xmath . In @xmath , the edges represent code symbols and
nodes, the parity checks. While the constraints imposed by the staircase
structure of the parity-check matrix @xmath are necessary for a
rate-optimal S-LR code, they are not sufficient to guarantee
(sequential) recovery from @xmath erasures. It turns out that however,
that adding the further constraint that @xmath have girth @xmath , leads
to a necessary and sufficient condition. Section 3 presents the
graphical representation @xmath dictated by the staircase form of the
p-c matrix. The section following, Section 5 shows how to construct this
graph @xmath that satisfies in addition, the girth requirement, and this
completes the construction of rate-optimal S-LR codes since the code
follows from the graph. Section 6 discusses the construction of S-LR
codes that are not only rate-optimal, but which are also optimal in
terms of having the shortest possible block length. This leads to a
class of graphs known as Moore graphs. The final section present a
summary. Throughout this chapter whenever we refer to a code, it refers
to an S-LR code with parameters @xmath defined in Chapter Erasure Codes
for Distributed Storage: Tight Bounds and Matching Constructions . We
use the term nodes or vertices to indicate the vertices of a graph. We
use the term vertex set to denote the set of all vertices in a graph and
the term edge set to denote the set of all edges in a graph. The vertex
set of a graph @xmath is denoted by @xmath . Throughout this thesis, for
a code @xmath , a parity check refers to a codeword @xmath in the dual
code @xmath or to the equation @xmath for a generator matrix @xmath of
@xmath .

### 3 A Graphical Representation for the Rate-Optimal Code

We show in this section, that the staircase form of the parity-check
(p-c) matrix forced on a rate-optimal code (Chapter Erasure Codes for
Distributed Storage: Tight Bounds and Matching Constructions , equations
( 31 ), ( 32 )) leads to a tree-like graphical representation of the
code. The structure is slightly different for @xmath odd and @xmath
even. We begin with the @xmath -even case.

#### 3.1 @xmath Even case

In the case @xmath even, the p-c matrix of a rate-optimal code can be
put into the form (Chapter Erasure Codes for Distributed Storage: Tight
Bounds and Matching Constructions , equation ( 31 )):

  -- -------- -- ------
     @xmath      (95)
  -- -------- -- ------

where @xmath , or equivalently, @xmath . Our goal is to show that @xmath
forces the code to have a certain graphical representation. We note
first that, each column in @xmath , apart from the columns associated to
diagonal sub-matrix @xmath have (Hamming) weight @xmath . To bring in
symmetry, we add an additional row to @xmath at the top, which has all
@xmath s in the columns associated to @xmath and zeros elsewhere to
obtain the matrix @xmath , shown in ( 96 ).

  -- -- -------- -------- -- ------
        @xmath   @xmath      (96)
  -- -- -------- -------- -- ------

Since each column of @xmath has weight @xmath , the matrix has a natural
interpretation as the node-edge incidence matrix of a graph where the
incidence matrix of the graph is obtained by replacing every non-zero
entry of @xmath with @xmath . Hence the vertices of the graph are in one
to one correspondence with the rows of the matrix @xmath and edges of
the graph are in one to one correspondence with the columns of the
matrix @xmath where an edge corresponding to a column with non-zero
entries at rows @xmath connects the nodes corresponding to the rows
@xmath of @xmath . The nodes of the graph corresponding to those rows of
@xmath containing the rows of @xmath will be denoted by @xmath and
similarly, the edges corresponding to those columns of @xmath containing
the columns of @xmath will be denoted by @xmath . The edges associated
with those columns of @xmath containing the columns of @xmath will be
denoted by @xmath . We use @xmath to denote the node associated with the
row at the very top of @xmath (see ( 96 )). Each node except the node
@xmath has degree @xmath .

##### 3.1.1 Unravelling the Graph

We can unravel the structure of this graph as follows. Fig. 20 shows the
graph for the case @xmath with @xmath . Since each row of @xmath , apart
from the top row, has weight @xmath , it follows that in the resultant
graph, every node except the node @xmath has degree @xmath . Node @xmath
has degree @xmath , since @xmath is a diagonal matrix. The @xmath edges
originating from @xmath are terminated in the @xmath nodes making up
@xmath . We will use @xmath to denote this collection of edges. There
are @xmath other edges that emanate from each node in @xmath , each of
these edges is terminated at a distinct node in @xmath . We use @xmath
to denote this collection of edges. Each of the other @xmath edges that
emanate from each node in @xmath , terminate in a distinct node in
@xmath . We use @xmath to denote this collection of edges. We continue
in this fashion, until we reach the nodes in @xmath via edge-set @xmath
. Here, the @xmath other edges outgoing from each node in @xmath are
terminated among themselves. We use @xmath to denote this last
collection of edges. As can be seen, the graph has a tree-like
structure, except for the edges (corresponding to edge-set @xmath )
linking the leaf nodes @xmath at the very bottom. We use @xmath to
denote the overall graph and use @xmath to denote the restriction of
@xmath to node-set @xmath i.e., @xmath denotes the subgraph of @xmath
induced by the nodes @xmath for @xmath . Thus the graphs are nested:

  -- -------- --
     @xmath   
  -- -------- --

Fig. 20 identifies the graphs @xmath for the case @xmath .

##### 3.1.2 Connecting the Graph to the Code

Each node in the graph @xmath is associated to a row of the p-c matrix
@xmath of the code and hence to a p-c. The one exception is the
fictitious node @xmath which does not correspond to a p-c. When the p-c
matrix @xmath is over @xmath , the top row of @xmath is the binary sum
of all other rows of @xmath . Hence in case when @xmath is over @xmath ,
@xmath also represent a p-c matrix of the code and the node @xmath is
also associated to a p-c. Also, each edge in the graph @xmath is
associated to a unique code symbol as the edges are associated with
columns of the matrix @xmath . The structure of @xmath is mostly
determined once we specify @xmath as @xmath with edges in @xmath removed
is just a tree with @xmath as the root node. Hence the only freedom lies
in selecting the pairs of nodes in node-set @xmath that are linked by
the edges in edge-set @xmath . The p-c matrix requires however, that
these edges be selected such that each node in @xmath is of degree
@xmath .

#### 3.2 @xmath Odd Case

In the case @xmath odd, the p-c matrix of a rate-optimal code can be put
into the form (Chapter Erasure Codes for Distributed Storage: Tight
Bounds and Matching Constructions , equation ( 32 )):

  -- -- -------- -------- -- ------
        @xmath   @xmath      (97)
  -- -- -------- -------- -- ------

where @xmath , or equivalently, @xmath . Our goal once again, is to show
that @xmath forces the code to have a certain graphical representation.
We add here as well, an additional row to @xmath at the top, which has
all @xmath s in the columns associated to @xmath and zeros elsewhere to
obtain the matrix @xmath , shown in ( 98 ).

  -- -------- -- ------
     @xmath      (98)
  -- -------- -- ------

Since each column of @xmath also has weight @xmath , the matrix again
has an interpretation as the node-edge incidence matrix of a graph where
the incidence matrix of the graph is obtained by replacing every
non-zero entry of @xmath with @xmath . We retain the earlier notation
with regard to node sets @xmath and node @xmath and edge sets @xmath but
now w.r.t the matrix @xmath (see ( 98 )). Here also each node except the
node @xmath has degree @xmath .

##### 3.2.1 Unravelling the Graph

We can unravel the structure of this graph exactly as in the case of
@xmath even. Fig. 21 shows the graph for the case @xmath with @xmath .
Differences compared to the case @xmath even, appear only when we reach
the nodes @xmath via edge-set @xmath . Here, the @xmath other edges
outgoing from each node in @xmath are terminated in the node set @xmath
. We use @xmath to denote this last collection of edges. As can be seen,
the graph has a tree-like structure, except for the edges (corresponding
to edge-set @xmath ) linking nodes in @xmath and @xmath .  The
restriction of the overall graph to @xmath i.e., the subgraph induced by
the nodes @xmath can be seen to be a bipartite graph @xmath where each
node in @xmath has degree @xmath while each node in @xmath has degree
@xmath . We use @xmath to denote the overall graph and use @xmath to
denote the restriction of @xmath to node-set @xmath i.e., @xmath denotes
the subgraph of @xmath induced by the nodes @xmath for @xmath . Thus the
graphs are nested:

  -- -------- --
     @xmath   
  -- -------- --

Fig. 21 identifies the graphs @xmath for the case @xmath .

###### Remark 6.

We note here that the bipartite graph @xmath must be bi-regular with
nodes of degree @xmath above, corresponding to nodes in @xmath and nodes
of degree @xmath corresponding to nodes below (i..e, nodes in @xmath ).
From the tree-like structure of the graph @xmath it follows that the
number of nodes in @xmath and @xmath are respectively given by

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (99)
     @xmath   @xmath   @xmath      (100)
  -- -------- -------- -------- -- -------

Since @xmath are co-prime, this forces @xmath to be a multiple of @xmath
.

###### Theorem 3.1.

For @xmath odd, @xmath must be a multiple of @xmath .

##### 3.2.2 Connecting the Graph to the Code

Each node in the graph @xmath is associated to a row of the p-c matrix
@xmath of the code and hence to a p-c. The one exception is the
fictitious node @xmath which does not correspond to a p-c. When the p-c
matrix @xmath is over @xmath , the top row of @xmath is the binary sum
of all other rows of @xmath . Hence in case when @xmath is over @xmath ,
@xmath also represent a p-c matrix of the code and the node @xmath is
also associated to a p-c. Also, each edge in the graph @xmath is
associated to a unique code symbol as the edges are associated with
columns of the matrix @xmath . The structure of @xmath is mostly
determined once we specify @xmath as @xmath with edges in @xmath removed
is just a tree with @xmath as the root node. Hence the only freedom lies
in selecting the edges that make up the bipartite graph @xmath .

### 4 Girth Requirement

###### Theorem 4.1.

For both @xmath even and @xmath odd,

1.   The code associated to graph @xmath with each node representing a
    p-c over @xmath can recover sequentially from @xmath erasures iff
    @xmath has girth @xmath .

2.   The code associated to graph @xmath with each node representing a
    p-c over @xmath with @xmath , can recover sequentially from @xmath
    erasures if @xmath has girth @xmath .

###### Proof.

Let us assume that there is an erasure pattern involving @xmath erased
code symbols and that it is not possible to recover from this erasure
pattern sequentially. These @xmath erasures correspond to @xmath
distinct edges @xmath of the graph @xmath . Let @xmath and let us
restrict our attention to the subgraph @xmath of @xmath with edge set of
@xmath exactly the set @xmath with vertex set of @xmath exactly the set
@xmath of nodes that the edges in @xmath are incident upon. We note that
in graph @xmath every node in @xmath must have degree @xmath because in
@xmath a node in @xmath of degree one would imply that the code word in
the row of p-c matrix corresponding to the degree one node can be used
to recover the erased code symbol corresponding to the edge incident on
it in @xmath .

1.  First let us assume that @xmath . We start with edge @xmath , this
    must be linked to a p-c node @xmath which is linked to a second
    erased symbol @xmath and so on, as degree of each node in @xmath is
    @xmath . In this way, we can create a path in @xmath with distinct
    edges. But since there are only a finite number of nodes, this must
    eventually force us to revisit a previous node, thereby establishing
    that the graph @xmath has girth @xmath .

2.  Next, for the case when @xmath is a node in @xmath , we start at an
    erased edge incident upon node @xmath and move to the node at the
    other end of the edge. Since that node has degree @xmath , there
    must be an edge corresponding to a second erased symbol that it is
    connected to the node and so on. Again the finiteness of the graph
    will force us to revisit either @xmath or else, a previously-visited
    node proving once again that an unrecoverable erasure pattern
    indicates a cycle and hence the graph @xmath has girth @xmath .

We have thus established that having a girth @xmath will guarantee
recovery from @xmath erasures. For @xmath , it is easy to see that a
girth of @xmath is necessary since if the girth is @xmath , then the set
of erasures lying on a cycle of length @xmath is uncorrectable
regardless of whether or not the nodes associated with this cycle
includes @xmath . ∎

###### Theorem 4.2.

For the graph @xmath to have girth @xmath , the degree @xmath of @xmath
or equivalently, the number @xmath of nodes in @xmath , has the lower
bound, @xmath .

###### Proof.

Case @xmath odd: As shown in Theorem 3.1 , @xmath must be in fact be a
multiple of @xmath . Case @xmath even: Let the set containing all those
nodes of @xmath which are at distance atmost @xmath from a particular
node @xmath be denoted by @xmath where distance between vertex @xmath is
measured by the number of edges in the shortest path between @xmath .
Note that @xmath , @xmath . Since every node in @xmath has a path of
length (measured in terms of number of edges) @xmath edges leading to
@xmath and hence these paths does not involve any edge from @xmath , no
two nodes in @xmath can be connected by an edge in @xmath in @xmath .
For, if two nodes in @xmath were connected by an edge in @xmath then
there would be a cycle of length at most @xmath present as @xmath .
Further, a node in @xmath cannot connect to two nodes in @xmath via
edges in @xmath , @xmath , for the same reason. It would once again
imply the presence of a cycle of length @xmath . Hence each node in
@xmath must connects to @xmath nodes with @xmath th node belonging to
@xmath , @xmath respectively for some set of @xmath distinct nodes
@xmath . It follows that there must be at least @xmath distinct nodes in
@xmath . Thus @xmath . ∎

### 5 Code Construction by Meeting Girth Requirement

As noted in Section 4 , to complete the construction of rate-optimal
code over @xmath , we need to ensure that the graph @xmath has girth
@xmath for both @xmath even and @xmath odd. Since the code is over
@xmath , the node-edge incidence matrix of the graph @xmath with girth
@xmath will directly give the required p-c matrix for a rate-optimal
code. Hence in the following we focus only in designing @xmath with
girth @xmath . The steps followed are outlined below.

#### 5.1 Step @xmath : Construction and Coloring of the Base Graph

1.  We begin by deleting the edges in the graph @xmath connecting @xmath
    to the nodes in @xmath . One is then left with a graph where the
    nodes in @xmath have degree @xmath and all the remaining nodes have
    degree @xmath . In particular, every node has degree @xmath . We
    shall call the resultant graph the base graph @xmath . Note that if
    @xmath has girth @xmath , the construction ends here but we do not
    need @xmath to have girth @xmath , as we will modify @xmath to
    construct another graph @xmath having same incidence matrix as ( 96
    ) for @xmath even, ( 98 ) for @xmath odd which will have girth
    @xmath .

2.  By Vizing’s theorem [ 95 ] , the edges of @xmath can be colored
    using @xmath colors in such a way that each edge is colored with
    some color from the set of @xmath colors and for every @xmath , all
    the edges incident on @xmath are of a different colors. However it
    is possible to color the edges of @xmath using @xmath colors:

    -    Case @xmath even: In this case, by selecting @xmath for @xmath
        , i.e., @xmath , and through careful selection of the edges
        connecting nodes in @xmath , one can ensure that the base graph
        can be colored using @xmath colors (See Appendix Erasure Codes
        for Distributed Storage: Tight Bounds and Matching Constructions
        ); thus the base graph will in this case have

          -- -------- -------- -------- -- -------
             @xmath   @xmath   @xmath      (101)
          -- -------- -------- -------- -- -------

        nodes. For @xmath , @xmath , we can choose @xmath to be a
        complete graph of degree @xmath and the complete construction of
        rate-optimal code ends here as the graph @xmath has girth @xmath
        . For @xmath , @xmath , we can construct @xmath with @xmath with
        girth @xmath and the complete construction of rate-optimal code
        ends here. We skip the description for @xmath case here.

    -    Case @xmath odd: In this case, by selecting @xmath and through
        careful selection of the edges connecting nodes in @xmath with
        nodes in @xmath , one can ensure that the base graph can be
        colored using @xmath colors (See Appendix Erasure Codes for
        Distributed Storage: Tight Bounds and Matching Constructions );
        thus the base graph will have in this case, a total of

          -- -------- -------- -------- --
             @xmath   @xmath   @xmath   
          -- -------- -------- -------- --

        nodes.

    The coloring is illustrated in Fig. 22 for the case @xmath and
    @xmath .

In summary, the base graph is graph @xmath with node @xmath removed and
has all nodes of degree @xmath and can be colored using @xmath colors;
these are the only properties of the base graph that are carried forward
to the next steps of the construction; we will number the colors @xmath
through @xmath and speak of color @xmath as the @xmath th color. The
steps that follow, are the same for both @xmath even and @xmath odd.

#### 5.2 Step @xmath : Construction and Coloring of the Auxiliary Graph

Next, we begin by identifying a second graph which we shall call the
auxiliary graph @xmath . The following properties are required of the
auxiliary graph @xmath :

1.  @xmath has an even number @xmath of nodes and every node in @xmath
    has degree @xmath ,

2.  @xmath is of girth @xmath ,

3.  @xmath should permit a coloring of a subset of the edges of @xmath
    using the same @xmath colors used to color the base graph in such a
    way that

    -   Every vertex of @xmath contains an edge incident upon it of
        color @xmath , for any @xmath such that @xmath ;

    -   For @xmath , if @xmath denotes the subgraph of @xmath , with
        edge set of @xmath exactly equal to the set of edges in @xmath
        of color @xmath , with @xmath , then @xmath is a bipartite graph
        corresponding to a perfect matching of @xmath i.e., there are
        @xmath nodes on either side of the bipartite graph and every
        vertex of @xmath is of degree @xmath .

    This is illustrated in Fig. 23 .

It follows from Hall’s Theorem (1935) [ 96 ] that an @xmath -regular
bipartite graph @xmath of girth @xmath can be colored with @xmath colors
in such a way that @xmath satisfies the conditions necessary for an
auxiliary graph @xmath . It can also be shown that an @xmath -regular
graph with girth @xmath with @xmath nodes can be converted to an @xmath
-regular bipartite graph with girth @xmath with @xmath nodes. We skip
this description. Since @xmath -regular graph with girth @xmath can be
constructed with small number of nodes (close to smallest possible) [ 97
, 98 , 99 , 100 , 101 ] , we can construct the auxiliary graph @xmath
with necessary conditions with small number of nodes.

#### 5.3 Step @xmath : Using the Auxiliary Graph to Expand the Base
Graph

In this step, we use the graph @xmath to expand the graph @xmath ,
creating in the process, a new graph @xmath as follows:

1.  We start with @xmath and replace each vertex @xmath with @xmath
    vertices @xmath , each corresponding to a vertex in @xmath i.e.,
    @xmath . The resultant graph will be termed as the expanded graph
    @xmath . We will now define the edge set of @xmath . The edge set of
    @xmath is defined in such a way that every edge in @xmath is colored
    with some color @xmath .

2.  For every @xmath and for every @xmath : We add the edges @xmath in
    @xmath with both edges of color @xmath iff

    1.  in @xmath , vertex @xmath and vertex @xmath were connected by an
        edge of color @xmath and

    2.  @xmath i.e., the nodes @xmath and @xmath are connected by an
        edge of color @xmath in @xmath .

###### Theorem 5.1.

@xmath has girth @xmath .

###### Proof.

This follows simply because corresponding to every path traversed
through @xmath with successive edges in the path with some sequence of
colors, there is a corresponding path in @xmath with successive edges in
the path corresponding to the same sequence of colors. The edge coloring
of the base graph ensures that we never retrace our steps in the
auxiliary graph i.e., any two successive edges in the path in auxiliary
graph are not the same. It follows that since @xmath has girth @xmath ,
the same must hold for @xmath . ∎

Let @xmath . We add a node @xmath to @xmath and connect it to each node
in @xmath through an edge. Call the resulting graph @xmath . It is clear
from the construction that the graph @xmath is graph @xmath with girth
@xmath but now with @xmath replacing @xmath in the description of @xmath
because:

1.  @xmath is @xmath with node @xmath removed,

2.  @xmath is just an expanded version of @xmath i.e, a node @xmath is
    expanded into the set of nodes @xmath ,

3.  Edges are defined in @xmath preserving the tree-like structure of
    @xmath as the sub graph of @xmath induced by @xmath is isomorphic to
    @xmath with all nodes having degree one for very edge @xmath in
    @xmath of color @xmath .

Hence @xmath has the same node-edge incidence matrix as ( 96 ) over
@xmath for @xmath even, ( 98 ) over @xmath for @xmath odd with @xmath
replacing @xmath and also has girth @xmath . This node-edge incidence
matrix gives the required p-c matrix for a rate-optimal code over @xmath
with parameters @xmath . Hence we have constructed the required p-c
matrix for a rate-optimal code over @xmath for both @xmath even and
@xmath odd, for any @xmath . Hence we conclude our construction of
rate-optimal code for both @xmath even and @xmath odd.

### 6 S-LR Codes that are Optimal with Respect to Both Rate and Block
Length

In this section, we begin by presenting a construction for S-LR codes
given in [ 36 ] . We improve upon the lower bound for the rate of this
construction provided in [ 36 ] and show that for a given @xmath , there
is a unique block length for which this improved lower bound on rate is
equal to the right hand side of the upper bound on rate derived in
Chapter Erasure Codes for Distributed Storage: Tight Bounds and Matching
Constructions in Theorem 15.1 . For a given @xmath , codes based on this
construction with the unique block length for which the improved lower
bound is equal to the maximum possible rate turn out to correspond to
codes based on a graph known as Moore graph with degree @xmath and girth
@xmath . Unfortunately, for @xmath (see [ 102 ] ), Moore graphs are
known to exist only when @xmath .

###### Construction 6.1.

( [ 36 ] ) Consider an @xmath -regular bipartite graph @xmath having
girth @xmath . Let the number of nodes in @xmath be @xmath . Let @xmath
be the @xmath node-edge incidence matrix of the graph @xmath with each
row representing a distinct node and each column representing a distinct
edge. The code @xmath with p-c matrix @xmath thus defined is an S-LR
code with parameters @xmath over @xmath . The constrution takes @xmath
as input and constructs code @xmath as output.

###### Proof.

(sketch of proof) Let @xmath be the code obtained as the output of the
construction 6.1 with an @xmath -regular bipartite graph @xmath with
girth @xmath as input. The code @xmath is an S-LR code with parameters
@xmath , simply because a set of erased symbols with least cardinality
which cannot be recovered through sequential recovery must correspond to
a set of linearly dependent columns in @xmath with least cardinality and
hence corresponds to a set of edges forming a cycle in @xmath . Since
@xmath has girth @xmath , the number of edges in this cycle must be
@xmath and hence the number of erased symbols is @xmath . The code
parameters follow from a simple calculation. ∎

The graph @xmath described in Construction 6.1 need not have the
tree-like structure of @xmath .  Let @xmath be the code obtained as the
output of the construction 6.1 with an @xmath -regular bipartite graph
@xmath with girth @xmath as input. Since the graph @xmath need not have
the tree-like structure of @xmath , it may be impossible for the code
@xmath to have a p-c matrix similar to ( 96 ) for @xmath even, ( 98 )
for @xmath odd. We will now see that it is possible to write a p-c
matrix for @xmath similar to ( 96 ) for @xmath even, ( 98 ) for @xmath
odd iff @xmath is a Moore graph. It follows from Construction 6.1 , as
was observed in [ 36 ] , that the rate of the code @xmath is @xmath .
But we will shortly provide a precise value for the rate of this code.

###### Definition 5.

( Connected Component ) Let @xmath be a graph. Then a connected
component of @xmath is a subgraph @xmath such that @xmath is connected
as a graph and moreover, there is no edge in @xmath , connecting a
vertex in @xmath to a vertex in @xmath .

Of course if @xmath is a connected graph then there is just a single
connected component, namely the graph @xmath itself.

###### Theorem 6.2.

Let @xmath be an @xmath -regular bipartite graph with girth @xmath . Let
the graph @xmath be connected with exactly @xmath nodes. The code @xmath
obtained as the output of the construction 6.1 with the graph @xmath as
input is an S-LR code with parameters @xmath over @xmath and hence
having rate given by:

  -- -------- -- -------
     @xmath      (102)
  -- -------- -- -------

###### Proof.

Let @xmath be the node-edge incidence matrix of the graph @xmath . From
the description of Construction 6.1 , the matrix @xmath is a p-c matrix
of the code @xmath . The p-c matrix @xmath , has each row of Hamming
weight @xmath and each column of weight @xmath . It follows that the sum
of all the rows of @xmath is the all-zero vector. Thus the rank of
@xmath is @xmath . Next, let @xmath be the smallest integer such that a
set of @xmath rows of @xmath add up to the all-zero vector. Let @xmath
be the set of nodes in @xmath corresponding to a set of @xmath rows
@xmath in @xmath such that @xmath . We note that any edge @xmath in
@xmath with @xmath will be such that @xmath and similarly if @xmath then
@xmath . Let @xmath , it follows that the subgraph of @xmath with vertex
set equal to @xmath and the edge set equal to the edges associated to
columns of @xmath indexed by @xmath form a connected component of the
graph @xmath . But since @xmath is connected, @xmath and hence @xmath .
It follows that any set of @xmath rows of @xmath is linearly
independent. Hence the rank of @xmath equals @xmath . The parameters of
@xmath are thus given by:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

∎

We note here that while the Construction 6.1 made use of regular
bipartite graphs, the bipartite requirement is not a requirement as in
the argument above, we only used the fact that the graph @xmath is
regular. We collect together the above observations concerning rate and
sufficiency of the regular-graph requirement into a (slightly) modified
construction.

###### Construction 6.3.

(modified version of the construction in [ 36 ] ) Let @xmath be a
connected, regular graph of degree @xmath and of girth @xmath having
exactly @xmath vertices. Let @xmath be the @xmath node-edge incidence
matrix of the graph @xmath with each row representing a distinct node
and each column representing a distinct edge. The code @xmath with p-c
matrix @xmath is an S-LR code having parameters @xmath over @xmath . The
constrution takes @xmath as input and constructs code @xmath as output.

For the rest of this section: Let @xmath be arbitrary positive integers.
Let @xmath be a connected, regular graph of degree @xmath and of girth
@xmath having exactly @xmath vertices. Let @xmath be the S-LR code
having parameters @xmath over @xmath obtained as the output of the
construction 6.3 with the graph @xmath as input. Clearly, the rate of
the code @xmath is maximized by minimizing the block length @xmath of
the code, or equivalently, by minimizing the number of vertices @xmath
in @xmath . Thus there is interest in regular graphs of degree @xmath ,
having girth @xmath with the least possible number of vertices. This
leads us to the Moore bound and Moore graphs.

###### Theorem 6.4.

(Moore Bound) ( [ 102 ] ) The number of vertices @xmath in a regular
graph of degree @xmath and girth @xmath satisfies the lower bound :

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

###### Definition 6.

(Moore graphs) A regular graph with degree @xmath with girth atleast
@xmath with number of vertices @xmath satisfying @xmath is called a
Moore graph.

###### Lemma 6.5.

The rate @xmath of the code @xmath with block length @xmath satisfies:

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (103)
  -- -------- -------- -------- -- -------

The inequality ( 103 ) will become equality iff @xmath is a Moore graph.

###### Proof.

The Lemma follows from Theorem 6.2 and the Moore bound given in Theorem
( 6.4 ). ∎

It turns out interestingly, that the upper bound on rate of the code
@xmath given by the expression @xmath (Lemma 6.5 ) is numerically,
precisely equal to the right hand side of inequality ( 29 ) (for @xmath
even), ( 30 ) (for @xmath odd). Note that the inequality ( 29 ) (for
@xmath even), ( 30 ) (for @xmath odd) directly gives an upper bound on
rate of an S-LR code. As a result, we have the following Corollary.

###### Corollary 6.6.

The S-LR code @xmath is a rate-optimal code iff @xmath is a Moore graph.

###### Corollary 6.7.

Let @xmath be such that a Moore graph exists. If @xmath is the Moore
graph then the code @xmath is not only rate optimal, it also has the
smallest block length possible for a binary rate-optimal code for the
given parameters @xmath .

###### Proof.

From the discussions in Section 3 , a rate-optimal code must have the
graphical representation @xmath . From Theorem 4.1 , @xmath must have
girth @xmath , if the rate-optimal code is over @xmath . Hence from
Theorem 4.2 , the number @xmath of vertices in @xmath satisfies the
lower bound

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (104)
  -- -------- -------- -------- -- -------

Since the value of @xmath through numerical computation gives the number
of edges in @xmath , it can be seen that @xmath with girth @xmath and
@xmath leads to a binary rate-optimal code having block length equal to
@xmath . The code @xmath also has block length equal to @xmath when
@xmath is a Moore graph. Hence the corollary follows from ( 104 ) and
noting that the number of edges in the graph @xmath grows strictly
monotonically with @xmath and hence @xmath correspond to least possible
block length. ∎

In the following we fix @xmath . An example Moore graph with @xmath ,
known as the Hoffman-Singleton graph (with the removal of a single
vertex corresponding to eliminating an unneeded, linearly dependent
parity check) is shown in the Fig. 24 . If @xmath is this Moore graph
then the code @xmath is a rate-optimal code with least possible
block-length with @xmath .

## Chapter \thechapter Coloring @xmath with @xmath colours

In this section of the appendix, we show how to construct the base graph
@xmath in such a way that its edges can be colored using @xmath colors.
In our construction, for @xmath odd, we set @xmath which is the smallest
possible value fo @xmath by Theorem 3.1 . For the case of @xmath even,
we set @xmath . We begin with a key ingredient that we make use of in
the construction, namely, that the edges of a regular bipartite graph of
degree @xmath can be colored with @xmath colors.

###### Theorem .8.

The edges of a regular bipartite graph @xmath of degree @xmath can be
coloured with exactly @xmath colours such that each edge is associated
with a colour and adjacent edges of the graph does not have the same
colour.

###### Proof.

By Hall’s Theorem (1935) [ 96 ] , there exists a perfect matching in the
@xmath -regular bipartite graph @xmath . We first identify this perfect
matching and then remove the edges corresponding to this matching from
@xmath to form a @xmath -regular bipartite graph to which we can once
again apply Hall’s Theorem and so on until we have partitioned the edges
of the bipartite graph @xmath into the disjoint union of @xmath perfect
matchings. To conclude the proof, we simply choose @xmath different
colors say @xmath and color each edge in @xmath th perfect matching with
color @xmath , @xmath . ∎

## Appendix A The Construction of @xmath for @xmath Odd

The aim here is to show that we can construct the base graph @xmath and
color the edges of it using exactly @xmath colors such that adjacent
edges does not have the same color. In the case of @xmath odd, we set
@xmath . From Remark 6 , it follows that if we set @xmath , then

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

and hence from ( 99 ), ( 100 ), it is possible to connect nodes in
@xmath to nodes in @xmath so that @xmath is a bipartite graph with the
two sets of nodes in @xmath being equal to @xmath respectively where
each node in @xmath is of degree @xmath and each node in @xmath is of
degree @xmath . Since the graph @xmath is completely specified once
@xmath and @xmath are specified as mentioned in Section 3.2.2 , @xmath
can be constructed with @xmath . Let us recall that if we add a node
@xmath to @xmath and connect @xmath to all the nodes in @xmath , we will
recover the graph @xmath . It is easily seen that @xmath is an @xmath
-regular graph. By grouping together nodes in alternate layers in @xmath
i.e., by letting @xmath and @xmath , it can be verified that @xmath is
in fact, an @xmath -regular bipartite graph with node-set @xmath on the
left and node-set @xmath to the right. Hence by Theorem .8 , the edges
of the graph @xmath and hence the edges of the graph @xmath can be
colored with @xmath colors.

## Appendix B The Construction of @xmath for @xmath Even

Let @xmath be the tree (a subgraph of @xmath ) with root node @xmath
formed by all the paths of length at most @xmath starting from @xmath
i.e., @xmath is the subgraph of @xmath induced by the vertices which are
at distance atmost @xmath from @xmath where we remove in this induced
subgraph all the edges which are in @xmath . Let @xmath be the vertex
set of @xmath . The nodes @xmath are at depth @xmath in @xmath with the
root node @xmath at depth 0. We now color the edges of the tree @xmath
with the @xmath colors @xmath . It is clear that such a coloring of
edges of @xmath can be done. There are @xmath edges @xmath incident on
@xmath . Let the color of @xmath be @xmath , @xmath . Hence there is no
edge of color @xmath incident on @xmath .

1.  Let @xmath be one of the @xmath colors, @xmath . Let @xmath be the
    largest subset of @xmath (nodes at depth @xmath ) where each node in
    @xmath is connected by an edge of color @xmath to a distinct node at
    depth @xmath , i.e., a node in @xmath . Let @xmath . Let @xmath be
    the largest subset of @xmath (nodes at depth @xmath ) where each
    node in @xmath is connected by an edge of color @xmath to a distinct
    node in @xmath (nodes at depth @xmath ). Let @xmath . It is clear
    that @xmath . We set @xmath . It can be verified that:

      -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
      -- -------- -------- -------- --

    Hence @xmath . It follows that

      -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
      -- -------- -------- -------- --

    Since the set @xmath depends on @xmath , from now on we denote it as
    @xmath . Since the set @xmath depends on @xmath , from now on we
    denote it as @xmath .

2.  Next, set @xmath . Let @xmath . Hence there are @xmath such trees
    @xmath . Color the edges of the tree @xmath with @xmath colors as
    before such that there is no edge of color @xmath incident on @xmath
    , @xmath . An edge in @xmath which is incident on any node in @xmath
    cannot be colored with color @xmath but every color from @xmath can
    be used to color it, @xmath , @xmath . Similarly an edge in @xmath
    which is incident on any node in @xmath cannot be colored with color
    @xmath , @xmath . We can connect the set of nodes @xmath of size
    @xmath (or the set of nodes @xmath nodes of size @xmath ) to form
    bipartite graph of degree @xmath . This is possible because we can
    first construct a regular graph of degree @xmath with @xmath or
    @xmath nodes since @xmath , and then create two copies of the vertex
    set of this regular graph to create the left and right nodes of a
    bipartite graph @xmath . The edges of the bipartite graph can then
    be formed by connecting vertices in accordance with the regular
    graph, i.e., if nodes @xmath and @xmath were connected in the
    regular graph, then node @xmath on the left is connected to node
    @xmath on the right in the bipartite graph and vice versa. Hence a
    bipartite graph @xmath of degree @xmath with @xmath or @xmath nodes
    is constructed. Now connect the @xmath nodes in @xmath in accordance
    with this bipartite graph @xmath which also has @xmath nodes and
    color the edges by colors from @xmath , @xmath . This is possible by
    Theorem .8 . Similarly connect the @xmath nodes in @xmath in
    accordance with this bipartite graph @xmath which also has @xmath
    nodes and color the edges of this graph by colors from @xmath . This
    is again possible by Theorem .8 . The edges of these @xmath
    bipartite graphs form the edge set @xmath . The construction is
    complete as we have constructed @xmath and connected the nodes in
    @xmath according to an edge set @xmath and colored the edges of
    @xmath using @xmath colors.

## Chapter \thechapter Codes with Availability

Codes with availability are codes which are such that a desired code
symbol that is unavailable, can be reconstructed in @xmath different
ways from the remaining code symbols, i.e., reconstructed by employing
@xmath disjoint recovery sets, each recovery set being of size @xmath .
The desired symbol here will belong to either a collection of code
symbols that form an information set, or else, the entire set of code
symbols. The corresponding codes will be termed as information-symbol
availability or all-symbol availability codes respectively. The
motivation here is that the unavailable symbol could in practice
represent ‘hot’ data, i.e., data such as a newly-released music file,
that is in high demand. This requirement can be phrased in terms of the
parity check (p-c) matrix of the code. Corresponding to any desired
symbol @xmath , the rowspace of the p-c matrix will contain @xmath rows
whose support is of the form @xmath , @xmath , @xmath . Important
problems on this topic are determination of the rate @xmath and minimum
distance @xmath of such a code for given @xmath , as well as providing
constructions of codes that are efficient with respect to these two
metrics. It turns out that some of the best-performing codes satisfy a
more stringent criterion, which we term here as strict availability . A
strict availability code is an availability code possessing p-c matrix
@xmath with a sub matrix @xmath formed by a subset of rows of @xmath
with @xmath such that each row has Hamming weight equal to @xmath and
each column has Hamming weight @xmath . The rate and minimum distance of
codes with strict availability are also explored. Codes with
availability are similar to codes with single-step, majority-logic
decoding. The only difference is that in the case of an availability
code, each recover set is restricted to have weight @xmath whereas under
majority logic decoding, no such constraint is present.

###### Organization of the Chapter

The chapter is organized as follows. We begin by formally defining codes
with availability and strict availability (SA) in Section 3 . Section 4
examines constructions for availability codes in the literature.
Sections 5 and 6 examine alphabet-size dependent and alphabet-size
independent bounds respectively. Section 7 examines optimal tradeoff
between rate and fractional minimum distance for a special class of
availability codes. A lower bound on block length of codes with strict
availability is given in Section 8 . The final section, Section 9
presents a summary.

###### Contributions

Contributions of the thesis on the topic of availability codes include:

1.  Improved field-size dependent upper bounds (Section 5.2 ) on the
    minimum distance @xmath (for given @xmath ) and dimension (for given
    @xmath ) of an availability code.

2.  Improved field-size independent upper bound (Section 6.2.2 ) on the
    minimum distance of an availability code and improved upper bound on
    rate (Section 6.1.2 ) of an SA code.

3.  Exact expression for maximum possible fractional minimum distance
    (Section 7 ) for a given rate for a special class of availability
    codes as @xmath where each code in this special class is a subcode
    or subspace of direct product of @xmath copies of an availability
    code with parameters @xmath for some @xmath .

4.  A lower bound on block length of an SA code (Section 8 ) and showed
    that SA codes achieving the lower bound on block length exists iff a
    Balanced Incomplete Block Design (BIBD) with certain parameters
    exist. We then present two SA codes based on two well-known BIBDs.
    The codes based on the two BIBDs have maximum possible rate (for the
    minimum possible block length) assuming the validity of a well-known
    conjecture in the literature of BIBDs.

### 3 Definitions

In this chapter, we will restrict ourselves to linear codes. Throughout
the chapter, a codeword in an @xmath linear code will be represented by
@xmath where @xmath denotes the @xmath th code symbol. Throughout this
chapter, we will use the term weight to refer to Hamming Weight.

###### Definition 7.

An @xmath all-symbol (AS) availability code over a finite field @xmath ,
is an @xmath linear code @xmath over @xmath having the property that in
the event of a single but arbitrary erased code symbol @xmath , (i)
there exist @xmath recovery sets @xmath which are pair-wise disjoint and
of size @xmath with @xmath such that (ii) for each @xmath , @xmath can
be expressed in the form:

  -- -------- -- -------
     @xmath      (105)
  -- -------- -- -------

If in the definition above, instead of @xmath being an arbitrary erased
code symbol, we were to restrict @xmath to be an erased code symbol
belonging to an information set @xmath with @xmath of the code, then the
code @xmath would be called an information symbol (IS) availability
code. This chapter and the literature on the subject of availability
codes is concerned for the most part, with AS availability codes. For
this reason throughout the remainder of this chapter, when we use the
term an availability code, we will mean an AS-availability code. The
parameter @xmath is termed as the locality parameter. For an
availability code, let the set of codewords in the dual code associated
with the recovery sets @xmath , @xmath form the rows of a matrix @xmath
. The parity check matrix of an availability code can be written in the
form @xmath where the rows of @xmath are all the distinct codewords in
the rows of the matrix @xmath and where the matrix @xmath is a full rank
matrix containing the remaining codewords in the dual code such that
@xmath . Hence @xmath does not necessarily have linearly independent
rows unless otherwise specified. Clearly the Hamming weight of each row
of @xmath is @xmath and each column of @xmath , has weight @xmath .

###### Definition 8.

Codes with Strict Availability (SA codes) are simply the subclass of
availability codes with parity check matrix @xmath as described above
such that each row of @xmath has weight equal to @xmath and each column
of @xmath has weight equal to @xmath . Thus the number @xmath of rows of
@xmath must satisfy @xmath . Further, it is clear that if the support
sets of the rows in @xmath having a non-zero entry in the @xmath column
are given respectively by @xmath , then we must have by the disjointness
of the recovery sets, that @xmath . Each code symbol @xmath in an SA
code is thus protected by a collection of @xmath ‘orthogonal’ parity
checks, each of weight @xmath . An SA code with parameters @xmath will
also be called as an @xmath SA code.

### 4 Constructions of Availability Codes

In this section, we provide an overview of some efficient constructions
of availability codes to be found in the literature.

###### The Product Code

Consider the @xmath product code in @xmath dimensions. Clearly this is
an @xmath availability code, having rate @xmath .

###### The Wang et al. Construction

For any given parameter pair @xmath , Wang et al. [ 34 ] provide a
construction for an @xmath availability code which is defined through
its parity-check matrix @xmath . Let @xmath be a set of @xmath elements.
Then in the construction, rows of @xmath are indexed by distinct subsets
of @xmath of cardinality @xmath and columns are indexed by distinct
subsets of @xmath of cardinality @xmath . We set @xmath if the @xmath
-th subset of @xmath of cardinality @xmath belongs to the @xmath -th
subset of @xmath of cardinality @xmath and zero otherwise. Thus @xmath
is of size @xmath . It is easy to verify that each row of @xmath has
constant row weight @xmath and each column of @xmath has constant weight
@xmath . It turns out that the rank of @xmath is given by @xmath and
that @xmath defines an @xmath availability code, having parameters:
@xmath and rate @xmath . Thus this code provides improved rate in
comparison with the product code. Since @xmath , the code has smaller
block length as well.

###### Direct-Sum Construction

It is shown in [ 50 ] that the direct sum of @xmath copies of the @xmath
Simplex code yields an SA code with parameters @xmath having maximum
possible rate for a binary SA code for @xmath .

###### Availability Codes Derived from Steiner Triple Systems and Cyclic
Availability Codes

We will see that the incidence matrix of Steiner Triple System (STS)
yields parity check matrix for an availability code for @xmath and
specific @xmath of the form, @xmath . These results were obtained by [
44 ] in parallel with work in [ 29 ] carried out as part of the present
thesis, dealing with code construction based on STS and described later
in this chapter, see Section 8 . It was also shown in [ 44 ] that one
can puncture the incidence matrix of an STS and derive availability
codes for multiple, different values of @xmath for @xmath . Also present
in [ 44 ] is a construction of a cyclic availability code having
parameters: @xmath with @xmath and @xmath over @xmath where @xmath for
some prime number @xmath and positive number @xmath .

###### Availability Codes Based on Majority Logic Decodable Codes

In [ 43 ] , authors point out constructions for availability codes based
on majority logic decodable codes. One of these codes based on
projective geometry is presented in this chapter in Section 8 where we
present this construction as an example of a minimum block length
construction of SA codes based on BIBD.

###### Anticode based Construction

In [ 45 ] , the authors present constructions which are optimal w.r.t
the alphabet-dependent bounds given in ( 106 ) and the Griesmer bound
for linear codes given in Theorem 1 of [ 103 ] for @xmath and large
@xmath . The authors take the Simplex code and generate various codes
from it by puncturing the generator matrix of the code at positions
corresponding to the columns of the generator matrix of an anticode [
104 ] . By selecting multiple anticodes, they generate various
availability codes from the Simplex code. They consider both the binary
and the @xmath -ary Simplex code. They show that their constructions for
some parameters are optimal either w.r.t the alphabet-dependent bounds
given in ( 106 ) or else, the Griesmer bound of coding theory.

###### Constructions of Availability Codes having Large Minimum Distance

By an availability code with large minimum distance we mean here an
availability code where the minimum distance is a fraction of the block
length. Constructions with minimum distance @xmath for @xmath appear in
[ 46 ] . In [ 46 ] , the authors also provide a general method of
constructing codes with parameters @xmath with explicit constructions
provided for @xmath . A special case of this general construction given
in [ 46 ] , was provided in [ 49 ] having parameters @xmath which meets
the bound ( 129 ) given in [ 4 ] . Note that this construction is of low
rate. For other constructions where the minimum distance being a
fraction of block length, please refer to [ 4 , 47 , 48 ] .

### 5 Alphabet-Size Dependent Bounds on Minimum Distance and Dimension

The bounds that are discussed in this section, are bounds that are
alphabet-size dependent. For brevity, throughout this section, we will
just say upper bound on minimum distance when we mean a alphabet-size
dependent bound on minimum distance etc. We begin with a survey of
existing bounds on the minimum distance and dimension of an availability
code. All the existing results follow an approach based on shortening.
We next present a new improved upper bound on the minimum distance and
dimension of an availability code. This new bound makes full use of the
shortening-based approach and results in the tightest-known upper bounds
on the minimum distance and dimension of an availability code.

#### 5.1 Known Bounds on Minimum Distance and Dimension

In this section, we present a survey of upper bounds on minimum distance
and dimension of an availability code existing in the literature. All
the bounds presented here are based on code-shortening. Under this
approach, the shortening is carried out in such a way that the necessary
constraints on an LR code have either disappeared or are present in
weaker form. Bounds on classical codes are then applied to this
shortened code, which can be translated into bounds on the parent,
availability code. The theorem below which appeared in [ 71 ] provides
an upper bound on the dimension of an availability code with @xmath that
is valid even for nonlinear codes. The ‘dimension’ of a nonlinear code
@xmath over an alphabet @xmath of size @xmath is defined to be the
quantity @xmath .

1.  Let @xmath be the maximum possible minimum distance of a classical
    @xmath block code (not necessarily an LR code) over @xmath ,

2.  Let @xmath be the largest possible dimension of a code (not
    necessarily an LR code) over @xmath having block length @xmath and
    minimum distance @xmath .

###### Theorem 5.1.

[ 71 ] For any @xmath code @xmath that is an availability code with
@xmath and with locality parameter @xmath over an alphabet @xmath of
size @xmath ,

  -- -------- -- -------
     @xmath      (106)
  -- -------- -- -------

###### Proof.

(Sketch of proof) The bound holds for linear as well as nonlinear codes.
In the linear case, with @xmath , the derivation proceeds as follows.
Let @xmath be a @xmath generator matrix of the availability code @xmath
. Then it can be shown that for any integer @xmath , there exists an
index set @xmath such that @xmath and @xmath where @xmath refers to the
set of columns of @xmath indexed by @xmath . This implies that @xmath
has a generator matrix of the form (after permutation of columns):

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

In turn, this implies that the rowspace of @xmath defines an @xmath code
over @xmath , if @xmath . It follows that @xmath and the result follows.
Note that the row space of @xmath corresponds to a shortening @xmath of
@xmath with respect to the coordinates @xmath . The proof in the general
case is a (nontrivial) extension to the nonlinear setting. ∎

###### Key Ingredient of the Shortening-Based Bounds and Generalizations

Note that a key ingredient in the above proof is showing that there
exists a set of @xmath code symbols in the code, corresponding to which
there exist @xmath linearly independent codewords of weight @xmath in
the dual code, each of whose support is wholly contained in the support
of these @xmath code symbols. The same ingredient is also present in the
bounds derived in [ 43 , 48 ] which we will next present. As a
contribution of the present thesis on this topic, we will generalize
this idea in Section 5.2 and provide the best-known estimate of the
number of linearly independent codewords of weight @xmath in the dual
code, each having support contained in the support of a carefully chosen
set of (say) @xmath code symbols. This estimate will yield the
tightest-known bound todate. In [ 43 ] , the authors provide an upper
bound on the minimum distance and dimension of a @xmath code with IS
availability (the bound thus also applies to AS availability codes as
well) over an alphabet of size @xmath :

###### Theorem 5.2.

[ 43 ] For any @xmath code @xmath that is an information-symbol
availability code with parameters @xmath over @xmath :

  -- -------- -- -------
     @xmath      (107)
     @xmath      (108)
  -- -------- -- -------

where

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

The above bound is derived by finding a set of @xmath code symbols such
that there are at least @xmath linearly independent codewords in the
dual each of whose support is completely contained in the support of
these @xmath code symbols. This is the key ingredient, we had alluded to
above. In [ 105 ] , as part of the contribution of the thesis on this
topic, we provide the bound below on the minimum distance and dimension
of a @xmath availability code over an alphabet of size @xmath :

###### Theorem 5.3.

[ 105 ] For any @xmath availability code @xmath over @xmath with minimum
distance @xmath :

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (109)
     @xmath   @xmath   @xmath      (110)
  -- -------- -------- -------- -- -------

² ² 2 The above bounds ( 109 ),( 110 ) were first derived by us in [ 105
] (see version 1). Subsequently, the same bounds were derived in [ 48 ]
.

The above bounds ( 109 ),( 110 ) are derived by finding a set of @xmath
code symbols such that there are at least @xmath linearly independent
codewords in the dual each of whose support is completely contained in
the support of these @xmath code symbols. Once again, this is the key
ingredient, we had alluded to above. We will now generalize these ideas
in this following and provide the best-known estimate of the number of
linearly independent codewords of weight @xmath in the dual code, each
having support contained in the support of a carefully chosen set of
(say) @xmath code symbols. This estimate will give the tightest known
bound and is based on this shortening approach. This is estimated as
Minimum Support Weight (MSW) Sequence defined in the following which
gives an upper bound @xmath on the size of a set of code symbols that
can be chosen so that we are guaranteed that there exists @xmath
linearly independent codewords of weight @xmath in the dual code each of
whose support is contained in the support of this set of code symbols.
Before we look at MSW sequence, we define Generalized Hamming Weight
(GHW) of a code for which MSW Sequence acts as an upper bound. As
explained before, our new bound makes use of the technique of code
shortening and the GHWs of a code provide valuable information about
shortened codes.

##### 5.1.1 GHW and the Minimum Support Weight Sequence

We will first define the Generalized Hamming Weights of a code,
introduced in [ 73 ] , and also known as Minimum Support Weights (MSW)
(see [ 74 ] ) of a code. In this thesis we will use the term Minimum
Support Weight (MSW).

###### Definition 9.

The @xmath Minimum Support Weight (MSW) @xmath (equivalently, the @xmath
th Generalized Hamming Weight) of an @xmath code @xmath is the minimum
possible value of the cardinality of the support of an @xmath
-dimensional subcode of C, i.e.,

  -- -------- -- -------
     @xmath      (111)
  -- -------- -- -------

where the notation @xmath denotes a subcode @xmath of @xmath and where
@xmath (called the support of the code @xmath ).

Although the MSW definition applies to any code, the interest in this
thesis, is on its application to a restricted class of codes that we
re-introduce (it is already introduced in Chapter Erasure Codes for
Distributed Storage: Tight Bounds and Matching Constructions ) here.

###### Definition 10 (Canonical Dual Code).

By a canonical dual code, we will mean an @xmath linear code @xmath
satisfying the following: @xmath contains a set @xmath of @xmath
linearly independent codewords of Hamming weight @xmath , such that the
sets @xmath , @xmath cover @xmath , i.e.,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

As it turns out, the dual code of an availability code is an example of
a canonical dual code and this is the reason for our interest in the
MSWs of this class of codes.

###### Theorem 5.4.

[ 28 ] Let @xmath be a canonical dual code with parameters @xmath and
support sets @xmath as defined in Definition 10 . Let @xmath denote the
@xmath th @xmath MSW of @xmath . Let @xmath be the minimum possible
value of cardinality of the union of any @xmath distinct support sets
@xmath , @xmath , @xmath i.e.,

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath . Let the integers @xmath be recursively defined as follows:

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (112)
     @xmath   @xmath   @xmath      (113)
  -- -------- -------- -------- -- -------

Then

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

Note that in [ 28 ] , the Theorem 5.4 is proved for the case @xmath ,
@xmath , @xmath but we observe from the proof of Theorem 5.4 (proof for
@xmath ) given in [ 28 ] that the Theorem 5.4 is also true for any
@xmath , @xmath , @xmath . We will refer to the sequence @xmath
appearing in the Theorem 5.4 above as the Minimum Support Weight (MSW)
Sequence associated to parameter set @xmath . Hence this MSW sequence
gives an upper bound @xmath on the size of a set of code symbols that
can be chosen so that we are guaranteed that there exists @xmath
linearly independent codewords of weight @xmath in the dual code each of
whose support is contained in the support of this set of code symbols.
In the subsection below, we derive a new alphabet-size dependent bound
on minimum distance and dimension, that are expressed in terms of the
MSW sequence.

#### 5.2 New Alphabet-Size Dependent Bound on Minimum Distance and
Dimension Based on MSW

In this subsection, we present a new field-size dependent bound on the
minimum distance and dimension of an availability code @xmath with
parameters @xmath . The bound is derived in terms of the MSW sequence
associated with the dual code @xmath of @xmath . The basic idea is to
shorten the availability code to a code with shortened block length with
size of the set of code symbols set to zero equal to the @xmath th term
@xmath in the MSW sequence for some @xmath . Theorem 5.4 provides a
lower bound on the dimension of this shortened code. Classical bounds on
the parameters of this shortened code are shown to yield bounds on the
parameters of the parent availability code.

###### Theorem 5.5.

Let

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

Let @xmath be an @xmath availability code over a field @xmath with
minimum distance @xmath . Let @xmath be the maximum possible minimum
distance of an @xmath availability code over the field @xmath . Then:

  -- -------- -- -------
     @xmath      (114)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (115)
  -- -------- -- -------

where

1.  @xmath ,

2.  @xmath ,

3.  @xmath , @xmath .

###### Proof.

Since @xmath is an availability code with parameters @xmath , we have
that @xmath is an @xmath canonical dual code with locality parameter
@xmath and @xmath . @xmath because @xmath is an upper bound on maximum
possible rate of an availability code with parameters @xmath (due to
equations ( 29 ), ( 30 ), ( 121 )). We now set @xmath . Hence from
Theorem 5.4 , @xmath , @xmath . For simplicity, let us write @xmath ,
@xmath . Next, fix @xmath with @xmath . Let @xmath be the support of an
@xmath dimensional subspace or subcode of @xmath with the support having
cardinality exactly @xmath in @xmath . Add @xmath arbitrary extra
indices to @xmath and let the resulting set be @xmath . Hence @xmath and
@xmath . Now shorten the code @xmath in the co-ordinates indexed by
@xmath i.e., take @xmath where @xmath is the compliment of @xmath and
@xmath for a set @xmath with @xmath , @xmath . The resulting code @xmath
has block length @xmath , dimension @xmath and minimum distance @xmath
(if @xmath ) and the resulting code @xmath is also an availability code
with parameters @xmath . Hence:

  -- -------- --
     @xmath   
  -- -------- --

The proof of ( 115 ) follows from the fact that @xmath and the fact

  -- -------- -- -------
     @xmath      (116)
  -- -------- -- -------

∎

In the following, we will analyze MSW sequence and form an upper bound
on MSW sequence. Based on this upper bound, we will conclude that our
upper bound on minimum distance and dimension given in ( 114 ),( 115 )
are the tightest known bounds.

###### Analysis of MSW sequence

###### Lemma 5.6.

Let @xmath be as defined in Theorem 5.4 . Let @xmath for some @xmath .
Then for @xmath ,

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (117)
  -- -------- -------- -------- -- -------

###### Proof.

We will prove the upper bound on @xmath inductively. Let by induction
hypothesis,

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (118)
  -- -------- -------- -------- -- -------

For @xmath , the inequality ( 118 ) becomes:

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (119)
  -- -------- -------- -------- -- -------

which is true. Hence we proved the intial condition of induction.
Substituting the above expression ( 118 ) for @xmath in the recursion
given in ( 113 ) without ceiling:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

Hence the inequality ( 118 ) is proved by induction. Hence we proved
that:

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (120)
  -- -------- -------- -------- -- -------

∎

###### Corollary 5.7.

Let @xmath be as defined in Theorem 5.4 and let @xmath and @xmath for
some @xmath . Then the following inequalities hold:

1.  @xmath whenever @xmath where @xmath or whenever @xmath .

2.  @xmath , @xmath @xmath .

where @xmath are as defined in the Theorem 5.5 .

###### Proof.

From the inequality ( 117 ), it can seen that @xmath whenever @xmath
where @xmath . It can also be seen from the inequality @xmath that
@xmath . Choosing @xmath , we get @xmath whenever @xmath . Further it
can seen by using the recursion ( 113 ) that @xmath , @xmath @xmath .
This is because if you substitute @xmath in the recursion ( 113 ) (with
ceiling), you will get @xmath . ∎

###### Tightness of the Bounds Given in Theorem 5.5

In the following the word bound refers to an upper bound. From the
Corollary 5.7 , it can be seen that the bound ( 114 ) is tighter than
the bounds ( 107 ),( 109 ). For the same reason, the bound ( 115 ) is
tighter than the bounds ( 106 ), ( 108 ), ( 110 ) . Hence our bounds (
114 ),( 115 ) are the tightest known bounds on minimum distance and
dimension of an availability code over @xmath . We here note that the
bounds ( 114 ), ( 115 ) apply even if we replace @xmath with any other
upper bound on @xmath MSW. Hence the bounds ( 114 ), ( 115 ) are general
in that sense. We give better upper bounds on @xmath MSW than @xmath in
[ 35 ] under restricted conditions. We do not include this result here
to keep things simple.

### 6 Bounds for Unconstrained Alphabet Size

In this section, we present our results on upper bound on rate of an SA
code and upper bound on minimum distance of an availability code. Our
upper bound on rate of an SA code is the tightest known bound as @xmath
increases for a fixed @xmath . Our upper bound on minimum distance of an
availability code is the tightest known bound on minimum distance of an
availability code for all parameters.

#### 6.1 Upper Bounds on Rate for unconstrained alphabet size

In this section, we first give a survey of upper bounds on rate of an
availability code from the existing literature. We will then present a
new upper bound on rate of an SA code. Our upper bound on rate of an SA
code is the tightest known bound as @xmath increases for a fixed @xmath
. This upper bound on rate of an SA code is based on the idea that
transpose of the matrix @xmath in the parity check matrix of an SA code
with parameters @xmath is also a parity check matrix of an SA code with
parameters @xmath , @xmath ( @xmath is the locality parameter and @xmath
is the number of disjoint recovery sets available for a code symbol).

##### 6.1.1 Known Bounds on Rate

The following upper bound on the rate of a code with availability was
given in [ 4 ] . The bound is derived based on a graphical approach.

###### Theorem 6.1.

[ 4 ] If @xmath is an @xmath availability code over a finite field
@xmath , then its rate @xmath must satisfy:

  -- -------- -- -------
     @xmath      (121)
  -- -------- -- -------

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

After our results were published in [ 35 ] (applicable for any field
size), the following upper bound on rate of an @xmath SA code over
@xmath was published in [ 50 ] :

  -- -------- -- -------
     @xmath      (122)
  -- -------- -- -------

Comparison of our bound ( 125 ) with the above bound ( 122 ) appears
later. For @xmath and @xmath , the tightest known upper bound on rate of
an @xmath SA code over @xmath is given in [ 50 ] :

  -- -------- -- -------
     @xmath      (123)
  -- -------- -- -------

Our bound in ( 125 ) is the tightest known bound as @xmath increases for
a fixed @xmath , it does not give good bounds for small @xmath like
@xmath for large @xmath .

##### 6.1.2 A Simple New Upper Bound on Rate of an @xmath SA Code:

Here we derive a new upper bound on rate of an @xmath SA code using a
very simple transpose trick. This upper bound on rate of an SA code is
the tightest known bound as @xmath increases for a fixed @xmath .

###### Theorem 6.2.

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (124)
     @xmath   @xmath   @xmath      (125)
  -- -------- -------- -------- -- -------

###### Proof.

Let @xmath be the maximum achievable rate of an SA code for a fixed
@xmath over the field @xmath . If @xmath SA code doesn’t exist for any
@xmath for fixed @xmath then we define @xmath as follows. @xmath if
there exists a parity check matrix @xmath satisfying the conditions of
an SA code but with @xmath and we set @xmath if there is no parity check
matrix @xmath satisfying the conditions of an SA code for any value of
@xmath . Let us choose @xmath such that @xmath . Let @xmath be an @xmath
SA code over the field @xmath with rate @xmath . By definition @xmath is
the null space of an @xmath matrix @xmath with each column having weight
@xmath and each row having weight @xmath ( @xmath is an empty matrix as
@xmath is a code with maximum possible rate for the given @xmath ). This
matrix @xmath contains all @xmath orthogonal parity checks protecting
any given symbol. Now the null space of @xmath (transpose of @xmath )
corresponds to an @xmath SA code over the field @xmath for some @xmath .
Hence we have the following inequality:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

Hence we have (Let @xmath ):

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath                     
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

Now swapping the roles of @xmath and @xmath in the above derivation
i.e., we take an @xmath SA code @xmath over the field @xmath with rate
@xmath (Note that @xmath ) and repeat the above argument in exactly the
same way. By doing so we get :

  -- -------- --
     @xmath   
  -- -------- --

Hence we get:

  -- -------- -- -------
     @xmath      (126)
  -- -------- -- -------

Now substituting the rate bound @xmath given in ( 121 ) into ( 126 ), we
get:

  -- -------- -- -------
     @xmath      (127)
  -- -------- -- -------

∎

###### Remark 7.

@xmath :
In the following the word bound refers to an upper bound and the word
rate bound refers to an upper bound on rate. The bound on @xmath given
in ( 125 ) becomes tighter than the bound @xmath given in ( 121 ) as
@xmath increases for a fixed @xmath (We skip a formal proof of this.
This can be seen by approximating ( 121 ) by @xmath for large @xmath for
fixed @xmath and by the fact that the bound ( 121 ) follows the
inequality @xmath . Simulation shows that our bound given in ( 125 ) is
tighter than the bound @xmath given in ( 121 ) for @xmath for @xmath and
@xmath ). As an example lets calculate @xmath . From ( 125 ):

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

The above abound on @xmath obtained from ( 124 ) and ( 125 ) is a tight
bound as it is known that rate @xmath is achievable for @xmath and any
@xmath using a complete graph code ( [ 28 ] ) and hence clearly tighter
than @xmath given in ( 121 ) . We show a plot of the bound given in (
125 ) for @xmath in Fig. 25 . The plot shows that the bound given by (
125 ) is tighter than the bound given in ( 121 ) for @xmath .
Even though our bound given in ( 125 ) becomes tighter as @xmath
increases for a fixed @xmath than the bound in ( 121 ), the bound given
in ( 121 ) is for @xmath availability codes but the bound in ( 124 ) and
( 125 ) is applicable only for @xmath SA codes. But we also would like
to draw attention to the fact that most of the high rate constructions
known in literature for @xmath availability codes are also @xmath SA
codes. In fact the most general high rate construction for @xmath
availability code is given in [ 34 ] (the Wang et al. Construction
explained in Section 4 ) and it is also an @xmath SA code. Hence there
is a very good reason to think that when it comes to rate-optimality
@xmath SA codes will give good candidate codes.

###### Rate Bound comparison for @xmath:

Note that the bound ( 122 ) is derived in [ 50 ] using our equation (
124 ) by substituting the upper bound on rate for @xmath SA codes given
in ( 123 ). Note that the bounds ( 122 ),( 123 ) are specific to binary
codes whereas our bounds in ( 124 ) and ( 125 ) and the bound we derived
in [ 35 ] for @xmath are applicable over any finite field. But
nonetheless, we compare our bounds with ( 122 ) for @xmath . When @xmath
, the following conclusions are true for SA codes.

1.   Rate bound we derived in [ 35 ] for @xmath (we are skipping this
    result from this thesis as it involves relatively complicated
    analysis. For interested reader please see [ 35 ] .) is tighter than
    ( 122 ) for @xmath .

2.   Rate bound in ( 125 ) is tighter than ( 122 ) for @xmath .

3.   Rate bound we derived in [ 35 ] for @xmath is tighter than ( 125 )
    for @xmath .

4.   All these 3 bounds ( 125 ), bound in [ 35 ] for @xmath , ( 122 ),
    are tighter than ( 121 ).

5.   Hence the conclusion is that for @xmath , rate bound we derived in
    [ 35 ] for @xmath is the tightest known bound and for @xmath , ( 125
    ) is the tightest known bound and for @xmath , ( 122 ) (applicable
    only for binary codes) is the tightest known bound. On plotting it
    can be seen that even for @xmath , the difference between bound in (
    122 ) and bound in ( 125 ) is very small around @xmath .

6.   For @xmath (non-binary codes), the conclusion is that for @xmath ,
    rate bound we derived in [ 35 ] for @xmath is the tightest known
    bound and for @xmath , ( 125 ) is the tightest known bound.

#### 6.2 Upper Bounds on Minimum Distance of an Availability Code for
Unconstrained Alphabet Size

##### 6.2.1 Known Bounds on Minimum Distance

Let @xmath be the largest possible minimum distance of an @xmath
availability code. In [ 33 ] , the following upper bound on the minimum
distance of an information symbol availability code with parameters
@xmath (and hence the upper bound is also applicable to the case of
all-symbol availability codes as well) was presented:

  -- -------- -- -------
     @xmath      (128)
  -- -------- -- -------

This bound was derived by adopting the approach employed in Gopalan
et.al [ 8 ] to bound the minimum distance of an LR code. An improved
upper bound on minimum-distance for (All Symbol) availability codes
appears in [ 4 ] :

  -- -------- -- -------
     @xmath      (129)
  -- -------- -- -------

The following upper bound on minimum distance of an availability code
appeared in [ 48 ] .

  -- -------- -- -------
     @xmath      (130)
  -- -------- -- -------

##### 6.2.2 A New Upper Bound on Minimum Distance of an availability
code with Unconstrained Alphabet Size

Here we derive an upper bound on minimum distance of an availability
code (not necessarily an SA code.). This bound is the tightest known
bound on minimum distance of an availability code for all parameters.
This bound is derived by taking supremum w.r.t field size on both sides
(first on right hand side and then on left hand side) of ( 114 ).

###### Theorem 6.3.

The field-size dependency of the bound ( 114 ) can be removed and
written as:

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (131)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

where @xmath and @xmath are the MSW sequence defined in Theorem 5.4 and
@xmath is same as that defined in Theorem 5.5 .

###### Proof.

Let @xmath . From ( 114 ) :

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

The second inequality in ( 131 ) follows from ( 129 ). ∎

###### Remark 8.

Tightness of the Bound : From the Corollary 5.7 , @xmath whenever @xmath
and @xmath , @xmath @xmath . Hence it can be the seen that ( 131 ) is
the tightest known upper bound on minimum distance of an availability
code for all parameters. This is because:

1.   For @xmath , the right hand side of the bounds given in ( 128 ),(
    129 ),( 130 ) are lower bound by the following expression:

      -- -------- --
         @xmath   
      -- -------- --

2.   For all values of @xmath , the right hand side of the bounds given
    in ( 128 ),( 129 ),( 130 ) are lower bound by the following
    expression:

      -- -------- --
         @xmath   
      -- -------- --

We plot our bound for @xmath in Fig. 26 . It can be seen from the plot
in Fig. 26 that our upper bound given in ( 131 ) is tigher than the
upper bounds ( 128 ),( 129 ),( 130 ).

We derived another upper bound on minimum distance of an availability
code in [ 35 ] based on our own calculation of an upper bound on @xmath
th MSW which is better than @xmath . This bound uses the rank of @xmath
and an upper bound on weight of any column of @xmath . This bound is
tighter than the bounds ( 128 ),( 129 ),( 130 ) without any constraint
i.e., even after maximizing over all possible values of rank of @xmath
and all possible values of upper bound on weight of any column of @xmath
. But it is tighter than the bound ( 131 ) only when we do calculation
of our upper bound on @xmath th MSW given in [ 35 ] knowing specifically
the rank of @xmath and the upper bound on weight of any column of the
matrix @xmath . We do not mention this bound to keep things simple.
Interested reader please refer to [ 35 ] .

### 7 A Tight Asymptotic Upper Bound on Fractional Minimum Distance for
a Special class of Availability Codes

Let @xmath be an availability code with parameters @xmath over a finite
field @xmath with parity check matrix @xmath as described in the
following where @xmath will also be defined in the following. We have
already seen that parity check matrix @xmath of @xmath can be written in
the form @xmath . In this section, we assume @xmath (writing only
linearly independent rows) can be written as follows:

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (132)
  -- -------- -------- -------- -- -------

where @xmath is an @xmath matrix and @xmath is an @xmath matrix and
@xmath is a parity check matrix of an availability code with parameters
@xmath with rate @xmath over @xmath ( @xmath is an extension field of
@xmath ). Let @xmath be a @xmath matrix with rank @xmath with entries
from @xmath where @xmath . Hence the parity check matrix @xmath is an
@xmath matrix with @xmath and @xmath . Let @xmath . Note that @xmath
depends on @xmath . We assume throughout this section that @xmath .

###### Theorem 7.1.

The code @xmath over @xmath with @xmath parity check matrix @xmath with
@xmath defined by ( 132 ) as described above is an availability code
with parameters @xmath with minimum distance @xmath satisfying:

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (133)
  -- -------- -------- -------- -- -------

###### Proof.

Recall that @xmath . Since the code @xmath has minimum distance @xmath ,
any set of @xmath columns of @xmath must be linearly independent. If we
take the first @xmath columns of @xmath then its rank is upper bounded
by @xmath . Hence:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

∎

###### Corollary 7.2.

Let @xmath . Let @xmath be such that @xmath rate( @xmath ) with each
code @xmath as defined before defined by @xmath parity check matrix
@xmath over @xmath with @xmath defined by ( 132 ) where @xmath depends
on @xmath . Let @xmath where @xmath is the minimum distance of @xmath
and @xmath . Then,

  -- -------- -- -------
     @xmath      (134)
  -- -------- -- -------

###### Proof.

Proof follows from Theorem 7.1 . ∎

###### Theorem 7.3.

For @xmath , there exists a choice of @xmath such that the code @xmath
over @xmath as defined before with @xmath parity check matrix @xmath
with @xmath defined by ( 132 ) is an availability code with parameters
@xmath with minimum distance @xmath satisfying:

  -- -------- -- -------
     @xmath      (135)
  -- -------- -- -------

###### Proof.

For an @xmath matrix @xmath and @xmath , let @xmath refer to the @xmath
sub-matrix of @xmath containing exactly the columns indexed by @xmath .
Since the code @xmath has minimum distance @xmath , any set of @xmath
columns of @xmath must be linearly independent. Let @xmath . Recall that
@xmath is an @xmath matrix. If we take any @xmath , @xmath , then @xmath
. This is because @xmath . Now choose the entries of @xmath to be
variables taking values from @xmath . We will now choose the values of
these variables so that @xmath . Let @xmath be a @xmath matrix with
first @xmath rows of @xmath containing some @xmath linearly independent
rows from @xmath and the last @xmath rows of @xmath containing the first
@xmath rows of @xmath . The rank of the matrix @xmath can be made at
least @xmath by making sure that @xmath by choosing the values of
variables in @xmath appropriately. Note that @xmath is a submatrix of
@xmath restricted to these @xmath rows. This condition can be formulated
as a condition requiring the determinant of a @xmath square sub-matrix
of @xmath to be non-zero where the first @xmath rows of the sub-matrix
are chosen to be linearly independent. This property of the first @xmath
rows of the sub-matrix is possible since the first @xmath rows of @xmath
are linearly independent. Each such condition corresponds to making a
non-zero polynomial in variables introduced in @xmath to take a non-zero
value. We can multiply all these non-zero polynomials corresponding to
each such subset @xmath and choose values for variables in @xmath so
that this product of all these polynomials takes a non-zero value by
choosing @xmath by the Combinatorial Nullstellensatz Theorem [ 106 ] .
Hence: @xmath and since by above argument we can make any set of @xmath
columns to have rank at least @xmath , we can choose largest @xmath such
that @xmath for @xmath and @xmath . Hence we can choose @xmath . Hence:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

∎

###### Corollary 7.4.

Let @xmath . For sufficiently large @xmath there exists a choice of
@xmath with @xmath and for @xmath with @xmath being an extension field
of @xmath there exists a choice of @xmath such that the code @xmath over
@xmath as defined before with @xmath parity check matrix @xmath with
@xmath defined by ( 132 ) is an availability code with parameters @xmath
with minimum distance @xmath satisfying:

  -- -------- -- -------
     @xmath      (136)
  -- -------- -- -------

###### Proof.

The proof follows from applying Theorem 7.3 with @xmath such that @xmath
. This is always possible by definition of @xmath and by choosing @xmath
appropriately. ∎

###### Corollary 7.5.

Let @xmath . Then there exists a sequence of codes @xmath with each code
@xmath as defined before defined by @xmath parity check matrix @xmath
over @xmath with @xmath defined by ( 132 ) where @xmath depends on
@xmath such that @xmath rate( @xmath ) and

  -- -------- -- -------
     @xmath      (137)
  -- -------- -- -------

where @xmath where @xmath is the minimum distance of @xmath and @xmath .

###### Proof.

We choose different @xmath for each code @xmath . Note that @xmath
depends on @xmath here. For each @xmath , we choose @xmath to be
sufficiently large such that there exists @xmath with @xmath for @xmath
for some @xmath such that @xmath . Hence for each @xmath , by choosing
@xmath to be sufficiently large and @xmath with @xmath being an
extension field of @xmath , by Corollary 7.4 , we can choose @xmath such
that @xmath and @xmath satisfies ( 136 ) with @xmath . Since @xmath , it
is trivial to make sure @xmath rate( @xmath ). By Corollary 7.2 and
Corollary 7.4 , for the chosen sequence of codes @xmath :

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath the equation ( 137 ) follows. ∎

###### Remark 9.

Note that Corollary 7.2 and Corollary 7.5 completely characterizes the
optimal tradeoff between rate and fractional minimum distance for the
class of codes defined by @xmath as @xmath .

### 8 Minimum Block-Length Codes with Strict Availability

In this section, we present a new lower bound on block length of an SA
code. We also show that the lower bound on block length is achieved if
and only if BIBD (Balanced Incomplete Block Design) [ 107 ] with certain
parameters exist. We then present two well-known BIBD. Based on our
observation, the SA codes based on these two BIBD has least possible
block length and maximum possible rate (for the given block length)
based on a famous conjecture. From definition, parity check matrix of an
SA code @xmath over a finite field @xmath can be written in the form
@xmath where @xmath is an @xmath matrix with each row of weight @xmath
and each column of weight @xmath which includes the @xmath orthogonal
parities ( @xmath parity checks corresponding to @xmath disjoint
recovery sets @xmath ) for each code symbol @xmath . The matrix @xmath
may also be viewed as parity check matrix of a @xmath -regular LDPC
code. The corresponding Tanner graph of the code must necessarily have
no cycles of length @xmath . Let @xmath entry of @xmath be @xmath . Let
@xmath be an @xmath matrix over @xmath with entries from @xmath given by
@xmath when @xmath and @xmath else.

###### Theorem 8.1.

Let @xmath be an @xmath SA code. Then:

  -- -------- -- -------
     @xmath      (138)
  -- -------- -- -------

###### Proof.

Let @xmath and @xmath be @xmath matrices as defined above in this
section. From the requirements on an SA code, we must have:

  -- -- --
        
  -- -- --

Using the relation @xmath , we obtain

  -- -- --
        
  -- -- --

∎

Our interest is in the minimum-block-length case, where ( 138 ) holds
with equality and for which a necessary condition is that @xmath .

###### Corollary 8.2.

For @xmath , let us define the sets @xmath by

  -- -------- -- -------- --
     @xmath      @xmath   
  -- -------- -- -------- --

where @xmath and @xmath are as defined before in this section. When
equality holds in ( 138 ), the sets @xmath form a @xmath balanced
incomplete block design (BIBD) having parameters

  -- -------- -- -------
     @xmath      (139)
  -- -------- -- -------

Conversely a BIBD with the parameter values as in ( 139 ) will yield an
SA code @xmath with parameters @xmath having block length satisfying (
138 ) with equality where the parity check matrix of the code @xmath is
given by the incidence matrix of the BIBD. The rate @xmath of @xmath
clearly satisfies @xmath .

###### Proof.

Proof follows from the fact that equality holds in ( 138 ) iff the inner
product of every pair of distinct rows of @xmath is exactly equal to
@xmath . ∎

{example}

Let @xmath and let @xmath denote the projective plane over @xmath having
@xmath points and @xmath lines. Let the lines and points be indexed by
@xmath in some order. Each line contains @xmath points and there are
@xmath lines passing through a point. Set @xmath . Let @xmath be the
@xmath parity-check matrix of a binary code @xmath given by @xmath if
the @xmath point lies on the @xmath line and @xmath otherwise. Then it
is known that @xmath has rank @xmath over @xmath ( [ 108 ] ), and that
@xmath has minimum distance @xmath , thus @xmath is a binary @xmath SA
code. A plot comparing the rate of this code @xmath with the upper bound
on rate in ( 121 ) as a function of the parameter @xmath , is shown in
Fig. 27 . While this code is well-known in the literature on LDPC codes,
the intention here is to draw attention to the fact that this code is an
SA having minimum block length. The parameters of a minimum-block-length
code obtained by a similar construction involving lines in the affine
plane and @xmath are given by @xmath , where @xmath .

###### Conjecture 8.3 (Hamada-Sachar Conjecture [109]).

Every Projective plane of order @xmath , p a prime, has p rank at least
@xmath with equality if and only if its desarguesian.

The above conjecture is as yet unproven, but if true, would show the
projective-plane code described in example 8 to have minimum block
length and maximum rate among all SA binary codes with @xmath and @xmath
where @xmath . {example} Consider the code @xmath obtained by making use
of the Steiner Triple System (STS) associated to the point-line
incidence matrix of @xmath dimensional projective space @xmath over
@xmath where this incidence matrix defines the parity check matrix
@xmath of the code @xmath . Hence once again the rows of @xmath
correspond to points in the projective space and the columns to lines (2
dimensional subspace). Let @xmath . From [ 110 ] , @xmath . The code
@xmath is a binary SA code having parameters @xmath , @xmath . The
corresponding comparison with the rate bound in ( 121 ) appears in Fig.
27 .

###### Conjecture 8.4 (Hamada’s Conjecture [107]).

The p-rank of any design @xmath with parameters of a geometric design
@xmath in PG(n,q) or AG(n,q) @xmath is at least the p-rank of @xmath
with equality if and only if @xmath is isomoprhic to @xmath .

In [ 110 ] , this conjecture has been shown to hold true for the Steiner
Triple system appearing in Example 8 . Thus the code in the example has
the minimum block length and maximum rate among all binary SA codes with
@xmath where @xmath .

### 9 Summary and Contributions

In this chapter, we presented new field size dependent and field size
independent upper bounds on minimum distance and dimension of an
availability code based on shortening approach and MSW sequence. We also
presented a new upper bound on rate of an SA code. Before presenting new
results we presented the appropriate results from literature for
comparison. We showed evidence that the new bounds presented in this
chapter are the tightest known bounds. We then completely characterized
the optimal tradeoff between rate and fractional minimum distance for a
special class of availability codes defined by @xmath as @xmath . We
finally presented a lower bound on block length of an SA code and showed
that codes achieving the lower bound on block length exists iff BIBD
with certain parameters exists. We then presented two well known BIBD
and SA codes based on them. These codes based on the two BIBD have
maximum possible rate (for the minimum possible block length) based on a
famous conjecture.

## Chapter \thechapter Tight Bounds on the Sub-Packetization Level of
MSR and Vector-MDS Codes

In this chapter we will begin by introducing a class of codes called
Regenerating (RG) codes. These codes are vector codes with block length
@xmath and dimension @xmath where each vector code symbol have @xmath
components. The variable @xmath is also known as sub-packetization. Each
vector code symbol is stored in a distinct node or a disk. A special
property of RG codes is that it can replace the contents of a failed
node or a vector code symbol by downloading @xmath symbols from each
node from a set of @xmath other nodes. This property is called repair
property. These @xmath symbols could be any function of @xmath symbols
stored in the node. The RG codes are such that this value @xmath is the
smallest possible value. Another property of RG codes is that it can
recover the @xmath message symbols by using the contents of any @xmath
vector code symbols or contents of any @xmath nodes. This property is
called data collection property. A special case of RG codes
corresponding to the smallest possible value of @xmath for a given value
of @xmath is called Minimum Storage Regenerating (MSR) codes. It turns
out that MSR codes are also vector MDS codes. In an MSR code, repair
property holds for any failed node but if we restrict the repair
property to any node belonging to a fixed subset of @xmath nodes, we
call it MDS codes with optimal repair of @xmath nodes. Another special
case of RG codes corresponding to the smallest possible value of @xmath
for a given value of @xmath is called Minimum Bandwidth Regenerating
(MBR) codes. It turns out that the value of @xmath cannot be too small
for the design of an MSR code or MDS code with optimal repair of @xmath
nodes. Hence it is an interesting problem to find a lower bound on
@xmath as @xmath plays an important role in the encoding and decoding
complexity as well as in complexity in implementing the repair property.
In this chapter, we will give a tight lower bound on @xmath for an MSR
or MDS code with optimal repair of @xmath nodes for the case when repair
property is such that we directly use @xmath out of @xmath symbols from
each of @xmath nodes and carry out repair property. This repair property
is called optimal access repair or repair by help-by-transfer. The fact
that our bound is tight is shown by pointing out constructions from
literature which achieves our lower bound. We also present a theorem on
structure of these codes when @xmath is exactly equal to our lower
bound. We point out through an example that the structure we deduced is
present in existing codes. We also very briefly describe a construction
of MSR code with minimum possible value of @xmath and field size of
@xmath and point out to our paper for further details. Below is a
summary of our results in this chapter.

###### Organization of the Chapter

The chapter is organized as follows. We begin by formally defining RG
codes and MSR codes in Section 10 . Section 11 presents new lower bounds
on @xmath for MSR codes. Section 12 presents new lower bounds on @xmath
for MDS codes with optimal access repair of @xmath nodes. Section 13
presents a theorem on structure of MDS codes with optimal access repair
of @xmath nodes. In Section 14 we briefly gave an intuitive outline of a
new construction of an optimal access MSR code with optimal
sub-packetization.

###### Contributions

Contributions of the thesis on the topic of MSR codes and MDS codes with
optimal access repair of a set of @xmath nodes:

1.  A tabular summary of the new lower bounds on sub-packetization-level
    @xmath derived here appears in Table 4 and amounts to a summary of
    the main results presented in this chapter.

2.  We prove new lower bounds on sub-packetization-level @xmath of MSR
    codes and vector MDS codes with optimal repair of @xmath nodes with
    optimal access repair property. We first derive the lower bounds for
    the case of MSR codes for @xmath (Sections 11.2 , 11.3 ) and extend
    it to general @xmath ( 11.4 ). We then derive the lower bounds for
    the case of MDS codes with optimal access repair of @xmath nodes for
    @xmath (Section 12.2.1 ) and extend it to general @xmath (Section
    12.2.2 ). We show that our lower bounds on @xmath are tight by
    comparing with existing code constructions.

3.  We study the structure of a vector MDS code (Section 13 ) with
    optimal access repair of a set of @xmath nodes, and which achieve
    our lower bound on sub-packetization level @xmath . It turns out
    interestingly, that such a code must necessarily have a
    coupled-layer structure, similar to that of the Ye-Barg code [ 65 ]
    .

4.  Finally we give a very brief intuitive overview of a new
    construction of optimal access MSR codes (Section 14 ) with @xmath
    field size with optimal sub-packetization for some specific
    parameters. This construction of ours appeared in [ 111 ] .

Our approach to lower bounds on @xmath is along the lines of that
adopted in [ 51 ] but with the difference that here we consider
non-constant repair subspaces and consider all-node repair and also
consider MDS codes with optimal access repair of @xmath nodes.

### 10 Regenerating Codes

Data is stored in distributed storage by distributing it across disks or
nodes. So one of the important problems in distributed storage is to
repair a node on its failure. The coding theory community has come up
with two types of coding techniques for this. They are called
Regenerating (RG) codes and Locally Recoverable (LR) codes. The focus in
a Regenerating (RG) code is on minimizing the amount of data download
needed to repair a failed node, termed the repair bandwidth while LR
codes seek to minimize the number of helper nodes contacted for node
repair, termed the repair degree . In a different direction, coding
theorists have also re-examined the problem of node repair in RS codes
and have come up with new and more efficient repair techniques. There
are two principal classes of RG codes, namely Minimum Bandwidth
Regenerating (MBR) and Minimum Storage Regeneration (MSR). These two
classes of codes are two extreme ends of a tradeoff known as the
storage-repair bandwidth (S-RB) tradeoff. MSR codes tries to minimize
the storage overhead whereas MBR codes tries to minimize the repair
bandwidth. There are also codes that correspond to the interior points
of this tradeoff. The theory of regenerating codes has also been
extended in several directions.

###### Definition 11 ([6]).

Let @xmath denote a finite field of size @xmath . Then a regenerating
(RG) code @xmath over @xmath having integer parameter set @xmath where
@xmath , @xmath , @xmath , maps a file @xmath on to a collection @xmath
where @xmath using an encoding map

  -- -------- --
     @xmath   
  -- -------- --

with the @xmath components of @xmath stored on the @xmath -th node in
such a way that the following two properties are satisfied: Data
Collection: The message @xmath can be uniquely recovered from the
contents @xmath of any @xmath nodes.
Node Repair: If the @xmath -th node storing @xmath fails, then a
replacement node (the node which replaces the failed node @xmath ) can

1.   contact any subset @xmath of the remaining @xmath nodes of size
    @xmath ,

2.   Each node @xmath (called a helper node) maps the @xmath symbols
    @xmath stored in it on to a collection of @xmath repair symbols
    @xmath ,

3.   pool together the @xmath repair symbols @xmath thus computed to use
    them to create a replacement vector @xmath whose @xmath components
    are stored in the replacement node, in a such a way that the
    contents of the resultant nodes, with the replacement node replacing
    the failed node, once again forms a regenerating code.

A regenerating code is said to be exact-repair (ER) regenerating code if
the contents of the replacement node are exactly same as that of the
failed node, ie., @xmath . Else the code is said to be functional-repair
(FR) regenerating code. A regenerating code is said to be linear if

1.  @xmath , @xmath and

2.  the map mapping the contents @xmath of the @xmath -th helper node on
    to the corresponding @xmath repair symbols @xmath is linear over
    @xmath .

Thus a regenerating code is a code over a vector alphabet @xmath and the
quantity @xmath is termed the sub-packetization level of the
regenerating code. The total number @xmath of @xmath symbols @xmath
transferred for repair of failed node @xmath is called the repair
bandwidth of the regenerating code. Note that this number @xmath is same
for any failed node @xmath . The rate of the regenerating code is given
by @xmath . Its reciprocal @xmath is the storage overhead .

#### 10.1 Cut-Set Bound

Let us assume that @xmath is a functional-repair regenerating code
having parameter set: @xmath . Since an exact-repair regenerating code
is also a functional-repair code, this subsumes the case when @xmath is
an exact-repair regenerating code. Over time, nodes will undergo
failures and every failed node will be replaced by a replacement node.
Let us assume to begin with, that we are only interested in the
behaviour of the regenerating code over a finite-but-large number @xmath
of node repairs. For simplicity, we assume that repair is carried out
instantaneously. Then at any given time instant @xmath , there are
@xmath functioning nodes whose contents taken together comprise a
regenerating code. At this time instant a data collector could connect
to @xmath nodes, download all of their contents and decode to recover
underlying message vector . Thus in all, there are at most @xmath
distinct data collectors which are distinguished based on the particular
set of @xmath nodes to which the data collector connects. Next, we
create a source node @xmath that possesses the @xmath message symbols
@xmath , and draw edges connecting the source to the initial set of
@xmath nodes. We also draw edges between the @xmath helper nodes that
assist a replacement node and the replacement node itself as well as
edges connecting each data collector with the corresponding set of
@xmath nodes from which the data collector downloads data. All edges are
directed in the direction of information flow. We associate a capacity
@xmath with edges emanating from a helper node to a replacement node and
an @xmath capacity with all other edges. Each node can only store @xmath
symbols over @xmath . We take this constraint into account using a
standard graph-theory construct, in which a node is replaced by @xmath
nodes separated by a directed edge (leading towards a data collector) of
capacity @xmath . We have in this way, arrived at a graph (see Fig. 29 )
in which there is one source @xmath and at most @xmath sinks @xmath .
Each sink @xmath would like to be able to reconstruct all the @xmath
source symbols @xmath from the symbols it receives. This is precisely
the multicast setting of network coding. A principal result in network
coding tells us that in a multicast setting, one can transmit messages
along the edges of the graph in such a way that each sink @xmath is able
to reconstruct the source data, provided that the minimum capacity of a
cut separating @xmath from @xmath is @xmath . A cut separating @xmath
from @xmath is simply a partition of the nodes of the network into
@xmath sets: @xmath containing @xmath and @xmath containing @xmath . The
capacity of the cut is the sum of capacities of the edges leading from a
node in @xmath to a node in @xmath . A careful examination of the graph
will reveal that the minimum capacity @xmath of a cut separating a sink
@xmath from source @xmath is given by @xmath (see Fig. 29 for an example
of a cut separating source from sink). This leads to the following upper
bound on file size [ 6 ] :

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (140)
  -- -------- -------- -------- -- -------

###### Interpretaion of Bound on File Size:

The above bound could be interpreted as satisfying data collection
property by collecting the information on a set of @xmath nodes. First
we collect @xmath symbols for recovering the information of node one.
Since we have information on node one the set of symbols needed to
recover the information in another node is at most @xmath . Continuing
this way we can collect the information on node @xmath by using the
information we already have on node @xmath to @xmath with excess
information of at most @xmath . The above bound hence could be
interpreted as saying that the actual file size is upper bound by the
number of symbols with which we can recover it. Network coding also
tells us that when only a finite number of regenerations take place,
this bound is achievable and furthermore achievable using linear network
coding, i.e., using only linear operations at each node in the network
when the size @xmath of the finite field @xmath is sufficiently large.
In a subsequent result [ 7 ] , Wu established using the specific
structure of the graph, that even in the case when the number of sinks
is infinite, the upper bound in ( 140 ) continues to be achievable using
linear network coding. In summary, by drawing upon network coding, we
have been able to characterize the maximum file size of a regenerating
code given parameters @xmath for the case of functional repair when
there is constraint placed on the size @xmath of the finite field @xmath
. Note interestingly, that the upper bound on file size is independent
of @xmath . Quite possibly, the role played by @xmath is that of
determining the smallest value of field size @xmath for which a linear
network code can be found having file size @xmath satisfying ( 140 ). A
functional regenerating code having parameters: @xmath is said to be
optimal (a) the file size @xmath achieves the bound in ( 140 ) with
equality and (b) reducing either @xmath or @xmath will cause the bound
in ( 140 ) to be violated.

#### 10.2 Storage-Repair Bandwidth Tradeoff

We have thus far, specified code parameters @xmath and asked what is the
largest possible value of file size @xmath . If however, we fix
parameters @xmath and ask instead what is the smallest values of @xmath
for which one can hope to achieve ( 140 ), it turns out as might be
evident from the form of the summands on the RHS of ( 140 ), that there
are several pairs @xmath for which equality holds in ( 140 ). In other
words, there are different flavors of optimality. For a given file size
@xmath , the storage overhead and normalized repair bandwidth are given
respectively by @xmath and @xmath . Thus @xmath reflects the amount of
storage overhead while @xmath determines the normalized repair
bandwidth. For fixed @xmath there are several pairs @xmath for which
equality holds in ( 140 ). These pairs represent a tradeoff between
storage overhead on the one hand and normalized repair bandwidth on the
other as can be seen from the example plot in Fig: 30 . Clearly, the
smallest value of @xmath for which the equality can hold in ( 140 ) is
given by @xmath . Given @xmath , the smallest permissible value of
@xmath is given by @xmath . This represents the minimum storage
regeneration point and codes achieving ( 140 ) with @xmath and @xmath
are known as minimum storage regenerating (MSR) codes. At the other end
of the tradeoff, we have the minimum bandwidth regenerating (MBR) code
whose associated @xmath values are given by @xmath , @xmath .

###### Remark 10.

Since a regenerating code can tolerate @xmath erasures by the data
collection property, it follows that the minimum Hamming weight @xmath
of a regenerating code must satisfy @xmath . By the Singleton bound, the
largest size @xmath of a code of block length @xmath and minimum
distance @xmath is given by @xmath , where @xmath is the size of
alphabet of the code. Thus @xmath in the case of regenerating code and
it follows therefore that size @xmath of a regenerating code must
satisfy @xmath , or equivalently @xmath or @xmath . But @xmath in the
case of MSR code and it follows that an MSR code is an MDS code over a
vector alphabet. Such codes also go by the name MDS array code.

From a practical perspective, exact-repair regenerating codes are easier
to implement as the contents of the @xmath nodes in operation do not
change with time. Partly for this reason and partly for reasons of
tractability, with few exceptions, most constructions of regenerating
codes belong to the class of exact-repair regenerating codes. Examples
of functional-repair regenerating code include the @xmath construction
in [ 112 ] as well as the construction in [ 59 ] . Early constructions
of regenerating codes focused on the two extreme points of the
storage-repair bandwidth (S-RB) tradeoff, namely the MSR, MBR points.
The storage industry places a premium on low storage overhead. This is
not too surprising, given the vast amount of data, running into the
exa-bytes, stored in today’s data centers. In this connection, we note
that the maximum rate of an MBR code is given by:

  -- -------- --
     @xmath   
  -- -------- --

which can be shown to be upper bounded by @xmath which is achieved when
@xmath . This makes MSR codes of greater practical interest when
minimization of storage overhead is of primary interest.

#### 10.3 MSR Codes

An @xmath MSR code is an @xmath MDS codes over the vector alphabet
@xmath satisfying the additional constraint that a failed node can be
repaired by contacting @xmath helper nodes, while downloading @xmath
symbol over @xmath from each helper node. In this case, we have @xmath
of message symbls over @xmath encoded by an MSR code. Thus, MSR codes
are characterized by the parameter set

  -- -------- --
     @xmath   
  -- -------- --

where

-   @xmath is the underlying finite field,

-   @xmath is the number of code symbols @xmath each stored on a
    distinct node or storage unit and

-   each code symbol @xmath is an element of @xmath .

As we have already seen, the number @xmath of symbols downloaded from
each helper node in an MSR code is given by

  -- -------- --
     @xmath   
  -- -------- --

As we have seen, each code symbol @xmath is typically stored on a
distinct node. Thus the index @xmath of a code symbol is synonymous with
the index of the node upon which that code symbol is stored. Throughout
this chapter, we will focus on a linear MSR code i.e., the encoding is
done by: @xmath where @xmath is an @xmath generator matrix over @xmath
and @xmath is a @xmath message vector over @xmath comprising of @xmath
message symbols encoded by the MSR code.

##### 10.3.1 Linear Repair

Throughout this chapter, we will assume linear repair of the failed
node. By linear repair, we mean that the @xmath symbols passed on from a
helper node @xmath to the replacement of a failed node @xmath are
obtained through a linear transformation:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is an @xmath matrix over @xmath . The matrix @xmath is
called a repair matrix. While the matrix @xmath could potentially be a
function of which nodes are participating in the repair of failed node
@xmath , this chapter is mostly concerned with the case @xmath , in
which case, all the remaining @xmath nodes participate as helper nodes.
As a result, when @xmath , the input to the replacement of failed node
@xmath is the set:

  -- -------- --
     @xmath   
  -- -------- --

By linear repair of a node @xmath we also mean: the code symbol @xmath
where @xmath is a deterministic linear function. We refer to an MSR code
as an optimal access MSR code if the matrix @xmath has rows picked from
the standard basis @xmath for @xmath ., i.e., the symbols over @xmath
downloaded for the repair of a failed node from node @xmath are simply a
subset of size @xmath , of the @xmath components of the vector @xmath .
This property of repair is also termed as as help-by-transfer repair.
When @xmath @xmath , we say repair matrices are independent of
helper-node index @xmath (or constant repair matrix case). Now lets
consider the MSR code for any @xmath . Let, @xmath be @xmath the repair
matrix where @xmath is downloaded for the repair of the node @xmath when
the helper nodes ( @xmath nodes from which data is downloaded for the
repair of node @xmath ) belong to the set @xmath . As a result, the
input to the replacement of failed node @xmath is the set:

  -- -------- --
     @xmath   
  -- -------- --

for helper nodes in the set @xmath such that @xmath . Here also: the
code symbol @xmath where @xmath is a deterministic linear function. When
@xmath , we say that repair matrices are independent of identity of
remaining helper nodes. Similar to @xmath case, the term optimal access
MSR code or repair by help-by-transfer for any @xmath means that the
rows of @xmath are picked from standard basis @xmath of @xmath . We drop
the superscript @xmath in @xmath when @xmath . An open problem in the
literature on regenerating codes is that of determining the smallest
value of sub-packetization level @xmath of an optimal-access
(equivalently, help-by-transfer) MSR code, given the parameters @xmath .
This question is addressed in [ 51 ] , where a lower bound on @xmath is
given for the case of a regenerating code that is MDS and where only the
systematic nodes are repaired in help-by-transfer fashion with minimum
repair bandwidth. In the literature these codes are often referred to as
optimal access MSR codes with systematic node repair. The authors of [
51 ] establish that:

  -- -------- --
     @xmath   
  -- -------- --

in the case of an optimal access MSR code with systematic node repair.
In a slightly different direction, lower bounds are established in [ 52
] on the value of @xmath in a general MSR code that does not necessarily
possess the help-by-transfer repair property. In [ 52 ] it is
established that:

  -- -------- --
     @xmath   
  -- -------- --

while more recently, in [ 53 ] the authors prove that:

  -- -------- --
     @xmath   
  -- -------- --

##### 10.3.2 Desirable Properties of MSR Code

Desirable properties of MSR codes are high rate, low field size, low
sub-packetization, repair property for all nodes (All node repair
property), help by transfer repair of failed node and optimal update
(optimal update is not defined here. Please see [ 51 ] .). The
properties mentioned above such as high rate, low field size, low
sub-packetization, repair property for all nodes are clearly desirable.
Help-by-Transfer repair is useful because a node may not have
computational power on its own and hence it is desirable that repair
only requires nodes to access its data and transmit it. In this chapter,
we present new results on lower bounds on sub-packetization of MSR codes
with help-by-transfer repair property.

##### 10.3.3 Overview of Existing Constructions of MSR Code

The existing constructions of MSR codes are summarized in the Table 3 .

There are several known MSR constructions. In [ 112 , 113 ] authors
provide low rate constructions with small sub-packetization and small
field size. The product matrix construction presented in [ 114 ] is for
any @xmath and low sub-packetization and low field size. In [ 115 ] ,
the authors provide a high-rate MSR construction using Hadamard designs
for systematic node repair. In [ 116 ] , high-rate systematic node
repair MSR codes called Zigzag codes were constructed for @xmath with
help-by-transfer repair property. These codes however had large field
size and sub-packetization that is exponential in in @xmath but provided
low field size for @xmath . This construction was extended in [ 117 ] to
enable the repair of all nodes. The existence of MSR codes for any value
of @xmath as @xmath tends to infinity is shown in [ 120 ] . In [ 118 ]
authors provided high rate construction for any @xmath but with large
sub-packetization and small field size with help-by-transfer repair
property. Construction of MSR codes with small sub-packetization with
small field size with help-by-transfer repair property is presented in [
65 ] , [ 66 ] , [ 119 ] . As we have seen already, in [ 51 ] , [ 52 ] ,
[ 53 ] lower bounds for sub-packetization( @xmath ) were presented. In [
51 ] a lower bound @xmath for the special case of an optimal-access MSR
code with systematic node repair was provided. This bound was recently
improved by Balaji et al. in [ 121 ] (These lower bounds on
sub-packetization are presented in this chapter. A summary of lower
bounds presented in this chapter is in Table 4 .). Results in Table 4 ,
proves sub-packetization-optimality of the explicit codes provided in [
65 ] , [ 66 ] , [ 119 ] for @xmath and the non-explicit code in [ 122 ]
for @xmath . Though the literature contains prior optimal access
constructions for @xmath , the resultant codes were either non-explicit
[ 122 ] , or else have large sub-packetization [ 118 ] , or are of high
field size [ 123 ] . In the present chapter, we also give a brief
overview of our optimal-access MSR codes that have optimal
sub-packetization for any @xmath and which can be constructed using a
field of size @xmath .

###### Comparison of our Lower Bound on @xmath with Exisitng Code
Constructions:

1.  When @xmath , our bound on @xmath (Theorem 11.1 ) for optimal access
    MSR code with @xmath becomes:

      -- -------- --
         @xmath   
      -- -------- --

    For @xmath , this reduces to @xmath . The latter lower bound on
    @xmath is achievable by the constructions in [ 65 , 66 ] . Hence our
    lower bound on @xmath is tight. Although our bound on @xmath is
    shown to be tight only for @xmath not a multiple of @xmath , the
    lower bound is valid for all parameters (with @xmath ) and when
    @xmath divides @xmath .

2.  Our bound on @xmath (Corollary 12.1 ) for MDS code with optimal
    access repair (repair with minimum repair bandwidth and
    help-by-transfer repair) for a failed node when it belongs to a
    fixed set of @xmath nodes (see Section 12 ) is:

      -- -------- --
         @xmath   
      -- -------- --

    The above bound for @xmath becomes @xmath . This lower bound is
    achieved by the construction given in [ 119 ] . Hence our lower
    bound on @xmath for the repair of @xmath nodes is tight.

3.  The constructions in [ 65 , 66 ] have repair matrices that are
    independent of the helper node index @xmath i.e., @xmath and has
    sub-packetization @xmath which achieves our lower bound on @xmath
    (Corollary 11.3 ) under the assumption that @xmath for an optimal
    access MSR code with @xmath .

4.  Our bound given in Corollary 11.4 is achieved by construction in [
    122 ] when @xmath does not divide @xmath under the assumption @xmath
    .

Hence our lower bounds on @xmath are tight for four cases.

### 11 Bounds on Sub-Packetization Level of an Optimal Access MSR Code

In this section, we derive three lower bounds on sub-packetization of
optimal access MSR codes for three different cases. First, we derive a
lower bound on sub-packetization of optimal access MSR codes for @xmath
case with no assumptions. Second, we derive a more tighter lower bound
on sub-packetization of optimal access MSR codes for @xmath case with
assumptions on repair matrices. Third, we derive a lower bound on
sub-packetization of MSR codes for @xmath case with assumptions on
repair matrices. Tightness of each of these bounds is discussed already.
The proof of the last two bounds follows as a corollary of the first
bound. The first bound is proved by analysing the intersection of row
space of repair matrices.

#### 11.1 Notation

We adopt the following notation throughout the chapter.

1.  Given a matrix @xmath , we use @xmath to refer to the row space of
    the matrix @xmath ,

2.  Given a subspace @xmath and a matrix @xmath , by @xmath we will mean
    the subspace @xmath obtained through transformation of @xmath by
    @xmath .

    -   Thus for example, @xmath will indicate the subspace obtained by
        transforming the intersection subspace @xmath through right
        multiplication by @xmath .

#### 11.2 Improved Lower Bound on Sub-packetization of an Optimal Access
MSR Code with @xmath

###### Theorem 11.1.

(Sub-packetization Bound) : Let @xmath be a linear optimal access MSR
code having parameter set

  -- -------- --
     @xmath   
  -- -------- --

with @xmath and linear repair for all @xmath nodes. Let @xmath . Then we
must have:

  -- -------- --
     @xmath   
  -- -------- --

The proof of the theorem will make use of Lemma 11.2 below. We begin
with some helpful notations. We will use the indices (two disjoint
subsets of @xmath ):

  -- -------- --
     @xmath   
  -- -------- --

to denote the @xmath nodes in the network over which the code symbols
are stored (Note that the code symbol @xmath is stored in node @xmath
.). Let @xmath , be an integer and set

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

Note that our choice of @xmath ensures that neither @xmath nor @xmath is
empty. We refer to the row space of @xmath as a repair subspace.

###### Lemma 11.2.

(Repair Subspace Intersection) : For the code @xmath which is a linear
optimal access MSR code with @xmath and linear repair for all @xmath
nodes. We must have:

  -- -------- -- -------
     @xmath      (141)
  -- -------- -- -------

where @xmath are as defined above in this section and @xmath is an
arbitrary node in @xmath . Furthermore, @xmath is the same for all
@xmath .

###### Proof.

###### Invariance of @xmath-fold Intersection of Repair Subspaces
Contributed by a Parity Node

Let us consider the nodes in @xmath as systematic nodes and nodes in
@xmath as parity nodes. Note that the sets @xmath are pairwise disjoint
and are arbitrary subsets of @xmath , under the size restrictions @xmath
and @xmath . First we prove that @xmath is the same for all @xmath .
Note that @xmath is the row space of the repair matrix carrying repair
information from helper (parity) node @xmath to the replacement of the
failed node @xmath . Thus we are seeking to prove that the @xmath -fold
intersection of the @xmath subspaces @xmath obtained by varying the
failed node @xmath is the same, regardless of the parity node @xmath
from which the helper data originates. To show this, consider a
generator matrix @xmath for the code @xmath in which the nodes of @xmath
are the parity nodes and the nodes in @xmath are the systematic nodes.
Then @xmath will take on the form:

  -- -------- -- -------
     @xmath      (142)
  -- -------- -- -------

Wolog the generator matrix assumes an ordering of the nodes in which the
first @xmath nodes in @xmath (node @xmath in @xmath correspond to rows
@xmath ) correspond respectively to @xmath and the remaining @xmath
nodes in @xmath to @xmath in the same order. A codeword is formed by
@xmath where @xmath is the vector of @xmath message symbols over @xmath
encoded by the MSR code and the index of code symbols in the codeword is
according to the nodes in @xmath i.e., the codeword will be of the form
@xmath .

By the interference-alignment conditions [ 112 ] , [ 51 ] applied to the
repair of a systematic node @xmath (Fig 31 ), we obtain (see Lemma .2 in
the Appendix, for a more complete discussion on interference alignment
equations given below):

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (143)
  -- -------- -------- -------- -- -------

Equation ( 143 ) and Lemma .1 (in Appendix) implies (as @xmath are
invertible for all @xmath ) that for every pair @xmath :

  -- -------- -- -------
     @xmath      (144)
  -- -------- -- -------

It follows then from the non-singularity of the matrices @xmath and
equation ( 144 ), that @xmath is the same for all @xmath . It remains to
prove the main inequality ( 141 ).

###### @xmath-fold Intersection of Repair Subspaces

We proceed similarly in the case of an @xmath -fold intersection,
replacing @xmath by @xmath in ( 144 ). We will then obtain:

  -- -------- -- -------
     @xmath      (145)
  -- -------- -- -------

###### Relating @xmath-fold and @xmath-fold intersections

Next consider the repair of the node @xmath . Then from the full-rank
condition of node repair, (see Lemma .2 in the Appendix), we must have
that

  -- -- -------- -------- -- -------
        @xmath   @xmath      (146)
  -- -- -------- -------- -- -------

It follows as a consequence, that

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (147)
  -- -------- -------- -------- -- -------

and hence, for every @xmath , we must have that

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (148)
  -- -------- -------- -------- -- -------

It follows from ( 145 ) that for any @xmath and all @xmath :

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (149)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

As a consequence of ( 148 ) and ( 149 ) we can make the stronger
assertion:

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (150)
  -- -------- -------- -------- -- -------

Since the @xmath are nonsingular, this allows us to conclude that:

  -- -------- -- -------
     @xmath      (151)
  -- -------- -- -------

However, since @xmath is an arbitrary node in @xmath , this can be
rewritten in the form:

  -- -------- -- -------
     @xmath      (152)
  -- -------- -- -------

for any @xmath , which is precisely the desired equation ( 141 ). ∎

###### Proof.

(Proof of Theorem 11.1 )

1.   Invariance of Repair Matrices to Choice of Generator Matrix We
    first observe that the repair matrices can be kept constant, even if
    the generator matrix of the code changes. This is because the repair
    matrices only depend upon relationships that hold among code symbols
    of any codeword in the code and are independent of the particular
    generator matrix used in encoding. In particular, the repair
    matrices are insensitive to the characterization of a particular
    node as being either a systematic or parity-check node.

2.   Implications for the Dimension of the Repair Subspace From Lemma
    11.2 , we have that

      -- -------- -- -------
         @xmath      (153)
      -- -------- -- -------

    and moreover that @xmath is the same for all @xmath . It follows
    that

      -- -- -- -------
               (154)
      -- -- -- -------

    i.e.,

      -- -------- -------- -------- -------- -------
         @xmath   @xmath   @xmath            (155)
                           @xmath   @xmath   
                           @xmath   @xmath   
                           @xmath   @xmath   
      -- -------- -------- -------- -------- -------

    Lemma 11.2 and its proof holds true for any set @xmath of size
    @xmath . As a result, equation ( 155 ), also holds for any set
    @xmath of size @xmath . We would like to extend the above inequality
    to hold even for the case when @xmath is of size @xmath . We get
    around the restriction on @xmath as follows. It will be convenient
    in the argument, to assume that @xmath does not contain the @xmath
    th node, i.e., @xmath and @xmath . Let us next suppose that @xmath
    and that @xmath is of size @xmath . We would then have:

      -- -------- -------- -------- -- -------
         @xmath   @xmath   @xmath      (156)
      -- -------- -------- -------- -- -------

    which is possible iff

      -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
      -- -------- -------- -------- --

    But this would imply that

      -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
      -- -------- -------- -------- --

    for any subset @xmath of nodes of size @xmath satisfying @xmath . We
    are therefore justified in extending the inequality in ( 155 ) to
    the case when @xmath is replaced by a subset @xmath whose size now
    ranges from @xmath to @xmath , i.e., we are justified in writing:

      -- -------- -------- -------- -- -------
         @xmath   @xmath   @xmath      (157)
      -- -------- -------- -------- -- -------

    for any @xmath , of size @xmath . A consequence of the inequality (
    157 ) is that

      -- -------- -------- -- --
         @xmath   @xmath      
      -- -------- -------- -- --

    implies that @xmath . In other words, a given non-zero vector can
    belong to at most @xmath repair subspaces among the repair
    subspaces: @xmath .

3.   Counting in a Bipartite Graph The remainder of the proof then
    follows the steps outlined in Tamo et. al [ 51 ] . We form a
    bipartite graph with @xmath (standard basis) as left nodes and
    @xmath as right nodes as shown in Fig. 32 . We declare that edge
    @xmath belongs to the edge set of this bipartite graph iff @xmath .
    Now since the MSR code is an optimal access code, the rows of each
    repair matrix @xmath must all be drawn from the set @xmath .

    Counting the number of edges of this bipartite graph in terms of
    node degrees on the left and the right, we obtain:

      -- -------- --
         @xmath   
         @xmath   
         @xmath   
         @xmath   
      -- -------- --

    Thus we have shown that if @xmath , we must have @xmath . It follows
    that

      -- -------- --
         @xmath   
      -- -------- --

∎

In the following, we will derive lower bounds on sub-packetization of
optimal access MSR code for @xmath and for arbitrary @xmath under
specific assumptions. These bounds are derived based on the proof of
Theorem 11.1 and hence we state them as corollaries.

#### 11.3 Sub-packetization Bound for @xmath and Constant Repair
Subspaces

In the following, we will derive a lower bound on sub-packetization of
an optimal access MSR code for @xmath under the assumption that the
repair matrix @xmath is independent of @xmath .

###### Corollary 11.3.

Given a linear optimal access @xmath MSR code @xmath with @xmath and
linear repair for all @xmath nodes with @xmath , @xmath (thus the repair
matrix @xmath is independent of @xmath ), we must have:

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

For a given subset @xmath of nodes, of size @xmath , let @xmath be a
second subset disjoint from @xmath (i.e., @xmath ), of size @xmath . In
this setting, the proof of Lemma 11.2 will go through for the pair of
subsets @xmath and we will obtain that for any @xmath :

  -- -------- --
     @xmath   
  -- -------- --

As before, we next extend the validity of the above inequality for any
@xmath by assuming that @xmath and following the same steps as in the
proof of Theorem 11.1 . Following this, we repeat the bipartite-graph
construction and subsequent counting argument as in the proof of Theorem
11.1 with one important difference. In the bipartitie graph constructed
here, there are @xmath nodes on the right (as opposed to @xmath ), with
@xmath as nodes in the right. The result then follows. ∎

#### 11.4 Sub-packetization Bound for Arbitrary @xmath and Repair
Subspaces that are Independent of the Choice of Helper Nodes

In the following, we will derive a lower bound on sub-packetization of
an optimal access MSR code for any @xmath under the assumption that the
repair matrix @xmath is independent of the choice of the remaining
@xmath helper nodes in @xmath .

###### Corollary 11.4.

Let @xmath be a linear optimal-access @xmath MSR code for some @xmath ,
@xmath , and linear repair for all @xmath nodes. We assume in addition,
that every node @xmath can be repaired by contacting a subset @xmath ,
@xmath of helper nodes in such a way that the repair matrix @xmath is
independent of the choice of the remaining @xmath helper nodes, i.e.,
@xmath , @xmath and @xmath , @xmath . Then we must have:

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Given a set @xmath , of nodes of size @xmath , @xmath , let us form a
set @xmath of size @xmath , such that @xmath , with @xmath , @xmath .
Let @xmath be any subset of @xmath such that @xmath . Next, consider the
punctured code obtained by restricting attention to the node subset
@xmath . The proof of Lemma 11.2 applied to the subset @xmath of nodes
will then go through and we will obtain:

  -- -------- --
     @xmath   
  -- -------- --

We then repeat the process of extending the above inequality for any
@xmath by assuming @xmath and following the proof of Theorem 11.1 .
Following this, we construct the bipartite graph as always and repeat
the counting argument employed in the proof of Theorem 11.1 with one
difference. On the right side of the bipartite graph, we now have the
@xmath repair matrices @xmath as the right nodes of the bipartite graph.
This will give us the desired result. We omit the details. ∎

### 12 Vector MDS Codes with Optimal Access Repair of @xmath Nodes:

In this section, we define optimal access MDS codes with repair of a
subset of @xmath nodes with each node repaired in help-by-transfer
method with optimal repair bandwidth. We then derive two lower bounds on
sub-packetization of such codes. One lower bound for @xmath and another
for @xmath . These two lower bounds on sub-packetization are derived
again based on the proof of Theorem 11.1 and hence stated as
corollaries.

#### 12.1 Optimal-Access MDS Codes

In this section, we derive results which correspond to an @xmath MDS
code over the vector alphabet @xmath , having the property that it can
repair the failure of any node in a subset of @xmath nodes where repair
of each particular node on failure, can be carried out using linear
operations, by uniformly downloading @xmath symbols from a collection of
@xmath helper nodes. Linear repair of any node among the @xmath nodes is
defined as in MSR code using repair matrices. It can be shown that even
here, the minimum amount @xmath of data download needed for repair of a
single failed node, is given by @xmath (minimum repair bandwidth). Our
objective here as well, is on lower bounds on the sub-packetization
level @xmath of an MDS code that can carry out repair of any node in a
subset of @xmath nodes, @xmath where each node is repaired (linear
repair) by help-by-transfer with minimum repair bandwidth (Such codes
will be referred to as optimal access MDS codes). We prove a lower bound
on @xmath of such codes for the case of @xmath in Corollary 12.1 . This
bound holds for any @xmath and is already shown to be tight in the
beginning, by comparing with a recent code constructions [ 119 ] . Also
provided, are bounds for the case @xmath . The @xmath case correspond to
the MSR code case which is described before.

#### 12.2 Bounds on Sub-Packetization Level of a Vector MDS Code with
Optimal Access Repair of @xmath Nodes

In this section, we will derive lower bounds on sub-packetization of
vector MDS codes which can repair any node in a subset of @xmath nodes
by help-transfer method with optimal repair bandwidth both for @xmath
case and any @xmath case.

##### 12.2.1 Sub-packetization Bound for Optimal Access MDS codes with
@xmath

In this section, we will derive a lower bound on sub-packetization of
optimal accees MDS codes for @xmath .

###### Corollary 12.1.

Let @xmath be a linear @xmath MDS code over the vector alphabet @xmath
containing a distinguished set @xmath of @xmath nodes. Each node in
@xmath can be repaired, through linear repair, by accessing and
downloading, precisely @xmath symbols over @xmath from each of the
remaining @xmath nodes. In other words, the repair of each node in
@xmath can be carried out through help-by-transfer with minimum repair
bandwidth with only linear operations. Then we must have

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

We remark that even in this setting, it is known that @xmath is the
minimum repair bandwidth needed to repair the nodes in @xmath , hence
the nodes in @xmath are those for which the repair is optimal. To prove
the corollary, consider a subset @xmath and @xmath and repeat the steps
used to prove Lemma 11.2 with this set @xmath and by choosing any @xmath
disjoint from @xmath with (wolog we assume @xmath ) @xmath and @xmath .
Since in the proof of Lemma 11.2 , we only consider equations regarding
repair of nodes in @xmath , the proof of Lemma 11.2 will go through. We
will arrive at the following analogue of ( 155 ):

  -- -------- -- -------
     @xmath      (158)
  -- -------- -- -------

For the case when @xmath , we again extend the range of validity of this
inequality to the case when @xmath is any subset of @xmath , by first
assuming that @xmath and proceeding as in the proof of Theorem 11.1
above. For the case when @xmath , no such extension is needed. We then
repeat the bipartite-graph-counting argument used in the proof of
Theorem 11.1 , with the difference that the number of nodes on the right
equals @xmath with @xmath as nodes in the right. This will then give us
the desired result. ∎

##### 12.2.2 Sub-packetization Bound for Optimal Access MDS codes for an
Arbitrary Number @xmath of Helper Nodes

In this section, we will derive a lower bound on sub-packetization of
optimal accees MDS codes for any @xmath .

###### Corollary 12.2.

Let @xmath be a linear @xmath MDS code over the vector alphabet @xmath
containing a distinguished set @xmath of @xmath nodes. Each node @xmath
in @xmath can be repaired, through linear repair, by accessing and
downloading, precisely @xmath symbols over @xmath from each of the
@xmath helper nodes where the @xmath helper nodes are any set of @xmath
nodes apart from the failed node @xmath . In other words, the repair of
each node in @xmath can be carried through help by transfer with minimum
repair bandwidth with only linear operations. Then we must have

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

To prove this, we simply restrict our attention to the (punctured) code
obtained by selecting a subset of nodes of size @xmath that includes the
subset @xmath for which optimal repair is possible. Applying the results
of Corollary 12.1 then gives us the desired result. ∎

An optimal access MDS code is said to have optimal sub-packetization if
the code has @xmath equal to the lower bound we derived.

### 13 Structure of a Vector MDS Code with Optimal Access Repair of
@xmath Nodes with Optimal Sub-packetization

In this section, we deduce the structure of optimal access MDS code with
optimal sub-packetization with repair matrices of the form @xmath and
show that the structure we deduced is also present in existing
constructions.

#### 13.1 Deducing the Structure of Optimal access MDS Code with Optimal
Sub-packetization with Repair matrices of the form @xmath

###### Theorem 13.1.

(Structure of Optimal access MDS Code with Optimal Sub-packetization
with Repair Matrices of the form @xmath ) : Let @xmath be a linear
@xmath MDS code over the vector alphabet @xmath . Let the nodes @xmath
be systematic and @xmath be parity nodes with @xmath . Let @xmath be a
set of @xmath nodes such that any node @xmath in @xmath can be repaired,
through linear repair, by accessing and downloading, precisely @xmath
symbols over @xmath from each of the set of @xmath helper nodes where
the @xmath helper nodes are any set of @xmath nodes apart from the
failed node @xmath . In other words, the repair of each node in @xmath
can be carried out through help-by-transfer with minimum repair
bandwidth with only linear operations. We assume in addition, that every
node @xmath can be repaired by contacting a subset @xmath , @xmath of
helper nodes in such a way that the repair matrix @xmath is dependent
only on the node @xmath , i.e., @xmath , @xmath , @xmath . Let @xmath
and @xmath . Let the generator matrix be of the form,

  -- -------- -- -------
     @xmath      (159)
  -- -------- -- -------

1.   Case 1: @xmath (repair with optimal bandwidth is possible for all
    systematic nodes) and @xmath divides @xmath :
    By Corollary 12.2 , @xmath . We assume optimal subpacketization
    i.e., @xmath (achieving the lower bound). Under the above
    conditions, wlog we must have:
    For @xmath , @xmath and assuming @xmath :

      -- -- -- -------
               (160)
      -- -- -- -------

    where

    1.   If @xmath then the row vectors @xmath will be in rows @xmath of
        @xmath respectively and the rest of @xmath rows of @xmath will
        be termed as @xmath ,

    2.   Disjointness of support of vectors with uniform cardinality of
        support: @xmath is a @xmath vector such that @xmath , @xmath and
        @xmath , @xmath ,

    3.   Same Support independent of @xmath : any given @xmath , @xmath
        , @xmath ,

    4.   For any given @xmath , @xmath are a set of @xmath vectors such
        that they form the columns of generator matrix of an @xmath MDS
        code over @xmath ,

    5.  @xmath is a @xmath matrix such that each distinct row is a
        distinct standard basis vector from the set @xmath after getting
        scaled by some element from @xmath .

2.   Case 2: @xmath and @xmath :
    By Corollary 12.2 , @xmath . We assume optimal subpacketization
    i.e., @xmath (achieving the lower bound). Hence @xmath . Under the
    above conditions, wlog we must have:
    For @xmath , @xmath :

      -- -- -- -------
               (161)
      -- -- -- -------

    where @xmath is a @xmath matrix and @xmath is a @xmath matrix such
    that

      -- -------- -------- -------- -- -------
         @xmath   @xmath   @xmath      (162)
         @xmath   @xmath   @xmath      (163)
      -- -------- -------- -------- -- -------

    where @xmath is a @xmath diagonal matrix and @xmath is a @xmath
    diagonal matrix and @xmath are a set of @xmath vectors forming the
    columns of the generator matrix of an @xmath MDS code over @xmath .
    For @xmath , @xmath :

      -- -------- -- -------
         @xmath      (164)
      -- -------- -- -------

    where @xmath is a @xmath diagonal matrix such that for a fixed
    @xmath , @xmath is a @xmath matrix such that every square submatrix
    is invertible, where @xmath is the @xmath entry of the matrix @xmath
    .

###### Proof.

Proof is given in the Appendix Erasure Codes for Distributed Storage:
Tight Bounds and Matching Constructions . ∎

#### 13.2 A Coupled Layer Interpretation of Theorem 13.1

In this subsection we show evidence that the structure we deduced in
Theorem 13.1 is present in existing codes. We restrict our attention to
the case when @xmath and @xmath . From Theorem 13.1 , we get the general
form of @xmath . Since the generator matrix is systematic, we can
directly write down the parity-check matrix by inspection. In the
following, we show that this parity-check matrix can be written such
that the construction can be viewed to have a coupled-layer structure,
that is present in the Ye-Barg [ 65 ] construction (see [ 66 ] for
details). We do note however, that we are only considering the case
where @xmath nodes are repaired, not all the nodes as in Ye-Barg [ 65 ]
. For simplicity we illustrate the connection via an example. Let @xmath
, so that @xmath . Let @xmath and @xmath . From Theorem 13.1 , we obtain
that:

  -- -------- -- -------
     @xmath      (165)
  -- -------- -- -------

@xmath is a non zero scalar in @xmath . The other matrices can similarly
be written down leading to the expression below for the parity-check
matrix :

  -- -------- -- -------
     @xmath      (166)
  -- -------- -- -------

Upon substituting for the @xmath , we obtain:

  -- -------- --
     @xmath   
  -- -------- --

which in turn can be written in the form:

  -- -------- --
     @xmath   
  -- -------- --

The coupled nature of the construction is now apparent: the columns in
the matrix can be divided into three sections, each can be viewed as
corresponding to a different plane. While for the most part, each
parity-check equations runs over elements of a single plane, there are
exceptions and this corresponds to the coupling across planes.

### 14 Construction of an MSR code for @xmath

The following is a collaborative work with a person named Myna Vajha. In
this section we briefly describe a construction of optimal access MSR
code with optimal sub-packetization with @xmath with field size of
@xmath . Here we only we briefly give an intuitive outline. For complete
description of the construction please refer to [ 111 ] . Based on the
structure theorem for vector MDS codes in the last section we can
formulate a general form of parity check matrix of optimal access MSR
code with optimal sub-packetization but with help-by-transfer for all
nodes. The code symbols will be divided into several planes of code
symbols with parity check equations given such that each parity check
equation involves code symbols within a plane and some symbols out of
plane. The main property to prove is MDS property of the code. Since the
code is defined in terms of planes by recursive argument we can write a
general form of matrices we want to be non-singular for the MDS
property. An example submatrix to prove non-singular is given by :

  -- -- --
        
  -- -- --

for some matrices @xmath where @xmath is the variable indicating plane
index. We prove that these matrices are non-singular by formulating the
entries of @xmath as variables and finding the degree of a root of the
determinant polynomial i.e., the polynomial @xmath and by repeating this
we determine all the factors which involves a particular variable. For
more details please refer to [ 111 ] .

### 15 Summary and Contributions

In this chapter, we derived lower bounds on sub-packetization of MSR
codes and vector MDS codes with optimal access repair of a subset of
nodes for any @xmath . Some of our lower bounds were general and some of
them had assumptions on repair matrices. We compared our lower bounds
with existing constructions and showed the tightness of our lower bounds
on sub-packetization. We then deduced the structure of vector MDS codes
with optimal access repair of a subset of nodes for any @xmath with
optimal sub-packetization assuming a certain form of repair matrices
(this form of repair matrices that we assumed is present in a lot of
existing constructions) and showed evidence that this structure is
present in existing codes. We then briefly gave an intuitive outline of
a new construction of an optimal access MSR code with optimal
sub-packetization.

## Chapter \thechapter Row Spaces

###### Lemma .1.

Let @xmath be nonsingular @xmath matrices and @xmath be matrices of size
@xmath . Then if

  -- -------- -- -------- --
     @xmath      @xmath   
  -- -------- -- -------- --

we have

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

###### Proof.

For the case when @xmath is nonsingular, we have that:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

The result then follows from noting that:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

∎

## Chapter \thechapter Condition for Repair of a systematic Node

###### Lemma .2.

Let the linear @xmath MSR code @xmath be encoded in systematic form,
where nodes @xmath are the systematic nodes and nodes @xmath are the
parity nodes. Let the corresponding generator matrix @xmath be given by:

  -- -------- -- -------
     @xmath      (167)
  -- -------- -- -------

Let @xmath be the repair matrices associated to the repair of the
systematic node @xmath . Then we must have that for @xmath , @xmath :

  -- -------- -- -------
     @xmath      (168)
  -- -------- -- -------

and

  -- -------- -- -------
     @xmath      (169)
  -- -------- -- -------

###### Proof.

Let @xmath be the @xmath message symbols (each message symbol @xmath
being a @xmath row vector) encoded by the MSR code i.e., the resultant
codeword is given by @xmath . For the repair of systematic node @xmath ,
the data collected by the replacement node is given by:

  -- -------- -------- -- --
     @xmath   @xmath      
  -- -------- -------- -- --

Let

  -- -------- -------- -- --
     @xmath   @xmath      
  -- -------- -------- -- --

be the @xmath matrix used to derive the contents of the replacement node
@xmath , with each @xmath being an @xmath submatrix. Then we must have
that

  -- -- -------- -------- --
        @xmath   @xmath   
  -- -- -------- -------- --

Since this must hold for all data vectors @xmath , we can equate the
matrices that premultiply @xmath on the left, on both sides. If we carry
this out for @xmath , we will obtain that:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

which implies:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

which in turn, forces:

  -- -------- -- -------
     @xmath      (170)
  -- -------- -- -------

and

  -- -------- -- -------
     @xmath      (171)
  -- -------- -- -------

The above equation proves ( 169 ). For the case @xmath :

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

and

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (172)
  -- -------- -------- -------- -- -------

Equations ( 172 ) and ( 171 ) imply:

  -- -------- -- -------
     @xmath      (173)
  -- -------- -- -------

Given that @xmath for all @xmath and @xmath , this implies

  -- -------- -- -------
     @xmath      (174)
  -- -------- -- -------

The above equation proves ( 168 ). ∎

## Chapter \thechapter Proof of Theorem 13.1

###### Proof.

1.  Case @xmath : @xmath and @xmath divides @xmath :
    Let us recall the counting argument on bipartite graph implicit in
    the proof of Corollary 12.2 (which is based on the counting argument
    on bipartite graph in the proof of Corollary 12.1 ). Let @xmath be
    left nodes and @xmath be right nodes of the bipartite graph. Form an
    edge between @xmath and @xmath iff @xmath . Now count the edges on
    both sides. The degree of @xmath is atmost @xmath which is proved
    implicitly in the proof of Corollary 12.2 (which is clear from the
    extension of inequality ( 158 ) to more than @xmath nodes by
    assuming @xmath and the counting argument in the proof of Corollary
    12.1 by applying them to the punctured code with @xmath nodes
    containing @xmath .). The degree of @xmath is exactly @xmath . Hence
    counting and equating the number of edges from @xmath and @xmath ,
    for @xmath :

      -- -------- -- -------
         @xmath      (175)
         @xmath      (176)
      -- -------- -- -------

    By above inequality ( 176 ), since @xmath and @xmath is true as
    @xmath and @xmath , we have that degree of @xmath is exactly equal
    to @xmath . Let @xmath be the nodes to which node @xmath is
    connected. Let @xmath be the nodes to which node @xmath is
    connected. If nodes to which @xmath is connected is @xmath then we
    define @xmath . Since, @xmath , @xmath (follows from the proof of
    Corollary 12.2 and Corollary 12.1 by extending dimension inequality
    ( 158 ) for more than @xmath nodes by applying it to the punctured
    code with @xmath nodes containing @xmath as @xmath ), we have that
    for @xmath : @xmath . Hence @xmath . We must have for @xmath and
    @xmath , by applying interference alignment conditions (i.e., by
    applying equation ( 168 ) in the Appendix for the punctured code
    with @xmath nodes containing @xmath for the repair of node @xmath ):

      -- -------- -------- -------- -- -------
         @xmath   @xmath   @xmath      (177)
      -- -------- -------- -------- -- -------

    Equation ( 177 ) implies that for @xmath , @xmath such that @xmath
    and @xmath :

      -- -------- -------- -------- -- -------
         @xmath   @xmath   @xmath      
         @xmath   @xmath   @xmath      (178)
         @xmath   @xmath   @xmath      (179)
      -- -------- -------- -------- -- -------

    Let us fix @xmath such that @xmath . Throughout the rest of the
    proof, we use this @xmath and proof is applicable for any @xmath
    such that @xmath . For any given @xmath , equation ( 179 ) is true
    for all @xmath . Since @xmath (wlog let @xmath ), this proves the
    @xmath part of the matrix @xmath in equation ( 160 ).
    For @xmath :

      -- -------- -- -------
         @xmath      (180)
      -- -------- -- -------

    Hence for @xmath , @xmath , from equation @xmath with @xmath
    replaced by @xmath and from equation ( 180 ), we have :

      -- -------- --
         @xmath   
         @xmath   
      -- -------- --

    for some @xmath . Hence for @xmath , @xmath independent of @xmath .
    Hence this proves that @xmath , @xmath independent of @xmath . Let
    @xmath be a set such that @xmath and @xmath . By applying full rank
    condition of repair (i.e., by applying equation ( 169 ) in the
    Appendix for the punctured code with @xmath nodes containing @xmath
    for the repair of node @xmath ):

      -- -------- -------- -------- -- -------
         @xmath   @xmath   @xmath      (181)
      -- -------- -------- -------- -- -------

    Equation ( 181 ) implies:

      -- -------- --
         @xmath   
      -- -------- --

    but

      -- -------- --
         @xmath   
         @xmath   
         @xmath   
      -- -------- --

    Hence for @xmath , @xmath and independent of @xmath , @xmath . This
    proves that (Since @xmath is arbitrary) @xmath , @xmath and @xmath ,
    @xmath . We have also proved that for fixed @xmath , support of
    @xmath is same independent of @xmath . By equation ( 181 ):

      -- -------- -------- -------- -- -------
         @xmath   @xmath   @xmath      
                                       
         @xmath   @xmath   @xmath      (182)
      -- -------- -------- -------- -- -------

    For a fixed @xmath , from equation ( 182 ), since @xmath is an
    arbitrary subset of @xmath of cardinality @xmath , any subset of
    @xmath of cardinality @xmath must be of rank @xmath and we have
    already seen that for a fixed @xmath , @xmath is same for all @xmath
    and is of cardinality @xmath . Hence @xmath form a set of @xmath
    vectors with support of all @xmath vectors equal and of cardinality
    @xmath such that they form the columns of generator matrix of an
    @xmath MDS code over @xmath . This completes the proof of case
    @xmath .

2.  Case @xmath : @xmath and @xmath :
    We first note that wlog for @xmath , @xmath . This is because as
    @xmath and hence @xmath (follows from the proof of Corollary 12.2
    and Corollary 12.1 by applying the dimension inequality ( 158 ) to
    the punctured code with @xmath nodes containing @xmath ).
    We next look at interference alignment conditions. We must have for
    @xmath , @xmath , @xmath and @xmath (by applying equation ( 168 ) in
    the Appendix for the punctured code with @xmath nodes containing
    @xmath for the repair of node @xmath ):

      -- -------- -- -------
         @xmath      (183)
      -- -------- -- -------

    The above interference alignment proves the structure of @xmath and
    @xmath for @xmath . It also proves that @xmath is a diagonal matrix
    for @xmath . Let @xmath be a set such that @xmath and @xmath . By
    applying full rank conditions (i.e., applying equation ( 169 ) in
    the Appendix for the punctured code with @xmath nodes containing
    @xmath for the repair of node @xmath ): for @xmath :

      -- -------- -------- -------- -- -------
         @xmath   @xmath   @xmath      (184)
      -- -------- -------- -------- -- -------

    Hence any subset of @xmath vectors from @xmath must form a basis of
    @xmath which implies @xmath form the columns of generator matrix of
    an @xmath MDS code.
    We finally look at MDS property of code @xmath to conclude the
    proof. For MDS property of @xmath , we must have:
    For @xmath any matrices of the following form must be full rank.

      -- -------- -- -------
         @xmath      (185)
      -- -------- -- -------

    Applying the above condition for @xmath and observing that
    determinant of above matrix is the product of determinant of the
    matrices @xmath for @xmath , (as @xmath is a diagonal matrix) we
    have the condition that any @xmath sub matrix of the matrix @xmath
    must be invertible.

∎

## Chapter \thechapter Partial Maximal and Maximal Recoverable Codes

In this chapter, we discuss a sub-class of Locally Recoverable (LR)
codes (defined in Chapter Erasure Codes for Distributed Storage: Tight
Bounds and Matching Constructions ) called Maximal Recoverable (MR)
codes. MR codes of block length @xmath and dimension @xmath are a class
of sub-codes of a given code @xmath with block length @xmath and
dimension @xmath such that an MR code can correct all possible erasure
patterns which is not precluded by the code @xmath . We will refer to
the parity check matrix of @xmath as @xmath in this chapter. If @xmath
with @xmath , then MR codes coincide with MDS codes. When @xmath is an
LR code then MR codes are a subclass of LR codes. In this case, MR codes
are LR counterpart of MDS codes. Hence as with MDS codes, it is an
important problem to construct MR codes with low field size. In this
chapter, we will discuss MR codes with low field size. We also consider
a relaxation of MR codes called Partial Maximal Recoverable (PMR) codes.
Below is a summary of our results in this chapter.

###### Organization of the Chapter

The chapter is organized as follows. We begin by formally defining MR
codes in Section 1 and PMR codes in Section 2 . Section 2 and Section 3
presents results on PMR codes. Section 4 presents three constructions of
MR codes with low field size. Finally Section 5 , presents a summary of
this chapter.

###### Contributions

Contributions of the thesis on the topic of PMR and MR codes include:

1.  Introduction of a relaxation of MR codes called PMR codes. A general
    form of parity check matrix of PMR codes is given in Section 2.1 .
    This general structure is also applicable to MR codes. In Section
    2.2 , we present a simple, high-rate, low field size construction of
    PMR codes. Also provided is an approach for a general construction
    of PMR codes (Section 3 ) for parameters with slightly lower rate.

2.  Also contained in the chapter are three constructions of MR codes
    with improved parameters, primarily field size.

3.  The first construction of MR codes is for the special case of @xmath
    (Section 4.1 ). The code is obtained by puncturing codes constructed
    in [ 46 ] . The code constructed has field size better than the
    existing constructions in the rate regime @xmath although the
    construction gives codes for all rates in the rate regime @xmath .

4.  We next give two explicit constructions of MR codes (Section 4.2 and
    Section 4.3 ) with block length @xmath with field size of @xmath for
    the case when dimension @xmath .

### 1 Maximal Recoverable Codes

An @xmath MDS code can recover from any pattern of @xmath erasures.
Maximal Recoverable (MR) codes are codes that operate under some
pre-specified linearity constraints and which can recover from any
pattern of @xmath erasures that is not precluded by the pre-specified
linearity constraints imposed. In the context of locality, the
pre-specified constraints are of course, the constraints imposed for
satisfying the definition of an LR code.

###### Definition 12.

Let @xmath be a @xmath matrix over a finite field @xmath with @xmath
whose row space has @xmath nonzero vectors with respective support sets
@xmath . We view @xmath as the matrix that imposes constraints necessary
to satisfy the definition of an LR code. Let us define a subset @xmath
to be a @xmath -core with respect to @xmath if @xmath and @xmath . Then
with respect to @xmath , an MR code is an @xmath code @xmath over @xmath
possessing a @xmath generator matrix @xmath with @xmath satisfying the
property that @xmath and for any @xmath -core @xmath ,

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (186)
  -- -------- -------- -------- -- -------

###### Remark 11.

Let @xmath denote the parity-check matrix of the MR code, where @xmath
is an @xmath matrix representing the additional parity-checks imposed in
such a way that the code with parity check matrix @xmath satisfies the
requirements of an MR code. It could happen that the elements of @xmath
belong to a small base field @xmath and over that field it is not
possible to find a matrix @xmath which will result in an MR code. It
turns out that in such instances, one can always choose the elements of
@xmath to lie in a suitable extension field @xmath of @xmath , resulting
in an MR code over @xmath .

###### Remark 12.

The condition in ( 186 ) imposed on the @xmath -core subsets @xmath is
equivalent to the following condition: Let @xmath be such that @xmath ,
@xmath . Then @xmath is a generator matrix of an @xmath MDS code. This
follows since any @xmath columns of @xmath are required to be linearly
independent.

#### 1.1 General Construction with Exponential Field Size

Saying that @xmath is a @xmath -core is equivalent to saying that @xmath
is an information set since the @xmath underlying message symbols can be
uniquely recovered from the @xmath code symbols @xmath . From the
perspective of the parity check matrix @xmath , @xmath is a @xmath -core
if only if @xmath . This suggests a construction technique. Setting
@xmath as earlier, we regard the symbols in the @xmath matrix @xmath as
variables. We need to select @xmath such that any @xmath sub-matrix of
@xmath corresponding to columns indexed by the complement @xmath of a
@xmath -core, has nonzero determinant. Let @xmath be the polynomial in
the symbols of @xmath obtained by taking the product of these
determinants. Note that the definition of a @xmath -core ensures that
each of these determinants are non-zero polynomials. The product
polynomial is a polynomial in the entries (variables) of the matrix
@xmath and each variable appears with degree at most @xmath . By the
Combinatorial Nullstellensatz [ 106 ] , it follows that there is a field
of size @xmath such that this product of determinants can be made
nonzero. Thus a MR code always exists of field size @xmath . The
interest is of course, in explicit constructions of MR codes having low
field size @xmath . It is also possible to use linearized polynomials to
construct MR codes, but while this results in an explicit construction,
the field size is still in general, of exponential size.

#### 1.2 Partial MDS Codes

In the literature, the focus motivated by practical considerations, is
on the following subclass of MR code, also sometimes termed as Partial
MDS (P-MDS) codes.

###### Definition 13.

An @xmath MR code or partial MDS code over a finite field @xmath is
defined as an @xmath code over @xmath in which the @xmath code symbols
can be arranged as an array of @xmath code symbols in such a way that
each row in the array forms a @xmath MDS code and upon puncturing any
@xmath code symbols from each row of the array, the resulting code
becomes an @xmath MDS code.

A tabular listing of some constructions of @xmath MR codes or
partial-MDS codes appears in Table 5 .

In [ 131 ] , the authors characterize the weight enumerators and higher
support weights of an @xmath MR code. Throughout this chapter, a local
parity check refers to a code word in dual code of Hamming weight @xmath
.

### 2 Partial Maximal Recoverability

We have already seen in Chapter Erasure Codes for Distributed Storage:
Tight Bounds and Matching Constructions that the following upper bound
on the minimum distance of a code under a weaker notion called
information-symbol locality was derived in [ 8 ] :

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (187)
  -- -------- -------- -------- -- -------

The same bound also applies to codes with all-symbol locality and is
often (but not always) tight, see [ 28 ] for instance. We will be
constructing codes achieving this bound ( 187 ) along with further
constraints on erasure correction capabilities. In this chapter, we will
refer to an @xmath LR code with AS (All Symbol) locality (defined in
Chapter Erasure Codes for Distributed Storage: Tight Bounds and Matching
Constructions ) over a finite field @xmath with locality parameter
@xmath and minimum distance @xmath as an @xmath LR code over @xmath .
Given that the construction of MR codes having small field size is
challenging, we seek here to construct codes that satisfy a weaker
condition which we will refer to in this chapter as the partial maximal
recoverable (PMR) condition. Let @xmath be an @xmath LR code with @xmath
satisfying the bound in ( 187 ) with equality. Let @xmath denote the
support of distinct local parity checks such that @xmath (Recall that
the term local parity check refers to a codeword of weight @xmath in the
dual code). Wolog we asssume @xmath as otherwise we can remove @xmath
and relabel the sets again until it satisfies this condition. In the
context of PMR codes, an admissible puncturing pattern @xmath is one
which satisfy the condition:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

###### Definition 14.

An @xmath PMR code over a finite field @xmath is then defined simply as
an @xmath LR code @xmath over @xmath whose minimum distance @xmath
satisfies the bound in ( 187 ) with equality and which becomes an MDS
code upon puncturing the code @xmath in co-ordinates corresponding to
one admissible puncturing pattern @xmath i.e., the code @xmath is an MDS
code for one admissible puncturing pattern @xmath .

The parity-check matrix of a PMR code is characterized below. We assume
w.l.o.g. in the section below, that @xmath (can be ensured by symbol
re-ordering) is an admissible puncturing pattern.

#### 2.1 Characterizing @xmath for a PMR Code

###### Theorem 2.1.

Let @xmath be an @xmath PMR code over a finite field @xmath as defined
above which becomes MDS on puncturing at the co-ordinates corresponding
to the admissible puncturing pattern @xmath . Let @xmath for some
natural numbers @xmath . Then the parity check matrix @xmath of @xmath
(upto a paermutation of columns) can be written in the form:

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (188)
  -- -------- -------- -------- -- -------

where @xmath is a parity-check matrix of a @xmath MDS code and @xmath is
of the form:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

in which each @xmath is a vector of Hamming weight at most @xmath .

###### Proof.

Clearly, @xmath (upto a permutation of columns) can be written in the
form (where the first @xmath rows are codewords corresponding to the
support sets @xmath )

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

which can be transformed, upon row reduction to the form:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

For the code @xmath to be a PMR code it is necessary that upon
puncturing the first @xmath coordinates (corresponding to columns of the
identity matrix @xmath in the upper left), the code become an MDS code.
But since the dual code of a punctured code is the shortened code in the
same coordinates, it follows that @xmath must be a parity-check matrix
of an MDS code. ∎

#### 2.2 A Simple Parity-Splitting Construction for a PMR Code when
@xmath

Let,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

for some natural numbers @xmath . Assume that @xmath . Let @xmath be a
@xmath parity-check matrix of a @xmath MDS code. Let @xmath be the last
row of @xmath and @xmath be @xmath with the last row deleted, i.e.,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

In the construction, we will require that @xmath also be a parity-check
matrix of an MDS code and set

  -- -------- -- -------
     @xmath      (189)
  -- -------- -- -------

For example, this is the case when @xmath is either a Cauchy or a
Vandermonde matrix. Let @xmath be the @xmath contiguous component @xmath
vectors of @xmath defined through

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (190)
  -- -------- -------- -------- -- -------

Let @xmath be given by

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (191)
  -- -------- -------- -------- -- -------

###### Theorem 2.2 (Parity-Splitting Construction).

The @xmath code @xmath having parity-check matrix @xmath given by

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

with @xmath and @xmath as defined by equations ( 189 ),( 191 ) and
@xmath , is a @xmath PMR code and hence with minimum distance @xmath
achieving the bound

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

###### Proof.

Since on puncturing the first @xmath co-ordinates the code becomes MDS
and the rows corresponding to @xmath gives the all symbol locality
property to the code, it is enough to show that any @xmath columns of
@xmath are linearly independent. From the structure of @xmath and the
properties of the matrix @xmath , it is not hard to see that it suffices
to show that any @xmath columns of

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

are linearly independent. But the rowspace of @xmath contains the vector
@xmath , hence it suffices to show that any @xmath columns of

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

are linearly independent, but this is clearly the case, since @xmath is
the parity-check matrix of an MDS code having redundancy @xmath . ∎

In the above construction @xmath represents the number of “global”
parity checks imposed on top of @xmath “local” parity checks where the
@xmath local parity checks refers to the @xmath codewords in the dual
code which are in the rows of @xmath corresponding to @xmath support
sets @xmath .

###### Remark 13.

The construction gives rise to @xmath LR code which is also a PMR code
over a field size of at most @xmath (small field size) and also, high
rate:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

### 3 A General Approach to PMR Construction

We attempt to handle the general case

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

in this section and outline one approach. At this time, we are only able
to provide constructions for selected parameters with @xmath and field
size that is cubic in the block length of the code and hold out hope
that this construction can be generalized. Let,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

for some natural numbers @xmath with @xmath . Let @xmath be an @xmath LR
code with parity check matrix @xmath as given in equation ( 188 ) in
Theorem 2.1 and @xmath chosen to be a Vandermonde matrix and with
Hamming weight of all the rows of @xmath being exactly @xmath (i.e.,
Hamming Weight of @xmath ) and intersection of support of any two rows
of @xmath being an empty set (i.e., @xmath , @xmath ). In the following
we will derive conditions so that the code @xmath becomes a PMR code.
For @xmath to be a PMR code, the desired minimum distance code @xmath
can be shown to equal in this case,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

It follows that even the code on the right having parity-check matrix

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

must have the same value of @xmath and therefore, the sub matrix formed
by any @xmath columns of @xmath must have full rank. Let @xmath be the
column indices of a subset of @xmath columns of @xmath . Let @xmath have
non-empty intersection with the support of some @xmath rows @xmath of
@xmath . Let the intersection of @xmath with the support of row @xmath
be @xmath of size @xmath . Let wolog row @xmath refer to the @xmath th
row of @xmath , @xmath . The corresponding sub matrix formed by columns
of @xmath indexed by @xmath will then take on the form:

  -- -------- -- -------
     @xmath      (192)
  -- -------- -- -------

where @xmath is the polynomial whose evaluations provide the components
of the @xmath th row of @xmath , @xmath as indicated in equation ( 192
). Since we want this matrix to have full rank @xmath it must be that
the left null space of the matrix must be of dimension @xmath .
Computing the dimension of this null space is equivalent (by polynomial
interpolation formula) to computing the number of solutions @xmath to
the equation:

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (193)
  -- -------- -------- -------- -- -------

where @xmath is generic notation for a polynomial of degree @xmath . Let
us define

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

and note that each @xmath will in general, have degree @xmath . Consider
the matrix @xmath whose rows correspond to the coefficients of @xmath .

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

@xmath is a @xmath matrix and we want to calculate the number of
solutions @xmath such that @xmath has first @xmath compoenents equal to
@xmath to calculate number of solutions to equation ( 193 ). It follows
that the first @xmath columns of @xmath must have full rank so that the
dimension of solution space of equation ( 193 ) and hence the dimension
of left null space of the matrix in equation ( 192 ) is exactly @xmath .

#### 3.1 Restriction to the Case @xmath, i.e., @xmath

We now assume that @xmath so that @xmath and we need the first @xmath
columns of @xmath to have rank @xmath . We consider the @xmath sub
matrix made up of the first two rows and first two columns of @xmath .
The determinant of this @xmath upper-left matrix of @xmath is given by

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

where

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

This is equal to

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath and @xmath , @xmath , @xmath and @xmath for some prime power
@xmath with @xmath . Then this becomes:

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

with @xmath which will be nonzero if the minimum polynomial of @xmath
over @xmath has degree @xmath , unless all the coefficients are equal to
zero.

###### Numerical Evidence

Computer verification was carried out for the @xmath case for @xmath
over @xmath ( @xmath ) and @xmath over @xmath ( @xmath ) with @xmath
where @xmath is the primitive element of @xmath and @xmath respectively
for the two cases and @xmath is fifth and seventh root of unity
respectively for the two cases (the choice of fifth and seventh roots of
unity varies for each @xmath ). For both cases, it was found that the
elements @xmath never simultaneously vanished for all instances.

### 4 Maximal Recoverable Codes

In this section, we give three constructions of MR codes with low field
size. The first construction is a construction of @xmath MR code and the
rest of the two constructions correspond to MR codes with @xmath . The
last two constructions has field size of @xmath .

#### 4.1 A Coset-Based Construction with Locality @xmath

We now give a construction of @xmath MR code. Since this construction is
based on Construction @xmath in [ 46 ] of all-symbol LR codes, we
briefly review the construction @xmath here. This construction 1 in [ 46
] is already described in Chapter Erasure Codes for Distributed Storage:
Tight Bounds and Matching Constructions .

###### Construction 4.1.

[ 46 ] Let @xmath for some natural numbers @xmath and @xmath be a power
of a prime such that @xmath , for example, @xmath could equal @xmath and
@xmath . Let @xmath be a primitive element of @xmath and @xmath . Note
that @xmath is an element of order @xmath . Let @xmath be the group of
@xmath th roots of identity element of the field @xmath . Let

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

Note that @xmath are pairwise disjoint and @xmath and @xmath are cosets
of the group @xmath . Let @xmath . Note that the monomial @xmath
evaluates to a constant on elements of any set @xmath where the constant
depends only on @xmath . Let the set of @xmath message symbols be @xmath
. Let

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

where the second term is vacuous for @xmath , i.e., is not present when
@xmath . Let @xmath . Consider the code @xmath of block length @xmath
and dimension @xmath defined by the following encoding function @xmath :

  -- -------- --
     @xmath   
  -- -------- --

where @xmath indicate the code symbols forming the codeword @xmath .
Hence the message symbols @xmath are encoded by evaluating the
polynomial @xmath at the field elements in @xmath . The code @xmath is
an @xmath LR code over @xmath with @xmath satisfying ( 187 ) with
equality where the conditions of LR code are satisfied by a set of
@xmath local parity checks (i.e., @xmath codewords in the dual code of
weight @xmath ) with support sets covering [n]. The support of the
@xmath local parity check are the indices of those code symbols of the
codeword @xmath obtained by evaluating @xmath at elements of @xmath .

Note that the exponents @xmath in the monomial terms forming each
polynomial @xmath satisfy @xmath . It is this property that makes the
code @xmath an LR code. Our construction of @xmath MR code here is based
on Construction 4.1 corresponding to the code @xmath with parameters
given by @xmath so that @xmath and @xmath . Thus all @xmath local parity
checks have support of cardinality @xmath . Let us denote the algebraic
closure of @xmath by @xmath .

###### Theorem 4.2.

Given positive integers @xmath with @xmath and @xmath , Let @xmath be
the code over @xmath described in Construction 4.1 with parameters
@xmath @xmath for @xmath , a prime power with @xmath such that:

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (194)
  -- -------- -------- -------- -- -------

where

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

Then there exists an @xmath MR code @xmath over @xmath with block length
@xmath and dimension @xmath that is obtained from @xmath by puncturing
the code @xmath at the co-ordinates @xmath corresponding to code symbols
obtained by evaluating @xmath at a carefully selected set of @xmath
cosets (evaluating positions) given by @xmath i.e., @xmath where @xmath
.

###### Proof.

For proof please refer to the Appendix Erasure Codes for Distributed
Storage: Tight Bounds and Matching Constructions . ∎

###### Remark 14.

Although Theorem 4.2 , states that @xmath MR code @xmath over @xmath
exists, one can construct such codes by following the procedure given in
the proof of Theorem 4.2 . The proof of Theorem 4.2 is essentially a
greedy algorithm which exploits the structure of the code @xmath . Hence
can be interpreted as a construction of MR code.

###### Analysis of Field size:

###### Theorem 4.3.

Theorem 4.2 showing the existence of @xmath MR code @xmath over @xmath
with block length @xmath and dimension @xmath obtained by puncturing
@xmath is true with field size @xmath such that:

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (195)
  -- -------- -------- -------- -- -------

where:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

###### Proof.

Theorem 4.2 is clearly true with field size @xmath such that:

  -- -------- --
     @xmath   
  -- -------- --

Note that @xmath . Hence (the value of @xmath in each equation in the
following could be different):

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (196)
  -- -------- -------- -------- -- -------

For @xmath , from equation ( 196 ):

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (197)
  -- -------- -------- -------- -- -------

For @xmath , from equation ( 196 ) (in the following we assume that
maximum over a null set is @xmath ):

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (198)
  -- -------- -------- -------- -- -------

Using the inequality ( [ 132 ] ) @xmath and using equation ( 198 ):

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

For @xmath , equation ( 195 ) directly follows from the fact that for
@xmath , the code @xmath is a repetition code and hence an MR code. Note
that repetition code can be constructed over binary field but since here
we constructing by puncturing @xmath we give a field size of @xmath . ∎

The following conclusions are true as @xmath increases for a fixed
@xmath . Table 6 shows a comparison of field size of the code @xmath
given in Theorem 4.3 with existing constructions for @xmath i.e., @xmath
. From the Table 6 , it can be seen that the code @xmath given in
Theorem 4.3 has the smallest field size compared to all the existing
constructions for @xmath as it can be verfied that @xmath for @xmath .
For the range @xmath (rate regime of @xmath ), from ( 195 ), the code
@xmath has field size of at most @xmath . Hence our construction is not
that far off from the smallest known field size for @xmath MR codes in
the rate range @xmath also as from Table 6 , it can be seen that the
best known field size is @xmath for @xmath . Note that although the
field size of @xmath is exponential, there are no constructions in
literature in rate regime @xmath for any @xmath , which as block length
tends to infinity (for a fixed rate) gives a polynomial (in block
length) field size MR code.

#### 4.2 Explicit @xmath MR codes with field Size of @xmath

In this section we give an explicit construction of @xmath MR codes
using the structure of @xmath matrix given in Theorem 2.1 with @xmath
chosen as Vandermonde matrix. We choose co-efficients corresponding to
local parity checks from cosets of a subfield which will ensure
appropriate erasure correcting capability required for MR codes.

###### Theorem 4.4.

Let @xmath be positive integers and @xmath and @xmath , @xmath . Let,

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (199)
  -- -------- -------- -------- -- -------

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a primitive element of a finite field @xmath and @xmath
is a @xmath root of unity and @xmath is the smallest integer such that
@xmath with @xmath satisfying equation ( 200 ). The code @xmath with
parity check matrix @xmath (described in ( 199 )) is an @xmath MR code
over @xmath with block length @xmath and dimension @xmath with @xmath
satisfying:

  -- -------- -- -------
     @xmath      (200)
  -- -------- -- -------

and @xmath is the smallest integer satisfying the above inequality
(where @xmath roots of unity form a subfield of @xmath ).

###### Proof.

Since we need to show that we can correct any erasure pattern with two
random erasures at any positions with rest of the erasures such that
there is atmost one erasure in the support of each local parity check (a
local parity check refers to one of the codewords (or rows) in the first
@xmath rows of @xmath ), It is enough to check that any @xmath submatrix
of @xmath with @xmath columns having non zero intersection with support
of exactly @xmath local parity checks ( @xmath rows among the first
@xmath rows of @xmath ), is full rank. Since a single erasure in the
support of a local parity check can be corrected, its enough to consider
@xmath . We choose the @xmath roots of unity @xmath from @xmath to be a
subfield of @xmath . This can be done as @xmath with @xmath (Just take
the field @xmath and take @xmath to be its primitive element and go to
an finite extension to get @xmath satisfying the inequality @xmath ).
Note that all @xmath are distinct. If not @xmath but this can’t be true
as elements in both sides of the equation belong to different cosets of
@xmath roots of unity due to the condition @xmath . Based on the
discussion above its enough to check non singularity of following
matrices:
@xmath local parity check with @xmath erasures in it:

  -- -------- -------- -- --
     @xmath   @xmath      
  -- -------- -------- -- --

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

@xmath is a Vandermonde matrix (with rows permuted) and hence non
singular and its clear the @xmath is also non singular for any local
code @xmath and any erasure locations @xmath . The case of @xmath local
parity checks with @xmath erasure in the support of first local parity
check and @xmath erasure in the support of second local parity check
boils down to the above @xmath cases as the @xmath erasure in the
support of second local parity check can be corrected by locality.
Submatrix of @xmath corresponding to @xmath local parity checks with
each having @xmath erasures in it:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

The matrix @xmath is clearly non singular.

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

we do permuation of rows or columns and row and column operations (as
these do not change the singularity or non singularity of the matrix) on
@xmath :
we substitute @xmath in @xmath : wlog we assume @xmath and @xmath and
@xmath

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

multiply the non zero scalars @xmath to rows @xmath respectively as this
does not affect the non-singularity of the matrix. The matrix @xmath now
becomes

  -- -------- --
     @xmath   
  -- -------- --

multiply row 1 with @xmath and add it row @xmath as this won’t change
the determinant. The matrix now becomes:

  -- -------- --
     @xmath   
  -- -------- --

the determinant of the above matrix is:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

for some @xmath as @xmath roots of unity form a subfield.
@xmath , as @xmath because @xmath
@xmath as @xmath which implies @xmath . Hence for @xmath , @xmath which
can’t be true as @xmath is odd. Hence for @xmath to have zero
determinant :

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

for some @xmath which is not true as @xmath because @xmath (due to
condition @xmath given in the theorem). In otherwords both sides of the
equation belong to different cosets which can’t be true. Hence @xmath is
non - singular. The non singularity of @xmath can be checked similarly.
∎

#### 4.3 Construction of @xmath MR code with field size of @xmath based
on [1]

In [ 1 ] , the authors provide a construction for an @xmath MR code (the
code is referred to as a partial MDS code in [ 1 ] ). We present a
modification of this construction here. The modification essentially
amounts to a different choice of finite-field elements in the
construction of the parity check matrix given in [ 1 ] for the partial
MDS code. The modified parity-check matrix is provided below.

###### Theorem 4.5.

Let @xmath be positive integers. Let @xmath . Let,

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (201)
  -- -------- -------- -------- -- -------

where

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (202)
  -- -------- -------- -------- -- -------

and

  -- -------- -------- -- -- -------
     @xmath   @xmath         (203)
  -- -------- -------- -- -- -------

where, @xmath is a primitive element of a finite field @xmath and @xmath
is a @xmath th root of unity for any @xmath and hence @xmath divides
@xmath . The code @xmath with parity check matrix @xmath is a @xmath MR
code over @xmath with block length @xmath and dimension @xmath with
@xmath satisfying @xmath @xmath block length.

###### Proof.

Using the derivation of closed-form expression for the determinant given
in Lemma @xmath in [ 1 ] , it can be seen that the code @xmath is an
@xmath MR code. ∎

Note that the field size @xmath is dependent on @xmath only through
block length and the code @xmath is an MR code for any @xmath a prime
power such that @xmath and @xmath @xmath block length whereas the
construction in [ 1 ] has field size @xmath @xmath @xmath block length.
The construction given in [ 124 ] has field size of @xmath and the
construction given in [ 125 ] has field size of @xmath .

### 5 Summary and Contributions

In this chapter, we introduced a class of codes called PMR codes and
presented a general structure of parity check matrix of such codes and
used it to give explicit low field size, high rate construction of such
codes. We also presented a possible generalization of our construction
of PMR codes. We then presented three constructions of MR codes with low
field size. The first construction is a construction of @xmath MR code
which has low field size compared to existing constructions in
literature and the rest of the two constructions correspond to MR codes
with @xmath . The last two constructions had field size of @xmath .

## Chapter \thechapter Proof of Theorem 4.2

###### Proof.

The code @xmath has (optimum) maximum possible minimum distance with all
symbol locality @xmath [ 46 ] for the given @xmath . Hence it can be
seen that puncturing @xmath at co-ordinates corresponding to any number
of cosets in @xmath (puncturing local codes) without changing @xmath
will maintain the optimum minimum distance. Let @xmath be the algebraic
closure of @xmath . Throughout the proof whenever we say a pattern
@xmath or just @xmath , it refers to an admissible puncturing pattern
for an @xmath code (the code will be clear from the context) with all
symbol locality @xmath . Throughout the discussion any @xmath code
referred to are polynomial evaluation codes and we assume that the set
of all finite field elements (evaluation positions) say @xmath at which
we evaluate a message polynomial to form the @xmath code to be ordered.
If @xmath , it is also used to refer to @xmath . Hence @xmath is used to
indicate both indices of the co-ordinates as well as corresponding
finite field elements at which we evaluate.

###### Maximal Recoverability:

Let @xmath .
We denote an encoding (message) polynomial of @xmath by @xmath and let
@xmath . Let @xmath denote the cyclic group of cube roots of unity in
@xmath . Let @xmath be a primitive element in @xmath . If @xmath are the
roots of @xmath then it must satisfy (because co-efficients of monomials
@xmath in @xmath such that @xmath are @xmath ):

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

where @xmath refers to the @xmath th elementary symmetric function. Lets
denote the above set of conditions based on elementary symmetric
functions on @xmath by @xmath . If we have a MR code with block length
@xmath and dimension @xmath based on the theorem we are proving now
(assuming the theorem is true ) and let @xmath be the cosets of cube
roots of unity chosen for the evaluation positions for forming the
codeword of the @xmath MR code and if we puncture this @xmath code by a
pattern @xmath then for the resulting @xmath (assuming @xmath does not
change after puncturing) code to be MDS we need @xmath . Based on the
degree of @xmath (degree of @xmath is @xmath ), we know that @xmath .
Hence out of @xmath roots of @xmath , we want atmost @xmath distinct
roots to lie in @xmath for any admissible @xmath . In other words, its
enough if we choose @xmath cosets of cube roots of unity @xmath such
that for any @xmath which satisfies the condition @xmath , atmost only
@xmath distinct elements out of them will lie in @xmath for any @xmath .
Note that there is another set of conditions like @xmath , if @xmath is
of the form @xmath . If @xmath are the roots of @xmath then:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

where @xmath refers to the @xmath th elementary symmetric function. We
call the above set of conditions @xmath . Similar to above, we need to
make sure that for any @xmath which satisfies the condition @xmath ,
atmost only @xmath distinct elements out of them will lie in @xmath for
any @xmath . We repeat the following procedure parallely for the set of
conditions represented by @xmath as well. For brevity we only explain
choosing cosets based on satisfying the above set of conditions based on
@xmath . At the end of it, we account for these extra set of conditions
represented by @xmath by multiplying by a factor of @xmath in the
resulting constraint on field size. If @xmath then we multiply by
appropriate power of @xmath to bring the @xmath to the form @xmath and
make sure atmost @xmath roots of @xmath lie inside @xmath for any @xmath
. Note that this condition will also ensure that the dimension of a
@xmath length punctured code obtained by puncturing the @xmath code by a
pattern @xmath is @xmath for any @xmath . If not there are 2 distinct
non zero message polynomials @xmath which after evaluating at @xmath
cosets (evaluation positions) of the @xmath code yields the same
codeword after puncturing by a pattern @xmath to @xmath length. This
means @xmath is another non zero message or evaluation polynomial with
@xmath zeros in the chosen @xmath cosets but by the condition of
choosing cosets mentioned previously (as roots of @xmath satisfies
@xmath or @xmath ) there can be atmost @xmath distinct zeros in the
@xmath evaluation positions obtained after puncturing by @xmath . This
is a contradiction as @xmath (by the condition @xmath given in the
theorem). Hence if we choose @xmath cosets such that for any pattern
@xmath and any @xmath distinct elements @xmath from the @xmath cosets
after puncturing by @xmath , none of @xmath from @xmath such that @xmath
satisfies @xmath which are distinct from @xmath lie in the chosen @xmath
cosets after puncturing by @xmath then we are done. There is an
equivalent set of conditions based on @xmath . We do the following
procedures parallely for @xmath as well but we skip the description.

###### Proposition 1.

Let @xmath be a set of @xmath elements from @xmath satisfying @xmath and
@xmath contains @xmath for some @xmath then @xmath (set difference of
the sets @xmath and @xmath ) satisfies @xmath .

###### Proof.

Since @xmath satisfies @xmath , this implies @xmath for @xmath .

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

Hence,

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (204)
  -- -------- -------- -------- -- -------

For @xmath , @xmath as @xmath has only @xmath elements.
Hence,

  -- -------- --
     @xmath   
  -- -------- --

Hence,

  -- -------- --
     @xmath   
  -- -------- --

For @xmath ,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

Since, @xmath and @xmath , this implies that @xmath . By induction, if
we assume, @xmath then by equation ( 204 ) since @xmath , we have :
@xmath ( @xmath is the starting condition of the induction which we
already proved).
Hence @xmath satisfies @xmath . ∎

@xmath :
Its enough to choose @xmath cosets such that for any @xmath and any
@xmath (contained in the chosen @xmath cosets) which are distinct and
contains atmost one element from each coset, none of the @xmath from
@xmath such that @xmath satisfies @xmath , which are distinct from
@xmath lies in the chosen @xmath cosets after puncturing by @xmath for
any @xmath disjoint from @xmath . @xmath :
This is because if @xmath satisfying @xmath contains at least 2 element
from some coset @xmath for some @xmath , since the polynomial @xmath
restricted to any coset is a degree @xmath polynomial, the third element
from coset is also a root of @xmath . Hence the entire coset is
contained in @xmath and by similar reasoning @xmath can be written as
@xmath for some @xmath where @xmath contains at most one element from
each coset and satisfies @xmath by proposition @xmath . Now by the
property of the chosen cosets, we have that for any distinct @xmath from
the chosen @xmath cosets containing atmost one element from each coset,
any of @xmath @xmath which are distinct from @xmath such that @xmath
satisfies @xmath will not lie inside the chosen cosets after puncturing
by @xmath for any @xmath such that @xmath . Wlog this implies the chosen
cosets after puncturing by any @xmath can contain atmost only (writing
only distinct elements) @xmath among the @xmath elements @xmath . Hence
there can be atmost @xmath distinct roots out of @xmath roots inside the
chosen cosets after puncturing by any @xmath . Hence we are done. From
here we term a set of @xmath cosets satisfying the above claim, to be
satisfying @xmath .
We are going put another set of conditions @xmath on a set of @xmath
cosets. The necessity of this condition will be clear in the proof.
@xmath
A given set of @xmath cosets, is said to satisfy condition @xmath if,
For any @xmath and any @xmath (contained in the given @xmath cosets)
which are distinct and contains atmost one element from each of the
@xmath cosets, the matrix @xmath given by

  -- -------- --
     @xmath   
  -- -------- --

is non-singular, where @xmath . Furthermore, for any @xmath and any
@xmath (contained in the given @xmath cosets) which are distinct and
contains at most one element from each of @xmath cosets, the matrix
@xmath given by

  -- -------- --
     @xmath   
  -- -------- --

is non-singular, where @xmath . From here on we proceed to find a set of
@xmath cosets satisfying @xmath and @xmath . We proceed by choosing
@xmath new coset at each step inductively until we choose the required
set of @xmath cosets. At each step we select and add one coset to our
list and throw away a collection of cosets from the cosets not chosen.
The cosets are thrown away such that it is straight forward to pick a
coset from the cosets not thrown away so that the collection of chosen
cosets satisfy @xmath and @xmath . Let the cosets chosen upto @xmath
step (including step @xmath ) be @xmath and the cosets thrown away upto
@xmath step be @xmath and let the total collection of cosets in the
field @xmath be @xmath .

1.  Step 1: The first coset is chosen to be any coset. Hence @xmath
    consists of just the coset chosen. We don’t throw away any cosets at
    this step. Hence @xmath is empty. @xmath satisfies @xmath and @xmath
    trivially.

2.  Step 2: The second coset is also chosen to be any coset from @xmath
    . Hence @xmath consists of the @xmath chosen cosets.

    1.  @xmath :
        For @xmath , and for any @xmath distinct elements @xmath , one
        from each coset in @xmath , any @xmath such that @xmath cannot
        be distinct from @xmath and lie in any of the cosets in @xmath .
        If it does, wlog let @xmath and @xmath lie in same coset which
        is in @xmath then @xmath but every coset is a coset of cube
        roots of unity. Hence @xmath where X is the third element from
        the same coset as @xmath . Hence @xmath which implies @xmath but
        X is in the same coset as @xmath and @xmath is in the other
        coset in @xmath . Hence a contradiction. For @xmath , @xmath ,
        we need to pick 4 distinct elements, from distinct cosets but
        there are only 2 cosets in @xmath . Hence @xmath is satisfied.

    2.  @xmath :
        For @xmath , @xmath , hence non-singular.
        For @xmath , we need to pick @xmath and @xmath distinct elements
        from distinct cosets but there are only 2 cosets. Hence @xmath
        is satisfied.

    3.  @xmath :
        For every two distinct elements @xmath chosen one from each of
        the 2 cosets in @xmath , find the third element @xmath such that
        @xmath and throw away the coset in @xmath which contains it.
        Since @xmath satisfies @xmath , @xmath will either not lie in
        any coset in @xmath or won’t be distinct from @xmath . In the
        first case, we throw the coset and in the latter case, we don’t
        do anything. There are 3x3=9 possible summations @xmath but if
        @xmath then @xmath and @xmath is in the same coset as @xmath for
        any cube root of unity @xmath . Hence solutions for 9 possible
        summations @xmath lie in atmost 3 cosets and we throw away these
        3 cosets.

3.  Step @xmath (Assumng that a new coset is chosen at step @xmath and
    @xmath satifying @xmath is formed, we are now showing the cosets we
    are throwing away into @xmath at step @xmath ): Let @xmath and
    assume we have @xmath satifying @xmath .

    ###### @xmath: For every @xmath , Choose @xmath cosets (say @xmath )
    from the cosets in @xmath , and choose @xmath @xmath @xmath one from
    each of these @xmath cosets i.e., say @xmath , now find the set of
    all @xmath @xmath @xmath from @xmath such that @xmath and throw away
    all the cosets in which @xmath @xmath @xmath lies. Since @xmath
    satisfies @xmath , the elements in @xmath will either be not
    distinct from @xmath or elements in @xmath which are distinct from
    @xmath will lie in cosets outside @xmath . In the first case we do
    not do anything and in the latter case, we throw away any coset
    containing any of @xmath .
    To find the number of solutions @xmath such that
    @xmath , we solve for @xmath given @xmath . It can be seen that to
    satisfy @xmath ,…, @xmath , @xmath @xmath has to satsify a linear
    equation of the form
    @xmath @xmath

    Since @xmath satifies @xmath , @xmath is non singular and there is a
    unique solution, for @xmath which implies a unique solution for
    @xmath … @xmath because @xmath are the co-efficients of the
    polynomial with roots exactly equal to @xmath . Hence for a given
    distinct @xmath , from distinct cosets, there is a unique solution
    for @xmath such that @xmath @xmath . Hence its enough to throw away
    @xmath cosets which contain @xmath (unique solution). The above
    procedure is done for every choice of @xmath cosets from cosets in
    @xmath and every choice of @xmath from the chosen @xmath cosets such
    that one element is chosen from each coset. Hence the total number
    of cosets thrown away are atmost @xmath but if @xmath , @xmath put
    together satisfies @xmath then @xmath … @xmath , @xmath … @xmath
    (which doesn’t change the cosets in which @xmath lie for any cube
    root of unity @xmath ) also satisfies @xmath and this choice is
    unique as seen before. Hence out of @xmath choices for @xmath from a
    given chosen @xmath cosets, its enough to throw away cosets for
    @xmath choices of @xmath . Hence the total number of cosets thrown
    away are atmost @xmath . For every @xmath , Choose @xmath cosets
    (say @xmath ) from the cosets in @xmath , and choose @xmath one from
    each of these @xmath cosets i.e., say @xmath , now find the set of
    all @xmath from @xmath such that @xmath is singular. This @xmath
    cannot be in any coset in @xmath which does not contain @xmath as
    @xmath satisfies @xmath . If @xmath lies in the coset which contains
    any of @xmath , then we do not do anything. If @xmath lies outside
    @xmath , we throw away the coset. To find the number of solutions of
    @xmath for a given @xmath such that @xmath is singular, let @xmath
    and @xmath .
    @xmath @xmath @xmath

    @xmath

    The determinant of above matrix @xmath can be seen as a polynomial
    in @xmath and its degree is atmost @xmath . The constant term of
    this polynomial is the determinant of following matrix:
    @xmath

    The above matrix is equal to @xmath and is non-singular for @xmath
    since @xmath satisfies @xmath . Hence the @xmath as a polynomial in
    @xmath is a non-zero polynomial (has a non zero constant term), and
    since its degree is atmost @xmath , it can have atmost @xmath
    solutions for @xmath . Hence its enough to throw away these @xmath
    cosets containing these @xmath solutions.
    The above procedure is done for every choice of @xmath cosets from
    cosets in @xmath and every choice of @xmath from the chosen @xmath
    cosets such that one element is chosen from each coset. The number
    of cosets thrown away are atmost: @xmath . It can be seen that
    @xmath is a homogenous polynomial in @xmath @xmath @xmath and hence
    as before if for @xmath … @xmath @xmath , @xmath then for @xmath
    also @xmath . Hence its enough to throw away atmost: @xmath cosets.
    For @xmath , @xmath which is trivially non- singular and we don’t do
    anything. For @xmath , choose 3 cosets from @xmath and choose
    distinct @xmath one from each of these distinct cosets, now find the
    set of all @xmath such that @xmath is singular. This @xmath can’t be
    in any coset in @xmath which does not contain @xmath as @xmath
    satisfies @xmath . If @xmath lies in the coset which contains any of
    @xmath , then we don’t do anything. If @xmath lies outside @xmath ,
    we throw the coset. To find the number of solutions of @xmath for a
    given @xmath such that @xmath is singular,
    @xmath

    Given the chosen @xmath , the above expression for @xmath can be
    seen as a linear expression in @xmath . if @xmath then @xmath . Here
    @xmath constitutes the solution set for @xmath but @xmath also
    constitutes 3 solutions for the equation @xmath but there can be
    atmost 3 solutions for the equation @xmath . Hence @xmath which
    implies they all belong to same coset which is a contradiction.
    Hence either @xmath or @xmath which implies @xmath is a non zero
    polynomial in @xmath with degree atmost @xmath . Hence we can find
    the solution and throw away the coset containing it. The number of
    cosets thrown are atmost: @xmath but by similar argument as before
    we can see that the number of cosets thrown are atmost: @xmath . For
    every @xmath , Choose @xmath cosets (say @xmath ) from the cosets in
    @xmath , and choose @xmath one from each of these @xmath cosets
    i.e., say @xmath , now find the set of all @xmath from @xmath such
    that @xmath is singular. This @xmath cannot be in any coset in
    @xmath which does not contain @xmath as @xmath satisfies @xmath . If
    @xmath lies in the coset which contains any of @xmath , then we
    don’t do anything. If @xmath lies outside @xmath , we throw away the
    coset. To find the number of solutions of @xmath for a given @xmath
    such that @xmath singular,
    let @xmath and @xmath .
    @xmath

    @xmath

    The determinant of above matrix @xmath can be seen as a polynomial
    in @xmath and its degree is atmost @xmath . The constant term of
    this polynomial is the determinant of following matrix:
    @xmath (205)

    Now @xmath (because this is just the product of @xmath non zero
    elements) , since the determinant of the matrix:
    @xmath

    is non zero as @xmath satisfies @xmath and this matrix is equal to
    @xmath , we have that the determinant of the matrix mentioned in
    equation ( 205 ) corresponding to the constant term of the
    polynomial @xmath is also non zero. Hence the @xmath as a polynomial
    in @xmath is a non-zero polynomial (has a non zero constant term),
    and since its degree is atmost @xmath , it can have atmost @xmath
    solutions for @xmath . Hence its enough to throw away these @xmath
    cosets containing these @xmath solutions.
    The above procedure is done for every choice of @xmath cosets from
    cosets in @xmath and every choice of @xmath from the chosen @xmath
    cosets such that one element is chosen from each coset. The number
    of cosets thrown away are atmost: @xmath . It can be seen that
    @xmath is a homogenous polynomial in @xmath and hence as before if
    for @xmath , @xmath then for @xmath also @xmath . Hence its enough
    to throw away atmost: @xmath cosets.

4.  Step @xmath (only showing how to choose a new coset such that @xmath
    satisfies @xmath and the cosets thrown away at this step follows the
    same procedure indicated in previous point step @xmath ): Following
    the previous step, we want to select one more coset to form @xmath
    such that it satisfies @xmath :
    Choose any coset (say) @xmath from the collection @xmath . Hence
    @xmath . It can be easily shown that @xmath satisfies @xmath using
    the properties of @xmath and @xmath .

    1.  @xmath :
        For any @xmath , for any distinct @xmath from distinct cosets
        (say wlog @xmath and @xmath , @xmath ) from @xmath , find the
        set of all @xmath such that @xmath satisfies @xmath . The
        following argument is for each such @xmath . If some @xmath
        distinct from @xmath lies in a coset in @xmath , and @xmath for
        any @xmath , then this gives a contradiction because @xmath
        satisfies @xmath (gives contradiction if @xmath is in a coset
        distinct from @xmath ) and since @xmath , gives contradiction if
        @xmath as previous step implies @xmath . Hence its enough to
        consider the case where @xmath for some @xmath . Wlog let @xmath
        .
        If some @xmath distinct from @xmath lies in a coset in @xmath
        which is different from @xmath , then @xmath ,…, @xmath , @xmath
        ( @xmath elements), @xmath , @xmath ,…, @xmath , @xmath ,…,
        @xmath form a solution such that @xmath ,…, @xmath , @xmath ,
        @xmath , @xmath ,…, @xmath , @xmath ,…, @xmath satisfies @xmath
        . But by previous step we are throwing away all cosets
        containing @xmath , @xmath , …, @xmath , @xmath , …, @xmath as
        @xmath , …, @xmath , @xmath are distinct @xmath elements from
        distinct cosets in @xmath which implies @xmath (as @xmath )
        which is a contradiction to the fact that @xmath .
        If some @xmath distinct from @xmath lies in a coset from @xmath
        , if @xmath , @xmath , then there exists @xmath and @xmath such
        that @xmath (this is because @xmath is a degree 1 polynomial
        when restricted to a coset. Hence if there are 2 distinct zeros
        of @xmath in a coset then @xmath restricted to the coset must
        be 0. Hence the entire coset is subset of roots of @xmath ).
        Hence @xmath . By proposition 1, @xmath satisfies @xmath and
        @xmath is a set of distinct @xmath elements from distinct cosets
        in @xmath and hence the cosets in which @xmath lies will be
        thrown away in previous step and @xmath . Hence @xmath which is
        a contradiction to the fact @xmath .
        If some @xmath ( @xmath ) distinct from @xmath lies in a coset
        from @xmath , and if @xmath , then there exists @xmath and
        @xmath such that @xmath (for the same as explained before).
        Hence @xmath . By proposition 1, @xmath satisfies @xmath .
        @xmath is a set of distinct @xmath elements from distinct cosets
        in @xmath and @xmath is in a coset other than @xmath in @xmath
        and @xmath satisfies @xmath which is a contradiction to the fact
        @xmath satisfies @xmath .

    2.  @xmath :
        For any @xmath , for any distinct @xmath from distinct cosets
        (say wlog @xmath i.e., @xmath , @xmath ) from @xmath , find the
        set of all @xmath such that @xmath makes @xmath singular. The
        following argument is for each such @xmath . If @xmath lie in a
        coset in @xmath and in a coset in @xmath , we don’t care as this
        doesn’t violate @xmath . If @xmath lie in a coset in @xmath and
        in a coset distinct from in @xmath , it gives a contradiction as
        @xmath satisfies @xmath (gives contradiction if none of @xmath
        lie in @xmath as this means we have 2A distinct elements from
        distinct cosets from @xmath making @xmath singular) and since
        @xmath , gives contradiction if one of @xmath lie in @xmath as
        this would imply @xmath .
        For any @xmath , for any distinct @xmath from distinct cosets
        (say wlog @xmath i.e., @xmath , @xmath ) from @xmath , find the
        set of all @xmath such that @xmath makes @xmath singular. The
        following argument is for each such @xmath @xmath . If @xmath
        lie in a coset in @xmath and in a coset in @xmath , we don’t
        care as this doesn’t violate @xmath . If @xmath lie in a coset
        in @xmath and in a coset distinct from in @xmath , it gives a
        contradiction as @xmath satisfies @xmath (gives contradiction if
        none of @xmath lie in @xmath as this means we have @xmath
        distinct elements from distinct cosets from @xmath making @xmath
        singular) and since @xmath , gives contradiction if one of
        @xmath lie in @xmath as this would imply @xmath .

5.  For @xmath : The argument for throwing cosets is similar to the
    above arguments (step @xmath ) except that we skip the parts where
    it becomes vacuous. The procedure for selecting new coset to form
    @xmath and showing that it satisfies @xmath and @xmath can be done
    in a similar manner.

We repeat the procedure described above in steps @xmath and step @xmath
until we pick @xmath cosets. Note that the set of cosets thrown away at
@xmath step contains the set of cosets thrown away at @xmath step. Hence
the total number of cosets thrown until @xmath step is (from step @xmath
described above):

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

we can pick @xmath coset to form @xmath as long as @xmath . Now the
procedure for throwing away cosets for satisfying parallel conditions
based on @xmath as mentioned in the beginning is similar to above and
hence we throw away atmost @xmath cosets and satisfy all the necessary
conditions. Hence we can pick @xmath coset to form @xmath as long as
@xmath . Hence we can pick @xmath cosets (evaluating positions) to form
maximally recoverable code of block length @xmath and dimension @xmath
as long as @xmath . @xmath . Hence we can form @xmath maximally
recoverable code as long as:

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

Using @xmath , it can be seen that the above inequality is implied by:

  -- -------- --
     @xmath   
  -- -------- --

where, @xmath for @xmath and @xmath even and @xmath otherwise. Hence:

  -- -------- --
     @xmath   
  -- -------- --

The cosets @xmath given in the Theorem statement are actually the set of
cosets in @xmath . ∎