## Acknowledgements

My thanks go first and foremost to Nathaniel Craig for his continual
support and encouragement. From Nathaniel I learned not only an enormous
amount of exciting physics, but also how to prepare engaging lessons and
talks, how to be an effective mentor, how to think intuitively about the
natural world and determine the particle physics to describe it, and how
to be a caring, welcoming, community-focused academic. It goes without
saying that Nathaniel’s influence pervades every word written below and
the physical understanding behind them. I would not have made it here
were it not for the many senior academics who have charitably given
their own time to encourage and support me. As a young undergraduate,
Tom Lubensky’s patient help during many office hours and his explicit
encouragement for me to continue studying physics were vital. Cullen
Blake took a risk on me as an undergrad with little to show but
excitement and enthusiasm, taught me how to problem-solve like a
physicist and a researcher, and truly made a huge difference in my life.
And there were many such professors who kindly gave me their time and
support—Mirjam Cvetič, Larry Gladney, Justin Khoury, H. H. “Brig”
Williams, Ned Wright, and others—and if I listed all the ways they have
all supererogatorily supported me and my education this section would be
too long. In graduate school I have also benefited from the kindness of
a cadre of senior academics. Dave Sutherland and John Mason spent hours
engaging me in many elucidating conversations. Don Marolf generously
included me in gravity theory activities and answered my endless
questions. Nima Arkani-Hamed has graciously given me his time and made
me feel welcome at every turn. And there have been many other particle
theorists who have offered me their advice and support over the
years—Tim Cohen, Patrick Draper, Matthew McCullough, and Flip Tanedo,
among others. Of course I have not worked alone, and little of this
science would have been accomplished were it not for my many
collaborators who have helped me learn and problem-solve and provided
guidance and been patient when I was overwhelmed with being pulled in
too many directions. Let me especially mention those undergrads I have
spent significant time mentoring during my time in graduate school,
namely Aidan Herderschee, Samuel Alipour-fard, and Umut Can Öktem.
Indeed, they each took a chance on me as well, and working with them has
taught me how to be a better teacher and physicist—not to mention all of
the great science we worked out together. My physics knowledge would
have also been stunted were it not for the countless hours spent
discussing all manner of high energy theory with friends and
peers—primarily Matthew Brown, Brianna Grado-White, Alex Kinsella,
Robert McGehee, and Gabriel Treviño Verastegui. And my enjoyment of
graduate school would have been stunted were it not for the board games,
hiking, trivia, art walks, biking, and late-night philosophical
discussions with them and with Dillon Cisco, Neelay Fruitwala, Eric
Jones, Farzan Vafa, Sicheng Wang, and others. No one has enriched my
life here moreso than Nicole Iannaccone—a summary statement of far too
few words. Nor would this have been possible were it not for my ‘medical
support team’ consisting primarily of endocrinologists Dr. Mark Wilson
and Dr. Ashley Thorsell, student health physician Dr. Miguel Pedroza,
and my therapist Dr. Karen Dias, who have helped me immensely in my time
here. Even putting aside physiological issues, graduate school can be
and has been incredibly mentally taxing. I’m not sure I could not have
made it through were it not for the psychological assistance I have
received, both psychotherapeutic and pharmacological. Of course a
special ‘shout-out’ goes to Mother Nature. The idyllic weather, scenic
ocean, beautiful mountains, and clear night skies of Santa Barbara may
have just been too distracting for me to progress through my degree were
it not for the Rey fire, the Whittier fire, the Cave fire, the Santa
Barbara microburst, the recurring January flooding, the Ridgecrest
earthquakes, the month of unbreathable air from the Thomas fire, the
threat of the Thomas fire itself, the Montecito mudslides, and of course
the COVID-19 pandemic, all of which have kept me indoors thinking about
physics. Finally, I would like to thank all those who kindly read drafts
of (parts of) this thesis and provided invaluable feedback, including
Samuel Alipour-fard, Ian Banta, Manuel Buen-Abad, Changha Choi,
Nathaniel Craig, David Grabovsky, Adolfo Holguin, Samuel Homiller, Lucas
Johns, Soubhik Kumar, Umut Can Öktem, Robert McGehee, Alex Meiburg,
Gabriel Treviño Verastegui, Farzan Vafa, and George Wojcik. {vitae}
{vitaesection} Education Ph.D. in Physics, University of California,
Santa Barbara. M.A. in Physics, University of California, Santa Barbara.
M.S. in Physics, University of Pennsylvania. B.A. in Mathematics and
Physics, Astrophysics Concentration, Honors Distinction in Physics,
University of Pennsylvania. Publications {comment} \bibentry
Cohen:2020ohi \bibentry Craig:2019zbn \bibentry Koren:2019iuv \bibentry
Craig:2019fdy \bibentry Giddings:2019ujs \bibentry Koren:2019wwi
\bibentry Herderschee:2019ofc \bibentry Herderschee:2019dmc \bibentry
Alipour-fard:2018mre \bibentry Craig:2018yvw \bibentry
Alipour-Fard:2018lsf \bibentry Craig:2016lyx “Supersoft Stops”
T. Cohen, N. Craig, S. Koren, M. McCullough, J. Tooby-Smith
Accepted to Phys. Rev. Lett., [arXiv:2002.12630 [hep-ph]] [
Cohen:2020ohi ]
“IR Dynamics from UV Divergences: UV/IR Mixing, NCFT, and the Hierarchy
Problem”
N. Craig and S. Koren
JHEP 03 (2020) 037, [arXiv:1909.01365 [hep-ph]] [ Craig:2019zbn ]
“Freezing-in Twin Dark Matter”
S. Koren and R. McGehee
Phys. Rev. D101 (2020) 055024, [arXiv:1908.03559 [hep-ph]] [
Koren:2019iuv ]
“The Weak Scale from Weak Gravity”
N. Craig, I. Garcia Garcia, S. Koren
JHEP 09 (2019) 081, [arXiv:1904.08426 [hep-ph]] [ Craig:2019fdy ]
“Exploring Strong-Field Deviations From General Relativity via
Gravitational Waves”
S. Giddings, S. Koren, G. Treviño
Phys. Rev. D100 (2019) 044005, [arXiv:1904.04258 [gr-qc]] [
Giddings:2019ujs ]
“Neutrino - DM Scattering and Coincident Detections of UHE Neutrinos
with EM Sources”
S. Koren
JCAP 09 (2019) 013, [arXiv:1903.05096 [hep-ph]] [ Koren:2019wwi ]
“Constructing N=4 Coulomb Branch Superamplitudes”
A. Herderschee, S. Koren, T. Trott
JHEP 08 (2019) 107, [arXiv:1902.07205 [hep-th]] [ Herderschee:2019dmc ]
“Massive On-Shell Supersymmetric Scattering Amplitudes”
A. Herderschee, S. Koren, T. Trott
JHEP 10 (2019) 092, [arXiv:1902.07204 [hep-th]] [ Herderschee:2019ofc ]
“The second Higgs at the lifetime frontier”
S. Alipour-fard, N. Craig, S. Gori, S. Koren, D. Redigolo
JHEP 07 (2020) 029, [arXiv:1812.09315 [hep-ph]] [ Alipour-fard:2018mre ]
“Discrete Gauge Symmetries and the Weak Gravity Conjecture”
N. Craig, I. Garcia Garcia, S. Koren
JHEP 05 (2019) 140, [arXiv:1812.08181 [hep-th]] [ Craig:2018yvw ]
“Long Live the Higgs Factory: Higgs Decays to Long-Lived Particles at
Future Lepton Colliders”
S. Alipour-fard, N. Craig, M. Jiang, S. Koren
Chin. Phys. C43 (2019) 053101, [arXiv:1812.05588 [hep-ph]] [
Alipour-Fard:2018lsf ]
“Cosmological Signals of a Mirror Twin Higgs”
N. Craig, S. Koren, T. Trott
JHEP 05 (2017) 038, [arXiv:1611.07977 [hep-ph]] [ Craig:2016lyx ]
“The Low-Mass Astrometric Binary LSR1610-0040”
S. C. Koren, C. H. Blake, C. C. Dahn, H. C. Harris
The Astronomical Journal 151 (2016) 57, [arXiv:1511.02234 [astro-ph.SR]]
[ 2016AJ....151...57K ]
“Characterizing Asteroids Multiply-Observed at Infrared Wavelengths”
S. C. Koren, E. L. Wright, A. Mainzer
Icarus 258 (2015) 82-91, [arXiv:1506.04751 [astro-ph.EP]] [
2015Icar..258...82K ]

### Permissions and Attributions

1.  The content of Chapter The Hierarchy Problem: From the Fundamentals
    to the Frontiers is the result of collaboration with Nathaniel Craig
    and Timothy Trott, and separately with Robert McGehee. This work
    previously appeared in the Journal of High Energy Physics (JHEP 05
     (2017) 038) and Physical Review D (Phys. Rev. D101  (2020) 055024),
    respectively.

2.  The content of Chapter The Hierarchy Problem: From the Fundamentals
    to the Frontiers is the result of collaboration with Samuel
    Alipour-fard, Nathaniel Craig, and Minyuan Jiang, and previously
    appeared in Chinese Physics C (Chin. Phys. C43  (2019) 053101).

3.  The content of Chapter The Hierarchy Problem: From the Fundamentals
    to the Frontiers is the result of collaboration with Nathaniel Craig
    and previously appeared in the Journal of High Energy Physics (JHEP
    03  (2020) 037).

### Preface

The first four chapters of this thesis are introductory material which
has not previously appeared in any public form. My intention has been to
write the guide that would have been most useful for me toward the
beginning of my graduate school journey as a field theorist interested
in the hierarchy problem. My aim has been to make these chapters
accessible to beginning graduate students in particle physics and
interested parties in related fields—background at the level of a single
semester of quantum field theory should be enough for them to be
understandable in broad strokes. Chapter 1 introduces fundamental tools
and concepts in quantum field theory which are essential for particle
theory, spending especial effort on discussing renormalization from a
variety of perspectives. Chapter 2 discusses the hierarchy problem and
how to think about it—primarily through the pedagogical device of
refuting a variety of common misconceptions and pitfalls. Chapter 3
introduces in brief a variety of classic strategies and solutions to the
hierarchy problem which also constitute important frameworks in
theoretical particle physics beyond the Standard Model. Chapter 4
discusses more-recent ideas about the hierarchy problem in light of the
empirical pressure supplied by the lack of observed new physics at the
Large Hadron Collider. Throughout I also make note of interesting
research programs which, while they lie too far outside the main
narrative for me to explain, are too fascinating not to be mentioned.
The first half of this thesis is thus mostly an introduction to and
review of material I had no hand in inventing. As always, I am ‘standing
on the shoulders of giants’, and I have benefited enormously from the
pedagogical efforts of those who came before me. When my thinking on a
topic has been especially informed by a particular exposition, or when I
present an example which was discussed in a particular source, I will
endeavor to say so and refer to that presentation. As to the rest, it’s
somewhere between difficult and impossible to distinguish precisely how
and whose ideas I have melded together in my own understanding of the
topics—to say nothing of any insight I may have had myself—but I have
included copious references to reading material I enjoyed as a guide.
Ultimately this is a synthesis of ideas in high energy theory aimed
toward the particular purpose of understanding the hierarchy problem,
and I have attempted to include the most useful and pedagogical
explanations of these topics I could find, if not invent. I then present
some work on the subject by myself and my collaborators. Chapter 5
contains work constructing a viable cosmological history for mirror twin
Higgs models, an exemplar of the modern Neutral Naturalness approach to
the hierarchy problem. Chapter 6 focuses on searching for long-lived
particles produced at particle colliders as a discovery channel for a
broad class of such models. Chapter 7 is an initial exploration of a new
approach to the hierarchy problem which follows a maximalist
interpretation of the lack of new observed TeV scale physics, and so
relies on questioning and modifying some core assumptions of
conventional particle physics. In Chapter 8 we conclude with some brief
parting thoughts. If you enjoy reading this work, or find it useful, or
have questions, or comments, or recommendations for good references,
please do let me know—at whatever point in the future you’re reading
this. As of autumn 2020, I can be reached at sethk@uchicago.edu.

###### Contents

-    Acknowledgements
-    Permissions and Attributions
-    Preface
-    1 EFT Basics
    -    1.1 Scale-dependence
    -    1.2 Bottom-up or Top-down
-    2 Renormalization
    -    Loops are necessary
    -    2.1 To Remove Divergences
        -    Physical input is required
        -    Renormalizability
        -    Wilsonian renormalization of @xmath
        -    Renormalization and locality
    -    2.2 To Repair Perturbation Theory
        -    Renormalization group equations
        -    Decoupling
        -    Renormalized perturbation theory
        -    Continuum renormalization
        -    Renormalization group improvement
    -    2.3 To Relate Theories
        -    Mass-independent schemes and matching
        -    Flowing in theory space
        -    Trivialities
    -    2.4 To Reiterate
-    3 Naturalness
    -    3.1 Technical Naturalness and Fine-Tuning
        -    Technical naturalness and masses
    -    3.2 Spurion Analysis
    -    3.3 Dimensional Transmutation
-    4 The Higgs in the Standard Model
-    5 Non solutions to the Hierarchy Problem
    -    5.1 An End to Reductionism
    -    5.2 Waiter, there’s Philosophy in my Physics
    -    5.3 The Lonely Higgs
    -    5.4 Mass-Independent Regulators
-    6 The Hierarchy Problem
-    7 Supersymmetry
-    8 Extra Dimensions
    -    8.1 Technology: Kaluza-Klein Reduction
    -    8.2 Quantum Gravity at the TeV Scale
    -    8.3 Technology: Orbifold Reduction
    -    8.4 Nonlocal Symmetry-Breaking
-    9 Compositeness
    -    9.1 Technicolor
    -    9.2 A Composite Goldstone Higgs
-    10 The ‘Little Hierarchy Problem’
    -    10.1 The Twin Higgs
    -    10.2 Neutral Naturalness or The Return Of The Orbifold
        -    Example 1: Folded Supersymmetry
        -    Example 2: The @xmath -plet Higgs
-    11 The Loerarchy Problem
-    12 Violations of Effective Field Theory
    -    12.1 Gravity and EFT
-    13 Particle Cosmology
-    14 Asymmetric Reheating
-    15 Freeze- Tw in Dark Matter
-    A Scalar Two-Point Function
-    B Fermion Two-Point Function
-    C Three-Point Function

## Chapter \thechapter Effective Field Theory

The formulation and understanding of the hierarchy problem is steeped
heavily in the principles and application of effective field theory
(EFT) and renormalization, so we begin with an introductory overview to
set the stage for our main discussion. As is clear from the table of
contents, I have prioritized clarity over brevity—especially when it
comes to renormalization. The reader with a strong background in
particle physics may find much of this to be review, so may wish to skip
ahead directly to Chapter The Hierarchy Problem: From the Fundamentals
to the Frontiers and circle back to sections of this chapter if and when
the subtleties they discuss become relevant. We will endeavor to discuss
the conceptual points which will be useful later in understanding the
hierarchy problem, and more generally to clarify common confusions with
ample examples. Of course we will be unable to discuss everything, and
will try to provide references to more detailed explanations when we
must needs say less than we would like. Some generally useful
introductions to effective field theory can be found from Cohen [
Cohen:2019wxr ] and Georgi [ Georgi:1994qn ] , and useful, pedagogical
perspectives on renormalization are to be found in Srednicki [
Srednicki:2007qs ] , Peskin & Schroeder [ Peskin:1995ev ] , Zee [
Zee:2003mt ] , Polchinski [ Polchinski:1983gv ] , and Schwartz [
Schwartz:2013pla ] , among others.

### 1 EFT Basics

Effective field theory is simply the familiar strategy to focus on the
important degrees of freedom when understanding a physical system. For a
simple example from an introductory Newtonian mechanics course, consider
studying the motion of balls on inclined planes in a freshman lab. It is
neither necessary nor useful to model the short-distance physics of the
atomic composition of the ball, nor the high-energy physics of special
relativity. Inversely, it is also unnecessary to account for the
long-distance physics of Hubble expansion or the low-energy physics of
air currents in the lab. In quantum field theories this intuitive course
of action is formalized in decoupling theorems, showing precisely the
sense in which field theories are amenable to this sort of analysis: the
effects of short-distance degrees of freedom may be taken into account
as slight modifications to the interactions of long-distance degrees of
freedom, instead of including explicitly those high-energy modes. Of
course when one returns to the mechanics laboratory armed with an atomic
clock and a scanning tunneling microscope, one begins to see deviations
from the Newtonian predictions. Indeed, the necessary physics for
describing a situation depends not only on the dynamics under
consideration but also on the precision one is interested in attaining
with the description. So it is crucial that one is able to correct the
leading-order description by systematically adding in subdominant
effects, as organized in a suitable power series in, for example, @xmath
, where @xmath is the ball’s velocity and @xmath is the speed of light.
Of course when the full description of the physics is known it’s in
principle possible to just use the full theory to compute
observables—but I’d still rather not begin with the QED Lagrangian to
predict the deformation of a ball rolling down a ramp.

The construction of an appropriate effective description relies on three
ingredients. The first is a list of the important degrees of freedom
which specify the system under consideration—in particle physics this is
often some fields @xmath . The second is the set of symmetries which
these degrees of freedom enjoy. These constrain the allowed interactions
between our fields and so control the dynamics of the theory. Finally we
need a notion of power counting, which organizes the effects in terms of
importance. This will allow us to compute quantities to the desired
precision systematically. Frequently in effective field theories of use
in particle physics this role is played by @xmath , where @xmath is an
energy and @xmath is a heavy mass scale or cutoff above which we expect
to require a new description of the physics.

#### 1.1 Scale-dependence

We will often be interested in determining the appropriate description
of a system at some scale, so it is necessary to understand which
degrees of freedom and which interactions will be important as a
function of energy. We can gain insight into when certain modes or
couplings are important by studying the behavior of our system under
scale transformations. Consider for example a theory of a real scalar
field @xmath , with action

  -- -------- -- -----
     @xmath      (1)
  -- -------- -- -----

where @xmath is the dimensionality of spacetime, @xmath are couplings of
interactions involving different numbers of @xmath , the @xmath denote
terms with higher powers of @xmath , and we’ve imposed a @xmath symmetry
for simplicity. We’ve set @xmath leaving us with solely the mass
dimension to speak of. From the fact that regardless of spacetime
dimension we have @xmath and @xmath , where @xmath denotes the mass
dimension, we first calculate from the kinetic term that @xmath , and
then we can read off @xmath , @xmath , @xmath . These are known as the
classical dimension of the associated operators and are associated to
the behavior of the operators at different energies, for the following
reason. If we wish to understand how the physics of this theory varies
as a function of scale, we can perform a transformation @xmath and study
the long-distance limit @xmath with @xmath fixed. The measure transforms
as @xmath and the derivatives @xmath . Then to restore canonical
normalization of the kinetic term such that the one-particle states are
properly normalized for the LSZ formula to work, we must perform a field
redefinition @xmath , and the action becomes

  -- -------- -- -----
     @xmath      (2)
  -- -------- -- -----

As a reminder, in the real world (at least at distances @xmath ) we have
@xmath . As you look at the theory at longer distances the mass term
becomes more important, so is known as a ‘relevant’ operator. One says
that the operator @xmath has classical dimension @xmath . The quartic
interaction is classically constant under such a transformation, so is
known as ‘marginal’ with @xmath , and interactions with more powers of
@xmath shrink at low energies and are termed ‘irrelevant’, e.g. @xmath .
We have been careful to specify that these are the classical dimension
of the operators, also called the ‘engineering dimension’ or ‘canonical
dimension’, which has a simple relation to the mass dimension as @xmath
for some operator @xmath . If the theory is not scale-invariant then
quantum corrections modify this classical scaling by an ‘anomalous
dimension’ @xmath which is a function of the couplings of the theory,
and the full behavior is known as the ‘scaling dimension’. The terms
‘marginally (ir)relevant’ are used for operators whose classical
dimension is zero but whose anomalous dimensions push them to one side.
The connection to the typical EFT power counting in @xmath is immediate.
In an EFT with UV cutoff @xmath , it’s natural to normalize all of our
couplings with this scale and rename e.g. @xmath where @xmath is now
dimensionless. It’s easy to see that the long-distance limit is
equivalently a low-energy limit by considering the derivatives, which
pull down a constant @xmath and scale as @xmath —or by simply invoking
the uncertainty principle. Operators with negative scaling dimension
contribute subleading effects at low energies precisely because of these
extra powers of a large inverse mass scale.

#### 1.2 Bottom-up or Top-down

  Then he made the tank of cast metal, 10 cubits across from brim to
  brim, completely round; it was 5 cubits high, and it measured 30
  cubits in circumference.

  God on the merits of working to finite precision
  1 Kings 7:23, Nevi’im
  New Jewish Publication Society Translation (1985) [ jewish2007jewish ]

The procedure of writing down the most general Lagrangian with the given
degrees of freedom and respecting the given symmetries up to some degree
of power counting is termed ‘bottom-up EFT’ as we’re constructing it
entirely generally and will have to fix coefficients by making
measurements. A great example is the Standard Model Effective Field
Theory (SMEFT), of which the Standard Model itself is described by the
SMEFT Lagrangian at zeroth order in the power counting. It is defined by
being an @xmath gauge theory with three generations of the following
representations of left-handed Weyl fermions:

  ----------------- -------- -------- --------
  @xmath Fermions   @xmath   @xmath   @xmath
  @xmath            3        2        @xmath
  @xmath            @xmath   -        @xmath
  @xmath            @xmath   -        @xmath
  @xmath            -        2        @xmath
  @xmath            -        -        1
  ----------------- -------- -------- --------

In addition the Standard Model contains one scalar, the Higgs boson,
which is responsible for implementing the
Anderson-Brout-Englert-Guralnik-Hagen-Higgs-Kibble-’t Hooft mechanism [
Anderson:1963pc , Englert:1964et , Guralnik:1964eu , Higgs:1964ia ,
Higgs:1964pj , tHooft:1971qjg ] to break the electroweak symmetry @xmath
down to electromagnetism @xmath at low energies:

  -------- --- --- --------
  @xmath   -   2   @xmath
  -------- --- --- --------

The Standard Model Lagrangian contains all relevant and marginal
gauge-invariant operators which can be built out of these fields, and
has the following schematic form

  -- -------- -------- -- -----
     @xmath   @xmath      (3)
     @xmath   @xmath      (4)
  -- -------- -------- -- -----

with @xmath a gauge field strength, @xmath a fermion, @xmath the gauge
covariant derivative in the kinetic term Lagrangian on the first line,
and the second line containing the Higgs’ Yukawa couplings and
self-interactions. If a refresher on the Standard Model would be useful,
the introduction to its structure toward the end of Srednicki’s textbook
[ Srednicki:2007qs ] will suffice for our purposes, while further
discussion from a variety of perspectives can be found in Schwartz [
Schwartz:2013pla ] , Langacker [ Langacker:2010zza ] , and Burgess &
Moore [ Burgess:2007zi ] . The SMEFT power-counting is in energies
divided by an as-yet-unknown UV scale @xmath , so the dimension- @xmath
SMEFT Lagrangian consists of all gauge-invariant combinations of these
fields with scaling dimension @xmath . At dimension five there is solely
one operator, @xmath , which contains a Majorana mass for neutrinos. In
even-more-schematic form, the dimension six Lagrangian contains
operators with the field content

  -- -------- -- -----
     @xmath      (5)
  -- -------- -- -----

where for aesthetics we have multiplied through by the scale @xmath and
haven’t bothered writing down couplings. After understanding the
structure of the independent symmetry-preserving operators (see e.g. [
Grzadkowski:2010es , Gripaios:2018zrz , deBlas:2017xtg ] ), the job of
the bottom-up effective field theorist is to measure or constrain the
coefficients of these higher-dimensional operators [ Brivio:2017btx ] .
Useful data comes from both the energy frontier with searches at
colliders for the production or decay of high-energy particles through
these higher-dimensional operators and from the precision frontier
measuring fundamental processes very well to look for deviations from
the Standard Model predictions (e.g. [ Falkowski:2017pss , Ellis:2018gqa
, deBlas:2017wmn ] ). For more detail, see the introduction to SMEFT by
Brivio & Trott [ Brivio:2017vri ] . Another approach is possible when we
already have a theory and just want to focus on some particular degrees
of freedom. Then we may construct a ‘top-down EFT’ by taking our theory
and ‘integrating out’ the degrees of freedom we don’t care about—for
example by starting with the Standard Model above and getting rid of the
electroweak bosons to focus on processes occurring at lower energies
(e.g. Fermi’s model of the weak interaction [ Fermi:1934hr ] ). We can’t
necessarily just ignore those degrees of freedom though; what we need to
do is modify the parameters of our EFT such that they reproduce the
results of the full theory (to some finite precision) using only the
low-energy degrees of freedom. Such a procedure can be illustrated
formally by playing with path integrals. Consider the partition function
for a theory with some light fields @xmath and some heavy fields @xmath
:

  -- -------- -- -----
     @xmath      (6)
  -- -------- -- -----

This contains all of the physics in our theory, and so in principle we
may use it to compute anything we wish. But if we’re interested in
low-energy processes involving solely the @xmath fields, we could split
up our path integral and first do the integral over the @xmath fields.
The light @xmath fields are the only ones left, so we can then write the
partition function as

  -- -------- -- -----
     @xmath      (7)
  -- -------- -- -----

where this defines @xmath . Thus far this still contains all the same
physics, as long as we don’t want to know about processes with external
heavy fields ¹ ¹ 1 Since we haven’t made any approximations and have the
same object @xmath , one may be confused as to why we’ve lost access to
the physics of the @xmath fields. In fact I’ve been a bit sloppy. If we
want to compute correlation functions of our fields @xmath , we must
couple our fields to classical sources @xmath as @xmath . Physically,
those sources allow us to ‘turn on’ particular fields so that we can
then calculate their expectation values. Mathematically, we really need
the partition function as a functional of these sources @xmath , and we
take functional derivatives with respect to these sources as a step to
calculating correlation functions or scattering amplitudes. In
integrating out our heavy field @xmath , we no longer have a source we
can put in our Lagrangian to turn on that field, as it no longer appears
in the action. . But having decided that we are interested in the
infrared physics of the @xmath fields, we can say that the effects of
the heavy @xmath fields will be suppressed by factors of the energies of
interest divided by the mass of @xmath , and we should expand the
Lagrangian @xmath in an appropriate series:

  -- -------- -- -----
     @xmath      (8)
  -- -------- -- -----

where @xmath is the part of the full Lagrangian that had no heavy fields
in it, @xmath is an operator of classical dimension @xmath , @xmath is a
dimensionless coupling, and @xmath defines the precision to which one
works in this effective theory. This is the procedure to find a top-down
effective field theory in the abstract.

A great example of a top-down EFT is in studying the Standard Model
fields in the context of a Grand Unified Theory (GUT). Broadly, Grand
Unification is the hope that there is some simpler, more symmetric
theory behind the Standard Model which explains its structure. A GUT is
a model in which the gauge groups of the SM are (partially [ Pati:1974yy
] ) unified in the UV. If there is a full unification to a single gauge
factor, then this requires ‘gauge coupling unification’ in the UV until
the symmetry is broken down to the SM gauge group at a high scale [
Georgi:1974sy ] . While one’s first exposure to this idea today may be
in the context of a UV theory like string theory which roughly demands
such unification, this was in fact first motivated by the observed
infrared SM structure. It is frankly amazing that not only are the
values of the SM gauge couplings consistent with this idea, and not only
does @xmath fit nicely inside @xmath , but the SM fermion
representations precisely fit into the @xmath representations of @xmath
(see Figure 2 ). It’s hard to imagine a discovery that would have felt
much more like one was obviously learning something deep and important
about Nature than when Georgi realized how nicely all of this worked
out. I’m reminded of Einstein’s words on an analogous situation in the
early history of electromagnetism—the original unified theory:

  The precise formulation of the time-space laws of those fields was the
  work of Maxwell. Imagine his feelings when the differential equations
  he had formulated proved to him that electromagnetic fields spread in
  the form of polarised waves, and with the speed of light! To few men
  in the world has such an experience been vouchsafed. At that thrilling
  moment he surely never guessed that the riddling nature of light,
  apparently so completely solved, would continue to baffle succeeding
  generations.
  — Albert Einstein
  Considerations concerning the Fundaments of Theoretical Physics , 1940
  [ Einstein:1940 ]

And just as with Maxwell, the initial deep insight into Nature was not
the end of the story. As of yet, Grand Unification remains an unproven
ideal, and indeed further empirical data has brought into question the
simplest such schemes. But it’s hard to imagine all of this beautiful
structure is simply coincidental, and I would wager that most high
energy theorists still have a GUT in the back of their minds when they
think about the UV structure of the universe, so this is an important
story to understand. To learn generally about GUTs, I recommend the
classic books by Kounnas, Masiero, Nanopoulos, & Olive [ Kounnas:1985cj
] and Ross [ Ross:1985ai ] or the recent book by Raby [ Raby:2017ucc ]
for the more formally-minded. Shorter introductions can be found in
Sher’s TASI lectures [ TASI:2001rya ] or in the Particle Data Group’s
Review of Particle Physics [ Tanabashi:2018oca ] from Hebecker & Hisano.
The structure of the simplest @xmath GUT is that the symmetry group
breaks down to the SM at energies @xmath via the Higgs mechanism. ² ² 2
The scale @xmath is determined from low-energy data by computing the
scale-dependence of the SM gauge couplings, evolving them up to high
energies, and looking for an energy scale at which they meet. Since we
have three gauge couplings at low energies, it is quite non-trivial that
the curves @xmath meet at a single scale @xmath . The @xmath so computed
is approximate not only due to experimental uncertainties on the
low-energy values of parameters in the SM, but also because additional
particles with SM charges affect slightly how the couplings evolve
toward high energies. Indeed, adding supersymmetry makes the
intersection of the three curves even more accurate than it is in the SM
itself. More generally, unification may proceed in stages as, for
example, @xmath , and the breaking may occur via other mechanisms, as
we’ll discuss further in Section The Hierarchy Problem: From the
Fundamentals to the Frontiers . Back to our simple single-breaking
example, as is familiar in the SM this means that the gauge bosons
corresponding to broken generators get masses of order this GUT-breaking
scale. As this is a far higher scale than we are currently able to
directly probe, it is neither necessary nor particularly useful to keep
these degrees of freedom fully in our description if we’re interesting
in understanding the effects of GUT-scale fields. Rather than
constructing the complete top-down EFT of the SM from a GUT, let’s focus
on one particularly interesting effect. One of the best ways to
indirectly probe GUTs is by looking for proton decay. The GUT
representations unify quarks and leptons, so the extra @xmath gauge
bosons have nonzero baryon and lepton number and fall under the label of
‘leptoquarks’. It’s worth considering in detail why proton decay is a
feature of GUTs and not of the SM, as it’s a subtler story than is
usually discussed. While @xmath , the baryon number, is an accidental
global symmetry of the SM ³ ³ 3 ‘Accidental’ here means that imposing
this symmetry on the SM Lagrangian does not forbid any operators which
would otherwise be allowed. The SM is defined, as above, by the gauge
symmetries @xmath and the field content. Writing down the most general
dimension-4 Lagrangian 3 , 4 invariant under these symmetries gives a
Lagrangian which is automatically invariant under @xmath . This no
longer holds at higher order in SMEFT, and indeed the dimension-6
Lagrangian (Equation 5 ) does contain baryon-number-violating operators.
If one wants to study a baryon-number-conserving version of SMEFT, one
needs to explicitly impose that symmetry on the dimension-6 Lagrangian,
so @xmath is no longer an accidental symmetry of SMEFT. , it’s an
anomalous symmetry and so is not a symmetry of the quantum world. The
‘baryon minus lepton’ number, @xmath , is non-anomalous, but this is a
good symmetry both of the SM and of a GUT and clearly does not prevent
e.g. @xmath . What’s really behind the stability of the proton is that,
though @xmath and @xmath are not good quantum symmetries, the fact that
they are good classical symmetries means their only violation is
nonperturbatively by instantons. Such configurations yield solely baryon
number violation by three units at once, corresponding to the number of
generations, and thus the proton with @xmath is stable ⁴ ⁴ 4 Convincing
yourself fully that @xmath is the smallest allowed transition is not
straightforward, but let me try to make it believable for anyone with
some exposure to anomalies and instantons. The existence of a mixed
@xmath anomaly—equivalently a nonvanishing triangle diagram with two
@xmath gauge legs and a baryon current insertion—means that the baryon
current will no longer be divergenceless, @xmath , where @xmath is the
field strength and @xmath its Hodge dual. Instantons are field
configurations interpolating between vacuum field configurations of
different topology, and there are nontrivial instantons in 4d Minkowski
space for @xmath but not for @xmath as a result of topological
requirements on the gauge group. So while there is also a mixed anomaly
with hypercharge, we can ignore this for our purposes. The SM fermions
contributing to the @xmath anomaly are then only @xmath (the left-handed
quark doublet with @xmath , with @xmath a color index) and similarly for
the lepton anomaly only the doublet @xmath matters. There are three
generations of each, which leads simply to a factor of three in the
divergence of the global currents. Thinking about it in terms of
triangle diagrams, this is simply because there are thrice as many
fermions running in the loop. The extent to which an instanton solution
changes the topology is given by the integral of @xmath over spacetime,
which as a total derivative localizes to the boundary, and furthermore
turns out to be a topological invariant of the gauge field configuration
known as the winding number, an integer (technically the change in
winding number between the initial and final vacua). Then the anomaly,
by way of the nonvanishing divergence, relates the winding number of
such a configuration to the change in baryon and lepton number it
induces. The factor of the number of generations means that each unit of
winding number ends up producing @xmath . And that is why the proton is
stable. Classic, detailed references on anomalies and instantons include
Coleman [ Coleman:1985rnk ] , Bertlmann [ bertlmann2000anomalies ] , and
Rajamaran [ rajaraman1982solitons ] . .

But in GUTs, baryon number and lepton number are no longer accidental
symmetries, so no such protection is available and the GUT gauge bosons
mediate tree-level proton decay processes as in Figure 3 . We can find
the leading effect by integrating these out—in particular we’ll here
look just at a four-fermion baryon-number-violating operator. The
tree-level amplitude is simply

  -- -------- -- -----
     @xmath      (9)
  -- -------- -- -----

so integrating out the gauge boson from this diagram gives us one of the
contributions to the low-energy operators in Equation 8

  -- -- -- ------
           (10)
  -- -- -- ------

where, in the notation of Equation 8 , @xmath and @xmath . The
calculation of the proton lifetime from this operator is quite
complicated, but the dimensional analysis estimate of @xmath actually
works surprisingly well. The job of the top-down effective field
theorist is to calculate the effects of some particular UV physics on IR
observables and by doing so understand how to search for their
particular effects. While the effects will, by necessity, be some subset
of the operators that the bottom-up effective field theorist has written
down, the patterns and correlations present from a particular UV model
can suggest or require particular search strategies. In the present
context, a GUT may suggest the most promising final states to look for
when searching for proton decay. If we wanted to calculate the lifetime
and branching ratios more precisely we would have to deal with loop
diagrams (among many complications), which of course is a generic
feature. So we now turn our attention to the new aspects and challenges
of field theory that appear once one goes beyond tree-level.

### 2 Renormalization

Renormalization is a notoriously challenging topic for beginning quantum
field theorists to grok, and explanations often get bogged down in the
details of one particular perspective or scheme or purpose and ‘miss the
forest for the trees’, so to speak. ⁵ ⁵ 5 Not that I begrudge QFT
textbooks or courses for it, to be clear. There is so much technology to
introduce and physics to learn in a QFT class that discussion of all of
these various perspectives and issues would be prohibitive. We’ll
attempt to overcome that issue by discussing a variety of uses for and
interpretations of renormalization, as well as how they relate. And, of
course, by examining copious examples and pointing out a variety of
conceptual pitfalls.

##### Loops are necessary

At the outset the only fact one needs to have in mind is that
renormalization is a procedure which lets quantum field theories make
physical predictions given some physical measurements. Such a procedure
was not necessary for a classical field theory, which is roughly
equivalent to a quantum field theory at tree-level. A natural question
for beginners to ask then is why we should bother with loops at all: Why
don’t we just start off with the physical, measured values in the
classical Lagrangian and be done with it? That is, if we measure, say,
the mass and self-interaction of some scalar field @xmath , let’s just
define our theory

  -- -------- -- ------
     @xmath      (11)
  -- -------- -- ------

for some definitions of these physical parameters, and compute
everything at tree-level. However, this does not constitute a sensible
field theory, as the optical theorem tells us this is not consistent. We
define @xmath as the S-matrix which encodes how states in the theory
scatter, where the @xmath is the ‘trivial’ no-interaction part. Quantum
mechanics turns the logical necessity that probabilities add up to 1
into the technical demand of ‘unitarity’, @xmath , which tells us the
nontrivial part must satisfy

  -- -------- -- ------
     @xmath      (12)
  -- -------- -- ------

Sandwiching this operator equation between initial and final states, we
find that the left hand side is the imaginary part of the amplitude
@xmath , which is nonzero solely due to loops. This is depicted
schematically in Figure 4 . We can see why this is by examining a scalar
field propagator. Taking the imaginary part one finds

  -- -------- -- ------
     @xmath      (13)
  -- -------- -- ------

This vanishes manifestly as @xmath except for when @xmath , and an
integral to find the normalization yields

  -- -------- -- ------
     @xmath      (14)
  -- -------- -- ------

So internal propagators are real except for when the particle is put
on-shell. In a tree-level diagram this occurs solely at some
measure-zero set of exceptional external momenta, but in a loop-level
diagram we integrate over all momenta in the loop, so an imaginary part
is necessarily present. Now we see the necessity of loops solely from
the conservation of probability and the framework of quantum mechanics ⁶
⁶ 6 A natural question to ask is whether this structure can be perturbed
at all, but in fact it really is quite rigid. After Hawking—motivated by
black hole evaporation—proposed that the scattering matrix in a theory
of quantum gravity should not necessarily obey unitarity [
Hawking:1982dj ] , the notion of modifying the S-matrix to a non-unitary
‘ @xmath ’-matrix (pronounced ‘dollar matrix’) received heavy scrutiny.
This was found to necessarily lead to large violations of energy
conservation, among other maladies [ Banks:1983by ] . . The lesson to
take away from this is that classical field theories produce correlation
functions with some particular momentum dependence, which can be
essentially read off from the Lagrangian. But a consistent theory
requires momentum dependence of a sort that does not appear in such a
Lagrangian, which demands that calculations must include loops. In
particular it is the analyticity properties of these higher-order
contributions that are required by unitarity, and there is an
interesting program to understand the set of functions satisfying those
properties at each loop order as a way to bootstrap the structure of
multi-loop amplitudes (see e.g. [ Goncharov:2009 , Goncharov:2010jf ,
Duhr:2011zq , Gaiotto:2011dt , Duhr:2014woa , Arkani-Hamed:2016byb ] ).
So far from being ‘merely’ a way to deal with seemingly unphysical
predictions, renormalization is very closely tied to the physics. We
begin in the next section with understanding its use for removing
divergences, as this is the most basic application and is often the
first introduction students receive to renormalization. We will then
move on to discuss other, more physical interpretations of
renormalization.

#### 2.1 To Remove Divergences

##### Physical input is required

As a first pass, let’s look again at a @xmath theory

  -- -------- -- ------
     @xmath      (15)
  -- -------- -- ------

and now treat it properly as a quantum field theory. As a simple
example, let us consider @xmath scattering in this theory, our
discussion of which is particularly influenced by Zee [ Zee:2003mt ] .
At lowest-order this is extremely simple, and the tree-level amplitude
is @xmath . But if we’re interested in a more precise answer, we go to
the next order in perturbation theory and we have the one-loop diagrams
of Figure 5 .

Defining @xmath as the momentum flowing through the loop in the @xmath
-channel diagram, that diagram is evaluated as

  -- -------- -- ------
     @xmath      (16)
  -- -------- -- ------

Just from power-counting, we can already see that this diagram will be
divergent. In the infrared, as @xmath , the diagram is regularized by
the mass of the field, but in the ultraviolet @xmath , the integral
behaves as @xmath which is logarithmically divergent. Though one might
be tempted now to give up, we note that this divergence is appearing
from an integral over very high energy modes—far larger than whatever
energies we’ve verified our @xmath model to, so let’s try to ignore
those modes and see if we can’t get a sensible answer. The general term
for removing these divergences is ‘regularization’ and we will here
regularize (or ‘regulate’) this diagram by imposing a hard momentum
cutoff @xmath in Euclidean momentum space, which is the maximum energy
of modes we let propagate in the loop. The loop amplitude may then be
calculated with elementary methods detailed in, for example, Srednicki’s
textbook [ Srednicki:2007qs ] . First we introduce Feynman parameters to
combine the denominators, using @xmath , which here tells us

  -- -------- -------- -- ------
     @xmath   @xmath      (17)
              @xmath      (18)
  -- -------- -------- -- ------

where we’ve skipped the algebra letting us rewrite this with @xmath and
@xmath . The change of variables @xmath has trivial Jacobian, so the
next step is to Wick rotate—Euclideanize the integral by defining @xmath
, such that @xmath . The measure simply picks up a factor of @xmath ,
@xmath , and we can then go to polar coordinates via @xmath . Lorentz
invariance then means the angular integral gives us the area of the unit
sphere in @xmath dimensions, @xmath , where @xmath , and the radial
integral becomes

  -- -------- -------- -- ------
     @xmath   @xmath      (19)
              @xmath      (20)
  -- -------- -------- -- ------

In fact it is possible to do the @xmath integral analytically here, but
we’ll take @xmath to find a simple answer

  -- -------- -- ------
     @xmath      (21)
  -- -------- -- ------

Now putting all that together and including all the diagrams up to
one-loop, we get the form

  -- -------- -- ------
     @xmath      (22)
  -- -------- -- ------

where @xmath are the Mandelstam variables and @xmath is just a numerical
coefficient. Now we see explicitly that the divergence has led to
dependence of our amplitude on our regulator @xmath . Of course this is
problematic because we introduced @xmath as a non-physical parameter,
and it would not be good if our calculation of a physical low-energy
observable depended sensitively on how we dealt with modes in the far
UV. But let us try to connect this with an observable anyway. We note
that the theory defined by the Lagrangian in Equation 15 can not yet be
connected to an observable because we have not yet given a numerical
value for @xmath . So let’s imagine an experimentalist friend of ours
prepares some @xmath s and measures this scattering amplitude at some
particular angles and energies corresponding to values of the Mandelstam
variables @xmath . They find some value @xmath , which is a pure number.
If our theory is to describe this measurement accurately, this tells us
a relation between our parameters and a physical quantity

  -- -------- -- ------
     @xmath      (23)
  -- -------- -- ------

This is known as a ‘renormalization condition’ which tells us how to
relate our quantum field theories to observations at non-trivial loop
order. Since the left hand side is a physical quantity, it may worry us
that the right hand side contains a non-physical parameter @xmath . But
we still haven’t said what @xmath is, so perhaps we’ll be able to find a
sensible answer if we choose @xmath in a correlated way with our
regularization scheme. We call this ‘promoting @xmath to a running
coupling’ by changing from the ‘bare coupling’ @xmath to one which
depends on the cutoff. So let’s solve for @xmath in terms of @xmath and
@xmath . Rearranging we have

  -- -------- -------- -- ------
     @xmath   @xmath      (24)
     @xmath   @xmath      (25)
  -- -------- -------- -- ------

where in the second line the replacement @xmath modifies the right side
only at higher-order and so that is absorbed into our @xmath
uncertainty. To see what this has done for us, let us plug this back
into our one-loop amplitude Equation 22 . This will impose our
renormalization condition that our theory successfully reproduces our
experimentalist friend’s result. We find

  -- -------- -------- -- ------
     @xmath   @xmath      (26)
              @xmath      
  -- -------- -------- -- ------

where again we liberally shunt higher-order corrections into our
uncertainty term. Taking advantage of the nice properties of logarithms,
we rearrange to get

  -- -------- -- ------
     @xmath      (27)
  -- -------- -- ------

We see that our renormalization procedure of relating our theory to a
physical observable has enabled us to write the full amplitude in terms
of physical quantities, and remove the divergence entirely. This one
physical input at some particular kinematic configuration has enabled us
to fully predict any @xmath scattering in this theory. We thus see how
the renormalization procedure removes the divergences in a naïve
formulation of a field theory and allows us to make finite predictions
for physical observables. While we did need to introduce a regulator,
once we make the replacement @xmath as defined in Equation 25 (and
similar replacements for the coefficients of the other operators), the
one-loop divergences are gone. We are guaranteed that any one-loop
correlation function we calculate is finite in the @xmath limit, which
removes the regulator. If we wanted to increase our precision and
calculate now at two loops, we would first renormalize the theory at two
loops analogously to the above, and would find a more precise definition
for @xmath which included terms of order @xmath . At each loop order,
replacing the bare couplings with running couplings suffices to entirely
rid the theory of divergences.

##### Renormalizability

An important question is for which quantum field theories do a finite
set of physical inputs allow the theory to be fully predictive, in
analogy to the example above. Such a theory is called ‘renormalizable’
and means that after some finite number of experimental measurements, we
can predict any other physics in terms of those values. Were this not
the case, and no finite number of empirical measurements would fix the
theory, it would not be of much use. Within the context of perturbation
theory, a theory will be renormalizable if its Lagrangian contains
solely relevant and marginal operators, and indeed for our @xmath theory
three renormalization conditions are needed—one for each such operator.
The simplest way to understand why we must restrict to relevant and
marginal operators is that irrelevant operators inevitably lead to the
generation of a tower of more-and-more irrelevant operators. To see
this, imagine now including a @xmath interaction, as depicted in Figure
5(a) . At one loop this leads to a @xmath scattering process with the
same sort of divergence we saw in our previous loop diagram. So this
loop is probing the UV physics, but we cannot absorb the unphysical
divergence into a local interaction in our Lagrangian unless we now
include a @xmath term. But then we can draw a similar one-loop diagram
with the @xmath interaction which will require a @xmath interaction, and
so on. Note that in our @xmath theory we also have @xmath scattering at
one loop, seen in Figure 5(b) , but there it comes from a box diagram
which is finite, and so there is no need to include more local
operators.

However, we emphasized above that the most useful description of a
system depends on the precision at which one wishes to measure
properties of the system. Thus in the study of effective field theories
a broader definition of renormalizability should be used. For a theory
with cutoff @xmath , one decides to work to precision @xmath where
@xmath is a typical energy scale of a process and @xmath is some
integer. There are then a finite number of operators which contribute to
processes to that precision—only those up to scaling dimension @xmath
—and so there is a notion of ‘effective renormalizability’ of the
theory. We still require solely a finite number of inputs to set the
behavior of the theory to whatever precision we wish, but such a theory
nevertheless fails the original criterion, which may be termed ‘power
counting renormalizability’ in comparison.

##### Wilsonian renormalization of @xmath

Above we characterized our cutoff as an unphysical parametrization of
physics at high scales that we do not know and we found that its precise
value dropped out of our physically observable amplitude. To some extent
this is rather surprising, as it’s telling us that the high energy modes
in our theory have little effect on physics at long distances—we can
compensate for their effects by a small shift in a coupling. We can gain
insight into the effects of these high energy modes by taking the cutoff
seriously and looking at what happens when the cutoff is lowered. This
brilliant approach due to Wilson [ Wilson:1974mb ] is aimed at providing
insight as to the particular effects of these high-energy modes by
integrating out ‘shells’ of high-energy Euclidean momenta and looking at
the low-energy effects. This discussion is closely inspired by that in
Peskin & Schroeder’s chapter 12 [ Peskin:1995ev ] , as well as
Srednicki’s chapter 29 [ Srednicki:2007qs ] . It is easiest to see how
to implement this by considering the path integral formulation. We can
equally well integrate over position space paths as over momentum modes:

  -- -------- -- ------
     @xmath      (28)
  -- -------- -- ------

and here it is clear that we may integrate over particular momentum
modes separately if we so choose. In the condensed matter application in
which Wilson originally worked, a cutoff appears naturally due to the
lattice spacing @xmath giving an upper bound on momenta @xmath . In a
general application we can imagine defining the theory with a
fundamental cutoff @xmath by including in the path integral only modes
with Euclidean momentum @xmath . ⁷ ⁷ 7 It is necessary to define this
cutoff in Euclidean momentum space for the simple fact that in
Lorentzian space a mode with arbitrarily high energy @xmath may have
tiny magnitude by being close to the light cone @xmath . It is left as
an exercise for the reader to determine what deep conclusion should be
taken away from the fact that we generally perform all our QFT
calculations in Euclidean space. The theory is defined by the values of
the parameters in the theory with that cutoff—our familiar relevant and
marginal operators @xmath and in principle all of the ‘Wilson
coefficients’ of irrelevant operators as well, since the theory is
manifestly finite. The idea is to effectively lower the cutoff by
explicitly integrating out modes with @xmath for some @xmath . This will
leave us with a path integral over modes with @xmath —which is a theory
of the same fields now with a cutoff @xmath . By integrating out the
high-energy modes we’ll be able to track precisely their effects in this
low-energy theory. Peskin & Schroeder perform this path integral
explicitly by splitting the high energy modes into a different field
variable and quantizing it, but since we’ve already introduced the
conceptual picture of integrating fields out we’ll take the
less-involved approach of Srednicki. To repeat what we discussed above,
the diagrammatic idea of integrating out a field is to remove it from
the spectrum of the theory and reproduce its effects on low-energy
physics by modifying the interactions of the light fields. Performing
the path integral over some fields does not change the partition
function, so the physics of the other fields must stay the same. We want
to do the same thing here, but integrate out solely the high energy
modes of a field and reproduce the physics in terms of the light modes.
We’ll continue playing around with @xmath theory and define our
(finite!) theory with a cutoff of @xmath , which in full generality
looks like:

  -- -------- -- ------
     @xmath      (29)
  -- -------- -- ------

For simplicity we will decree that at our fundamental scale @xmath we
have a canonically normalized field @xmath and no irrelevant
interactions @xmath , but just some particular @xmath and @xmath . Let’s
look first at the one-loop four-point amplitude, which we must ensure is
the same in both the theory with cutoff @xmath and that with cutoff
@xmath . In the original theory, the amplitude at zero external momentum
is

  -- -------- -- ------
     @xmath      (30)
  -- -------- -- ------

When we evaluate this in the theory with a lowered cutoff @xmath , the
modification is simply to everywhere make the replacement @xmath . In
order for the physics to remain the same without the high-energy modes,
the vertex function must not change. We’ll take full advantage of the
perturbativity of the result—that is, @xmath , @xmath —to swap out
quantities evaluated at @xmath in the second-order term for those
evaluated at @xmath at the cost solely of higher-order terms which we
ignore.

  -- -------- -------- -- ------
     @xmath   @xmath      (31)
              @xmath      
              @xmath      
              @xmath      
              @xmath      
     @xmath   @xmath      (32)
  -- -------- -------- -- ------

The effect of high energy modes on the four-point vertex function can be
simply absorbed into a shift in the coupling constant! This procedure
explicitly transfers loop-level physics in the theory defined at @xmath
into tree-level physics at @xmath . We can repeat this for the two point
function to find the behavior of @xmath and @xmath .

  -- -------- -------- -- ------
     @xmath   @xmath      (33)
              @xmath      
              @xmath      
              @xmath      
              @xmath      
     @xmath   @xmath      (34)
     @xmath   @xmath      (35)
  -- -------- -------- -- ------

We have again liberally ignored subleading terms. We see that the
wavefunction normalization @xmath does not run at one-loop in this
theory, since the only one-loop diagram contributing to the two-point
function does not have external momentum routed through the loop. This
is merely ‘accidental’ as @xmath is not symmetry-protected and does run
at two-loops. We also see the first hints of a somewhat worrisome
situation with scalar masses. The mass @xmath receives large one-loop
corrections which tend to raise the mass up to near the cutoff,
regardless of whether we originally had @xmath . We will investigate
this in great detail later. Now imagine we want to measure some
properties of @xmath particles with external momenta far below our
fundamental cutoff @xmath . By construction, our procedure of
integrating out high-energy momentum modes keeps the physics of these
low-energy particles the same. But if we calculate this scattering
amplitude using @xmath , it is not easy to see from the Lagrangian how
these low-energy modes will behave, since important effects are hidden
in loop diagrams. If we instead first integrate out momentum shells down
to some @xmath with @xmath , then the effects of the high energy modes
have been absorbed into the parameters of our Lagrangian, and we can
read off much more of how @xmath particles will behave at low energies
simply by looking at the parameters. We can see further value in this
approach if we consider scattering more low-energy @xmath . Let’s look
at the @xmath -point vertex function at zero momentum—in the theory at
@xmath , we start with @xmath and a one-loop diagram where momenta up to
@xmath run in the loop:

  -- -- -- ------
           (36)
  -- -- -- ------

Now in the theory at @xmath , the loop only contains momenta up to
@xmath , so we must account for the difference with a contact
interaction @xmath :

  -- -------- -- ------
     @xmath      (37)
  -- -------- -- ------

Again we should ensure that the physics is the same upon lowering the
cutoff:

  -- -------- -------- -- ------
     @xmath   @xmath      (38)
     @xmath   @xmath      (39)
              @xmath      (40)
  -- -------- -------- -- ------

So this renormalization procedure is especially useful for understanding
the behavior of irrelevant interactions. In our original theory nothing
about the six-particle interaction was obvious from the Lagrangian, but
in our theory with cutoff @xmath we can simply read off the strength of
this interaction at lowest order. Note also the inverse behavior to that
of the mass corrections—for the irrelevant interaction, the most
significant contributions to the infrared behavior come from the low
-energy part of the loop integral, and the UV contributions are
suppressed relative to this. Similarly, if we had started with a nonzero
@xmath which was small in units of the cutoff @xmath (so perturbative),
such a UV contribution will also be subdominant. Then fully generally
here, we have

  -- -------- -- ------
     @xmath      (41)
  -- -------- -- ------

as we evolve to low scales @xmath . The Wilsonian approach we have
discussed here gives useful intuition for how renormalization works as a
coarse-graining procedure wherein one changes the fundamental
‘resolution’ of the theory, but in practice can make calculations
cumbersome. Furthermore, the hard momentum-space cutoff we used is not
gauge-invariant, which causes difficulties in realistic applications.
The benefit, however, is that this is a ‘physical renormalization
scheme’ in which the renormalization condition relates the bare
parameters to physical observables. For this reason, this
renormalization scheme satisfies the Appelquist-Carrazone decoupling
theorem [ Appelquist:1974tg ] , which is enormously powerful. This
guarantees for us that the effects of massive fields can, at low
energies, simply be absorbed into modifications of the parameters in an
effective theory containing solely light fields. In the next section
we’ll return to a clarifying example of the meaning of the decoupling
theorem, and also discuss a renormalization scheme which does not
satisfy the requirements for this theorem to operate, but is far simpler
to use for calculations. The winning strategy will be to input
decoupling by hand, which will allow us to get sensible physical results
without the computational difficulty. Before we get to that, though,
we’ll take a couple detours.

##### Renormalization and locality

We have seen that the need to remove divergences in our theory led to
the introduction of running couplings which change as a function of
scale. In our example above we see that renormalization has the
operational effect of transferring loop-level physics into the
tree-level parameters. This is an interesting perspective which bears
further exploration—if there is hidden loop-level physics that really
has the same physical effects as the tree-level bare parameters, perhaps
this is a sign that there is a better way to organize our perturbation
theory. Indeed, at some very general level renormalization can be
thought of as a method for improving the quality of perturbation theory.
For useful discussions at this level of abstraction of how
renormalization operates, see [ Delamotte:2002vw ] for its natural
appearance whenever infinities are encountered in naïve perturbative
calculations, and [ neumaier_2015 ] for its usefulness even when
infinities are not present. We’ll discuss this perspective on
renormalization further in the next section. However it’s clear that
loops also give rise to physics that is starkly different from the
lowest-order result (e.g. non-trivial analytic structure), so how do we
know what higher-order physics we can stuff into tree-level? In a
continuum quantum field theory, a Lagrangian is a local object @xmath
—that is, it contains operators like @xmath which give an interaction
between three @xmath modes at a single spacetime point @xmath . Such
effects are known as ‘contact interactions’, but even at tree-level a
local Lagrangian can clearly produce non-local (that is, long-range)
physics effects. For example, consider the amplitude for @xmath
scattering in a @xmath theory at second order in the coupling.

In position space the non-locality here is obvious, as in Figure 7 : A
simple tree-level diagram corresponds to a particle at point @xmath and
a particle at point @xmath exchanging a @xmath quantum, but one may
forget this important fact when working in momentum space. There the
result is

  -- -------- -- ------
     @xmath      (42)
  -- -------- -- ------

and indeed, Fourier transforming the cross-section for this process
yields a Yukawa scattering potential for our @xmath particles, showing
that they mediate a long-range force over distances @xmath . We
obviously cannot redefine the Lagrangian to put this effect into the
lowest order of perturbation theory since this is not a local effect.
But if we do have a continuum quantum field theory, then because of
locality it describes fluctuations on all scales. When we go to
loop-level, we must integrate over all possible internal states, which
includes integrating over arbitrarily large momenta or equivalently
fluctuations on arbitrarily small scales. Heuristically, when the loop
integral is sensitive to the ultraviolet of the theory, it is computing
effects that operate on all scales—that is, it gives a contribution to
local physics. This tells us that the pieces of this higher-order
contribution which we can reshuffle into our Lagrangian are connected
with ultraviolet sensitivity, leading to a close connection of
renormalization with divergences. A couple notes are warranted about the
notion of locality we rely on here. Firstly, it’s clear that this
criterion of local effects appears because we began with a local
Lagrangian. If we postulated that our fundamental theory contained
nonlocal interactions, say @xmath , then we could clearly absorb further
nonlocalities with the same structure into this coupling as well.
However, this sort of nonlocality is different from the nonlocality we
saw appearing out of the local theory at tree-level. In particular it
would break the standard connection between locality and the analytic
structure of amplitudes—see e.g. Schwartz [ Schwartz:2013pla ] or
Weinberg [ Weinberg:1995mt ] on ‘polology’ and locality. Secondly, our
notion of locality should be modified in a low-energy theory with an
energy-momentum cutoff @xmath , as can be seen in hindsight in our
Wilsonian discussion above. As @xmath defines a maximum energy scale we
can probe, there is equivalently a minimum time and length scale we can
probe due to the uncertainty principle, heuristically @xmath . As a
result, any fluctuations on shorter length scales are effectively local
from the perspective of the low-energy theory. An exchange of a massive
field with @xmath or of a light field with high frequency @xmath appears
instantaneous to low-energy observers. This explains how it’s sensible
to use renormalization techniques in, for example, condensed matter
applications, where systems are fundamentally discrete. We can see this
concretely by imagining the light @xmath s in the tree-level example
above instead exchanged a heavier scalar @xmath with mass @xmath . While
the amplitude @xmath is still nonlocal in the continuum theory, if we’re
only interested in physics at energies @xmath we may Taylor expand the
result @xmath . We may then absorb the leading effects of this heavy
scalar into an effectively local interaction @xmath among the light
fields—as long as we work at energies below @xmath . In the next section
we’ll explore concretely how these insights enable us to transfer
loop-level physics to tree-level physics, and so improve our
calculations.

#### 2.2 To Repair Perturbation Theory

##### Renormalization group equations

The astute reader may notice a potential issue with our one-loop results
in the @xmath theory, Equations 32 , 34 , 35 . We’ve derived this
behavior as the first subleading terms in a series expansion in the
number of loops. In relation to the tree-level results, the one-loop
contributions are suppressed by a factor @xmath , where I’ve switched
notation to now have @xmath as a high scale and @xmath as the lower
scale we integrate down to. Higher @xmath -loop contributions will be
further suppressed by @xmath loop factors. But what if we wanted to
study physics at a scale far lower than @xmath ? Eventually this factor
becomes large enough that we will need to compute many loops to obtain
high precision, and then large enough that we have reason to question
the convergence of the series ⁸ ⁸ 8 Please excuse my slang. Perturbative
series in QFT are quite generally not convergent but we can trust the
answers anyway to order @xmath because they are asymptotic series. So
really when this parameter becomes large enough we worry that our series
is not even asymptotic. Thinking deeply about this leads to many
interesting topics in field theory, from accounting for nonpertubative
instanton effects which are (partially) behind the lack of convergence;
to the program of ‘resurgence’, the idea that there are secret relations
between the perturbative and nonperturbative pieces. This is all far
outside my purview here, but some introductions aimed at a variety of
audiences can be found in [ Schafer:1996wv , Vandoren:2008xg ,
Dunne:2012ae , Dunne:2014bca , Dorigoni:2014hea , Marino:2015yie ] . .
Keep in mind that we are in the era of precision measurements of the
Standard Model, so these one-loop expressions are very restrictive. For
a concrete example, say we wanted to check the SM prediction for a
measurement of a coupling @xmath with @xmath whose experimental
uncertainty was @xmath . Let’s define a theoretical uncertainty on a
perturbative calculation to @xmath order as

  -- -------- -- ------
     @xmath      (43)
  -- -------- -- ------

where our Wilsonian calculation in Section 2.1 gave at first order, as a
reminder (and with modified notation)

  -- -------- -- ------
     @xmath      (44)
  -- -------- -- ------

and our heuristic estimate for the size of the second order correction
is @xmath . When the result is simply a series, the uncertainty is very
simple to calculate, as the numerator is then our estimate of the @xmath
order correction, which is roughly the square of the first order
correction in this case for @xmath . Here we have @xmath . Then in order
for our theoretical uncertainty to be subdominant to the experimental
precision, @xmath , we must go past the one-loop result if we wish to
look at energies below @xmath , the two-loop result is only sufficient
until @xmath , and if we can manage to calculate the three-loop
corrections that only gets us down to something like @xmath . If we’re
interested in taking the predictions of a Grand Unified Theory defined
at @xmath and comparing them to predictions at SM energies, how in the
world are we to do so? Fortunately, we can do better by applying our
one-loop results more cleverly. It is clear by looking at Equations 32 ,
34 , 35 that the results have the same form no matter the values of
@xmath . So if we take @xmath to be only slightly smaller than @xmath
(corresponding to @xmath in our previous notation) the expansion
parameter becomes very small and the one-loop result becomes very
trustworthy. What we would like is some sort of iterative procedure to
gradually lower the cutoff, which we could then use to find the one-loop
result for energies far lower than the range of our perturbative series.
This is in fact precisely the sort of problem that a differential
equation solves, and we can derive such an equation by differentiating
both sides by @xmath and then taking @xmath infinitesimally close to
@xmath . That exercise yields

  -- -------- -------- -- ------
     @xmath   @xmath      (45)
     @xmath   @xmath      (46)
     @xmath   @xmath      (47)
  -- -------- -------- -- ------

These are known as ‘renormalization group equations’ and they indeed
allow us to evolve the coupling down to low energies—one says we use
them to ‘resum the logarithm’. Then to study physics at a very low scale
we can bring these couplings down to a lower scale and do our loop
expansion using those couplings, which is known as ‘renormalization
group improved perturbation theory’, and which we will discuss in more
detail soon. Explicitly solving with our boundary condition at @xmath
yields

  -- -------- -- ------
     @xmath      (48)
  -- -------- -- ------

Turning back to our effective field theory language, we see that quantum
corrections have generated an anomalous dimension for @xmath , @xmath ,
correcting the leading order scaling behavior. Since @xmath , we’ve
determined that the quartic interaction is marginally irrelevant, which
we will return to later. We can now look at the theoretical uncertainty
in this one-loop resummed calculation by including an estimate of the
next order correction to the running of the quartic @xmath and resumming
that expression. This can no longer be done analytically, but numerical
evaluation easily reveals that the theoretical uncertainty here stays
below @xmath for many, many orders of magnitude below @xmath . Resumming
the logarithmic corrections allows us to use our loop results to far
greater effect.

##### Decoupling

The physical meaning and technical statement of the decoupling theorem
commonly confuse even prominent practitioners of effective field theory,
so it’s worth going clearly through an example to refine our
understanding. Indeed, one may be confused just at zeroth order about
how decoupling is sensible against the background of the hierarchy
problem—which is an issue of sensitivity of a scalar mass to heavy mass
scales. How can we claim QFT obeys a decoupling theorem and then go on
to worry at length about quantum corrections @xmath ? The correct way to
think about the decoupling theorem is not whether a top-down calculation
could yield a result that depends on heavy mass scales, but whether a
bottom-up effective field theorist and low-energy observer could gain
information about the heavy mass scales through low-energy measurements.
We can clarify this important difference by looking at a one-loop mass
correction to a light scalar @xmath of mass @xmath from a heavy scalar
@xmath of mass @xmath through the interaction @xmath . We again take a
Wilsonian perspective and begin at a scale @xmath . In close analogy to
what we had before, we now find

  -- -------- -------- -- ------
     @xmath   @xmath      (49)
              @xmath      
              @xmath      
              @xmath      
              @xmath      
     @xmath   @xmath      (50)
  -- -------- -------- -- ------

And we may already exhibit the confusion. If we use this to calculate
the mass at a low scale @xmath , we see that @xmath does depend on the
heavy mass scale, and gets a contribution which goes like @xmath .
However, the effect on the light scalar is an additive shift of the
mass. If we go out and measure the mass at a single scale @xmath we
can’t tell empirically which ‘parts’ of that came from @xmath and which
came from @xmath or whatever else is in there, so we have no idea of how
this low-energy measurement depends on heavy scales. To get information
about the various contributions to the light scalar mass, we can measure
it at different scales and look at how it changes. Of course this
information is contained in the renormalization group equation for
@xmath . At @xmath , we can find this by differentiating the above, and
we find

  -- -------- -- ------
     @xmath      (51)
  -- -------- -- ------

Now we can see the difference. If we perform low-energy observations
where we can take the cutoff below the mass of the heavy scalar @xmath ,
then the physics of the heavy scalar decouples from the running of the
light scalar mass. It is only by studying this running at low energies
that we can gain information about the ultraviolet, and we see that this
information is contained solely in small corrections scaling as @xmath .
At low energies, to learn about short-distance physics we must make very
precise measurements of the low-energy physics. This is the sense in
which heavy mass scales decouple from the theory in the infrared.

##### Renormalized perturbation theory

Now let us study another, slightly more complex theory and apply
renormalization techniques to simplify our calculations. We avoid the
complication of gauge symmetries and focus instead on a Yukawa theory of
a Dirac fermion interacting with a parity-odd scalar. Our first
improvement to perturbation theory will be to switch from ‘bare’ to
‘renormalized perturbation theory’. Let’s first recap our procedure in
Section 2.1 . We began with a Lagrangian with bare parameters @xmath ,
introduced a regulator, computed the physical parameters @xmath in terms
of the bare ones, inverted those relationships, and then plugged in for
the bare parameters in terms of the renormalized ones, after which we
were left with an amplitude which remains finite as we remove the
regulator. This procedure works to remove the divergences in any
renormalizable theory, but is obviously rather cumbersome. Furthermore
one may question the validity of performing a perturbative expansion in
a bare parameter which we later discover is formally infinite in the
continuum theory @xmath . It is both conceptually and computationally
easier to instead start off by performing perturbation theory in terms
of the renormalized parameters which we know to be finite by definition.
Fortunately we can improve our accounting simply by reshuffling the
Lagrangian as follows. In terms of the bare parameters and fields, the
Lagrangian reads

  -- -------- -------- -- ------
     @xmath   @xmath      (52)
     @xmath   @xmath      (53)
  -- -------- -------- -- ------

where we’ve split up the free and interaction parts. Just as in our
earlier example, when we compute at one-loop these parameters will get
corrections such that the bare parameters are no longer the physical
parameters we measure. Anticipating that fact, let us rewrite the
Lagrangian to explicitly account for those corrections from the outset.
Although it was not a feature of our simple example above, in general
there will be ‘wavefunction renormalization’ which changes the
normalization of our field operators, so we define @xmath where @xmath
are now renormalized fields, We do the same to define renormalized
masses related to the bare masses as @xmath , and for the couplings
@xmath . Next we use the brilliant strategy of adding zero to split
these @xmath -factors into a piece with the same form we started with
and a ‘counterterm’ proportional to @xmath . Since at tree-level there’s
no renormalization needed, we know @xmath . At nontrivial loop level, we
must choose the @xmath -factors to implement our chosen renormalization
scheme. The Lagrangian now takes the form

  -- -------- -------- -- ------
     @xmath   @xmath      (54)
     @xmath   @xmath      (55)
     @xmath   @xmath      (56)
  -- -------- -------- -- ------

where we’ve split off the counterterms into @xmath . We can now treat
the terms in @xmath simply as additional lines and vertices contributing
to our Feynman diagrams. We’ll see how useful this is once we begin
renormalizing the theory. This is done in full in Srednicki’s chapters
51-52 [ Srednicki:2007qs ] , so we will not go through every detail.

##### Continuum renormalization

We’ll regulate this theory using ‘dimensional regularization’ (dim reg)
which analytically continues the theory to general dimension @xmath .
That this will regulate our theory is not obvious, but I recommend
Georgi [ Georgi:1994qn ] to convince yourself of this and Collins [
Collins:1984xc ] for a full construction of dim reg; we’ll content
ourselves with seeing it in action. Our renormalization scheme will be
‘modified minimal subtraction’ and denoted @xmath , where ‘minimal
subtraction’ means we’ll choose our counterterms solely to cancel off
the divergent pieces (rather than to enforce some relation to physical
observables, as we did previously) and ‘modified’ means that actually
it’s a bit nicer if we cancel off a couple annoying constants as well.
Since we’re using @xmath , the mass parameters @xmath will not quite be
the physical masses, which are always the locations of the poles in the
propagators, and the fields will not be normalized to have unity residue
on those poles. So we’ll have to relate these parameters to the physical
ones later. We’ll briefly go through renormalizing the scalar two-point
function at one loop to evince dim reg and @xmath . In our one-loop
diagrams we use propagators given by @xmath , since we know the
counterterms begin at higher order. The full details of the one-loop
renormalization of this theory can be found in Srednicki’s Chapter 51.

At one-loop, the scalar two-point function gets corrections due to both
interactions, as seen in Figure 8 . There is the diagram we had in the
@xmath theory above, but we must recompute this in dim reg

  -- -------- -------- -- ------
     @xmath   @xmath      (57)
              @xmath      (58)
              @xmath      (59)
              @xmath      (60)
              @xmath      (61)
              @xmath      (62)
  -- -------- -------- -- ------

where we have analytically continued to @xmath dimensions including
replacing @xmath , with @xmath a mass scale, to keep @xmath
dimensionless; performed the integral in general dimension; expanded for
@xmath ; and defined @xmath , where @xmath is the Euler-Mascheroni
constant, to simplify the expression. Details on these calculational
steps are laid out in Srednicki’s Chapter 14, but there’s an important
and naïvely surprising feature of the above that we should discuss. In
previous sections our regularization scheme explicitly introduced a mass
scale @xmath which we could think of as having a physical interpretation
as some sort of short-distance cutoff, if we wished. We then saw that by
cleverly studying how the parameters in the theory are modified as we
change @xmath and demand the physics stays the same, we could make a
variety of things easier to calculate and make the physical content of
the theory more transparent. Note that in doing so, we’re stretching the
meaning of the scale @xmath away slightly from that physical picture—we
don’t care what the ‘real’ cutoff of our system is, or if there really
is any sort of cutoff; we simply know that allowing such
scale-dependence in our couplings and studying the theory at different
values of @xmath makes our lives easier, so we imagine varying it. Now
the way this new regularization scheme works is somewhat opaque, but it
still necessitates the introduction of a new scale. In this case, the
unphysical scale @xmath is required to ensure that our couplings remain
dimensionless away from @xmath . This scheme thus invites us to further
broaden our notion of varying a scale to study the theory at different
energies—this time the scale explicitly never had a physical
interpretation. We can view @xmath as labeling a one-parameter family of
calculational schemes. We’ve ensured by construction that the physics is
the same no matter what @xmath we choose, but by cleverly using the
scale-dependence we can make our lives far easier. The intuition should
be the same as in the previous case, and lowering @xmath does likewise
transfer loop-level physics to tree-level physics and can be used to
improve the convergence of perturbative calculations. The connection is
now slightly more opaque, which is why we began by discussing a cutoff
in Euclidean momentum space, but the calculations become far simpler.
There’s another diagram with a @xmath loop

  -- -------- -------- -- ------
     @xmath   @xmath      (63)
              @xmath      (64)
  -- -------- -------- -- ------

with @xmath , whose evaluation follows similar steps but we skip for
brevity. Adding these together, @xmath tells us the @xmath counterterms
must take the values

  -- -------- -------- -- ------
     @xmath   @xmath      (65)
     @xmath   @xmath      (66)
  -- -------- -------- -- ------

For the fermion, evaluating the one-loop diagrams gives us the
counterterms

  -- -------- -------- -- ------
     @xmath   @xmath      (67)
     @xmath   @xmath      (68)
  -- -------- -------- -- ------

Since we didn’t choose the counterterm to keep the location of the pole
in the propagator fixed, @xmath is no longer the physical scalar mass.
But we can find the physical, ‘pole’ mass precisely from that condition:

  -- -------- -------- -- ------
     @xmath   @xmath      (69)
              @xmath      (70)
              @xmath      (71)
     @xmath   @xmath      (72)
  -- -------- -------- -- ------

where we have used our favorite trick to replace @xmath with @xmath in
the one-loop correction, since it is already higher order in couplings.

As for the interactions, we have a triangle diagram for the Yukawa
coupling and a new contribution to the quartic with a fermion running in
the loop, as depicted in Figures 8(a) and 8(b) respectively. These lead
to the counterterms

  -- -------- -------- -- ------
     @xmath   @xmath      (73)
     @xmath   @xmath      (74)
  -- -------- -------- -- ------

from which we’ll be able to understand how the strength of the
interactions changes as a function of the energy at which the theory is
probed.

##### Renormalization group improvement

Now the second improvement to perturbation theory is the RG-improved
perturbation theory we mentioned above. This takes on an even more
useful role in our continuum renormalization scheme here. In the
Wilsonian picture, @xmath was a high cutoff and we ensured the physics
was invariant under evolution of @xmath , but this scale still needed to
stay far above the scales of interest in the problem @xmath . Now the
scale @xmath is entirely unphysical and we are free to bring it all the
way down to the scales of kinematic interest—in fact doing so will
vastly simplify calculations. As a result we are able to make even more
use of the RG-improvement than we could above. We find the running
couplings by again using the fact that the bare parameters are
independent of the unphysical renormalization scale @xmath . Having
utilized a mass-independent regulator, a Wilsonian interpretation of
couplings running with the value of the regulator is nonsensical here
and so renormalization group improvement is the way to extract
predictions from this theory. We already have the relations between the
bare and renormalized quantities, e.g.

  -- -------- -- ------
     @xmath      (75)
  -- -------- -- ------

And since we know that the bare parameters are independent of @xmath by
definition, we have

  -- -------- -------- -- ------
     @xmath   @xmath      (76)
              @xmath      (77)
     @xmath   @xmath      (78)
              @xmath      (79)
              @xmath      (80)
  -- -------- -------- -- ------

If we expand @xmath order by order in @xmath , then matching the @xmath
terms gives @xmath and matching the @xmath terms tells us that, in the
@xmath limit, we have

  -- -------- -- ------
     @xmath      (81)
  -- -------- -- ------

This @xmath -independent piece is known as the ‘beta function’ for the
coupling, @xmath . Of course there are higher-order terms in @xmath
which are needed to match the @xmath terms, and which one can solve for.
But these vanish in the @xmath limit, so will not contribute to the
running of @xmath . We can now resum this logarithm to find the
evolution of this coupling with renormalization scale

  -- -------- -- ------
     @xmath      (82)
  -- -------- -- ------

where we’ve used the boundary condition @xmath . As before, the resummed
version will allow us to maintain precision to far lower scale than we
could with simply its leading order approximation. It’s useful to keep
in mind the Wilsonian picture as a clearer example because our regulator
had a physical interpretation. The point is that the logarithms are
really what’s encoding how couplings change as a function of scale; in
the Wilsonian calculation it was obvious that the logarithmic
contribution @xmath is present no matter the initial cutoff. One says
that couplings which receive logarithmic quantum corrections ‘get
contributions from all scales’. Then it’s clear why this RG-improvement
is sensible—though we may start at some particular @xmath or @xmath , a
one-loop calculation offers information on the lowest-order logarithmic
running over all momenta, and we may sum up those modifications to
improve our perturbation theory.

#### 2.3 To Relate Theories

##### Mass-independent schemes and matching

We’ve seen already the necessity of renormalization when a theory
produces naïvely divergent results, and its enormous use in improving
the precision of perturbative calculations in a given theory. The last
facet we’ll discuss is its use in connecting theories. This is necessary
to use the computationally-simple scheme of dim reg with @xmath in
theories with different mass scales, and is very closely related to the
effective field theory philosophy we discussed in Section 1 . Cohen’s
monograph [ Cohen:2019wxr ] goes into far more depth than I will be able
to, and is a fantastic introduction to these ideas and their
application. This perspective on renormalization has also been of
enormous use in condensed matter to understand behaviors that appear in
many distinct systems in the long-distance limit, and has applications
in formal field theory to understand better the properties of QFT
itself. In the previous section we derived the beta function for Yukawa
theory in the @xmath scheme. As promised by our terming of this as a
‘mass-independent’ scheme, the beta functions indeed have no reliance on
the masses. But this should seem remarkably peculiar, as it suggests
that there is no decoupling at all. Were that the case, by measuring the
low-energy beta functions of QED we could tell how many charged
particles existed up to arbitrarily large mass scales! What has gone
wrong is that @xmath does not meet the criterion of a physical scheme
which is necessary for the Appelquist-Carrazone theorem to operate. In
@xmath the renormalization condition has nothing to do with physical
values of the parameters so, while it makes calculations far simpler,
@xmath has broken decoupling. To restore decoupling and allow us to
properly use a mass-independent scheme, we must implement the
mass-dependence ourselves by ‘matching’ the Yukawa theory at energies
above the fermion mass to a theory of solely light scalars at energies
below the spinor mass. To ‘match’, we consider some process which exists
in both theories—for example, @xmath scattering—and ensure that at the
matching scale @xmath both theories agree on the physics. In the
high-energy Yukawa theory we can run the RG scale all the way down from
a high scale @xmath to @xmath , the physical, pole mass of the fermion.
To get simple closed-form expressions, we’ll take the couplings small
enough that working to lowest order gives a good approximation. We’ll
denote all of the UV values with bars, e.g. @xmath . Firstly, we use the
counterterms to find the anomalous dimension of the fermion mass

  -- -------- -------- -- ------
     @xmath   @xmath      (83)
              @xmath      (84)
     @xmath   @xmath      (85)
              @xmath      (86)
  -- -------- -------- -- ------

We then find the fermion pole mass as

  -- -------- -------- -- ------
     @xmath   @xmath      (87)
     @xmath   @xmath      (88)
              @xmath      (89)
              @xmath      (90)
  -- -------- -------- -- ------

Now we need the value of the other parameters at that mass threshold

  -- -------- -------- -- ------
     @xmath   @xmath      (91)
     @xmath   @xmath      (92)
     @xmath   @xmath      (93)
     @xmath   @xmath      (94)
     @xmath   @xmath      (95)
     @xmath   @xmath      (96)
  -- -------- -------- -- ------

Now we are ready to proceed to even lower energies. We enforce
decoupling by matching to the low-energy theory of just a
self-interacting scalar. We have

  -- -------- -------- -- ------
     @xmath   @xmath      (97)
     @xmath   @xmath      (98)
     @xmath   @xmath      (99)
  -- -------- -------- -- ------

The counterterms and beta functions in this theory can be conveniently
found by truncating those found above. Of course, by construction, we
find that the heavy fermion @xmath no longer contributes to the running
of parameters at low-energies. To make sure we’re getting the physics
correct, we must impose the boundary condition that the predictions
match at @xmath , which here is quite simple—we just use the values at
@xmath in the UV theory as literal boundary conditions for our running
in the IR theory. In the IR theory, for @xmath , we have

  -- -------- -------- -- -------
     @xmath   @xmath      (100)
     @xmath   @xmath      (101)
              @xmath      (102)
              @xmath      (103)
  -- -------- -------- -- -------

The benefit is now clear. While the RGEs in the UV theory were very
complicated, the running of @xmath in the low energy theory is simple.
Our mass-independent scheme allows us to explicitly factorize these and
contain all the UV physics in the boundary condition, which lets us
study the low-energy theory in a simple manner. The general procedure of
renormalization group evolution in mass-independent schemes is called
‘running and matching’. The parameters in the Lagrangian run as you
evolve down in energies, but at a mass threshold @xmath we must match
the UV theory at @xmath from above to a theory without this field at
@xmath from below. When we match we ensure that the physics of the
low-energy fields stays constant as we cross that threshold and remove
that particle from the spectrum of our theory. This becomes less trivial
when we have multiple mass scales, so consider now upgrading our Yukawa
theory with additional fermions.

  -- -------- -- -------
     @xmath      (104)
  -- -------- -- -------

Now if we are given the theory at very high energies @xmath and we want
to understand what it looks like at very low energies, there is a
cascade of EFTs we evolve through. As depicted in Figure 10 , we run the
parameters down to the largest fermion mass, match to a theory with one
less fermion, run down again until the next mass scale, and so on.

As an example which is closer to the real world, consider QED with our
three generations of leptons (and ignoring the strong sector for
simplicity). At low energies we measure the asymptotic value @xmath ,
and in colliders we measure the value of the gauge coupling at the
@xmath mass @xmath . To compare these, we must run the high-energy value
all the way down into the infrared. Above @xmath we have a theory where
all of @xmath run in loops and giving an @xmath beta function @xmath .
But at @xmath we should remove the @xmath from our theory, such that
from @xmath down to @xmath we have @xmath . Below the @xmath we solely
have the electron and recover the textbook @xmath , and finally as we
cross the electron mass threshold @xmath we remove the electron from the
spectrum and find that the gauge coupling stops running @xmath .
Physically this corresponds to the fact that pure QED is
scale-invariant, meaning that the coupling will not evolve at all in a
theory with no charged particles. This is the regime in which classical
electrodynamics holds precisely (up to the presence of additional
interactions suppressed by powers of @xmath , that is). A possible
confusion is to conflate the mass-independence of the regularization
scheme with that of the renormalization scheme, and conclude that
dimensional regularization cannot be used if one wants decoupling
without having to integrate out and match. So lest one confuse the roles
let’s quickly look at an example of using dimensional regularization
with a renormalization scheme which does satisfy decoupling, known as
‘off-shell momentum subtraction’. For simplicity, we’ll look at the
anomalous dimension of our Yukawa scalar @xmath , and we’ll perform
wavefunction renormalization by subtracting the value of the graphs at
the off-shell momentum scale @xmath . In symbols this amount to the
prescription

  -- -------- -- -------
     @xmath      (105)
  -- -------- -- -------

Since we’re still using the same regularization scheme, we have the same
result for @xmath as above. We can then simply calculate the anomalous
scaling dimension as defined by @xmath ,

  -- -------- -------- -- -------
     @xmath   @xmath      (106)
              @xmath      (107)
  -- -------- -------- -- -------

This integral can be performed analytically, but the full expression is
unilluminating. However, it is useful to look at the limits

  -- -------- --
     @xmath   
  -- -------- --

to check if they agree with our expectations. At energies far above the
fermion mass its contribution to the scalar anomalous dimension cannot
know about that scale, and at energies far below its mass we expect
inverse dependence on the mass for decoupling to occur. This is
precisely what we find, so the lesson is that even if we didn’t want to
go through the trouble of integrating out the fermion and matching, we
could still make use of the magical regularization scheme that is dim
reg.

##### Flowing in theory space

Our interpretation of the renormalization group thus far has been as a
way of understanding what a particular theory looks like at different
energies. But there is another way of looking at it that is also useful,
for which we shall follow an example of Peskin & Schroeder, though I
recommend Skinner’s lecture notes [ skinner ] for clear explanation of
these concepts which goes farther than we have time to. Let’s return to
the idea of the Wilsonian path integral and successively integrating out
Euclidean momentum shells. In the previous section we began with a
scalar field theory

  -- -------- -- -------
     @xmath      (108)
  -- -------- -- -------

We then integrated over momentum shells from @xmath down to @xmath with
@xmath , and found we could express our result as (schematically; see 32
, 34 , 35 , 41 )

  -- -------- -------- -- -------
     @xmath   @xmath      (109)
     @xmath   @xmath      (110)
  -- -------- -------- -- -------

Above we interpreted this in terms of looking at the same theory at
lower energies, having coarse-grained over the largest momentum modes,
which is a useful way of comparing the two path integrals. Another
useful way to compare is to get them to a form where they look similar,
so let’s now define a change of variables @xmath , in terms of which the
path integral now looks like

  -- -------- -------- -- -------
     @xmath   @xmath      (111)
     @xmath   @xmath      (112)
  -- -------- -------- -- -------

We can transform the kinetic terms back to the canonical form with the
field redefinition @xmath , after which we can write the effective
Lagrangian as

  -- -------- -- -------
     @xmath      (113)
  -- -------- -- -------

with @xmath , and it’s clear that we could write such an effective
action regardless of what sort of coefficients we began with before
integrating out this momentum shell. Now our series of transformations
has effected the change (up to normalization)

  -- -------- -- -------
     @xmath      (114)
  -- -------- -- -------

Since all of our dynamical variables are integrated over in calculating
the partition function, we can view this as a transition in the space of
Lagrangians, @xmath . So this gives us an interpretation of the
renormalization group as a flow in ‘theory space’. This interpretation
invites us to conceptualize renormalization group flow as a path through
theory space between two conformal field theories (CFTs), as depicted in
Figure 11 . CFTs are quantum field theories with an enlarged spacetime
symmetry group ⁹ ⁹ 9 This statement may appear confusing if you have
come across the Coleman-Mandula theorem [ Coleman:1967ad ] , which
roughly says that the most amount of symmetry you can have in a QFT is
the direct product of the Poincaré spacetime symmetry and whatever
internal symmetries you have. However, that beautiful result relies on
properties of the S-matrix, and CFTs do not have S-matrices because they
do not have mass gaps, meaning this enlarged spacetime symmetry group
does not violate the theorem. We’ll see another, even more interesting
loophole in this theorem exploited in Section 7 , which comes from
enlarging our notion of what sorts of symmetries an S-matrix could
possess. , consisting essentially of a scaling symmetry. CFTs are fixed
points of RG flows---since they possess scaling symmetry they look the
same at all energy scales, so if an RG flow is to have an endpoint it
clearly must be a CFT ¹⁰ ¹⁰ 10 I’ve elided a subtlety here, which is
that it is not entirely known whether scale invariance in fact implies
conformality in four-dimensional QFTs, the latter of which includes also
invariance roughly under inversion of spacetime through a point. No
counterexamples have been found, despite much effort. Polchinski’s early
paper on the topic is a classic [ Polchinski:1987dy ] , and a recent
review can be found from Nakayama [ Nakayama:2013is ] . . There’s a
terminological confusion here, which is that the ‘renormalization group’
isn’t actually a group at all, since the operation of integrating out a
momentum shell is irreversible. This came up already above when we saw
that integrating out heavy fields means we can no longer compute
processes which have them as external fields (see Footnote 1 ). Flowing
to lower energies, or toward the IR, is really a coarse-graining
operation which does lose information about small scales, in precise
analogy to decreasing the resolution of an image. This means that RG
evolution is a directed flow, so there is a difference between fixed
points in the UV and in the IR.

Quantum field theories can have different sorts of fixed points. As a
familiar example, if the theory has a ‘mass gap’—no zero-energy
excitations in the infrared, which may be because one began solely with
massive fields or through dynamical mechanisms like Higgsing and
confinement—then one finds a ‘trivial’ fixed point. In the far infrared,
everything has been integrated out and there is not enough energy to
excite any modes. We know phenomenologically that this happens in QCD.
In the other familiar case one can have a ‘Gaussian’ or ‘free’ fixed
point if the theory contains massless fields which don’t interact, such
as in QED. At energies far below the mass of the lightest charged
particle this is a theory of free electromagnetism, though one can still
excite photons of arbitrarily-long wavelength. Such a Gaussian fixed
point occurs in the UV for QCD—the celebrated result of ‘asymptotic
freedom’ [ Gross:1973id , Politzer:1973fx ] —because the strong coupling
flows toward zero, giving a free theory. This famously cannot occur for
@xmath gauge theories whose couplings necessarily grow with increasing
energies, leading them to herald their own breakdown with the prediction
of a ‘Landau pole’ [ Landau ] , a finite UV energy where the
perturbative theory predicts the coupling becomes infinite. From one
perspective this is an inverse to the prediction of a confinement scale
in QCD, where the perturbative prediction is a blowup of the coupling at
low energies, as we’ll discuss further in Section 3.3 . In either case
the theory cannot make predictions for energies above the Landau pole or
below the confinement scale, respectively, and so can be called
inconsistent. It’s clear that the divergence of a coupling either in the
IR or the UV is problematic for a complete, consistent interpretation of
a QFT. This is precisely why the formal perspective on a well-defined
QFT is that it describes an RG flow between two CFTs, such that in
neither direction does a coupling grow uncontrollably. In fact this
provides an incredibly important perspective on renormalizability, which
we’ll get to momentarily. First, let us introduce somewhat of a
generalization of the ‘relevant/irrelevant’ terminology which we
introduced in Section 1 . We implicitly had in mind that we were
studying a theory in the vicinity of the Gaussian fixed point which we
perturbed with various field operators—indeed, this is precisely how we
normally carry out perturbative calculations—and our terminology
depended on that. Generally one wishes to imagine perturbing a CFT by a
particular operator, flowing down in energy, and seeing whether the
operator grows in importance—a relevant operator—or shrinks—an
irrelevant operator. Perturbing a CFT by an irrelevant operator does not
induce an RG flow (to a different CFT), so interesting dynamical RG
flows come from CFTs perturbed by relevant operators. This clearly
agrees with our power-counting notion of relevance when we’re near the
Gaussian fixed point, but works also if one is near a strongly-coupled,
interacting fixed point where one may not know how to do such
power-counting and anomalous dimensions of operators may grow to
overpower their classical dimensions. The importance of this language
was realized in particular by Polchinski in his pioneering article [
Polchinski:1983gv ] . He showed that in fact the intuitive notion of
‘power-counting renormalizability’ that the field had been building—that
for a theory near the Gaussian fixed point we could see whether it was
renormalizable merely by checking whether it has any coefficients of
negative mass dimensions—in fact maps on to a very general statement.
This is enormously powerful, as prior arguments for renormalizability
were made on a case by case basis and were complicated and messy and
graph-theoretic. His derivation of this fact is brilliant but requires
much work, so we’ll merely try to get a sense for why it should be true
by building on our intuition from our Wilsonian renormalization of
@xmath theory in Section 2.1 . So long as your theory has a finite
number of fields, all of which have mass dimension @xmath when
canonically normalized, then there are a finite number of relevant or
marginal operators. As we flow down in energy through theory space, we
saw above the sense in which those coefficients are UV-sensitive—the IR
coefficients of those operators are determined primarily by their UV
values and IR corrections are subdominant. Contrariwise, the
coefficients of the infinite number of possible irrelevant operators are
UV-insensitive, being determined primarily by the IR physics of the
relevant and marginal operators. That is, the RG flows are attracted to
a finite-dimensional submanifold of the space of Lagrangians. Consider a
Wilsonian RG flow where we start off, as above, by specifying a cutoff
@xmath and the values of coefficients @xmath of all the @xmath marginal
and relevant operators at that scale, as well as the coefficients @xmath
of however many irrelevant operators we wish to turn on. We can then
flow downwards in energy as normal, and at an energy scale @xmath let’s
say we measure the relevant and marginal coefficients @xmath at that
scale. The coefficients of the irrelevant operators are then dominated
by the infrared @xmath and @xmath up to precision @xmath from subleading
corrections, with @xmath the scaling dimension of the irrelevant
operator. So indeed, the RG flow is attracted to the @xmath -dimensional
surface described by @xmath and separate trajectories through theory
space as a function of scale @xmath which reach the same @xmath will
differ only by positive powers of @xmath . For less abstract discussion,
Polchinski goes through a simple example which may provide further
insight, and Schwartz discusses the same example in Chapter 23 of his
textbook [ Schwartz:2013pla ] . To see why this implies
renormalizability, recall that the program of Wilsonian renormalization
is to define renormalized, ‘running’ couplings as a function of scale to
keep infrared physics at @xmath fixed while ‘removing the cutoff’. A bit
more formally, we want a family of Lagrangians @xmath with coefficients
chosen as a function of @xmath such that each Lagrangian yields the same
low energy physics @xmath , in terms of which all the IR observables can
be calculated up to subleading corrections in @xmath . When the cutoff
is removed by taking @xmath , one thus recovers precisely the correct
physics, specified by those chosen values of the @xmath , which are the
renormalization conditions. If we can find such a family of Lagrangians,
then we say this theory is power counting renormalizable. Polchinski’s
argument shows that this can be done so long as one wishes solely to fix
the IR values of relevant and marginal couplings.

##### Trivialities

The attentive reader may at this point notice an inconsistency due to
imprecise language. We’ve seen now that the criterion for
renormalizability, which Polchinski provided a robust basis for, is
power-counting of the operators near the IR fixed point. This would
suggest that the @xmath theory we’ve studied by means of an example is
renormalizable. However, recall the result of resumming its
renormalization group equation:

  -- -------- -- -------
     @xmath      (115)
  -- -------- -- -------

which has a Landau pole for @xmath , preventing us from taking the limit
we required above. A mathematical physicist would say @xmath theory is
‘trivial’ or ‘quantum trivial’, as if we demand the existence of a
continuum limit, that sets @xmath . The issue is that the tree-level,
classical scaling dimension captures only the scaling of the operators
infinitesimally close to the infrared Gaussian fixed point. If we move a
finite distance upward in energy scale, we’ve seen above that the @xmath
operator gets an anomalous dimension @xmath and so is marginally
irrelevant. So it’s clear that Polchinski’s picture of renormalization
is only getting at a perturbative sense of renormalizability, and cannot
tell us whether there truly exists an RG flow from a UV fixed point down
to the IR theory we want to study. So what are we to make of @xmath
theory—or for that matter of QED, which has the same problem? Of course
we know empirically that QED works fantastically well and we can
absolutely make finite, accurate predictions after a finite number of
inputs. To understand this, we must appeal the language of effective
field theory, which we’ve already discussed. In fact the feature we’re
really relying on is effective renormalizability, which tells us we
require solely a finite number of inputs to set the behavior of the
theory to a given precision. It’s clear that in this sense QED itself is
an effective field theory whose validity breaks down somewhere below its
Landau pole. Finally, let me mention another reason not to be too
worried that our most beloved quantum field theories don’t exist in the
continuum limit: Our universe is not described by a QFT at its smallest
scales! It’s indeed true that only RG flows between a UV CFT and an IR
CFT can hope to define fully consistent and mathematically well-defined
QFTs. But the existence of gravity—and the very strong evidence that a
quantum field theory of gravity is inconsistent—means that at some
energy scale effects not present in quantum field theory must become
relevant. And since gravity couples universally to everything [
vonMeyenn1990 , Weinberg:1964ew ] , we have no strict empirical need for
a UV complete, interacting quantum field theory that does not include
gravity. It is entirely consistent, and overwhelmingly likely, that a
quantum-field-theoretic description of the world works only
approximately and some inherently quantum gravitational theory provides
a sensible UV complete theory.

#### 2.4 To Reiterate

Before moving on, let us reiterate what we’ve discussed about
renormalization. As we’ve seen, renormalization is so important and so
useful and fulfills so many purposes that an entirely general statement
risks becoming vague. But if a single sentence summary is demanded:
Renormalization reveals for us the scale-dependence of a quantum
mechanical field theory. The effects of this seemingly innocuous
statement, however, are powerful and manifold, including:

-   Correctly accounting for this scale-dependence is necessary to have
    well-defined quantum field theories, which otherwise appear
    nonpredictive.

-   Bringing the non-scale-invariance of the quantum mechanical theory
    into clear scale-dependence of the couplings makes it simple to read
    off the qualitative behavior at different scales from the
    renormalized Lagrangian.

-   Including this scale-dependence in the couplings allows us to
    reorganize our perturbative series such that we can efficiently
    calculate the behavior of the theory over a far wider range of
    energies than a naïve treatment allows.

-   Properly accounting for the scale dependence allows us to harness
    the full power of effective field theory, as we can study a theory
    of low-energy fields which correctly accounts for corrections from
    the high-energy physics.

-   Understanding the perspective of single quantum field theories as
    flows through theory space as a function of the scale allowed us to
    develop a nonperturbative definition of a fully UV-complete quantum
    field theory and how it behaves.

All of these various perspectives will be of use in the following
chapters as we apply this technology to understanding what the hierarchy
problem is and how we can solve it.

### 3 Naturalness

Naturalness is the notion that we have the right to ask about the
origins of the dimensionless numbers in our theories—past solely fitting
them to the data. It was Dirac who first introduced such a notion to
particle physics in 1938 [ Dirac:1938mt ] . In modern language, what is
referred to as ‘Dirac naturalness’ consists of the idea that
dimensionless parameters in a fundamental physical theory should take
values of order 1. In the language of EFT, in a theory with a cutoff
@xmath and an operator @xmath with scaling dimension @xmath , we expect
its coefficient to take a value @xmath . As stated this is essentially
dimensional analysis, as @xmath is the only scale we’ve introduced, but
we will discuss below that quantum mechanics gives additional credence
to this expectation—indeed we’ve already seen this feature in our
one-loop examples above. ’t Hooft pointed out a refinement of this
principle [ tHooft:1979rat ] , which has come to be called ‘technical
naturalness’. If the operator @xmath breaks a symmetry which is
respected by the action in the limit @xmath , then one says it is
‘technically natural’ for @xmath to take on a small value. The reasoning
here is simple—as we saw above, in a quantum field theory defined at a
high scale, one finds corrections @xmath to such coupling constants as
they run to low energies. If there is an enhanced symmetry of the theory
in the limit that @xmath , then such quantum mechanical corrections
cannot generate that operator and break the symmetry, so we know that
@xmath . The low-energy effective field theorist says of such couplings
that one can ‘set it and forget it’: if one has @xmath at the cutoff
@xmath , that coupling will remain small as one flows to lower energies.
Conversely, we can emphasize the connection to Dirac naturalness by
looking at this picture in reverse. We know of mechanisms to generate
small technically natural couplings at low energies from Dirac natural
ones, as we will discuss in detail below. Imagine one measures a small
coupling @xmath at long distances in the low-energy theory with cutoff
@xmath that does not have a Dirac natural explanation. If that parameter
is technically natural, it remains small up to the cutoff, and so the
next generation of physicists can explain its small size at @xmath in
the UV theory, as emphasized nicely by Zhou [ zhou ] . If @xmath is not
technically natural, then its RG evolution up to the cutoff yields a
value to which the low-energy physics is very sensitive, and we must
explain why it has a very specific value such that the correct physics
emerges at long distances.

#### 3.1 Technical Naturalness and Fine-Tuning

  A model is fine-tuned if a plot of the allowed parameter space makes
  you wanna puke.

  David E. Kaplan (2007)

It’s useful to make this less abstract by looking at a simple example.
Consider a @xmath dimensional effective field theory of a real scalar
field @xmath of mass @xmath which is odd under a @xmath symmetry, which
we expect is a good description of our system up to a cutoff @xmath with
@xmath . If we add a small explicit breaking @xmath with @xmath at low
energies, @xmath is technically natural and stays small up to the
cutoff, so we can easily write down a UV completion which generates this
small value Dirac naturally. However, if we add another @xmath -odd real
scalar @xmath and give it a large @xmath -breaking interaction with
@xmath , then @xmath is no longer technically natural. Its low-energy
value becomes extremely sensitive to the values of the parameters at the
cutoff. It then becomes difficult to understand an ultraviolet reason
for why those values take the precise values they need to realize small
@xmath in the far IR. Consider the bare action

  -- -------- -- -------
     @xmath      (116)
  -- -------- -- -------

where we’ve given the two fields the same mass for simplicity. This is
not stable under radiative corrections, but that’s a higher-order effect
which will not come into our one-loop calculation of the RG evolution of
the cubic couplings. We will renormalize this theory at one loop using
dim reg with @xmath . As discussed above, we will compute the one-loop
1PI diagrams and add counterterms to cancel solely the @xmath pieces of
the results. With counterterms, the action is

  -- -------- -- -------
     @xmath      (117)
  -- -------- -- -------

where these parameters and fields are the renormalized parameters, and
for compactness we have not written down the split of these terms as we
did above in Equation 97 . At tree level the relation to the bare
quantities is trivial and so @xmath . To get an accurate picture of how
the strength of the interactions vary as they’re probed at different
energy scales, we must fully renormalize the theory. Since our focus is
on the interactions, we simply state the results for the quadratic part
of the action, where we have

  -- -------- -------- -- -------
     @xmath   @xmath      (118)
     @xmath   @xmath      (119)
     @xmath   @xmath      (120)
     @xmath   @xmath      (121)
  -- -------- -------- -- -------

These tell us that the physical mass of the fields and the normalization
of their one-particle states has changed. The relation to these can be
found using the quantum-corrected propagator @xmath as @xmath to define
the mass and @xmath to define the normalization @xmath , but solving for
these relations explicitly will not be necessary for our purposes.

The one-loop three-point functions each have two diagrams, whose
evaluation only differs in the coupling constants. For the correction to
@xmath , we have triangle diagrams with either @xmath or @xmath running
in the loop. We can evaluate them in @xmath dimensions as

  -- -------- -------- -- -------
     @xmath   @xmath      (122)
              @xmath      (123)
              @xmath      (124)
              @xmath      (125)
              @xmath      (126)
  -- -------- -------- -- -------

The counterterm vertex contributes to this as @xmath , meaning that
@xmath prescribes we set

  -- -------- -- -------
     @xmath      (127)
  -- -------- -- -------

For the other vertex correction there are diagrams with either one or
two of each internal line, which give

  -- -------- -------- -- -------
     @xmath   @xmath      (128)
              @xmath      (129)
  -- -------- -------- -- -------

leading to the counterterm

  -- -------- -- -------
     @xmath      (130)
  -- -------- -- -------

This gives us the beta functions

  -- -------- -------- -- -------
     @xmath               (131)
     @xmath   @xmath      (132)
  -- -------- -------- -- -------

Now we are finally in a place to mathematically evince our physics point
about technical naturalness. Without @xmath , the coupling @xmath is the
only one which violates the @xmath and so the beta function is
necessarily proportional to @xmath . ¹¹ ¹¹ 11 This is trivial in our
case as @xmath would be the only interaction, but you’ll find the
feature persists if you add other symmetry-respecting interactions, for
example a @xmath -even scalar @xmath with an interaction @xmath . The
general argument for this fact is given in the next section. Let’s say
we recruit an experimentalist friend of ours to measure the @xmath
scattering cross-section of @xmath s and we find that at @xmath , the
theory fits the data for @xmath with @xmath . Solving the beta function,
we find that to lowest order

  -- -------- -- -------
     @xmath      (133)
  -- -------- -- -------

So @xmath is indeed radiatively stable. If @xmath is small, then it
takes until the enormous scale @xmath for @xmath to change by an order
one fraction. So running @xmath up to wherever the cutoff @xmath of our
theory is, @xmath will still be small. If by @xmath we haven’t
discovered any explanation for the size of @xmath , we can ask the
theory above @xmath to produce this small value of @xmath from Dirac
natural parameters at yet higher energies. Perhaps the high-energy Dirac
natural value of @xmath has been relaxed toward zero by an axion-type
mechanism, or perhaps @xmath is the vev of another @xmath -odd field
which spontaneously broke the @xmath via confinement. We don’t need to
know a particular mechanism; the fact that @xmath is technically natural
means that if we don’t find an explanation for its size there is hope
yet that our academic descendants will. One says that here @xmath is
UV-insensitive as its low-energy value does not depend strongly on the
physics at high energies. On the other hand, if @xmath is not
technically natural, we have a much more difficult issue. If we now have
the theory with both @xmath and @xmath , and our experimentalist
measures @xmath and @xmath , then to lowest order the RG evolution of
@xmath will be @xmath leading to

  -- -------- -- -------
     @xmath      (134)
  -- -------- -- -------

And we can see the issue, as we are no longer guaranteed that a small
@xmath is related to a small @xmath . For concreteness, if @xmath ,
@xmath and @xmath , then (using the full one-loop RG), we have @xmath
and @xmath . How are we to ensure these values at @xmath ? We know how
to produce small numbers, but not incredibly specific ones. To see that
we do need to produce these values very precisely, let’s switch
directions and consider the RG evolution down in energy from @xmath to
@xmath . In the theory with solely @xmath , the coupling @xmath runs
incredibly slowly, so an @xmath change in @xmath , evolved down to the
scale @xmath , results in an @xmath change to @xmath . But in the theory
with two sources of breaking, @xmath is enormously sensitive to the
values of the couplings at @xmath . With the same cutoff @xmath , if we
very slightly change the input value to @xmath and leave @xmath as
above, evolving down now gives us @xmath —a @xmath change in input
parameters has resulted in a @xmath change in our low energy observable!
It’s even more sensitive to the input value of @xmath ; a @xmath
modification solely to @xmath trickles down to give @xmath , a @xmath
change. In this theory @xmath is now a UV-sensitive parameter, whose
low-energy behavior depends strongly on the high-energy physics. To say
the least, it seems difficult to find a natural way to achieve the
precise values needed to reproduce the observed low-energy physics in
this theory. We’ll return to this issue at length in Section 5.2 .

##### Technical naturalness and masses

Our understanding of technical naturalness allows us to already see
another warning sign of the hierarchy problem. An elementary spin-1
field comes along with a gauge symmetry @xmath which is broken by a mass
term @xmath . So a mass for a gauge boson is technically natural and one
necessarily finds @xmath . Similarly, a massive Dirac fermion @xmath has
a @xmath global symmetry under which @xmath . In the @xmath limit, the
symmetry is enhanced to @xmath as arbitrary rephasings of the two Weyl
fermions become symmetries, so again @xmath . But an elementary scalar
does not automatically come with any such protective symmetry, and we’ve
already seen in all our examples above that scalar mass corrections
indeed get contributions not proportional to the mass itself. In fact
for discussing the technical naturalness of masses there is an even
simpler argument: A massless spin-1 boson has two degrees of freedom and
a massive one has three. Quantum corrections cannot generate a degree of
freedom ex nihilo , so a massless gauge boson must be protected.
Similarly a massless chiral fermion has two degrees of freedom, but a
massive Dirac fermion has four. So for charged spinors and for vectors,
it is simply the representation theory of the Lorentz group that is
responsible for the stability of their masses. In either of these cases
mass must arise from interactions of the field with a scalar (very
broadly defined) as in the Higgs mechanism, which can pair up chiral
spinors together and lend vectors another degree of freedom. But a
massless scalar and a massive scalar have the same number of degrees of
freedom. If we want a scalar mass to be technically natural, it must
come from some symmetry past simply the Lorentz group. We’ll see some
examples of how to arrange this in Chapter The Hierarchy Problem: From
the Fundamentals to the Frontiers .

#### 3.2 Spurion Analysis

An important tool for understanding symmetries and their violation is
known as ‘spurion analysis’. The basic idea is simple: for a theory
which respects a symmetry except for the coupling @xmath , this coupling
parametrizes the breaking of the symmetry and any effects which violate
the symmetry are proportional to @xmath . More concretely, one assigns
such couplings spurious transformation properties under the symmetry
such that the action becomes invariant under the symmetry. Physically
one can imagine that the observed values of such couplings come from the
vacuum expectation values of some heavy fields which are above the
cutoff of the theory. This can be viewed as imagining a UV completion
where the explicit symmetry breaking in the low-energy effective theory
comes microscopically from some spontaneous symmetry breaking, but the
value of spurions is not dependent upon a particular realization of the
UV completion. We can quickly see the utility of this by looking at a
simple example of a complex scalar field @xmath with an interaction
which explicitly breaks the @xmath global symmetry @xmath .

  -- -------- -- -------
     @xmath      (135)
  -- -------- -- -------

where “ @xmath denotes the addition of the Hermitian conjugate of the
non-Hermitian interaction term. A naïve effective field theorist would
say that our Lagrangian has no symmetries, and so we have no control and
should expect that quantum corrections give us any polynomials in @xmath
at low energies. However, we may note that if we assign @xmath a charge
of @xmath such that @xmath , then the theory is invariant under that
@xmath global symmetry. So quantum corrections cannot violate our
spurious symmetry, and as a result we know that we can only generate
terms like @xmath , and there are no @xmath or @xmath interactions
generated at any order in perturbation theory. We can also usefully
apply this to the example of technical naturalness studied in the
preceding section. If @xmath is given the spurious transformation @xmath
then the @xmath term is invariant. Then it’s simple to see that no
matter what other sorts of interactions we add, so long as they respect
the @xmath symmetry we must have @xmath . But having added @xmath we see
that this term can also be made invariant with @xmath , and this allows
@xmath as well. An important real-world example where spurion analysis
is useful is in understanding the flavor structure of the SM. With all
masses turned off, the SM has a large global symmetry group @xmath ,
corresponding to arbitrary unitary reshufflings of the three generations
of each fermion representation. These symmetries are explicitly broken
by the Yukawa matrices which generate hierarchically different masses
for the three generations.

  -- -------- -- -------
     @xmath      (136)
  -- -------- -- -------

where @xmath are generation indices, and the Yukawa couplings are
matrices in this generation space. We don’t understand why these
hierarchies are present, but we can carry out a spurion analysis to see
how worried we should be. We see that our theory will be invariant under
the full flavor group if we assign the Yukawa matrices the following
transformation properties under the various @xmath symmetry groups

  -- -------- -------- -- -------
     @xmath   @xmath      (137)
     @xmath   @xmath      (138)
     @xmath   @xmath      (139)
  -- -------- -------- -- -------

Since these are the only flavor-violating couplings in the SM and they
are all in distinct spurious flavor representations, this tells us the
quantum corrections to these matrices must be proportional to the
matrices themselves e.g. @xmath . Thus this pattern of Yukawa couplings
is stable under RG evolution to higher scales, and we are justified in
thinking that the generation of this pattern may take place at large,
currently-inaccessible scales. This eases our minds about when we need
to discover the origin of these flavor hierarchies, but this holds true
only as long as these remain the only flavor-violating couplings.
Fantastic work in precision flavor measurements and theory has provided
lower bounds on the scale at which additional flavor violation can
occur. Searches for flavor-violating processes have constrained these
violations to take place at scales enormously higher than scales we are
able to directly probe at colliders, which poses a puzzle. If there is
new physics near the TeV scale, how is it arranged to respect the flavor
structure of the SM? A phenomenological approach known as Minimal Flavor
Violation [ DAmbrosio:2002vsn ] demands that all flavor violation is
proportional to these Yukawa couplings, but no fundamental explanation
for this is known. For recent introductions to flavor in the Standard
Model, see e.g. [ Grossman:2017thq , Zupan:2019uoi , Gori:2019ybw ] .

#### 3.3 Dimensional Transmutation

Perhaps the most important example of a Dirac natural field theory
generating a small number is that of ‘dimensional transmutation’. In
particular, in quantum chromodynamics (QCD) the theory is
‘asymptotically free’—meaning that the interaction strength vanishes in
the far UV—and the gauge coupling @xmath grows as one goes to lower
energies. We skip the Nobel-worthy calculation (see e.g. Srednicki’s
chapter 73 [ Srednicki:2007qs ] ) and simply quote the results for the
beta function of QCD (here parametrized via @xmath ), which dictates the
dependence of the gauge coupling on energy. In @xmath , the calculation
finds

  -- -------- -- -------
     @xmath      (140)
  -- -------- -- -------

where @xmath is the energy scale of interest and @xmath is the number of
quarks with masses below @xmath , which at high energies is @xmath .
Then if we know the gauge coupling at a fundamental scale like @xmath ,
we can follow the procedure discussed in Section 2.3 of running and
matching to sequentially evolve the coupling down to low energies. We
end up with a result like

  -- -------- -- -------
     @xmath      (141)
  -- -------- -- -------

where @xmath is somewhere between the @xmath value it takes above the
top quark mass and the @xmath value it has below the charm quark mass.
This tells us that eventually the QCD coupling blows up in the infrared,
and the theory becomes strongly coupled—we expect our perturbative
understanding of the theory to break down. While there is no proof of
the precise effects of this, there is strong evidence that this is
responsible for the observed phenomenon of ‘color confinement’— at low
energies colored particles form bound states which are color-neutral.
The intuition being that the gluon interaction is so strong that trying
to pull quarks in a color-singlet apart from each other requires so much
energy that it is energetically favorable for a quark-antiquark pair to
be created out of the vacuum and to end up with two color singlets. We
may define a new scale @xmath as being the energy at which @xmath
diverges

  -- -------- -- -------
     @xmath      (142)
  -- -------- -- -------

So for some reasonable fundamental coupling @xmath at high energies, the
theory generates a new scale which is exponentially far removed from the
fundamental physics. Since the mass of the proton is mainly from QCD
binding energy, @xmath this explains the huge hierarchy @xmath . This is
an extremely important mechanism and historically one of the first
suggestions for how to generate the electroweak scale was by copying
this strategy, as we’ll discuss in Section 9 .

## Chapter \thechapter The Hierarchy Problem

### 4 The Higgs in the Standard Model

The physical question of the hierarchy problem is how to get an infrared
scale @xmath out of a microscopic theory whose degrees of freedom live
at the much higher scale @xmath , with @xmath . The tools introduced in
Section The Hierarchy Problem: From the Fundamentals to the Frontiers
have already provided a window into why this can be difficult in a
quantum field theory. Our aim in this section is to expand on that
notion for the generation of the electroweak symmetry breaking scale in
the Standard Model, where this scale is provided by the Higgs mass. In
the Standard Model the Higgs mass is not technically natural, so the
example discussed in Section 3 suggests the issue that may appear. It is
well-known that the Higgs plays a central role in the Standard Model,
but the tagline ‘it provides mass’ doesn’t go far enough in underscoring
its importance. The Higgs is needed because the fermions of the Standard
Model have a chiral spectrum: There are no representations with opposite
charges under the full SM gauge group. This means that there are no
gauge-invariant fermion bilinears, so no fermions can be paired up to
form mass terms. If the Standard Model were not chiral, we could write
mass terms directly in the Lagrangian. In such a case there’s no reason
to expect small masses for the fermions, and indeed in the familiar case
of right-handed neutrinos---which are SM gauge-neutral themselves, so
can have Majorana masses---we generally expect them to be very heavy. In
some sense the natural expectation for such ‘vector-like’ (non-chiral)
fermions would be to have Planck-scale masses, as in the absence of
other particle physics, this is the only scale. So macroscopic structure
in the universe is solely made possible by the chiral nature of the
Standard Model. ¹² ¹² 12 We note that it’s also true that the existence
of a macroscopic universe relies on the smallness of the cosmological
constant, which is the other pressing fine-tuning issue present in the
Standard Model. This way of viewing the naturalness problems of the
Standard Model has been beautifully articulated by Nima Arkani-Hamed in
[ Arkani-Hamed:2012ekl ] and in many seminars. There may well be other
vector-like sectors which indeed contain Planck-scale masses. The fact
that the particles which comprise us have a weakly-coupled description
where quantum gravitational effects are suppressed—and so notions like
locality and Riemannian geometry work well—would not hold in such a
vector-like sector. Thus the fact that the Standard Model is chiral and
so requires the Higgs mechanism to provide masses answers a deep and
important question about our place in the universe. But it doesn’t
provide a full answer, as the scale at which the Standard Model sits
still needs to be generated somehow. And in the absence of a mechanism
to make it light, we must worry once more about losing our macroscopic
existence with a mass scale which is again naturally of order the only
other mass scale, the Planck scale. So, far more than the hierarchy
problem being a small detail to clean up after having empirically
verified the structure of the Standard Model, the question of why @xmath
has serious physical importance. ¹³ ¹³ 13 It’s worth noting that in the
complete absence of the Higgs, electroweak symmetry is broken by the QCD
chiral condensate [ Samuel:1999am , Quigg:2009xr ] . It’s interesting to
ponder why Nature did not choose to let QCD confinement solely fill the
role, but we know empirically that there is EWSB at @xmath scales, so we
need to understand the generation of that separate scale. If we want an
answer to why we live in a world with macroscopic structure, we must
grapple with the hierarchy problem. This section is devoted to
understanding the technical statement of this physical question in the
framework of effective field theory. We pursue this by introducing,
discussing, and refuting some common confusions about the hierarchy
problem.

### 5 Nonsolutions to the Hierarchy Problem

In this section I will introduce a few common confusions and
misconceptions about the hierarchy problem. Discussion and refutation of
these arguments provides a natural backdrop for introducing how the
hierarchy problem should be properly understood and why it is important.

#### 5.1 An End to Reductionism

A first point of confusion is that the Higgs mass is a free input
parameter in the Standard Model, so a natural objection is that we
should just set @xmath and call it a day. Indeed, this hits on a basic
and important point: There is no hierarchy problem in the Standard
Model. The hierarchy problem exists for a more-fundamental theory which
predicts the Higgs mass—that is, one in which the Higgs mass is an
output parameter. We can evince this in a simple toy model of a scalar
@xmath interacting with other general fields @xmath where a tree-level,
‘bare’ mass term is allowed by the symmetries. This is our toy version
of the Standard Model, in which the scalar mass is likewise an input.

  -- -------- -- -------
     @xmath      (143)
  -- -------- -- -------

Of course, as is familiar, @xmath itself is not measurable. When one
calculates the two-point function of @xmath perturbatively in couplings
¹⁴ ¹⁴ 14 Perturbative calculations are an expansion in couplings, not
@xmath [ Holstein:2004dn ] , though this subtlety is commonly elided.
one finds quantum corrections @xmath , where these are generically large
because the mass is not technically natural—there’s no symmetry
protecting it. When one measures the mass of @xmath with e.g. some
scattering experiment, it is @xmath which one measures. And luckily so,
because @xmath may well be formally infinite in a continuum quantum
field theory, and a similarly infinite bare mass term is necessary to
end up with the correct finite physical mass. Indeed, we are justified
in this theory in choosing @xmath such that @xmath matches the measured
value. This is just the familiar procedure of renormalization,
stretching back many decades and first understood in the context of
quantum electrodynamics. To define QED one needs to input some
definitions of the electron mass and the electromagnetic coupling based
on experimental data, and these two inputs then determine all other
predictions of the theory—e.g. the differential cross section of Coulomb
scattering, the lifetime of positronium, and anything else you could
hope to measure. However, consider now a theory which has a global
@xmath symmetry which is spontaneously broken at a high scale @xmath .
We want to understand how to get a light scalar degree of freedom out of
this theory—that is, we’ve measured @xmath . This is a toy model of a
Grand Unified Theory, where the microscopic physics exists at a high
scale @xmath , and indeed it was in this guise that the hierarchy
problem was first recognized. Let’s say our light scalar degree of
freedom @xmath originated from a doublet @xmath . Our microscopic theory
now does not have a bare mass term for @xmath but rather solely for
@xmath as a whole, since it must respect the symmetry. A difference in
the masses of @xmath and @xmath can only come from the spontaneous
breaking of the symmetry—let’s say when another fundamental scalar
@xmath gets a vev @xmath . This vev is a physical, measurable parameter
related to the mass of @xmath and its self-interactions. Our action is
controlled by the symmetries, as ever.

  -- -------- -- -------
     @xmath      (144)
  -- -------- -- -------

where @xmath is the bare mass of the @xmath doublet and @xmath is its
bare interaction strength. In the absence of any other scales we
generally expect @xmath and @xmath . In this theory there is no reason
for the values of these parameters to be connected to each other in any
way. When @xmath gets a vev, @xmath , it breaks the @xmath symmetry and
gives a mass splitting between the two degrees of freedom in @xmath ,
since @xmath . We then have the masses @xmath . In this theory our
tree-level inputs are @xmath and @xmath (and the interactions
controlling the value of @xmath , which for simplicity we don’t write
down) and the scalar mass @xmath is an output . In fact here the
hierarchy problem occurs at tree-level, simply as a result of wishing to
produce a small mass via splitting a multiplet. If we wish to have, say,
@xmath and @xmath —the values of the GUT scale and the electroweak scale
in the real world—we need to fine-tune @xmath enormously so it takes a
value like @xmath . Of course when we look at our theory at loop level
there will again be quantum corrections to our tree-level parameters
@xmath and @xmath , and again it will be their corrected values which
are physical and measurable @xmath , @xmath . But if our theory is
renormalizable, we know that quantum corrections will merely change the
values of these parameters, and not the operators we have. The point is
that the quantum corrections are @xmath invariant, so the masses of both
@xmath and @xmath will receive the same loop contributions. We will then
still predict the mass of @xmath as @xmath . Now at the level of
inventing the theory we may still tune these parameters to get a small
@xmath , but we’re tuning physical, observable parameters. In the theory
described by Equation 143 , one might have also said that we needed to
fine-tune @xmath against @xmath in order to get a small @xmath ,
especially if we calculated that the quantum correction @xmath was
large. But there the fine-tuning was of a different sort, since @xmath
and @xmath were only ever observable in the combination @xmath . Here
the fine-tuning has a much sharper meaning. This tuning translates into
a physical demand on our theory that at high energies the strength of
the interaction between our two scalars @xmath for some reason has a
value extremely close to @xmath , despite having nothing to do with
either of these parameters. The tuning is now a physical feature of our
theory and demands explanation. ¹⁵ ¹⁵ 15 Let me mention parenthetically
a confusion one may encounter if one reads older literature on the
hierarchy problem in GUTs. It was common to speak of having to ‘re-tune
the parameters at every order in perturbation theory’, as if imagining
an algorithmic process where one first computed a tree-level prediction,
tuned that to be correct, and then computed the one-loop corrections,
re-tuned those parameters to get it right again, etc. This is framed as
being ‘worse’ than just requiring ‘one’ set of tunings. This is moronic,
for the simple physical reason that Nature does not compute via
perturbation theory. There is a physical problem, which is how to get
the electroweak scale out of other physical parameters in the theory.
Whether you compute the predictions in perturbation theory, or on a
lattice, or whilst standing on your head is immaterial. So we see
explicitly that the hierarchy problem is present when the light scalar
mass is an output of the theory, rather than an input. If one is so
inclined, one can say the words that the Higgs mass is simply an input,
but this possibility spells the end of scientific reductionism. It is
indeed conceivable that this is how the universe works. However, we know
there is physics beyond the Standard Model at smaller length scales, and
our best ideas for what those could be involve theories where the inputs
are defined in the ultraviolet and the Higgs mass comes out. Whether the
Higgs ultimately originates as a component of a larger multiplet, or a
bound state of fermions, or an excitation of a string, we expect that
the Higgs mass is a parameter that comes out in the low-energy theory.

#### 5.2 Waiter, there’s Philosophy in my Physics

  You can always use the history of physics to illustrate any polemic
  point you want to make.

  Nima Arkani-Hamed

Now having exhibited that getting a light scalar truly does involve some
fine-tuning of physical parameters, one may still say ‘ So? ’. In the
real world, we’ve observed (the analogue of) @xmath , but the physics
we’ve discussed at the heavy scale @xmath is new physics, and we don’t
have experimental measurements of @xmath telling us the value is not
that perfect value to get our light scalar. One might thus object that
there’s no fundamental inconsistency, and as long as our theory can fit
the data anything else is just philosophy . But the criterion of it
being not literally impossible for a theory to fit the data is an
incredibly low bar, and scientists always use additional criteria to
select theories. While there is ultimately a degree of subjectivity in
any notion of ‘naturalness’, this is really the same subjectivity that
one constantly uses in science to decide which of two explanations for
some data to accept. A simple (approximately) historical example of this
can be seen in epicyclic theories of the motion of the planets. The
Ptolemaic, geocentric model of the universe predicted at first that the
heavenly bodies orbited the Earth in circles, but eventually
astronomical data was accurate enough to show that the motion of the
planets and sun around the earth was not circular. In the Ptolemaic
model, this was dealt with by adding an epicycle , the suggestion that
the heavenly bodies moved on smaller circular orbits about their
circular orbit around the Earth. As astronomical observations became
more and more detailed over the ensuing centuries, multiple layers of
epicyclic motion were needed to explain the data—circles on circles on
circles, as depicted in Figure 13 . The description never stopped
working though; if you’d like to, you may describe the orbit of any
solar system object via @xmath , and for some large but finite @xmath
you’ll be able to fit any orbit to within observational precision.

So why did sixteenth century physicists favor Kepler’s laws and
heliocentrism? The discriminating factor is manifestly not which model
better-fitted the data. Rather, the choice comes down to Occam’s razor,
to explanatory power, to simplicity and to fine-tuning of parameters.
Physicists favor theories which do more with less---theories which
explain more about the world while requiring fewer inputs. This is a
subjective bias about how we think the universe should work, and it’s
possible that this philosophy will ultimately fail---but it’s been
working well thus far. ¹⁶ ¹⁶ 16 As a semi-autobiographical aside, I had
the honor and pleasure of being in the inaugural cohort of the
Integrated Studies Program for Benjamin Franklin Scholars in the School
of Arts & Sciences at the University of Pennsylvania. The program,
founded and spearheaded by the classicist Prof. Peter Struck, offered a
dedicated interdisciplinary experience wherein, each semester, three
diverse fields gave courses offering perspectives around a central
topic, which were concurrently collectively compared and contrasted. In
my year we studied biology, anthropology, classics, political science,
physics, and literature, all taught by preeminent professors in their
respective fields. After noticing a pattern, the group kept track of
(among other things) how many times each professor mentioned, discussed,
or appealed to ‘beauty’. The winner in this regard was Prof. Vijay
Balasubramanian—lecturing on the way the universe works—by a country
mile. While keeping the above intuition firmly in the back of our minds,
it can be useful to introduce a mathematical classification of this
fine-tuning, with the understanding that no such measure is god-given
and so what to do with such a measure is up to us. We’ll discuss a
couple such schemes, the first being a mathematical formalization of the
dependence of an output of interest on the values of the inputs. This
has the benefit that it is intuitive and simple to compute, so it is
widely used in the particle physics literature. However, it lacks
independence under how variables are parametrized so can lead to
misleading conclusions if used without care. Furthermore, it will assign
a measure of fine-tuning to individual points in the parameter space of
a model, whereas we’d like to characterize the naturalness of a model as
a whole—if a model only produces predictions that match the real world
in a small region of its parameter space, that’s another important
element of fine-tuning [ Anderson:1994dz ] . For example, if new data
removes all but a small fraction of the viable parameter space in a
given model, we want to regard that model as being less natural
afterward. An approach based on Bayesian statistics allows us to
incorporate these issues and gives unambiguous comparisons of the
relative naturalness of models upon the collection of new data [
Fichet:2012sn ] , but loses out on simplicity. A simple and often-used
measure was introduced by Giudice and Barbieri [ Barbieri:1987fn ] , who
suggested the definition

  -- -------- -- -------
     @xmath      (145)
  -- -------- -- -------

which may be called a measure of the fine-tuning of the input parameter
@xmath necessary to get out the correct output parameter @xmath . The
logarithmic dependence naturally gives a measure of relative sensitivity
and removes dependence on overall scale or choice of units. If @xmath is
large, this denotes a large sensitivity of @xmath to the value of @xmath
, and implies that one must choose the value of @xmath very carefully to
get out the right physics. In the example in Equation 144 above we have
@xmath , indeed signaling enormous fine-tuning, and likewise @xmath .
Contrast this with the familiar case of a seesaw mechanism where the
light neutrino mass is given by a formula like @xmath , with @xmath a
heavy mass scale and @xmath a weak scale mass. We can check whether this
mechanism requires fine-tuning with @xmath , and we find that seesaw
mechanisms are natural. So @xmath matches our intuition here, and can be
quite useful.

However, for a model with free parameters a notion of fine-tuning at a
single point in parameter space does not capture the full picture, and
we should incorporate a notion of the volume of viable parameter space
into our naturalness criterion [ Athron:2007ry ] , as diagrammed in
Figure 14 . This necessity should be intuitive in the context of
constraining models of new physics. Models which achieve their aim
throughout parameter space are viewed more favorably than models which
only work in some small region of their parameter space. For a relevant
example, there are still corners of the MSSM parameter space that are
natural under the Giudice-Barbieri measure. But the fact that the LHC
has ruled out large swathes of this parameter space means that we should
surely view weak-scale supersymmetry as less natural now than we did a
decade ago. A pointwise measure of fine-tuning misses this. An approach
based on Bayesian statistics can be used to better match what we want
from a measure of naturalness, as is discussed well in [ zhou ] . The
definition of a model in a Bayesian framework requires priors on its
free parameters @xmath , denoted @xmath , and different choices of
@xmath should be considered different models. The probabilist’s notation
@xmath may be interpreted as ‘the probability of @xmath given that
@xmath is true’. We also require a prior probability @xmath that the
model @xmath is true as a whole. After we receive data @xmath , Bayes’
theorem gives us the posterior probability for the model as

  -- -------- -- -------
     @xmath      (146)
  -- -------- -- -------

where @xmath is known as the likelihood. Both @xmath and @xmath are
explicitly subjective, but if we take the ratio of the posterior
probabilities for two models @xmath and @xmath we find

  -- -------- -- -------
     @xmath      (147)
  -- -------- -- -------

This expresses how the ratio of the likelihood of these two models
changes after receiving new data. So while different physicists may
disagree on the prior and posterior probabilities, the ‘Bayes update
factor’ @xmath is unambiguous and shows that the physicists agree on how
the relative naturalness of the two models is affected by the new data.
The likelihood in a model with free parameters @xmath is calculated as

  -- -------- -- -------
     @xmath      (148)
  -- -------- -- -------

This is an integration over parameter space of the likelihood of
producing the data in this model, weighted by the prior probability
distribution we’ve placed on our parameters. This balances the competing
effects of how well parameter points fit the data with the principle of
parsimony—models with large regions of parameter space which don’t fit
the data are penalized. The need to compare models to define naturalness
in a Bayesian formalism is easily seen by the fact that the likelihood
for any new physics model decreases monotonically as more data is
collected and previously-viable regions of parameter space are ruled out
(in the absence of a discovery, of course). An interesting playground
for these ideas is the strong CP problem. In brief, this is the
smallness of the so-called ‘theta angle’ @xmath in QCD, which controls
the amount of CP breaking in the strong sector. While @xmath , empirical
measurements now constrain @xmath . Although @xmath is not technically
natural, in the Standard Model it runs very slowly—since the other
source of CP breaking is from the CKM matrix—such that if one sets it
tiny in the UV, it stays small down to the IR. The Guidice-Barbieri
measure would thus produce @xmath , as the measured value is insensitive
to small changes in the UV value. And yet, the small theta angle is
regarded as a naturalness problem, which can be justified in a Bayesian
approach. The necessity of a prior does help quantify one’s surprise at
the small value of @xmath in the context of the Standard Model, but to
really think about naturalness we need to have a comparison. We know of
simple theories which produce vanishing @xmath starting from generic
values of the parameters—for example, an axion naturally produces @xmath
. Then if we have new data which pushes down the upper bound on @xmath ,
axion models receive large Bayesian update factors in relation to the
Standard Model. When we know of a simple model which automatically
explains some data, it’s puzzling if our current model requires precise
choices for free parameters in order to explain the data. We can see
these notions play out for the hierarchy problem by comparing our toy
model of a GUT to one with weak-scale supersymmetry. Our toy model in
Section 5.1 produced a scalar through a cancellation of GUT-scale @xmath
contributions. A toy model with supersymmetry (to be discussed in
Section 7 ) would remove sensitivity to the ultraviolet, and replace the
scale of GUT-breaking with the effective scale of supersymmetry-breaking
in the SM sector @xmath . And while the GUT scale is (more or less)
fixed by the running of SM couplings, the SUSY-breaking scale in the SM
can be far lower. We use @xmath here as a one-parameter avatar of the
scale of superpartners. The general prediction for supersymmetry before
the LHC was @xmath , with multiple species of superpartners appearing
below the TeV scale. Such a model produces an improvement in
Giudice-Barbieri tuning of @xmath over our model without SUSY. Let’s
take @xmath to be a logarithmic prior from @xmath to @xmath , where an
upper limit @xmath is justified by the requirement that the model give
small Higgs mass corrections and a weak-scale dark matter candidate. If
we collect collider data for a decade and find that much of the
parameter space is ruled out, say with a limit @xmath we should update
our thoughts on the naturalness of the model. This data has no effect on
our non-SUSY GUT model, as it predicts no new light particles, but there
has been a large effect on our SUSY model as its most favored parameter
space has been ruled out. So we have a large Bayesian update factor.

  -- -------- -------- -- -------
     @xmath   @xmath      (149)
              @xmath      
              @xmath      
  -- -------- -------- -- -------

Of course supersymmetric extensions of the SM have a large number of
parameters, and how to translate to an upper bound @xmath on our
one-parameter version isn’t well-defined, but certainly much of the
previously-favored parameter space has been ruled out. The general
lesson is that models which don’t predict new visible states near the
weak scale have received large Bayesian update factors from the LHC.
This doesn’t give a strict mandate for our overall relative belief, as
one may argue there are good reasons to take a large prior for
supersymmetry and expect always to find superpartners right around the
corner. After all, even if @xmath , it would still have @xmath
Giudice-Barbieri tuning better than the GUT without SUSY. But it does
motivate further investigation of models which don’t succumb to this
issue, be they neutral naturalness modules which push the scale of new
visible states up by a loop factor or more radical ideas about the
origin of the electroweak scale. Furthermore, we can directly input
physics into making a sensible choice of the prior one places on a
model, and there has been much discussion of justifying simple choices.
The dictum of Dirac naturalness mandates priors peaked at @xmath values.
But with a model which makes some parameter technically natural one, a
prior that allows small values is justified, such as a logarithmic
prior. An explicit example of this modification of priors by physics can
be seen in the application of the Weak Gravity Conjecture to the
hierarchy problem [ Cheung:2014vva , Ooguri:2016pdq , Ibanez:2017kvh ,
Ibanez:2017oqr , Hamada:2017yji , Lust:2017wrl , Gonzalo:2018tpb ,
Gonzalo:2018dxi , Craig:2018yvw , Craig:2019fdy , March-Russell:2020lkq
] , which will be discussed in more detail in Section 12.1 . This
mechanism explicitly modifies the prior one should have on UV theories
by linking the notion of the Swampland—that some string vacua don’t
admit universes like ours—to the allowed range of Higgs masses. This
addresses the hierarchy problem by constructing a model where the priors
are forced by UV physics to favor a light Higgs. While these measures of
naturalness are useful to help us clarify our expectations, we emphasize
again that they must be used sensibly. But it’s clear that some notion
of naturalness appears solely from the axioms of probability, and is
indeed baked-in to the practice of science inquiry. That’s not to say
that we should be epistemologically committed to the naturalness of the
universe, but we can still see it as a useful guide toward new physics
which has worked well in the past.

#### 5.3 The Lonely Higgs

  All models are wrong, but some are useful.

  George E. P. Box
  Robustness in the Strategy of
  Scientific Model Building (1979) [ BOX1979201 ]

There is another obvious suggestion that is useful to discuss: perhaps
the Higgs does not interact with any physics at higher mass scales, such
that despite in principle worries about the hierarchy problem there is
no hierarchy problem in practice . The physics which can destabilize the
Higgs mass must, as seen in the above example, both be heavier than the
Higgs and interact with it in order to generate a contribution. Since
the Higgs mass in the SM is not technically natural (that is, it is UV
sensitive), large mass corrections from such particles are generic, as
we saw in Section 3 . One can explore the idea that perhaps there are no
such particles. This faces a number of challenges. The first difficulty
is that we know there must be new physics. The Standard Model does not
explain neutrino masses nor dark matter, though it is possible that both
of these be resolved without introducing new heavy particles. But a
deeper issue is that we know the SM cannot be a fundamental theory
because the Landau pole in hypercharge makes it inconsistent. This
demands that something must happen to rid the theory of this pole, and
it will interact with the Higgs because the Higgs is hypercharged. This
is one motivation for thinking that something like Grand Unification
must take place, and of course its breaking introduces a heavy
fundamental scale. One can try to get around this by appealing to
quantum gravity coming in at scales below that of the Landau pole. But
the Higgs certainly interacts gravitationally, so for this program to
succeed one needs a quantum gravitational theory which does not
introduce any scales. This is interesting to explore, but does not seem
to be the way the universe works, though we leave detailed criticism of
this idea to the gravity theorists.

Furthermore, even if somehow all heavy particles are neutral under the
SM gauge groups, there is no way for them to escape gravitational
interactions with the Higgs. This leads to irreducible three-loop
corrections to the Higgs mass [ deGouvea:2014xba ] , as depicted in
Figure 15 . Consider a fermion @xmath with mass @xmath . The obvious
two-loop diagram where the graviton couples to the Higgs directly gives
a correction proportional to @xmath , as the graviton coupling to a
massless on-shell particle vanishes at zero momentum. However, we can
draw a diagram where a @xmath loop talks gravitationally to an off-shell
top loop contribution to the Higgs two-point function, and integrating
this particle out yields a correction

  -- -------- -- -------
     @xmath      (150)
  -- -------- -- -------

The powers of @xmath appear because the only other possible mass scale
for the numerator is @xmath , and the top loop power-law correction to
the Higgs mass does not vanish in the @xmath limit. The sensitivity of
the Higgs mass is softened by three loop factors as well as by the
Planck mass from the gravitational couplings, but insisting that @xmath
places an upper ‘naturalness’ limit on such fermions of @xmath . While
far better than the @xmath limit for SM-charged particles, this is still
well below the Planck scale and amounts to an enourmous constraint on UV
physics. In fact the problem is a bit worse than this estimate, as we
should sum over all SM loops that couple to the Higgs, but this suffices
already to see the issue. So asking for the Higgs to be lonely enough to
cure the hierarchy problem is a humongous requirement on the ultraviolet
of the universe, and this approach faces a number of important hurdles.
We mention that there is work on interesting theories which touch on
some of these points, but as a whole Nature seems not to have taken this
approach.

#### 5.4 Mass-Independent Regulators

One may hear the statement that the hierarchy problem disappears if you
use a mass-independent regularization scheme, for example dimensional
regularization. Unlike the prior nonsolutions we’ve considered, this one
is definitively incorrect. The mass scale one introduces in EFT is a
stand-in for genuine physical effects of any sort which appear at
shorter distance scales. So the cutoff regularization is useful for
seeing an avatar of the hierarchy problem even when one does not know
the ultraviolet theory. With a mass-independent scheme, one must instead
put in specific short-distance physics to see the problem, but we can
easily see the general issue. As a simple example, take a theory with
two real scalars - our light @xmath and a heavier @xmath . If we impose
a @xmath symmetry for simplicity, the action is

  -- -------- -- -------
     @xmath      (151)
  -- -------- -- -------

Doing continuum effective field theory with dimensional regularization
and the @xmath renormalization scheme, we must upgrade the masses and
couplings to running parameters which depend on the renormalization
scale @xmath , as usual (introduced in Section 2.2 ). Since our
renormalization scheme is mass-independent, if we want to study physics
at an energy scale @xmath , we should implement the decoupling theorem
by hand. We integrate out the heavy degree of freedom at the scale
@xmath and match to a low-energy effective field theory which only
contains the low-energy degree of freedom @xmath (introduced in Section
2.3 ). So let us go ahead and integrate out the heavy scalar @xmath .
The effect on the @xmath mass comes from the simple diagram of Figure 16
.

We go to general dimension @xmath and replace @xmath , where @xmath is
an arbitrary scale which soaks up the mass dimension of @xmath away from
@xmath . The resources needed to compute the integrals for general
dimension and to take the limit @xmath can be found in Srednicki’s
textbook [ Srednicki:2007qs ] .

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      (152)
  -- -------- -------- -- -------

Where @xmath is the Euler-Mascheroni constant, and we have Wick rotated
@xmath , integrated in general dimension, expanded in the limit @xmath
and defined @xmath to soak up the annoying constants. Indeed, we see
that there’s no quadratic divergence, which ultimately is due to the
fact that scaleless integrals vanish in dimensional regularization
@xmath , as must be true simply by dimensional analysis. Now we follow
the @xmath renormalization scheme by adding a counterterm which cancels
off the divergent piece and we match at @xmath to ensure that our
low-energy EFT produces the same predictions as the UV theory, as
discussed in Section 2.3 . This gives us

  -- -------- -- -------
     @xmath      (153)
  -- -------- -- -------

We see that the high-energy degree of freedom @xmath contributes a
threshold correction when we flow to lower energies and remove it from
the spectrum. While there was never a quadratic divergence, we still
found a large quadratic correction to the mass of the scalar @xmath
which is proportional to the scale of new physics. This underscores the
importance of not getting confused by unphysical features of
renormalization. There is a physical issue, which is the sensitivity of
the physical low-energy scalar mass to the physics in the ultraviolet.
Indeed if we tame the UV in different ways, we find different avatars of
this sensitivity. It’s true that a Wilsonian cutoff acts as a stand-in
for arbitrary scaleful physics, which is why it’s more direct to see the
issue in that picture, but the same physical problem appears regardless
of the regularization.

### 6 The Hierarchy Problem

After discussing those misconceptions, we may give a one-sentence
description of the hierarchy problem:
In a theory of physics beyond the Standard Model where the Higgs mass is
an output, physical parameters must be finely-tuned in order to produce
a mass which is far below the scale of new physics, in tension with the
principle of parsimony.
With that in hand, we are prepared to delve in to how the hierarchy
problem may be solved in the next section. Our discussion below will not
take place within a UV complete extension of the Standard Model, so one
might worry that we are attacking a problem without knowing its source.
While true, the point from Section 5.3 is that the sensitivity to UV
physics is so general that we expect to need a mechanism which
stabilizes the Higgs mass to whatever new heavy physics is out there.
Our toy calculation of the relative naturalness of SUSY already evinces
this point—SUSY tamps UV sensitivity no matter what it is, so the
details of the UV completion are immaterial. As a result of this idea,
we will mostly worry just about finding a way to produce a light scalar
and assume that it can be embedded into whatever physics exists at high
scales, rather than committing to a particular framework of grand
unification or what have you. Of course it’s possible that interesting
mechanisms to produce an IR scale do rely on particular properties of
the UV, and we’ll discuss this important idea in Section 12 . But even
there, our initial goal is simply to produce a light scalar which is
compatible with the Standard Model, rather than to write down a full
theory of the universe on all scales. If we can first solve the problem
in a toy model which shares some features of our universe, then we can
hope to abstract what we learn from that to solve the real problem.

## Chapter \thechapter The Classic Strategies

  Without going out of my door
  I can know all things on earth
  Without looking out of my window
  I could know the ways of heaven

  George Harrison satirizing the
  past decades of particle theory
  in light of LHC data
  The Inner Light (1968) [ harrison_1968 ]

### Fantastic Symmetries and How To Break Them

In large part the story of particle physics over the past decades is the
story of attempts to solve the hierarchy problem. Much theoretical
effort has been put into understanding interesting symmetries and
mechanisms for breaking them, and more generally ways that small numbers
can pop out of physical theories; and much experimental effort has
focused on locating empirical hints of these ideas. However, the past
few years have seen many practitioners turn their attention toward
topics like dark matter, cosmology, and astrophysics. And for good
reason—on the experimental side, this is largely where the new data is
and will be for the foreseeable future, and at the purely theoretical
level the hierarchy problem has become a lot more challenging, as we
will argue below. But this has lead to a new generation of particle
theorists who are largely unfamiliar with the fantastic and brilliant
ideas which drove the field in the prior couple decades. Despite the
fact that we will argue below that these ideas largely appear to not be
the way the world works at the weak scale, understanding this prior work
can be enormously helpful for inventing novel ideas in the future. It is
with this in mind that we introduce below the basics of a variety of
interesting ideas and methods in particle physics against the backdrop
of their relevance to the hierarchy problem. These are ideas that have
not yet made their way into standard textbooks on field theory, but are
nevertheless essential topics for students of particle theory to absorb.
We will endeavor to explicate the core of these ideas in the simplest
models possible, and will largely avoid discussing phenomenological
considerations past producing a light scalar. The discussion will not be
at the level of depth required for research in the field, but will
hopefully be a nice overview of interesting topics for which references
to serious introductions and reviews will be provided as well. So how
does one solve the hierarchy problem? The classical solutions may be
conceptually divided into two steps. First one introduces some structure
above the electroweak scale which protects the Higgs mass from large
contributions due to UV physics. This could be something like a new
symmetry which forbids a scalar mass term, or a modification to
spacetime on small length scales, or the dissolution of a
non-fundamental Higgs into component fields. However, the Higgs is not
exactly massless, which is due to the fact that whatever structure we
add is not a feature of the low-energy Standard Model. There must thus
be some IR dynamics that break that UV structure at the electroweak
scale to ensure that we end up with the Standard Model at low energies.
Depending on the UV structure this may be something like spontaneous
symmetry breaking or moduli stabilization or dimensional transmutation.
There are two big categories of classical solutions. One is to find a
field-theoretic mechanism which prevents contributions to the Higgs mass
in the UV. Supersymmetry is the prime example here. The other is to
bring the fundamental cutoff of the theory down to the infrared, such
that in the UV there’s no Higgs to talk about. This is exemplified by
composite Higgs theories or theories where the cutoff of quantum gravity
is lowered to the weak scale. To evince these strategies, we’ll go
through a couple examples of ways to forbid scalar masses and to break
those structures. Our aim here is not to construct realistic theories of
the Higgs but rather to understand these general principles, so we’ll
study simple toy models which allow us to appreciate the essential
points.

### 7 Supersymmetry

  Superpartners aren’t essential
  But would have been consequential
  Such wasted superpotential
  Super once, super twice
  Super chicken soup with rice

  Maurice Sendak on his disappointment with the LHC data
  Lost Stanza of Chicken Soup with Rice (1962) [ sendak1962chicken ]

Supersymmetry exploits a loophole in the classic Coleman-Mandula theorem
[ Coleman:1967ad ] by introducing fermionic symmetry generators, which
in layman’s terms turn bosons into fermions and vice-versa. By the
Haag–Łopuszański–Sohnius theorem [ Haag:1974qh ] , this is the unique
extension to the Poincaré algebra. Since we know that symmetries tend to
make physics easier, it is not surprising that supersymmetry is an
indispensable tool in high energy theory, regardless of how or whether
it is realized in the real world. Some useful general introductions to
supersymmetry in @xmath and its application to the real world are
Terning’s book [ Terning:2006bq ] , Martin’s periodically-updated
lecture notes [ Martin:1997ns ] , and Shih’s video lectures [ shih_2014
] , in roughly increasing order of friendliness to neophytes. In a
supersymmetric theory fields come in multiplets which include particles
of different spins (so called ‘supermultiplets’) all having the same
mass and quantum numbers. We add fermionic generators @xmath and @xmath
, called supercharges, with the defining (anti)commutation relations

  -- -------- -- -------
                 (154)
     @xmath      (155)
  -- -------- -- -------

where @xmath is the generator of spacetime translations and @xmath with
@xmath the Pauli matrices. These may be determined simply by writing
down all objects with the correct index structure. For later use, recall
that spinor indices are raised/lowered with the invariant antisymmetric
symbols @xmath , as used for example in defining the conjugate
invariants @xmath . We want to find irreducible representations of the
supersymmetry algebra, called supermultiplets. Since @xmath commutes
with the generators @xmath , @xmath , the different particles in a
supermultiplet will have the same mass. As an example of how to generate
supermultiplet states, consider a massive particle. We can go to its
rest frame, where it has momentum @xmath with @xmath its mass. Then the
supersymmetry algebra greatly simplifies to @xmath , and we see that
this is just a Clifford algebra of raising and lowering operators. We
define a lowest weight state, or Clifford vacuum @xmath such that it is
annihilated by the undotted generators

  -- -------- -- -------
     @xmath      (156)
     @xmath      (157)
  -- -------- -- -------

Now we can use the dotted generators as raising operators to generate
the entire multiplet.

  -- -------- -- -------
     @xmath      (158)
     @xmath      (159)
     @xmath      (160)
  -- -------- -- -------

A single fermionic supersymmetry generator must change the spin of a
state by @xmath . Starting at the top with a spin @xmath particle gives
us states of spin @xmath and @xmath on the middle line, and another
state of spin @xmath on the bottom line. In @xmath the supermultiplet
formed from a vacuum state of spin @xmath is called a ‘vector multiplet’
and contains four states with spins @xmath . That formed from spin 0 is
called a ‘chiral multiplet’, and has states with spins @xmath . Note
that this is fewer degrees of freedom, since negative spins are not
allowed. Beginning with spins higher than @xmath leads to states with
spins greater than @xmath , which will take us into supergravity and
will not be necessary for our purposes. We could repeat this exercise
for massless supermultiplets, labelling states by their energy and
helicity @xmath . We would find a Clifford algebra with only one set of
raising/lowering operators, and find supermultiplets with helicities
@xmath and @xmath for some starting @xmath . Then CPT invariance would
force us to add states of helicity @xmath and @xmath . Merely from the
definition of the symmetry group there are already a few interesting
immediate results. For a start, we show that physical states have
nonnegative energy in a supersymmetric theory, and the vacuum energy is
an order parameter for supersymmetry breaking. First, let’s give a
simple expression for the Hamiltonian operator of supersymmetry. We act
on our anticommutation relation with @xmath and recall various
identities to note that @xmath , which gives us

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

where we have used the fact that the zeroth component of the generator
of spacetime translations is the generator of time translations, which
is the Hamiltonian operator. Then we can write the energy of some state
state @xmath as

  -- -------- -- -------
     @xmath      (161)
  -- -------- -- -------

so the energy of @xmath is non-negative. Furthermore, consider a vacuum
state @xmath of our theory. In a standard QFT, the vacuum energy @xmath
is non-physical—we can just shift the Hamiltonian arbitrarily to remove
it. But here, the supersymmetry algebra gives a preferred frame. If a
vacuum state @xmath is supersymmetric then it is annihilated by the
supercharges @xmath , otherwise the vacuum would not be invariant under
supersymmetry transforms. This implies that it will have vanishing total
energy @xmath . Conversely, if the vacuum state is non-supersymmetric,
then its energy is strictly positive. We say supersymmetry is broken in
such a state. Thus the vacuum energy acts as an order parameter for SUSY
breaking. Connected to that fact is that each supermultiplet contains
the same number of fermionic and bosonic degrees of freedom. We can see
this by defining an operator @xmath which counts the fermion number of a
state, so that bosonic states have eigenvalue @xmath under @xmath , and
fermionic states have eigenvalue @xmath . Since the SUSY generators
interchange bosonic and fermionic states, they must anticommute with
@xmath . Now, for a given supermultiplet consider the states @xmath with
the same given four-momentum @xmath , @xmath . Since the supercharges
commute with @xmath , we know that these must form a complete set of
states in this subspace @xmath . Now consider the trace of the weighted
energy operator @xmath .

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      (162)
  -- -------- -------- -- -------

where we have suppressed the contracted spinorial indices. This implies
that the number of bosonic degrees of freedom is the same as the number
of fermionic degrees of freedom in our supermultiplet, which we found to
be true in the example we considered above. There is a beautiful
formalism of ‘superspace’ which can be used to make supersymmetric
theories far more transparent, but introducing this would be too large
of a digression for our purposes. ¹⁷ ¹⁷ 17 Martin’s notes [
Martin:1997ns ] serve as a good introduction to traditional ‘off-shell’
superspace for @xmath theories, and Thaler’s TASI lecture notes [
Bertolini:2013via ] are also a fantastic resource. There is a related
but distinct formalism of ‘on-shell’ superspace, which falls under the
heading of the amplitudes/on-shell/S-matrix program. This was first
introduced very early on by Nair [ Nair:1988bq ] and was used to great
effect by Arkani-Hamed, Cachazo, & Kaplan [ ArkaniHamed:2008gz ] much
later. A pedagogical introduction to on-shell techniques including
superspace can be found in the textbook by Elvang & Huang [
Elvang:2015rqa ] . Until recently, the on-shell program was mostly
restricted to massless particles. As it so happens, after Arkani-Hamed,
Huang, & Huang [ Arkani-Hamed:2017jhn ] introduced a beautiful extension
of the formalism to include massive particles, it was Timothy Trott, my
undergrad mentee Aidan Herderschee, and myself who formulated an
extension of the on-shell superspace formalism for massive particles [
Herderschee:2019ofc ] . The on-shell program is another fascinating line
of work that I suggest any aspiring particle or field theorist learn
about. We simply want to see the effects of supersymmetry on
(in)sensitivity of low-energy physics to the ultraviolet, for which
studying a simple theory of chiral superfields will do. The Wess-Zumino
model is the simplest such example which is not free, consisting of a
single self-interacting chiral supermultiplet, and was historically the
first non-trivial four-dimensional theory proved to be supersymmetric.
We may write down the Wess-Zumino Lagrangian as

  -- -------- -- -------
     @xmath      (163)
  -- -------- -- -------

where for compactness we’ve left off the Hermitian conjugate terms. To
avoid introducing certain technical complications we eschew the proof
that this is indeed invariant under a supersymmetric transformation and
instead evince its UV insensitivity. For fun we’ll compute without
assuming the masses are the same @xmath , as can happen in the presence
of soft breaking of the symmetry. {comment} It is a simple exercise to
show that this theory is invariant under the supersymmetry
transformations

  -- -------- -- -------
     @xmath      (164)
  -- -------- -- -------

where @xmath is an anti-commuting Weyl spinor parametrizing an
infinitesimal transformation. Let’s look at the vacuum energy, which
we’ll calculate generally but schematically. A quantum harmonic
oscillator has ground state energy @xmath for bosonic and fermionic
states respectively, with the sign being familiar from the Casimir
effect. If we consider a box of side length @xmath , the energy of the
fields inside it is

  -- -------- -- -------
     @xmath      (165)
  -- -------- -- -------

where @xmath , @xmath . Now in QFT each mode has energy @xmath , and as
we make the box bigger @xmath , the sum turns into an integral

  -- -------- -- -------
     @xmath      (166)
  -- -------- -- -------

with @xmath a boson mass and @xmath a fermion mass, where the sum over
species is implicit. We also recognize @xmath as the vacuum energy
density, denoted @xmath , which we can write as

  -- -------- -- -------
     @xmath      (167)
  -- -------- -- -------

Now if we specialize to @xmath and introduce a cutoff @xmath up to which
we’re confident that our description of particle physics holds, the
schematic form is simply

  -- -------- -- -------
     @xmath      (168)
  -- -------- -- -------

Now we see quite generally and explicitly that in a supersymmetric state
the vacuum energy vanishes, since there are equal numbers of bosonic and
fermionic fields with degenerate masses. Furthermore, spontaneous
breaking of supersymmetry breaks the degeneracy but does not change the
numbers of fields, so softly broken supersymmetry retains protection
from the largest contribution.

Let’s look now more sharply at the one-loop contributions to the scalar
mass in the Wess-Zumino model, regularized with a hard cutoff @xmath .
Take care that we’ve written the Lagrangian in terms of two-component
spinors, an exhaustive guide to which can be found in [ Dreiner:2008tw ]
. The three diagrams are shown in Figure 17 , and their evaluation
proceeds as

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      (169)
              @xmath      (170)
              @xmath      (171)
              @xmath      (172)
  -- -------- -------- -- -------

We see that the UV sensitivity of the scalar mass in this theory has
disappeared, even if the two fields have different masses. In the limit
of unbroken supersymmetry, the contribution vanishes identically. This
fact of removing the UV sensitivity of the mass of a scalar persists
generally, no matter which other superfields are added, so long as
supersymmetry is at most softly broken. The connection to the hierarchy
problem is clear: If, in the UV, all fields come in supermultiplets,
then the Higgs mass is protected from UV contributions. Of course we do
not observe mass-degenerate superpartners, so this soft supersymmetry
breaking is a necessary feature of any implementation of supersymmetry
to the real world. The Minimal Supersymmetric Standard Model [
Dimopoulos:1981zb ] embeds each of our fermions in a chiral
supermultiplet and each gauge boson in a vector multiplet. The Higgs
sector must be enlarged to two chiral multiplets containing the up and
down Yukawas respectively, as is necessary for anomaly cancellation ¹⁸
¹⁸ 18 Perhaps the more urgent reason for needing two Higgs multiplets is
that the interactions in supersymmetric theories are highly constrained
by ‘holomorphy’, a full explanation of which here would require too much
machinery but which leads to the conclusion that the same multiplet
cannot have Yukawa interactions with both the up- and down-type quarks.
However see [ Davies:2011mp ] for the interesting possibility that at
high energies only the up-type Yukawa interactions exist, and the
down-type and charged lepton masses are induced by
supersymmetry-breaking. . The question of supersymmetry-breaking is a
very non-trivial one. At the level of a phenomenological accounting of
possible soft breaking terms in the MSSM, there are 105 physical
parameters [ Chung:2003fi ] . However, constraints on flavor-violating
couplings and on CP violation tell us empirically that the soft terms
that appear must be very non-generic. In fact, looking at the MSSM in
detail it turns out there are no places for supersymmetry-breaking to
enter directly, and indeed there are general arguments that such
breaking must take place in another, hidden sector and be indirectly
communicated to the MSSM fields (see e.g. Martin’s Section 7.4 [
Martin:1997ns ] ). The origins of supersymmetry-breaking being a
separate sector does force us to expand our model of particle physics,
but on the upshot this sequestering means we can explore interesting
phenomenology in sectors which are unconstrained. One can write down
models where supersymmetry breaking is mediated by supergravity effects
[ Chamseddine:1982jx , Barbieri:1982eh , Ibanez:1982ee , Hall:1983iz ,
Ohta:1982wn , Ellis:1982wr , AlvarezGaume:1983gj ] , communicated to the
SM fields by our gauge bosons from a sector with new, massive SM-charged
particles [ Dine:1981gu , Nappi:1982hm , AlvarezGaume:1981wy ,
Dine:1993yw , Dine:1994vc , Dine:1995ag ] , or takes place at a
physically separate location in an extra dimension [ Mirabelli:1997aj ,
Kaplan:1999ac , Chacko:1999mi , Schmaltz:2000ei , Schmaltz:2000gy ,
Csaki:2001em , Cheng:2001an , Randall:1998uk , Giudice:1998ck ,
Bagger:1999rd ] , for a few examples. A full discussion of the
mechanisms and strategies for models of supersymmetry-breaking is beyond
our scope, but we highly recommend Intriligator & Seiberg’s lecture
notes [ Intriligator:2007cp ] as a general reference along with Martin’s
notes [ Martin:1997ns ] . Of course we would like this phase transition
to originate as spontaneous symmetry breaking, rather than explicitly
putting it in by hand, since we want the far UV to be supersymmetric.
Such spontaneous breaking requires the generation of a scale, and so it
would be great if such a scale were generated dynamically , as in the
dimensional transmutation we saw in QCD in Section 3.3 . This would then
be a natural mechanism for SUSY breaking. This phenomenological prospect
lead to and benefited from a fantastic body of work understanding the
details of supersymmetric gauge theories e.g. [ Witten:1981nf ,
Affleck:1983mk , Affleck:1983rr , Affleck:1984uz , Affleck:1984xz ,
Seiberg:1994bz , Intriligator:1995au , Shifman:1995ua , Peskin:1997qi ,
Poppitz:1998vd , Intriligator:2006dd ] , which can be found reviewed in
textbooks by Terning [ Terning:2006bq ] and Shifman [ Shifman:2012zz ]
and in TASI notes by Strassler [ Strassler:2003qg ] . We stated above
that fields in a given supermultiplet share all the same quantum
numbers, but there is in fact one exception. The supersymmetry algebra
in Equation 154 is invariant under opposite rephasings of the
supercharges @xmath , @xmath . So there is a generator of a global
internal symmetry that we may add that has nontrivial commutation
relations with the supercharges:

  -- -------- -- -------
     @xmath      (173)
  -- -------- -- -------

This generator is known as an @xmath -symmetry generator, and if the
theory is invariant under an @xmath -symmetry that means that each
supermultiplet @xmath can be assigned an @xmath -charge @xmath and the
theory is invariant under transformations of each multiplet @xmath ,
schematically, where @xmath is the collection of fields in that
multiplet. Since @xmath does not commute with the supercharges, the
different fields in the supermultiplet have different @xmath -charges.
For example if @xmath is a chiral superfield consisting of @xmath then
under a global rotation by @xmath they transform as

  -- -------- -- -------
     @xmath      (174)
  -- -------- -- -------

which is simply because @xmath . It’s important to emphasize that this
@xmath -symmetry is not part of the supersymmetry algebra, so one may
have supersymmetric theories which do or do not implement @xmath
-symmetry. Nelson & Seiberg showed a fascinating connection between
@xmath -symmetry and supersymmetry-breaking [ Nelson:1993nf ] . They
show roughly that for a low-energy Wess-Zumino model (possibly after
having integrated out confined strong dynamics) as long as one has a
‘generic’ potential (in the sense that a generic set of @xmath equations
in @xmath unknowns has a solution), then a vacuum spontaneously breaks
supersymmetry if and only if it spontaneously breaks @xmath -symmetry.
The reasoning is simply that such a symmetry imposes an additional
constraint on the potential minimization equations, leading to a
solution no longer being generically present. For later use we mention
the possibility of ‘extended supersymmetry’, where additional
supercharges are added

  -- -------- -- -------
     @xmath      (175)
  -- -------- -- -------

where @xmath index the supercharges. The construction of supermultiplets
proceeds as before, but there are now more nonvanishing combinations of
supercharges to act on the Clifford vacuum, so supermultiplets are
enlarged. In four dimensions, the most supersymmetry we can have without
gravity is @xmath , which contains enough supercharges to relate the
helicity @xmath vector all the way to the helicity @xmath vector; any
more supercharges would necessarily yield particles of spins @xmath . If
we are willing to include these degrees of freedom we can only go up to
@xmath ‘supergravity’ (SUGRA), as more charges would lead to a theory
with fundamental particles of spins @xmath which is pathological ¹⁹ ¹⁹
19 There’s an important exception here, which is a theory which includes
particles of all spins. This is necessary for string theory to operate,
but has also led to the formulation of novel field theories commonly
called ‘Vasiliev gravity’ [ Vasiliev:1990en ] . . The classic references
for supergravity are Wess & Bagger [ Wess:1992cp ] and Freedman & Van
Proeyen [ Freedman:2012zz ] . Ultimately if supersymmetry is a
field-theoretic feature of the ultraviolet of our universe we must have
SUGRA as well, as it simply results from applying the supercharges to
the graviton field, but we won’t discuss SUGRA any further. When we
enlarge our superalgebra we also enlarge the (potential) R-symmetry
group—the group of symmetries which do not commute with the
supercharges—since we can now shuffle around the supercharges in
addition to rephasing them. We’ll revisit this in Section 8.3 in the
context of utilizing non-trivial R-symmetry representations to break
supersymmetry.

### 8 Extra Dimensions

  I exist in the hope that these memoirs, in some manner, I know not
  how, may find their way to the minds of humanity in Some Dimension,
  and may stir up a race of rebels who shall refuse to be confined to
  limited Dimensionality.

  Edwin Abbott Abbott,
  Flatland , 1884 [ abbott1885flatland ]

One of the most important ideas in theoretical physics developed in the
latter part of the @xmath century is that there may be additional
spatial dimensions past the familiar three of our everyday experiences.
Theories in which additional spatial dimensions are present were first
studied in the context of unifying gravity and electromagnetism, first
by Nordström [ Nordstrom:1988fi ] ( before General Relativity!) and then
by Kaluza [ Kaluza:1921tu ] and Klein [ Klein:1926tv ] . These ideas saw
a resurgence of interest some half-century later with the advent of
string theory, and the vision of all features of the universe being
fundamentally geometrized. Against that backdrop, it is clearly prudent
to consider the interplay of such theories with the puzzle of the
hierarchy problem. As we shall see, extra dimensional theories can
produce terribly interesting physics, and the possibilities are manifold
.

#### 8.1 Technology: Kaluza-Klein Reduction

To consider the possibility that there are additional microscopic
dimensions, we develop a picture of the effects of fundamentally @xmath
-dimensional fields where @xmath . On our manifold @xmath , where @xmath
is compact, we write down an action

  -- -------- -- -------
     @xmath      (176)
  -- -------- -- -------

where our fields are in irreducible representations of the @xmath
-dimensional Poincaré algebra, and the Lagrangian manifestly obeys
@xmath -dimensional Lorentz invariance. We’ll use boldface for @xmath
-dimensional fields and Latin letters for @xmath -dimensional Lorentz
indices. However, since @xmath is compact, it places constraints on the
mode expansions of our fields. Then when we want to study the effective
four-dimensional theory we first need to decompose our fields into
irreducible representations of the @xmath -dimensional Poincaré algebra.
The @xmath -dimensional vectors and tensors will become multiplets of
@xmath -dimensional fields. Then we can explicitly integrate over the
compact manifold @xmath , which will produce a tower of states. As an
easy, explicit example, take the compact manifold to be the circle
@xmath of length @xmath , and consider a single complex scalar. We start
with

  -- -------- -- -------
     @xmath      (177)
  -- -------- -- -------

From introductory quantum mechanics, we know that the boundary
conditions of being single-valued around the circle constrains the mode
expansion of @xmath . We can write

  -- -------- -- -------
     @xmath      (178)
  -- -------- -- -------

where the normalization will produce canonically normalized kinetic
terms in the @xmath -dimensional action, and should generally be @xmath
. Plugging this into the action gives

  -- -------- -- -------
     @xmath      (179)
  -- -------- -- -------

where the first and second terms come from either the @xmath derivatives
or the @xmath derivatives acting on the mode expansion. The integral
over the compact direction now gives, using orthogonality @xmath , a
@xmath -dimensional action

  -- -------- -- -------
     @xmath      (180)
  -- -------- -- -------

We see that we now have a tower of @xmath -dimensional states that have
arisen from the one @xmath -dimensional scalar, as shown schematically
in Figure 18 . There is a ‘zero-mode’ which has a mass given by the
@xmath -dimensional mass term, and then there are states with larger
masses. If @xmath is small, these will be heavy, and so we would only
see them at, say, a high energy particle collider. Note that all of
these higher levels are doubly-degenerate, since @xmath ranges over all
the integers. If we had included the graviton in this compactification
procedure, we would see the @xmath -dimensional graviton split up into

  -- -------- --
     @xmath   
  -- -------- --

so we see the emergence of a @xmath -dimensional scalar, vector, and
two-index symmetric tensor. And it was in this context that
compactification was originally studied as a unification of general
relativity and electromagnetism. There’s another very important effect
of the compactification. Let’s consider the Einstein Hilbert action

  -- -------- -- -------
     @xmath      (181)
  -- -------- -- -------

Noting that the Ricci scalar @xmath always has mass dimension @xmath ,
we see that the @xmath -dimensional Newton’s constant must have @xmath ,
so to rewrite the action in natural units we must define a @xmath
-dimensional Planck mass as @xmath . If we take the metric to be
independent of the @xmath coordinates, then the integration over @xmath
just gives a factor of the volume @xmath of the compact space

  -- -------- -- -------
     @xmath      (182)
  -- -------- -- -------

Putting this into the form of the four-dimensional Einstein-Hilbert
action, we find @xmath . Taking @xmath for some radius @xmath , we see
that if the compact dimensions are not Planck-sized @xmath but larger
for whatever reason, then the effective Planck mass at long distances
can be much larger than the fundamental Planck mass. Now we don’t see
zero-mode, different spin partners of our particles which fill out
representations of the higher-dimensional symmetry, so this simple
compactification cannot be the real way the world is. If we have compact
dimensions, they must be such that the symmetry of zero-modes is somehow
broken for the SM fields. This is important—we don’t just want to have
different @xmath fields with zero modes or not, we need different
four-dimensional components of them to have or not have zero modes.
We’ll solve this problem in Section 8.3 .

#### 8.2 Quantum Gravity at the TeV Scale

In the previous section we noticed that extra dimensions dilute the
fundamental Planck mass in the higher-dimensional theory to produce a
weaker effective four-dimensional Planck mass. So perhaps we can fix the
hierarchy between the electroweak scale and Planck scale by lowering the
fundamental scale of quantum gravity. Arkani-Hamed, Dimopoulos, and
Dvali proposed that the fundamental Planck scale can be @xmath with the
weakness of four-dimensional gravity resulting from the dilution of
gravitational flux into the extra @xmath dimensions [ ArkaniHamed:1998rs
] . Using

  -- -------- -- -------
     @xmath      (183)
  -- -------- -- -------

where @xmath is the radius of the extra dimensions, we find that a
parsimonious @xmath spherical dimensions of radius @xmath suffice to
remove the hierarchy problem and accord with Eöt-Wash constraints on the
behavior of gravity at scales down to @xmath [ Hoyle:2000cv ,
Adelberger:2009zz , Lee:2020zjt ] . Now a nanometer is tiny compared to
human scales, but the associated energy scale is @xmath , which is a
scale we have quite a bit of information about. In particular, if the
Standard Model fields were to propagate in all @xmath dimensions then it
would be easy to excite the ‘winding modes’ with momentum in the extra
dimensions, and we should have observed many finely-space Kaluza-Klein
resonances with the same quantum numbers. This is obviously not how the
universe works, so to dilute gravity with large extra dimensions one
must trap Standard Model fields to the four-dimensional manifold we know
and love. This can be done by imagining we live on a @xmath -dimensional
topological defect which is embedded in the larger @xmath -dimensional
space. ‘Topological defect’ sounds exotic, but these are just
(semi-)familiar non-perturbative objects such as the branes of string
theory [ Polchinski:1996fm , Polchinski:1996na , Johnson:2000ch ] , or
the cosmic strings or domain walls that can appear in Higgsed gauge
theories and which are introduced well in Shifman’s textbook [
Shifman:2012zz ] . The original proposal suggests a weak-scale vortex in
which zero-modes of our familiar fields are trapped, which is super
cool. Note that the new physics appearing at the TeV scale in this
scenario is about as violent as you could imagine: quantum gravity
appears at a TeV! This leads to a variety of fascinating signatures and
constraints, even in the absence of concrete model of quantum gravity,
though embeddings into string theory have also been found [
Antoniadis:1998ig , Shiu:1998pa , Ibanez:2001nd ] . Very generically
there are corrections to the Newtonian gravitational laws [
ArkaniHamed:1998nn ] , all sorts of effects on precision observables [
Rizzo:1999br ] , a loss of flux of high energy particles into the
ambient space either astrophysically [ Cullen:1999hc ] or at a collider
[ Giudice:1998ck ] , violations of the global symmetries of the SM since
quantum gravity does not respect them [ Dienes:1998vg , Dienes:1998vh ,
ArkaniHamed:1999dc ] , and production of Kaluza-Klein gravitons [
Hewett:1998sn ] and black holes at TeV-scale colliders [ Argyres:1998qn
, Emparan:2000rs , Giddings:2001bu , Eardley:2002re ] . This is a
fascinating field which is well worth studying in detail, but
unfortunately we do not have the space to do it justice. For more detail
we refer to the reviews by Rubakov [ Rubakov:2001kp ] and Maartens &
Koyama [ Maartens:2010ar ] , and the more introductory notes from Csáki
[ Csaki:2004ay ] , Kribs [ Kribs:2006mq ] , Pérez-Lorenzana [
PerezLorenzana:2005iv ] , Cheng [ Cheng:2010pt ] , and Csáki, Hubisz,
and Meade [ Csaki:2005vy ] . However, shrewd readers will be eager to
point out that we haven’t actually solved the hierarchy problem; we’ve
merely traded the @xmath hierarchy for a @xmath hierarchy. And indeed,
it can be difficult to stabilize the size of the extra dimensions in
this scheme. But the conceptual leap of considering geometric solutions
to the hierarchy problem is incredibly important and leads to many
further interesting directions. The next ingredient we need is to
control the existence of opposite-spin partners.

#### 8.3 Technology: Orbifold Reduction

Let us consider a spacetime manifold @xmath with @xmath four-dimensional
Minkowski space and @xmath a @xmath -dimensional compact ‘orbifold’. An
orbifold is constructed by ‘modding out’ a manifold @xmath by a discrete
symmetry group @xmath . A manifold is a space that looks locally like
Euclidean space; an orbifold is one which locally looks like the
quotient space of Euclidean space quotiented by a finite group. In
layman’s terms, quotienting or modding out is just identifying points
which are transformed into each other under @xmath —our space becomes
the space of equivalence classes of @xmath under the action of elements
of @xmath . As an illustrative example, consider the line @xmath and the
action of the discrete symmetry @xmath . When we form the quotient space
and identify points under this @xmath action, we find the half line
@xmath . You’ll notice that this space has a boundary at @xmath , which
was a fixed point of the symmetry group. Mathematicians would say that
the @xmath acts freely except at this point. This is a generic feature,
and in fact what makes orbifolds interesting for our purposes. We could
have also imagined ‘orbifolding’ @xmath by the translation @xmath , but
this would have produced a compact manifold without boundary, @xmath ,
because this symmetry has no fixed points. Part of the power of
orbifolds comes because the quotient group structure gives us some
information about what happens at fixed points - you could imagine just
studying the half-line, but it seems natural to consider smooth
structures on the manifold and then look at the effects under the
identification. This should be evinced later in our examples.

Compactifying on orbifolds also gives us a way to cure our missing
partner ills. Consider quotienting the circle by a @xmath which folds
the circle over onto itself, @xmath . This produces a line segment with
boundaries at both ends @xmath . We can alternatively think of this as
modding out the real line by both translation and mirroring, producing
@xmath . There are then two different sorts of fixed points— @xmath is
fixed by @xmath , and @xmath is fixed by @xmath . You can then envision
this as the circle of length @xmath with these two discrete
identifications, that is @xmath , as depicted in Figure 20 .

How does this affect the resulting compactification? As a first step
should take our results and rewrite them in terms of eigenfunctions of
our @xmath symmetry. We write

  -- -------- -- -------
     @xmath      (184)
  -- -------- -- -------

where all we have done is rearrange things by defining

  -- -------- -- -------
     @xmath      (185)
  -- -------- -- -------

where the superscript denotes their eigenvalue under the @xmath . Now
the way to change our @xmath compactification to an @xmath
compactification is to impose in our @xmath -dimensional action that
@xmath transform with definite parity, which must be the case if the
@xmath is a good symmetry ²⁰ ²⁰ 20 Note that since we’re getting our
eigenmodes through this orbifold reduction our reduced action will still
need to be produced by integrating from @xmath to @xmath because it’s
this domain over which our eigenfunctions are orthonormal. . You’ll
notice our free action does not demand a particular choice of parity for
@xmath , so we are free to choose @xmath either even or odd. But in
either case we are forced to get rid of half of our states. If @xmath is
even we must set @xmath , and if @xmath is odd we must set @xmath
—including, importantly, getting rid of the zero mode. If we wanted to
include certain interactions in the higher-dimensional theory, that
could dictate the transformation of @xmath —for example, the interaction
term @xmath would necessitate an even @xmath . This is just the same as
we’re familiar with in four dimensions. More interestingly, let us
consider the effect of the orbifold on larger Lorentz representations.
Imagine a @xmath -dimensional gauge field with free action

  -- -------- -- -------
     @xmath      (186)
  -- -------- -- -------

where @xmath is the field strength. The mixed terms here read @xmath ,
and this must transform coherently under the symmetry in order for the
kinetic term to be invariant. But notice that @xmath . So @xmath and
@xmath are forced to have opposite transformations under the reflection
symmetry! Thus in this example one and only one of the four-dimensional
vector and scalar multiplet has a massless zero mode; so orbifold
compactifications generally enable us to have light fields without
partners. For another example of the usefulness of orbifolding, consider
a five dimensional theory with the minimal amount of supersymmetry and
we want to ensure our four-dimensional theory has only @xmath
supersymmetry. First let’s recall why we cannot have more than @xmath in
four dimensions. As emphasized above, the SM is a chiral theory, wherein
the different chiral components of its Dirac fields are in different
gauge symmetry representations. This means that @xmath supersymmetry in
four dimensions is too much for us, since the irreducible
representations of super-Poincaré in that case don’t allow for chiral
matter. In particular, the @xmath vector multiplet must transform in the
adjoint, and the @xmath hypermultiplet must transform in a real
representation in order for it to be CPT self-conjugate, which means the
two Weyl fermions must transform in conjugate representations. Thus if
we want to imagine that the world came from a higher-dimensional
supersymmetric theory, orbifold compactification is necessary. The same
problem appears just for five dimensional spinor fields, since there is
no such thing as chirality in odd spacetime dimensions. So under
dimensional reduction, one five-dimensional spinor breaks into two
conjugate four-dimensional spinors, and you cannot get chiral matter.
This is really the same problem as the above, since the possible
supersymmety comes exactly from the possible spinor representations.
Compactifying a theory on an orbifold, rather than a manifold, will
allow us to solve both of these problems—obtaining chiral matter, and
reducing the amount of supersymmetry we have. First note that we can
build @xmath supermultiplets out of two @xmath supermultiplets, which is
really just thinking about a particular ordering for the construction of
the supermultiplet by acting with supercharges. So for each @xmath
superfield we want to have, we need to arrange for it to be even under
the parity, and so have a zero-mode, and its partner superfield to be
odd under the parity, and so only have @xmath Kaluza-Klein modes. In
this way we get a set of zero modes which are chiral and @xmath
supersymmetric, while the towers reflect the full @xmath supersymmetry
and are non-chiral. As an explicit example, consider an @xmath vector
multiplet, which consists of a real scalar @xmath , two fermions @xmath
with @xmath , and a vector @xmath . Under the @xmath symmetry, the
scalar and vector are singlets and the fermions form a doublet. As in
the example above, the @xmath Lagrangian for this multiplet dictates
that the two @xmath multiplets living inside it transform differently
under the @xmath . So on the four-dimensional boundary one gets a
zero-mode for either @xmath or @xmath , corresponding to a vector
multiplet or a chiral multiplet respectively. In slightly more
group-theoretic language we can say that we’ve embedded the @xmath in
the @xmath , and have been forced by the physics to put the doublet in a
two-dimensional representation

  -- -------- -- -------
     @xmath      (187)
  -- -------- -- -------

and to pair each of these fermions with two bosonic degrees of freedom,
@xmath . This gives us either an @xmath chiral multiplet or an @xmath
vector multiplet on the boundary. More discussion and details can be
found in Quirós’ TASI notes [ Quiros:2003gg ] , and in e.g. [
Delgado:1998qr , Barbieri:2001dm , Hall:2001pg ] . In fact orbifolding
can do even more symmetry-breaking for us. As mentioned above, we can
alternatively consider compactification on an interval as the orbifold
@xmath , which then means the two boundaries are fixed points of
independent @xmath symmetries. In particular this means we can choose
different embeddings of the @xmath in the full symmetry on the two ends
@xmath [ Scherk:1978ta , Scherk:1979zr ] . Now let’s apply this
technology to the @xmath case we considered above. We saw we had two
different choices for nontrivial embeddings of the @xmath into the
R-symmetry to break the supersymmetry down to @xmath at the boundary.
These correspond to choosing which half of the fields are even under the
@xmath and so get zero-modes. Now that we have independent @xmath s on
either end, we can choose these to leave different @xmath symmetries
unbroken. In a microscopic picture, what we end up with is a theory on
@xmath where the bulk is ( @xmath )-supersymmetric and the boundaries
respect different @xmath supersymmetries. When we look at the effective
four-dimensional theory at distances much larger than @xmath , we have
fully broken supersymmetry with the breaking being a nonlocal effect—one
must be sensitive to physics on both boundaries in order to see the full
breaking. As a result of this nonlocality, supersymmetry-breaking is
guaranteed to be ‘soft’ and the effects cannot depend on positive powers
of UV scales, as we will discuss further momentarily. This leads to
fantastically predictive models of BSM physics from around the turn of
the millennium which really do read like they have it all figured out
(see e.g. [ Antoniadis:1993jp , Delgado:1998qr , Antoniadis:1998sd ,
Pomarol:1998sd , Barbieri:2000vh , Barbieri:2001dm , Barbieri:2001yz ,
Hall:2001pg ] ). It’s worth understanding these in some detail simply
because of how beautiful they are, but they uniformly lead to lots of
(thus far) unseen structure near the TeV scale as KK partners become
excited.

#### 8.4 Nonlocal Symmetry-Breaking

Consider a single extra circular dimension of radius @xmath with gravity
and some gauge field. Upon restriction to the four-dimensional Lorentz
group, the five-dimensional graviton breaks up into a four-dimensional
graviton, vector, and scalar, while the five-dimensional vector breaks
up into a four-dimensional vector and scalar. Each of these must be
massless by five-dimensional gauge-invariance above the scale @xmath ,
but below that they can pick up mass corrections up to that cutoff.
Either of these possibilities then amounts to a mechanism for UV
protection of a scalar mass. In implementing this in the SM, the first
possibility is known as the Higgs being a radion—the scalar which
controls fluctuations of the size of the fifth dimension, @xmath , and
the latter is denoted ‘gauge-Higgs unification’ for obvious reasons. A
particular motivation for gauge-Higgs unification is as an extension of
the strategy of grand unification. As mentioned in Section The Hierarchy
Problem: From the Fundamentals to the Frontiers , the SM gauge bosons
and fermions beautifully fit into representations of larger gauge
groups, but in 4d GUTs the Higgs is left out in the cold as an extra
puzzle piece. But ‘grand gauge-Higgs unification’ in higher dimensions
may allow even further frugality of ingredients [ Hall:2001zb ,
Haba:2004qf , Lim:2007jv , Furui:2016owe ] . A very interesting feature
of this sort of construction is that the symmetry-breaking which is
responsible for producing the light scalar is nonlocal —one must
traverse around the fifth dimension to see the effects of the breaking.
This should be intuitively clear, as at distances small compared to
@xmath the theory looks like five-dimensional Minkowski space. If it
isn’t obvious, I recommend musing by analogy on how we could tell
whether or not the universe is a sphere with radius far, far larger than
the Hubble scale @xmath . As a result of this nonlocal
symmetry-breaking, the scalar mass must be finite and calculable in the
low-energy theory below @xmath . There cannot be any sensitivity to
ultraviolet energy scales @xmath , as this corresponds to a ‘local
counterterm’, but in the theory above @xmath we know that the mass
vanishes identically by gauge invariance, so such a counterterm cannot
occur (recall our discussion in Section 2.1 ). This is powerful because
symmetry-breaking generally leaves residual logarithmic dependence on
large scales even when quadratic dependence has been eliminated, as we
saw in the example of supersymmetry above. Of course the theory can
generate finite corrections to the mass of such a scalar at and below
the scale @xmath . If we look only at the low-energy effective theory
then these look divergent, but we know they must get cut off at @xmath .
Since we know the high-energy theory we can ask how the scalar gets a
mass at all, which seems to be impossible from gauge invariance. In fact
the scalar mass comes from the Wilson loop wrapping the non-trivial
cycle around the fifth dimension [ Manton:1979kb , Forgacs:1979zs ,
Fairlie:1979at , Fairlie:1979zy , Hosotani:1983vn , Hosotani:1983xw ,
Hosotani:1988bm ] . In any gauge theory there is a gauge-invariant
operator called a ‘Wilson loop’,

  -- -------- -- -------
     @xmath      (188)
  -- -------- -- -------

where @xmath is the gauge coupling, @xmath is some closed path through
spacetime, and @xmath denotes ‘path ordering’ of the operators along
@xmath in similarity to the time ordering in the definition of the
Feynman propagator. In an Abelian theory, the gauge field transforms as
@xmath and we see that @xmath . Integration by parts leaves us only with
gauge transformations at the endpoints of @xmath , of which there are
none if @xmath is a loop (equivalently if we started more generally
looking at a ‘Wilson line’, we can say that for a loop the endpoints are
connected and so their gauge transformations cancel each other). This
remains true in non-Abelian gauge theories, though we do not go through
the proof. In our case we can consider a path which goes solely around
the fifth dimension, in which case the Wilson loop contains only our 4d
scalar @xmath and yet is fully gauge-invariant. Quantum corrections can
thus generate the operator

  -- -------- -- -------
     @xmath      (189)
  -- -------- -- -------

We note that on a circle the four- and five-dimensional gauge couplings
are related as @xmath . Proceeding naïvely and expanding @xmath , we see
that this operator includes a four-dimensional mass for the zero-mode,

  -- -------- -- -------
     @xmath      (190)
  -- -------- -- -------

where we note that the natural size for the Wilson coefficient @xmath
yields a scalar mass expectation which is @xmath , unsurprisingly as
@xmath is the only scale in the problem, and the dependence on the gauge
coupling reveals the scalar’s five-dimensional origins. For a successful
such model of gauge-Higgs unification, we need not only to get a light
scalar but also to endow that scalar with dynamics pushing it to break
electroweak symmetry [ Antoniadis:1990ew , Antoniadis:1993jp ,
Hatanaka:1998yp , Hall:2001tn , Burdman:2002se , Csaki:2002ur ,
Scrucca:2003ra , Choi:2003kq , Hosotani:2004ka , Hosotani:2004wv ] .
There is far too much rich physics involved here to mention, but
discussions of the calculation of the one-loop effective potential in
these models can be found in [ Antoniadis:2001cv , Hasegawa:2004zz ,
Cacciapaglia:2005da ] . Gauge symmetry-breaking by a higher-dimensional
gauge field component getting a vev is sometimes referred to as the
‘Hosotani mechanism’ [ Hosotani:1983vn , Hosotani:1983xw ,
Hosotani:1988bm ] . As opposed to the Large Extra Dimensional model of
the previous section, in these models the extra dimensions are
‘universal’—all of the SM fields can propagate around the small
dimension, not just gravitons. This means that all of our familiar
fields are the zero modes of KK towers with spacing @xmath . These KK
partners have the same gauge charges, and so should be produced
copiously in interactions with @xmath , yet none have been observed at
the LHC (see e.g. [ Han:2010rf , Nishiwaki:2011gk , Belanger:2012mc ,
Belyaev:2012ai ] for some constraints). As a result, the lower bound on
the KK scale is now far above the Higgs mass, which makes all of these
sorts of models increasingly less attractive as solutions to the
hierarchy problem.

### 9 Compositeness

We have saved for last what is, in some sense, the most obvious strategy
to pursue. As discussed in Section 3 , the Standard Model already breaks
a symmetry and generates a mass scale in a natural manner with the
chiral condensate of QCD. Perhaps Nature has only this one trick, and
repeats the same mechanism to generate the weak scale. After all, we
mentioned above that the QCD condensate does in fact break the
electroweak symmetry, just not at the right scale.

#### 9.1 Technicolor

The idea is to introduce a new gauge sector which is asymptotically free
and so becomes strong and confines at the electroweak scale. If the
condensate has electroweak quantum numbers, then it breaks electroweak
symmetry just as a Higgs field would, but now without any Higgs. This
strategy is known as ‘technicolor’, and was proposed in its simplest
form by Weinberg [ Weinberg:1975gm ] and Susskind [ Susskind:1978ms ] .
A modern pedagogical introduction can be found in TASI lectures from
Chivukula [ Chivukula:2000mb ] or Contino [ Contino:2010rs ] , which has
heavily influenced this discussion, and a more detailed classic review
is from Hill & Simmons [ Hill:2002ap ] . We introduce a technicolor
sector which is an @xmath gauge group with @xmath technicolor
fundamentals which are electroweak doublets and their singlet partners,
together enjoying a global @xmath symmetry which is broken in the
infrared to @xmath by confinement in the @xmath sector at @xmath . This
structure exactly matches that of the SM QCD sector (save for the values
of @xmath ), so we expect all the same phenomenology, for example with
composite technipions appearing close to the electroweak scale. But in
this case since the technicolor chiral condensate is the leading
breaking of the gauged @xmath , the technipions will be predominantly
‘eaten’ and appear as the longitudinal polarizations of our @xmath
bosons. Technicolor can ‘Higgs’ the electroweak gauge symmetry just like
a fundamental Higgs field would, but without having any scale at which
it looks like there is a scalar field breaking electroweak symmetry.
However, the SM Higgs not only breaks electroweak symmetry but also
gives mass to the quarks, and thus far we haven’t introduced any
coupling between the quarks and the technicolor sector. To get the
appropriate couplings we can embed both of these gauge groups in a
larger ‘extended technicolor’ group, @xmath . After this extended group
undergoes spontaneous symmetry breaking at @xmath , the broken gauge
bosons generate the appropriate four-Fermi interactions

  -- -------- -- -------
     @xmath      (191)
  -- -------- -- -------

and then when the technicolor group confines at a scale @xmath , we see
the emergence of quark masses

  -- -------- -- -------
     @xmath      (192)
  -- -------- -- -------

But this clearly suffices solely to generate a single quark mass scale,
since the Yukawa coupling is originating from a single gauge coupling,
so it was quickly realized that accounting for flavor physics required
far more structure and significantly larger gauge groups [
Eichten:1979ah , Dimopoulos:1979es , Dimopoulos:1980fj , Dine:1981za ] .
To generate the variety of quark mass scales with this mechanism would
require a cascade of breakings from @xmath down to @xmath . Furthermore
the same massive gauge boson exchanges which generate those needed
four-Fermi operators also generate four-quark interactions @xmath which
can lead to large flavor violation. While there were insights on how
various aspects of this could be tackled, the death-knell for
technicolor came with Peskin & Takeuchi’s parametrization of oblique
corrections to the two-point functions of the electroweak vector bosons
from BSM physics [ Peskin:1990zt ] . These efficiently parametrize
deviations from the tree-level form of the vector boson propagators, and
can be simply connected with experiment. Ensuing estimates for the sizes
of these parameters in strongly-interacting models were very far from
empirical measurements [ Peskin:1991sw ] . The program of technicolor
lives on with ‘walking technicolor’, the idea that the confining
dynamics may be due to a strongly-coupled gauge theory which behaves
very differently from QCD [ Holdom:1981rm , Holdom:1984sk ,
Yamawaki:1985zg , Akiba:1985rr , Appelquist:1986an , Appelquist:1986tr ,
Appelquist:1987fc ] . This is too large a digression for us to
introduce, but we mention that there is interesting recent work relating
the existence of ‘walking’ dynamics to proximity of the theory to a
fixed point at complex value of the coupling [ Gorbenko:2018dtm ,
Gorbenko:2018ncu , Benini:2019dfy , Faedo:2019nxw , Antipin:2020rdw ] .

#### 9.2 A Composite Goldstone Higgs

However, there is another way that compositeness can be useful for us in
securing a light electroweak-symmetry-breaking scalar: It will allow us
to realize the dream of a pseudo-Nambu-Goldstone Higgs. Recall that
whenever a continuous global symmetry is spontaneously broken, there
appear scalar Goldstone bosons @xmath which parametrize excitations
about the vacuum state in the direction of the broken generators. Since
the theory had a global symmetry, the potential along these directions
is flat, and thus the Goldstones are massless. More strongly, they
contain a shift symmetry:

  -- -------- -- -------
     @xmath      (193)
  -- -------- -- -------

where @xmath is independent of @xmath . This means they may only be
‘derivatively coupled’; they may appear in the Lagrangian solely as
@xmath . Thus, a non-zero mass for such a scalar is technically
natural—if such an operator is present, @xmath is said to be a pseudo
-Goldstone and corresponds to the breaking of an approximate symmetry.
This is a familiar story in the context of the QCD condensate breaking
the approximate chiral symmetry of the quarks, leading to light but not
massless pions. In the case of technicolor, the phenomenon of
confinement of a strongly interacting sector is directly responsible for
breaking the gauged electroweak symmetry, leading to no separation
between the two scales @xmath and @xmath . This means the technipions
are immediately eaten, and there is no scale at which it looks like a
scalar field is responsible for symmetry-breaking, which leads to large
electroweak precision constraints. In a general composite Higgs
scenario, we’ll attempt to arrange for separation between the scale of
confinement and the scale of electroweak symmetry breaking by having
confinement break an enhanced global symmetry, leading to pseudo-Nambu
Goldstone bosons whose masses are protected. We then want to radiatively
generate a potential for these pNGBs, leading to them getting a vev and
breaking electroweak symmetry at a lower scale. A degree of separation
between confinement at the scale @xmath and EWSB at @xmath will reduce
the difficulties with electroweak precision constraints, as this
scenario returns to the SM elementary Higgs sector in the limit @xmath .
More pressingly, now that we have gained experimental access to energies
close to @xmath it’s even more clear that scale separation is
needed—confining dynamics lead generically to lots of resonances at the
scale @xmath , which would be seen in all sorts of ways. The general
setup is a group @xmath of (approximate) global symmetries, of which a
subgroup @xmath is gauged. We will have strong dynamics at the scale
@xmath break the global symmetry to @xmath , and in full generality
allow for some part of the gauge symmetries to be broken, such that the
unbroken gauge symmetry is @xmath . This leads to @xmath Goldstone
bosons (where @xmath is the dimension of a group) of which @xmath are
eaten by the broken gauge bosons. The uneaten, light pNGBs transform
non-trivially under the remaining gauge symmetry @xmath , so we can hope
to arrange for them to break this symmetry. In a realistic minimal
model, we can have the strong dynamics not break any gauge symmetry,
@xmath . Such a minimal model may be constructed with @xmath [
Agashe:2004rs ] . We diagram the general structure in Figure 20(a) and
the minimal model in 20(b) .

To evince the ideas in the simplest scenario possible, we’ll discuss an
even simpler model for a composite pNGB which then breaks a @xmath gauge
symmetry. This is just a toy model to understand the features, which has
already been kindly worked out in the extensive review from Panico &
Wulzer [ Panico:2015jxa ] , and which we’ll call a ‘composite Abelian
Higgs’ model. To find the most minimal model we’ll ask for a composite
pNGB which breaks the smallest continuous gauge symmetry, @xmath , for
which we simply need two uneaten degrees of freedom (since a charged
scalar is necessarily complex). We can make the even more minimal choice
@xmath , as we’re not worried about having additional unbroken global
symmetries. Since @xmath , we then just need to choose a group @xmath
with at least @xmath to get the right number of pNGBs. We’ll study the
breaking @xmath , which is especially nice because we have geometric
intuition for the Lie algebras of these groups. We’ll study this using a
‘linear sigma model’, of which the ‘chiral Lagrangian’ describing the
QCD pions is the most familiar example. Much of the general technology
was developed by Callan, Coleman, Wess & Zumino [ Coleman:1969sm ,
Callan:1969sn ] , and some modern introductions to this technology can
be found in Schwartz’ textbook [ Schwartz:2013pla ] , in a pedagogical
review of Little Higgs models by Schmaltz & Tucker-Smith [
Schmaltz:2005ky ] , and in exhaustive detail in the review by Scherer [
Scherer:2002tk ] . The big idea is one of bottom-up effective field
theory: Given knowledge of the symmetry-breaking structure, we can
cleverly parametrize our fields to easily see the structure of the
Lagrangian which is demanded both before and after symmetry-breaking. In
our case we must start with an @xmath -invariant Lagrangian of a
fundamental field @xmath , which is a familiar @xmath vector.

  -- -------- -- -------
     @xmath      (194)
  -- -------- -- -------

where @xmath rotations act as @xmath with @xmath , where @xmath , @xmath
are the generators of @xmath . The potential of @xmath is minimized for
@xmath , so @xmath gets a nonzero vev and the symmetry is broken down to
rotations keeping @xmath fixed, which is simply @xmath . There are a
two-sphere worth of vacua corresponding to possible angles for @xmath ,
which parametrize the Goldstone directions. We can make the split
between broken and unbroken generators explicit by parametrizing our
field as

  -- -------- -- -------
     @xmath      (195)
  -- -------- -- -------

where in the first equality @xmath are the two broken generators, @xmath
are the massless Goldstones corresponding to fluctuations along the
vacuum manifold, and @xmath is the massive ‘radial mode’ giving
fluctuations about the vev. We eschew writing down the generators
explicitly and assert that in this case one finds the compact latter
expression, with @xmath . We can find an explicit expression for the
interaction of the Goldstones and the radial modes by simply plugging
this parametrization into the Lagrangian above. We indeed find a mass
for @xmath of @xmath , massless pions @xmath and a tower of all possible
interactions between these fields which are consistent with the
symmetries. We now have a theory of massless scalars transforming under
an unbroken symmetry; our pions form a doublet of @xmath transforming as
@xmath , corresponding to rotations about the unbroken @xmath generator.
We can complexify by introducing @xmath to view @xmath as acting @xmath
. As it stands, @xmath is an exact Goldstone boson, so cannot pick up a
potential to then itself get a vev and break @xmath . However, when we
gauge a @xmath subgroup of our original @xmath global symmetry we’re
introducing explicit breaking of the symmetry and resultingly @xmath
becomes a pNGB and can pick up a mass. The tree-level effect of this
gauging is simply to upgrade derivatives to gauge covariant derivatives,
@xmath . Now as a result of @xmath rotations no longer being an exact
symmetry, @xmath is free to pick up a potential, which in general will
be radiatively generated as a result of this gauging. It is easy to draw
diagrams in which loops of our @xmath gauge bosons generate a nonzero
mass and quartic for @xmath , as in Figure 22 , and in a realistic model
the corrections from loops of top quarks will be especially important.
These loop diagrams come along with an obvious cutoff: Above the
compositeness scale @xmath , the fields @xmath no longer exist in the
spectrum. There is a beautiful general method for deriving the
radiatively-generated potential for @xmath by resumming the one-loop
diagrams with various numbers of external @xmath legs, known as the
Coleman-Weinberg potential [ Coleman:1973jx ] .

Unfortunately, absent any of other structure, finding @xmath still
requires some degree of cancellation between various contributions to
the potential of @xmath . However any amount of @xmath will help
alleviate the pressure from electroweak precision observables which thus
far look empirically as expected for an elementary Higgs field, while
still retaining the benefit of forbidding corrections to the Higgs mass
above @xmath . But one still can’t get away from requiring lots of
structure near the electroweak scale, which has not (yet) been observed.
There are a variety of important aspects and interesting directions we
do not have the space to discuss. Even past understanding the best sorts
of group structures to which to apply this strategy, it is clearly
important to understand the sorts of field theories which can confine to
break @xmath , as well as the detailed structure of the potential
radiatively-generated by SM fields. Then it’s important to explore the
possibility of a natural structure which dictates @xmath —while there
has been much work on this, we mention in particular the interesting
strategy of ‘collective’ symmetry breaking in which a symmetry is broken
only by an interplay between different couplings. This class of models
is known as the ‘Little Higgs’ [ Schmaltz:2005ky , ArkaniHamed:2002qx ,
ArkaniHamed:2002qy , Kaplan:2003uc , Schmaltz:2002wx , Perelstein:2005ka
] , and can be seen as a purely four-dimensional application of the
strategy of nonlocal symmetry breaking through ‘nonlocality in theory
space’ [ ArkaniHamed:2001ca , ArkaniHamed:2001nc ] , which is a
fascinating topic. The TASI notes by Csaki, Lombardo, Telem [
Csaki:2018muy ] provide a pedagogical introduction to these topics.
Finally, let me mention that composite Higgs models may be understood as
being dual to a novel class of extra-dimensional models known as
Randall-Sundrum models [ Randall:1999ee , Randall:1999vf ] . Unlike in
the simple cases we discussed in Section 8 , in these models the
spacetime does not have a product structure (as did @xmath ) and the
geometry is said to be ‘non-factorizable’. In this scenario our
four-dimensional universe is seen as a brane living on one end of a
five-dimensional orbifold of anti-de Sitter space. The minuteness of the
electroweak scale compared to the Planck scale is a result of a large
fifth-dimensional AdS ‘warp factor’ between the brane we live on (the
‘IR brane’) and the brane on the other end of the space (the ‘UV
brane’). As in Section 8.2 , this trades the electroweak hierarchy into
a geometric hierarchy, but now in AdS we can find novel, natural ways of
generating such a hierarchy of scales [ Goldberger:1999uk ,
Goldberger:1999un ] . Furthermore, embedding our universe into an AdS
spacetime means we can take advantage of the enormously powerful program
of AdS/CFT holography, which enables us to study the strong-coupling
phenomena of composite Higgs models via their weakly-coupled
gravitational duals. Pedagogical introductions to holography can be
found in lecture notes from Sundrum [ Sundrum:2011ic ] and Kaplan [
KaplanJ ] , and with more background in the textbook by Ammon &
Erdmenger [ Ammon:2015wua ] . That machinery is not all necessary to
appreciate the workings of Randall-Sundrum models, though, and there are
a wide variety of great lectures notes aimed at particle theorists, for
example those of Sundrum [ Sundrum:2005jf ] , Csaki & Tanedo [
Csaki:2016kln ] , and Gherghetta [ Gherghetta:2010cj ] .

## Chapter \thechapter The Loerarchy Problem

  The great tragedy of science — the slaying of a beautiful hypothesis
  by an ugly fact.

  Thomas Henry Huxley
  Biogenesis and Abiogenesis (1870) [ huxley_2011 ]

### 10 The ‘Little Hierarchy Problem’

We’ve seen in Chapter The Hierarchy Problem: From the Fundamentals to
the Frontiers a cadre of theories which can produce a light scalar
naturally, and there’s one feature all the classic approaches have in
common: they predict new states with Standard Model charges close to the
mass of the Higgs. This is seemingly inevitable simply from the
structure of effective field theory—whatever extra structure protects
the Higgs mass at UV scales must be broken close to the electroweak
scale to allow the Standard Model, which does not have that extra
structure. This feature means that smashing together protons at scales
much greater than the electroweak scale would surely reveal the physics
of whatever mechanism solves the hierarchy problem. And so the Large
Hadron Collider was eagerly awaited to tell us which of these ideas was
correct. Yet even years before the LHC turned on, those who could
clearly read the tea leaves were realizing that something was amiss with
our naturalness expectations (see e.g. the ‘LEP paradox’ [
Barbieri:2000gf ] , also [ Cheng:2003ju ] ), and exploring the idea that
supersymmetry would not show up to solve the hierarchy problem (e.g.
‘split supersymmetry’ [ Wells:2003tf , ArkaniHamed:2004fb ,
Giudice:2004tc ] ). Perhaps there was something else present which made
weak-scale supersymmetry unnecessary for protecting the Higgs. Of course
the LHC has been a fantastic success. It has confirmed for us the
existence of a light Higgs resonance that looks SM-like, and made many
great measurements of the SM besides. But rather than revealing to us
which TeV-scale new physics kept the Higgs light, we’ve instead had a
march of increasingly powerful constraints on new particles which couple
to the Standard Model. These null results for physics beyond the
Standard Model from run 1 of the LHC rapidly popularized the idea that
something else might be responsible for stabilizing the Higgs mass at
the electroweak scale up to a higher scale where, say, supersymmetry
came in. This line of thinking is termed the ‘Little Hierarchy
Problem’—the idea being that one of those classic solutions would appear
at @xmath to solve the ‘Big Hierarchy Problem’ and explain why the Higgs
mass was not at the Planck scale, leaving a smaller hierarchy of a
couple orders of magnitude between @xmath and @xmath unexplained.
Perhaps rather than minimal supersymmetry there was another module which
provided this last bit of protection. But this has to be a special
module to protect the Higgs mass without introducing new colored
particles.

#### 10.1 The Twin Higgs

The first such proposal in fact appeared before the LHC had even turned
on. The mirror twin Higgs (MTH) [ Chacko:2005pe ] introduces a second
copy of the SM gauge group and states related to ours by a @xmath
symmetry. Since these ‘twin’ states are neutral under the SM gauge
group, they are subject only to indirect bounds from precision Higgs
coupling measurements. The two sectors are connected solely by Higgs
portal-type interactions between the two @xmath doublet scalars. ²¹ ²¹
21 We return in Section 15 to the prospect of kinetic mixing between the
two @xmath factors, which is also allowed by the symmetries. Subject to
conditions on the quartic coupling, the Higgs sector enjoys an
approximate @xmath global symmetry, and the breaking of this symmetry
leads naturally to a pseudo-Nambu Goldstone boson. Seemingly magically,
this structure is accidentally respected by the quadratically-divergent
one-loop corrections to the Higgs potential, and the pNGB continues to
be protected through one-loop from large corrections to its mass. The
Twin Higgs thus allows the postponement of a solution to the ‘Big
Hierarchy Problem’ until scales a loop factor @xmath above the Higgs
mass. While the space of Neutral Naturalness models has now been
explored more thoroughly and we will discuss some generalities below,
the mirror twin Higgs remains perhaps the most aesthetically pleasing of
all these approaches and serves as a useful avatar for this general
strategy. As a result, in Chapter The Hierarchy Problem: From the
Fundamentals to the Frontiers we consider cosmological signatures of the
MTH specifically, so we give here a more-detailed introduction to the
twin Higgs in particular. This is necessarily slightly more technical
than the rest of this chapter, so the reader who is not planning on
reading Chapters The Hierarchy Problem: From the Fundamentals to the
Frontiers or The Hierarchy Problem: From the Fundamentals to the
Frontiers in detail may skip ahead @xmath pages to Section 10.2 without
loss of continuity. The scalar potential in this model is best organized
in terms of the accidental @xmath symmetry involving the @xmath Higgs
doublets of the SM and twin sectors, @xmath and @xmath . The general
tree-level twin Higgs potential is given by (see e.g. [ Craig:2015pha ]
)

  -- -------- -- -------
     @xmath      (196)
  -- -------- -- -------

The first term respects the accidental @xmath global symmetry, as can be
seen by writing it in terms of @xmath , which transforms as a complex
@xmath fundamental. The second term breaks @xmath but preserves the
@xmath and the final term softly breaks the @xmath . In order for the
@xmath to be a good symmetry of the potential, we require @xmath .
However, the gauging of an @xmath subgroup constitutes explicit breaking
of the @xmath , so we should worry about whether quantum corrections
reintroduce large masses for the would-be Goldstones when @xmath is
broken. But writing down the one-loop corrections reveals a fortuitous
accidental symmetry. The one-loop effective scalar potential gets the
following leading corrections from the gauge bosons at the quadratic
level:

  -- -------- -- -------
     @xmath      (197)
  -- -------- -- -------

where we see explicitly that if the @xmath symmetry is good at the level
of the gauge couplings, then these largest one-loop corrections continue
to respect the @xmath . It is easy to see from here by power counting
that this holds for all the quadratically-divergent pieces so long as
the @xmath is a good symmetry for the interactions involved. There is
radiative @xmath -breaking at the level of the quartic, since the @xmath
symmetry no longer suffices to form the Higgses into an @xmath
invariant. The coupling @xmath should naturally be of the order of these
corrections, the largest of which comes from the Yukawa interactions
with the top/twin top, @xmath for a cut-off @xmath TeV ( @xmath being
the top quark Yukawa coupling and @xmath its mass). Requiring @xmath
therefore implies @xmath . As the SM and twin isospin gauge groups are
disjoint subgroups of the @xmath , the spontaneous breaking of the
@xmath coincides with the SM and twin electroweak symmetry breaking.
This gives seven Goldstone bosons, six of which are ‘eaten’ by the
@xmath gauge bosons of the two sectors, which leaves one Goldstone
remaining. This will acquire mass through the breaking of the @xmath
that is naturally smaller than the twin scale @xmath . For future
reference, it is convenient to define the real scalar degrees of freedom
in the gauge basis as @xmath and @xmath , where @xmath and @xmath . The
surviving Goldstone boson should be dominantly composed of the @xmath
gauge eigenstate in order to be SM-like. The soft @xmath -breaking
coupling @xmath is required to tune the potential so that the vacuum
expectation values (vevs) are asymmetric and that the Goldstone is
mostly aligned with the @xmath field direction. The (unique) minimum of
the Twin Higgs potential ( 196 ) occurs at @xmath and @xmath . The
required alignment of the vacuum in the @xmath direction occurs if
@xmath , which has been assumed in these expressions for the minimum.
The consequences of this are that @xmath and @xmath (where @xmath is the
vev of the SM Higgs, although @xmath GeV is the vev that determines the
SM particle masses and electroweak properties), so that the SM-like
Higgs @xmath is identified with the Goldstone mode and is naturally
lighter than the other remaining real scalar, a radial mode @xmath whose
mass is set by the scale @xmath . The component of @xmath in the @xmath
gauge eigenstate is @xmath (to lowest order in @xmath ). Measurements of
the Higgs couplings restrict @xmath [ Burdman:2014zta , Craig:2015pha ]
, and the Giudice-Barbieri tuning of the weak scale associated with this
asymmetry is of order @xmath . The spectrum of states in the broken
phase consists of a SM-like pseudo-Goldstone Higgs @xmath of mass @xmath
, a radial twin Higgs mode @xmath of mass @xmath , a conventional
Standard Model sector of gauge bosons and fermions and a corresponding
mirror sector. The masses of quarks, gauge bosons, and charged leptons
in the twin sector are larger than their Standard Model counterparts by
@xmath , while the twin QCD scale is larger by a factor @xmath due to
the impact of the higher mass scale of heavy twin quarks on the
renormalisation group (RG) evolution of the twin strong coupling. The
relative mass of twin neutrinos depends on the origin of neutrino
masses, some possibilities being @xmath for Dirac masses and @xmath for
Majorana masses from the Weinberg operator. Mixing in the scalar sector
implies that the SM-like Higgs couples to twin sector matter with an
@xmath mixing angle, as does the radial twin Higgs mode to Standard
Model matter. These mixings provide the primary portal between the
Standard Model and twin sectors. The Goldstone Higgs is protected from
radiative corrections from @xmath -symmetric physics above the scale
@xmath . While the mirror Twin Higgs addresses the little hierarchy
problem, it does not address the big hierarchy problem, as nothing
stabilizes the scale @xmath against radiative corrections. However, the
scale @xmath can be stabilized by supersymmetry, compositeness, or
perhaps additional copies of the twin mechanism [ Asadi:2018abu ]
without requiring new states beneath the TeV scale. Minimal
supersymmetric UV completions can furthermore remain perturbative up to
the GUT scale [ Chang:2006ra , Craig:2013fga ] . As mentioned, the
collider constraints on twin Higgs models are very mild and pertain
mostly to a lower bound on the soft breaking of the @xmath . In this
respect, the Twin Higgs naturally reconciles the observation of a light
Higgs with the absence of evidence for new physics thus far at the LHC.
The primary challenge to these models comes from cosmology due to the
effects of additional light particles on the cosmic microwave
background. We will discuss these issues in Chapter The Hierarchy
Problem: From the Fundamentals to the Frontiers and propose a natural
resolution.

#### 10.2 Neutral Naturalness or The Return Of The Orbifold

More broadly, the twin Higgs is just the simplest example of the more
general ‘Neutral Naturalness’ paradigm in which the states responsible
for stabilizing the electroweak scale are not charged under (some of)
the Standard Model (SM) gauge symmetries [ Chacko:2005pe , Chacko:2005un
, Burdman:2006tz , Poland:2008ev , Cai:2008au , Craig:2014aea ,
Batell:2015aha , Curtin:2015bka , Cheng:2015buv , Craig:2016kue ,
Cohen:2018mgv , Cheng:2018gvu , Serra:2019omd ] , thus explaining the
lack of expected signposts of naturalness. In the symmetry-based
solutions to the hierarchy problem discussed in Chapter The Hierarchy
Problem: From the Fundamentals to the Frontiers , modifications of the
Higgs mass were technically natural as a result of a continuous symmetry
which commuted with the SM gauge groups. This naïvely seems necessary to
ensure that the necessary degrees of freedom are present and couple to
the Higgs with the right strength to cancel divergences. How is the top
quark contribution @xmath (where @xmath is the number of colors) to be
canceled if not by another colored particle with the same coupling to
the Higgs, which gives the opposite contribution? Well we saw above that
the twin Higgs nevertheless works with a quark charged under a separate
gauge group. Indeed, at one-loop @xmath is really just a ‘counting
factor’ and we are free to get those three opposite-sign contributions
in a variety of ways. To some extent the space of Neutral Naturalness
models is an exercise in interesting ways to find that color factor.
We’ll be able to see that picture more clearly in the language of
orbifold projection, which will also give us good reason to expect that
these models with naïvely strange symmetries in fact do have nice UV
completions. In Section 8.3 we saw how orbifolds could be useful
dynamically—that is, in affording a higher-dimensional, symmetric theory
which at low energies looks like a less symmetric, four-dimensional
theory due to boundary conditions imposed by the orbifold discrete
symmetries. But these theories had interesting properties in their
low-energy behavior below the scale of compactification; we didn’t need
to make reference to their origins in studying them. Now we want to
understand the variety of low energy theories we can get very generally,
but only at the level of the zero-mode spectrum. Rather than decomposing
our fields into modes and integrating over the compact manifold and
noticing that only those fields invariant under the orbifold symmetry
are left with zero-modes, we’re going to skip to the answer and look at
the spectrum of fields left invariant under our discrete symmetry. We’ll
find that these theories have enhanced symmetry properties at one loop.
The underlying reason lies in the ‘orbifold correspondence’ in large
@xmath gauge theories [ Schmaltz:1998bg , Bershadsky:1998cb ,
Kachru:1998ys ] . Given a ‘mother’ field theory, you can create a
‘daughter’ theory by embedding some given discrete symmetry in the
symmetries of the mother theory and projecting out states which are not
invariant under that discrete symmetry; we call this process
‘orbifolding’. Then at leading order in large @xmath , the correlation
functions of the daughter theory match those of the mother theory. This
is nothing short of amazing—a theory with no supersymmetry to speak of
can nevertheless ‘accidentally’ exhibit supersymmetric behavior at
leading order. The general case of the orbifold correspondence and how
to construct Neutral Naturalness models is beautiful and I do recommend
reading [ Schmaltz:1998bg , Burdman:2006tz , Craig:2014roa ] , but the
group theory required for a full discussion would be too large a detour
from our main narrative. Fortunately we can get a good sense for what’s
going on by considering a few explicit examples, which will not require
much mathematical machinery.

##### Example 1: Folded Supersymmetry

Let’s first consider the example of ‘Folded Supersymmetry’ [
Burdman:2006tz ] which was the first Neutral Naturalness model
constructed explicitly via orbifolding. The idea is precisely to
consider a supersymmetric theory and orbifold project onto a theory with
no explicit supersymmetry but in which supersymmetric cancellations
still occur at one loop. As a pedagogical example, consider an @xmath
supersymmetric @xmath gauge theory with @xmath flavors of left and right
fundamental chiral superfields @xmath which enjoy a @xmath global flavor
symmetry. Let’s decree also that the theory respects @xmath -symmetry.
We will orbifold by the discrete group @xmath as before, but we must
choose how to embed the @xmath in each of these symmetry groups. That
is, our original ‘mother’ theory is invariant under a large symmetry
group @xmath , which contains many @xmath subgroups, and we must decide
precisely which @xmath we want to orbifold by. We choose the following
embeddings:

  -- -------- -- -------
     @xmath      (198)
  -- -------- -- -------

where the first matrix is in color-space, the second is in flavor-space,
and the third transformation is by fermion number. Each of these
generates a @xmath subgroup of one of the symmetry factors of the mother
theory. To see which fields are invariant under this @xmath , we must
only act them on the fields and see how they transform. Gauge bosons
@xmath and their superpartner gauginos @xmath are in the adjoint of the
gauge group. Indexing @xmath separately over the two halves of the gauge
indices, we have

  -- -------- -------- -- -------
     @xmath               (199)
     @xmath   @xmath      (200)
  -- -------- -------- -- -------

where the difference here is because of the different @xmath
transformations. We see that if we project down to only those states
invariant under this transformation, our gauge group dissolves from
@xmath down to disconnected pieces @xmath . We see that embedding the
@xmath non-trivially in the @xmath -symmetry group means the daughter
theory will be non-supersymmetric. The superpartners of our gauge fields
are no longer present, but rather the gauginos have been twisted into
bifundamentals under the two gauge factors. In the matter sector,
letting @xmath similarly index the two halves of the flavor indices and
writing the fields as matrices in a combined color and flavor space, we
have

  -- -------- -------- -- -------
     @xmath   @xmath      (201)
     @xmath               (202)
  -- -------- -------- -- -------

Again we see that we have broken supersymmetry. The flavor group has
also broken down to @xmath and the invariant states are squarks which
are bifundamentals under ‘diagonal’ combinations of the gauge and flavor
groups, and quarks which are bifundamentals under the ‘off-diagonal’
combinations.

It is not too hard to roughly see the magic of how the
orbifold-projected theory continues to protect scalar masses. Draw the
one-loop the diagrams in the mother theory which would contribute to a
calculation of the mass of, say, @xmath , as in Figure 24 . In the
mother theory we know there are no quadratic corrections by
supersymmetry. In the daughter theory, half of each sort of diagram will
be eliminated by the orbifold projection, so it’s clear that there will
still be no quadratic divergences. But because it is different internal
states that have been eliminated for different classes of diagrams, the
daughter theory has no supersymmetry to speak of! Again, as we saw in
the example of the twin Higgs, the magic is in that at one-loop we
really only need to get the counting right, and so we can use
orbifolding to construct theories which do that in clever ways. The
structure of the theory can be succinctly summarized in a ‘quiver’ or
‘moose’ diagram where the various symmetry groups correspond to nodes in
a graph and the matter is represented by links between these groups
which represent their charges, as in Figure 23 . Despite the fact that
our daughter theory has no supersymmetry, the orbifold correspondence
guarantees that in the @xmath limit the correlation functions have full
supersymmetric protection. For finite @xmath the supersymmetric
relations are broken, but only by @xmath corrections. Going through this
explicitly is useful, and pedagogical discussions of it can be found in
[ Schmaltz:1998bg , Craig:2014aea ] .

##### Example 2: The @xmath-plet Higgs

We can understand the twin Higgs as the simplest example of a @xmath
-siblings Higgs which is the orbifold projection @xmath , where the
first two factors are gauged and the last is a flavor symmetry. We focus
on the sector which contains the Yukawa interaction of the top quark, as
the top partners have the most effect on the naturalness of the Higgs.
In the mother theory the matter content is:

  -------- -------- -------- --------
           @xmath   @xmath   @xmath
  @xmath   -        @xmath   @xmath
  @xmath   @xmath   @xmath   -
  @xmath   @xmath   -        @xmath
  -------- -------- -------- --------

where ‘ @xmath ’ denotes the fundamental representation and ‘ @xmath ’
denotes the antifundamental, and we note that the charges allow the
operator @xmath . The Abelian generalization of the twin Higgs is found
by embedding the @xmath in these groups as the block-diagonal element
@xmath , where @xmath corresponds to which @xmath factor the element
belongs to and @xmath is the @xmath identity matrix. The pattern of
orbifolding is extremely simple in this example. We write the Higgs
field as a matrix of @xmath columns and @xmath rows in blocks of two,
with the field in block @xmath being @xmath where @xmath simply labels
the column and @xmath is an @xmath index. Now we can look at how this
field transforms under the chosen @xmath ,

  -- -------- -------- -- -------
     @xmath   @xmath      (203)
              @xmath      (204)
  -- -------- -------- -- -------

and we see immediately that the only invariant elements are those on the
diagonal. It is simple to repeat this for the @xmath fields to find the
same feature. Then in this example there are no off-diagonal fields at
all, and the daughter theory consists of @xmath SM-like sectors. These
sectors are all identical and so have a @xmath rearrangement symmetry,
leading to a one-loop quadratic potential of the form

  -- -- -- -------
           (205)
  -- -- -- -------

That is, just as in the twin Higgs, the @xmath symmetry of the scalar
potential is respected by the one-loop quadratic corrections as a result
of the discrete symmetry despite being explicitly broken by the gauge
groups. This clearly opens up a much wider space of Neutral Naturalness
models where the Higgs is a pseudo-Nambu Goldstone boson and so receives
some protection of its mass without new colored particles at the
electroweak scale. The general orbifolding approach to constructing
models of Neutral Naturalness was fully laid out in [ Craig:2014roa ,
Craig:2014aea ] , where they in particular explore a ‘regular
representation’ embedding of the discrete group into the continuous
symmetries of the mother theory. This approach of orbifolding to find
low-energy models with accidental symmetries is useful also because such
models come along with guides for how to UV-complete them. When we wrote
down the twin Higgs model above, it was perhaps not obvious that there
is a nice UV completion of this theory. But now we see that the twin
Higgs is an orbifold projection of a @xmath gauge theory by @xmath , so
we expect we can uplift this to a five-dimensional UV completion where
the twin Higgs emerges dynamically at low energies from orbifold
boundary conditions. So we can confidently study solely the low-energy
effective theory of the zero-modes we’ve projected out without worrying
explicitly about whether a UV completion exists.

### 11 The Loerarchy Problem

Modern particle theorists must now confront a new version of the issue
of electroweak naturalness. When originally understood, the pressing
problem was understanding what sorts of UV structure could protect a
scalar from large mass corrections. Of course this structure needed to
be broken to get the SM structure in the IR and there’s lots of
interesting physics and subtleties on that end as well. But with the
structure of the weak scale barely explored, the ways in which this
could be done were abundant. Over the ensuing decades we explored
electroweak physics with increasing precision, which has provided
invaluable guidance for how the SM structure must appear. Gradually the
IR dynamics became more and more constrained to the point where now, as
we have emphasized, we have enormous constraints on any appearance of
new physics with SM quantum numbers up to mass scales that are often
many times the electroweak scale. We thus have a qualitative change in
the electroweak naturalness issue over the past decades. We term the
modern, empirical, low-energy puzzle of electroweak naturalness without
visible structure around the weak scale as ‘The Loerarchy Problem’, for
obvious reasons. ²² ²² 22 For readers who do not share my sense of
humor, the reasoning is an implied fake etymology for the word
‘hierarchy’ as ‘high + erarchy’, and a retcon of the term ‘The Hierarchy
Problem’ as emphasizing the ‘high’ energy, UV aspects of the issue, by
which we are comparatively emphasizing the ‘low’ energy, IR aspects,
suggesting that a natural parallel term would similarly combine ‘low +
erarchy’ to form ‘loerarchy’, which is not a dictionary word. In this
language, the little hierarchy problem is just one approach toward this
problem, which assumes that one of the classic solutions is just out of
reach and another module is needed to postpone the appearance of SM
charged particles. While it’s more than worthwhile to continue looking
for and exploring those theories, in the face of increasingly powerful
LHC data in excellent agreement with the Standard Model it’s worth
thinking transversely. As intriguing as the Neutral Naturalness models
are, these classes of models rely on one-loop accidents and so do not
completely relieve the empirical pressure from a lack of new physics at
the energy frontier. Such models are gradually also being constrained by
the LHC, so what else are we to do? The maximalist interpretation of the
LHC data is that Nature may be leading us to the conclusion that there
is no new physics at the weak scale . With every inverse femtobarn of
LHC data without a signal of new physics, the impetus for such a
paradigm shift becomes stronger. But how can we generate a scale without
additional structure appearing surrounding it? Within the context of
effective field theory, decoupling theorems demand that if the RG
evolution is to change around a scale @xmath it must be due to fields
close to the scale @xmath . This was perhaps clearest in dim reg with
@xmath , where the beta functions explicitly change solely at mass
thresholds, though the Wilsonian approach is more useful for physical
intuition. So how are we to break this feature? This undertaking is the
maximalist approach to the Loerarchy Problem.

### 12 Violations of Effective Field Theory

  There are more things in heaven and earth, Horatio,
  Than are dreamt of in your philosophy.

  William Shakespeare
  Hamlet , c. 1600 [ shakespeare_hamlet ]

The line of thought we suggest here is that perhaps the apparent
violation of EFT expectations at the weak scale is a sign of the
breakdown of EFT itself. Depending on how much background in particle
physics one has this statement may seem more or less heretical, but the
idea is not as radical as it may at first seem—for one reason, the
cosmological constant problem has inspired sporadic reexaminations of
the validity of effective field theory in our universe for decades. The
cosmological constant problem is the fine-tuning issue with the other
dimensionful parameter in the Standard Model. Just as with the Higgs
mass, there is no protective symmetry in the Standard Model for the
vacuum energy, and so the natural expectation is @xmath , some @xmath
orders of magnitude higher than observations suggest. However there’s an
important difference in the severity of these problems—for the Higgs, as
emphasized in Section 5.1 , the worrisome mass corrections are those
from new physics. This is why the severity of the problem has only
ratcheted up in recent years, as we have seen nothing to protect the
Higgs from BSM mass corrections. But for the vacuum energy, there are
finite, calculable, physical contributions in the Standard Model itself!
For a start, EWSB by the Higgs yields a contribution @xmath , and chiral
symmetry-breaking yields a contribution @xmath . How is it that these
can be nearly perfectly canceled off in the late universe? There have
been important attempts to address the cosmological constant problem
with a violation of effective field theory, from Coleman’s suggestion [
Coleman:1988tj ] that nonlocality induced by wormholes may allow the
early universe to be sensitive to late-time requirements to
Cohen-Kaplan-Nelson’s suggestion [ Cohen:1998zx ] that the Bekenstein
bound demands an infrared cutoff on the validity of any EFT. From one
perspective, our suggestion to extend this philosophy to the hierarchy
problem appears natural in light of its apparent need in cosmology. We
can point to an even-more-general motivation with the realization that
gravity necessarily violates EFT.

#### 12.1 Gravity and EFT

The perturbative quantum field theory of the Einstein-Hilbert Lagrangian
[ Hilbert1915 ] is quite clearly an effective field theory of a
symmetric two-index tensor field which obeys diffeomorphism invariance
and with power counting in @xmath [ Donoghue:1994dn ] , as we can easily
see by writing it out and expanding around flat space @xmath :

  -- -------- -- -------
     @xmath      (206)
  -- -------- -- -------

where we have only given the schematic form of the operators in terms of
the number of derivatives and linearized gravitational fields they
contain, as the full expressions quickly become complicated [
Feynman:1963ax , DeWitt:1967ub ] . Then we may expect that this
effective field theory will be a good approximation to infrared
gravitational physics until we get to energies close to the Planck
scale, at which point the higher-dimensional operators are unsuppressed
and we need a UV completion. From this bottom-up approach it’s not clear
why gravity should be particularly special. But we can get some insight
at a very basic level by thinking about gravitational scattering. Let’s
compare two effective field theories: the four-Fermi theory of the weak
interaction below the weak scale @xmath and the perturbative theory of
quantum gravity below the Planck mass. If we consider scattering two
leptons at @xmath , we can make very precise predictions by computing in
the four-Fermi theory—the possible final states, the differential
cross-section; whatever we’d like. And similarly if we scatter two
particles at @xmath , we can compute to high precision in quantum
gravitational corrections what will happen. Now inversely, imagine
scattering two leptons at @xmath in the four-Fermi theory. At a scale
like @xmath , the EFT has obviously broken down and we can say
essentially nothing about what this process will look like—any
calculation we tried would be hopelessly divergent, and we have no idea
what sorts of states might exist at energies that large. However, what
if we scatter two particles at @xmath , say a ridiculously
trans-Planckian scale like @xmath ? In this case, in fact, we know what
will happen to incredible precision—a black hole will form! This will be
a ‘big’ black hole of mass @xmath with a lifetime of order days. It will
be well-described by classical general relativity for some macroscopic
time, and then semiclassical GR for an @xmath fraction of the full
lifetime. In fact, unless you are interested in incredibly detailed
measurements which involve collecting essentially every emitted Hawking
quanta (of which there will be @xmath in this case!) and finding their
entanglement structure, we know how to describe the evaporation nearly
completely. ²³ ²³ 23 I recommend Giddings’ Erice lectures [
Giddings:2011xs ] for more on the perspective of quantum gravitational
behavior as a function of the Mandelstam variables. So something
profoundly weird is going on. The key point being that in gravity, the
far UV of the theory is controlled by classical, infrared physics. This
is obviously a feature that we do not see in other EFTs. This is not a
new idea; it has long been known that gravity contains low-energy
effects which cannot be understood in the context of EFT. The fact that
black holes radiate at temperatures inversely proportional to their
masses [ Hawking:1974sw ] necessitates some sort of ‘UV/IR mixing’ in
gravity—infrared physics must somehow ‘know about’ heavy mass scales in
violation of a naïve application of decoupling. As a
perhaps-more-fundamental raison d’être for such behavior, the demand
that observables in a theory of quantum gravity must be gauge-(that is,
diffeomorphism-)invariant dictates that they must be nonlocal (see e.g.
[ Torre:1993fq , Giddings:2005id , Donnelly:2015hta , Donnelly:2016rvo ,
Giddings:2018umg ] ), again a feature which standard EFT techniques do
not encapsulate. In view of this, the conventional position is that EFT
should remain a valid strategy up to the Planck scale, at which quantum
gravitational effects become important. But once locality and decoupling
have been given up, how and why are violations of EFT expectations to be
sequestered to inaccessible energies? Indeed, the ‘firewall’ argument [
Almheiri:2012rt ] evinces tension with EFT expectations in semiclassical
gravity around black hole backgrounds at arbitrarily low energies and
curvatures, as does recent progress finding the Page curve from
semiclassical gravity [ Penington:2019npb , Almheiri:2019hni ,
Almheiri:2019psf , Almheiri:2019qdq , Penington:2019kki , Marolf:2020xie
] . That quantum gravitational effects will affect infrared particle
physics is likewise not a new idea. This has been the core message of
the Swampland program [ Vafa:2005ui ] , which has been cataloging—to
varying degrees of concreteness and certainty—ways in which otherwise
allowable EFTs may conjecturally be ruled out by quantum gravitational
considerations. These are EFTs which would look perfectly sensible and
consistent to an infrared effective field theorist, yet the demand that
they be UV-completed to theories which include Einstein gravity reveals
a secret inconsistency. While this is powerful information, the extent
to which the UV here meddles with the IR is relatively minor—just
dictating where one must live in the space of infrared theories. Even
so, they have been found to have possible applications to SM puzzles,
including the hierarchy problem [ Cheung:2014vva , Ooguri:2016pdq ,
Ibanez:2017kvh , Ibanez:2017oqr , Hamada:2017yji , Lust:2017wrl ,
Gonzalo:2018tpb , Gonzalo:2018dxi , Craig:2018yvw , Craig:2019fdy ,
March-Russell:2020lkq ] . Let us review briefly the approach to connect
the Weak Gravity Conjecture (WGC) [ ArkaniHamed:2006dz ] to the
hierarchy problem. The WGC is one of the earliest and most well-tested
Swampland conjectures, its formulation is relatively easy to understand,
and it’s emblematic of the way one might try to connect the hierarchy
problem to Swampland conjectures in general. The prime motivation for
formulating the WGC was the well-known folklore that quantum gravity
does not respect global symmetries. The simple argument for this fact is
that ‘black holes have no hair’—the only quantum numbers a black hole
has correspond to its mass, spin, and gauge charges. This means that if
we make a black hole by smashing together a bunch of neutrons, there is
no reason why it cannot decay into solely photons, which violates the
global @xmath symmetry of the SM but no gauge symmetries. The authors
suggested that the fact that Abelian gauge symmetries smoothly become
global symmetries as the gauge coupling vanishes means that something
must go wrong with very small gauge couplings as well, so as for the
physics to be smooth in this limit. Another line of thinking for quantum
gravity not respecting global symmetries, which connects more closely to
the WGC formulation, is based on entropic arguments. If global
symmetries could be exact, then we could create a big black hole with an
arbitrary global charge, and wait a very very long time while it Hawking
evaporates down to the Planck scale. Unlike gauge charges, the global
charge does not affect the metric, so the black hole does not shed this
charge as it evaporates. We then end up with a Planck-sized black hole
with an arbitrarily large global charge, which would necessitate the
existence of arbitrarily-many black hole microstates for a fixed-mass
black hole. But this is a disaster! In calculating a scattering
amplitude one has to sum over all possible intermediate states, in
principle including black holes. These effects are surely
Boltzmann-suppressed by enormous amounts, but if there are an infinite
number of possible black holes then any nonzero contribution from a
single black hole will lead to a divergence. A cuter (albeit somewhat
tongue-in-cheek) ‘hand waving argument’ is provided by [ Marolf:2003wu ]
: Were there incredibly large numbers of super-Planckian states which
could populate a thermal bath, a vigorous wave of your hand would
produce them in Unruh radiation, and you could feel them against your
hand as they evaporated. Now if we have a gauge symmetry, there are no
longer Planck-sized black holes with arbitrarily large charge because
there is an extremality bound: @xmath , with @xmath the gauge charge and
@xmath the mass, in units of the gauge coupling @xmath and the Planck
mass. If this were disobeyed a naked singularity would appear, which
would violate cosmic censorship [ Penrose:1969pc ] . But for a very tiny
gauge coupling, while we no longer have strictly infinitely-many black
holes at each mass, there are still an enormous number of black hole
microstates for a Planck-sized black hole as @xmath . So a worry like
the hand-waving argument still applies. This sort of reasoning motivated
Arkani-Hamed, Motl, Nicolis, and Vafa to conjecture that just as quantum
gravitational theories must not have exact global symmetries, they must
also suffer some physical malady which disallows the limit where Abelian
gauge symmetries become global symmetries. Their conjecture has two
forms: The ‘magnetic’ conjecture dictates that a quantum gravitational
theory with a @xmath gauge symmetry must be enlarged into a theory
allowing magnetic monopoles by a cutoff @xmath , which connects to such
a description never being valid in the limit @xmath . The ‘electric’
form of the WGC is that such a theory must contain a particle which is
‘super-extremal’—it has charge greater than its mass @xmath . The
existence of such a particle would destabilize the extremal, charged
black holes, allowing them to decay (though the extent to which this
really soothes our entropic worries is unclear). Well this
super-extremality bound should look very interesting to us, as it
provides an upper bound on a mass scale. While we cannot apply this
directly to the Higgs because it is not charged under any unbroken
Abelian gauge symmetries, we know that one of the Higgs’ jobs is to
provide mass to other particles. So if the weak gravity conjecture bound
must apply to some state @xmath with mass @xmath , with @xmath the Higgs
vev, then this still amounts to a bound on the electroweak scale. This
was suggested in the context of gauged @xmath with very tiny coupling
giving an upper bound on the lightest neutrino mass [ Cheung:2014vva ] ,
but the magnetic form of the WGC is difficult to deal with in this
context. This can be circumvented by introducing a new dark Abelian
gauge group @xmath and charged states which get (some of) their mass
from the Higgs [ Craig:2019fdy ] . The way such a model solves the
hierarchy problem is by changing the shape of our prior for the
electroweak scale, as mentioned in Section 5.2 . As naïve effective
field theorists with no information about quantum gravity, we assumed
some sort of flat prior in @xmath . But in fact much of this space is
ruled out by quantum gravitational constraints, consisting of theories
‘in the Swampland’, meaning that our prior should be reshaped to include
only values which can actually be produced by a theory of quantum
gravity. This is how much of the connection from Swampland conjectures
to the real world has worked schematically: one says that Quantum
Gravity demands one live in a subregion of the EFT parameter space. In
theory far more flagrant violations of low-energy expectations are
permissible—that is, the extent to which quantum gravitational violation
of EFT will affect the infrared of our universe is not at all certain.
Of course any proposal to see new effects from a breakdown of EFT must
contend with the rampant success of the SM EFT in the IR—though not in
the far IR, recalling the cosmological constant problem. Certainly a
violation of EFT must both come with good reason and be deftly organized
to spoil only those observed EFT puzzles. For the former, the need for
quantum gravity is obviously compelling. As to the latter, it is
interesting to note that the most pressing mysteries involve the
relevant parameters in the SM Lagrangian. Ultimately, our ability to
address the hierarchy problem through quantum gravitational violations
of EFT is limited by our incomplete understanding of quantum gravity.
This motivates finding non-gravitational toy models that violate EFT
expectations on their own, providing a calculable playground in which to
better understand the potential consequences of UV/IR mixing. In Chapter
The Hierarchy Problem: From the Fundamentals to the Frontiers we pursue
the idea that UV/IR mixing may have more direct effects on the SM by
considering noncommutative field theory (NCFT) as such a toy model.
These theories model physics on spaces where translations do not commute
[ Snyder:1946qz , Connes:1994yd ] , and have many features amenable to a
quantum gravitational interpretation—indeed, noncommutative geometries
have been found arising in various limits of string theory [
Connes:1997cr , Douglas:1997fm , Seiberg:1999vs , Myers:1999ps ] .

## Chapter \thechapter Neutral Naturalness in the Sky

  In the beginning the Universe was created.
  This has made a lot of people very angry and been widely regarded as a
  bad move.

  Douglas Adams
  The Restaurant at the End of the Universe (1980) [ adams1980restaurant
  ]

### 13 Particle Cosmology

It is an amazing and serendipitous fact that the universe started off
hot. As a result of the initially high energies and densities, the
details of microscopic physics greatly affected the large-scale
evolution of the universe. Since the speed of light is finite, by
looking out in the sky at enormous distances we can not only learn about
the history of the universe but we can use this information to learn
about particle physics. While cosmology doesn’t give us probes of
arbitrarily high temperatures, there’s still a humongous amount to be
learned—in part due to further serendipity. The fact that the universe
transitions from radiation domination to matter domination shortly
before it becomes transparent to photons means that the cosmic microwave
background (CMB) encodes information both about the light,
radiation-like degrees of freedom as well as the matter density in the
early universe. Had radiation domination ended far before recombination,
it would be far more difficult to use the CMB to constrain light degrees
of freedom like extra neutrinos. Had radiation domination ended far
after recombination, there would be little evidence of dark matter in
the CMB, which is the strongest evidence for particle dark matter
instead of, say, a modification of gravity at large distances. In fact
such a ‘cosmic coincidence’ also occurs much later, as there is a very
long epoch of matter domination before the universe transitions to dark
energy domination. Were there just slightly less dark energy, its
effects would be essentially invisible thus far in the history of the
universe, and it would be very difficult to measure dark energy at all.
All that is to say that there is enormous value in collaboration between
particle physics and cosmology. In this chapter we investigate this
connection for the twin Higgs model in particular, though our findings
are relevant for general Neutral Naturalness theories as well. In
Section 10.1 we noted that the energy frontier does not effectively
probe these theories. Since they do not introduce new particles with
Standard Model charges, it is only precision electroweak measurements
made at colliders that constrain them at all. However, such theories are
in fact probed very well by cosmology, as they introduce new light
degrees of freedom. Despite the fact that these do not directly interact
with normal matter, their gravitational effects still contribute to the
evolution of the universe, and so the CMB provides a powerful constraint
on new light particles. It is this cosmological effect that provided the
biggest obstacle to the original twin Higgs proposal [ Chacko:2005pe ] ,
which became an urgent issue after the null results of run 1 of the LHC
and the increased interest in models where the lightest states
responsible for Higgs naturalness were SM-neutral. The landmark approach
taken in [ Craig:2015pha ] was to pare the model down to a ‘minimal’
version where only those states necessary for Higgs naturalness appeared
in the twin spectrum. This revived the twin Higgs as a solution to the
little hierarchy problem, and their ‘fraternal’ version brought about
many interesting phenomenological possibilities. The ‘fraternal twin
Higgs’ has a twin sector consisting—at energies below its cutoff—solely
of the third generation of fermions, and with ungauged twin hypercharge.
This brilliantly removes all light particles from the spectrum, so their
effects would not cause trouble in the early universe. But this approach
leaves perhaps a niggling unpleasant taste for those worried about
parsimony. Yes the fraternal twin Higgs introduces fewer new particles
than the mirror twin Higgs, so a naïve desire to solve problems with few
ingredients might suggest that this is a windfall. However, the mirror
twin Higgs really consists of only two ‘ingredients’: a @xmath symmetry
and some soft breaking to misalign the resulting Higgs vevs—whereas the
fraternal twin Higgs has much more structure. While those statements
seem to be of a very subjective sort, we can ground this unease in
physics by considering what’s needed to UV-complete such a model. At the
cutoff of this model @xmath , we need the @xmath to still be a
relatively good symmetry among the largest couplings in the twin
sector—that is, the gauge couplings @xmath and the top Yukawa @xmath
—such that the cancellation of contributions from the two sectors to the
Higgs potential works well. Yet in other parts of the theory we have
done great violence to the structure of the theory, having broken twin
hypercharge and removed parts of the spectrum. How are we to ensure
firstly that the correct degrees of freedom gain large masses, and
secondly that this does not radiatively feed into the remaining light
degrees of freedom? In the midst of this digression, I should mention a
terminological confusion. In the literature, the phrase ‘mirror twin
Higgs’ often refers to models in which the full collection of twin
degrees of freedom are present in the low-energy theory, regardless of
how much @xmath -breaking is present. Judicious introduction of such
asymmetries has been used to create models which avoid cosmological
issues by making all the twin fermions heavy, while still staying within
the technical definition of the mirror twin Higgs. But this is an
overreliance on a definition; the fraternal twin Higgs is merely a limit
of these theories in which the @xmath -breaking is severe enough to push
some degrees of freedom above the cutoff. The real distinction between
classes of Neutral Naturalness models should be between those which
break the @xmath only minimally and those which do greater violence to
the symmetry. It is this distinction which classifies the difficulty
involved in finding a UV completion. Now let me emphasize that this is
not to undercut the value of such a model. After all, the Yukawa
interactions in the Standard Model badly break the large global
symmetries it would otherwise have. Indeed, the fraternal twin Higgs
showcased interesting phenomena, pointed to new general experimental
probes, and provided a basis for many intriguing lines of research. In
fact we will return to this model in Chapter The Hierarchy Problem: From
the Fundamentals to the Frontiers to study a novel collider search
strategy to which it lent credence and which turns out to be a broadly
useful probe of many theories of BSM physics. Despite the fact that the
vast, vast majority of theory papers written in particle physics will
not ultimately be the exact right model of the universe, they still
contain value. They may guide experimental searches toward interesting
classes of signals to look for, or teach us new things about the range
of particle phenomenology or quantum field theories. Regardless, we
obviously don’t know in advance which model will be correct, so
exploring all possible directions is crucial. Yet when there is the
possibility for a more parsimonious model, it’s certainly worth pursuing
that option. This is the philosophy that led to my collaborators and me
looking into the prospect of attaining a realistic twin Higgs cosmology
that respected the @xmath symmetry.

### 14 Asymmetric Reheating

### 15 Freeze-Twin Dark Matter

## Chapter \thechapter Neutral Naturalness in the Ground

  There is nothing like looking, if you want to find something. You
  certainly usually find something, if you look, but it is not always
  quite the something you were after.

  J.R.R. Tolkien
  The Hobbit , 1937 [ tolkien_hobbit ]

### Long-Lived Particles

When we introduced models of Neutral Naturalness in Section 10.2 , we
were motivated by the lack of signals of new, SM-charged particles at
colliders. However, experimentalists are very clever, and it is in fact
possible to observe the effects of such particles if you know where to
look. In particular, Neutral Naturalness models usually exhibit a
‘hidden valley’ type phenomenology, wherein a dark sector is connected
to the SM only through some heavy states. At a collider, a high energy
collision can transfer energy from our sector to these heavy dark sector
states, which may then decay just as heavy SM particles do. If the dark
sector contains absolutely stable particles, then the energy may cascade
into those states, which simply leave the detector. But without a
symmetry dictating that stability, dark sector states may be
destabilized by the effects of that same high-energy link to the SM.
That connection may induce higher-dimensional operators which allow the
decay of the dark sector states back into SM states. Since this decay
channel comes from interactions of heavy states, the width may be highly
suppressed, leaving to a macroscopically long lifetime, even when the
scale set by the particle’s mass is microscopically small. This leads to
the appearance of SM particles out of nowhere inside a detector a
macroscopic distance away from the interaction point in a collider.

In fact there are many reasons an unstable particle may be long-lived,
and we see a multitude of examples in the SM itself. In analogy with the
hidden valley phenomenology, charged pions are long-lived compared to
the QCD scale because their decay must take place through a heavy W
boson. Neutrons are similarly very long-lived as a result of their small
mass splitting with protons. Protons themselves are very long-lived as a
result of an approximate global symmetry (see Section 1.2 for some
little-appreciated subtleties in this reasoning). The SM Higgs is
long-lived compared to the electroweak scale because its leading decay
mode proceeds through the small bottom quark Yukawas. Given the
genericity of long-lived particles in our sector, it’s entirely
reasonable to imagine that some dark sector will have similar
phenomenology. So even aside from our Neutral Naturalness motivation,
such searches are generically useful and interesting things to look for.
In this section we forecast how well an electron-positron collider will
be able to probe Higgs decays to long-lived particles, using the
parameters for some machines which have been proposed by the community
and search strategies of our own design. Such forecasting is crucially
important at the present time, as the community is still discussing what
the next collider is that we will build ²⁴ ²⁴ 24 Note that I have no
idea what the ‘present time’ is for you, the reader, but I am confident
this statement remains true regardless. . It’s clearly necessary to know
what sort of physics program we expect we can carry out before we build
a machine to do it. And these studies are used to motivate different
types of detectors, their detailed design features and how trigger
bandwidth is allocated.

## Chapter \thechapter New Trail for Naturalness

  Is commuting dead?

  The Connecticut Mirror Headline
  June 2, 2020 [ cameron_2020 ]

## Chapter \thechapter Conclusion

  Scientists are baffled: What’s up with the universe?

  The Washington Post Headline
  November 1, 2019 [ achenbach_2019 ]

We end the way we began: Declaring it to be an exciting time in particle
physics. The picture we have painted above on the state of the field is
one of uncertainty—and indeed we have barely even touched on many of the
important problems of the Standard Model. Dark matter and neutrino
masses, while having had canonical, obvious, beautiful solutions in the
context of supersymmetric grand unified theories, are also as yet
mysterious. These fields have likewise turned their focus toward
alternative mechanisms in the past few years as a result of the lack of
observational evidence for their standard solutions. But these facts all
make the universe a more exciting place to study. Imagine if we had
found weak-scale supersymmetry at the LHC, and our job now was simply to
interpret the data in terms of which of the supersymmetric extensions
proposed and well-studied in the past decades were correct. Or even
worse, if technicolor had really been the answer and we had to watch
Nature repeat the same trick she used at the strong scale again at the
weak scale. How dreadfully boring! Yes, yes, this attitude is selfish
and a bit flippant, but what we now have is the chance to learn more
about the universe and about the spectrum of possibilities in physics,
and to explore new, radical ideas. Let me end with a reminder of
another, prior era in which theoretical physicists had thought they had
everything figured out, recalled by no less than Max Planck in a 1924
talk at the University of Munich, and bring to your mind the outcome of
those predictions:

  As I began my university studies [in 1878] I asked my venerable
  teacher Philipp von Jolly for advice regarding the conditions and
  prospects of my chosen field of study. He described physics to me as a
  highly developed, nearly fully matured science, that through the
  crowning achievement of the discovery of the principle of conservation
  of energy it will arguably soon take its final stable form. It may yet
  keep going in one corner or another, scrutinizing or putting in order
  a jot here and a tittle there, but the system as a whole is secured,
  and theoretical physics is noticeably approaching its completion to
  the same degree as geometry did centuries ago. That was the view fifty
  years ago of a respected physicist at the time. ²⁵ ²⁵ 25 In fairness
  to von Jolly (1809-1884), he really was a respected experimental
  physicist in his day—enough so to have been knighted—and earlier in
  his life made important contributions to the understanding of gravity
  and of osmosis [ Wells2016InPO ] . This attitude was not rare at the
  time, and he wouldn’t be remembered for it were it not for a student
  of his having played a role in revolutionizing physics.
  — Max Planck
  As translated in Wells (2016) [ Wells2016InPO ] from Planck (1933) [
  planck1933wege ]

May the universe continue to surprise us. @xmath \dsp

## Chapter \thechapter Kinetic Mixing in the Mirror Twin Higgs

Since kinetic mixing plays a central role in freeze-twin dark matter, we
discuss here at some length the order at which it is expected in the
low-energy EFT. Of course, there may always be UV contributions which
set @xmath to the value needed for freeze-in. However, if the UV
completion of the MTH disallows such terms - for example, via
supersymmetry, an absence of fields charged under both sectors, and
eventually grand unification in each sector (see e.g. [
Berezhiani:2005ek , Falkowski:2006qq , Chang:2006ra , Craig:2013fga ,
Katz:2016wtw , Badziak:2017syq ] )- then the natural expectation is for
mixing of order these irreducible IR contributions. To be concrete, we
imagine that @xmath at the UV cutoff of the MTH, @xmath . To find the
kinetic mixing in the regime of relevance, at momenta @xmath , we must
run down to this scale. As we do not have the technology to easily
calculate high-loop-order diagrams, our analysis is limited to whether
we can prove diagrams at some loop order are vanishing or finite, and so
do not generate mixing. Thus our conclusions are strictly always ‘we
know no argument that kinetic mixing of this order is not generated’,
and there is always the possibility that further hidden cancellations
appear. With that caveat divulged, we proceed and consider diagrammatic
arguments in both the unbroken and broken phases of electroweak
symmetry. Starting in the unbroken phase, we compute the mixing between
the hypercharge gauge bosons. Two- and three-loop diagrams with Higgs
loops containing one gauge vertex and one quartic insertion vanish. By
charge conjugation in scalar QED, the three-leg amplitude of a gauge
boson and a complex scalar pair must be antisymmetric under exchange of
the scalars. However, the quartic coupling of the external legs ensures
that their momenta enter symmetrically. As this holds off-shell, the
presence of a loop which looks like

[]

causes the diagram to vanish. However, at four loops the following
diagram can be drawn which avoids this issue:

[]

where the two hypercharges are connected by charged fermion loops in
their respective sectors and the Higgs doublets’ quartic interaction.
This diagram contributes at least from the MTH cutoff @xmath down to
@xmath , the scale at which twin and electroweak symmetries are broken.
We have no argument that this vanishes nor that its unitarity cuts
vanish. We thus expect a contribution to kinetic mixing of @xmath , with
@xmath the twin and SM hypercharge coupling and @xmath appearing as the
contribution to the photon mixing operator. In this estimate we have
omitted any logarithmic dependence on mass scales, as it is subleading.
In the broken phase, we find it easiest to perform this analysis in
unitary gauge. The Higgs radial modes now mass-mix, but the emergent
charge conjugation symmetries in the two QED sectors allow us to argue
vanishing to higher-loop order. The implications of the formal statement
of charge conjugation symmetry are subtle because we have two QED
sectors, so whether charge conjugation violation is required in both
sectors seems unclear. However, similarly to the above case, there is a
symmetry argument which holds off-shell. The result we rely on here is
that in a vector-like gauge theory, diagrams with any fermion loops with
an odd number of gauge bosons cancel pairwise. Thus, each fermion loop
must be sensitive to the chiral nature of the theory, so the first
non-vanishing contribution is at five loops as in:

[]

where the crosses indicate mass-mixing insertions between the two Higgs
radial modes which each contribute @xmath . Thus, both the running down
to low energies and the finite contributions are five-loop suppressed.
From such diagrams, one expects a contribution @xmath , where with
@xmath and @xmath we denote the vector and axial-vector couplings of the
@xmath , respectively. We note there are other five loop diagrams in
which Higgses couple to massive vectors which are of similar size or
smaller. Depending on the relative sizes of these contributions, one
then naturally expects kinetic mixing of order @xmath . If @xmath is
indeed generated at these loop-levels, then mixing on the smaller end of
this range likely requires that it becomes disallowed not far above the
scale @xmath . However, we note that our ability to argue for
higher-loop order vanishing in the broken versus unbroken phase is
suggestive of the possibility that there may be further cancellations.
We note also the possibility that these diagrams, even if nonzero,
generate only higher-dimensional operators. Further investigation of the
generation of kinetic mixing through a scalar portal is certainly
warranted.

## Chapter \thechapter How to Formulate Field Theory on a Noncommutative
Space

In this appendix we provide detail on how to formulate field theories on
a space which is defined by Equation LABEL:eqn:ncdef , which we repeat
here for convenience:

  -- -------- -- -------
     @xmath      (207)
  -- -------- -- -------

To construct a field theory on this space we must specify the algebra of
observables. First we briefly recall the familiar, commutative case. For
simplicity, we consider a scalar field theory on flat Euclidean space.
We denote by @xmath the commutative, @xmath -algebra of Schwartz
functions of @xmath -dimensional Euclidean space with the standard
point-wise product, and this constitutes our algebra of observables. A
convenient basis for the vector space is that of plane waves @xmath .
The case of interest here is noncommutative flat Euclidean space, on
which we define @xmath . This now consists of such functions of @xmath
variables @xmath related by Equation LABEL:eqn:ncdef , and so is a
noncommutative algebra, although we’ve specified again the normal
‘point-wise’ product. A useful basis will again be that of plane waves,
which we may define as the eigenfunctions of appropriately-defined
derivatives on the noncommutative space, and which look familiar @xmath
. To can get a sense for this algebra it is useful to carry out the
simple exercise of multiplying two plane waves by simply applying
Baker-Campbell-Hausdorff

  -- -------- -- -------
     @xmath      (208)
  -- -------- -- -------

As in quantum mechanics, we will wish to study noncommutative versions
of familiar commutative theories, and so it will be useful to view
@xmath as a ‘deformation’ of @xmath . We then wish to construct a map
from our commutative algebra to our noncommutative one which returns
smoothly to the identity as @xmath . The standard such choice is the
Weyl-Wigner map @xmath , which one may roughly think of as merely
replacing @xmath s with @xmath s. The procedure is simply to Fourier
transform from commutative space to momenta, and then inverse Fourier
transform to noncommutative space. Given a commutative space Schwartz
function @xmath , we may compose the two operations and write

  -- -------- -- -------
     @xmath      (209)
  -- -------- -- -------

Note that this is an injective map of Schwartz functions on @xmath to
those on @xmath which respects the vector space structure but not the
structure of the algebra. This property is familiar from quantum
mechanics. We may now construct noncommutative versions of field
variables, but we still don’t know how to do physics on these spaces.
That is, we can write down the Lagrangian for noncommutative @xmath
theory, and we could even determine an action after we formulate a
notion of an integral over a noncommutative space. But our familiar
results about how to go from the action of a field theory to a
calculation for a physical observable most certainly depended implicitly
on living on a commutative space, and so it seems we must re-formulate
physics from the bottom up. Fortunately, such a drastic measure may not
be necessary, as one may formulate QFT on noncommutative spaces as a
simple modification of our normal field theory structure. The core idea
is to find an algebra of functions on @xmath which is isomorphic to
@xmath by pushing the noncommutativity into a new field product, known
as a Groenewold-Moyal product (or star-product). We diagram the
structure we wish to look for in Figure 26 .

In particular, we may do this by demanding that our quantization map
@xmath is upgraded to an isomorphism between @xmath and an algebra on
the vector space of functions of commutative Euclidean space, with a
multiplication operation which is chosen to preserve the algebraic
structure. That is, we must satisfy

  -- -------- -- -------
     @xmath      (210)
  -- -------- -- -------

for any Schwartz functions @xmath on commutative Euclidean space. But we
can guarantee this by ensuring it for plane waves, the calculation of
which we’ve essentially already done above in Equation 208 :

  -- -------- -------- -- -------
     @xmath               
              @xmath      
     @xmath   @xmath      (211)
  -- -------- -------- -- -------

where the @xmath subscript merely tells us the star-product will depend
on the noncommutativity tensor, and this will henceforth be dropped.
This gives a position-space representation of the star-product,

  -- -------- -- -------
     @xmath      (212)
  -- -------- -- -------

The general procedure to construct a noncommutative field theory from a
commutative one is then by application of the Weyl-Wigner map. As an
example, for a simple @xmath theory we find

  -- -------- -- -------
     @xmath      (213)
  -- -------- -- -------

## Chapter \thechapter Wilsonian Interpretations of NCFTs from Auxiliary
Fields

In this appendix we discuss various generalizations of the procedure
introduced in [ Minwalla:1999px , VanRaamsdonk:2000rr ] to account for
the new structures appearing in the noncommutative quantum effective
action via the introduction of additional auxiliary fields.

## Appendix A Scalar Two-Point Function

It is simple to generalize the procedure discussed in Section
LABEL:sec:phi4 to add to the quadratic effective action of @xmath any
function we wish through judicious choice of the two-point function for
an auxiliary field @xmath which linearly mixes with it. In position
space, if we wish to add to our effective Lagrangian

  -- -------- -- -------
     @xmath      (214)
  -- -------- -- -------

where @xmath is any function of momenta, and @xmath is a coupling we’ve
taken out for convenience, then we simply add to our tree-level
Lagrangian

  -- -------- -- -------
     @xmath      (215)
  -- -------- -- -------

where @xmath is the operator inverse of @xmath . It should be obvious
that this procedure is entirely general. As applied to the Euclidean
@xmath model, we may use this procedure to add a second auxiliary field
to account for the logarithmic term in the quadratic effective action as

  -- -------- -- -------
     @xmath      (216)
  -- -------- -- -------

where we point out that the argument of the log is just @xmath in
position space. We may then try to interpret @xmath also as a new
particle. As discussed in [ VanRaamsdonk:2000rr ] , its logarithmic
propagator may be interpreted as propagation in an additional dimension
of spacetime. Alternatively, we may simply add a single auxiliary field
which accounts for both the quadratic and logarithmic IR singularities
by formally applying the above procedure. But having assigned them an
exotic propagator, it then becomes all the more difficult to interpret
such particles as quanta of elementary fields.

## Appendix B Fermion Two-Point Function

To account for the IR structure in the fermion two-point function, we
must add an auxiliary fermion @xmath . If we wish to find a contribution
to our effective Lagrangian of

  -- -------- -- -------
     @xmath      (217)
  -- -------- -- -------

where @xmath is any operator on Dirac fields, then we should add to our
tree-level Lagrangian

  -- -------- -- -------
     @xmath      (218)
  -- -------- -- -------

with @xmath the operator inverse of @xmath . In the Lorentzian Yukawa
theory of Section LABEL:sec:yukawa , if we add to the Lagrangian

  -- -------- -- -------
     @xmath      (219)
  -- -------- -- -------

we again find a one-loop quadratic effective Lagrangian which is equal
to the @xmath value of the original, but now for any value of @xmath .

## Appendix C Three-Point Function

We may further generalize the procedure for introducing auxiliary fields
to account for IR poles to the case of poles in the three-point
effective action. It’s clear from the form of the IR divergences in
Equation LABEL:eqn:3ptfct that they ‘belong’ to each leg, and so naïvely
one might think this means that the divergences we’ve already found in
the two point functions already fix them. However those corrections only
appear in the internal lines and were already proportional to @xmath ,
and so they will be higher order corrections. Instead we must generate a
correction to the vertex function itself which only corrects one of the
legs. To do this we must introduce auxiliary fields connecting each
possible partition of the interaction operator. However, while an
auxiliary scalar @xmath coupled as @xmath would generate a contribution
to the vertex which includes the @xmath propagator with the @xmath
momentum flowing through it, it would also generate a new @xmath contact
operator, which we don’t want. To avoid this we introduce two auxiliary
fields with off-diagonal two-point functions, a trick used for similar
purposes in [ VanRaamsdonk:2000rr ] . By abandoning minimality, we can
essentially use an auxiliary sector to surgically introduce insertions
of functions of momenta wherever we want them. We can first see how this
works on the scalar leg. We add to our tree-level Lagrangian

  -- -------- -- -------
     @xmath      (220)
  -- -------- -- -------

Now to integrate out the auxiliary fields we note that for a three point
vertex, one may use momentum conservation to put all the
noncommutativity between two of the fields. That is, @xmath as long as
this is not being multiplied by any other functions of @xmath . So we
may use this form of the interaction to simply integrate out the
auxiliary fields. We end up with

  -- -------- -- -------
     @xmath      (221)
  -- -------- -- -------

which is exactly of the right form to account for an IR divergence in
the three-point function which only depends on the @xmath momentum. For
the fermionic legs, we need to add fermionic auxiliary fields which
split the Yukawa operator in the other possible ways. We introduce Dirac
fields @xmath and a differential operator on such fields @xmath . Then
if we add to the Lagrangian

  -- -------- -- -------
     @xmath      (222)
  -- -------- -- -------

we now end up with a contribution to the effective Lagrangian

  -- -- -- -------
           (223)
  -- -- -- -------

where we have abused notation and now the argument of @xmath specifies
which fields it acts on. These terms have the right form to correct both
vertex orderings. Now that we’ve introduced interactions between
auxiliary fields and our original fields, the obvious question to ask is
whether we can utilize the same auxiliary fields to correct both the
two-point and three-point actions. In fact, using two auxiliary fields
with off-diagonal propagators per particle we may insert any corrections
we wish. The new trick is to endow the auxiliary field interactions with
extra momentum dependence. For a first example with a scalar, consider
differential operators @xmath , @xmath , and add to the Lagrangian

  -- -------- -- -------
     @xmath      (224)
  -- -------- -- -------

We may now integrate out the auxiliary fields and find

  -- -------- -- -------
     @xmath      (225)
  -- -------- -- -------

where we’ve assumed that @xmath and @xmath commute. If we take @xmath
then we have the interpretation of merely inserting the @xmath two-point
function in both the two-and three-point functions. But we are also free
to use some nontrivial @xmath , and thus to make the corrections to the
two- and three-point functions have whatever momentum dependence we
wish. It should be obvious how to generalize this to insert momentum
dependence into the scalar lines of arbitrary @xmath point functions.
The case of a fermion is no more challenging in principle. For
differential operators @xmath , we add

  -- -------- -- -------
     @xmath      (226)
  -- -------- -- -------

and upon integrating out the auxiliary fields we find

  -- -- -- -------
           (227)
  -- -- -- -------

where the generalization to @xmath -points is again clear. Note that in
the fermionic case it’s crucial that we be allowed to insert different
momentum dependence in the corrections to the two- and three-point
functions, as these have different Lorentz structures. Now we cannot
quite implement this for the two- and three-point functions calculated
in Section LABEL:sec:yukawa , for the simple reason that we regulated
these quantities differently. That is, we have abused notation and the
symbol ‘ @xmath ’ means different things in the results for the two- and
three-point functions. In order to carry out this procedure, we could
simply regulate the two-point functions in @xmath Schwinger space,
though we run into the technical obstruction that the integration method
above only calculates the leading divergence, which is not good enough
for the scalar case. \ssp