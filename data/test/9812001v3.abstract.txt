Structural disambiguation in sentence analysis is still a central
problem in natural language processing. Past researches have verified
that using lexical semantic knowledge can, to a quite large extent, cope
with this problem. Although there have been many studies conducted in
the past to address the lexical knowledge acquisition problem, further
investigation, especially that based on a principled methodology is
still needed, and this is, in fact, the problem I address in this
thesis.

The problem of acquiring and using lexical semantic knowledge,
especially that of case frame patterns, can be formalized as follows. A
learning module acquires case frame patterns on the basis of some case
frame instances extracted from corpus data. A processing
(disambiguation) module then refers to the acquired knowledge and judges
the degrees of acceptability of some number of new case frames,
including previously unseen ones.

The approach I adopt has the following characteristics: (1) dividing the
problem into three subproblems: case slot generalization, case
dependency learning, and word clustering (thesaurus construction). (2)
viewing each subproblem as that of statistical estimation and defining
probability models for each subproblem, (3) adopting the Minimum
Description Length (MDL) principle as learning strategy, (4) employing
efficient learning algorithms, and (5) viewing the disambiguation
problem as that of statistical prediction.

The need to divide the problem into subproblems is due to the
complicatedness of this task, i.e., there are too many relevant factors
simply to incorporate all of them into a single model. The use of MDL
here leads us to a theoretically sound solution to the ‘data sparseness
problem,’ the main difficulty in a statistical approach to language
processing.

In Chapter 3, I define probability models for each subproblem: (1) the
hard case slot model and the soft case slot model; (2) the word-based
case frame model, the class-based case frame model, and the slot-based
case frame model; and (3) the hard co-occurrence model and the soft
co-occurrence model. These are respectively the probability models for
(1) case slot generalization, (2) case dependency learning, and (3) word
clustering. Here the term ‘hard’ means that the model is characterized
by a type of word clustering in which a word can only belong to a single
class alone, while ‘soft’ means that the model is characterized by a
type of word clustering in which a word can belong to several different
classes.

In Chapter 4, I describe one method for learning the hard case slot
model, i.e., generalizing case slots. I restrict the class of hard case
slot models to that of tree cut models by using an existing thesaurus.
In this way, the problem of generalizing the values of a case slot turns
out to be that of estimating a model from the class of tree cut models
for some fixed thesaurus tree. I then employ an efficient algorithm,
which provably obtains the optimal tree cut model in terms of MDL. This
method, in fact, conducts generalization in the following way. When the
differences between the frequencies of the nouns in a class are not
large enough (relative to the entire data size and the number of the
nouns), it generalizes them into the class. When the differences are
especially noticeable, on the other hand, it stops generalization at
that level.

In Chapter 5, I describe one method for learning the case frame model,
i.e., learning dependencies between case slots. I restrict the class of
case frame models to that of dependency forest models. Case frame
patterns can then be represented as a dependency forest, whose nodes
represent case slots and whose directed links represent the dependencies
that exist between these case slots. I employ an efficient algorithm to
learn the optimal dependency forest model in terms of MDL. This method
first calculates a statistic between all node pairs and sorts these node
pairs in descending order with respect to the statistic. It then puts a
link between the node pair highest in the order, provided that this
value is larger than zero. It repeats this process until no node pair is
left unprocessed, provided that adding that link will not create a loop
in the current dependency graph.

In Chapter 6, I describe one method for learning the hard co-occurrence
model, i.e., automatically conducting word clustering. I employ an
efficient algorithm to repeatedly estimate a suboptimal MDL model from a
class of hard co-occurrence models. The clustering method iteratively
merges, for example, noun classes and verb classes in turn, in a bottom
up fashion. For each merge it performs, it calculates the decrease in
empirical mutual information resulting from merging any noun (or verb)
class pair, and performs the merge having the least reduction in mutual
information, provided that this reduction in mutual information is less
than a threshold, which will vary depending on the data size and the
number of classes in the current situation.

In Chapter 7, I propose, for resolving ambiguities, a new method which
combines the use of the hard co-occurrence model and that of the tree
cut model. In the implementation of this method, the learning module
combines with the hard co-occurrence model to cluster words with respect
to each case slot, and it combines with the tree cut model for
generalizing the values of each case slot by means of a hand-made
thesaurus. The disambiguation module first calculates a likelihood value
for each interpretation on the basis of hard co-occurrence models and
outputs the interpretation with the largest likelihood value; if the
likelihood values are equal (most particularly, if all of them are 0),
it uses likelihood values calculated on the basis of tree cut models; if
the likelihood values are still equal, it makes a default decision.

The accuracy achieved by this method is @xmath , which is higher than
that of state-of-the-art methods.
