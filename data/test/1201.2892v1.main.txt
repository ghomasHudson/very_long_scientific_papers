##### Contents

-    Abstract
-    1 Introduction
    -    1.1 Outline and contributions of the thesis
        -    1.1.1 Related publications
-    2 Complexity of Deciding Convexity
    -    2.1 Introduction
        -    2.1.1 Related Literature
        -    2.1.2 Contributions and organization of this chapter
    -    2.2 Complexity of deciding convexity
        -    2.2.1 Definitions and basics
        -    2.2.2 Degrees that are easy
        -    2.2.3 Degrees that are hard
    -    2.3 Complexity of deciding strict convexity and strong
        convexity
        -    2.3.1 Definitions and basics
        -    2.3.2 Degrees that are easy
        -    2.3.3 Degrees that are hard
    -    2.4 Complexity of deciding quasiconvexity and pseudoconvexity
        -    2.4.1 Definitions and basics
        -    2.4.2 Degrees that are easy
            -    Quasiconvexity of polynomials of odd degree
            -    Pseudoconvexity of polynomials of odd degree
        -    2.4.3 Degrees that are hard
    -    2.5 Summary and conclusions
-    3 Convexity and SOS-Convexity
    -    3.1 Introduction
        -    3.1.1 Nonnegativity and sum of squares
        -    3.1.2 Convexity and sos-convexity
        -    3.1.3 Contributions and organization of this chapter
    -    3.2 Preliminaries
        -    3.2.1 Background on nonnegativity and sum of squares
        -    3.2.2 Connection to semidefinite programming and matrix
            generalizations
        -    3.2.3 Background on convexity and sos-convexity
    -    3.3 Equivalent algebraic relaxations for convexity of
        polynomials
    -    3.4 Some constructions of convex but not sos-convex polynomials
        -    3.4.1 The first example
        -    3.4.2 A “clean” example
    -    3.5 Characterization of the gap between convexity and
        sos-convexity
        -    3.5.1 Proofs of Theorems 3.8 and 3.9 : cases where @xmath
        -    3.5.2 Proofs of Theorems 3.8 and 3.9 : cases where @xmath
            -    Minimal convex but not sos-convex polynomials/forms
            -    Convex but not sos-convex polynomials/forms in all
                higher degrees and dimensions
    -    3.6 Concluding remarks and an open problem
    -    3.7 Appendix A: How the first convex but not sos-convex
        polynomial was found
    -    3.8 Appendix B: Certificates complementing the proof of Theorem
        3.16
-    4 Lyapunov Analysis of Polynomial Differential Equations
    -    4.1 Introduction
        -    4.1.1 Contributions and organization of this chapter
    -    4.2 Complexity considerations for deciding stability of
        polynomial vector fields
        -    4.2.1 Reduction from ONE-IN-THREE 3SAT to positivity of
            quartic forms
        -    4.2.2 Reduction from positivity of quartic forms to
            asymptotic stability of cubic vector fields
    -    4.3 Non-existence of polynomial Lyapunov functions
    -    4.4 (Non)-existence of sum of squares Lyapunov functions
        -    4.4.1 A motivating example
        -    4.4.2 A counterexample
        -    4.4.3 Converse sos Lyapunov theorems
    -    4.5 Existence of sos Lyapunov functions for switched linear
        systems
    -    4.6 Some open questions
-    5 Joint Spectral Radius and Path-Complete Graph Lyapunov Functions
    -    5.1 Introduction
        -    5.1.1 Contributions and organization of this chapter
    -    5.2 Path-complete graphs and the joint spectral radius
    -    5.3 Duality and examples of families of path-complete graphs
    -    5.4 Path-complete graphs with two nodes
        -    5.4.1 The set of path-complete graphs
        -    5.4.2 Comparison of performance
    -    5.5 Further analysis of a particular family of path-complete
        graphs
        -    5.5.1 Duality and invariance under transposition
        -    5.5.2 An approximation guarantee
        -    5.5.3 Numerical examples
    -    5.6 Converse Lyapunov theorems and approximation with arbitrary
        accuracy
    -    5.7 Conclusions and future directions

## Acknowledgements

The opportunities that have been available to me as a graduate student
at MIT have been endless, but without a doubt, the greatest opportunity
of all has been the chance to work with my advisor Pablo Parrilo. What
inspires me most about Pablo—aside from his well-known traits like
superior intelligence, humility, and professional attitude—is his
never-ending passion for understanding things deeply. The joy that Pablo
takes in interpreting a mathematical result from all different angles,
placing it in the right level of abstraction, and simplifying it to the
point that it cannot be made simpler is a virtue that I hope to take
with me throughout my career. As Feynman once said, “The truth always
turns out to be simpler than you thought.” On several occasions in this
thesis, Pablo’s insights, or at times simply his questions, have made me
realize this fact, and for that I am very grateful.

I would also like to thank Pablo for creating the perfect environment
for me to pursue my research ideas. Not once did he ask me to work on a
problem that I didn’t choose to work on myself, not once did he inquire
about a research result before I felt ready to present my progress, and
not once did I ever have to worry about funding for my research or for
going to various conferences. Pablo’s approach was to meet with me
regularly during my Master’s studies, but to be more hands-off
throughout my Ph.D. years. This worked out perfectly. I recall as a
child, my father taught me how to ride a bicycle by running alongside my
bike and holding on to its back, but then gradually letting go of his
hands (without me realizing) so I could ride on my own. I feel that
Pablo has very much done the same thing in the past few years in placing
me on the path to becoming a great researcher. I will be grateful to him
for as long as I continue on this ride.

I couldn’t have asked for better thesis committee members than Vincent
Blondel and John Tsitsiklis. Among their many other accomplishments,
Vincent and John are two of the pioneering figures in complexity theory
in control and optimization, a subject that as parts of this thesis
reflect has become of much interest to me over the last couple of years.
From discussions about complexity and the joint spectral radius to
conversations about my business idea for the MIT $100K Entrepreneurship
Competition, both Vincent and John have always been generous with their
time and advice. In the case of Vincent, I was very fortunate that my
last year at LIDS coincided with the year that he joined us as a
visiting faculty member from Belgium. Some of the most creative talks I
have ever attended have been given by Vincent. (A memorable example is
in his LIDS seminar talk on privacy in social networks , where he fooled
me into believing that his wife, Gilbert Strang, and Mark Zuckerberg
sent him “private” messages which we saw popping up on the screen as he
was speaking!) I am thankful to Vincent for sincerely caring about my
thesis work, either in the form of a call from Belgium providing
encouragement the day prior to my defense, or by deriving valuable
results related to this thesis even after the completion of my defense.

My acquaintance with John goes back to my first year at MIT. I remember
walking out of the first lecture of his probability course telling
myself that I should attend every class that is taught by this
professor. Surely, I have done that. John has an amazing ability to make
every concept look simple and intuitive. I am indebted to him for
everything he has taught me both in and out of the classroom. I would
also like to thank him for his invaluable contributions to a joint work
that led to Chapter 2 of this thesis.

My gratitude extends to my other coauthors: Gerg Blekherman, Raphaël
Jungers, Miroslav Krstic, Alex Olshevsly, and Mardavij Roozbehani. I
have learned a great deal from interactions with all of them. I am
thankful to Greg for settling two (out of two!) mathematical bets that I
made with Markus Schweighofer at a meeting in 2009. Had he not done
that, it is not clear how long it would have taken me to get over my
obsession with those problems and move on to the problems addressed in
this thesis. (I lost one of the two bets when Greg proved that
polynomials with a certain special property exist [ 26 ] ;
interestingly, no explicit examples are known to this day, see Section
3.6 .) Raphaël, Alex, and Mardavij have all been great friends
throughout my years at LIDS. I particularly want to express my gratitude
to Mardavij for always being there when I needed someone to talk to, and
to Raphaël for his nice gesture in giving me a copy of his book [ 85 ]
as a gift in our first meeting, which he kindly signed with the note:
“Let’s collaborate!” I am grateful to Miroslav for hosting me (together
with Tara Javidi) at the Cymer Center for Control Systems and Dynamics
at UCSD, which led to immediate and fruitful collaborations between us.

This thesis has benefited from my interactions with several
distinguished researchers outside of MIT. Among them I would like to
mention Amitabh Basu, Stephen Boyd, Etienne de Klerk, Jesús De Loera,
Bill Helton, Monique Laurent, Jiawang Nie, Bruce Reznick, Claus
Scheiderer, Eduardo Sontag, Bernd Sturmfels, and André Tits. I had the
good fortune to TA the Convex Optimization class that Stephen co-taught
with Pablo while visiting MIT from Stanford. I am grateful to Amitabh
and Jesús for hosting me at the Mathematics Dept. of UC Davis, and to
Eduardo for having me at the SontagFest in DIMACS. These have all been
memorable and unique experiences for me. I wish to also thank Nima
Moshtagh for the internship opportunity at Scientific Systems.

I am deeply grateful to the faculty at LIDS, especially Sasha Megretski,
Asu Ozdaglar, Munther Dahleh, and Emilio Frazzoli for making LIDS a
great learning environment for me. I have also learned a great deal from
classes offered outside of LIDS, for example from the beautiful lectures
of Michael Sipser on Theory of Computation, or from the exciting course
of Constantinos Daskalakis on Algorithmic Game Theory. Special thanks
are also due to members of the LIDS staff. I am grateful to Lisa Gaumond
for her friendly smile and for making sure that I always got reimbursed
on time, to Jennifer Donovan for making organizational tasks easy, and
to Brian Jones for going out of his way to fix my computer twice when I
was on the verge of losing all my files. Janet Fischer from the EECS
graduate office and my academic advisor, Vladimir Stojanovic, are two
other people that have always helped me stay on track.

This is also a good opportunity to thank my mentors at the University of
Maryland, where I spent four amazing years as an undergraduate student.
Thomas Antonsen, Thomas Murphy, Edward Ott, Reza Salem, André Tits, and
(coach!) James Yorke helped set the foundation on which I am able to do
research today.

Some of the results of this thesis would not have been possible without
the use of software packages such as YALMIP [ 98 ] , SOSTOOLS [ 132 ] ,
and SeDuMi [ 157 ] . I am deeply grateful to the people who wrote these
pieces of software and made it freely available. Special thanks go to
Johan Löfberg for his patience in answering my questions about YALMIP.

My best memories at MIT are from the moments shared with great friends,
such as, Amir Khandani, Emmanuel Abbe, Marco Pavone, Parikshit Shah,
Noah Stein, Georgios Kotsalis, Borjan Gagoski, Yola Katsargyri, Mitra
Osqui, Ermin Wei, Hoda Eydgahi, Ali ParandehGheibi, Sertac Karaman,
Michael Rinehart, Mark Tobenkin, John Enright, Mesrob Ohannessian, Rose
Faghih, Ali Faghih, Sidhant Misra, Aliaa Atwi, Venkat Chandrasekaran,
Ozan Candogan, James Saunderson, Dan Iancu, Christian Ebenbauer, Paul
Njoroge, Alireza Tahbaz-Salehi, Kostas Bimpikis, Ilan Lobel, Stavros
Valavani, Spyros Zoumpoulis, and Kimon Drakopoulos. I particularly want
to thank my longtime officemates, Pari and Noah. Admittedly, I didn’t
spend enough time in the office, but whenever I did, I had a great time
in their company. Together with Amir and Emmanuel, I had a lot of fun
exploring the night life of Boston during my first two years. Some of
the memories there are unforgettable. I am thankful to Marco for making
everyday life at MIT more enjoyable with his sense of humor. His
presence was greatly missed during my last year when NASA decided to
threaten the security of all humans on Earth and aliens in space by
making the disastrous mistake of hiring him as a Research Technologist.
I also owe gratitude to Marco’s lovely wife-to-be, Manuela, for her
sincere friendship. Unfortunately, just like NASA, Manuela is a victim
of Marco’s deception.

Aside from friends at MIT, my childhood friends who live in Iran and
Washington DC have made a real effort to keep our friendships close by
paying frequent visits to Boston, and for that I am forever grateful. I
would also like to thank the coaches of the MIT tennis team, Dave
Hagymas, Spritely Roche, and Charlie Maher, for always welcoming me on
practice sessions, which allowed me to continue to pursue my childhood
passion at MIT and maintain a balanced graduate life.

The work in this thesis was partially supported by the NSF Focused
Research Group Grant on Semidefinite Optimization and Convex Algebraic
Geometry DMS-0757207, and by AFOSR MURI subaward 07688-1.

My heart is full of gratitude for my parents, Maryam and Hamid Reza, my
sister, Shirin, and my brother-in-law, Karim, who have always filled my
life with unconditional love and support. I am so sorry and embarrassed
for all the times I have been too “busy” to return the love, and so
grateful to you for never expecting me to do so. “Even after all this
time the sun never says to the earth, ‘you owe me.’ Look what happens
with a love like that. It lights the whole sky.”

I finally want to thank my girlfriend and best friend, Margarita, who
has the heart of an angel and who has changed my life in so many ways
since the moment we met. As it turns out, my coming to Boston was not to
get a Ph.D. degree, but to meet you. “Who could be so lucky? Who comes
to a lake for water and sees the reflection of moon.”

\lhead

[ \fancyplain 0 ] \fancyplain \sfeight \rhead [ \fancyplain \sfeight ]
\fancyplain 0

## Chapter 1 Introduction

With the advent of modern computers in the last century and the rapid
increase in our computing power ever since, more and more areas of
science and engineering are being viewed from a computational and
algorithmic perspective—the field of optimization and control is no
exception. Indeed, what we often regard nowadays as a satisfactory
solution to a problem in this field—may it be the optimal allocation of
resources in a power network or the planning of paths of minimum fuel
consumption for a group of satellites—is an efficient algorithm that
when fed with an instance of the problem as input, returns in a
reasonable amount of time an output that is guaranteed to be optimal or
near optimal.

Fundamental concepts from theory of computation, such as the notions of
a Turing machine, decidability, polynomial time solvability, and the
theory of NP-completeness, have allowed us to make precise what it means
to have an (efficient) algorithm for a problem and much more remarkably
to even be able to prove that for certain problems such algorithms do
not exist. The idea of establishing “hardness results” to provide
rigorous explanations for why progress on some problems tends to be
relatively unsuccessful is commonly used today across many disciplines
and rightly so. Indeed, when a problem is resisting all attempts for an
(efficient) algorithm, little is more valuable to an unsatisfied
algorithm designer than the ability to back up the statement “I cannot
do it” with the claim that “it cannot be done”.

Over the years, the line between what can or cannot be efficiently
computed has shown to be a thin one. There are many examples in
optimization and control where complexity results reveal that two
problems that on the surface appear quite similar have very different
structural properties. Consider for example the problem of deciding
given a symmetric matrix @xmath , whether @xmath is nonnegative for all
@xmath , and contrast this to the closely related problem of deciding
whether @xmath is nonnegative for all @xmath ’s in @xmath that are
elementwise nonnegative. The first problem, which is at the core of
semidefinite programming, can be answered in polynomial time (in fact in
@xmath ), whereas the second problem, which forms the basis of
copositive programming, is NP-hard and can easily encode many hard
combinatorial problems [ 109 ] . Similar scenarios arise in control
theory. An interesting example is the contrast between the problems of
deciding stability of interval polynomials and interval matrices. If we
are given a single univariate polynomial of degree @xmath or a single
@xmath matrix, then standard classical results enable us to decide in
polynomial time whether the polynomial or the matrix is (strictly)
stable, i.e, has all of its roots (resp. eigenvalues) in the open left
half complex plane. Suppose now that we are given lower and upper bounds
on the coefficients of the polynomial or on the entries of the matrix
and we are asked to decide whether all polynomials or matrices in this
interval family are stable. Can the answer still be given in polynomial
time? For the case of interval polynomials, Kharitonov famously
demonstrated [ 87 ] that it can: stability of an interval polynomial can
be decided by checking whether four polynomials obtained from the family
via some simple rules are stable. One may naturally speculate whether
such a wonderful result can also be established for interval matrices,
but alas, NP-hardness results [ 110 ] reveal that unless P=NP, this
cannot happen.

Aside from ending the quest for exact efficient algorithms, an
NP-hardness result also serves as an insightful bridge between different
areas of mathematics. Indeed, when we give a reduction from an NP-hard
problem to a new problem of possibly different nature, it becomes
apparent that the computational difficulties associated with the first
problem are intrinsic also to the new problem. Conversely, any algorithm
that has been previously developed for the new problem can now readily
be applied also to the first problem. This concept is usually
particularly interesting when one problem is in the domain of discrete
mathematics and the other in the continuous domain, as will be the case
for problems considered in this thesis. For example, we will give a
reduction from the canonical NP-complete problem of 3SAT to the problem
of deciding stability of a certain class of differential equations. As a
byproduct of the reduction, it will follow that a certificate of
unsatisfiability of instances of 3SAT can always be given in form of a
Lyapunov function.

In general, hardness results in optimization come with a clear practical
implication: as an algorithm designer, we either have to give up
optimality and be content with finding suboptimal solutions, or we have
to work with a subclass of problems that have more tractable attributes.
In view of this, it becomes exceedingly relevant to identify structural
properties of optimization problems that allow for tractability of
finding optimal solutions.

One such structural property, which by and large is the most fundamental
one that we know of, is convexity . As a geometric property, convexity
comes with many attractive consequences. For instance, every local
minimum of a convex problem is also a global minimum. Or for example, if
a point does not belong to a convex set, this nonmembership can be
certified through a separating hyperplane. Due in part to such special
attributes, convex problems generally allow for efficient algorithms for
solving them. Among other approaches, a powerful theory of
interior-point polynomial time methods for convex optimization was
developed in [ 111 ] . At least when the underlying convex cone has an
efficiently computable so-called “barrier function”, these algorithms
are efficient both in theory and in practice.

Extensive and greatly successful research in the applications of convex
optimization over the last couple of decades has shown that surprisingly
many problems of practical importance can be cast as convex optimization
problems. Moreover, we have a fair number of rules based on the calculus
of convex functions that allow us to design—whenever we have the freedom
to do so—problems that are by construction convex. Nevertheless, in
order to be able to exploit the potential of convexity in optimization
in full, a very basic question is to understand whether we are even able
to recognize the presence of convexity in optimization problems. In
other words, can we have an efficient algorithm that tests whether a
given optimization problem is convex?

We will show in this thesis—answering a longstanding question of N.Z.
Shor—that unfortunately even for the simplest classes of optimization
problems where the objective function and the defining functions of the
feasible set are given by polynomials of modest degree, the question of
determining convexity is NP-hard. We also show that the same
intractability result holds for essentially any well-known variant of
convexity (generalized convexity). These results suggest that as
significant as convexity may be in optimization, we may not be able to
in general guarantee its presence before we can enjoy its consequences.

Of course, NP-hardness of a problem does not stop us from studying it,
but on the contrary stresses the need for finding good approximation
algorithms that can deal with a large number of instances efficiently.
Towards this goal, we will devote part of this thesis to a study of
convexity from an algebraic viewpoint. We will argue that in many cases,
a notion known as sos-convexity , which is an efficiently checkable
algebraic counterpart of convexity, can be a viable substitute for
convexity of polynomials. Aside from its computational implications,
sos-convexity has recently received much attention in the area of convex
algebraic geometry [ 26 ] , [ 55 ] , [ 75 ] , [ 89 ] , [ 90 ] , [ 91 ] ,
mainly due to its role in connecting the geometric and algebraic aspects
of convexity. In particular, the name “sos-convexity” comes from the
work of Helton and Nie on semidefinite representability of convex sets [
75 ] .

The basic idea behind sos-convexity is nothing more than a simple
extension of the concept of representation of nonnegative polynomials as
sums of squares. To demonstrate this idea on a concrete example, suppose
we are given the polynomial

  -- -------- -- -------
     @xmath      (1.1)
  -- -------- -- -------

and we are asked to decide whether it is nonnegative, i.e, whether
@xmath for all @xmath in @xmath . This may seem like a daunting task
(and indeed it is as deciding nonnegativity of quartic polynomials is
also NP-hard), but suppose that we could “somehow” come up with a
decomposition of the polynomial a sum of squares (sos):

  -- -------- -- -------
     @xmath      (1.2)
  -- -------- -- -------

Then, we have at our hands an explicit certificate of nonnegativity of
@xmath , which can be easily checked (simply by multiplying the terms
out).

It turns out (see e.g. [ 118 ] , [ 119 ] ) that because of several
interesting connections between real algebra and convex optimization
discovered in recent years and quite well-known by now, the question of
existence of an sos decomposition can be cast as a semidefinite program,
which can be solved efficiently e.g. by interior point methods. As we
will see more formally later, the notion of sos-convexity is based on an
appropriately defined sum of squares decomposition of the Hessian matrix
of a polynomial and hence it can also be checked efficiently with
semidefinite programming. Just like sum of squares decomposition is a
sufficient condition for polynomial nonnegativity, sos-convexity is a
sufficient condition for polynomial convexity.

An important question that remains here is the obvious one: when do
nonnegative polynomials admit a decomposition as a sum of squares? The
answer to this question comes from a classical result of Hilbert. In his
seminal 1888 paper [ 77 ] , Hilbert gave a complete characterization of
the degrees and dimensions in which all nonnegative polynomials can be
written as sums of squares. In particular, he proved that there exist
nonnegative polynomials with no sum of squares decomposition, although
explicit examples of such polynomials appeared only 80 years later. One
of the main contributions of this thesis is to establish the counterpart
of Hilbert’s results for the notions of convexity and sos-convexity. In
particular, we will give the first example of a convex polynomial that
is not sos-convex, and by the end of the first half of this thesis, a
complete characterization of the degrees and dimensions in which
convexity and sos-convexity are equivalent. Some interesting and
unexpected connections to Hilbert’s results will also emerge in the
process.

In the second half of this thesis, we will turn to the study of
stability in dynamical systems. Here too, we will take a computational
viewpoint with our goal will being the development and analysis of
efficient algorithms for proving stability of certain classes of
nonlinear and hybrid systems.

Almost universally, the study of stability in systems theory leads to
Lyapunov’s second method or one of its many variants. An outgrowth of
Lyapunov’s 1892 doctoral dissertation [ 99 ] , Lyapunov’s second method
tells us, roughly speaking, that if we succeed in finding a Lyapunov
function —an energy-like function of the state that decreases along
trajectories—then we have proven that the dynamical system in question
is stable. In the mid 1900s, a series of converse Lyapunov theorems were
developed which established that any stable system indeed has a Lyapunov
function (see [ 72 , Chap. 6] for an overview). Although this is
encouraging, except for the simplest classes of systems such as linear
systems, converse Lyapunov theorems do not provide much practical
insight into how one may go about finding a Lyapunov function.

In the last few decades however, advances in the theory and practice of
convex optimization and in particular semidefinite programming (SDP)
have rejuvenated Lyapunov theory. The approach has been to parameterize
a class of Lyapunov functions with restricted complexity (e.g.,
quadratics, pointwise maximum of quadratics, polynomials, etc.) and then
pose the search for a Lyapunov function as a convex feasibility problem.
A widely popular example of this framework which we will revisit later
in this thesis is the method of sum of squares Lyapunov functions [ 118
] , [ 121 ] . Expanding on the concept of sum of squares decomposition
of polynomials described above, this technique allows one to formulate
semidefinite programs that search for polynomial Lyapunov functions for
polynomial dynamical systems. Sum of squares Lyapunov functions, along
with many other SDP based techniques, have also been applied to systems
that undergo switching; see e.g. [ 136 ] , [ 131 ] , [ 122 ] . The
analysis of these types of systems will also be a subject of interest in
this thesis.

An algorithmic approach to Lyapunov theory naturally calls for new
converse theorems. Indeed, classical converse Lyapunov theorems only
guarantee existence of Lyapunov functions within very broad classes of
functions (e.g. the class of continuously differentiable functions) that
are a priori not amenable to computation. So there is the need to know
whether Lyapunov functions belonging to certain more restricted classes
of functions that can be computationally searched over also exist. For
example, do stable polynomial systems admit Lyapunov functions that are
polynomial? What about polynomial functions that can be found with sum
of squares techniques? Similar questions arise in the case of switched
systems. For example, do stable linear switched systems admit sum of
squares Lyapunov functions? How about Lyapunov functions that are the
pointwise maximum of quadratics? If so, how many quadratic functions are
needed? We will answer several questions of this type in this thesis.

This thesis will also introduce a new class of techniques for Lyapunov
analysis of switched systems. The novel component here is a general
framework for formulating Lyapunov inequalities between multiple
Lyapunov functions that together guarantee stability of a switched
system under arbitrary switching. The relation between these
inequalities has interesting links to concepts from automata theory.
Furthermore, the technique is amenable to semidefinite programming.

Although the main ideas behind our approach directly apply to broader
classes of switched systems, our results will be presented in the more
specific context of switched linear systems. This is mainly due to our
interest in the notion of the joint spectral radius of a set of matrices
which has intimate connections to stability of switched linear systems.
The joint spectral radius is an extensively studied quantity that
characterizes the maximum growth rate obtained by taking arbitrary
products from a set of matrices. Computation of the joint spectral
radius, although notoriously hard [ 35 ] , [ 161 ] , has a wide range of
applications including continuity of wavelet functions, computation of
capacity of codes, convergence of consensus algorithms, and
combinatorics, just to name a few. Our techniques provide several
hierarchies of polynomial time algorithms that approximate the JSR with
guaranteed accuracy.

A more concrete account of the contributions of this thesis will be
given in the following section. We remark that although the first half
of the thesis is mostly concerned with convexity in polynomial
optimization and the second half with Lyapunov analysis, a common theme
throughout the thesis is the use of algorithms that involve algebraic
methods in optimization and semidefinite programming.

### 1.1 Outline and contributions of the thesis

The remainder of this thesis is divided into two parts each containing
two chapters. The first part includes our complexity results on deciding
convexity in polynomial optimization (Chapter 2 ) and our study of the
relationship between convexity and sos-convexity (Chapter 3 ). The
second part includes new results on Lyapunov analysis of polynomial
differential equations (Chapter 4 ) and a novel framework for proving
stability of switched systems (Chapter 5 ). A summary of our
contributions in each chapter is as follows.

###### Chapter 2.

The main result of this chapter is to prove that unless P=NP, there
cannot be a polynomial time algorithm (or even a pseudo-polynomial time
algorithm) that can decide whether a quartic polynomial is globally
convex. This answers a question of N.Z. Shor that appeared as one of
seven open problems in complexity theory for numerical optimization in
1992 [ 117 ] . We also show that deciding strict convexity, strong
convexity, quasiconvexity, and pseudoconvexity of polynomials of even
degree four or higher is strongly NP-hard. By contrast, we show that
quasiconvexity and pseudoconvexity of odd degree polynomials can be
decided in polynomial time.

###### Chapter 3.

Our first contribution in this chapter is to prove that three natural
sum of squares (sos) based sufficient conditions for convexity of
polynomials via the definition of convexity, its first order
characterization, and its second order characterization are equivalent.
These three equivalent algebraic conditions, which we will refer to as
sos-convexity, can be checked by solving a single semidefinite program.
We present the first known example of a convex polynomial that is not
sos-convex. We explain how this polynomial was found with tools from sos
programming and duality theory of semidefinite optimization. As a
byproduct of this numerical procedure, we obtain a simple method for
searching over a restricted family of nonnegative polynomials that are
not sums of squares that can be of independent interest.

If we denote the set of convex and sos-convex polynomials in @xmath
variables of degree @xmath with @xmath and @xmath respectively, then our
main contribution in this chapter is to prove that @xmath if and only if
@xmath or @xmath or @xmath . We also present a complete characterization
for forms (homogeneous polynomials) except for the case @xmath which
will appear elsewhere [ 2 ] . Our result states that the set @xmath of
convex forms in @xmath variables of degree @xmath equals the set @xmath
of sos-convex forms if and only if @xmath or @xmath or @xmath . To prove
these results, we present in particular explicit examples of polynomials
in @xmath and @xmath and forms in @xmath and @xmath , and a general
procedure for constructing forms in @xmath from nonnegative but not sos
forms in @xmath variables and degree @xmath .

Although for disparate reasons, the remarkable outcome is that convex
polynomials (resp. forms) are sos-convex exactly in cases where
nonnegative polynomials (resp. forms) are sums of squares, as
characterized by Hilbert.

###### Chapter 4.

This chapter is devoted to converse results on (non)-existence of
polynomial and sum of squares polynomial Lyapunov functions for systems
described by polynomial differential equations. We present a simple,
explicit example of a two-dimensional polynomial vector field of degree
two that is globally asymptotically stable but does not admit a
polynomial Lyapunov function of any degree. We then study whether
existence of a polynomial Lyapunov function implies existence of one
that can be found with sum of squares techniques. We show via an
explicit counterexample that if the degree of the polynomial Lyapunov
function is fixed, then sos programming can fail to find a valid
Lyapunov function even though one exists. On the other hand, if the
degree is allowed to increase, we prove that existence of a polynomial
Lyapunov function for a planar vector field (under an additional mild
assumption) or for a homogeneous vector field implies existence of a
polynomial Lyapunov function that is sos and that the negative of its
derivative is also sos. This result is extended to prove that asymptotic
stability of switched linear systems can always be proven with sum of
squares Lyapunov functions. Finally, we show that for the latter class
of systems (both in discrete and continuous time), if the negative of
the derivative of a Lyapunov function is a sum of squares, then the
Lyapunov function itself is automatically a sum of squares.

This chapter also includes some complexity results. We prove that
deciding asymptotic stability of homogeneous cubic polynomial vector
fields is strongly NP-hard. We discuss some byproducts of the reduction
that establishes this result, including a Lyapunov-inspired technique
for proving positivity of forms.

###### Chapter 5.

In this chapter, we introduce the framework of path-complete graph
Lyapunov functions for approximation of the joint spectral radius. The
approach is based on the analysis of the underlying switched system via
inequalities imposed between multiple Lyapunov functions associated to a
labeled directed graph. The nodes of this graph represent Lyapunov
functions, and its directed edges that are labeled with matrices
represent Lyapunov inequalities. Inspired by concepts in automata theory
and symbolic dynamics, we define a class of graphs called path-complete
graphs, and show that any such graph gives rise to a method for proving
stability of the switched system. This enables us to derive several
asymptotically tight hierarchies of semidefinite programming relaxations
that unify and generalize many existing techniques such as common
quadratic, common sum of squares, and maximum/minimum-of-quadratics
Lyapunov functions.

We compare the quality of approximation obtained by certain families of
path-complete graphs including all path-complete graphs with two nodes
on an alphabet of two matrices. We argue that the De Bruijn graph of
order one on @xmath symbols, with quadratic Lyapunov functions assigned
to its nodes, provides good estimates of the JSR of @xmath matrices at a
modest computational cost. We prove that the bound obtained via this
method is invariant under transposition of the matrices and always
within a multiplicative factor of @xmath of the true JSR (independent of
the number of matrices).

Approximation guarantees for analysis via other families of
path-complete graphs will also be provided. In particular, we show that
the De Bruijn graph of order @xmath , with quadratic Lyapunov functions
as nodes, can approximate the JSR with arbitrary accuracy as @xmath
increases. This also proves that common Lyapunov functions that are the
pointwise maximum (or minimum) of quadratics always exist. Moreover, the
result gives a bound on the number of quadratic functions needed to
achieve a desired level of accuracy in approximation of the JSR, and
also demonstrates that these quadratic functions can be found with
semidefinite programming.

A list of open problems for future research is presented at the end of
each chapter.

#### 1.1.1 Related publications

The material presented in this thesis is in the most part based on the
following papers.

###### Chapter 2.

A. A. Ahmadi, A. Olshevsky, P. A. Parrilo, and J. N. Tsitsiklis.
NP-hardness of deciding convexity of quartic polynomials and related
problems. Mathematical Programming , 2011. Accepted for publication.
Online version available at arXiv:.1012.1908.

###### Chapter 3.

A. A. Ahmadi and P. A. Parrilo. A convex polynomial that is not
sos-convex. Mathematical Programming , 2011. DOI:
10.1007/s10107-011-0457-z.

A. A. Ahmadi and P. A. Parrilo. A complete characterization of the gap
between convexity and sos-convexity. In preparation, 2011.

A. A. Ahmadi, G. Blekherman, and P. A.Parrilo. Convex ternary quartics
are sos-convex. In preparation, 2011.

###### Chapter 4.

A. A. Ahmadi and P. A. Parrilo. Converse results on existence of sum of
squares Lyapunov functions. In Proceedings of the 50 @xmath IEEE
Conference on Decision and Control , 2011.

A. A. Ahmadi, M. Krstic, and P. A. Parrilo. A globally asymptotically
stable polynomial vector field with no polynomial Lyapunov function. In
Proceedings of the 50 @xmath IEEE Conference on Decision and Control ,
2011.

###### Chapter 5.

A. A. Ahmadi, R. Jungers, P. A. Parrilo, and M. Roozbehani. Analysis of
the joint spectral radius via Lyapunov functions on path-complete
graphs. In Hybrid Systems: Computation and Control 2011 , Lecture Notes
in Computer Science. Springer, 2011.

\sfbHuge

Part I:

Computational and Algebraic

Aspects of Convexity

## Chapter 2 Complexity of Deciding Convexity

In this chapter, we characterize the computational complexity of
deciding convexity and many of its variants in polynomial optimization.
The material presented in this chapter is based on the work in [ 5 ] .

### 2.1 Introduction

The role of convexity in modern day mathematical programming has proven
to be remarkably fundamental, to the point that tractability of an
optimization problem is nowadays assessed, more often than not, by
whether or not the problem benefits from some sort of underlying
convexity. In the famous words of Rockafellar [ 143 ] :

-   “In fact the great watershed in optimization isn’t between linearity
    and nonlinearity, but convexity and nonconvexity.”

But how easy is it to distinguish between convexity and nonconvexity?
Can we decide in an efficient manner if a given optimization problem is
convex?

A class of optimization problems that allow for a rigorous study of this
question from a computational complexity viewpoint is the class of
polynomial optimization problems. These are optimization problems where
the objective is given by a polynomial function and the feasible set is
described by polynomial inequalities. Our research in this direction was
motivated by a concrete question of N. Z. Shor that appeared as one of
seven open problems in complexity theory for numerical optimization put
together by Pardalos and Vavasis in 1992 [ 117 ] :

-   “Given a degree- @xmath polynomial in @xmath variables, what is the
    complexity of determining whether this polynomial describes a convex
    function?”

As we will explain in more detail shortly, the reason why Shor’s
question is specifically about degree @xmath polynomials is that
deciding convexity of odd degree polynomials is trivial and deciding
convexity of degree @xmath (quadratic) polynomials can be reduced to the
simple task of checking whether a constant matrix is positive
semidefinite. So, the first interesting case really occurs for degree
@xmath (quartic) polynomials. Our main contribution in this chapter
(Theorem 2.1 in Section 2.2.3 ) is to show that deciding convexity of
polynomials is strongly NP-hard already for polynomials of degree @xmath
.

The implication of NP-hardness of this problem is that unless P=NP,
there exists no algorithm that can take as input the (rational)
coefficients of a quartic polynomial, have running time bounded by a
polynomial in the number of bits needed to represent the coefficients,
and output correctly on every instance whether or not the polynomial is
convex. Furthermore, the fact that our NP-hardness result is in the
strong sense (as opposed to weakly NP-hard problems such as KNAPSACK)
implies, roughly speaking, that the problem remains NP-hard even when
the magnitude of the coefficients of the polynomial are restricted to be
“small.” For a strongly NP-hard problem, even a pseudo-polynomial time
algorithm cannot exist unless P=NP. See [ 61 ] for precise definitions
and more details.

There are many areas of application where one would like to establish
convexity of polynomials. Perhaps the simplest example is in global
minimization of polynomials, where it could be very useful to decide
first whether the polynomial to be optimized is convex. Once convexity
is verified, then every local minimum is global and very basic
techniques (e.g., gradient descent) can find a global minimum—a task
that is in general NP-hard in the absence of convexity [ 124 ] , [ 109 ]
. As another example, if we can certify that a homogeneous polynomial is
convex, then we define a gauge (or Minkowski) norm based on its convex
sublevel sets, which may be useful in many applications. In several
other problems of practical relevance, we might not just be interested
in checking whether a given polynomial is convex, but to parameterize a
family of convex polynomials and perhaps search or optimize over them.
For example we might be interested in approximating the convex envelope
of a complicated nonconvex function with a convex polynomial, or in
fitting a convex polynomial to a set of data points with minimum error [
100 ] . Not surprisingly, if testing membership to the set of convex
polynomials is hard, searching and optimizing over that set also turns
out to be a hard problem.

We also extend our hardness result to some variants of convexity,
namely, the problems of deciding strict convexity , strong convexity ,
pseudoconvexity , and quasiconvexity of polynomials. Strict convexity is
a property that is often useful to check because it guarantees
uniqueness of the optimal solution in optimization problems. The notion
of strong convexity is a common assumption in convergence analysis of
many iterative Newton-type algorithms in optimization theory; see, e.g.,
[ 38 , Chaps. 9–11] . So, in order to ensure the theoretical convergence
rates promised by many of these algorithms, one needs to first make sure
that the objective function is strongly convex. The problem of checking
quasiconvexity (convexity of sublevel sets) of polynomials also arises
frequently in practice. For instance, if the feasible set of an
optimization problem is defined by polynomial inequalities, by
certifying quasiconvexity of the defining polynomials we can ensure that
the feasible set is convex. In several statistics and clustering
problems, we are interested in finding minimum volume convex sets that
contain a set of data points in space. This problem can be tackled by
searching over the set of quasiconvex polynomials [ 100 ] . In
economics, quasiconcave functions are prevalent as desirable utility
functions [ 92 ] , [ 18 ] . In control and systems theory, it is useful
at times to search for quasiconvex Lyapunov functions whose convex
sublevel sets contain relevant information about the trajectories of a
dynamical system [ 44 ] , [ 8 ] . Finally, the notion of pseudoconvexity
is a natural generalization of convexity that inherits many of the
attractive properties of convex functions. For example, every stationary
point or every local minimum of a pseudoconvex function must be a global
minimum. Because of these nice features, pseudoconvex programs have been
studied extensively in nonlinear programming [ 101 ] , [ 48 ] .

As an outcome of close to a century of research in convex analysis,
numerous necessary, sufficient, and exact conditions for convexity and
all of its variants are available; see, e.g., [ 38 , Chap. 3] , [ 104 ]
, [ 60 ] , [ 49 ] , [ 92 ] , [ 102 ] and references therein for a by no
means exhaustive list. Our results suggest that none of the exact
characterizations of these notions can be efficiently checked for
polynomials. In fact, when turned upside down, many of these equivalent
formulations reveal new NP-hard problems; see, e.g., Corollary 2.6 and
2.8 .

#### 2.1.1 Related Literature

There are several results in the literature on the complexity of various
special cases of polynomial optimization problems. The interested reader
can find many of these results in the edited volume of Pardalos [ 116 ]
or in the survey papers of de Klerk [ 54 ] , and Blondel and Tsitsiklis
[ 36 ] . A very general and fundamental concept in certifying
feasibility of polynomial equations and inequalities is the
Tarski–Seidenberg quantifier elimination theory [ 158 ] , [ 154 ] , from
which it follows that all of the problems that we consider in this
chapter are algorithmically decidable . This means that there are
algorithms that on all instances of our problems of interest halt in
finite time and always output the correct yes–no answer. Unfortunately,
algorithms based on quantifier elimination or similar decision algebra
techniques have running times that are at least exponential in the
number of variables [ 24 ] , and in practice can only solve problems
with very few parameters.

When we turn to the issue of polynomial time solvability, perhaps the
most relevant result for our purposes is the NP-hardness of deciding
nonnegativity of quartic polynomials and biquadratic forms (see
Definition 2.2 ); the main reduction that we give in this chapter will
in fact be from the latter problem. As we will see in Section 2.2.3 , it
turns out that deciding convexity of quartic forms is equivalent to
checking nonnegativity of a special class of biquadratic forms, which
are themselves a special class of quartic forms. The NP-hardness of
checking nonnegativity of quartic forms follows, e.g., as a direct
consequence of NP-hardness of testing matrix copositivity, a result
proven by Murty and Kabadi [ 109 ] . As for the hardness of checking
nonnegativity of biquadratic forms, we know of two different proofs. The
first one is due to Gurvits [ 70 ] , who proves that the entanglement
problem in quantum mechanics (i.e., the problem of distinguishing
separable quantum states from entangled ones) is NP-hard. A dual
reformulation of this result shows directly that checking nonnegativity
of biquadratic forms is NP-hard; see [ 59 ] . The second proof is due to
Ling et al. [ 97 ] , who use a theorem of Motzkin and Straus to give a
very short and elegant reduction from the maximum clique problem in
graphs.

The only work in the literature on the hardness of deciding polynomial
convexity that we are aware of is the work of Guo on the complexity of
deciding convexity of quartic polynomials over simplices [ 69 ] . Guo
discusses some of the difficulties that arise from this problem, but he
does not prove that deciding convexity of polynomials over simplices is
NP-hard. Canny shows in [ 40 ] that the existential theory of the real
numbers can be decided in PSPACE. From this, it follows that testing
several properties of polynomials, including nonnegativity and
convexity, can be done in polynomial space. In [ 112 ] , Nie proves that
the related notion of matrix convexity is NP-hard for polynomial
matrices whose entries are quadratic forms.

On the algorithmic side, several techniques have been proposed both for
testing convexity of sets and convexity of functions. Rademacher and
Vempala present and analyze randomized algorithms for testing the
relaxed notion of approximate convexity [ 135 ] . In [ 91 ] , Lasserre
proposes a semidefinite programming hierarchy for testing convexity of
basic closed semialgebraic sets; a problem that we also prove to be
NP-hard (see Corollary 2.8 ). As for testing convexity of functions, an
approach that some convex optimization parsers (e.g., CVX [ 66 ] ) take
is to start with some ground set of convex functions and then check
whether the desired function can be obtained by applying a set of
convexity preserving operations to the functions in the ground set [ 50
] , [ 38 , p. 79] . Techniques of this type that are based on the
calculus of convex functions are successful for a large range of
applications. However, when applied to general polynomial functions,
they can only detect a subclass of convex polynomials.

Related to convexity of polynomials, a concept that has attracted recent
attention is the algebraic notion of sos-convexity (see Definition 2.4 )
[ 75 ] , [ 89 ] , [ 90 ] , [ 8 ] , [ 100 ] , [ 44 ] , [ 11 ] . This is a
powerful sufficient condition for convexity that relies on an
appropriately defined sum of squares decomposition of the Hessian
matrix, and can be efficiently checked by solving a single semidefinite
program. The study of sos-convexity will be the main focus of our next
chapter. In particular, we will present explicit counterexamples to show
that not every convex polynomial is sos-convex. The NP-hardness result
in this chapter certainly justifies the existence of such
counterexamples and more generally suggests that any polynomial time
algorithm attempted for checking polynomial convexity is doomed to fail
on some hard instances.

#### 2.1.2 Contributions and organization of this chapter

The main contribution of this chapter is to establish the computational
complexity of deciding convexity, strict convexity, strong convexity,
pseudoconvexity, and quasiconvexity of polynomials for any given degree.
(See Table 2.1 in Section 2.5 for a quick summary.) The results are
mainly divided in three sections, with Section 2.2 covering convexity,
Section 2.3 covering strict and strong convexity, and Section 2.4
covering quasiconvexity and pseudoconvexity. These three sections follow
a similar pattern and are each divided into three parts: first, the
definitions and basics, second, the degrees for which the questions can
be answered in polynomial time, and third, the degrees for which the
questions are NP-hard.

Our main reduction, which establishes NP-hardness of checking convexity
of quartic forms, is given in Section 2.2.3 . This hardness result is
extended to strict and strong convexity in Section 2.3.3 , and to
quasiconvexity and pseudoconvexity in Section 2.4.3 . By contrast, we
show in Section 2.4.2 that quasiconvexity and pseudoconvexity of odd
degree polynomials can be decided in polynomial time. A summary of the
chapter and some concluding remarks are presented in Section 2.5 .

### 2.2 Complexity of deciding convexity

#### 2.2.1 Definitions and basics

A (multivariate) polynomial @xmath in variables @xmath is a function
from @xmath to @xmath that is a finite linear combination of monomials:

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

where the sum is over @xmath -tuples of nonnegative integers @xmath . An
algorithm for testing some property of polynomials will have as its
input an ordered list of the coefficients @xmath . Since our complexity
results are based on models of digital computation, where the input must
be represented by a finite number of bits, the coefficients @xmath for
us will always be rational numbers, which upon clearing the denominators
can be taken to be integers. So, for the remainder of the chapter, even
when not explicitly stated, we will always have @xmath .

The degree of a monomial @xmath is equal to @xmath . The degree of a
polynomial @xmath is defined to be the highest degree of its component
monomials. A simple counting argument shows that a polynomial of degree
@xmath in @xmath variables has @xmath coefficients. A homogeneous
polynomial (or a form ) is a polynomial where all the monomials have the
same degree. A form @xmath of degree @xmath is a homogeneous function of
degree @xmath (since it satisfies @xmath ), and has @xmath coefficients.

A polynomial @xmath is said to be nonnegative or positive semidefinite
(psd) if @xmath for all @xmath . Clearly, a necessary condition for a
polynomial to be psd is for its degree to be even. We say that @xmath is
a sum of squares (sos) , if there exist polynomials @xmath such that
@xmath . Every sos polynomial is obviously psd. A polynomial matrix
@xmath is a matrix with polynomial entries. We say that a polynomial
matrix @xmath is PSD (denoted @xmath ) if it is positive semidefinite in
the matrix sense for every value of the indeterminates @xmath . (Note
the upper case convention for matrices.) It is easy to see that @xmath
is PSD if and only if the scalar polynomial @xmath in variables @xmath
is psd.

We recall that a polynomial @xmath is convex if and only if its Hessian
matrix, which will be generally denoted by @xmath , is PSD.

#### 2.2.2 Degrees that are easy

The question of deciding convexity is trivial for odd degree
polynomials. Indeed, it is easy to check that linear polynomials (
@xmath ) are always convex and that polynomials of odd degree @xmath can
never be convex. The case of quadratic polynomials ( @xmath ) is also
straightforward. A quadratic polynomial @xmath is convex if and only if
the constant matrix @xmath is positive semidefinite. This can be decided
in polynomial time for example by performing Gaussian pivot steps along
the main diagonal of @xmath [ 109 ] or by computing the characteristic
polynomial of @xmath exactly and then checking that the signs of its
coefficients alternate [ 79 , p. 403] .

Unfortunately, the results that come next suggest that the case of
quadratic polynomials is essentially the only nontrivial case where
convexity can be efficiently decided.

#### 2.2.3 Degrees that are hard

The main hardness result of this chapter is the following theorem.

###### Theorem 2.1.

Deciding convexity of degree four polynomials is strongly NP-hard. This
is true even when the polynomials are restricted to be homogeneous.

We will give a reduction from the problem of deciding nonnegativity of
biquadratic forms. We start by recalling some basic facts about
biquadratic forms and sketching the idea of the proof.

###### Definition 2.2.

A biquadratic form @xmath is a form in the variables
@xmath and @xmath that can be written as

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

Note that for fixed @xmath , @xmath becomes a quadratic form in @xmath ,
and for fixed @xmath , it becomes a quadratic form in @xmath . Every
biquadratic form is a quartic form, but the converse is of course not
true. It follows from a result of Ling et al. [ 97 ] that deciding
nonnegativity of biquadratic forms is strongly NP-hard. This claim is
not precisely stated in this form in [ 97 ] . For the convenience of the
reader, let us make the connection more explicit before we proceed, as
this result underlies everything that follows.

The argument in [ 97 ] is based on a reduction from CLIQUE (given a
graph @xmath and a positive integer @xmath , decide whether @xmath
contains a clique of size @xmath or more) whose (strong) NP-hardness is
well-known [ 61 ] . For a given graph @xmath on @xmath nodes, if we
define the biquadratic form @xmath in the variables @xmath and @xmath by

  -- -------- --
     @xmath   
  -- -------- --

then Ling et al. [ 97 ] use a theorem of Motzkin and Straus [ 108 ] to
show

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

Here, @xmath denotes the clique number of the graph @xmath , i.e., the
size of a maximal clique. ¹ ¹ 1 Equation ( 2.3 ) above is stated in [ 97
] with the stability number @xmath in place of the clique number @xmath
. This seems to be a minor typo. From this, we see that for any value of
@xmath , @xmath if and only if

  -- -------- --
     @xmath   
  -- -------- --

which by homogenization holds if and only if the biquadratic form

  -- -------- --
     @xmath   
  -- -------- --

is nonnegative. Hence, by checking nonnegativity of @xmath for all
values of @xmath , we can find the exact value of @xmath . It follows
that deciding nonnegativity of biquadratic forms is NP-hard, and in view
of the fact that the coefficients of @xmath are all integers with
absolute value at most @xmath , the NP-hardness claim is in the strong
sense. Note also that the result holds even when @xmath in Definition
2.2 . In the sequel, we will always have @xmath .

It is not difficult to see that any biquadratic form @xmath can be
written in the form

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

(or of course as @xmath ) for some symmetric polynomial matrix @xmath
whose entries are quadratic forms. Therefore, it is strongly NP-hard to
decide whether a symmetric polynomial matrix with quadratic form entries
is PSD. One might hope that this would lead to a quick proof of
NP-hardness of testing convexity of quartic forms, because the Hessian
of a quartic form is exactly a symmetric polynomial matrix with
quadratic form entries. However, the major problem that stands in the
way is that not every polynomial matrix is a valid Hessian . Indeed, if
any of the partial derivatives between the entries of @xmath do not
commute (e.g., if @xmath ), then @xmath cannot be the matrix of second
derivatives of some polynomial. This is because all mixed third partial
derivatives of polynomials must commute.

Our task is therefore to prove that even with these additional
constraints on the entries of @xmath , the problem of deciding positive
semidefiniteness of such matrices remains NP-hard. We will show that any
given symmetric @xmath matrix @xmath , whose entries are quadratic
forms, can be embedded in a @xmath polynomial matrix @xmath , again with
quadratic form entries, so that @xmath is a valid Hessian and @xmath is
PSD if and only if @xmath is. In fact, we will directly construct the
polynomial @xmath whose Hessian is the matrix @xmath . This is done in
the next theorem, which establishes the correctness of our main
reduction. Once this theorem is proven, the proof of Theorem 2.1 will
become immediate.

###### Theorem 2.3.

Given a biquadratic form @xmath , define the the @xmath polynomial
matrix @xmath by setting

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

and let @xmath be the largest coefficient, in absolute value, of any
monomial present in some entry of the matrix @xmath . Let @xmath be the
form given by

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

Then, @xmath is psd if and only if @xmath is convex.

###### Proof.

Before we prove the claim, let us make a few observations and try to
shed light on the intuition behind this construction. We will use @xmath
to denote the Hessian of @xmath . This is a @xmath polynomial matrix
whose entries are quadratic forms. The polynomial @xmath is convex if
and only if @xmath is psd. For bookkeeping purposes, let us split the
variables @xmath as @xmath , where @xmath and @xmath each belong to
@xmath . It will also be helpful to give a name to the second group of
terms in the definition of @xmath in ( 2.6 ). So, let

  -- -------- -- -------
     @xmath      (2.7)
  -- -------- -- -------

We denote the Hessian matrices of @xmath and @xmath with @xmath and
@xmath respectively. Thus, @xmath . Let us first focus on the structure
of @xmath . Observe that if we define

  -- -------- --
     @xmath   
  -- -------- --

then @xmath depends only on @xmath , and

  -- -------- -- -------
     @xmath      (2.8)
  -- -------- -- -------

Similarly, if we let

  -- -------- --
     @xmath   
  -- -------- --

then @xmath depends only on @xmath , and

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

From Eq. ( 2.8 ), we have that @xmath is psd if and only if @xmath is
PSD; from Eq. ( 2.9 ), we see that @xmath is psd if and only if @xmath
is PSD.

Putting the blocks together, we have

  -- -------- -- --------
     @xmath      (2.10)
  -- -------- -- --------

The matrix @xmath is not in general symmetric. The entries of @xmath
consist of square-free monomials that are each a multiple of @xmath for
some @xmath , @xmath , with @xmath ; (see ( 2.2 ) and ( 2.5 )).

The Hessian @xmath of the polynomial @xmath in ( 2.7 ) is given by

  -- -------- -- --------
     @xmath      (2.11)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (2.12)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (2.13)
  -- -------- -- --------

Note that all diagonal elements of @xmath and @xmath contain the square
of every variable @xmath and @xmath respectively.

We fist give an intuitive summary of the rest of the proof. If @xmath is
not psd, then @xmath and @xmath are not PSD and hence @xmath is not PSD.
Moreover, adding @xmath to @xmath cannot help make @xmath PSD because
the dependence of the diagonal blocks of @xmath and @xmath on @xmath and
@xmath runs backwards. On the other hand, if @xmath is psd, then @xmath
will have PSD diagonal blocks. In principle, @xmath might still not be
PSD because of the off-diagonal block @xmath . However, the squares in
the diagonal elements of @xmath will be shown to dominate the monomials
of @xmath and make @xmath PSD.

Let us now prove the theorem formally. One direction is easy: if @xmath
is not psd, then @xmath is not convex. Indeed, if there exist @xmath and
@xmath in @xmath such that @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

For the converse, suppose that @xmath is psd; we will prove that @xmath
is psd and hence @xmath is convex. We have

  -- -------- --
     @xmath   
  -- -------- --

Because @xmath and @xmath are psd by assumption (see ( 2.8 ) and ( 2.9
)), it suffices to show that @xmath is psd. In fact, we will show that
@xmath is a sum of squares.

After some regrouping of terms we can write

  -- -------- -- --------
     @xmath      (2.14)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (2.16)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (2.17)
  -- -------- -- --------

We show that ( 2.14 ) is sos by showing that @xmath , @xmath , and
@xmath are each individually sos. To see that @xmath is sos, simply note
that we can rewrite it as

  -- -------- --
     @xmath   
  -- -------- --

The argument for @xmath is of course identical. To show that @xmath is
sos, we argue as follows. If we multiply out the first term @xmath , we
obtain a polynomial with monomials of the form

  -- -------- -- --------
     @xmath      (2.18)
  -- -------- -- --------

where @xmath , by the definition of @xmath . Since

  -- -------- -- --------
     @xmath      (2.19)
  -- -------- -- --------

by pairing up the terms of @xmath with fractions of the squared terms
@xmath and @xmath , we get a sum of squares. Observe that there are more
than enough squares for each monomial of @xmath because each such
monomial @xmath occurs at most once, so that each of the terms @xmath
and @xmath will be needed at most @xmath times, each time with a
coefficient of at most @xmath . Therefore, @xmath is sos, and this
completes the proof. ∎

We can now complete the proof of strong NP-hardness of deciding
convexity of quartic forms.

###### Proof of Theorem 2.1.

As we remarked earlier, deciding nonnegativity of biquadratic forms is
known to be strongly NP-hard [ 97 ] . Given such a biquadratic form
@xmath , we can construct the polynomial @xmath as in ( 2.6 ). Note that
@xmath has degree four and is homogeneous. Moreover, the reduction from
@xmath to @xmath runs in polynomial time as we are only adding to @xmath
@xmath new monomials with coefficient @xmath , and the size of @xmath is
by definition only polynomially larger than the size of any coefficient
of @xmath . Since by Theorem 2.3 convexity of @xmath is equivalent to
nonnegativity of @xmath , we conclude that deciding convexity of quartic
forms is strongly NP-hard. ∎

###### An algebraic version of the reduction.

Before we proceed further with our results, we make a slight detour and
present an algebraic analogue of this reduction, which relates sum of
squares biquadratic forms to sos-convex polynomials. Both of these
concepts are well-studied in the literature, in particular in regards to
their connection to semidefinite programming; see, e.g., [ 97 ] , [ 11 ]
, and references therein.

###### Definition 2.4.

A polynomial @xmath , with its Hessian denoted by @xmath , is sos-convex
if the polynomial @xmath is a sum of squares in variables (x;y). ² ² 2
Three other equivalent definitions of sos-convexity are presented in the
next chapter.

###### Theorem 2.5.

Given a biquadratic form @xmath , let @xmath be the quartic form defined
as in ( 2.6 ). Then @xmath is a sum of squares if and only if @xmath is
sos-convex.

###### Proof.

The proof is very similar to the proof of Theorem 2.3 and is left to the
reader. ∎

We will revisit Theorem 2.5 in the next chapter when we study the
connection between convexity and sos-convexity.

###### Some NP-hardness results, obtained as corollaries.

NP-hardness of checking convexity of quartic forms directly establishes
NP-hardness ³ ³ 3 All of our NP-hardness results in this chapter are in
the strong sense. For the sake of brevity, from now on we refer to
strongly NP-hard problems simply as NP-hard problems. of several
problems of interest. Here, we mention a few examples.

###### Corollary 2.6.

It is NP-hard to decide nonnegativity of a homogeneous polynomial @xmath
of degree four, of the form

  -- -------- --
     @xmath   
  -- -------- --

for some homogeneous quartic polynomial @xmath .

###### Proof.

Nonnegativity of @xmath is equivalent to convexity of @xmath , and the
result follows directly from Theorem 2.1 .∎

###### Definition 2.7.

A set @xmath is basic closed semialgebraic if it can be written as

  -- -------- -- --------
     @xmath      (2.20)
  -- -------- -- --------

for some positive integer @xmath and some polynomials @xmath .

###### Corollary 2.8.

Given a basic closed semialgebraic set @xmath as in ( 2.20 ), where at
least one of the defining polynomials @xmath has degree four, it is
NP-hard to decide whether @xmath is a convex set.

###### Proof.

Given a quartic polynomial @xmath , consider the basic closed
semialgebraic set

  -- -------- --
     @xmath   
  -- -------- --

describing the epigraph of @xmath . Since @xmath is convex if and only
if its epigraph is a convex set, the result follows. ⁴ ⁴ 4 Another proof
of this corollary is given by the NP-hardness of checking convexity of
sublevel sets of quartic polynomials (Theorem 2.24 in Section 2.4.3 ). ∎

###### Convexity of polynomials of even degree larger than four.

We end this section by extending our hardness result to polynomials of
higher degree.

###### Corollary 2.9.

It is NP-hard to check convexity of polynomials of any fixed even degree
@xmath .

###### Proof.

We have already established the result for polynomials of degree four.
Given such a degree four polynomial @xmath and an even degree @xmath ,
consider the polynomial

  -- -------- --
     @xmath   
  -- -------- --

in @xmath variables. It is clear (e.g., from the block diagonal
structure of the Hessian of @xmath ) that @xmath is convex if and only
if @xmath is convex. The result follows. ∎

### 2.3 Complexity of deciding strict convexity and strong convexity

#### 2.3.1 Definitions and basics

###### Definition 2.10.

A function @xmath is strictly convex if for all @xmath and all @xmath ,
we have

  -- -------- -- --------
     @xmath      (2.21)
  -- -------- -- --------

###### Definition 2.11.

A twice differentiable function @xmath is strongly convex if its Hessian
@xmath satisfies

  -- -------- -- --------
     @xmath      (2.22)
  -- -------- -- --------

for a scalar @xmath and for all @xmath .

We have the standard implications

  -- -------- -- --------
     @xmath      (2.23)
  -- -------- -- --------

but none of the converse implications is true.

#### 2.3.2 Degrees that are easy

From the implications in ( 2.23 ) and our previous discussion, it is
clear that odd degree polynomials can never be strictly convex or
strongly convex. We cover the case of quadratic polynomials in the
following straightforward proposition.

###### Proposition 2.12.

For a quadratic polynomial @xmath , the notions of strict convexity and
strong convexity are equivalent, and can be decided in polynomial time.

###### Proof.

Strong convexity always implies strict convexity. For the reverse
direction, assume that @xmath is not strongly convex. In view of ( 2.22
), this means that the matrix @xmath is not positive definite. If @xmath
has a negative eigenvalue, @xmath is not convex, let alone strictly
convex. If @xmath has a zero eigenvalue, let @xmath be the corresponding
eigenvector. Then @xmath restricted to the line from the origin to
@xmath is linear and hence not strictly convex.

To see that these properties can be checked in polynomial time, note
that @xmath is strongly convex if and only if the symmetric matrix
@xmath is positive definite. By Sylvester’s criterion, positive
definiteness of an @xmath symmetric matrix is equivalent to positivity
of its @xmath leading principal minors, each of which can be computed in
polynomial time. ∎

#### 2.3.3 Degrees that are hard

With little effort, we can extend our NP-hardness result in the previous
section to address strict convexity and strong convexity.

###### Proposition 2.13.

It is NP-hard to decide strong convexity of polynomials of any fixed
even degree @xmath .

###### Proof.

We give a reduction from the problem of deciding convexity of quartic
forms. Given a homogenous quartic polynomial @xmath and an even degree
@xmath , consider the polynomial

  -- -------- -- --------
     @xmath      (2.24)
  -- -------- -- --------

in @xmath variables. We claim that @xmath is convex if and only if
@xmath is strongly convex. Indeed, if @xmath is convex, then so is
@xmath . Therefore, the Hessian of @xmath is PSD. On the other hand, the
Hessian of the term @xmath is the identity matrix. So, the minimum
eigenvalue of the Hessian of @xmath is positive and bounded below by
one. Hence, @xmath is strongly convex.

Now suppose that @xmath is not convex. Let us denote the Hessians of
@xmath and @xmath respectively by @xmath and @xmath . If @xmath is not
convex, then there exists a point @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

where @xmath here denotes the minimum eigenvalue. Because @xmath is
homogenous of degree four, we have

  -- -------- --
     @xmath   
  -- -------- --

for any scalar @xmath . Pick @xmath large enough such that @xmath . Then
it is easy to see that @xmath has a negative eigenvalue and hence @xmath
is not convex, let alone strongly convex. ∎

###### Remark 2.3.1.

It is worth noting that homogeneous polynomials of degree @xmath can
never be strongly convex (because their Hessians vanish at the origin).
Not surprisingly, the polynomial @xmath in the proof of Proposition 2.13
is not homogeneous.

###### Proposition 2.14.

It is NP-hard to decide strict convexity of polynomials of any fixed
even degree @xmath .

###### Proof.

The proof is almost identical to the proof of Proposition 2.13 . Let
@xmath be defined as in ( 2.24 ). If @xmath is convex, then we
established that @xmath is strongly convex and hence also strictly
convex. If @xmath is not convex, we showed that @xmath is not convex and
hence also not strictly convex. ∎

### 2.4 Complexity of deciding quasiconvexity and pseudoconvexity

#### 2.4.1 Definitions and basics

###### Definition 2.15.

A function @xmath is quasiconvex if its sublevel sets

  -- -------- -- --------
     @xmath      (2.25)
  -- -------- -- --------

for all @xmath , are convex.

###### Definition 2.16.

A differentiable function @xmath is pseudoconvex if the implication

  -- -------- -- --------
     @xmath      (2.26)
  -- -------- -- --------

holds for all @xmath and @xmath in @xmath .

The following implications are well-known (see e.g. [ 25 , p. 143] ):

  -- -------- -- --------
     @xmath      (2.27)
  -- -------- -- --------

but the converse of neither implication is true in general.

#### 2.4.2 Degrees that are easy

As we remarked earlier, linear polynomials are always convex and hence
also pseudoconvex and quasiconvex. Unlike convexity, however, it is
possible for polynomials of odd degree @xmath to be pseudoconvex or
quasiconvex. We will show in this section that somewhat surprisingly,
quasiconvexity and pseudoconvexity of polynomials of any fixed odd
degree can be decided in polynomial time. Before we present these
results, we will cover the easy case of quadratic polynomials.

###### Proposition 2.17.

For a quadratic polynomial @xmath , the notions of convexity,
pseudoconvexity, and quasiconvexity are equivalent, and can be decided
in polynomial time.

###### Proof.

We argue that the quadratic polynomial @xmath is convex if and only if
it is quasiconvex. Indeed, if @xmath is not convex, then @xmath has a
negative eigenvalue; letting @xmath be a corresponding eigenvector, we
have that @xmath is a quadratic polynomial in @xmath , with negative
leading coefficient, so @xmath is not quasiconvex, as a function of
@xmath . This, however, implies that @xmath is not quasiconvex.

We have already argued in Section 2.2.2 that convexity of quadratic
polynomials can be decided in polynomial time. ∎

##### Quasiconvexity of polynomials of odd degree

In this subsection, we provide a polynomial time algorithm for checking
whether an odd-degree polynomial is quasiconvex. Towards this goal, we
will first show that quasiconvex polynomials of odd degree have a very
particular structure (Proposition 2.20 ).

Our first lemma concerns quasiconvex polynomials of odd degree in one
variable. The proof is easy and left to the reader. A version of this
lemma is provided in [ 38 , p. 99] , though there also without proof.

###### Lemma 2.18.

Suppose that @xmath is a quasiconvex univariate polynomial of odd
degree. Then, @xmath is monotonic.

Next, we use the preceding lemma to characterize the complements of
sublevel sets of quasiconvex polynomials of odd degree.

###### Lemma 2.19.

Suppose that @xmath is a quasiconvex polynomial of odd degree @xmath .
Then the set @xmath is convex.

###### Proof.

Suppose not. In that case, there exist @xmath such that @xmath is on the
line segment connecting @xmath and @xmath , and such that @xmath but
@xmath . Consider the polynomial

  -- -------- --
     @xmath   
  -- -------- --

This is, of course, a quasiconvex polynomial with @xmath , @xmath , and
@xmath , for some @xmath . If @xmath has degree @xmath , then, by Lemma
2.18 , it must be monotonic, which immediately provides a contradiction.

Suppose now that @xmath has degree less than @xmath . Let us attempt to
perturb @xmath to @xmath , and @xmath to @xmath , so that the new
polynomial

  -- -------- --
     @xmath   
  -- -------- --

has the following two properties: (i) @xmath is a polynomial of degree
@xmath , and (ii) @xmath , @xmath . If such perturbation vectors @xmath
can be found, then we obtain a contradiction as in the previous
paragraph.

To satisfy condition (ii), it suffices (by continuity) to take @xmath
with @xmath small enough. Thus, we only need to argue that we can find
arbitrarily small @xmath that satisfy condition (i). Observe that the
coefficient of @xmath in the polynomial @xmath is a nonzero polynomial
in @xmath ; let us denote that coefficient as @xmath . Since @xmath is a
nonzero polynomial, it cannot vanish at all points of any given ball.
Therefore, even when considering a small ball around @xmath (to satisfy
condition (ii)), we can find @xmath in that ball, with @xmath , thus
establishing that the degree of @xmath is indeed @xmath . This completes
the proof.

∎

We now proceed to a characterization of quasiconvex polynomials of odd
degree.

###### Proposition 2.20.

Let @xmath be a polynomial of odd degree @xmath . Then, @xmath is
quasiconvex if and only if it can be written as

  -- -------- -- --------
     @xmath      (2.28)
  -- -------- -- --------

for some nonzero @xmath , and for some monotonic univariate polynomial
@xmath of degree @xmath . If, in addition, we require the nonzero
component of @xmath with the smallest index to be equal to unity, then
@xmath and @xmath are uniquely determined by @xmath .

###### Proof.

It is easy to see that any polynomial that can be written in the above
form is quasiconvex. In order to prove the converse, let us assume that
@xmath is quasiconvex. By the definition of quasiconvexity, the closed
set @xmath is convex. On the other hand, Lemma 2.19 states that the
closure of the complement of @xmath is also convex. It is not hard to
verify that, as a consequence of these two properties, the set @xmath
must be a halfspace. Thus, for any given @xmath , the sublevel set
@xmath can be written as @xmath for some @xmath and @xmath . This of
course implies that the level sets @xmath are hyperplanes of the form
@xmath .

We note that the sublevel sets are necessarily nested: if @xmath , then
@xmath . An elementary consequence of this property is that the
hyperplanes must be collinear, i.e., that the vectors @xmath must be
positive multiples of each other. Thus, by suitably scaling the
coefficients @xmath , we can assume, without loss of generality, that
@xmath , for some @xmath , and for all @xmath . We then have that @xmath
. Clearly, there is a one-to-one correspondence between @xmath and
@xmath , and therefore the value of @xmath is completely determined by
@xmath . In particular, there exists a function @xmath such that @xmath
. Since @xmath is a polynomial of degree @xmath , it follows that @xmath
is a univariate polynomial of degree @xmath . Finally, we observe that
if @xmath is not monotonic, then @xmath is not quasiconvex. This proves
that a representation of the desired form exists. Note that by suitably
scaling @xmath , we can also impose the condition that the nonzero
component of @xmath with the smallest index is equal to one.

Suppose that now that @xmath can also be represented in the form @xmath
for some other polynomial @xmath and vector @xmath . Then, the gradient
vector of @xmath must be proportional to both @xmath and @xmath . The
vectors @xmath and @xmath are therefore collinear. Once we impose the
requirement that the nonzero component of @xmath with the smallest index
is equal to one, we obtain that @xmath and, consequently, @xmath . This
establishes the claimed uniqueness of the representation. ∎

Remark. It is not hard to see that if @xmath is homogeneous and
quasiconvex, then one can additionally conclude that @xmath can be taken
to be @xmath , where @xmath is the degree of @xmath .

###### Theorem 2.21.

For any fixed odd degree @xmath , the quasiconvexity of polynomials of
degree @xmath can be checked in polynomial time.

###### Proof.

The algorithm consists of attempting to build a representation of @xmath
of the form given in Proposition 2.20 . The polynomial @xmath is
quasiconvex if and only if the attempt is successful.

Let us proceed under the assumption that @xmath is quasiconvex. We
differentiate @xmath symbolically to obtain its gradient vector. Since a
representation of the form given in Proposition 2.20 exists, the
gradient is of the form @xmath , where @xmath is the derivative of
@xmath . In particular, the different components of the gradient are
polynomials that are proportional to each other. (If they are not
proportional, we conclude that @xmath is not quasiconvex, and the
algorithm terminates.) By considering the ratios between different
components, we can identify the vector @xmath , up to a scaling factor.
By imposing the additional requirement that the nonzero component of
@xmath with the smallest index is equal to one, we can identify @xmath
uniquely.

We now proceed to identify the polynomial @xmath . For @xmath , we
evaluate @xmath , which must be equal to @xmath . We thus obtain the
values of @xmath at @xmath distinct points, from which @xmath is
completely determined. We then verify that @xmath is indeed equal to
@xmath . This is easily done, in polynomial time, by writing out the
@xmath coefficients of these two polynomials in @xmath and verifying
that they are equal. (If they are not all equal, we conclude that @xmath
is not quasiconvex, and the algorithm terminates.)

Finally, we test whether the above constructed univariate polynomial
@xmath is monotonic, i.e., whether its derivative @xmath is either
nonnegative or nonpositive. This can be accomplished, e.g., by
quantifier elimination or by other well-known algebraic techniques for
counting the number and the multiplicity of real roots of univariate
polynomials; see [ 24 ] . Note that this requires only a constant number
of arithmetic operations since the degree @xmath is fixed. If @xmath
fails this test, then @xmath is not quasiconvex. Otherwise, our attempt
has been successful and we decide that @xmath is indeed quasiconvex. ∎

##### Pseudoconvexity of polynomials of odd degree

In analogy to Proposition 2.20 , we present next a characterization of
odd degree pseudoconvex polynomials, which gives rise to a polynomial
time algorithm for checking this property.

###### Corollary 2.22.

Let @xmath be a polynomial of odd degree @xmath . Then, @xmath is
pseudoconvex if and only if @xmath can be written in the form

  -- -------- -- --------
     @xmath      (2.29)
  -- -------- -- --------

for some @xmath and some univariate polynomial @xmath of degree @xmath
such that its derivative @xmath has no real roots.

Remark. Observe that polynomials @xmath with @xmath having no real roots
comprise a subset of the set of monotonic polynomials.

###### Proof.

Suppose that @xmath is pseudoconvex. Since a pseudoconvex polynomial is
quasiconvex, it admits a representation @xmath where @xmath is
monotonic. If @xmath for some @xmath , then picking @xmath , we have
that @xmath , so that by pseudoconvexity, @xmath is minimized at @xmath
. This, however, is impossible since an odd degree polynomial is never
bounded below. Conversely, suppose @xmath can be represented as in Eq. (
2.29 ). Fix some @xmath , and define the polynomial @xmath . Since
@xmath , we have that either (i) @xmath is constant, or (ii) @xmath has
no real roots. Now if @xmath , then @xmath . Regardless of whether (i)
or (ii) holds, this implies that @xmath everywhere, so that @xmath or
@xmath . ∎

###### Corollary 2.23.

For any fixed odd degree @xmath , the pseudoconvexity of polynomials of
degree @xmath can be checked in polynomial time.

###### Proof.

This is a simple modification of our algorithm for testing
quasiconvexity (Theorem 2.21 ). The first step of the algorithm is in
fact identical: once we impose the additional requirement that the
nonzero component of @xmath with the smallest index should be equal to
one, we can uniquely determine the vector @xmath and the coefficients of
the univariate polynomial @xmath that satisfy Eq. ( 2.29 ) . (If we
fail, @xmath is not quasiconvex and hence also not pseudoconvex.) Once
we have @xmath , we can check whether @xmath has no real roots e.g. by
computing the signature of the Hermite form of @xmath ; see [ 24 ] .

∎

###### Remark 2.4.1.

Homogeneous polynomials of odd degree @xmath are never pseudoconvex. The
reason is that the gradient of these polynomials vanishes at the origin,
but yet the origin is not a global minimum since odd degree polynomials
are unbounded below.

#### 2.4.3 Degrees that are hard

The main result of this section is the following theorem.

###### Theorem 2.24.

It is NP-hard to check quasiconvexity/pseudoconvexity of degree four
polynomials. This is true even when the polynomials are restricted to be
homogeneous.

In view of Theorem 2.1 , which established NP-hardness of deciding
convexity of homogeneous quartic polynomials, Theorem 2.24 follows
immediately from the following result.

###### Theorem 2.25.

For a homogeneous polynomial @xmath of even degree @xmath , the notions
of convexity, pseudoconvexity, and quasiconvexity are all equivalent. ⁵
⁵ 5 The result is more generally true for differentiable functions that
are homogeneous of even degree. Also, the requirements of homogeneity
and having an even degree both need to be present. Indeed, @xmath and
@xmath are both quasiconvex but not convex, the first being homogeneous
of odd degree and the second being nonhomogeneous of even degree.

We start the proof of this theorem by first proving an easy lemma.

###### Lemma 2.26.

Let @xmath be a quasiconvex homogeneous polynomial of even degree @xmath
. Then @xmath is nonnegative.

###### Proof.

Suppose, to derive a contradiction, that there exist some @xmath and
@xmath such that @xmath . Then by homogeneity of even degree we must
have @xmath . On the other hand, homogeneity of @xmath implies that
@xmath . Since the origin is on the line between @xmath and @xmath ,
this shows that the sublevel set @xmath is not convex, contradicting the
quasiconvexity of @xmath . ∎

###### Proof of Theorem 2.25.

We show that a quasiconvex homogeneous polynomial of even degree is
convex. In view of implication ( 2.27 ), this proves the theorem.

Suppose that @xmath is a quasiconvex polynomial. Define @xmath . By
homogeneity, for any @xmath with @xmath , we have that

  -- -------- --
     @xmath   
  -- -------- --

By quasiconvexity, this implies that for any @xmath with @xmath , any
point on the line connecting @xmath and @xmath is in @xmath . In
particular, consider

  -- -------- --
     @xmath   
  -- -------- --

Because @xmath can be written as

  -- -------- --
     @xmath   
  -- -------- --

we have that @xmath , i.e., @xmath . By homogeneity, this inequality can
be restated as

  -- -------- --
     @xmath   
  -- -------- --

and therefore

  -- -------- -- --------
     @xmath      (2.30)
  -- -------- -- --------

where the last inequality is due to the convexity of @xmath .

Finally, note that for any polynomial @xmath , the set @xmath } is dense
in @xmath (here we again appeal to the fact that the only polynomial
that is zero on a ball of positive radius is the zero polynomial); and
since @xmath is nonnegative due to Lemma 2.26 , the set @xmath } is
dense in @xmath . Using the continuity of @xmath , it follows that Eq. (
2.30 ) holds not only when @xmath satisfy @xmath , but for all @xmath ,
@xmath . Appealing to the continuity of @xmath again, we see that for
all @xmath , @xmath , for all @xmath . This establishes that @xmath is
convex.

∎

###### Quasiconvexity/pseudoconvexity of polynomials of even degree
larger than four.

###### Corollary 2.27.

It is NP-hard to decide quasiconvexity of polynomials of any fixed even
degree @xmath .

###### Proof.

We have already proved the result for @xmath . To establish the result
for even degree @xmath , recall that we have established NP-hardness of
deciding convexity of homogeneous quartic polynomials. Given such a
quartic form @xmath , consider the polynomial

  -- -------- -- --------
     @xmath      (2.31)
  -- -------- -- --------

We claim that @xmath is quasiconvex if and only if @xmath is convex.
Indeed, if @xmath is convex, then obviously so is @xmath , and therefore
@xmath is quasiconvex. Conversely, if @xmath is not convex, then by
Theorem 2.25 , it is not quasiconvex. So, there exist points @xmath ,
with @xmath on the line connecting @xmath and @xmath , such that @xmath
, @xmath , but @xmath . Considering points @xmath , @xmath , @xmath , we
see that @xmath is not quasiconvex. It follows that it is NP-hard to
decide quasiconvexity of polynomials of even degree four or larger. ∎

###### Corollary 2.28.

It is NP-hard to decide pseudoconvexity of polynomials of any fixed even
degree @xmath .

###### Proof.

The proof is almost identical to the proof of Corollary 2.27 . Let
@xmath be defined as in ( 2.31 ). If @xmath is convex, then @xmath is
convex and hence also pseudoconvex. If @xmath is not convex, we showed
that @xmath is not quasiconvex and hence also not pseudoconvex. ∎

### 2.5 Summary and conclusions

In this chapter, we studied the computational complexity of testing
convexity and some of its variants, for polynomial functions. The
notions that we considered and the implications among them are
summarized below:

strong convexity @xmath strict convexity @xmath convexity @xmath
pseudoconvexity @xmath quasiconvexity.

Our complexity results as a function of the degree of the polynomial are
listed in Table 2.1 .

We gave polynomial time algorithms for checking pseudoconvexity and
quasiconvexity of odd degree polynomials that can be useful in many
applications. Our negative results, on the other hand, imply (under P
@xmath NP) the impossibility of a polynomial time (or even
pseudo-polynomial time) algorithm for testing any of the properties
listed in Table 2.1 for polynomials of even degree four or larger.
Although the implications of convexity are very significant in
optimization theory, our results suggest that unless additional
structure is present, ensuring the mere presence of convexity is likely
an intractable task. It is therefore natural to wonder whether there are
other properties of optimization problems that share some of the
attractive consequences of convexity, but are easier to check.

The hardness results of this chapter also lay emphasis on the need for
finding good approximation algorithms for recognizing convexity that can
deal with a large number of instances. This is our motivation for the
next chapter as we turn our attention to the study of algebraic
counterparts of convexity that can be efficiently checked with
semidefinite programming.

## Chapter 3 Convexity and SOS-Convexity

The overall contribution of this chapter is a complete characterization
of the containment of the sets of convex and sos-convex polynomials in
every degree and dimension. The content of this chapter is mostly based
on the work in [ 9 ] , but also includes parts of [ 11 ] and [ 2 ] .

### 3.1 Introduction

#### 3.1.1 Nonnegativity and sum of squares

One of the cornerstones of real algebraic geometry is Hilbert’s seminal
paper in 1888 [ 77 ] , where he gives a complete characterization of the
degrees and dimensions in which nonnegative polynomials can be written
as sums of squares of polynomials. In particular, Hilbert proves in [ 77
] that there exist nonnegative polynomials that are not sums of squares,
although explicit examples of such polynomials appeared only about 80
years later and the study of the gap between nonnegative and sums of
squares polynomials continues to be an active area of research to this
day.

Motivated by a wealth of new applications and a modern viewpoint that
emphasizes efficient computation, there has also been a great deal of
recent interest from the optimization community in the representation of
nonnegative polynomials as sums of squares (sos). Indeed, many
fundamental problems in applied and computational mathematics can be
reformulated as either deciding whether certain polynomials are
nonnegative or searching over a family of nonnegative polynomials. It is
well-known however that if the degree of the polynomial is four or
larger, deciding nonnegativity is an NP-hard problem. (As we mentioned
in the last chapter, this follows e.g. as an immediate corollary of
NP-hardness of deciding matrix copositivity [ 109 ] .) On the other
hand, it is also well-known that deciding whether a polynomial can be
written as a sum of squares can be reduced to solving a semidefinite
program, for which efficient algorithms e.g. based on interior point
methods is available. The general machinery of the so-called “sos
relaxation” has therefore been to replace the intractable nonnegativity
requirements with the more tractable sum of squares requirements that
obviously provide a sufficient condition for polynomial nonnegativity.

Some relatively recent applications that sum of squares relaxations have
found span areas as diverse as control theory [ 118 ] , [ 76 ] , quantum
computation [ 59 ] , polynomial games [ 120 ] , combinatorial
optimization [ 71 ] , geometric theorem proving [ 123 ] , and many
others.

#### 3.1.2 Convexity and sos-convexity

Aside from nonnegativity, convexity is another fundamental property of
polynomials that is of both theoretical and practical significance. In
the previous chapter, we already listed a number of applications of
establishing convexity of polynomials including global optimization,
convex envelope approximation, Lyapunov analysis, data fitting, defining
norms, etc. Unfortunately, however, we also showed that just like
nonnegativity, convexity of polynomials is NP-hard to decide for
polynomials of degree as low as four. Encouraged by the success of sum
of squares methods as a viable substitute for nonnegativity, our focus
in this chapter will be on the analogue of sum of squares for polynomial
convexity: a notion known as sos-convexity .

As we mentioned in our previous chapters in passing, sos-convexity
(which gets its name from the work of Helton and Nie in [ 75 ] ) is a
sufficient condition for convexity of polynomials based on an
appropriately defined sum of squares decomposition of the Hessian
matrix; see the equivalent Definitions 2.4 and 3.4 . The main
computational advantage of sos-convexity stems from the fact that the
problem of deciding whether a given polynomial is sos-convex amounts to
solving a single semidefinite program. We will explain how this is
exactly done in Section 3.2 of this chapter where we briefly review the
well-known connection between sum of squares decomposition and
semidefinite programming.

Besides its computational implications, sos-convexity is an appealing
concept since it bridges the geometric and algebraic aspects of
convexity. Indeed, while the usual definition of convexity is concerned
only with the geometry of the epigraph, in sos-convexity this geometric
property (or the nonnegativity of the Hessian) must be certified through
a “simple” algebraic identity, namely the sum of squares factorization
of the Hessian. The original motivation of Helton and Nie for defining
sos-convexity was in relation to the question of semidefinite
representability of convex sets [ 75 ] . But this notion has already
appeared in the literature in a number of other settings [ 89 ] , [ 90 ]
, [ 100 ] , [ 44 ] . In particular, there has been much recent interest
in the role of convexity in semialgebraic geometry [ 89 ] , [ 26 ] , [
55 ] , [ 91 ] and sos-convexity is a recurrent figure in this line of
research.

#### 3.1.3 Contributions and organization of this chapter

The main contribution of this chapter is to establish the counterpart of
Hilbert’s characterization of the gap between nonnegativity and sum of
squares for the notions of convexity and sos-convexity. We start by
presenting some background material in Section 3.2 . In Section 3.3 , we
prove an algebraic analogue of a classical result in convex analysis,
which provides three equivalent characterizations for sos-convexity
(Theorem 3.5 ). This result substantiates the fact that sos-convexity is
the right sos relaxation for convexity. In Section 3.4 , we present two
explicit examples of convex polynomials that are not sos-convex, one of
them being the first known such example. In Section 3.5 , we provide the
characterization of the gap between convexity and sos-convexity (Theorem
3.8 and Theorem 3.9 ). Subsection 3.5.1 includes the proofs of the cases
where convexity and sos-convexity are equivalent and Subsection 3.5.2
includes the proofs of the cases where they are not. In particular,
Theorem 3.16 and Theorem 3.17 present explicit examples of convex but
not sos-convex polynomials that have dimension and degree as low as
possible, and Theorem 3.18 provides a general construction for producing
such polynomials in higher degrees. Some concluding remarks and an open
problem are presented in Section 3.6 .

This chapter also includes two appendices. In Appendix A, we explain how
the first example of a convex but not sos-convex polynomial was found
with software using sum of squares programming techniques and the
duality theory of semidefinite optimization. As a byproduct of this
numerical procedure, we obtain a simple method for searching over a
restricted family of nonnegative polynomials that are not sums of
squares. In Appendix B, we give a formal (computer assisted) proof of
validity of one of our minimal convex but not sos-convex polynomials.

### 3.2 Preliminaries

#### 3.2.1 Background on nonnegativity and sum of squares

For the convenience of the reader, we recall some basic concepts from
the previous chapter and then introduce some new ones. We will be
concerned throughout this chapter with polynomials with real
coefficients. The ring of polynomials in @xmath variables with real
coefficients is denoted by @xmath . A polynomial @xmath is said to be
nonnegative or positive semidefinite (psd) if @xmath for all @xmath . We
say that @xmath is a sum of squares (sos) , if there exist polynomials
@xmath such that @xmath . We denote the set of psd (resp. sos)
polynomials in @xmath variables and degree @xmath by @xmath (resp.
@xmath ). Any sos polynomial is clearly psd, so we have @xmath . Recall
that a homogeneous polynomial (or a form ) is a polynomial where all the
monomials have the same degree. A form @xmath of degree @xmath is a
homogeneous function of degree @xmath since it satisfies @xmath for any
scalar @xmath . We say that a form @xmath is positive definite if @xmath
for all @xmath in @xmath . Following standard notation, we denote the
set of psd (resp. sos) homogeneous polynomials in @xmath variables and
degree @xmath by @xmath (resp. @xmath ). Once again, we have the obvious
inclusion @xmath . All of the four sets @xmath are closed convex cones.
The closedness of the sum of squares cone may not be so obvious. This
fact was first proved by Robinson [ 141 ] . We will make crucial use of
it in the proof of Theorem 3.5 in the next section.

Any form of degree @xmath in @xmath variables can be “dehomogenized”
into a polynomial of degree @xmath in @xmath variables by setting @xmath
. Conversely, any polynomial @xmath of degree @xmath in @xmath variables
can be “homogenized” into a form @xmath of degree @xmath in @xmath
variables, by adding a new variable @xmath , and letting

  -- -------- --
     @xmath   
  -- -------- --

The properties of being psd and sos are preserved under homogenization
and dehomogenization [ 138 ] .

A very natural and fundamental question that as we mentioned earlier was
answered by Hilbert is to understand in what dimensions and degrees
nonnegative polynomials (or forms) can be represented as sums of
squares, i.e, for what values of @xmath and @xmath we have @xmath or
@xmath . Note that because of the argument in the last paragraph, we
have @xmath if and only if @xmath . Hence, it is enough to answer the
question just for polynomials or just for forms and the answer to the
other one comes for free.

###### Theorem 3.1 (Hilbert, [77]).

@xmath if and only if @xmath or @xmath or @xmath . Equivalently, @xmath
if and only if @xmath or @xmath or @xmath .

The proofs of @xmath and @xmath are relatively simple and were known
before Hilbert. On the other hand, the proof of the fairly surprising
fact that @xmath (or equivalently @xmath ) is rather involved. We refer
the interested reader to [ 130 ] , [ 128 ] , [ 46 ] , and references in
[ 138 ] for some modern expositions and alternative proofs of this
result. Hilbert’s other main contribution was to show that these are the
only cases where nonnegativity and sum of squares are equivalent by
giving a nonconstructive proof of existence of polynomials in @xmath and
@xmath (or equivalently forms in @xmath and @xmath ). From this, it
follows with simple arguments that in all higher dimensions and degrees
there must also be psd but not sos polynomials; see [ 138 ] . Explicit
examples of such polynomials appeared in the 1960s starting from the
celebrated Motzkin form [ 107 ] :

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

which belongs to @xmath , and continuing a few years later with the
Robinson form [ 141 ] :

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

which belongs to @xmath .

Several other constructions of psd polynomials that are not sos have
appeared in the literature since. An excellent survey is [ 138 ] . See
also [ 139 ] and [ 27 ] .

#### 3.2.2 Connection to semidefinite programming and matrix
generalizations

As we remarked before, what makes sum of squares an appealing concept
from a computational viewpoint is its relation to semidefinite
programming. It is well-known (see e.g. [ 118 ] , [ 119 ] ) that a
polynomial @xmath in @xmath variables and of even degree @xmath is a sum
of squares if and only if there exists a positive semidefinite matrix
@xmath (often called the Gram matrix) such that

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the vector of monomials of degree up to @xmath

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

The set of all such matrices @xmath is the feasible set of a
semidefinite program (SDP). For fixed @xmath , the size of this
semidefinite program is polynomial in @xmath . Semidefinite programs can
be solved with arbitrary accuracy in polynomial time. There are several
implementations of semidefinite programming solvers, based on interior
point algorithms among others, that are very efficient in practice and
widely used; see [ 162 ] and references therein.

The notions of positive semidefiniteness and sum of squares of scalar
polynomials can be naturally extended to polynomial matrices, i.e.,
matrices with entries in @xmath . We say that a symmetric polynomial
matrix @xmath is positive semidefinite if @xmath is positive
semidefinite in the matrix sense for all @xmath , i.e, if @xmath has
nonnegative eigenvalues for all @xmath . It is straightforward to see
that this condition holds if and only if the polynomial @xmath in @xmath
variables @xmath is psd. A homogeneous polynomial matrix @xmath is said
to be positive definite, if it is positive definite in the matrix sense,
i.e., has positive eigenvalues, for all @xmath in @xmath . The
definition of an sos-matrix is as follows [ 88 ] , [ 62 ] , [ 152 ] .

###### Definition 3.2.

A symmetric polynomial matrix @xmath , @xmath is an sos-matrix if there
exists a polynomial matrix @xmath for some @xmath , such that @xmath .

It turns out that a polynomial matrix @xmath , @xmath is an sos-matrix
if and only if the scalar polynomial @xmath is a sum of squares in
@xmath ; see [ 88 ] . This is a useful fact because in particular it
gives us an easy way of checking whether a polynomial matrix is an
sos-matrix by solving a semidefinite program. Once again, it is obvious
that being an sos-matrix is a sufficient condition for a polynomial
matrix to be positive semidefinite.

#### 3.2.3 Background on convexity and sos-convexity

A polynomial @xmath is (globally) convex if for all @xmath and @xmath in
@xmath and all @xmath , we have

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

Since polynomials are continuous functions, the inequality in ( 3.4 )
holds if and only if it holds for a fixed value of @xmath , say, @xmath
. In other words, @xmath is convex if and only if

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

for all @xmath and @xmath ; see e.g. [ 148 , p. 71] . Recall from the
previous chapter that except for the trivial case of linear polynomials,
an odd degree polynomial is clearly never convex.

For the sake of direct comparison with a result that we derive in the
next section (Theorem 3.5 ), we recall next a classical result from
convex analysis on the first and second order characterization of
convexity. The proof can be found in many convex optimization textbooks,
e.g. [ 38 , p. 70] . The theorem is of course true for any twice
differentiable function, but for our purposes we state it for
polynomials.

###### Theorem 3.3.

Let @xmath be a polynomial. Let @xmath denote its gradient and let
@xmath be its Hessian, i.e., the @xmath symmetric matrix of second
derivatives. Then the following are equivalent.

(a) @xmath ; (i.e., @xmath is convex).

(b) @xmath

(c) @xmath ; (i.e., @xmath is a positive semidefinite polynomial
matrix).

Helton and Nie proposed in [ 75 ] the notion of sos-convexity as an sos
relaxation for the second order characterization of convexity (condition
(c) above).

###### Definition 3.4.

A polynomial @xmath is sos-convex if its Hessian @xmath is an
sos-matrix.

With what we have discussed so far, it should be clear that
sos-convexity is a sufficient condition for convexity of polynomials
that can be checked with semidefinite programming. In the next section,
we will show some other natural sos relaxations for polynomial
convexity, which will turn out to be equivalent to sos-convexity.

We end this section by introducing some final notation: @xmath and
@xmath will respectively denote the set of convex and sos-convex
polynomials in @xmath variables and degree @xmath ; @xmath and @xmath
will respectively denote set of convex and sos-convex homogeneous
polynomials in @xmath variables and degree @xmath . Again, these four
sets are closed convex cones and we have the obvious inclusions @xmath
and @xmath .

### 3.3 Equivalent algebraic relaxations for convexity of polynomials

An obvious way to formulate alternative sos relaxations for convexity of
polynomials is to replace every inequality in Theorem 3.3 with its sos
version. In this section we examine how these relaxations relate to each
other. We also comment on the size of the resulting semidefinite
programs.

Our result below can be thought of as an algebraic analogue of Theorem
3.3 .

###### Theorem 3.5.

Let @xmath be a polynomial of degree @xmath in @xmath variables with its
gradient and Hessian denoted respectively by @xmath and @xmath . Let
@xmath , @xmath , and @xmath be defined as

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

Then the following are equivalent:

(a) @xmath is sos ¹ ¹ 1 The constant @xmath in @xmath of condition (a)
is arbitrary and is chosen for convenience. One can show that @xmath
being sos implies that @xmath is sos for any fixed @xmath . Conversely,
if @xmath is sos for some @xmath , then @xmath is sos. The proofs are
similar to the proof of (a) @xmath (b) . .

(b) @xmath is sos.

(c) @xmath is sos; (i.e., @xmath is an sos-matrix).

###### Proof.

(a) @xmath (b) : Assume @xmath is sos. We start by proving that @xmath
will also be sos for any integer @xmath . A little bit of
straightforward algebra yields the relation

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

The second term on the right hand side of ( 3.7 ) is always sos because
@xmath is sos. Hence, this relation shows that for any @xmath , if
@xmath is sos, then so is @xmath . Since for @xmath , both terms on the
right hand side of ( 3.7 ) are sos by assumption, induction immediately
gives that @xmath is sos for all @xmath .

Now, let us rewrite @xmath as

  -- -------- --
     @xmath   
  -- -------- --

We have

  -- -------- -- -------
     @xmath      (3.8)
  -- -------- -- -------

Next, we take the limit of both sides of ( 3.8 ) by letting @xmath as
@xmath . Because @xmath is differentiable, the right hand side of ( 3.8
) will converge to @xmath . On the other hand, our preceding argument
implies that @xmath is an sos polynomial (of degree @xmath in @xmath
variables) for any @xmath . Moreover, as @xmath goes to zero, the
coefficients of @xmath remain bounded since the limit of this sequence
is @xmath , which must have bounded coefficients (see ( 3.6 )). By
closedness of the sos cone, we conclude that the limit @xmath must be
sos.

(b) @xmath (a) : Assume @xmath is sos. It is easy to check that

  -- -------- --
     @xmath   
  -- -------- --

and hence @xmath is sos.

(b) @xmath (c) : Let us write the second order Taylor approximation of
@xmath around @xmath :

  -- -------- --
     @xmath   
  -- -------- --

After rearranging terms, letting @xmath (for @xmath ), and dividing both
sides by @xmath we get:

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

The left hand side of ( 3.9 ) is @xmath and therefore for any fixed
@xmath , it is an sos polynomial by assumption. As we take @xmath , by
closedness of the sos cone, the left hand side of ( 3.9 ) converges to
an sos polynomial. On the other hand, as the limit is taken, the term
@xmath vanishes and hence we have that @xmath must be sos.

(c) @xmath (b) : Following the strategy of the proof of the classical
case in [ 160 , p. 165] , we start by writing the Taylor expansion of
@xmath around @xmath with the integral form of the remainder:

  -- -------- -- --------
     @xmath      (3.10)
  -- -------- -- --------

Since @xmath is sos by assumption, for any @xmath the integrand

  -- -------- --
     @xmath   
  -- -------- --

is an sos polynomial of degree @xmath in @xmath and @xmath . From ( 3.10
) we have

  -- -------- --
     @xmath   
  -- -------- --

It then follows that @xmath is sos because integrals of sos polynomials,
if they exist, are sos.

∎

We conclude that conditions (a) , (b) , and (c) are equivalent
sufficient conditions for convexity of polynomials, and can each be
checked with a semidefinite program as explained in Subsection 3.2.2 .
It is easy to see that all three polynomials @xmath , @xmath , and
@xmath are polynomials in @xmath variables and of degree @xmath . (Note
that each differentiation reduces the degree by one.) Each of these
polynomials have a specific structure that can be exploited for
formulating smaller SDPs. For example, the symmetries @xmath and @xmath
can be taken advantage of via symmetry reduction techniques developed in
[ 62 ] .

The issue of symmetry reduction aside, we would like to point out that
formulation (c) (which was the original definition of sos-convexity) can
be significantly more efficient than the other two conditions. The
reason is that the polynomial @xmath is always quadratic and homogeneous
in @xmath and of degree @xmath in @xmath . This makes @xmath much more
sparse than @xmath and @xmath , which have degree @xmath both in @xmath
and in @xmath . Furthermore, because of the special bipartite structure
of @xmath , only monomials of the form @xmath will appear in the vector
of monomials ( 3.3 ). This in turn reduces the size of the Gram matrix,
and hence the size of the SDP. It is perhaps not too surprising that the
characterization of convexity based on the Hessian matrix is a more
efficient condition to check. After all, this is a local condition
(curvature at every point in every direction must be nonnegative),
whereas conditions (a) and (b) are both global.

###### Remark 3.3.1.

There has been yet another proposal for an sos relaxation for convexity
of polynomials in [ 44 ] . However, we have shown in [ 8 ] that the
condition in [ 44 ] is at least as conservative as the three conditions
in Theorem 3.5 and also significantly more expensive to check.

###### Remark 3.3.2.

Just like convexity, the property of sos-convexity is preserved under
restrictions to affine subspaces. This is perhaps most directly seen
through characterization (a) of sos-convexity in Theorem 3.5 , by also
noting that sum of squares is preserved under restrictions. Unlike
convexity however, if a polynomial is sos-convex on every line (or even
on every proper affine subspace), this does not imply that the
polynomial is sos-convex.

As an application of Theorem 3.5 , we use our new characterization of
sos-convexity to give a short proof of an interesting lemma of Helton
and Nie.

###### Lemma 3.6.

(Helton and Nie [ 75 , Lemma 8] ) . Every sos-convex form is sos.

###### Proof.

Let @xmath be an sos-convex form of degree @xmath . We know from Theorem
3.5 that sos-convexity of @xmath is equivalent to the polynomial @xmath
being sos. But since sos is preserved under restrictions and @xmath ,
this implies that

  -- -------- --
     @xmath   
  -- -------- --

is sos. ∎

Note that the same argument also shows that convex forms are psd.

### 3.4 Some constructions of convex but not sos-convex polynomials

It is natural to ask whether sos-convexity is not only a sufficient
condition for convexity of polynomials but also a necessary one. In
other words, could it be the case that if the Hessian of a polynomial is
positive semidefinite, then it must factor? To give a negative answer to
this question, one has to prove existence of a convex polynomial that is
not sos-convex, i.e, a polynomial @xmath for which one (and hence all)
of the three polynomials @xmath and @xmath in ( 3.6 ) are psd but not
sos. Note that existence of psd but not sos polynomials does not imply
existence of convex but not sos-convex polynomials on its own. The
reason is that the polynomials @xmath and @xmath all possess a very
special structure. ² ² 2 There are many situations where requiring a
specific structure on polynomials makes psd equivalent to sos. As an
example, we know that there are forms in @xmath . However, if we require
the forms to have only even monomials, then all such nonnegative forms
in 4 variables and degree 4 are sums of squares [ 57 ] . For example,
@xmath has the structure of being quadratic in @xmath and a Hessian in
@xmath . (Not every polynomial matrix is a valid Hessian.) The Motzkin
or the Robinson polynomials in ( 3.1 ) and ( 3.2 ) for example are
clearly not of this structure.

#### 3.4.1 The first example

In [ 11 ] , [ 7 ] , we presented the first example of a convex
polynomial that is not sos-convex ³ ³ 3 Assuming P @xmath NP, and given
the NP-hardness of deciding polynomial convexity proven in the previous
chapter, one would expect to see convex polynomials that are not
sos-convex. However, we found the first such polynomial before we had
proven the NP-hardness result. Moreover, from complexity considerations,
even assuming P @xmath NP, one cannot conclude existence of convex but
not sos-convex polynomials for any fixed finite value of the number of
variables @xmath . :

  -- -------- -- --------
     @xmath      (3.11)
  -- -------- -- --------

As we will see later in this chapter, this form which lives in @xmath
turns out to be an example in the smallest possible number of variables
but not in the smallest degree.

In Appendix A, we will explain how the polynomial in ( 3.11 ) was found.
The proof that this polynomial is convex but not sos-convex is omitted
and can be found in [ 11 ] . However, we would like to highlight an idea
behind this proof that will be used again in this chapter. As the
following lemma demonstrates, one way to ensure a polynomial is not
sos-convex is by enforcing one of the principal minors of its Hessian
matrix to be not sos .

###### Lemma 3.7.

If @xmath is an sos-matrix, then all its @xmath principal minors ⁴ ⁴ 4
The principal minors of an @xmath matrix @xmath are the determinants of
all @xmath ( @xmath ) sub-blocks whose rows and columns come from the
same index set @xmath . are sos polynomials. In particular, @xmath and
the diagonal elements of @xmath must be sos polynomials.

###### Proof.

We first prove that @xmath is sos. By Definition 3.2 , we have @xmath
for some @xmath polynomial matrix @xmath . If @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

and the result is immediate. If @xmath , the result follows from the
Cauchy-Binet formula ⁵ ⁵ 5 Given matrices @xmath and @xmath of size
@xmath and @xmath respectively, the Cauchy-Binet formula states that

@xmath

where @xmath is a subset of @xmath with @xmath elements, @xmath denotes
the @xmath matrix whose columns are the columns of @xmath with index
from @xmath , and similarly @xmath denotes the @xmath matrix whose rows
are the rows of @xmath with index from @xmath . . We have

  -- -------- --
     @xmath   
  -- -------- --

Finally, when @xmath , @xmath is zero which is trivially sos. In fact,
the Cauchy-Binet formula also holds for @xmath and @xmath , but we have
separated these cases for clarity of presentation.

Next, we need to prove that the minors corresponding to smaller
principal blocks of @xmath are also sos. Define @xmath and let @xmath
and @xmath be nonempty subsets of @xmath . Denote by @xmath a sub-block
of @xmath with row indices from @xmath and column indices from @xmath .
It is easy to see that

  -- -------- --
     @xmath   
  -- -------- --

Therefore, @xmath is an sos-matrix itself. By the proceeding argument
@xmath must be sos, and hence all the principal minors are sos. ∎

###### Remark 3.4.1.

Interestingly, the converse of Lemma 3.7 does not hold. A counterexample
is the Hessian of the form @xmath in ( 3.15 ) that we will present in
the next section. All 7 principal minors of the @xmath Hessian this form
are sos polynomials, even though the Hessian is not an sos-matrix. This
is in contrast with the fact that a polynomial matrix is positive
semidefinite if and only if all its principal minors are psd
polynomials. The latter statement follows immediately from the
well-known fact that a constant matrix is positive semidefinite if and
only if all its principal minors are nonnegative.

#### 3.4.2 A “clean” example

We next present another example of a convex but not sos-convex form
whose construction is in fact related to our proof of NP-hardness of
deciding convexity of quartic forms from Chapter 2 . The example is in
@xmath and by contrast to the example of the previous subsection, it
will turn out to be minimal in the degree but not in the number of
variables. What is nice about this example is that unlike the other
examples in this chapter it has not been derived with the assistance of
a computer and semidefinite programming:

  -- -------- -- --------
     @xmath      (3.12)
  -- -------- -- --------

The proof that this polynomial is convex but not sos-convex can be
extracted from Theorems 2.3 and 2.5 of Chapter 2 . The reader can
observe that these two theorems put together give us a general procedure
for producing convex but not sos-convex quartic forms from any example
of a psd but not sos biquadratic form ⁶ ⁶ 6 The reader can refer to
Definition 2.2 of the previous chapter to recall the definition of a
biquadratic form. . The biquadratic form that has led to the form above
is that of Choi in [ 45 ] .

The example in ( 3.12 ) also shows that convex forms that possess strong
symmetry properties can still fail to be sos-convex. The symmetries in
this form are inherited from the rich symmetry structure of the
biquadratic form of Choi (see [ 62 ] ). In general, symmetries are of
interest in the study of positive semidefinite and sums of squares
polynomials because the gap between psd and sos can often behave very
differently depending on the symmetry properties; see e.g. [ 28 ] .

### 3.5 Characterization of the gap between convexity and sos-convexity

Now that we know there exist convex polynomials that are not sos-convex,
our final and main goal is to give a complete characterization of the
degrees and dimensions in which such polynomials can exist. This is
achieved in the next theorem.

###### Theorem 3.8.

@xmath if and only if @xmath or @xmath or @xmath .

We would also like to have such a characterization for homogeneous
polynomials. Although convexity is a property that is in some sense more
meaningful for nonhomogeneous polynomials than for forms, one motivation
for studying convexity of forms is in their relation to norms [ 140 ] .
Also, in view of the fact that we have a characterization of the gap
between nonnegativity and sums of squares both for polynomials and for
forms, it is very natural to inquire the same result for convexity and
sos-convexity. The next theorem presents this characterization for
forms.

###### Theorem 3.9.

@xmath if and only if @xmath or @xmath or @xmath .

The result @xmath of this theorem is to be presented in full detail in [
2 ] . The remainder of this chapter is solely devoted to the proof of
Theorem 3.8 and the proof of Theorem 3.9 except for the case @xmath .
Before we present these proofs, we shall make two important remarks.

###### Remark 3.5.1.

Difficulty with homogenization and dehomogenization. Recall from
Subsection 3.2.1 and Theorem 3.1 that characterizing the gap between
nonnegativity and sum of squares for polynomials is equivalent to
accomplishing this task for forms. Unfortunately, the situation is more
complicated for convexity and sos-convexity and that is the reason why
we are presenting Theorems 3.8 and 3.9 as separate theorems. The
difficulty arises from the fact that unlike nonnegativity and sum of
squares, convexity and sos-convexity are not always preserved under
homogenization. (Or equivalently, the properties of being not convex and
not sos-convex are not preserved under dehomogenization.) In fact, any
convex polynomial that is not psd will no longer be convex after
homogenization. This is because convex forms are psd but the
homogenization of a non-psd polynomial is a non-psd form. Even if a
convex polynomial is psd, its homogenization may not be convex. For
example the univariate polynomial @xmath is convex and psd, but its
homogenization @xmath is not convex. ⁷ ⁷ 7 What is true however is that
a nonnegative form of degree @xmath is convex if and only if the @xmath
-th root of its dehomogenization is a convex function [ 140 , Prop. 4.4]
. To observe the same phenomenon for sos-convexity, consider the
trivariate form @xmath in ( 3.11 ) which is convex but not sos-convex
and define @xmath . Then, one can check that @xmath is sos-convex (i.e.,
its @xmath Hessian factors) even though its homogenization which is
@xmath is not sos-convex [ 11 ] .

###### Remark 3.5.2.

Resemblance to the result of Hilbert. The reader may have noticed from
the statements of Theorem 3.1 and Theorems 3.8 and 3.9 that the cases
where convex polynomials (forms) are sos-convex are exactly the same
cases where nonnegative polynomials are sums of squares! We shall
emphasize that as far as we can tell, our results do not follow (except
in the simplest cases) from Hilbert’s result stated in Theorem 3.1 .
Note that the question of convexity or sos-convexity of a polynomial
@xmath in @xmath variables and degree @xmath is about the polynomials
@xmath or @xmath defined in ( 3.6 ) being psd or sos. Even though these
polynomials still have degree @xmath , it is important to keep in mind
that they are polynomials in @xmath variables . Therefore, there is no
direct correspondence with the characterization of Hilbert. To make this
more explicit, let us consider for example one particular claim of
Theorem 3.9 : @xmath . For a form @xmath in 2 variables and degree 4,
the polynomials @xmath and @xmath will be forms in 4 variables and
degree 4. We know from Hilbert’s result that in this situation psd but
not sos forms do in fact exist. However, for the forms in 4 variables
and degree 4 that have the special structure of @xmath or @xmath , psd
turns out to be equivalent to sos.

The proofs of Theorems 3.8 and 3.9 are broken into the next two
subsections. In Subsection 3.5.1 , we provide the proofs for the cases
where convexity and sos-convexity are equivalent. Then in Subsection
3.5.2 , we prove that in all other cases there exist convex polynomials
that are not sos-convex.

#### 3.5.1 Proofs of Theorems 3.8 and 3.9: cases where @xmath

When proving equivalence of convexity and sos-convexity, it turns out to
be more convenient to work with the second order characterization of
sos-convexity, i.e., with the form @xmath in ( 3.6 ). The reason for
this is that this form is always quadratic in @xmath , and this allows
us to make use of the following key theorem, henceforth referred to as
the “biform theorem”.

###### Theorem 3.10 (e.g. [47]).

Let @xmath be a form in the variables @xmath and @xmath that is a
quadratic form in @xmath for fixed @xmath and a form (of however large
degree) in @xmath for fixed @xmath . Then @xmath is psd if and only if
it is sos. ⁸ ⁸ 8 Note that the results @xmath and @xmath are both
special cases of this theorem.

The biform theorem has been proven independently by several authors. See
[ 47 ] and [ 20 ] for more background on this theorem and in particular
[ 47 , Sec. 7] for a an elegant proof and some refinements. We now
proceed with our proofs which will follow in a rather straightforward
manner from the biform theorem.

###### Theorem 3.11.

@xmath for all @xmath . @xmath for all @xmath .

###### Proof.

For a univariate polynomial, convexity means that the second derivative,
which is another univariate polynomial, is psd. Since @xmath , the
second derivative must be sos. Therefore, @xmath . To prove @xmath ,
suppose we have a convex bivariate form @xmath of degree @xmath in
variables @xmath . The Hessian @xmath of @xmath is a @xmath matrix whose
entries are forms of degree @xmath . If we let @xmath , convexity of
@xmath implies that the form @xmath is psd. Since @xmath meets the
requirements of the biform theorem above with @xmath and @xmath , it
follows that @xmath is sos. Hence, @xmath is sos-convex. ∎

###### Theorem 3.12.

@xmath for all @xmath . @xmath for all @xmath .

###### Proof.

Let @xmath and @xmath . Let @xmath be a quadratic polynomial. The
Hessian of @xmath in this case is the constant symmetric matrix @xmath .
Convexity of @xmath implies that @xmath is psd. But since @xmath ,
@xmath must be sos. Hence, @xmath is sos-convex. The proof of @xmath is
identical. ∎

###### Theorem 3.13.

@xmath .

###### Proof.

Let @xmath be a convex bivariate quartic polynomial. Let @xmath denote
the Hessian of @xmath and let @xmath . Note that @xmath is a @xmath
matrix whose entries are (not necessarily homogeneous) quadratic
polynomials. Since @xmath is convex, @xmath is psd. Let @xmath be a
@xmath matrix whose entries are obtained by homogenizing the entries of
@xmath . It is easy to see that @xmath is then the form obtained by
homogenizing @xmath and is therefore psd. Now we can employ the biform
theorem (Theorem 3.10 ) with @xmath and @xmath to conclude that @xmath
is sos. But upon dehomogenizing by setting @xmath , we conclude that
@xmath is sos. Hence, @xmath is sos-convex. ∎

###### Theorem 3.14 (Ahmadi, Blekherman, Parrilo [2]).

@xmath .

Unlike Hilbert’s results @xmath and @xmath which are equivalent
statements and essentially have identical proofs, the proof of @xmath is
considerably more involved than the proof of @xmath . Here, we briefly
point out why this is the case and refer the reader to [ 2 ] for more
details.

If @xmath is a ternary quartic form, its Hessian @xmath is a @xmath
matrix whose entries are quadratic forms. In this case, we can no longer
apply the biform theorem to the form @xmath . In fact, the matrix

  -- -------- -- --------
     @xmath      (3.13)
  -- -------- -- --------

due to Choi [ 45 ] serves as an explicit example of a @xmath matrix with
quadratic form entries that is positive semidefinite but not an
sos-matrix; i.e., the biquadratic form @xmath is psd but not sos.
However, the matrix @xmath above is not a valid Hessian, i.e., it cannot
be the matrix of the second derivatives of any polynomial. If this was
the case, the third partial derivatives would commute. On the other
hand, we have in particular

  -- -------- --
     @xmath   
  -- -------- --

A biquadratic Hessian form is a biquadratic form @xmath where @xmath is
the Hessian of some quartic form. Biquadratic Hessian forms satisfy a
special symmetry property. Let us call a biquadratic form @xmath
symmetric if it satisfies the symmetry relation @xmath . It is an easy
exercise to show that biquadratic Hessian forms satisfy @xmath and are
therefore symmetric biquadratic forms. This symmetry property is a
rather strong condition that is not satisfied e.g. by the Choi
biquadratic form @xmath in ( 3.13 ).

A simple dimension counting argument shows that the vector space of
biquadratic forms, symmetric biquadratic forms, and biquadratic Hessian
forms in variables @xmath respectively have dimensions @xmath , @xmath ,
and @xmath . Since the symmetry requirement drops the dimension of the
space of biquadratic forms significantly, and since sos polynomials are
known to generally cover much larger volume in the set of psd
polynomials in presence of symmetries (see e.g. [ 28 ] ), one may
initially suspect (as we did) that the equivalence between psd and sos
ternary Hessian biquadratic forms is a consequence of the symmetry
property. Our next theorem shows that interestingly enough this is not
the case.

###### Theorem 3.15.

There exist symmetric biquadratic forms in two sets of three variables
that are positive semidefinite but not a sum of squares.

###### Proof.

We claim that the following biquadratic form has the required
properties:

  -- -------- -- --------
     @xmath      (3.14)
  -- -------- -- --------

The fact that @xmath can readily be seen from the order in which we have
written the monomials. The proof that @xmath is psd but not sos is given
in [ 2 ] and omitted from here. ∎

In view of the above theorem, it is rather remarkable that all positive
semidefinite biquadratic Hessian forms in @xmath turn out to be sums of
squares, i.e., that @xmath .

#### 3.5.2 Proofs of Theorems 3.8 and 3.9: cases where @xmath

The goal of this subsection is to establish that the cases presented in
the previous subsection are the only cases where convexity and
sos-convexity are equivalent. We will first give explicit examples of
convex but not sos-convex polynomials/forms that are “minimal” jointly
in the degree and dimension and then present an argument for all
dimensions and degrees higher than those of the minimal cases.

##### Minimal convex but not sos-convex polynomials/forms

The minimal examples of convex but not sos-convex polynomials (resp.
forms) turn out to belong to @xmath and @xmath (resp. @xmath and @xmath
). Recall from Remark 3.5.1 that we lack a general argument for going
from convex but not sos-convex forms to polynomials or vice versa.
Because of this, one would need to present four different polynomials in
the sets mentioned above and prove that each polynomial is (i) convex
and (ii) not sos-convex. This is a total of eight arguments to make
which is quite cumbersome. However, as we will see in the proof of
Theorem 3.16 and 3.17 below, we have been able to find examples that act
“nicely” with respect to particular ways of dehomogenization. This will
allow us to reduce the total number of claims we have to prove from
eight to four.

The polynomials that we are about to present next have been found with
the assistance of a computer and by employing some ‘‘tricks’’ with
semidefinite programming similar to those presented in Appendix A. ⁹ ⁹ 9
The approach of Appendix A, however, does not lead to examples that are
minimal. But the idea is similar. In this process, we have made use of
software packages YALMIP [ 98 ] , SOSTOOLS [ 132 ] , and the SDP solver
SeDuMi [ 157 ] , which we acknowledge here. To make the chapter
relatively self-contained and to emphasize the fact that using rational
sum of squares certificates one can make such computer assisted proofs
fully formal, we present the proof of Theorem 3.16 below in the Appendix
B. On the other hand, the proof of Theorem 3.17 , which is very similar
in style to the proof of Theorem 3.16 , is largely omitted to save
space. All of the proofs are available in electronic form and in their
entirety at http://aaa.lids.mit.edu/software .

###### Theorem 3.16.

@xmath is a proper subset of @xmath . @xmath is a proper subset of
@xmath .

###### Proof.

We claim that the form

  -- -------- -- --------
     @xmath      (3.15)
  -- -------- -- --------

belongs to @xmath , and the polynomial ¹⁰ ¹⁰ 10 The polynomial @xmath
turns out to be sos-convex, and therefore does not do the job. One can
of course change coordinates, and then in the new coordinates perform
the dehomogenization by setting @xmath .

  -- -------- -- --------
     @xmath      (3.16)
  -- -------- -- --------

belongs to @xmath . Note that since convexity and sos-convexity are both
preserved under restrictions to affine subspaces (recall Remark 3.3.2 ),
it suffices to show that the form @xmath in ( 3.15 ) is convex and the
polynomial @xmath in ( 3.16 ) is not sos-convex. Let @xmath , @xmath ,
@xmath , @xmath , and denote the Hessian of @xmath and @xmath
respectively by @xmath and @xmath . In Appendix B, we provide rational
Gram matrices which prove that the form

  -- -------- -- --------
     @xmath      (3.17)
  -- -------- -- --------

is sos. This, together with nonnegativity of @xmath and continuity of
@xmath , implies that @xmath is psd. Therefore, @xmath is convex. The
proof that @xmath is not sos-convex proceeds by showing that @xmath is
not an sos-matrix via a separation argument. In Appendix B, we present a
separating hyperplane that leaves the appropriate sos cone on one side
and the polynomial

  -- -------- -- --------
     @xmath      (3.18)
  -- -------- -- --------

on the other. ∎

###### Theorem 3.17.

@xmath is a proper subset of @xmath . @xmath is a proper subset of
@xmath .

###### Proof.

We claim that the form

  -- -- -- --------
           (3.19)
  -- -- -- --------

belongs to @xmath , and the polynomial

  -- -------- -- --------
     @xmath      (3.20)
  -- -------- -- --------

belongs to @xmath . Once again, it suffices to prove that @xmath is
convex and @xmath is not sos-convex. Let @xmath , @xmath , and denote
the Hessian of @xmath and @xmath respectively by @xmath and @xmath . The
proof that @xmath is convex is done by showing that the form

  -- -------- -- --------
     @xmath      (3.21)
  -- -------- -- --------

is sos. ¹¹ ¹¹ 11 The choice of multipliers in ( 3.17 ) and ( 3.21 ) is
motivated by a result of Reznick in [ 137 ] explained in Appendix A. The
proof that @xmath is not sos-convex is done again by means of a
separating hyperplane. ∎

##### Convex but not sos-convex polynomials/forms in all higher degrees
and dimensions

Given a convex but not sos-convex polynomial (form) in @xmath variables
, it is very easy to argue that such a polynomial (form) must also exist
in a larger number of variables. If @xmath is a form in @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

belongs to @xmath . Convexity of @xmath is obvious since it is a sum of
convex functions. The fact that @xmath is not sos-convex can also easily
be seen from the block diagonal structure of the Hessian of @xmath : if
the Hessian of @xmath were to factor, it would imply that the Hessian of
@xmath should also factor. The argument for going from @xmath to @xmath
is identical.

Unfortunately, an argument for increasing the degree of convex but not
sos-convex forms seems to be significantly more difficult to obtain. In
fact, we have been unable to come up with a natural operation that would
produce a from in @xmath from a form in @xmath . We will instead take a
different route: we are going to present a general procedure for going
from a form in @xmath to a form in @xmath . This will serve our purpose
of constructing convex but not sos-convex forms in higher degrees and is
perhaps also of independent interest in itself. For instance, it can be
used to construct convex but not sos-convex forms that inherit
structural properties (e.g. symmetry) of the known examples of psd but
not sos forms. The procedure is constructive modulo the value of two
positive constants ( @xmath and @xmath below) whose existence will be
shown nonconstructively.

Although the proof of the general case is no different, we present this
construction for the case @xmath . The reason is that it suffices for us
to construct forms in @xmath for @xmath even and @xmath . These forms
together with the two forms in @xmath and @xmath presented in ( 3.15 )
and ( 3.19 ), and with the simple procedure for increasing the number of
variables cover all the values of @xmath and @xmath for which convex but
not sos-convex forms exist.

For the remainder of this section, let @xmath and @xmath .

###### Theorem 3.18.

Let @xmath be a ternary form of degree @xmath (with @xmath necessarily
even and @xmath ) satisfying the following three requirements:

  R1:  

    @xmath is positive definite.

  R2:  

    @xmath is not a sum of squares.

  R3:  

     The Hessian @xmath of @xmath is positive definite at the point
    @xmath .

Let @xmath be any bivariate form of degree @xmath whose Hessian is
positive definite.
Then, there exists a constant @xmath , such that the form @xmath of
degree @xmath given by

  -- -------- -- --------
     @xmath      (3.22)
  -- -------- -- --------

is convex but not sos-convex.

Before we prove this theorem, let us comment on how one can get examples
of forms @xmath and @xmath that satisfy the requirements of the theorem.
The choice of @xmath is in fact very easy. We can e.g. take

  -- -------- --
     @xmath   
  -- -------- --

which has a positive definite Hessian. As for the choice of @xmath ,
essentially any psd but not sos ternary form can be turned into a form
that satisfies requirements R1 , R2 , and R3 . Indeed if the Hessian of
such a form is positive definite at just one point, then that point can
be taken to @xmath by a change of coordinates without changing the
properties of being psd and not sos. If the form is not positive
definite, then it can made so by adding a small enough multiple of a
positive definite form to it. For concreteness, we construct in the next
lemma a family of forms that together with the above theorem will give
us convex but not sos-convex ternary forms of any degree @xmath .

###### Lemma 3.19.

For any even degree @xmath , there exists a constant @xmath , such that
the form

  -- -------- -- --------
     @xmath      (3.23)
  -- -------- -- --------

satisfies the requirements R1 , R2 , and R3 of Theorem 3.18 .

###### Proof.

The form

  -- -------- --
     @xmath   
  -- -------- --

is the familiar Motzkin form in ( 3.1 ) that is psd but not sos [ 107 ]
. For any even degree @xmath , the form

  -- -------- --
     @xmath   
  -- -------- --

is a form of degree @xmath that is clearly still psd and less obviously
still not sos; see [ 138 ] . This together with the fact that @xmath is
a closed cone implies existence of a small positive value of @xmath for
which the form @xmath in ( 3.23 ) is positive definite but not a sum of
squares, hence satisfying requirements R1 and R2 .

Our next claim is that for any positive value of @xmath , the Hessian
@xmath of the form @xmath in ( 3.23 ) satisfies

  -- -------- -- --------
     @xmath      (3.24)
  -- -------- -- --------

for some positive constants @xmath , therefore also passing requirement
R3 . To see the above equality, first note that since @xmath is a form
of degree @xmath , its Hessian @xmath will have entries that are forms
of degree @xmath . Therefore, the only monomials that can survive in
this Hessian after setting @xmath and @xmath to zero are multiples of
@xmath . It is easy to see that an @xmath monomial in an off-diagonal
entry of @xmath would lead to a monomial in @xmath that is not even. On
the other hand, the form @xmath in ( 3.23 ) only has even monomials.
This explains why the off-diagonal entries of the right hand side of (
3.24 ) are zero. Finally, we note that for any positive value of @xmath
, the form @xmath in ( 3.23 ) includes positive multiples of @xmath ,
@xmath , and @xmath , which lead to positive multiples of @xmath on the
diagonal of @xmath . Hence, @xmath , and @xmath are positive. ∎

Next, we state a lemma that will be employed in the proof of Theorem
3.18 .

###### Lemma 3.20.

Let @xmath be a trivariate form satisfying the requirements R1 and R3 of
Theorem 3.18 . Let @xmath denote the Hessian of the form @xmath . Then,
there exists a positive constant @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

on the set

  -- -------- -- --------
     @xmath      (3.25)
  -- -------- -- --------

###### Proof.

We observe that when @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

which by requirement R1 is positive when @xmath . By continuity of the
form @xmath , we conclude that there exists a small positive constant
@xmath such that @xmath on the set

  -- -------- --
     @xmath   
  -- -------- --

Next, we leave it to the reader to check that

  -- -------- --
     @xmath   
  -- -------- --

Therefore, when @xmath , requirement R3 implies that @xmath is positive
when @xmath . Appealing to continuity again, we conclude that there
exists a small positive constant @xmath such that @xmath on the set

  -- -------- --
     @xmath   
  -- -------- --

If we now take @xmath , the lemma is established. ∎

We are now ready to prove Theorem 3.18 .

###### Proof of Theorem 3.18.

We first prove that the form @xmath in ( 3.22 ) is not sos-convex. By
Lemma 3.7 , if @xmath was sos-convex, then all diagonal elements of its
Hessian would have to be sos polynomials. On the other hand, we have
from ( 3.22 ) that

  -- -------- --
     @xmath   
  -- -------- --

which by requirement R2 is not sos. Therefore @xmath is not sos-convex.

It remains to show that there exists a positive value of @xmath for
which @xmath becomes convex. Let us denote the Hessians of @xmath ,
@xmath , and @xmath , by @xmath , @xmath , and @xmath respectively. So,
we have

  -- -------- --
     @xmath   
  -- -------- --

(Here, @xmath is a @xmath matrix whose first row and column are zeros.)
Convexity of @xmath is of course equivalent to nonnegativity of the form
@xmath . Since this form is bi-homogeneous in @xmath and @xmath , it is
nonnegative if and only if @xmath on the bi-sphere

  -- -------- --
     @xmath   
  -- -------- --

Let us decompose the bi-sphere as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is defined in ( 3.25 ) and

  -- -------- --
     @xmath   
  -- -------- --

Lemma 3.20 together with positive definiteness of @xmath imply that
@xmath is positive on @xmath . As for the set @xmath , let

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

By the assumption of positive definiteness of @xmath , we have @xmath .
If we now let

  -- -------- --
     @xmath   
  -- -------- --

then

  -- -------- --
     @xmath   
  -- -------- --

Hence @xmath is nonnegative (in fact positive) everywhere on @xmath and
the proof is completed. ∎

Finally, we provide an argument for existence of bivariate polynomials
of degree @xmath that are convex but not sos-convex.

###### Corollary 3.21.

Consider the form @xmath in ( 3.22 ) constructed as described in Theorem
3.18 . Let

  -- -------- --
     @xmath   
  -- -------- --

Then, @xmath is convex but not sos-convex.

###### Proof.

The polynomial @xmath is convex because it is the restriction of a
convex function. It is not difficult to see that

  -- -------- --
     @xmath   
  -- -------- --

which is not sos. Therefore from Lemma 3.7 @xmath is not sos-convex. ∎

Corollary 3.21 together with the two polynomials in @xmath and @xmath
presented in ( 3.16 ) and ( 3.20 ), and with the simple procedure for
increasing the number of variables described at the beginning of
Subsection 3.5.2 cover all the values of @xmath and @xmath for which
convex but not sos-convex polynomials exist.

### 3.6 Concluding remarks and an open problem

A summary of the results of this chapter is given in Figure 3.1 . To
conclude, we would like to point out some similarities between
nonnegativity and convexity that deserve attention: (i) both
nonnegativity and convexity are properties that only hold for even
degree polynomials, (ii) for quadratic forms, nonnegativity is in fact
equivalent to convexity, (iii) both notions are NP-hard to check exactly
for degree 4 and larger, and most strikingly (iv) nonnegativity is
equivalent to sum of squares exactly in dimensions and degrees where
convexity is equivalent to sos-convexity. It is unclear to us whether
there can be a deeper and more unifying reason explaining these
observations, in particular, the last one which was the main result of
this chapter.

Another intriguing question is to investigate whether one can give a
direct argument proving the fact that @xmath if and only if @xmath .
This would eliminate the need for studying polynomials and forms
separately, and in particular would provide a short proof of the result
@xmath given in [ 2 ] .

Finally, an open problem related to the work in this chapter is to find
an explicit example of a convex form that is not a sum of squares .
Blekherman [ 26 ] has shown via volume arguments that for degree @xmath
and asymptotically for large @xmath such forms must exist, although no
examples are known. In particular, it would interesting to determine the
smallest value of @xmath for which such a form exists. We know from
Lemma 3.6 that a convex form that is not sos must necessarily be not
sos-convex. Although our several constructions of convex but not
sos-convex polynomials pass this necessary condition, the polynomials
themselves are all sos. The question is particularly interesting from an
optimization viewpoint because it implies that the well-known sum of
squares relaxation for minimizing polynomials [ 155 ] , [ 124 ] may not
be exact even for the easy case of minimizing convex polynomials.

### 3.7 Appendix A: How the first convex but not sos-convex polynomial
was found

In this appendix, we explain how the polynomial in ( 3.11 ) was found by
solving a carefully designed sos-program ¹² ¹² 12 The term “sos-program”
is usually used to refer to semidefinite programs that have sum of
squares constraints. . The simple methodology described here allows one
to search over a restricted family of nonnegative polynomials that are
not sums of squares. The procedure can potentially be useful in many
different settings and this is our main motivation for presenting this
appendix.

Our goal is to find a polynomial @xmath whose Hessian @xmath satisfies:

  -- -------- -- --------
     @xmath      (3.26)
  -- -------- -- --------

Unfortunately, a constraint of type ( 3.26 ) that requires a polynomial
to be psd but not sos is a non-convex constraint and cannot be easily
handled with sos-programming. This is easy to see from a geometric
viewpoint. The feasible set of an sos-program, being a semidefinite
program, is always a convex set. On the other hand, for a fixed degree
and dimension, the set of psd polynomials that are not sos is
non-convex. Nevertheless, we describe a technique that allows one to
search over a convex subset of the set of psd but not sos polynomials
using sos-programming. Our strategy can simply be described as follows:
(i) Impose the constraint that the polynomial should not be sos by using
a separating hyperplane (dual functional) for the sos cone. (ii) Impose
the constraint that the polynomial should be psd by requiring that the
polynomial times a nonnegative multiplier is sos.

By definition, the dual cone @xmath of the sum of squares cone @xmath is
the set of all linear functionals @xmath that take nonnegative values on
it, i.e,

  -- -------- --
     @xmath   
  -- -------- --

Here, the dual space @xmath denotes the space of all linear functionals
on the space @xmath of forms in @xmath variables and degree @xmath , and
@xmath represents the pairing between elements of the primal and the
dual space. If a form is not sos, we can find a dual functional @xmath
that separates it from the closed convex cone @xmath . The basic idea
behind this is the well known separating hyperplane theorem in convex
analysis; see e.g. [ 38 , 142 ] .

As for step (ii) of our strategy above, our approach for guaranteeing
that of a form @xmath is nonnegative will be to require @xmath be sos
for some integer @xmath . Our choice of the multiplier @xmath as opposed
to any other psd multiplier is motivated by a result of Reznick [ 137 ]
on Hilbert’s 17th problem. The 17th problem, which was answered in the
affirmative by Artin [ 19 ] , asks whether every psd form must be a sum
of squares of rational functions. The affirmative answer to this
question implies that if a form @xmath is psd, then there must exist an
sos form @xmath , such that @xmath is sos. Reznick showed in [ 137 ]
that if @xmath is positive definite, one can always take @xmath , for
sufficiently large @xmath . For all polynomials that we needed prove psd
in this chapter, taking @xmath has been good enough.

For our particular purpose of finding a convex but not sos-convex
polynomial, we apply the strategy outlined above to make the first
diagonal element of the Hessian psd but not sos (recall Lemma 3.7 ).
More concretely, the polynomial in ( 3.11 ) was derived from a feasible
solution to the following sos-program:

-   Parameterize @xmath and compute its Hessian @xmath .

-   Impose the constraints

      -- -------- -- --------
         @xmath      (3.27)
      -- -------- -- --------

      -- -------- -- --------
         @xmath      (3.28)
      -- -------- -- --------

    (for some dual functional @xmath ).

The decision variables of this sos-program are the coefficients of the
polynomial @xmath that also appear in the entries of the Hessian matrix
@xmath . (The polynomial @xmath in ( 3.28 ) denotes the first diagonal
element of @xmath .) The dual functional @xmath must be fixed a priori
as explained in the sequel. Note that all the constraints are linear in
the decision variables and indeed the feasible set described by these
constraints is a convex set. Moreover, the reader should be convinced by
now that if the above sos-program is feasible, then the solution @xmath
is a convex polynomial that is not sos-convex.

The reason why we chose to parameterize @xmath as a form in @xmath is
that a minimal case where a diagonal element of the Hessian (which has
@xmath fewer degree) can be psd but not sos is among the forms in @xmath
. The role of the dual functional @xmath in ( 3.28 ) is to separate the
polynomial @xmath from @xmath . Once an ordering on the monomials of
@xmath is fixed, this constraint can be imposed numerically as

  -- -------- -- --------
     @xmath      (3.29)
  -- -------- -- --------

where @xmath denotes the vector of coefficients of the polynomial @xmath
and @xmath represents our separating hyperplane, which must be computed
prior to solving the above sos-program.

There are several ways to obtain a separating hyperplane for @xmath .
Our approach was to find a hyperplane that separates the Motzkin form
@xmath in ( 3.1 ) from @xmath . This can be done in at least a couple of
different ways. For example, we can formulate a semidefinite program
that requires the Motzkin form to be sos. This program is clearly
infeasible. Any feasible solution to its dual semidefinite program will
give us the desired separating hyperplane. Alternatively, we can set up
an sos-program that finds the Euclidean projection @xmath of the Motzkin
form @xmath onto the cone @xmath . Since the projection is done onto a
convex set, the hyperplane tangent to @xmath at @xmath will be
supporting @xmath , and can serve as our separating hyperplane.

To conclude, we remark that in contrast to previous techniques of
constructing examples of psd but not sos polynomials that are usually
based on some obstructions associated with the number of zeros of
polynomials (see e.g. [ 138 ] ), our approach has the advantage that the
resulting polynomials are positive definite. Furthermore, additional
linear or semidefinite constraints can easily be incorporated in the
search process to impose e.g. various symmetry or sparsity patterns on
the polynomial of interest.

### 3.8 Appendix B: Certificates complementing the proof of Theorem 3.16

Let @xmath , @xmath , @xmath , @xmath , and let @xmath and @xmath be as
in the proof of Theorem 3.16 . This appendix proves that the form @xmath
in ( 3.17 ) is sos and that the polynomial @xmath in ( 3.18 ) is not
sos, hence proving respectively that @xmath is convex and @xmath is not
sos-convex.

A rational sos decomposition of @xmath , which is a form in @xmath
variables of degree @xmath , is as follows:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the vector of monomials

  -- -------- --
     @xmath   
  -- -------- --

and @xmath is the @xmath positive definite matrix ¹³ ¹³ 13 Whenever we
state a matrix is positive definite, this claim is backed up by a
rational @xmath factorization of the matrix that the reader can find
online at http://aaa.lids.mit.edu/software . presented on the next page

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

Next, we prove that the polynomial @xmath in ( 3.18 ) is not sos. Let us
first present this polynomial and give it a name:

  -- -------- --
     @xmath   
  -- -------- --

Note that @xmath is a polynomial in @xmath variables of degree @xmath
that is quadratic in @xmath . Let us denote the cone of sos polynomials
in @xmath variables @xmath that have degree @xmath and are quadratic in
@xmath by @xmath , and its dual cone by @xmath . Our proof will simply
proceed by presenting a dual functional @xmath that takes a negative
value on the polynomial @xmath . We fix the following ordering of
monomials in what follows:

  -- -------- -- --------
     @xmath      (3.30)
  -- -------- -- --------

Let @xmath represent the vector of coefficients of @xmath ordered
according to the list of monomials above; i.e., @xmath . Using the same
ordering, we can represent our dual functional @xmath with the vector

  -- -------- --
     @xmath   
  -- -------- --

We have

  -- -------- --
     @xmath   
  -- -------- --

On the other hand, we claim that @xmath ; i.e., for any form @xmath , we
should have

  -- -------- -- --------
     @xmath      (3.31)
  -- -------- -- --------

where @xmath here denotes the coefficients of @xmath listed according to
the ordering in ( 3.30 ). Indeed, if @xmath is sos, then it can be
written in the form

  -- -------- --
     @xmath   
  -- -------- --

for some symmetric positive semidefinite matrix @xmath , and a vector of
monomials

  -- -------- --
     @xmath   
  -- -------- --

It is not difficult to see that

  -- -------- -- --------
     @xmath      (3.32)
  -- -------- -- --------

where by @xmath we mean a matrix where each monomial in @xmath is
replaced with the corresponding element of the vector @xmath . This
yields the matrix

  -- -------- --
     @xmath   
  -- -------- --

which is positive definite. Therefore, equation ( 3.32 ) along with the
fact that @xmath is positive semidefinite implies that ( 3.31 ) holds.
This completes the proof.

\sfbHuge

Part II:

Lyapunov Analysis and Computation

## Chapter 4 Lyapunov Analysis of Polynomial Differential Equations

In the last two chapters of this thesis, our focus will turn to Lyapunov
analysis of dynamical systems. The current chapter presents new results
on Lyapunov analysis of polynomial vector fields. The content here is
based on the works in [ 10 ] and [ 4 ] , as well as some more recent
results.

### 4.1 Introduction

We will be concerned for the most part of this chapter with a continuous
time dynamical system

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

where @xmath is a polynomial and has an equilibrium at the origin, i.e.,
@xmath . Arguably, the class of polynomial differential equations are
among the most widely encountered in engineering and sciences. For
stability analysis of these systems, it is most common (and quite
natural) to search for Lyapunov functions that are polynomials
themselves. When such a candidate Lyapunov function is used, then
conditions of Lyapunov’s theorem reduce to a set of polynomial
inequalities. For instance, if establishing global asymptotic stability
of the origin is desired, one would require a radially unbounded
polynomial Lyapunov candidate @xmath to vanish at the origin and satisfy

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (4.2)
     @xmath   @xmath   @xmath      (4.3)
  -- -------- -------- -------- -- -------

Here, @xmath denotes the time derivative of @xmath along the
trajectories of ( 4.1 ), @xmath is the gradient vector of @xmath , and
@xmath is the standard inner product in @xmath . In some other variants
of the analysis problem, e.g. if LaSalle’s invariance principle is to be
used, or if the goal is to prove boundedness of trajectories of ( 4.1 ),
then the inequality in ( 4.3 ) is replaced with

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

In any case, the problem arising from this analysis approach is that
even though polynomials of a given degree are finitely parameterized,
the computational problem of searching for a polynomial @xmath
satisfying inequalities of the type ( 4.2 ), ( 4.3 ), ( 4.4 ) is
intractable. An approach pioneered in [ 118 ] and widely popular by now
is to replace the positivity (or nonnegativity) conditions by the
requirement of the existence of a sum of squares (sos) decomposition:

  -- -------- -------- -- -- -------
     @xmath                  (4.5)
     @xmath   @xmath         (4.6)
  -- -------- -------- -- -- -------

As we saw in the previous chapter, sum of squares decomposition is a
sufficient condition for polynomial nonnegativity that can be
efficiently checked with semidefinite programming. For a fixed degree of
a polynomial Lyapunov candidate @xmath , the search for the coefficients
of @xmath subject to the constraints ( 4.5 ) and ( 4.6 ) is a
semidefinite program (SDP). We call a Lyapunov function satisfying both
sos conditions in ( 4.5 ) and ( 4.6 ) a sum of squares Lyapunov function
. We emphasize that this is the sensible definition of a sum of squares
Lyapunov function and not what the name may suggest, which is a Lyapunov
function that is a sum of squares. Indeed, the underlying semidefinite
program will find a Lyapunov function @xmath if and only if @xmath
satisfies both conditions ( 4.5 ) and ( 4.6 ).

Over the last decade, the applicability of sum of squares Lyapunov
functions has been explored and extended in many directions and a
multitude of sos techniques have been developed to tackle a range of
problems in systems and control. We refer the reader to the by no means
exhaustive list of works [ 76 ] , [ 41 ] , [ 43 ] , [ 83 ] , [ 131 ] , [
114 ] , [ 133 ] , [ 42 ] , [ 6 ] , [ 21 ] , [ 159 ] and references
therein. Despite the wealth of research in this area, the converse
question of whether the existence of a polynomial Lyapunov function
implies the existence of a sum of squares Lyapunov function has remained
elusive. This question naturally comes in two variants:

Problem 1: Does existence of a polynomial Lyapunov function of a given
degree imply existence of a polynomial Lyapunov function of the same
degree that satisfies the sos conditions in ( 4.5 ) and ( 4.6 )?

Problem 2: Does existence of a polynomial Lyapunov function of a given
degree imply existence of a polynomial Lyapunov function of possibly
higher degree that satisfies the sos conditions in ( 4.5 ) and ( 4.6 )?

The notion of stability of interest in this chapter, for which we will
study the questions above, is global asymptotic stability (GAS); see
e.g. [ 86 , Chap. 4] for a precise definition. Of course, a fundamental
question that comes before the problems mentioned above is the
following:

Problem 0: If a polynomial dynamical system is globally asymptotically
stable, does it admit a polynomial Lyapunov function?

#### 4.1.1 Contributions and organization of this chapter

In this chapter, we give explicit counterexamples that answer Problem 0
and Problem 1 in the negative. This is done in Section 4.3 and
Subsection 4.4.2 respectively. On the other hand, in Subsection 4.4.3 ,
we give a positive answer to Problem 2 for the case where the vector
field is homogeneous (Theorem 4.8 ) or when it is planar and an
additional mild assumption is met (Theorem 4.10 ). The proofs of these
two theorems are quite simple and rely on powerful Positivstellensatz
results due to Scheiderer (Theorems 4.7 and 4.9 ). In Section 4.5 , we
extend these results to derive a converse sos Lyapunov theorem for
robust stability of switched linear systems. It will be proven that if
such a system is stable under arbitrary switching, then it admits a
common polynomial Lyapunov function that is sos and that the negative of
its derivative is also sos (Theorem 4.11 ). We also show that for
switched linear systems (both in discrete and continuous time), if the
inequality on the decrease condition of a Lyapunov function is satisfied
as a sum of squares, then the Lyapunov function itself is automatically
a sum of squares (Propositions 4.14 and 4.15 ). We list a number of
related open problems in Section 4.6 .

Before these contributions are presented, we establish a hardness result
for the problem of deciding asymptotic stability of cubic homogeneous
vector fields in the next section. We also present some byproducts of
this result, including a Lyapunov-inspired technique for proving
positivity of forms.

### 4.2 Complexity considerations for deciding stability of polynomial
vector fields

It is natural to ask whether stability of equilibrium points of
polynomial vector fields can be decided in finite time. In fact, this is
a well-known question of Arnold that appears in [ 17 ] :

-   ‘‘Is the stability problem for stationary points algorithmically
    decidable? The well-known Lyapounov theorem ¹ ¹ 1 The theorem that
    Arnold is referring to here is the indirect method of Lyapunov
    related to linearization. This is not to be confused with Lyapunov’s
    direct method (or the second method), which is what we are concerned
    with in sections that follow. solves the problem in the absence of
    eigenvalues with zero real parts. In more complicated cases, where
    the stability depends on higher order terms in the Taylor series,
    there exists no algebraic criterion.

    Let a vector field be given by polynomials of a fixed degree, with
    rational coefficients. Does an algorithm exist, allowing to decide,
    whether the stationary point is stable?”

Later in [ 51 ] , the question of Arnold is quoted with more detail:

-   “In my problem the coefficients of the polynomials of known degree
    and of a known number of variables are written on the tape of the
    standard Turing machine in the standard order and in the standard
    representation. The problem is whether there exists an algorithm (an
    additional text for the machine independent of the values of the
    coefficients) such that it solves the stability problem for the
    stationary point at the origin (i.e., always stops giving the answer
    “stable” or “unstable”).

    I hope, this algorithm exists if the degree is one. It also exists
    when the dimension is one. My conjecture has always been that there
    is no algorithm for some sufficiently high degree and dimension,
    perhaps for dimension @xmath and degree @xmath or even @xmath . I am
    less certain about what happens in dimension @xmath . Of course the
    nonexistence of a general algorithm for a fixed dimension working
    for arbitrary degree or for a fixed degree working for an arbitrary
    dimension, or working for all polynomials with arbitrary degree and
    dimension would also be interesting.”

To our knowledge, there has been no formal resolution to these
questions, neither for the case of stability in the sense of Lyapunov,
nor for the case of asymptotic stability (in its local or global
version). In [ 51 ] , da Costa and Doria show that if the right hand
side of the differential equation contains elementary functions (sines,
cosines, exponentials, absolute value function, etc.), then there is no
algorithm for deciding whether the origin is stable or unstable. They
also present a dynamical system in [ 52 ] where one cannot decide
whether a Hopf bifurcation will occur or whether there will be parameter
values such that a stable fixed point becomes unstable. In earlier work,
Arnold himself demonstrates some of the difficulties that arise in
stability analysis of polynomial systems by presenting a parametric
polynomial system in @xmath variables and degree @xmath , where the
boundary between stability and instability in parameter space is not a
semialgebraic set [ 16 ] . A relatively larger number of undecidability
results are available for questions related to other properties of
polynomial vector fields, such as reachability [ 73 ] or boundedness of
domain of definition [ 65 ] , or for questions about stability of hybrid
systems [ 30 ] , [ 35 ] , [ 34 ] , [ 29 ] . We refer the interested
reader to the survey papers in [ 37 ] , [ 73 ] , [ 156 ] , [ 33 ] , [ 36
] .

We are also interested to know whether the answer to the undecidability
question for asymptotic stability changes if the dynamics is restricted
to be homogeneous. A polynomial vector field @xmath is homogeneous if
all entries of @xmath are homogeneous polynomials of the same degree.
Homogeneous systems are extensively studied in the literature on
nonlinear control [ 149 ] , [ 14 ] , [ 68 ] , [ 23 ] , [ 74 ] , [ 146 ]
, [ 106 ] , and some of the results of this chapter (both negative and
positive) are derived specifically for this class of systems. A basic
fact about homogeneous vector fields is that for these systems the
notions of local and global stability are equivalent. Indeed, a
homogeneous vector field of degree @xmath satisfies @xmath for any
scalar @xmath , and therefore the value of @xmath on the unit sphere
determines its value everywhere. It is also well-known that an
asymptotically stable homogeneous system admits a homogeneous Lyapunov
funciton [ 72 ] , [ 146 ] .

Naturally, questions regarding complexity of deciding asymptotic
stability and questions about existence of Lyapunov functions are
related. For instance, if one proves that for a class of polynomial
vector fields, asymptotic stability implies existence of a polynomial
Lyapunov function together with a computable upper bound on its degree,
then the question of asymptotic stability for that class becomes
decidable. This is due to the fact that given any polynomial system and
any integer @xmath , the question of deciding whether the system admits
a polynomial Lyapunov function of degree @xmath can be answered in
finite time using quantifier elimination.

For the case of linear systems (i.e., homogeneous systems of degree
@xmath ), the situation is particularly nice. If such a system is
asymptotically stable, then there always exists a quadratic Lyapunov
function. Asymptotic stability of a linear system @xmath is equivalent
to the easily checkable algebraic criterion that the eigenvalues of
@xmath be in the open left half complex plane. Deciding this property of
the matrix @xmath can formally be done in polynomial time, e.g. by
solving a Lyapunov equation [ 36 ] .

Moving up in the degree, it is not difficult to show that if a
homogeneous polynomial vector field has even degree, then it can never
be asymptotically stable; see e.g. [ 72 , p. 283] . So the next
interesting case occurs for homogeneous vector fields of degree @xmath .
We will prove below that determining asymptotic stability for such
systems is strongly NP-hard. This gives a lower bound on the complexity
of this problem. It is an interesting open question to investigate
whether in this specific setting, the problem is also undecidable.

One implication of our NP-hardness result is that unless P=NP, we should
not expect sum of squares Lyapunov functions of “low enough” degree to
always exist, even when the analysis is restricted to cubic homogeneous
vector fields. The semidefinite program arising from a search for an sos
Lyapunov function of degree @xmath for such a vector field in @xmath
variables has size in the order of @xmath . This number is polynomial in
@xmath for fixed @xmath (but exponential in @xmath when @xmath grows
linearly in @xmath ). Therefore, unlike the case of linear systems, we
should not hope to have a bound on the degree of sos Lyapunov functions
that is independent of the dimension.

We postpone our study of existence of sos Lyapunov functions to Section
4.4 and proceed for now with the following complexity result.

###### Theorem 4.1.

Deciding asymptotic stability of homogeneous cubic polynomial vector
fields is strongly NP-hard.

The main intuition behind the proof of this theorem is the following
idea: We will relate the solution of a combinatorial problem not to the
behavior of the trajectories of a cubic vector field that are hard to
get a handle on, but instead to properties of a Lyapunov function that
proves asymptotic stability of this vector field. As we will see
shortly, insights from Lyapunov theory make the proof of this theorem
quite simple. The reduction is broken into two steps:

ONE-IN-THREE 3SAT

@xmath

positivity of quartic forms

@xmath

asymptotic stability of cubic vector fields

In the course of presenting these reductions, we will also discuss some
corollaries that are not directly related to our study of asymptotic
stability, but are of independent interest.

#### 4.2.1 Reduction from ONE-IN-THREE 3SAT to positivity of quartic
forms

As we remarked in Chapter 2 , NP-hardness of deciding nonnegativity
(i.e., positive semidefiniteness) of quartic forms is well-known. The
proof commonly cited in the literature is based on a reduction from the
matrix copositivity problem [ 109 ] : given a symmetric @xmath matrix
@xmath , decide whether @xmath for all @xmath ’s that are elementwise
nonnegative. Clearly, a matrix @xmath is copositive if and only if the
quartic form @xmath , with @xmath , is nonnegative. The original
reduction [ 109 ] proving NP-hardness of testing matrix copositivity is
from the subset sum problem and only establishes weak NP-hardness.
However, reductions from the stable set problem to matrix copositivity
are also known [ 56 ] , [ 58 ] and they result in NP-hardness in the
strong sense. Alternatively, strong NP-hardness of deciding
nonnegativity of quartic forms follows immediately from NP-hardness of
deciding convexity of quartic forms (proven in Chapter 2 ) or from
NP-hardness of deciding nonnegativity of biquadratic forms (proven in [
97 ] ).

For reasons that will become clear shortly, we are interested in showing
hardness of deciding positive definiteness of quartic forms as opposed
to positive semidefiniteness. This is in some sense even easier to
accomplish. A very straightforward reduction from 3SAT proves
NP-hardness of deciding positive definiteness of polynomials of degree
@xmath . By using ONE-IN-THREE 3SAT instead, we will reduce the degree
of the polynomial from @xmath to @xmath .

###### Proposition 4.2.

It is strongly ² ² 2 Just like our results in Chapter 2 , the
NP-hardness results of this section will all be in the strong sense.
From here on, we will drop the prefix “strong” for brevity. NP-hard to
decide whether a homogeneous polynomial of degree @xmath is positive
definite.

###### Proof.

We give a reduction from ONE-IN-THREE 3SAT which is known to be
NP-complete [ 61 , p. 259] . Recall that in ONE-IN-THREE 3SAT, we are
given a 3SAT instance (i.e., a collection of clauses, where each clause
consists of exactly three literals, and each literal is either a
variable or its negation) and we are asked to decide whether there
exists a @xmath assignment to the variables that makes the expression
true with the additional property that each clause has exactly one true
literal.

To avoid introducing unnecessary notation, we present the reduction on a
specific instance. The pattern will make it obvious that the general
construction is no different. Given an instance of ONE-IN-THREE 3SAT,
such as the following

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

we define the quartic polynomial @xmath as follows:

  -- -------- -- -------
     @xmath      (4.8)
  -- -------- -- -------

Having done so, our claim is that @xmath for all @xmath (or generally
for all @xmath ) if and only if the ONE-IN-THREE 3SAT instance is not
satisfiable. Note that @xmath is a sum of squares and therefore
nonnegative. The only possible locations for zeros of @xmath are by
construction among the points in @xmath . If there is a satisfying
Boolean assignment @xmath to ( 4.7 ) with exactly one true literal per
clause, then @xmath will vanish at point @xmath . Conversely, if there
are no such satisfying assignments, then for any point in @xmath , at
least one of the terms in ( 4.8 ) will be positive and hence @xmath will
have no zeros.

It remains to make @xmath homogeneous. This can be done via introducing
a new scalar variable @xmath . If we let

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

then we claim that @xmath (which is a quartic form) is positive definite
if and only if @xmath constructed as in ( 4.8 ) has no zeros. ³ ³ 3 In
general, homogenization does not preserve positivity. For example, as
shown in [ 138 ] , the polynomial @xmath has no zeros, but its
homogenization @xmath has zeros at the points @xmath and @xmath .
Nevertheless, positivity is preserved under homogenization for the
special class of polynomials constructed in this reduction, essentially
because polynomials of type ( 4.8 ) have no zeros at infinity. Indeed,
if @xmath has a zero at a point @xmath , then that zero is inherited by
@xmath at the point @xmath . If @xmath has no zeros, then ( 4.9 ) shows
that @xmath can only possibly have zeros at points with @xmath .
However, from the structure of @xmath in ( 4.8 ) we see that

  -- -------- --
     @xmath   
  -- -------- --

which cannot be zero (except at the origin). This concludes the proof. ∎

We present a simple corollary of the reduction we just gave on a problem
that is of relevance in polynomial integer programming. ⁴ ⁴ 4 We are
thankful to Amitabh Basu and Jesús De Loera for raising this question
during a visit at UC Davis, and for later insightful discussions. Recall
from Chapter 2 (Definition 2.7 ) that a basic semialgebraic set is a set
defined by a finite number of polynomial inequalities:

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

###### Corollary 4.3.

Given a basic semialgebraic set, it is NP-hard to decide if the set
contains a lattice point, i.e., a point with integer coordinates. This
is true even when the set is defined by one constraint ( @xmath ) and
the defining polynomial has degree @xmath .

###### Proof.

Given an instance of ONE-IN-THREE 3SAT, we define a polynomial @xmath of
degree @xmath as in ( 4.8 ), and let the basic semialgebraic set be
given by

  -- -------- --
     @xmath   
  -- -------- --

Then, by Proposition 4.2 , if the ONE-IN-THREE 3SAT instance is not
satisfiable, the set @xmath is empty and hence has no lattice points.
Conversely, if the instance is satisfiable, then @xmath contains at
least one point belonging to @xmath and therefore has a lattice point. ∎

By using the celebrated result on undecidability of checking existence
of integer solutions to polynomial equations (Hilbert’s 10th problem),
one can show that the problem considered in the corollary above is in
fact undecidable [ 129 ] . The same is true for quadratic integer
programming when both the dimension @xmath and the number of constraints
@xmath are allowed to grow as part of the input [ 84 ] . The question of
deciding existence of lattice points in polyhedra (i.e., the case where
degree of @xmath in ( 4.10 ) is @xmath for all @xmath ) is also
interesting and in fact very well-studied. For polyhedra, if both @xmath
and @xmath are allowed to grow, then the problem is NP-hard. This can be
seen e.g. as a corollary of the NP-hardness of the INTEGER KNAPSACK
problem (though this is NP-hardness in the weak sense); see [ 61 , p.
247] . However, if @xmath is fixed and @xmath grows, it follows from a
result of Lenstra [ 94 ] that the problem can be decided in polynomial
time. The same is true if @xmath is fixed and @xmath grows [ 153 , Cor.
18.7c] . See also [ 115 ] .

#### 4.2.2 Reduction from positivity of quartic forms to asymptotic
stability of cubic vector fields

We now present the second step of the reduction and finish the proof of
Theorem 4.1 .

###### Proof of Theorem 4.1.

We give a reduction from the problem of deciding positive definiteness
of quartic forms, whose NP-hardness was established in Proposition 4.2 .
Given a quartic form @xmath , we define the polynomial vector field

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

Note that the vector field is homogeneous of degree @xmath . We claim
that the above vector field is (locally or equivalently globally)
asymptotically stable if and only if @xmath is positive definite. First,
we observe that by construction

  -- -------- -- --------
     @xmath      (4.12)
  -- -------- -- --------

Suppose @xmath is positive definite. By Euler’s identity for homogeneous
functions, ⁵ ⁵ 5 Euler’s identity is easily derived by differentiating
both sides of the equation @xmath with respect to @xmath and setting
@xmath . we have @xmath Therefore, positive definiteness of @xmath
implies that @xmath cannot vanish anywhere except at the origin. Hence,
@xmath for all @xmath . In view of Lyapunov’s theorem (see e.g. [ 86 ,
p. 124] ), and the fact that a positive definite homogeneous function is
radially unbounded, it follows that the system in ( 4.11 ) is globally
asymptotically stable.

For the converse direction, suppose ( 4.11 ) is GAS. Our first claim is
that global asymptotic stability together with @xmath implies that
@xmath must be positive semidefinite. This follows from the following
simple argument, which we have also previously presented in [ 12 ] for a
different purpose. Suppose for the sake of contradiction that for some
@xmath and some @xmath we had @xmath . Consider a trajectory @xmath of
system ( 4.11 ) that starts at initial condition @xmath , and let us
evaluate the function @xmath on this trajectory. Since @xmath and @xmath
, we have @xmath for all @xmath . However, this contradicts the fact
that by global asymptotic stability, the trajectory must go to the
origin, where @xmath , being a form, vanishes.

To prove that @xmath is positive definite, suppose by contradiction that
for some nonzero point @xmath we had @xmath . Since we just proved that
@xmath has to be positive semidefinite, the point @xmath must be a
global minimum of @xmath . Therefore, as a necessary condition of
optimality, we should have @xmath . But this contradicts the system in (
4.11 ) being GAS, since the trajectory starting at @xmath stays there
forever and can never go to the origin. ∎

Perhaps of independent interest, the reduction we just gave suggests a
method for proving positive definiteness of forms. Given a form @xmath ,
we can construct a dynamical system as in ( 4.11 ), and then any method
that we may have for proving stability of vector fields (e.g. the use of
various kinds of Lyapunov functions) can serve as an algorithm for
proving positivity of @xmath . In particular, if we use a polynomial
Lyapunov function @xmath to prove stability of the system in ( 4.11 ),
we get the following corollary.

###### Corollary 4.4.

Let @xmath and @xmath be two forms of possibly different degree. If
@xmath is positive definite, and @xmath is positive definite, then
@xmath is positive definite.

One interesting fact about this corollary is that its algebraic version
with sum of squares replaced for positivity is not true. In other words,
we can have @xmath sos (and positive definite), @xmath sos (and positive
definite), but @xmath not sos. This gives us a way of proving positivity
of some polynomials that are not sos, using only sos certificates. Given
a form @xmath , since the expression @xmath is linear in the
coefficients of @xmath , we can use sos programming to search for a form
@xmath that satisfies @xmath sos and @xmath sos, and this would prove
positivity of @xmath . The following example demonstrates the potential
usefulness of this approach.

###### Example 4.2.1.

Consider the following form of degree @xmath :

  -- -------- -- --------
     @xmath      (4.13)
  -- -------- -- --------

One can check that this polynomial is not a sum of squares. (In fact,
this is the Motzkin form presented in equation ( 3.1 ) of Chapter 3
slightly perturbed.) On the other hand, we can use YALMIP [ 98 ]
together with the SDP solver SeDuMi [ 157 ] to search for a form @xmath
satisfying

  -- -------- -- --------
     @xmath      (4.14)
  -- -------- -- --------

If we parameterize @xmath as a quadratic form, no feasible solution will
be returned form the solver. However, when we increase the degree of
@xmath from @xmath to @xmath , the solver returns the following
polynomial

  -- -------- --
     @xmath   
  -- -------- --

that satisfies both sos constrains in ( 4.14 ). The Gram matrices in
these sos decompositions are positive definite. Therefore, @xmath and
@xmath are positive definite forms. Hence, by Corollary 4.4 , we have a
proof that @xmath in ( 4.13 ) is positive definite. @xmath

Interestingly, approaches of this type that use gradient information for
proving positivity of polynomials with sum of squares techniques have
been studied by Nie, Demmel, and Sturmfels in [ 113 ] , though the
derivation there is not Lyapunov-inspired.

### 4.3 Non-existence of polynomial Lyapunov functions

As we mentioned at the beginning of this chapter, the question of global
asymptotic stability of polynomial vector fields is commonly addressed
by seeking a Lyapunov function that is polynomial itself. This approach
has become further prevalent over the past decade due to the fact that
we can use sum of squares techniques to algorithmically search for such
Lyapunov functions. The question therefore naturally arises as to
whether existence of polynomial Lyapunov functions is necessary for
global stability of polynomial systems. In this section, we give a
negative answer to this question by presenting a remarkably simple
counterexample. In view of the fact that globally asymptotically stable
linear systems always admit quadratic Lyapunov functions, it is quite
interesting to observe that the following vector field that is arguably
“the next simplest system” to consider does not admit a polynomial
Lyapunov function of any degree.

###### Theorem 4.5.

Consider the polynomial vector field

  -- -------- -- --------
     @xmath      (4.15)
  -- -------- -- --------

The origin is a globally asymptotically stable equilibrium point, but
the system does not admit a polynomial Lyapunov function.

###### Proof.

Let us first show that the system is GAS. Consider the Lyapunov function

  -- -------- --
     @xmath   
  -- -------- --

which clearly vanishes at the origin, is strictly positive for all
@xmath , and is radially unbounded. The derivative of @xmath along the
trajectories of ( 4.15 ) is given by

  -- -------- --
     @xmath   
  -- -------- --

which is obviously strictly negative for all @xmath . In view of
Lyapunov’s stability theorem (see e.g. [ 86 , p. 124] ), this shows that
the origin is globally asymptotically stable.

Let us now prove that no positive definite polynomial Lyapunov function
(of any degree) can decrease along the trajectories of system ( 4.15 ).
The proof will be based on simply considering the value of a candidate
Lyapunov function at two specific points. We will look at trajectories
on the nonnegative orthant, with initial conditions on the line @xmath
for some constant @xmath , and then observe the location of the crossing
of the trajectory with the horizontal line @xmath . We will argue that
by taking @xmath large enough, the trajectory will have to travel “too
far east” (see Figure 4.1 ) and this will make it impossible for any
polynomial Lyapunov function to decrease.

To do this formally, we start by noting that we can explicitly solve for
the solution @xmath of the vector field in ( 4.15 ) starting from any
initial condition @xmath :

  -- -------- -- --------
     @xmath      (4.16)
  -- -------- -- --------

Consider initial conditions

  -- -------- --
     @xmath   
  -- -------- --

parameterized by @xmath and for some fixed constant @xmath . From the
explicit solution in ( 4.16 ) we have that the time @xmath it takes for
the trajectory to cross the line @xmath is

  -- -------- --
     @xmath   
  -- -------- --

and that the location of this crossing is given by

  -- -------- --
     @xmath   
  -- -------- --

Consider now any candidate nonnegative polynomial function @xmath that
depends on both @xmath and @xmath (as any Lyapunov function should).
Since @xmath (and thus, @xmath ), for @xmath to be a valid Lyapunov
function, it must satisfy @xmath , i.e.,

  -- -------- --
     @xmath   
  -- -------- --

However, this inequality cannot hold for @xmath large enough, since for
a generic fixed @xmath , the left hand side grows exponentially in
@xmath whereas the right hand side grows only polynomially in @xmath .
The only subtlety arises from the fact that @xmath could potentially be
a constant for some particular choices of @xmath . However, for any
polynomial @xmath with nontrivial dependence on @xmath , this may happen
for at most finitely many values of @xmath . Therefore, any generic
choice of @xmath would make the argument work. ∎

###### Example of Bacciotti and Rosier.

After our counterexample above was submitted for publication, Christian
Ebenbauer brought to our attention an earlier counterexample of
Bacciotti and Rosier [ 22 , Prop. 5.2] that achieves the same goal
(though by using irrational coefficients). We will explain the
differences between the two examples below. At the time of submission of
our result, we were under the impression that no such examples were
known, partly because of a recent reference in the controls literature
that ends its conclusion with the following statement [ 126 ] , [ 127 ]
:

-   “Still unresolved is the fundamental question of whether globally
    stable vector fields will also admit sum-of-squares Lyapunov
    functions.”

In [ 126 ] , [ 127 ] , what is referred to as a sum of squares Lyapunov
function (in contrast to our terminology here) is a Lyapunov function
that is a sum of squares, with no sos requirements on its derivative.
Therefore, the fundamental question referred to above is on existence of
a polynomial Lyapunov function. If one were to exist, then we could
simply square it to get another polynomial Lyapunov function that is a
sum of squares (see Lemma 4.6 ).

The example of Bacciotti and Rosier is a vector field in @xmath
variables and degree @xmath that is GAS but has no polynomial (and no
analytic) Lyapunov function even around the origin. Their very clever
construction is complementary to our example in the sense that what
creates trouble for existence of polynomial Lyapunov functions in our
Theorem 4.5 is growth rates arbitrarily far away from the origin,
whereas the problem arising in their example is slow decay rates
arbitrarily close to the origin. The example crucially relies on a
parameter that appears as part of the coefficients of the vector field
being irrational . (Indeed, one easily sees that if that parameter is
rational, their vector field does admit a polynomial Lyapunov function.)
In practical applications where computational techniques for searching
over Lyapunov functions on finite precision machines are used, such
issues with irrationality of the input cannot occur. By contrast, the
example in ( 4.15 ) is much less contrived and demonstrates that
non-existence of polynomial Lyapunov functions can happen for extremely
simple systems that may very well appear in applications.

In [ 125 ] , Peet has shown that locally exponentially stable polynomial
vector fields admit polynomial Lyapunov functions on compact sets. The
example of Bacciotti and Rosier implies that the assumption of
exponential stability indeed cannot be dropped.

### 4.4 (Non)-existence of sum of squares Lyapunov functions

In this section, we suppose that the polynomial vector field at hand
admits a polynomial Lyapunov function, and we would like to investigate
whether such a Lyapunov function can be found with sos programming. In
other words, we would like to see whether the constrains in ( 4.5 ) and
( 4.6 ) are more conservative than the true Lyapunov inequalities in (
4.2 ) and ( 4.3 ). We think of the sos Lyapunov conditions in ( 4.5 )
and ( 4.6 ) as sufficient conditions for the strict inequalities in (
4.2 ) and ( 4.3 ) even though sos decomposition in general merely
guarantees non-strict inequalities. The reason for this is that when an
sos feasibility problem is strictly feasible, the polynomials returned
by interior point algorithms are automatically positive definite (see [
1 , p. 41] for more discussion). ⁶ ⁶ 6 We expect the reader to recall
the basic definitions and concepts from Subsection 3.2.1 of the previous
chapter. Throughout, when we say a Lyapunov function (or the negative of
its derivative) is positive definite , we mean that it is positive
everywhere except possibly at the origin.

We shall emphasize that existence of nonnegative polynomials that are
not sums of squares does not imply on its own that the sos conditions in
( 4.5 ) and ( 4.6 ) are more conservative than the Lyapunov inequalities
in ( 4.2 ) and ( 4.3 ). Since Lyapunov functions are not in general
unique, it could happen that within the set of valid polynomial Lyapunov
functions of a given degree, there is always at least one that satisfies
the sos conditions ( 4.5 ) and ( 4.6 ). Moreover, many of the known
examples of nonnegative polynomials that are not sos have multiple zeros
and local minima [ 138 ] and therefore cannot serve as Lyapunov
functions. Indeed, if a function has a local minimum other than the
origin, then its value evaluated on a trajectory starting from the local
minimum would not be decreasing.

#### 4.4.1 A motivating example

The following example will help motivate the kind of questions that we
are addressing in this section.

###### Example 4.4.1.

Consider the dynamical system

  -- -------- -- --------
     @xmath      (4.17)
  -- -------- -- --------

A typical trajectory of the system that starts from the initial
condition @xmath is plotted in Figure 4.2 . Our goal is to establish
global asymptotic stability of the origin by searching for a polynomial
Lyapunov function. Since the vector field is homogeneous, the search can
be restricted to homogeneous Lyapunov functions [ 72 ] , [ 146 ] . To
employ the sos technique, we can use the software package SOSTOOLS [ 132
] to search for a Lyapunov function satisfying the sos conditions ( 4.5
) and ( 4.6 ). However, if we do this, we will not find any Lyapunov
functions of degree @xmath , @xmath , or @xmath . If needed, a
certificate from the dual semidefinite program can be obtained, which
would prove that no polynomial of degree up to @xmath can satisfy the
sos requirements ( 4.5 ) and ( 4.6 ).

At this point we are faced with the following question. Does the system
really not admit a Lyapunov function of degree @xmath that satisfies the
true Lyapunov inequalities in ( 4.2 ), ( 4.3 )? Or is the failure due to
the fact that the sos conditions in ( 4.5 ), ( 4.6 ) are more
conservative?

Note that when searching for a degree @xmath Lyapunov function, the sos
constraint in ( 4.5 ) is requiring a homogeneous polynomial in @xmath
variables and of degree @xmath to be a sum of squares. The sos condition
( 4.6 ) on the derivative is also a condition on a homogeneous
polynomial in @xmath variables, but in this case of degree @xmath .
(This is easy to see from @xmath .) Recall from Theorem 3.1 of the
previous chapter that nonnegativity and sum of squares are equivalent
notions for homogeneous bivariate polynomials, irrespective of the
degree. Hence, we now have a proof that this dynamical system truly does
not have a Lyapunov function of degree @xmath (or lower).

This fact is perhaps geometrically intuitive. Figure 4.2 shows that the
trajectory of this system is stretching out in @xmath different
directions. So, we would expect the degree of the Lyapunov function to
be at least @xmath . Indeed, when we increase the degree of the
candidate function to @xmath , SOSTOOLS and the SDP solver SeDuMi [ 157
] succeed in finding the following Lyapunov function:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
                       @xmath   
                       @xmath   
  -- -------- -------- -------- --

The level sets of this Lyapunov function are plotted in Figure 4.2 and
are clearly invariant under the trajectory. @xmath

#### 4.4.2 A counterexample

Unlike the scenario in the previous example, we now show that a failure
in finding a Lyapunov function of a particular degree via sum of squares
programming can also be due to the gap between nonnegativity and sum of
squares. What will be conservative in the following counterexample is
the sos condition on the derivative. ⁷ ⁷ 7 This counterexample has
appeared in our earlier work [ 1 ] but not with a complete proof.

Consider the dynamical system

  -- -------- -- --------
     @xmath      (4.18)
  -- -------- -- --------

One can verify that the origin is the only equilibrium point for this
system, and therefore it makes sense to investigate global asymptotic
stability. If we search for a quadratic Lyapunov function for ( 4.18 )
using sos programming, we will not find one. It will turn out that the
corresponding semidefinite program is infeasible. We will prove shortly
why this is the case, i.e, why no quadratic function @xmath can satisfy

  -- -------- -- --------
     @xmath      (4.19)
  -- -------- -- --------

Nevertheless, we claim that

  -- -------- -- --------
     @xmath      (4.20)
  -- -------- -- --------

is a valid Lyapunov function. Indeed, one can check that

  -- -------- -- --------
     @xmath      (4.21)
  -- -------- -- --------

where @xmath is the Motzkin polynomial [ 107 ] :

  -- -------- --
     @xmath   
  -- -------- --

This polynomial is just a dehomogenized version of the Motzkin form
presented before, and it has the property of being nonnegative but not a
sum of squares. The polynomial @xmath is strictly negative everywhere,
except for the origin and three other points @xmath , @xmath , and
@xmath , where @xmath is zero. However, at each of these three points we
have @xmath . Once the trajectory reaches any of these three points, it
will be kicked out to a region where @xmath is strictly negative.
Therefore, by LaSalle’s invariance principle (see e.g. [ 86 , p. 128] ),
the quadratic Lyapunov function in ( 4.20 ) proves global asymptotic
stability of the origin of ( 4.18 ).

The fact that @xmath is zero at three points other than the origin is
not the reason why sos programming is failing. After all, when we impose
the condition that @xmath should be sos, we allow for the possibility of
a non-strict inequality. The reason why our sos program does not
recognize ( 4.20 ) as a Lyapunov function is that the shifted Motzkin
polynomial in ( 4.21 ) is nonnegative but it is not a sum of squares.
This sextic polynomial is plotted in Figure 4.3(a) . Trajectories of (
4.18 ) starting at @xmath and @xmath along with level sets of @xmath are
shown in Figure 4.3(b) .

So far, we have shown that @xmath in ( 4.20 ) is a valid Lyapunov
function but does not satisfy the sos conditions in ( 4.19 ). We still
need to show why no other quadratic Lyapunov function

  -- -------- -- --------
     @xmath      (4.22)
  -- -------- -- --------

can satisfy the sos conditions either. ⁸ ⁸ 8 Since we can assume that
the Lyapunov function @xmath and its gradient vanish at the origin,
linear or constant terms are not needed in ( 4.22 ). We will in fact
prove the stronger statement that @xmath in ( 4.20 ) is the only valid
quadratic Lyapunov function for this system up to scaling, i.e., any
quadratic function @xmath that is not a scalar multiple of @xmath cannot
satisfy @xmath and @xmath . It will even be the case that no such @xmath
can satisfy @xmath alone. (The latter fact is to be expected since
global asymptotic stability of ( 4.18 ) together with @xmath would
automatically imply @xmath ; see [ 12 , Theorem 1.1] .)

So, let us show that @xmath implies @xmath is a scalar multiple of
@xmath . Because Lyapunov functions are closed under positive scalings,
without loss of generality we can take @xmath . One can check that

  -- -------- --
     @xmath   
  -- -------- --

so to have @xmath , we need @xmath . Similarly,

  -- -------- --
     @xmath   
  -- -------- --

which implies that @xmath . Let us now look at

  -- -------- -- --------
     @xmath      (4.23)
  -- -------- -- --------

If we let @xmath , the term @xmath dominates this polynomial. Since
@xmath and @xmath , we conclude that @xmath . Once @xmath is set to zero
in ( 4.23 ), the dominating term for @xmath large will be @xmath .
Therefore to have @xmath as @xmath we must have @xmath . Hence, we
conclude that @xmath , and this finishes the proof.

Even though sos programming failed to prove stability of the system in (
4.18 ) with a quadratic Lyapunov function, if we increase the degree of
the candidate Lyapunov function from @xmath to @xmath , then SOSTOOLS
succeeds in finding a quartic Lyapunov function

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
                       @xmath   
                       @xmath   
  -- -------- -------- -------- --

which satisfies the sos conditions in ( 4.19 ). The level sets of this
function are close to circles and are plotted in Figure 4.3(c) .

Motivated by this example, it is natural to ask whether it is always
true that upon increasing the degree of the Lyapunov function one will
find Lyapunov functions that satisfy the sum of squares conditions in (
4.19 ). In the next subsection, we will prove that this is indeed the
case, at least for planar systems such as the one in this example, and
also for systems that are homogeneous.

#### 4.4.3 Converse sos Lyapunov theorems

In [ 126 ] , [ 127 ] , it is shown that if a system admits a polynomial
Lyapunov function, then it also admits one that is a sum of squares.
However, the results there do not lead to any conclusions as to whether
the negative of the derivative of the Lyapunov function is sos, i.e,
whether condition ( 4.6 ) is satisfied. As we remarked before, there is
therefore no guarantee that the semidefinite program can find such a
Lyapunov function. Indeed, our counterexample in the previous subsection
demonstrated this very phenomenon.

The proof technique used in [ 126 ] , [ 127 ] is based on approximating
the solution map using the Picard iteration and is interesting in
itself, though the actual conclusion that a Lyapunov function that is
sos exists has a far simpler proof which we give in the next lemma.

###### Lemma 4.6.

If a polynomial dynamical system has a positive definite polynomial
Lyapunov function @xmath with a negative definite derivative @xmath ,
then it also admits a positive definite polynomial Lyapunov function
@xmath which is a sum of squares.

###### Proof.

Take @xmath . The negative of the derivative @xmath is clearly positive
definite (though it may not be sos). ∎

We will next prove a converse sos Lyapunov theorem that guarantees the
derivative of the Lyapunov function will also satisfy the sos condition,
though this result is restricted to homogeneous systems. The proof of
this theorem relies on the following Positivstellensatz result due to
Scheiderer.

###### Theorem 4.7 (Scheiderer, [151]).

Given any two positive definite homogeneous polynomials @xmath and
@xmath , there exists an integer @xmath such that @xmath is a sum of
squares.

###### Theorem 4.8.

Given a homogeneous polynomial vector field, suppose there exists a
homogeneous polynomial Lyapunov function @xmath such that @xmath and
@xmath are positive definite. Then, there also exists a homogeneous
polynomial Lyapunov function @xmath such that @xmath is sos and @xmath
is sos.

###### Proof.

Observe that @xmath and @xmath are both positive definite and
homogeneous polynomials. Applying Theorem 4.7 to these two polynomials,
we conclude the existence of an integer @xmath such that @xmath is sos.
Let

  -- -------- --
     @xmath   
  -- -------- --

Then, @xmath is clearly sos since it is a perfect even power. Moreover,

  -- -------- --
     @xmath   
  -- -------- --

is also sos by the previous claim. ⁹ ⁹ 9 Note that @xmath constructed in
this proof proves GAS since @xmath is positive definite and @xmath
itself being homogeneous and positive definite is automatically radially
unbounded. ∎

Next, we develop a similar theorem that removes the homogeneity
assumption from the vector field, but instead is restricted to vector
fields on the plane. For this, we need another result of Scheiderer.

###### Theorem 4.9 (Scheiderer, [150, Cor. 3.12]).

Let @xmath and @xmath be two homogeneous polynomials in three variables,
with @xmath positive semidefinite and @xmath positive definite. Then,
there exists an integer @xmath such that @xmath is a sum of squares.

###### Theorem 4.10.

Given a (not necessarily homogeneous) polynomial vector field in two
variables, suppose there exists a positive definite polynomial Lyapunov
function @xmath with @xmath positive definite, and such that the highest
order term of @xmath has no zeros ¹⁰ ¹⁰ 10 This requirement is only
slightly stronger than the requirement of radial unboundedness, which is
imposed on @xmath by Lyapunov’s theorem anyway. . Then, there also
exists a polynomial Lyapunov function @xmath such that @xmath is sos and
@xmath is sos.

###### Proof.

Let @xmath . So, @xmath . Consider the (non-homogeneous) polynomials
@xmath and @xmath in the variables @xmath . Let us denote the (even)
degrees of these polynomials respectively by @xmath and @xmath . Note
that @xmath is nowhere zero and @xmath is only zero at the origin. Our
first step is to homogenize these polynomials by introducing a new
variable @xmath . Observing that the homogenization of products of
polynomials equals the product of homogenizations, we obtain the
following two trivariate forms:

  -- -------- -- --------
     @xmath      (4.24)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (4.25)
  -- -------- -- --------

Since by assumption the highest order term of @xmath has no zeros, the
form in ( 4.24 ) is positive definite . The form in ( 4.25 ), however,
is only positive semidefinite. In particular, since @xmath has to vanish
at the origin, the form in ( 4.25 ) has a zero at the point @xmath .
Nevertheless, since Theorem 4.9 allows for positive semidefiniteness of
one of the two forms, by applying it to the forms in ( 4.24 ) and ( 4.25
), we conclude that there exists an integer @xmath such that

  -- -------- -- --------
     @xmath      (4.26)
  -- -------- -- --------

is sos. Let @xmath Then, @xmath is clearly sos. Moreover,

  -- -------- --
     @xmath   
  -- -------- --

is also sos because this polynomial is obtained from ( 4.26 ) by setting
@xmath . ¹¹ ¹¹ 11 Once again, we note that the function @xmath
constructed in this proof is radially unbounded, achieves its global
minimum at the origin, and has @xmath positive definite. Therefore,
@xmath proves global asymptotic stability. ∎

### 4.5 Existence of sos Lyapunov functions for switched linear systems

The result of Theorem 4.8 extends in a straightforward manner to
Lyapunov analysis of switched systems. In particular, we are interested
in the highly-studied problem of stability analysis of arbitrary
switched linear systems:

  -- -------- -- --------
     @xmath      (4.27)
  -- -------- -- --------

@xmath . We assume the minimum dwell time of the system is bounded away
from zero. This guarantees that the solutions of ( 4.27 ) are
well-defined. Existence of a common Lyapunov function is necessary and
sufficient for (global) asymptotic stability under arbitrary switching
(ASUAS) of system ( 4.27 ). The ASUAS of system ( 4.27 ) is equivalent
to asymptotic stability of the linear differential inclusion

  -- -------- --
     @xmath   
  -- -------- --

where @xmath here denotes the convex hull. It is also known that ASUAS
of ( 4.27 ) is equivalent to exponential stability under arbitrary
switching [ 15 ] . A common approach for analyzing the stability of
these systems is to use the sos technique to search for a common
polynomial Lyapunov function [ 131 ] , [ 42 ] . We will prove the
following result.

###### Theorem 4.11.

The switched linear system in ( 4.27 ) is asymptotically stable under
arbitrary switching if and only if there exists a common homogeneous
polynomial Lyapunov function @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

for @xmath , where the polynomials @xmath and @xmath are all positive
definite.

To prove this result, we will use the following theorem of Mason et al.

###### Theorem 4.12 (Mason et al., [103]).

If the switched linear system in ( 4.27 ) is asymptotically stable under
arbitrary switching, then there exists a common homogeneous polynomial
Lyapunov function @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

for @xmath .

The next proposition is an extension of Theorem 4.8 to switched systems
(not necessarily linear).

###### Proposition 4.13.

Consider an arbitrary switched dynamical system

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a homogeneous polynomial vector field of degree @xmath
(the degrees of the different vector fields can be different). Suppose
there exists a common positive definite homogeneous polynomial Lyapunov
function @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

is positive definite for all @xmath . Then there exists a common
homogeneous polynomial Lyapunov function @xmath such that @xmath is sos
and the polynomials

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath , are also sos.

###### Proof.

Observe that for each @xmath , the polynomials @xmath and @xmath are
both positive definite and homogeneous. Applying Theorem 4.7 @xmath
times to these pairs of polynomials, we conclude the existence of
positive integers @xmath such that

  -- -------- -- --------
     @xmath      (4.28)
  -- -------- -- --------

for @xmath . Let

  -- -------- --
     @xmath   
  -- -------- --

and let

  -- -------- --
     @xmath   
  -- -------- --

Then, @xmath is clearly sos. Moreover, for each @xmath , the polynomial

  -- -------- --
     @xmath   
  -- -------- --

is sos since @xmath is sos by ( 4.28 ), @xmath is sos as an even power,
and products of sos polynomials are sos. ∎

The proof of Theorem 4.11 now simply follows from Theorem 4.12 and
Proposition 4.13 in the special case where @xmath for all @xmath .

Analysis of switched linear systems is also of great interest to us in
discrete time. In fact, the subject of the next chapter will be on the
study of systems of the type

  -- -------- -- --------
     @xmath      (4.29)
  -- -------- -- --------

where at each time step the update rule can be given by any of the
@xmath matrices @xmath . The analogue of Theorem 4.11 for these systems
has already been proven by Parrilo and Jadbabaie in [ 122 ] . It is
shown that if ( 4.29 ) is asymptotically stable under arbitrary
switching, then there exists a homogeneous polynomial Lyapunov function
@xmath such that

  -- -------- --
     @xmath   
  -- -------- --

for @xmath . We will end this section by proving two related
propositions of a slightly different flavor. It will be shown that for
switched linear systems, both in discrete time and in continuous time,
the sos condition on the Lyapunov function itself is never conservative,
in the sense that if one of the “decrease inequalities” is sos, then the
Lyapunov function is automatically sos. These propositions are really
statements about linear systems, so we will present them that way.
However, since stable linear systems always admit quadratic Lyapunov
functions, the propositions are only interesting in the context where a
common polynomial Lyapunov function for a switched linear system is
seeked.

###### Proposition 4.14.

Consider the linear dynamical system @xmath in discrete time. Suppose
there exists a positive definite polynomial Lyapunov function @xmath
such that @xmath is positive definite and sos. Then, @xmath is sos.

###### Proof.

Consider the polynomial @xmath that is sos by assumption. If we replace
@xmath by @xmath in this polynomial, we conclude that the polynomial
@xmath is also sos. Hence, by adding these two sos polynomials, we get
that @xmath is sos. This procedure can obviously be repeated to infer
that for any integer @xmath , the polynomial

  -- -------- -- --------
     @xmath      (4.30)
  -- -------- -- --------

is sos. Since by assumption @xmath and @xmath are positive definite, the
linear system must be GAS, and hence @xmath converges to the zero matrix
as @xmath . Observe that for all @xmath , the polynomials in ( 4.30 )
have degree equal to the degree of @xmath , and that the coefficients of
@xmath converge to the coefficients of @xmath as @xmath . Since for a
fixed degree and dimension the cone of sos polynomials is closed [ 141 ]
, it follows that @xmath is sos. ∎

Similarly, in continuous time, we have the following proposition.

###### Proposition 4.15.

Consider the linear dynamical system @xmath in continuous time. Suppose
there exists a positive definite polynomial Lyapunov function @xmath
such that @xmath is positive definite and sos. Then, @xmath is sos.

###### Proof.

The value of the polynomial @xmath along the trajectories of the
dynamical system satisfies the relation

  -- -------- --
     @xmath   
  -- -------- --

Since the assumptions imply that the system is GAS, @xmath as @xmath
goes to infinity. (Here, we are assuming, without loss of generality,
that @xmath vanishes at the origin.) By evaluating the above equation at
@xmath , rearranging terms, and substituting @xmath for the solution of
the linear system at time @xmath starting at initial condition @xmath ,
we obtain

  -- -------- --
     @xmath   
  -- -------- --

By assumption, @xmath is sos and therefore for any value of @xmath , the
integrand @xmath is an sos polynomial. Since converging integrals of sos
polynomials are sos, it follows that @xmath is sos. ∎

###### Remark 4.5.1.

The previous proposition does not hold if the system is not linear. For
example, consider any positive form @xmath that is not a sum of squares
and define a dynamical system by @xmath . In this case, both @xmath and
@xmath are positive definite and @xmath is sos, though @xmath is not
sos.

### 4.6 Some open questions

Some open questions related to the problems studied in this chapter are
the following. Regarding complexity, of course the interesting problem
is to formally answer the questions of Arnold on undecidability of
determining stability for polynomial vector fields. Regarding existence
of polynomial Lyapunov functions, Mark Tobenkin asked whether a globally
exponentially stable polynomial vector field admits a polynomial
Lyapunov function. Our counterexample in Section 4.3 , though GAS and
locally exponentially stable, is not globally exponentially stable
because of exponential growth rates in the large. The counterexample of
Bacciotti and Rosier in [ 22 ] is not even locally exponentially stable.
Another future direction is to prove that GAS homogeneous polynomial
vector fields admit homogeneous polynomial Lyapunov functions. This,
together with Theorem 4.8 , would imply that asymptotic stability of
homogeneous polynomial systems can always be decided via sum of squares
programming. Also, it is not clear to us whether the assumption of
homogeneity and planarity can be removed from Theorems 4.8 and 4.10 on
existence of sos Lyapunov functions. Finally, another research direction
would be to obtain upper bounds on the degree of polynomial or sos
polynomial Lyapunov functions. Some degree bounds are known for Lyapunov
analysis of locally exponentially stable systems [ 127 ] , but they
depend on uncomputable properties of the solution such as convergence
rate. Degree bounds on Positivstellensatz result of the type in Theorems
4.7 and 4.9 are known, but typically exponential in size and not very
encouraging for practical purposes.

## Chapter 5 Joint Spectral Radius and Path-Complete Graph Lyapunov
Functions

In this chapter, we introduce the framework of path-complete graph
Lyapunov functions for analysis of switched systems. The methodology is
presented in the context of approximation of the joint spectral radius.
The content of this chapter is based on an extended version of the work
in [ 3 ] .

### 5.1 Introduction

Given a finite set of square matrices @xmath , their joint spectral
radius @xmath is defined as

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

where the quantity @xmath is independent of the norm used in ( 5.1 ).
The joint spectral radius (JSR) is a natural generalization of the
spectral radius of a single square matrix and it characterizes the
maximal growth rate that can be obtained by taking products, of
arbitrary length, of all possible permutations of @xmath . This concept
was introduced by Rota and Strang [ 147 ] in the early 60s and has since
been the subject of extensive research within the engineering and the
mathematics communities alike. Aside from a wealth of fascinating
mathematical questions that arise from the JSR, the notion emerges in
many areas of application such as stability of switched linear dynamical
systems, computation of the capacity of codes, continuity of wavelet
functions, convergence of consensus algorithms, trackability of graphs,
and many others. See [ 85 ] and references therein for a recent survey
of the theory and applications of the JSR.

Motivated by the abundance of applications, there has been much work on
efficient computation of the joint spectral radius; see e.g. [ 32 ] , [
31 ] , [ 122 ] , and references therein. Unfortunately, the negative
results in the literature certainly restrict the horizon of
possibilities. In [ 35 ] , Blondel and Tsitsiklis prove that even when
the set @xmath consists of only two matrices, the question of testing
whether @xmath is undecidable. They also show that unless P=NP, one
cannot compute an approximation @xmath of @xmath that satisfies @xmath ,
in a number of steps polynomial in the bit size of @xmath and the bit
size of @xmath [ 161 ] . It is not difficult to show that the spectral
radius of any finite product of length @xmath raised to the power of
@xmath gives a lower bound on @xmath [ 85 ] . However, for reasons that
we explain next, our focus will be on computing upper bounds for @xmath
.

There is an attractive connection between the joint spectral radius and
the stability properties of an arbitrary switched linear system; i.e.,
dynamical systems of the form

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

where @xmath is a map from the set of integers to the set of indices. It
is well-known that @xmath if and only if system ( 5.2 ) is absolutely
asymptotically stable (AAS), that is, (globally) asymptotically stable
for all switching sequences. Moreover, it is known [ 95 ] that absolute
asymptotic stability of ( 5.2 ) is equivalent to absolute asymptotic
stability of the linear difference inclusion

  -- -------- -- -------
     @xmath      (5.3)
  -- -------- -- -------

where @xmath here denotes the convex hull of the set @xmath . Therefore,
any method for obtaining upper bounds on the joint spectral radius
provides sufficient conditions for stability of systems of type ( 5.2 )
or ( 5.3 ). Conversely, if we can prove absolute asymptotic stability of
( 5.2 ) or ( 5.3 ) for the set @xmath for some positive scalar @xmath ,
then we get an upper bound of @xmath on @xmath . (This follows from the
scaling property of the JSR: @xmath .) One advantage of working with the
notion of the joint spectral radius is that it gives a way of rigorously
quantifying the performance guarantee of different techniques for
stability analysis of systems ( 5.2 ) or ( 5.3 ).

Perhaps the most well-established technique for proving stability of
switched systems is the use of a common (or simultaneous) Lyapunov
function . The idea here is that if there is a continuous, positive, and
homogeneous (Lyapunov) function @xmath that for some @xmath satisfies

  -- -------- -- -------
     @xmath      (5.4)
  -- -------- -- -------

(i.e., @xmath decreases no matter which matrix is applied), then the
system in ( 5.2 ) (or in ( 5.3 )) is AAS. Conversely, it is known that
if the system is AAS, then there exists a convex common Lyapunov
function (in fact a norm); see e.g. [ 85 , p. 24] . However, this
function is not in general finitely constructable. A popular approach
has been to try to approximate this function by a class of functions
that we can efficiently search for using convex optimization and in
particular semidefinite programming. As we mentioned in our introductory
chapters, semidefinite programs (SDPs) can be solved with arbitrary
accuracy in polynomial time and lead to efficient computational methods
for approximation of the JSR. As an example, if we take the Lyapunov
function to be quadratic (i.e., @xmath ), then the search for such a
Lyapunov function can be formulated as the following SDP:

  -- -------- -- -------
     @xmath      (5.5)
  -- -------- -- -------

The quality of approximation of common quadratic Lyapunov functions is a
well-studied topic. In particular, it is known [ 32 ] that the estimate
@xmath obtained by this method ¹ ¹ 1 The estimate @xmath is the
reciprocal of the largest @xmath that satisfies ( 5.5 ) and can be found
by bisection. satisfies

  -- -------- -- -------
     @xmath      (5.6)
  -- -------- -- -------

where @xmath is the dimension of the matrices. This bound is a direct
consequence of John’s ellipsoid theorem and is known to be tight [ 13 ]
.

In [ 122 ] , the use of sum of squares (sos) polynomial Lyapunov
functions of degree @xmath was proposed as a common Lyapunov function
for the switched system in ( 5.2 ). As we know, the search for such a
Lyapunov function can again be formulated as a semidefinite program.
This method does considerably better than a common quadratic Lyapunov
function in practice and its estimate @xmath satisfies the bound

  -- -------- -- -------
     @xmath      (5.7)
  -- -------- -- -------

where @xmath . Furthermore, as the degree @xmath goes to infinity, the
estimate @xmath converges to the true value of @xmath [ 122 ] . The
semidefinite programming based methods for approximation of the JSR have
been recently generalized and put in the framework of conic programming
[ 134 ] .

#### 5.1.1 Contributions and organization of this chapter

It is natural to ask whether one can develop better approximation
schemes for the joint spectral radius by using multiple Lyapunov
functions as opposed to requiring simultaneous contractibility of a
single Lyapunov function with respect to all the matrices. More
concretely, our goal is to understand how we can write inequalities
among, say, @xmath different Lyapunov functions @xmath that imply
absolute asymptotic stability of ( 5.2 ) and can be checked via
semidefinite programming.

The general idea of using several Lyapunov functions for analysis of
switched systems is a very natural one and has already appeared in the
literature (although to our knowledge not in the context of the
approximation of the JSR); see e.g. [ 136 ] , [ 39 ] , [ 81 ] , [ 80 ] ,
[ 64 ] . Perhaps one of the earliest references is the work on
“piecewise quadratic Lyapunov functions” in [ 136 ] . However, this work
is in the different framework of state dependent switching, where the
dynamics switches depending on which region of the space the trajectory
is traversing (as opposed to arbitrary switching). In this setting,
there is a natural way of using several Lyapunov functions: assign one
Lyapunov function per region and “glue them together”. Closer to our
setting, there is a body of work in the literature that gives sufficient
conditions for existence of piecewise Lyapunov functions of the type
@xmath , @xmath , and @xmath , i.e, the pointwise maximum, the pointwise
minimum, and the convex envelope of a set of quadratic functions [ 81 ]
, [ 80 ] , [ 64 ] , [ 82 ] . These works are mostly concerned with
analysis of linear differential inclusions in continuous time, but they
have obvious discrete time counterparts. The main drawback of these
methods is that in their greatest generality, they involve solving
bilinear matrix inequalities, which are non-convex and in general
NP-hard. One therefore has to turn to heuristics, which have no
performance guarantees and their computation time quickly becomes
prohibitive when the dimension of the system increases. Moreover, all of
these methods solely provide sufficient conditions for stability with no
performance guarantees.

There are several unanswered questions that in our view deserve a more
thorough study: (i) With a focus on conditions that are amenable to
convex optimization, what are the different ways to write a set of
inequalities among @xmath Lyapunov functions that imply absolute
asymptotic stability of ( 5.2 )? Can we give a unifying framework that
includes the previously proposed Lyapunov functions and perhaps also
introduces new ones? (ii) Among the different sets of inequalities that
imply stability, can we identify some that are less conservative than
some other? (iii) The available methods on piecewise Lyapunov functions
solely provide sufficient conditions for stability with no guarantee on
their performance. Can we give converse theorems that guarantee the
existence of a feasible solution to our search for a given accuracy?

The contributions of this chapter to these questions are as follows. We
propose a unifying framework based on a representation of Lyapunov
inequalities with labeled graphs and by making some connections with
basic concepts in automata theory. This is done in Section 5.2 , where
we define the notion of a path-complete graph (Definition 5.2 ) and
prove that any such graph provides an approximation scheme for the JSR
(Theorem 5.4 ). In Section 5.3 , we give examples of families of
path-complete graphs and show that many of the previously proposed
techniques come from particular classes of simple path-complete graphs
(e.g., Corollary 5.8 , Corollary 5.9 , and Remark 5.3.2 ). In Section
5.4 , we characterize all the path-complete graphs with two nodes for
the analysis of the JSR of two matrices. We determine how the
approximations obtained from all of these graphs compare (Proposition
5.12 ). In Section 5.5 , we study in more depth the approximation
properties of a particular pair of “dual” path-complete graphs that seem
to perform very well in practice. Subsection 5.5.1 contains more general
results about duality within path-complete graphs and its connection to
transposition of matrices (Theorem 5.13 ). Subsection 5.5.2 gives an
approximation guarantee for the graphs studied in Section 5.5 (Theorem
5.16 ), and Subsection 5.5.3 contains some numerical examples. In
Section 5.6 , we prove a converse theorem for the method of
max-of-quadratics Lyapunov functions (Theorem 5.17 ) and an
approximation guarantee for a new class of methods for proving stability
of switched systems (Theorem 5.18 ). Finally, some concluding remarks
and future directions are presented in Section 5.7 .

### 5.2 Path-complete graphs and the joint spectral radius

In what follows, we will think of the set of matrices @xmath as a finite
alphabet and we will often refer to a finite product of matrices from
this set as a word . We denote the set of all words @xmath of length
@xmath by @xmath . Contrary to the standard convention in automata
theory, our convention is to read a word from right to left. This is in
accordance with the order of matrix multiplication. The set of all
finite words is denoted by @xmath ; i.e., @xmath .

The basic idea behind our framework is to represent through a graph all
the possible occurrences of products that can appear in a run of the
dynamical system in ( 5.2 ), and assert via some Lyapunov inequalities
that no matter what occurrence appears, the product must remain stable.
A convenient way of representing these Lyapunov inequalities is via a
directed labeled graph @xmath . Each node of this graph is associated
with a (continuous, positive definite, and homogeneous) Lyapunov
function @xmath , and each edge is labeled by a finite product of
matrices, i.e., by a word from the set @xmath . As illustrated in Figure
5.1 , given two nodes with Lyapunov functions @xmath and @xmath and an
edge going from node @xmath to node @xmath labeled with the matrix
@xmath , we write the Lyapunov inequality:

  -- -------- -- -------
     @xmath      (5.8)
  -- -------- -- -------

The problem that we are interested in is to understand which sets of
Lyapunov inequalities imply stability of the switched system in ( 5.2 ).
We will answer this question based on the corresponding graph.

For reasons that will become clear shortly, we would like to reduce
graphs whose edges have arbitrary labels from the set @xmath to graphs
whose edges have labels from the set @xmath , i.e, labels of length one.
This is explained next.

###### Definition 5.1.

Given a labeled directed graph @xmath , we define its expanded graph
@xmath as the outcome of the following procedure. For every edge @xmath
with label @xmath , where @xmath , we remove the edge @xmath and replace
it with @xmath new edges @xmath , where @xmath and @xmath . ² ² 2 It is
understood that the node index @xmath depends on the original nodes
@xmath and @xmath . To keep the notation simple we write @xmath instead
of @xmath . (These new edges go from node @xmath through @xmath newly
added nodes @xmath and then to node @xmath .) We then label the new
edges @xmath with @xmath respectively.

An example of a graph and its expansion is given in Figure 5.2 . Note
that if a graph has only labels of length one, then its expanded graph
equals itself. The next definition is central to our development.

###### Definition 5.2.

Given a directed graph @xmath whose edges are labeled with words from
the set @xmath , we say that the graph is path-complete , if for all
finite words @xmath of any length @xmath (i.e., for all words in @xmath
), there is a directed path in its expanded graph @xmath such that the
labels on the edges of this path are the labels @xmath up to @xmath .

In Figure 5.3 , we present seven path-complete graphs on the alphabet
@xmath . The fact that these graphs are path-complete is easy to see for
graphs @xmath and @xmath , but perhaps not so obvious for graphs @xmath
and @xmath . One way to check if a graph is path-complete is to think of
it as a finite automaton by introducing an auxiliary start node (state)
with free transitions to every node and by making all the other nodes be
accepting states. Then, there are well-known algorithms (see e.g. [ 78 ,
Chap. 4] ) that check whether the language accepted by an automaton is
@xmath , which is equivalent to the graph being path-complete. At least
for the cases where the automata are deterministic (i.e., when all
outgoing edges from any node have different labels), these algorithms
are very efficient and have running time of only @xmath . Similar
algorithms exist in the symbolic dynamics literature; see e.g. [ 96 ,
Chap. 3] . Our interest in path-complete graphs stems from the Theorem
5.4 below that establishes that any such graph gives a method for
approximation of the JSR. We introduce one last definition before we
state this theorem.

###### Definition 5.3.

Let @xmath be a set of matrices. Given a path-complete graph @xmath and
@xmath functions @xmath , we say that @xmath is a graph Lyapunov
function (GLF) associated with @xmath if

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the label associated with edge @xmath going from node
@xmath to node @xmath .

###### Theorem 5.4.

Consider a finite set of matrices @xmath . For a scalar @xmath , let
@xmath . Let @xmath be a path-complete graph whose edges are labeled
with words from @xmath . If there exist positive, continuous, and
homogeneous ³ ³ 3 The requirement of homogeneity can be replaced by
radial unboundedness which is implied by homogeneity and positivity.
However, since the dynamical system in ( 5.2 ) is homogeneous, there is
no conservatism in asking @xmath to be homogeneous. functions @xmath ,
one per node of the graph, such that @xmath is a graph Lyapunov function
associated with @xmath , then @xmath .

###### Proof.

We will first prove the claim for the special case where the edge labels
of @xmath belong to @xmath and therefore @xmath . The general case will
be reduced to this case afterwards. Let @xmath be the degree of
homogeneity of the Lyapunov functions @xmath , i.e., @xmath for all
@xmath . (The actual value of @xmath is irrelevant.) By positivity,
continuity, and homogeneity of @xmath , there exist scalars @xmath and
@xmath with @xmath for @xmath , such that

  -- -------- -- -------
     @xmath      (5.9)
  -- -------- -- -------

for all @xmath and for all @xmath , where @xmath here denotes the
Euclidean norm of @xmath . Let

  -- -------- -- --------
     @xmath      (5.10)
  -- -------- -- --------

Now consider an arbitrary product @xmath of length @xmath . Because the
graph is path-complete, there will be a directed path corresponding to
this product that consists of @xmath edges, and goes from some node
@xmath to some node @xmath . If we write the chain of @xmath Lyapunov
inequalities associated with these edges (cf. Figure 5.1 ), then we get

  -- -------- --
     @xmath   
  -- -------- --

which by homogeneity of the Lyapunov functions can be rearranged to

  -- -------- -- --------
     @xmath      (5.11)
  -- -------- -- --------

We can now bound the spectral norm of @xmath as follows:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

where the last three inequalities follow from ( 5.9 ), ( 5.11 ), and (
5.10 ) respectively. From the definition of the JSR in ( 5.1 ), after
taking the @xmath -th root and the limit @xmath , we get that @xmath and
the claim is established.

Now consider the case where at least one edge of @xmath has a label of
length more than one and hence @xmath We will start with the Lyapunov
functions @xmath assigned to the nodes of @xmath and from them we will
explicitly construct @xmath Lyapunov functions for the nodes of @xmath
that satisfy the Lyapunov inequalities associated to the edges in @xmath
. Once this is done, in view of our preceding argument and the fact that
the edges of @xmath have labels of length one by definition, the proof
will be completed.

For @xmath , let us denote the new Lyapunov functions by @xmath . We
give the construction for the case where @xmath The result for the
general case follows by iterating this simple construction. Let @xmath
be the added node in the expanded graph, and @xmath be such that @xmath
and @xmath with @xmath and @xmath as the corresponding labels
respectively. Define

  -- -------- -- --------
     @xmath      (5.12)
  -- -------- -- --------

By construction, @xmath and @xmath and subsequently, @xmath and @xmath
are uniquely defined and hence, @xmath is well defined. We only need to
show that

  -- -------- -------- -- --------
     @xmath   @xmath      (5.13)
     @xmath   @xmath      (5.14)
  -- -------- -------- -- --------

Inequality ( 5.13 ) follows trivially from ( 5.12 ). Furthermore, it
follows from ( 5.12 ) that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where the inequality follows from the fact that for @xmath , the
functions @xmath satisfy the Lyapunov inequalities of the edges of
@xmath ∎

###### Remark 5.2.1.

If the matrix @xmath is not invertible, the extended function @xmath as
defined in ( 5.12 ) will only be positive semidefinite. However, since
our goal is to approximate the JSR, we will never be concerned with
invertibility of the matrices in @xmath . Indeed, since the JSR is
continuous in the entries of the matrices [ 85 ] , we can always perturb
the matrices slightly to make them invertible without changing the JSR
by much. In particular, for any @xmath there exist @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

is invertible and ( 5.12 ) @xmath ( 5.14 ) are satisfied with @xmath

To understand the generality of the framework of “path-complete graph
Lyapunov funcitons” more clearly, let us revisit the path-complete
graphs in Figure 5.3 for the study of the case where the set @xmath
consists of only two matrices. For all of these graphs if our choice for
the Lyapunov functions @xmath or @xmath and @xmath are quadratic
functions or sum of squares polynomial functions, then we can formulate
the well-established semidefinite programs that search for these
candidate Lyapunov functions.

Graph @xmath , which is clearly the simplest possible one, corresponds
to the well-known common Lyapunov function approach. Graph @xmath is a
common Lyapunov function applied to all products of length two. This
graph also obviously implies stability. ⁴ ⁴ 4 By slight abuse of
terminology, we say that a graph implies stability meaning that the
associated Lyapunov inequalities imply stability. But graph @xmath tells
us that if we find a Lyapunov function that decreases whenever @xmath ,
@xmath , and @xmath are applied (but with no requirement when @xmath is
applied), then we still get stability. This is a priori not obvious and
we believe this approach has not appeared in the literature before.
Graph @xmath is also an example that explains why we needed the
expansion process. Note that for the unexpanded graph, there is no path
for any word of the form @xmath or of the form @xmath , for any @xmath
However, one can check that in the expanded graph of graph @xmath ,
there is a path for every finite word, and this in turn allows us to
conclude stability from the Lyapunov inequalities of graph @xmath .

The remaining graphs in Figure 5.3 which all have two nodes and four
edges with labels of length one have a connection to the method of
min-of-quadratics or max-of-quadratics Lyapunov functions [ 81 ] , [ 80
] , [ 64 ] , [ 82 ] . If Lyapunov inequalities associated with any of
these four graphs are satisfied, then either @xmath or @xmath or both
serve as a common Lyapunov function for the switched system. In the next
section, we assert these facts in a more general setting (Corollaries
5.8 and 5.9 ) and show that these graphs in some sense belong to
“simplest” families of path-complete graphs.

### 5.3 Duality and examples of families of path-complete graphs

Now that we have shown that any path-complete graph introduces a method
for proving stability of switched systems, our next focus is naturally
on showing how one can produce graphs that are path-complete. Before we
proceed to some basic constructions of such graphs, let us define a
notion of duality among graphs which essentially doubles the number of
path-complete graphs that we can generate.

###### Definition 5.5.

Given a directed graph @xmath whose edges are labeled from the words in
@xmath , we define its dual graph @xmath to be the graph obtained by
reversing the direction of the edges of @xmath , and changing the labels
@xmath of every edge of @xmath to its reversed version @xmath .

An example of a pair of dual graphs with labels of length one is given
in Figure 5.4 . The following theorem relates dual graphs and
path-completeness.

###### Theorem 5.6.

If a graph @xmath is path-complete, then its dual graph @xmath is also
path-complete.

###### Proof.

Consider an arbitrary finite word @xmath . By definition of what it
means for a graph to be path-complete, our task is to show that there
exists a path corresponding to this word in the expanded graph of the
dual graph @xmath . It is easy to see that the expanded graph of the
dual graph of @xmath is the same as the dual graph of the expanded graph
of @xmath ; i.e, @xmath . Therefore, we show a path for @xmath in @xmath
. Consider the reversed word @xmath . Since @xmath is path-complete,
there is a path corresponding to this reversed word in @xmath . Now if
we just trace this path backwards, we get exactly a path for the
original word @xmath in @xmath . This completes the proof. ∎

The next proposition offers a very simple construction for obtaining a
large family of path-complete graphs with labels of length one.

###### Proposition 5.7.

A graph having any of the two properties below is path-complete.

Property (i): every node has outgoing edges with all the labels in
@xmath .

Property (ii): every node has incoming edges with all the labels in
@xmath .

###### Proof.

If a graph has Property (i), then it is obviously path-complete. If a
graph has Property (ii), then its dual has Property (i) and therefore by
Theorem 5.6 it is path-complete. ∎

Examples of path-complete graphs that fall in the category of this
proposition include graphs @xmath and @xmath in Figure 5.3 and all of
their dual graphs. By combining the previous proposition with Theorem
5.4 , we obtain the following two simple corollaries which unify several
linear matrix inequalities (LMIs) that have been previously proposed in
the literature. These corollaries also provide a link to
min/max-of-quadratics Lyapunov functions. Different special cases of
these LMIs have appeared in [ 81 ] , [ 80 ] , [ 64 ] , [ 82 ] , [ 93 ] ,
[ 53 ] . Note that the framework of path-complete graph Lyapunov
functions makes the proof of the fact that these LMIs imply stability
immediate.

###### Corollary 5.8.

Consider a set of @xmath matrices and the switched linear system in (
5.2 ) or ( 5.3 ). If there exist @xmath positive definite matrices
@xmath such that

  -- -------- -- --------
     @xmath      
     @xmath      (5.15)
  -- -------- -- --------

for some @xmath , then the system is absolutely asymptotically stable.
Moreover, the pointwise minimum

  -- -------- --
     @xmath   
  -- -------- --

of the quadratic functions serves as a common Lyapunov function.

###### Proof.

The inequalities in ( 5.8 ) imply that every node of the associated
graph has outgoing edges labeled with all the different @xmath matrices.
Therefore, by Proposition 5.7 the graph is path-complete, and by Theorem
5.4 this implies absolute asymptotic stability. The proof that the
pointwise minimum of the quadratics is a common Lyapunov function is
easy and left to the reader. ∎

###### Corollary 5.9.

Consider a set of @xmath matrices and the switched linear system in (
5.2 ) or ( 5.3 ). If there exist @xmath positive definite matrices
@xmath such that

  -- -------- -- --------
     @xmath      
     @xmath      (5.16)
  -- -------- -- --------

for some @xmath , then the system is absolutely asymptotically stable.
Moreover, the pointwise maximum

  -- -------- --
     @xmath   
  -- -------- --

of the quadratic functions serves as a common Lyapunov function.

###### Proof.

The inequalities in ( 5.9 ) imply that every node of the associated
graph has incoming edges labeled with all the different @xmath matrices.
Therefore, by Proposition 5.7 the graph is path-complete and the proof
of absolute asymptotic stability then follows. The proof that the
pointwise maximum of the quadratics is a common Lyapunov function is
again left to the reader. ∎

###### Remark 5.3.1.

The linear matrix inequalities in ( 5.8 ) and ( 5.9 ) are (convex)
sufficient conditions for existence of min-of-quadratics or
max-of-quadratics Lyapunov functions. The converse is not true. The
works in [ 81 ] , [ 80 ] , [ 64 ] , [ 82 ] have additional multipliers
in ( 5.8 ) and ( 5.9 ) that make the inequalities non-convex but when
solved with a heuristic method contain a larger family of
min-of-quadratics and max-of-quadratics Lyapunov functions. Even if the
non-convex inequalities with multipliers could be solved exactly, except
for special cases where the @xmath -procedure is exact (e.g., the case
of two quadratic functions), these methods still do not completely
characterize min-of-quadratics and max-of-quadratics functions.

###### Remark 5.3.2.

The work in [ 93 ] on “path-dependent quadratic Lyapunov functions” and
the work in [ 53 ] on “parameter dependent Lyapunov functions”–when
specialized to the analysis of arbitrary switched linear systems–are
special cases of Corollary 5.8 and 5.9 respectively. This observation
makes a connection between these techniques and min/max-of-quadratics
Lyapunov functions which is not established in [ 93 ] , [ 53 ] . It is
also interesting to note that the path-complete graph corresponding to
the LMIs proposed in [ 93 ] (see Theorem 9 there) is the well-known De
Bruijn graph [ 67 ] .

The set of path-complete graphs is much broader than the set of simple
family of graphs constructed in Proposition 5.7 . Indeed, there are many
graphs that are path-complete without having outgoing (or incoming)
edges with all the labels on every node; see e.g. graph @xmath in Figure
5.5 . This in turn means that there are several more sophisticated
Lyapunov inequalities that we can explore for proving stability of
switched systems. Below, we give one particular example of such
“non-obvious” inequalities for the case of switching between two
matrices.

###### Proposition 5.10.

Consider the set @xmath and the switched linear system in ( 5.2 ) or (
5.3 ). If there exist a positive definite matrix @xmath such that

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

for some @xmath , then the system is absolutely asymptotically stable.

###### Proof.

The graph @xmath associated with the LMIs above and its expanded version
@xmath are drawn in Figure 5.5 . We leave it as an exercise for the
reader to show (e.g. by induction on the length of the word) that there
is path for every finite word in @xmath . Therefore, @xmath is
path-complete and in view of Theorem 5.4 the claim is established. ∎

###### Remark 5.3.3.

Proposition 5.10 can be generalized as follows: If a single Lyapunov
function decreases with respect to the matrix products

  -- -------- --
     @xmath   
  -- -------- --

for some integer @xmath , then the arbitrary switched system consisting
of the two matrices @xmath and @xmath is absolutely asymptotically
stable. We omit the proof of this generalization due to space
limitations. We will later prove (Theorem 5.18 ) a bound for the quality
of approximation of path-complete graphs of this type, where a common
Lyapunov function is required to decrease with respect to products of
different lengths.

When we have so many different ways of imposing conditions for
stability, it is natural to ask which ones are better. The answer
clearly depends on the combinatorial structure of the graphs and does
not seem to be easy in general. Nevertheless, in the next section, we
compare the performance of all path-complete graphs with two nodes for
analysis of switched systems with two matrices. The connections between
the bounds obtained from these graphs are not always obvious. For
example, we will see that the graphs @xmath and @xmath always give the
same bound on the joint spectral radius; i.e, one graph will succeed in
proving stability if and only if the other will. So, there is no point
in increasing the number of decision variables and the number of
constraints and impose @xmath or @xmath in place of @xmath . The same is
true for the graphs in @xmath and @xmath , which makes graph @xmath
preferable to graph @xmath . (See Proposition 5.12 .)

### 5.4 Path-complete graphs with two nodes

In this section, we characterize the set of all path-complete graphs
consisting of two nodes, an alphabet set @xmath and edge labels of unit
length. We will elaborate on the set of all admissible topologies
arising in this setup and compare the performance—in the sense of
conservatism of the ensuing analysis—of different path-complete graph
topologies.

#### 5.4.1 The set of path-complete graphs

The next lemma establishes that for thorough analysis of the case of two
matrices and two nodes, we only need to examine graphs with four or
fewer edges.

###### Lemma 5.11.

Let @xmath be a path-complete graph with labels of length one for @xmath
. Let @xmath be a graph Lyapunov function for @xmath If @xmath then,
either
(i) there exists @xmath such that @xmath is a path-complete graph,
or
(ii) either @xmath or @xmath or both are common Lyapunov functions for
@xmath

###### Proof.

If @xmath then at least one node has three or more outgoing edges.
Without loss of generality let node @xmath be a node with exactly three
outgoing edges @xmath , and let @xmath Let @xmath denote the destination
node of an edge @xmath If @xmath then @xmath (or @xmath ) can be removed
without changing the output set of words. If @xmath assume, without loss
of generality, that @xmath and @xmath Now, if @xmath then regardless of
its destination node, @xmath can be removed. If @xmath and @xmath , then
@xmath is a common Lyapunov function for @xmath . The only remaining
possibility is that @xmath and @xmath Note that there must be an edge
@xmath from node @xmath to node @xmath , otherwise either node @xmath
would have two self-edges with the same label or @xmath would be a
common Lyapunov function for @xmath . If @xmath then it can be verified
that @xmath is path-complete and thus all other edge can be removed. If
there is no edge from node @xmath to node @xmath with label @xmath then
@xmath and node @xmath must have a self-edge @xmath with label @xmath ,
otherwise the graph would not be path-complete. In this case, it can be
verified that @xmath can be removed without affecting the output set of
words. ∎

It can be verified that a path-complete graph with two nodes and less
than four edges must necessarily place two self-loops with different
labels on one node, which necessitates existence of a common Lyapunov
function for the underlying switched system. Since we are interested in
exploiting the favorable properties of graph Lyapunov functions in
approximation of the JSR, we will focus on graphs with four edges.

Before we proceed, for convenience we introduce the following notation:
Given a labeled graph @xmath associated with two matrices @xmath and
@xmath , we denote by @xmath , the graph obtained by swapping of @xmath
and @xmath in all the labels on every edge.

#### 5.4.2 Comparison of performance

It can be verified that for path-complete graphs with two nodes, four
edges, and two matrices, and without multiple self-loops on a single
node, there are a total of nine distinct graph topologies to consider.
Of the nine graphs, six have the property that every node has two
incoming edges with different labels. These are graphs @xmath and @xmath
(Figure 5.3 ). Note that @xmath and @xmath . The duals of these six
graphs, i.e., @xmath and @xmath have the property that every node has
two outgoing edges with different labels. Evidently, @xmath and @xmath
are self-dual graphs , i.e., they are isomorphic to their dual graphs.
The self-dual graphs are least interesting to us since, as we will show,
they necessitate existence of a common Lyapunov function for @xmath (cf.
Proposition 5.12 , equation ( 5.18 )) @xmath

Note that all of these graphs perform at least as well as a common
Lyapunov function because we can always take @xmath . Furthermore, we
know from Corollaries 5.9 and 5.8 that if Lyapunov inequalities
associated with @xmath and @xmath are satisfied, then @xmath is a common
Lyapunov function, whereas, in the case of graphs @xmath , and @xmath ,
the function @xmath would serve as a common Lyapunov function. Clearly,
for the self-dual graphs @xmath and @xmath both @xmath and @xmath are
common Lyapunov functions.

Notation: Given a set of matrices @xmath a path-complete graph @xmath
and a class of functions @xmath we denote by @xmath the upper bound on
the JSR of @xmath that can be obtained by numerical optimization of GLFs
@xmath defined over @xmath With a slight abuse of notation, we denote by
@xmath the upper bound that is obtained by using a common Lyapunov
function @xmath

###### Proposition 5.12.

Consider the set @xmath and let @xmath , and @xmath be the path-complete
graphs shown in Figure 5.3 . Then, the upper bounds on the JSR of @xmath
obtained by analysis via the associated GLFs satisfy the following
relations:

  -- -- -- --------
           (5.17)
  -- -- -- --------

and

  -- -- -- --------
           (5.18)
  -- -- -- --------

and

  -- -- -- --------
           (5.19)
  -- -- -- --------

and

  -- -------- -- --------
     @xmath      (5.20)
  -- -------- -- --------

###### Proof.

A proof of ( 5.17 ) in more generality is provided in Section 5.5 (cf.
Corollary 5.15 ). The proof of ( 5.18 ) is based on symmetry arguments.
Let @xmath be a GLF associated with @xmath ( @xmath is associated with
node @xmath and @xmath is associated with node @xmath ). Then, by
symmetry, @xmath is also a GLF for @xmath (where @xmath is associated
with node @xmath and @xmath is associated with node @xmath ). Therefore,
letting @xmath , we have that @xmath is a GLF for @xmath and thus,
@xmath is also a common Lyapunov function for @xmath which implies that
@xmath The other direction is trivial: If @xmath is a common Lyapunov
function for @xmath then @xmath is a GLF associated with @xmath and
hence, @xmath Identical arguments based on symmetry hold for @xmath and
@xmath . We now prove the left equality in ( 5.19 ), the proofs for the
remaining equalities in ( 5.19 ) and ( 5.20 ) are analogous. The
equivalence between @xmath and @xmath is a special case of the relation
between a graph and its reduced model, obtained by removing a node
without any self-loops, adding a new edge per each pair of incoming and
outgoing edges to that node, and then labeling the new edges by taking
the composition of the labels of the corresponding incoming and outgoing
edges in the original graph; see [ 145 ] , [ 144 , Chap. 5] . Note that
@xmath is an offspring of @xmath in this sense. This intuition helps
construct a proof. Let @xmath be a GLF associated with @xmath It can be
verified that @xmath is a Lyapunov function associated with @xmath and
therefore, @xmath Similarly, if @xmath is a Lyapunov function associated
with @xmath then one can check that @xmath is a GLF associated with
@xmath and hence, @xmath ∎

###### Remark 5.4.1.

Proposition 5.12 (equation 5.17 ) establishes the equivalence of the
bounds obtained from the pair of dual graphs @xmath and @xmath . This,
however, is not true for graphs @xmath and @xmath as there exist
examples for which

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

The diagram in Figure 5.6 summarizes the results of this section. We
remark that no relations other than the ones given in Figure 5.6 can be
made among these path-complete graphs. Indeed, whenever there are no
relations between two graphs in Figure 5.6 , we have examples of
matrices @xmath (not presented here) for which one graph can outperform
the other.

The graphs @xmath and @xmath seem to statistically perform better than
all other graphs in Figure 5.6 . For example, we ran experiments on a
set of @xmath random @xmath matrices @xmath with elements uniformly
distributed in @xmath to compare the performance of graphs @xmath and
@xmath . If in each case we also consider the relabeled matrices (i.e.,
@xmath ) as our input, then, out of the total @xmath instances, graph
@xmath produced strictly better bounds on the JSR @xmath times, whereas
graphs @xmath and @xmath each produced the best bound of the three
graphs only @xmath times. (The numbers do not add up to @xmath due to
ties.) In addition to this superior performance, the bound @xmath
obtained by analysis via the graph @xmath is invariant under (i)
permutation of the labels @xmath and @xmath (obvious), and (ii)
transposing of @xmath and @xmath (Corollary 5.15 ). These are desirable
properties which fail to hold for @xmath and @xmath or their duals.
Motivated by these observations, we generalize @xmath and its dual
@xmath in the next section to the case of @xmath matrices and @xmath
Lyapunov functions and establish that they have certain appealing
properties. We will prove (cf. Theorem 5.16 ) that these graphs always
perform better than a common Lyapunov function in 2 steps (i.e., the
graph @xmath in Figure 5.3 ), whereas, this is not the case for @xmath
and @xmath or their duals.

### 5.5 Further analysis of a particular family of path-complete graphs

The framework of path-complete graphs provides a multitude of
semidefinite programming based techniques for the approximation of the
JSR whose performance vary with computational cost. For instance, as we
increase the number of nodes of the graph, or the degree of the
polynomial Lyapunov functions assigned to the nodes, or the number of
edges of the graph that instead of labels of length one have labels of
higher length, we obtain better results but at a higher computational
cost. Many of these approximation techniques are asymptotically tight,
so in theory they can be used to achieve any desired accuracy of
approximation. For example,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denotes the class of sum of squares homogeneous polynomial
Lyapunov functions of degree @xmath . (Recall our notation for bounds
from Section 5.4.2 .) It is also true that a common quadratic Lyapunov
function for products of higher length achieves the true JSR
asymptotically [ 85 ] ; i.e. ⁵ ⁵ 5 By @xmath we denote the class of
quadratic homogeneous polynomials. We drop the superscript “SOS” because
nonnegative quadratic polynomials are always sums of squares. ,

  -- -------- --
     @xmath   
  -- -------- --

Nevertheless, it is desirable for practical purposes to identify a class
of path-complete graphs that provide a good tradeoff between quality of
approximation and computational cost. Towards this objective, we propose
the use of @xmath quadratic Lyapunov functions assigned to the nodes of
the De Bruijn graph of order @xmath on @xmath symbols for the
approximation of the JSR of a set of @xmath matrices. This graph and its
dual are particular path-complete graphs with @xmath nodes and @xmath
edges and will be the subject of study in this section. If we denote the
quadratic Lyapunov functions by @xmath , then we are proposing the use
of linear matrix inequalities

  -- -------- -- --------
     @xmath      (5.21)
  -- -------- -- --------

or the set of LMIs

  -- -------- -- --------
     @xmath      (5.22)
  -- -------- -- --------

for the approximation of the JSR of @xmath matrices. Throughout this
section, we denote the path-complete graphs associated with ( 5.21 ) and
( 5.22 ) with @xmath and @xmath respectively. (The De Bruijn graph of
order @xmath , by standard convention, is actually the graph @xmath .)
Observe that @xmath and @xmath are indeed dual graphs as they can be
obtained from each other by reversing the direction of the edges. For
the case @xmath , our notation is consistent with the previous section
and these graphs are illustrated in Figure 5.4 . Also observe from
Corollary 5.8 and Corollary 5.9 that the LMIs in ( 5.21 ) give rise to
max-of-quadratics Lyapunov functions, whereas the LMIs in ( 5.22 ) lead
to min-of-quadratics Lyapunov functions. We will prove in this section
that the approximation bound obtained by these LMIs (i.e., the
reciprocal of the largest @xmath for which the LMIs ( 5.21 ) or ( 5.22 )
hold) is always the same and lies within a multiplicative factor of
@xmath of the true JSR, where @xmath is the dimension of the matrices.
The relation between the bound obtained by a pair of dual path-complete
graphs has a connection to transposition of the matrices in the set
@xmath . We explain this next.

#### 5.5.1 Duality and invariance under transposition

In [ 63 ] , [ 64 ] , it is shown that absolute asymptotic stability of
the linear difference inclusion in ( 5.3 ) defined by the matrices
@xmath is equivalent to absolute asymptotic stability of ( 5.3 ) for the
transposed matrices @xmath . Note that this fact is immediately seen
from the definition of the JSR in ( 5.1 ), since @xmath . It is also
well-known that

  -- -------- --
     @xmath   
  -- -------- --

Indeed, if @xmath is a common quadratic Lyapunov function for the set
@xmath , then it is easy to show that @xmath is a common quadratic
Lyapunov function for the set @xmath . However, this nice property is
not true for the bound obtained from some other techniques. For
instance, the next example shows that

  -- -------- -- --------
     @xmath      (5.23)
  -- -------- -- --------

i.e, the upper bound obtained by searching for a common quartic sos
polynomial is not invariant under transposition.

###### Example 5.5.1.

Consider the set of matrices @xmath with

  -- -------- --
     @xmath   
  -- -------- --

We have @xmath but @xmath (up to three significant digits). @xmath

Similarly, the bound obtained by non-convex inequalities proposed in [
63 ] is not invariant under transposing the matrices. For such methods,
one would have to run the numerical optimization twice—once for the set
@xmath and once for the set @xmath —and then pick the better bound of
the two. We will show that by contrast, the bound obtained from the LMIs
in ( 5.21 ) and ( 5.22 ) are invariant under transposing the matrices.
Before we do that, let us prove a general result which states that for
path-complete graphs with quadratic Lyapunov functions as nodes,
transposing the matrices has the same effect as dualizing the graph.

###### Theorem 5.13.

Let @xmath be a path-complete graph, and let @xmath be its dual graph.
Then,

  -- -------- -- --------
     @xmath      (5.24)
  -- -------- -- --------

###### Proof.

For ease of notation, we prove the claim for the case where the edge
labels of @xmath have length one. The proof of the general case is
identical. Pick an arbitrary edge @xmath going from node @xmath to node
@xmath and labeled with some matrix @xmath . By the application of the
Schur complement we have

  -- -------- --
     @xmath   
  -- -------- --

But this already establishes the claim since we see that @xmath and
@xmath satisfy the LMI associated with edge @xmath when the matrix
@xmath is transposed if and only if @xmath and @xmath satisfy the LMI
associated with edge @xmath . ∎

###### Corollary 5.14.

@xmath if and only if @xmath .

###### Proof.

This is an immediate consequence of the equality in ( 5.24 ). ∎

It is an interesting question for future research to characterize the
topologies of path-complete graphs for which one has @xmath For example,
the above corollary shows that this is obviously the case for any
path-complete graph that is self-dual. Let us show next that this is
also the case for graphs @xmath and @xmath despite the fact that they
are not self-dual.

###### Corollary 5.15.

For the path-complete graphs @xmath and @xmath associated with the
inequalities in ( 5.21 ) and ( 5.22 ), and for any class of continuous,
homogeneous, and positive definite functions @xmath , we have

  -- -------- -- --------
     @xmath      (5.25)
  -- -------- -- --------

Moreover, if quadratic Lyapunov functions are assigned to the nodes of
@xmath and @xmath , then we have

  -- -------- -- --------
     @xmath      (5.26)
  -- -------- -- --------

###### Proof.

The proof of ( 5.25 ) is established by observing that the GLFs
associated with @xmath and @xmath can be derived from one another via
@xmath (Note that we are relying here on the assumption that the
matrices @xmath are invertible, which as we noted in Remark 5.2.1 , is
not a limiting assumption.) Since ( 5.25 ) in particular implies that
@xmath , we get the rest of the equalities in ( 5.26 ) immediately from
Corollary 5.14 and this finishes the proof. For concreteness, let us
also prove the leftmost equality in ( 5.26 ) directly. Let @xmath ,
@xmath satisfy the LMIs in ( 5.21 ) for the set of matrices @xmath .
Then, the reader can check that

  -- -------- --
     @xmath   
  -- -------- --

satisfy the LMIs in ( 5.21 ) for the set of matrices @xmath . ∎

#### 5.5.2 An approximation guarantee

The next theorem gives a bound on the quality of approximation of the
estimate resulting from the LMIs in ( 5.21 ) and ( 5.22 ). Since we have
already shown that @xmath it is enough to prove this bound for the LMIs
in ( 5.21 ).

###### Theorem 5.16.

Let @xmath be a set of @xmath matrices in @xmath with JSR @xmath . Let
@xmath be the bound on the JSR obtained from the LMIs in ( 5.21 ). Then,

  -- -------- -- --------
     @xmath      (5.27)
  -- -------- -- --------

###### Proof.

The right inequality is just a consequence of @xmath being a
path-complete graph (Theorem 5.4 ). To prove the left inequality,
consider the set @xmath consisting of all @xmath products of length two.
In view of ( 5.6 ), a common quadratic Lyapunov function for this set
satisfies the bound

  -- -------- --
     @xmath   
  -- -------- --

It is easy to show that

  -- -------- --
     @xmath   
  -- -------- --

See e.g. [ 85 ] . Therefore,

  -- -------- -- --------
     @xmath      (5.28)
  -- -------- -- --------

Now suppose for some @xmath , @xmath is a common quadratic Lyapunov
function for the matrices in @xmath ; i.e., it satisfies

  -- -------- --
     @xmath   
  -- -------- --

Then, we leave it to the reader to check that

  -- -------- --
     @xmath   
  -- -------- --

satisfy ( 5.21 ). Hence,

  -- -------- --
     @xmath   
  -- -------- --

and in view of ( 5.28 ) the claim is established. ∎

Note that the bound in ( 5.27 ) is independent of the number of
matrices. Moreover, we remark that this bound is tighter, in terms of
its dependence on @xmath , than the known bounds for @xmath for any
finite degree @xmath of the sum of squares polynomials. The reader can
check that the bound in ( 5.7 ) goes asymptotically as @xmath .
Numerical evidence suggests that the performance of both the bound
obtained by sum of squares polynomials and the bound obtained by the
LMIs in ( 5.21 ) and ( 5.22 ) is much better than the provable bounds in
( 5.7 ) and in Theorem 5.16 . The problem of improving these bounds or
establishing their tightness is open. It goes without saying that
instead of quadratic functions, we can associate sum of squares
polynomials to the nodes of @xmath and obtain a more powerful technique
for which we can also prove better bounds with the exact same arguments.

#### 5.5.3 Numerical examples

In the proof of Theorem 5.16 , we essentially showed that the bound
obtained from LMIs in ( 5.21 ) is tighter than the bound obtained from a
common quadratic applied to products of length two. Our first example
shows that the LMIs in ( 5.21 ) can in fact do better than a common
quadratic applied to products of any finite length.

###### Example 5.5.2.

Consider the set of matrices @xmath with

  -- -------- --
     @xmath   
  -- -------- --

This is a benchmark set of matrices that has been studied in [ 13 ] , [
122 ] , [ 6 ] because it gives the worst case approximation ratio of a
common quadratic Lyapunov function. Indeed, it is easy to show that
@xmath , but @xmath . Moreover, the bound obtained by a common quadratic
function applied to the set @xmath is

  -- -------- --
     @xmath   
  -- -------- --

which for no finite value of @xmath is exact. On the other hand, we show
that the LMIs in ( 5.21 ) give the exact bound; i.e., @xmath . Due to
the simple structure of @xmath and @xmath , we can even give an
analytical expression for our Lyapunov functions. Given any @xmath , the
LMIs in ( 5.21 ) with @xmath are feasible with

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath and @xmath @xmath

###### Example 5.5.3.

Consider the set of randomly generated matrices @xmath with

  -- -------- --
     @xmath   
  -- -------- --

A lower bound on @xmath is @xmath . The upper approximations for @xmath
that we computed for this example are as follows:

  -- -------- -- --------
     @xmath      (5.29)
  -- -------- -- --------

The bound @xmath matches the lower bound numerically and is most likely
exact for this example. This bound is slightly better than @xmath .
However, a simple calculation shows that the semidefinite program
resulting in @xmath has 25 more decision variables than the one for
@xmath . Also, the running time of the algorithm leading to @xmath is
noticeably larger than the one leading to @xmath . In general, when the
dimension of the matrices is large, it can often be cost-effective to
increase the number of the nodes of our path-complete graphs but keep
the degree of the polynomial Lyapunov functions assigned to its nodes
relatively low. @xmath

### 5.6 Converse Lyapunov theorems and approximation with arbitrary
accuracy

It is well-known that existence of a Lyapunov function which is the
pointwise maximum of quadratics is not only sufficient but also
necessary for absolute asymptotic stability of ( 5.2 ) or ( 5.3 ); see
e.g. [ 105 ] . This is perhaps an intuitive fact if we recall that
switched systems of type ( 5.2 ) and ( 5.3 ) always admit a convex
Lyapunov function. Indeed, if we take “enough” quadratics, the convex
and compact unit sublevel set of a convex Lyapunov function can be
approximated arbitrarily well with sublevel sets of max-of-quadratics
Lyapunov functions, which are intersections of ellipsoids. This of
course implies that the bound obtained from max-of-quadratics Lyapunov
functions is asymptotically tight for the approximation of the JSR.
However, this converse Lyapunov theorem does not answer two natural
questions of importance in practice: (i) How many quadratic functions do
we need to achieve a desired quality of approximation? (ii) Can we
search for these quadratic functions via semidefinite programming or do
we need to resort to non-convex formulations? Our next theorem provides
an answer to these questions.

###### Theorem 5.17.

Let @xmath be a set of @xmath matrices in @xmath . Given any positive
integer @xmath , there exists an explicit path-complete graph @xmath
consisting of @xmath nodes assigned to quadratic Lyapunov functions and
@xmath edges with labels of length one such that the linear matrix
inequalities associated with @xmath imply existence of a
max-of-quadratics Lyapunov function and the resulting bound obtained
from the LMIs satisfies

  -- -------- -- --------
     @xmath      (5.30)
  -- -------- -- --------

###### Proof.

Let us denote the @xmath quadratic Lyapunov functions by @xmath , where
@xmath is a multi-index used for ease of reference to our Lyapunov
functions. We claim that we can let @xmath be the graph dual to the
De Bruijn graph of order @xmath on @xmath symbols. The LMIs associated
to this graph are given by

  -- -------- -- --------
     @xmath      (5.31)
  -- -------- -- --------

The fact that @xmath is path-complete and that the LMIs imply existence
of a max-of-quadratics Lyapunov function follows from Corollary 5.9 .
The proof that these LMIs satisfy the bound in ( 5.30 ) is a
straightforward generalization of the proof of Theorem 5.16 . By the
same arguments we have

  -- -------- -- --------
     @xmath      (5.32)
  -- -------- -- --------

Suppose @xmath is a common quadratic Lyapunov function for the matrices
in @xmath ; i.e., it satisfies

  -- -------- --
     @xmath   
  -- -------- --

Then, it is easy to check that ⁶ ⁶ 6 The construction of the Lyapunov
function here is a special case of a general scheme for constructing
Lyapunov functions that are monotonically decreasing from those that
decrease only every few steps; see [ 1 , p. 58] .

  -- -------- --
     @xmath   
  -- -------- --

satisfy ( 5.31 ). Hence,

  -- -------- --
     @xmath   
  -- -------- --

and in view of ( 5.32 ) the claim is established. ∎

###### Remark 5.6.1.

A converse Lyapunov theorem identical to Theorem 5.17 can be proven for
the min-of-quadratics Lyapunov functions. The only difference is that
the LMIs in ( 5.31 ) would get replaced by the ones corresponding to the
dual graph of @xmath .

Our last theorem establishes approximation bounds for a family of
path-complete graphs with one single node but several edges labeled with
words of different lengths. Examples of such path-complete graphs
include graph @xmath in Figure 5.3 and graph @xmath in Figure 5.5 .

###### Theorem 5.18.

Let @xmath be a set of matrices in @xmath Let @xmath be a path-complete
graph, and @xmath be the length of the shortest word in @xmath Then
@xmath provides an estimate of @xmath that satisfies

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

The right inequality is obvious, we prove the left one. Since both
@xmath and @xmath are homogeneous in @xmath we may assume, without loss
of generality, that @xmath . Suppose for the sake of contradiction that

  -- -------- -- --------
     @xmath      (5.33)
  -- -------- -- --------

We will show that this implies that @xmath . Towards this goal, let us
first prove that @xmath Indeed, if we had @xmath , then there would
exist ⁷ ⁷ 7 Here, we are appealing to the well-known fact about the JSR
of a general set of matrices @xmath : @xmath See e.g. [ 85 , Chap. 1] .
an integer @xmath and a product @xmath such that

  -- -------- -- --------
     @xmath      (5.34)
  -- -------- -- --------

Since we also have @xmath (for some @xmath ), it follows that

  -- -------- -- --------
     @xmath      (5.35)
  -- -------- -- --------

The inequality in ( 5.34 ) together with @xmath gives

  -- -------- --
     @xmath   
  -- -------- --

But this contradicts ( 5.35 ). Hence we have shown

  -- -------- --
     @xmath   
  -- -------- --

Now, by our hypothesis ( 5.33 ) above, we have that @xmath Therefore,
there exists @xmath such that @xmath It then follows from ( 5.6 ) that
there exists a common quadratic Lyapunov function for @xmath Hence,
@xmath which immediately implies that @xmath a contradiction. ∎

A noteworthy immediate corollary of Theorem 5.18 (obtained by setting
@xmath is the following: If @xmath , then there exists a quadratic
Lyapunov function that decreases simultaneously for all products of
lengths @xmath , for any desired value of @xmath . Note that this fact
is obvious for @xmath , but nonobvious for @xmath .

### 5.7 Conclusions and future directions

We introduced the framework of path-complete graph Lyapunov functions
for the formulation of semidefinite programming based algorithms for
approximating the joint spectral radius (or equivalently establishing
absolute asymptotic stability of an arbitrary switched linear system).
We defined the notion of a path-complete graph, which was inspired by
concepts in automata theory. We showed that every path-complete graph
gives rise to a technique for the approximation of the JSR. This
provided a unifying framework that includes many of the previously
proposed techniques and also introduces new ones. (In fact, all families
of LMIs that we are aware of are particular cases of our method.) We
shall also emphasize that although we focused on switched linear systems
because of our interest in the JSR, the analysis technique of multiple
Lyapunov functions on path-complete graphs is clearly valid for switched
nonlinear systems as well.

We compared the quality of the bound obtained from certain classes of
path-complete graphs, including all path-complete graphs with two nodes
on an alphabet of two matrices, and also a certain family of dual
path-complete graphs. We proposed a specific class of such graphs that
appear to work particularly well in practice and proved that the bound
obtained from these graphs is invariant under transposition of the
matrices and is always within a multiplicative factor of @xmath from the
true JSR. Finally, we presented two converse Lyapunov theorems, one for
the well-known methods of minimum and maximum-of-quadratics Lyapunov
functions, and the other for a new class of methods that propose the use
of a common quadratic Lyapunov function for a set of words of possibly
different lengths.

We believe the methodology proposed in this chapter should
straightforwardly extend to the case of constrained switching by
requiring the graphs to have a path not for all the words, but only the
words allowed by the constraints on the switching. A rigorous treatment
of this idea is left for future work.

Vincent Blondel showed that when the underlying automaton is not
deterministic, checking path-completeness of a labeled directed graph is
an NP-hard problem (personal communication). In general, the problem of
deciding whether a non-deterministic finite automaton accepts all finite
words is known to be PSPACE-complete [ 61 , p. 265] . However, we are
yet to investigate whether the same is true for automata arising from
path-complete graphs which have a little more structure. At the moment,
the NP-hardness proof of Blondel remains as the strongest negative
result we have on this problem. Of course, the step of checking
path-completeness of a graph is done offline and prior to the run of our
algorithms for approximating the JSR. Therefore, while checking
path-completeness is in general difficult, the approximation algorithms
that we presented indeed run in polynomial time since they work with a
fixed (a priori chosen) path-complete graph. Nevertheless, the question
on complexity of checking path-completeness is interesting in many other
settings, e.g., when deciding whether a given set of Lyapunov
inequalities imply stability of an arbitrary switched system.

Some other interesting questions that can be explored in the future are
the following. What are some other classes of path-complete graphs that
lead to new techniques for proving stability of switched systems? How
can we compare the performance of different path-complete graphs in a
systematic way? Given a set of matrices, a class of Lyapunov functions,
and a fixed size for the graph, can we efficiently come up with the
least conservative topology of a path-complete graph? Within the
framework that we proposed, do all the Lyapunov inequalities that prove
stability come from path-complete graphs? What are the analogues of the
results of this chapter for continuous time switched systems? To what
extent do the results carry over to the synthesis (controller design)
problem for switched systems? These questions and several others show
potential for much follow-up work on path-complete graph Lyapunov
functions.