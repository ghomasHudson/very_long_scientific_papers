##### Acknowledgements. The work in this thesis is only possible due to
the continual support of many wonderful people, whom I feel privileged
to thank here. My advisors, Peter Ramadge and Ingrid Daubechies, have
had a particularly important role in my direction over the last six
years. I thank Peter for teaching me to be direct, careful, and
professional in my work and my career; these lessons will stay with me
always. I thank Ingrid for her warmness, her excitement (about
everything!), and her friendship over these years. I thank them both for
their flexibility in allowing me to explore my interests throughout my
PhD, and for the many discussions that have directed the development,
the tone, and the substance of my work. I owe a great deal to my
collaborators, with whom it has been a great pleasure to work, and who I
am lucky to call friends: Shannon Hughes, Rodolfo Ríos-Zertuche,
Amit Singer, Hau-Tieng Wu, Gaurav Thakur, Neven S. Fučkar, and
Frederik J. Simons. Without their energy and expertise, this thesis
would not have been possible. Many people have changed my life over the
last six years, and I especially want to thank them here. Carol, for her
support and guidance. My Magie open-door roommates, more like brothers:
Abhishek, Aman, Deep, and Sushobhan. Adam, Reza, and Dano: always far
and always near. Arvind, and Theodor, roommates extraordinaire! My
Princeton Anonymous group: Stephanie, Yury, Lorne, Kevin, Daniel, Jeff,
Matt, Ana, CJ, DJ, Seth, Manos, Kostas, Silvana, Jessbee, Katie, Sibren,
Tanushree, Sharon, and Sahar. My fellow Ramadge groupies: Mert Rory,
Bryan, Taylor, James, Alex, David, and Hao. Most importantly, I thank my
parents and my sister Marina for their calm and unwavering confidence in
my ability to always achieve my goals. And Taya, whose support,
encouragement, and love throughout has been nothing short of miraculous.
Thank you. \makefrontmatter

## Chapter 1 Introduction

This chapter introduces the main topics covered in this thesis, and the
major tools and ideas that link these topics. The discussion is
philosophical and purely motivational, and should not be considered
mathematically rigorous or exhaustive. Chapters 2 , 3 , 5 , and 7
contain their own relevant introductory discussions, descriptions of
prior work, and literature reviews.

An inverse problem is a general question of the following form:

You are given observed measurements @xmath , where @xmath , is a mapping
from domain @xmath to range @xmath . Provide an estimate of a physical
object or underlying set of parameters @xmath .

Inverse problems encompass a variety of types of questions in many
fields, from geophysics and medical imaging to computer graphics. In a
high level sense, all of the following questions are inverse problems:

-   Given the output of a CT scanner (sinogram), reconstruct the 3D
    volumetric image of the patient’s chest.

-   Given noisy and limited satellite observations of the magnetic field
    over Australia, estimate the magnetic field everywhere on the
    continent.

-   Given the lightcurve of a periodic dwarf star, extract the
    significant slowly time-varying modes of oscillation. Determine
    which modes are associated with the rotation of the star, which are
    caused by transiting giant planets, and which are caused by one or
    more earth-sized planets.

-   Given a spline curve corrupted by additive Gaussian noise, identify
    its order, knot positions, and the parameters at each knot.

-   Given a social network graph with a variety of both known and
    unknown parameters at each node, and given a sample of known zip
    codes on a subset of the graph, estimate the zip codes of the rest
    of the nodes.

Of the questions above, only the first two are traditionally considered
to be inverse problems because they have a clear physical system @xmath
mapping inputs to outputs, and the domain and range are also clear from
the physics of the problem. Nevertheless, all of these problems can be
formulated as general inverse problems: a domain and range are given, as
are observations, and the simplest, most accurate, or most statistically
likely parameters to match the observations are required. This basic
premise encourages the “borrowing” and mixing of ideas from many fields
of mathematics and engineering: graph theory, statistics, functional
analysis, differential equations, and geometry.

This thesis focuses on finding representations of the data @xmath , or
of the underlying transform @xmath , that lead to either simpler or more
robust ways to solve particular inverse problems, or that provide a new
perspective for existing solutions.

We especially focus on finding representations within domains @xmath
which are important in fields such as medical imaging and geoscience, or
are becoming important in the signal processing community due to the
recent explosion of “high dimensional” data sets. These domains are:

-   General compact Riemannian manifolds.

-   Large graphs (especially nearest neighbor graphs from point samples
    of Riemannian manifolds embedded in @xmath ).

-   The Sphere @xmath .

A short technical description of Riemannian manifolds, and a set of
references, is given in App. A . A description of (Nearest Neighbor)
graphs can be found in § 2.2 .

The central tool that brings together all of the problems,
constructions, and insights in our work is the Laplacian. In a way, the
thrust of this thesis is the importance and flexibility of the Laplacian
as a data analysis and problem solving tool. Roughly speaking, the
Laplacian is defined as follows:

Given a domain @xmath , with measure @xmath and knowing for any point
@xmath its neighborhood @xmath , the Laplacian @xmath is a linear
operator that, for any well defined function @xmath , subtracts some
weighted average of @xmath in @xmath from its value at @xmath :

@xmath

where the weights are determined by the intrinsic relationship (e.g.,
distance) between @xmath and @xmath on @xmath .

The Laplacian is a powerful tool because it synthesizes local
information at each point @xmath in domain @xmath into global
information about the domain; and this in turn can be used to analyze
functions and other operators on this domain. This statement is
formalized in the classical language of Fourier analysis and the recent
work of Mikhail Belkin, Ronald Coifman, Peter Jones, and Amit Singer [ 6
, 24 , 53 , 96 ] : the eigenfunctions of the Laplacian are a powerful
tool for both the analysis of points in @xmath and functions defined on
@xmath .

Throughout this thesis, we use and study a variety of Laplacians, each
for a slightly different purpose:

-   The weighted graph Laplacian: its “averaging” component in Chapter 2
    and its regularized inverse in Chapters 3 and 4 .

-   The Laplace-Beltrami on a compact Riemannian manifold in Chapters 3
    and 4 .

-   The Fourier transform (projections on the eigenfunctions of the
    Laplacian on @xmath ) in Chapters 5 and 6 .

-   Spherical harmonics (eigenfunctions of the Laplace-Beltrami
    operator on @xmath ) and their use in analyzing bandlimited
    functions on the sphere in Chapter 7 .

The definition and properties of the weighted graph Laplacian are given
in § 3.3.3 . A definition of the Laplacian for a compact Riemannian
manifold is given in App. A , with specific definitions for the real
line and the sphere in App. B . This appendix also contains a
comprehensive discussion of Fourier analysis on Riemannian manifolds,
and on the Sphere in particular.

Though the definition of the Laplacian differs depending on the
underlying domain @xmath , all Laplacians are intricately linked: the
Laplacian on a domain @xmath converges to that on @xmath as the former
domain converges to the latter, or when one is a special case of the
other. We study these relationships when they are relevant (e.g., in §
3.3.3 ).

The term Nonlinear in the title of this thesis refers to the ways in
which our constructions use properties of the Laplacian, or of its
eigenfunctions, to represent data:

-   In Chapter 2 , the regularized inverse of the graph Laplacian is
    used to denoise graphs (this is not directly clear from the context
    but see § 4.6 for a discussion).

-   In Chapters 3 and 4 , the regularized inverse of both the graph
    Laplacian, and the Laplace Beltrami operator, are analyzed; and a
    special “trick” of taking the nonlinear logarithm is used to both
    perform regression and to estimate geodesic distances.

-   In Chapters 5 and 6 , the Fourier transform of the Wavelet
    representation of harmonic signals is used to motivate a time
    localized estimate of amplitude and frequency. This is followed up
    by a nonlinear reassignment of the Wavelet representation.

-   In Chapter 7 , we construct a multi-scale dictionary composed of
    bandlimited Slepian functions on the sphere (which are in turn
    constructed via Fourier analysis). This dictionary is then combined
    with nonlinear ( @xmath ) methods for signal estimation.

We hope that the underlying theme of this thesis is now clear: as the
movie Manhattan is Woody Allen’s ode to the city of New York, the work
herein is a testament to the flexibility and power of the Laplacian.

Without further ado, we now describe in detail the focus of the
individual chapters of this thesis.

### 1.1 Denoising and Inference on Graphs

Many unsupervised and semi-supervised learning problems contain
relationships between sample points that can be modeled with a graph. As
a result, the weighted adjacency matrix of a graph, and the associated
graph Laplacian, are often used to solve such problems. More
specifically, the spectrum of the graph Laplacian, and its regularized
inverse, can both be used to determine relationships between observed
data points (such as “neighborliness” or “connectedness”), and to
perform regression when a partial labeling of the data points is
available. Chapters 2 , 3 , and 4 study the inverse regularized graph
Laplacian.

Chapter 2 focuses on the unsupervised problem of detecting “bad” edges,
or bridges, in a graph constructed with erroneous edges. Such graphs
arise, e.g., as nearest neighbor graphs from high dimensional points
sampled under noise. A novel bridge detection rule is constructed, based
on Markov random walks with restarts, that robustly identifies bridges.
The detection rule uses the regularized inverse of the graph’s Laplacian
matrix, and its structure can be analyzed from a geometric point of view
under certain assumptions of the underlying sample points. We compare
this detection rule to past work and show its improved performance as a
preprocessing step in the estimation of geodesic distances on the
underlying graph, a global estimation problem. We also show its superior
performance as a preprocessing tool when solving the random projection
computational tomography inverse problem.

Chapter 3 studies a closely related problem, that of performing
regression on points sampled from a high dimensional space, only some of
which are labeled. We focus on the common case when the regression is
performed via the nearest neighbor graph of the points, with ridge and
Laplacian regularization. This common solution approach reduces to a
matrix-vector product, where the matrix is the regularized inverse of
the graph Laplacian, and the vector contains the partially known label
information.

In this chapter, we focus on the geometric aspects of the problem.
First, we prove that in the noiseless, low regularization case, when the
points are sampled from a smooth, compact, Riemannian manifold, the
matrix-vector product converges to a sampling of the solution to a
elliptic PDE. We use the theory of viscosity solutions to show that in
the low regularization case, the solution of this PDE encodes geodesic
distances between labeled points and unlabeled ones. This geometric PDE
framework provides key insights into the original semisupervised
regression problem, and into the regularized inverse of the graph
Laplacian matrix in general.

Chapter 4 follows on the theoretical analysis in Chapter 3 by displaying
a wide variety of applications for the Regularized Laplacian PDE
framework. The contributions of this chapter include:

-   A new consistent geodesics distance estimator on Riemannian
    manifolds, whose complexity depends only on the number of sample
    points @xmath , rather than the ambient dimension @xmath or the
    manifold dimension @xmath .

-   A new multi-class classifier, applicable to high-dimensional
    semi-supervised learning problems.

-   New explanations for negative results in the machine learning
    literature associated with the graph Laplacian.

-   A new dimensionality reduction algorithm called Viscous ISOMAP.

-   A new and satisfying interpretation for the bridge detection
    algorithm constructed in Chapter 2 .

### 1.2 Instantaneous Time-Frequency Analysis

Time frequency analysis in the form of the Fourier transform, short time
Fourier transform (STFT), and their discrete variants (e.g. the power
spectrum), have long been standard tools in science, engineering, and
mathematical analysis. Recent advances in time-frequency reassignment,
wherein energies in the magnitude plot of the STFT are shifted in the
time-frequency plane, have found application in “sharpening” images of,
e.g., STFT magnitude plots, and have been used to perform ridge
detection and other types of time- and frequency-localized feature
detection in time series.

Chapters 5 and 6 focus on a novel time-frequency reassignment transform,
Synchrosqueezing, which can be constructed to work “on top of” many
invertible transforms (e.g., the Wavelet transform or the STFT). As
Synchrosqueezing is itself an invertible transform, it can be used to
filter and denoise signals. Most importantly, it can be used to extract
individual components from superpositions of “quasi-harmonic” functions:
functions that take the form @xmath , where @xmath and @xmath are slowly
varying.

Chapter 5 focuses on two aspects of Synchrosqueezing. First, we develop
a fast new numerical implementation of Wavelet-based Synchrosqueezing.
This implementation, and other useful utilities, have been included in
the Synchrosqueezing MATLAB toolbox. Second, we present a stability
theorem, showing that Synchrosqueezing can extract components from
superpositions of quasi-harmonic signals when the original observations
have been corrupted by bounded perturbations (of the form often
encountered during the pre-processing of signals).

Chapter 6 builds upon the work of the previous chapter to develop novel
applications of Synchrosqueezing. We present a wide variety of problem
domains in which the Synchrosqueezing transform is a powerful tool for
denoising, feature extraction, and more general scientific analysis. We
especially focus on estimation in inverse problems in medical signal
processing and the geosciences. Contributions include:

-   The extraction of respiratory signals from ECG (Electrocardiogram)
    signals.

-   Precise new analyses of paleoclimate simulations (solar insolation
    models), individual paleoclimate proxies, and proxy stacks in the
    last 2.5 Myr. These results are compared to Wavelet- and STFT-based
    analyses, which are less precise and harder to interpret.

### 1.3 Multiscale Dictionaries of Slepian Functions on the Sphere

Just as audio signals and images are constrained by the physical
processes that generate them, and by the sensors that observe them, so
too are many geophysical and cosmological signals, which reside on the
sphere @xmath . On the real line and in the plane, both physical
constraints and sampling constraints lead to assumptions of a bandlimit:
that a signal contains zero energy outside some supporting region in the
frequency domain. Similarly, bandlimited signals on the sphere are zero
outside the low-frequency spherical harmonic components.

In Chapter 7 , following up on Claude Shannon’s initial investigations
into sampling, we describe Slepian, Landau, and Pollak’s spatial
concentration problem on subsets of the real line, and the resulting
Slepian functions. The construction of Slepian functions have led to
many important modern algorithms for the inversion of bandlimited
signals from their samples within an interval, and especially Thompson’s
multitaper spectral estimator. We then study Simons and Dahlen’s
extension of these results to subsets of the Sphere, where now the
definition of frequency and bandlimit has been appropriately modified.

Building upon these results, we develop an algorithm for the
construction of dictionary elements that are bandlimited, multiscale,
and localized. Our algorithm is based on a subdivision scheme that
constructs a binary tree from subdivisions of the region of interest
(ROI). We show, via numerous examples, that this dictionary has many
nice properties: it closely overlaps with the most concentrated Slepian
functions on the ROI, and most element pairs have low coherence.

The focus of this construction is to solve ill-posed inverse problems in
geophysics and cosmology. Though the new dictionary is no longer
composed of purely orthogonal elements like the Slepian basis, it can be
combined with modern inversion techniques that promote sparsity in the
solution, to provide significantly lower residual error after
reconstruction (as compared to classically optimal Slepian inversion
techniques).

We provide additional numerical results showing the solution path that
these techniques take when combined with the multiscale dictionaries,
and their efficacy on a standard model of the Earth’s magnetic field,
POMME-4. Finally, we show via randomized trials that the combination of
the multiscale construction and @xmath -based estimation provides
significant improvement, over the current state of the art, in the
inversion of bandlimited white and pink random processes within subsets
of the sphere.

## Chapter 2 Graph Bridge Detection via Random Walks111This chapter is
based on work in collaboration with Peter J. Ramadge, Department of
Electrical Engineering, Princeton University. A preliminary version
appears in [13].

### 2.1 Introduction

Many new problems in machine learning and signal processing require the
robust estimation of geodesic distances between nodes of a nearest
neighbors (NN) graph. For example, when the nodes represent points
sampled from a manifold, estimating feature space distances between
these points can be an important step in unsupervised [ 105 ] and
semi-supervised [ 4 ] learning. This problem often reduces to that of
having accurate estimates of each point’s neighbors, as described below.

In the simplest approach to estimating geodesic distances, the NN
graph’s edges are estimated from either the @xmath nearest neighbors
around each point, or from all of the neighbors within an ambient
(Euclidean) @xmath -ball around each point. Each graph edge is then
assigned a weight: the ambient distance between its nodes. A graph
shortest path (SP) algorithm, e.g. Dijkstra’s [ 25 , §24.3] , is then
used to estimate geodesic distances between pairs of points.

When the manifold is sampled with noise, or contains outliers, bridges
(short circuits between distant parts of the manifold) can appear in the
NN graph and this has a catastrophic effect on geodesics estimation [ 3
] .

In this chapter, we develop a new approach for calculating point
“neighborliness” from the NN graph. This approach allows the robust
removal of bridges from NN graphs of manifolds sampled with noise. This
metric, which we call “neighbor probability,” is based on a Markov
Random walk with independent restarts. The bridge decision rule based on
this metric is called the neighbor probability decision rule (NPDR), and
reduces to removing edges from the NN graph whose neighbor probability
is below a threshold. We study some of the NPDR’s geometric properties
when the number of samples grow large. We also compare the efficacy of
the NPDR to other decision rules and show its superior performance on
removing bridges in simulated data, and in the novel inverse problem of
computational tomography with random (and unknown) projection angles.

### 2.2 Preliminaries

Let @xmath be nonuniformly sampled points from manifold @xmath . We
observe @xmath , where @xmath is noise. A nearest neighbor (NN) graph
@xmath is constructed from @xmath -NN or @xmath -ball neighborhoods of
@xmath with the scale ( @xmath or @xmath ) chosen via cross-validation
or prior knowledge. The map @xmath assigns cost @xmath to edge @xmath .
Let @xmath . The set @xmath gives initial estimates of neighbors on the
manifold. Let @xmath denote the neighbors of @xmath in @xmath .

In [ 105 ] the geodesic distance between @xmath is estimated by @xmath
where @xmath is a minimum cost path from @xmath to @xmath in @xmath
(this can be calculated via Dijkstra’s algorithm). When there is no
noise, this estimate converges to the true geodesic distance on @xmath
as @xmath and neighborhood size @xmath . However, in the presence of
noise bridges form in the NN graph and this results in significant
estimation error. Forming the shortest path in @xmath is too greedy in
the presence of bridges.

If bridges could be detected, their anomalous effect could be removed
without disconnecting the graph by substituting a surrogate weight:
@xmath where @xmath is the set of detected bridges and @xmath , larger
than the diameter of @xmath , is a penalty. Let @xmath and @xmath be a
minimum cost path between @xmath and @xmath in @xmath . The adjusted
estimate of geodesic distance is @xmath .

With this in mind, we first review some bridge detection methods, and
discuss recent theoretical work in random walks on graphs.

### 2.3 Prior Work

The greedy nature of the SP solution encourages the traversal of
bridges, thereby significantly underestimating geodesic distances.
Previous work has considered denoising the nearest neighbors graph via
rejection of edges based on local distance statistics [ 21 , 94 ] , or
via local tangent space estimation [ 63 , 18 ] . However, unlike the
method we propose (NPDR), these methods use local rather that global
statistics. We have found that using only local statistics can be
unreliable. For example, with state of the art robust estimators of the
local tangent space (as in [ 103 ] ), local rejection of neighborhood
edges is not reliable with moderate noise or outliers. Furthermore, edge
removal (pruning) based on local edge length statistics is based on
questionable assumptions. For example, a thin chain of outliers can form
a bridge without unusually long edge lengths.

As an example, we first describe the simplest class of bridge decision
rules (DRs): ones that classify bridges by a threshold on edge length.
We call this the length decision rule (LDR); it is similar to the DR of
[ 21 ] . It is calculated with the following steps:

1.  Normalize edge lengths for local sampling density by setting

      -- -------- --
         @xmath   
      -- -------- --

    where @xmath sums outgoing edge lengths from @xmath .

2.  Let @xmath . Select a “good edge percentage” @xmath (e.g. @xmath )
    and calculate the detected bridge set @xmath via:

      -- -------- --
         @xmath   
      -- -------- --

    where @xmath is the @xmath -th quantile of the set @xmath .

The second decision rule, Jaccard similarity DR (JDR), classifies
bridges as edges between points with dissimilar neighborhoods [ 94 ] .
As opposed to the LDR, the JDR uses information from immediate neighbors
of two points to detect bridges:

1.  The Jaccard similarity between the neighborhoods of @xmath and
    @xmath is

      -- -------- --
         @xmath   
      -- -------- --

2.  Let @xmath be the set of Jaccard similarities. Select a @xmath ; the
    estimated bridge set is

      -- -------- --
         @xmath   
      -- -------- --

We now describe a more global neighborliness metric. The main motivation
is that bridges are short cuts for shortest paths. This suggests
detecting bridges by counting the traversals of each edge by estimated
geodesics. This is the concept of edge centrality ( edge betweenness )
in networks [ 76 ] . In a network, the centrality of edge @xmath is

  -- -------- --
     @xmath   
  -- -------- --

Edge centrality can be calculated in @xmath time and @xmath space using
algorithms of Newman or Brandes [ 76 , 11 ] . However, caution is
required in using edge centrality for our purpose. Consider a bridge
@xmath having high centrality. Suppose there exists a point @xmath with
@xmath such that @xmath , @xmath small. These edges are never preferred
over @xmath in a geodesic path, hence have low centrality. However, once
@xmath is placed into @xmath and given increased weight, @xmath reveals
itself as a secondary bridge in @xmath . So detection by centrality must
be done in rounds, each adding edges to @xmath . This allows secondary
bridges to be detected in subsequent rounds. We now describe the Edge
Centrality Decision Rule (ECDR):

Select quantile @xmath and iterate the following steps @xmath times on
@xmath :

1.  Calculate @xmath for each edge @xmath .

2.  Place @xmath of the most central edges into @xmath and update @xmath
    .

The result is a bridge set @xmath containing approximately @xmath edges,
matching the @xmath -th quantile sets of the previous DRs. The iteration
count parameter @xmath trades off between computational complexity
(higher @xmath implies more iterations of edge centrality estimation)
and robustness (it also more likely to detect bridges). To our
knowledge, the use of centrality as a bridge detector is new. While an
improvement over LDR, the deterministic greedy underpinnings of ECDR are
a limitation: it initially fails to see secondary bridges, and may also
misclassify true high centrality edges as bridges, e.g. the narrow path
in the dumbbell manifold [ 23 ] .

The Diffusion Maps approach to estimating feature space distances [ 23 ]
, has experimentally exhibited robustness to noise, outliers, and finite
sampling. Diffusion distances, based on random walks on the NN graph,
are closely related to “natural” distances ( commute times ) on a
manifold [ 45 ] . Furthermore, Diffusion Maps coordinates (based on
these distances) converge to eigenfunctions of the Laplace Beltrami
operator on the underlying manifold.

The neighbor probability metric we construct is a global measure of edge
reliability based on diffusion distances. The NPDR, based on this
metric, is then used to inform geodesic estimates.

### 2.4 Neighbor Probability Decision Rule

We now propose a DR based on a Markov random walk that assigns a
probability that two points are neighbors. This steps back from
immediately looking for a shortest path and instead lets a random walk
“explore” the NN graph. To this end, let @xmath be a row-stochastic
matrix with @xmath if @xmath and @xmath . Let @xmath be a parameter and
@xmath . Consider a random walk @xmath , @xmath , on @xmath starting at
@xmath and governed by @xmath . For each @xmath , with probability
@xmath we stop the walk and declare @xmath a neighbor of @xmath . Let
@xmath be the matrix of probabilities that the walk starting at node
@xmath , stops at node @xmath . The @xmath -th row of @xmath is the
neighbor distribution of node @xmath . This distribution can be
calculated:

  -- -------- -------- -- -------
     @xmath   @xmath      (2.1)
              @xmath      (2.2)
              @xmath      (2.3)
              @xmath      (2.4)
  -- -------- -------- -- -------

In ( 2.1 ) we decompose the stopping event into the disjoint events of
stopping at time @xmath . In ( 2.2 ) we separate the independent events
of stopping from being at the current position @xmath . In ( 2.3 ) we
use the well known Markov property @xmath (with @xmath ). Finally, in (
2.4 ) we recall that stopping at time @xmath means we choose not to
stop, independently, for each @xmath , and finally stop at time @xmath ,
so the probability of this event is @xmath . Thus, the neighbor
probabilities are:

  -- -------- --
     @xmath   
  -- -------- --

A smaller stopping probability @xmath induces greater weighting of
long-time diffusion effects, which are more dependent on the topology of
@xmath . @xmath is closely related to the normalized commute time of [
114 ] . Its computation requires @xmath time and @xmath space ( @xmath
is sparse but @xmath may not be).

The neighborhood probability matrix @xmath is heavily dependent on the
choice of underlying random walk matrix @xmath . First we relate several
key properties of @xmath to those of @xmath , then we introduce a
geometrically intuitive choice of @xmath when the sample points are
sampled from a manifold.

###### Lemma 2.4.1.

@xmath is row-stochastic, shares the left and right eigenvectors of
@xmath , and has spectrum @xmath , @xmath .

###### Proof.

Clearly, @xmath is row-stochastic, shares the left and right
eigenvectors of @xmath and has spectrum @xmath . ∎

Following [ 23 ] , we select @xmath to be the popular Diffusion Maps
kernel on @xmath :

1.  Choose @xmath (e.g. @xmath ) and let @xmath if @xmath , @xmath if
    @xmath , and @xmath otherwise.

2.  Let @xmath be diagonal with @xmath , and normalize for sampling
    density by setting @xmath .

3.  Let @xmath be diagonal with @xmath and set @xmath .

@xmath is row-stochastic and, as is well known, has bounded spectrum:

###### Lemma 2.4.2.

The spectrum of @xmath is contained in @xmath .

###### Proof.

@xmath , @xmath and @xmath , are symmetric PSD. @xmath is row-stochastic
and similar to @xmath . ∎

We define the Neighbor Probability Decision Rule (NPDR) as follows.

1.  Let @xmath and find the restriction @xmath of @xmath to @xmath .

2.  Choose a @xmath and let @xmath , i.e., edges connecting nodes with a
    low probability of being neighbors, are bridges.

Calculating @xmath can be prohibitive for very large datasets.
Fortunately, we can effectively order the elements of @xmath using a low
rank approximation to @xmath .

By Lemmas 2.4.1 , 2.4.2 , we calculate @xmath for @xmath , where @xmath
, @xmath and @xmath are the @xmath largest eigenvalues of @xmath and the
associated right and left eigenvectors. In practice, an effective
ordering of @xmath is obtained with @xmath and since @xmath is sparse,
its largest eigenvalues and eigenvectors can be computed efficiently
using a standard iterative algorithm.

Matrix @xmath has, thanks to our choice of @xmath , a more geometric
interpretation which we now discuss.

#### 2.4.1 Geometric Interpretation of @xmath

In this section, we show the close relationship between the matrices
@xmath , @xmath , and the weighted graph Laplacian matrix (to be defined
soon). One important property of @xmath is its relationship to the
Laplace Beltrami operator on @xmath . Specifically, when @xmath , and as
@xmath at the appropriate rate ³ ³ 3 We discuss this convergence in
greater detail in Chapter 3 . , @xmath both pointwise and in spectrum [
23 ] . Here @xmath is the weighted graph Laplacian and @xmath is the
Laplace Beltrami operator on @xmath and @xmath is a constant. Thus, for
large @xmath and small @xmath , @xmath is a neighborhood averaging
operator. This property also holds for @xmath :

###### Theorem 2.4.3.

As @xmath and @xmath , @xmath .

###### Proof.

For @xmath invertible, @xmath . From Lem. 2.4.2 , @xmath is invertible.
Thus

  -- -------- -------- -- -------
     @xmath   @xmath      (2.5)
              @xmath      (2.6)
  -- -------- -------- -- -------

Therefore

  -- -------- --
     @xmath   
  -- -------- --

The first factor on the RHS converges to @xmath and the second to @xmath
since @xmath . ∎

By Theorem 2.4.3 , as @xmath and @xmath , @xmath acts like @xmath . For
finite sample sizes, however, experiments indicate that @xmath is more
informative of neighborhood relationships. We provide here a simple
justification based on the original random walk construction.

Were we to replace @xmath with @xmath in the implementation of the NPDR,
edge @xmath would be marked as a bridge essentially according to its
normalized weight (that is, proportional to @xmath and normalized for
sampling density). As @xmath , this edge would essentially be marked as
a bridge if the pairwise distance between its associated points is above
a threshold. NPDR would in this case yield a performance very similar to
that of LDR. In contrast, NPDR via @xmath uses multi-step probabilities
with an exponential decay weighting to determine whether an edge is a
bridge. Thus, the more ways there are to get from @xmath to @xmath over
a wide variety of possible step counts, the less likely that @xmath is a
bridge.

We now provide some synthetic examples comparing NPDR to the other
decision rules for finite sample sizes.

### 2.5 Denoising the Swiss Roll

We first test our method on the synthetic Swiss roll, parametrized by
@xmath in @xmath via the embedding @xmath . True geodesic distances
@xmath are computed via:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath , and @xmath is the differential of @xmath evaluated at
@xmath . We sampled @xmath points uniformly in the ambient space @xmath
with @xmath fixed ( @xmath ). For @xmath ( @xmath ) we generated @xmath
random noise values, @xmath , uniformly on @xmath . Then @xmath , @xmath
, where @xmath is the normal to @xmath at @xmath . Each experiment was
repeated for @xmath .

The initial NN graph @xmath was constructed using @xmath -balls ( @xmath
). The median bridge counts over @xmath realizations are shown in Fig.
2.0(b) . Bridges first appear at @xmath . Fig. 2.0(a) shows one
realization of @xmath and the NN graph @xmath (note bridges). We compare
the simple SP with the LDR, ECDR ( @xmath ), and NPDR ( @xmath ) -based
estimators by plotting the estimates of geodesic distance versus ground
truth (sorted by distance from @xmath ). We plot the median estimate,
33%, and 66% quantiles over the @xmath runs, for @xmath and @xmath . The
LDR and JDR based estimators’ performance is comparable to SP for @xmath
(plots not included).

With no noise: SP provides excellent estimates; NPDR estimates are
accurate even after removing @xmath of the graph edges (Fig. 2.3(a) );
however, ECDR removes important edges (Fig. 2.2(a) ). At @xmath with
approximately @xmath bridges: SP has failed (Fig. 2.1(b) ); ECDR is
removing bridges but also important edges, resulting in an upward
estimation bias (Fig. 2.2(b) ); in contrast, NPDR is successfully
discounting bridges without any significant upward bias even at @xmath
(Fig. 2.3(b) ). This supports our claim that bridges occur between edges
with low neighbor probability in the NPDR random walk. Lower values of
@xmath remove more edges, including bridges, but removing non-bridges
always increases SP estimates, and can lead to an upward bias. The
choice of @xmath should be based on prior knowledge of the noise or
cross-validation.

Table 2.1 compares the performance of DRs at moderate noise levels. For
this experiment, we chose @xmath points, @xmath , well distributed over
@xmath . Over @xmath noise realizations, we calculated the mean of the
value

  -- -------- --
     @xmath   
  -- -------- --

the average absolute error of the geodesic estimate from all of the
points to these 5. As seen in this figure, given an appropriate choice
of @xmath , NPDR outperforms the other DRs at moderate noise levels. As
expected, @xmath must grow with the noise level as more bridges are
found in the initial graph.

### 2.6 Denoising a Random Projection Graph

We now consider the random projection tomography problem of [ 94 ] .
Random projections of @xmath are taken at angles @xmath . More
specifically, these projections are @xmath , where @xmath is the Radon
transform at angle @xmath .

In [ 94 ] , we observe @xmath random projections:

  -- -------- --
     @xmath   
  -- -------- --

for which the ambient dimension of the projection is @xmath and @xmath ,
where the signal power is @xmath . The image used in all experiments is
the Shepp-Logan phantom (Fig. 2.4(a) ), and the projection angles are
unknown (Fig. 2.4(b) ). After some initial preprocessing, a NN graph (
@xmath ) is constructed from the noisy projections, and JDR is used to
detect bridges in this graph. After detected bridges are removed
(pruned), nodes with less than two remaining edges (that is, isolated
nodes) are removed from the graph. An eigenvalue problem on the new
graph’s adjacency matrix is then solved to find an angular ordering of
the remaining projections (nodes). Finally, @xmath is reconstructed via
an inverse Radon transform of these resorted projections.

We compared JDR to NPDR pruning at a SNR of @xmath . Exhaustive search
finds the optimal @xmath for JDR at @xmath . For NPDR we used @xmath
(all edges in @xmath have weight @xmath ), @xmath , and @xmath . After
pruning, JDR disconnected 277 nodes compared to 21 for NPDR. The
estimated sorted angles are shown in Figs. 2.5(a) , 2.5(b) , and the
rotated reconstructions in Figs. 2.5(c) , 2.5(d) . Under the similarity
metric @xmath , with alignment of @xmath with @xmath , the increase in
NPDR similarity (0.15) over JDR similarity ( @xmath ) is 25%. Note the
clearer boundaries in the NPDR phantom, thanks to 256 additional
(unpruned) projections (best viewed on screen). At moderate noise
levels, NPDR removes fewer NN graph nodes and yields a more accurate
reconstruction. As more projections are left after pruning, the final
accuracy is higher.

### 2.7 Conclusion and Connections

We studied the problem of estimating geodesics in a point cloud. A
slight revision for removing edges from a neighborhood graph allows us
to avoid disconnecting weakly connected groups. Building on this
framework, we studied several global measures for detecting topological
bridges in the NN graph. In particular, we developed and analyzed the
NPDR bridge detection rule, which is based on a special type of Markov
random walk. Using a special random walk matrix derived from the
geometry of the sample points, we constructed the NPDR to detect bridges
by thresholding entries of the neighborhood matrix @xmath . The entries
of column @xmath in this matrix converge to those of a special averaging
operator in the neighborhood of point @xmath in @xmath : the averaging
intrinsically performed by the Laplace Beltrami operator around @xmath .

Our experiments indicate that NPDR robustly detects bridges in the NN
graph without misclassifying edges important for geodesic estimation or
tomographic angle estimation. Furthermore, it does so over a wider noise
range than competing methods, e.g. LDR and ECDR. It can be calculated
efficiently via a sparse eigenvalue decomposition. Preliminary evidence
from synthetic experiments indicates that, as § 2.4 suggests, for NPDR
one should choose @xmath as small as possible while retaining numerical
conditioning of @xmath .

For very large @xmath and small @xmath , the matrices @xmath and @xmath
are equivalent, but in practical cases @xmath yields significantly
better performance. More testing of NPDR on non-synthetic datasets is
needed. Possible applications include determining bridges in social and
webpage (hyperlink) network graphs, and in the common line graphs
estimated in the blind 3D tomography “Cryo-EM” problem [ 95 ] .

Furthermore, the matrix @xmath is closely related to the regularized
inverse of the graph Laplacian. The term @xmath in ( 2.5 ) is
proportional to the weighted graph Laplacian @xmath , and from this
equation it is clear that @xmath is proportional to the inverse of the
Tikhonov regularized weighted graph Laplacian (with regularization
parameter @xmath ). The efficacy of the NPDR, and the initial
theoretical results developed in § 2.4.1 lead us to study the
regularized inverse of the graph Laplacian in more detail; this is the
focus of Chapter 3 .

## Chapter 3 The Inverse Regularized Laplacian: Theory111This chapter,
and the next, are based on work in collaboration with Peter J. Ramadge,
Department of Electrical Engineering, Princeton University, as submitted
in [14].

### 3.1 Introduction

Semi-supervised learning (SSL) encompasses a class of machine learning
problems in which both labeled data points and unlabeled data points are
available during the training (fitting) stage [ 19 ] . In contrast to
supervised learning, the goal of SSL algorithms is to improve future
prediction accuracy by including information from the unlabeled data
points during training. SSL extends a wide class of standard problems,
such as classification and regression.

A number of recent nonlinear SSL algorithms use aggregates of nearest
neighbor (NN) information to improve inference performance. These
aggregates generally take the form of some transform, or decomposition,
of the weighted adjacency, or weight, matrix of the NN graph. Formal
definitions of NN graphs, and associated weight matrices, are given in §
2.2 ; we will also review them in the SSL learning context in § 3.3.1 .

Motivated by several of these algorithms, we show a connection between a
certain nonlinear transform of the NN graph weight matrix, the
regularized inverse of the graph Laplacian matrix, and the solution to
the regularized Laplacian partial differential equation (PDE), when the
underlying data points are sampled from a compact Riemannian manifold.
We then show a connection between this PDE and the Eikonal equation,
which generates geodesics on the manifold.

These connections lead to intuitive geometric interpretations of
learning algorithms whose solutions include a regularized inverse of the
graph Laplacian. As we will show in chapter 4 , it also enables us to
build a robust geodesic distance estimator, a competitive new multiclass
classifier, and a regularized version of ISOMAP.

This chapter is organized as follows: § 3.3.1 -§ 3.3.4 motivate our
study by showing that in a certain limiting case, a standard SSL problem
can be modeled as a regularized Laplacian (RL) PDE problem. § 3.3.5 -§
3.3.8 derive the relationship between the regularized Laplacian and
geodesics and discusses convergence issues as an important
regularization term (viscosity) goes to zero.

### 3.2 Prior Work

The graph Laplacian is an important tool for regularizing the solutions
of unsupervised and semi-supervised learning problems, such as
classification and regression, in high-dimensional data analysis [ 101 ,
5 , 4 , 115 , 65 ] . Similarly, estimating geodesic distances from
sample points on a manifold has important applications in manifold
learning and SSL (see, e.g., chapter 2 and [ 105 , 20 , 8 ] ). Though
heavily used in the learning and geometry communities, these methods
still raise many questions. For example, with dimension @xmath , graph
Laplacian regularized SSL does not work as expected in the large sample
limit [ 74 ] . It is also desirable to have geometric intuition about
the behavior of the solutions of models like those proposed in [ 4 , 115
] in the limit when the data set is large.

To this end, we elucidate a connection between three important
components of analysis of points sampled from manifolds:

1.  The inverse of the regularized weighted graph Laplacian matrix.

2.  A special type of elliptic Partial Differential Equation (PDE) on
    the manifold.

3.  Geodesic distances on the manifold.

This connection provides a novel geometric interpretation for machine
learning algorithms whose solutions require the regularized inverse of a
graph Laplacian matrix. It also leads to a consistent geodesic distance
estimator with two desired properties: the complexity of the estimator
depends only on the number of sample points and the estimator naturally
incorporates a smoothing penalty.

### 3.3 Manifold Laplacian, Viscosity, and Geodesics

We motivate our study by first looking at a standard semisupervised
learning problem (§ 3.3.1 ). We show that as the amount of data
increases and regularization is relaxed, this problem reduces to a PDE
(§ 3.3.2 -§ 3.3.3 ). We then analyze this PDE in the low regularization
setting to uncover new geometric insights into its solution (§ 3.3.5 -§
3.3.7 ). In § 3.3.9 , these insights will allow us to analyze the
original SSL problem from a geometric perspective.

#### 3.3.1 A Standard SSL Problem

We present a classic SSL problem in which points are sampled, some with
labels, and a regularized least squares regression is performed to
estimate the labels on the remaining points. The regression contains two
regularization penalties: a ridge regularization penalty and a graph
Laplacian “smoothing” penalty.

Data points @xmath are sampled from a space @xmath . The first @xmath of
these @xmath points have associated labels: @xmath . For binary
classification, one could take @xmath . The goal is to find a vector
@xmath that approximates the @xmath samples at the points in @xmath of
an unknown smooth function @xmath on @xmath and minimizes the regression
penalty @xmath . The solution is regularized by two penalties: the ridge
penalty @xmath and the graph Laplacian penalty @xmath ; details of the
construction of @xmath are given in § 3.3.3 . The second penalty
approximately penalizes the gradient of @xmath . It is a discretization
of the functional: @xmath where @xmath is the sampling density.

To find @xmath we solve the convex minimization problem:

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

where the nonnegative regularization parameters @xmath and @xmath depend
on @xmath and @xmath .

We can rewrite ( 3.1 ) in its matrix form. Let @xmath have @xmath ones
followed by @xmath zeros on its diagonal, and let @xmath . Further, let
@xmath be a vector of the labels @xmath for the first @xmath sample
points, and zeros for the unlabeled points. Then Eq. ( 3.1 ) can be
written as:

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

This is a quadratic program for @xmath . Setting the gradient, with
respect to @xmath , to zero yields the linear system

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

The optimization problem ( 3.1 ) and the linear system ( 3.3 ) are
related to two previously studied problems. The first is graph
regression with Tikhonov regularization [ 5 ] . Problem ( 3.3 ) is
closely related to the one solved in Algorithm 1 of that paper, where we
replace their general penalty @xmath term with the more specific form
@xmath . The ridge penalty @xmath encourages stability in the solution,
replacing their zero-sum constraint on @xmath . The second related
problem is Laplacian Regularized Least Squares (LapRLS) of [ 4 ] .
Specifically, ( 3.3 ) is identical to Eq. 35 of [ 4 ] with one of the
regularization terms removed by setting the kernel matrix @xmath equal
to the identity. In that framework, the matrix @xmath is defined as
@xmath , @xmath for some @xmath . A choice of @xmath regularizes for
finite sample size and sampling noise [ 4 , Thm. 2, Remark 2] . Eq. (
3.3 ) is thus closely related to the limiting solution to the LapRLS
problem in the noiseless case, where the sampling size @xmath grows and
@xmath shrinks quickly with @xmath .

We study the following problem: suppose the function @xmath is sampled
without noise on specific subsets of @xmath . The estimate @xmath
represents an extension of @xmath to the rest of @xmath . What does this
extension look like, and how does it depend on the geometry of @xmath ?
The first step is to understand the implications of the noiseless case
on ( 3.3 ); we study this next.

#### 3.3.2 SSL Problem – Assumptions

We now list our main assumptions for the SSL problem in ( 3.3 ):

1.  @xmath is a @xmath -dimensional ( @xmath ) manifold @xmath , which
    is compact, and Riemannian.

2.  The labeled points @xmath are sampled from a regular and closed
    nonempty subset @xmath .

3.  The labels @xmath are sampled from a smooth (e.g. @xmath or
    Lipschitz) function @xmath .

4.  The density @xmath is nonzero everywhere on @xmath , including on
    @xmath .

These assumptions ensure that the points @xmath are sampled without
noise from a bounded, and smooth space, that the labels are sampled
without noise, and that the label data is also sampled from a bounded
and smooth function. We will use these assumptions to show the
convergence of @xmath to a smooth function @xmath on @xmath .

In assumption 2 we use the term regular in the PDE sense [ 47 ,
Irregular Boundary Point] ; we discuss this further in § 4.4 . We call
@xmath the anchor set (or anchor); note that it is not necessarily
connected. In addition, let @xmath denote the complement set. In
assumption 3 , for @xmath to be smooth it suffices that it is smooth on
all connected components of @xmath . Thus we can allow @xmath to take on
discrete values, as long as the classes they represent are separated
from each other on @xmath . We call the function @xmath the anchor
condition or anchor function . Note finally that assumption 4 implies
that the labeled data size @xmath grows with @xmath .

As we have assumed that there is no noise on the labels (assumptions 2
and 3 ), we will not apply a regularization penalty to the labeled data.
On the labeled points, therefore, ( 3.3 ) reduces to @xmath . Hence, the
regularized problem becomes an interpolation problem. The ridge penalty,
now restricted to the unlabeled data, changes from @xmath to @xmath .
The Laplacian penalty function becomes @xmath , and the discretization
of this penalty similarly changes from @xmath to @xmath . The original
linear system ( 3.3 ) thus becomes

  -- -- -------- -- -------
        @xmath      (3.4)
        @xmath      
  -- -- -------- -- -------

We note that the dimensions of @xmath , @xmath , and @xmath in ( 3.4 )
grow with @xmath .

A more detailed explanation of the component terms in the original
problem ( 3.2 ) and the linear systems ( 3.3 ) and ( 3.4 ) are available
in § 3.4.2 , where we show that the solution to the simplified problem (
3.4 ) depends only on the ratio @xmath . We will return to this in §
3.3.3 .

#### 3.3.3 SSL Problem – the Large Sample Limit – Preliminary Results

We study the linear system ( 3.4 ) as @xmath , and prove that given an
appropriate choice of graph Laplacian @xmath and growth rate of the
regularization parameters @xmath and @xmath , the solution @xmath
converges to the solution @xmath of a particular PDE on @xmath . The
convergence occurs in the @xmath sense with probability @xmath .

Our proof of convergence has two parts. First, we must show that @xmath
models a forward PDE with increasing accuracy as @xmath grows large;
this is called consistency . Second, we must show that the norm of
@xmath does not grow too quickly as @xmath grows large; this is called
stability . These two results will combine to provide the desired proof.
As all of our estimates rely on the choice of graph Laplacian matrix on
the sample points @xmath , we first detail the specific construction
that we use throughout:

1.  Construct a weighted nearest neighbor (NN) graph @xmath from @xmath
    -NN or @xmath -ball neighborhoods of @xmath , where edge @xmath is
    assigned the distance @xmath .

2.  Choose @xmath and let @xmath if @xmath , @xmath if @xmath , and
    @xmath otherwise.

3.  Let @xmath be diagonal with @xmath , and normalize for sampling
    density by setting @xmath .

4.  Let @xmath be diagonal with @xmath and define the row-stochastic
    matrix @xmath .

5.  The asymmetric normalized graph Laplacian is @xmath .

We will use @xmath from now on in place of the generic Laplacian matrix
@xmath .

Regardless of the sampling density, as @xmath and @xmath at the
appropriate rate, @xmath converges (with probability 1) to the
Laplace-Beltrami operator on @xmath : @xmath , for some @xmath ,
uniformly pointwise [ 48 , 97 , 24 , 108 ] and in spectrum [ 6 ] . The
concept of correcting for sampling density was first suggested in [ 60 ]
.

This convergence forms the basis of our consistency argument. We first
introduce a result of [ 97 ] , which shows that with probability 1, as
@xmath , the system ( 3.4 ) with @xmath consistently models the
Laplace-Beltrami operator in the @xmath sense.

Let @xmath map any square integrable function on the manifold to the
vector of its samples on the discrete set @xmath .

###### Theorem 3.3.1 (Convergence of @xmath: [97], Eq. 1.7).

Suppose we are given a compact Riemannian manifold @xmath and smooth
function @xmath . Suppose the points @xmath are sampled iid from
everywhere on @xmath . Then, for @xmath large and @xmath small, with
probability @xmath

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the negatively defined Laplace-Beltrami operator on
@xmath , and @xmath is a constant. Choosing @xmath (where @xmath depends
on the geometry of @xmath ) leads to the optimal bound of @xmath .
Following [ 48 ] , the convergence is uniform.

As convergence is uniform in Thm. 3.3.1 , we may write the bound in
terms that we will use throughout:

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

where @xmath and @xmath is the vector infinity norm.

We now show that @xmath (as defined in ( 3.4 )) is consistent:

###### Corollary 3.3.2 (Consistency of @xmath).

Assume that @xmath or that @xmath . Let @xmath be the following operator
for functions @xmath :

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

Then, under the same conditions as in Thm. 3.3.1 , with probability
@xmath for @xmath large and @xmath chosen as in Thm. 3.3.1 , @xmath .

###### Proof.

This follows directly from Thm. 3.3.1 and the fact that @xmath for any
set @xmath . ∎

Our notion of stability is described in terms of certain limiting
inequalities. We use the notation @xmath to mean that there exists some
@xmath such that for all @xmath , @xmath .

###### Proposition 3.3.3 (Stability of @xmath).

Suppose that @xmath , @xmath . Then @xmath .

The proof of Prop. 3.3.3 is mainly technical and is given in § 3.4.1 .

If we modify @xmath as

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

we can also state the following corollary, which will be useful in
chapter 4 .

###### Corollary 3.3.4 (Stability of @xmath).

Suppose @xmath , @xmath . Then @xmath .

#### 3.3.4 SSL Problem – the Large Sample Limit – Convergence Theorem

We are now ready to state and prove our main theorem about the
convergence of @xmath . We assume that @xmath has empty boundary, or
that @xmath . For the case of nonempty manifold boundary @xmath , there
are additional constraints at this boundary in the resulting PDE. We
discuss this case in § 3.4.3 . Note, by assumption 2 of § 2 , the anchor
set @xmath and its boundary @xmath are not empty.

###### Theorem 3.3.5 (Convergence of @xmath under @xmath).

Consider the solution of @xmath (Eq. ( 3.4 ) with @xmath ), with @xmath
, @xmath , and @xmath as described in § 3.3.2 , and with either @xmath
or @xmath . Further, assume that

1.  @xmath shrinks as given in Thm. 3.3.1 .

2.  @xmath .

3.   As in Prop. 3.3.3 , @xmath for some @xmath .

Then for @xmath large, with probability 1,

  -- -------- --
     @xmath   
  -- -------- --

where the function @xmath is the unique, smooth, solution to the
following PDE for the given @xmath :

  -- -------- -- -------
     @xmath      (3.8)
  -- -------- -- -------

###### Proof of Thm. 3.3.5, Convergence.

Let @xmath be given by ( 3.6 ) and let @xmath be the solution to ( 3.8
). The existence and uniqueness of @xmath , under assumptions 1 - 3 of §
3.3.2 , is well known (we show it in § 3.3.5 ).

We bound @xmath as follows:

  -- -------- -------- --
     @xmath            
              @xmath   
              @xmath   
  -- -------- -------- --

From stability (Prop. 3.3.3 ), @xmath , and by consistency (Cor. 3.3.2
), with probability 1 for large @xmath , @xmath .

The theorem follows upon applying assumption 2 . ∎

If we modify @xmath in Thm. 3.3.5 as in ( 3.7 ), we obtain Cor. 3.3.6
below.

###### Corollary 3.3.6 (Convergence of @xmath under @xmath).

Consider the solution of @xmath , with @xmath , @xmath , and @xmath as
described in § 3.3.2 , and with @xmath or @xmath . Further, assume
@xmath shrinks as given in Thm. 3.3.1 , and @xmath as in Cor. 3.3.4 .
Then for @xmath large, with probability 1,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath again solves ( 3.8 ):

  -- -------- --
     @xmath   
  -- -------- --

###### Proof of Cor. 3.3.6.

Similar to that of Thm. 3.3.5 , with stability given by Cor. 3.3.4 . ∎

We will call ( 3.8 ) the Regularized Laplacian PDE (RL PDE) with the
regularization parameter @xmath . By assumptions 1 and 2 of Thm. 3.3.5 ,
@xmath as @xmath . This, and the analysis in § 3.3.2 , motivate us to
study the RL PDE when @xmath is small to gain some insight into its
solution, and hence into the behavior of the original SSL problem.

#### 3.3.5 The Regularized Laplacian PDE

We now study the RL PDE in greater detail. We will assume a basic
knowledge of differential geometry on compact Riemannian manifolds
throughout. For basic definitions and notation, see App. A .

We rewrite the RL PDE ( 3.8 ), now denoting the explicit dependence on a
parameter @xmath , and making the problem independent of sampling:

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

where @xmath , @xmath , and @xmath are defined as in § 3.3.2 . The idea
is that @xmath is specified smoothly on @xmath and by solving ( 3.9 ) we
seek a smooth extension of @xmath to all of @xmath .

The RL PDE, ( 3.9 ), has been well studied [ 109 , Thm. 6.22] , [ 54 ,
App. A] . It is uniformly elliptic, and for @xmath it admits a unique,
bounded, solution @xmath . The boundedness of @xmath follows from the
strong maximum principle [ 109 , Thm. 3.5] . One consequence is that
@xmath will not extrapolate beyond an interval determined by the anchor
values.

###### Proposition 3.3.7 ([54], §a.2).

The RL PDE ( 3.9 ) has a unique, smooth solution that is bounded within
the range @xmath for @xmath , where

  -- -------- --
     @xmath   
  -- -------- --

Our goal is to understand the solution of ( 3.9 ) as the regularization
term vanishes, i.e., @xmath . To do so, we introduce the Viscous Eikonal
Equation.

#### 3.3.6 The Viscous Eikonal Equation

The RL PDE is closely related to what we will call the Viscous Eikonal
(VE) equation. This is the following “smoothed” Hamilton-Jacobi equation
of Eikonal type:

  -- -------- -- --------
     @xmath      (3.10)
  -- -------- -- --------

The term containing the Laplacian is called the viscosity term , and
@xmath is called the viscosity parameter .

The two PDEs, ( 3.9 ) and ( 3.10 ), are connected via the following
proposition:

###### Proposition 3.3.8.

Consider ( 3.9 ) with @xmath and @xmath , and ( 3.10 ) with @xmath .
When solution @xmath exists for ( 3.9 ), then @xmath is the unique,
smooth, bounded solution to ( 3.10 ).

###### Proof.

Let @xmath be the unique solution of ( 3.9 ). From Prop. 3.3.7 , @xmath
for @xmath . Apply the inverse of the smooth monotonic bijection @xmath
, @xmath to @xmath . Let @xmath , hence @xmath .

We will need the standard product rule for the divergence “ @xmath ”.
When @xmath is a differentiable function and @xmath is a differentiable
vector field,

  -- -------- -- --------
     @xmath      (3.11)
  -- -------- -- --------

As @xmath is harmonic, on @xmath :

  -- -------- -------- -- --------
     @xmath   @xmath      (3.12)
              @xmath      (3.13)
              @xmath      (3.14)
              @xmath      (3.15)
  -- -------- -------- -- --------

Here, from ( 3.12 ) to ( 3.13 ) we use the chain rule, and from ( 3.13 )
to ( 3.14 ) we use the product rule ( 3.11 ). After dropping the
positive multiplier in ( 3.15 ), we see that that @xmath satisfies the
first part of ( 3.10 ). Further, @xmath because @xmath is a smooth
bijection. Similarly, @xmath is bounded because @xmath is: @xmath .

Finally, on the boundary @xmath we have @xmath , equivalently @xmath .
Hence, @xmath solves ( 3.10 ).

∎

To summarize: for @xmath , ( 3.10 ) has a unique solution @xmath (Prop.
3.3.8 ; see also [ 109 , 54 ] ).

We are interested in the solutions of the VE Eq. for the case of @xmath
as well as for solutions obtained for @xmath small and for more general
@xmath . When @xmath and @xmath , it is well known that on a compact
Riemannian manifold @xmath , ( 3.10 ) models propagation from @xmath
through @xmath along shortest paths. Results are known for a number of
important cases, and we will discuss them after describing the following
assumption.

###### Assumption 3.3.1.

For @xmath and @xmath sufficiently regular, ( 3.10 ) has the unique
viscosity solution:

  -- -------- -- --------
     @xmath      (3.16)
  -- -------- -- --------

where @xmath is the geodesic distance between @xmath and @xmath through
@xmath . Furthermore, as @xmath , @xmath converges to @xmath in @xmath ,
@xmath , and in @xmath (i.e. essentially pointwise) when @xmath
converges to @xmath in the same sense. The rate of convergence is @xmath
.

From now on we will denote @xmath simply by @xmath .

###### Discussion of Assum. 3.3.1 on compact @xmath.

To our knowledge, a complete proof of ( 3.16 ) for compact Riemannian
@xmath is not known; the theory of unique viscosity solutions
(nondifferentiable in some areas), on manifolds is an open area of
research [ 26 , 2 ] . However, below we cite known partial results.

Eq. ( 3.16 ) was shown to hold for @xmath on compact @xmath in [ 68 ,
Thm. 3.1] , and for @xmath sufficiently regular on bounded, smooth, and
connected subsets of @xmath in [ 64 , Thms. 2.1, 6.1, 6.2] , and e.g.,
when @xmath is Lipschitz [ 58 , Eq. 4.23] . Convergence and the
convergence rate of @xmath to @xmath were also shown on such Euclidean
subsets in [ 64 , Eq. 69] . Conditions of convergence to a viscosity
solution are not altered under the exponential map [ 2 , Cor. 2.3] ,
thus convergence in local coordinates around @xmath (which follows from
[ 64 , Thm. 6.5] and Prop. 3.3.8 ) implies convergence on open subsets
of @xmath . However, global convergence of @xmath to @xmath on @xmath is
still an open problem.

Not surprisingly, despite the lack of formal proof, and in light of the
above evidence, our numerical experiments on a variety of nontrivial
compact Riemannian manifolds (e.g. compact subsets of hyperbolic
paraboloids) give additional evidence that this convergence is achieved.
∎

#### 3.3.7 What happens when @xmath converges to @xmath: Transport Terms

To study the relationship between @xmath and @xmath , we look for a
higher order expansion of @xmath using a tool called Transport Equations
[ 87 ] .

Assume @xmath can be expanded into the following form:

  -- -------- -- --------
     @xmath      (3.17)
  -- -------- -- --------

with @xmath . The terms @xmath , @xmath are called the transport terms.
Substitution of this form into ( 3.9 ) will give us the conditions
required on @xmath and @xmath .

###### Theorem 3.3.9.

If ( 3.17 ), (with @xmath ) is a solution to the RL PDE ( 3.9 ) for all
@xmath , then:

  -- -------- -- --------
     @xmath      (3.18)
  -- -------- -- --------

and ( 3.9 ) reduces to a series of PDEs:

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (3.19)
     @xmath   @xmath      
  -- -------- -------- -- --------

In particular, letting @xmath denote the shortest geodesic distance from
@xmath to @xmath , we have that @xmath everywhere.

###### Proof.

The anchor conditions follow from the fact that for all @xmath , @xmath
(thus forcing @xmath and therefore @xmath , and @xmath ).

Plugging ( 3.17 ) into ( 3.9 ), and applying the product and chain
rules, we get

  -- -------- --
     @xmath   
  -- -------- --

Eqs. ( 3.19 ) follow after collecting like powers of @xmath and
simplifying. ∎

Thm. 3.3.9 shows first that @xmath is determined by the Eikonal equation
with zero boundary conditions. Second, it shows that @xmath is the
dominant term affected by the boundary values @xmath as @xmath . For
@xmath , the transport terms @xmath are affected by @xmath via @xmath ,
but these are not the dominant terms for small @xmath . The existence,
uniqueness, and smoothness of @xmath on @xmath and within the cut locus
of @xmath , is proved in § 3.4.1 (Thm. 3.4.4 ).

Note that the choice of @xmath is not arbitrary. For @xmath in ( 3.17 ),
( 3.9 ) does not admit a consistent set of solvable transport equations.
For @xmath , the resulting transport equations reduce to those of Eqs. (
3.19 ) (the nonzero odd @xmath terms are forced to zero and the even
@xmath terms are related to each other via Eqs. ( 3.19 )).

#### 3.3.8 Manifold Laplacian and Vanishing Viscosity

We now combine Assum. 3.3.1 and Thm. 3.3.9 in a way that summarizes the
solution of the RL PDE ( 3.9 ) for small @xmath , taking into account
possible arbitrary nonnegative boundary conditions.

###### Theorem 3.3.10.

Let @xmath and let @xmath . Further, define @xmath where @xmath . Then
for a situation where Assum. 3.3.1 holds, and for small @xmath , the
solution of ( 3.9 ) with sufficiently regular anchor @xmath , satisfies:

  -- -------- -------- -- --------
     @xmath   @xmath      (3.20)
     @xmath   @xmath      (3.21)
  -- -------- -------- -- --------

###### Proof.

First, apply Thm. 3.3.9 to decompose @xmath in terms of @xmath and the
transport terms @xmath , @xmath . Next, by Thm. 3.1 of [ 68 ] , as
discussed in Assum. 3.3.1 , we obtain @xmath . We can therefore write
@xmath . Further, @xmath is unique and smooth within an intersection of
@xmath and a cut locus of @xmath , and satisfies the boundary conditions
( @xmath for @xmath ) . This can be shown using the method of
characteristics (Thm. 3.4.4 ). This verifies ( 3.20 ).

Showing that ( 3.21 ) holds requires more work due to possible zero
boundary conditions on @xmath . To prove ( 3.21 ), we find a sequence of
PDEs, parametrized by viscosity @xmath and “height” @xmath ; we denote
these solutions @xmath . These solutions match @xmath as @xmath . We
then show that for large @xmath , they also match @xmath for nonzero
@xmath .

Let @xmath and @xmath . We define @xmath as the solution to ( 3.9 ) with
the modified boundary conditions

  -- -------- -- --------
     @xmath      (3.22)
  -- -------- -- --------

This is a modification of the original problem with a lower bound
saturation point of @xmath . Clearly, as @xmath , @xmath on the
boundary.

As in Prop. 3.3.8 , for fixed @xmath we can write @xmath for any @xmath
and @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath and @xmath , and define

  -- -------- --
     @xmath   
  -- -------- --

Then

  -- -------- --
     @xmath   
  -- -------- --

As @xmath is regular and @xmath is compact, @xmath pointwise on @xmath ,
and the convergence is also uniform. Clearly, then, @xmath almost
everywhere on @xmath . Furthermore, for any @xmath , @xmath everywhere
on @xmath . Therefore @xmath in @xmath for all @xmath [ 59 , Prop. 6.4]
. The rate of convergence, @xmath , is determined by the set of points
@xmath . Thus, by Prop. 3.3.8 and Assum. 3.3.1 ,

  -- -------- -- --------
     @xmath      (3.23)
  -- -------- -- --------

To match the boundary conditions of @xmath to those of @xmath for a
fixed @xmath , we must choose @xmath large in ( 3.22 ). Subsequently,
when @xmath is large in ( 3.23 ), e.g., when @xmath , we have @xmath .
This verifies ( 3.21 ). ∎

When @xmath on @xmath , and for small @xmath , the exponent of @xmath
directly encodes @xmath . The following simple example illustrates Thm.
3.3.10 . Additional examples on the Torus @xmath and on a complex
triangulated mesh are included in § 4.8 of chapter 4 .

###### Example 3.3.1 (The Annulus in @xmath).

Let @xmath , where @xmath is the distance to the origin. Let @xmath be
the inner and outer circles. Letting @xmath ( @xmath ), we get @xmath .
For symmetry reasons, we can assume a radially symmetric solution to the
RL Eq. For a given dimension @xmath , the radial Laplacian is @xmath .
So ( 3.9 ) becomes: @xmath for @xmath , and @xmath . The solution, as
calculated in Maple [ 72 ] , is

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath are the @xmath ’th order modified Bessel
functions of the first kind and second kind, respectively. A series
expansion of @xmath around @xmath (partially calculated with Maple)
gives

  -- -------- -- --------
     @xmath      (3.24)
  -- -------- -- --------

As the limiting behavior of @xmath , as @xmath grows small, depends on
the exponents of the two terms in ( 3.24 ), one can check that the limit
depends on whether @xmath is nearer to @xmath or @xmath . Depending on
this, one of the terms drops out in the limit. From here, it is easy to
check that @xmath , confirming ( 3.21 ).

We simulated this problem with @xmath by sampling @xmath points from the
ball @xmath , rescaling points having @xmath to @xmath , and rescaling
points having @xmath to @xmath . @xmath is approximated up to a constant
using the numerical discretization, via ( 3.7 ), of ( 3.8 ). For the
graph Laplacian we used a @xmath NN graph and @xmath .

Fig. 3.1 shows (in the @xmath axis) the estimate @xmath as @xmath grows
small. The colors of the points reflect the true distance to @xmath :
@xmath . Note the convergence as @xmath , and also the clear offset of
@xmath which is especially apparent in the right panel at @xmath and
@xmath .

From the second of Eqs. ( 3.19 ) and the fact that @xmath , we have
@xmath for @xmath , and @xmath for @xmath . To solve this near @xmath ,
we use the boundary condition @xmath and get @xmath . Likewise, near
@xmath we use the boundary condition @xmath and get @xmath . Near @xmath
, the solution becomes @xmath , and near @xmath , it becomes @xmath ,
which match the earlier series expansion of the full solution.
Furthermore, upon an additional Taylor expansion near @xmath , we have
@xmath . Note the extra term in the @xmath estimate, which has a large
effect when @xmath is small (as seen in the right pane of Fig. 3.1 ). A
similar expansion can be made around the outer circle, at @xmath .

#### 3.3.9 The SSL Problem of §3.3.1, Revisited

Armed with our study of the RL PDE, we can now return to the original
SSL problem of § 3.3.1 .

Suppose the anchor is composed of two simply connected domains @xmath
and @xmath , where @xmath takes on the constant values @xmath and @xmath
, respectively, within each domain. When @xmath , we can directly apply
the result of Thm. 3.3.10 to ( 3.8 ). The solution, for @xmath , is
given by ( 3.20 ) with @xmath :

  -- -------- --
     @xmath   
  -- -------- --

The solution depends on both the geometry of @xmath (via the geodesic
distance to @xmath or @xmath ) and on the values chosen to represent the
class labels. For example, suppose @xmath . As @xmath grows large and
@xmath grows small, we apply ( 3.21 ) to see that the classifier is
biased towards the class in @xmath :

  -- -------- -- --------
     @xmath      (3.25)
  -- -------- -- --------

Choosing the symmetric labels @xmath is more natural. In this case, we
decompose ( 3.9 ) into two problems:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

and note that by linearity of the problem and the separation of the
anchor conditions, the solution to ( 3.9 ) is given by @xmath .
Therefore, by taking @xmath , we separate ( 3.8 ) into two problems with
nonnegative anchor conditions (one in @xmath and one in @xmath ).
Applying the result of Thm. 3.3.10 to each of these individually, and
combining the solutions, yields

  -- -------- -- --------
     @xmath      (3.26)
  -- -------- -- --------

This solution is zero when @xmath , positive when @xmath , and negative
otherwise. That is, in the noiseless, low regularization regime with
symmetric anchor values, algorithms like LapRLS classification assign
the point @xmath to the class that is closest in geodesic distance. We
illustrate this with a simple example of classification on the sphere
@xmath .

###### Example 3.3.2.

We sample @xmath points from the sphere @xmath at random, and define the
two anchors @xmath and @xmath . Here @xmath is a cap of angle @xmath
around point @xmath . The associated anchor labels are @xmath and @xmath
. We discretize the Laplacian @xmath using @xmath , and @xmath and solve
( 3.4 ). Fig. 3.2 compares the numerical solutions at small @xmath to
our estimates from ( 3.26 ). The two solutions are comparable up to a
positive multiplicative factor (due to the fact that @xmath converges to
@xmath times a constant). @xmath

### 3.4 Technical Details

#### 3.4.1 Deferred Proofs

##### Stability of @xmath

To prove the stability of @xmath , we first need to present some
notation. The matrices @xmath and @xmath can be written in terms of
submatrices to simplify the exposition. Separating these matrices into
submatrices associated with the @xmath labeled points and the @xmath
unlabeled points, we write:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Note that the two identities in the definition of @xmath are of size
@xmath and @xmath , respectively.

We will also also need a lemma bounding the spectrum of the matrix
@xmath .

###### Lemma 3.4.1.

The matrix @xmath is bounded in spectrum between @xmath and @xmath .

###### Proof.

The hermitian matrix @xmath has eigenvalues bounded between @xmath and
@xmath . The eigenvalues of its lower right principal submatrix, @xmath
, are therefore also bounded between @xmath and @xmath [ 49 ,
Thm. 4.3.15] . Finally, @xmath is similar to @xmath via the
transformation @xmath . ∎

We are now ready to prove the stability of @xmath .

###### Proof of Prop. 3.3.3, Stability.

We first expand @xmath in block matrix form:

  -- -------- --
     @xmath   
  -- -------- --

From the block matrix inverse formula, the inverse of @xmath is:

  -- -------- --
     @xmath   
  -- -------- --

and the norm may be bounded as:

  -- -------- -- --------
     @xmath      (3.27)
  -- -------- -- --------

where we use the inequalities @xmath and @xmath .

We first expand @xmath . By Lem. 3.4.1 , @xmath is bounded in spectrum
between @xmath and @xmath , and by the first assumption in the
proposition, when @xmath we have @xmath . Thus there exists some @xmath
so that for all @xmath we can write

  -- -------- --
     @xmath   
  -- -------- --

Now we use this expansion to bound @xmath . Let @xmath . As the norm is
subadditive,

  -- -------- -- --------
     @xmath      (3.28)
  -- -------- -- --------

Furthermore, we can bound @xmath as follows. Since the entries of @xmath
are nonnegative, @xmath where @xmath is a vector of all ones. Further,
since @xmath is a submatrix of a stochastic matrix, @xmath . Thus since
@xmath , for @xmath large enough ( 3.28 ) is bounded by the geometric
sum:

  -- -------- --
     @xmath   
  -- -------- --

and this last term is bounded based on our initial assumption: @xmath .
Thus, for large enough @xmath , @xmath .

Now we bound @xmath . Note that @xmath . As @xmath and @xmath is
stochastic, @xmath . Putting together these two steps, we have @xmath .

Combining these two bounds, ( 3.27 ) finally becomes

  -- -------- --
     @xmath   
  -- -------- --

For small @xmath , the second term is the maximum and the result
follows. ∎

##### Characterization of @xmath

We first need some preliminary definitions and results.

We define the cut locus of the set @xmath as closure of the set of
points in @xmath where @xmath is not differentiable (i.e., where there
is more than one minimal geodesic between @xmath and @xmath ):

  -- -------- --
     @xmath   
  -- -------- --

The cut locus and @xmath have several important properties, which we now
list:

1.  The Hausdorff dimension of @xmath is at most @xmath [ 68 , Cor.
    4.12] .

2.  @xmath is closed in @xmath .

3.  The open set @xmath can be continuously retracted to @xmath .

4.  If @xmath then @xmath is @xmath in @xmath .

Items 2 - 4 are proved in [ 68 , Prop 4.6] .

Property 1 shows that @xmath is smooth almost everywhere on @xmath .
Properties 2 - 3 show that @xmath is composed of a finite number of
disjoint connected components, each touching @xmath . Finally, property
4 shows that @xmath is as smooth as the boundary @xmath .

Let @xmath be a @xmath -dimensional Riemannian manifold and let @xmath
be a Riemannian submanifold such that @xmath is regular (in the PDE
sense). As in Thm. 3.3.9 , define the differential equation in @xmath as

  -- -------- -------- -- --------
     @xmath   @xmath      (3.29)
     @xmath   @xmath      
  -- -------- -------- -- --------

We first show that @xmath of Thm. ( 3.3.9 ) has a unique, smooth, local
solution in a chart at @xmath . To do this we will use the method of
characteristics [ 38 , Chap. 3] . We will need an established result for
the local solutions of PDEs on open subsets of @xmath .

Let @xmath be an open subset in @xmath and let @xmath . Let @xmath and
@xmath be its derivative on @xmath . Finally, suppose @xmath and let
@xmath . We study the first-order PDE

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Note that we can write @xmath . The main test for existence, uniqueness,
and smoothness is the test for noncharacteristic boundary conditions.

###### Definition 3.4.1 ([38], Noncharacteristic boundary condition).

Let @xmath , @xmath and and @xmath . We say the triple @xmath is
noncharacteristic if

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the outward unit normal to @xmath at @xmath . We also
say that the noncharacteristic boundary condition holds at @xmath .

This test is sufficient for local existence:

###### Proposition 3.4.2 ([38], §3.3, Thm. 2 (Local Existence)).

Assume that @xmath is smooth and that the noncharacteristic boundary
condition holds on @xmath for some triple @xmath . Then there exists a
neighborhood @xmath of @xmath in @xmath and a unique, @xmath function
@xmath that solves the PDE

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

We are now ready to prove the existence, uniqueness, and smoothness of
@xmath .

###### Lemma 3.4.3.

Let @xmath be one of the connected components of @xmath . Then on any
chart @xmath that satisfies @xmath and for which @xmath is sufficiently
regular, the differential equation ( 3.29 ) has a unique, and smooth
solution.

###### Proof.

Under the diffeomorphism @xmath , ( 3.29 ) is modified. Choose a point
@xmath and apply @xmath . The boundary @xmath becomes a boundary @xmath
in @xmath . Let @xmath represent the rest of the mapped space. Eq. (
3.29 ) then becomes

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where we use the abusive notation @xmath for a function @xmath , and
where @xmath is the (smooth) Laplacian of @xmath mapped into local
coordinates. Using the notation of Lem. 3.4.1 , we can write the
equation above as @xmath where

  -- -------- --
     @xmath   
  -- -------- --

and therefore @xmath becomes

  -- -------- --
     @xmath   
  -- -------- --

for @xmath . At the point @xmath , the outward unit normal is @xmath ,
which in local coordinates is given by the vector @xmath for @xmath .

The uniqueness, existence, and smoothness of @xmath near @xmath in this
chart follows by Prop. 3.4.2 after checking the noncharacteristic
boundary condition for @xmath at @xmath :

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where the last equality follows by definition of the distance function
in terms of the Eikonal equation. ∎

###### Theorem 3.4.4.

Let @xmath be one of the connected components of @xmath . The
differential equation ( 3.29 ) has a unique, and smooth solution on
@xmath .

###### Proof of Thm. 3.4.4.

A local solution exists in an open ball around each point @xmath in the
region @xmath , due to Lem. 3.4.3 . The size of each ball is bounded
from below, so by compactness we can find a finite number of subsets
@xmath that cover @xmath , for which ( 3.29 ) has a smooth unique
solution, and which overlap. As the charts overlap and the associated
mappings are diffeomorphic, a consistent, smooth, unique solution
therefore exists near @xmath .

To extend this solution away from the boundary, we choose a small
distance @xmath such that for all @xmath with @xmath , that @xmath is
also in the initially solved region @xmath . This set, which we call
@xmath , is a contour of @xmath within @xmath . From the previous
argument, @xmath has been solved up to this contour, and we now look at
an updated version of ( 3.29 ) by setting the new Dirichlet anchor
conditions at @xmath from the solved-for @xmath , and setting the
interior of the updated problem domain to the remainder of @xmath .

Let @xmath and let @xmath . The method of characteristics also applies
on @xmath near @xmath . We apply Lem. 3.4.3 with the updated Dirichlet
boundary conditions. As @xmath defines a contour of @xmath , its outward
normal direction is @xmath . Similarly, @xmath (of Lem. ( 3.4.3 )) has
not changed. A solution therefore exists locally around each point
@xmath . The process above can be repeated to “fill in” the solution
within all of @xmath . ∎

#### 3.4.2 Details of the Regression Problem of §3.3.1

In this deferred section, we decompose the problem ( 3.2 ) into two
parts: elements associated with the first @xmath labeled points (these
are given subscript @xmath ) and elements associated with the remaining
unlabeled points (given subscript @xmath ). This decomposition provides
a more direct look into the how the assumptions in § 3.3.2 simplify the
original problem, and how the resulting optimization problem depends
only on the ratio of the two parameters @xmath and @xmath .

We first rewrite ( 3.2 ), expanding all the parts:

  -- -------- --
     @xmath   
  -- -------- --

In this system, the optimization problems on @xmath and @xmath are
coupled by the matrix @xmath .

The assumptions in § 3.3.2 decouple ( 3.2 ). This comes from the
equality constraint @xmath , equivalently @xmath . The problem is
further simplified by the restriction of the integral domain from @xmath
to @xmath in the modified penalty @xmath . We can write @xmath , and the
discretization of this term is @xmath . After these reductions, and the
reduction of the ridge term to @xmath , the problem ( 3.2 ) becomes:

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

The solution to this problem, combined with the constraint @xmath ,
leads to ( 3.4 ).

The @xmath term above normalizes the Euclidean norm of @xmath , thus
earning it the mnemonic “ambient regularizer”. The first @xmath term is
an inner product between @xmath and @xmath . As @xmath is an averaging
operator with negative coefficients, the component @xmath contains the
negative average of the labels for points in @xmath near @xmath . If
@xmath is far from @xmath , this component is near zero. Minimizing
@xmath therefore encourages points near @xmath to take on the labels of
their labeled neighbors. For points away from @xmath it has no direct
effect. Minimizing the second @xmath term encourages a diffusion of
values between points in @xmath , thus diffusing these near-boundary
labels to the rest of the space. This process earns the @xmath term the
mnemonic “intrinsic regularizer”, because it encourages diffusion of the
labels across @xmath .

Dividing the problem by @xmath , we see that the solution depends only
on the ratio @xmath . When @xmath the solution of ( 3.1 ) is biased
towards a constant [ 56 ] on @xmath , equivalent to solving the Laplace
equation @xmath on @xmath with the anchor conditions @xmath on @xmath .
This case of heavy regularization is useful when @xmath is small, but
offers little insight about how the solution depends on the geometry of
@xmath . We are interested in the situation of light regularization:
@xmath . We also independently see this assumption as a requirement for
convergence of @xmath in § 3.3.3 .

#### 3.4.3 The RL PDE with Nonempty Boundary (@xmath)

When the boundary of @xmath is not empty, Thm. 3.3.5 and Cor. 3.3.6 no
longer apply in their current form. In this section, we provide a road
map for how these results must be modified. We also argue why in the
case of small @xmath , the limiting results (expressions for @xmath and
@xmath in Assum. 3.3.1 , Thm. 3.3.9 , and Thm. 3.3.10 ) are not affected
by these modifications.

Let @xmath where @xmath . For points in the intersection of @xmath and
@xmath (for example, when @xmath , and the anchor “covers” the boundary
of @xmath ), we need only consider the standard anchor conditions. For
other cases, we proceed thus:

It has been shown [ 24 , Prop. 11] that as @xmath :

1.  @xmath for @xmath (this matches Thm. 3.3.1 ).

2.  For @xmath , @xmath , where @xmath is the nearest point in @xmath to
    @xmath and @xmath is the outward normal at @xmath . That is, near
    the boundary @xmath takes the outward normal derivative.

3.  This region @xmath is small, and shrinks with decreasing @xmath :
    @xmath .

One therefore expects that Thm. 3.3.5 and Cor. 3.3.6 still hold, albeit
with the norms restricted to points in @xmath . More specifically, the
set @xmath must necessarily become @xmath . Furthermore, as @xmath ,
this set grows to encompass more of @xmath .

As a result, the domains of the RL PDE ( 3.9 ) change. It is hard to
write down the boundary condition at @xmath , precisely because there is
no analytical description for how @xmath acts on functions in @xmath .
However, from item 2 above, we can model it as an unknown Neumann
condition.

Fortunately, for vanishing viscosity (small @xmath ), the effect of this
second boundary condition disappears: the Eikonal equation depends only
on the (Dirichlet) conditions at @xmath . More specifically, regardless
of other Neumann boundary conditions away from @xmath , Assum. 3.3.1
still holds and, as a result, so do Thm. 3.3.9 and Thm. 3.3.10 . This
follows because the Eikonal equation is a first order differential
equation, and so some of the boundary conditions may be dropped in the
small @xmath approximation. A more rigorous discussion requires a
perturbation analysis (see, e.g., [ 75 ] ). We instead provide an
example, mimicking Ex. 3.3.1 , except now we let the anchor domain be
the inner circle only.

###### Example 3.4.1 (The Annulus in @xmath with reduced anchor).

Let @xmath , where @xmath is the distance to the origin. Let @xmath be
the inner circle. Letting @xmath ( @xmath ), we get @xmath . We again
assume a radially symmetric solution to the RL Eq. and ( 3.9 ) becomes:
@xmath for @xmath , @xmath . Furthermore, since the boundary condition
at @xmath is unknown, we set it to be an arbitrary Neumann condition:
@xmath . The solution is

  -- -------- --
     @xmath   
  -- -------- --

A series expansion of @xmath around @xmath gives @xmath , and therefore
@xmath (again confirming ( 3.21 )).

We simulated this problem with @xmath by sampling @xmath points from the
ball @xmath , and rescaling points with @xmath to @xmath . @xmath is
approximated up to a constant using the numerical discretization, via (
3.7 ), of ( 3.8 ). For the graph Laplacian we used a @xmath NN graph and
@xmath .

Fig. 3.3 shows (in the @xmath axis) the estimate @xmath as @xmath grows
small. The colors of the points reflect the true distance to @xmath :
@xmath . Note the convergence as @xmath , and also the clear offset of
@xmath which is especially apparent in the right pane near @xmath .

From the second of Eqs. ( 3.19 ) and the fact that @xmath , we have
@xmath for @xmath and @xmath for @xmath . Solving this we get @xmath .
The solution becomes @xmath , which matches the earlier series expansion
of the full solution. Furthermore, upon an additional Taylor expansion
we have @xmath . As before, the extra term in the @xmath estimate has a
large effect when @xmath is small (as seen in the right pane of Fig. 3.3
).

### 3.5 Conclusion and Future Work

We have proved that the solution to the SSL problem ( 3.4 ) converges to
the sampling of a smooth solution of a Regularized Laplacian PDE, in
certain limiting cases. Furthermore, we have applied the established
theory of Viscosity PDE solutions to analyze this Regularized Laplacian
PDE. Our analysis leads to a geometric framework for understanding the
regularized graph Laplacian in the noiseless, low regularization regime
(where @xmath ). This framework provides intuitive explanations for, and
validation of, machine learning algorithms that use the inverse of a
regularized Laplacian matrix.

We have taken the first steps in extending the theoretical analysis in
this chapter to manifolds with boundary (§ 3.4.3 ) While the results
within this section can be confirmed numerically, in some cases
additional work must be done to confirm them in full generality.
Furthermore, Assum. 3.3.1 awaits confirmation within the viscosity
theory community.

There are a host of applications derived from the work in this chapter,
and we turn our focus to them in chapter 4 .

## Chapter 4 The Inverse Regularized Laplacian: Applications

### 4.1 Introduction

Thanks to the theoretical development in chapter 3 , we now have a
framework within which we can construct new tools for learning (e.g. a
regularized geodesic distance estimator and a new multiclass
classifier). These tools can also shed light on other results in the
literature (e.g. a result of [ 74 ] ). Throughout this chapter we will
use the notation developed in chapter 3 .

### 4.2 Regularized Nearest Sub-Manifold (NSM) Classifier

We now construct a new robust geodesic distance estimator and employ it
for classification. We then demonstrate the classifier’s efficacy on
several standard data sets. To construct the estimator, first choose
some anchor set @xmath , and suppose the points @xmath are sampled from
@xmath . To calculate the distance @xmath for @xmath , construct the
normalized graph Laplacian @xmath . Choosing @xmath appropriately, solve
the linear system ( 3.7 ):

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

where @xmath is a vector of all zeros for sample points in @xmath and
all ones for sample points in @xmath . For @xmath large, @xmath small,
and @xmath small, this linear system approximates ( 3.9 ) with @xmath .
Applying Thm. 3.3.10 , we see that @xmath .

While the estimator @xmath is approximate and only valid up to a
constant, it is also simple to implement and consistent (due to Cor.
3.3.6 ).

We know of two other consistent geodesics estimators that work on point
samples from @xmath . One performs fast marching by constructing complex
local upwind schemes that require the iterative solution of sequences of
high dimensional quadratic systems [ 88 ] . Another performs fast
marching in @xmath on offsets of @xmath and is also approximate [ 71 ] .
The first scheme is complex to implement; the second is exponential in
the ambient dimension @xmath . Our estimator, on the other hand, can be
implemented in Matlab in under 10 lines, given one of many fast
approximate NN estimators. Furthermore, it requires the solution of a
linear system of size essentially @xmath , so its complexity depends
only on the number of samples @xmath , not on the ambient dimension
@xmath . Finally, our scheme allows for a natural regularization by
tweaking the viscosity parameter @xmath . § 4.8 contains numerical
comparisons between our estimator and, e.g. Dijkstra’s Shortest Path and
Sethian’s Fast Marching estimators.

The lack of dynamic range in the estimator @xmath , following ( 4.1 ),
leads to important numerical considerations. According to Thm. 3.3.10 ,
for a given sampling @xmath one would choose @xmath to have an accurate
estimate of geodesics for all point samples. In this case, however, many
points far from @xmath may have their associated estimate @xmath drop
below the machine epsilon. In this case an iterative multiscale approach
will work: estimates are first calculated for points nearest to @xmath
for which no estimate yet exists (but @xmath is above machine epsilon),
then @xmath is multiplied by some factor @xmath , and the process is
repeated.

We now use the above estimator to form the Nearest Sub-Manifold (NSM)
classifier. The classifier is based on two simplifications. First, for
noisy samples, one would want to select @xmath based on the noise level
or via cross-validation; it therefore becomes a regularization term.
Second, as seen in § 3.3.9 , for classification the exact estimate of
geodesic distance is less important than relative distances; hence there
is no need to estimate scaling constants.

As before, suppose we are given @xmath samples from a manifold @xmath .
Of these, each of the first @xmath belong to one of @xmath classes; that
is, @xmath , @xmath . We assume that all points within class @xmath
belong to a smooth closed subset of @xmath , which we call anchor @xmath
, @xmath . For each anchor, we define the anchor data vector @xmath via
@xmath , @xmath . To classify, first choose @xmath and solve ( 4.1 ) for
each of the @xmath different anchor sets @xmath (and associated @xmath
), to get solutions @xmath . Then for each unlabeled point @xmath ,
@xmath encodes its distance to anchor @xmath . The decision rule is
@xmath .

For @xmath and @xmath large, @xmath , @xmath small, and no noise, @xmath
will accurately estimate the class which is closest in geodesic distance
to @xmath . In the noisy, finite sample case with irregular boundaries,
@xmath provides a regularized estimate of the same.

### 4.3 NSM Classifier: Performance

We compare the classification performance of the NSM classifier to
several state-of-the-art classifiers using the test set from [ 19 ]
(testing protocol and datasets:
http://www.kyb.tuebingen.mpg.de/ssl-book/ ). For the NSM classifier, we
performed a parameter search as described in [ 19 , §21.2.5] , and
additionally cross-validated over the viscosity parameter @xmath scaled
by the median distance between pairs of points in @xmath .

We compare our results with publicly available implementations of:

-   LapRLS from M. Belkin’s website, with obvious modifications for
    one-vs-all multiclassification and with the exception that, as
    opposed to [ 19 ] , we used @xmath in the kernel @xmath instead of
    @xmath . Here, we also performed a parameter search as in [ 19 ,
    §21.2.5] .

-   LDS from O. Chapelle’s website with parameters optimized as in [ 19
    , §21.2.11] .

-   Kernel TSVM using primal gradient descent (available in the LDS
    package) with parameters optimized as in [ 19 , §21.2.1] .

For testing, we also included the LIBRAS (LIB) dataset with 12 splits of
@xmath labeled points and the ionosphere (Ion) dataset with 12 splits of
@xmath labeled points. All datasets have @xmath (the task is binary
classification) except COIL, which has @xmath .

Table 4.1 shows percent classification error vs. percentage labeled
points, over 12 randomized splits of the testing and training data set.
Parameter optimization (cross-validation) was always performed on the
training splits only; classification error is reported over the testing
data. Note that that the NSM classifier is competitive with the others,
especially on those datasets where we expect a manifold structure (e.g.
the image sets USPS and COIL).

### 4.4 Irregular Boundaries and the counterexample of Nadler et al.

We relate the Annulus example (Ex. 3.3.1 ) to a negative result of [ 74
, Thm. 2] , which essentially states that no solution exists for ( 3.9 )
for @xmath with @xmath and the anchor set a countable number of points.
This yields a special case of a result known in PDE theory: no solution
exists to ( 3.9 ) when @xmath is irregular; and isolated points on
subsets of @xmath , @xmath , are irregular [ 47 , Irregular Boundary
Point] .

This is very clearly seen in Ex. 3.3.1 , where attempting to let @xmath
(thus forcing a single point anchor) forces the first term of the
solution @xmath in ( 3.24 ) to zero for any @xmath , regardless of the
anchor condition at @xmath and of @xmath . The major culprit here is the
@xmath term that appears in the radial Laplacian and is unbounded at the
origin. Note, however, that viscosity solutions to ( 3.10 ) do exist
even for singular anchors [ 68 ] .

In many practical cases (i.e., if we had chosen single point anchors in
§ 3.3.9 , Ex. 3.3.2 , etc), the sampling size is finite and we keep
@xmath for some @xmath . In these cases, the issues raised here do not
affect the numerical analysis because even single points act like balls
of radius @xmath in @xmath .

### 4.5 Beyond Classification: Graph Denoising, Manifold Learning

The ideas presented in the previous sections can also be applied to
other areas of machine learning. As illustrations, we show that the
graph denoising scheme of [ 13 ] is a special case of our geodesics
estimator. Further, we show how to construct a regularized variant of
ISOMAP and provide some numerical examples of geodesics estimation.

### 4.6 The Graph Denoising Algorithm of Chapter 2

In chapter 2 , we studied decision rules for denoising (removing) edges
from NN graphs that have been corrupted by sampling noise. We examine
the Neighborhood Probability Decision Rule (NPDR) of § 2.4 , and show
that in the low noise, low regularization regime, it removes graph edges
between geodesically distant points.

The NPDR is constructed in three stages: (a) the NN graph @xmath is
constructed from the sample points @xmath . @xmath contains an initial
estimate of neighbors in @xmath , but may contain incorrect edges due to
sampling noise; (b) a special Markov random walk is constructed on
@xmath , resulting in the transition probability matrix @xmath for
@xmath ; (c) the edges @xmath with the smallest associated entries
@xmath are removed from @xmath . In chapter 2 , we provide a
probabilistic interpretation for the coefficients of @xmath .

We show that @xmath encodes geodesic distances by reducing @xmath to
look like ( 3.9 ):

  -- -------- -------- -- -------
     @xmath   @xmath      (4.2)
  -- -------- -------- -- -------

For @xmath small, after applying the RHS of ( 4.2 ), @xmath
approximately solves ( 3.9 ) with @xmath , @xmath (where @xmath is a
function of @xmath ), and @xmath . Then by Thm. 3.3.10

  -- -------- --
     @xmath   
  -- -------- --

Thus in the noiseless case and with @xmath , the NPDR algorithm will
remove edges in the graph between points that are geodesically far from
each other. As edges of this type are the most detrimental to learning [
3 ] , the NPDR is a powerful denoising rule. As shown in chapter 2 , for
noisy samples one would choose @xmath to regularize for noisy edges. In
this case, one can think of @xmath as a highly regularized encoding of
pairwise geodesic distances.

### 4.7 Viscous ISOMAP

As a second example of how the ideas from § 3.3.9 can be used, we
construct a regularized variant of ISOMAP [ 105 ] , which we call
Viscous ISOMAP.

ISOMAP is a dimensionality reduction algorithm that constructs an
embedding for @xmath points sampled from a high-dimensional space by
performing Multidimensional Scaling (MDS) on the estimated geodesic
distance matrix of the NN graph of these points.

The first step of ISOMAP is to estimate all pairwise geodesic distances.
Traditionally this is done via Dijkstra’s Shortest Path algorithm. We
replace this step with our regularized geodesics estimator. A direct
implementation requires @xmath calculations of ( 4.1 ). However, a
faster estimator can be constructed, based on our analysis of the NPDR
algorithm in § 4.6 . Specifically, to calculate pairwise distances,
first calculate @xmath . Then the symmetrized geodesics estimates are
@xmath , where the logarithm is taken elementwise. Finally, perform MDS
on the matrix @xmath to calculate the ISOMAP embedding.

For small @xmath , the Viscous ISOMAP embedding matches that of standard
ISOMAP. For large @xmath , the additional regularization can remove the
effects of erroneous edges caused by noise and outliers.

We provide a rather simple numerical example. It confirms that for small
viscosity @xmath , Viscous ISOMAP embeddings match standard ISOMAP
embeddings, and that for larger viscosities the embeddings are less
sensitive to outliers in the original sampling set @xmath and in @xmath
.

Fig. 4.1 compares Viscous ISOMAP to regular ISOMAP on a noisy Swiss Roll
with topological shortcuts. We used the same @xmath samples and @xmath
for NN estimation for both algorithms, and @xmath for Viscous ISOMAP.
Note how for small @xmath , the Viscous ISOMAP embedding matches the
standard one. Also note how increasing the viscosity term @xmath leads
to the an accurate embedding in the principal direction, “unrolling” the
Swiss Roll.

### 4.8 Numerical Examples of Geodesics Estimation

We provide two examples of Geodesic Estimation: on the Torus, and on a
triangulated mesh. For the Torus, we used the normalized Graph Laplacian
of § 3.3.1 and ground truth geodesic distances were given by Dijkstra’s
Shortest Path algorithm. On the mesh, we used the mesh surface Laplacian
of [ 7 ] , and for ground truth geodesic distances the mesh Fast
Marching algorithm of [ 57 ] (As implemented in Toolbox Fast Marching at
http://www.ceremade.dauphine.fr/~peyre/ ). In both cases, @xmath was
calculated via the geodesics estimator of § 4.2 ; thus, as always,
@xmath estimates geodesic distances up to a constant.

###### Example 4.8.1 (The Torus @xmath in @xmath).

The torus @xmath is defined by the points @xmath for @xmath and @xmath .
We used @xmath randomly sampled points, with @xmath neighbors for the
initial NN graph, @xmath and @xmath . Setting @xmath , @xmath where
@xmath corresponds to @xmath , and @xmath is a randomly chosen point, we
can calculate geodesic distances of all points in @xmath to these
anchors. The results are shown in Fig. 4.2 .

###### Example 4.8.2 (The Dancing Children Mesh).

The Dancing Children mesh is a complex (high genus) mesh from the Aim
@xmath Shape Repository ( http://shapes.aimatshape.net/ ). The mesh
@xmath is composed of @xmath vertices; and we used @xmath (times the
mean edge distance), and @xmath for the estimation procedure. The anchor
point @xmath was chosen randomly. Our results are shown in Fig. 4.3 .
Note that minor discrepancies between the Fast Marching estimate @xmath
and our estimate @xmath occur near areas with complex topology and areas
of high curvature (e.g. near a hole in the mesh).

## Chapter 5 Synchrosqueezing111This chapter is based on work in
collaboration with Hau-Tieng Wu, Department of Mathematics, and
Gaurav Thakur, Program in Applied and Computational Mathematics,
Princeton University, as submitted in [12].

### 5.1 Introduction

In this chapter, we analyze the Synchrosqueezing transform, a consistent
and invertible time-frequency analysis tool that can identify and
extract oscillating components (of time-varying frequency and amplitude)
from regularly sampled time series. We first describe a fast algorithm
implementing the transform. Second, we show Synchrosqueezing is robust
to bounded perturbations of the signal. This stability property extends
the applicability of Synchrosqueezing to the analysis of nonuniformly
sampled and noisy time series, which are ubiquitous in engineering and
the natural sciences. Numerical simulations show that Synchrosqueezing
provides a natural way to analyze and filter a variety of signals. In
Chapter 6 , we use Synchrosqueezing to analyze a variety of data,
including ECG signals and climate proxies.

The purpose of this chapter is twofold. We first describe the
Synchrosqueezing transform in detail and highlight the subtleties of a
new fast numerical implementation. Second, we show both numerically and
theoretically that Synchrosqueezing is stable under bounded signal
perturbations. It is therefore robust to noise and to errors incurred by
preprocessing using approximations, such as interpolation.

The chapter is organized as follows. We first describing
Synchrosqueezing, and in § 5.4 we provide a fast new implementation ³ ³
3 The Synchrosqueezing Toolbox for MATLAB, and the codes used to
generate all of the figures in this chapter, are available at
http://math.princeton.edu/~ebrevdo/synsq/ . . In § 5.5 we provide
theoretical evidence that Synchrosqueezing analysis and reconstruction
are stable to bounded perturbations. In § 5.6 , we numerically compare
Synchrosqueezing to other common transforms, and provide examples of its
stability properties. Conclusions and ideas for future theoretical work
are in § 5.8 .

Comprehensive numerical examples and applications are deferred to
Chapter 6 .

### 5.2 Prior Work

Synchrosqueezing is a tool designed to extract and compare oscillatory
components of signals that arise in complex systems. It provides a
powerful method for analyzing signals with time-varying behavior and can
give insight into the structure of their constituent components. Such
signals @xmath have the general form

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

where each component @xmath is an oscillating function, possibly with
smoothly time-varying amplitude and frequency, and @xmath represents
noise or observation error. The goal is to extract the amplitude factor
@xmath and the Instantaneous Frequency (IF) @xmath for each @xmath .

Signals of the form ( 5.1 ) arise naturally in engineering and
scientific applications, where it is often important to understand their
spectral properties. Many time-frequency (TF) transforms exist for
analyzing such signals, such as the Short Time Fourier Transform (STFT),
Wavelet Transform, and Wigner-Ville distribution [ 41 ] , but these
methods can fail to capture key short-range characteristics of the
signals. As we will see, Synchrosqueezing deals well with such complex
data.

Synchrosqueezing is a TF transform that is ostensibly similar to the
family of time-frequency reassignment (TFR) algorithms, methods used in
the estimation of IFs in signals of the form given in ( 5.1 ). TFR
analysis originates from a study of the STFT, which smears the energy of
the superimposed IFs around their center frequencies in the spectrogram.
TFR analysis “reassigns” these energies to sharpen the spectrogram [ 40
, 42 ] . However, there are some significant differences between
Synchrosqueezing and most standard TFR techniques.

Synchrosqueezing was originally introduced in the context of audio
signal analysis [ 30 ] . In [ 32 ] , it was further analyzed
theoretically as an alternative way to understand the Empirical Mode
Decomposition (EMD) algorithm [ 50 ] . EMD has proved to be a useful
tool for analyzing and decomposing natural signals. Like EMD,
Synchrosqueezing can extract and clearly delineate components with time
varying spectrum. Furthermore, like EMD, and unlike most TFR techniques,
it allows individual reconstruction of these components.

### 5.3 Synchrosqueezing: Analysis

Synchrosqueezing is performed in three steps. First, the Continuous
Wavelet Transform (CWT) @xmath of @xmath is calculated [ 31 ] . Second,
an initial estimate of the FM-demodulated frequency, @xmath , is
calculated on the support of @xmath . Finally, this estimate is used to
squeeze @xmath via reassignment; we thus get the Synchrosqueezing
representation @xmath . Synchrosqueezing is invertible: we can calculate
@xmath from @xmath . Our ability to extract individual components stems
from filtering @xmath by keeping energies from specific regions of the
support of @xmath during reconstruction.

Note that Synchrosqueezing, as originally proposed [ 30 ] , estimates
the FM-demodulated frequency from the wavelet representation @xmath
before performing reassignment. However, it can be adapted to work on
“on top of” many invertible transforms (e.g. the STFT [ 106 ] ). We
focus on the original wavelet version as described in [ 32 ] .

We now detail each step of Synchrosqueezing, using the harmonic signal
@xmath for motivation. As a visual aid, Fig. 5.1 shows each step on the
signal @xmath with @xmath and @xmath . Note that Figs. 5.1 (b,d) show
that Synchrosqueezing is more “precise” than the CWT.

#### 5.3.1 CWT of @xmath

For a given mother wavelet @xmath , the CWT of @xmath is given by @xmath
where @xmath is the scale and @xmath is the time offset. We assume that
@xmath has fast decay, and that its Fourier transform @xmath is
approximately zero in the negative frequencies ⁴ ⁴ 4 More details about
the Fourier transform, and analysis on intervals, are available in App.
B : @xmath for @xmath , and is concentrated around some positive
frequency @xmath [ 32 ] . Many wavelets have these properties (several
examples and compared in § 5.7 ). For @xmath , the harmonic signal
above, upon applying our assumptions we get @xmath .

#### 5.3.2 Calculate the FM-demodulated frequency @xmath

The wavelet representation of the harmonic signal @xmath (with frequency
@xmath ) will have its energy spread out in the time-scale plane around
the line @xmath , and this frequency will be encoded in the phase [ 30 ,
32 ] . In those regions where @xmath we would like to remove the effect
of the Wavelet on this frequency. We perform a type of FM demodulation
by taking derivatives: @xmath . This simple model leads to an estimate
of the frequency in the time-scale plane:

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

#### 5.3.3 Squeezing in the time-frequency plane: @xmath

The final step of Synchrosqueezing is reassigning energy in the
time-scale plane to the TF plane according to the frequency map @xmath .
Reassignment follows from the inversion property of the CWT: when @xmath
is real,

  -- -------- -- -------
     @xmath      (5.3)
  -- -------- -- -------

where @xmath is a normalizing constant.

We first break up the integrand in ( 5.3 ) according to the
FM-demodulated frequency estimate @xmath . Define frequency divisions
@xmath s.t. @xmath and @xmath for all @xmath . Further, let the
frequency bin @xmath be the set of points @xmath closer to @xmath than
to any other @xmath . We define the Discrete-Frequency Wavelet
Synchrosqueezing transform of @xmath as:

  -- -------- -- -------
     @xmath      (5.4)
  -- -------- -- -------

In other words, @xmath is the “volume” of the frequency preimage set
@xmath under the signed measure @xmath .

This definition has several favorable properties. First, it allows us to
reconstruct @xmath from @xmath :

  -- -------- -- -------
     @xmath      (5.5)
  -- -------- -- -------

Second, for the harmonic signal @xmath , with @xmath , there will be a
single @xmath such that @xmath is closest to @xmath . From ( 5.3 ), we
have @xmath . Further, the magnitude of @xmath is proportional to that
of @xmath : @xmath .

More generally, for a wide class of signals with slowly varying @xmath
and well separated @xmath , given a sufficiently fine division of the
frequency bins @xmath , each of the @xmath components can be well
concentrated into its own “curve” in the TF plane (see Thm. 5.5.1
below). This allows us to analyze such signals: by looking at @xmath to
identify and extract the curves, and to reconstruct their associated
components.

### 5.4 A Fast Implementation

In practice, we observe the vector @xmath , @xmath , where @xmath is a
nonnegative integer. Its elements, @xmath , correspond to a uniform
discretization of @xmath taken at the time points @xmath . To prevent
boundary effects, we pad @xmath on both sides (using, e.g., reflecting
boundary conditions).

We now describe a fast numerical implementation of Synchrosqueezing. The
speed of our algorithm lies in two key steps. First, we calculate the
Discrete Wavelet Transform (DWT) of the vector @xmath using the Fast
Fourier Transform (FFT). Second, we discretize the squeezing operator
@xmath in a way that lends itself to a fast numerical implementation.

#### 5.4.1 DWT of sampled signal @xmath

The DWT samples the CWT @xmath at the locations @xmath , where @xmath ,
@xmath , and the number of voices @xmath is a user-defined “voice
number” parameter [ 43 ] (we have found that @xmath works well). The DWT
of @xmath can be calculated in @xmath operations using the FFT. We
outline the steps below.

First note that @xmath , where @xmath denotes convolution over @xmath .
In the frequency domain, this relationship becomes: @xmath . We use this
to calculate the DWT, @xmath . Let @xmath ( @xmath ) be the standard
(inverse) circular Discrete Fourier Transform. Then

  -- -------- -- -------
     @xmath      (5.6)
  -- -------- -- -------

Here @xmath denotes elementwise multiplication and @xmath is an @xmath
-length vector with @xmath ; @xmath are samples in the unit frequency
interval: @xmath , @xmath .

#### 5.4.2 A Stable Estimate of @xmath: @xmath

We first require a slight modification of the FM-demodulated frequency
estimate ( 5.2 ),

  -- -------- -- -------
     @xmath      (5.7)
  -- -------- -- -------

This definition is equivalent to ( 5.2 ) when Synchrosqueezing is
performed via ( 5.4 ), and simplifies the algorithm.

In practice, signals have noise and other artifacts due to, e.g.,
sampling errors, and the phase of @xmath is unstable when @xmath . As
such the user should choose some @xmath (we often use @xmath ) as a hard
threshold on @xmath . We define the numerical support of @xmath , on
which @xmath can be estimated:

@xmath , for @xmath .

The estimate of @xmath , @xmath , can be calculated by taking
differences of @xmath with respect to @xmath before applying ( 5.7 ),
but we provide a more direct way. Let Using the property @xmath , we
estimate the FM-demodulated frequency, for @xmath , as

@xmath

with the time derivative of @xmath estimated via (e.g., [ 104 ] ):

@xmath

where @xmath for @xmath .

Finally, we normalize @xmath by @xmath so that the dominant frequency
estimate is @xmath when @xmath .

#### 5.4.3 Fast estimation of @xmath from @xmath and @xmath

The representation @xmath is given with respect to @xmath log-scale
samples of the scale @xmath , and this leads to several important
considerations when estimating @xmath via ( 5.3 ) and ( 5.4 ). First,
due to lower resolutions in coarser scales, we expect to get lower
resolutions in the lower frequencies. We thus divide the frequency
domain into @xmath components on a log scale. Second, sums with respect
to @xmath on a log scale, @xmath with @xmath , lead to the modified
integrand @xmath in ( 5.4 ).

To choose the frequency divisions, note that the discretization period
@xmath limits the maximum frequency @xmath that can be estimated. The
Nyquist theorem suggests that this frequency is @xmath . Further, if we
assume periodicity, the maximum period of an input signal is @xmath ;
thus the minimum frequency is @xmath . Combining these limits with the
log scaling of the @xmath ’s we get the divisions: @xmath , @xmath ,
where @xmath . Note, the voice number @xmath has a big effect on the
frequency resolution.

We can now calculate the Synchrosqueezed estimate @xmath . Our fast
implementation of ( 5.4 ) finds the associated @xmath for each @xmath
and adds it to the correct sum, instead of performing a search over all
scales for each @xmath . This is possible because @xmath only ever lands
in one frequency bin. We provide pseudocode for this @xmath
implementation in Alg. 1 .

for @xmath to @xmath do {Initialize @xmath for this @xmath }

@xmath

end for

for all @xmath do {Calculate ( 5.4 )}

{Find frequency bin via @xmath , and @xmath }

@xmath

if @xmath then

{Add normalized term to appropriate integral; @xmath }

@xmath

end if

end for

Algorithm 1 Fast calculation of @xmath for fixed @xmath

#### 5.4.4 IF Curve Extraction and Filtered Reconstruction

A variety of signals, especially sums of quasi-harmonic signals with
well-separated IFs, will have a frequency image @xmath composed of
several curves in the @xmath plane. The image of the @xmath th curve
corresponds to both the IF @xmath , and the entire component @xmath .

To extract a discretized curve @xmath we maximize a functional of the
energy of the curve that penalizes variation ⁵ ⁵ 5 The implementation of
this step in the Synchrosqueezing Toolbox is a heuristic (greedy)
approach that maximizes the objective at each time index, assuming the
objective has been maximized for all previous time indices. :

  -- -------- -- -------
     @xmath      (5.8)
  -- -------- -- -------

where @xmath is the normalized energy of @xmath . The user-defined
parameter @xmath determines the “smoothness” of the resulting curve
estimate (we use @xmath ). Its associated component @xmath can be
reconstructed via ( 5.5 ), by restricting the sum over @xmath , at each
@xmath , to the neighborhood @xmath (we use the window size @xmath ).
The next curve is extracted by setting @xmath for all @xmath and
repeating the process above.

### 5.5 Consistency and Stability of Synchrosqueezing

We first review the main theorem on wavelet-based Synchrosqueezing, as
developed in [ 32 ] (Thm. 5.5.1 ). Then we show that the components
extracted via Synchrosqueezing are stable to bounded perturbations such
as noise and discretization error.

We specify a class of functions on which these results hold. In
practice, Synchrosqueezing works on a wider function class.

###### Definition 5.5.1 (Sums of Intrinsic Mode Type (IMT) Functions).

The space @xmath of superpositions of IMT functions, with smoothness
@xmath and separation @xmath , consists of functions having the form
@xmath with @xmath . For @xmath the IF components @xmath are ordered and
relatively well separated (high frequency components are spaced further
apart than low frequency ones):

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Functions in the class @xmath are essentially composed of components
with time-varying amplitudes. Furthermore, the amplitudes vary slowly,
and the individual IFs are sufficiently smooth. For each @xmath ,

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

For the theoretical analysis, we also define the Continuous Wavelet
Synchrosqueezing transform, a smooth version of @xmath .

###### Definition 5.5.2 (Continuous Wavelet Synchrosqueezing).

Let @xmath be a smooth function such that @xmath . The Continuous
Wavelet Synchrosqueezing transform of function @xmath , with accuracy
@xmath and thresholds @xmath and @xmath , is defined by

  -- -------- -- -------
     @xmath      (5.9)
  -- -------- -- -------

where @xmath . We also denote @xmath and @xmath , where the condition
@xmath is replaced by @xmath .

The continuous ( @xmath ) and discrete frequency ( @xmath )
Synchrosqueezing transforms are equivalent for small @xmath and large
@xmath , respectively. The frequency term @xmath in ( 5.9 ) is
equivalent to @xmath in ( 5.4 ), and the integrand term @xmath in ( 5.9
) takes the place of constraining the frequencies to @xmath in ( 5.4 ).
Signal reconstruction and filtering analogues via the continuous
Synchrosqueezing transform thus reduce to integrating @xmath over @xmath
, similar to summing over @xmath in ( 5.5 ).

The following consistency theorem was proved in [ 32 ] :

###### Theorem 5.5.1 (Synchrosqueezing Consistency).

Suppose @xmath . Pick a wavelet @xmath such that its Fourier transform
@xmath is supported in @xmath for some @xmath . Then for sufficiently
small @xmath , Synchrosqueezing can identify and extract the components
@xmath from @xmath :

1 . The Synchrosqueezing plot @xmath is concentrated around the IF
curves @xmath . For each @xmath , define the “scale band” @xmath . For
sufficiently small @xmath , the FM-demodulated frequency estimate @xmath
is accurate inside @xmath where @xmath is sufficiently large ( @xmath ):

@xmath .

Outside the scale bands @xmath , @xmath is small:

@xmath .

2 . Each component @xmath may be reconstructed by integrating @xmath
over a neighborhood around @xmath . Choose the Wavelet threshold @xmath
and let @xmath . For sufficiently small @xmath , there is a constant
@xmath such that for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Note that, as expected, Thm. 5.5.1 implies that components @xmath with
low amplitude may be difficult to identify and extract (as their Wavelet
magnitudes may fall below @xmath ).

Thm. 5.5.1 also applies to discrete Synchrosqueezing, with the following
modifications: letting @xmath is equivalent to letting @xmath . For
reconstruction via ( 5.5 ), the integral over @xmath should be replaced
by a sum over @xmath in the discrete neighborhood @xmath . Finally, the
threshold @xmath in Thm. 5.5.1 part 2 can be applied numerically by
letting @xmath when calculating the discrete support @xmath .

We prove the following theorem in [ 12 ] :

###### Theorem 5.5.2 (Synchrosqueezing stability to small
perturbations).

The statements in Thm. 5.5.1 essentially still hold if @xmath is
corrupted by a small error @xmath , especially for mid-range IFs.

Let @xmath and suppose we have a corresponding @xmath , @xmath , @xmath
, @xmath , and @xmath as given in Thm. 5.5.1 . Furthermore, assume that
@xmath , where @xmath is a bounded perturbation such that @xmath , where
@xmath . For each @xmath define the “maximal frequency range” @xmath
such that @xmath for all @xmath . A mid-range IF is defined as having
@xmath near @xmath .

1 . The Synchrosqueezing plot @xmath is concentrated around the IF
curves @xmath . For sufficiently small @xmath , the FM-demodulated
frequency estimate @xmath is accurate inside @xmath where @xmath is
sufficiently large ( @xmath ):

@xmath ,

where @xmath . Outside the scale bands @xmath , @xmath is small:

@xmath .

2 . Each component @xmath may be reconstructed with accuracy
proportional to the noise magnitude and its maximal frequency range by
integrating @xmath over a neighborhood around @xmath . Choose the
wavelet threshold @xmath and let @xmath , where (as before) @xmath . For
sufficiently small @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath .

Thm. 5.5.2 has two important implications. First, components with
mid-range IF tend to have the best estimates and lowest reconstruction
error under bounded noise. Second, to best identify signal component
@xmath with IF @xmath , from a noisy signal, the threshold @xmath should
be chosen proportional to @xmath , where @xmath is an estimate of the
noise magnitude.

#### 5.5.1 Stability under Spline Interpolation

In many applications, samples of a signal @xmath are only given at
irregular sample points @xmath , and these are spline interpolated to a
function @xmath . Thm. 5.5.2 bounds the error incurred due to this
preprocessing:

###### Corollary 5.5.3.

Let @xmath and let @xmath . Then the error in the estimate of the @xmath
th IF of @xmath is @xmath , and the error in extracting @xmath is @xmath
.

###### Proof.

This follows from Thm. 5.5.2 and the following standard estimate on
cubic spline approximations [ 102 , p. 97] :

@xmath

∎

Thus, we can Synchrosqueeze @xmath instead of @xmath and, as long as the
minimum sampling rate @xmath is high enough, the results will match.
Furthermore, in practice errors are localized in time to areas of low
sampling rate, low component amplitude, and/or high component frequency
(see, e.g., § 5.6 ).

### 5.6 Examples of Synchrosqueezing Properties

We now provide numerical examples of several important properties of
Synchrosqueezing. First, we compare Synchrosqueezing with two common
analysis transforms.

#### 5.6.1 Comparison of Synchrosqueezing to the CWT and STFT

We compare Synchrosqueezing to the Wavelet transform and the Short Time
Fourier Transform (STFT) [ 77 ] . We show its superior precision, in
both time and frequency, at identifying components of sums of
quasi-harmonic signals.

In Fig. 5.2 we focus on a signal @xmath defined on @xmath , that
contains an abrupt transition at @xmath , and time-varying AM and FM
modulation. It is discretized to @xmath points and is composed of the
following components:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

We used the shifted bump wavelet (see § 5.7 ) and @xmath for both the
Wavelet and Synchrosqueezing transforms, and a Hamming window with
length 300 and overlap of length 285 for the STFT. These STFT parameters
focused on optimal precision in frequency, but not in time [ 77 ] . For
@xmath , the harmonic components of @xmath are clearly identified in the
Synchrosqueezing plot @xmath (Fig. 5.2 (d)) and the STFT plot (Fig. 5.2
(b)), though the frequency estimate is more precise in @xmath . The
higher frequency components are better estimated up to the singularity
at @xmath in @xmath , but in the STFT there is mixing at the
singularity. For @xmath , the frequency components are more clearly
visible in @xmath due to the smearing of lower frequencies in the STFT.
The temporal resolution in the STFT is also significantly lower than for
Synchrosqueezing due to the selected parameters. A shorter window in the
STFT will provide higher temporal resolution, but lower frequency
resolution and more smearing between the three components.

#### 5.6.2 Nonuniform Sampling and Splines

We now demonstrate how Synchrosqueezing and extraction work for a more
complicated signal that contains multiple time-varying amplitude and
frequency components, and has been irregularly subsampled. Let

  -- -------- -------- -- --------
     @xmath   @xmath      (5.10)
              @xmath      
              @xmath      
  -- -------- -------- -- --------

and let the sampling times be perturbations of uniformly spaced times
having the form @xmath , where @xmath and @xmath is sampled from the
uniform distribution on @xmath . Here we fix @xmath and @xmath . This
leads to @xmath samples on the interval @xmath . To correct for
nonuniform sampling, we fit a spline through @xmath to get the function
@xmath and discretize on the finer grid @xmath , with @xmath and @xmath
. The resulting vector, @xmath , is a discretization of the original
signal plus a spline error term. Fig. 5.3 (a) shows @xmath for @xmath .

Figs. 5.3 (b-e) show the results of Synchrosqueezing and component
extraction of @xmath , for @xmath . All three components are well
separated in the TF domain. The second component is the most difficult
to reconstruct, as it contains the highest frequency information. Due to
stability (Thm. 5.5.2 and Cor. 5.5.3 ), extraction of components with
mid-range IFs is more stable to the error @xmath . Fig. 5.3 shows that
reconstruction errors are time localized to the locations of errors in
@xmath .

#### 5.6.3 White Noise and Reconstruction

We take the signal @xmath of ( 5.10 ), now regularly sampled on the fine
grid with @xmath ( @xmath samples) as before, and corrupt it with white
Gaussian noise having a standard deviation of @xmath . This signal,
@xmath (see Fig. 5.4 (a)) has an SNR of @xmath dB.

Figs. 5.4 (b-e) show the results of Synchrosqueezing and component
extraction of @xmath , for @xmath . As seen in Fig. 5.4 (b), most of the
additional energy, caused by the white noise, appears in the higher
frequencies. Again, all three components are well separated in the TF
domain, though now the third, lower-amplitude, component experiences a
“split” at @xmath . Reconstruction of signal components is less reliable
in locations of high frequencies and low magnitudes (note the axis in
Fig. 5.4 (e) is half that of the others). This again numerically
confirms Thm. 5.5.2 : components with mid-range IFs and higher
amplitudes are more stable to the noise.

### 5.7 Invariance to the underlying transform

As mentioned in § 5.3 and in [ 32 ] , Synchrosqueezing is invariant to
the underlying choice of transform. The only differences one sees in
practice are due to two factors: the time compactness of the underlying
analysis atom (e.g. mother wavelet), and the frequency compactness of
this atom. That is, @xmath should fall off quickly away from zero,
@xmath is ideally zero for @xmath , and @xmath (of Thm. 5.5.1 ) is
small.

Fig. 5.5 shows the effect of Synchrosqueezing the discretized spline
signal @xmath of the synthetic nonuniform sampling example in § 5.6.2 ,
using three different complex CWT mother wavelets. These wavelets are:

  -- -------- --
              
     @xmath   
              
     @xmath   
              
     @xmath   
     @xmath   
  -- -------- --

where for @xmath we use @xmath , for @xmath we use @xmath , for and for
@xmath we use @xmath and @xmath .

The Wavelet representations of @xmath differ due to differing mother
wavelets, but the Synchrosqueezing representation is mostly invariant to
these differences. As expected from Thm. 5.5.1 , more accurate
representations are given by wavelets having compact frequency support
on @xmath away from @xmath .

### 5.8 Conclusions and Future Work

Synchrosqueezing can be used to extract the instantaneous spectra of,
and filter, a wide variety of signals that include complex simulation
data (e.g. dynamical models), and physical signals (e.g. climate
proxies). A careful implementation runs in @xmath time, and is stable
(in theory and in practice) to errors in these types of signals.

Areas in which Synchrosqueezing has shown itself to be an important
analysis tool include ECG analysis (respiration and T-end detection),
meteorology and oceanography (large-scale teleconnection and
ocean-atmosphere interaction), and climatology. Some of these examples
are described in the next chapter.

Additional future work includes theoretical analysis of the
Synchrosqueezing transform, including the development of
Synchrosqueezing algorithms that directly support nonuniform sampling,
the analysis of Synchrosqueezing when the signal is perturbed by
Gaussian, as opposed to bounded, noise, and extensions to higher
dimensional data.

## Chapter 6 Synchrosqueezing: Applications111Section 6.2 of this
chapter are based on work in collaboration with Hau-Tieng Wu and
Gaurav Thakur. Section 6.3 is based on work in collaboration with
Neven S. Fučkar, International Pacific Research Center, University of
Hawaii, as submitted in [12].

### 6.1 Introduction

The theoretical results of Chapter 5 provide important guarantees and
guidelines for the use of Synchrosqueezing in data analysis techniques.
Here, we focus on two specific applications in which Synchrosqueezing,
in combination with preprocessing methods such as spline interpolation,
provides powerful new analysis tools.

This chapter is broken down into two sections. First, we use
Synchrosqueezing and spline interpolation to estimate patients’
respiration from the R-peaks (beats) in their Electrocardiogram (ECG)
signals. This extends earlier work on the ECG-Derived Respiration
problem.

Second, we visit open problems in paleoclimate studies of the last
2.5 Myr, where Synchrosqueezing provides improved insights. We compare a
calculated solar flux index with a deposited @xmath paleoclimate proxy
over this period. Synchrosqueezing cleanly delineates the orbital cycles
of the solar radiation, provides an interpretable representation of the
orbital signals of @xmath , and improves our understanding of the effect
that the solar flux distribution has had on the global climate. Compared
to previous analyses of these data, the Synchrosqueezing representation
provides more robust and precise estimates in the time-frequency plane.

### 6.2 ECG Analysis: Respiration Estimation

We first demonstrate how Synchrosqueezing can be combined with
nonuniform subsampling of a single lead ECG recording to estimate the
instantaneous frequency of, and in some cases extract, a patients’s
respiration signal. We verify the accuracy of our estimates by comparing
them with the instantaneous frequency (IF) extracted from a
simultaneously recorded respiration signal.

The respiratory signal is usually recorded mechanically via, e.g.,
spirometry or plethysmography. There are two common disadvantages to
these techniques. First, they require the use of complicated devices
that might interfere with natural breathing. Second, they are not
appropriate in many situations, such as ambulatory monitoring. However,
having the respiratory signal is often important, e.g. for the diagnosis
of obstructive sleep apnea. Thus, finding a convenient way to directly
record, or indirectly estimate, information about the respiration signal
is important from a clinical perspective.

ECG is a cheap, non-invasive, and ubiquitous technique, in which voltage
differences are passively measured between electrodes (leads) connected
to a patient’s body (usually the chest and arms). The change of the
thoracic electrical impedance caused by inhalation and exhalation, and
thus physiological respiratory information, is reflected in the ECG
amplitude. The respiration-induced distortion of ECG was first studied
in [ 36 ] and [ 39 ] . A well-known ECG-Derived Respiration (EDR)
technique [ 73 ] experimentally showed that “electrical rotation” during
the respiratory cycle is the main contributor to the distortion of ECG
amplitude, and that the contribution of thoracic impedance variations is
relatively minor. These prior work confirm that analyzing ECG may enable
us to estimate respiration. More details about EDR are available in [ 66
] .

Relying on the coupling between physiological respiration and R-peak
amplitudes (the tall spikes in Fig. 6.1 (a)), we use the R-peaks as a
proxy for the respiration signal. More specifically, we hypothesize that
the R peaks, taken as samples of the envelope of the ECG signal @xmath ,
have the same IF profile as the true respiration signal @xmath . By
sampling @xmath at the R peaks and performing spline interpolation on
the resulting samples, we hope to see a time shifted, amplitude scaled,
version of @xmath near the respiratory frequency (0.25Hz).

In Fig. 6.1 , we show the lead II ECG signal and the true respiration
signal (via respiration belt) of a healthy @xmath year old male,
recorded over a @xmath minute interval ( @xmath sec). The sampling rates
of the ECG and respiration signals are respectively 400Hz and 50Hz
within this interval. There are @xmath R peaks appearing at nonuniform
times @xmath , @xmath . We run cubic spline interpolation on the R-peaks
@xmath to get @xmath , which we discretize at 50Hz (with @xmath ) to get
@xmath . Fig. 6.2 shows the result of running Synchrosqueezing on @xmath
and @xmath . The computed IF, @xmath , turns out to be a good (shifted
and scaled) approximation to the IF of the true respiration, @xmath . It
can be seen, from Figs. 6.1 and 6.2 , that the spacing of respiration
cycles in @xmath is reflected by the main IF of @xmath : closer spacing
corresponds to higher IF values, and wider spacing to lower values.

These results were confirmed by tests on several subjects. Thanks to the
stability of Synchrosqueezing (Thm. 5.5.2 and Cor. 5.5.3 ), this
algorithm has the potential for broader clinical usage.

#### 6.2.1 Notes on Data Collection and Analysis Parameters

The ECG signal @xmath was collected at 400Hz via a MSI MyECG E3-80. The
respiration signal @xmath was collected at 50Hz via a respiration belt
and PASCO SW750. The ECG signal was filtered to remove the worst
nonstationary noise by thresholding signal values below the @xmath and
above the @xmath quantiles, to these quantile values. The ECG R-peaks
were then extracted from @xmath by first running the physionet ecpguwave
³ ³ 3 ecgpuwave may be found at:
http://www.physionet.org/physiotools/ecgpuwave/ program, followed by a
“maximum” peak search within a 0.2 sec window of each of the
ecgpuwave-estimated R-peaks.

For Synchrosqueezing, the parameters @xmath and @xmath were used for
thresholding @xmath and extracting contours from both the R-peak spline
and respiratory signals.

### 6.3 Paleoclimatology: Aspects of the mid-Pleistocene transition

Next, we apply Synchrosqueezing to analyze the characteristics of a
calculated index of the incoming solar radiation (insolation) and to
measurements of repeated transitions between glacial (cold) and
interglacial (warm) climates during the Pleistocene epoch: @xmath
1.8 Myr to 12 kyr before the present.

The Earth’s climate is a complex, multi-component nonlinear, system with
significant stochastic elements [ 79 ] . The key external forcing field
is the insolation at the top of the atmosphere (TOA). Local insolation
has predominately harmonic characteristics in time (diurnal cycle,
annual cycle and Milanković orbital cycles). However, response of
planetary climate, which varies at all time scales [ 51 ] , also depends
on random perturbations (e.g., volcanism), solid boundary conditions
(e.g., plate tectonics and global ice distribution), internal
variability and feedbacks (e.g., global carbon cycle). Various
paleoclimate records or proxies provide us with information about past
climates beyond observational records. Proxies are biogeochemical
tracers, i.e., molecular or isotopic properties, imprinted into various
types of deposits (e.g., deep-sea sediment), and they indirectly
represent physical conditions (e.g., temperature) at the time of
deposition. We focus on climate variability during the last 2.5 Myr
(that also includes the late Pliocene) as recorded by @xmath in
foraminiferal shells at the bottom of the ocean (benthic forams).
Benthic @xmath is the deviation of the ratio of @xmath to @xmath in sea
water with respect to the present-day standard, as imprinted in benthic
forams during their growth. It increases with glaciation during cold
climates because @xmath evaporates more readily and accumulates in ice
sheets. Thus, benthic @xmath can be interpreted as a proxy for either
high-latitude temperature or global ice volume.

We first examine a calculated element of the TOA solar forcing field.
Fig. 6.3 (a) shows @xmath , the mid-June insolation at @xmath at 1 kyr
intervals [ 9 ] . This TOA forcing index does not encompass the full
complexity of solar radiation structure and variability, but is commonly
used to gain insight into the timing of advances and retreats of ice
sheets in the Northern Hemisphere in this period (e.g., [ 46 ] ). The
Wavelet and Synchrosqueezing decompositions in Fig. 6.4 (a) and Fig. 6.5
(a), respectively, show the key harmonic components of @xmath . The
application of a shifted bump mother wavelet (see § 5.7 ) yields an
upward shift of the spectral features along the scale axis in each of
the the representations in Fig. 6.4 . Therefore the scale @xmath should
not be used to directly infer periodicities. In contrast, the
Synchrosqueezing spectrums in Fig. 6.5 explicitly present time-frequency
(or here specifically time-periodicity) decompositions with a sharper
structure, and are not affected by the scale shift inherent in the
choice of mother wavelet.

Fig. 6.5 (a) clearly shows the presence of strong precession cycles (at
periodicities @xmath =19 kyr and 23 kyr), obliquity cycles (primary at
41 kyr and secondary at 54 kyr), and very weak eccentricity cycles
(primary periodicities at 95 kyr and 124 kyr, and secondary at 400 kyr).
This is in contrast with Fig. 6.4 (a), which contains blurred and
shifted spectral structures only qualitatively similar to Fig. 6.5 (a).

We next analyze the climate response during the last 2.5 Myr as
deposited in benthic @xmath in long sediments cores. (in which deeper
layers contain forams settled further back in time). Fig. 6.3 (b) shows
@xmath : benthic @xmath , sampled at irregular time intervals from a
single core, DSDP Site 607, in the North Atlantic [ 84 ] . This signal
was spline interpolated to 1 kyr intervals prior to the spectral
analyses. Fig. 6.3 (c) shows @xmath : the benthic @xmath stack (H07)
calculated at 1 kyr intervals from fourteen cores (most of them from the
Northern Hemisphere, including DSDP607) using the extended depth-derived
age model [ 52 ] . Prior to combining the cores in the H07 stack, the
record mean between 0.7 Myr ago and the present was subtracted from each
@xmath record; this is the cause of the differing vertical ranges in
Figs. 6.3 (b-c). Noise due to local climate characteristics and
measurement errors of each core is reduced when we shift the spectral
analysis from DSDP607 to the stack; and this is particularly visible in
the finer scales and higher frequencies.

The Synchrosqueezing decomposition in Fig. 6.5 (c) is a more precise
time-frequency representation of the stack than a careful STFT analysis
[ 52 , Fig. 4] . In addition, it shows far less stochasticity above the
obliquity band as compared to Fig. 6.5 (b), enabling the 23 kyr
precession cycle to become mostly coherent over the last 1 Myr. Thanks
to the stability of Synchrosqueezing, the spectral differences below the
obliquity band are less pronounced between Fig. 6.5 (b) and Fig. 6.5
(c). Overall, the stack reveals sharper time-periodicity evolution of
the climate system than DSDP607 or any other single core possibly could.
The Wavelet representations in Figs. 6.4 (b-c) also show this
suppression of noise in the stack (in more diffuse and scale shifted
patterns). Figs. 6.6 (a) through 6.6 (c) show that the time average of
Synchrosqueezing magnitudes (normalized by @xmath ) is directly
comparable with the Fourier spectrum, but delineates the harmonic
components much more clearly (not shown).

During the last 2.5 Myr, the Earth experienced a gradual decrease in
global background temperature and @xmath concentration, and an increase
in mean global ice volume accompanied with glacial-interglacial
oscillations that have intensified towards the present (this is evident
in Fig. 6.3 (b) and 6.3 (c)). The mid-Pleistocene transition, occurring
gradually or abruptly sometimes between 1.2 Myr and 0.6 Myr ago, was the
shift from 41 kyr-dominated glacial cycles to 100 kyr-dominated glacial
cycles recorded in deep-sea proxies (e.g., [ 85 , 22 , 80 ] ). The
origin of this strong 100 kyr cycle in the late-Pleistocene climate and
the prior incoherency of the precession band are still unresolved
questions. Both types of spectral analyses of selected @xmath records
indicate that the climate system does not respond linearly to external
periodic forcing.

Synchrosqueezing enables the detailed time-frequency decomposition of a
noisy, nonstationary, climate time series due to stability (Thm. 5.5.2 )
and more precisely reveals key modulated signals that rise above the
stochastic background. The gain (the ratio of the climate response
amplitude to insolation forcing amplitude) at a given frequency or
period, is not constant. The response to the 41 kyr obliquity cycle is
present almost throughout the entire Pleistocene in Fig. 6.5 (c). The
temporary incoherency of the 41 kyr component starting about 1.25 Myr
ago roughly coincides with the initiation of a lower frequency signal (
@xmath 70 kyr) that evolves into a strong 100 kyr component in the late
Pleistocene (about 0.6 Myr ago). Inversion (e.g., spectral integration)
of the Synchrosqueezing decomposition of @xmath and @xmath across the
key orbital frequency bands in Fig. 6.7 again emphasize the nonlinear
relation between insolation and climate evolution. Specifically, in Fig.
6.7 (a) the amplitude of the filtered precession signal of @xmath
abruptly rises 1 Myr ago, while in Fig. 6.7 (c) the amplitude of the
eccentricity signal shows a gradual increase.

Synchrosqueezing analysis of the solar insolation index and benthic
@xmath makes a significant contribution in three important ways. First,
it produces spectrally sharp traces of complex system evolution through
the high-dimensional climate state space (compare with, e.g., [ 22 ,
Fig. 2] ). Second, it delineates the effects of noise on specific
frequency ranges when comparing a single core to the stack. Low
frequency components are mostly robust to noise induced by both local
climate variability and the measurement process. Third, thanks to its
precision, Synchrosqueezing allows the filtered reconstruction of signal
components within frequency bands.

Questions about the key physical processes governing large scale climate
variability over the last 2.5 Myr can be answered with sufficient
accuracy only by precise data analysis and the development of a
hierarchy of models at various levels of complexity that reproduce the
key aspects of Pleistocene history. The resulting dynamic stochastic
understanding of past climates may benefit our ability to predict future
climates.

## Chapter 7 Multiscale Dictionaries of Slepian Functions on the
Sphere111This chapter is based on ongoing work in collaboration with
Frederik J. Simons, Department of Geosciences, Princeton University.

### 7.1 Introduction

The estimation and reconstruction of signals from their samples on a
(possibly irregular) grid is an old and important problem in engineering
and the natural sciences. Over the last several centuries, both
approximation and sampling techniques have been developed to address
this problem. Approximation theorems provide (possibly probabilistic)
guarantees that a function can be approximated to a specified precision
with a bounded number of coefficients in an alternate basis or frame. In
general, such guarantees put constraints on the function (e.g.,
differentiability) and the domain (e.g., smoothness and compactness).
Sampling theorems guarantee that a function can be reconstructed to a
given precision from either point samples or some other form of sampling
technique (e.g., linear combinations of point observations, as in the
case of compressive sensing). Again, sampling theorems place
requirements on both the sampling (e.g., grid uniformity or a minimum
sampling rate), on the original function (e.g., a bandlimit), and/or on
the domain (e.g., smoothness, compactness).

Approximation and sampling techniques are closely linked due to their
similar goals. For example, a signal can be estimated via its
representation in an alternate basis (e.g., via Riemannian sums that
numerically calculate projections of point samples onto the basis
functions). The estimate then follows by expanding the function in the
given basis. Regularization (the approximation) of the estimate can be
performed by excluding the basis elements assumed to be zero.

In this chapter, we focus on the approximation and sampling problem for
subsets of the sphere @xmath . First, we are interested in the
representation of signals that are bandlimited ³ ³ 3 In the sense that
they have compact support in the spherical harmonic basis (see App. B ).
but whose contribution of higher frequencies arises from within a
certain region of interest (ROI), denoted @xmath (see, e.g., Fig. 7.1 ).
Second, we are interested in reconstructing such functions from point
samples within the ROI. To this end, we construct multiscale
dictionaries of functions that are bandlimited on the sphere @xmath and
space-concentrated in contiguous regions @xmath .

Our constructions are purely numerical, and are motivated by subdivision
schemes for wavelet constructions on the interval. By construction, the
functions have low coherency (their pairwise inner products are bounded
in absolute value). As a result, thanks to new methods in sparse
approximation (see, e.g., [ 15 , 44 ] ), they are good candidates for
the approximation and reconstruction of signals that are locally
bandlimited.

### 7.2 Notation and Prior Work

We will focus on the important prior work in signal representation,
working our way up to Slepian functions on the sphere — the functions
upon which our construction is based.

Before proceeding, we first introduce some notation. For two sequences
(vectors) @xmath and a subset of the natural numbers @xmath , we refer
to the product @xmath . When @xmath is omitted, we assume the sum is
over all indices. The norm of @xmath is denoted @xmath . For
square-integrable functions @xmath and @xmath on a Riemannian manifold
@xmath (that is, @xmath ), and @xmath some subset of @xmath , we denote
the inner product @xmath , where @xmath is the volume element associated
with the metric @xmath (see Apps. A – B ). The norm is again defined as
@xmath . The type of inner product, manifold, and metric will be clear
from the context. Notation referring specifically to functions on the
sphere @xmath and the spherical harmonics may be found in § B.2.2 .

The prior literature in this area pertains to sampling, interpolation,
and basis functions on the real line, and we focus on these next.

#### 7.2.1 Reconstruction of Bandlimited, Regularly Sampled Signals on
@xmath

A signal @xmath on the real line is defined as bandlimited when it has
no frequencies higher than some bandlimit @xmath . That is, @xmath for
@xmath . The simplest version of the sampling theorem, as given by
Shannon [ 89 , §II] , states that if @xmath is sampled at regular
intervals of with a frequency at or above the sampling limit @xmath , it
can be exactly reconstructed via convolution with the sinc low-pass
filter:

  -- -------- -- -------
     @xmath      (7.1)
  -- -------- -- -------

where @xmath are the samples and @xmath .

Shannon also heuristically describes [ 89 , §III] that if a bandlimited
function @xmath , with bandlimit @xmath , is also timelimited to an
interval @xmath (that is, all of its samples, taken at rate @xmath , are
exactly 0 outside of this interval), then it requires @xmath samples on
this interval to reconstruct. Thus a function that is both time- and
bandlimited as described above can be described using only @xmath
numbers—and the dimension of such functions is @xmath , which is called
the Shannon number.

Shannon’s heuristic definition was based on the (at that time)
well-known fact that a bandlimited, substantially spacelimited ⁴ ⁴ 4
From Shannon’s paper, we assume “substantially” implies
space-concentrated. function can be represented well with @xmath
numbers. In fact, it is impossible to construct exactly space- and
frequency- limited functions on the line; see, e.g., the Paley-Wiener
theorem ⁵ ⁵ 5 This theorem states that the Fourier transform of a
compact function is entire. The only entire function with an
accumulation point of zeros (e.g., a compactly supported one) is the
zero function. [ 86 , Thm. 7.22] . Thus a different technique is needed
for the estimation of bandlimited functions on an interval, from samples
only within that interval. As we will describe next the optimal
representation for this @xmath -dimensional space is given by the
ordered basis of Slepian functions. We will now focus on the
construction of Slepian functions, and will provide a more rigorous
definition of the Shannon number for the space of bandlimited,
space-concentrated functions.

#### 7.2.2 An Optimal Basis for Bandlimited Functions on the Interval

The estimation of bandlimited signals on an interval requires the
construction of an optimal basis to represent such functions. The
Prolate Spheroidal Wave Functions (PSWF) [ 61 , 62 , 98 ] , also known
as Slepian functions, are one such basis. The criterion of optimal
concentration is with respect to the ratio of @xmath norms. Let @xmath
be the interval on the line, and @xmath be the bandlimit in frequency.
The PSWF are the orthogonal set of solutions to the variational problem

  -- -------- -------- -- -------
     @xmath   @xmath      (7.2)
              @xmath      
  -- -------- -------- -- -------

where for @xmath , the value @xmath is achieved by @xmath , and we
impose the orthonormality constraint @xmath . Here @xmath is the measure
of concentration of @xmath on @xmath ; these eigenvalues are bounded:
@xmath . We use the standard ordering of the Slepian functions, wherein
@xmath (the first Slepian function is the most concentrated within
@xmath , the second is the second most concentrated, and so on).

The orthonormal solutions to ( 7.2 ) satisfy the integral eigenvalue
problem [ 98 , 92 ] :

  -- -------- -------- -- -------
     @xmath   @xmath      (7.3)
              @xmath      
  -- -------- -------- -- -------

Note that @xmath is a smooth, symmetric, positive-definite kernel with
eigenfunctions @xmath (and associated eigenvalues @xmath ). Mercer’s
theorem therefore applies [ 82 , §97,98] , and we can write

  -- -------- -- -------
     @xmath      (7.4)
  -- -------- -- -------

In practice, ( 7.3 ) can be solved exactly and efficiently using a
special “trick”: this integral equation commutes with a special
second-order differential operator and thus its solution can be found
via the solution of a PDE of Sturm-Liouville type. The values of the
@xmath ’s, or of the @xmath ’s, can be evaluated exactly at any set of
points on their domains, via the factorization of a special tridiagonal
matrix [ 91 , 99 ] . This construction is beyond the scope of this
chapter.

It has been shown that the Slepian functions are all either very well
concentrated within the interval, or very well concentrated outside of
it. That is, the eigenvalues of the Slepian functions are all either
nearly @xmath or nearly @xmath [ 98 , Table 1] . Furthermore the values
of any bandlimited signal with concentration @xmath on the interval, can
be estimated to within a squared error (in @xmath ) bounded by @xmath ,
for any arbitrarily @xmath , by approximating this signal using a linear
combination of all the Slepian functions whose eigenvalues are near
unity [ 62 , Thm. 3] , and no fewer [ 62 , Thm. 5] .

The number of Slepian functions required to approximate a bandlimited,
space-concentrated function can be therefore be calculated by summing
their energies:

  -- -------- -- -------
     @xmath      (7.5)
  -- -------- -- -------

where we used ( 7.4 ) after swapping the sum and integral.

The difference between this value of @xmath and Shannon’s version is due
to changes in normalization of Fourier transforms and the constant
@xmath inside the @xmath in ( 7.1 ).

#### 7.2.3 An Optimal Basis for Bandlimited Functions on subregions of
the Sphere @xmath

The construction of Slepian functions on the sphere proceeds similarly
to the interval case, with differences due to the compactness of @xmath
. Let @xmath be a closed and connected subset of @xmath and let the
frequency bandlimit be @xmath . The set of square-integrable functions
on @xmath with bandlimit @xmath , which we will call @xmath , is an
@xmath dimensional space. This follows because any @xmath can be written
as ⁶ ⁶ 6 The spherical harmonics @xmath and their properties are given
in App. B .

  -- -------- -- -------
     @xmath      (7.6)
  -- -------- -- -------

and therefore

  -- -------- --
     @xmath   
  -- -------- --

For the rest of the chapter, we will refer to the bandlimits “ @xmath ”
and “ @xmath ” interchangeably.

We now proceed as in [ 92 ] . Slepian functions concentrated on @xmath
with bandlimit @xmath are the orthogonal set of solutions to the
variational problem

  -- -------- -------- -- -------
     @xmath   @xmath      (7.7)
              @xmath      
  -- -------- -------- -- -------

where the value @xmath is achieved by @xmath , @xmath , and we impose
the orthonormality constraint

  -- -------- -- -------
     @xmath      (7.8)
  -- -------- -- -------

Finally, as before, we use the standard order for the Slepian functions:
@xmath (i.e., in decreasing concentration). Note that in contrast to the
1D case, the concentration inequalities are not strict due to possible
geometric degeneracy. Due to orthonormality, the Slepian functions also
fulfill the orthogonality constraint

  -- -------- -- -------
     @xmath      (7.9)
  -- -------- -- -------

The problem ( 7.7 ) admits an integral formulation equivalent to ( 7.3
). In addition, as the Fourier basis on @xmath is countable, the
construction can be reduced to a matrix eigenvalue problem.

Writing @xmath in ( 7.7 ) via its Fourier series, as in ( 7.6 ), reduces
the problem to [ 91 , Eq. 33]

  -- -------- -------- -- --------
     @xmath   @xmath      (7.10)
              @xmath      
  -- -------- -------- -- --------

From now on, we will denote by @xmath the @xmath matrix with
coefficients @xmath . The vector @xmath is thus an @xmath -element
vector, indexed by coefficients @xmath , with @xmath for any @xmath .
The matrix @xmath is called the spectral localization kernel; it is
real, symmetric and positive-definite. We can now rewrite ( 7.7 ) as a
proper eigenvalue problem: we solve

  -- -------- -- --------
     @xmath      (7.11)
  -- -------- -- --------

where @xmath is the orthonormal matrix of eigenvectors and the diagonal
matrix @xmath is composed of eigenvalues in decreasing order: @xmath .
As @xmath is positive-definite and symmetric, its eigenvectors, @xmath ,
form an orthogonal set that spans the space @xmath . The spatial
functions can be easily calculated via ( 7.6 ) and efficient recursion
formulas for the spherical harmonics.

The Shannon number, @xmath is again defined as the sum of the
eigenvalues,

  -- -------- -- --------
     @xmath      (7.12)
  -- -------- -- --------

It can also be shown (see, e.g., [ 92 , §4.3] ) that the Shannon number
is given by a formula similar to ( 7.5 ):

  -- -- -- --------
           (7.13)
  -- -- -- --------

where @xmath is the area of @xmath .

#### 7.2.4 Calculation of the Spectral Localization Kernel @xmath

In contrast to the 1D construction of § 7.2.2 , and with the exception
of the cases when the domain @xmath has azimuthal and/or equatorial
symmetry (see, e.g., [ 92 ] ), there is no known differential operator
that commutes with the spatial integral version of ( 7.10 ).
Nevertheless, thanks to the discrete nature of the problem, we can find
tractable solutions using simple numerical analysis.

We now briefly discuss the calculation of the symmetric
positive-definite matrix @xmath of ( 7.10 ); basing the discussion on
the work in [ 90 , §4.2] . As the calculation of the largest eigenvalues
and associated eigenvectors of @xmath can be performed using standard
efficient iterative solvers, the main computational complexity lies in
constructing the matrix itself.

The problem of calculating @xmath reduces to numerically estimating the
constrained spatial inner product between spherical harmonics,

  -- -------- -- --------
     @xmath      (7.14)
  -- -------- -- --------

when we are given the (splined) boundary @xmath (a closed simple curve
in @xmath ). This is performed via a semi-analytic integration over a
grid. We first find the northernmost and southernmost colatitudes,
@xmath and @xmath , of @xmath . For a given colatitude @xmath , we can
find the westernmost and easternmost points, @xmath and @xmath of @xmath
. If @xmath is nonconvex, there will be some @xmath number of such
points, which we denote @xmath and @xmath . The integral ( 7.14 ) thus
becomes

  -- -------- -------- -- --------
     @xmath   @xmath      (7.15)
     @xmath   @xmath      (7.16)
     @xmath   @xmath      
     @xmath   @xmath      
  -- -------- -------- -- --------

Above, @xmath is the colatitudinal portion of @xmath ; for more details,
see App. B .

Equation ( 7.15 ) is calculated via Gauss-Legendre integration using the
Nyström method, first by discretizing the colatitudinal integral into
@xmath points @xmath , and then evaluating the integral ( 7.16 )
analytically at each point @xmath . The discretization number @xmath for
the numerical integration is chosen large enough that the spatial-domain
eigenfunctions, as calculated via the diagonalization of @xmath and
application of ( 7.6 ), satisfy the Slepian orthogonality relations (
7.8 ) and ( 7.9 ) to within machine precision.

A second way involves the expansion of @xmath into spherical harmonics —
the expansion coefficients are the quantum-mechanical Wigner @xmath
functions, which can be calculated recursively. The remaining integral
over a single spherical harmonic can be performed recursively in the
manner of [ 78 ] , which is exact. See also [ 37 ] .

### 7.3 Multiscale Trees of Slepian Functions

We now turn our focus to numerically constructing a dictionary @xmath of
functions that can be used to approximate mostly low bandwidth signals
on the sphere. As we will see in the next section, this dictionary
allows for the reconstruction of a variety of signals from their point
samples.

To construct @xmath , we first need some definitions. Let @xmath be a
simply connected subset of the sphere. Let @xmath be the bandwidth: the
dictionary @xmath will be composed of functions bandlimited to harmonic
degrees @xmath . The construction is based on a binary tree. Choose a
positive integer (the node capacity) @xmath ; each node of the tree
corresponds to the first @xmath Slepian functions with bandlimit @xmath
and concentrated on a subset @xmath . The top node of the tree
corresponds to the entire region @xmath , and each node’s children
correspond to a division of @xmath into two roughly equally sized
subregions (the subdivision scheme will be described soon). As the child
nodes will be concentrated in disjoint subsets of @xmath , all of their
corresponding functions and children are effectively incoherent.

We now fix a height @xmath of the tree: the number of times to subdivide
@xmath . The height is determined as the maximum number of binary
subdivisions of @xmath that can have @xmath well concentrated functions.
That is, we find the minimum integer @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

with the solution

  -- -------- --
     @xmath   
  -- -------- --

A complete binary tree with height @xmath has @xmath nodes, so from now
on we will denote the dictionary

  -- -------- --
     @xmath   
  -- -------- --

as the set of @xmath functions thus constructed on region @xmath with
bandlimit @xmath and node capacity @xmath . Fig. 7.2 shows the tree
diagram of the subdivision scheme. We use the standard enumeration of
nodes wherein node @xmath is subdivided into child nodes @xmath and
@xmath , and at a level @xmath , the nodes are indexed from @xmath .
More specifically, for @xmath , we have @xmath . Furthermore, letting
@xmath be the @xmath ’th Slepian function on @xmath (the solution to (
7.7 ) with concentration region @xmath ), we have that

  -- -------- --
     @xmath   
  -- -------- --

Fig. 7.3 shows an example of the construction when @xmath is the African
continent. Note how, for example, @xmath and @xmath are the first
Slepian functions associated with the subdivided domains of @xmath .

To complete the top-down construction, it remains to decide how to
subdivide a region @xmath into equally sized subregions. For roughly
circular connected domains, the first Slepian function has no sign
changes, and the second Slepian function has a single zero-level curve
that subdivides the region into approximately equal areas; when @xmath
is a spherical cap, the subdivision is exact [ 92 ] . We thus subdivide
a region @xmath into the two nodal domains associated with the second
Slepian function on that domain; see Fig. 7.4 for a visualization of the
subdivision scheme as applied to the African continent.

### 7.4 Concentration, Range, and Incoherence

The utility of the Tree construction presented above depends on its
ability to represent bandlimited functions in a region @xmath , and its
efficacy at reconstructing functions from point samples in @xmath .
These properties, in turn, reduce to questions of concentration, range,
and incoherence:

-   Dictionary @xmath is concentrated in @xmath if its functions are
    concentrated in @xmath .

-   The range of dictionary @xmath is the subspace spanned by its
    elements. Ideally, the basis formed by the first @xmath Slepian
    functions on @xmath is a subspace of the range of @xmath .

-   When @xmath is incoherent, pairwise inner products of its elements
    have low amplitude: pairs of functions are approximately orthogonal.
    This, in turn, is a useful property when using @xmath to estimate
    signals from point samples, as we will see in the next section.

In this section, we provide several techniques for analyzing these
properties for a given dictionary @xmath , providing numerical examples
as we go along.

Unlike the eigenvalues of the Slepian functions on @xmath , not all of
the eigenvalues of the elements of @xmath reflect their concentration
within this top-level (parent) region. We thus define the modified
concentration value

  -- -------- -- --------
     @xmath      (7.17)
  -- -------- -- --------

Recalling that @xmath , the value @xmath is simply the percentage of
energy of the @xmath element that is concentrated in @xmath . This value
is always larger than the element’s eigenvalue, which relates its
fractional energy within the smaller subset @xmath . Figs. 7.5 , 7.6
, and 7.7 compare the eigenvalues of the Slepian functions on the
African continent with those of the Tree construction, as well as with
numerically calculated ⁷ ⁷ 7 Calculations performed using gridded
Gauss-Legendre integration similar to that in § 7.2.4 . values of @xmath
.

The size of dictionary @xmath is generally larger than the Shannon
number @xmath for any node capacity @xmath , and as a result it cannot
form a proper basis (it has too many functions). Ideally, then, we
require that elements of the range of the dictionary spans the space of
the first @xmath Slepian functions. We discuss two visual approaches for
determining if this is the case.

Though the spatial nature of the construction makes it clear that
dictionary elements tend to cover the entire domain @xmath , we also
investigate the spectral energies of these elements; and compare them
with the energies of the @xmath Slepian functions on @xmath , which
“essentially” form a basis for bandlimited functions in @xmath . The
spectral energy density of a function @xmath is given for each degree
@xmath by [ 28 , Eq. 38] :

  -- -------- --
     @xmath   
  -- -------- --

Figs. 7.8 and 7.9 compare the power spectra of the Slepian functions on
the African continent with the power spectra of two dictionaries given
by the tree construction. While the Slepian functions are concentrated
within specific ranges of the harmonics, the tree construction leads to
spectra that depend on the degree. The dictionary elements with @xmath
tend to either contain mainly low-frequency harmonics or, for elements
concentrated on smaller regions, have a more flat harmonic response
within the bandlimit. Dictionary elements associated with higher order
Slepian functions have a more pass-band response when concentrated on
larger regions and a more flat response within the higher frequencies of
the bandlimit when concentrated on smaller regions. So while it is clear
that the Slepian functions span the bandlimited frequencies, the
spectral amplitude plots do not relay this as clearly for the tree
construction.

A complementary answer to the question of the range of @xmath is given
by studying the angle between the subspaces spanned by elements of
@xmath and the first @xmath functions of the Slepian basis, for @xmath [
111 ] . The angle between two subspaces @xmath and @xmath of @xmath
(having possibly different dimensions), is given by the formula

  -- -------- -------- -- --------
     @xmath   @xmath      (7.18)
     @xmath               (7.19)
  -- -------- -------- -- --------

Here, @xmath is the orthogonal projection operator onto space @xmath and
all of the norms are with respect to the given subspace. The angle
@xmath is symmetric, nonnegative, and zero iff @xmath or @xmath ;
furthermore it is invariant under unitary transforms applied to both on
@xmath and @xmath (such as Fourier synthesis), and admits a triangle
inequality. It is thus a good indicator of distance between two
subspaces; furthermore, it can be calculated accurately ⁸ ⁸ 8 See MATLAB
function subspace . given two matrices whose columns span @xmath and
@xmath . We can therefore identify the matrices @xmath and @xmath with
the subspaces spanned by their columns.

Let @xmath denote the matrix containing the first @xmath column vectors
of @xmath from ( 7.11 ). Further, let @xmath denote the @xmath matrix
containing the spherical harmonic representations of the elements of
@xmath . Fig. 7.10 shows @xmath for @xmath with @xmath . The Shannon
number is @xmath (see Fig. 7.5 and ( 7.12 )). From this figure, it is
clear that while the dictionaries @xmath and @xmath do not strictly span
the space of functions bandlimited to @xmath and optimally concentrated
in Africa, they are a close approximation: the column span of @xmath is
nearly linearly dependent with the spans of @xmath and @xmath , for
@xmath significantly larger than @xmath .

Thanks to novel approaches to signal approximation, which we will
discuss in the next section, the requirement that the dictionary
elements form an orthogonal basis is less important than the property of
mutual incoherence. Mutual incoherence in a dictionary means that the
inner product (the angle, when elements are of unit norm) between pairs
of elements is almost always very low. Figs. 7.11 and 7.12 numerically
show that the two tree constructions on continental Africa have good
incoherency properties: most dictionary element pairs are nearly
orthogonal.

In Fig. 7.11 (a), most pairwise inner products are nearly zero, with the
exception of nodes and their ancestors, which share their parents’
regions. More specifically, as expected, dictionary elements @xmath ,
@xmath , @xmath , @xmath , @xmath , @xmath , @xmath , tend to have large
inner products, while those elements with non-overlapping borders do
not. This exact property is also visible in the two diagonal submatrices
of Fig. 7.11 (b). In the off-diagonals, due to the orthonormality of the
construction, elements of the form @xmath and @xmath are orthogonal. In
contrast, due to the nature of the tree subdivision scheme, elements of
the form @xmath or @xmath and @xmath have a large magnitude inner
product. However, the number of connections between nodes and their
ancestors is @xmath , while the total number of pairwise inner products
is @xmath ; and for reasonably sized values of @xmath the ratio of
ancestral connections to pairwise inner products grows small (see Fig.
7.12 ).

### 7.5 Solution Approaches for Linear Systems

As we will show in § 7.6 , the signal approximation problem on the
sphere reduces to a linear problem of the form

  -- -------- -- --------
     @xmath      (7.20)
  -- -------- -- --------

where @xmath are samples of a function on a region @xmath , @xmath are
estimate coefficients in the given dictionary, and @xmath represents a
spatial discretization of the dictionary elements.

While the number of samples @xmath is usually larger than the number of
dictionary elements, in practice, due to the nature of sampling and
discretization (e.g., [ 100 , Fig. 1b] ), the rank @xmath of @xmath is
significantly lower than @xmath . There are several examples in the
literature concerning spherical harmonics, for which ( 7.20 ) is
invertible (that is, with @xmath ). For example, it can be shown via a
Shannon-type theorem that when @xmath represents the spherical harmonic
coefficients of a function with bandlimit @xmath , @xmath represents its
samples on a special semi-regular grid, and @xmath represents the
discretization of the harmonics to the grid, the signal @xmath can be
reconstructed exactly [ 35 , Thm. 3] . In this case, the semi-regular
grid consists of @xmath points on the entire sphere @xmath , and the
number of harmonic coefficients (also the rank of @xmath ) is @xmath .
In the limit of large bandlimit @xmath , the ratio of the samples to
unknowns is @xmath . As such, the sampling scheme presented in that
paper is in a sense optimal in order of magnitude. Nevertheless, even
there the rank is significantly lower than the sample number.

Under the assumption that @xmath , we can write the
compact Singular Value Decomposition (SVD) [ 49 , Chapter 7] of @xmath :

  -- -------- -- --------
     @xmath      (7.21)
  -- -------- -- --------

Here @xmath , @xmath is a diagonal matrix of positive singular values,
and @xmath . We can thus rewrite ( 7.20 ) as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath may be smaller than @xmath .

We must therefore focus on ( 7.20 ) with all three possible cases: the
overdetermined case @xmath (with rank @xmath ), the case @xmath (with
rank @xmath ), and especially the underdetermined case @xmath (with rank
@xmath ). We first quickly review the overdetermined case.

#### 7.5.1 Overdetermined and Square Cases: @xmath

When the system ( 7.20 ) is overdetermined, there many not be one exact
solution @xmath . However, in this case it is possible to minimize the
sum of squared errors

  -- -- -- --------
           (7.22)
  -- -- -- --------

Taking gradients with respect to vector @xmath and setting the resulting
system equal to zero, we get the least squares solution

  -- -------- -- --------
     @xmath      (7.23)
  -- -------- -- --------

Note that ( 7.22 ) arises from the Maximum Likelihood formulation of a
statistical model in which the samples @xmath are observed as [ 10 ,
Chapter 3]

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is i.i.d Gaussian noise.

When @xmath , ( 7.20 ) has exactly one solution. The matrix @xmath and
its conjugate are invertible and, using the identity @xmath , ( 7.23 )
reduces to @xmath , as expected.

#### 7.5.2 Underdetermined Case: @xmath

When the linear system @xmath has more unknowns than equations (or rank
@xmath ), additional modeling or regularization is required. We discuss
two possible statistical models on @xmath , their limiting cases, and
the resulting computational considerations.

##### Prior: @xmath distributed according to a Gaussian distribution

One possible way to model the underdetermined case is via a statistical
model in which the coefficients of @xmath are generated i.i.d. with a
zero-mean, fixed variance Gaussian distribution:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

The maximum a posteriori (MAP) estimate follows from Bayes’ rule:

  -- -------- -------- --
     @xmath   @xmath   
                       
  -- -------- -------- --

When @xmath and @xmath are fixed, maximizing @xmath is equivalent to
minimizing its negative logarithm. The MAP problem in this case becomes

  -- -- -- --------
           (7.24)
  -- -- -- --------

where @xmath .

Simple calculus again provides the solution, also known as the Tikhonov
regularized solution to ( 7.24 ):

  -- -------- -- --------
     @xmath      (7.25)
  -- -------- -- --------

When the model noise power @xmath grows small with respect to the signal
power @xmath , the regularization term @xmath goes to zero. In this
limiting case, we can write the limiting solution as [ 49 , pp 421-422]

  -- -------- -- --------
     @xmath      (7.26)
  -- -------- -- --------

where @xmath is the Moore-Penrose generalized inverse of @xmath . For
overdetermined and exactly determined systems, @xmath has a well defined
inverse and @xmath coincides with the matrix in ( 7.23 ). For
underdetermined systems, the matrix @xmath is still well defined, and is
given by

  -- -------- --
     @xmath   
  -- -------- --

where @xmath are given by the SVD as in ( 7.21 ). Note that @xmath also
solves the convex, quadratic optimization problem

  -- -------- --
     @xmath   
  -- -------- --

The Tikhonov and Moore-Penrose solutions ( 7.24 ) and ( 7.26 ) are a
common approach to solving underdetermined inverse problems in the
Geosciences literature (see e.g., [ 113 ] ). However, depending on the
dictionary used for the representation of @xmath the Gaussian prior may
not be an ideal one; as it encourages all of the coefficients to be
nonzero.

One of the major underlying foundations of this work includes recent
results in representation theory, which have shown that overcomplete
(redundant) multiscale frames and dictionaries with certain incoherency
properties can provide stable and noise-robust estimates to ill-posed
inversion problems. The basic requirement in the estimation stage is
that the solution is as “simple” as possible: most of the coefficients
in of the solution @xmath are zero; @xmath is sparse . We discuss this
next.

##### Prior: @xmath distributed according to a Laplace distribution

It is well known in the statistics community (and, most recently, in the
Compressive Sensing literature), that applying a super Gaussian prior
@xmath induces MAP solutions that have many zero components and a few
large magnitude ones. The zero-mean Laplace distribution is one such
particularly convenient distribution. We model each component of @xmath
, @xmath as i.i.d. Laplace distributed:

  -- -------- --
     @xmath   
  -- -------- --

In a manner identical to that of the previous section, for fixed noise
power @xmath and signal scale @xmath , the MAP problem can be reduced to

  -- -- -- --------
           (7.27)
  -- -- -- --------

where @xmath . For any @xmath , there is a @xmath such that the
following problem is identical:

  -- -------- -- --------
     @xmath      (7.28)
  -- -------- -- --------

where for a given @xmath , there exists a @xmath such that ( 7.28 )
gives a solution identical to ( 7.27 ), and @xmath decreases
monotonically with @xmath . Furthermore, for any given @xmath to ( 7.28
), an @xmath can be found for ( 7.27 ) that provides the identical
solution [ 67 , §12.4.2] ; this result essentially follows from the
method of Lagrange multipliers. In other words, the two convex problems
are completely equivalent. They are also equivalent to the popularly
studied Compressive Sensing problem

  -- -------- --
     @xmath   
  -- -------- --

where @xmath grows monotonically in @xmath and/or @xmath .

Fast and robust solvers for the convex quadratic optimization problems (
7.27 ) and ( 7.28 ) have been the subject of study for many years. For
our calculations we use the LASSO solver ⁹ ⁹ 9 The LASSO glmnet package:
http://www-stat.stanford.edu/~tibs/lasso.html . (see, e.g., [ 107 ] ).
LASSO is an iterative solution method that provides the full solution
paths to ( 7.28 ). It is computationally efficient for small- to medium-
scale linear systems. For systems with more than tens of thousands of
unknowns, there are a variety of other techniques for the solution of (
7.28 ) that are much more computationally tractable, though possibly
less accurate (see, e.g., [ 29 ] and [ 67 , §12.4, §12.5] ).

##### Debiasing the @xmath solution

An alternative approach to finding a sparse solution of ( 7.20 ) is to
attempt to minimize the problem

  -- -------- -- --------
     @xmath      (7.29)
  -- -------- -- --------

where @xmath and @xmath . That is, find the best matching data to the
model where the number of nonzero coefficients of the model is bounded
by @xmath . Unfortunately, this combinatorial problem is nonconvex and
therefore usually intractable. In some cases, it can be shown that the
solution is equivalent to the @xmath problem ( 7.28 ), when the matrix
@xmath fulfills one of a number of special properties, e.g., the
Restricted Isometry Property (RIP) or Null Space Property (NSP). This
result is, in fact, a celebrated equivalence result in Compressive
Sensing [ 17 , 34 ] . Unfortunately, for physical discretization
matrices @xmath , the RIP and its equivalents are difficult to check [
27 , 55 ] .

In practice, problem ( 7.29 ) can be reduced into two parts:

1.  Estimate the support of @xmath , @xmath , such that @xmath .

2.  Solve the overdetermined system ( 7.22 ) via ( 7.23 ) on the reduced
    set @xmath by keeping only the columns of @xmath associated with
    @xmath , @xmath , when solving the least squares problem.

A tractable solution to the first part is to use the output of the
@xmath (LASSO) estimator:

  -- -------- --
     @xmath   
  -- -------- --

The final estimate is the vector @xmath where

  -- -------- -------- -- --------
     @xmath               (7.30)
     @xmath   @xmath      
  -- -------- -------- -- --------

This alternative solution is also sometimes called the debiased @xmath
solution, because after the @xmath minimization step, the bias of the
@xmath penalty is removed via least squares on the estimated support.

As we will show in the next section, this final combination of support
estimate based on a sparsity-inducing prior, followed by the solution to
an overcomplete least squares problem on this support set, allows for
improvements in signal approximation over currently standard techniques
in geophysics.

### 7.6 Signal Approximation Models for Subsets of the Sphere @xmath

We now turn to the problem of estimating a signal from noisy and/or
incomplete observations on a subset @xmath of the sphere. Following the
notation of [ 91 ] , suppose we observe data (samples of some function
@xmath ) on a set of points within the region @xmath , consisting of
signal @xmath plus noise @xmath . We are interested in estimating the
signal within @xmath from these samples.

While in practice, most signals of interest in geophysics are not
bandlimited, this assumption allows us to perform estimates, and can be
thought of as a regularization of the signal, similar in nature to
assumptions of a maximum frequencies in audio analysis. Furthermore, as
in 1D signal processing, constraints on physical sampling and high
frequency noise always reduce the maximum determinable frequency. See,
for example, a noise analysis for satellite observations in the GRACE
mission [ 110 , Fig. 1] , and the effects of noise on power spectral
estimation for the CMB dataset [ 28 , Fig. 12] .

Let @xmath be a set of points on which data @xmath is observed, and let
the corresponding observations be @xmath , which we denote with the
vector @xmath . Then via the harmonic expansion ( 7.6 ), we can write

  -- -------- -- --------
     @xmath      (7.31)
  -- -------- -- --------

where the @xmath are the harmonic expansion coefficients of the signal
@xmath and @xmath is a realization of the noise. We will also denote by
@xmath the vector of samples of the noise process. Let @xmath be the
@xmath harmonic sensing matrix, with

  -- -------- --
     @xmath   
  -- -------- --

Then we can rewrite ( 7.31 ) as

  -- -------- -- --------
     @xmath      (7.32)
  -- -------- -- --------

By restricting the bandlimit to @xmath , we restrict the function @xmath
to lie in @xmath . Moreover, we are only interested in estimating @xmath
on @xmath .

The Slepian functions are another basis for @xmath , in which the
functions are ordered in terms of their concentration on @xmath . As
such, we may rewrite the samples via their Slepian expansion:

  -- -------- -- --------
     @xmath      (7.33)
  -- -------- -- --------

where now the @xmath are the Slepian expansion coefficients, and we are
free to constrain @xmath from @xmath through @xmath . By setting @xmath
to @xmath , we concentrate the estimate to @xmath , while choosing
@xmath leads to a representation equivalent to ( 7.31 ).

Using the harmonic expansion of the Slepian functions, we rewrite ( 7.33
) via the spherical harmonics:

  -- -------- -- --------
     @xmath      (7.34)
  -- -------- -- --------

and, using the terminology @xmath of § 7.4 to denote the harmonic
expansion matrix of the Slepian functions, and the “colon” notation to
denote restrictions of matrices and vectors to specific index subsets,
we can rewrite ( 7.33 ) in matrix notation:

  -- -------- -- --------
     @xmath      (7.35)
  -- -------- -- --------

As just described, by setting @xmath this model assumes that all but the
first @xmath of the Slepian coefficients @xmath are zero.

Finally, using the Tree dictionary construction of § 7.3 and the
notation of § 7.4 , for a given dictionary @xmath we can model the
signal with the linear model

  -- -------- -- --------
     @xmath      (7.36)
  -- -------- -- --------

Writing the dictionary elements via their harmonic expansions, the
matrix formulation of ( 7.36 ) becomes

  -- -------- -- --------
     @xmath      (7.37)
  -- -------- -- --------

As we will see next, this alternative way of describing bandlimited
functions on @xmath has a number of advantages.

#### 7.6.1 Regularized Inversion and Numerical Experiments

In practice, the sensing matrix @xmath is highly rank deficient:
depending on the sensing grid points @xmath , its rank @xmath tends to
be significantly smaller than the maximum possible value @xmath , @xmath
. As such, the estimation of @xmath via direct inversion of ( 7.32 ) is
ill conditioned: it must be regularized.

As discussed in § 7.5 , the most common form of regularization is via
the Moore-Penrose pseudoinverse: ( 7.23 ) for overdetermined systems or
( 7.26 ) for underdetermined ones. Following the discussion of § 7.2.3
and the statistical analyses in [ 91 , §3] and [ 93 , §7] , the
“classically” optimal way to estimate @xmath is by restricting the
reconstruction to be concentrated within @xmath : that is, first by
choosing a small @xmath in ( 7.35 ), such that @xmath , and then
applying ( 7.23 ) to estimate @xmath . In practice, we can consider
@xmath ranging from @xmath to @xmath because the Shannon number is less
than rank @xmath . We will call this first estimation method
Slepian Truncated Least Squares (STLS).

We now propose, first, a simple alternative approach: assume sparsity of
@xmath (i.e., with respect to the Slepian basis). As the Slepian basis
was initially constructed to promote sparsity in the representation of
bandlimited functions concentrated on @xmath [§3.1.2] [ 91 ] , we expect
that this assumption should lead to estimates that are equivalent to
STLS, if not better. The basic idea is to let @xmath in ( 7.35 ), and
use the solution method ( 7.30 ) with sparsity penalties @xmath
sufficiently large that only a few nonzero coefficients are found in the
support. We consider a range of values @xmath from a maximum @xmath that
induces only one nonzero coefficient, to a minimum @xmath that induces
@xmath . We call this estimation method Slepian @xmath + Debias (SL1D).
Though in this case we do not explicitly require the estimate to be well
concentrated in @xmath via choice of basis functions, by minimizing the
squared error between sample values on @xmath we expect that most of
estimated support will be within the first Slepian functions.

With the Tree construction of § 7.3 , we have a new dictionary of
elements that are both concentrated in @xmath , bandlimited, and
multiscale. As such, these dictionaries are excellent candidates for
estimation via the @xmath + Debias technique ( 7.30 ). This method is
similar to the previous one: apply ( 7.30 ) to the model ( 7.37 ),
choosing a range of @xmath penalties @xmath that lead to between @xmath
and @xmath dictionary elements in the support of @xmath . We call this
the Slepian Tree @xmath Debias (STL1D) method.

The experiments below numerically show that the two new estimation
(inversion) methods SL1D and STL1D provide improved performance over the
classic STLS, in terms of average reconstruction error over the domain
of interest @xmath using a small number of coefficients, for several
important types of bandlimited signals. Furthermore, as expected the
multiscale and spatially concentrated dictionary elements of the Tree
construction provide improved estimation performance when the signal is
“red”, i.e., when it contains more energy in the lower harmonic
components.

##### Bandpass Filtered POMME Model

Fig. 7.13 shows a bandpass filtered version of the radial component of
Earth’s crustal magnetic field, which we will call @xmath : a
preprocessed version of the output of the POMME model [ 70 ] . The
signal @xmath has been:

1.  Bandpassed between @xmath and @xmath .

2.  Spatially tapered (multiplied) by the first Slepian function
    bandlimited to @xmath and concentrated within Africa.

3.  Low-pass filtered to have maximum frequency @xmath via direct
    projection onto the first @xmath spherical harmonics using standard
    Riemannian sum-integral approximations [ 91 , Eq. 80] , i.e., direct
    inversion.

It can be shown [ 112 , §2] that the harmonics of the tapered signal, at
degree @xmath , receive contributions from the original coefficients in
the range from @xmath to @xmath . As a result, only the first @xmath
degree coefficients are reliable estimates of the original signal’s
harmonics.

Samples of @xmath are given via the forward model ( 7.31 ) (with @xmath
), from the low-pass filtered “ground truth” signal @xmath , on the
intersection of the African continent @xmath with the grid

  -- -------- --
     @xmath   
  -- -------- --

We denote this reduced set @xmath ; it contains @xmath points. For
@xmath , as before, the Shannon number of Africa is @xmath , the
dimension of the bandlimited space is @xmath , and the rank of the
discretization matrix in ( 7.31 ), @xmath , is @xmath .

Figs. 7.14 , 7.15 , and 7.16 show intermediate results in the estimation
of @xmath via the three methods: STLS, SL1D, and STL1D, respectively.
Specifically, they show the absolute error between the original sampled
signal @xmath , and the expansion via ( 7.6 ) of the three estimates. In
Fig. 7.14 , the number of nonzero coefficients in the estimate is
determined by the Slepian truncation number @xmath , while in Figs. 7.15
and 7.16 , the number of nonzero coefficients are indirectly determined
by the parameter @xmath after the support estimation stage, as per our
earlier discussion. As such, the number of nonzero components do not
always match that of Fig. 7.14 ; instead nearby values are used when
found.

As the dictionary elements given by the Tree construction are localized
in both scale and location, we can graphically show which elements are
“turned on” through the solution path, as more and more elements in the
support are chosen to be nonzero. Fig. 7.17 shows the supporting regions
@xmath of dictionary @xmath associated the solutions given in the
corresponding panels of Fig. 7.16 . Clearly, larger scale dictionary
elements are chosen first; these reduce the residual error the most. As
more and more dictionary elements are added during the @xmath based
inversion process, finer and finer details are included in the
reconstruction.

Fig. 7.18 compares, on a logarithmic scale, the spatial residual errors
(sum of squared differences) between the three estimates, as a function
of the number of nonzero components allowed. Clearly, when a small
number of nonzero components is allowed, the sparsity-based estimators
outperform the standard Slepian truncation-based inversion.

To study the consistency of the @xmath -based SL1D and STL1D estimators,
and as a measure of how closely the SL1D method matches the classical
truncation strategy, we plot the solution paths of these two estimators.
Fig. 7.19 shows that while lower order (better-concentrated in @xmath )
Slepian functions were chosen early on, when @xmath was small. However,
as more and more nonzero indices were allowed, less well concentrated
Slepian functions, with lower magnitudes, were included in the solution,
probably as small “tweaks” to the estimate near the edges of the region.
Once a Slepian function was included into the solution, its magnitude
did not change much throughout the solution path (as other elements were
added).

In contrast to the behavior of SL1D, the Slepian Tree solution chose
particular elements localized to the main features of the signal, not
simply elements that are well concentrated in all of Africa on a large
scale. In addition, as the size of the support was allowed to increase,
the magnitudes of some coefficients were decreased as new elements were
added. This supports the general statement that multiscale dictionaries,
when combined with sparsity-inducing reconstruction techniques, “fit”
the support to the nature of the data. Figs. 7.18 and 7.19 thus help to
clarify the behavior of the STL1D estimator.

##### White and Pink Noise

As a second experiment, we generated multiple observations of either
pink or white noise fields, with a bandlimit of @xmath , on the sphere
@xmath , via randomization in the spherical harmonic representation of
@xmath . We then sampled these on @xmath to and attempted to reconstruct
them only in Africa, as in the previous example.

A random field @xmath with spectral slope @xmath , up to degree @xmath ,
is defined as having the harmonic coefficients

  -- -------- -------- -- --------
     @xmath   @xmath      (7.38)
     @xmath   @xmath      
     @xmath   @xmath      
  -- -------- -------- -- --------

For a white noise process, with equal signal power across its spectrum,
@xmath . Most spatial processes in geophysics, however, have some @xmath
; their power drops off with degree @xmath . Fields with @xmath near
@xmath are considered to be “pink”, while fields with @xmath near @xmath
are “red”. The more red a noise process, the higher its spatial
correlation, the less “random” it looks. The earth’s geopotential field,
for example, is modeled as having @xmath . In our experiments, we use
@xmath to generate white noise processes and @xmath for pink.

For each of @xmath iterations, we generated both pink and white noise
fields @xmath (bandlimited to @xmath ) and sampled them on @xmath (the
grid over the African continent) to get the vectors @xmath . As before,
the discretization matrix @xmath is of rank @xmath so direct inversion
is impossible. We again performed reconstruction via the three methods
STLS, SL1D, and STL1D. As in Fig. 7.18 , for each of the iterations we
calculated the normalized residual: the sum of squared differences
between the estimates, as expanded on @xmath , and the original samples,
normalized by the sum of squares @xmath .

Figs. 7.20 and 7.21 show the mean normalized error over the @xmath
iterations for white and pink noise, respectively. For white noise, the
ability to use any of the Slepian functions clearly provides an
advantage for the SL1D algorithm. In contrast, for pink noise the
spatial localization of the dictionary elements and the smaller size of
the dictionary give an edge to the Tree-based estimator. Clearly,
however, both estimators lead to lower normalized observation error on
average, as compared with the classically optimal Slepian truncation
method STLS.

### 7.7 Conclusion and Future Work

We have motivated and described a construction for dictionaries of
multiscale, bandlimited functions on the sphere. When paired with the
modern inversion techniques of § 7.5.2 , these dictionaries provide a
powerful tool for the approximation (inversion) of bandlimited signals
concentrated on subsets of the sphere. The numerical examples in § 7.6.1
provide good evidence for the efficacy of the estimators SL1D and STL1D.
More simulations are required to confirm and explore their numerical
accuracy.

In addition, more theoretical analysis of the existing dictionary
constructions (e.g., their concentration properties) is also required.
Especially when working in concert with the @xmath -based estimators,
questions of coherence are especially important [ 44 , 17 ] .

The theoretical underpinnings of the SL1D estimator have not been
studied, to our knowledge. In contrast, the identically equivalent
question of estimating the support of the Fourier transform of a signal,
given its (possibly nonuniform) samples, is one that has been studied
extensively in the Compressive Sensing community (starting with, e.g., [
17 , 16 ] ).

The top-down subdivision based scheme described in this chapter is not
the only way to construct multiscale dictionaries. Followup work may
include one or more of the following ideas:

-   Instead of estimating an optimal height @xmath during construction,
    simply prune a tree element @xmath if its spectral concentration
    @xmath or concentration in @xmath , @xmath , is below a minimum
    threshold. This allows for more adaptive and better concentrated
    dictionary elements near high-curvature borders.

-   While the dictionaries described here describe “summary” functions
    (for @xmath ), it is possible to use Gram-Schmidt orthogonalization
    to construct an alternate “difference” dictionary by orthogonalizing
    each node with its parent and sibling. Such dictionaries would be
    better tuned to find “edges”, and would provide sparser
    representations for mostly smooth data. In practice, this leads to
    better performance of @xmath -based estimators like STL1D.

-   Other subdivision construction schemes should be considered. For
    example, when the subregion @xmath is highly nonconvex (e.g., when
    @xmath is the interior of the Earth’s oceans), even the second
    Slepian function contains more than one mode. In this case, it is
    unclear how to subdivide the domain from the top down. Instead, a
    bottom-up approach would work, wherein a fine grid is constructed on
    the region @xmath , and grid elements are “merged” until their area
    is large enough that reasonably well concentrated Slepian functions
    with bandwidth @xmath will fit in them.

The ultimate goal of the constructions in this chapter is an
overcomplete multiscale frame of bandlimited functions that are well
concentrated on @xmath , can be constructed quickly, and admit fast
forward and inverse transforms. That is, we seek a methodology similar
to the Wavelet transforms but allowing for bandlimits. The work here
should be considered a stepping stone in that direction as it shares
many of the properties of third generation Wavelets treated elsewhere,
especially the one that may be most important: numerical accuracy in the
solution of ill-posed inverse problems.

## Appendix A Differential Geometry: Definitions

The manifold @xmath with metric tensor @xmath admits an atlas of charts
@xmath , where @xmath is a diffeomorphism from the open subset @xmath [
33 , Chs. 0,4] . The choice of a standard orthonormal basis in @xmath
defines a corresponding basis for the tangent plane to @xmath at each
@xmath , as well as a local coordinate system @xmath near @xmath . The
components of the tensor @xmath are defined as the inner products
between the partial derivatives of @xmath in this coordinate system:
@xmath where @xmath . The inverse tensor, denoted by @xmath , is smooth,
everywhere symmetric and positive definite because @xmath is. We use the
notation @xmath . Note that we will often drop the position term @xmath
; for example, @xmath .

We assign to @xmath the standard gradient @xmath , inner product @xmath
, divergence @xmath , and Laplacian @xmath at a point @xmath [ 83 , Ch.
1] . For @xmath a twice-differentiable function on @xmath (i.e. @xmath )
and @xmath and @xmath differentiable vector fields on @xmath ,

  -- -------- --
     @xmath   
              
  -- -------- --

where again @xmath and @xmath . We use the definitions above throughout,
as well as the norm definition @xmath ; thus @xmath .

## Appendix B Spectral Theory

In this appendix we state some basic facts about the existence of the
Fourier transform for functions in @xmath . We also discuss the
existence and properties of Fourier series representations for functions
in @xmath , where @xmath is a Riemannian manifold with metric @xmath
(see App. A ). A comprehensive review of Fourier transforms and
convolutions on general (possibly non-compact) spaces is available in [
86 , Ch. 7] , and on Riemannian Manifolds in particular in [ 83 , Ch. 1]
.

### b.1 Fourier Transform on @xmath

The Fourier transform of a function @xmath is formally defined, for all
@xmath , as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath solves the eigenvalue problem

  -- -------- --
     @xmath   
  -- -------- --

with @xmath and @xmath .

The Fourier inversion theorem and its extensions [ 86 , Thms. 7.7 and
7.15] state that @xmath is well defined, and that @xmath can be
represented via @xmath . For all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

### b.2 Fourier Analysis on Riemannian Manifolds

Suppose @xmath is a separable Hilbert space, e.g., @xmath where @xmath
is a compact set. Further, suppose @xmath is a bounded, compact,
self-adjoint operator. Then the Hilbert-Schmidt theorem [ 81 , Thm.
VI.16] states that there is a complete orthonormal basis @xmath for
@xmath such that @xmath , @xmath , and @xmath as @xmath . The Laplacian
operator @xmath defined on compact subsets of @xmath therefore admits a
complete orthonormal basis (as its inverse is compact), and from now on
we refer to @xmath as the eigenfunctions of the Laplacian. This set is
called the Fourier basis.

We can therefore write any function @xmath as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath are called the Fourier coefficients. The calculation of
Fourier coefficients is called analysis, and the reconstruction of
@xmath by expansion in the Fourier basis is called synthesis.

On a Riemannian manifold @xmath a similar analysis applies [ 83 , Thm.
1.29] :

###### Proposition B.2.1 (Hodge Theorem for Functions).

Let @xmath be a compact connected oriented Riemannian manifold. There
exists an orthonormal basis of @xmath consisting of eigenfunctions of
the Laplacian. All the eigenvalues are positive, except that zero is an
eigenvalue with multiplicity one. Each eigenvalue has finite
multiplicity, and the eigenvalues accumulate only at infinity.

The Laplacian given above is the negative of the Laplace-Beltrami
operator defined in § A , and the eigenfunctions have Neumann boundary
conditions.

Fourier analysis and synthesis can be written as

  -- -------- --
     @xmath   
  -- -------- --

where the analysis integral above is with respect to the volume metric
@xmath . Fourier analysis is a unitary operation (this is known as
Parseval’s theorem). Note that practical definitions of the forward and
inverse Fourier operators often differ by the choice and placement of
normalization constants, in order to simplify notation (as is the case
throughout this thesis).

#### b.2.1 Fourier Analysis on the Circle @xmath

On the circle, @xmath , parametrized by @xmath with @xmath , we have
@xmath for @xmath . We can therefore decompose @xmath via projections
onto @xmath . That is, we can write

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath .

The analysis, as defined, is an isometry. By Parseval’s theorem, for
@xmath ,

  -- -------- --
     @xmath   
  -- -------- --

This analysis can also be applied to functions on @xmath with periodic
boundary conditions by identifying the interval with @xmath .

#### b.2.2 Fourier Analysis on the Sphere @xmath

The unit sphere, @xmath , can be parametrized by @xmath where @xmath is
the colatitude and @xmath is the longitude. In this case, the Laplacian
is

  -- -------- --
     @xmath   
  -- -------- --

and the volume element is @xmath .

For our purposes, we are interested in real-valued functions on the
sphere. As such, we study the real-valued eigenfunctions of the
Laplacian on @xmath . The real surface spherical harmonics, @xmath , are
parametrized by the degree @xmath and order @xmath , where @xmath and
@xmath . These can be given by [ 93 ] :

  -- -------- -------- -- -------
     @xmath   @xmath      (B.1)
     @xmath   @xmath      (B.2)
  -- -------- -------- -- -------

and @xmath are the associated Legendre functions of degree @xmath and
degree @xmath [ 1 , §8.1.1] . Each spherical harmonic @xmath fulfills
the eigenvalue relationship @xmath .

We can therefore decompose @xmath via projections onto @xmath , where
now @xmath . That is, we can write

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath .

The analysis, as defined, is an isometry. By a Parseval’s theorem, for
real-valued functions @xmath ,

  -- -------- --
     @xmath   
  -- -------- --