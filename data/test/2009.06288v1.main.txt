## Chapter 1 Introduction

### 1.1 Motivation

Medical imaging has widely proven to be an essential tool for modern
medicine. The ability to non-invasively visualize in-vivo
representations of the interior of the human body constituted an
unprecedented breakthrough in the diagnosis, prognosis and follow-up
processes of the diseases ( Mcrobbie2007 ) . The first medical image
acquisition dates back to 1895 with the discovery of X-rays (
Rontgen1898 ) , however it was not until the end of the 20th century
that medical imaging took a qualitative leap with the development of the
MRI ( Damadian1971 ; Lauterbur1973 ; Lauterbur1974 ; Mansfield1977 ) .
Over the years, medical imaging has evolved rapidly, reaching
sophisticated techniques capable of quantifying large amounts of
information of the anatomy and functionality of human tissues.
Consequently, the analysis of such complex information has become a
specialized discipline that requires advanced computational techniques
to make the most of the knowledge contained therein.

In this context, ML emerges as a solid candidate for medical image
analysis. ML is an application of AI that provides systems the ability
to learn and identify complex patterns in multi-dimensional data, and
perform specific tasks without being explicitly programmed. The
beginnings of ML in medical imaging dates back to the middle of the 20th
century with the birth of the expert systems ( Russell2016 ) . From that
moment on, an unstoppable proliferation of ML systems for multitude of
clinical problems took place, reaching its first hype cycle peak at the
end of 20th century. Today, with the advent of the Deep Learning ( DL )
techniques, medical image analysis is undergoing a new deep revolution
that is settling the ML as an indisputable instrument in the modern
clinical practice.

However, ML has historically addressed medical problems from the
perspective of automatizing arduous complex tasks for humans. Tied to
medical imaging, ML has successfully addressed complicated problems such
as identifying and delineating abnormal tissues in multiple images or
classifying and grading lesions from medical acquisitions ( Azuaje2019 ;
Levine2019 ; Kann2019 ) . Supervised learning, which is a family of
techniques under the ML umbrella, has led this approach with
unquestionable effectiveness. However, due to its learning nature,
supervised learning is only able to address problems where humans
already know the answer ( Duda2000 ) . This family of techniques builds
models by means of learning the relations between tuples of @xmath ,
which necessarily requires that all algorithm’s possible outputs are
explicitly known. This approach, without underestimating its
unquestionable usefulness, reduces the ML to an instrument for solving
and automatizing tasks with already well-known targets, with the added
value of high accuracy, repeatability and reliability.

On the contrary, we would like ML to help us to discover new knowledge
from the medical data beyond what humans can devise. At this point
unsupervised learning arises as a tailor-made solution to this purpose.
Unsupervised learning, unlike supervised learning, inspects unlabeled
data for hidden patterns and inner relationships that describe the
latent structure of the data ( Bishop2006 ) . This alternative approach
has the intrinsic capability to discover new knowledge from the data in
the form of new hypothesis about the structure and arrangement of the
information. In this sense, unsupervised learning should assume a
relevant role in medical imaging and must drive ML to serve not only as
a tool for automating complex processes, but also as an instrument for
exploring and extracting hidden knowledge from medical images.

A canonical example in which the convergence of medical imaging and ML
must be reoriented towards the discovery of new knowledge is the study
of highly aggressive heterogeneous tumors ( Rudie2019 ; Tandel2019 ) .
Specifically, glioblastoma tumor represents a major challenge, as it
remains a lethal tumor that, to date, lacks a satisfactory therapy,
presenting one of the poorest prognosis among all human cancers, with a
median survival time of 12-15 months despite aggressive treatment (
Jain2018 ) . Since the introduction of the Stupp treatment ( Stupp2005 )
in 2005, there have been no significant changes in the therapies that
have led to an improvement in patient prognosis. Therefore, the
blockbuster model of “the same treatment for all” is considered to be
depleted. In this regard, efforts today must be placed into the
extraction of new knowledge from medical data that allow us to move
towards new personalized more effective therapies.

One of the reasons believed to be behind this tumor’s malignancy is its
highly heterogeneous nature ( Soeda2015 ) . Glioblastomas are malignant
masses characterized by hyper-cellularity, pleomorphism, micro-vascular
proliferation and high necrosis mitotic activity ( Gladson2010 ) .
Particularly, vascular heterogeneity has been largely studied since it
is considered crucial for glioblastoma propagation and survival (
Das2013 ) . Glioblastoma presents strong abnormal vascular
proliferation, robust angiogenesis, and extensive micro-vasculature
heterogeneity ( Kargiotis2006 ) , which have been shown to have a direct
effect on prognosis ( Hardee2012 ) . Therefore, the early assessment of
the heterogeneous vascular architecture of the tumor is thought to
provide important information to improve and design new therapies.

However, measuring vascular heterogeneity from medical images is
currently an uncertain task. There is no a consensus nor an accepted
method to assess it. Consequently, there are no imaging protocols nor
tools that includes such information in the clinical routine, hence
ignoring this valuable knowledge in the management of the disease.
Likewise, there are no expert manual annotations nor medical imaging
datasets from where supervised ML algorithms can learn models. Such an
undefined problem provides an excellent opportunity to apply
unsupervised learning as an instrument for exploring new knowledge about
the vascular profile of the tumor.

This thesis confronts all the factors mentioned above. The thesis
focuses on the conjugation of unsupervised ML techniques on medical
imaging data to non-invasively measure and describe the vascular
heterogeneity of glioblastomas. This premise has established the main
motivation and goals of this thesis, leading to the following research
questions and objectives.

### 1.2 Research questions and objectives

The application of unsupervised ML techniques to medical images for
brain tumor analysis poses a number of challenges that need to be
addressed. First of all, by definition, unsupervised learning is a more
open and undefined task than supervised learning. Unlike the latter,
unsupervised learning solutions generally lack of semantic meaning. On
the contrary, they usually consist of an interpretation of the data
structure in terms of subgroups that share similar patterns with each
other. Therefore, these algorithms require intelligent strategies, both
during training and in the posterior analysis of the solution, to guide
them towards the extraction of useful, consistent and interpretable
knowledge.

Tied to the characterization of the vascular heterogeneity of
glioblastomas, unsupervised learning methods need to be guided to find
hidden patterns in perfusion medical images that are consistent with
physiologically plausible hypotheses. In this sense, these unsupervised
methods must serve not only as a mere algorithms for the
characterization of the vascular profile of the tumor, but to tools to
measure advanced imaging biomarkers that could give clues about the
underlying physiological process taking place in the tumor.

Under a more technical point of view, learning patterns from imaging
data also poses important challenges that must take into a account.
Imaging data presents patterns of local regularity and spatial
redundancy that suggest that they cannot be assumed to be independent
and identically distributed ( i.i.d. ). Robust image analysis algorithms
- both supervised and unsupervised learning - must consider this
structured nature of the images and must include mechanisms to take
advantage from this latent information.

As a consequence the next research questions are proposed in this
thesis:

-   Is unsupervised learning an adequate and reliable solution to
    extract valuable knowledge from complex MRI data?

-   Can we contribute to the unsupervised learning family with new
    structured prediction algorithms to improve the extraction of
    knowledge from images?

-   Can we contribute to a better understanding of the vascular
    heterogeneity of the glioblastoma by means of an advanced analysis
    of MRI through unsupervised learning methods?

-   Can useful measurements be obtained from the vascular heterogeneity
    description of glioblastoma that provide meaningful information on
    relevant patient’s clinical outcomes?

-   Can the unsupervised learning method to describe the vascular
    heterogeneity of glioblastomas be part of a medical imaging software
    for a complete analysis of the tumor through MRI ?

The research work conducted in this thesis aims to provide solutions to
these questions by means of theoretical and empirically validated
scientific methods applied to the study of the vascular heterogeneity of
glioblastomas by MRI . To this end the following objectives were
defined:

-   Review of the state-of-the-art in unsupervised learning algorithms,
    especially focused on image oriented algorithms and paying special
    attention to those applied to the analysis of brain tumor MRI
    images.

-   Evaluate the feasibility of unsupervised learning algorithms to
    analyze and extract knowledge from MRI data.

-   Develop a new unsupervised learning algorithm to exploit the
    structured nature of the images and take advantage from the spatial
    redundancy and local information contained therein.

-   Develop a methodology based on image processing and unsupervised
    learning algorithms to detect and describe the vascular
    heterogeneity of glioblastoma through MRI .

-   Validate the vascular heterogeneity assessment method on real
    clinical routine MRI , by exploring associations between the
    proposed heterogeneity description and relevant clinical outcomes of
    the patient.

-   Implement a reliable tool capable of providing an advanced
    state-of-the-art analyses for glioblastoma MRI studies, including
    the methods to describe the vascular heterogeneity profile of the
    tumor.

The proposed objectives enclose the main goal of this thesis: the study
of unsupervised ML techniques for medical imaging data to non-invasively
describe the vascular heterogeneity of glioblastomas . Such goal can
also be decomposed in two strands depending on the research scope: the
technical goal , which aims to consolidate the unsupervised learning as
a reliable tool for the future of medical image analysis; and the
clinical goal , which intends to non-invasively describe the vascular
heterogeneity of glioblastoma trough MRI to improve tumor understanding
and clinical decision making. In this sense, the following scientific
contributions support the achievement of the proposed objectives.

### 1.3 Thesis contributions

This section presents the main contributions of this thesis. First, a
summary of the most relevant aspects of each contribution is presented.
Next, the scientific publications in high impact journals and
conferences are listed. Finally, the technological and software results,
as well as clinical studies, industrial patents and transfer actions are
compiled.

#### 1.3.1 Main contributions

  C1 -  

     Comparative study of unsupervised learning algorithms for
    glioblastoma segmentation
    In this study, a comparison of unsupervised learning algorithms,
    including structured and non-structured methods was performed for
    the task of high grade glioma segmentation. The study describes the
    statistical model underlying each algorithm and also proposes a
    general post-processing stage to identify which classes of an
    unsupervised segmentation correspond to pathological or healthy
    tissues. An independent evaluation of the performance of the
    unsupervised learning algorithms was carried out in a public real
    dataset, which demonstrated the capability of unsupervised learning
    to extract relevant knowledge from MRI data. This work was published
    in the journal contribution P1 ( JuanAlbarracin2015a ) and presented
    in the conference P2 ( JuanAlbarracin2015b ) .

  C2 -  

     An unsupervised learning algorithm for structured prediction
    A new variant of the Spatially Varying Finite Mixture Model s (
    SVFMM s ) family is proposed in this thesis. The algorithm, named
    Non Local Spatially Varying Finite Mixture Model ( NLSVFMM ),
    successfully merges the SVFMM s with the Non Local Means ( NLM )
    framework, proposing a continuous Markov Random Field ( MRF ) that
    simultaneously enforces smooth constraints in homogeneous regions of
    the image while preserves the edges and structures without
    degradation. This approximation improves the existing approaches in
    terms of complexity of the model, as the NLM weighting function does
    not introduce additional parameters into the model to be estimated.
    Moreover, it outperforms current methods in terms of performance in
    a segmentation task of real world images. This work was published in
    the journal contribution P3 ( JuanAlbarracin2019b ) .

  C3 -  

     A method for the vascular heterogeneity assessment of glioblastoma
    The Hemodynamic Tissue Signature ( HTS ) method analyzes the
    perfusion MRI of a glioblastoma using an unsupervised learning
    approach to delineate four habitats within the lesion that exhibit
    different hemodynamic activity. The habitats describe the High
    Angiogenic Tumor ( HAT ) and Low Angiogenic Tumor ( LAT ) regions of
    the glioblastoma, and the potentially Infiltrated Peripheral Edema (
    IPE ) and Vasogenic Peripheral Edema ( VPE ) of the lesion. Such
    approximation establishes a conceptual frame for the description of
    the tumor heterogeneity by means of the detection of clinically
    relevant sub-regions, a.k.a habitats, with differentiated imaging
    biomarkers. The preliminar results of this work were first presented
    in the conference contribution P4 ( JuanAlbarracin2016 ) and it was
    finally published in the journal contribution P5 (
    JuanAlbarracin2018 ) .

  C4 -  

     Preliminary validation of the vascular heterogeneity assessment
    method on a local cohort of glioblastomas
    A preliminary validation study was performed to assess the
    association of the HTS habitats with relevant clinical outcomes.
    Specifically, measurements on the distributions of the hemodynamic
    biomarkers confined at each HTS habitat were explored for potential
    correlations and predictive capabilities with the overall survival
    of the patients. Additionally, a technical study was conducted to
    measure the degree of dissimilarity between these distributions, in
    order to confirm the physiological differences of the hemodynamic
    activity of the habitats. Results on a real cohort from a local
    hospital were published in the journal articles P5 (
    JuanAlbarracin2018 ) and P6 ( FusterGarcia2018 ) .

  C5 -  

     International retrospective multicenter clinical study for
    validating the vascular heterogeneity assessment method
    The relevant findings obtained in the experiments for the
    preliminary validation of the aforementioned vascular heterogeneity
    assessment method led us to initiate an international multi-center
    validation study of the technology. This constituted the first
    clinical study in which the Universitat Politècnica de València (
    UPV ) was sponsor. The clinical study was formally registered in the
    ClinicalTrial.gov platform from the U.S. National Library of
    Medicine with identifier NCT03439332, as seen in contribution CS .
    It consists of a multi-center observational retrospective study with
    data collected from 7 international hospitals, with a total of 305
    patients enrolled since 1st of January of 2012 until February
    of 2018. Results obtained with this large heterogeneous cohort of
    untreated glioblastomas were published in the journal contribution
    P7 ( Alvarez2019 ) , consolidating the previous findings about the
    predictive potential of the habitats.

  C6 -  

     An online open-access system for glioblastoma MRI analysis
    This contribution consists of the development of a web-based system
    for the analysis of glioblastomas by means of MRI . The system,
    named ONCOhabitats ( https://www.oncohabitats.upv.es ), provides
    free access to all the methods developed and validated in this
    thesis, but also to other state-of-the-art algorithms in the field
    of medical image analysis, to offer a complete solution for the
    study of glioblastoma from raw unprocessed MRI . ONCOhabitats
    implements two main services to describe the morphological and
    vascular heterogeneity of the glioblastoma, generating for each
    service an automated LaTeX-based report summarizing all the findings
    of the study. The details of the system were presented in the
    journal contribution P8 and conference contribution P9 , and the
    software was registered in the technological catalogue of the UPV ,
    as shown in contributions S1 and S2 .

  C7 -  

     An industrial patent for generating multi-parametric nosological
    images
    In addition to the scientific and academic contributions, the
    methods, technologies and original ideas conceived in this thesis
    were protected under the international patent mentioned in
    contribution PT . The patent issued “Method and system for
    generating multi-parametric nosological images” was registered in
    Spain (ES201431289A), with the added value of being evaluated with
    previous exam; was extended to the European (EP3190542A1) territory
    and United States (US20170287133A1) through the Patent Cooperation
    Treaty ( PCT ) programme. The patent protects a method to produce
    nosological images from multiple medical image acquisitions, with
    the aim of facilitating the diagnosis and treatment of diseases. In
    this sense, this thesis has contributed not only with advances in
    knowledge in the fields of ML and medical imaging, but with a
    technological asset of high value for the UPV , which opens the door
    to transfer actions for the creation of new business opportunities.

  C8 -  

     Foundation of the ONCOANALYTICS CDX, S.L. company
    The issuance of the patent led us to participate in two of the most
    cutting-edge national programmes for the generation of business
    models and new start-ups in the field of healthcare technologies.
    The author of this thesis, together with the advisors, participated
    in the EIT Health Headstart Proof of Concept 2016 programmme, in
    which we were awarded the best Proof of Concept Spain for a
    technology-based start-up; and in the CaixaImpulse acceleration
    programme for facilitating entrepreneurship in biomedicine. Such
    mentoring activities finally led to the foundation of ONCOANALYTICS
    CDX, S.L. in 2018, with the commercial name Texture CDx , as shown
    in contribution TR . The company was framed into the business model
    of companion diagnostics for pharmaceutical compounds, with the aim
    of using the aforementioned vascular heterogeneity assessment
    technology to help in the stratification of patients affected by
    glioblastoma during the clinical trial of a drug. ONCOANALYTICS CDX
    was established by a multi-disciplinary team made up of computer
    scientists, physicists, oncologists, biomedical engineers and
    financial experts, including 6 UPV graduates and 4 Phd, thus
    contributing to the generation of professional opportunities for
    highly qualified personnel.

The work developed in this thesis has been framed in several national
research projects, one of which has obtained the A+ rating, i.e. the
best rating available. This has made it possible to raise public funds,
new research projects and doctoral grants that have consolidated the
research line in the Biomedical Data Science Laboratory ( BDSLab ) of
the UPV .

#### 1.3.2 Scientific publications

The scientific contributions of this thesis have been published in six
scientific top-ranked journals and five conference proceedings in the
fields of Machine Learning , Statistics and Probability, Radiology &
Nuclear Medicine, Medical Imaging and Biomedical Data Mining. The
publications are listed as follows:

-    Javier Juan-Albarracín , Elies Fuster-Garcia, José V. Manjón,
    Montserrat Robles, F. Aparici-Robles, L. Martí-Bonmatí and Juan M.
    García-Gómez. ‘Automated Glioblastoma Segmentation Based on a
    Multiparametric Structured Unsupervised Classification’ . PLoS One;
    2015; 10(5):e0125143. May 2015. ( JuanAlbarracin2015a ) .

-    IF: 3.057 (JCR 2015): 11/63 Multi-disciplinary sciences (Q1).

-    Javier Juan-Albarracín , Elies Fuster-Garcia and Juan M.
    García-Gómez. ‘Hierarchical Tissue-Guided Glioblastoma Segmentation
    based on DCA-SVFMM’ . II International Symposium on Clinical and
    Basic Investigation in Glioblastoma. GBM2015. 3(1):101. Toledo,
    Spain. September 2015. ( JuanAlbarracin2015b ) .

-    Javier Juan-Albarracín , Elies Fuster-Garcia and Juan M.
    García-Gómez. ’Non-Local Spatially Varying Finite Mixture Models for
    Image Segmentation’ . Statistics and Computing; September 2019;
    Accepted for publication. ( JuanAlbarracin2019b ) .

-    IF: 2.383 (JCR 2018): 16/123 Statistics & Probability (Q1), 31/105
    Computer Science, Theory & Methods (Q2).

-    Javier Juan-Albarracín , Elies Fuster-Garcia and Juan M.
    García-Gómez. ‘An online platform for the automatic reporting of
    multi-parametric tissue signatures: A case study in Glioblastoma’ .
    In: Crimi A., Menze B., Maier O., Reyes M., Winzeck S., Handels H.
    (eds) Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic
    Brain Injuries. BrainLes 2016. Lecture Notes in Computer Science,
    vol 10154. Springer, Cham. Athens, Greece. October 2016. (
    JuanAlbarracin2016 ) .

-    Javier Juan-Albarracín , Elies Fuster-Garcia, Alexandre
    Pérez-Girbés, F. Aparici-Robles, Ángel Alberich-Bayarri, Antonio
    Revert-Ventura, L. Martí-Bonmatí and Juan M. García-Gómez.
    ‘Glioblastoma: Vascular Habitats Detected at Preoperative Dynamic
    Susceptibility-weighted Contrast-enhanced Perfusion MR Imaging
    Predict Survival’ . Radiology; 2018; 287(3):944-954. Jun 2018. (
    JuanAlbarracin2018 ) .

-    IF: 7.608 (JCR 2018): 4/129 Radiology, Nuclear Medicine & Magnetic
    Resonance Imaging (Q1).

-    Elies Fuster-Garcia, Javier Juan-Albarracín , Germán A.
    García-Ferrando, L. Martí-Bonmatí, F. Aparici-Robles and Juan M.
    García-Gómez. ‘Improving the estimation of prognosis for
    glioblastoma patients by MR based hemodynamic tissue signatures’ .
    NMR in Biomedicine; 2018; 31(12):e4006. December 2018. (
    FusterGarcia2018 ) .

-    IF: 3.414 (JCR 2018): 5/41 Spectroscopy (Q1), 30/129 Radiology,
    Nuclear Medicine & Magnetic Resonance Imaging (Q1), 22/73 Biophysics
    (Q2).

-    María Del Mar Álvarez-Torres and Javier Juan-Albarracín and Elies
    Fuster-Garcia and Fuensanta Bellvís-Bataller and David Lorente and
    Gaspar Reynés and Jaime Font de Mora and Fernando Aparici-Robles and
    Carlos Botella and Jose Muñoz-Langa and Raquel Faubel and Sabina
    Asensio-Cuesta and Germán A. García-Ferrando and Eduard Chelebian
    and Cristina Auger and Jose Pineda and Alex Rovira and Laura Oleaga
    and Enrique Mollà-Olmos and Antonio J. Revert and Luaba Tshibanda
    and Girolamo Crisi and Kyrre E. Emblem and Didier Martin and Paulina
    Due-Tønnessen and Torstein R. Meling and Silvano Filice and Carlos
    Sáez and Juan M García-Gómez. ‘Robust association between vascular
    habitats and patient prognosis in glioblastoma: an international
    retrospective multicenter study’ . Journal of Magnetic Resonance
    Imaging; 2019; 31(12):e4006. October 2019. ( Alvarez2019 ) .

-    IF: 3.732 (JCR 2018): 26/129 Radiology, Nuclear Medicine & Magnetic
    Resonance Imaging (Q1).

-    Javier Juan-Albarracín , Elies Fuster-Garcia, Germán A.
    García-Ferrando and Juan M. García-Gómez. ‘ONCOhabitats: A system
    for glioblastoma heterogeneity assessment through MRI’ .
    International Journal of Medical Informatics; 2019; 128():53-61.
    August 2019. ( JuanAlbarracin2019a ) .

-    IF: 2.731 (JCR 2018): 57/155 Computer Science and Information
    Systems (Q2), 28/98 Healthcare Sciences & Services (Q2), 11/26
    Medical Informatics (Q2).

-    Javier Juan-Albarracín , Elies Fuster-Garcia and Juan M.
    García-Gómez. ‘MTSimaging: multiparametric image analysis services
    for vascular characterization of glioblastoma’ . The European
    Society of Magnetic Resonance in Medicine and Biology Congress.
    ESMRMB 2017. 30 (Suppl 1): S501–S692. Barcelona, Spain. October
    2017. ( JuanAlbarracin2017 ) .

-    Javier Juan-Albarracín , Elies Fuster-Garcia and María del Mar
    Álvarez-Torres and Eduard Chelebian and Juan M. García-Gómez.
    ‘ONCOhabitats glioma segmentation model’ . In: Crimi A., Menze B.,
    Maier O., Reyes M., Winzeck S., Handels H. (eds) Brainlesion:
    Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries.
    BrainLes 2019. Lecture Notes in Computer Science, vol 10154.
    Springer, Cham. Shenzhen, China. October 2019. ( JuanAlbarracin2019c
    ) .

-    Juan Ortiz-Pla, Elies Fuster-Garcia, Javier Juan-Albarracín and
    Juan M. García-Gómez. ‘GBM Modeling with Proliferation and Migration
    Phenotypes: A Proposal of Initialization for Real Cases’ . In:
    Tsaftaris S., Gooya A., Frangi A., Prince J. (eds) Simulation and
    Synthesis in Medical Imaging. SASHIMI 2016. Lecture Notes in
    Computer Science, vol 9968. Springer, Cham. Athens, Greece. October
    2016. ( OrtizPla2016 ) .

#### 1.3.3 Software

The research conducted in this thesis has led to the creation of
ONCOhabitats platform ( https://www.oncohabitats.upv.es ). ONCOhabitats
is an online professional system for glioblastoma analysis using MRI ,
which encapsulates all the original methods and algorithms developed in
this thesis, and several state-of-the-art algorithms for medical image
analysis. Preliminary versions of some methods of ONCOhabitats were
first registered in the technological catalogue of the UPV under the
acronym CURIAM BT+, while the final updated complete version of the
ONCOhabitats system has been recently registered as an asset of high
value for the technological offer of the UPV .

-    Javier Juan-Albarracín , Elies Fuster-Garcia, Juan M. García-Gómez,
    Carlos Sáez, Montserrat Robles and Miguel Esparza. ‘R-16874-2014 -
    Caracterización de firmas biológicas de glioblastomas (CURIAM BT+)’
    . CARTA Registry of the Universitat Politècnica de València.
    28/02/2014.

-    Javier Juan-Albarracín , Elies Fuster-Garcia, Juan M. García-Gómez.
    ‘R-XXXXX-2019 - Detección de hábitats para evaluación de
    heterogeneidad biológica de glioblastomas (ONCOhabitats)’ . CARTA
    Registry of the Universitat Politècnica de València. In process.

#### 1.3.4 Clinical studies

The aforementioned ONCOhabitats platform is currently enrolled in an
international multicenter observational restrospective clinical study
registered at ClinicalTrials.gov from the U.S. National Library of
Medicine. The aim of the study is to validate the prognostic
capabilities of the HTS habitats for patients affected with
glioblastoma. To this end, the primary and secondary outcomes fixed for
the clinical study are: the “correlation between overall survival and
progression-free survival (in days) of patients undergoing
standard-of-care treatment and the tumor vascular heterogeneity
described by the four habitats obtained by the HTS biomarker” . The
clinical study involves data collected from 7 international hospitals,
with a total of 305 patients recruited since 1st of January of 2012
until February of 2018.

-    Multicenter Retrospective Observational Clinical Study NCT03439332
    . ‘Multicentre Validation of How Vascular Biomarkers From Tumor Can
    Predict the Survival of the Patient With Glioblastoma
    (ONCOhabitats)’ . https://clinicaltrials.gov/ct2/show/NCT03439332 .
    Universitat Politècnica de València ( UPV ). 20/02/2018.

#### 1.3.5 Patents

The know-how in medical image analysis generated by the author in this
thesis was protected under the international patent “Method and system
for generating multi-parametric nosological images” . The patent is
currently registered in Spain (ES201431289A) with previous exam, and was
extended to Europe (EP3190542A1) and United States (US20170287133A1)
following the PCT procedure for patent internationalization. It
describes a procedure based on multi-parametric medical images to
generate nosological masks capable of describing the underlying
physiological processes taking place in the lesion. The patent
materializes the interest that the scientific research conducted in this
thesis has originated in both the academic and business spheres.
Moreover, it represents the efforts of this thesis to generate tangible
assets for a later phase of business development.

-    Javier Juan-Albarracín , Elies Fuster-Garcia, Juan M. García-Gómez,
    Miguel Esparza-Manzano, Jose V. Manjón-Herrra, Monserrat
    Robles-Viejo, Carlos Sáez. ‘Method and system for generating
    multi-parametric nosological images’ . Asignee: Universitat
    Politècnica de València ( UPV ).

-    ES201431289A: Oficina Española de Patentes y Marcas. 05/09/2014.
    Legal status: Active.

-    EP3190542A4: European Patent Office. PCT/ES2015/070584. 28/07/2015.
    Legal status: Active.

-    US9990719B2: United States Patent and Trademark Office.
    PCT/ES2015/070584. 28/07/2015. Legal status: Active.

#### 1.3.6 Transference

The experience, knowledge and original ideas conceived in this thesis,
together with the issuance of the patent, aroused the author’s interest
in taking a step beyond the academic field. This led the author and the
thesis advisors to the conceptualization of a business plan to
capitalize the results obtained in the thesis. In this sense, we
participated in the EIT Health Headstart Proof of Concept 2016
programmme, in which we were awarded the best business plan for a
biotech start-up; and in the CaixaImpulse programme for facilitating
entrepreneurship in biomedicine. The experience obtained in these
programmes in conjunction with the background of the advisors in
generating spin-offs of the UPV , finally led to the foundation of
ONCOANALYTICS CDX company. ONCOANALYTICS CDX is formed by a
multi-disciplinary team made up of computer scientists, physicists,
oncologists, biomedical engineers and financial experts, with a total of
6 UPV graduates and 4 Phd. Supported by the aforementioned ONCOhabitats
platform, the company is focused in developing image-based CDx for
glioblastoma, to facilitate patient stratification during the clinical
trial of a drug.

-    Javier Juan-Albarracín , Elies Fuster-Garcia, Juan M. García-Gómez,
    Germán A. García-Ferrando, Carlos Vidal-Trujillo, José Muñoz-Langa,
    David Lorente-Estellés, Ana González-Segura, Fuensanta
    Bellvís-Bataller. ‘ONCOANALYTICS CDX S.L.’ . Commercial name:
    Texture CDx. CIF: B98981889. 01/03/2018.

### 1.4 Projects and partners

During the development of this thesis the author has actively
participated in several national, European and private research projects
in collaboration with several hospitals and clinical institutions. The
projects related to this thesis are listed below:

###### Curiam Bt+

Caracterización de firmas biológicas de glioblastomas mediante modelos
no-supervisados de predicción estructurada basados en biomarcadores de
imagen. Funded by the Spanish Ministry of Economy and Competitiveness
(TIN2013-43457-R, 2014-2016).

Objectives : This project aims to develop a computational medical
imaging system to obtain radiological profiles of the different areas of
the tumor, to accurately measure the vascular properties of the
glioblastoma. Such profiles will also provide information about the
tumor grading and the expected survival of the patient.

Partners : Biomedical Data Science Laboratory ( BDSLab )-ITACA group of
the Universitat Politècnica de Valencia (Valencia, Spain). Hospital
Universitario y Politécnico La Fe ( HUPLF ) (Valencia, Spain).

###### MTS4Up

Biomarcadores dinámicos basados en firmas tisulares multiparamétricas
para el seguimiento y evaluación de la respuesta a tratamiento de
pacientes con glioblastoma y cáncer de próstata. Funded by the Spanish
National Research Agency (DPI2016-80054-R, 2017-2018).

Objectives : This project extends the TIN2013-43457-R project by
improving the technology to obtain radiological signatures of the
glioblastoma incorporating diffusion MRI to describe not only tumor
vascularity, but also the cell density properties of the tissues. Such
improvements will allow an early evaluation of tumor progression and an
accurate assessment of the patient’s response to treatment. Finally, the
technology will also be evaluated on other pathologies such as prostate
tumor to measure the versatility of the methodology in other solid
tumors.

Partners : Biomedical Data Science Laboratory ( BDSLab )-ITACA group of
the Universitat Politècnica de Valencia (Valencia, Spain).

###### Glio-Markers

Estudio integrado de biomarcadores moleculares y de imagen en pacientes
con glioblastoma. Funded by the Universitat Politècnica de València and
Hospital Universitario y Politécnico La Fe (Prueba de Concepto 2015,
UPV-FE-15-B, 2015-2016).

Objectives : This project aims to combine and integrate the analysis of
glioblastoma biomarkers from three different physiological areas: blood
circulating proteins, immunohistologic biomarkers and MRI biomarkers.
The purpose is to develop predictive models for response to treatment
assessment and measuring tumor progression, as well as finding
correlations between imaging biomarkers and circulating proteins.

Partners : Biomedical Data Science Laboratory ( BDSLab )-ITACA group of
the Universitat Politècnica de Valencia (Valencia, Spain), Universitat
Politècnica de València ( UPV ) (Valencia, Spain).

###### Dssradioplan

Inclusión de las tecnologías de firma tisular y modelos mutiescala para
el soporte a la planificación de la radioterapia en el tratamiento del
glioblastoma. Funded by the Universitat Politècnica de València and
Hospital Universitario y Politécnico La Fe (Prueba de Concepto 2016,
UPV-FE-16-B, 2016-2017).

Objectives : The main purpose of this project is to plan and carry out
the necessary actions to evaluate the applicability and added value of
the HTS technology to provide clinical decision support in the
management and planning of radiotherapy in patients affected by
glioblastoma.

Partners : Biomedical Data Science Laboratory ( BDSLab )-ITACA group of
the Universitat Politècnica de Valencia (Valencia, Spain), Universitat
Politècnica de València ( UPV ) (Valencia, Spain).

###### Multibioim

Multiparametric nosological images for supporting clinical decisions in
solid tumors. Funded by the EIT Health E.V. (Proof of Concept 2016,
POC-2016-SPAIN-07, 2016-2017).

Objectives : This project aims to develop a Proof of Concept (PoC) of
the patented procedure ES201431289A for generation of multiparametric
tissue signatures based on structural and functional MRI for solid
tumors. This PoC aims to solve specific technical, strategical, legal,
and commercial barriers to generate a reliable business model for the
medical image analysis software market and convert a cutting-edge
technology into a clinically validable product.

Partners : BDSLab -ITACA group of the Universitat Politècnica de
Valencia (Valencia, Spain).

The projects on which the author was involved in previously and in
parallel to the development of this thesis are listed as follows:

###### Dqv-Mineco

Servicio de evaluación y rating de la calidad de repositorios de datos
biomédicos. Funded by the Spanish Ministry of Economy and
Competitiveness (Retos-Colaboración 2013 programme, RTC-2014-1530-1,
2013-2016).

Objectives : This project aims to define a data quality evaluation and
rating service to assure the data value aimed to its reuse in clinical,
strategic and scientific decision making. It will be based on two
software services. The first will evaluate nine data quality dimensions.
The second will generate a data quality rating positioning the evaluated
datasets according to several reuse knowledge extraction purposes.

Partners : VeraTech for Health S.L. (Valencia, Spain) and IBIME-ITACA
group of the Universitat Politècnica de Valencia, (Spain)

###### Help4mood

A Computational Distributed System to Support the Treatment of Patients
with Major Depression. Funded by the European Commission. VII Framework
Program (FP7-ICT-2009-4; 248765, 2011-2013).

Objectives : This project focuses on major depression disease. Patients
with major depression typically recover through antidepressant drugs,
psychological therapy or hospitalization. However, it has been shown
that in many situations such recovery is either slow or incomplete.
Research shows that psychological therapies can be delivered effectively
without face to face contact at individual’s home by computerized
cognitive behavioral therapy. The project aims to advance the
state-of-the-art in computerized support for people with major
depression by monitoring mood, thoughts, physical activity and voice
characteristics, by means of intelligent systems based on virtual agent.

Partners : BDSLab -ITACA group of the Universitat Politècnica de
Valencia, (Spain), University of Edinburgh (United Kingdom), Fundació
I2CAT (Spain), Universitatea Babes Bolyai (Romania), FVA SAS di Louis
Ferrini (Italy), OBS Medical Ltd. (Italy), Universitat Politècnica de
Catalunya, (Spain), Heriot-Watt University (United Kingdom).

### 1.5 Thesis outline

The thesis is structured in eight chapters that thoroughly describe the
research work carried out during the thesis. The Chapter 1 has
introduced the motivations, research objectives and main contributions.
Chapter 2 describes the thesis rationale, introducing the clinical
problems addressed as well as the theoretical background needed to
complement the description of the methods developed in the thesis.
Chapter 3 presents a preliminary study on the viability of the
unsupervised learning paradigm to identify and delineate pathological
tissues in glioblastomas based on MRI patterns. Chapter 4 presents the
mathematical development of a new unsupervised structured learning
algorithm for image segmentation, and a comparison of its performance
against alternative approaches. Chapter 5 introduces the HTS method: an
unsupervised learning method based on perfusion MRI to delineate
vascular habitats within the glioblastoma to assess its vascular
heterogeneity. An study on the association of the vascular habitats and
the patient OS is presented. Chapter 6 describes the international
multi-center validation of the HTS method, under the framework of the
observational retrospective clinical study NCT03439332 . The validation
of the association of the vascular habitats and the patient OS , as well
as the stratification capabilities of the HTS habitats is presented.
Chapter 7 presents ONCOhabitats platform (
https://www.oncohabitats.upv.es ). ONCOhabitats encapsulates all the
work conducted in this thesis in a public open-access platform, offering
medical image analysis services to analyze both the morphological and
vascular heterogeneity of the glioblastoma. Finally, chapter 8 ends this
dissertation with the concluding remarks and recommendations to continue
with the research developed in this thesis.

Figure 1.1 outlines the thesis contributions structured among the thesis
chapters, along with the publications, research projects, transfer
actions, patents and the software developed during this study.

## Chapter 2 Rationale

This chapter describes the thesis rationale divided in five sections.
First, the glioblastoma tumor is introduced, describing its
epidemiology, etiology, biologic behavior, morphological features,
diagnosis and treatment. Second, MRI technique is disclosed,
illustrating their physical mechanisms, theoretical foundations and
acquisition protocols and sequences. Third a general review on the
theoretical background probability and statistical parameters estimation
recommended for the understanding of the methods developed in the thesis
is provided. Fourth, an in-depth explanation of Finite Mixture Model and
Spatially Varying Finite Mixture Model and their parameter estimation is
presented to lay the foundation for many of the methods developed in
this thesis. Finally, a general review of the DL paradigm, as well as a
revision of Artificial Neural Network s , Convolutional Neural Network s
and the back-propagation mechanics is provided. This review is intended
to establish a common basis to complement the descriptions of background
and methods explained in the following chapters.

### 2.1 Glioblastoma

The first recorded clinical report identifying glioblastoma as a tumor
originating from neuroglial cells dates back to 1863 by virchow2018 .
Since then, enormous progress has been made in the understanding of this
neoplasm thanks to an exhaustive multidisciplinary research in the
clinical, pathological, radiological, molecular and genetic aspects of
the tumor. Such efforts have led nowadays to a detailed description of
the glioblastoma that has given us crucial, but still not sufficient,
information to design successful treatments for this disease.

Glioblastoma is a grade IV World Health Organization ( WHO ) deadly
primary brain tumor considered the most aggressive neoplasm of the
central nervous system. It is the most frequent and malignant
astrocytoma in humans, accounting for more than 60% of all brain tumors
in adults. Glioblastoma has a global incidence of 4.67 to 5.73 per
100000 people and presents a poor prognosis of 14-15 months despite
aggressive treatments. Although it can debute at any age, more than the
70% of the cases are seen in patients between the ages of 45 and 70.
Likewise, the incidence in males is 1.6 higher than in females and it is
2 times higher in Caucasians than in other races ( Tamimi2017 ) .

Glioblastomas are infiltrative and deeply invasive heterogeneous masses
characterized by hypercellularity, pleomorphism, microvascular
proliferation and high necrosis mitotic activity ( Gladson2010 ) .
Typically, glioblastoma exhibits diffuse margins with co-existence of
different tissues including active tumor, cysts, necrosis and edema; all
of them exhibiting a high variability related to the aggressiveness of
the neoplasm ( Hardee2012 ) . Strong vascular proliferation, robust
angiogenesis, and extensive microvasculature heterogeneity are major
pathological hallmarks that differentiate glioblastomas from low-grade
gliomas ( Kargiotis2006 ) .

Figure 2.1 shows an example of a brain affected by glioblastoma. Note
the deep invasive ability of the tumor, evidenced by the tumor foci
crossing the midline towards the opposite hemisphere of the largest
mass.

Heterogeneity has therefore been considered crucial to understand the
aggressiveness of this tumor and its resistance to effective therapies.
Glioblastoma heterogeneity manifests itself at both macroscopic and
microscopic levels. At the macroscopic level, co-existance of an amalgam
of blended malignant tissues including enhancing tumor, non-enhancing
tumor, hemorrhage, cyst, inflammation, necrosis or edema, results in a
chaotic mass highly complicated to manage clinically. At the microscopic
level, different glioblastoma molecular sub-types and genetic
alterations have been discovered in the past years. In 2010, Verhaak2010
established a classification for glioblastoma into four sub-types
associated to mutations in EGFR, TP53, NF1 and PDGFRA/IDH1 genes: the
classical, mesenchymal, neural and proneural sub-types. The main
characteristics of these tumor sub-types are summarized in Table 2.1 .

In 2016, the WHO revisited the official classification for glioblastoma
sub-types, distinguishing between two groups: the IDH-wildtype (90% of
cases) and the IDH-mutant (10% of cases), which are closely related to
primary and secondary glioblastomas respectively. Table 2.2 summarizes
the most relevant aspects of the IDH-wildtype and IDH-mutant
glioblastomas.

The study of these transcriptional subtypes has yielded relevant
findings such as significant correlation with patient prognosis (
Parsons2008 ) . IDH-mutant glioblastomas show a significant improvement
in OS with a median survival of 31 months, with respect to IDH-wildtype
glioblastomas that present a median survival of 15 months Louis2016 .
However, although tumor subtypes tend to correlate with relevant
clinical outcomes, the degree of correlation is often moderate and
contradictory studies constantly appear with confronted conclusions (
Akagi2018 ) . Moreover, survival rates have shown no notable improvement
since the last three decades ( Stupp2005 ) , so alternative approaches
are required to study the glioblastoma heterogeneity and its association
with the tumor evolution.

In this sense, significant interest has been placed recently in the
analysis of glioblastoma heterogeneity through medical imaging. The
ability to discover non-invasive markers associated with tumor
sub-types, OS , Progression Free Survival ( PFS ) or response to
treatment has received much attention as it may help in improving
clinical decision making at an early stage of the disease. In this
sense, MRI emerged as one of the most reliable tool for quantifying
in-vivo non-invasive imaging features capable to accurately describe the
heterogeneity of the glioblastoma.

### 2.2 Magnetic Resonance Imaging

Magnetic Resonance Imaging ( MRI ) is a medical imaging technique used
to provide in-vivo internal representations of the human body. This
technique was developed in the decade of 1970 by the professors
Damadian1971 ; Lauterbur1973 ; Lauterbur1974 ; Mansfield1977 . Although
Damadian’s work on the Nuclear Magnetic Resonance ( NMR ) relaxation of
different tissues laid the groundwork for many further developments in
MRI , it was Paul Lauterbur who finally developed a reliable technique
based on gradient magnets to generate the first 2D and 3D Magnetic
Resonance ( MR ) images of the interior of the human body. A few years
later Peter Mansfield developed a mathematical formulation that
dramatically accelerated the acquisition of MR images (seconds rather
than hours), making it a practical technique for the clinical routine.
Paul Lauterbur and Peter Mansfield finally awarded the Nobel prize in
2003 for their contributions and advances in MRI .

MRI is based on the magnetic properties of the atomic nuclei,
specifically on the spin angular momentum of the hydrogen nucleus @xmath
. At a resting natural state, all the hydrogen @xmath nucleus in the
human body spin randomly, thus canceling the angular momentum each other
and producing an overall zero spin magnetic momentum. Under the
influence of an external uniform magnetic field @xmath , the @xmath
nuclei align their spin with @xmath in a parallel (low energy) or
anti-parallel (high energy) state, producing an overall spin magnetic
momentum @xmath , with @xmath the direction of @xmath (see figure 2.2 ).
The influence of @xmath also makes the @xmath nuclei to precess at a
specific frequency (denominated the Larmor frequency), which depends on
the strength of @xmath and the gyromagnetic properties of the hydrogen
nucleus.

The NMR phenomena is related to the excitation produced to the @xmath
nuclei by the influence of an additional temporal magnetic field
(radio-frequency pulse) with a direction different from @xmath . When a
radio-frequency pulse at the Larmor frequency is triggered at @xmath ,
an energy exchange occurs with some @xmath nuclei, lifting them to the
high energy anti-parallel state. In addition, a synchronization of the
precession of all the @xmath nuclei is induced, so that they all begin
to precess in phase. This event causes two magnetic effects in the
system: 1) a decrease in the overall spin magnetic momentum @xmath due
to the lifted @xmath nuclei; and 2) the apparition of a spin magnetic
momentum @xmath transverse to the @xmath field due to the phase
coherence precession. Such transverse magnetization @xmath generates a
magnetic signal called the Nuclear Magnetic Resonance Signal . Figure
2.3 shows a diagram of the effect produced by the radio-frequency pulse
on the spin angular momentum of the @xmath nuclei.

Once the radio-frequency pulse has ended, the system begins to relax,
recovering its initial state of equilibrium. The precession of the
@xmath nuclei begins to lose the phase coherence due to the differences
in the chemical context of each @xmath nucleus. Therefore, the
transverse spin magnetic momentum @xmath begins to disappear, leading to
the so-called spin-spin relaxation or transverse T @xmath relaxation
process. Likewise, the previously lifted @xmath nuclei return to their
original low energy parallel state, restoring the longitudinal
magnetization @xmath to its original value. This process is called the
spin-lattice relaxation or longitudinal T @xmath relaxation (see figure
2.4 ).

T @xmath and T @xmath relaxation times are significantly different
between them, with T @xmath a longer process than T @xmath . Generally,
T @xmath relaxation time ranges from 300 to 2000 ms while T @xmath
relaxation is about 30 to 150 ms. Moreover, it is difficult to determine
the end of the T @xmath and T @xmath relaxations exactly. Therefore, it
is considered that the T @xmath relaxation is completed once the signal
recovers the 63% of the original @xmath magnetization. Similarly, the T
@xmath relaxation is considered ended once the @xmath signal falls under
the 37% of its original value.

As stated above, the T @xmath and T @xmath relaxation is different for
each @xmath nucleus depending on its chemical context. Hence, different
NMR signals are emitted during the relaxation process, inducing
dissimilar electric signals in the receiver coils of the MR machine.
Such disparities will in essence determine the intensities for each
tissue in the MR image. There are three different types of contrast in
MR images: T @xmath -weighted, T @xmath -weighted and Proton Density (
PD ) images; which are related to the so-called Repetition Time ( TR )
and the Echo Time ( TE ) times. TR is the time between successive
radio-frequency pulses and affects the speed in which @xmath nuclei
realigns to the @xmath field. The TE refers to the time at which the
electrical signal induced by the @xmath nuclei is measured in the
magnetic coils and concerns the degree of dephase of the spins of the
@xmath nuclei. Thus, the TR is closely connected to T @xmath relaxation
effects, while TE is more related to T @xmath relaxation events.

Figure 2.5 shows the relation between the TR , TE and the contrast
produced on the MR images.

#### 2.2.1 Anatomical Mri

So far we have seen that NMR relaxation process is different for each
tissue depending on its chemical context. This property allows to
visually differentiate human tissues via the measurement of their
associated electric signals, generating images that depict the
anatomical structures of the region under study.

In the case of central nervous system tumors such as glioblastoma,
several images are typically acquired to best approach the lesion. These
images intend to anatomically describe the morphology of the lesion and
its associated structures.

A common protocol for a MRI glioblastoma study should include the
following sequences:

  T @xmath -weighted  

    : typically a volumetric 3D Fast Spin Echo ( FSE ) or
    Magnetization-Prepared Rapid Acquisition with Gradient Echo ( MPRAGE
    ) sequence. The aim of the sequence is the anatomical overview of
    the lesion, including the soft tissues below the base of skull.

  T @xmath -weighted  

    : typically a post-contrast volumetric 3D FSE or MPRAGE sequence. It
    is a T @xmath -weighted image acquired with administration of
    Gadolinium-Based Contrast Agent ( GBCA ). Its main purpose is to
    assess vascular structures of the region under study. The list of
    approved contrast-agents for human use is: gadoterate meglumine
    (Dotarem ^(®) ), gadobutrol (Gadavist ^(®) ), gadopentetate
    dimeglumine (Magnevist ^(®) ), gadobenate dimeglumine (MultiHance
    ^(®) ), gadodiamide (Omniscan ^(®) ), gadoversetamide (OptiMARK ^(®)
    ), gadoteridol (ProHance ^(®) ).

  T @xmath -weighted  

    : typically a 2D axial FSE sequence. Its purpose is the evaluation
    of basal cisterns, ventricular system and subdural spaces,
    evaluation of vasogenic edema and good visualization of flow-voids
    in vessels.

  FLAIR  

    : typically a 2D axial Fluid Attenuation Inversion Recovery ( FLAIR
    ) T @xmath -weighted sequence. It is a special inversion recovery
    sequence with a long inversion time. Its main characteristic is that
    it suppress the signal from the cerebrospinal fluid so that it
    appears similar to a T @xmath -weighted image but with the
    cerebrospinal fluid dark instead of bright. Its purpose is the
    assessment of white-matter tumor involvement and related vasogenic
    edema.

Figure 2.6 shows an example of the aforementioned MR images for a case
affected by glioblastoma.

#### 2.2.2 Quantitative Mri

Quantitative Magnetic Resonance Imaging ( qMRI ) emerged in the 1980’s
in the attempt to measure the biological MR properties of the tissues (
Tofts2004 ) . Therefore, its purpose is to turn the MRI into a
scientific measurement instrument and not only on an image acquisition
system. This concept-shift is the essence of qMRI . qMRI attempts to
develop quantitative and reproducible techniques based on MR data, able
to measure biomarkers related to the underlying biological processes of
the tissues ( Yankeelov2011 ) . MRI Relaxometry ( Deoni2010 ) ,
Diffusion Weighted Imaging ( DWI ) ( Schaefer2000 ) and Perfusion
Weighted Imaging ( PWI ) ( Svolos2014 ) composes some of the most
important techniques of qMRI .

Regarding glioblastoma tumors, one of the most relevant qMRI techniques
is PWI . As previously mentioned, glioblastoma tumors are characterized
by a strong vascular proliferation, robust angiogenesis, and extensive
microvasculature heterogeneity. In this sense, PWI allows the
measurement of the kinetic properties of a paramagnetic contrast agent,
which is intravenously injected to the patient. PWI biomarkers are able
to reveal the local vascular properties of the tissues and their
hemodynamic behavior. There are three main PWI techniques: Dynamic
Susceptibility Contrast ( DSC ) perfusion, Dynamic Contrast Enhanced (
DCE ) perfusion and Arterial Spin Labeling ( ASL ) perfusion. For
reasons of simplicity, and given that only DSC perfusion was used in the
development of this thesis, DCE and ASL perfusion quantification will
not be covered in this dissertation.

##### Dsc perfusion Mri

DSC is the most frequently used technique for MRI perfusion of the
brain. It relies on the susceptibility-induced signal loss on T @xmath
-weighted sequences caused by the pass of a GBCA bolus through a
capillary bed. A rapid repeated image acquisition is performed during
the bolus injection, resulting into a series of images with the signal
at each voxel representing the susceptibility-induced signal loss of the
corresponding tissue, which is proportional to the amount of contrast
present in the microvasculature. A mathematical model is then fit to the
intensity-time signals to derive various perfusion parameters. The most
commonly calculated parameters are relative Cerebral Blood Volume ( rCBV
), relative Cerebral Blood Flow ( rCBF ) and Mean Transit Time ( MTT ).

Let @xmath the susceptibility-induced signal loss observed at a given
voxel. To quantify the rCBV , rCBF and MTT parameters, a GBCA
concentration-time conversion of @xmath must be performed. It is
normally assumed that the tissue concentration of the contrast agent is
proportional to the change in T @xmath relaxation rate, i.e. @xmath , by
means of:

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

where @xmath indicates the time, @xmath refers to the echo time of the
DSC sequence and @xmath refers to the intensity baseline of @xmath .

Hence, DSC quantification allows to calculate the amount of GBCA tracer
remaining in the tissue at each time step at which the @xmath is
measured. This is described by the so-called tissue impulse residue
function, which is denoted as @xmath . Therefore, @xmath represents the
fraction of the tracer still present in the tissue at time @xmath after
an instantaneous infinitesimal injection (i.e. a Dirac-delta function)
of tracer into a tissue-feeding artery ( Knutsson2010 ) :

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

where @xmath is the distribution of the capillary transit time. By
definition, it is considered that @xmath , meaning that all the tracer
is present in the tissue at @xmath , and @xmath , meaning that the
tracer lefts the tissue after a sufficiently long time (assuming an
intact blood–brain barrier with no extravasation).

In practice, however, the arterial tracer bolus arrives at the tissue
with a delay dependent on the circulatory path from the injection site
to the tissue of interest. Consequently, the measured @xmath signal in
the tissue does not reflect the response to an instantaneous arterial
input bolus, but to the convolution of a kernel given by @xmath and the
@xmath signal in the tissue-feeding artery, i.e. the so-called Arterial
Input Function ( AIF ) ( Rempp1994 ) :

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

where @xmath stands for the convolution product and CBF is the Cerebral
Blood Flow.

From equation 2.3 we can rapidly observe that knowing the AIF , the CBF
can be determined by deconvolution of the peak height of the @xmath
signal, given that @xmath .

Dozens of models has been proposed in the literature to successfully
estimate the CBF by deconvolution: fast truncated Singular Value
Decomposition ( SVD ) family (e.g. s SVD , c SVD , o SVD (
Ostergaard1996 ; Wu2003 ; Zanderigo2009 ) ), nonlinear stochastic
regularization ( Zanderigo2009 ; Pillonetto2010 ) , Fourier-Hunt
frequency-domain deconvolution ( Ostergaard1996 ; Chen2005 ) , wavelet
thresholding ( Connelly2006 ) , classical Tikhonov regularization (
Ostergaard1996 ; Zanderigo2009 ; Calamante2003 ) , maximum likelihood
estimation ( Vonken1999 ) , maximum entropy deconvolution ( Drabycz2005
) , Gaussian process deconvolution ( Andersen2002 ) , or hierarchical
Bayesian models ( Schmid2007 ; Schmid2011 ) .

In this thesis, the delay-insensitive o SVD method was employed to
estimate the CBF, as it has demonstrated to be robust to differences in
tracer arrival times and to AIF selection variability. Following this
approach, the deconvolution problem can be formulated as a matrix
equation of the form @xmath :

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

The above equation can be solved for @xmath , with @xmath . By
decomposing @xmath , with @xmath and @xmath orthogonal matrices and
@xmath a non-negative square diagonal matrix, the inverse of @xmath can
be expressed as @xmath where @xmath along the diagonal only where values
are higher than a threshold (truncation of the diagonal for numerical
stability reasons), and zero elsewhere. Therefore, we can estimate
@xmath by solving for @xmath and calculating its maximum value:

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

This procedure assumes that the @xmath always arrive before the voxel
signal @xmath , which in some situations may not be true (for example if
the @xmath is not the true @xmath of the corresponding tissue). In such
situations, the computation of the @xmath results in an incorrect
estimation, typically an underestimation of the @xmath , due to a
shifting in the @xmath function.

As proposed by Wu2003 circular deconvolution can be employed to solve
this problem. By zero-padding the @xmath point-time series @xmath and
@xmath to length @xmath , with @xmath , time aliasing can be avoided.
Thus, replacing also the matrix @xmath by a block-circulant matrix
@xmath of the form:

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

and solving using the equation 2.5 , the @xmath can be estimated safely
also even if delay effects exist in the @xmath .

The MTT can be calculated by Zierler’s area-to-height relationship (
Zierler1962 ) , by the equation:

  -- -------- -- -------
     @xmath      (2.7)
  -- -------- -- -------

According to the central volume theorem ( Meier1954 ; Weisskoff1993 ) ,
the CBV can be calculated following the relation:

  -- -------- -- -------
     @xmath      (2.8)
  -- -------- -- -------

Following this relation and combining equations 2.3 and 2.7 , the CBV
can also be estimated by:

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

Leakage correction

Computational kinetic models for DSC quantification explicitly assume
that the GBCA remains in the intra-vascular space for the duration of
the perfusion acquisition. However, this assumption is typically not
valid for glioblastomas, since they often present blood-brain barrier
breakdown. Therefore, GBCA extravasates to the extra-vascular
interstitial space producing a distortion in the @xmath signal that
leads to an overestimation or underestimation of perfusion parameters if
non leakage correction is performed. GBCA leakage can be manifested in
@xmath signals in generally two ways: the T @xmath -dominant
extravasation and the T @xmath -dominant extravasation effects.

Numerous methods have been proposed in the literature to correct the
leakage effect in perfusion, and the deviation it produces in the
estimation of the perfusion markers: use of pre-dosing ( Donahue2000 ) ,
double-echo acquisitions ( Vonken2000 ; Uematsu2001 ) or parametric
modeling ( @xmath -variate fit of the @xmath signals) ( Thompson1964 ) .
The most common and reference technique is the one proposed by
Weisskoff1994 and later elaborated by Boxerman2006 , which corrects the
leaky signal by estimating its deviation from a non-leaky reference
signal of the same patient.

Following Boxerman’s method, the @xmath observed at a given voxel can be
determined as:

  -- -------- -- --------
     @xmath      (2.10)
  -- -------- -- --------

where @xmath stands for a whole-brain average signal in non-enhancing
voxels, @xmath is a scale-constant of the whole-brain average signal to
fit the observed signal at the corresponding voxel, and @xmath is the
term that reflects the effects of the leakage (both T @xmath and T
@xmath dominant effects). Therefore, by simple manipulation (please
refer to Boxerman2006 for more details), the leakage corrected signal
can be determined as:

  -- -------- -- --------
     @xmath      (2.11)
  -- -------- -- --------

Both @xmath and @xmath can be determined by simple least squares
fitting, and then corrected @xmath signals can be computed to quantify
@xmath , @xmath and @xmath as stated above. Figure 2.7 shows an example
of the Boxerman’s correction of a T @xmath - and a T @xmath -leaky
curve.

Recirculation correction

DSC quantification involves the kinetic analysis of the first pass of a
intravenously injected GBCA . However, the @xmath signal observed at
each voxel primarily reflects the first pass of the tracer, but also the
second wave of GBCA that recirculates into the tissue, after it has been
shunted through the renal and coronary circulations and back into the
heart. This second wave, affects the @xmath , preventing the signal from
returning to its original baseline.

Hence, in order not to overestimate the perfusion parameters, typically
the @xmath , which is obtained from the integral or area under the
@xmath signal, the recirculation phase must be corrected. One of the
most popular methods is to use a @xmath -variate fitting.

The @xmath -variate function commonly employed to describe the first
pass of the GBCA is written as:

  -- -------- -- --------
     @xmath      (2.12)
  -- -------- -- --------

where @xmath is a scaling factor, @xmath is the bolus arrival time, and
@xmath and @xmath determine the shape and scale of the distribution.

Levenberg-Marquardt algorithm was employed in this thesis to perform the
@xmath -variate fitting of all the @xmath signals of the DSC perfusions.
Levenberg-Marquardt algorithms searches for minimizing parameters in the
form:

  -- -------- -- --------
     @xmath      (2.13)
  -- -------- -- --------

Figure 2.8 shows an example of a @xmath -variate fitting of a @xmath
signal.

Contralateral white matter normalization

Finally, the rCBV and rCBF parameters are obtained by normalizing the
CBV and CBF quantities against the contralateral unaffected white matter
value. This procedure has shown to successfully standardize perfusion
values among individuals, regardless of their MRI protocol, hence
allowing inter-patient multi-center studies ( Alvarez2019 ) .

### 2.3 Probability theory

#### 2.3.1 Basic concepts

In the following sections, the basic concepts of probability theory will
be described to facilitate the reading and understanding of subsequent
chapters and contributions of this thesis.

##### Random variables

A random variable can be informally defined as a variable whose possible
values are the outcomes of a random phenomenon. Thus, random variables
conceptually represent an abstraction that allows addressing uncertainty
in the measurement of quantities in real world scenarios.

More formally, a random variable @xmath is mathematically defined as a
measurable function from a possible set of outcomes @xmath to a
measurable space @xmath . Random variables can be of two types: discrete
random variables, whose domain is a countable number of distinct values
(typically @xmath ); and continuous random variables, whose domain takes
an infinite number of values in a predefined interval (typically @xmath
). Usually, random variables are denoted by uppercase letters, e.g.
@xmath and their realizations by lowercase letters, e.g. @xmath . Hence,
for example, a random variable that represents the volume of the brain
in cubic centimeters in a population could denoted as @xmath , and the
particular measurement of a patient within this population could be
denoted as @xmath cm ³ .

Random variables are usually governed by probability distributions that
represent the likelihood that any of the possible values that the
variable can take would occur. Such distributions constitute a
fundamental principle to ML as they represent the mathematical
approximation to deal with the uncertainty in the process of learning
patterns from real data.

##### Probability distribution

A probability distribution is a mathematical function that describes the
likelihood of obtaining the possible values that a random variable can
assume. Given a random variable @xmath , the probability that it takes
the value @xmath is denoted as:

  -- -------- -- --------
     @xmath      (2.14)
  -- -------- -- --------

where for the shake of simplicity we can assume @xmath .

Probability distributions are generally divided into two classes
depending on the type of the random variable. For discrete random
variables, the probability distribution is known as probability mass
function ( pmf ) , denoted as @xmath , and gives the probability mass
that the random variable @xmath takes exactly the value @xmath .

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

Since the probability mass is distributed among all the possible
outcomes that the discrete random variable can take, then:

  -- -------- -- --------
     @xmath      (2.16)
  -- -------- -- --------

The cumulative distribution function ( cdf ) , denoted as @xmath , is
defined as the probability that the random variable @xmath takes values
less than or equal to @xmath , and takes the form:

  -- -------- -- --------
     @xmath      (2.17)
  -- -------- -- --------

On the other hand, the probability distribution of a continuous random
variable is known as probability density function ( pdf ) , denoted also
as @xmath . Since for continuous random variables there is an infinite
number of values in any interval, the pdf for an exact specific value is
not meaningful. Instead, the pdf for a continuous random variable is
commonly defined over an interval @xmath in the form:

  -- -------- -- --------
     @xmath      (2.18)
  -- -------- -- --------

For the shake of simplicity, we can assume that for continuous random
variables @xmath refers to the pdf of @xmath bounded to the
infinitesimal interval @xmath :

  -- -------- -- --------
     @xmath      (2.19)
  -- -------- -- --------

Likewise the discrete case, the probability density is distributed among
all the interval of possible values that the continuous random variable
can take, which in the most general case is @xmath , thus:

  -- -------- -- --------
     @xmath      (2.20)
  -- -------- -- --------

The cdf for a continuous random variable is finally defined as:

  -- -------- -- --------
     @xmath      (2.21)
  -- -------- -- --------

##### The joint, marginal and conditional probability distributions

In many situations we will manage more than one random variable that
operates on the same probability space. In that situations, the joint,
marginal and conditional probability distributions arise to model
uncertain events from different interrelated events.

The joint probability distribution, denoted as @xmath , is defined as
the probability distribution of two random variables operating in the
same probability space whose outcomes occur simultaneously. The
conditional distribution, denoted as @xmath , is defined as the
probability distribution of a random variable when another random
variable is known to have a particular value. Both distributions are
closely related through the product rule, which takes the form:

  -- -------- -- --------
     @xmath      (2.22)
  -- -------- -- --------

We can also relate these distributions with the marginal probability
distribution through the sum rule, i.e. by integrating out a random
variable that takes all the values in its subset. For discrete random
variables the marginal probability distribution is defined as:

  -- -------- -- --------
     @xmath      (2.23)
  -- -------- -- --------

while for continuous random variables it is expressed as:

  -- -------- -- --------
     @xmath      (2.24)
  -- -------- -- --------

##### The Bayes’ theorem and Bayes decision rule

The Bayes’ theorem plays a central role in ML as it provides a
relationship between conditional and joint probability distributions of
two random variables, which defines the so-called minimum-risk decision
rule ( Duda2000 ; Bishop2006 ) . Therefore, the Bayes’ theorem allows to
take optimal decisions for uncertain events based on prior knowledge and
conditions related to that events.

The Bayes’ theorem, also named Bayes’ rule or Bayes’ law, is defined as:

  -- -------- -- --------
     @xmath      (2.25)
  -- -------- -- --------

Under the context of a pattern recognition classification problem, these
distributions have specific interpretations related to the degree of
belief they give to the occurrence of different events. In this sense,
the Bayes’ formula can be informally expressed as:

  -- -------- -- --------
     @xmath      (2.26)
  -- -------- -- --------

where each term semantically means:

    [leftmargin=0cm]

 @xmath 

    : posterior probability , that is the quantity that should be
    estimated and represents the degree of belief of observing the event
    (or class in a classification problem) @xmath , after taking into
    account the evidence @xmath .

 @xmath 

    : likelihood distribution , it acts as a model of the random
    variable @xmath that specifies the most probable outcome of @xmath
    given a specific state of @xmath .

 @xmath 

    : prior distribution , it provides the degree of initial belief of
    observing the event @xmath of the random variable @xmath .

 @xmath 

    : evidence distribution , it acts as a normalization term that
    provides the total degree of belief of the evidences @xmath .

Note that the equation 2.25 can also be expressed in terms of joint
probability distributions in the form:

  -- -------- -- --------
     @xmath      (2.27)
  -- -------- -- --------

from where it can be observed that the denominator acts as a
normalization constant to ensure the conditional probability to sum 1.

Therefore, assuming @xmath to be a discrete random variable that models
the different classes of a pattern recognition classification problem,
the Bayes’ minimum-risk decision rule, sometimes also called minimum
error-rate classifier, can be expressed as:

  -- -------- -- --------
     @xmath      (2.28)
  -- -------- -- --------

or more formally defined as:

  -- -------- -- --------
     @xmath      (2.29)
  -- -------- -- --------

where @xmath refers to the optimal class for the decision problem.

Following the Bayes’ theorem, we can reformulate the equation 2.29 to:

  -- -------- -- --------
     @xmath      (2.30)
  -- -------- -- --------

from where it can be observed that we can drop out the @xmath term as it
is constant for the maximization (which is only dependent on @xmath ).

The previous result directly leads to the definition of the two main
families of models within the ML classification problem: the generative
models and the discriminative models. Generative models explicitly model
the joint distribution @xmath , which semantically means that they learn
a model to describe each class. By the opposite, the discriminative
models directly learn the posterior distribution @xmath , which just
models the decision boundaries between the different classes of the
problem, ignoring the properties or characteristics of the classes.

Figure 2.9 illustrates both approaches, with the corresponding
associated probability distributions and learning goals for each model.

#### 2.3.2 Probability distributions

##### The exponential family

The exponential family of probability distributions is an unified set of
distributions that can be expressed in the form:

  -- -------- -- --------
     @xmath      (2.31)
  -- -------- -- --------

for some functions @xmath , @xmath , @xmath and @xmath ; where @xmath
refers to the set of parameters of the distribution.

The exponential family represents one of the most important families of
distributions in statistics as it covers a wide range of distributions
that naturally arise in many natural phenomena. Among others, the most
important probability distributions of the exponential family are: the
Normal , t-Student , Gamma , Multinomial and Dirichlet distributions,
which are employed in some contributions of this thesis.

##### The Normal distribution

The most important distribution of the exponential family, and in
statistics in general, is the Normal distribution. The Normal
distribution, also called Gaussian distribution, is a continuous
symmetric probability distribution defined in the range @xmath , with
parameters @xmath , that takes the form:

  -- -------- -- --------
     @xmath      (2.32)
  -- -------- -- --------

where @xmath is referred as the mean and represents the central tendency
or expected value of the random variable governed by the distribution;
and @xmath is referred as the variance and represents the degree of
divergence of the realizations of the random variable from the central
tendency.

The Normal distribution is also a fundamental distribution for
probability theory as it arises from the central limit theorem , which
states that the summation of independent random variables (properly
normalized) tends to be normally distributed. This theorem has important
implications since it allows to employ statistical methods designed for
normal distributions to other problems involving non-normal
distributions.

The generalization of the one-dimensional Normal distribution to the
@xmath -dimensional multivariate case with @xmath is defined as:

  -- -------- -- --------
     @xmath      (2.33)
  -- -------- -- --------

where @xmath refers to the covariance matrix, and models the variance of
each independent variable and the pair-wise covariance interactions
between them.

The Normal distribution is typically denoted as @xmath for the
univariate case and @xmath for the multivariate case.

##### The t-Student distribution

The t-Student distribution is another continuous distribution of the
exponential family that arises from the estimation of the mean of a
population that is normally distributed. It is a symmetric zero-centered
distribution, also defined in the range @xmath , with parameters @xmath
, that takes the form:

  -- -------- -- --------
     @xmath      (2.34)
  -- -------- -- --------

with @xmath referring to the degrees of freedom of the distribution.

The t-Student distribution is very similar to the Normal distribution.
In the limit @xmath the t-Student distribution exactly converges to a
Normal typified distribution, while for values of @xmath the
distribution presents heavier and thicker tails, making it a natural
choice for robust data modeling in presence of outliers.

The distribution can be generalized to a location-scale distribution (
Bishop2006 ) by compounding a Normal distribution of mean @xmath and
unknown variance with an inverse Gamma distribution placed over the
variance with parameters @xmath and @xmath . The resulting density has
the form:

  -- -------- -- --------
     @xmath      (2.35)
  -- -------- -- --------

The generalization of the one-dimensional t-Student distribution to the
@xmath -dimensional multivariate case is defined as:

  -- -------- -- --------
     @xmath      (2.36)
  -- -------- -- --------

where @xmath refers to the dimensions of the random variable and @xmath
refers to the scale or shape matrix, which in general is not equivalent
to the covariance matrix of the Normal distribution.

The t-Student distribution is typically denoted as @xmath , or @xmath
and @xmath for the location-scale univariate and multivariate versions
respectively.

##### The Gamma distribution

The Gamma distribution is another continuous probability distribution of
the exponential family, which is very common in the biomedical field as
many biological phenomena follow a Gamma distribution. It is a
positive-only right-skewed distribution defined in the range @xmath ,
with parameters @xmath , that takes the form:

  -- -------- -- --------
     @xmath      (2.37)
  -- -------- -- --------

where @xmath is referred to as the shape parameter and @xmath is
referred to as the scale parameter.

Such distribution is widely employed to model time-dependent biological
phenomena that have a natural minimum of 0, such as the MR signal decay
produced by the first pass of the bolus of a paramagnetic contrast agent
intravenously injected to a patient.

The generalization of the univariate Gamma distribution to the
multivariate case is the Wishart distribution. This distribution is
defined for positive-definite @xmath matrices that represents the
scatter matrix of two @xmath -variate Normal-distributed random
variables, which falls far outside the scope of this thesis.

The Gamma distribution is typically denoted as @xmath .

##### The Multinomial distribution

The Multinomial distribution is a discrete probability distribution that
models the probability of observing each of @xmath different outcomes in
an experiment involving @xmath repeated trials. Therefore, the
distribution governs a discrete random variable @xmath , whose
realizations are vectors of the form @xmath , with @xmath representing
the number of times the @xmath outcome has been observed after the
@xmath trials.

The Multinomial distribution has the parameters @xmath , with @xmath ,
representing the prior probability of observing each of the @xmath
possible outcomes before the trials, and @xmath indicating the number of
trials. The pmf of the distribution takes the form:

  -- -------- -- --------
     @xmath      (2.38)
  -- -------- -- --------

The Multinomial distribution is the generalization of several discrete
distributions, those ones taking specific values for @xmath and @xmath
parameters. Thus, the Multinomial distribution converts to the Bernoulli
distribution when @xmath and @xmath ; to the Binomial distribution when
@xmath and @xmath ; and to the Categorical distribution when @xmath and
@xmath .

The Multinomial distribution can also be expressed using the Gamma
function as:

  -- -------- -- --------
     @xmath      (2.39)
  -- -------- -- --------

whose form is very similar to the Dirichlet distribution, which is its
conjugate prior.

The Multinomial distribution is typically denoted as @xmath .

##### The Dirichlet distribution

The Dirichlet distribution is a continuous probability distribution
typically used in Bayesian statistics as conjugate prior of the
Multinomial distribution. It is the multivariate generalization of the
Beta distribution, which is commonly used to model prior knowledge about
the probability of an event. Therefore, the Dirichlet distribution
governs a random variable @xmath whose outcomes are the realizations of
a Multinomial distribution in the form @xmath , with @xmath . The
distribution has the parameters @xmath and is defined as:

  -- -------- -- --------
     @xmath      (2.40)
  -- -------- -- --------

with @xmath called the concentration parameter.

As stated above, the Dirichlet distribution is the conjugate prior of
the Multinomial distribution, which means that if a random variable
@xmath follows a Multinomial distribution, and we assume a prior
Dirichlet distribution over its parameter @xmath , then the posterior
distribution of @xmath is also Dirichlet distributed. Under the Bayesian
statistics, this relationship leads to a powerful mechanism to, given a
new observation @xmath , update the parameter @xmath in a pure algebraic
manner, without recalculating the joint probability distribution @xmath
, which normally remains intractable.

The Dirichlet distribution is typically denoted as @xmath .

#### 2.3.3 Parameter estimation

Statistical inference is the process of deducing properties and making
generalizations about a population, by estimating the parameters of an
underlying probability distribution used to model observations taken
from that population. Therefore, statistical modeling typically consists
of two steps: 1) choosing the distribution that best fits the data, and
2) determining the parameters @xmath of the distribution.

However, @xmath is generally considered unobservable, as the whole
population cannot be observed either. Therefore, an estimation of the
parameters @xmath is usually calculated, based on a random sample drawn
from the population. The most typical analytical methods of estimating
the parameters of a probability distribution given its pdf (or pmf )
are: Maximum Likelihood Estimation ( MLE ), Maximum A Posteriori ( MAP )
estimation and Bayesian inference, of which the first two are briefly
described below as they are extensively used in this thesis. On the
other hand, numerical optimization methods such as the
Expectation-Maximization ( EM ) algorithm are also typically employed to
estimate the parameters of complex models that do not have closed-form
analytical solution. The EM algorithm is also discussed below as it is
extensively used in this thesis for several contributions.

##### Maximum Likelihood Estimation

The Maximum Likelihood Estimation ( MLE ) focuses on obtaining the best
distribution parameters @xmath by maximizing the likelihood function
@xmath . This approach can be semantically interpreted as “maximizing
the probability of observing the random sample @xmath given a current
guess about the parameters @xmath of the model” .

Let @xmath a set of observations of a random variable. Formally, the MLE
estimate is defined as:

  -- -- -- --------
           (2.41)
  -- -- -- --------

Differentiation is employed to maximize @xmath , by setting the partial
derivatives of the function with respect to each parameter to zero.
However, maximizing @xmath is hard as it involves a product over all
realizations of @xmath . Typically, natural logarithm is taken to
simplify the expression in various senses: first, natural logarithm is a
monotonically increasing function so the maximum value of @xmath occurs
at the same point as the maximum value of @xmath ; next, the logarithm
of the product converts to the sum of the logarithms, which is far
easier to maximize than the product since the derivative of a sum is the
sum of derivatives; additionally, probability distributions involving
the exponential function take benefit from the logarithm to simplify the
expression to be maximized; finally, computing a sum of log-derivatives
is computationally more stable than calculating a product of
probabilities, which quickly tend to zero leading to numerical
representation problems.

Therefore, the log-likelihood is usually maximized in MLE , which is
defined as:

  -- -------- -- --------
     @xmath      (2.42)
  -- -------- -- --------

##### Maximum A Posteriori

Maximum A Posteriori ( MAP ) estimation is closely related to MLE in the
sense that both look for the parameters that equal to the mode of a
distribution (the maximum of the distribution). However, while MLE
maximizes the likelihood (or log-likelihood) distribution of the
parameters @xmath , MAP estimation maximizes the posterior distribution
of @xmath with respect to the observations @xmath . Therefore, in this
case, MAP estimate can be semantically interpreted as “maximizing the
probability of a specific setting of the @xmath parameters of the model
given the current observations of the random variable @xmath ” . In this
sense, MAP is also closely related to Bayesian estimation, since it
considers @xmath parameters as a random variable that in turn is
governed by some distribution with its own parameters. Therefore,
assuming @xmath as a random variable and taking the Bayes’ rule:

  -- -------- -- --------
     @xmath      (2.43)
  -- -------- -- --------

where we can ignore the normalizing constant as we are strictly speaking
about normalization, so proportionality is sufficient. Hence, the MAP
estimate is defined as:

  -- -------- -- --------
     @xmath      (2.44)
  -- -------- -- --------

from where it can be clearly observed that MAP is an augmentation of MLE
that allows to introduce a density @xmath over the parameters of the
model, to inject initial beliefs and/or prior knowledge about them.
Moreover, it can also be quickly deduced that MLE is a particular case
of MAP estimation when @xmath is assumed to be constant. Thus, MAP can
also be thought as a regularization/generalization of the MLE method.

As in MLE , differentiation is employed to maximize @xmath and obtain
closed-form solutions for the estimation of the parameters of the model.
At this point, conjugate priors, briefly introduced in section 2.3.2 ,
become important when modeling @xmath , since assuming an adequate
distribution for @xmath may be crucial to update the parameters in a
straightforward manner.

##### Expectation-Maximization algorithm

As stated above, MLE and MAP estimation consist in the maximization of
the likelihood and posterior functions respectively, by mathematical
differentiation. However, setting partial derivatives for the parameters
of many complex models usually does not find closed-form expressions,
hence preventing direct analytical solutions.

A common way to deal with the complexity in optimizing parameters of
models that do not have closed form solution is to introduce a set of
latent variables @xmath that allow the model to be formulated in a more
tractable way. Therefore, defining a joint distribution @xmath over the
augmented space of observed and latent variables allows to estimate the
parameters of the model @xmath in a more straightforward manner. Then,
once @xmath is estimated, the distribution of the observed variables
@xmath can be obtained by marginalization. The EM algorithm takes
advantage of such idea.

The EM algorithm, formalized in 1977 by Arthur Dempster, Nan Laird and
Donald Rubin ( Dempster1977 ) , provides an iterative numerical
optimization framework for the MLE and MAP estimate of models involving
latent variables, when a closed-form analytical solution is not
possible. The algorithm relies on the introduction of an informative
latent variables that allow the model to be reformulated in such a way
that a closed-form solution for the estimation of the parameters can be
found.

Of course, these latent variables @xmath are unknown quantities that
prevent a direct estimation of the model, so maximizing @xmath (for the
MLE example case) is not possible. However, some state of knowledge
about the value of @xmath can be obtained by computing its posterior
distribution given the observations @xmath and a guess about the
parameters of the model @xmath , i.e. @xmath . Therefore, instead of
maximizing @xmath the EM algorithm maximizes an auxiliary function,
typically referred as the @xmath -function, which is the expected value
of @xmath under this posterior distribution. That is:

  -- -------- -- --------
     @xmath      (2.45)
  -- -------- -- --------

This formulation quickly suggest an iterative scheme based on
alternating between computing the conditional expectations of the latent
variables given the observations, i.e. computing @xmath , and updating
the parameters of the model via the closed-form solutions obtained
thanks to the augmented formulation @xmath .

Therefore, the general scheme of the EM algorithm (for the MLE case) is:

{siderules}

  Initialization:  

    Choose an initial setting for @xmath .

  Expectation step:  

    Evaluate the @xmath -function:

      -- -------- --
         @xmath   
      -- -------- --

    which actually means estimating the unknown quantity @xmath

  Maximization step:  

    Maximize the @xmath -function:

      -- -------- --
         @xmath   
      -- -------- --

    which consist of updating the parameters @xmath of the model based
    on @xmath

  Convergence:  

    Stop if @xmath ; otherwise @xmath and go to Expectation step .

Parameter estimation via EM algorithm is only guaranteed to converge to
a local maxima or to a saddle point of the function under maximization.
Convergence to the global maxima is not assured since the algorithm is
only guaranteed to iteratively increase the likelihood (or posterior)
function after each iteration. Hence, optimizing multi-modal functions
can easily lead to local maximas, also conditioned by the initialization
of @xmath . An in-depth dissertation on the convergence properties of
the EM was made by professor Wu1983 .

In the following chapters, several applications of MLE and MAP parameter
estimation of complex models via EM will be presented.

### 2.4 Mixture Models

So far we have seen that probability distributions give us a
mathematical form to describe the properties of a population represented
by a random variable. However, assuming that each observation is drawn
from a single unimodal distribution leads to a very simplistic scenario
that generally does not apply in real-world situations. In fact,
real-world data typically present more complex patterns such as
multi-modality, sparsity, noise or presence of outliers. In this sense,
models capable of capturing such variability are required to describe
the data in a more reliable sense.

In the following sections, three different families of mixture models
grouped by their structured or non-structured nature will be discussed.

#### 2.4.1 Finite Mixture Model

Finite Mixture Model s are probabilistic models that model a random
variable as a convex combination of pdf s . Therefore, FMM provides a
statistical formal framework to describe heterogeneous data trough a
weighted sum of single distributions, each one representing a
sub-population within a dataset.

Let @xmath a set of observations of a random variable, where @xmath . A
FMM of @xmath components assumes a pdf over @xmath in the form:

  -- -------- -- --------
     @xmath      (2.46)
  -- -------- -- --------

where @xmath are the parameters of the model, with @xmath the parameters
of the @xmath component of the mixture modeled by the pdf @xmath . The
parameters @xmath are typically called mixing coefficients and can be
seen as the prior probability of each component of the mixture
describing the data. Therefore, it follows that @xmath and:

  -- -------- -- --------
     @xmath      (2.47)
  -- -------- -- --------

Due to its mathematical tractability, Gaussian (or t-Student)
distributions are typically employed to model the data, thus @xmath .
Figure 2.11 illustrates the pdf of a 3-component Gaussian Mixture Model
( GMM ) (in purple), with their associated Normal distributions (blue,
red and yellow).

A common way to estimate the parameters of a FMM could be via MLE . From
2.46 we can set:

  -- -------- -- --------
     @xmath      (2.48)
  -- -------- -- --------

from where we can immediately see that the maximization is very complex
due to the summation over @xmath inside the logarithm. In fact, there is
no closed-form analytical solution for the MLE (nor MAP estimate) of a
FMM . In this sense, alternative methods are required to estimate the
parameters of these models in a reliable but affordable manner.

##### Em inference for Fmm

As stated in chapter 2.3.3 , a common way to deal with the complexity in
optimizing parameters of models that do not have closed form solution is
to introduce a set of latent variables @xmath that allow the model to be
formulated in a more tractable way. Under the FMM ’s point of view, a
latent variable @xmath is typically introduced indicating the component
of the mixture to which the observation belongs to. Formally, let @xmath
a random variable where @xmath represents a binary @xmath -dimensional
one-hot encoding variable, where @xmath indicates that the @xmath
observation has originated by the @xmath component of the mixture, and
hence @xmath . Under this assumption, we can reformulate the FMM by
defining a joint density over the latent and observed variables in the
form:

  -- -------- -- --------
     @xmath      (2.49)
  -- -------- -- --------

where we assume that @xmath follows a Multinomial unit-length
distribution given by equation 2.38 with @xmath (i.e. Categorical),
i.e.:

  -- -------- -- --------
     @xmath      (2.50)
  -- -------- -- --------

The MLE estimate of this augmented FMM can be expressed as:

  -- -------- -- --------
     @xmath      (2.51)
  -- -------- -- --------

which is far easier to optimize than the original FMM . In fact,
closed-form solutions can be found for all the parameters of the model
depending on the form of @xmath .

Optimizing @xmath requires considering the constraint that @xmath ,
which can be easily achieved by using Lagrange multipliers. Therefore,
setting the derivatives of @xmath with respect @xmath and @xmath
Lagrange multiplier yield:

  -- -------- -- --------
     @xmath      (2.52)
  -- -------- -- --------

Solving the system of equations and rearranging for @xmath provides:

  -- -------- -- --------
     @xmath      (2.53)
  -- -------- -- --------

Note that the denominator of the above expression ( 2.53 ) is actually
equal to @xmath , since it is a summation of N one-hot encoding
variables @xmath , which take a value 1 for one of their components and
0 for the rest.

For the most typical case of FMM where @xmath (i.e. GMM ), means and
covariance matrices must be optimized. Setting the derivatives of @xmath
with respect to the mean yields:

  -- -------- -- --------
     @xmath      (2.54)
  -- -------- -- --------

where rearranging:

  -- -------- -- --------
     @xmath      (2.55)
  -- -------- -- --------

and solving for the covariance matrix yields:

  -- -------- -- --------
     @xmath      (2.56)
  -- -------- -- --------

where rearranging:

  -- -------- -- --------
     @xmath      (2.57)
  -- -------- -- --------

Up to this point we have seen that assuming that we have both the
observations and latent variables @xmath , typically referred as the
complete dataset, the MLE (and also the MAP ) estimate find closed-form
solutions for the parameters of the FMM . However, actually, we only
have the observations @xmath (i.e. the incomplete dataset), so, as
previously described in section 2.3.3 , the only information we can get
from the latent variable @xmath is given by its posterior distribution
conditioned to the observations @xmath and to a current estimate of the
parameters of the model @xmath , i.e. @xmath . Following the Bayes’ rule
it is given by:

  -- -------- -- --------
     @xmath      (2.58)
  -- -------- -- --------

Therefore, because it is not possible to compute the MLE of @xmath (due
to we only can observe @xmath ), we just can compute its expected value
under the posterior distribution @xmath , which yields the so-called
@xmath -function:

  -- -------- -- --------
     @xmath      (2.59)
  -- -------- -- --------

which yields the same equation than 2.51 but substituting the latent
variable @xmath by its posterior quantity @xmath . Setting derivatives
of this expression obviously gives the same results as 2.53 , 2.55 and
2.57 , but replacing @xmath by @xmath .

This formulation hence suggest an iterative scheme for MLE or MAP
estimate, based on alternating between computing @xmath , referred as
the Expectation -step or E -step; and updating the parameters of the
model based on this distribution, referred as the Maximization -step or
M -step. Therefore, the EM algorithm can be finally summarized as:

{siderules}

  Initialization:  

    Choose an initial setting for @xmath .

  Expectation step:  

    Estimate @xmath

      -- -------- --
         @xmath   
      -- -------- --

  Maximization step:  

    Update the parameters of the model given @xmath

      -- -- --
            
      -- -- --

    where considering the GMM of the example:

      -- -- --
            
      -- -- --

  Convergence:  

    Stop if @xmath ; otherwise @xmath and go to Expectation step .

#### 2.4.2 Spatially Varying Finite Mixture Model

So far, we have seen that FMM s provide a rigorous statistical framework
for modeling heterogeneous data in a precise and formal manner. The
optimization of the likelihood function of a FMM provides a
computationally feasible yet powerful solution to fit and evaluate
complex models to model random variables.

However, FMM s make an important assumption that can turn into a severe
drawback when it comes to structured data, such as images. FMM s assume
observations @xmath to be independent and identically distributed, hence
implying no correlations between them. This assumption does not hold for
imaging data in which observations, i.e. pixels in 2D images or voxels
in 3D volumes, are strictly arranged in a structure that inherently
defines explicit correlations between them. Therefore, ignoring the
prior knowledge that adjacent observations are more likely to belong to
the same class wastes a highly useful information that allows images to
be described in a more concise, adequate and realistic manner.

The Spatially Varying Finite Mixture Model ( SVFMM ) is an extension of
the FMM for structured data, that aims to facilitate the modeling of the
spatial correlations inherent in images. The SVFMM mainly differs from
the FMM in the definition of the mixing coefficients which, as we saw,
can also be interpreted as the prior probabilities of the components of
the mixture. Specifically, the SVFMM assumes that each observation
@xmath has its own different vector of mixing coefficients @xmath ,
denoted as contextual mixing coefficients , rather than sharing a global
vector for all observations. Additionally, the SVFMM assumes @xmath to
be a vector of random variables rather than parameters, allowing a prior
density to be defined to introduce statistical correlations among them.

Let @xmath a set of observations of a random variable, where @xmath . A
SVFMM of @xmath components assumes a pdf over @xmath in the form:

  -- -------- -- --------
     @xmath      (2.60)
  -- -------- -- --------

where @xmath and @xmath are the parameters of the model. Note that
@xmath and @xmath are actually treated as random variables, and that
each observation @xmath has its own associated vector of contextual
mixing coefficients @xmath . Likewise FMM , contextual mixing
coefficients must also satisfy @xmath and:

  -- -------- -- --------
     @xmath      (2.61)
  -- -------- -- --------

A MAP estimate of the model is usually employed to introduce a proper
density over @xmath , typically in the form of a MRF , to establish
dependencies between adjacent contextual mixing coefficients. Therefore,
the MAP estimate of the SVFMM is defined as:

  -- -------- -- --------
     @xmath      (2.62)
  -- -------- -- --------

where we have assumed independence in @xmath and also assumed a constant
uniform distribution for @xmath that gets rid out from the maximization.

Several densities for @xmath has been proposed in the literature (
Sanjay1998 ; Woolrich2005 ; Blekas2005 ; Sfikas2008 ) to codify the
concept that neighboring observations tend to share the same component
of the mixture. A family of densities that has been widely used with
successful results are the Gauss- MRF s ( Nikou2007 ) . This family of
priors encodes the general idea that the estimator of the contextual
mixing coefficients can be defined as the average of its spatial
neighbors:

  -- -------- -- --------
     @xmath      (2.63)
  -- -------- -- --------

where @xmath indicates the set of neighbors of the @xmath observation.
In other words, this prior assumes that differences between adjacent
contextual mixing coefficients for a given component @xmath of the
mixture are Gaussian distributed in the form:

  -- -------- -- --------
     @xmath      (2.64)
  -- -------- -- --------

This approach is somewhat naïve and can be refined to capture
variability among different components of the mixture and spatial
directions of the images. The Directional Class-Adaptive Gauss-Markov
Random Field ( DCAGMRF ) prior, which lies into the family of
Simultaneous Auto-Regressive ( SAR ) models, has been used to regularize
ill-posed inverse problems with successful results. The DCAGMRF takes
the form:

  -- -------- -- --------
     @xmath      (2.65)
  -- -------- -- --------

where sub-index @xmath refers to the different spatial adjacency
directions (i.e. horizontals, verticals or diagonals), and @xmath
indicates the set of neighbors of the @xmath observation that lies in
the @xmath spatial direction.

Merging 2.62 with 2.60 finally yields:

  -- -------- -- --------
     @xmath      (2.66)
  -- -------- -- --------

which again is analytically intractable, so numerical approximations
should be employed.

##### Em inference for Svfmm

As in the FMM case, a binary @xmath -dimensional one-hot encoding latent
variable @xmath is introduced to simplify the estimation of the model,
by assuming knowledge about the component of the mixture to which the
observation belongs to. The formulation of the SVFMM assuming the
existence of the latent variables results in:

  -- -- -- --------
           (2.67)
  -- -- -- --------

The MAP estimate of the augmented SVFMM can be expressed as:

  -- -------- -- --------
     @xmath      (2.68)
  -- -------- -- --------

where again is far easier to optimize than 2.66 and from where we can
quickly observe that the maximization of @xmath will be exactly the same
as in the classic FMM . Only the estimation of @xmath will be affected
by the density @xmath as expected.

Following the same reasoning than in classic FMM , the only information
we can get from the latent variable @xmath is given by its posterior
density @xmath . Therefore, the conditional expectation of @xmath under
this density is used as auxiliar function for the optimization.
Rearranging 2.65 into 2.68 , substituting @xmath by their posterior
quantity @xmath and dropping constant terms that do not affect the
maximization yields the following @xmath -function:

  -- -- -- --------
           (2.69)
  -- -- -- --------

Optimizing @xmath from expression 2.69 must take into account several
considerations. First, note that @xmath appears in the term @xmath once
as the central individual and @xmath times as the neighbor of @xmath
individuals. Second, no closed-form solution can be obtained when
introducing the constraint @xmath in the maximization. Instead,
reparatory methods such as the quadratic programming algorithm proposed
in ( Blekas2005 ) , must be employed to project the solutions onto the
constraints to ensure probabilities sum up to 1.

Taking all this into consideration, the updates for @xmath are obtained
as the roots of the following second degree equation obtained by setting
partial derivatives of 2.69 with respect @xmath (without restrictions):

  -- -------- -- --------
     @xmath      (2.70)
  -- -------- -- --------

which can easily demonstrated that always have a real non negative
solution.

The DCAGMRF prior also introduces a new set of parameters @xmath that
govern the variances of the Gauss- MRF . Setting derivatives of 2.69
with respect @xmath yields:

  -- -------- -- --------
     @xmath      (2.71)
  -- -------- -- --------

Therefore, the MAP EM algorithm for the SVFMM with the DCAGMRF prior can
be finally summarized as:

{siderules}

  Initialization:  

    Choose an initial setting for @xmath and @xmath .

  Expectation step:  

    Estimate @xmath

      -- -------- --
         @xmath   
      -- -------- --

  Maximization step:  

    Update the parameters of the model given @xmath

      -- -------- --
         @xmath   
      -- -------- --

    where considering the GMM example:

      -- -------- --
         @xmath   
      -- -------- --

    (*) Choose the real non-negative solution and project onto the
    constraints using the quadratic programming algorithm ( Blekas2005 )
    to ensure @xmath

  Convergence:  

    Stop if @xmath ; otherwise @xmath and go to Expectation step .

#### 2.4.3 Dirichlet Compound Multinomial-Spatially Varying Finite
Mixture Model

An important limitation of the SVFMM is that it does not inherently
preserves the condition of @xmath . Instead, reparatory projections must
be employed to accomplish with this constraint, thereby compromising the
assumed Bayesian framework.

An interesting alternative is to consider that @xmath follows a
Dirichlet Compound Multinomial ( DCM ) distribution ( Nikou2010 ) . The
DCM distribution is a hierarchical discrete multivariate distribution,
where an observation @xmath is drawn from a Multinomial distribution
governed by a parameter vector @xmath , which in turn is drawn from a
Dirichlet distribution governed by parameter vector @xmath .

  -- -------- -- --------
     @xmath      (2.72)
  -- -------- -- --------

Such scheme presents two important advantages for the SVFMM . First, the
Dirichlet distribution draws probability vectors that intrinsically
satisfies the constraint @xmath , so no reparatory corrections will be
required. Next, the @xmath parameters of the Dirichlet distribution are
not subject to any restriction (rather than @xmath ) so a Gauss- MRF can
be imposed over them to introduce the spatial correlations and local
regularity in a more straightforward manner.

In this sense, the pdf of the @xmath variable given the parameters of
the Categorical distribution (Multinomial unit-length with @xmath ) is:

  -- -------- -- --------
     @xmath      (2.73)
  -- -------- -- --------

The pdf of the @xmath variable given the parameters of the Dirichlet
distribution is

  -- -------- -- --------
     @xmath      (2.74)
  -- -------- -- --------

with @xmath the Gamma function. Integrating out both pdf s we obtain the
following density for the @xmath variables conditioned to @xmath :

  -- -------- -- --------
     @xmath      (2.75)
  -- -------- -- --------

Taking into account that @xmath , the vector @xmath of label prior
probabilities can be finally computed as:

  -- -------- -- --------
     @xmath      (2.76)
  -- -------- -- --------

Therefore, the previously proposed DCAGMRF of equation 2.65 can be
imposed over the parameters of the Dirichlet distribution in the form:

  -- -------- -- --------
     @xmath      (2.77)
  -- -------- -- --------

Arranging this density into the SVFMM and substituting @xmath by its
conditional density under the Dirichlet process yields:

  -- -------- -- --------
     @xmath      (2.78)
  -- -------- -- --------

which, as expected, is again analytically intractable.

##### Em inference for Dcm-Svfmm

As in the previous cases, a binary @xmath -dimensional one-hot encoding
latent variable @xmath is introduced to simplify the estimation of the
model. The formulation of the DCM - SVFMM assuming the existence of this
latent variable results in:

  -- -------- -- --------
     @xmath      (2.79)
  -- -------- -- --------

The MAP estimate of the augmented DCM - SVFMM is expressed as:

  -- -- -- --------
           (2.80)
  -- -- -- --------

which, as usual, is easier to optimize than the original model.

The conditional expectation of @xmath under the posterior density @xmath
is again used as auxiliar function for the optimization, yielding the
following @xmath -function:

  -- -- -- --------
           (2.81)
  -- -- -- --------

As in the SVFMM case, the optimization of @xmath is exactly equal than
in classic FMM s . The optimization of @xmath , however, does not
require to introduce any constraint to preserve the condition @xmath .
Then, it is only necessary to consider that @xmath appears in the term
@xmath once as the central individual and @xmath times as the neighbor
of @xmath individuals.

Setting partial derivatives of 2.81 with respect @xmath yields the
following third degree equation:

  -- -------- -- --------
     @xmath      (2.82)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (2.83)
  -- -------- -- --------

where, under polynomial theory, it can be demonstrated that there is
always a real non negative solution that satisfies @xmath . The
Cardano’s or Vieta’s methods can be employed to algebraically obtain the
roots of the proposed third degree equation.

The optimization of @xmath yields the same equation than 2.71 but
expressed in terms of @xmath variables:

  -- -------- -- --------
     @xmath      (2.84)
  -- -------- -- --------

Therefore, the MAP EM algorithm for the DCM - SVFMM with the DCAGMRF
prior can be finally summarized as:

{siderules}

  Initialization:  

    Choose an initial setting for @xmath and @xmath .

  Expectation step:  

    Estimate @xmath

      -- -------- --
         @xmath   
      -- -------- --

    where

      -- -------- --
         @xmath   
      -- -------- --

  Maximization step:  

    Update the parameters of the model given @xmath

      -- -------- --
         @xmath   
      -- -------- --

    where considering the GMM example:

      -- -- --
            
      -- -- --

      -- -------- --
         @xmath   
      -- -------- --

    (*) Choose the real non-negative solution.

  Convergence:  

    Stop if @xmath ; otherwise @xmath and go to Expectation step .

### 2.5 Deep Learning

At the beginning of the 2000’s the ML started an important revolution
with the birth of the Deep Learning ( DL ) techniques ( Lecun2015 ) .
Until that time, ML was generally constituted by two disjoint and
clearly differentiated stages: the feature extraction step and the
classification/regression step. The feature extraction step was
typically addressed manually, computing hand-crafted features from the
raw data guided by the expertise, knowledge and intuition of the
researcher. These features were then fed to the classifier/regressor,
expecting to be discriminant informational inputs to successfully solve
the corresponding task. Therefore, a crucial step for the good
performance of classic ML methods typically relied in a powerful feature
extraction process.

The qualitative leap made by DL techniques concerns mainly the feature
extraction stage. Instead of computing hand-crafted features, DL models
attempt to simultaneously learn a discriminant representation of the raw
input data at the same time as the classifier (regressor) is trained.
This concept shift allows machines to learn the optimal set of features
appropriate for the task. Therefore, DL models not only learn the
decision boundaries (or regression model) of a discriminative
classifier, but they also learn a manifold to optimally project the data
into it in such a way that the classification task becomes as trivial as
possible: the so-called representation learning . Figure 2.12 depicts a
general conceptual schema of the concept shift between classic ML
approach and the current DL paradigm.

DL addresses the representation learning problem by building concepts
out of simpler concepts. This kind of hierarchical learning perfectly
fits into the nature of Artificial Neural Network s . ANN s are ML
models that process the input hierarchically by passing it through a
nested set of layers, each of which learns a deeper level of abstraction
of the input. Therefore, concepts inside a ANN are built by
concatenating the output of a layer, which represents a simpler abstract
representation of the input, and the input of the successive layer,
which learns a more complex and richer composite representation from
those previous outputs. Figure 2.13 shows an example of the hierarchical
concept representation of images showing faces by a DL model.

The following sections briefly describe the basics and fundamental
principles of ANN s , and present the Convolutional Neural Network s :
the current state-of-the-art classifier for most of the supervised
visual computing related tasks.

#### 2.5.1 Artificial Neural Networks

ANN s are connectionist computational models in which a number of
processors, named artificial neurons or perceptrons are interconnected
in a manner inspired by the connections between neurons in a human
brain. ANN s were born in the mid-twentieth century, in a combined
effort of multiple researchers. In 1943 Mcculloch1943 created the first
computational model of a biological neuron. Few years later Hebb2002
designed the first learning rule for ANN s based on the concept of
neural plasticity , whose premise was that if two neurons were active
simultaneously, then the strength of the connection between them should
increase. In 1958, Rosenblatt1958 created the perceptron, which is the
basis of the ANN s , and Ivakhnenko1967 developed the first multi-layer
ANN . In subsequent years, Kelley1960 laid the basis of the
backpropagation algorithm, but it was not until the work of
Rumelhart1986 where the first ANN was effectively trained using the
backpropagation algorithm by adjusting the weights of the network
proportional to the gradient computed from the error.

Today, ANN s are the most powerful and widely used ML algorithms, as
they have proven to largely outperform other approaches in many tasks
such as visual object recognition, object detection and segmentation and
many other domains such as speech recognition, handwritten text
recognition or financial time series prediction. The following sections
provides an overview of the basics of ANN s to facilitate the reading
and understanding of subsequent chapters and contributions of this
thesis.

##### The Perceptron

ANN s are composed by basic units called perceptrons or artificial
neurons . The perceptron (see Figure 2.14 ), in its most elementary
form, is a simplified model of a biological neuron, approximated as a
mathematical function for learning a binary linear classifier. Let
@xmath a @xmath vector of features representing an observation.
Mathematically, a perceptron is defined as:

  -- -------- -- --------
     @xmath      (2.85)
  -- -------- -- --------

where @xmath represents a weight associated to the @xmath feature, and
@xmath stands for the bias of the linear model. The Heaviside step
activation function is employed in the most basic perceptron algorithm,
setting the output of the neuron to 1 (”activated”) if @xmath is greater
than a threshold (typically 0); or to 0 (”deactivated”) otherwise.

Perceptrons are only able to solve linearly separable problems
regardless of the activation function employed. Therefore, they are only
guaranteed to converge if the training set is linearly separable.
Otherwise, the perceptron will fail and no approximate solution is
returned.

##### The Multi-Layer Perceptron

A Multi-Layer Perceptron ( MLP ) is a class of feedforward ANN composed
of at least three layers of interconnected perceptrons. Figure 2.15
shows an example of a simple MLP with an input layer, a hidden layer and
an output layer. Except of the input layer, all the neurons in the other
layers are perceptrons.

The arrangement of multiple layers of perceptrons in combination with
the non-linear activation functions allow MLP s to solve non-linearly
separable problems. Indeed, it is demonstrated that MLP s are universal
mathematical function approximators through superposition of
non-linearly activated perceptrons.

##### Activation functions

Activation functions are determinant for ANN s since they are
fundamental to allow the network to solve non-linear problems. An ANN of
multiple layers, each one activated by a linear function passed to the
next layer, results in a final function that is a combination of linear
functions in a linear form, which by definition can be replaced by a
single linear function. In other words, no matter how many layers are
stacked in an ANN , if only linear activation functions are used, the
network is still equivalent to a single layer with linear activation,
and therefore is only capable of solving linear problems.

To mitigate this effect multitude of activation functions are proposed
in the literature. The most important are summarized below:

###### Linear activation

The most simple activation function. The output of the perceptron is
proportional to the input. It is defined as:

  -- -------- -- --------
     @xmath      (2.86)
  -- -------- -- --------

It allows a continuous output range, which is more powerful than a
binary output, however its derivative with respect to @xmath is
constant. This means that the gradient has no relation to @xmath and,
therefore, regardless of the magnitude of the prediction error, the
changes made by backpropagation are constant. Additionally, as said
above, a multi-layer ANN fully activated by linear functions is
equivalent to a single-layer ANN only capable of solving linear
problems.

###### Heaviside step activation

A binary activation function. The output of the perceptron is
”activated” if @xmath is greater than a threshold, or is ”deactivated”
otherwise. It is typically defined as:

  -- -------- -- --------
     @xmath      (2.87)
  -- -------- -- --------

where ”activated” is typically represented by 1, and ”deactivated” fires
the 0 value. Likewise, the typical threshold employed to determine the
output is the 0 value.

The Heaviside function outputs a binary discrete value, which presents
some disadvantages. By definition it can only properly represent binary
classification problems, in which the ANN has only one output neuron.
For multi-label problems where ANN s have multiple output neurons firing
Heaviside step activations, it is not possible to correctly identify the
corresponding class, hence preventing their training. Moreover,
continuous activation functions allow a smoother training process, less
prone to fall into bad local minima.

###### Sigmoid / Logistic activation

One of the most historically used activation functions. It represents
the continuous soft approximation of the Heaviside step function. It is
mathematically defined as:

  -- -------- -- --------
     @xmath      (2.88)
  -- -------- -- --------

Its non-linearity allows the network to learn complex decision
boundaries by stacking layers sequentially. Moreover, the gradient is
related to the input, so it propagates the magnitude of the error
through the entire network. Finally, the output is continuous and
bounded to the @xmath range, allowing a smoother training process
without gradient explosion.

The main disadvantage of this activation function is that it originates
the so-called gradient vanishing problem. The gradient vanishing problem
is related to the update process of the @xmath weights of a network
trained through gradient-based learning algorithms such as
backpropagation . In these algorithms, the weights of the network are
updated proportionally to the partial derivative of the error function
with respect to the weights. Depending on the nature of the activation
function, it is possible for the gradient to become extremely small,
barely modifying the weights of the network, and therefore slowing or
even preventing the network from further training.

In the specific case of the sigmoid activation function, it can be seen
that for @xmath values above @xmath and below @xmath , the slope of the
function is almost near-horizontal. Therefore, there is no effectively
change in the gradient, resulting in the network refusing to learn
further. For small networks, this does not represent a big problem,
however, the more layers stacked in the network, the problem grows
exponentially, eventually collapsing the training of the model.

###### Tahn activation

A similar activation function than the sigmoid function. Indeed, the
tanh activation is a scaled version of the sigmoid function:

  -- -------- -- --------
     @xmath      (2.89)
  -- -------- -- --------

It has similar properties than the sigmoid function. Its gradient is
stronger than the sigmoid but it also suffers from the vanishing
gradient problem.

###### Softmax activation

Typically an activation function only employed in the final layer of a
network to convert the activations into probabilities (posteriors). It
is defined as:

  -- -------- -- --------
     @xmath      (2.90)
  -- -------- -- --------

The aim of this activation function is to normalize the outputs so that
each neuron triggers a value in the range @xmath and all of them add up
to 1, giving the probability of the input value being in a specific
class.

###### ReLU activation

First presented in 2009, the Rectified Linear Unit (ReLU) activation
function can be considered a milestone in DL . It is mathematically
defined as:

  -- -------- -- --------
     @xmath      (2.91)
  -- -------- -- --------

This simple function provides a set of benefits that have made ReLU as
the most widely used activation function nowadays. First, ReLU is a
non-linear function, so it allows the network to model complex functions
by stacking layers. Moreover, the function is ranged between @xmath ,
hence allowing the gradient to not vanish as there is no saturation in
any range of the function. It also provides sparsity activation since
multiple neurons can fire 0 activation because of a negative input of
the ReLU, therefore becoming a lighter network.

However, ReLU activation suffers from the so-called dying ReLU problem.
This problem arises when a neuron continuously trigger negative values
to the ReLU. In these cases, the gradient become 0, preventing the
neuron from responding to changes in error, and updating its weights. If
the problem affects multiple neurons in the network, it could lead to a
substantial part of the network passive.

###### Leaky-ReLU activation

It consist of a simple modification of the ReLU function to avoid the
dying ReLU problem:

  -- -------- -- --------
     @xmath      (2.92)
  -- -------- -- --------

with @xmath a parameter, typically adopting small values @xmath , to
allow a small positive slope for the negative range of the function.
Such modification allows the neuron to prevent its paralysis, eventually
reactivating it during the training process.

##### Backpropagation algorithm

The backpropagation algorithm was originally introduced in the 1970s,
but its important was not fully recognized until the 1986 paper by
Rumelhart1986 . Backpropagation is an algorithm to efficiently train an
ANN by adjusting its weights so that the network output minimizes a
given loss function. More formally, the backpropagation is an iterative
optimization algorithm based on a gradient descent technique, to update
the weights of a network by computing the gradient of the loss function
with respect to each weight following the chain rule.

To illustrate the mechanics of the backpropagation algorithm, lets
consider the simple ANN shown in figure 2.16 (example taken from
https://www.anotsorandomwalk.com/backpropagation-example-with-numbers-step-by-step/
with kind permission).

The following relations are established between the neurons of the
network:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath refers to the input of the corresponding neuron, and @xmath
indicates the output of the neuron after the activation function @xmath
is applied, which in the example will be the sigmoid function.

Let assume the input values of the network @xmath and the desired target
@xmath . The forward pass of the network yields the following results:

  -- -------- --
     @xmath   
  -- -------- --

Once the forward pass is completed, weights are updated by setting
partial derivatives of the error function @xmath with respect to each
weight, following the chain rule. As an example, setting partial
derivatives to update the weight @xmath yield:

  -- -------- -- --------
     @xmath      (2.93)
  -- -------- -- --------

For the shake of simplicity lets consider @xmath the sum of squared
errors of the predictions with respect to the targets:

  -- -------- -- --------
     @xmath      (2.94)
  -- -------- -- --------

Solving for the first term @xmath of the partial derivatives in equation
2.93 yields:

  -- -------- -- --------
     @xmath      (2.95)
  -- -------- -- --------

The next term to solve is @xmath , which only engages the activation
function of the neuron. The activation function chosen for the example
is the sigmoid function, whose derivative is @xmath . Applying to the
update process of the weights yields:

  -- -- -- --------
           (2.96)
  -- -- -- --------

Finally, the partial derivative @xmath involves the derivation of @xmath
with respect @xmath , which is trivial:

  -- -------- -- --------
     @xmath      (2.97)
  -- -------- -- --------

Therefore, the gradient of the error with respect to the weight @xmath
is:

  -- -------- -- --------
     @xmath      (2.98)
  -- -------- -- --------

Once the gradient of the loss function with respect to the corresponding
weight is known, the final update of the weight is typically performed
by:

  -- -------- -- --------
     @xmath      (2.99)
  -- -------- -- --------

with @xmath referring to the learning rate typically set to small values
@xmath .

The updates of the remaining weights follow the same mechanics, with a
slightly more cumbersome computation due to the increase in the number
of neurons interconnected in deeper layers. The results are:

  -- -------- --
     @xmath   
  -- -------- --

After weight updating, computing the output of the network in the
iteration @xmath yields:

  -- -------- --
     @xmath   
  -- -------- --

Comparing the errors produced by the network in the first @xmath and
second iterations @xmath :

  -- -------- --
     @xmath   
  -- -------- --

which initially may not seem too much, but after repeating the process
100.000 times, the error decreases to 0.0000351085, and the output of
the network becomes:

  -- -------- --
     @xmath   
  -- -------- --

concluding the training of the network. For an in-depth dissertation on
ANN s , training procedures, numerical optimizers and convergence
properties of DL models please refer to Bengio2017 .

#### 2.5.2 Convolutional Neural Networks

CNN s are a specialized type of ANN s inspired in the human visual
cortex. In the 1950s, experiments conducted by Dr. Hubel and Dr. Wiesel
identified that sets of different specific neurons in a cat’s visual
cortex responded very quickly when observing images with lines at
specific angles, with light and dark patterns, or by observing movement
in a certain direction. These results laid the foundation for further
research into animal’s visual understanding.

Human visual cortex process the information by passing it from one
cortical area to another, where each cortical area is more specialized
than the last one. For example, the primary visual cortex focuses on
preserving spatial location of visual information, i.e. orientation of
edges and lines. The secondary visual cortex feeds on the response of
the primary visual cortex and focuses on collecting spatial frequencies,
size, colors and shape of the object, while third visual cortex process
the global motion and provides a complete visual representation. CNN s
adopts this philosophy and tries to act like a computational visual
cortex.

CNN s are a type of ANN mainly oriented for image processing, that arose
in the attempt to solve the limitations that classic MLP s have when
dealing with images. First, fully-connected ANN s do not scale well for
images. Consider the example of a small image of @xmath pixels. Each
neuron of the second layer of a fully-connected ANN has @xmath weights
to learn. Therefore, assuming a simple MLP with a hidden layer of @xmath
neurons has @xmath M of weights only in the second layer, making it
unaffordable to optimize. Second, MLP also ignore the spatial
information and local redundancy inherent in images. In a MLP , the
pixels of an image are independently connected to the layers of the
network, thus loosing the semantic meaning of the position and its
relation with their neighbors. This wastes a highly useful information
that allows images to be described in a more concise, adequate and
realistic manner. Finally, classic MLP s are not translation-invariant.
Since the input image is connected in a static manner to the layers of
the network, the patterns learned during the training process are
statically linked to the position in which they appeared. Therefore,
same objects appearing at different positions will change the weights
associated to the corresponding pixels, intrinsically learning the
position in which they appeared.

CNN s , by the opposite, are non-fully connected ANN s , primarily made
of a set of stacked convolution layers, where each layer has a small set
of shared weights that intelligently adapt to the properties of images.
They are by definition translation invariant and context-sensitive,
allowing for learning patterns related to objects of interest appearing
anywhere in the image.

The following sections overall introduce the main components and
state-of-the-art architectures for image segmentation with CNN s .

##### Convolution layer

A convolution layer is a special arrangement of neurons, typically in a
2D or 3D grid, that acts as a bank of learnable filters (sometimes also
called kernels ). These filters are used to perform a sliding dot
product with an image to extract the set of optimal features to solve
the classification/regression task. The dot product operation, also
named convolution , is defined as:

  -- -------- -- ---------
     @xmath      (2.100)
  -- -------- -- ---------

where @xmath and @xmath are arrays of the same dimension, and @xmath
represents the index to traverse such arrays. Figure 2.17 shows an
example of a convolution between a kernel of size @xmath and a region of
an image of the same size.

Therefore, unlike the classic fully-connected layers of an ANN , a
convolution layer is not fully connected to its input, but it contains
small banks of filters of shared weights that traverse the different
locations of the image. This introduces several benefits: first, the
number of weights to train in a CNN is drastically lower than on a
classic ANN . Considering the previous aforementioned example, a CNN
with a first layer of @xmath learnable filters of size @xmath has only
@xmath weights to train, regardless of the image size. Moreover, the
@xmath weights of each filter are shared across the entire image, making
the pattern learned by the filter invariant to location and translation.
Finally, the 2D or 3D arrangement of the filter weights inherently
captures the spatial information and the semantic meaning of the
positions in which the intensities appear in the images.

In addition to the kernel size, convolution operation also involves two
parameters that must be taken into account: the stride and the padding
methods. The stride controls how the filter convolves around the input
image. In other words, the stride fixes how many pixels the kernel is
moved in one direction between two successive convolutions. Therefore, a
stride of @xmath means that, after a convolution, the kernel must be
moved one pixel in only one direction and then perform the next
convolution. The padding controls the behavior of the convolution in
terms of the size of the resulting map after the convolution. The
convolution operation, by definition, shrinks the image into a factor
related to the size of the kernel used for the operation. Additionally,
pixels in the corners and edges of the images are not visited the same
number of times as central pixels, thus, giving more importance to the
latter. To compensate for this effect, a padding method can be employed
to pad the input image so that the resulting map after the convolution
has the same size than the input image. There are different types of
padding: zero-padding, mirror-padding and reflect-padding; with
zero-padding the most widely used in CNN s .

##### Batch Normalization layer

Batch normalization is a technique employed to normalize the inputs of a
layer, with the main purpose of mitigating the internal covariate shift
problem. The covariate shift problem refers to the continuous shift that
the internal activations of the network undergo during the training
process, due to continuous changes in the distribution of the inputs of
the layers. Since the inputs of a layer are the activations of the
previous layer, a significant change in the distribution of these
activations forces the intermediate layers to continuously adapt itself
to these numerical fluctuations. This leads to a situation where each
layer in the ANN wastes training iterations in this adaptation rather
than learning the relations between the inputs and the desired target.

The batch normalization technique prevents this situation by normalizing
the activations of a layer, before feeding them as the input of the next
layer. Let @xmath a batch of @xmath inputs of a layer, the batch
normalization performs as follows:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the mean of the batch, @xmath is the variance of the
batch and @xmath is the @xmath normalized sample of the batch. Batch
normalization technique introduces two learnable parameters, @xmath and
@xmath , to allow the network to perform a scale and shift of the batch
if the training process requires it.

Besides mitigating the internal covariate shift, batch normalization
also provides other related benefits to the training process of an ANN .
First, batch normalization allows each layer to learn a little bit more
independently of other layers. Since the inputs are always normalized in
the same numerical range, the layer can focus on learning the relations
between the inputs rather than the relations between the adjacent
layers. Second, batch normalization also significantly accelerates the
training process. Higher learning rates can be employed when using batch
normalization since it ensures that there are no a outlier activations.
Finally, it reduces overfitting because it has a slight regularization
effect over the entire network.

##### Pooling layer

The function of the pooling layers is to progressively reduce the size
of the activation maps in order to also reduce the number of weights and
the computational cost of the network, while retaining the most
discriminant information. Additionally, pooling also allows to increase
the receptive field of the network by compressing the information in
smaller activation maps.

There are three main types of pooling layers: max pooling, average
pooling and sum pooling. Max pooling - the most frequently pooling layer
used in CNN s - uses filters to take the largest elements of the
activation maps in the pooling window. Figure 2.18 shows an example of
the result of a max pooling layer with filters of @xmath and stride
@xmath .

As expected, average pooling computes the average value within the
pooling layer, while sum pooling computes the summation of the values
inside each pooling layer.

Pooling operation also provides benefits for preventing overfitting. In
a general sense, pooling is a feature selection method by reducing the
dimensions of input. Therefore, selecting a subset of activations from
the output of the layers drop useless information and introduce slight
variations in the new activation maps that helps in overfitting to
specific patterns.

##### U-Net architecture

The U-Net architecture was first proposed by Ronneberger2015 as a
modification of the classic Fully convolutional network presented in (
Long2015 ) . It consist of a contracting path similar than the fully
convolutional network, followed by and expanding path where pooling
layers are replaced by up-sampling layers. This gives the architecture a
“U” shape that justifies its name.

The main innovation of this architecture was the expanding path made up
of up-sampling convolutions that allowed to reconstruct the
low-dimensional activation maps into high-dimensional maps of the same
size of input image. This enabled to address image segmentation problems
through CNN s in a complete naturally manner, since the output
activation maps could directly represent the logits of each class for
each pixel of the image.

Figure 2.19 shows the original architecture proposed in the article of
Ronneberger2015 . The network consisted in 5 levels of depth, with 64
activation maps in the first level, 128 in the second, 256 in the third,
512 in the fourth and 1024 in the final level. The contracting path was
formed by blocks of two sequential @xmath convolutions layers followed
by max pooling layers with filters of size @xmath at each level. The
expanding path performed the opposite job by gradually projecting the
low-dimensional activation maps into the space of the inputs, so that
the final activation map had as many channels as the number of classes
in which each pixel could be classified.

Another important contribution was the loss employed to train the
network. The work of Ronneberger2015 was the first that proposed to
apply a pixel-wise softmax to the final activation map, in order to
convert the activations into probabilities that represent the degree of
confidence of each pixel to belong to each class. Following this
approach, the network was finally trained by minimizing the
cross-entropy loss function between the softmax predictions and the
groundthruth segmentation.

The final significant contribution of the paper was the so-called
long-term skip connections between same levels in the contracting and
expanding paths of the U-Net architecture (denoted as gray arrows in the
figure 2.19 ). These connections are also biologically inspired in the
human visual cortex, where neurons of the first areas of the cortex are
directly connected with neurons in deeper regions, skipping intermediary
cortex areas. This allows to directly propagate useful signals between
different regions of the cortex to properly understand the scene. The
same mechanism is mimicked in U-Net architectures with the long-term
skip connections. These connections introduce two main benefits in the
network: first, they allow to easily propagate the gradient to the first
layers, where it is more difficult to adjust weights (due to gradient
vanishing problem). Second, these connections bring the ability to
directly pass useful information captured in the first layers that is
later required to reconstruct the activation maps during the
up-sampling.

Since 2012, as a result of the breakthrough brought about by AlexNet (
Krizhevsky2012 ) , CNN s , and specifically U-Net architectures, clearly
dominate most of the computer vision and image understanding challenges,
positioning them as the de facto standard DL classifiers for addressing
these tasks.

## Chapter 3 Comparative study of unsupervised learning algorithms for
glioblastoma segmentation

Unsupervised learning constitutes one of the most important roles in
automated image understanding. Specifically, it has historically played
a key role in the medical image segmentation task, as it provided the
first approaches to identify tissues in MRI acquisitions without
requiring manual human intervention. However, currently, medical image
segmentation is often dominated by supervised learning methods because
of their superior performance over unsupervised learning methods.
Nevertheless, supervised learning approaches have several limitations.
The performance of supervised learning models is directly conditioned by
the size and quality of the training corpus, whose collection is often
tedious, time-consuming and sometimes even unaffordable. Though, most
importantly, supervised learning can only learn tasks for which humans
already know the solution. Because of their learning scheme, supervised
approaches are only able to solve problems with well-known defined
outputs. Unsupervised learning, on the contrary, is capable of
recognizing patterns within the data in completely unexplored and
unknown tasks. Regarding the image segmentation problem, unsupervised
learning is able to delineate regions within the image with common MRI
properties, typically representing the same tissue or physiological
area. However, due to the unguided blind learning approach of
unsupervised methods, these generally do not reach as accurate results
as those of a supervised learning model in well-known tasks.

In this sense, the aim of this chapter is to demonstrate that
unsupervised learning can achieve competitive results comparable to
those obtained by supervised learning methods in a well-known task. The
purpose is to ratify if unsupervised learning is capable to detect
consistent patterns in MRI , discriminant enough to solve a segmentation
task. For that purpose, we performed a comparative study of several
unsupervised learning algorithms in the task of automated high grade
glioma segmentation. A postprocessing stage was also developed to
automatically map each label of an unsupervised segmentations to a
specific tissue of the brain. The comparative was performed with the
public reference BRAin Tumor Segmentation ( BRATS ) 2013 Test and
Leaderboard datasets.

The contents of this chapter were published in the publications (
JuanAlbarracin2015a ; JuanAlbarracin2015b ) —thesis contributions C1, P1
and P2.

### 3.1 Introduction

Medical imaging plays an indisputable key role in the diagnosis and
management of brain tumors. The intracranial location of these lesions
and the unspecificity of clinical symptoms make medical imaging a
necessary tool to diagnose and monitor the evolution of these diseases (
Wen2010 ) . Specifically, the early identification and delineation of
the different tissues composing the tumor becomes crucial to take
decisions that can improve the patient survival. However, the manual
segmentation of these tissues constitutes a complex, time-consuming and
biased task, which caught the attention of the ML community ( Bauer2013
) . Particularly, glioblastoma tumor has received most of this
attention, as it is the most common and aggressive malignant tumor of
the central nervous system ( Dolecek2012 ; Deimling2009 ) .
Glioblastomas are heterogeneous lesions that present different areas of
active tumor, necrosis and edema, all of them exhibiting a high
variability related to the aggressiveness of the tumor. Hence, the
automated segmentation of these lesions becomes a desired solution from
the clinical standpoint and an interesting challenge to address from the
ML community.

Extensive reviews of brain tumor segmentation have been presented by
Wadhwa2019 ; Saman2019 ; Gordillo2013 ; Bauer2013 . Most of these
techniques fall into the supervised learning approach. Verma2008 and
Ruan2011 employed Support Vector Machine s to segment healthy and
pathological tissues, and additionally subcompartiments inside these
areas. Jensen2009 used neural networks to detect brain tumor invasion.
Lee2008 used a combination of Conditional Random Field ( CRF ) and SVM
to perform tumor segmentation. Bauer2011 also used SVM and Hierarchical
CRF to segment both healthy and tumor tissues including
sub-compartments. Recently, Random Forest ( RF ) techniques have shown
high success in the supervised brain tumor segmentation task. Meier2013
; Festa2013 ; Reza2013 ; Tustison2013 proposed several approaches based
on RF variants for the BRATS challenge of Medical Image Computing and
Computer Assisted Intervention ( MICCAI ) 2013 Conference, reaching the
first positions in the competition. Nowadays, with the advent of novel
deep learning techniques, the current state-of-the art is mostly
dominated by CNN classifiers. CNN s are a class of deep feed-forward
neural networks whose architecture is particularly well suited for
computer vision recognition tasks. CNN s have outperformed most of
algorithms in many medical image segmentation problems, arising as the
winner technique in most challenges such as BRATS , ISLES or PROMISE12
challenges. BRATS2015 ; BRATS2016 ; BRATS2017 ; BRATS2018 summarize the
most relevant contributions to the BRATS challenge from 2015 to 2018,
demonstrating the superiority of CNN s in the glioma segmentation task.

However, supervised learning performance depends directly on the quality
and size of the training dataset, which often requires an expensive,
time-consuming and biased task to collect ( Gordillo2013 ) . Moreover,
changes in MRI protocols, acquisition routines or clinical environments
may distort the data and hence affect the performance of the supervised
models ( Duda2000 ) . However, the major drawback of supervised learning
is that it is only capable to address tasks that humans have already
solved before. The mimic nature of supervised learning limits the
paradigm to the set of problems with known solution, which, without
underestimating its unquestionable usefulness, reduces the ML to an
instrument for automatizing well-known tasks.

On the contrary, unsupervised learning address these limitations in a
more straightforward manner. Unsupervised learning does not require a
training corpus from which to learn a model to solve the task, but it
analyzes unlabeled data searching for hidden patterns and inner
relationships that describes their latent structure ( Wittek2014 ) .
Therefore, besides solving already known tasks, this approach has the
innate ability to discover new knowledge from the data by exploring the
arrangement of the information. Sticking to the glioblastoma
segmentation task, unsupervised learning typically fits a customized
segmentation model to the patient’s MRI , describing their own imaging
patterns. This makes it possible to characterize the lesion in each case
just by discriminating its own patterns and not by reinterpreting them
under the knowledge drawn from other examples. By the opposite, the
absence of manual segmentations to guide the learning process makes
segmentation more challenging and often lead to a worse performance with
respect to supervised approaches.

Some attempts for brain tissue segmentation have been made under the
unsupervised paradigm. Fletcher2001 proposed an approach based on fuzzy
clustering and domain knowledge for multi-parametric non-enhancing tumor
segmentation. Domain knowledge and parenchymal tissue detection was
based on heuristics related to geometric shapes and lesion locations,
which may not be robust when high deformation is presented. Moreover,
several assumptions such as prior knowledge about the number of existing
tumor foci or the minimum required thickness of the MRI slices
introduced significant limitations to the method. Nie2009 used Gaussian
clustering with a spatial accuracy-weighted Hidden Markov Random Field (
HMRF ) that allowed them to deal with images at different resolutions
without interpolation. Nowadays, advanced reconstruction techniques such
as super-resolution enables to work in a high resolution voxel space by
reconstructing the low resolution images without interpolation.
Moreover, no automated method was provided to differentiate between
tumor labels and normal tissue labels after the unsupervised
segmentation ends. Zhu2012 developed a software based on the
segmentation approach proposed by Zhang2001 , which performs an GMM
clustering combined whit HMRF s . Zhu et al extended Zhang’s approach
through a sequence of additionally morphological and thresholding
operations to refine the segmentation. Such operations are not fully
specified and only overall commented, so the reproducibility of their
results is not possible. Vijayakumar2007 proposed a method based on
Self-Organizing Map s to segment tumor, necrosis, cysts, edema and
normal tissues using MRI . Although the learning process of SOM s was
performed in an unsupervised manner, the dataset from which to infer the
net structure was determined manually, similar than in a supervised
approach. In their work, 700 pattern observations, corresponding to 7
different tissues, where selected manually, hence converting the process
in a supervised task. Prastawa2003 proposed a similar approach than the
followed in this study. They performed an unsupervised classification
based on FMM s and also used a brain atlas to characterize the normal
tissue labels. However, they made important simplifying assumptions to
allow them to use the atlas without registration. Moreover, they also
simplify the segmentation task in 2 labels (tumor and edema), ignoring
other important tissues such as necrosis or non-enhancing tumor.
Doyle2013 also proposed an approach based on GMM clustering and MRF
priors. They defined different penalizations in the MRF depending on the
adjacency of the labels to prevent incoherent segmentations, however
they did not clearly specify how they related the glioblastoma tissues
with these labels before running the unsupervised segmentation.
Furthermore, all the unsupervised approaches described above applied
their algorithms on its own datasets, making difficult a general
comparison of the methods.

In this work, we propose a fully automated pipeline for unsupervised
glioblastoma segmentation. Our contributions concern the assessment of
the performance of several unsupervised segmentation methods, including
both structured and non-structured classification algorithms, on a real
public and reference dataset; and we also provide a generalized method
to automatically identify pathological labels in an unsupervised
segmentation that represent abnormal tissues in the brain. Our aim is to
demonstrate that unsupervised segmentation algorithms can achieve
competitive results, comparable to supervised approaches, by detecting
imaging patterns that describe the tissue’s MRI profiles.

We evaluated our unsupervised segmentation method using the public BRATS
2013 Leaderboard and Test datasets provided for the International Image
Segmentation Challenge of MICCAI Conference. The proposed method with
the GMM algorithm improves the results obtained by most of the
supervised approaches evaluated with the Leaderboard BRATS 2013 set,
reaching the 2nd position in the rank. Our variant using the Gauss- HMRF
improves the results obtained by the best unsupervised segmentation
methods evaluated with the BRATS 2013 Test set, and also reaches the 7th
position in the general Test rank, mainly against supervised approaches.

### 3.2 Materials

In order to make our results comparable, we have used the public
multi-modal BRATS dataset 2013 ( Menze2015 ) , provided for the
international BRATS 2013 challenge in image segmentation of MICCAI
conference. We have evaluated our method with the Test set and the
Leaderboard set, and we have made a comparison between our proposed
method and the best algorithms that participated in the challenge.

The BRATS 2013 Test set consists of multi-contrast MR scans of 10
high-grade glioma patients without the manual expert labeling. The
Leaderboard set consists of @xmath multi-contrast MR scans of high-grade
glioma patients, also without the manual expert labeling. The first 11
Leaderboard patients come from to the Test set of BRATS 2012 Challenge,
while the next 10 cases refer to the new Leaderboard cases for 2013
Challenge.

For each patient of the datasets, T @xmath -weighted, T @xmath
-weighted, FLAIR and post-gadolinium T @xmath -weighted MR images were
provided. All images were linearly co-registered to the post-gadolinium
T @xmath -weighted sequence, skull stripped, and interpolated to 1 mm ³
isotropic resolution. No inter-patient registration was made to put all
the images in a common reference space.

Expert manual annotations of the MRI studies were performed, considering
five possible labels for the lesion:

    [itemsep=0pt]

  Label 0:  

    background, brain and everything else not corresponding to labels 1,
    2, 3 and 4.

  Label 1:  

    non-brain, non-tumor, necrosis, cyst and hemorrhage.

  Label 2:  

    surrounding edema.

  Label 3:  

    non-enhancing tumor.

  Label 4:  

    enhancing tumor.

An evaluation web page was provided to assess the quality of the
segmentations, computing different metrics such as Dice coefficient,
positive predictive value, sensitivity and Kappa indices. Furthermore,
the evaluation was made over different sub-compartments of the lesion,
to properly measure the performance of the different segmentation
methods. The set of labels composing each sub-compartment are described
in section 3.3.5 .

### 3.3 Methods

This section describes the proposed pipeline for the automated
unsupervised segmentation of glioblastoma, including a generalized
postprocessing designed to identify the pathological classes of an
unsupervised segmentation that represent abnormal tissues in the brain.

#### 3.3.1 Mri preprocessing

MRI preprocessing is an active field of research that attempts to
enhance and correct MR images for their posterior analysis. In an
unsupervised segmentation approach there is no reference nor manual
labeling from which to learn tumor tissue models. Therefore, common
artifacts such as noise, NMR inhomogeneities or registration
missalignments can introduce undesired patterns into images that could
led to erroneous classifications, describing MRI artifacts rather than
physiological tissues. This clearly increases the importance of an
accurate and effective preprocess of the MRI if an unsupervised
segmentation is going to be performed. We propose the following scheme
for the BRATS 2013 data: 1) Denoising, 2) Skull stripping, 3) Bias field
correction and 4) Super-resolution.

##### Denoising

Denoising is a standard MRI preprocessing task that aims to reduce or
ideally remove the noise from an MR image. Although MRI noise has been
usually modeled as Gaussian distributed, by definition MRI noise follows
a Rician distribution ( Gudbjartsson1995 ) . Diaz2011 presented a
comprehensive analysis of different denoising methods, discussing their
weaknesses and strengths. Recent filters such as the NLM introduced by
Buades2005a has improved the existing techniques for MR data. Based on
this approach, Manjon2010a introduced a variant of the NLM filter,
called Adaptive- NLM filter, which does not assume an uniform
distribution of the noise over the image, thereby adapting the strength
of the filter depending on a local estimation of the noise. The filter
also deals with both correlated Gaussian and Rician noise. We used the
Adaptive- NLM filter to remove the noise from the BRATS images.

##### Brain extraction

Brain extraction, also called skull-stripping, comprises the process of
removing skull, extra-meningeal and non-brain tissues from the MRI
sequences. Although BRATS 2013 dataset is already skull stripped, we
detected several cases that include partial areas of the cranium and
extra-meningeal tissues. In order to improve the preprocessing of the
data, we recomputed the skull stripping masks for all patients using the
Brain Suite Software , removing the non desired tissues of the MR
images. Figure 3.1 shows an example of a patient of the BRATS 2013
dataset with the original skull stripping, the resultant image after our
new skull stripping and the remaining residual.

##### Bias field correction

Intensity inhomogeneity is another common artifact present in MRI
acquisitions. Magnetic field inhomogeneities are unavoidable effects
consisting on low frequency signals that corrupt the images and affect
their intensity levels. Typically, automated segmentation approaches
based on MRI are built upon the assumption that tissues have the same
distribution of intensity across the image. Therefore, intensity
inhomogeneities must be corrected to ensure a correct coherent
segmentation. The popular non-parametric non-uniform intensity
normalization N3 algorithm was proposed in 1998 by Sled1998 , becoming a
reference technique for bias field correcting. Tustison2010 proposed in
2010 a new implementation of N3, called N4, which improves the N3
algorithm with a better B-spline fitting function and a hierarchical
optimization scheme for the bias field correction. N4 was used in our
study to correct MRI inhomogeneities.

##### Super-resolution

In a brain tumor lesion protocol, several MR sequences are commonly
acquired normally at different resolutions, thereby introducing spatial
inconsistencies when a multi-modal MR study is performed. In these
cases, an upsampling or interpolation is needed to set a common voxel
space for all sequences. Classic interpolation such as linear, cubic or
splines interpolation could rise as a solution, but at the cost of
introducing artifacts such as partial volume effects or stair-case
artifacts. In contrast, more powerful and sophisticated methods such as
super resolution could improve classic interpolation by reconstructing
the low resolution images recovering its high frequency components.
Several super resolution schemes for MRI are available in the literature
( Plenge2013 ; Manjon2010b ; Rousseau2010 ; Protter2009 ) .

BRATS 2013 dataset comes with a 1mm ³ isotropic voxel size resolution
achieved through classic interpolation. In order to improve the
resolution of these images, we employed the super resolution algorithm
proposed by Manjon2010c , which exploits the self-similarity present in
MR images through a patch-based non-local reconstruction process. Such
method iteratively reconstructs a high resolution image by applying a
NLM filter with different strengths, aimed to increase image regularity
while constraining intensity ranges so that they be coherent among
scales through a local back-projection approach. Figure 3.2 shows an
example of a super resolved FLAIR sequence of a patient of the BRATS
2013 dataset with a detailed zoom of an axial slice.

#### 3.3.2 Feature Extraction and Dimensionality Reduction

Feature extraction comprises the process of obtaining new features from
the MR images to improve discrimination between different labels.
Although MRI intensities are the most common used features to
differentiate between the brain tissues, it has been shown that
including texture features in combination with MR intensities increases
the performance of the segmentation algorithms ( Kassner2010 ; Ahmed2011
) . In this sense, we have implemented the first order statistical
texture features, also called histogram derived metrics or first order
central moments.

For each patient we initially obtained an additional image, named T
@xmath , which consists on the absolute difference between the T @xmath
and the T @xmath images ( Prastawa2003 ) . This image highlights the
contrast enhanced areas of the patient, such as the active tumor,
helping in their discrimination. Next, for each MR image of the patient
(T @xmath , T @xmath , T @xmath , FLAIR and T @xmath ), we computed
their first order texture features. Such features consist on the
computation of local histograms in 3D patches centered at each voxel of
the image, and calculate the mean, skewness and kurtosis of these
histograms. We used local 3D patches of @xmath voxels.

Thus, a set @xmath of 20 images is obtained for each patient, consisting
on the following images:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath , @xmath , and @xmath prefixes refers to the mean, skewness
and kurtosis features of the corresponding image.

In order to reduce the complexity and number of parameters to estimate
in the models, a dimensionality reduction process was performed.
Dimensionality reduction seeks for an efficient representation of the
original high dimensional data into a lower dimensional space, retaining
or increasing the most relevant information. In our study, we used
Principal Component Analysis ( PCA ) for dimensionality reduction. We
run PCA on the @xmath set and selected the principal components, which
together explained at least the 99% of the variance of the data,
reducing in most cases from 20 dimensions to 5 dimensions. These images
make up the final stack of imaging data used for the posterior
unsupervised segmentations.

An slice example of the feature extraction and PCA dimensionality
reduction process of a MR study is shown in Figure 3.3 .

#### 3.3.3 Unsupervised voxel classification

The BRATS 2013 dataset comprises 5 labels to be segmented, which in some
cases a single label encloses different brain tissues (for example 0 or
1 label). This intra-class heterogeneity can severely affect the
performance of unsupervised methods, since heterogeneity is naturally
explained in unsupervised learning through the definition of different
clusters. Hence, the same semantic class in a problem can be modeled
through a set of different clusters.

In this sense, in order to increase the expression capacity of the
unsupervised models to capture such heterogeneity, we assumed that each
tissue can be initially represented by 2 clusters. That is, we will
assume that there exist 7 tissues in the brain, which are labels 1, 2, 3
and 4 proposed in BRATS 2013 Challenge plus Grey Matter ( GM ), White
Matter ( WM ) and Cerebro-Spinal Fluid ( CSF ); each one of them with
initially 2 clusters to represent their heterogeneity. Note that,
therefore, we will estimate an unsupervised clustering of 14 classes
throughout the brain, but we do not require each tissue to use exactly 2
classes. Hence, a tissue showing a high degree of heterogeneity can be
modeled using a set of 3 or 4 clusters, while an homogeneous tissue can
be segmented by a single cluster. Such assumption provided us a balance
between the number of parameters to estimate to the models and the
degrees of freedom required to explain the heterogeneity of the tissues.

We evaluated the most popular unsupervised classification algorithms. We
divided the algorithm comparison in two groups: structured and
non-structured methods. Non-structured algorithms classify data assuming
an i.i.d. condition between the voxels of the images. Structured
prediction covers the range of algorithms that involve the
classification of data with a specific structure, such as an image.
Under the non-structured paradigm, we evaluated three methods: K-means,
Fuzzy K-means and GMM clustering. In the structured prediction case we
evaluated the Gauss- HMRF as the archetype of unsupervised structured
learning models.

Let @xmath the set of voxels to be classified, where @xmath represents a
feature vector of @xmath dimensions for the @xmath voxel. Let @xmath the
set of labels associated to each voxel, where @xmath .

##### K-means

K-means ( Lloyd1982 ; Macqueen1967 ) is an unsupervised non-structured
iterative partitional clustering based on a distance minimization
criterion. Its aim is to divide the data space @xmath into @xmath
clusters @xmath , so that each observation of @xmath belongs to the
cluster with nearest centroid. The distance criterion minimized by
K-means is:

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

where @xmath represents the centroid of the @xmath cluster.

The K-means algorithm performs as follow:

{siderules}

  Initialization:  

    Make an initial guess about the cluster centroids @xmath

  Until  

    there are no changes in centroids @xmath (or in clusters @xmath )

      Assign  

        @xmath to the cluster with nearest centroid @xmath :

          -- -------- --
             @xmath   
          -- -------- --

      Update  

        cluster centroids @xmath

          -- -------- --
             @xmath   
          -- -------- --

From a statistical point of view, the iterative distance minimization
criterion followed by K-means is equivalent to find the most likelihood
parameters of a GMM ( Duda2000 ) , assuming shared identity covariance
matrices and uniform prior probabilities for all classes. The iterative
approach followed by K-means is also demonstrated a special limit of the
EM algorithm ( Dempster1977 ; Bishop1995 ) , called Hard- EM , where
each observation is uniquely assigned to a class with posterior
probability equal to 1.

##### Fuzzy K-means

Likewise K-means, Fuzzy K-means ( Dunn1973 ; Bezdek1981 ) is a
non-structured iterative partitional clustering base on a distance
minimization criterion. Under a probabilistic paradigm, it is also
equivalent to a GMM assuming shared identity covariance matrices and
uniform prior probabilities for all classes. However, Fuzzy K-means
differs from K-means in which the assignment of an observation to a
cluster is not hard but fuzzy . This means that each observation keeps a
degree of membership to each cluster (equivalent to the posterior
probability of a GMM ) rather than a unique assignment of the
observation to a class with posterior probability equal to 1.

In the same manner as K-means, Fuzzy K-means aims to divide the data
space @xmath into @xmath clusters @xmath , but it also keeps a vector
@xmath for each observation that determines the degree of membership of
the @xmath observation to the different clusters. The distance
minimization criterion followed by Fuzzy K-means is:

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

where @xmath represents the centroid of the @xmath cluster.

The membership variable @xmath is typically defined as:

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

where @xmath controls the degree of fuzziness of the @xmath cluster,
with @xmath and typically set to 2 in absence of domain knowledge.

The Fuzzy K-means algorithm performs as follow:

{siderules}

  Initialization:  

    Make an initial guess about the cluster centroids @xmath

  Until  

    there are no changes in @xmath greater than @xmath

      Estimate  

        the membership coefficients @xmath for each observation and
        cluster:

          -- -------- --
             @xmath   
          -- -------- --

      Update  

        cluster centroids @xmath as the expected value of the
        observations given @xmath :

          -- -------- --
             @xmath   
          -- -------- --

##### Gaussian Mixture Model

A GMM is probabilistic model that model a random variable as a convex
combination of Gaussian pdf s . Thus, it provides a statistical
framework to describe the heterogeneity of a dataset trough a weighted
sum of single distributions, each one representing a sub-population
within a data. An in-depth explanation of FMM s and particularly GMM s
is described in section 2.4 . Please refer to this section for more
details.

As a short remainder, the GMM is defined as:

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

where @xmath are the parameters of the model. The parameters @xmath are
typically called mixing coefficients and can be seen as the prior
probability of each component of the mixture describing the data.

Estimation on GMM s is performed via EM algorithm ( Dempster1977 )
because direct MLE estimate does not yield closed-form solutions for the
parameters of the model. For a complete derivation of the MLE estimate
and the EM procedure in FMM s please refer to section 2.4 . The
algorithm for GMM clustering performs as follow:

{siderules}

  Initialization:  

    Choose an initial setting for @xmath .

  Expectation step:  

    Estimate @xmath

      -- -- --
            
      -- -- --

  Maximization step:  

    Update the parameters of the model given @xmath

      -- -- --
            
      -- -- --

    where sticking to GMM s :

      -- -- --
            
      -- -- --

  Convergence:  

    Stop if @xmath ; otherwise @xmath and go to Expectation step .

As stated above, GMM clustering can be seen as the generalization of
K-means and Fuzzy K-means algorithms, where the hard constraints related
to the shared covariance matrices and the uniform prior probabilities
are dropped.

##### Gauss-Hidden Markov Random Field

MRF s are probabilistic undirected graphical models that define a family
of joint probability distributions by means of an undirected graph (
Hammersley1971 ) . These graphs are used to introduce conditional
dependencies between random variables of the model, which in the case of
the image segmentation task, allows capturing the self-similarity and
local redundancy present in the images. These dependencies are
explicitly denoted via an undirected and cyclic graph, whose vertices
represent the pixels/voxels of the images and whose edges represent the
dependencies between them. In this sense, a generative model with a
given set of parameters @xmath incorporating a HMRF can be defined as:

  -- -- -- -------
           (3.5)
  -- -- -- -------

MRF s are usually used to model the prior density @xmath of a
probabilistic generative model. According to the Hammersley–Clifford
theorem, assuming that the prior density is strictly positive, a MRF can
be defined in terms of local energy functions and therefore it can be
expressed as a Gibbs measure in the form:

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

where @xmath is an energy function that defines the conditional
dependencies between the random variables via the graphical model, and
@xmath is called the partition function and acts as a normalizer to
ensure the density to sum 1:

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

Hammersey-Clifford theorem also states that @xmath can be factorized
over the cliques of the undirected graphical model. A clique is defined
as a subset of vertices in the graph such that there exist an edge
between all pairs of vertices in the subset. Let @xmath the set of all
cliques of the graph, the energy function @xmath is defined as:

  -- -------- -- -------
     @xmath      (3.8)
  -- -------- -- -------

Nowadays, if complexity is considered, the inference algorithms for MRF
s are only able to optimize undirected graphs with cliques of order 2
(pairwise cliques), i.e. @xmath . Hence, the most widely used graphical
model is the Ising model . The Ising model defines a graph lattice with
as many vertices as pixels/voxels exist in the image, where conditional
dependencies of each variable are expressed in terms of its orthogonal
adjacent neighborhood. The clique factorization for the Ising model is
then performed in the form:

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

where @xmath is a weight defined for the corresponding clique, and
@xmath is a function that measures the dissimilarity between the clique.
Typically, in absence of domain knowledge, @xmath is usually set to 1
and @xmath is defined as:

  -- -------- -- --------
     @xmath      (3.10)
  -- -------- -- --------

Typically, the class conditional density @xmath is also expressed in
terms of Gibbs measures to take advantage from MRF solvers, so it is
usually rewritten as:

  -- -------- -- --------
     @xmath      (3.11)
  -- -------- -- --------

where @xmath can be assumed i.i.d. and is proportional to the class
conditional @xmath density, and @xmath is again a partition function to
ensure the distribution to sum 1. Under a Gaussian model, @xmath can be
defined as:

  -- -------- -- --------
     @xmath      (3.12)
  -- -------- -- --------

As a results, the complete structured model is defined as:

  -- -------- -- --------
     @xmath      (3.13)
  -- -------- -- --------

Exact inference on this model is intractable due to the sum over all
possible set of labels in @xmath , which becomes a @xmath problem.
However, approximate efficient algorithms to compute the best labeling
@xmath are proposed in the literature. Iterated Conditional Modes (
Birchfield1998 ) , Monte Carlo Sampling ( Geyer1992 ) , Loopy Belief
Propagation ( Yedidia2003 ) , Mean Field Approximation ( Parisi1998 ) or
Graph cuts ( Boykov2001 ) are some of the available algorithms to solve
pairwise MRF s -based models. In our study we used the FastPD algorithm
proposed by Komodakis2007 ; Komodakis2008 , which has been demonstrated
to be superior to most of the aforementioned algorithms. This algorithm
is based on a combination of Graph cuts with primal-dual strategies.
Therefore the best labeling is computed as:

  -- -------- -- --------
     @xmath      (3.14)
  -- -------- -- --------

In this sense, we can think of @xmath as a binary one-hot encoding
variable holding the posterior probability of each label for each
pixel/voxel, with @xmath for the winner class and @xmath for the
remaining classes.

Therefore, likewise GMM , Gauss- HMRF also finds the MLE parameters of a
mixture of Gaussian distributions that better fits the data, but
imposing the structured MRF prior. A Hard- EM version (given that exact
inference is not possible) of the EM algorithm is employed to estimate
the parameters of the model in the form:

{siderules}

  Initialization:  

    Choose an initial setting for @xmath .

  Expectation step:  

    Estimate @xmath by using FastPD (or similar) algorithm

      -- -------- --
         @xmath   
      -- -------- --

  Maximization step:  

    Update the parameters of the model given @xmath

      -- -- --
            
      -- -- --

    where sticking to GMM s :

      -- -- --
            
      -- -- --

  Convergence:  

    Stop if @xmath ; otherwise @xmath and go to Expectation step .

##### Algorithms initialization

A well-known requirement of unsupervised learning is the good initial
seeding. Although the global minima is not usually reached even if a
good initialization is provided, a bad initialization can lead the model
to a very sub-optimal local minimum, thereby providing a poor
segmentation. Several strategies such as multiple replications or
intelligent initial seeding are proposed to palliate this effect. In our
study, we implemented the K-means++ algorithm ( Arthur2007 ) , which
provides an initialization that attempts to avoid local sub-optimal
minimums.

We propose the following procedure to ensure a competitive unsupervised
segmentation: First, generate 100 different initializations using
K-means++ algorithm. Next, automatically select the 10 most promising
initializations by minimizing the average intra-cluster sums of
point-to-centroid distances of the initializations. Finally, run each
unsupervised segmentation algorithm with the 10 most promising
initializations and choose the best solution considering the following
criteria:

    [itemsep=0pt]

  K-means:  

    choose the solution with lowest intra-cluster sums of
    point-to-centroid distances.

  Fuzzy K-means:  

    choose the solution with lowest intra-cluster sums of
    point-to-centroid distances.

   GMM    :  

    choose the solution with highest log-Likelihood value.

  Gauss-   HMRF   :  

    choose the solution with highest log-Likelihood value.

#### 3.3.4 Automated pathological label identification

Unsupervised segmentation produces a partitioning of the data space into
several classes, each one without semantic sense. In other words, in the
brain tumor unsupervised approach, labels do not directly identify a
specific tissue but only distinguish between MRI data different enough
from each other to be considered equal. Moreover, labelings among
different patient segmentations may not always represent the same
tissue, complicating its biological interpretation. Hence, an automated
pathological label identification is mandatory to provide a powerful and
competitive unsupervised segmentation method. We propose the following
method to automatically isolate pathological labels:

1.  [itemsep=0pt]

2.  Identify and remove WM , GM and CSF labels.

3.  Remove outlier and partial volume labels.

4.  Merge labels by statistical distribution similarities.

##### Identify and remove WM, GM and CSF labels

Under the International Consortium of Brain Mapping ( ICBM ) project, an
unbiased standard MR brain atlas was provided by the McConnell Brain
Imaging Centre in 2009 ( Fonov2011 ; Fonov2009 ) . The ICBM atlas
include a T @xmath , T @xmath and Proton density MR images, with the
associated WM , GM and CSF tissue probability maps. Such tissue
probability maps indicate the probability for each voxel @xmath of the
brain to belong to a normal tissue @xmath , with:

  -- -------- -- --------
     @xmath      (3.15)
  -- -------- -- --------

In our study we used these tissue probability maps to detect which
labels of a segmentation explain the WM , GM and CSF tissues. However,
taking into account that the ICBM template represents a healthy brain,
it is necessary to corrected these maps by setting to zero (or a smaller
@xmath value) the probability of any voxel @xmath in the area of the
lesion. Therefore, we first performed a non-linear registration of the
ICBM T @xmath template to the T @xmath sequence of the patient and
applied the warp transformation to the tissue probability maps.
Following the study conducted by Klein2009 , we used the SyN algorithm (
Avants2008 ) implemented in the ANTS software with cross-correlation
similarity metric. Once we obtained the patient aligned healthy-tissue
probability maps, a roughly approximate mask of the lesion of each
patient was computed to correct the probability maps. The typical
delineation of the lesion performed by expert radiologists is usually
based on the hyper-intensity areas in the T @xmath and T @xmath
sequences ( Bauer2013 ) . Following a similar criteria, we computed an
approximate mask of the lesion by selecting the voxels whose value in
the FLAIR and T @xmath images were higher than the median plus the
standard deviation of the corresponding image. Next, we automatically
filled the holes of the computed masks and removed the voxels that fell
in the perimeter of the brain (typically showing hyper-intensities due
to cranial traces). Finally, we corrected the healthy-tissue probability
maps of each patient by setting an @xmath probability in the area
determined by their corresponding lesion masks. Figure 3.4 shows an
illustration of the algorithm to compute the corrected tissue
probability maps for a patient.

Based on these patient healthy-tissue probability maps, we identified
which classes of an unsupervised segmentation mostly represent a normal
tissue. Let @xmath a segmentation obtained through any unsupervised
method, and let @xmath the set of voxels @xmath of @xmath classified
with label @xmath . Let @xmath a normal tissue where @xmath . To perform
the pathological tissue identification we computed the following
probability mass:

  -- -------- -- --------
     @xmath      (3.16)
  -- -------- -- --------

Simplifying, the @xmath determines how much of the normal tissue @xmath
is explained by the label @xmath in the segmentation @xmath .

Therefore, for a given tissue @xmath , and based on these probabilities,
we constructed two vectors: one with the @xmath values sorted in
descending order, denoted as @xmath , and the other with the
corresponding label codes sorted in the same manner, denoted as @xmath .

  -- -------- -- --------
     @xmath      (3.17)
  -- -------- -- --------

Then, we computed the cumulative sum of @xmath , denoted as @xmath , and
finally choose the set of labels of @xmath whose @xmath value exceed a
threshold @xmath .

  -- -------- -- --------
     @xmath      (3.18)
  -- -------- -- --------

The @xmath set contains the classes of @xmath that have a very low
probability of explain the normal tissue @xmath . Hence, we repeated the
same procedure for each normal tissue @xmath and computed the
intersection between the @xmath sets to finally isolate the labels that
do not explain any normal tissue, i.e. the pathological classes:

  -- -------- -- --------
     @xmath      (3.19)
  -- -------- -- --------

Given that our aim is to evaluate the performance of each unsupervised
segmentation algorithm, all of them in the same conditions, we do not
carried out any particular optimization of the @xmath threshold for each
algorithm. Instead, we fixed a general threshold for all the methods,
set to @xmath , as a reasonably, high confidence and compatible value to
perform the pathological labels identification. Note that @xmath is not
possible since this implies that all the classes of the segmentation
will be required to explain only a single normal tissue.

##### Remove outlier and partial volume labels

The above process of identifying and removing the healthy-tissue labels
may leave some spurious labels that must be deleted. We found that these
labels frequently appear in the perimeter of the brain or in a very low
proportion compared to the other labels of the segmentation. The labels
located at the perimeter of the brain typically correspond to remaining
traces of cranium, intensity artifacts between brain and background or
partial volume effects that super resolution cannot resolve. The low
proportion labels often match to outlier voxels in terms of abnormal
intensity values, usually produced by artifacts in the MR acquisition.

In order to remove the perimeter unwanted labels, we deleted all the
connected components of all the remaining labels of the segmentation,
which overlapped more than the 50% of its area with a binary dilated
mask of the perimeter of the brain. To remove the smaller low proportion
labels we removed those ones with a prevalence less than the 1% the
remaining segmentation after the above processes.

##### Merge labels by statistical distribution similarities

The heterogeneity of the pathological labels led us to assume that each
tissue was initially modeled by two clusters. However, this was a
general but not strict assumption, i.e. we did not enforce to use
exactly two cluster per tissue. Instead, we estimated a general
clustering of 14 components for each case. Hence, a tissue may have been
represented by two or more labels in the segmentation or, conversely, by
a single label depending on its homogeneity. Therefore, it was mandatory
to design a mechanism to find which labels were explaining the same
semantic concept, i.e. the same pathological tissue.

Based on the work proposed by Saez2017 , we analyzed the MRI intensity
distributions of the remaining labels to find potential clusters
representing the similar information. To do so, we estimated the pdf of
each label through a kernel smoothing density estimation, and used the
Jensen-Shannon divergence to measure the pairwise distances among them.
Therefore, we constructed a pairwise matrix of statistical distribution
distances and used Hierarchical Agglomerative Clustering ( HAC ) with
average link ( Unweighted Pair Group Method with Arithmetic mean ( UPGMA
)) to merge the similar labels.

Due to the BRATS 2013 labeling considers 4 pathological labels to be
segmented, we enforced the clustering to return a maximum of 4 classes.
Note that the method is able to return less than 4 classes if the HAC
finds enough similarities between them, however, in any other case the
method is enforced to return a maximum of 4 labels. Figure 3.5 shows and
example of the pathological labels isolation procedure.

#### 3.3.5 Evaluation

We evaluated our unsupervised brain tumor segmentation framework with
the BRATS 2013 Leaderboard and Test datasets. Segmentations provided by
the different unsupervised methods in combination with the proposed
preprocessing and postprocessing pipelines were sent to the BRATS
evaluation web page. The figures of merit provided to assess the quality
of the segmentations were:

-   [itemsep=0.5pt]

-   Dice: @xmath

-   PPV: @xmath

-   Sensitivity: @xmath

-   Kappa: @xmath

where @xmath refers to the true positives in the segmentation, @xmath to
the true negatives, @xmath to the false positives, @xmath to the false
negatives, @xmath to the real positives of the ground truth, @xmath to
the real negatives of the ground truth, @xmath to the estimated
positives of the proposed segmentation, @xmath to the estimated
negatives of the proposed segmentation, @xmath to the accuracy of the
segmentation and @xmath to a term that measures the probability of
success by chance, defined as: @xmath .

Furthermore, three different sub-compartments of the lesion were
evaluated to properly assess the quality of the segmentation methods.
Table 3.1 describes the labels involved in each sub-compartment
considered in the evaluation.

### 3.4 Results

Tables 3.2 and 3.3 show the results obtained in the Test and Leaderboard
datasets respectively, grouped by the unsupervised algorithms tested in
this study.

As expected, Gauss- HMRF and GMM demonstrate their superiority with
respect the other algorithms. Almost all the metrics reveal that both
algorithms obtain the best results in all the sub-compartments
segmentations. Only the enhancing tumor sub-compartment in the
Leaderboard set yielded worse results for the Gauss HMRF compared to the
results obtained in the other sub-compartments and datasets. Such effect
typically occurs because of the smoothing prior of the Gauss- HMRF ,
which is later discussed in the Discussion section.

Tables 3.4 and 3.5 show the published ranking of the BRATS competition
grouped by the learning paradigm adopted by each method and the metrics
and sub-compartments evaluated in the Challenge. As shown in Table 3.4 ,
we achieved the @xmath position in the ranking of the unsupervised
methods of the Test set, and the @xmath position in the general ranking,
mostly against supervised approaches. Table 3.5 shows the Leaderboard
ranking and the results achieved by our method. The proposed approach in
combination with the GMM algorithm reaches the @xmath position of the
Leaderboard ranking, improving the results obtained by many supervised
methods, mainly in the enhancing tumor sub-compartment.

Table 3.6 shows the average time in minutes required to obtain a
segmentation for a single patient, including the preprocessing and
postprocessing stages. Segmentations were computed in an Intel Xeon
E5-2620 with 64GB of RAM using multi-threading. The preprocessing stage
includes the denoising, bias field correction, skull-stripping and super
resolution steps. The unsupervised classification time involves the
parallel computation of the 10 different segmentations starting from the
K-means++ initialization, and the posterior selection of the best
solution. As expected, the more complex and sophisticated the algorithm
is, the longer it takes to reach the solution. The postprocessing stage
consist in the automated pathological label identification method, the
outlier label removal and the merging process of similar statistical
distribution labels. Such process includes the non-linear registration
of the ICBM template to the patient T @xmath image, which practically
covers the entire time of the postprocessing stage. It is worth noting
that the non-linear ICBM registration is performed only once for all the
unsupervised segmentation algorithms.

Finally, examples of segmentations achieved by the different
unsupervised segmentation algorithms evaluated are shown in Figure 3.6 .

### 3.5 Discussion

In this study we have conducted an evaluation of the performance of
several unsupervised learning algorithms for the glioblastoma
segmentation task. In addition to the comparative, we have proposed a
complete pipeline for automated brain tumor segmentation, including a
postprocessing stage to automatically identify labels corresponding to
pathological tissues in an unsupervised segmentation.

The proposed method is confirmed as a viable alternative for
glioblastoma segmentation, as it has demonstrated to achieve competitive
results in a public real reference dataset for brain tumor segmentation.
The method improved the results obtained by the other unsupervised
segmentation approaches evaluated in the BRATS 2013 Challenge, and
obtained competitive results with respect to supervised methods.
Moreover, this study confirmed the capability of unsupervised learning
to detect consistent patterns in medical imaging data related to MRI
properties of the tissue, which will serve as basis for the next work on
this thesis.

The proposed unsupervised segmentation pipeline comprises four stages:
MRI preprocessing, feature extraction and dimensionality reduction,
unsupervised voxel classification and automatic pathological label
identification. Concerning the preprocessing stage, consolidated state
of the art techniques that provide efficient solutions to enhance the
information of the MR images were employed. However, some preprocessing
techniques are primarily oriented to non-pathological brains. This is
the case of bias field correction. In our experiments, we found that the
estimation of the magnetic field inhomogeneities with the N4 algorithm
presented problems primarily with FLAIR sequences. The hyper-intensity
presented in the FLAIR sequence related to the edema was confused
frequently with inhomogeneities of the magnetic field, thereby reducing
its intensity. In order to overcome this problem we reduced the number
of iterations of the algorithm to remove as much inhomogeneities as
possible, while keeping the intensities of the lesion. Such solution
assumed a non optimal removal of the magnetic field inhomogeneities, but
allowed to save the information contained in the lesion area, which
becomes more important to the segmentation task. We empirically set a
maximum of 10 iterations at each scale of the multi-scale approach of
the N4 algorithm.

Several unsupervised classification algorithms were evaluated to assess
its pros and cons, ranging from the most restrictive algorithms in terms
of class-conditional probabilistic models (K-means and Fuzzy K-means) to
more sophisticated models with more degrees of freedom such as GMM or
Gauss- HMRF . The last one, also introduces statistical dependencies
between adjacent variables of the model, that penalizes neighboring
voxels with different labels. Hence, this structured prior aims to model
the self similarity of the images, leading the algorithm to a more
homogeneous segmentation than the non-structured classification
techniques.

Therefore, the less restrictive algorithms were expected to achieve
better results based on the hypothesis that these algorithms learn a
more flexible model that best fits the data to be classified. Moreover,
structured algorithms were also expected to obtain better results based
on the hypothesis that these algorithms introduce mechanisms to model
the self similarity of the images. Tables 3.2 and 3.3 confirm such
hypotheses. Both GMM and Gauss- HMRF rose as the best algorithm tested
in almost all the metrics returned by the evaluation web page. Only the
results obtained by the Gauss- HMRF model in the enhancing tumor
sub-compartment of the Leaderboard set were not comparable with the
other sub-compartments and datasets results. This effect was due to the
smoothing prior imposed by the Gauss- HMRF , which was too strong in
some cases. We revised the cases that achieved low results in the
enhancing tumor sub-compartment and realized that most of them had a
large necrotic core with a thin low-brightness enhancing tumor ring. We
also revised the K-means++ initializations and realized that the
enhancing tumor was partially segmented in some cases but finally lost
in the final segmentation due to the hard smoothing prior in the
necrotic class. We are currently working on the introduction of
different penalizations for the labels, depending on their statistical
distribution similarities to avoid this over-smoothing.

It is worth noting that we obtained better results on the Leaderboard
set (Table 3.5 ) than in the Test set (Table 3.4 ), in contrast with the
rest of participants. This effect may have been produced by the fact
that the Leaderboard set may include more heterogeneities and
differences with respect to the Training set than to the Test set,
thereby directly affecting the supervised approaches performance.
Unsupervised paradigm avoids this possible overfitting by building a
particular model for each patient considering only its own data,
therefore achieving better results in the Leaderboard set against most
of the supervised approaches evaluated.

In future work, we plan to improve our feature extraction process by
analyzing the influence of the texture images in the final segmentations
and including more sophisticated textures such as the Haralick texture
features. Furthermore, we plan to extend our unsupervised methodology to
the analysis and segmentation of PWI in combination with anatomical
images. The biomarkers obtained from PWI might discover relevant
segmentations by adding additional valuable functional information about
the tissues. We consider that research efforts should be aligned with
quantitative MRI by providing powerful systems that leverage the
information contained in these images.

## Chapter 4 Non Local Spatially Varying Finite Mixture Models for
unsupervised image segmentation

As stated in the previous chapter, image segmentation is one of the most
important core problems in computer vision. It constitutes one of the
basic and fundamental steps for automated image understanding since its
purpose is to delineate objects in the image with semantic meaning.
Innumerable approaches have been proposed in the literature to address
this problem, ranging from supervised to unsupervised ML approaches. The
latter are indispensable to the image understanding and segmentation
task as they provide a robust and reliable solution to all the problems
that do not have manually annotated datasets, which ultimately represent
the vast majority of real-life image segmentation problems.

Images are structured arrangements of data in which, in addition to the
pixel intensities, the location of the pixels provides important
information to properly understand its content. Structured learning
models capable to properly capture the patterns of local regularity and
spatial redundancy of the images have demonstrated their superiority in
the image segmentation task.

In this chapter a Bayesian model for unsupervised image segmentation
based on a combination of the SVFMM s and the NLM framework is
presented. Such model successfully integrates a gauss-markov random
field into a classic FMM , to simultaneously codify the idea that
neighboring pixels tend to belong to the same semantic object, but
preserving the edges and structure of the image. The chapter introduces
the mathematical foundations of the model and their estimation via a MAP
- EM scheme. We present an evaluation of the performance of the model in
a synthetic medical imaging corpus and with a reference dataset of
real-world images.

The contents of this chapter were published in the journal publication (
JuanAlbarracin2019b ) —thesis contributions C2 and P3.

### 4.1 Introduction

Unsupervised learning has historically played a key role in the image
segmentation task, constituting one of the first paradigms to
automatically identify objects and structures in an image ( Zhang2008 )
. Specifically, clustering has gathered most of the efforts in
unsupervised image segmentation research. Clustering is the task of
finding natural groupings of data within a population, sharing a similar
set of properties ( Rokach2005 ) . Many clustering techniques have been
proposed in the literature during the past decades ( Saxena2017 ) ,
ranging from distance based techniques such as partitional clustering or
hierarchical clustering; density-based techniques such as DBSCAN (
Ester1996 ) or Mean Shift ( Cheng1995 ) ; graph based algorithms such as
graph-cuts ( Boykov2001 ) ; or probabilistic models such as Finite
Mixture Model s ( FMM s ) ( Pal1993 ) .

Specifically, probabilistic models intend to learn the pdf of an image
by means of fitting a multi-parametric statistical model to the data. In
particular, FMM s fit a weighted sum of probabilistic distributions,
each one representing a component of the image, to capture the
heterogeneity nature of the image information. GMM s are the most
extended FMM s , being widely employed for image segmentation, as they
have proven to successfully capture the complexity of an image (
JuanAlbarracin2015a ) . Moreover, GMM s can be efficiently estimated by
means of MLE via the Expectation-Maximization ( EM ) algorithm (
Dempster1977 ) .

However, learning from an image has several particularities that must be
taken into account. Images are structured arrangements of data in which,
in addition to the pixel intensities, the location of these intensities
provide important information to properly understand its content. Images
show patterns of local regularity and spatial intensity redundancy that
enclose the idea that adjacent pixels tend to belong to the same
semantic object. Conventional FMM s , by the opposite, do not inherently
take into account this information. FMM s make the heavy assumption that
data in an image is i.i.d. , ignoring the spatial information that has
demonstrated to be useful to generate more accurate and realistic
models.

To overcome this limitation, several solutions have been proposed in the
literature ( Blake2011 ) . Most of them rely on the inclusion of a MRF
to model the local dependencies between pixels in an image.
Specifically, a variant to the FMM called SVFMM was proposed by
Sanjay1998 , which replaces the classics mixing coefficients of the FMM
by contextual mixing coefficients for each pixel of the image. This
approximation allows to introduce a continuous MRF over these contextual
mixing coefficients to incorporate the idea that neighboring pixels
tends to share the same intensity properties.

Many variants of MRF s have been proposed in the literature to capture
the local information contained in an image. Nikou2007 proposed a family
of Gauss- MRF s , successfully achieving better results than the classic
FMM s . However, such approximation introduces a local isotropic
smoothing over the contextual mixing coefficients, that ignores the
presence of edges in the image. Therefore, the contextual mixing
coefficients estimated under the Gauss- MRF approximation are
iteratively smoothed, yielding prior probability maps that lose the
information of image edges. Sfikas2008 proposed a t-Student MRF that
allowed to regulate the smoothing between pixels in an edge. However,
this approximation introduces new parameters to be estimated in the
model, yielding a non closed-form analytic solution for it.

In this chapter a fully Bayesian SVFMM model, called NLSVFMM , that
combines the SVFMM framework with the NLM filtering schema is proposed.
The model has 2 variants: the pixel-wise version (NLv- SVFMM ) and the
patch-wise version (NLp- SVFMM ). The proposed model introduces a Gauss-
MRF weighted by the probabilistic NLM function proposed by Wu2013 to
adaptively adjust the spatial regularization depending on the structure
of the image. Such approximation avoids the introduction of new
parameters, reducing the degrees of freedom of the model and the number
of samples required for a reliable estimation of the parameters.

### 4.2 Background on Spatially Varying Finite Mixture Models

The SVFMM is a modification of the classic FMM , focused mainly on
imaging data, in which the coefficients of the mixture are related to
each other through a structured graph that defines the statistical
dependencies between them. SVFMM s are thoroughly introduced in section
2.4 , however, in order for this chapter to be self-contained, a short
remainder will be made.

Let @xmath a set of observations corresponding to the pixels of an
image, where @xmath and represents a vector of @xmath features for the
@xmath pixel. The SVFMM is defined as:

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

where @xmath is a pdf used to model the data (typically a Normal or
t-Student distribution) and @xmath the set of parameters of the model,
with @xmath called the contextual mixing coefficients , which must
comply with:

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

A MAP estimate of @xmath is typically conducted to impose a proper prior
over @xmath to introduce the idea that neighboring pixels in an image
tend to belong to the same semantic object.

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

Several variants of @xmath have been proposed in the literature.
Specifically, the DCAGMRF , introduced in equation 2.65 , has proven to
successfully capture the local redundancy and spatial regularity
inherent in images, regularizing many ill-posed inverse problems with
successful results (please refer to section 2.4.2 for a detailed
description of the DCAGMRF prior).

As stated in section 2.4.2 , inference on SVFMM s is not analytically
tractable. Numerical optimization methods are therefore required to
estimate @xmath in a tractable manner. Typically, a MAP - EM algorithm
is used to iteratively find the updates of the model parameters, based
on the conditional expectation of the log-likelihood function, used to
guide the estimation procedure. Nevertheless, no closed-form solution
can be obtained for @xmath when considering the restriction stated in
4.2 . To overcome this limitation, an alternative solution is to
consider @xmath as a random variable governed by a DCM distribution.
Nikou2010 demonstrated that, following such approach, @xmath can be
computed as:

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

with @xmath the parameters of the Dirichlet distribution. It is easy to
see that such approach always guarantees that @xmath , hence satisfying
the condition settled in 4.2 . In addition, @xmath parameters only
require to satisfy @xmath , making easier their optimization.

Therefore, following this DCM - SVFMM approach, the DCAGMRF density can
be imposed over @xmath (instead of over @xmath ), to enforce the desired
local regularity:

  -- -- -- -------
           (4.5)
  -- -- -- -------

where sub-index @xmath refers to the different spatial adjacency
directions (i.e. horizontals, verticals or diagonals), and @xmath
indicates the set of neighbors of the @xmath pixel that lies in the
@xmath spatial direction.

The complete step-by-step estimation of the parameters of the DCM -
SVFMM via the EM algorithm is described in section 2.4.3 . We encourage
the reader to review this section for an in-depth explanation of the
estimation procedure of the DCM - SVFMM s .

### 4.3 Background on Probabilistic Non Local Means

The NLM filter ( Buades2005b ) proposes a schema for image filtering
where pixels are restored by a weighted sum of similar neighbor patches.

The core of NLM schema is the weight function that relates neighboring
patches, which has taken a lot of variants in the literature. Specially,
Wu2013 derived the probabilistic version of the NLM algorithm and its
associated probabilistic weighting function.

In order to relate the description of the probabilistic NLM with the
SVFMM background, let’s consider @xmath as the distance between a pair
of adjacent Dirichlet parameters in the form:

  -- -------- -- -------
     @xmath      (4.6)
  -- -------- -- -------

Assuming that local differences are i.i.d. , we have @xmath . For a
patch-based version of the algorithm, the distance between two patches
centered at @xmath and @xmath locations is defined as:

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

where @xmath is the set of offsets that define a local patch around a
given pixel. If patches are completely disjoint, then @xmath , however,
in most cases, overlapping occurs between patches, so the i.i.d.
assumption does not hold. In such cases, an approximation to the sum of
a set of correlated @xmath distributions can be computed as:

  -- -------- -- -------
     @xmath      (4.8)
  -- -------- -- -------

where

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

and

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

with @xmath the set of overlapping pixels between the patches centered
at @xmath and @xmath pixels.

Hence, the weight function @xmath proposed in the probabilistic NLM
approach, associated to the SVFMM framework, is defined as

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

### 4.4 The Non Local Spatially Variant Finite Mixture Model

One of the main drawbacks of the SVFMM -based models is that they
enforce a local smoothness on the contextual mixing coefficients (or
Dirichlet parameters) without taking into account the structure of the
image. In other words, the SVFMM iteratively applies an isotropic local
Gaussian smoothing to these parameters, leading to a over-smoothed prior
probability map that losses the information of the edges and structures
in the image.

To overcome this limitation, Sfikas2008 proposed a variant of the SVFMM
where local differences between Dirichlet parameters follow a t-Student
distribution. Such an approach was intended to exploit the heavy-tailed
nature of the t-Student distribution, to perform a robust estimation of
the Dirichlet coefficients when edges and structures are present in
their local neighborhoods.

  -- -------- -- --------
     @xmath      (4.12)
  -- -------- -- --------

Following the Bishop’s development in ( Bishop2006 ) , a @xmath
distribution can be expressed as:

  -- -------- -- --------
     @xmath      (4.13)
  -- -------- -- --------

This model introduces a new set of latent variables @xmath , whose
posterior density should be estimated at the E-step, and a new set of
parameters @xmath , with non closed-form analytic estimation. Therefore,
numerical optimization methods such as Newton-Raphson or Brent’s methods
should be employed to estimate @xmath .

In this sense, and similar in spirits than the t-Student model, in this
chapter we propose the NLSVFMM as a modification of the Sfikas’
t-Student model, by replacing the @xmath random variable by the
probabilistic NLM @xmath weight. Therefore, we propose to reformulate
the local differences between contextual Dirichlet parameters to follow
a NLM -related distribution, denoted by:

  -- -------- -- --------
     @xmath      (4.14)
  -- -------- -- --------

with @xmath being latent variables of the model.

Following the conventional EM scheme, the posterior densities of @xmath
should be calculated at the E-step. However, this leads to a different
calculation of @xmath than the proposed by Wu2013 (see Section 4.3 ).
Therefore, in order to preserve the use of the original NLM weights, we
will follow a Variational EM approach ( Neal1999 ; Bishop2006 ) . The
Variational EM framework introduces the concept of partial E-step, in
which a functor of the latent variables can be used when the posterior
densities of these variables cannot be calculated, or when it is
desirable to calculate them differently for reasons of efficiency or
performance. As demonstrated by Neal1999 , such functor can take any
form as long as the log-likelihood function is increased at each
iteration, effectively driving the model to a local optimum of the
function, and hence to an optimum of the parameters of the model.
Therefore, following this framework, the @xmath latent variables are
estimated at the E-step as the standard quantitative Chi-squared test
proposed by Wu2013 :

  -- -------- --
     @xmath   
  -- -------- --

Once these latent variables are estimated, the @xmath weights are
calculated at the M-step following @xmath . Since @xmath depends on both
the @xmath and @xmath observations, this model specifies a different
instance of a Gaussian distribution for each @xmath pair of contextual
Dirichlet coefficients in the MRF . This allows @xmath to regulate the
variance of the corresponding Gaussian between the @xmath and @xmath
observations, if an edge or an homogeneous area is detected at this
location. Thus, as @xmath increases, the Gaussian distribution for the
corresponding pair shrinks around zero imposing a hard smoothing between
the observations. On the contrary, as @xmath decreases, the variance of
the Gaussian distributions increases producing a lower pdf value that
prevents the smooth.

This approximation avoids the introduction of new parameters since
@xmath and @xmath are completely known once @xmath and @xmath are fixed.
Therefore, no numerical approximate methods are required, simplifying
the model and reducing its degrees of freedom and the number of samples
required for its statistically reliable estimation.

The graphical model of the NL- SVFMM is shown in Figure 4.1 .

Imposing the DCAGMRF prior to the proposed NLSVFMM model, the new
density for @xmath becomes

  -- -------- -- --------
     @xmath      (4.15)
  -- -------- -- --------

which setting @xmath yields a third degree equation of the form:

  -- -- -- --------
           (4.16)
  -- -- -- --------

where

  -- -------- -- --------
     @xmath      (4.17)
  -- -------- -- --------

Likewise the conventional DCM - SVFMM , it can be demonstrated that,
under polynomial theory, there is always a real non negative solution
that satisfies @xmath . The Vieta’s method is used to algebraically
obtain the roots of the proposed third degree equation.

Finally, @xmath is estimated as:

  -- -- -- --------
           (4.18)
  -- -- -- --------

Hereafter, the pixel-wise @xmath version of the proposed NLSVFMM will be
referred as NLv- SVFMM , while the patch-wise @xmath will be referred as
NLp- SVFMM .

### 4.5 Experimental results

Both variants of the proposed NLSVFMM algorithm have been evaluated in a
simulated and a real-world scenario. First an evaluation on a synthetic
high grade glioma dataset of the BRATS 2013 Challenge was conducted (
Menze2015 ) . Next, and evaluation over the 300 real-world images of the
Berkeley Segmentation dataset was performed ( Martin2001 ) . We have
compared our proposed NLv- and NLp- SVFMM model with the conventional
FMM , the SVFMM and the @xmath t- SVFMM . For the spatially varying
algorithms we have employed the DCM Bayesian approximation and the
DCAGMRF prior specified in 4.15 . All algorithms in all experiments were
initialized with a deterministic version of K-means++ ( Arthur2007 ) to
ensure a fair comparative. In addition, results on the behavior of the
weighting functions and weighting maps of the NLSVFMM and t-Student
model are shown. The evaluation is presented below.

#### 4.5.1 Evaluation on the synthetic BRATS 2013 high grade glioma
dataset

The BRATS 2013 high grade glioma synthetic dataset is composed of 25
cases, each one segmented into 7 labels: 1) WM , 2) GM , 3) CSF , 4)
peripheral edema (ED), 5) tumor core (split into enhancing tumor (5.1)
and necrotic core (5.2)) (TC) and 6) vessels (VS). For each voxel,
intensities on pre- and post-gadolinium T @xmath -weighted, T @xmath
-weighted and FLAIR MRI sequences were employed for the segmentation.

Figure 4.2 compares the behavior of the weighting functions @xmath for
the @xmath t- SVFMM model and @xmath for the NLv- SVFMM model (NLp-
SVFMM weighting function is not depicted because is not numerically
comparable to the @xmath t and NLv- SVFMM models).

As figure shows, @xmath function behaves more aggressive for differences
between observations than the @xmath t- SVFMM , hence yielding more
dichotomous weighting maps (see Figure 4.3 ). For the shake of
simplicity, each pixel of each picture of Figure 4.3 represent @xmath ,
with @xmath for @xmath t- SVFMM and @xmath for NLv- or NLp- SVFMM models
respectively. The weighting maps of Figure 4.3 demonstrates that the
NLSVFMM -based algorithms better differentiates among tissues than than
the @xmath t- SVFMM .

Table 4.1 and Figure 4.4 show the superiority of the proposed NL- SVFMM
(in both variants) to generate higher confidence prior probability maps
for each component. An example of the contextual mixing coefficient maps
for the HG0014 case of the BRATS 2013 dataset and its associated mixing
coefficient values for different pixels obtained by each method is
shown. In almost all evaluations, the NLp- SVFMM version achieves the
best results, indicating that the patch-based probabilistic NLM
weighting function better captures the local similarities in the images.

Table 4.2 shows the Dice coefficients obtained for the evaluation based
on the BRATS 2013 dataset. Consistently with previous results, the NLp-
SVFMM variant achieves the best results in terms of segmentations based
on the maximization of the posterior probabilities (Bayes minimum
classification error). An improvement of about 3 points in Dice is
obtained when comparing the NLp- SVFMM with the standard SVFMM and more
than 1 point in Dice with respect to the @xmath t- SVFMM , thanks to the
proposed prior density. Moreover, in order to explore the capabilities
of the proposed prior densities to yield accurate segmentations, we have
also computed the Dice coefficients for the segmentations based only on
the maximization of the prior probability maps generated by each method.
As Table 4.2 shows, the NLv- SVFMM method, followed by the NLp- SVFMM ,
achieves the best results. Of course the Dice coefficients are
significantly low because the segmentations do not take into account the
pixel intensities. However, the aim is to evaluate which method better
captures the local similarities in the images, hence producing more
accurate prior information about the different segments in the image.

#### 4.5.2 Evaluation on the Berkeley Segmentation Dataset

In addition to the synthetic evaluation, we have assessed the
performance of the proposed model with the 300 real-world images of the
Berkeley Segmentation dataset. In our experimentation, we have employed
a 3-dimensional feature vector to represent each each pixel of the
images, comprising the 3 channels of the L*a*b color space. We also
applied a local median smoothing to each channel using a @xmath window
centered at each pixel. We have evaluated the performance of each
algorithm for different values of @xmath .

Table 4.3 shows the results for the evaluation of the 300 images of the
Berkeley dataset. Rand Index ( RI ) is employed to measure the degree of
concordance between the automated segmentation and the manual
segmentations. Our experiments show that the proposed methods perform
favorably in terms of RI to the other approaches in almost all
situations. The NLv- SVFMM performs comparable to the @xmath t variant
in most of cases, achieving very similar results. However, the NLv-
SVFMM requires less parameters, hence reducing the degrees of freedom of
the model. Nevertheless, the NLp- SVFMM method achieves, both in average
and median cases, the best results in most cases. Only in the @xmath
case (the simplest segmentation), the SVFMM method outperforms the rest
of the models. However, as segmentation complexity increases, the models
including edge preserving priors performs better in all cases.

Figure 4.5 shows several examples of segmentations of images of the
Berkeley dataset obtained with the NLp- SVFMM method.

#### 4.5.3 Evaluation of computational time requirements

Additionally, a comparison in terms of the computational time required
by each method has been performed. Table 4.4 shows the average times (in
seconds) and the std. deviation of each method evaluated in the Berkeley
300 dataset for different number of segments calculated in the images.

As expected, the SVFMM is the fastest method since it doesn’t carry the
extra computation of the weights for constrain the @xmath variances. It
should be noted that only the NLv- SVFMM and the @xmath t- SVFMM are
directly comparable since both perform the calculation of the @xmath and
@xmath weights respectively, and those weights are computed pixel-wise.
It can be seen that both methods perform very similar, with no
significant difference between them. Although the @xmath t- SVFMM model
requires a numerical iterative approximation of the @xmath parameters,
which is often a slow procedure, the complexity in the computation of
the @xmath weights is lighter than the @xmath weights. That is the
reason why the NLv- SVFMM is a bit slower than the @xmath t- SVFMM . The
calculation of @xmath weights requires the computation of @xmath @xmath
pdf values, which ultimately equals or even slightly increases the
computational time with respect the @xmath weights. The NLp- SVFMM
performs the best in terms of Dice and RI scores, but also requires more
time to compute the segmentation since it carries the extra computation
of the patch-based similarity.

### 4.6 Discussion

In this study we have proposed a new unsupervised image clustering
algorithm that successfully merges the SVFMM framework with the
well-known NLM filtering scheme. The main advantage of this algorithm is
the proposed new MRF density over the contextual mixing proportions,
which enforces local smoothness while preserving edges and the structure
of the image. This MRF improves the previously proposed @xmath t- MRF
both in performance and in complexity of the model by reducing the
number of parameters to be estimated. Experimental results demonstrated
the superiority of the proposed method with respect to previous
state-of-the-art algorithms proposed in the literature when evaluated in
a public reference dataset.

As figure 4.2 shows, the proposed probabilistic NLM weighting function
behaves more aggressive in the NLSVFMM than its analogous in the @xmath
t- SVFMM . This shrinks or widens the covariance matrices more
dramatically when there are differences (even small ones) between
adjacent contextual mixing coefficients, leading to a sharper prior
density. This behavior tends to estimate more radical prior maps, which
hypothetically should provide stronger information during the learning
and inference of the model. On the contrary, uniform flat prior
probability maps rapidly fade into an uninformative element in the
model. Figure 4.4 and table 4.1 corroborates this hypothesis. In figure
4.4 , it can be seen that the prior probability maps obtained by the
NLSVFMM variants achieve the highest degrees of confidence for the
different labels among all models compared. Table 4.1 compares the
contextual mixing coefficient values obtained by each method for
different voxels corresponding to different tissues of a BRATS case. As
it can be seen, the highest values are always obtained by the proposed
NLSVFMM variants, leading to a highest degree of confidence about the
presence of a specific tissue.

Results on the BRATS synthetic dataset allowed to evaluate the
performance of the methods in a controlled environment where the number
of labels existing in the data is known. This enabled us to directly
measure the impact of the proposed NLM -based prior density in the
results. Table 4.2 compares the Dice scores of the segmentations
obtained by each method, based on the posterior probabilities (Bayes
decision rule) and only on the contextual mixing coefficient maps
estimated by each method. Regarding the posterior probabilities case,
which is the optimal decision rule, an improvement of about 3 points in
Dice with respect to the SVFMM and about 1 point with respect to the
@xmath t- SVFMM was obtained only by changing the prior density.
Considering that the prior density acts as a guide during the learning
and segmentation process, this improvement in final results is highly
valuable given its limited impact. Similarly, results obtained by the
NLSVFMM variants only based on the prior probability maps (contextual
mixing coefficient maps) outperform the other methods in a similar
manner. Obviously, Dice results are significantly lower than those
obtained based on the posterior probability (which takes into account
also the intensities of the images), however the comparison among them
demonstrates the superiority of the NLSVFMM approach.

The evaluation performed on the Berkeley dataset also yielded favorably
results in terms of RI for the NLSVFMM s methods. In almost all cases,
the NLSVFMM s variants performed superior or comparable to the @xmath t
model, with the advantage that they introduce less parameters that must
be estimated. Moreover, as expected, the patch-based NLp- SVFMM method
achieved the best results, confirming that the probabilistic NLM
weighting function better captures the local redundancy of the images.
Only in the @xmath case (the simplest segmentation), the SVFMM method
outperforms the rest of the models. However, as segmentation complexity
increases, the models including edge preserving priors perform better in
all cases.

It is worth noting that standard deviations are high and similar for all
methods. This is due to several images that are intrinsically difficult
to segment and present poor RI results across all methods uniformly,
rather than to a high variability in the methods themselves. To
corroborate it, we measured the percentage of cases of the Berkeley300
dataset that showed RI improvement when segmented by our methods with
respect to the SVFMM and the @xmath t- SVFMM . For most of the @xmath
states, our NLp- SVFMM approach showed RI improvement in approximately
more than the 85% of cases compared with the SVFMM , and more than the
80% of cases compared with the @xmath t- SVFMM . This behavior indicates
that there is a systematic improvement of our algorithms with respect to
the previous approaches in the literature, which is not a product of
random fluctuations due to the high standard deviations.

On the other side, it is also worth noting that differences in RI are
not significantly large between methods, which is a reasonably behavior.
Under the Bayes’ decision rule, the prior probability @xmath (or @xmath
) acts as an initial degree of belief of each label at each location of
the image before observing the image, which ultimately represents a less
informative distribution compared to the class conditional @xmath .
Therefore, the impact on the final results of changing the prior
distributions is limited, and thus the segmentation results are less
affected. In addition, prior distributions become weaker as the number
of observations in a problem increase, which is the case of pixel
classification in an image. In those cases, variations in the prior
densities also have a lesser impact in final results, which is observed
in our experimentation.

In future work, we plan to study the inclusion of prior distributions
for other parameters of the model, i.e. @xmath and @xmath , to introduce
prior knowledge and constraints that can help in the estimation of more
accurate and realistic models.

## Chapter 5 Vascular heterogeneity assessment of glioblastoma through
the Hemodynamic Tissue Signature

Understanding glioblastoma intra- and inter-tumoral heterogeneity
represents one of the most important challenges in advancing the fight
against this lethal cancer. Over the years, much evidence has
accumulated to suggest that this heterogeneity is highly responsible for
the poor prognosis of the tumor and its resistance to effective
therapies. Specifically, vascular heterogeneity has been identified as
one of the most important pathological hallmarks of glioblastoma. The
study of the aberrant vasculature of this lethal tumor, its hemodynamic
local behavior and its angiogenesis mechanism is crucial to design new
effective therapies that improve patient prognosis. However, to date,
determining the extent and characteristics of this intra-tumor
heterogeneity is still poorly understood.

In this chapter we present the Hemodynamic Tissue Signature ( HTS ), an
unsupervised machine learning method to describe the vascular
heterogeneity of glioblastoma by means of perfusion MRI analysis. The
method analyzes the perfusion markers to automatically draw four
reproducible habitats that describe the tumor vascular heterogeneity:
the High Angiogenic Tumor ( HAT ) and Low Angiogenic Tumor ( LAT )
habitats of the enhancing tumor, the potentially tumor Infiltrated
Peripheral Edema ( IPE ) and the Vasogenic Peripheral Edema ( VPE ).

The purpose of the work presented in this chapter was to assess if
preoperative vascular heterogeneity of glioblastoma predicts OS of
patients undergoing standard of care treatment by using the HTS method.
To do so we conducted Kaplan-Meier and Cox proportional hazard analyses
to study the prognostic potential of the HTS habitats on a cohort of 50
retrospective patients from a local hospital. Additionally, we explored
the ability of the HTS habitats to improve the conventional prognostic
models based on clinical, morphological, and demographic features.

The contents of this chapter were published in the journal publications
( JuanAlbarracin2018 ; FusterGarcia2018 ) —thesis contributions C3, C4,
P5 and P6.

### 5.1 Introduction

Glioblastoma heterogeneity has been identified as one of the factors
responsible for the high aggressiveness of these neoplasms ( Lemee2015 )
and as a key hallmark to understanding their resistance to effective
therapies ( Soeda2015 ) . Molecular characterization of glioblastomas
has advanced the understanding of the biology and heterogeneity of these
tumors, improving routine diagnosis, prognosis, and response to therapy
( Parsons2008 ; Verhaak2010 ) . However, significant interest has been
placed in the past years in the analysis of glioblastoma heterogeneity
based on medical imaging, to discover non-invasive tumor features
related to different clinical outcomes such as OS , tumor grading or
molecular sub-typing Wangaryattawanich2015 .

Glioblastoma is characterized by highly infiltrative and deeply invasive
behavior ( Dang2010 ) . Strong vascular proliferation, robust
angiogenesis, and extensive microvasculature heterogeneity are major
pathological features that differentiate glioblastomas from low-grade
gliomas ( Alves2011 ; Hardee2012 ; Kargiotis2006 ) . Such factors have
been shown to have a direct effect on prognosis ( Hardee2012 ) .
Therefore, the early assessment of the highly heterogeneous vascular
architecture of glioblastomas could provide powerful information to
improve therapeutic decision making.

Dynamic Susceptibility Contrast ( DSC ) MRI has been used widely to
retrieve physiologic information on glioblastoma vasculature ( Shah2010
; Knopp1999 ; Lupo2005 ) . DSC quantification involves the computation
of the hemodynamic indexes obtained from the kinetic analysis of the T
@xmath concentration time curves retrieved from the first pass of an
intravenously injected paramagnetic contrast agent ( Ostergaard2005 ) .
Section 2.2.2 performs an in depth description of the techniques
employed in this thesis to estimate the perfusion parametric maps from
the DSC sequence. Such perfusion indexes have demonstrated powerful
capabilities for a wide range of applications such as tumor grading (
Law2003 ; Emblem2008 ) , neovascularization assessment ( Thompson2011 ;
Tykocinski2012 ) , early response to treatment assessment ( Vidiri2012 ;
Elmghirbi2017 ) , recurrence versus radionecrosis ( Hu2009 ; Barajas2009
) and prediction of clinical outcome ( Mangla2010 ; Jain2014 ) .

Numerous studies have been focused on the analysis of pretreatment
perfusion indexes to assess tumor heterogeneity ( Jackson2007 ; Liu2017a
; SanzRequena2013 ; Ulyte2016 ) . The most common practice is the manual
definition of Region Of Interest s within the tumor to study vascular
properties that correlate with clinical outcomes. However, these manual
approaches impair the reproducibility studies and the analysis of
high-dimensional multiparametric MRI data ( Young2007 ) .

An alternative novel approach to describe the heterogeneity of
glioblastomas is by means of the definition of lesion sub-compartments
or radiological habitats that express a specific biological behavior
observable from MRI . Several attempts have been made in the literature
to describe the glioblastoma heterogeneity through this technique (
Dextraze2017 ; Zhou2014 ; Lee2015 ; Zhou2017 ) . However most of them
are based on morphological MRI and classical image processing techniques
such as histogram analysis, intensity thresholding, texture measurements
or histogram derived features. The preoperative characterization of the
vascular heterogeneity of glioblastomas through a multiparametric search
of habitats drawn from an unsupervised machine learning process has not
previously been well established in the literature.

In this sense, we hypothesize that vascular-related habitats obtained in
the preoperative evaluation of glioblastoma are early predictors of OS
in patients who subsequently undergo standard-of-care treatment. In this
work, we present the Hemodynamic Tissue Signature ( HTS ) method: an
unsupervised machine learning-based algorithm that delineates a set of
vascular habitats within the glioblastoma obtained through a
multiparametric structured clustering of morphologic and DSC MRI
features. HTS includes consideration of four habitats: the High
Angiogenic Tumor ( HAT ) (the more perfused area of the enhancing
tumor), the Low Angiogenic Tumor ( LAT ) (the area of the enhancing
tumor with a lower angiogenic profile), the potentially Infiltrated
Peripheral Edema ( IPE ) (the surrounding non-enhancing region adjacent
to the tumor with elevated perfusion indexes), and the Vasogenic
Peripheral Edema ( VPE ) (the remaining edema with a lower perfusion
profile).

To determine whether the preoperative vascular heterogeneity of
glioblastoma allows early prediction of OS of patients who undergo
standard-of-care treatment, we conducted a survival analysis on the
basis of perfusion measures obtained from the HTS habitats. In addition,
we also studied the contribution of the HTS habitats to improve the
estimation of OS with respect to models based solely on clinical,
morphological and demographic variables; and models including perfusion
markers measured from the conventional enhancing tumor and edema ROI s
instead of HTS habitats.

### 5.2 Materials

#### 5.2.1 Patient selection

Our institutional review board approved this retrospective study, and
the requirement for patient informed consent was waived. Eighty-four
patients from January 2012 to December 2016 with suspected glioblastoma
were included. The inclusion criteria were: (a) confirmation of
glioblastoma through biopsy; (b) access to preoperative MRI
examinations, including unenhanced and GBCA –enhanced T @xmath
-weighted, T @xmath -weighted, FLAIR , and DSC sequences; and (c)
patients who underwent standard Stupp treatment ( Stupp2005 ) .

Of the 84 initial patients, six were excluded because of an incomplete
MRI study, three were excluded because of motion or spike artifacts on
the DSC images that prevented the quantification (gamma variate @xmath
goodness of fit @xmath 0.95), 10 patients were excluded because of
unconfirmed or unconventional glioblastomas (giant cell glioblastoma and
glioblastoma with oligodendroglioma component), and 10 patients were
excluded because they did not undergo resection because of their tumor
location (only biopsy results available) or they did not undergo
radiation therapy and chemotherapy treatment. In addition, five patients
who presented with glioblastomas with contiguous leptomeningeal
extensions were excluded from the study because of the inability to
accurately differentiate the tumor vascularity from the reactive
meningeal enhancement in the perfusion signal intensity. Finally, 50
patients constituted the study group, including 33 men with an average
age of 60.94 years (range, 25–80 years); 17 women with an average age of
62.53 years (range, 36–75 years); and an overall mean age of 60.08 years
(range, 25–80 years).

#### 5.2.2 Magnetic Resonance Imaging

Standard-of-care examinations were obtained with 1.5-T or 3-T imagers
(Signa HDxt; GE Healthcare, Waukesha, Wisconsin) with an
eight-channel-array head coil. MRI examinations included unenhanced and
GBCA –enhanced T @xmath -weighted three-dimensional spoiled
gradient-echo sequences with inversion recovery (repetition times
msec/echo times msec, 6–10/2–4; matrix, @xmath ; section thickness,
@xmath mm; field of view, @xmath cm; inversion time, @xmath msec; flip
angle, @xmath ), fast spin-echo T @xmath -weighted imaging ( @xmath ;
matrix, @xmath ; section thickness, @xmath mm; field of view, @xmath cm;
one signal acquired; intersection gap, @xmath mm) and a FLAIR sequence (
@xmath ; matrix, @xmath ; section thickness, @xmath mm; field of view,
@xmath cm; one signal acquired; intersection gap, @xmath mm; inversion
time, @xmath msec).

The DSC T @xmath -weighted gradient-echo perfusion study was performed
during the injection of GBCA (Multihance; Bracco, Milan, Italy). A bolus
injection of @xmath mmol/kg of GBCA was administered at @xmath mL/sec by
using a power injector (no pre-bolus administration). Saline solution
was injected after the bolus injection. The study was performed with the
following parameters: @xmath ; matrix, @xmath ( @xmath mm in-plane
resolution); section thickness, @xmath mm; flip angle, @xmath ; @xmath
cm full-coverage cranio-caudal ( @xmath sections), @xmath sequential
temporally equidistant volumes, each one with an acquisition time of
@xmath seconds. The baseline before injection of the bolus was five
dynamics. Table 5.1 summarizes the aforementioned parameters.

### 5.3 Methods

#### 5.3.1 Quantification of DSC parametric maps

DSC T @xmath -weighted quantification involves the computation of the
hemodynamic indices obtained from a kinetic analysis of the first pass
of a intravenously injected paramagnetic contrast agent ( Ostergaard2005
) . A detailed explanation of the calculation of perfusion parametric
maps is performed in section 2.2.2 . In order for this chapter to be
self-contained, a short remainder will be made.

Previous to DSC T @xmath -weighted quantification, registration of the
sequence was performed to the morphologic T @xmath -weighted image of
the patient. Next, signal intensity curves were converted to
concentration time curves by using the following equation 2.1 . Contrast
material leakage correction was performed by using the technique
proposed by Boxerman2006 and recirculation was corrected by means of
gamma-variate curve fitting (please refer to section 2.2.2 for more
details).

rCBV and rCBF maps were obtained using standard algorithms previously
described ( Knutsson2010 ) . rCBV was computed according to numerical
integration of the area under the curve of the gamma-variate fittings (
Knutsson2010 ) ; while rCBF was calculated by means of the
block-circulant SVD deconvolution technique proposed by Wu2003 . Both
perfusion maps were normalized against the contra-lateral unaffected
white matter value.

The AIF was automatically selected by following a divide-and-conquer
approach. The method recursively dichotomizes the set of concentration
time curves of the perfusion study into two groups, selecting those
curves with higher peak height, earliest time to peak, and lowest
full-width at half maximum. We used the median as a threshold to split
the groups. The process is repeated until 10 or fewer curves are
conserved, finally fixing the AIF as the average of those curves.

#### 5.3.2 Enhancing tumor and edema segmentation

In this work the enhancing tumor and edema ROI delineation was performed
by using an unsupervised segmentation method based on a variant of the
work proposed in a study by JuanAlbarracin2015a . This method is based
on DCM - SVFMM ( Nikou2007 ) , which consists of a clustering algorithm
that combines GMM with continuous DCAGMRF s to take advantage of the
self-similarity and local redundancy of the images. The method includes
the unenhanced and GBCA –enhanced T @xmath -weighted sequences, the T
@xmath -weighted sequence, and the FLAIR sequence in combination with
atlas-based prior knowledge of healthy tissues to perform the
segmentation. The automated enhancing tumor and edema ROI s obtained
with the method were manually revised and validated by two experienced
radiologists in consensus (F.A., with 14 years of experience; L.M.B.,
with 25 years of experience).

Nowadays, this method has been replaced by a more robust and powerful
approach based on CNN s ( JuanAlbarracin2019a ) (to be presented in the
next chapter), which has proven to be comparable and competitive with
manual segmentations performed by expert radiologists. The new method
achieves a Dice score index of 0.89 on a large public real glioblastomas
dataset, of more than 300 cases manually annotated by more than 3 expert
radiologist each case.

#### 5.3.3 Hemodynamic Tissue Signature habitats

The HTS consists of a set of vascular habitats detected in glioblastomas
and obtained by means of a multiparametric unsupervised analysis of DSC
MRI patterns within the tumor. The technology used to compute the HTS of
the glioblastoma is publicly accessible for non-commercial research
purposes at https://www.oncohabitats.upv.es .

The HTS defines four habitats within the glioblastoma: the High
Angiogenic Tumor ( HAT ) and Low Angiogenic Tumor ( LAT ) habitats, and
the Infiltrated Peripheral Edema ( IPE ) and Vasogenic Peripheral Edema
( VPE ) habitats. Table 5.2 summarizes the relationships among HTS
habitats, glioblastoma tissue, and DSC observed vascularity.

HTS habitats are obtained by means of a DCM - SVFMM structured
clustering (with the DCAGMRF prior) of rCBV and rCBF maps. The
clustering consists of two stages (see figure 5.1 ): (a) a two-class
clustering of the rCBV and rCBF data at the whole enhancing tumor and
edema ROI s defined morphologically; and (b) a two-class clustering also
of the rCBV and rCBF data within each ROI obtained in stage a). To
ensure the reproducibility of the HTS , both stages are initialized with
a deterministic seed method. We fix the seeds of every two-class
clustering to the extremes of the rCBV and rCBV distributions (5% and
95% percentiles, respectively, for each class).

The aim of the first stage is to refine the enhancing tumor and the
edema ROI s , previously delineated through the anatomical MRI
segmentation, but introducing the perfusion information. Therefore, a
spatially varying mixture of two components is fit to the distributions
of rCBV and rCBF observed in the regions previously labeled as enhancing
tumor and edema. We named these two components: enhancing tumor at DSC (
@xmath ) and edema at DSC ( @xmath ). During the fitting process, we
introduce several constraints to avoid misclassifications of nearby
healthy vascular structures @xmath . Thus, we constrained the apparition
of the @xmath class to a neighborhood of less than 1 cm around the
enhancing tumor observed on the GBCA –enhanced T @xmath -weighted MRI (
Guo2016 ) . This constraint allows the correction of misalignments
during the DSC registration and the removal of healthy vascular
structures far from the enhancing area of the tumor, which may distort
the HTS . Moreover, the also enforce the @xmath class to explain at
least the 80% of the enhancing tumor ROI obtained from the anatomical
segmentation.

The second stage includes DCM - SVFMM clustering within the @xmath and
@xmath ROI s , to delineate the two potential hemodynamic habitats
inside each tissue. Likewise, the first stage, a spatially varying
mixture of two components is fit to the distributions of rCBV and rCBF
for both the @xmath and @xmath ROI s . In this stage, several
constraints are also introduced. First, we force a minimum size of the
habitats of at least 10% of the whole lesion ROI to avoid habitat
vanishing. Second, infiltrated peripheral edema habitat is also
constrained to a nearby region around the @xmath class. Following the
definition of the clinical target volume proposed by Guo2016 , we fix a
2 cm margin around the enhancing tumor observed at GBCA -enhanced T
@xmath -weighted MRI as the maximum distance where the infiltrated
peripheral edema habitat could appear. Voxels classified as infiltrated
peripheral edema outside this region are automatically removed from the
HTS , since they have a similar vascular pattern to that of infiltrated
peripheral edema but far from the plausible region of tumor
infiltration.

#### 5.3.4 Statistical analysis

All analyses were performed with software (Matlab R2015a; MathWorks,
Natick, Massachusetts) on a personal workstation.

First, an evaluation of the statistical differences between habitats was
conducted to assess the degree of separability of the rCBV and rCBF
distributions within each habitat to confirm their different hemodynamic
activity. To do so, global probabilistic deviation ( Saez2017 ) was used
as a metric to control the degree of concordance among several
statistical distributions. Such a metric is bounded to a @xmath range,
with 0 referring to absolute overlapping between distributions, and 1
indicating non-overlapped, completely separated distributions.
Therefore, for each perfusion parameter, the distributions of the four
proposed habitats and their global probabilistic deviation metric were
calculated.

Second, Cox proportional hazards modeling was conducted to investigate
the relationship between patient survival and the @xmath and @xmath at
the HTS habitats. We used the maximum of the perfusion parameters
because it has been reported to be the most reliable measure for
inter-observer and intra-observer reproducibility ( Wetzel2002 ) .
Proportional hazard ratios with 95% confidence intervals were reported,
while the Wald test was used to determine the significance of the Cox
regression model results.

Finally, Kaplan-Meier survival analyses between populations dichotomized
according to the median value of each perfusion biomarker at each HTS
habitat were also conducted. A log-rank test was used to determine the
statistical significance of the differences in observed population
survival. Average survival for each population was also reported.
Benjamini-Hochberg false discovery rate correction at an @xmath level of
.05 was used to correct for multiple hypothesis testing ( Benjamini1995
) in all analyses.

On the other side, we also conducted a study to determine the added
value of HTS habitats for predicting patient OS when these are added to
classical models based on clinical, demographic and morphological
variables. To do so, first we conducted an analysis to measure the
importance of each clinical, demographic or MRI -related variable
independently to predict OS . For the categorical variables we conducted
Kaplan-Meier survival analysis, while for the continuous variables Cox
proportional hazards modeling was used.

Next, we studied the added value of the HTS habitats by adding its
information to the models built with the variables that showed
significant association with OS in the aforementioned independent study.
To do so we constructed three models:

    [itemsep=0.5pt, topsep=0pt]

  Model 1:  

    Clinical variables + demographic variables

  Model 2:  

    Clinical variables + demographic variables + @xmath and @xmath at
    enhancing tumor

  Model 3:  

    Clinical variables + demographic variables + @xmath and @xmath at
    HTS habitats

Cox proportional hazard regression models were fit for each model and a
comparison between the predicted OS and the real patient’s OS was
performed. A Kaplan-Meier survival study was next performed by using Cox
the predicted OS of each model to split the population in two groups:
long-survivors, whose Cox predicted OS was greater than the real average
OS of the population (402 days); and short-survivors, whose Cox
predicted survival was less or equal than the real average OS of the
population (402 days). Root Mean Squared Error ( RMSE ) was employed to
measure the deviance between the predicted OS and the real OS of the
population. Single-tailed Wilcoxon paired signed rank test was used to
determine if there are stastistically significant differences between
predicted RMSE s among models.

### 5.4 Results

Figure 5.2 shows examples of HTS maps. GBCA -enhanced T @xmath -weighted
and T @xmath -weighted images, as well as rCBV and rCBF maps are shown
with the HTS map of the patient. The global probabilistic deviation
analysis of the different hemodynamic activity among habitats yielded
the following average results: @xmath for rCBV and @xmath for rCBF .
These results indicate separated perfusion distributions between
habitats of the patients. Figure 5.3 shows an example of the rCBV
distributions for each HTS habitat. The Cox proportional hazard analysis
is presented in Table 5.3 .

Significant results were obtained for @xmath and @xmath in the
high-angiogenic habitat (hazard ratios, @xmath @xmath and @xmath @xmath
, respectively), @xmath and @xmath in the low-angiogenic habitat (hazard
ratios, @xmath @xmath and @xmath @xmath , respectively) and @xmath in
the infiltrated peripheral edema habitat (hazard ratio, @xmath @xmath ).
Non-significant results were obtained for @xmath in the infiltrated
peripheral edema (hazard ratio, @xmath @xmath ), and @xmath in the
vasogenic peripheral edema habitat (hazard ratio, @xmath @xmath ) and
@xmath at vasogenic peripheral edema (hazard ratio, @xmath @xmath ).
Figure 5.4 shows the scatterplots of the combinations of perfusion
biomarkers and HTS habitats that yielded significant correlation in the
Cox survival analysis. Total versus partial maximum safe resection,
complete versus incomplete concomitant radiation therapy and
chemotherapy and adjuvant temozolomide plus bevacizumab administration
are also shown.

Kaplan-Meier survival analysis yielded significant differences for the
survival times observed for the populations dichotomized by low and high
@xmath in the high-angiogenic habitat (log-rank test @xmath ), @xmath in
the high-angiogenic habitat (log-rank test @xmath ), @xmath at
low-angiogenic habitat (log-rank test @xmath ) and @xmath in the
low-angiogenic habitat (log-rank test @xmath ). An average difference of
230 days in overall survival between populations was observed. Mean
survival of the population was 459 days @xmath 286.15 (range, 121–1656
days). Table 5.4 shows the average observed survival times for each
population and the corrected P-values for the log-rank survival test.
Figure 5.5 demonstrates the Kaplan-Meier estimated survival functions
for the different populations dichotomized according to the @xmath and
@xmath at different HTS habitats.

Tables 5.5 and 5.6 show the log-rank test and the Cox-Wald test to
measure the potential of each categorical and additional continuous
variable to independently correlate with patient OS .

Figure 5.6 shows the OS predictions generated by Models 1, 2 and 3
during the leave-one-out evaluation. Model 1, based solely on clinical
and demographic variables, obtained a @xmath ; Model 2, based on
clinical, demographic and perfusion markers at enhancing tumor region,
obtained a @xmath ; while Model 3, which was based on clinical,
demographic and perfusion markers measured from the HTS habitats,
obtained a @xmath .

The improvement achieved in the prognostic estimation obtained by the
Model 2 with respect the Model 1 was 7.7%, while the improvement
obtained by the Model 3, which incorporates the HTS habitats, reached
16.3% in terms of RMSE . Wilcoxon paired signed test yielded no
significant differences between Model 1 and Model 2, however it did
obtained statistically significant differences between Model 3 and Model
1 @xmath .

### 5.5 Discussion

In this study, we investigated whether the perfusion heterogeneity in
the four vascular habitats of the HTS is predictive of survival in
untreated glioblastomas. Our results demonstrate that the preoperative
perfusion heterogeneity contains relevant information about patient
survival, even considering the effect of other known relevant factors
such as standard-of-care treatment. Gradually longer survival times were
found for patients who presented with lower preoperative perfusion
indexes in different HTS habitats. The influence of standard-of-care
treatment on patient survival was also directly observed. As expected,
patients who underwent maximal safe resection plus concomitant adjuvant
chemotherapy and radiation therapy showed better survival times.
However, a tendency of longer survival times within subgroups of
patients who underwent the same specific treatment and had lower
perfusion indexes at several habitats was also observed. This indicates
that preoperative perfusion heterogeneity contains early important
information about patient survival.

Cox proportional hazard analysis substantiated these conclusions. High-
and low-angiogenic habitats arose as those with the highest prognostic
abilities, yielding significant correlations between survival and @xmath
and @xmath (with multiple-test false discovery rate correction). @xmath
in the infiltrated peripheral edema habitat also was significantly
correlated with survival, while @xmath in the infiltrated peripheral
edema was significantly correlated without multiple-test correction
@xmath . These results suggest that relevant information about patient
survival is also contained in the peripheral edema ( Jain2014 ;
Akbari2014 ; Artzi2014 ) .

Significant differences also were observed in the Kaplan-Meier estimated
survival functions for populations divided according to the median
@xmath and @xmath at several HTS habitats. An improvement of
approximately 230 days in overall survival was observed for patients who
had lower @xmath and @xmath in the high- and low-angiogenic habitats.
These results support the potential of HTS to accurately describe the
preoperative vascular heterogeneity of glioblastomas and its prognostic
abilities at early stages.

Regarding the added value provided by the HTS to predict OS in
combination with clinical and demographic variables, we found that, in
agreement with the literature, variables such as age, biopsy instead of
resection, long or short distance of the lesion to the ventricles,
frontal tumor location, complete vs incomplete radiochemotherapy or
enhancing tumor volumetry presented strong association with patient OS .

The comparison of the prognostic capabilities of the three proposed
regression models also confirmed the added value of the HTS markers in
the prognosis estimation of the patient. As expected, the models
including perfusion MRI performed better than those based solely on
clinical and demographic variables. However, it is worth noting that the
inclusion of perfusion related measurements from the enhancing tumor ROI
improved the prognosis estimation by 7.7% in terms of RMSE , however,
the improvement obtained when including the HTS perfusion-based
measurements outperformed the previous model by 16.3%. Additionally,
only this latter improvement proved to be statistically significant.
These results reinforce the evidence of the need for a better
characterization of the heterogeneity of glioblastoma in order to
improve the current management of the disease.

Several studies have been conducted to analyze the vascular
heterogeneity of the glioblastoma, many of them focusing on the
enhancing tumor region. Law2008 found that patients who presented with
an @xmath of less than 1.75 in the enhancing tumor had longer
progression-free survival times; however, they did not find a
significant correlation with overall survival. Sawlani2010 also
correlated time to progression with several hyper-perfused regions
delineated in the @xmath . However, they also observed no significant
correlation between patients’ overall survival and @xmath in the
enhancing tumor. Hirai2008 and Jain2013 , studied the potential for
prediction of survival of @xmath in the enhancing tumor of high-grade
gliomas. They showed that patients who presented with an @xmath of
greater than 2.3 had significantly shorter survival times. These results
are consistent with our findings, hence aligning HTS with the results of
previous studies in the literature. However, these studies were only
based on the @xmath ; other perfusion indexes such as @xmath , which may
also add important information about instantaneous capillary flow in the
tissues to the analysis, were not considered. Moreover, manual
delineation of ROI s based on @xmath was used, which may affect
reproducibility and may not fully capture the tumor information
available in a multiparametric study.

Authors of other studies focused on peripheral edema of glioblastoma.
Akbari2014 , Jain2014 and Artzi2014 studied the peritumoral region of
the glioblastoma to account for heterogeneity and possible tumor
infiltration in the peripheral edema. Akbari et al used ROI s to train a
support vector machine, which was then used to generate heterogeneity
maps. Jain et al analyzed the association of Visually Accessible
Rembrandt Images, or VASARI, features and molecular data with overall
survival and progression-free survival, while Artzi et al used
diffusion, perfusion, and morphologic MR imaging with an unsupervised
segmentation algorithm to analyze the edema region. Their results
correlate with our findings in the infiltrated peripheral edema habitat,
because they also found that vascular heterogeneity in the peripheral
edema correlate with overall patient survival. However, ROI s to
describe tumor heterogeneity in these studies were also delineated
manually. Moreover, statistical tests were conducted without multiple
comparison correction, which decreases the statistical power of the
conclusions.

One of the main limitations of our study and similar studies in which
authors attempted to describe the vascular heterogeneity of glioblastoma
by means of discovery of new habitats was the unavailability of a ground
truth for validating the habitat’s segmentation ( Akbari2014 ; Hirai2008
; Jain2013 ) . Multiple biopsy or pathological sampling could confirm
the accuracy of the habitats; however, such techniques cannot always be
performed in clinical practice. To overcome this limitation, alternative
validation should be conducted to demonstrate the clinical relevance of
the habitats. In this study, we have analyzed the relationship between
the preoperative vascular heterogeneity of glioblastomas described
through HTS and patient survival.

Another important limitation was the lack of molecular markers in the
population of our study. Molecular markers are currently considered a
standard of care for WHO glioblastoma classification and are also known
to affect prognosis of patients with glioblastoma. Positive correlation
of genetic markers with the HTS habitats would strengthen the study and
the predictive potential of the proposed method and should be performed
in the future.

Finally, a limitation of the HTS arises in the presence of highly
vascularized healthy structures close to the glioblastoma, such as
nearby vessels or arteries. In such cases, these structures can be
misidentified as high- and low-angiogenic or infiltrated peripheral
edema habitats depending on their degree of vascularity. Although HTS
implements several constraints to remove these healthy structures,
nearby vessels may influence the HTS , modifying measures obtained from
the habitats. Results of future studies should improve vessel detection
by using a vascular probability atlas to weight the HTS inference
process.

In conclusion, preoperative vascular heterogeneity of glioblastomas
demonstrated by the habitats of HTS is associated with patient survival.
HTS separates glioblastomas into four vascular habitats with early
prognostic capabilities, offering an opportunity to define refined
imaging biomarkers surrogated to clinical outcomes.

## Chapter 6 Multi-center international validation of the Hemodynamic
Tissue Signature for glioblastoma

In the previous chapter it was shown that early-stage vascular
heterogeneity of glioblastoma has a direct effect on prognosis and a
strong association with tumor aggressiveness. The proposed Hemodynamic
Tissue Signature ( HTS ) method provides an unsupervised ML solution to
study the vascular heterogeneity of glioblastomas by analyzing patterns
of local hemodynamic activity in perfusion MRI . The four habitats
delineated by the HTS method have demonstrated strong associations with
patient OS and early prognostic capabilities. However, the validation of
the method was conducted on a single-center cohort of patients, thus
avoiding the variability inherent in real-life multi-center
heterogeneous scenarios.

In this chapter we present a multi-center retrospective international
validation of the HTS method. The validation was performed under the
umbrella of the clinical study NCT03439332 , which involved seven
international centers with more than 180 patients. The purpose of this
chapter is to validate the association between the hemodynamic markers
obtained from the HTS habitats and the patient OS , considering the
inter-center variability of MRI acquisition protocols, patient
demographics and lesion heterogeneity. Kaplan-Meier and Cox proportional
hazard analyses were conducted to study the prognostic potential of the
HTS habitats under the proposed environment.

The contents of this chapter were published in the journal publications
( JuanAlbarracin2018 ; Alvarez2019 ) —thesis contributions C4, C5, P5,
P6 and P7.

### 6.1 Introduction

Glioblastoma is the most aggressive malignant primary brain tumor in
adults with a median survival rate of 12-15 months ( Louis2016 ;
Gately2017 ) . It still carries a poor prognosis despite aggressive
treatment, which includes tumor resection followed by chemo-radiotherapy
( Bae2018 ; Akbari2014 ) . One of the main factors thought to be
responsible of glioblastoma aggressiveness is its vascular heterogeneity
( Akbari2014 ; Soeda2015 ) , mainly defined by a strong angiogenesis
that supplies the glioblastoma metabolic requirements and accounts for
its rapid progression ( Weis2011 ; Palma2017 ) . The early-vascular
profile of the tumor is strongly associated with molecular
characteristics of the lesion ( Palma2017 ) , which in combination with
the local micro-environment are both directly related to the
glioblastoma progression ( Weis2011 ) .

The negative association between patient survival rates and vascular
markers extracted from perfusion MRI has been widely demonstrated in the
literature ( Akbari2014 ; Jain2014 ; Jensen2014 ) . Perfusion indexes
such as rCBV or capillary heterogeneity were found to be associated with
prognosis and patient survival rates. Dozens of methodologies are
proposed in the literature to assess these perfusion indexes, ranging
from manually defined ROI s , which introduce high uncertainty and lack
of repeatability; to more up-to-date techniques based on artificial
intelligence methods able to analyze imaging patterns to describe tumor
heterogeneity ( Demerath2017 ; Jena2016 ; Price2016 ; Chang2017 ;
Cui2016 ) .

In 2018, JuanAlbarracin2018 proposed the Hemodynamic Tissue Signature (
HTS ) method able to characterize the vascular heterogeneity of
glioblastomas by means of delineating vascular habitats obtained from
perfusion MRI . The HTS method draws four habitats within the lesion
related to: the High Angiogenic Tumor ( HAT ) region, the Low Angiogenic
Tumor ( LAT ) region, the potentially Infiltrated Peripheral Edema ( IPE
) and the Vasogenic Peripheral Edema ( VPE ) habitats. The HTS method is
publicly accessible at ONCOhabitats website
https://www.oncohabitats.upv.es for non-commercial research purposes.

The study conducted by JuanAlbarracin2018 found statistically
significant correlations between OS and several measures obtained from
the HTS markers. In 2018, FusterGarcia2018 demonstrated the ability of
these imaging markers to improve the prognosis of conventional models
based on clinical, morphological and demographic features. Both studies
were conducted on a single-center cohort of 50 patients from a local
institution.

However, the current road map to validate an imaging marker into
clinical routine requires to overcome two translational gaps (
Abramson2015 ; Oconnor2017 ) : the marker validation with pre-clinical
or clinical datasets from a single or a few expert centers, and the
subsequent extension of the evaluation to multiple centers, along with
the biological validation of the biomarkers. The aforementioned studies
of JuanAlbarracin2018 and FusterGarcia2018 addressed the first
translational gap, however it is still necessary to validate the HTS
markers in a multi-center heterogeneous cohort with the purpose of
demonstrating their robustness and stability under highly variable
clinical conditions.

The purpose of this work is to determine if the habitats obtained by the
HTS method are predictive of the OS of glioblastoma patients undergoing
standard-of-care treatment. To this end, we have involved the HTS
technology in an international multi-center observational retrospective
clinical study registered at the ClinicalTrial.gov official platform
with name “Multicentre Validation of How Vascular Biomarkers From Tumor
Can Predict the Survival of the Patient With Glioblastoma
(ONCOhabitats)” and identifier NCT03439332 . We have analyzed the
possible association between the HTS markers and patients OS , as well
as their capability to stratify groups of patients according to these
markers, in a large heterogeneous international cohort. Additionally, we
also have assessed the robustness of the HTS method operating under a
highly variable MRI acquisition protocols from multiple centers.

### 6.2 Materials

#### 6.2.1 Patient selection

Seven European clinical centers participated in the clinical study
NCT03439332 : the Hospital Universitario de La Ribera, Alzira, Spain;
Hospital de Manises, Manises, Spain; Hospital Clinic, Barcelona, Spain;
Hospital Universitario Vall d’Hebron, Barcelona, Spain; Azienda
Ospedaliero-Universitaria di Parma, Parma, Italy; Centre Hospitalier
Universitaire de Liege, Liege, Belgium and the Oslo University Hospital,
Oslo, Norway.

A material transfer agreement document was approved by all the
participating centers and an acceptance report was issued by the ethical
committee of each center. The institution review board of each center
also approved this retrospective study and the requirement for
patient-informed consent was waived.

The inclusion criteria for patients participating in the study were: (a)
adult patients (age @xmath 18 y.o.) with histopathological confirmation
of glioblastoma diagnosed between January 1, 2012 and January 1, 2018;
(b) access to preoperative MRI studies, including: pre- and
post-gadolinium T @xmath -weighted, T @xmath -weighted, FLAIR and DSC T
@xmath -weighted perfusion sequences; and (c) patients who underwent
standard Stupp treatment ( Stupp2005 ) with a minimum survival of 30
days.

From the initial cohort consisting of 196 patients, two cases were
excluded due to incomplete DSC perfusion acquisitions; five cases were
excluded due to excessive noise in DSC concentration curves that
prevented quantification (gamma variate goodness of fit @xmath ); four
cases were excluded due to MRI processing errors; and one case was
excluded due to inability to differentiate between tumor vascularity and
reactive meningeal enhancement. Table 6.1 summarizes the number of
patients initially contributed by each center and the number of patients
finally excluded due to noncompliance with the inclusion criteria.

The final cohort enrolled in the study was of 184 patients. Those who
were still alive during the study were considered as censored
observations. The date of censorship was the last date of contact with
the patient or, if was not available, the date of the last MRI exam.
Table 6.2 summarizes the most important demographic and clinical
characteristics of the population.

#### 6.2.2 Magnetic Resonance Imaging

Standard-of-care MR examinations were obtained with 1.5-T or 3-T
imagers. Pre- and post- GBCA T @xmath -weighted MRI , as well as T
@xmath -weighted, FLAIR and DSC perfusion MRI sequences were collected
from each center. Table 6.3 summarizes the MRI acquisition protocol
employed by each center.

### 6.3 Methods

#### 6.3.1 Vascular heterogeneity assessment of glioblastoma based on
Hts habitats

The HTS method, available at ONCOhabitats (
https://www.oncohabitats.upv.es ), was used to describe the vascular
heterogeneity of the glioblastomas enrolled in the multicenter study.
The methodology comprises the following stages:

1.  [itemsep=0.5pt, topsep=0pt]

2.   MRI preprocessing: including denoising, magnetic field
    inhomogeneity correction, multi-modal registration, brain
    extraction, motion correction and intensity standardization.

3.   Glioblastoma segmentation: implementing a state-of-the-art deep
    learning 3D CNN that delineates the enhancing tumor, edema and
    necrotic tissues.

4.   Perfusion quantification: to calculate the parametric rCBV , rCBF ,
    MTT and K2 maps derived from the DSC perfusion sequence.

5.   HTS habitats: in which an unsupervised segmentation algorithm
    performs the detection of the HAT , LAT , IPE and VPE habitats to
    describe the vascular heterogeneity within the lesion.

According to Wetzel2002 , for each habitat we defined the HTS marker as
the maximum rCBV @xmath , computed as the @xmath percentile of the rCBV
distribution within the region defined by the corresponding habitat.

#### 6.3.2 Association among Os and Hts markers - whole cohort study

Cox proportional hazard regression analysis was used to quantify the
associations between patient OS and HTS markers. The proportional Hazard
Ratio s with their 95% confidence intervals were reported, as well as
the associated P-values corrected for multiple-test with
Benjamini-Hochberg false discovery rate correction at an @xmath level of
@xmath .

Kaplan-Meier analyses were also conducted to study the survival
evolution of the population stratified into two groups according to HTS
markers: the high-vascular and the low-vascular groups. We defined the
high-vascular and low-vascular groups as the set of patients with a
@xmath higher or lower than an optimal cut-off threshold calculated with
the C-index method. Log-rank test was used to determine the statistical
differences between the estimated survival functions of the
aforementioned groups.

#### 6.3.3 Association among Os and Hts markers - inter-center study

In order to determine the degree of agreement in describing the vascular
heterogeneity of glioblastomas, we conducted an study measuring the
similarities of the HTS marker distributions among the clinical centers
enrolled in the study. To this end we conducted a pair-wise Mann-Whitney
U-test @xmath , followed by a post-hoc Tukey’s honest significant
difference criterion test.

Cox regression analyses were also conducted to assess whether the
association between patient OS and HTS markers differed among the
centers. Kaplan-Meier analyses were conducted after dividing the
population of each center using the same cut-off thresholds previously
calculated for the whole cohort study.

All the statistical analyses were performed on Matlab R2017b (MathWorks,
Natick, MA).

### 6.4 Results

#### 6.4.1 Association among Os and Hts markers - whole cohort study

Table 6.4 summarizes the Cox proportional hazard analysis between HTS
markers and patient OS . Statistically significant negative associations
were found for @xmath at HAT , LAT and IPE habitats and patient OS ,
with the IPE marker showing the highest HR @xmath . Kaplan-Meier results
are presented in Table 6.5 , including estimated optimal cut-off
thresholds, the number of patients assigned to each group, the estimated
C-index area under the curve, the median OS calculated per group, and
the log-rank P-values.

Significant differences in OS between low and high vascular groups
divided by HTS marker values were found. Consistently with previous
results in the literature ( JuanAlbarracin2018 ) , patients with a low
@xmath at HAT , LAT and IPE habitats presented a higher median survival
rate.

Figure 6.1 shows the Kaplan-Meier estimated survival functions for the
populations divided in high-vascular and low-vascular groups according
to the optimal-cutoff threshold for the @xmath estimated with the
C-Index method.

#### 6.4.2 Association among Os and Hts markers - inter-center study

No statistical differences were found between the @xmath values at the
different HTS habitats among most of the centers, specially for the IPE
habitat, which was the most correlated with OS in the whole cohort study
of section 6.4.2 (see tables 6.6 , 6.7 , 6.8 , 6.9 ; @xmath indicates
statistical significant difference).

Box-whisker plot shown in Figure 6.2 summarizes the information
contained in tables 6.6 , 6.7 , 6.8 , 6.9 . Significant overlapping
among the distributions of @xmath of each hospital can be observed,
indicating no statistical differences in the HTS markers among centers.

Table 6.10 shows the results of the Cox regression analysis grouped per
hospital, to investigate the association of the HTS markers with patient
OS at each center. Due to the small sample sizes of some centers,
confidence intervals are wider, so the results of this analysis are more
uncertain. However, overall, results are consistent with those obtained
in the whole cohort study, consolidating the significant association
between HTS habitats and patient OS in highly heterogeneous scenarios.

Figure 6.3 shows a diagram of the HR s and confidence intervals of the
HAT , LAT and IPE habitats per center, and using all the data together
in the analysis. The figure shows a significant overlap between
confidence intervals for most of the centers, suggesting no significant
differences among them, and consolidating the results obtained in the
whole cohort study.

Figure 6.4 show the Kaplan-Meier estimated survival functions per
center, dividing the population of each center in high and low @xmath
values at HAT , LAT and IPE habitats according to the optimal thresholds
obtained with the C-index method.

### 6.5 Discussion

In this work we have conducted an international multi-center validation
of the HTS method published in ( JuanAlbarracin2018 ) . Using data from
seven European centers, significant negative associations have been
found between patient OS and the HTS markers at HAT , LAT and IPE
habitats, consolidating the results obtained in the aforementioned
single-center study conducted by JuanAlbarracin2018 .

Addressing heterogeneity between centers in the estimation of MRI
markers is not an easy task. Several authors in the bibliography have
pointed out the uncertainty and low reproducibility of MRI markers,
especially across multiple centers ( Abramson2015 ; Oconnor2017 ;
Schnack2004 ; Deguio2016 ) . The non-quantitative nature of several MRI
acquisitions and the manual procedure for obtaining MR -based biomarkers
introduce important sources of variability, making it difficult to
validate new robust and stable MRI markers ( Schnack2004 ) .

The HTS method focuses its efforts on the automated delineation of
habitats related to perfusion patterns within the lesion in a robust and
reliable manner. In the current study, a cohort with large variations in
terms of patient demographics as well as MRI acquisition protocols was
used to measure the robustness of the method. The experiments conducted
in our study did not show relevant differences among the distributions
of the HTS markers obtained from MRI studies acquired at the different
centers. Only for a small number of cases, significant differences were
found for the HAT and LAT markers among centers. These results strongly
suggest that the HTS method is robust against inter-center variability
in the task of describing the vascular heterogeneity of the
glioblastoma. Furthermore, the results of the Cox and Kaplan-Meier
analyses per center showed robust associations between patient OS and
the HTS markers, regardless of the center of origin. The proposed
thresholds were also effective in stratifying patients from different
centers into low- and high-vascular groups, presenting different OS
tendencies.

Consistently with the literature, the HTS method strongly correlates
with patient OS when observing the @xmath values at HAT habitat. Such
values represent the most hyper-perfused measures of the glioblastoma,
which aligns with the measurements proposed in previous studies in the
literature ( Jain2014 ; Liu2017a ; Hirai2008 ) . As expected, shorter OS
survival rates were found for patients with higher @xmath values at the
HAT habitat. Similarly, we also found that LAT habitat present strong
association with OS and high stratification abilities. Both results have
been replicated under highly variable conditions of MRI acquisition
protocols and patient demographics, hence demonstrating the robustness
of the HTS method in describing the vascular arrangement of the
glioblastoma.

One of the most important finding presented in JuanAlbarracin2018 was
the correlation between long-term OS s and lower @xmath values in the
IPE habitat. The peritumoral region of the glioblastoma is the most
heterogeneous area of the tumor, in which uncontrolled infiltration
occurs. Moreover, the inter-patient variability and the inter-center
heterogeneity significantly increases the uncertainty in this region,
obscuring the important information that it contains. However, in the
present study we found statistical association between perfusion markers
at this region and patient OS , even under the large heterogeneous
nature of the proposed cohort. Moreover, effective stratification
capabilities were also found when employing this marker as indicator to
divide the population between high- and low-vascular glioblastomas.

Having demonstrated the influence of early-stage vascularity on the
prognosis of glioblastoma, we suggest the use of this factor in any
clinical study that includes population randomization. Authors consider
that the HTS method will help overcome current limitations and improve
patient recruitment and randomization by initiating a route map to avoid
the second translational gap cited previously in ( Abramson2015 ) .

One of the most important limitations of our study is the imbalance
between the number of patients in each center. Although the whole cohort
size is large enough for a powerful statistical study, some of the
participating centers provided a low number of patients (less than 15
patients), which introduces limitations and uncertainties into the
studies conducted per center. On the other hand, since the influence of
the molecular markers in patient prognosis has been clearly demonstrated
( Louis2016 ; Verhaak2010 ) , it may be of interest to add them as
co-factors in the regression survival models. In future studies we plan
to analyze the possible association between molecular and imaging
markers and their prognostic possibilities.

## Chapter 7 ONCOhabitats: A system for glioblastoma heterogeneity
assessment through Magnetic Resonance Imaging

Neuroimaging analysis is currently crucial for an early assessment of
glioblastoma, to help improving treatment and tumor follow-up. To this
end, multiple quantitative and morphological MRI sequences are usually
employed, requiring the development of automated tools capable to
extract the relevant information contained in these sources. Despite
major advances in MRI brain tumor technology, the latter is generally
private and inaccessible to the research community. This significantly
slows down the advances in tumor understanding, as many researchers
continually have to re-implement many software pieces for the typical
MRI analysis pipeline to conduct their investigation. This task is often
arduous or even unreachable to many research groups specialized on the
clinical aspects of the tumor, and with less knowledge about
state-of-the-art technology for MRI analysis. As a result, many efforts
are lost in developing this necessary technology before research begins.

In this thesis, several methods have been developed to analyze
glioblastoma through MRI . One of the aspects significantly taken into
consideration since the beginning of this thesis was to, parallel to the
academic and theoretical research, develop the required infrastructure
to facilitate public access to the technology developed in the thesis.
In this sense, this chapter presents ONCOhabitats (
https://www.oncohabitats.upv.es ): an online open access system for
glioblastoma heterogeneity assessment by MRI data. ONCOhabitats provides
two services for untreated glioblastomas: 1) malignant tissue
segmentation, and 2) vascular heterogeneity assessment of the tumor. The
segmentation service was validated against the BRATS 2017 reference
dataset, showing comparable results with current state-of-the-art
methods ( WT Dice score: 0.89). The vascular heterogeneity assessment
service was validated in a retrospective cohort of 50 patients, in a
study focused on predicting patient OS . Cox proportional hazard
regression analysis and Kaplan-Meier survival study showed significant
positive correlations (p-value @xmath .05) between the HTS habitats and
patient OS . ONCOhabitats system also generates radiological reports for
each service, including volumetries and perfusion measurements of the
different regions of the lesion. Additionally, ONCOhabitats gives access
to the scientific community to a computational cluster capable to
process about 300 cases per day.

The contents of this chapter were published in the journal publication (
JuanAlbarracin2019b ) —thesis contributions C6 and P4, P8, P9 and P10.

### 7.1 Introduction

Glioblastoma is a primary brain tumor presumed to arise from neuroglial
cells. It is the most malignant and frequent astrocytoma, accounting for
more than 60% of all brain tumors in adults. Glioblastoma has a global
incidence of 4.67 to 5.73 per 100,000 people, and presents a poor
prognosis of 14-15 months under the best treatment ( Stupp2005 ) .

Heterogeneity is a hallmark that has been identified as crucial to
understand the tumor aggressiveness and its resistance against therapies
( Lemee2015 ; Soeda2015 ) . Specifically, glioblastoma is characterized
by a high heterogeneity both at macroscopic tissue level, with
co-existence of different malignant tissues within the neoplasm (
Liu2017b ) ; as well as at microscopic cellular level, with different
molecular sub-types and genetic alterations ( Inda2014 ) . Such
heterogeneity rises this tumor as one of the deadliest malignant primary
brain tumor in adults ( Ostrom2015 ) .

Molecular analysis of glioblastoma has largely improved the
understanding of the biological heterogeneity of these tumors. Molecular
profiling of glioblastoma has allowed the identification of different
tumor sub-types, helping in the development of more efficient drugs (
Parsons2008 ; Verhaak2010 ) . However, in the past years, significant
interest has been placed in the analysis of glioblastoma heterogeneity
based on medical imaging, to discover non-invasive tumor features
related to different outcomes such as overall survival, tumor grading or
glioblastoma molecular sub-typing ( Wangaryattawanich2015 ) .

Characterization of glioblastoma heterogeneity based on MRI has been
addressed from a wide range of approaches. Glioblastoma tissue
segmentation has gathered most of these efforts. Automated
identification of the different tissues that co-exist in the lesion,
such as enhancing tumor, non-enhancing tumor, edema and necrosis, has
been largely addressed by the scientific community. This has lead to
initiatives such as the BRATS challenge, which was born in 2012 and has
become the reference benchmark ( Menze2015 ) to evaluate the
state-of-the-art of automated high-grade and low-grade glioma
segmentation algorithms. Nowadays, with the advent of novel deep
learning techniques, the current state-of-the art is mostly dominated by
CNN classifiers. CNN s are a class of deep feed-forward neural networks
whose architecture is particularly well suited for computer vision
recognition tasks. In medical image analysis field, CNN s have
outperformed most of algorithms in many problems, arising as the winner
technique in most challenges such as BRATS , ISLES or PROMISE12
challenges ( BRATS2016 ; BRATS2017 ; BRATS2018 ) .

In addition to glioblastoma tissue segmentation, PWI has played a key
role in the advanced characterization of the tumor heterogeneity based
on MRI ( Shah2010 ; Lupo2005 ; Knopp1999 ) . Glioblastoma is
characterized by a robust angiogenesis, strong vascular proliferation
and an aberrant microvasculature ( Alves2011 ; Hardee2012 ;
Kargiotis2006 ) . Numerous studies have focused on the analysis of
perfusion indices to assess tumor grading ( Law2003 ; Emblem2008 ) ,
early response to treatment assessment ( Elmghirbi2017 ; Vidiri2012 ) ,
recurrence vs radionecrosis ( Hu2009 ; Barajas2009 ) or clinical outcome
prediction ( Mangla2010 ; Jain2014 ) . More recent studies addressed the
local characterization of sub-regions within the glioblastoma using DSC
, DCE or Magnetic Resonance Spectroscopy Imaging ( MRSI ) ( Artzi2014 ;
Akbari2014 ; Sawlani2010 ; Raschke2019 ) . Specifically, in (
JuanAlbarracin2018 ) an unsupervised method called HTS was proposed to
characterize the vascular heterogeneity of glioblastoma based on DSC .
This method combined perfusion biomarkers and glioblastoma tissue
segmentation to discover habitats within the neoplasm, showing
significant correlation with patient overall survival.

However, despite the great advances in novel methods to describe
glioblastoma heterogeneity, most of them are based on private algorithms
and in-house technology developed by the authors, non-accessible for the
scientific community. MRI -dedicated libraries such as Advanced
Normalization Tools ( ANTs ) ( Avants2011 ; Tustison2014 ) , FSL (
Jenkinson2012 ) or ITK ( Avants2014 ) , as well as modern toolkits for
deep learning such as Tensorflow ^(™) or PyTorch , are provided to
develop such technologies. However, these libraries are just the pieces
to build the complex state-of-the-art models to analyze glioblastoma,
whose development requires considerable efforts, resources and arduous
learning curves, which are not often accessible to many researchers or
institutions. In this regard, open-access public platforms that
implement state-of-the-art techniques in a user-transparent manner are
highly desirable to bring to the scientific community the possibility to
conduct advanced multiparametric analysis of glioblastoma.

In this work we present ONCOhabitats : an online system aimed to provide
state-of-the-art analysis services for glioblastoma. ONCOhabitats
provides two main services for untreated glioblastoma: 1) High-grade
glioma tissue segmentation based on CNN ; and 2) glioblastoma vascular
heterogeneity assessment by means of the HTS method proposed in (
JuanAlbarracin2018 ) . For each service, ONCOhabitats returns the
preprocessed images, the tissue segmentation and habitats maps, and
automatically generates a radiological report summarizing all the
findings of the study.

### 7.2 Materials

To validate ONCOhabitats technology several datasets were employed for
the different services.

ONCOhabitats high-grade glioma segmentation service was evaluated with
the public BRATS 2017 challenge dataset, provided for the international
MICCAI 2017 conference. The training corpus of the BRATS 2017 dataset
consists of multi-parametric MR scans of 210 high-grade gliomas: 20
patients from the BRATS 2013 dataset, 88 from the Center for Biomedical
Image Computing and Analytics ( CBICA ) and 102 from the The Cancer
Imaging Archive ( TCIA ) corpus. The validation corpus consists of 46
multi-contrast MR scans of high-grade glioma patients distributed in 16
cases from the CBICA institution, 24 cases from the TCIA corpus and 6
cases from the University of Alabama at Birmingham ( UAB ) department.

For each patient, pre- and post-gadolinium T @xmath -weighted, T @xmath
-weighted and FLAIR MR exams were provided. All images were linearly
co-registered to the post-gadolinium T @xmath -weighted exam, skull
stripped, and interpolated to 1mm ³ isotropic resolution.

Manual expert annotations of this dataset comprise 4 classes: Class 1)
necrosis, cyst, hemorrhage and non-enhancing tumor; class 2) surrounding
edema; class 4) enhancing tumor core; and class 0) for everything else.
Evaluation is assessed for 3 different compartments, whose composition
is shown in table 7.1 .

The glioblastoma vascular heterogeneity service was validated with a
retrospective local dataset of 50 patients, including 33 men with an
average age of 60.94 years (range, 25–80 years) and 17 women with an
average age of 62.53 years. MRI included pre- and post-gadolinium T
@xmath -weighted, T @xmath -weighted and FLAIR MR exams, and DSC T
@xmath -weighted perfusion study. The institutional review board
approved this retrospective study, and the requirement for patient
informed consent was waived. The patient inclusion criteria and MRI
protocol are extensively detailed in section 5.2 .

### 7.3 Methods

ONCOhabitats provides two main services: 1) High-grade glioma
segmentation, and 2) glioblastoma vascular heterogeneity assessment.

Figure 7.1 shows an outline of the different sub-processes involved in
each service. The first pipeline performs a morphological segmentation
of high-grade glioma tumors by first pre-processing the MRI and then
using a 3D U-Net CNN classifier. The second pipeline extends the
morphological segmentation pipeline by incorporating DSC perfusion
pre-processing and quantification, and the HTS method to detect regions
within the glioblastoma with different hemodynamic activity.

We will first describe both services and then the ONCOhabitats on-line
system will be presented.

#### 7.3.1 High-grade glioma segmentation service

ONCOhabitats considers three tissues to morphologically describe
high-grade gliomas: 1) enhancing tumor, 2) edema and 3) necrotic and
non-enhancing regions of the tumor. The service is composed of two
stages: 1) MRI preprocessing and 2) Segmentation based on CNN s .

##### Mri preprocessing

Our preprocessing module includes the following steps: (1) voxel
isotropic resampling of all MR images, (2) denoising, (3) rigid
intra-patient MRI registration, (4) affine registration of all sequences
to Montreal Neurological Institute ( MNI ) ICBM space, (5)
skull-stripping and (6) magnetic field inhomogeneity correction. Voxel
resampling is performed at 1mm ³ by means of linear interpolation.
Denoising is carried out using the adaptive non local means filter (
Manjon2010a ) with a search window of @xmath voxels and a patch window
of @xmath voxels. Registration is conducted with the ANTs software (
Avants2008 ) , taking the T @xmath sequence as reference and using
Mutual Information metric. In a previous version of ONCOhabitats,
skull-stripping was performed with an in-house pipeline based on a
non-linear registration of the T @xmath sequence to a template with a
known intra-cranial mask. Nowadays, skull-stripping if performed with a
patch-based U-net CNN working with T @xmath patches of @xmath trained on
120 manually segmented glioblastomas. The network has a performance of
@xmath Dice score on an independent test set. Finally, magnetic field
inhomogeneities are corrected with the N4 software using the previously
computed intra-cranial mask ( Tustison2010 ) .

##### High-grade glioma segmentation

ONCOhabitats CNN takes as input the T @xmath , T @xmath and FLAIR MRI
and works with 3D patches of size @xmath . We followed a U-net
architecture ( Ronneberger2015 ; Soltaninejad2018a ; Dong2017 ) of 5
levels, with a contracting and expanding paths of 4 residual-blocks
preceded of 4 simple-blocks . A simple-block consists of the following
sequence of operations: convolution + batch normalization + ReLu
activation function, while a residual-block implements the proposal of
He et al ( He2016 ) : convolution + batch normalization + ReLu +
convolution + batch normalization + residual connection + ReLu
activation function. Max-pooling of size 2 is employed to down-sample
patches at each level, while transpose convolutions are employed to
up-sample patches in the expanding path. The number of filters per level
are: 16 at first level (native patch resolution of @xmath ), 32 filters
at second level (patch resolution of @xmath ), 64 filters at third level
(patch resolution of @xmath ), 128 filters at fourth level (patch
resolution of @xmath ) and 256 filters at fifth level (patch resolution
of @xmath ). Long-term concatenations are also employed to connect
blocks at each level.

Isotropic kernels of @xmath were employed for all convolutions. The
network was trained using Adam Optimizer with an initial learning rate
of @xmath and cross-entropy was used as loss function. @xmath
regularization with penalty @xmath was employed to avoid for
over-fitting. We employed a batch size of 64 individuals, forcing an
equal representation of enhancing tumor, edema, necrosis and healthy
patches to compensate for class imbalance. The network was trained for
@xmath iterations.

Figure 7.2 summarizes the network architecture and the internal design
of the simple and residual blocks.

#### 7.3.2 Glioblastoma vascular heterogeneity assessment service

This service extends the morphological segmentation service by
introducing perfusion information into the study. The service implements
the HTS method presented in ( JuanAlbarracin2018 ) , which aims to
describe the vascular heterogeneity of glioblastoma. The HTS combines
the glioblastoma morphological segmentation with perfusion indexes such
as rCBV and rCBF to discover habitats within each tissue with different
patterns of vascularity. We found that these habitats provide relevant
information to early predict patient survival, even taking into account
the variations in treatment.

The service includes four stages: 1) MRI preprocessing, 2) Segmentation
based on CNN s , 3) Perfusion quantification and 4) Vascular habitats
detection. The MRI preprocessing and glioblastoma segmentation are
inherited from the high-grade glioma segmentation service.

##### Perfusion quantification

ONCOhabitats rCBV and rCBF maps are quantified by means of standard
techniques proposed in the literature ( Knutsson2010 ) . For a detailed
explanation of the calculation of perfusion parametric maps, please
refer to section 2.2.2 . In order for this chapter to be self-contained,
a short remainder will be made.

T @xmath -weighted leakage effects are automatically corrected using the
Boxerman method ( Boxerman2006 ) , while gamma-variate curve fitting is
employed to correct for T @xmath extravasation phase. rCBV is computed
by numerical integration of the area under the gamma-variate curve (
Knutsson2010 ) , while rCBF is calculated based on the block-circulant
SVD devolution technique proposed in ( Wu2003 ) . The AIF is
automatically calculated using a divide and conquer algorithm, which
recursively dichotomizes the gadolinium concentration-time curves into
two groups, selecting those curves with higher peak height, earliest
time to peak and quickest wash-out (i.e. lowest full width at half
maximum). The AIF is finally computed as the average of the curves of
the final group that contains 10 or fewer curves.

##### Vascular habitats detection

The HTS method ( JuanAlbarracin2018 ) describes the vascular
heterogeneity of the glioblastoma by means of an unsupervised analysis
of the perfusion patterns detected within the lesion. Such analysis is
designed to yield four habitats: the HAT , the LAT , the potentially IPE
and the VPE .

The unsupervised analysis of perfusion patterns is carried out through
the DCM - SVFMM algorithm ( Sfikas2008 ) (with DCAGMRF prior ( Nikou2007
) ). Such algorithm is an extension of the classic FMM specially focused
on image data, which incorporates a continuous MRF on the spatial
coefficients of the model to capture the self-similarity and local
redundancy of the images. The HTS method consists of two stages: (a) an
initial re-definition of the enhancing tumor and edema ROI s obtained by
the morphological segmentation using perfusion information, and (b) a
cluster analysis of the perfusion heterogeneity within each previously
mentioned ROI to detect the different vascular behaviors expressed by
the neoplasm. A comprehensive detailed explanation of the HTS method is
performed in section 5.3.3 .

#### 7.3.3 Clinical report

Clinical reports are automatically generated after the finalization of
each ONCOhabitats job. The reports summarize all the findings of the
studies, including morphological and functional measurements of each
glioblastoma tissue and habitat. Regarding the morphological
segmentation service jobs, absolute tissue volumetry in cm ³ , as well
as relative volumetry with respect to the intra-cranial cavity are
calculated for the enhancing tumor, necrosis and edema tissues.
Concerning the vascular heterogeneity assessment service jobs, in
addition to the tissue volumetries, habitat’s absolute and relative
volumetries are also calculated. Moreover, a tendency analysis of the
perfusion biomarkers confined within each tissue and habitat is also
included in the report. Median perfusion values of each ROI as well as
Median Absolute Deviations are calculated to robustly determine the
vascular tendency of each sub-comparment of the lesion. Finally,
perfusion prototypical curves in combination with a radar chart of the
perfusion biomarkers at each region are also included in the report for
a visual representation of the functional behavior of the glioblastoma.
Figures 7.3 , 7.4 , 7.5 , 7.6 , 7.7 show an example of a vascular
heterogeneity assessment report.

#### 7.3.4 ONCOhabitats system

ONCOhabitats is a web service solution to carry out the previously
presented analyses. The platform implements a Software as a Service (
SaaS ) model to automatically analyze glioblastoma cases. Figure 7.8
shows the diagram scheme of ONCOhabitats system.

The system implements a Wordpress ^(®) landing web-page as front-end for
the user. Before applying for a job, user must be registered in
ONCOhabitats system. Registration requires a username, the first and
last names, the institution of provenance and a valid email only used to
inform the user about the status of their jobs. After registration, the
user can upload the MR images to a data storage secure server to launch
the jobs.

The system implements secure encrypted communication via HTTPS protocol
with a trusted certificate to enhance data protection and privacy. The
data storage includes file encryption and secure transfer protocol via
SFTP. ONCOhabitats currently supports DICOM and NIfTI (compressed and
uncompressed) medical imaging formats. An automated de-identification is
carried out for all DICOM files using the gdcmanon tool from the
Grassroots DICOM library. Once DICOM files are de-identified, they are
converted to compressed NIfTI format and automatically removed from the
server. The de-identified NIfTI files are stored in a secure server,
non-accessible through the ONCOhabitats website, to enhance security and
data protection. Once the analysis is complete, the pre-processed images
as well as the resulting segmentation masks and biomarker maps are
stored in a separated data storage, only accessible for 15 days by the
user owner of the job. After this period, all the data is completely
removed from the ONCOhabitats servers, unless the user explicitly
specifies through his account web-page that his data can be used for
research purposes, in which case the data is kept on a private server.
This procedure follows the guidelines recommended by the Data Protection
Officer (DPO) of the Universitat Politècnica de Valencia (UPV) and has
been approved by the ethical committee of our institution.

ONCOhabitats system equips 7 DELL PowerEdge R720 ^(®) dedicated servers,
each one shipping two Intel Xeon E5-2620 CPUs with a total of 12 cores
and 64 GB of RAM. Two NVidia Titan Xp with 3840 CUDA ^(®) cores and 12
GB of RAM supports the cluster for the deep learning tasks.

The ONCOhabitats pipelines are mostly implemented in C++ using ITK and
Eigen libraries. MATLAB ^(©) and PHP are also employed as scripting
languages for different tasks. Tensorflow ^(™) is used to develop the
deep learning segmentation models.

ONCOhabitats has been designed to deal with up to 14 concurrent jobs,
each job taking approximately one hour, which yields a theoretical limit
of 336 processed cases per day. The terms of use of the system are
available in the landing web page, paying particular attention to the
GDPR compliance and the non-commercial research purpose of the system.

### 7.4 Results

Since each ONCOhabitats service performs a different task, they have
been evaluated separately with different datasets and methodologies. The
results of the evaluations are presented below.

#### 7.4.1 High-grade glioma segmentation service

High-grade glioma tissue segmentation performance was evaluated
according to BRATS evaluation guidelines. Such evaluation comprises the
assessment of the segmentation quality of the WT region, the TC region
and the ET area. Dice metric, as well as sensitivity and specificity of
each region was computed to compare ONCOhabitats results with several
state-of-the-art methods.

The results of our high-grade glioma segmentation service are presented
in Table 7.2 . A comparison between the results obtained by ONCOhabitats
system and the ones obtained by each participant of the challenge for
the BRATS 2017 validation set are presented in Figure 7.10 . As the
figure shows, ONCOhabitats offers competitive comparable segmentations
with the state-of-the-art algorithms, always achieving results above the
median of the participants and in most cases close to the third
quartile. Moreover, CNN yield segmentations of high specificity, which
is a highly desirable property for a medical tool, ensuring a very low
false positive rate.

#### 7.4.2 Glioblastoma vascular heterogeneity assessment service

An extensive evaluation of the HTS method at different levels is
presented in section 5.4 .

First, a statistical evaluation to assess the degree of similarity among
the rCBV and rCBF distributions for the HAT , LAT , IPE and VPE habitats
was conducted. Global probabilistic deviation metric ( Saez2017 ) was
employed as a multi-dimensional extension of Jensen-Shannon divergence
to measure distances between distributions. The analysis yielded the
following average results for the population: @xmath for rCBV and @xmath
for rCBF . Such results indicated that the perfusion distributions of
the habitats for each patient of the study were statistically
significantly separated, hence corroborating that the HTS habitats
describe regions within the glioblastoma with different hemodynamic
behavior.

Second, the prognostic capabilities of the HTS habitats were studied.
Cox proportional hazard regression analysis and a Kaplan-Meier study
were conducted to measure the degree of correlation of the HTS habitats
with patient OS . The maximum rCBV and rCBF value at each habitat
(computed as the 95% percentile of the distribution) was the marker with
better results in concordance with previous studies in the literature (
Wetzel2002 ) . HAT and LAT habitats, as well as IPE yielded positive
correlations with overall survival for both Cox and Kaplan-Meier
studies. Benjamini-Hochberg false discovery rate at @xmath level of 0.05
was employed to correct for multiple hypothesis testing, increasing
confidence of the statistical results.

### 7.5 Discussion

In this work we presented ONCOhabitats: an on-line open-access system to
study different aspects of glioblastoma such as the tumor morphology and
the vascular tumor heterogeneity patterns. ONCOhabitats provides the
user with consolidated state-of-the-art techniques based on both deep
learning and unsupervised structured learning algorithms previously
published in the literature. The system also generates automated
radiological reports summarizing the findings of the analysis, in a
formal document easily integrable in clinical routine.

The ONCOhabitats high-grade glioma segmentation was compared against the
current state-of-the-art methods presented at BRATS 2017 challenge. Our
system yields comparable results with these approaches, demonstrating
competitive comparable results with no significant differences between
them. Our method is completely deterministic and reproducible, which is
a highly desirable property to conduct large population studies or
clinical trials. The higher reproducibility of a system, the greater the
likelihood to detect changes in the disease. Additionally, ONCOhabitats
implements the Hemodynamic Tissue Signature method presented in (
JuanAlbarracin2018 ) , to study the vascular heterogeneity of the tumor.
Glioblastoma vascular heterogeneity has been demonstrated to be a key
hallmark to understand the behavior of this masses. The HTS analysis
allows to study the different vascular patterns of the lesion, detecting
functional habitats within the tissues that have been demonstrated to
contain relevant information about patient’s survival at a very early
stage of the disease.

ONCOhabitats offers these analyses by means of a free web-based
solution. We give access to the scientific community not only to our
software services but also to our computational resources, avoiding the
requirement for medical imaging experts, expensive computational labs
and arduous learning curves to develop the technology. We provide a
system capable to process about 300 cases per day including MRI
preprocessing and standardization, tissue segmentation, DSC perfusion
quantification and vascular heterogeneity assessment of the lesion. The
system is designed to be immediately scalable by adding new computing
machines and also cloud-based services as the workload increases.

The major limitation of our system is the time to process a case.
Glioblastoma study involves the analysis of a considerable amount of MRI
to capture all the information contained in the lesion, requiring to
process a huge amount of data. Currently, MRI preprocessing accounts for
the most part of the processing time, so we are currently developing new
approaches to optimize this module.

Currently, ONCOhabitats system is participating in the international
clinical trial NCT03439332 , aimed to validate the ONCOhabitats
technology in a multi-centre observational study with 300 patients from
hospitals from Spain, Italy, Belgium and Norway. In future work, we plan
to extend ONCOhabitats system to handle DWI and Diffusion Tensor Imaging
( DTI ) sequences, as well as post-surgery and follow-up studies for a
longitudinal assessment of the glioblastoma.

## Chapter 8 Concluding remarks and recommendations

This chapter finalizes the work conducted in this thesis and summarizes
the main concluding remarks and recommendations derived from it.
Additionally, in this chapter we provide the guidelines for continuing
the scientific research and development based on this work.

### 8.1 Concluding remarks

AI medical image analysis is a cornerstone in the future of modern
precision medicine. The ability to non-invasively measure morphological
and quantitative characteristics of complicated diseases such as
glioblastoma is an invaluable aid in successfully combating these lethal
lesions. In the particular case of glioblastoma, to date, this tumor
still remains a major challenge, as there is no satisfactory therapy for
it. Understanding its high heterogeneity, and in particular its vascular
heterogeneity, constitutes a key element in advancing the design of
effective therapies. Therefore, it is essential to continue the study
and research of this neoplasm from different perspectives: its
pathology, its molecular and genetic processes, its immunology and, of
course, through neuroimaging. The latter is rapidly evolving towards
richer and more complex multiparametric acquisitions that require
increasingly advanced computational models to harness the raw
information they contain. In this sense, this thesis has contributed to
the assessment and characterization of the vascular heterogeneity of the
glioblastoma by means of unsupervised ML techniques applied to MRI data,
able to discover habitats within the lesion with early prognostic
capabilities.

This thesis has contributed to the state-of-the-art in the fields of
Medical Informatics, Statistics and Probability, Radiology and Nuclear
Medicine, Machine Learning and Data Mining and Biomedical Engineering.
The scientific publications in top-ranked journals and international
conferences derived from this thesis endorse the research carried out in
these fields. Furthermore, the methods and technology developed in this
thesis have been integrated in a public open-access platform for its use
by the medical and research community, or for its posterior
industrialization.

The specific concluding remarks of this thesis are listed as follows.

-   Unsupervised learning is confirmed as a viable tool for MRI analysis
    and pathological pattern detection. We found that, although
    supervised learning normally achieves better results in well-known
    tasks such as image segmentation, unsupervised learning is also able
    to accurately capture MRI patterns related to morphological and
    physiological characteristics of the lesion. The study conducted in
    chapter 3 demonstrated the potential of unsupervised learning
    approaches, showing a consistent behavior across the different
    datasets, which is a highly desirable property when dealing with
    heterogeneous data.

    This settled the basis for the subsequent contributions carried out
    in the thesis, where no ground-truth exist from where to learn
    supervised methods. The performance obtained in this preliminary
    study in segmenting well-known tissues based on MR intensity
    patterns gave us evidences and confidence in the following studies
    conducted in the thesis.

    This concluding remark responds to the research question RQ1, covers
    the objectives O1 and O2 and was derived from the works in
    publications P1, P2 and P3.

-   The SVFMM is a powerful and robust state-of-the-art framework for
    unsupervised learning of imaging data. The Bayesian nature of this
    model provides great flexibility to inject existing knowledge into
    the learning process, to successfully capture spatial redundancy of
    the images and introduce local regularization. Moreover, it also
    provides mechanisms to guide the learning process towards plausible
    solutions aligned with task-specific constraints.

    The SVFMM has been the basis for the HTS method presented in this
    thesis to describe the vascular heterogeneity of glioblastomas.
    Particularly, we have proposed a variant of the SVFMM combined with
    the probabilistic NLM scheme that achieves better results compared
    to the alternative approaches in their family. The proposed approach
    also simplifies the previous models since the probabilistic NLM
    weighting function does not introduce additional parameters to the
    model.

    This concluding remark responds to the research question RQ2, covers
    the objective O3 and was derived from the work in publication P3.

-   Early-stage vascular heterogeneity provide crucial information about
    expected survival of patients with glioblastoma undergoing
    standard-of-care treatment. The HTS habitats successfully capture
    this heterogeneity by analyzing the perfusion patterns within the
    lesion, showing improved association with OS with respect to
    alternative approaches. Consistently with the literature, habitats
    related to the enhancing tumor presented strong correlation with
    patient OS . However, the most important finding is the positive
    association of the @xmath in the IPE habitat with OS . The
    infiltrated edema today catches all the attention since it is
    identified as the critical region for many decisive treatments such
    as surgical resection or radiotherapy, thus increasing the
    importance of this finding.

    Moreover, the HTS relies on a conceptual framework to describe the
    heterogeneity of a lesion by means of detecting habitats with
    differentiated MRI functional profiles. This enables a new
    perspective in the characterization of complex lesions under the
    medical imaging paradigm. This approach introduces a concept-shift
    in the segmentation of lesions towards the delineation of regions
    sharing a similar physiological behavior rather than a common
    morphological appearance.

    This concluding remark responds to the research questions RQ3 and
    RQ4, covers the objectives O4 and O5 and was derived from the works
    in publications P5, P6 and P7.

-   ONCOhabitats ( https://www.oncohabitats.upv.es ) platform
    encapsulates all the original methods and algorithms developed in
    this thesis, and several state-of-the-art algorithms for medical
    image analysis, in a public open-access system free for the medical
    and research community. ONCOhabitats is a reliable system for the
    study of glioblastoma that provides an end-to-end analysis of the
    lesion, from the preprocessing of the raw MRI obtained from the
    scanner to the final measurement of volumetries and quantitative
    biomarkers of habitats representing regions with a specific
    physiological behavior.

    ONCOhabitats is designed modularly to allow an easy reuse of the
    technology, a sustainable development cycle and a high scalability.
    It is mainly written using open-access state-of-the-art libraries
    with the aim to positioning the system as a reference platform for
    the analysis of brain tumors through medical imaging. In this sense,
    ONCOhabitats not only offers cutting edge technology, but also
    provides access to their computational resources, allowing a
    case-analysis-rate of about 300 cases per day.

    The software is registered in the technological offer of the UPV and
    is protected under the patents ES201431289A in Spain, EP3190542A1 in
    Europe and US20170287133A1 in United States.

    This concluding remark responds to the research question RQ5, covers
    the objective O6 and was derived from the works in publications P4,
    P8 and P9.

### 8.2 Recommendations

Glioblastoma tumor still remains a lethal disease that requires a
tireless multi-disciplinary effort to understand its behavior,
evolution, proliferation and survival mechanisms that grant its
uncontrollable aggressiveness. Under this scenario, the future of the
analysis of such lethal diseases lies in the inclusion of ML techniques,
capable of analyzing the vast amount of complex multi-disciplinary
medical information available to a patient, with the aim of developing
personalized therapies that exploit the particularities of each
individual.

The developed methods and research findings performed in this thesis
points to the aforementioned direction and can serve as a starting point
for further research. In this sense, the following recommendations are
suggested.

-   Despite the unquestionable power, utility and performance that
    supervised learning is demonstrating nowadays, its ability to
    discover new knowledge from biomedical data is severely limited by
    its learning mechanism. This task is, by the opposite, perfectly
    suited for unsupervised learning. The exploratory nature inherent in
    unsupervised learning provides it with the ability to detect hidden
    patterns within the data, often imperceptible to the human.

    In this regard, we encourage the use of unsupervised learning for
    medical image analysis and biomedical data mining in general, to
    build descriptive models of the data capable of capturing subtle
    patterns that lead to undetectable important findings for the human
    being. Therefore, unsupervised learning must assume a relevant role
    in modern medicine to situate ML as an essential tool for the future
    of personalized therapies.

-   Learning from structured data, such as MR images, requires ML models
    capable to exploit the conditional dependencies and spatial
    correlations associated to these data. Historically, MRF s have
    proven to be powerful mathematical models to capture the local
    dependencies encoded in the images. Specifically, SVFMM s constitute
    a versatile statistical framework to describe heterogeneous
    structured imaging data under a strong mathematical foundation. On
    the other hand, the NLM image processing scheme has also proven to
    achieve state-of-the-art results in many image-related tasks such as
    denoising, super-resolution, in-painting or patch-based
    segmentation. We encourage the use of the NLSVFMM algorithm, as it
    brings together the potential of both approaches in a fully Bayesian
    statistical model that has demonstrated comparable state-of-the-art
    performance in a model mathematically less complex than those of its
    family.

    Nevertheless, the current trend in ML focuses on the use of CNN s ,
    since they have demonstrated to be the state-of-the-art models for
    most of image analysis related tasks. CNN s , however, are
    mathematical models mainly oriented to the supervised learning
    paradigm. Therefore, we advocate the need to investigate new
    architectures and learning schemes to exploit the potential of CNN s
    in an unsupervised learning scheme. Nowadays, Convolutional
    AutoEnconder s with latent space clustering losses are being raised
    as the most powerful alternatives for performing unsupervised
    learning segmentation in images based on CNN s .

-   The HTS method settles an innovative approach to characterize the
    vascular heterogeneity of glioblastomas by means of detecting
    functional habitats within the lesion with differentiated MRI
    profile. We consider that this conceptual framework provides a
    powerful tool to explore the internal behavior of a tumor in an
    objective non-biased data-driven manner. The underlying unsupervised
    ML approach behind the method allows the heterogeneity of the lesion
    to be easily explored from different points of view, ranging from
    varying the number of habitats to find within the lesion, to adding
    additional MRI sequences, such DWI or NMR relaxometry maps, to
    enrich the imaging profile of the tumor.

-   Enhancing tumor vascularity has historically demonstrated strong
    association with OS of patients affected by glioblastoma. However,
    there is a lack of consensus in the literature on the @xmath
    quantities that correlate with this outcome. This is probably due to
    the large variability in the perfusion MRI protocols and
    quantification methods employed, in addition to the manual ROI
    selection for tumor measurements and perfusion relative
    normalization techniques.

    In this sense, we encourage the use of the HTS method, as it has
    proven to be robust to highly variable MRI acquisition protocols and
    manufacturers from different international centers, yielding @xmath
    distributions for the different HTS habitats with no statistically
    significant differences among most of the centers. This is a highly
    desirable property to conduct large cross-sectional population
    studies, where the conclusions of the experiments must be
    extrapolable to the entire population.

-   Diffuse infiltration is one of the most crucial aspects of the
    glioblastoma, as it renders total resection impossible and,
    therefore, progression after surgery is almost inevitable. Thus,
    detecting the areas of potentially infiltrated tumor cells is of
    clinical significance for many targeted interventions such as
    surgery or radiotherapy. The findings of this thesis related to the
    IPE habitat strongly positions the HTS method as a primary tool to
    study the morphology, distribution, profile and characteristics of
    the infiltration region. The positive association of the IPE habitat
    with patient OS confirms the importance of this habitat and enhances
    the HTS as a method to study the heterogeneity of the glioblastoma.

-   Glioblastoma heterogeneity is evidenced at multiple levels, ranging
    from macroscopic co-existence of malignant tissues, to genetic
    alterations that derive in different glioblastoma molecular
    sub-types and to longitudinal evolution of mutant glioblastomas.
    Molecular analysis is nowadays crucial to investigate the response
    of the tumor to different therapies and its mechanisms of
    proliferation. Nevertheless, multi-parametric medical image analysis
    of the tumor has gained a lot of attention in the past years since
    it is demonstrating strong associations with relevant clinical
    outcomes such as OS , tumor grading or genetic mutations.

    In this sense, we recommend that further research on glioblastoma
    using medical imaging takes into account molecular and genetic
    alterations to enhance and complement the imaging information, with
    the goal of designing more accurate models capable of predicting
    clinical outcomes more reliably.

-   Multi-disciplinary cross-sectional research is nowadays necessary to
    find effective therapies for such a lethal disease as the
    glioblastoma. From the technological point of view, it is almost
    impossible for each research group to develop from scratch all the
    necessary state-of-the-art technology needed to analyze the
    different sources of information available for glioblastoma, i.e.
    MRI , genetic profiling, monitoring events, electronic health
    records, etc.

    In this regard, we encourage research groups specialized in
    particular topics must make an effort to provide open access to
    their methods to facilitate the cross-sectional research between
    different disciplines, since it will accelerate and improve the
    research on these complex and lethal diseases.
