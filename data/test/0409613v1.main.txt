###### Contents

-    List of Symbols
-    List of Acronyms
-    Glossary
-    \thechapter Introduction
    -    1 Phases and their stability
    -    2 Statistical mechanics : The formulation of the problem
        -    2.1 Key concepts and definitions
        -    2.2 The link to thermodynamics
        -    2.3 Summary
    -    3 Simulation tools
        -    3.1 The Metropolis algorithm
            -    3.1.1 Constructing the method
            -    3.1.2 Estimating the expectation of macrovariables
        -    3.2 Sampling strategies for estimating @xmath
            -    3.2.1 Reference State Technique
            -    3.2.2 Continuous Path Technique
            -    3.2.3 Phase Mapping Technique
    -    4 Summary
-    \thechapter Review
    -    5 Introduction
    -    6 Formulation of the problem
    -    7 The overlap problem
    -    8 Review of methods
        -    8.1 Introduction
        -    8.2 Canonical Perturbation Methods
        -    8.3 Umbrella Sampling and Multicanonical Methods
        -    8.4 Multistage Methods
        -    8.5 Weighted Histogram Analysis Method
        -    8.6 Simulated Tempering
        -    8.7 Phase Switching Method
        -    8.8 Fast Growth Method
        -    8.9 Path Sampling Fast Growth Methods
        -    8.10 Looking Forward
-    \thechapter Tuning the Representations
    -    9 Introduction
    -    10 The model system
    -    11 The Phase Mapping for crystalline solids
    -    12 Formulation of the Fourier Space Mapping
        -    12.1 Constructing the transformation
        -    12.2 Summary
    -    13 Analytic results
        -    13.1 Fourier Space Mapping
        -    13.2 Real Space Mapping
    -    14 Some numerical results
        -    14.1 Overlap
        -    14.2 Estimating the FEDs
    -    15 Summary
-    \thechapter Estimators
    -    16 Introduction
    -    17 Phase-constrained estimators
        -    17.1 Eliminating systematic errors via restricted
            expectations
        -    17.2 Eliminating systematic errors via @xmath
            -    17.2.1 Minimising the statistical errors : Bennett’s
                fermi function estimator
    -    18 Phase switch estimator
    -    19 Numerical results
    -    20 Conclusion
-    \thechapter Sampling Strategies
    -    21 Introduction
    -    22 The Multicanonical strategy
        -    22.1 The Exponential Perturbation estimator
        -    22.2 The Acceptance Ratio estimator
        -    22.3 The Phase Switch estimator
        -    22.4 Numerical results
    -    23 The Multihamiltonian strategy
        -    23.1 Theory
        -    23.2 Numerical Results
    -    24 The Fast Growth strategy
        -    24.1 Keeping @xmath constant, varying @xmath
        -    24.2 Keeping @xmath constant, varying @xmath
        -    24.3 Keeping @xmath constant
        -    24.4 The choice of estimator
    -    25 Conclusion
-    \thechapter Quantum Free Energy Differences
    -    26 Introduction
    -    27 Path integral formulation of statistical mechanics
        -    27.1 Quantum statistical mechanics
        -    27.2 The classical-quantum isomorphism & the path integral
        -    27.3 Heuristics of the polymeric system
        -    27.4 Temperature regimes in quantum simulations
        -    27.5 Estimating macrovariables
        -    27.6 Higher Order Approximants
        -    27.7 The classical limit
    -    28 Quantum FEDs via the Path Integral Formalism
        -    28.1 Quantum Real Space Mapping
        -    28.2 Quantum Fourier Space Mapping
    -    29 Implementation details and simulation results
        -    29.1 Motivation
        -    29.2 The Model System
        -    29.3 Sampling the polymer
        -    29.4 Testing the algorithm
            -    29.4.1 Harmonic Potential
            -    29.4.2 Lennard Jones System in harmonic regime
        -    29.5 Scaling of thermodynamic parameters with P
        -    29.6 Dependence of @xmath on P and T
        -    29.7 FEDs
    -    30 Discussion
-    \thechapter Conclusion
-    \thechapter Proof of the fluctuation theorem
    -    A Proof of the fluctuation theorem
-    \thechapter Fourier Space Mapping with periodic boundary conditions
-    \thechapter Perturbation theory for the Fourier Space Mapping
    -    B Preliminary Mathematical Properties of Gaussian Integrals
    -    C Temperature scaling properties of @xmath
-    \thechapter Perturbation theory for the Real Space Mapping
    -    D Low temperature limit of @xmath
    -    E Low temperature limit of @xmath
    -    F Temperature scaling of @xmath
-    \thechapter Determining Statistical Errors
    -    G Errors of averages
    -    H Errors in the free energy difference
-    \thechapter The overlap parameter and the Fermi function estimator
-    \thechapter Multihamiltonian method from the Fast Growth method
-    \thechapter Quantum Simulations: The details
-    \thechapter Interplay between kinetic and configurational actions

### List of Symbols

-   @xmath : equality up to a normalisation constant which is not known
    a-priori.

-   @xmath : this denotes the expectation of a macrovariable Q with
    respect to the sampling distribution @xmath (see Eq. \thechapter .30
    ).

-   @xmath : this variable denotes the phase label. The two possible
    values for @xmath are @xmath and @xmath .

-   @xmath where k is the Boltzmann constant

-   @xmath . This is the increment in the field parameter. For all the
    simulations employed in this thesis, the increments were the same,
    so that @xmath (see section 8.4 and section 8.8 ).

-   @xmath : this corresponds to the time (in the FG method) for which
    the system is equilibrated, before work is subsequently performed on
    it.

-   @xmath : variable which keeps track of the phase. It assumes the
    value unity if @xmath corresponds to a configuration generated in
    phase @xmath and zero if not. In order for this function to be able
    to work, one must (in the most general case) keep track of the phase
    label by appeal to the reference configuration (Eq. \thechapter .1 ,
    Eq. \thechapter .6 ) about which the particles are displaced.

-   @xmath : variable which assumes the value unity if @xmath and zero
    otherwise. In the case where @xmath and @xmath do not overlap (i.e.
    when M is an order parameter for the two systems), then this
    function may be used to identify the phases.

-   @xmath : the field parameter used to morph the configurational
    energy of one phase onto that of the other phase. See Eq.
    \thechapter .48 for a simple example of this.

-   @xmath : See Eq. \thechapter .22 .

-   @xmath : the Boltzmann sampling distribution, Eq. \thechapter .26

-   @xmath : the Boltzmann sampling distribution when constrained to
    phase @xmath (see Eq. \thechapter .46 ). Since a phase essentially
    corresponds to a local basin of attraction in the configurational
    energy @xmath , this distribution may be realised by implementing a
    simulation initiated in phase @xmath . The local structure of the
    configuration space will ensure that the simulation will remain in
    that phase.

-   @xmath : the canonical MH sampling distribution given by Eq.
    \thechapter .20 in terms of the collective configuration of the
    composite (multi-replica) system.

-   @xmath : the canonical PS sampling distribution (Eq. \thechapter .74
    ) in terms of the effective configuration @xmath of the system,
    where the phase label is a stochastic variable.

-   @xmath : the superscript m denotes a MUCA sampling distribution.

-   @xmath : the quantum sampling distribution of phase @xmath (see Eq.
    \thechapter .71 ).

-   @xmath : the effective temperature in the quantum system (Eq.
    \thechapter .16 ).

-   A(x) : the metropolis acceptance function, Eq. \thechapter .29

-   @xmath : global translation vector appearing in the PM formulation.
    Figure 3 , Eq. \thechapter .3 .

-   @xmath : The De Boer parameter (Eq. \thechapter .37 ) appears in
    quantum Lennard-Jones systems and fixes the temperatures at which
    the different quantum effects become important (see also appendix
    \thechapter ).

-   @xmath : the configurational energy of the system.

-   @xmath : the configurational energy of phase @xmath in the @xmath
    representation, Eq. \thechapter .8 .

-   @xmath : the difference in energies between the reference
    configuration of phase B and that of phase A (see Eq. \thechapter
    .11 ).

-   @xmath : the i’th component of the j’th eigenvector of the dynamical
    matrix @xmath of phase @xmath .

-   @xmath : difference between the actual energy of the system and the
    energy of the reference configuration (Eq. \thechapter .9 ). In the
    case of the reference configuration being the ground state (as it is
    in the case of crystalline solids, where the reference
    configurations correspond to the lattice sites), @xmath corresponds
    to the excitation energy of the system. We will frequently refer to
    this simply as the ’configurational energy’.

-   @xmath : the harmonic contribution to the excitation energy of phase
    @xmath (see Eq. \thechapter .7 and Eq. \thechapter .14 ).

-   @xmath : the anharmonic contribution to the excitation energy of
    phase @xmath .

-   @xmath : a configurational energy which is a function of the field
    parameter @xmath . One may use this function to construct a chain of
    configurational energies (see Eq. \thechapter .44 ) which links the
    configurational energy associated with phase A ( @xmath ) to that
    associated with phase B ( @xmath ). The most straightforward
    parameterisation, one which we will employ, is the linear one of Eq.
    \thechapter .48 .

-   @xmath : the configurational energy of the quantum system.

-   @xmath : the absolute free energy of phase @xmath , Eq. \thechapter
    .14 .

-   @xmath

-   @xmath : the anharmonic contribution to the FED.

-   @xmath : the histogram recording the number of times a data output
    of the simulation falls in bin @xmath under an experiment performed
    with the sampling distribution @xmath .

-   @xmath : In the MH method this corresponds to the configurational
    energy ’associated’ with the extended system (see Eq. \thechapter
    .15 ).

-   @xmath : In the quantum case this is used to refer to the
    configurational energy of the (classical) polymeric system
    representing phase @xmath (see Eq. \thechapter .26 ).

-   @xmath : denotes the hamiltonian operator of the (quantum) system.

-   @xmath : the dynamical matrix of phase @xmath (see Eq. \thechapter
    .8 ).

-   @xmath : the eigenvalue corresponding to the eigenvector @xmath .

-   @xmath : the set of macrostates consistent with phase @xmath . Note,
    however, that when one performs a simulation constrained to phase
    @xmath (via @xmath ), the simulation will only visit a subset of
    @xmath .

-   @xmath : the macrovariable in Eq. \thechapter .15 .

-   @xmath : bin i in @xmath space. (i=1,2,..,b where b = number of
    bins).

-   @xmath : the i’th output of @xmath during the course of the
    simulation. (i=1,2,…,t), t being the final output of the simulation.

-   @xmath : the MUCA weights.

-   @xmath : the weight associated with sub-ensemble i for the simulated
    tempering method (see Eq. \thechapter .58 ). Note that unlike the
    multicanonical weights @xmath these are not functions defined on
    configuration space.

-   N : number of particles

-   @xmath : the overlap parameter, Eq. \thechapter .18 . This variable
    assumes the value unity if there is perfect overlap between the two
    phase constrained distributions and 0 if there is no overlap.

-   P : the number of replicas in the polymeric system modelling the
    quantum phase (see Eq. \thechapter .25 ).

-   @xmath : the permutation operator.

-   @xmath : absolute canonical probability of observing a configuration
    @xmath (Eq. \thechapter .1 ).

-   @xmath : absolute canonical probability of observing a macrostate M
    (Eq. \thechapter .3 ).

-   @xmath : absolute canonical probability of observing a configuration
    @xmath conditional on being in phase @xmath (Eq. \thechapter .5 ).

-   @xmath : absolute canonical probability of observing a macrostate M
    conditional on being in phase @xmath (Eq. \thechapter .6 ).

-   @xmath : the joint probability of observing a macrostate M and being
    in phase @xmath (Eq. \thechapter .12 ).

-   @xmath : the distribution of @xmath at timeslice i (when the
    configurational energy @xmath has been changed from @xmath to @xmath
    , and after the system has been equilibrated for a time @xmath ).

-   @xmath : the estimator (see Eq. \thechapter .6 ) of @xmath for the
    @xmath FG process. In terms of notation, this is equivalent to
    @xmath .

-   @xmath : this denotes the probability of obtaining a path @xmath in
    the @xmath FG process, as described in section 8.8 .

-   @xmath : 3N dimensional column vector denoting the positions of all
    the particles.

-   @xmath : the ratio of the absolute partition functions, Eq.
    \thechapter .8

-   @xmath : the ratio of the partition functions as given in Eq.
    \thechapter .13

-   @xmath : the ratio of the partition functions of the P replica
    polymeric system representing the quantum phase (see Eq. \thechapter
    .64 ).

-   @xmath : a reference configuration in phase @xmath . For crystalline
    structures, an appropriate reference configuration is the underlying
    lattice structure, corresponding to the (classical) ground state
    configuration.

-   @xmath : the linear transformation used to map the displacements
    @xmath of phase A onto those of phase B, Eq. \thechapter .14 . This
    mapping ensures that the two phases share the same @xmath
    coordinates.

-   @xmath : the total action of the classical polymeric system
    modelling the quantum phase (Eq. \thechapter .28 ).

-   @xmath : the terms in the total action @xmath which contain
    information relating to the kinetic properties of the quantum system
    (Eq. \thechapter .29 ).

-   @xmath : the terms in the total action @xmath which contain
    information relating to the configurational properties of the
    quantum system (Eq. \thechapter .30 ).

-   T : temperature of the heat bath

-   @xmath : effective temperature appearing in the Lennard-Jones system
    (Eq. \thechapter .5 ).

-   @xmath : the linear transformation which relates the displacements
    @xmath to the effective configuration @xmath of phase @xmath , Eq.
    \thechapter .5 .

-   @xmath : denotes the kinetic energy operator (Eq. \thechapter .12 )
    of the (quantum) system.

-   @xmath : the displacement of the particles about the reference
    configuration @xmath .

-   @xmath : the effective configuration. These are generalised
    coordinates which may be used to parameterise the configuration
    space of the system. When the distinction between the configuration
    space as described by the @xmath coordinates and that described by
    the @xmath coordinates is necessary, we will refer to the space
    spanned by the variables @xmath as the absolute configuration space,
    and those spanned by the variables @xmath as the effective
    configuration space.

-   V : volume of the system.

-   @xmath : This measures the contribution of the macrostate @xmath to
    the numerator of the corresponding estimator. For the general DP
    estimator this given by Eq. \thechapter .10 , whereas for the EP
    estimator it is given by Eq. \thechapter .14 , and for the PS
    estimator it is given by Eq. \thechapter .48 .

-   @xmath : This measures the contribution of the macrostate @xmath to
    the denominator of the corresponding estimator. For the general DP
    estimator this given by Eq. \thechapter .11 , whereas for the EP
    estimator it is given by Eq. \thechapter .15 , and for the PS
    estimator it is given by Eq. \thechapter .49 . y

-   @xmath : the (temperature scaled) work incurred in incrementing the
    configurational energy from @xmath to @xmath whilst keeping the
    configuration @xmath constant (see Eq. \thechapter .93 ).

-   @xmath : the net (temperature scaled) work (which we will simply
    refer to as work) appearing in the FG method, obtained on changing
    the configurational energy @xmath from @xmath to @xmath through a
    series of steps in which at each stage one increments @xmath and
    then equilibrates the system with the new configurational energy for
    a time @xmath (see section 8.8 for details).

-   @xmath : the (reversible) work obtained in the limit of
    thermodynamic integration (see section 8.4 , Eq. \thechapter .52 ,
    Eq. \thechapter .29 ). This is also the point at which the two phase
    constrained distributions @xmath and @xmath intersect (see figure 19
    ) so that @xmath .

-   @xmath : the n-th cumulant (see Eq. \thechapter .24 ) of the
    probability distribution @xmath .

-   @xmath : this is used to denote the @xmath FG process, as described
    in section 8.8 .

-   Z : the absolute partition function, Eq. \thechapter .2

-   @xmath : the absolute partition function of phase @xmath , Eq.
    \thechapter .7

-   @xmath : the partition function associated with the configurational
    energy @xmath (see Eq. \thechapter .47 ).

### List of Acronyms

-   AR : acceptance ratio method denotes the estimator of the FED in
    which one performs two independent simulations, one in each phase,
    and estimates the expectations of the acceptance probabilities (see
    Eq. \thechapter .31 , or more generally Eq. \thechapter .104 ).

-   DP : dual phase. This refers to the most general canonical
    perturbation formula (see Eq. \thechapter .34 ).

-   EP : this refers to the exponential perturbation estimator (Eq.
    \thechapter .28 ) of @xmath in which the FED is estimated from data
    extracted from a simulation constrained to a single phase.

-   FED : free energy difference, Eq. \thechapter .15 , Eq. \thechapter
    .16 , Eq. \thechapter .10 . Since the problem of estimating @xmath
    is equivalent to that of estimating the FED of the two phases, we
    will frequently interchange the use of the terms @xmath and FED.

-   FF : this refers to the fermi function estimator corresponding to
    Eq. \thechapter .28 . The optimal C is obtained by recursively
    solving Eq. \thechapter .31 and Eq. \thechapter .32 .

-   FG : the fast growth method process (see section 8.8 , 24 ).

-   FSM : the fourier space mapping is a particular realisation of the
    general phase mapping (Eq. \thechapter .14 ) in which the fourier
    coordinates of one phase are mapped onto those of the other phase
    (see Eq. \thechapter .16 ).

-   HOA : higher order approximation.

-   LJ: Lennard Jones (refers to the pairwise interactomic potential
    given in Eq. \thechapter .2 ).

-   MH : the multi-hamiltonian strategy is an extended sampling strategy
    which involves the construction of several independent but
    overlapping distributions. These overlapping distribution then allow
    the construction of a path linking the typical macrostates of the
    two phases (see section 23 ).

-   MS : the multistage strategy is similar in principle to the MH
    methods (see section 8.4 ).

-   MH-PS : the PS method, as implemented within the framework of the MH
    extended sampling strategy (see section 23 ).

-   MUCA : multicanonical (see sections 8.3 , 8.7 , 22 ).

-   NVT : this refers to a system whose volume V and temperature T are
    maintained at a constant value and in which the number of particles
    within the system remains unchanged during the course of the
    simulation. Such a system is referred to as a canonical system and
    has a distribution given by the Boltzmann distribution (Eq.
    \thechapter .1 ).

-   PA : primitive approximation

-   PM : ’phase mapping’ refers to the scheme whereby the configurations
    of one phase are mapped onto those of the other phase. The
    employment of a phase mapping allows one to map the problem of
    estimating the FED between the two phases onto that of estimating
    the FED between two systems with different configurational
    energies (see Eq. \thechapter .13 ).

-   PS : the ’phase switch’ should not be confused with the PM. This
    corresponding to a simulation in which attempts to switch phases are
    actually made, and whose corresponding estimator for @xmath
    essentially amounts to measuring the (unbiased) ratio of the times
    spent in the two phases (see Eq. \thechapter .78 ). See chapter
    \thechapter for generalisations of this method.

-   Q-FSM : quantum fourier space mapping.

-   Q-RSM : quantum real space mapping.

-   REP : restricted exponential perturbation formula (see Eq.
    \thechapter .3 ).

-   RDP : restricted dual phase perturbation formula (Eq. \thechapter .3
    ).

-   RSM : the real space mapping is a particular realisation of the PM
    (see Eq. \thechapter .4 ).

-   ST : simulated tempering (see section 8.6 ).

-   WHAM : the weighted histogram analysis method (see section 8.5 ).

### Glossary

-   order parameter : macrovariable which assumes a different ranges of
    values in the different phases. These ranges are, by definition,
    non-overlapping

-   A : this is the phase label denoting the fcc structure.

-   B : this is the phase label denoting the hcp structure.

-   configuration space : this term is used both to refer to the space
    spanned by @xmath and that spanned by @xmath . When the distinction
    between the configuration space as described by the @xmath
    coordinates and that described by the @xmath coordinates is
    necessary, we will refer to the space spanned by the variables
    @xmath as the absolute configuration space, and that spanned by the
    @xmath coordinates as effective configuration space.

-   canonical : in the context of sampling, this refers to sampling from
    a Boltzmann distribution (see Eq. \thechapter .26 ).

-   conjugate phase : this corresponds to the phase which the simulation
    is currently not in and the phase onto which the configurations are
    being mapped. In the case of the phase switching method this changes
    during the course of the simulation. In the case of a phase
    constrained simulation this remains the same for the entire duration
    of the method.

-   dual phase : this refers to estimators of the form of Eq.
    \thechapter .105 , which explicitly involve the accumulation of data
    from two simulations, one constrained to each phase.

-   extended sampling strategy : this refers to the procedure whereby
    the sampling distribution is made to encompasses a wider (or
    extended) region of configuration space than is typically associated
    with the canonical distribution, which the expectations are
    performed with respect to. The desired expectations are recovered
    from Eq. \thechapter .32 .

-   macrostate : this corresponds to the collection of configurations
    which yield a particular value for a given macrovariable. (See also
    Eq. \thechapter .3 for the probability of observing a given
    macrostate).

-   parent phase : this refers to the phase which the simulation is
    currently in and from which the configurations are being mapped onto
    the other phase. In the case of the phase switching method this
    changes during the course of the simulation. In the case of a phase
    constrained simulation this remains the same for the entire duration
    of the method.

-   partial overlap : see note [ 2 ]

-   path : this refers to a sequence of (closely spaced) macrostates
    which are actually sampled during the course of a simulation and
    which connect the regions of (effective) configuration
    space associated with one phase to those associated with the other
    phase.

-   representation : this refers to the particular way in which one
    expresses the degrees of freedom ( @xmath ) of the phase (see Eq.
    \thechapter .8 ). Since the PM matches the @xmath coordinates of the
    two phases, the representation directly affects the overlap obtained
    under the operation of the PM.

-   system : We will frequently interchange this word with the word
    ”phase”

-   thermodynamic limit : limit of the system size tending to infinity.
    That is @xmath .

## Chapter \thechapter Introduction

### 1 Phases and their stability

The material world around us comprises of matter and its interactions.
Depending on the strengths and ranges of these interactions matter, on
the macroscopic scale, displays a variety of collective properties.
These collective properties result in the formation of different
”phases” of matter such as gas, liquid, and solid. For these phases
there are two levels of description, which are the microscopic and
macroscopic approaches. The microscopic picture describes matter in
terms of its constituent particles and their interactions, whereas the
macroscopic description coarse grains the configurational and kinetic
information of the constituent particles into a small set of so called
macrovariables.

In the case of equilibrium [ 3 ] - [ 6 ] these macrovariables fluctuate
in time about a mean which remains constant in time, and the
corresponding theory that describes the interrelation of the means of
these variables is thermodynamics. The fundamental parameters which
enter into the theory are certain macrovariables (such as the
configurational energy E), certain parameters called intensive variables
(such as the temperature and the chemical potential) which do not
explicitly depend on the system and instead describe the coupling of the
system with the environment, and the concept of entropy, which is a
measure of the amount of disorder present in the system.

Thermodynamics is useful in that it explains the interrelation amongst
some of the most important macrovariables. However, the theory does not
give one the power of being able to predict how these macrovariables (or
more precisely the means of these macrovariables) vary as one changes
the intensive parameters. In order to do this one must resort to the
microscopic description of the phenomena. A full blown microscopic
approach would be (in the classical case) to solve Newton’s equations
for the particles and then average the relevant macrovariables over
sufficiently long times, or (in the quantum case), to solve the
multi-particle Schrodinger equation and evaluate the time averages of
the expectation of the relevant macrovariables. Such an approach is,
however, analytically intractable and one must instead resort to a more
approximate microscopic approach.

The relevant microscopic theory is that of statistical mechanics, a
theory in which all temporal effects have been averaged out. The core
ingredients of the theory are the set of spatial configurations which
the system may assume, the set of intensive variables which describe the
system-environment coupling, and the configurational energy of the
system. Using these, one may then construct probabilities for finding
the system in a given configuration at any given instant of time. Since
the theory is independent of kinetics, a considerably reduced amount of
effort is required in describing phenomena. For a more in-depth
development of the points mentioned above, the reader is referred to
some standard texts on statistical mechanics [ 4 ] - [ 7 ] .

In order to describe phenomena directly via statistical mechanics we
must first explain more precisely what exactly we mean by a phase.
Within the framework of statistical mechanics a phase corresponds to the
group of microscopic configurations in which the constituent members of
any given group exhibit some common property unique to that phase. For
example, in the case of a crystalline solid phase, the associated group
would correspond to all configurations in which the particles are
displaced by ”small” amounts about some lattice structure. This lattice
structure, which is the common characteristic of all the configurations,
is what identifies the group and different lattice structures yield
different groups or different phases. By grouping the configurations in
this way one may also calculate the probability associated with a phase
simply by summing the probabilities of the constituent configurations.
The result is proportional to a quantity called the partition function
of the phase, which plays a central role in statistical mechanics.

The properties of a phase can, within the framework of thermodynamics,
be predicted through a central quantity called the free energy. On the
other hand all such predictions will, within the scheme of statistical
mechanics, stem from the probability distribution of the configurations
associated with that given phase. Not surprisingly it turns out that the
free energy of a phase is intimately related to the partition function
of that phase, with the intensive variables being the common parameters
in the two theories. By finding the partition function of a phase, one
is able to predict its behaviour quantitatively in the macroscopic
limit.

Of all the predictions that statistical mechanics can make, we shall
focus on one. Namely, given a set of candidate phases, which phase is
the one that is actually going to be found in nature for a given set of
constraints (of the environment on the system)? Within the framework of
statistical mechanics this translates to the task of finding out which
is the most probable phase, or correspondingly finding out which phase
has the largest partition function. In the context of determining phase
boundaries, where one is trying to determine the more probable out of
two candidate phases, one may reduce to number of calculations by merely
focusing on the ratio of the partition functions (which entails a single
calculation) as opposed to focusing ones efforts on the calculation of
the individual partition functions (an approach which will require two
separate calculations).

The analytic evaluation of the ratio of partition functions (or
equivalently the free energy difference) is, however, no simple task.
Despite the fact that temporal effects have been averaged out in
statistical mechanics so as to considerably simplify the theory, it
turns out that, for most complex systems of interest, calculation of the
desired properties via analytic techniques remains intractable. One
instead has to resort to computation. However even within the framework
of this approach, the task of determining the ratio of the partition
functions still remains a difficult one. In the rest of this thesis we
will primarily be concerned with the investigation and development of
computational methods of determining the FED (or equivalently the ratio
of the partition functions) of two different phases.

In the next section we will introduce the necessary machinery which will
enable us to define exactly what we mean by the partition function of a
phase. We will express the partition function of a phase as a
multidimensional integral and illustrate how the ratio of the partition
functions can be thought of as the ratio of two multidimensional
integrals in which the regions which contribute the most come from two
non-overlapping regions of the space over which the integrals are
defined. We will then briefly discuss the Metropolis algorithm, which is
a widely used computational technique to model equilibrium phase
behaviour. This will be followed by a discussion of how the method may,
in principle, be used to estimate the FED, and how in practice it fails.

### 2 Statistical mechanics : The formulation of the problem

#### 2.1 Key concepts and definitions

In this section we develop the necessary statistical mechanical theory [
8 ] . Suppose that we have a system with a fixed number of particles N,
at a fixed volume V, and at a fixed temperature T. Such a system is
referred to as a canonical or NVT system. Let @xmath denote the 3N
dimensional column vector containing the positions of all the particles.
Within the framework of equilibrium statistical mechanics it follows
that, when the dynamics of the system have been averaged out over
sufficiently long periods of time, the canonical probability of finding
the system assuming a configuration @xmath is given by the Boltzmann
distribution:

  -- -------- -- -------------------
     @xmath      ( \thechapter .1)
  -- -------- -- -------------------

where @xmath is the configurational energy of the system and Z is the
absolute partition function:

  -- -------- -- -------------------
     @xmath      ( \thechapter .2)
  -- -------- -- -------------------

and @xmath , where k is the Boltzmann constant. If we have a variable
@xmath which is a function on configuration space, which we call a
macrovariable, it follows that the canonical probability of @xmath
assuming a value @xmath (called a macrostate) is then given by:

  -- -------- -- -------------------
     @xmath      ( \thechapter .3)
  -- -------- -- -------------------

A macrostate is essentially a collection of microscopic configurations
@xmath for which a macrovariable assumes a particular value. The
canonical probability of a macrostate (Eq. \thechapter .3 ) accounts for
the fact that there may be a multiplicity of microscopic configurations
associated with a given macrostate.

Equipped with the armoury of the probabilities of microstates (Eq.
\thechapter .1 ) and of macrostates (Eq. \thechapter .3 ), we may now
proceed to define a phase. We first note that in thermodynamics one
generally identifies a phase (which we label @xmath ) through a
macrovariable @xmath , also called an order parameter, which spans a set
of values @xmath . For an order parameter the set of values (say @xmath
and @xmath ) associated with the two different phases ( @xmath and
@xmath ) do not overlap, allowing it to be used as an identifying
variable for the phases in question. Carrying this idea over into
statistical mechanics, one may construct a criterion for deciding
whether a configuration @xmath belongs to a phase or not by virtue of
the following function:

  -- -------- -- -------------------
     @xmath      ( \thechapter .4)
  -- -------- -- -------------------

If @xmath is 1, then the configuration @xmath belongs to phase @xmath ,
otherwise it does not. This function essentially uses the property of
@xmath being an order parameter (that is assuming a unique set of values
in the different phases) in order to determine whether a configuration
belongs to a phase or not.

Using Eq. \thechapter .4 one may immediately write down the partition
function and the (canonical) conditional probabilities of finding a
configuration @xmath and that of finding a macrostate @xmath , in phase
@xmath :

  -- -------- -- -------------------
     @xmath      ( \thechapter .5)
  -- -------- -- -------------------

and

  -- -------- -------- -------- -------- -------------------
     @xmath   @xmath   @xmath            ( \thechapter .6)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------------------

where @xmath denotes the partition function, or probabilistic weight,
associated with phase @xmath :

  -- -------- -- -------------------
     @xmath      ( \thechapter .7)
  -- -------- -- -------------------

Having now defined the concept of a phase and its corresponding weight
(the partition function, Eq. \thechapter .7 ) we may now write the ratio
of the partition functions of the two different phases as the ratio of
two multidimensional integrals:

  -- -------- -------- -------- -------- -------------------
     @xmath   @xmath   @xmath            ( \thechapter .8)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------------------

where the angular brackets @xmath denote an expectation with respect to
the distribution @xmath (defined more explicitly in Eq. \thechapter .30
below). Alternatively, by using the order parameter @xmath one may write
the ratio of the partition functions as the ratio of two one dimensional
integrals:

  -- -------- -------- -------- -------- -------------------
     @xmath   @xmath   @xmath            ( \thechapter .9)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------------------

where

  -- -------- -- --------------------
     @xmath      ( \thechapter .10)
  -- -------- -- --------------------

The strategy of re-writing @xmath as has been done in Eq. \thechapter .9
is a highly advantageous one since it reduces the multidimensional
problem in Eq. \thechapter .8 to the one dimensional problem of Eq.
\thechapter .9 . It crucially depends on ones ability to find a suitable
order parameter M, which may not be possible for smaller systems. In
such situations one may instead have to be content with a macrovariable
M which spans an overlapping range of values ( @xmath and @xmath ) in
the two phases. In this case one must distinguish the two phases on a
microscopic level. For example, in the case of crystalline phases, one
may do this by keeping track of the lattice vectors about which the
particles of the system are displaced. In this more general case Eq.
\thechapter .5 , Eq. \thechapter .7 , and Eq. \thechapter .8 will
continue to hold provided @xmath is more broadly defined as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .11)
  -- -------- -- --------------------

where @xmath denotes the set of configurations which one would typically
associate with phase @xmath . In the case where @xmath and @xmath
partially overlap, the expression in Eq. \thechapter .9 for @xmath no
longer holds, and must instead be expressed in terms of the joint
probability distribution of @xmath and @xmath :

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .12)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

where @xmath is now given by Eq. \thechapter .11 . It then follows that
the ratio of the partition functions may now be expressed more generally
as:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .13)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

It is clear that in Eq. \thechapter .13 the macrovariable @xmath is in
fact a redundant variable. Its utility, however, lies in the estimation
of Eq. \thechapter .13 via simulations, where the macrovariable M is
used to guide the simulation to certain regions of configuration space .
We will have more to say about this in section 8 (in particular sections
8.3 and 8.7 ).

#### 2.2 The link to thermodynamics

In order to establish the connection between statistical mechanics and
thermodynamics we first note that, in statistical mechanics, questions
as to the relative stability of phases may be entirely addressed through
the quantity @xmath , (Eq. \thechapter .8 , Eq. \thechapter .9 , and Eq.
\thechapter .13 , ). If this quantity is greater than unity, phase B is
the more stable. Otherwise phase A is the more stable of the two.
Thermodynamics, on the other hand, extracts the corresponding
information through the free energy ( @xmath ) of the phase. The phase
which has the lower free energy is the more stable of the two. The
identity which bridges the two theories is the following:

  -- -------- -- --------------------
     @xmath      ( \thechapter .14)
  -- -------- -- --------------------

It follows from Eq. \thechapter .14 that the ratio of the partition
functions is intimately related to the FED of the two phases:

  -- -------- -- --------------------
     @xmath      ( \thechapter .15)
  -- -------- -- --------------------

where @xmath is the free energy difference:

  -- -------- -- --------------------
     @xmath      ( \thechapter .16)
  -- -------- -- --------------------

From thermodynamics we know that the equilibrium phase (that is the one
which is found in nature, subject to the necessary constraints) is the
one with the minimum free energy. This is consistent with the
statistical mechanical formulation since from Eq. \thechapter .14 this
merely corresponds to the phase with the maximum probabilistic weight
@xmath . Furthermore since the free energy, and hence the free energy
difference, is an extensive quantity (that is @xmath ), it follows from
Eq. \thechapter .15 that in the limit of @xmath (called the
thermodynamic limit) the difference in partition functions of the two
phases will magnify so as to make one of the phases overwhelmingly more
probable than the other. This is in line with the thermodynamic
observation of there being only one phase that is consistent with the
constraints imposed on the system [ 9 ] .

#### 2.3 Summary

Summarising, if one has two candidate phases, and one wants to find out
which will appear in nature, one can construct a finite system and
calculate the ratio of the partition functions @xmath via Eq.
\thechapter .8 , Eq. \thechapter .13 , Eq. \thechapter .9 . This allows
one to determine the more stable (or more probable) of the two phases.
It then follows that in the thermodynamic limit this phase then becomes
overwhelmingly more probable that the other and as a result will be the
one found in nature [ 10 ] .

For most interesting systems, even for a finite system the underlying
complexity rules out any analytic approach. One must instead resort to
computational techniques. The Monte Carlo method is a computational
approach which is particularly suited for the simulation of equilibrium
systems in which one is not concerned with the dynamics but merely
static, time averaged quantities. In the next section we will briefly
introduce a particular type of Monte Carlo method, called the Metropolis
algorithm, and then discuss in section 3.2 how this method may, in
principle, be used to tackle the problem of estimating the ratio of the
partition functions @xmath .

### 3 Simulation tools

#### 3.1 The Metropolis algorithm

##### 3.1.1 Constructing the method

There are two main simulations techniques which are employed to sample
configuration space distributed according to Eq. \thechapter .1 , [ 11 ]
, [ 12 ] . The first is molecular dynamics, a method which we do not
employ in this thesis. For further information refer to [ 13 ] . The
second, and more natural (in the context of equilibrium statistical
mechanics) is the Metropolis algorithm [ 14 ] , [ 15 ] . Unlike
molecular dynamics, in which the dynamics is purely deterministic, the
Metropolis algorithm is a purely probabilistic method. We will now
describe the method in some detail.

The Metropolis algorithm works by generating a sequence of
configurations @xmath in which the probability of generating a
configuration @xmath is only dependent on the current configuration
@xmath . This algorithm may be constructed in such a way so as to ensure
that in the infinite time limit ( @xmath ), the relative probabilities
of configurations appearing in the chain satisfy any arbitrary sampling
distribution @xmath . To see how this is done consider the rate equation
for @xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .17)
  -- -------- -- --------------------

where @xmath denotes the transition (or sampling) probability of the
algorithm from a configuration @xmath to a configuration @xmath . If the
transition probability of the algorithm is to yield a process with a
stationary distribution (that is a distribution @xmath which does not
change in time) one must have:

  -- -------- -- --------------------
     @xmath      ( \thechapter .18)
  -- -------- -- --------------------

Clearly one way, but by no means the only way, in which Eq. \thechapter
.18 may be satisfied is by assuming that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .19)
  -- -------- -- --------------------

The constraint on @xmath in Eq. \thechapter .19 is called the condition
of detailed balance and is used by the Metropolis algorithm in order to
produce a chain of configurations in which different configurations
appear with relative frequencies which are consistent with @xmath [ 16 ]
.

In the Metropolis algorithm the procedure of sampling is divided into
two stages. The first stage involves generating a new configuration
@xmath given a current configuration @xmath . The second stage is that
of accepting or rejecting the proposed moves. Let @xmath denote the
probability of generating @xmath given @xmath , and let @xmath denote
the corresponding acceptance probability. It follows that the sampling
probability may be written as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .20)
  -- -------- -- --------------------

Using Eq. \thechapter .19 and \thechapter .20 it is easy to show that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .21)
  -- -------- -- --------------------

Using this one may easily verify that a suitable @xmath is of the form [
14 ] , [ 15 ] , [ 17 ] :

  -- -------- -- --------------------
     @xmath      ( \thechapter .22)
  -- -------- -- --------------------

Eq. \thechapter .22 is called the Metropolis acceptance criterion. An
alternative, which also satisfies Eq. \thechapter .21 , is given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .23)
  -- -------- -- --------------------

In the simulation of statistical mechanical systems a particular case of
Eq. \thechapter .22 is generally adopted. Consider a simulation
performed via the Metropolis algorithm in which the generation of a
trial configuration involves perturbing a randomly chosen particle to a
random position chosen to lie within a specified volume [ 18 ] about the
particle’s initial point [ 13 ] . For such an algorithm, the probability
of generating a new configuration @xmath , given a current configuration
@xmath , is symmetrical in the following way:

  -- -------- -- --------------------
     @xmath      ( \thechapter .24)
  -- -------- -- --------------------

As a result Eq. \thechapter .22 simplifies to:

  -- -------- -- --------------------
     @xmath      ( \thechapter .25)
  -- -------- -- --------------------

For the particular case where the sampling distribution is the Boltzmann
distribution (Eq. \thechapter .1 ):

  -- -------- -- --------------------
     @xmath      ( \thechapter .26)
  -- -------- -- --------------------

where @xmath denotes an equality up to a normalisation constant which is
not known [ 17 ] , Eq. \thechapter .25 may be written as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .27)
  -- -------- -- --------------------

where

  -- -------- -- --------------------
     @xmath      ( \thechapter .28)
  -- -------- -- --------------------

and

  -- -------- -- --------------------
     @xmath      ( \thechapter .29)
  -- -------- -- --------------------

is the Metropolis acceptance function. The procedure of employing a
@xmath with the property given in Eq. \thechapter .24 and the acceptance
probability @xmath given in Eq. \thechapter .27 forms the cornerstone of
the original Metropolis method, and will be the one that is used in the
canonical simulations (Eq. \thechapter .1 ) performed in this thesis.

Summarising, if one performs a simulation in which one stochastically
generates configurations and accepts via the acceptance probabilities of
Eq. \thechapter .22 , one generates a chain of configurations in which
the frequencies of the appearance of different configurations are
proportional to their probabilities @xmath . We will now show how this
property may be used to estimate the expectation of macrovariables.

##### 3.1.2 Estimating the expectation of macrovariables

Suppose now that one wants to evaluate the expectation of some function
Q of a macrovariable M with respect to the sampling distribution @xmath
:

  -- -------- -- --------------------
     @xmath      ( \thechapter .30)
  -- -------- -- --------------------

By using the Metropolis algorithm (that is the stochastic algorithm in
which proposed moves are accepted via eq. \thechapter .22 ), one may
estimate the expectation @xmath via the following scheme:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .31)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

where @xmath denotes the i-th output of the macrostate @xmath by the
simulation and where @xmath denotes the histogram count for bin @xmath
under a sampling experiment performed via the sampling distribution
@xmath . It is important to keep in mind that it is the lack of
knowledge of the normalising constant of @xmath which necessitates the
inclusion of the integral @xmath in the denominator of Eq. \thechapter
.30 [ 19 ] . This will have important consequences for the task of
estimating the FEDs (see section 8.3 ).

A finite sample estimate given in Eq. \thechapter .31 will generally
have a statistical error associated with it. This arises from the fact
that one is trying to reconstruct the relevant probability distribution
from a finite number of samples, or equivalently from a finite time
simulation [ 20 ] . In the case of FED calculations, one has, in
addition to this, systematic errors. These arise (in the context of FED
calculations) from not sampling the regions of configuration space which
contribute the most significantly to the relevant estimator. Once again
this arises from the fact that one is running the simulation for a
finite amount of time. The differences in the two types of errors lie in
the time scales needed to reduce the error to an acceptable level, and
therefore in some circumstances the distinction can become blurred. One
may generally think of statistical errors as those which may be
decreased to a desired level merely by running the simulation for
sufficiently long times, where the lengths of time in question are
generally those for which one would be prepared to run a simulation. In
the case of systematic errors, the times needed to reduce them to an
acceptable level are generally considerably greater (by at least several
orders) than one would be prepared to wait. The methods which are
successful in estimating the FEDs are those which overcome such
systematic errors. We will have more to say about the way in which they
do this in chapter \thechapter and chapter \thechapter .

Eq. \thechapter .31 tells us how we may estimate the expectation of a
macrovariable (with respect to a sampling distribution @xmath ) based on
an experiment performed with the same sampling distribution . More
generally one may need to estimate the expectation of Q with respect to
a distribution (say @xmath ) which is different from the sampling
distribution @xmath used to obtain the data. To do this we simply
re-write the expectation @xmath as an expectation with respect to the
sampling distribution @xmath :

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .32)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

where, as is the case in Eq. \thechapter .30 , the need to evaluate the
denominator of Eq. \thechapter .32 essentially arises from the lack of
knowledge of the relative normalisation constants of @xmath and @xmath .
Provided @xmath is well defined Eq. \thechapter .32 may be estimated by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .33)
  -- -------- -- --------------------

where @xmath denotes the sequence of configurations generated by the
simulation and where @xmath . In the special case where @xmath is a
function @xmath of @xmath , that is:

  -- -------- -- --------------------
     @xmath      ( \thechapter .34)
  -- -------- -- --------------------

we may write Eq. \thechapter .32 as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .35)
  -- -------- -- --------------------

and we may re-write the corresponding estimator (Eq. \thechapter .33 )
as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .36)
  -- -------- -- --------------------

Eq. \thechapter .33 and Eq. \thechapter .36 play a central role in the
task of estimating FEDs via computational techniques. We will see in
sections 8 , and more clearly in chapter \thechapter , that at the heart
of all the methods designed to tackle the problem of estimating FEDs is
the construction of a sampling distribution @xmath which differs from
the distribution @xmath with respect to which the expectations are
performed. We will refer to this as the extended sampling (ES) strategy
[ 21 ] . We will have more to say about these extended sampling
strategies in section 8.3 and chapter \thechapter . Before doing this we
will describe three general techniques which may be used to estimate Eq.
\thechapter .8 via simulation and will then proceed to focus on one of
these, namely the phase mapping method. In the next chapter we will then
proceed to review the various methods that are available for estimating
FEDs within the framework of this method.

#### 3.2 Sampling strategies for estimating @xmath

Broadly speaking there are (for NVT systems) three generic strategies
which one may pursue in order to estimate @xmath [ 22 ] . They are the
reference state technique, the continuous phase technique, and the phase
mapping (PM) technique. At the heart of all the techniques is the
concept of a path, which we define to be a series of overlapping
macrostates (obtained during the course of a simulation) connecting the
regions of configuration space associated with one phase to those
associated with the other.

In the reference state technique, a path is constructed which connects
each phase to a reference system for which the partition function is
known exactly. In this way one is able to estimate the absolute
partition function of each phase. In the continuous path technique, a
continuous path is constructed from one phase to the other, thereby
allowing one to estimate the ratio of the partition functions. In the
phase mapping (PM) technique, a path linking the two phases is
constructed in which one ”leaps” directly from one phase to the other,
omitting all the regions of configuration space lying in between the two
phases. We will now review these methods in greater detail.

##### 3.2.1 Reference State Technique

In the reference state technique (also called thermodynamic integration)
[ 23 ] , [ 24 ] , [ 25 ] , the basic idea is to construct a path which
connects the desired phase to a reference system for which the partition
function is known exactly. This allows one to compute the FED between
the given phase and the reference system. By performing two such
simulations, connecting each phase to an appropriate reference system [
26 ] , one may infer the FEDs between the phases and their respective
reference systems. Since the partition functions of these reference
systems are known a-priori, one may use these results to determine the
absolute values of the partition functions of each phase [ 27 ] . One
may then proceed to determine which is the more stable phase of the two.
A schematic is shown in figure 1 .

Technically, the way in which one links the desired system to the
reference phase is as follows. One constructs a configurational energy
@xmath in which the field parameter @xmath assumes any value between 0
and 1. Furthermore suppose that at the extremities of @xmath and @xmath
this configurational energy assumes the form of the configurational
energies of the reference and desired phases respectively (i.e. @xmath
and @xmath , where @xmath and @xmath are the configurational energies of
the reference and desired phases respectively). Then the fundamental
relation upon which the method is based is:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .37)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

where @xmath is given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .38)
  -- -------- -- --------------------

Eq. \thechapter .37 must be estimated numerically by first dividing up
the interval @xmath into a discrete set

@xmath , and then proceeding to estimate Eq. \thechapter .37 via:

  -- -------- -- --------------------
     @xmath      ( \thechapter .39)
  -- -------- -- --------------------

where @xmath denotes an estimate of the expectation @xmath .

Generally there will be two sources of error in the estimator of Eq.
\thechapter .39 . The first will be statistical errors in estimating
@xmath and the second will be systematic errors arising from the
discretisation of the interval @xmath [ 28 ] . Whereas the statistical
errors are made smaller simply by increasing the duration of the
simulations, the systematic errors can be made smaller by reducing the
size of the increments @xmath . By ensuring that the increments @xmath
are sufficiently small, one may ensure that the systematic errors are
smaller than the corresponding statistical errors, though a-priori it is
not clear how small these increments have to be in order to ensure that
this is indeed the case.

The presence of systematic errors is one point which makes the reference
state technique, as formulated here, a slightly unattractive one.
Furthermore the reference state technique requires the estimate of two
separate quantities, that is the partition functions of the individual
phases, when one is in fact only interested in the single quantity
corresponding to the ratio of these quantities. Clearly a single
calculation which directly estimates this ratio would be preferable [ 29
] .

##### 3.2.2 Continuous Path Technique

In the continuous path technique [ 30 ] , one performs a simulation
which travels from one phase to the other via a continuous path. In
order to estimate the ratio of the partition functions (or equivalently
the FEDs) of the two phases via Eq. \thechapter .8 , Eq. \thechapter .9
, one performs a simulation in which one keeps track of the order
parameter @xmath . Using Eq. \thechapter .31 one may then estimate
@xmath from Eq. \thechapter .9 via the identity:

  -- -------- -- --------------------
     @xmath      ( \thechapter .40)
  -- -------- -- --------------------

where @xmath is given by Eq. \thechapter .10 .

In the case where the set of values @xmath and @xmath overlap (that is
when @xmath is no longer strictly an order parameter), one must instead
use Eq. \thechapter .13 . In this case the macrovariable @xmath becomes
redundant, and one instead estimates @xmath via the identity:

  -- -------- -- --------------------
     @xmath      ( \thechapter .41)
  -- -------- -- --------------------

where @xmath is given by Eq. \thechapter .11 . A schematic is shown in
figure 2

There are three problems with the method as it stands. The first is the
fact that the estimators in Eq. \thechapter .40 and Eq. \thechapter .41
will generally fail. The reason for this lies in the fact that for
straightforward Boltzmann sampling, in which one samples according Eq.
\thechapter .26 , one generates configurations whose frequencies of
appearance are in accordance with their canonical probabilities. Since
the two phases are separated by a region of configuration space
characterised by macrostates @xmath of extremely low probability (see
figure 2 ) the probability of the simulation generating a sequence of
configurations which traverses this region will be vanishingly small. As
a result the simulation will remain stuck in one of the phases, making
it impossible to estimate @xmath from equation \thechapter .40 or
\thechapter .41 , since either the numerator or the denominator of these
estimators will be zero. This problem, which is called the overlap
problem and is the origin of the systematic errors we were alluding to
in section 3.1 , may be overcome with the adoption of appropriate
extended sampling strategies [ 30 ] . We will have more to say about
this in section 8 [ 31 ] .

The second problem, which is a problem afflicting the case of
(structurally) ordered phases, arises from the fact that in the process
of going from one phase to the other (via @xmath in figure 2 ) the
simulation will in general have to traverse through regions of
configuration space which are characterised by mixed-phase or disordered
configurations. That is the transition from one phase to the other will
involve the disassembling of a phase, followed by the organisational
restructuring resulting in the assembling of the other phase. This will
result in the formation of a defect-rich final structure in the case
where one of the phases is a crystalline solid. As a result one will not
obtain a correct estimate for @xmath [ 32 ] . Note that even though the
first problem, that is the problem of interphase traverse, may be
overcome by the use of extended sampling (see for example [ 30 ] ), this
second problem will continue to persist in the case of ordered phases [
33 ] .

The third problem, though not as serious as the previous two, arises
from the fact that the regions of configurations space @xmath which one
needs to traverse in going from one phase to the other do not actually
contribute to @xmath . As a result, the simulation will waste large
amounts of times in regions of configuration space which do not actually
contribute to the final estimate of @xmath .

##### 3.2.3 Phase Mapping Technique

A method which overcomes the last two problems of the continuous path
technique, namely that of sampling the regions of configuration space
@xmath characterised by mixed phase-configurations and that of sampling
regions of configuration space which do not contribute directly to
@xmath , is the phase mapping technique (PM) [ 1 ] , [ 34 ] - [ 37 ]
(see also [ 38 ] , [ 39 ] ). This method avoids the intermediate regions
(characterised by the set of macrostates @xmath in figure 2 ) altogether
by using a global configuration space shift [ 38 ] to map configurations
of one phase onto those of the other phase. Suppose that one is in phase
A, with a configuration @xmath . The basic idea is to find a constant
displacement @xmath such that @xmath + @xmath denotes a characteristic
configuration of phase B (see figure 3 ). The result is a mapping of the
configurations of one phase onto those of the other via the operation:

  -- -------- -- --------------------
     @xmath      ( \thechapter .42)
  -- -------- -- --------------------

In order to make use of this mapping one re-writes Eq. \thechapter .8
as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .43)
  -- -------- -- --------------------

By writing this as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .44)
  -- -------- -- --------------------

Eq. \thechapter .43 may be written as:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .45)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

where @xmath denotes that the expectation is performed with respect to
configurations constrained to phase A [ 40 ] :

  -- -------- -- --------------------
     @xmath      ( \thechapter .46)
  -- -------- -- --------------------

From Eq. \thechapter .31 it follows that @xmath may then be estimated
via:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .47)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

where the data is obtained from a simulation constrained to phase A.
Since Eq. \thechapter .47 is essentially an estimator involving a
sampling distribution constrained to a given phase, we will refer to Eq.
\thechapter .47 as a phase-constrained estimator. For future use, we
will refer to the phase which the simulation is actually in as the
parent phase (phase A, in the case of Eq. \thechapter .47 ), and the
phase onto which the configurations are being mapped as the conjugate
phase (which in this case is phase B). Eq. \thechapter .47 is just one
example of a phase-constrained estimator. Looking forward, we note that
these estimators may be most generally written of the form of Eq.
\thechapter .34 .

The expression in Eq. \thechapter .45 is essentially a way of estimating
FEDs by performing a simulation in a single phase. By employing a global
configuration space shift @xmath , one is able to bypass the
intermediate regions of configuration space region characterised by the
set of macrostates @xmath in figure 2 . The core idea behind the method
is to find a global configuration displacement @xmath which will allow
one to sample the configurations important to @xmath as well as those
important to @xmath in a single simulation performed in phase A.

The problem with the method as it stands is that it is not sufficient
that one finds a @xmath such that the macrostates @xmath of the parent
phase (A) are mapped onto those, @xmath , of the conjugate phase (B). In
order for Eq. \thechapter .47 to serve as an efficient estimator of
@xmath , one must ensure that the configurations of probable macrostates
in @xmath are mapped onto the configurations of probable macrostates in
@xmath . Generally one will not be able find a suitable @xmath which
ensures that this is the case. One will instead find that configurations
of probable macrostates belonging to @xmath are mapped onto
configurations of improbable macrostates of @xmath . This is another
form of the overlap problem and results, in a way that will be explained
in much greater detail in section 7 , in the failure of Eq. \thechapter
.47 to serve as an efficient estimator for @xmath . However since we
avoid the region of configuration space characterised by the macrostates
@xmath , the magnitude of the overlap problem that we face in dealing
with Eq. \thechapter .47 is considerably less than that of estimating
@xmath via the continuous path technique (Eq. \thechapter .41 ).

### 4 Summary

In seeking to model the equilibrium behaviour of bulk material in terms
of its constituent components a useful microscopic theory is that of
statistical mechanics. This theory works by associating a probability
with each configuration that the system may assume. Accordingly one may
associate a probability with a phase simply by summing up all the
probabilities of the configurations consistent with that phase. The net
result is proportional to a quantity called the partition function
@xmath , given by Eq. \thechapter .7 .

For a given set of external constraints, one finds that for many
non-trivial (finite) systems several candidate structures may be stable.
Each of these structures, or phases, will have an associated probability
(proportional to their respective partition functions @xmath ). As the
size of the system increases one of the phases becomes overwhelmingly
more probable than the others, resulting in that phase being the one
that is found in nature.

In the case of finding phase boundaries, one is merely interested in
determining the more probable out of two candidate structures [ 41 ] .
Therefore it suffices to concentrate ones efforts on the determination
of @xmath rather than the individual partition functions themselves. By
using the Metropolis algorithm  one may, in principle, estimate @xmath
by taking the ratio of the times spent in the two phases (Eq.
\thechapter .8 ). In practice however, a Metropolis algorithm which
generates (via the sampling distribution @xmath , Eq. \thechapter .46 )
macrostates according to their canonical probabilities (Eq. \thechapter
.6 ) will remain trapped in the phase in which it is initiated,
resulting in the systematic errors alluded to in section 3.1 . Though
this problem may be overcome by appealing to an appropriate extended
sampling strategy, the transition of the simulation through the
intermediate regions @xmath results (in the case of transitions to an
ordered phase) in the formation of defect-rich structures. One way
around this to use the reference state technique. An alternative is the
PM technique, in which one maps configurations of one phase directly
onto those of the other phase (see figure 3 ). This in principle allows
one to estimate @xmath from a simulation performed in a single phase via
Eq. \thechapter .47 . In practice however even this method fails (in a
way that will be described in greater detail in section 7 ) due to a
milder version of the original overlap problem, which, in the context of
the PM method, translates to configurations of probable macrostates of
@xmath being mapped onto configurations of improbable macrostates of
@xmath under the operation of the PM. As is the case with the continuous
path technique this overlap problem may be overcome with the aid of a
suitably refined extended sampling strategy (see section 8.3 , 8.7 and
chapter \thechapter ).

The focus of this thesis will be on the development of methodologies of
tackling the problem of estimating FEDs via the PM formalism, and will
ultimately lead to their application to the calculation of quantum FEDs.
In the next section we will start off by mapping (by using the framework
of the PM method) the problem of determining the ratio of the partition
functions given in Eq. \thechapter .8 onto that of evaluating the ratio
of the partition functions associated with two systems with different
configurational energies. This will allow us to present the overlap
problem in a quantitative fashion. We will then review the array of
methods that have been engineered over the last 30 years in order to
address this sort of computational problem, before proceeding to discuss
our own investigations on the problem.

## Chapter \thechapter Review

### 5 Introduction

In this section we will formulate the task of estimating the FED within
the PM formalism, thus mapping the problem of evaluating the ratio of
two integrals involving a single configurational energy (see Eq.
\thechapter .8 ) onto that of determining the ratio of integrals
involving two different configurational energies. We will proceed to
define what we mean by overlap and then discuss the overlap problem in
the context of the PM formalism. Following this we will review an array
of methods which have been designed to tackle the problem of evaluating
the ratio of integrals involving two different configurational energies.
Some of what is presented here is new (in particular the (unifying)
formulation of the estimators in terms of the macrovariable @xmath and
the process switching generalisation of the fast growth method, section
8.9 ). We include it here for the sake of providing a coherent
presentation.

### 6 Formulation of the problem

Suppose that @xmath denotes a reference configuration that is consistent
with phase @xmath . One may then express the position vector @xmath in
terms of the displacements @xmath about @xmath :

  -- -------- -- -------------------
     @xmath      ( \thechapter .1)
  -- -------- -- -------------------

The degrees of freedom may now be parameterised through the variable
@xmath (instead of @xmath ) so that Eq. \thechapter .8 may be written
as:

  -- -------- -- -------------------
     @xmath      ( \thechapter .2)
  -- -------- -- -------------------

where the Jacobian of the transformation from the variable @xmath to
@xmath is unity. By expressing the degrees of freedom in Eq. \thechapter
.2 in terms of the variables @xmath , we have effectively switched from
the @xmath representation (see Eq. \thechapter .8 ) to the @xmath
representation.

By comparing Eq. \thechapter .2 to Eq. \thechapter .43 , we immediately
see that the expression for @xmath in Eq. \thechapter .2 involves a PM
in which the configuration @xmath of phase A is mapped onto a
configuration @xmath of phase B via the global configuration space
displacement [ 42 ] :

  -- -------- -- -------------------
     @xmath      ( \thechapter .3)
  -- -------- -- -------------------

We will refer to this mapping, whereby configurations of one phase are
mapped onto those of the other via the global configuration space
displacement of Eq. \thechapter .3 and in which the displacements of the
particles from their lattice sites are matched in the two phases, as the
real-space mapping (RSM) [ 1 ] , [ 34 ] - [ 37 ] . It is realised via
the following operation:

  -- -------- -- -------------------
     @xmath      ( \thechapter .4)
  -- -------- -- -------------------

More generally, one may formulate the problem in an arbitrary
representation. Consider writing the displacement @xmath as a linear
transformation of some generalised coordinates @xmath (which we will
call the effective configuration of the system):

  -- -------- -- -------------------
     @xmath      ( \thechapter .5)
  -- -------- -- -------------------

so that:

  -- -------- -- -------------------
     @xmath      ( \thechapter .6)
  -- -------- -- -------------------

We may now use the effective configuration @xmath to parameterise the
degrees of freedom of the phase. Substituting Eq. \thechapter .6 into
Eq. \thechapter .7 we find that:

  -- -------- -- -------------------
     @xmath      ( \thechapter .7)
  -- -------- -- -------------------

where

  -- -------- -- -------------------
     @xmath      ( \thechapter .8)
  -- -------- -- -------------------

If we express the configurational energy  about that of the reference
configuration:

  -- -------- -- -------------------
     @xmath      ( \thechapter .9)
  -- -------- -- -------------------

then the free energy difference between the two phases may be written
as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .10)
  -- -------- -- --------------------

where

  -- -------- -- --------------------
     @xmath      ( \thechapter .11)
  -- -------- -- --------------------

is the contribution of the reference state configurations to the FED,
while

  -- -------- -- --------------------
     @xmath      ( \thechapter .12)
  -- -------- -- --------------------

with

  -- -------- -- --------------------
     @xmath      ( \thechapter .13)
  -- -------- -- --------------------

By comparing Eq. \thechapter .13 to Eq. \thechapter .2 , we see that the
expression for @xmath in Eq. \thechapter .13 now involves a PM in which
the effective configuration @xmath is preserved on the transition from
one phase to the other; in other words the PM matches the @xmath
coordinates of the two phases. This mapping, in which the coordinates
@xmath of phase @xmath are mapped onto the coordinates @xmath of phase
@xmath (such that they share the same effective configuration @xmath )
may be realised in @xmath space via the operation:

  -- -------- -- --------------------
     @xmath      ( \thechapter .14)
  -- -------- -- --------------------

where @xmath is given by Eq. \thechapter .12 . The quantity @xmath in
Eq. \thechapter .13 is ubiquitous in a variety of fields. It is also the
starting point for a string of literature concerned with the task of
estimating the FEDs [ 1 ] , [ 38 ] , [ 43 ] - [ 55 ] (for reviews see [
8 ] , [ 13 ] , [ 22 ] , [ 53 ] , [ 56 ] - [ 61 ] ). We note that whereas
Eq. \thechapter .8 is the ratio of two integrals involving a single
configurational energy, Eq. \thechapter .13 is the ratio of two
integrals with different configurational energies. In both cases the
regions which contribute most significantly to the two integrals come
from different regions of configuration space. However in the case of
Eq. \thechapter .13 the disparity that exists between these two regions
of (effective) configuration space may not be as great as it is for the
two regions of (absolute) configuration space within the original @xmath
formulation (see Eq. \thechapter .8 ). We illustrate this idea
schematically in figure 4 .

Despite this scope for improvement, the overlap problem, albeit a milder
version, generally persists. In figure 4 (b) this corresponds to the
fact that the most typical regions associated with phase A do not
overlap with the most typical regions associated with phase B. In the
following section we will explain exactly why this poses a problem for
the task of estimating the FED. In section 8 we will then proceed to
review the methods that have been developed in order to estimate
quantities of the form of Eq. \thechapter .13 .

### 7 The overlap problem

In order to discuss the overlap problem in a semi-quantitative way, let
us first define a quantity which measures the energy cost in switching
from a configuration in phase @xmath to a configuration in phase @xmath
via the mapping of Eq. \thechapter .14 :

  -- -------- -- --------------------
     @xmath      ( \thechapter .15)
  -- -------- -- --------------------

Following the earlier definition of the conditional probability of a
macrovariable (Eq. \thechapter .6 ), one may define the probability of
observing a macrostate @xmath conditional on the sampling distribution
of phase @xmath , @xmath , as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .16)
  -- -------- -- --------------------

We will refer to this as the phase constrained distribution of phase
@xmath .

In order to understand the behaviour of the phase-constrained
distribution @xmath let us analyse the behaviour of the macrovariable
@xmath . This macrovariable is under certain circumstances an order
parameter for the system. To see this let us first consider a hard
sphere system. For this type of system a mapping of the form of Eq.
\thechapter .14 will map a configuration of phase @xmath (for which the
hard spheres do not overlap) onto a configuration of phase @xmath in
which the hard spheres typically penetrate each other’s cores. As a
consequence @xmath will be positive and infinite. A similar thing will
happen for a simulation initiated in phase @xmath in which the mapping
of Eq. \thechapter .14 is performed in the opposite direction. In this
case @xmath will be negative and infinite. In this sense @xmath acts as
an order parameter for the system. This idea carries over into systems
with continuous configurational energies. In this case for typical
effective configurations @xmath of phase @xmath , the mapping in Eq.
\thechapter .14 induces a configuration @xmath (given by Eq. \thechapter
.6 ) of phase @xmath of generally higher energy than the configuration
@xmath . In other words one will find that on average @xmath for typical
configurations of phase @xmath . Likewise the opposite will be true
(that is that @xmath ) for the typical configurations of phase @xmath .
The resulting distributions are shown in figure 5 .

We are now in a position to define (in a way which is free of ambiguity)
what exactly we mean by overlap. Suppose that @xmath denotes the
histogram count for bin @xmath for a simulation performed via @xmath .
The estimator for @xmath is given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .17)
  -- -------- -- --------------------

We then define the concept of overlap as follows:

The region of overlap is defined to be the set @xmath over which @xmath
and @xmath ( or equivalently @xmath and @xmath ) are simultaneously
non-zero .

The overlap between @xmath and @xmath may be quantified [ 46 ] by
introducing the overlap parameter @xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .18)
  -- -------- -- --------------------

@xmath may be estimated by @xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .19)
  -- -------- -- --------------------

In the case where the estimators @xmath and @xmath overlap considerably,
both @xmath and @xmath will assume a value close to unity. In the case
where @xmath and @xmath do not overlap at all @xmath will assume a small
(but non-zero) value, whilst @xmath .

We now proceed to derive the overlap identity [ 62 ] , which relates the
probabilities of obtaining a given value of @xmath in the two phases.
From Eq. \thechapter .16 we see that:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .20)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

Rearranging this equation one obtains the overlap identity:

  -- -------- -- --------------------
     @xmath      ( \thechapter .21)
  -- -------- -- --------------------

This identity may be used to estimate @xmath via:

  -- -------- -- --------------------
     @xmath      ( \thechapter .22)
  -- -------- -- --------------------

where @xmath is any bin for which one has a non-zero count for both
simulations.

The overlap identity imposes several constraints on the arbitrariness of
the phase constrained distributions @xmath and @xmath . One such
constraint is the value of @xmath , which we label as @xmath , at which
point these two distributions intersect. Substituting @xmath into Eq.
\thechapter .21 one obtains:

  -- -------- -- --------------------
     @xmath      ( \thechapter .23)
  -- -------- -- --------------------

or:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .24)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

Therefore the FED of the phases manifests itself as an asymmetry of the
point at which the two phase-constrained distributions intersect [ 63 ]
.

Eq. \thechapter .22 is very important because it links the idea of the
overlapping of @xmath and @xmath to ones ability to estimate @xmath . It
is immediately clear that in order for Eq. \thechapter .22 to serve as
an efficient estimator for @xmath there must be some regions of @xmath
space over which @xmath and @xmath overlap. If this is not the case (as
is the situation in figure 5 ) then either the numerator or the
denominator of Eq. \thechapter .22 will be zero for any bin @xmath ,
yielding an incorrect estimate of @xmath . It is in this way that the
overlap problem prevents one from arriving at an estimate of @xmath
which is free of systematic errors. Furthermore as the system size
increases the extensivity of @xmath results in the means and variances
associated with the distributions @xmath scaling in such a way so as to
reduce the overlap of the two distributions (see figure 5 for an
explanation). As a consequence the overlap problem worsens as the system
size increases.

In order to tackle the overlap problem, one needs to understand the
factors which affect it. From our preceding discussion it is clear that
the overlap is dependent on two factors:

-   The choice of the translation vector @xmath which maps
    configurations of one phase onto those of the other

-   The choice of representation @xmath [ 64 ] , [ 65 ] .

The first point has been addressed before [ 34 ] and is briefly
discussed in section 11 . The second point has not been investigated
before and in chapter \thechapter , we will show how one may choose a
representation, based on the normal modes of the phase, which does, for
structurally ordered phases, cure the overlap problem as the harmonic
limit is approached. Before doing this we will (in the next section)
present an array of techniques that have been developed in order to
estimate quantities of the form of Eq. \thechapter .13 .

### 8 Review of methods

#### 8.1 Introduction

Over the years a spectrum of methods have been developed in order to
address the evaluation of Eq. \thechapter .13 . These methods include
thermodynamic integration methods [ 23 ] - [ 25 ] , the canonical
perturbation methods [ 43 ] , [ 44 ] , [ 46 ] , [ 53 ] , the simulated
tempering methods [ 49 ] - [ 52 ] , the slow growth methods [ 66 ] - [
68 ] , and the umbrella sampling and the multistage methods [ 45 ] , [
62 ] , [ 69 ] - [ 71 ] . More recent developments include the fast
growth methods [ 55 ] , [ 72 ] - [ 75 ] , and the phase switching method
of [ 1 ] . We will now review a selection of the methods available for
estimating FEDs, limiting ourselves to NVT systems [ 22 ] . Though these
approaches do, at first sight, appear to be quite different, there are
common themes that run through all the methods; we shall try to make
them clear. The interrelations between the methods can be most easily
seen by observing the way in which the sampling distributions @xmath are
constructed and by expressing the estimators in terms of the macrostates
@xmath . We point out that some of the insights offered in this chapter
were also part of the work of this thesis; in particular the @xmath
formulation of the simulated tempering (section 8.6 ), the @xmath
formulation of the phase switch method (section 8.7 ), and the path
sampling formulation of the fast growth method (section 8.9 ). However
for the sake of a coherent presentation of the ideas, we have put them
in this section.

#### 8.2 Canonical Perturbation Methods

The perturbation methods are probably the simplest and earliest methods
developed to tackle the problem of determining FEDs [ 43 ] - [ 46 ] . We
will now describe the two most well known examples.

The first method may be most simply derived by integrating the overlap
identity (Eq. \thechapter .20 ) over all values of @xmath , yielding:

  -- -------- -- --------------------
     @xmath      ( \thechapter .25)
  -- -------- -- --------------------

or

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .26)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

Eq. \thechapter .26 (see also Eq. \thechapter .45 ) is what we refer to
as the exponential perturbation (EP) method. @xmath may then be simply
estimated by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .27)
  -- -------- -- --------------------

or:

  -- -------- -- --------------------
     @xmath      ( \thechapter .28)
  -- -------- -- --------------------

using the fact that @xmath is given by Eq. \thechapter .17 .

Generally an attempt to estimate the FED via Eq. \thechapter .28 will
fail. The reason for this ultimately stems from the fact that the
regions which contribute the most to the numerator of Eq. \thechapter
.28 will be those regions over which @xmath is most significant [ 76 ] ,
which, in the general case illustrated in figure 5 , is not contained
within the regions typically explored in a sampling experiment performed
with @xmath . As a result systematic errors will be present in ones
estimate of @xmath .

The second method is based on the identity:

  -- -------- -- --------------------
     @xmath      ( \thechapter .29)
  -- -------- -- --------------------

where A is the Metropolis acceptance function (Eq. \thechapter .29 ).
Using this identity in Eq. \thechapter .21 we get:

  -- -------- -- --------------------
     @xmath      ( \thechapter .30)
  -- -------- -- --------------------

Integrating and rearranging this equation we find that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .31)
  -- -------- -- --------------------

Eq. \thechapter .31 is what is known as the acceptance ratio (AR) method
[ 46 ] . Since @xmath is the acceptance probability of a Monte Carlo
move in which one attempts to switch the configurational energies from
@xmath to @xmath whilst keeping the effective configuration @xmath
constant (we will refer to such a Monte Carlo move as a phase switch
(PS), see section 8.7 ), we see that the AR method estimates @xmath by
evaluating the expectations of the acceptance probabilities for the
phase switches in the two phases, without actually performing them as
Monte Carlo moves. It follows from Eq. \thechapter .31 that @xmath may
then be estimated via the identity:

  -- -------- -- --------------------
     @xmath      ( \thechapter .32)
  -- -------- -- --------------------

Since the AR method (Eq. \thechapter .31 ) rests on the overlap identity
(Eq. \thechapter .21 ), it follows that its estimator (Eq. \thechapter
.32 ) implicitly assumes that some sort of overlap exists between the
estimators @xmath and @xmath . It is not immediately clear to what
extent overlap is needed, and the insight into this shall be provided
later in chapter \thechapter . For the moment we shall remain content
with the fact that in the general case an attempt to estimate @xmath via
Eq. \thechapter .32 will fail due to the absence of overlap between the
estimators of the phase-constrained distributions of the two phases (see
figure 5 ).

Eq. \thechapter .26 and Eq. \thechapter .31 are particular cases of a
much more general formula originally derived by Bennett [ 46 ] . This
formula may be simply obtained by multiplying the left and right sides
of the overlap identity (Eq. \thechapter .21 ) by an arbitrary function
@xmath and integrating over all values of @xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .33)
  -- -------- -- --------------------

or:

  -- -------- -- --------------------
     @xmath      ( \thechapter .34)
  -- -------- -- --------------------

For the choices of @xmath one obtains the EP formula (Eq. \thechapter
.26 ) and for @xmath one obtains the AR formula (Eq. \thechapter .31 ).
In general Eq. \thechapter .34 will require two separate simulations,
one in each phase. For this reason it will be referred to as the dual
phase (DP) formula.

The perturbation methods described thus far (Eq. \thechapter .26 , Eq.
\thechapter .31 , and Eq. \thechapter .34 ) rest on simulations in which
configurations are sampled according to their canonical probabilities
(see Eq. \thechapter .46 ). The spectrum of methods that we will now
review go beyond this and rest on the employment of sampling
distributions which are different from the distributions with respect to
which the expectations are performed (see Eq. \thechapter .32 ).

#### 8.3 Umbrella Sampling and Multicanonical Methods

The underlying idea behind the umbrella sampling method of Torrie and
Valleau [ 45 ] , [ 62 ] , [ 69 ] - [ 71 ] is that of sampling with a
distribution @xmath which is different from the sampling distribution
@xmath with respect to which the expectations are evaluated. That is by
applying the identity of Eq. \thechapter .32 to Eq. \thechapter .26 they
obtain the following identity:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .35)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

Torrie and Valleau considerably simplify the problem of constructing an
alternative sampling distribution by noticing that the multidimensional
quantity in Eq. \thechapter .35 can in fact be solely expressed in terms
of the statistics of @xmath , provided that the ratio @xmath can be
expressed as a function of @xmath . By constructing an alternative
sampling distribution @xmath by appeal to a weight function @xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .36)
  -- -------- -- --------------------

they use this idea to rewrite Eq. \thechapter .35 as:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .37)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

@xmath may then be estimated via:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .38)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

Eq. \thechapter .38 also directly follows from Eq. \thechapter .27 by
noticing that instead of @xmath being given by Eq. \thechapter .17 , it
is now given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .39)
  -- -------- -- --------------------

As we saw in section 8.2 , the reason why the EP method (Eq. \thechapter
.26 ) fails to serve as a useful estimator is essentially due to the
fact that @xmath does not contain @xmath [ 77 ] . The umbrella sampling
method overcomes this problem by constructing a sampling distribution
@xmath so that @xmath contains both @xmath and @xmath .

The construction of @xmath is however, just as difficult a task as that
of finding the FED of the two phases [ 78 ] , and in the original papers
[ 45 ] , [ 62 ] , [ 69 ] - [ 71 ] no scheme was provided for
constructing the sampling distribution @xmath . Instead they
acknowledged that the task of finding an appropriate @xmath was
”tedious” and suggested performing several independent umbrella sampling
simulations, with the umbrella distributions overlapping at the edges,
in order to cover the desired region of (effective) configuration space.

Recently the work of Torrie and Valleau has come alive again in the
works of Berg and Neuhaus [ 79 ] , [ 80 ] in which umbrella sampling was
reinvented under the name of the multicanonical (MUCA) algorithm [ 81 ]
. This time, however, an efficient prescription for constructing the
umbrella sampling distribution was provided. Within the context of
umbrella sampling the MUCA algorithm may be thought of as the two fold
process:

-   One constructs an estimate for @xmath , which we will denote by
    @xmath , over an arbitrary range of @xmath space.

-   One then samples from the MUCA sampling distribution:

      -- -------- -- --------------------
         @xmath      ( \thechapter .40)
      -- -------- -- --------------------

    via the acceptance probability of Eq. \thechapter .25 .

It follows that the probability of observing a macrostate @xmath under
the MUCA sampling distribution @xmath is given by:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .41)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

In the case where the estimate is perfect (that is @xmath ) @xmath is
constant for all the range of @xmath space. Therefore by obtaining a
sufficiently accurate estimate of @xmath over the desired range of
@xmath space, one may construct an umbrella (or MUCA) sampling
distribution @xmath (via Eq. \thechapter .40 ) which is flat (in @xmath
space) over that range.

There are several methods of generating these MUCA weights and we will
discuss the two simplest procedures. The first method (which is called
the visited states method [ 79 ] , [ 82 ] ) starts off with an initial
estimate in which all the MUCA weights @xmath are set to be equal to
each other. One then performs a simulation (or iteration) for @xmath
Monte Carlo steps using the MUCA sampling distribution Eq. \thechapter
.40 . The histogram @xmath that is subsequently obtained is then used to
improve the estimate of the MUCA weights through the identity [ 79 ] , [
82 ] - [ 84 ] :

  -- -------- -- --------------------
     @xmath      ( \thechapter .42)
  -- -------- -- --------------------

where @xmath denotes the weights of the current iteration, and @xmath
denotes the weights of the next iteration. Under this updating scheme
macrostates which have been visited will have their weights increased,
and as a result the probability (see Eq. \thechapter .36 ) of visiting
these macrostates is reduced for the next iteration. On the other hand
the weights of macrostates which are not visited at all are left
unaffected, so as to increase their chance of being visited (relative to
those macrostates which have been visited) in the next iteration. By
iterating this procedure, one may eventually obtain an accurate set of
MUCA weights @xmath over the desired range, which one may then use to
construct a MUCA sampling distribution (Eq. \thechapter .40 ) which is
flat over an arbitrary span of @xmath space.

The second scheme for constructing the MUCA weights @xmath is a
modification of the Wang-Landau method [ 85 ] - [ 87 ] . In this method
the time for each iteration corresponds to a single Monte Carlo step.
That is one updates the weight after each Monte Carlo step ( @xmath )
via the update scheme:

  -- -------- -- --------------------
     @xmath      ( \thechapter .43)
  -- -------- -- --------------------

The idea is to start of with an f greater than unity, and perform the
update scheme of Eq. \thechapter .43 until one obtains a flat histogram
[ 88 ] . One then reduces f (but at the same time constraining it to be
greater than unity) and repeats the simulation, this time starting off
with the set of MUCA weights obtained at the end of the previous
simulation. This procedure is carried out until f has reduced to a value
close to unity, at which stage one will have obtained a sufficiently
accurate set of MUCA weights so as to ensure that @xmath is flat over
the range of @xmath space containing both @xmath and @xmath . Using this
estimate one may perform a final simulation (in which the weights @xmath
are unmodified) and use Eq. \thechapter .38 in order to estimate @xmath
. In chapter \thechapter we will illustrate in greater detail the use of
the umbrella sampling strategy (constructing the MUCA weights via the
Wang-Landau method) in estimating the desired FEDs.

#### 8.4 Multistage Methods

The multistage (MS) method may be considered to be a generalisation of
the DP (dual phase) methods [ 53 ] , [ 54 ] . The central idea of this
method [ 45 ] is to break up the task of evaluating the FED into a
series of independent tasks of estimating the FEDs between pairs of
systems whose overlap is considerably improved with respect to the
original pair. The method is based on the construction of a chain of
configurational energies:

  -- -------- -- --------------------
     @xmath      ( \thechapter .44)
  -- -------- -- --------------------

in which the configurational energy @xmath has the associated sampling
distribution:

  -- -------- -- --------------------
     @xmath      ( \thechapter .45)
  -- -------- -- --------------------

By noticing that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .46)
  -- -------- -- --------------------

where

  -- -------- -- --------------------
     @xmath      ( \thechapter .47)
  -- -------- -- --------------------

and by constructing a chain of configurational energies in such a way
that @xmath and @xmath sufficiently overlap, one may estimate each
@xmath via any one of the techniques presented in this review.

The standard (and simplest) way of constructing @xmath is via the
following linear interpolation scheme:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .48)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

where @xmath [ 89 ] . As an example one may evaluate Eq. \thechapter .46
via the EP method (Eq. \thechapter .26 ) [ 90 ] :

  -- -------- -- --------------------
     @xmath      ( \thechapter .49)
  -- -------- -- --------------------

which, for the case of Eq. \thechapter .48 , may be written as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .50)
  -- -------- -- --------------------

where

  -- -------- -- --------------------
     @xmath      ( \thechapter .51)
  -- -------- -- --------------------

Eq. \thechapter .50 is interesting in its own right since it reduces to
the well known method of thermodynamic integration [ 23 ] - [ 25 ] in
the limit of sufficiently small @xmath . To see this we note that for
sufficiently small @xmath one may expand the exponential in Eq.
\thechapter .50 as a power series in @xmath so that:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .52)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

where in expanding the exponential of Eq. \thechapter .50 we have
neglected powers of order @xmath and higher. This is valid provided that
there are a sufficient number of configurational energies linking @xmath
to @xmath (see Eq. \thechapter .48 ), so as to ensure that the @xmath
are sufficiently small. We make a point to note that though the
appearance of Eq. \thechapter .52 is identical to that of Eq.
\thechapter .37 , it has incorporated within (as do all the other
methods that are being reviewed in this section) it the PM formalism of
[ 1 ] . This allows it to be used to directly estimate the FED between
the two phases, rather than having to use it to evaluate the FED between
each system and some reference system, as is the case in the original
formulation (see Eq. \thechapter .37 ).

#### 8.5 Weighted Histogram Analysis Method

The weighted histogram analysis method (also called WHAM [ 91 ] - [ 94 ]
) is a generalisation of the histogram re-weighting techniques of [ 95 ]
and [ 96 ] , and employs a minimum variance estimator which uses the
re-weighting of data from several independent simulations in order to
calculate the FEDs. The original derivation is very involved and we
follow the simpler derivation given in [ 54 ] .

Suppose that one constructs a chain of configurational energies @xmath
linking @xmath to @xmath (which we take to be the particular case of Eq.
\thechapter .48 ), whose corresponding sampling distributions are
denoted by @xmath , Eq. \thechapter .45 . The WHAM method is based on
the observation that if one performs several independent sampling
experiments with the sampling distributions @xmath , @xmath , …., @xmath
, in which @xmath independent samples are obtained for the sampling
experiment performed with @xmath , then the probability of observing a
macrostate @xmath within the collection of data, as obtained from all
the simulations, is given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .53)
  -- -------- -- --------------------

  -- -------- -- --------------------
     @xmath      ( \thechapter .54)
  -- -------- -- --------------------

The underlying idea of the WHAM method is simple. The strategy is to
construct a set of distributions @xmath which overlap and connect the
regions of (effective) configuration space associated with one phase to
those of the other [ 97 ] . With this choice the resulting @xmath will
contain both @xmath and @xmath . To arrive at an expression which will
allow one to estimate the partition functions @xmath (up to a
multiplicative constant which is the same for all the estimates) one
starts off by inverting the expression given in Eq. \thechapter .53 :

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .55)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

or:

  -- -------- -- --------------------
     @xmath      ( \thechapter .56)
  -- -------- -- --------------------

Summing over all bins in Eq. \thechapter .56 and using Eq. \thechapter
.53 it is clear that @xmath may be estimated by:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .57)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

The set of equations given by Eq. \thechapter .57 form the core of the
WHAM method [ 98 ] . It is immediately clear from Eq. \thechapter .57
that in order to estimate the ratio @xmath one must estimate the
partition functions (up to a common constant) of all the sampling
distributions associated with the configurational energies @xmath (see
Eq. \thechapter .44 ) linking @xmath to @xmath . In order to estimate
the @xmath one must solve Eq. \thechapter .57 iteratively . One starts
off with a set of estimates for @xmath , which one then feeds into Eq.
\thechapter .57 to get a new set of estimates. One then feeds these
estimates back into Eq. \thechapter .57 to get yet another set of even
more accurate estimates. One carries out this iteration until the set of
estimates have suitably converged. At this point the estimate of @xmath
will yield an accurate estimate of @xmath . The power of the method
clearly lies in the enormous scope for parallelizability that exists in
the construction of the path linking phase A to phase B.

#### 8.6 Simulated Tempering

The simulated tempering method [ 49 ] - [ 52 ] , like the multistage
method, involves the construction of a chain of configurational energies
linking @xmath to @xmath (see Eq. \thechapter .44 ). The basic idea
behind the method is to simulate from a sampling distribution
characterised by the following partition function:

  -- -------- -- --------------------
     @xmath      ( \thechapter .58)
  -- -------- -- --------------------

where @xmath is the partition function associated with the sampling
distribution @xmath (see Eq. \thechapter .45 and Eq. \thechapter .47 )
and @xmath are some weights. We will refer to each @xmath as a
sub-ensemble. The idea of the method is to construct a @xmath so that
the corresponding sampling distribution explores (evenly) all the
regions of (effective) configuration space ’containing’ the two
phase-constrained distributions @xmath and @xmath and the regions in
between them. One way [ 46 ] of realising Eq. \thechapter .58 is to
sample via the sampling distribution:

  -- -------- -- --------------------
     @xmath      ( \thechapter .59)
  -- -------- -- --------------------

An alternative sampling distribution (the one used in [ 49 ] - [ 52 ] )
is one which ’hops’ between the sub-ensembles and in which two types of
Monte Carlo moves are employed. The first type of move involves the
usual particle displacement. Such moves are accepted with the
probability given by Eq. \thechapter .25 , where @xmath is used in place
of @xmath if the simulation is in sub-ensemble i. The second type of
move attempts to switch sub-ensembles. That is this move keeps the
effective configuration @xmath constant whilst trying to change the
sampling distribution from @xmath to @xmath (generally @xmath is chosen
to be an adjacent sub-ensemble of m). In order to satisfy detailed
balance (Eq. \thechapter .19 ) and in order to yield a sampling
distribution with a partition function given by Eq. \thechapter .58 ,
such a move must be accepted with probability:

  -- -------- -- --------------------
     @xmath      ( \thechapter .60)
  -- -------- -- --------------------

Accordingly we may write the sampling distribution as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .61)
  -- -------- -- --------------------

where i is a stochastic variable assuming any integer value between (and
inclusive of) 1 and n. Unless otherwise stated, we will assume hereafter
that @xmath corresponds to the sampling distribution given in Eq.
\thechapter .61 .

Suppose that @xmath denotes the probability of finding the simulation in
sub-ensemble m and suppose that @xmath denotes the time spent in the
sub-ensemble m:

  -- -------- -- --------------------
     @xmath      ( \thechapter .62)
  -- -------- -- --------------------

where @xmath denotes the sequence of effective configurations generated
by the simulation, and where:

  -- -------- -- --------------------
     @xmath      ( \thechapter .63)
  -- -------- -- --------------------

Under the sampling distribution @xmath , it follows from Eq. \thechapter
.58 that since the ratio of the probabilities of the simulation being
found in any two sub-ensembles is given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .64)
  -- -------- -- --------------------

and since the ratio of the probabilities of finding the simulation in
two sub-ensembles is estimated by the ratio of the times spent in the
two sub-ensembles:

  -- -------- -- --------------------
     @xmath      ( \thechapter .65)
  -- -------- -- --------------------

then the ratio of the partition functions of @xmath and @xmath may be
estimated via:

  -- -------- -- --------------------
     @xmath      ( \thechapter .66)
  -- -------- -- --------------------

It follows from Eq. \thechapter .66 that the quantity @xmath may then be
estimated from the ratio of the times spent in the sub-ensembles 1 and
n:

  -- -------- -- --------------------
     @xmath      ( \thechapter .67)
  -- -------- -- --------------------

In order to arrive at an estimator in terms of the macrovariable @xmath
(as has been formulated in sections 8.3 and 8.5 for the umbrella and
WHAM methods respectively) we first note that:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .68)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

where @xmath denotes the joint distribution of observing the macrostate
@xmath and of being in sub-ensemble i under the sampling distribution
@xmath , and where @xmath denotes the probability of being in
sub-ensemble i under the sampling distribution @xmath . From Eq.
\thechapter .58 and Eq. \thechapter .61 it follows that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .69)
  -- -------- -- --------------------

By noting that in the case where the configurational energy is linearly
parameterised (Eq. \thechapter .48 ):

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .70)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

One may use Eq. \thechapter .37 to arrive at an estimator for @xmath in
terms of the macrovariable @xmath :

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .71)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

The estimator in Eq. \thechapter .71 is also valid if, instead of @xmath
, one employs the sampling distribution given in Eq. \thechapter .59 .

By hopping between the sub-ensembles the simulation is able to explore a
wider region of (effective) configuration space than it would under a
sampling experiment performed with any one of the individual sampling
distributions @xmath . In order to ensure that the simulation visits all
sub-ensembles, one must first ensure that a sufficient number of
intermediate sub-ensembles have been constructed (so that @xmath
overlaps with @xmath ). Secondly one must also ensure that the correct
weights @xmath have been chosen so as to guarantee that the simulation
is able to frequently traverse between the regions of (effective)
configuration space typically associated with phase A and those
typically associated with phase B. One way to do this is to choose the
weights so that equal times are spent in all the sub-ensembles. In this
case one sets @xmath . However since a-priori the partition functions
are not known, it follows that the weights must be constructed in an
iterative fashion (e.g. via the visited states method or the Wang-Landau
method) as is done in the Umbrella Sampling method (see section 8.3 ).
Having obtained the weights one may then proceed to estimate the ratio
of the partition functions @xmath by appeal to the estimator in Eq.
\thechapter .67 or Eq. \thechapter .71 .

#### 8.7 Phase Switching Method

The Phase Switching (PS) method, along with the whole phase mapping (PM)
formalism, was originally developed in [ 1 ] (see [ 8 ] for a review and
see [ 34 ] - [ 37 ] for generalisations). In order to motivate the
method consider the case where one samples from a sampling distribution
which is associated with the following partition function (which we
refer to as the canonical PS partition function):

  -- -------- -- --------------------
     @xmath      ( \thechapter .72)
  -- -------- -- --------------------

An example of a sampling distribution associated with such a partition
function is:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .73)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

From the discussion of section 8.6 it is clear that Eq. \thechapter .72
may be equivalently realised by what we call the canonical PS sampling
distribution:

  -- -------- -- --------------------
     @xmath      ( \thechapter .74)
  -- -------- -- --------------------

where @xmath is a stochastic variable, which assumes one of two values:
@xmath or @xmath . Eq. \thechapter .74 is clearly a special case of the
ST sampling distribution @xmath in which one has only two sub-ensembles
corresponding to the two phases and in which the weights @xmath are the
same for both the phases. The sampling distribution @xmath then accepts
a ’switch’ between the two systems with the following probability:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

The PS sampling distribution @xmath (and also the alternative sampling
distribution in Eq. \thechapter .73 ) then yield the following
distribution:

  -- -------- -- --------------------
     @xmath      ( \thechapter .76)
  -- -------- -- --------------------

For the PS sampling distribution @xmath the ratio of the partition
functions is then given by:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .77)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

which may be estimated by the ratio of the times spent in the two phases
(see Eq. \thechapter .67 ):

  -- -------- -- --------------------
     @xmath      ( \thechapter .78)
  -- -------- -- --------------------

where @xmath is given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .79)
  -- -------- -- --------------------

More generally (see Eq. \thechapter .71 ) one may estimate @xmath (for
both Eq. \thechapter .73 and Eq. \thechapter .74 ) from the estimator
corresponding to:

  -- -------- -- --------------------
     @xmath      ( \thechapter .80)
  -- -------- -- --------------------

where @xmath is the fermi function:

  -- -------- -- --------------------
     @xmath      ( \thechapter .81)
  -- -------- -- --------------------

It is clear from Eq. LABEL:eq:psacc that a PS Monte Carlo move (in which
one attempts to switch phases whilst keeping the effective configuration
@xmath constant) is only likely to be accepted within the @xmath
regions. From section 7 we saw that the vast majority of equilibrium
configurations of the parent phase will be mapped (under the operation
of the PM) onto configurations of the conjugate phase which are of
higher excitation energy. Therefore the probability of visiting
configurations for which the two phases are of roughly the same energy
(under the operation of the PM) is negligibly small (see also figure 5
). Even if one considers the more general case of Eq. \thechapter .58
where one introduces two weights into Eq. \thechapter .72 so that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .82)
  -- -------- -- --------------------

the problem will not be solved since, in the absence of overlap, these
weights will at best allow the switching to take place only in one
direction. As we saw in section 8.6 the simulated tempering method
solves this problem by constructing a series of intermediate systems so
as to engineer overlap between adjacent systems. For such pairs of
(sufficiently overlapping) systems the weights can be chosen so as to
ensure that switching takes place frequently in both directions.

The way the PS method overcomes the overlap problem is by using a set of
weights which are themselves a function on (effective) configuration
space. That is rather than simulating via a sampling distribution
characterised by the partition function in Eq. \thechapter .82 one
instead employs a MUCA sampling distribution associated with the
partition function:

  -- -------- -- --------------------
     @xmath      ( \thechapter .83)
  -- -------- -- --------------------

The sampling distribution (which we call the MUCA-PS sampling
distribution) is then given by :

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .84)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

which has the associated @xmath distribution:

  -- -------- -- --------------------
     @xmath      ( \thechapter .85)
  -- -------- -- --------------------

Eq. \thechapter .83 may be equivalently realised via the sampling
distribution:

  -- -------- -- --------------------
     @xmath      ( \thechapter .86)
  -- -------- -- --------------------

It is immediately clear that the probability of a PS Monte Carlo move,
as dictated by Eq. \thechapter .84 , is given by Eq. LABEL:eq:psacc ,
since a PS does not change the value of @xmath :

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

On the other hand the probability of accepting a move from a
configuration @xmath to a configuration @xmath is now given by:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .88)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

Therefore the role of the MUCA weight function @xmath is not, as is the
case in the ST method, to aid the simulation (directly) in switching
between phases, but instead to guide the simulation to regions of
(effective) configuration space  from which the simulation can easily
switch phases.

In order to determine @xmath one applies the reweighting formula (Eq.
\thechapter .32 ) to Eq. \thechapter .77 (so as to remove the bias of
the MUCA weights in Eq. \thechapter .83 ):

  -- -------- -- --------------------
     @xmath      ( \thechapter .89)
  -- -------- -- --------------------

The corresponding estimator is then given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .90)
  -- -------- -- --------------------

Similarly the more general estimator given in Eq. \thechapter .80 ,
valid for both Eq. \thechapter .84 and Eq. \thechapter .86 , is now
replaced by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .91)
  -- -------- -- --------------------

Figure 6 (a) shows a schematic for @xmath and (b) shows a schematic for
@xmath . It is clear from (a) that a canonical PS sampling distribution
@xmath initiated in one of the two phases will remain stuck in that
phase, since the probability of visiting the @xmath regions is
negligibly small. In order to obtain the estimator of the canonical
distribution shown in (a), one would first have to perform a MUCA
simulation as shown in (b) and then reweight the data appropriately (see
[ 99 ] for details). The essential feature of the MUCA distribution
@xmath is that it contains both the phase constrained distributions
@xmath and @xmath . Figure 7 shows a schematic of the PS procedure.

#### 8.8 Fast Growth Method

The fast growth (FG) method [ 55 ] rests on a result called the
Fluctuation Theorem. This theorem has been proved for a variety of
non-equilibrium processes [ 55 ] , [ 72 ] - [ 74 ] , [ 100 ] - [ 105 ] .
The particular formulation that we will draw on is set out in [ 55 ] , [
72 ] - [ 74 ] .

Central to the Fluctuation Theorem is a non-equilibrium process that we
will now describe. One starts of by constructing a configurational
energy @xmath which is a function of a field parameter @xmath (for
example see Eq. \thechapter .48 ). The field parameter @xmath takes any
value between @xmath and @xmath , and the set @xmath forms a chain of
configurational energies linking @xmath to @xmath (see Eq. \thechapter
.44 ). The @xmath non-equilibrium process, which takes us from phase A
to phase B, involves the switching of the field parameter @xmath from an
initial value of @xmath to @xmath in a series of discrete steps at some
predefined, but arbitrary, set of times @xmath = @xmath .

The implementation procedure may be realised as follows. The initial
point @xmath is sampled with respect to the canonical distribution
@xmath . One then increments (at time @xmath ) the field parameter
@xmath from @xmath to @xmath ; in doing so one performs a (temperature
scaled) amount of work (which we will simply refer to as work for the
rest of this thesis) @xmath on the system. The system is then
equilibrated via the sampling distribution @xmath until a time @xmath
yielding the configuration @xmath . At this point one increments @xmath
from @xmath to @xmath so as to yield the work increment @xmath . One
continues this process until the value of the field parameter has
reached its terminal value, @xmath , at time @xmath .

As a result one obtains the path:

  -- -------- -------- -------- -- --------------------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      ( \thechapter .92)
  -- -------- -------- -------- -- --------------------

The net work for the @xmath process is then given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .93)
  -- -------- -- --------------------

In the case where one employs the linear parameterisation as given in
Eq. \thechapter .48 , Eq. \thechapter .93 may be written as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .94)
  -- -------- -- --------------------

where:

  -- -------- -- --------------------
     @xmath      ( \thechapter .95)
  -- -------- -- --------------------

A non-equilibrium process taking the simulation from phase B to phase A
may be similarly defined. For the sake of notational convenience it is
instructive to think of the @xmath process as an @xmath process, which
is performed backwards in time (that is the sequence of events is
reversed [ 73 ] , [ 74 ] ), and with the simple modification that the
initial configuration, now corresponding to @xmath at time @xmath , is
sampled from the distribution @xmath . That is the path is constructed
as follows. At time @xmath one decrements @xmath from its initial value
of @xmath to @xmath , thus performing an amount of work @xmath . One
then proceeds to equilibrate the system so as to obtain the
configuration @xmath at the time @xmath , at which point @xmath is
further decremented from @xmath to @xmath , thus incurring a work @xmath
. This procedure is repeated iteratively until time @xmath at which
point @xmath is decremented from @xmath to @xmath . It is clear from
this that the (temperature reduced) work increment performed at @xmath ,
when @xmath is changed from @xmath to @xmath , is given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .96)
  -- -------- -- --------------------

The net work for the resulting path:

  -- -------- -------- -------- -- --------------------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      ( \thechapter .97)
  -- -------- -------- -------- -- --------------------

is given by:

  -- -------- -------- -------- -- --------------------
     @xmath   @xmath   @xmath      
              @xmath   @xmath      
              @xmath   @xmath      
              @xmath   @xmath      ( \thechapter .99)
  -- -------- -------- -------- -- --------------------

Suppose now that @xmath corresponds to the probability of obtaining
@xmath for the @xmath process and suppose that @xmath [ 106 ] is the
corresponding quantity for the @xmath process (hereafter whenever we
mention @xmath in the context of the FG method we will in fact be
referring to the @xmath process, in which the initial distribution of
the configurations is given by @xmath ). The fluctuation theorem, which
we have also proved in appendix \thechapter , asserts that [ 72 ] - [ 74
] :

  -- -------- -- ---------------------
     @xmath      ( \thechapter .100)
  -- -------- -- ---------------------

or:

  -- -------- -- ---------------------
     @xmath      ( \thechapter .101)
  -- -------- -- ---------------------

In the special case of zero equilibration (which is equivalent to
changing @xmath directly from @xmath to @xmath in a single step):

  -- -------- -------- -------- -------- ---------------------
     @xmath   @xmath   @xmath            ( \thechapter .102)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- ---------------------

Eq. \thechapter .101 reduces to the overlap identity Eq. \thechapter .21
. For this reason we will refer to all the methods based on the identity
Eq. \thechapter .21 as the zero equilibration, or elementary, methods.
Since Eq. \thechapter .101 is simply a generalisation of Eq. \thechapter
.21 we will also refer to this formula as the overlap identity. It
immediately follows that we may generalise Eq. \thechapter .26 to:

  -- -------- -- ---------------------
     @xmath      ( \thechapter .103)
  -- -------- -- ---------------------

and Eq. \thechapter .31 to:

  -- -------- -- ---------------------
     @xmath      ( \thechapter .104)
  -- -------- -- ---------------------

Eq. \thechapter .34 may be replaced by the more general ’dual-phase’
(DP) formula [ 107 ] :

  -- -------- -- ---------------------
     @xmath      ( \thechapter .105)
  -- -------- -- ---------------------

Generally @xmath and @xmath will also face an overlap problem in the
sense described in section 7 . However the FG method does have a way of
getting around this; we will postpone our discussion of this point until
chapter \thechapter .

#### 8.9 Path Sampling Fast Growth Methods

All the simulations mentioned until now have been discussed in the
context of the sampling of individual configurations @xmath . Let us now
consider a simulation which, instead of jumping between configurations,
jumps between paths @xmath (such as those produced in the FG method,
section 8.8 ). Such a simulation can be though of as comprising of a two
fold procedure. In the first stage the simulation generates a path
@xmath . In the second stage a decision is made whether to accept or
reject the new path. The idea behind the path sampling formulation of
the FG method [ 108 ] is to express it in terms of the notion of the
sampling of paths, in the way that has just been described.

Suppose that @xmath denotes the underlying distribution of the paths
produced in the @xmath FG process as described in section 8.8 . It was
proved in appendix \thechapter that [ 73 ] , [ 74 ] :

  -- -------- -- ---------------------
     @xmath      ( \thechapter .106)
  -- -------- -- ---------------------

We will now show that the FG method described in section 8.8 may be
interpreted as a path sampling simulation in which paths are generated
according to @xmath in the @xmath process, and subsequently accepted
with probability 1. To be more specific, suppose that the current state
of the simulation is described by the path @xmath and suppose that
@xmath corresponds to the path that has just been generated. Then it is
clear that if one is to obtain a set of paths distributed according to
@xmath then the acceptance probability of moving from the path @xmath to
@xmath (in the @xmath process) is given by (see Eq. \thechapter .22 ):

  -- -------- -- ---------------------
     @xmath      ( \thechapter .107)
  -- -------- -- ---------------------

where @xmath denotes the probability of the simulation generating a path
@xmath given that the previous path was @xmath . In the procedure
described in section 8.8 the path @xmath is constructed from the initial
configuration @xmath , which itself is obtained from @xmath , the
initial configuration of @xmath , by equilibrating the system for a
fixed amount of time via @xmath . Using the notation of appendix
\thechapter it is clear that @xmath is given (for the @xmath process)
by:

  -- -------- -- ---------------------
     @xmath      ( \thechapter .108)
  -- -------- -- ---------------------

where @xmath denotes the probability of making a transition from the
configuration @xmath to the configuration @xmath when sampling from
@xmath for a fixed amount of time and where @xmath denotes the
probability of obtaining a path @xmath via the @xmath FG process of
section 8.8 , given an initial configuration of @xmath . Since from Eq.
\thechapter .3 we know that:

  -- -------- -- ---------------------
     @xmath      ( \thechapter .109)
  -- -------- -- ---------------------

it immediately follows that (for the @xmath process):

  -- -------- -------- -------- -------- ---------------------
     @xmath   @xmath   @xmath            ( \thechapter .110)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- ---------------------

where we have use the fact that:

  -- -------- -- ---------------------
     @xmath      ( \thechapter .111)
  -- -------- -- ---------------------

Eq. \thechapter .107 then becomes:

  -- -------- -- ---------------------
     @xmath      ( \thechapter .112)
  -- -------- -- ---------------------

Therefore we infer that the FG method described in section 8.8 may be
thought of as a path sampling experiment in which the paths are
generated according to the mechanism described in section 8.8 and in
which moves between old and new paths are accepted with probability 1 .

The benefit of the path sampling interpretation of the FG method is that
it allows us to generalise the PS method (Eq. \thechapter .74 ) so as to
be applicable within the framework of the FG method. To see this we
recall that the canonical PS sampling distribution @xmath (Eq.
\thechapter .74 ) realises the @xmath distribution given in Eq.
\thechapter .76 . It is not hard to show that the path sampling
distribution:

  -- -------- -- ---------------------
     @xmath      ( \thechapter .113)
  -- -------- -- ---------------------

in which @xmath is a stochastically sampled variable, realises the
following @xmath distribution:

  -- -------- -- ---------------------
     @xmath      ( \thechapter .114)
  -- -------- -- ---------------------

where we have used @xmath to denote @xmath in order bring out the links
with the zero equilibration cases. Eq. \thechapter .113 is essentially
the path sampling generalisation of Eq. \thechapter .74 . The
implementation of Eq. \thechapter .113 involves the employment of an
additional Monte Carlo move which allows one to switch between processes
. That is suppose that @xmath labels the @xmath FG process and @xmath
labels the @xmath FG process, in which paths are generated according to
Eq. \thechapter .110 . Then in this notation @xmath . As with the
original FG method, the FG phase-switch (FG-PS) procedure involves (for
the @xmath process) generating paths in the manner described by Eq.
\thechapter .108 and subsequently accepting with probability 1 (see Eq.
\thechapter .112 ). On top of this one introduces an additional Monte
Carlo move in which one attempts to switch between the @xmath and @xmath
processes whilst leaving the path @xmath unperturbed; the acceptance
probability for such a move is given by:

  -- -------- -------- -------- -------- ---------------------
     @xmath   @xmath   @xmath            ( \thechapter .115)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- ---------------------

where we have appealed to Eq. \thechapter .10 . In the case where @xmath
and @xmath partially overlap about the @xmath regions, the FG-PS method
allows one to sample all the paths that contribute non-negligibly to the
estimator of the FED. One may then proceed to estimate @xmath via:

  -- -------- -- ---------------------
     @xmath      ( \thechapter .116)
  -- -------- -- ---------------------

#### 8.10 Looking Forward

We are now in a position to explain more fully the scope of the study
presented here. We have seen that the key obstacle to the problem of
determining FEDs is the overlap problem. To deal with this problem one
must:

1.  Choose an appropriate global displacement @xmath in order to map
    configurations of one phase onto the other (see Eq. \thechapter .43
    , figure 3 ) and choose a representation @xmath (see Eq. \thechapter
    .5 )

2.  Choose an estimator [ 46 ] .

3.  Employ some form of extended sampling strategy [ 21 ] .

In succeeding chapters we take up each of these points in turn. In
chapter \thechapter we will deal with the first point and will show how
the overlap depends on the representation one chooses to work in.
Specifically by working in a representation in which the effective
configuration @xmath corresponds to the normal modes of a crystalline
solid we construct a transformation (called the fourier space mapping,
FSM) which, under certain conditions, is more efficient than the RSM
(see Eq. \thechapter .4 ).

In chapter \thechapter we provide some insight into the second point and
show how in the case of partial overlap, which is when there are @xmath
macrostates over which both the estimators @xmath and @xmath are
non-zero, and when there are other macrostates for which only one of the
estimators is non-zero, then the estimator of Eq. \thechapter .105 can
be guaranteed to work (in the sense that the estimate of @xmath is free
of systematic errors) for any @xmath simply by restricting the regions
of @xmath space from which the non-negligible contributions to the
expectations come. Furthermore we will also show that there is a family
of estimators of the form of Eq. \thechapter .105 for which no such
restrictions are required since by construction these estimators are
guaranteed to be free of systematic errors, provided that there exists
some overlap between @xmath and @xmath .

The third point is addressed in chapter \thechapter . We start off by
applying the basic theoretical techniques of umbrella sampling [ 21 ] ,
[ 62 ] , [ 79 ] to the problem of phase behaviour, and use the recently
developed Wang-Landau [ 85 ] technique to construct the MUCA weight
function needed in order to estimate the FED (see section 8.3 ). In
addition to this we present a new method of overcoming the overlap
problem (called the Multihamiltonian (MH) method) which involves
simulating several independent sampling distributions simultaneously.
Like the WHAM method, the benefit of this method is that it is highly
parallelizable. We then proceed to make an investigation of the FG
method, and show how it is able to effectively overcome the overlap
problem.

Having formulated the FED problem in a strictly classical framework we
proceed in chapter \thechapter to consider the quantum formulation as
provided by the path integral formalism. The quantum FED problem is even
more computationally intensive than its classical counterpart, and we
show that the MH method developed in chapter \thechapter provides an
efficient way of estimating the relevant quantities. In particular we
illustrate its power with a study of the role of zero-point motion in
determining crystal stability.

## Chapter \thechapter Tuning the Representations

### 9 Introduction

We saw in sections 2 and 3.2 that different phases of a given material
may be thought of as corresponding to different basins of attraction of
the configurational energy @xmath . For finite-system-constructs of the
relevant phases, there may exist several metastable basins of
attractions (corresponding to the different phases) and it is ones
desire to find the most probable one. In the thermodynamic limit this
basin of attraction becomes overwhelmingly more probable than the
others, resulting in the corresponding phase being the one that is found
in nature. In this thesis we will focus on the case where there are two
candidate phases, and our task will be limited to that of determining
the more stable out of the two [ 41 ] .

Computationally the task of finding the more probable out of the two
phases involves implementing a Metropolis simulation in which one visits
these two regions in a single simulation. One may then estimate the FED
via Eq. \thechapter .8 and Eq. \thechapter .15 . However the sequential
or pathwise nature of the Metropolis method and the presence of a region
of (absolute) configuration space, in between the two phases, of
intrinsically low probability means that a simulation initiated in a
given phase will remain trapped in that phase.

If instead of working in the @xmath representation (in which one
attempts to estimate the quantity @xmath in Eq. \thechapter .8 ) one
chooses to work in the @xmath representation (in which one attempts to
estimate the quantity @xmath in Eq. \thechapter .13 ), one greatly
alleviates the difficulty associated with the problem of estimating the
FED of the two phases by bypassing this intermediate region altogether
(compare figure 4 (a) and (b)). The residual difficulty that is left in
the associated problem is captured in the amount by which the two phases
overlap in @xmath space, and it is this difficulty which must be
overcome in order to estimate @xmath .

In order to gauge the amount of overlap between these two regions, one
must (by virtue of the overlap identity Eq. \thechapter .21 ) observe
the amount of overlap that is present between @xmath and @xmath . This
overlap in @xmath space is clearly dependent upon two factors:

1.  The choice of the reference configurations @xmath (see Eq.
    \thechapter .6 ). Different choices of @xmath and @xmath correspond
    to different choices of @xmath in Eq. \thechapter .3 (see also Eq.
    \thechapter .43 , Eq. \thechapter .45 , figure 3 ).

2.  The choice of representation @xmath [ 64 ] , [ 65 ] , which @xmath (
    @xmath ) depends on (Eq. \thechapter .15 ).

The ideal choices of the global translation @xmath and the
representation @xmath are ones for which @xmath and @xmath (see figure 5
) collapse onto the distribution:

  -- -------- -- -------------------
     @xmath      ( \thechapter .1)
  -- -------- -- -------------------

Departures from this ideal limit manifest in the bi-modality of @xmath ,
as shown in figure 5 . The more efficient the choice of @xmath and
representation @xmath , the closer the distributions @xmath and @xmath
will be to the ideal limit (Eq. \thechapter .1 ).

The general challenge to the problem of estimating the FED is that of
tuning the PM so as to maximise the overlap between the two regions of
effective configuration space (see figure 4 (b)). In this chapter we
will investigate this issue in the particular context of the two phases
being crystalline solids. We will primarily focus on the role of the
representation (issue B). Specifically we will see that a PM as
formulated in a Fourier space (normal mode) representation provides some
strategic advantages over the real-space representations (RSM, Eq.
\thechapter .4 ) utilised in previous studies [ 1 ] , [ 34 ] - [ 38 ] .

In order to set the context we will now introduce the model system (the
Lennard Jones solid) which has been employed in all the simulations
carried out in this work. This will be followed by a section
illustrating how the PM is implemented for our model systems, followed
by a brief discussion of the role of the global translation vector
(issue A) in the mappings between the two phases.

### 10 The model system

For the work in this thesis our model system will be comprised of
particles interacting via the pairwise Lennard-Jones (LJ) interatomic
potential:

  -- -------- -- -------------------
     @xmath      ( \thechapter .2)
  -- -------- -- -------------------

where @xmath corresponds to the interaction energy between particles i
and j separated by a distance @xmath , and where @xmath and @xmath are
the position vectors of particles i and j respectively. The overall
configurational energy @xmath is then given by:

  -- -------- -- -------------------
     @xmath      ( \thechapter .3)
  -- -------- -- -------------------

Generally the use of the full configurational energy in Eq. \thechapter
.3 in a simulation is prohibitively expensive, and one instead employs
some form of approximation whereby the potential is truncated at some
distance from the particle [ 109 ] . The ensuing phase diagram (a
schematic of which is shown in figure 8 ) is highly sensitive to this
truncation radius; the latter has to be chosen carefully if one is to
reproduce the true characteristics of the phase under consideration. By
analysing the fluctuations in the ground state energies and the harmonic
free energy differences as a function of the truncation radius, it was
found in [ 110 ] (where identical systems were employed to those used in
this work) that a truncation radius @xmath given by:

  -- -------- -- -------------------
     @xmath      ( \thechapter .4)
  -- -------- -- -------------------

yielded sufficiently accurate results (where @xmath is the nearest
neighbour distance). A truncation radius of this magnitude essentially
amounts to each particle interacting with both its first nearest
neighbour shell (comprising of 12 particles) and its second nearest
neighbour shell (comprising of 6 particles) [ 111 ] . This truncation
radius was also employed for all the simulations used in this thesis,
with the exception of those in chapter \thechapter (in which a
truncation radius of @xmath was employed).

Unless otherwise stated, the system size that we have employed is @xmath
, and the densities are @xmath . We will also quote all results in terms
of the reduced temperature:

  -- -------- -- -------------------
     @xmath      ( \thechapter .5)
  -- -------- -- -------------------

We also make a point here that it is our intention, in this thesis, only
to use the LJ system as a testbed for the various methods and not to
give definitive results for the LJ phase diagram (which has already been
done in [ 110 ] ).

### 11 The Phase Mapping for crystalline solids

Let us now focus out attention on the regions of the phase diagram near
the fcc-hcp boundary (see figure 8 ), and let phase label A refer to the
fcc structure and phase label B to the hcp structure. In the case of
crystalline solids, the reference configuration ( @xmath ) may most
conveniently be chosen to be the ground state configuration (i.e. the
lattice sites themselves). The full PM then involves a switch of lattice
vectors, accompanied by the mapping of the displacements (of the
particles from their lattice sites) of one phase onto the (possibly
modified) displacements of particles of the other phase. In the case of
the RSM, these displacements are unmodified on the transition of the
phases.

For the fcc and hcp structures, one may identify families of planes
which are common to the reference configurations of both the structures.
Whereas the fcc structure may be thought of as being comprised of three
families of close packed planes (see figure 9 (a)), the hcp structure
may be thought of as comprising of two layers of close packed planes
(see figure 9 (b)). The geometry of the planes are such that they permit
a simple mapping of the lattice structure of one phase onto that of the
other (see figure 10 ). In this case the operation of the RSM takes a
particularly simple form; one merely slips the planes as shown in figure
10 , whilst at the same time preserving the relative positions of the
particles within a given plane. For a more detailed illustration of
these planes and their corresponding structures, we refer the reader to
[ 110 ] and [ 112 ]

The choice that we have made for mapping the lattice sites of one
structure onto those of the other does not exhaust the possibilities for
@xmath . In fact for a given labelling scheme of the particles, one may
choose to map the lattice vector of particle i of structure @xmath onto
that of particle j of structure @xmath , instead of mapping it onto the
same particle of the corresponding phase [ 113 ] . This procedure may
equivalently be thought of as a permutation of the index labelling the
particles under the operation of the PM; there are of the order of N!
such permutations. This point was investigated to a limited extent for
the RSM in the case of hard spheres by Jackson et. al. [ 35 ] . In their
work they investigated cases where the planes were displaced a distance
greater than that shown in figure 10 , in transforming from one phase to
the other. They also investigated the cases where the planes of the fcc
structure were randomly stacked when forming the hcp structure, and also
the case where the displacements of a particle of the fcc structure were
mapped onto those of a randomly chosen particle of the hcp structure.
All the alternative mappings resulted in greater number of hard spheres
overlapping, as compared to the mapping presented in figure 10 . In this
work we do not investigate this issue any further [ 114 ] .

In the case of the solid-liquid phase boundary, an appropriate reference
configuration for the liquid phase is simply any typical configuration
of the fluid phase [ 36 ] . Though we will not have anything more to say
about this, we note that an investigation along this direction has been
made (for the hard sphere case) in [ 36 ] and more recently (for the
soft potential case) in [ 37 ] .

### 12 Formulation of the Fourier Space Mapping

In section 9 we mentioned that there are two issues at hand, the first
being the choice of a suitable reference configuration @xmath and the
second being the choice of an appropriate representation @xmath . Given
that the lattice sites themselves serve as both natural and convenient
choices for the reference configurations, we will now concentrate our
efforts on finding an optimal representation @xmath [ 115 ] .
Specifically we will choose a representation in which @xmath corresponds
to the normal modes (whose corresponding mapping Eq. \thechapter .14 we
call the Fourier Space Mapping, FSM), and compare the overlap that one
obtains in this case to that of the RSM (Eq. \thechapter .4 ). We will
show that in the harmonic limit the FSM ensures that the two phases have
identical excitation energies, so as to ensure perfect overlap (in the
sense of Eq. \thechapter .1 ) between the two phases in the effective
configuration space as parameterised by the coordinates @xmath . In
contrast to this we will also show that for the RSM this overlap will
never be perfect [ 116 ] .

#### 12.1 Constructing the transformation

To motivate the transformation, consider the Taylor expansion of the
(excitation) configurational energy (Eq. \thechapter .9 ) in powers of
the displacements @xmath with respect to the reference configuration
@xmath [ 117 ] :

  -- -------- -- -------------------
     @xmath      ( \thechapter .6)
  -- -------- -- -------------------

where the second term @xmath denotes the harmonic contributions
(containing powers of second order in the displacement @xmath ) and
@xmath denotes the anharmonic contributions (of at least third order in
the displacement @xmath ). The harmonic contributions may be written as:

  -- -------- -- -------------------
     @xmath      ( \thechapter .7)
  -- -------- -- -------------------

where @xmath is the @xmath dynamical matrix. If @xmath denotes the entry
in the i-th row and j-th column of the matrix @xmath then:

  -- -------- -- -------------------
     @xmath      ( \thechapter .8)
  -- -------- -- -------------------

Since @xmath is a symmetric matrix (i.e. it is Hermitian) we may (via
the Gram-Schmidt orthogonalisation procedure if necessary) construct a
set of orthonormal vectors @xmath which are the eigenvalue of @xmath .
In our case we will take @xmath to be the 3N column vector corresponding
to the j-th eigenvector of @xmath . If we set @xmath (Eq. \thechapter .5
) such that:

  -- -------- -- -------------------
     @xmath      ( \thechapter .9)
  -- -------- -- -------------------

or:

  -- -------- -- --------------------
     @xmath      ( \thechapter .10)
  -- -------- -- --------------------

where @xmath is the i-th component of the j-th eigenvector of @xmath and
where @xmath is the eigenvector of @xmath , then from Eq. \thechapter .5

  -- -------- -- --------------------
     @xmath      ( \thechapter .11)
  -- -------- -- --------------------

The summation in Eq. \thechapter .11 is performed over the 3N components
@xmath (which we refer to as the fourier coordinates) of the column
vector @xmath . Substituting Eq. \thechapter .11 into Eq. \thechapter .7
one finds that:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .12)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

where we have invoked the orthonormality of the eigenvectors:

  -- -------- -- --------------------
     @xmath      ( \thechapter .13)
  -- -------- -- --------------------

In other words, by choosing an appropriate representation @xmath (which
we call the fourier representation) in which the normal modes (or
fourier coordinates) of one crystalline solid are mapped onto those of
the other, one may cast @xmath into a form which contains no phase
labels:

  -- -------- -- --------------------
     @xmath      ( \thechapter .14)
  -- -------- -- --------------------

Using the fact that @xmath or

  -- -------- -- --------------------
     @xmath      ( \thechapter .15)
  -- -------- -- --------------------

we may use Eq. \thechapter .12 to write down the matrix elements of
@xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .16)
  -- -------- -- --------------------

This matrix has the associated determinant:

  -- -------- -- --------------------
     @xmath      ( \thechapter .17)
  -- -------- -- --------------------

The expression in Eq. \thechapter .10 for the FED may then be written
as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .18)
  -- -------- -- --------------------

where

  -- -------- -- --------------------
     @xmath      ( \thechapter .19)
  -- -------- -- --------------------

is the harmonic contribution to the overall FED (Eq. \thechapter .18 ).
Since the harmonic contributions to the excitation energy (Eq.
\thechapter .7 , \thechapter .14 ) are equal if they share the same
fourier coordinates, we see that only the anharmonic contributions to
the energy will survive in the evaluation of @xmath :

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .20)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

As a result the third term in Eq. \thechapter .18 :

  -- -------- -- --------------------
     @xmath      ( \thechapter .21)
  -- -------- -- --------------------

reflects the purely anharmonic contributions to the FED.

One may realise the associated mapping, which we call the fourier space
mapping (FSM), within the framework of the @xmath representation through
the operation in Eq. \thechapter .14 where @xmath is given by Eq.
\thechapter .16 . Figure 11 shows a schematic of the conceptual
procedure involved in the mapping.

For systems with periodic boundary conditions, additional considerations
must be taken into account in constructing the FSM. In appendix
\thechapter we outline the necessary modifications which must be
incorporated into the transformation @xmath in order to accommodate
these constraints. Generally what one finds is that the use of periodic
boundary conditions means that three of the eigenvectors of the
dynamical matrix @xmath (Eq. \thechapter .8 ) will have zero
eigenvalues. Suppose that @xmath , @xmath , and @xmath correspond to
these null eigenvectors. The findings of appendix \thechapter are that
one may simply omit these coordinates in the evaluation of the relevant
quantities. For example the displacements @xmath are now given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .22)
  -- -------- -- --------------------

and the transformation matrix @xmath (Eq. \thechapter .16 ) is now
replaced by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .23)
  -- -------- -- --------------------

We will assume that all subsequent summations over the fourier
coordinates @xmath will be of the form of that employed in Eq.
\thechapter .22 and Eq. \thechapter .23 , i.e. summations which exclude
the null modes.

#### 12.2 Summary

Summarising, we have constructed a transformation @xmath (Eq.
\thechapter .16 ) called the fourier space mapping (FSM) which maps a
configuration @xmath of phase @xmath onto a configuration @xmath of
phase @xmath so as to ensure that the two phases are of identical
excitation energies in the harmonic limit, thus guaranteeing perfect
overlap. As the harmonic contributions to the FED are already known
exactly via Eq. \thechapter .19 , the utility of this transformation
will not lie in the overwhelmingly harmonic regime but will instead lie
within the anharmonic regime. In particular, since the overlap can be
arbitrarily improved simply by reducing the temperature ( @xmath as
@xmath ), one might expect that the problem of estimating the anharmonic
contributions to the FED might become controllably small in the @xmath
limit. We will see that this is not quite the case.

Before discussing the limitations of the FSM we will first focus on the
efficiency with which it overcomes the overlap problem, and we will use
the RSM for comparison. In order to compare the efficiency with which
the FSM and RSM tackle the overlap problem, we will, in the next
section, investigate the issue of the overlap between the two
phase-constrained distributions of @xmath via analytic techniques.
Specifically we will show how, in the harmonic limit, the overlap
problem vanishes for the FSM whereas it tends to a constant value for
the RSM (in the sense that @xmath assumes a stationary form), thereby
serving as the most extreme illustration of the dependence of the
overlap problem on the representation. We will also use these results to
outline some of the basic limitations that the FSM faces in estimating
the anharmonic FEDs

### 13 Analytic results

Since a probability distribution is completely characterised by its
cumulants, one way to obtain insights into the behaviour of the overlap
problem is to focus on the cumulants of @xmath . Of particular
importance are the first two cumulants, since it is these which
correspond to the mean and variance of the distribution, and since it is
these which will be most important in indicating the amount of overlap
that will be present [ 119 ] . To define the cumulants let us expand
@xmath (using Eq. \thechapter .26 ) as the exponential of a power series
in @xmath [ 120 ] :

  -- -------- -- --------------------
     @xmath      ( \thechapter .24)
  -- -------- -- --------------------

where @xmath is the n-th cumulant. The first three cumulants are then
given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .25)
  -- -------- -- --------------------

and

  -- -------- -- --------------------
     @xmath      ( \thechapter .26)
  -- -------- -- --------------------

and:

  -- -------- -- --------------------
     @xmath      ( \thechapter .27)
  -- -------- -- --------------------

The FED (Eq. \thechapter .10 ) may be cast into a simple form using
these cumulants:

  -- -------- -- --------------------
     @xmath      ( \thechapter .28)
  -- -------- -- --------------------

In the next two sections we will analyse the behaviour of these
cumulants for the RSM and the FSM in the harmonic limit ( @xmath ).

#### 13.1 Fourier Space Mapping

Drawing on anharmonic perturbation theory one may expand the
configurational energy @xmath as a power series in @xmath in which the
contributions of successive orders become increasingly smaller. As the
harmonic limit is approached one may discard all but the terms which
scale, upon integration, with lowest order of T, thereby considerably
simplifying the analysis of the cumulants. The results of the theory
(appendix \thechapter ) may be summarised as follows. In the limit
@xmath (or @xmath ), the cumulants of @xmath scale in the following way
with temperature:

  -- -------- -- --------------------
     @xmath      ( \thechapter .29)
  -- -------- -- --------------------

From Eq. \thechapter .28 and Eq. \thechapter .29 one observes that in
the limit @xmath one may write a cumulant approximation expression for
the anharmonic contributions to the FED as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .30)
  -- -------- -- --------------------

#### 13.2 Real Space Mapping

The behaviour of the FSM in the low temperature regime is in sharp
contrast to the RSM in which all the temperature-scaled cumulants tend
to a constant value, indicating that the overlap of the
phase-constrained distributions of @xmath assume a constant value in
this limit. To see this we once again appeal to anharmonic perturbation
theory and expand the configurational energy as a power series of in
@xmath . The results have been worked out in appendix \thechapter and
may be summarised as follows:

  -- -------- -- --------------------
     @xmath      ( \thechapter .31)
  -- -------- -- --------------------

and:

  -- -------- -- --------------------
     @xmath      ( \thechapter .32)
  -- -------- -- --------------------

where @xmath are the eigenvalues of the matrix with elements:

  -- -------- -- --------------------
     @xmath      ( \thechapter .33)
  -- -------- -- --------------------

More generally one may conclude that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .34)
  -- -------- -- --------------------

so that all the cumulants contribute to the FED (Eq. \thechapter .28 )
at arbitrarily low T. The fact that the temperature-reduced cumulants
(Eq. \thechapter .34 ) tend to a constant value in the harmonic limit
translates to the fact that the overlap between @xmath and @xmath tends
to a constant amount in this limit [ 121 ] .

### 14 Some numerical results

In section (in section 14.1 ) we will start by investigating
(numerically) the overlap of the phase-constrained distributions for
both the FSM and the RSM in the low temperature and high temperature
regimes [ 122 ] . We will find that at sufficiently low temperatures the
overlap associated with the FSM is, as expected, better than the RSM.
However for higher temperatures it is the RSM which has the better
overlap. In section 14.2 we will then proceed to estimate the anharmonic
FEDs [ 123 ] in the low and high temperature regimes. We will find that
for sufficiently low temperatures, the FSM does not require any extended
sampling in order to arrive at an estimate of the FED which is free of
systematic errors. This is in contrast to the RSM, which will, in the
most general case, require extended sampling. On the transition to
higher temperatures we find that both the RSM and the FSM require
extended sampling in order to overcome the overlap problem. We will then
end this section with discussion of the relative efficiencies of the two
methods.

#### 14.1 Overlap

To start off with, let us consider the behaviour of the overlap problem
for the FSM and the RSM in the low temperature regime. In considering
the issue of the overlap, we recall (see section 7 ) that the difference
in the free energies of two phases manifests itself as an asymmetry
(about the origin) in the position at which the two phase-constrained
distributions @xmath and @xmath intersect. For systems which have a
small FED (as is the case for the systems employed here), this asymmetry
will be ever so slight. An illustration of this for the RSM is shown in
figure 12 . In characterising the overlap, the observed (approximate)
symmetry allows us to focus our attention on the behaviour on a single
phase-constrained distribution. The amount of overlap (which can, in an
approximate way, be measured by the peak to peak distance) may then be
gauged by measuring the distance of the peak of this phase-constrained
distribution from the origin. The smaller this distance is, the greater
will be the overlap between the two distributions [ 124 ] .

Figure 13 shows the scaling of the first two cumulants of @xmath with
temperature. In accordance with Eq. \thechapter .29 , linear scaling is
observed for the FSM. Moreover we see from figure 13 (b) that the
corresponding cumulants for the RSM tend to the limiting values as
predicted by Eq. \thechapter .31 and Eq. \thechapter .32 . Figure 14
investigates the overlap for a range of temperature by looking at the
phase-constrained distribution @xmath . One immediately observes that as
the temperature is reduced, the overlap of the FSM becomes considerably
better than that of the RSM. Whereas in the case of the RSM @xmath tends
to a limiting (stationary) form (see figures 14 (a) and (b)) [ 125 ] ,
in the case of FSM the corresponding distribution tends to the ideal
limit of the delta function (Eq. \thechapter .1 ), which in this case is
centred on the origin [ 126 ] .

To understand the low-temperature behaviour of the FSM we note that the
method only probes the anharmonic effects (see Eq. \thechapter .20 )
since by construction the harmonic contributions to the configurational
energy cancel out in the two phases. Since these anharmonic
contributions vanish as the harmonic limit is approached, the observed
behaviour is in accordance with what is expected. The RSM, on the other
hand, does not ’fold out’ the harmonic contributions. For this
representation @xmath must assume a constant value ( @xmath ) in the
harmonic limit. From the overlap identity (Eq. \thechapter .21 ) we see
that for this to be the case the ratio @xmath must approach a stationary
value in this limit. One way for this to be achieved is for both @xmath
and @xmath to tend to stationary non-singular distributions. This is
precisely what is observed.

So far we have analysed the behaviour of the FSM in the limit @xmath
limit. Let us now discuss the behaviour of the transformation as the
temperature is raised ( so as to make the anharmonic effects more
prominent). From figure 14 (c) and (d) we see that, though at low
temperatures the overlap of the FSM is better than that of the RSM, at
high temperatures the situation is reversed; the overlap of the RSM is
better than that of the FSM. This can be understood by first noting that
the FSM is a global transformation. That is, whereas for the RSM a
single particle perturbation in phase @xmath corresponds to a single
particle perturbation in phase @xmath (Eq. \thechapter .4 ), in the case
of the FSM a single particle perturbation of phase @xmath manifests
itself as a global perturbation in which all the particles of @xmath are
perturbed (Eq. \thechapter .14 , Eq. \thechapter .16 ). Therefore the
anharmonic corrections to the energy induced by the exploration of a
particle of phase @xmath into the anharmonic regions of the
configurational energy @xmath will, under the operation of the FSM,
propagate on a global level in phase @xmath . In contrast these
anharmonic effects only propagate on a local level under the operation
of the RSM. As a result one finds that in the highly anharmonic regimes
@xmath will be considerably amplified for the FSM as compared to the
RSM. In addition to this one finds that for the RSM which we have
employed (see section 11 ), intra-planar correlations are preserved [ 34
] on the transition from one phase to the other, a fact which is true
even in the anharmonic regime. The FSM, on the other hand, will preserve
little correlations in the anharmonic limit, since the anharmonic
effects effectively contaminate the transformation. The net result of
these two effects is that the RSM eventually becomes more efficient than
the FSM on the transition to sufficiently high temperatures.

An important point to note is that the deviations seen in the
distributions of the FSM from the ideal limit (Eq. \thechapter .1 ),
obtained on increasing the temperatures, are not due to the increasing
prominence of the intrinsic anharmonic effects but are instead due to
the inefficiency of the representation [ 65 ] . To see this we note that
in the harmonic limit the FSM maps configurations of phase @xmath onto
configurations of phase @xmath which are of the same effective
temperature. On increasing the temperature the contamination of the FSM
transformation by anharmonic effects results in configurations of phase
@xmath being mapped onto configurations of @xmath which are effectively
hotter than the typical configurations of phase @xmath . As a result the
anharmonic corrections to the total excitation energy will be amplified
, under the operation of the FSM, over those corrections that are
intrinsically present in phase @xmath at that temperature, so that
@xmath is not truly representative of the intrinsic anharmonic effects.
Therefore, based on an observation of @xmath , one may naively conclude
that the anharmonic effects are greater than they really are. This idea
is supported by figure 14 (b) where, despite the fact that at @xmath
@xmath has assumed its low temperature (harmonic) limiting form for the
RSM (see also figure 13 (b)), the corresponding distribution for the FSM
exhibits a significant departure from the ideal limit (Eq. \thechapter
.1 ). In fact from figure 13 (a) we see that departure (for the FSM)
from the linear scaling predictions (Eq. \thechapter .29 ) of leading
order anharmonic perturbation theory are observed at a temperature two
orders of magnitude lower than that for which departures from the
harmonic predictions are observed for the RSM. From figure 16 we see
that the anharmonic contributions to the FED are smaller than what one
would expect based on the observation of @xmath in figure 14 .

#### 14.2 Estimating the FEDs

Figure 15 shows the full @xmath probability distributions for the FSM-PS
method (see section 8.7 ); no form of extended sampling was employed
here. It is clear that on the transition to sufficiently low
temperatures, the FSM-PS method is no longer plagued with being
constrained to the phase which it is initiated in [ 127 ] signalling the
absence of an overlap problem since the full (effective) configuration
space associated with both the phases is visited. The overlap problem
(for this estimator at these temperatures) is effectively cured.

Figure 16 shows the estimates of the anharmonic contributions to the
temperature-scaled FED for the range of temperatures shown in Figure 15
. The significant feature is the agreement of the three estimators (the
FSM-EP estimator Eq. \thechapter .28 , the FSM cumulant approximation
Eq. \thechapter .30 , and the FSM-PS estimator Eq. \thechapter .78 ) at
low temperatures and the disagreement between them at high temperatures.
The FSM-PS estimator indicates that the anharmonic contributions are
unresolvably small throughout. This conclusion is consistent with what
one would expect based on the extrapolation of FSM-PS measurements at
higher temperatures (see figure 17 ). Moreover since the FSM-PS method
visits (for the range of temperatures investigated in figure 16 ) the
regions of the effective configuration spaces associated with both the
phases (as is clear from figure 15 ) we expect that it should be free of
systematic errors. Accordingly we will use the results of the FSM-PS
estimator as the benchmark (albeit a rather uninteresting one) for the
other methods.

Let us start by discussing the low temperature limit. The ability of the
FSM in overcoming the overlap problem (on the transition to sufficiently
low temperatures) for all the estimators is clearly evident from figure
16 . However contrary to initial expectations this does not mean that
the task of resolving the anharmonic contributions to FED becomes any
easier. To see this we note that for the FSM @xmath and @xmath (see Eq.
\thechapter .29 ), with all higher orders vanishing at a higher rate. As
a result the error in ones estimate of the mean ( @xmath ) of @xmath is
proportional to @xmath , since this quantity itself is proportional to
the standard deviation @xmath of @xmath . The fact that @xmath decays
faster, with decreasing temperature, than does its error is indicative
of a signal to noise problem that is present on the transition to lower
temperatures. That is, in the region where the overlap problem is
overcome the (small) anharmonic contributions are entirely masked by the
residual noise in the transformation in a way which is not cured by
going to (still) lower temperatures.

As the temperature is increased what is observed in figure 16 is the
eventual departure of the estimates of both the cumulant approximation
and the FSM-EP methods from the estimates of the FSM-PS method. The
first estimator to depart from the benchmark line is the cumulant
approximation (Eq. \thechapter .30 ), signalling the increasing
importance of the higher order cumulants in the expansion of Eq.
\thechapter .28 [ 129 ] . Upon inclusion of all the cumulants (which is
simply done by estimating @xmath via the FSM-EP method, Eq. \thechapter
.24 ) one does indeed estimate the quantity @xmath correctly, since the
results of the FSM-EP method and the FSM-PS method coincide. On
increasing the temperature further, the estimates of @xmath via the
FSM-EP method also begin to depart from those of the FSM-PS method. The
reason is that now systematic errors are arising from the fact that
@xmath and @xmath do not completely overlap. This is clearly the case in
figure 15 (d). We note that unlike the cumulant approximation, which
underestimates the FED [ 129 ] , the FSM-EP method overestimates the
desired quantity when systematic errors begin to set in [ 130 ] .

The decreasing amount of overlap between @xmath and @xmath obtained on
increasing the temperature means that eventually even the FSM-PS
estimators will not be free of systematic errors without the use of some
form of extended sampling strategy (see section 8 and chapter
\thechapter ). For the systems investigated here, the maximum
temperature at which the FSM-PS method could successfully be implemented
without the use of extended sampling was T=0.2 (figure 15 ). Beyond this
extended sampling was required. Figure 17 shows the temperature-scaled
anharmonic FEDs obtained with the MUCA [ 131 ] sampling distribution and
shows clear agreement between the RSM and FSM methods. The results are
consistent with those of [ 35 ] . In particular the anharmonic
contributions act so as to favour the hcp (B) phase.

The MUCA extended sampling strategy (and indeed all the other extended
sampling strategies, to be discussed in chapter \thechapter ) allows one
to tackle the overlap problem irrespective of the representation. The
choice of representation then manifests itself in the residual
statistical errors in the estimate of the FED. In comparing these
statistical errors for the FSM and the RSM, we first note from figure 14
that for all temperatures of up to T=0.5, the FSM will require a
narrower region of @xmath space to be reweighted (as compared to the
RSM) within the MUCA approach [ 132 ] . This has two consequences for
the simulation. Firstly this will translate to a smaller statistical
error in the estimate of @xmath for a given number of Monte Carlo steps
since the system will fluctuate over a narrower region of @xmath space.
Secondly this will correspond to a reduced computational effort in the
task of constructing the multicanonical weights. However on top of this
one must also give consideration to the differences in computational
effort (i.e. the time for each Monte Carlo step) between the FSM and
RSM. In order to understand this latter issue more fully, we note that
in a MUCA simulation the macrovariable @xmath will have to be evaluated
for each Monte Carlo step. Suppose the simulation is in phase @xmath and
suppose that one employs a short ranged potential. Then a single
particle perturbation will, for the RSM, require a local re-evaluation
of the configurational energies of both @xmath and @xmath in order to
compute the new value of @xmath . The number of computational steps
needed for such a task is @xmath . For the FSM the re-evaluation of
@xmath will also be local in nature. However since the FSM (see Eq.
\thechapter .16 ) induces a global rearrangement of the atoms of phase
@xmath (so that the calculation of @xmath is an @xmath calculation) the
number of computational steps needed for the reevaluation of @xmath for
the FSM will be @xmath . This significant advantage that the RSM holds
over the FSM vanishes when long ranged potentials are employed, in which
case both methods will involve @xmath calculations.

### 15 Summary

In this section we have clearly illustrated both analytically and
numerically the dependence of the overlap on the representation. We have
shown that adopting a fourier representation of the displacements allows
one to cure the overlap problem at sufficiently low temperatures. This
is in sharp contrast to the RSM, for which the overlap of the two
distributions tends to a limiting form. The main benefit of the FSM over
the RSM is that for sufficiently low temperatures, extended
sampling will not be needed in order to arrive at an estimate of the FED
which is free of systematic errors.

However our expectations of being able to estimate the FED via the FSM
with increasing ease are not fulfilled due to the presence of a signal
to noise problem which gets worse as the temperature decreases, even
though the overlap between the phase-constrained distributions improves
. Furthermore by being a transformation which is global in nature, one
must expend a considerably greater amount of computational effort in
dealing with the FSM than is required for the RSM.

In tackling the problem of estimating FEDs in the most general cases,
one must not only give consideration to the choice of representation but
one must also give consideration to the choices of estimators and the
choices of extended sampling strategies. The importance of the choice of
estimator has already been illustrated to a certain extent in figure 16
, where we have seen that the PS estimator (Eq. \thechapter .78 ) is
better than that of the EP method (Eq. \thechapter .28 ). In chapter
\thechapter we will address these issues in greater depth. In chapter
\thechapter we will then proceed to discuss the use of extended sampling
strategies in the task of estimating FEDs.

## Chapter \thechapter Estimators

### 16 Introduction

Imperative to the understanding of the FED problem is the appreciation
of the distinction that must be made between statistical and systematic
errors (see section 3.1 ). Whereas statistical errors may be reduced to
a desired level simply by running the simulation for a sufficient
duration of time, systematic errors in general can not be controlled in
this way. In the context of FED calculations the origin of these
systematic errors is the partial overlap [ 2 ] between @xmath and @xmath
(see section 7 ). As we have discussed in chapter \thechapter these
systematic errors may be minimised through efficient choices of PM (that
is choices of the global configuration space displacement @xmath and
representation @xmath ). In the case where it is possible to construct
an efficient PM so as to yield at least some overlap between @xmath and
@xmath , it is possible to eliminate the systematic errors that arise in
ones estimate of @xmath by constructing an appropriate estimator. In
this chapter we will investigate this issue in two stages. In the first
part we will show how one may restrict the regions of @xmath space which
contribute to the relevant expectations (which appear in the estimators)
so as to yield an estimate of @xmath which is free of systematic errors.
We will then show that an alternative strategy to this is that of
employing estimators which are unrestricted, in the sense just
described, and which are instead designed to have their most significant
contributions originating from those regions of (effective)
configuration space over which both @xmath and @xmath overlap [ 133 ] .
This latter idea has been studied (and understood) in a different way in
[ 46 ] , [ 53 ] , [ 54 ] , [ 134 ] - [ 137 ] . At the heart of our
insight is the appreciation that only within the region of overlap does
the estimator in Eq. \thechapter .22 yield an estimate which is free of
systematic errors. Since ultimately all estimators may be derived
directly from Eq. \thechapter .22 , it follows that the successful
estimators will be those which pool together the estimates of @xmath
made by Eq. \thechapter .22 within the region of overlap.

Consider figure 18 . In the most general case, there will be two types
of overlap that one encounters when one attempts to estimates the FED.
In the first case (see figure 18 (a)) the regions of (effective)
configuration space of one of the phases forms a subset of that of the
other phase. This type of situation typically arises in the calculation
of the chemical potential (via the insertion method), where one attempts
to determine the FED between an N particle system (A) and an N+1 (B)
particle system (see [ 53 ] for an excellent discussion). In this case
it has been argued [ 53 ] that the EP estimator (Eq. \thechapter .26 ,
in which A, the N particle system, is the parent phase) will yield an
estimate of the FED which is free of systematic errors. The basic idea
is that a sampling experiment performed in phase A will capture all the
regions of (effective) configuration space relevant to phase B [ 138 ] .

The latter type of overlap (see figure 18 (b)) appears when one attempts
to determine the FED between different phases of a single system (see
section 7 ). In regards to the estimator we have already seen in section
14 (in particular figure 16 ) how the estimator of the EP method (Eq.
\thechapter .28 ) fails in this case, due to the fact that a single
phase-constrained distribution fails to capture all the important
regions of (effective) configuration space which contribute to the FED.
One must instead employ estimators which involve the sampling of the
regions of (effective) configuration space associated with both phases [
46 ] .

These estimators, which involve the simulation of both phases, may be
broadly categorised into two groups: the phase-constrained estimators
and the phase-switching (PS) estimators. The phase-constrained
estimators involve expectations with respect to sampling distributions
confined to the phase in which they are initiated, whereas the
phase-switching estimator involves a sampling distribution which
actually switches between the phases. As we will now show, one must in
general take explicit steps so as to ensure that the phase-constrained
estimators are free of systematic errors. We will also show that, for a
particular subgroup of the phase-constrained estimators, no such steps
are needed since these estimators are, by construction, free of
systematic errors even in the case of partial overlap. In the case of
the PS estimator, we show that for partial overlap the method can be
guaranteed to be free of systematic errors simply by appropriately
weighting the two phases (in a way as prescribed in the simulated
tempering method [ 49 ] - [ 52 ] , section 8.6 ) so as to increase the
probability with which the simulation visits the phase with the smaller
partition function (greater free energy). In order to keep the
discussion as general as possible we will formulate our arguments within
the context of the FG method.

### 17 Phase-constrained estimators

#### 17.1 Eliminating systematic errors via restricted expectations

Suppose that @xmath and @xmath partially overlap [ 2 ] in the manner
shown in figure 19 . Using the same arguments employed in section 7 , it
follows that the point at which they intersect is given by:

  -- -------- -- -------------------
     @xmath      ( \thechapter .1)
  -- -------- -- -------------------

We will now proceed to show how, in the case of partial overlap, the FED
may be estimated from any estimator (see Eq. \thechapter .105 ) in a way
which is free of systematic errors, merely by restricting the range of
@xmath space from which the non-negligible contributions originate. Let
us first consider the overlap identity (Eq. \thechapter .101 ).
Multiplying by an arbitrary non-zero function @xmath and then
integrating both sides over the restricted range @xmath we arrive at a
formula which we call the restricted dual phase perturbation (RDP)
formula:

  -- -------- -- -------------------
     @xmath      ( \thechapter .2)
  -- -------- -- -------------------

or

  -- -------- -- -------------------
     @xmath      ( \thechapter .3)
  -- -------- -- -------------------

where:

  -- -------- -- -------------------
     @xmath      ( \thechapter .4)
  -- -------- -- -------------------

Eq. \thechapter .3 may then be used to estimate @xmath via:

  -- -------- -- -------------------
     @xmath      ( \thechapter .5)
  -- -------- -- -------------------

where:

  -- -------- -- -------------------
     @xmath      ( \thechapter .6)
  -- -------- -- -------------------

@xmath and @xmath are in principle arbitrary. In practise, however, they
are not if one is to arrive at an estimator which will yield an estimate
of @xmath which is free of systematic errors. In order to obtain the
necessary insights it is instructive to derive Eq. \thechapter .5
directly from the estimator for @xmath associated with the overlap
identity (Eq. \thechapter .101 ) itself:

  -- -------- -- -------------------
     @xmath      ( \thechapter .7)
  -- -------- -- -------------------

where @xmath is an estimate for @xmath . Rearranging Eq. \thechapter .7
and multiply both sides by
@xmath one obtains:

  -- -------- -- -------------------
     @xmath      ( \thechapter .8)
  -- -------- -- -------------------

Summing both sides over all the bins and rearranging leads to Eq.
\thechapter .5 .

The necessary restrictions that are needed become apparent when one
notices that, implicit in this derivation, is the assumption that the
histograms @xmath and @xmath of the bins @xmath over which the
summations are performed are simultaneously non-zero. This requirement
stems from Eq. \thechapter .7 , which itself assumes that both @xmath
and @xmath are non-zero. However the regions of (effective)
configuration space over which the estimators @xmath and @xmath of the
phase-constrained distributions are both non-zero is precisely what we
defined (in section 7 ) to be the region of overlap. In other words the
widest choice of @xmath should correspond directly to the region over
which the estimators @xmath and @xmath of the phase-constrained
distributions overlap (i.e. the shaded region in figure 19 ).

The key point is that within the overlapping region each bin has,
associated with it, an estimate of @xmath given by Eq. \thechapter .7 .
One may then pool these estimates together in different ways; the result
is the array of different estimators whose form is most generally given
by Eq. \thechapter .3 . This idea is illustrated in figure 20 . As we
will show in the next section the acceptance ratio (AR) and fermi
function (FF) are prime examples of estimators which pool the estimates
in this way and which do not require any restrictions (see Eq.
\thechapter .4 ) to be imposed. The phase switch (PS) method is another
such method, which accounts for all the regions of (effective)
configuration space which contribute non-negligibly to the FED by
actually switching phases (in the case of zero equilibration, see
section 8.7 ) or more generally switching between processes (in the case
of the arbitrary equilibration FG method, see section 8.9 ). The PS
method will be discussed later in section 18 .

Some flexibility does exist in setting the range @xmath . Specifically
the range @xmath can be widened so as to include @xmath macrostates
originating from outside the region of overlap, provided that they
contribute negligibly to the relevant estimators. Since from the overlap
identity (Eq. \thechapter .101 ) we know that:

  -- -------- -- -------------------
     @xmath      ( \thechapter .9)
  -- -------- -- -------------------

it follows that regions which are negligible to the estimator of the
numerator of Eq. \thechapter .5 are also negligible to the denominator,
and vice-versa, which makes them easily identifiable. For example, in
the case where @xmath , these regions are those over which @xmath is
negligible. This includes all the parts of @xmath which do not overlap
with @xmath , which means that in the task of estimating @xmath from Eq.
\thechapter .5 one may remove the upper restriction @xmath (when @xmath
). The sole purpose of this is merely to simplify the evaluation of the
relevant estimators (see section 19 ). The important point is that the
regions which do contribute non-negligibly to these summations are
limited to the regions of overlap.

#### 17.2 Eliminating systematic errors via @xmath

We have seen in the previous section how one may construct an estimator
based on a given @xmath which is free of systematic errors merely by
restricting the expectation, so as to ensure that the @xmath macrostates
which contribute non-negligibly to the estimator come from within the
overlapping region (see figure 19 , figure 20 ). Since the most
significant contributions to the numerator and the denominator of Eq.
\thechapter .3 come from the same regions of (effective) configuration
space (by virtue of Eq. \thechapter .9 ) we see that an alternative
strategy is to construct a @xmath so as to ensure that the
non-negligible contributions originate from the regions of @xmath space
over which @xmath and @xmath overlap. In this case the restrictions
imposed on the expectations in Eq. \thechapter .3 may be lifted.

In order to facilitate our analysis let us define a set of weight
functions (not to be confused with MUCA weights) for the estimator of
@xmath . In the case of the DP estimators (Eq. \thechapter .105 ) let us
define a weight function @xmath as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .10)
  -- -------- -- --------------------

and a weight function @xmath as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .11)
  -- -------- -- --------------------

These weight functions essentially measure the contribution of a
macrostate @xmath in the numerators and the denominator of the estimator
of Eq. \thechapter .105 . Since from the overlap identity (Eq.
\thechapter .101 ) we know that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .12)
  -- -------- -- --------------------

we will only concentrate on @xmath in the following analysis of the DP
estimators.

In the case of the EP estimator, which is obtained by setting @xmath in
Eq. \thechapter .105 , we will depart from the definitions given in Eq.
\thechapter .10 and Eq. \thechapter .11 and instead define the weights
in accordance to the contributions of macrostates to the numerator and
denominator of the corresponding estimator:

  -- -------- -- --------------------
     @xmath      ( \thechapter .13)
  -- -------- -- --------------------

That is we define the weights as:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .14)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

and:

  -- -------- -- --------------------
     @xmath      ( \thechapter .15)
  -- -------- -- --------------------

In this case one must separately analyse @xmath and @xmath since they
are no longer proportional, as is the case in Eq. \thechapter .12 .

Let us now motivate the construction of estimators in which no
restrictions of the form of Eq. \thechapter .4 are needed. From Eq.
\thechapter .9 it is clear that the choice @xmath in Eq. \thechapter
.105 results in the most significant contributions originating from the
regions where @xmath is most significant. On the other hand choosing
@xmath (this merely corresponds to performing the EP method in the other
phase) results in the contributing regions being those over which @xmath
is most significant. For reasons mentioned in section 17.1 , both these
choices can only be guaranteed to yield estimates of @xmath which are
free of systematic errors (in the case of partial overlap) by imposing
the restrictions mentioned in the previous section. One may, then,
naively expect that the construction of an interpolation @xmath , which
leads to the following formula:

  -- -------- -- --------------------
     @xmath      ( \thechapter .16)
  -- -------- -- --------------------

to lead to a more useful estimator of @xmath . This is not in fact the
case. To see this we first notice (from Eq. \thechapter .10 ) that the
weight function @xmath is given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .17)
  -- -------- -- --------------------

so that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .18)
  -- -------- -- --------------------

It is immediately apparent from Eq. \thechapter .18 and figure 19 that
the regions of @xmath space which contribute significantly are those
regions of @xmath space spanned by both @xmath and @xmath , and is not
simply limited to the regions of @xmath space over which the two
phase-constrained distributions overlap, as one might originally expect.
Therefore Eq. \thechapter .16 has not got the desired property that we
are looking for, namely the property of having the non-negligible
contributions coming solely from the region of overlap.

Now let us examine the choice of @xmath , which leads to the acceptance
ratio (AR) formula (Eq. \thechapter .104 ). As we will now see, the AR
formula is a prime example of an estimator which does not require
restrictions to be imposed on its corresponding estimator in order to
guarantee that it is free of systematic errors. In order to see this,
let us establish the AR formula in a slightly more general way. To do
this we first re-write the overlap identity (Eq. \thechapter .101 ) as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .19)
  -- -------- -- --------------------

where C is an arbitrary constant. Integrating both sides over @xmath and
rearranging gives:

  -- -------- -- --------------------
     @xmath      ( \thechapter .20)
  -- -------- -- --------------------

Following our earlier definitions (see Eq. \thechapter .10 and Eq.
\thechapter .11 ) we define the weight functions as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .21)
  -- -------- -- --------------------

and @xmath as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .22)
  -- -------- -- --------------------

The interrelation between the weights may now be written more generally
as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .23)
  -- -------- -- --------------------

Once again it suffices to focus ones attention on only one of these
weight function (which in the following analysis will be @xmath ).

The constant C in Eq. \thechapter .20 is important in that it directly
affects the statistical and systematic errors associated with the
corresponding estimator. We will return to the optimal choice of C
later. If one substitutes @xmath into Eq. \thechapter .20 one obtains a
generalisation of the AR formula, Eq. \thechapter .104 :

  -- -------- -- --------------------
     @xmath      ( \thechapter .24)
  -- -------- -- --------------------

whose weight function @xmath is given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .25)
  -- -------- -- --------------------

It then follows that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .26)
  -- -------- -- --------------------

Referring to figure 21 (a) and (b), it is clear from Eq. \thechapter .26
that if C lies within the overlapping region, then the regions which
contribute most significantly to the estimators of the expectations
appearing in the numerator and denominator of Eq. \thechapter .24 are
those over which @xmath and @xmath overlap. In the case where the FEDs
are small, it suffices to use the original AR formula Eq. \thechapter
.104 , since @xmath .

Therefore the AR method is only free of systematic errors for choices of
C lying within the overlapping regions. If the value of C lies outside
this region, it is not hard to see that the regions of (effective)
configuration space which contribute the most significantly to Eq.
\thechapter .24 will no longer be contained entirely within the region
of overlap. As a result restrictions will have to be imposed on the
corresponding estimator (for reasons mentioned in section 17.1 ) in
order to guarantee that the associated estimate of @xmath is free of
systematic errors. We will now discuss another estimator which, like the
AR method, has its most significant contributions originating from the
region of overlap and which is unique in that it is the estimator for
which the statistical variance is a minimum.

##### 17.2.1 Minimising the statistical errors : Bennett’s fermi
function estimator

The AR formula is but one of a family of estimators for which the
regions which contribute most significantly correspond to the
overlapping regions. The question that one may now proceed to ask is
which, out of the family of these estimators, is the one whose
corresponding estimator for @xmath is of minimum statistical variance [
139 ] . The task of finding a minimum-variance estimator has been
tackled by Bennett [ 46 ] . We will now present his estimator within the
more general context of the fast growth (FG) method. Consider the
following choice for G:

  -- -------- -- --------------------
     @xmath      ( \thechapter .27)
  -- -------- -- --------------------

where f is the fermi function. Substitution of Eq. \thechapter .27 into
Eq. \thechapter .20 yields [ 46 ] , [ 54 ] , [ 140 ] - [ 142 ] , [ 143 ]
:

  -- -------- -- --------------------
     @xmath      ( \thechapter .28)
  -- -------- -- --------------------

where we have used the fact that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .29)
  -- -------- -- --------------------

Bennett showed that the choice of G and of C which lead to a minimum
variance estimator of @xmath is that of Eq. \thechapter .28 in which C
is set to be:

  -- -------- -- --------------------
     @xmath      ( \thechapter .30)
  -- -------- -- --------------------

and where @xmath are the number of independent data samples obtained in
phase @xmath . Eq. \thechapter .30 has the following simple physical
interpretation. When @xmath , then @xmath so that (with respect to the
case @xmath ) increasing amounts of the tail of @xmath are included in
the contributions that come from the overlap region, whilst a smaller
proportion of the tail of @xmath is included. The reason for this is
that the statistics of the tail of @xmath will be a lot worse than that
of @xmath , and therefore it makes sense to take contributions from a
larger proportion of its tail and a smaller proportion of the tail of
@xmath . When @xmath the opposite is true. That is since C is now less
than @xmath , a larger proportion of the tail of @xmath is taken into
account, whereas a smaller proportion of the tail of @xmath contributes,
thus balancing the fluctuations contributed by @xmath and @xmath in the
estimate of the FED. This is illustrated in figure 22

In order to estimate the FED from Eq. \thechapter .28 and Eq.
\thechapter .30 Bennett’s prescription involves iteratively solving the
set of equations:

  -- -------- -- --------------------
     @xmath      ( \thechapter .31)
  -- -------- -- --------------------

and:

  -- -------- -- --------------------
     @xmath      ( \thechapter .32)
  -- -------- -- --------------------

where @xmath is the estimate for @xmath (see Eq. \thechapter .1 ). That
is one starts off with an arbitrary estimate of @xmath , say unity. One
then uses Eq. \thechapter .32 to calculate a value of C, which one then
substitutes into Eq. \thechapter .31 . From this one obtains a new
estimate @xmath which one then substitutes back into Eq. \thechapter .32
to get yet another value of C. This value of C is then fed back into Eq.
\thechapter .31 and one continues this procedure until convergence is
obtained. That is the process is carried out in an iterative fashion
until the value of @xmath obtained from Eq. \thechapter .31 for a
particular value of C also agrees with Eq. \thechapter .32 . At this
point @xmath yields (in the case of partial overlap between @xmath and
@xmath ) a minimum variance unbiased estimate of the true value of
@xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .33)
  -- -------- -- --------------------

Like the generalised AR formula (Eq. \thechapter .24 ), no restrictions
are required (when C does not differ too greatly from @xmath ) in order
to ensure that the associated estimate is free of systematic errors (in
the case of partial overlap). To see this we first note that the weight
function @xmath is given by:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .34)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

where in going from the first to the second line we have employed the
overlap identity (Eq. \thechapter .101 ).

From Eq. \thechapter .34 we see that (provided C does not differ too
greatly from @xmath , so that @xmath ) the regions of @xmath space for
which the weight function @xmath is most significant are those regions
over which @xmath and @xmath overlap (the shaded region of figure 19 ).
Therefore like the generalised AR formula (Eq. \thechapter .24 ), the
fermi function (FF) formula (Eq. \thechapter .28 ) is free of systematic
errors (provided C does not differ too greatly from @xmath ).

Let us now analyse the case where C differs significantly from @xmath .
If @xmath is considerably different from unity then it follows that the
weights @xmath may be approximated by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .35)
  -- -------- -- --------------------

Therefore if C is too large, then the @xmath macrostates contributing to
the estimators of Eq. \thechapter .28 correspond to the regions of
@xmath space for which @xmath is significant. In this case the estimator
of Eq. \thechapter .28 will not yield an estimate free of systematic
errors, since the contributing regions no longer come from the regions
of overlap, and the necessary steps outlined in section 17.1 will need
to be taken. Likewise if C is too negative, then the important regions
will be those for which @xmath is significant, once again leading to
systematic errors. From this analysis we also see that in the limit of
the number of independent samples obtained in the two phases ( @xmath
and @xmath ) becoming very disparate, the Bennett prescription for
constructing the optimal C, given by Eq. \thechapter .30 , will lead to
systematic errors for the reasons that we have just mentioned. In his
paper [ 46 ] however, Bennett does advocate the use of an equal number
of independent samples in each phase, so that @xmath . This choice leads
to the contributions coming from the regions of overlap, resulting in
the estimator of Eq. \thechapter .28 yielding estimates which are free
of systematic errors. Further insight into Bennett’s approach (Eq.
\thechapter .28 and Eq. \thechapter .30 ) can be obtained by noticing
the links that exist between the method and the task of estimating the
overlap parameter @xmath (Eq. \thechapter .18 ). We refer the interested
reader to appendix \thechapter for the relevant discussion.

### 18 Phase switch estimator

In section 8.9 we saw how the PS method could be generalised so as to be
applicable within the framework of the (arbitrary equilibration) FG
method. In order for the method to work @xmath and @xmath should be
non-zero for the @xmath regions. It is only in this case that a
simulation initiated in either of the phases will be able to reach the
@xmath regions, from which it will have a non-negligible chance of
switching phases. In the general case where the FED differs
significantly from @xmath , the estimators @xmath and @xmath will lie to
one side of the axis (see figure 19 ). In this case one might find that
the @xmath regions are not visited, thus preventing the simulation from
switching phases. As we will now show, provided @xmath and @xmath
overlap , one may make a slight modification to the method so as to
allow it to switch phases. The basic idea is to weight the two phases so
as to increase the probability of the simulation visiting the phase with
smaller partition function (or larger free energy). We are essentially
performing the ST tempering for the case of two sub-ensembles, within
the more general context of the FG method.

We recap that in its most general form the @xmath distribution of the PS
method may be written as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .36)
  -- -------- -- --------------------

where:

  -- -------- -- --------------------
     @xmath      ( \thechapter .37)
  -- -------- -- --------------------

In the particular version employed in Eq. \thechapter .76 , we have
@xmath . In general, this quantity may be arbitrary. It then follows
that the acceptance probability for switching phases (strictly
processes) is given by:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .38)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

Since @xmath is unknown a-priori, it is convenient to factor it out of
the acceptance probability of Eq. \thechapter .38 . Therefore we
conveniently write @xmath as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .39)
  -- -------- -- --------------------

where @xmath are some arbitrary weights, which are known a-priori.
Substituting Eq. \thechapter .39 into Eq. \thechapter .38 we obtain:

  -- -------- -- --------------------
     @xmath      ( \thechapter .40)
  -- -------- -- --------------------

or:

  -- -------- -- --------------------
     @xmath      ( \thechapter .41)
  -- -------- -- --------------------

where:

  -- -------- -- --------------------
     @xmath      ( \thechapter .42)
  -- -------- -- --------------------

Running the argument in reverse it follows that if one adopts the PS
acceptance probability given in Eq. \thechapter .40 , then the absolute
probability of finding the simulation in the @xmath process is given by
Eq. \thechapter .39 . Therefore if one implements a FG-PS simulation in
which the phase (or process) switching probabilities are given by Eq.
\thechapter .41 then @xmath may be estimated via:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .43)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

where:

  -- -------- -- --------------------
     @xmath      ( \thechapter .44)
  -- -------- -- --------------------

Eq. \thechapter .43 merely expresses the fact that @xmath may be
estimated by the weighted ratio of the times spent in the two processes.
An alternative expression for @xmath may also be found which expresses
Eq. \thechapter .43 as an expectation over the macrostates @xmath . By
substituting Eq. \thechapter .37 into Eq. \thechapter .36 and by
appealing to the overlap identity (Eq. \thechapter .101 ) one finds
that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .45)
  -- -------- -- --------------------

Using Eq. \thechapter .37 and Eq. \thechapter .45 we see that Eq.
\thechapter .43 may instead be written as:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .46)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

or:

  -- -------- -- --------------------
     @xmath      ( \thechapter .47)
  -- -------- -- --------------------

The close resemblance of this estimator with that of the FF method (Eq.
\thechapter .28 ) is striking. However the estimator of Eq. \thechapter
.47 is markedly different from that of Eq. \thechapter .28 in one
respect. To see this consider the weight function for the numerator and
denominator of the estimator of Eq. \thechapter .47 :

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .48)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

and:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .49)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

We notice that whereas in the case of the DP estimators (Eq. \thechapter
.105 ) the weights @xmath and @xmath are directly proportional to each
other (see Eq. \thechapter .23 ), in the case of the PS estimator they
are not. For the DP methods, the contributions to the estimators of the
expectations appearing in the numerator and denominator come from the
same region of (effective) configuration space, though the sampling
distributions actually employed are different. In the case of the PS
method one employs the same sampling distribution for the two
expectations; though now the contributions to the two expectations come
from different regions of (effective) configuration space. Whereas the
DP methods can prevent the appearance of systematic errors by ensuring
that the non-negligible contributions come from the region of overlap,
the PS method avoids systematic errors by actually switching phases and
separately sampling each phase. As with the DP methods the correct
choice of C must be made in order for the PS method to work.

In order to address the choice of C we note that if the weights @xmath
and @xmath are the same for the two phases (so that C=0, Eq. \thechapter
.42 ), as is the case in the original PS formulation (see section 8.7
and section 8.9 ), and if the region over which the two
phase-constrained distributions overlap (see figure 19 ) is sufficiently
displaced from the origin, then it is clear that the PS sampling
distribution will not be able to successfully switch between the phases
in both directions . The way to remedy this is to choose a C which lies
within the region of overlap (see figure 19 ). In particular if one
chooses:

  -- -------- -- --------------------
     @xmath      ( \thechapter .50)
  -- -------- -- --------------------

then from Eq. \thechapter .43 one finds that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .51)
  -- -------- -- --------------------

so that the simulation spends an equal time in the two phases. By
setting C as prescribed in Eq. \thechapter .50 what one does is to
effectively bias the phase with the larger free energy, so as to
increase the probability with which the simulation visits it, as
compared to the case where C is set to unity.

### 19 Numerical results

We saw earlier in section 17.1 how any estimator can, in principle, be
modified by imposing appropriate ’restrictions’ so as to guarantee that
the resulting estimate of the FED is free of systematic errors when
there is some overlap between the estimators of the phase-constrained
distributions @xmath and @xmath . In this section we illustrate the
application of these restrictions in the case of the EP estimator and
compare the resulting statistical errors to those of the PS, AR, FF, and
EP methods.

In order compare the estimators we used the same simulation setup as
that used to obtain the data of figure 16 [ 144 ] . This represents the
rather uninteresting case of estimating @xmath when its assumes a value
of approximately unity. However it is useful for the reason that, since
the value of @xmath hardly changes for the range of temperatures
investigated, one may effectively probe the behaviour of the statistical
errors purely as a function of the overlap; the overlap being changed
simply by varying the temperature.

We start by recalling (see section 17.1 ) that in the case of @xmath the
form of Eq. \thechapter .3 may be simplified by discarding the upper
limit @xmath . Furthermore since @xmath (and therefore @xmath ) varies
negligibly over the range of conditions investigated here (see the
results of the FSM-PS method in figure 16 ), it is convenient to set
@xmath to @xmath . The result is that in the case of zero equilibration
Eq. \thechapter .3 reduces to:

  -- -------- -- --------------------
     @xmath      ( \thechapter .52)
  -- -------- -- --------------------

where:

  -- -------- -- --------------------
     @xmath      ( \thechapter .53)
  -- -------- -- --------------------

Using the fact that @xmath for the conditions investigated here, we see
that in the case of zero equilibration Eq. \thechapter .52 simplifies
to:

  -- -------- -- --------------------
     @xmath      ( \thechapter .54)
  -- -------- -- --------------------

We note that though Eq. \thechapter .54 does omit some of the region of
(effective) configuration space over which the two phase-constrained
distributions overlap, and therefore has greater statistical errors than
it would if all the regions of overlap were included, its advantage lies
in its simplicity and in the fact that Eq. \thechapter .54 may be used
for the spectrum of overlaps investigated without requiring one to
modify the restrictions as the overlap changes. It is for this reason
that we will use Eq. \thechapter .54 in our comparison of the
estimators.

Figure 23 shows the statistical errors (and the associated errors of the
errors) in the estimates of @xmath for the different estimators as a
function of the overlap parameter @xmath . In comparing the different
estimators we once again use the PS estimator as the benchmark. The
first observation that we make is that the AR and the FF estimators (the
latter of which is not shown in figure 23 since its results were
identical to those of the AR method) yielded statistical errors of
roughly the same size as those of the PS method for the whole range of
overlaps @xmath investigated. The EP estimator, on the other hand,
yielded a markedly different behaviour to that of the PS estimator.

For high values of overlap @xmath the EP estimator clearly yields
roughly the same statistical errors as those associated with the PS
estimator, whereas for low overlaps the errors of the EP estimator are
significantly greater. This may be qualitatively understood by noting
that for high overlaps, the distributions @xmath and @xmath sufficiently
overlap so as to ensure that the statistics of the regions which
contribute to both the numerator and denominator of Eq. \thechapter .28
are good under a sampling experiment constrained to a single phase. In
the case of the PS method, the statistics of the macrostates relevant to
the numerator and the denominator (Eq. \thechapter .78 ) are good
because the method (by switching phases) separately visits the regions
of (effective) configuration space associated with the two phases.

As @xmath decreases, the two phase-constrained distributions
increasingly separate (see figure 15 ) until the point is reached where
the main body of @xmath resides in the tail of @xmath . Under these
conditions even though systematic errors will not be present for the EP
estimator, the statistical errors will be greater than those associated
with the PS estimator since now the macrostates which contribute to the
numerator of Eq. \thechapter .28 will be visited with a small
probability, even though they contribute significantly to the estimate
of the FED. On the other hand the PS method visits macrostates with
probability in direct proportion to their contribution to the relevant
estimator (see Eq. \thechapter .76 ). As a result the statistical errors
of the PS estimator in this case will be lower than that of the EP
method. For overlaps even lower than this, the systematic errors will
begin to set in for the EP estimator, since the regions associated with
both phases will not be visited (as is required) by a simulation
constrained to a single phase, even though systematic errors will not be
present for the PS method (see also section 14 ). For the experiments
conducted here, it was found that systematic errors begin to set in for
the EP estimator for overlaps below @xmath .

In contrast to the EP estimator, the estimator of the REP formula (Eq.
\thechapter .54 ) does not suffer from any systematic errors for the
whole range of overlaps investigated, and its statistical errors were
only marginally greater than those of the AR, PS, and FF estimators. The
main reason for the increased statistical errors is because the
restriction, as imposed in Eq. \thechapter .54 , excludes some of the
overlapping region. That is some of the ’useful’ contributions are
unnecessarily discarded, resulting in slightly higher statistical
errors. It is also for this reason that at high overlaps the error of
the EP estimator falls below that of the REP estimator. We stress that
this property of the REP estimator is merely an artifact of the
particular version of the restrictions we employ in Eq. \thechapter .54
and that, upon inclusion of all the region of overlap, the statistical
errors should fall to roughly those of the other methods.

We finally note that in between the high and low overlap regimes, there
is a range of overlaps for which the EP method is free of systematic
errors and yet for which the statistical errors are greater than those
of the REP method, despite the fact that the particular version of the
REP that we use discards some of the data originating from the overlap
region. In figure 23 this roughly corresponds to the range @xmath . To
understand this we recall that as the overlap is increased, systematic
errors will disappear for the EP method on the onset of the main body of
@xmath being contained in the tail of @xmath . However in this regime
the statistical errors in the estimate of @xmath will be large since the
statistics of the regions over which @xmath is significant will be poor,
since this is contained in the tail of @xmath , thus offsetting any
gains it has over the REP method. This drawback of the EP method will
reduce as the overlap increases, until eventually the EP method becomes
more efficient than the version of the REP formula that we use.

From this numerical study it is clear that statistical considerations
lead one to the conclusion that in the case of partial overlap (see
figure 18 (b) and 19 ), estimators which involve the accumulation of
data from both the phases (e.g. the PS, AR, FF, and REP estimators) are
preferable to those that involve estimators which use the data acquired
from a single phase (namely EP method). Out of these estimators, the PS,
FF, and AR method are preferable (to the REP method) since one does not
need to expend the additional effort of restricting these estimators (as
is done in Eq. \thechapter .3 ). In comparing the AR, FF, and PS methods
we note that the PS method requires a single simulation to estimate
@xmath , whereas the FF and AR methods both require two separate
simulations. This latter property can be viewed as an advantage for both
groups of methods. On the one hand it affords the FF and the AR method
an avenue for parallelisation which is not available to the PS method,
since the two phase-constrained simulations may be performed
independently. On the other hand the ability of the PS method to
estimate @xmath from data extracted from a single simulation makes it,
in some sense, tidier. Another important difference is that the
adjustment of C, so as to yield an estimate of @xmath which is free of
systematic errors when there is partial overlap, have to be made before
the simulation is run in the case of the PS method. In the case of the
FF and AR methods, these adjustments are made after the simulation is
run, when one is trying to estimate @xmath from the data already
obtained; this may be easily automated. Since a-priori we do not know
where the region of overlap is, it is clear that in this case the FF and
AR methods have an advantage over the PS method. This, however, is not a
significant advantage since one may run two short simulations, one in
each phase, in order to roughly determine the point @xmath where @xmath
and @xmath intersect, thereby yielding an appropriate value of C (see
Eq. \thechapter .50 ).

### 20 Conclusion

We saw in section 7 that central to ones ability to estimate the FED is
the concept of overlap between the estimators @xmath and @xmath of the
phase-constrained distributions. For systems characterised by a
(effective) configuration space structure as shown in figure 18 (b),
successful estimators based on the sampling of the phase-constrained
distributions will have their most significant contributions originating
from the region of overlap [ 46 ] , [ 53 ] , [ 54 ] , [ 62 ] , [ 134 ] .
The way that we have realised this idea is by appreciating that all
estimators are based on the overlap identity (Eq. \thechapter .101 ).
Since the corresponding estimator of the overlap identity, Eq.
\thechapter .7 , is itself only valid within the region of overlap, we
see that estimators which are free of systematic errors can only have
their non-negligible contributions coming from this region. In a sense
one may think of these estimators as pooling together the estimates of
@xmath , as made by Eq. \thechapter .7 , from within the region of
overlap (see figure 20 ).

An alternative, and equally suitable, strategy to the phase-constrained
simulations is the PS strategy [ 1 ] in which one actually switches
between phases (or more generally between @xmath and @xmath processes in
the case of the arbitrary equilibration FG method). In this case the
method overcomes the problem presented by partial overlap by actually
switching between the phases (or processes) thereby sampling each phase
(process) separately.

Generally however, the scope for refinement of the estimator is limited.
In the absence of overlap one must resort to some sort of extended
sampling strategy [ 21 ] in which one engineers overlap by forcing the
simulation to visit regions of (effective) configuration space which it
would not otherwise sample (under the influence of the canonical
sampling distribution Eq. \thechapter .46 ). This, after the choice of
representation (chapter \thechapter ) and the choice of estimator (the
present chapter), forms the final part of the overall strategy of
tackling the overlap problem. This topic of discussion will form the
core of the next chapter.

## Chapter \thechapter Sampling Strategies

### 21 Introduction

In the case where one is unable to construct a PM which ensures some
overlap between the two phase-constrained distributions, one must
engineer overlap by refining the sampling strategy [ 21 ] . Studies
until now have focused on methods which fall into one of three broad
categories:

1.  They sample from some form of extended sampling distribution [ 21 ]
    and extract the FED via an appropriate reweighting scheme (see Eq.
    \thechapter .32 ). The extended sampling strategy involves the
    employment of a non-canonical sampling distribution so as to allow
    the simulation to visit wider regions of (effective) configuration
    space than it normally would under the canonical distributions. Such
    methods include Umbrella sampling [ 45 ] , [ 62 ] , [ 69 ] - [ 71 ]
    , PS method [ 1 ] , [ 34 ] - [ 37 ] , Simulated Tempering [ 49 ] - [
    52 ] , and the Weighted Histogram Analysis Method [ 54 ] , [ 91 ] -
    [ 94 ] .

2.  The fine tuning of the Fast Growth (FG) Method [ 55 ] , [ 72 ] - [
    75 ] , [ 108 ] . By making the incremental perturbations to the
    configurational energy (which constitute the work elements of the
    process, Eq. \thechapter .93 ) sufficiently small and by choosing
    sufficiently long equilibration times between these work elements,
    this method allows for the engineering of overlap between @xmath and
    @xmath (see section 24 ).

3.  They split the calculation of @xmath (Eq. \thechapter .13 ) into
    many small and separate FED calculations, between pairs of systems
    whose phase-constrained distributions overlap considerably better
    than that exhibited by the original pair of systems. These methods
    are generally referred to as the multistage (MS) methods [ 45 ] , [
    47 ] , [ 48 ] , [ 62 ] , [ 69 ] - [ 71 ] , [ 145 ] . In the limit of
    an infinite number of stages we arrive at the thermodynamic
    integration method (Eq. \thechapter .52 ) [ 23 ] - [ 25 ] .

In this chapter we will study these three strategies in the following
manner. First we will deal with point 1 by showing how the EP, AR, and
PS methods can be made to work by appealing to the MUCA extended
sampling strategy, as described in section 8.3 . The generalisation of
this strategy to the case of an arbitrary estimator (Eq. \thechapter .34
) is straightforward. Following this we construct a new way of
estimating the FED in which one employs a series of parallel simulations
(as is the case for the WHAM method, section 8.5 ). In certain limiting
cases this new method may be thought of as a realisation of both the FG
method (point 2) and the MS method (point 3). In the final part we
illustrate point 2 by applying the FG method to the model systems under
consideration here (see section 10 ), and show how it overcomes the
overlap problem by means of the fine-tuning of the relevant parameters
(see sections 8.8 and 24 ).

### 22 The Multicanonical strategy

The MUCA strategy [ 146 ] is a serial strategy (used in the case of
zero-equilibration FG simulations, @xmath = @xmath ) and involves, as we
saw in section 8.3 and 8.7 (see figure 6 ), the ’warping’ of the
canonical distribution so as to produce the necessary bridging
distribution [ 147 ] . Suppose that @xmath and @xmath denote the
canonical and the MUCA sampling distributions respectively. By accepting
moves via [ 148 ] :

  -- -------- -- -------------------
     @xmath      ( \thechapter .1)
  -- -------- -- -------------------

one realises:

  -- -------- -- -------------------
     @xmath      ( \thechapter .2)
  -- -------- -- -------------------

In order for @xmath to be flat over the desired regions of (effective)
configuration space one sets:

  -- -------- -- -------------------
     @xmath      ( \thechapter .3)
  -- -------- -- -------------------

In this section we will use the Wang-Landau [ 85 ] method to obtain the
weights (see section 8.3 ),and we will focus (our discussion) on three
estimators, the EP, AR, and PS estimators. Generalisation to the general
estimator of Eq. \thechapter .34 is straightforward.

#### 22.1 The Exponential Perturbation estimator

In section 8.2 we saw that it was the failure of a simulation
constrained to a single phase to account for the typical configurations
of both phases which ultimately led to the failure of the EP estimator,
even in the case of partial overlap. That is a sampling experiment
performed in phase A (via @xmath ) only samples the macrostates for
which the weights @xmath (see Eq. \thechapter .15 ) are non-negligible,
and fails to capture all the regions of (effective) configuration
space for which @xmath (see Eq. \thechapter .14 ) is non-negligible.

The way this problem is remedied (see [ 62 ] and section 8.3 ) is by
constructing a MUCA distribution @xmath which contains @xmath and @xmath
, so that the simulation visits the regions of (effective) configuration
space associated with both phases.

#### 22.2 The Acceptance Ratio estimator

Consider the use of the AR formula (Eq. \thechapter .31 ) in the absence
of overlap (see figure 5 ). In this case the MUCA strategy involves the
construction of two separate MUCA distributions @xmath and @xmath .
@xmath has to sample all the macrostates @xmath which contribute
non-negligibly to @xmath and @xmath has to sample those which contribute
non-negligibly to @xmath . As we saw in section 17.2 these correspond to
the regions of overlap. However, unlike the case where the estimators
@xmath and @xmath overlap, in the case where they do not overlap it is
not clear a priori where these regions are. To determine them one may
plot a graph of the weight function @xmath versus @xmath , as one
constructs the MUCA weights. Once the MUCA distributions @xmath and
@xmath are wide enough so as to contain all the regions over which
@xmath is non-negligible [ 149 ] one may then proceed to estimate @xmath
via:

  -- -------- -- -------------------
     @xmath      ( \thechapter .4)
  -- -------- -- -------------------

where:

  -- -------- -- -------------------
     @xmath      ( \thechapter .5)
  -- -------- -- -------------------

#### 22.3 The Phase Switch estimator

We saw in section 8.7 that for the PS method @xmath may be evaluated by
appeal to Eq. \thechapter .80 . This identity corresponds to a PS
simulation in which the probability of switching phases is given by Eq.
LABEL:eq:psacc . In this case it is clear that a PS will have a
non-negligible chance of being accepted only around the @xmath regions,
and therefore the MUCA sampling distribution should ensure that these
regions are visited by the simulation. In this case (see section 8.7 ) a
suitable MUCA distribution @xmath is one which is flat and which
contains both @xmath and @xmath [ 1 ] , [ 34 ] - [ 37 ] . If @xmath
denotes the number of data entries falling in bin @xmath under the
MUCA-PS sampling distribution @xmath , then the estimator for @xmath is
given by [ 150 ] :

  -- -------- -- -------------------
     @xmath      ( \thechapter .6)
  -- -------- -- -------------------

where:

  -- -------- -- -------------------
     @xmath      ( \thechapter .7)
  -- -------- -- -------------------

where @xmath is the associated MUCA weight function. Like the EP method,
the essential feature of the MUCA distribution @xmath is that it
contains both @xmath and @xmath , so as to account for all the regions
of (effective) configuration space over which the weights @xmath (see
Eq. \thechapter .48 ) and @xmath (see Eq. \thechapter .49 ) are
significant.

#### 22.4 Numerical results

Figure 24 shows an illustration of the application of the MUCA strategy
to the EP and the PS methods. Figure (b) shows the MUCA-PS distribution
which allows switching between the two phases. Figure (a) shows an
estimate of the canonical distribution @xmath as obtained from Eq.
\thechapter .7 . Since the two peaks in figure 24 (a) do not overlap,
one may (by virtue of Eq. \thechapter .76 ) think of these peaks as
effectively corresponding to (scaled versions of) the phase-constrained
distributions @xmath and @xmath . From this figure it is clear that the
(canonical) probability of the simulation visiting the @xmath regions is
negligible, and it is for this reason that one must sample from the
distribution shown in figure 24 (b) in order for the simulation to be
able to switch phases. Figure 24 (c) shows the MUCA distribution (for a
simulation initiated in phase B) that is required in order to ensure
that the EP estimator (Eq. \thechapter .26 ) is free of systematic
errors. Figure 24 (d) shows the convergence of the FED per particle as
the MUCA-EP distribution @xmath is extended so as to include increasing
proportions of the distribution associated with phase A . It is clear
that convergence is obtained in the limit of the MUCA distribution
@xmath containing the whole of @xmath .

The underlying feature of the form of the two MUCA distributions (Figure
24 (b) and (c)) is that they both ’contain’ the two canonical
distributions @xmath and @xmath . The difference lies in the way in
which they achieve this. In the case of the EP method, one employs a
single sampling distribution @xmath . In the case of the PS method, one
employs either @xmath or @xmath , depending on which phase the
simulation is in. This difference manifests itself in the range of
@xmath space over which multicanonicalisation must be performed. Whereas
in the case of PS method one explicitly constructs the weights (via Eq.
\thechapter .3 ) over the region of @xmath space lying between the
maxima of the two peaks, in the case of the EP method one performs
enhancement on the whole region between the maximum of the peak of the
parent phase (left hand peak in Figure 24 (a)) and the tail (and not
merely the peak) of the conjugate distribution (right hand peak of
Figure 24 (a)).

The reason for this can be understood as follows. When the PS method
‘switches phases’, it switches the sampling distributions to that which
would naturally lead to the exploration of the conjugate phase, even
without the aid of MUCA weights. As a consequence the role of
multicanonicalisation is merely to ensure that the @xmath regions are
accessible to simulations initiated in either of the phases. This
entails the peak-to-peak reweighting, which is evident in the MUCA
sampling distribution shown in figure 24 (b). In the case of the EP
method, the canonical sampling distribution (which in our case is @xmath
and is associated with the @xmath regions) is fixed and is ill-suited to
sampling of the regions of (effective) configuration space associated
with the conjugate phase (which in our case is phase A and corresponds
to the @xmath regions). As a consequence the MUCA weights must not only
take the simulation to the @xmath regions, but must also force the
simulation to visit the entire region of (effective) configuration
space relevant to the conjugate phase ( @xmath regions), since the
sampling distribution @xmath will typically try to direct the simulation
back in the direction of @xmath space associated with the parent phase (
@xmath regions). It is for this reason that the EP method requires the
additional construction of multicanonical weights (Eq. \thechapter .3 )
over the regions spanning from the maximum of the distribution of the
conjugate phase to its tail (compare figure 24 (b) with (c)).

This difference manifests itself in the MUCA weights. Figure 25 shows a
comparison of the MUCA weights for the two methods. It is clear that for
the @xmath regions, the MUCA weights for the EP and PS methods are the
same; the reason for this is that the canonical distribution associated
with these regions is @xmath for both methods. This property holds until
the @xmath regions. For the @xmath regions, the profiles of the two
weight functions diverge. In the case of the EP method the weights
decrease as @xmath increases, whereas the weights of the PS method
increase before levelling off. The reason for this is due to the
switching of the phases that takes place in the PS method. That is for
the @xmath regions, the probability of a switch of phases being accepted
will be unity. On switching phases the PS simulation will naturally
explore the @xmath regions, even without the aid of MUCA weights. The
presence of weights in the @xmath region is merely to guarantee that the
simulation is, once it has jumped from phase B to phase A, able to come
back at a later time from the @xmath regions to the @xmath regions, so
as to allow the simulation to switch back to phase B, which will then
allow it to naturally explore the @xmath regions once again. On the
other hand in the EP method the simulation will have to be forced to
visit the @xmath regions of (effective) configuration space; figure 25
clearly illustrates this.

On the transition to larger system sizes, the differences seen in the
MUCA distributions of the EP and PS methods become less noticeable. The
reason for this lies in the ways the means and the spreads of the peaks
scale with the system size. Since for each peak the mean will scale as
N, whereas the standard deviation (which measures the spread) scales as
@xmath , we see that the additional amount of @xmath space which will
require reweighting in the case of the EP method, over that of the PS
method, will become smaller as a fraction of the peak-to-peak distance
(which scales as N), on the transition to larger system sizes.

For finite systems there will also be a difference between the MUCA-EP
distributions associated with a simulation constrained to phase A as
compared with one constrained to phase B. In the case of a MUCA-EP
simulation constrained to phase A, the MUCA reweighting will now need to
be performed from the maximum of the right hand peak in figure 24 (a)
(which is now the peak corresponding to the parent phase) to the tail of
the left hand peak (which now corresponds to the conjugate phase). This
asymmetry that appears, due to the peak to tail reweighting, will
disappears on the transitions to larger system sizes, for the same
reasons cited above.

The intrinsic similarity of the MUCA distributions indicates a
connection between the MUCA weights of the different methods. For
example by appeal to the overlap identity (Eq. \thechapter .21 ) we see
that:

  -- -------- -- -------------------
     @xmath      ( \thechapter .8)
  -- -------- -- -------------------

Assuming that the two MUCA distributions are flat, one may infer that
for the regions over which the two distributions overlap [ 152 ] :

  -- -------- -- -------------------
     @xmath      ( \thechapter .9)
  -- -------- -- -------------------

Similarly the fact that the MUCA-EP and the MUCA-PS distributions are
approximately the same means that one may arrive at a similar identity
relating the MUCA weights of the two methods. To arrive at the result we
substitute Eq. \thechapter .37 into Eq. \thechapter .45 (in which the
weights @xmath have been set to be equal) so as to obtain the following
relation:

  -- -------- -- --------------------
     @xmath      ( \thechapter .10)
  -- -------- -- --------------------

Using Eq. \thechapter .2 one obtains:

  -- -------- -- --------------------
     @xmath      ( \thechapter .11)
  -- -------- -- --------------------

where @xmath and @xmath are some constants. This identity also naturally
follows from Eq. \thechapter .73 . Since these constants do not affect
the simulation in any way (since it is only the relative values of these
constants that matter) we may, without loss of generality, set these two
constants to zero. The inset in figure 25 shows a plot of @xmath and a
plot of @xmath , where both @xmath and @xmath have been estimated via
simulation. The clear agreement between the two curves verifies that the
relation in Eq. \thechapter .11 does indeed hold.

So far we have noted that the EP and PS method are different in two
respects. Firstly they require different ranges of @xmath space to be
reweighted, and secondly the MUCA weights are different. However these
differences mask the underlying similarity of the two methods. The first
difference, that is the difference in the range of @xmath space which
requires reweighting, vanishes on the transition to sufficiently large
system sizes. The second difference merely arises from the fact that the
canonical sampling distributions are different. However since the MUCA
distributions are the same, and since the weight of macrostates are
proportional (compare Eq. \thechapter .14 to Eq. \thechapter .48 and Eq.
\thechapter .15 to Eq. \thechapter .49 ), we see that the methods are
essentially identical . This equivalence between the two methods may be
most readily expressed through the corresponding estimators:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .12)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

where we have used the fact that the MUCA distributions @xmath and
@xmath are the same, so that @xmath . Eq. \thechapter .12 establishes
the equivalence between the PS estimator (Eq. \thechapter .6 ) to that
of the EP estimator (Eq. \thechapter .38 ) in the MUCA limit.

In the absence of MUCA weights, the two estimators are no longer
equivalent. The reason for this is because now the @xmath distributions
are no longer the same, even though the weights remain proportional to
each other. As a result the statistical errors will be different for the
two methods in a finite run simulation.

### 23 The Multihamiltonian strategy

#### 23.1 Theory

In the previous section we have seen that the MUCA strategy provides an
efficient framework (in the case of zero equilibration) for tackling of
the overlap problem. The basic idea is to construct a sampling
distribution which contains the two phase-constrained distributions
@xmath and @xmath so as to allow for the construction of a path (in a
piecewise but serial manner) from the region of (effective)
configuration space associated with phase A to that of phase B. This
allows one to determine the weight of the typical macrostates associated
with phase B relative to those of phase A. The important thing to notice
is that this path can also be constructed in a parallel manner (that is
piecewise and independent fashion). This is effectively what the MS and
WHAM methods do [ 153 ] and was originally proposed by Geyer [ 154 ] - [
157 ] . The essential ingredient of all these methods is that the
independent simulations overlap in some region of the (effective)
configuration space that they explore. It is only when they overlap that
the data of a simulation obtained with @xmath , say, may be reweighted
with respect to @xmath , which overlaps with @xmath , so as to yield a
set of macrostates whose relative probabilities are in agreement with
@xmath and which, at the same time, extend outside the range normally
explored by @xmath (see Eq. \thechapter .32 ). In this way one may use
the idea of reweighting to estimate the probabilities of the regions of
(effective) configuration space typically associated with phase B in
relation to those of phase A.

With this in mind let us proceed to construct a new way of estimating
the FED based on the idea of simulating several independent systems.
Consider the construction of a chain of configurational energies, as has
been done in Eq. \thechapter .44 , whose associated sampling
distributions (Eq. \thechapter .45 ) overlap in a manner so as to yield
a path connecting the two regions of (effective) configuration
space associated with the two phases. Then instead of writing the ratio
of the partition functions as has been done in Eq. \thechapter .46 , one
may instead choose to write it as:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .13)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

or:

  -- -------- -- --------------------
     @xmath      ( \thechapter .14)
  -- -------- -- --------------------

where @xmath = @xmath denotes the collection of the configurations of
the n-1 independent replicas, and where:

  -- -------- -- --------------------
     @xmath      ( \thechapter .15)
  -- -------- -- --------------------

By writing @xmath as has been done in Eq. \thechapter .14 , a new
strategy immediately becomes apparent. That is rather than simulating
the actual systems with the configurational energies @xmath and @xmath
(see Eq. \thechapter .13 ) one may instead simulate the composite
systems described by the extended configurational energies @xmath and
@xmath . If one now generalises the PM operation from the original
version:

  -- -------- -- --------------------
     @xmath      ( \thechapter .16)
  -- -------- -- --------------------

to that of a PM between the composite systems in which the extended
configuration @xmath is matched for the two systems:

  -- -------- -- --------------------
     @xmath      ( \thechapter .17)
  -- -------- -- --------------------

then it is clear that the array of estimators and techniques used to
estimate Eq. \thechapter .13 may also be used here. The key idea is that
the independent sampling distribution @xmath provides the necessary
extended sampling strategy that is needed in order to overcome the
overlap problem. The greater the number of configurational energies in
the chain, the greater is the weight of the set of configurational
energies @xmath that the two hamiltonians @xmath and @xmath share, and
therefore the greater is the overlap between the effective configuration
space of the two composite systems.

In order to be able to quantify the overlap, it is useful to once again
define a macrovariable:

  -- -------- -- --------------------
     @xmath      ( \thechapter .18)
  -- -------- -- --------------------

@xmath essentially corresponds to the (temperature scaled) work (which,
as before, we will subsequently refer to as work) incurred in switching
(in an instantaneous fashion) between the extended configurational
energies @xmath and @xmath whilst preserving the extended configuration
@xmath . In the case of the linear parameterisation given in Eq.
\thechapter .48 , Eq. \thechapter .18 may be written as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .19)
  -- -------- -- --------------------

In order to quantify the overlap in a meaningful way one must be able to
relate the probabilities of a macrostate @xmath associated with one
system relative to that of the other. Suppose that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .20)
  -- -------- -- --------------------

denotes the sampling distribution of the composite system (which may be
realised by independently simulating the n-1 sampling distributions
@xmath ). The procedure of sampling via @xmath will be referred to as
the multihamiltonian (MH) method. Suppose that @xmath denotes the
probability of obtaining the @xmath when sampling with @xmath . It
immediately follows from the form of Eq. \thechapter .14 (compare this
to Eq. \thechapter .13 ) that the probability of obtaining @xmath (as
defined in Eq. \thechapter .18 ) when sampling with @xmath relative to
that when sampling with respect to @xmath , is simply given by the
overlap identity (Eq. \thechapter .101 ). To see this more explicitly we
observe that:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .21)
                       @xmath   @xmath   
                                @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

which is the overlap identity given in Eq. \thechapter .101 . Once again
the distribution of @xmath will look something similar to that shown in
figure 19 with the point of intersection being located at @xmath . We
note that this also follows from the fact that the MH method can be
viewed as a limiting case of the FG method (see appendix \thechapter ).

The crucial point to realise is that as the number of configurational
energies in Eq. \thechapter .15 increases, the overlap @xmath increases
and tends to unity. We recall that heuristically this may be understood
by noting that the greater the number of replicas, the greater is the
weight of the set of configurational energies @xmath that the two
hamiltonians @xmath and @xmath share (see also figure 26 for an
alternative explanation). In the case of the n=2 this set has zero
weight. As @xmath , the weight of this set dominates over the
configurational energies @xmath and @xmath which are at the edge of the
chain of the configurational energies in Eq. \thechapter .44 and which
describe the two phases whose FED one is trying to measure. It is these
edge configurational energies that give rise to the difference between
@xmath and @xmath (see Eq. \thechapter .15 ). In the limit of the number
of configurational energies in the chain tending to infinity, one may
write:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .22)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

Similarly, it follows that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .23)
  -- -------- -- --------------------

Therefore in the limit of the number of configurational energies in Eq.
\thechapter .15 tending to infinity, we find that @xmath approaches the
value obtained by thermodynamic integration:

  -- -------- -- --------------------
     @xmath      ( \thechapter .24)
  -- -------- -- --------------------

It follows that in this limit @xmath must have the distribution
corresponding to perfect overlap:

  -- -------- -- --------------------
     @xmath      ( \thechapter .25)
  -- -------- -- --------------------

Comparing Eq. \thechapter .14 to Eq. \thechapter .13 it is immediately
apparent that the array of estimators as parameterised by Eq.
\thechapter .34 are available for the task of estimating FEDs. More
generally one could also perform an arbitrary switching FG process in
which @xmath is gradually switched into @xmath so as to allow an
estimate of the FED to be obtained either from the phase-constrained
methods Eq. \thechapter .101 , Eq. \thechapter .20 , or from the PS
formulae (Eq. \thechapter .47 ) . For example in the case of
zero-equilibration the MH version of the PS sampling distribution is
given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .26)
  -- -------- -- --------------------

where @xmath is the weight which biases phase @xmath , and @xmath is a
stochastically sampled variable (see section 18 ). @xmath may then be
estimated either via Eq. \thechapter .43 or Eq. \thechapter .47 (where
@xmath = @xmath ).

Inspection of the MH sampling distribution (Eq. \thechapter .20 ) may
lead one to believe that the MH method is equivalent to the MS method.
This is generally true if one can write Eq. \thechapter .34 as:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .27)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

which only holds if G(x) is of the exponential form:

  -- -------- -- --------------------
     @xmath      ( \thechapter .28)
  -- -------- -- --------------------

where a is some constant. Therefore the only choice of @xmath for which
the MS and MH methods are equivalent is the EP (Eq. \thechapter .26 )
method. In this case the only difference that arises for the two methods
is that in the case of MH estimator one only deals with a single
estimate of the error, where are in the case of the MS method one must
combine n-1 such errors in determining the error of the final value for
the FED between the two phases. For other estimators (including the PS
estimator) the two strategies are not equivalent.

We finally note that the potential for parallelising the sampling of Eq.
\thechapter .20 is a clear advantage of the MH method. Furthermore, in
comparison to the WHAM method, the storage requirements are far less. If
one records the value of @xmath during the course of the simulation (as
one would do if one wanted to avoid the systematic errors introduced by
employing finite bin width histograms) then the use of the MH method
will yield significant gains in regards to this issue, since for this
method one will need to record only a single temporal sequence of @xmath
’s, as opposed to recording one such sequence for each and every
replica.

#### 23.2 Numerical Results

In this section we illustrate the application of the MH method to the
systems investigated in this thesis. Figure 26 shows the distributions
of the macrovariable @xmath for the replicas of the chain of
configurational energies (Eq. \thechapter .44 , Eq. \thechapter .15 ) in
which n=7 and in which the configurational energies are linearly
parameterised as prescribed in Eq. \thechapter .48 . The crucial feature
of this figure is the way in which the distributions overlap, so as to
provide a continuous path (in @xmath space) from the region of
(effective) configuration space associated with phase A (right hand most
peak in figure 26 ) to that of phase B (left hand most peak). Figure 27
shows the corresponding MH-PS distribution which employs these replicas
in the sampling distribution Eq. \thechapter .26 , and shows how the
MH-PS method is able to effectively overcome the overlap problem (notice
that the overlap between the phase A and phase B, the right and left
hand most peaks respectively, do not overlap at all). Figure 28 shows
the probability distributions of @xmath for the MH-PS method (Eq.
\thechapter .26 ) for different n. It is evident that, as the number of
replicas n increases, the overlap increases, with the distribution
@xmath tending to the ideal limit of a delta function centred on @xmath
, which follows from Eq. \thechapter .76 and Eq. \thechapter .25 .

### 24 The Fast Growth strategy

An alternative to the MUCA and the MH strategies is the FG method (see
section 8.8 ). Under this scheme, one performs work on the system so as
to morph the configurational energy from that of phase @xmath ( @xmath )
to that of phase @xmath ( @xmath ). In the process a path is constructed
linking the set of macrostates associated with phase A to those
associated with phase B. The key parameters in the method are the
increments @xmath of the field parameter @xmath and the equilibration
times @xmath (where @xmath ). For simplicity, we will limit ourselves to
the case where all the increments are equal , that is @xmath , and when
all the equilibration times are equal, that is @xmath . It is only if
the equilibration times @xmath are sufficiently long and the
perturbations to the configurational energy ( @xmath ) are sufficiently
small that one is able to construct a path (an overlapping sequence of
macrostates) connecting the two phases.

One may immediately identify two limiting cases. In the case where
@xmath , one obtains the zero equilibration methods, irrespective of
@xmath , as one does in the case where @xmath [ 55 ] . In the limit
where @xmath , @xmath (which we will refer to as adiabatic equilibration
since such a method takes an infinite time to switch from the
configurational energy of phase A to that of phase B), one obtains the
thermodynamic integration method (Eq. \thechapter .52 ) [ 55 ] :

  -- -------- -- --------------------
     @xmath      ( \thechapter .29)
  -- -------- -- --------------------

Both limits have undesirable features as they stand. On the one hand the
choice of zero equilibration induces systematic errors (as described in
section 8.2 ) which must be overcome by appeal to some form of extended
sampling strategy ( [ 21 ] , section 22 , section 23 ). On the other
hand the choice of adiabatic equilibration is time consuming.
Furthermore it is not clear how one accounts for the systematic errors
induced in making an approximate evaluation of the integral in Eq.
\thechapter .29 . Generally some intermediate strategy is preferable in
which Eq. \thechapter .3 , Eq. \thechapter .47 is used to estimate
@xmath .

We will now focus our attention on such intermediate strategies. Namely
we will investigate the way in which the FG method overcomes the overlap
problem for these intermediate strategies through control of the
parameters @xmath and @xmath [ 158 ] . In particular we will examine
three variations. In the first case we will keep @xmath constant and
vary @xmath . In the second we keep @xmath constant and vary @xmath . In
the third we investigate the case when @xmath is held constant. Since
the total time allocated to obtaining each work term @xmath is given by
@xmath , we see that the last case corresponds to the variation of the
parameters @xmath and @xmath so as to ensure that the amount of time
allocated to performing work on the system is held constant. In all
cases we will demonstrate the improvement in the overlap by
demonstrating the convergence of the FED estimate as obtained by the EP
estimator (Eq. \thechapter .103 ) relative to that obtained via the
MUCA-PS method. Again we illustrate the overlap by examining only one of
the phase constrained distributions, which in this case is @xmath ,
since its conjugate partner @xmath is roughly symmetrically positioned
about the origin (which is where @xmath , see figure 19 , roughly lies).
An example of this approximate symmetry is illustrated in figure 29 .
The particular parameterisation of @xmath that we employ is the linear
parameterisation given by Eq. \thechapter .48 .

#### 24.1 Keeping @xmath constant, varying @xmath

Figure 30 (a) shows the probability distribution of @xmath as the
increment @xmath is kept fixed but the equilibration time @xmath is
varied, and figure 30 (b) shows the estimate of @xmath (the normalised
value of @xmath with respect to the corresponding value as obtained by
the MUCA-PS simulation) as a function of @xmath .

Figure 30 (a) clearly shows that as the equilibration time @xmath is
increased, the mean of the distribution @xmath and its associated
variance both decrease. To understand this we first note that each time
a work increment @xmath is performed, a lag develops in the ensemble of
configurations immediately associated with the system @xmath after this
operation. Namely, when the switch from a configurational energy @xmath
to @xmath is made, the configuration @xmath will not be typical of the
set of configurations associated with @xmath . Furthermore this lag
accumulates as one performs the FG process. A consequence of this is
that the distributions of the energies @xmath associated with the system
immediately after its configurational energy has been incremented from
@xmath to @xmath will not be the same as the equilibration distribution
which we denote by @xmath . Typically the values within the set @xmath
will be higher than those of @xmath . Increasing the equilibration time
@xmath decreases this lag, and this is precisely what is observed in
Figure 30 (a).

Further insight into the workings of the FG method may be obtained by
noticing that the components @xmath of the overall work term (
\thechapter .94 ) can be both positive and negative. Suppose that one
performs a FG simulation in which the canonical distribution of the
’intermediate’ stages are given by those shown in figure 26 . For zero
equilibration time ( @xmath = 0) @xmath for all i, where @xmath is the
starting value of @xmath , namely @xmath . Since this corresponds to a
value of @xmath chosen from @xmath (the right hand peak of 26 ) we see
that @xmath will be almost always positive. Suppose that @xmath denotes
the probability distribution of @xmath at time @xmath , when the
configurational energy has been incremented from @xmath to @xmath and
after the system has been equilibrated with @xmath for a time @xmath .
Then as the equilibration time @xmath increases, the distributions
@xmath will shift from the right hand peak in figure 26 towards the
left. In the limiting case of @xmath one will find that @xmath , so as
to yield the collection of distributions in figure 26 [ 159 ] .
Therefore we see that the increase of the equilibration time @xmath will
eventually lead to significant cancellations between terms in Eq.
\thechapter .94 , resulting in a decrease (on average) of @xmath from
the value it assumes in the case of zero equilibration. As a result one
obtains improved overlap between @xmath and @xmath , resulting in the
convergence of @xmath in the limit of large @xmath , as can be seen from
figure 30 (b) (note that the convergence is conditional on @xmath being
small enough).

#### 24.2 Keeping @xmath constant, varying @xmath

Figure 31 (b) illustrates that as the equilibration time @xmath is kept
constant, whilst @xmath is decreased, @xmath converges as the overlap
between @xmath and @xmath increases. In order to understand this
consider the following argument. As @xmath decreases, less equilibration
time is needed between successive work increments, until eventually
@xmath matches the equilibration time ’needed’ in order for the lag to
be absent so that @xmath . In this case the distribution of @xmath at
each timeslice will be something reminiscent of what is shown in figure
26 . Subsequent decrease of @xmath will just correspond to increasing
the number of configurational energies in the chain of Eq. \thechapter
.44 , which will lead to ’better’ cancellations in Eq. \thechapter .94 ,
thus taking @xmath closer to the ideal limit in Eq. \thechapter .25 .
Eventually the overlap between the two phase-constrained distributions
will be sufficiently great so as to ensure the convergence of @xmath
even when estimated via the EP method (Eq. \thechapter .103 ), as is
clearly verified in figure 31 .

Let us now analyse the behaviour of the statistical and systematic
errors for the FG-EP estimator in the context of the systems that we
have studied. Insight into the interplay between statistical and
systematic errors may be obtained from figure 31 (b). It is clear from
this figure that whereas for large @xmath systematic errors are present
(since @xmath ), for small @xmath they are absent (since @xmath ). In
between these two limits, one finds that as @xmath decreases, the
systematic errors decrease. The behaviour of the statistical errors, on
the other hand, is quite different. In this case the statistical errors
are small in both the large @xmath and small @xmath limits, and in
between these limits there is a transient regime where the statistical
errors greatly increase. This is merely an artifact of the EP estimator,
and may be understood as follows.

In the case of negligible overlap the weights @xmath (see Eq.
\thechapter .14 ) of the macrostates actually sampled are small in value
in comparison to the macrostates which contribute most significantly to
the numerator of Eq. \thechapter .13 . As a consequence the variance of
the estimate of @xmath (see Eq. \thechapter .13 ) will be small, since
the estimate of @xmath will itself be small. As the overlap increases,
one eventually enters a regime where the main body of @xmath resides
within the tail of @xmath . In this case, the macrostates which
contribute significantly to the numerator of Eq. \thechapter .13 will
originate from the tail of @xmath , and as a consequence their
statistics will be bad, resulting in large statistical errors in the FED
estimate, despite the absence of systematic errors. As the overlap
improves more of @xmath gets contained in the main body of @xmath , and
as a result the statistical error in the estimate of the FED improves.

#### 24.3 Keeping @xmath constant

We also considered the convergence of @xmath as @xmath is varied, given
the constraint that the total time spent obtaining each work term @xmath
is kept constant. Since @xmath corresponds to the number of times the
configurational energy @xmath is ’perturbed’ the constraint of keeping
the time ( @xmath ) spent obtaining each work term @xmath constant
corresponds to:

  -- -------- -- --------------------
     @xmath      ( \thechapter .30)
  -- -------- -- --------------------

It is clear that if @xmath is too large, then one will approach the
limit of zero equilibration, resulting in the appearance of the
systematic errors described in section 8.2 . This is exactly what is
observed in figure 32 , since as @xmath @xmath . As @xmath is decreased
these systematic errors vanish, resulting in @xmath converging to unity
[ 160 ] .

The behaviour exhibited in figure 32 may be understood as follows. After
each work increment a lag develops. This lag can be overcome by
appropriately equilibrating the system. However the lag obtained from
going from @xmath to @xmath (as measured by the deviations of @xmath
from @xmath ) can only be removed by performing equilibration after the
@xmath measurement has been made. Therefore this lag persists in the
value of @xmath that one obtains. As a consequence if @xmath is too
large, @xmath will become large, leading to less overlap and eventually
to systematic errors.

We finish off this section by noting that it was observed from the
numerical data that the errors seemed to be essentially independent of
@xmath , provided @xmath was sufficiently small. In other words our
numerical work seems to indicate that the error in ones estimate of
@xmath (based on the estimator of the EP method, Eq. \thechapter .103 )
essentially depends on the total time allocated to obtaining each
estimate of @xmath (provided @xmath is sufficiently small), and not on
@xmath .

#### 24.4 The choice of estimator

Our investigations into the way in which the FG method deals with the
overlap problem have primarily focused on the EP estimator (Eq.
\thechapter .103 ). As we have noted in chapter \thechapter this
estimator will require a significantly greater amount of overlap between
the phase-constrained distributions than will be the case of the
dual-phase (DP) estimators (Eq. \thechapter .3 , Eq. \thechapter .20 )
or the PS estimator (Eq. \thechapter .47 ) which only require partial
overlap [ 2 ] between the two distributions. Since the application of
these estimators is straightforward, we will only illustrate the use of
the PS estimator, which has not been formulated before within the
context of FG.

Figure 33 illustrates the application of the FG-PS estimator, as
described in section 8.9 , to the systems studied here. In this
experiment the equilibration time @xmath has been kept constant whilst
@xmath has been gradually decreased. As we saw earlier in section 24.1
that this leads to increased overlap between @xmath and @xmath . This is
also clearly evident in figure 33 , since the two peaks begin to merge
into one as @xmath decreases, and is reminiscent of what is observed in
figure 28 . Until now all FED calculations have been limited to the EP
estimator and figure 33 (in which the overlap problem has been cured)
shows the scope for improvement in using more ’intelligent’ estimators.

### 25 Conclusion

Given a choice of representation one obtains two phase constrained
distributions @xmath and @xmath . In a finite run simulation their
estimators @xmath and @xmath may or may not overlap. If they overlap
then one may choose an estimator (Eq. \thechapter .3 , Eq. \thechapter
.20 , or Eq. \thechapter .47 ) which yields an estimate of the FED which
is free of systematic errors. In the absence of overlap, one must
engineer overlap via one of three possible strategies:

1.   MUCA method: In this approach MUCA weights are employed to force
    the simulation to visit the regions of (effective) configuration
    space which it would not visit under the canonical sampling
    distribution and which at the same time contribute non-negligibly to
    the estimator of the FED. In this way a path is constructed from one
    region of (effective) configuration space to the other.

2.   MH method: In this method one employs a series of independent
    simulations which overlap in the regions of (effective)
    configuration space that they explore, so as to provide a path
    linking the two phases. Specifically, one engineers overlap by
    increasing n, the number of configurational energies comprising the
    composite systems. The benefit of this approach is that it is highly
    parallelizable, albeit at the expense of additional overheads in
    terms of memory requirements.

3.   FG method: In this approach the path is constructed by performing
    non-equilibrium work on the system so as to take it from the regions
    of (effective) configuration space associated with one phase to
    those of the other. One engineers overlap by making the work
    increments @xmath sufficiently small and by adequately equilibrating
    the system between successive work increments.

These extended sampling strategies may be combined in a straightforward
way. For example, the MH method may be incorporated into the framework
of the FG method simply by performing work on the hamiltonians @xmath
and @xmath . Other combinations (such as MUCA and MH) are also possible.
In deciding what combinations to use, it is important to bear in mind
that both the MUCA and FG are serial strategies, whereas the MH strategy
is a parallel strategy. Since speedup offered by the MH strategy comes
at the expense of additional memory requirements, combinations of this
method with either the MUCA or FG methods may be an attractive option.

## Chapter \thechapter Quantum Free Energy Differences

### 26 Introduction

Our focus until now has been limited to the classical regime of the
phase diagram, and in this chapter we will concentrate our attention on
the task of estimating the FEDs within the quantum regimes. Specifically
we will use the Path Integral Formalism of statistical mechanics [ 7 ] ,
[ 161 ] - [ 170 ] to map the problem onto that of determining the ratio
of two multidimensional integrals of the form used in Eq. \thechapter
.13 (see also [ 38 ] , [ 161 ] , [ 171 ] - [ 179 ] ). This will make
available to us the spectrum of methods discussed in the previous
chapters. For a more rigorous and in depth presentation of the following
material, we refer the reader to [ 6 ] , [ 165 ] , [ 168 ] , and [ 180 ]
.

### 27 Path integral formulation of statistical mechanics

#### 27.1 Quantum statistical mechanics

We recap that in the canonical case, classical statistical mechanics [ 3
] - [ 6 ] begins with the construct of a system of N particles and of
volume V which is weakly coupled to a macroscopic reservoir. Suppose
that @xmath denotes a state of the system (which we call a microstate of
the system) and that @xmath denotes a state (or microstate) of the
reservoir. Also let us suppose for the moment that the set of states
@xmath is finite [ 181 ] . The core assumption of classical statistical
mechanics is that if the collection of the system and the reservoir is
itself considered to be an isolated system of total energy @xmath , then
this collective system is equally likely to be in any one of the
microstates @xmath accessible to it (i.e. of energy @xmath ) when the
dynamics of this collective system have been averaged out over
sufficiently long periods of time. A consequence of this so called
‘Equal A Priori Probabilities’ assumption and the weak coupling
assumption is that one finds that the absolute probability of the system
being in microstate @xmath at any given instant of time is given by:

  -- -------- -- -------------------
     @xmath      ( \thechapter .1)
  -- -------- -- -------------------

where the partition function Z is given by:

  -- -------- -- -------------------
     @xmath      ( \thechapter .2)
  -- -------- -- -------------------

@xmath is the hamiltonian of the system and @xmath denotes a summation
over all microstates @xmath that are available to the system [ 182 ] .
The expectation of a general macrovariable (or ’observable’, as will be
more appropriate in the quantum case) is then given by:

  -- -------- -- -------------------
     @xmath      ( \thechapter .3)
  -- -------- -- -------------------

In the quantum mechanical case the construct follows a similar procedure
(see [ 5 ] , [ 6 ] ). At the heart of this procedure is the realisation
that because the system is coupled to a macroscopic heat bath, the
system will be in one of the eigenstates of the hamiltonian operator of
the system, and not in a superposition of states [ 183 ] . Furthermore,
in the derivation of Eq. \thechapter .1 the exponential comes directly
from the entropic properties of the reservoir ( [ 4 ] - [ 6 ] ). Since
the reservoir is classical even in the quantum formulation of
statistical mechanics, we deduce that in the quantum mechanical case the
probability of finding the system in a microstate @xmath is once again
given by Eq. \thechapter .1 . What actually changes is, firstly, what
one actually means by a microstate and, secondly, the link that one
makes between the observable O and the microstate @xmath . In classical
statistical mechanics the observable takes a precise value for each
microstate, since a microstate essentially corresponds to a fixed
spatial and momentum configuration of the system. In the quantum
mechanical case it will, in the most general case, no longer be the case
that the observable takes a definite value for each microstate, since
now the microstate @xmath corresponds to a quantum state (or
wavefunction). At best one will only be able to specify the quantum
mechanical expectation of the observable with respect to a given
microstate. Therefore in the quantum mechanical case one replaces Eq.
\thechapter .3 by:

  -- -------- -- -------------------
     @xmath      ( \thechapter .4)
  -- -------- -- -------------------

where @xmath denotes the quantum mechanical expectation of the operator
@xmath with respect to the i-th quantum microstate, or wavefunction,
@xmath . By comparison of Eq. \thechapter .3 and Eq. \thechapter .4 it
is clear that whereas in classical statistical mechanics one only
performs one type of averaging, in the quantum mechanical case one must
perform two sorts of averaging. The first average is the quantum
mechanical expectation (with respect to a microstate) of the operator
@xmath , and the second is the averaging of this expectation over the
quantum microstates accessible to the system. This first averaging, the
quantum mechanical expectation, is not something which one performs due
to our ignorance of the constituent system, but it is something we have
to do because of the inherent quantumness of systems.

To formally develop the theory let us begin by denoting the set of
eigenstates of the hamiltonian operator @xmath of the system (see [ 184
] ) by @xmath . The hermiticity of @xmath will mean that eigenstates of
different energy eigenvalues will be orthogonal, though states with the
same energy eigenvalue are not necessarily orthogonal. One may, however,
employ the Gram-Schmidt orthogonalisation procedure (see [ 185 ] ) to
construct a new set of states corresponding to the degenerate eigenvalue
which are mutually orthogonal. Therefore there is no loss in generality
if we assume @xmath to be a mutually orthonormal set (and therefore
correspond to a basis set, see [ 184 ] ). It follows from Eq.
\thechapter .4 that the statistical mechanical expectation of an
observable O may now be written as:

  -- -------- -- -------------------
     @xmath      ( \thechapter .5)
  -- -------- -- -------------------

where @xmath denotes the energy eigenvalue associated with the
eigenvector @xmath , @xmath denotes a sum over eigenstates, and where
the partition function Z is once again given by Eq. \thechapter .2 .
@xmath denotes the quantum mechanical expectation of the observable O
for a given eigenstate @xmath and accounts for the quantum mechanical
properties of the system. As is the case in the classical formula (Eq.
\thechapter .3 ) the weighted summation @xmath essentially describes the
coupling between the quantum system and the classical reservoir.

Eq. \thechapter .5 may be written in a more general way as follows:

  -- -------- -------- -------- -- -------------------
     @xmath   @xmath   @xmath      ( \thechapter .6)
              @xmath   @xmath      ( \thechapter .7)
  -- -------- -------- -------- -- -------------------

where @xmath , the density matrix, is written as:

  -- -------- -- -------------------
     @xmath      ( \thechapter .8)
  -- -------- -- -------------------

and where denotes a trace over the matrix elements of the operator
@xmath . The partition function may then be written as:

  -- -------- -- -------------------
     @xmath      ( \thechapter .9)
  -- -------- -- -------------------

The tracing operation performed in Eq. \thechapter .7 and Eq.
\thechapter .9 has only been implemented with respect to the orthonormal
set corresponding to the energy eigenstates. A-priori these eigenstates,
and their associated eigenvalues @xmath , are not known. Progress is
made by noting that the trace is independent of the basis in which it is
carried out [ 184 ] , and therefore one is free to choose any
representation. A convenient representation is the position
representation, in which case the partition function of Eq. \thechapter
.9 for distinguishable (identical but localised) particles simply
becomes:

  -- -------- -- --------------------
     @xmath      ( \thechapter .10)
  -- -------- -- --------------------

The position representation is useful for the simple reason that, as we
will soon see, it allows one to map the problem of determining Eq.
\thechapter .2 onto that of determining an integral of the form of Eq.
\thechapter .2 . This mapping is known as the classical-quantum
isomorphism, and is what forms the basis of the path integral
computational techniques.

#### 27.2 The classical-quantum isomorphism & the path integral

The partition function, as formulated in Eq. \thechapter .10 , is not
fully quantum mechanical in that it ignores exchange. Exchange (see [ 6
] , [ 165 ] , [ 168 ] ) is a quantum mechanical property that arises out
of the indistinguishability of identical particles [ 184 ] , [ 186 ] .
In order to incorporate this property into Eq. \thechapter .10 , one
rewrites it as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .11)
  -- -------- -- --------------------

where @xmath denotes a permutation of the particles, @xmath denotes the
sum over all such permutations, and @xmath denotes the sign of the
permutation. For bosons @xmath and for fermions @xmath assumes the
values 1 and -1 depending on the sign of the permutation ( [ 6 ] , [ 165
] , [ 168 ] ).

The expression in Eq. \thechapter .11 is still not suitable, as it
stands, for use in simulation. What remains to be done is to find a way
to project the density matrix operator @xmath onto the position
representation @xmath so as to ensure that one is left with an
expression involving only real numbers. To do this we first decompose
our hamiltonian into the sum of a kinetic part @xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .12)
  -- -------- -- --------------------

and a configurational part:

  -- -------- -- --------------------
     @xmath      ( \thechapter .13)
  -- -------- -- --------------------

so that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .14)
  -- -------- -- --------------------

where @xmath denotes the momentum operator corresponding to the
classical variable @xmath , which represents the collective momenta of
all the particles, where @xmath denotes Planck’s constant, and where m
is the mass of the particle. Then the main obstacle to expressing Eq.
\thechapter .11 in terms of real numbers is the fact that @xmath and
@xmath are non-commuting operators, which means that there exists no
basis in which @xmath and @xmath are simultaneously diagonal [ 184 ] .
Clearly in order to achieve our goal of recasting Eq. \thechapter .11 as
an expression involving only real numbers, we must find a way of
separating out the kinetic and the configurational terms in @xmath so
that we can separately diagonalise each contribution; @xmath with
respect to the position representation and @xmath with respect to the
momentum representation (a representation which it is diagonal in).

The fundamental identity which allows this to be done is ( [ 187 ] [ 190
] ):

  -- -------- -- --------------------
     @xmath      ( \thechapter .15)
  -- -------- -- --------------------

where:

  -- -------- -- --------------------
     @xmath      ( \thechapter .16)
  -- -------- -- --------------------

The Trotter theorem essentially states that in the limit of small @xmath
one may approximate the operator @xmath as the product of a ’kinetic’
operator @xmath and a ’configurational’ operator @xmath .

We may now use this (Eq. \thechapter .15 ) to write the partition
function as an integral over real numbers. To do this we use the
identity @xmath to re-write Eq. \thechapter .11 as:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .17)
                       @xmath   @xmath   
                       @xmath            
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

where @xmath and @xmath represents the collective displacements of all
the particles of replica system i. Applying Eq. \thechapter .15 to Eq.
\thechapter .17 we see that:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .18)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

where @xmath denotes the total configurational energy of replica i. To
recast @xmath we use the identity [ 184 ] , [ 191 ] :

  -- -------- -- --------------------
     @xmath      ( \thechapter .19)
  -- -------- -- --------------------

where @xmath is the identity operator. Substituting Eq. \thechapter .19
into Eq. \thechapter .18 yields:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .20)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

which, using the identity [ 184 ] @xmath , becomes:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .21)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

and where @xmath is given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .22)
  -- -------- -- --------------------

Eq. \thechapter .21 represents the kinetic component appearing in Eq.
\thechapter .18 . Collating the results of Eq. \thechapter .17 , Eq.
\thechapter .18 , and Eq. \thechapter .21 we finally see that the
quantum partition function in Eq. \thechapter .11 may be written as the
following limit:

  -- -------- -- --------------------
     @xmath      ( \thechapter .23)
  -- -------- -- --------------------

or [ 192 ] :

  -- -------- -- --------------------
     @xmath      ( \thechapter .24)
  -- -------- -- --------------------

where:

  -- -------- -- --------------------
     @xmath      ( \thechapter .25)
  -- -------- -- --------------------

and:

  -- -------- -- --------------------
     @xmath      ( \thechapter .26)
  -- -------- -- --------------------

@xmath corresponds to the classical groundstate, @xmath denotes the
total configurational energy minus the groundstate energy @xmath , and
@xmath corresponds to a permutation of the particles in replica . In the
absence of exchange (where particles are localised, so as to make them
distinguishable) Eq. \thechapter .25 may be simplified to:

  -- -------- -- --------------------
     @xmath      ( \thechapter .27)
  -- -------- -- --------------------

Eq. \thechapter .27 represents the partition function of a classical
system which is isomorphic to the quantum system of interest. This is
generally what is referred to as the classical-quantum isomorphism and
forms the starting point, in one form or the other, for the majority of
path integral based simulations [ 193 ] .

#### 27.3 Heuristics of the polymeric system

The partition function of Eq. \thechapter .25 and Eq. \thechapter .26
contains all the equilibrium, time independent, information of the
quantum system, and serves as the starting point for the Path Integral
Monte Carlo (PIMC) methods [ 162 ] - [ 169 ] , [ 194 ] , [ 195 ] . Its
usefulness lies in the fact that it maps the problem of dealing with an
expression involving operators (Eq. \thechapter .9 ) onto one involving
only real numbers. The classical system, represented by Eq. \thechapter
.25 and Eq. \thechapter .26 , can be thought of as a system of
interacting polymers (see figure 34 ) with the following action:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .28)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

where @xmath is the kinetic action:

  -- -------- -- --------------------
     @xmath      ( \thechapter .29)
  -- -------- -- --------------------

and @xmath is the configurational action:

  -- -------- -- --------------------
     @xmath      ( \thechapter .30)
  -- -------- -- --------------------

Two main parameters control the behaviour of this system of classical
polymers. The first is the effective inverse temperature @xmath and the
second is @xmath (Eq. \thechapter .22 ). It is the interplay between
these two quantities which determines the strengths of the harmonic
interactions in the kinetic action @xmath relative to those originating
from the configurational action @xmath .

This system of interacting polymers is unique (as compared to classical
polymers) in that beads of a given polymer interact only with beads of
other polymers which are in the same replica, or timeslice [ 196 ] as we
will also call it, labelled by the index i, via the configurational
energy @xmath appearing in the configurational action @xmath . In
addition to this beads of a given polymer interact, through the term
@xmath in the kinetic action @xmath , with the two adjacent beads (of
the same polymer) belonging to the two neighbouring replicas, resulting
in a coupling between consecutive replicas (see figure 34 ). @xmath
essentially contains the forces which propagate along a given polymer
and @xmath contains the forces which give rise to interactions between
polymers.

These polymers are also unique in the sense that they have a special
boundary condition, namely that @xmath . For the case of distinguishable
quantum mechanical particles ( @xmath ), the endpoints of the polymers
connect to form loops. Distinguishability of the particles then arises
from the fact that one may identify each particle with a given loop. In
the presence of exchange the endpoints of loops coalesce with the
starting points of other loops so as to form larger loops, making it
impossible to distinguish the exchanging particles since now a single
loop may represent more than one particle. It is in this way that
indistinguishability is incorporated into the theoretical framework of
the model.

#### 27.4 Temperature regimes in quantum simulations

In the case of quantum systems one may identify three distinct
temperature regimes. In the high temperature limit, the system resides
within the classical regime where quantum effects may be safely ignored
and where the particles are localised to regions in the immediate
vicinity of their lattice sites. As the temperature is lowered, one
first enters the weak quantum regime where quantum discreteness effects
begin to become important. By quantum discreteness we mean those effects
arising from the quantisation of energies that accompanies the
confinement of particles in their interatomic potential wells. A
characteristic of this regime is the increased amplitudes of vibrations
of the particles about their lattice sites (relative to the classical
predictions) . This is called the zero point motion and arises from the
Heisenberg uncertainty principle. At this point the quantum effects are
not strong enough to give rise to exchange, and the particles may,
therefore, still considered to be distinguishable. As the temperature is
further reduced one may enter the strong quantum regime, where exchange
effects can no longer be ignored and where one must explicitly take into
account the indistinguishability of the particles. For the Lennard-Jones
potential one may easily identity these regimes.

Consider the Lennard-Jones potential given in Eq. \thechapter .2 . The
parameter @xmath roughly measures the well depth, so that one is in the
classical regime when the temperature is of the order:

  -- -------- -- --------------------
     @xmath      ( \thechapter .31)
  -- -------- -- --------------------

In this regime the classical effects mask the quantum effects.

As the temperature is further reduced, one eventually enters the weak
quantum regime in which the typical particle energy is less than that of
the well depth and is instead of the order of the typical phonon
excitation energy:

  -- -------- -- --------------------
     @xmath      ( \thechapter .32)
  -- -------- -- --------------------

In this case the quantum zero-point motion effects will be important,
but at the same time the exchange effects will not show up in the
system. To determine this temperature we note that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .33)
  -- -------- -- --------------------

where m is the mass of the particle and @xmath is the ’force constant’,
given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .34)
  -- -------- -- --------------------

Substituting Eq. \thechapter .33 and Eq. \thechapter .34 into Eq.
\thechapter .32 one finds that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .35)
  -- -------- -- --------------------

It then follows that the difference in the orders between @xmath and
@xmath is given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .36)
  -- -------- -- --------------------

where @xmath , the De Boer parameter, is given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .37)
  -- -------- -- --------------------

As the temperature is further reduced one may eventually enter a regime
where the exchange effects may no longer be ignored. This will happen if
the de Broglie wavelength becomes of the order of the interparticle
spacing a:

  -- -------- -- --------------------
     @xmath      ( \thechapter .38)
  -- -------- -- --------------------

where @xmath is given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .39)
  -- -------- -- --------------------

where @xmath is the momentum. Since there is a minimum non-zero value
that the total energy of the system can assume it follows that there
will be a minimum characteristic value @xmath that the absolute value of
the momentum can assume. Since for a harmonic oscillator the expectation
of the kinetic energy is equal to the expectation of the configurational
energy, we see that this @xmath may be crudely estimated by setting:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .40)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

so that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .41)
  -- -------- -- --------------------

Substituting this in Eq. \thechapter .38 :

  -- -------- -- --------------------
     @xmath      ( \thechapter .42)
  -- -------- -- --------------------

Rearranging this equation we get:

  -- -------- -- --------------------
     @xmath      ( \thechapter .43)
  -- -------- -- --------------------

Therefore if @xmath is sufficiently large (obtained, for example, by
having a particle of small enough mass) then an additional temperature
scale @xmath will appear at which point exchange between particles may
no longer be neglected. These temperature regimes are shown
schematically in figure 35 .

In what follows we address the effects of quantum discreteness but not
those of quantum exchange.

#### 27.5 Estimating macrovariables

In order to extract useful information from PIMC simulations, one must
find the estimators for the relevant observables within the path
integral framework. In the case of estimating thermodynamic quantities,
one may derive the estimators merely by taking derivatives of the
polymer partition function given in Eq. \thechapter .27 . For example
the mean kinetic energy may be derived by using the following relations:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .44)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

where the subscript P denotes the fact that the expectation is taken
with respect to the distribution associated with a P replica polymer
(see Eq. \thechapter .27 ). Similarly the mean total energy of the
quantum system may be obtained via the relation:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .45)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

where @xmath is the configurational energy of the quantum system :

  -- -------- -- --------------------
     @xmath      ( \thechapter .46)
  -- -------- -- --------------------

It must be noted that the estimator in Eq. \thechapter .45 (known as the
Barker estimator [ 198 ] ) is not unique. An alternative is the virial
estimator [ 199 ] . Studies as to the relative efficiencies of these
estimators have been made in [ 200 ] - [ 203 ] . The general findings
are that the more efficient of the two depends on the conditions under
which they are used. For example in [ 201 ] it was found that for low
temperature systems or systems in which the gradients of the
configurational energies are high (which is when the quantum effects are
typically more significant [ 204 ] ) the Barker estimator is preferable.
On the other hand it was found that the virial estimator is preferable
at high temperatures or for systems with low gradients of the
configurational energy (which is when the quantum effects tend to be
less significant). In [ 203 ] , other findings were also made, one being
that as the number of replicas P were increased, the virial estimator
eventually became more efficient than the Barker estimator. In this
thesis we chose to use the Barker estimator, mainly due to its
simplicity.

The estimators that we have discussed thus far have all been based on
finite replica approximations of the quantum partition function Eq.
\thechapter .11 . As a result these estimates will have an associated
systematic error (see Eq. \thechapter .24 ). If @xmath denotes the
finite replica expectation of an operator O [ 205 ] and if @xmath
corresponds to the infinite replica estimate, then it follows [ 189 ]
that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .47)
  -- -------- -- --------------------

Eq. \thechapter .47 provides a clear prescription with which one may
proceed to arrive at an estimate which is free of systematic error. In
order to estimate the asymptotic limit @xmath what one does is to plot a
graph of @xmath versus @xmath . Provided that P is sufficiently large,
so that the corrections in Eq. \thechapter .24 and Eq. \thechapter .47
in which P is raised to a power higher than two may be neglected, the
corresponding plot should yield a straight line graph whose intercept
gives an estimate for @xmath [ 206 ] . An illustration of this will be
given in section 29 .

The partition function, with @xmath given by Eq. \thechapter .26 ,
corresponds to what is known as the primitive approximation (PA) in the
Quantum Monte Carlo literature [ 166 ] - [ 169 ] . The widespread
prevalence of the use of the PA in current literature is due to its
underlying simplicity. By exploring more accurate decomposition schemes
(section 27.6 ) to that used in Eq. \thechapter .15 , one may derive
what we will refer to as the higher order approximants (HOA). These HOA
methods are more accurate than their PA counterparts in that the error
terms in equations of the form of Eq. \thechapter .47 decay faster than
@xmath . However they come at the expense of increased complexity and
computational expenditure. In the next section we will briefly discuss
the HOA method.

#### 27.6 Higher Order Approximants

The primitive approximation, leading to Eq. \thechapter .27 with @xmath
given by Eq. \thechapter .26 , is so called due to the fact that the
Trotter decomposition given in Eq. \thechapter .15 are the simplest such
breakups of the hamiltonian. Other breakups do exist, and may be written
in the form [ 188 ] - [ 190 ] , [ 207 ] , and are called the higher
order approximants. These higher order approximants allow one to use a
smaller number of replicas than the PA methods in order to achieve a
desired level of accuracy [ 208 ] , since the systematic error
associated with these methods decay faster with increasing P. However
these gains have to be appropriately balanced against the increased
complexity and increased computational cost that accompanies their
implementation. One such approximant is based on a Wigner-Kirkwood like
expansion [ 168 ] , [ 207 ] , [ 209 ] - [ 211 ] ). For this method the
systematic error in a finite replica approximation scales as @xmath .
The method is based on the identity [ 207 ] :

  -- -------- -- --------------------
     @xmath      ( \thechapter .48)
  -- -------- -- --------------------

where

  -- -------- -- --------------------
     @xmath      ( \thechapter .49)
  -- -------- -- --------------------

and:

  -- -------- -- --------------------
     @xmath      ( \thechapter .50)
  -- -------- -- --------------------

Application of this identity [ 207 ] once again yields Eq. \thechapter
.23 and Eq. \thechapter .27 , where @xmath is now given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .51)
  -- -------- -- --------------------

where:

  -- -------- -- --------------------
     @xmath      ( \thechapter .52)
  -- -------- -- --------------------

By appropriately differentiating Eq. \thechapter .25 (with @xmath given
by Eq. \thechapter .51 ) one may also obtain the estimator for the
expectation of the kinetic, configurational, and total energies of the
system:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .53)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .54)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .55)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

It can be shown [ 189 ] , [ 207 ] that within the HOA scheme, the finite
replica estimate of the expectation of an observable O, @xmath , will
scale towards the asymptotic limit, @xmath in the following way:

  -- -------- -- --------------------
     @xmath      ( \thechapter .56)
  -- -------- -- --------------------

Similarly it is not hard to show that (Eq. \thechapter .24 ):

  -- -------- -- --------------------
     @xmath      ( \thechapter .57)
  -- -------- -- --------------------

As before we note that a finite replica estimate will have a systematic
error associated with it. The extrapolation technique described in
section 27.5 may then be used to estimate the asymptotic value of the
appropriate expectation. That is if one plots a graph of @xmath against
@xmath , then the intercept of the graph on the vertical axis should,
provided P is large enough, yield an estimate of @xmath which is free of
systematic errors.

#### 27.7 The classical limit

In order to develop an intuition for the polymeric-like system described
by Eq. \thechapter .25 it is instructive to observe the emergence of the
classical limit out of Eq. \thechapter .25 by considering the interplay
between the kinetic and the configurational actions @xmath and @xmath .
To do this consider a simulation in which a sufficient number of
replicas P have been employed so as to ensure that the quantum effects
are modelled to the desired accuracy. Now consider increasing the
temperature, so as to reduce @xmath [ 212 ] . The effect of this is to
increase the strength of the spring constant @xmath associated with the
harmonic-like kinetic term @xmath , resulting in increased rigidity of
the polymers. This increased rigidity has two effects. The first is to
make permutations, other than the identity permutation, increasingly
unlikely. The second is to make the spatial arrangements of the
particles of the various replicas increasingly similar ( @xmath , so
that @xmath where j denotes any replica). It is clear that what we are
seeing is the emergence of classical behaviour, something we would
expect on the transition to higher temperatures. That is in this limit
Eq. \thechapter .25 reduces to:

  -- -------- -- --------------------
     @xmath      ( \thechapter .58)
  -- -------- -- --------------------

where:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .59)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

or

  -- -------- -- --------------------
     @xmath      ( \thechapter .60)
  -- -------- -- --------------------

Eq. \thechapter .60 is simply the classical partition function.

### 28 Quantum FEDs via the Path Integral Formalism

By comparing Eq. \thechapter .27 to Eq. \thechapter .2 , it is clear
that the generalisation of the expression for the ratio of the partition
functions (see Eq. \thechapter .8 and Eq. \thechapter .13 ) so as to
account for quantum effects is a trivial one (see [ 38 ] , [ 161 ] , [
171 ] - [ 178 ] ). In this section we formulate the quantum version of
the real space mapping (Q-RSM) and a quantum version of the fourier
space mapping (Q-FSM) for estimating FEDs of phases. Though the Q-RSM is
quite similar to its classical counterpart, the Q-FSM is quite
different, and takes into account the harmonic interactions propagated
by the intra-polymer (or inter-replica) interaction term @xmath (Eq.
\thechapter .29 ). In both formulations we will neglect exchange, and we
will formulate both methods within the scheme of the PA. Generalisation
to the case of HOA methods is straightforward, with the discretisation
errors scaling as @xmath instead of @xmath .

#### 28.1 Quantum Real Space Mapping

Limiting ourselves to the case of distinguishable particles, it is clear
that if we have two phases A and B and we want to measure the quantum
mechanical FEDs then from Eq. \thechapter .27 (see also [ 38 ] , [ 161 ]
, [ 171 ] - [ 178 ] ) the ratio of the partition functions is simply
determined by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .61)
  -- -------- -- --------------------

  -- -------- -- --------------------
     @xmath      ( \thechapter .62)
  -- -------- -- --------------------

where [ 206 ] :

  -- -------- -- --------------------
     @xmath      ( \thechapter .63)
  -- -------- -- --------------------

and:

  -- -------- -- --------------------
     @xmath      ( \thechapter .64)
  -- -------- -- --------------------

Following the derivation of Eq. \thechapter .13 one may map the problem
of determining the ratio of the integrals in Eq. \thechapter .64 , in
which a single hamiltonian is employed, onto that of determining the
ratio of two integrals in which the hamiltonians are different . To do
this we express the position of the particles in terms of the
displacements about some reference configuration @xmath , which in the
crystalline case is conveniently chosen to be the lattice sites:

  -- -------- -- --------------------
     @xmath      ( \thechapter .65)
  -- -------- -- --------------------

where the subscript i denotes the replica. It then follows that in the
@xmath representation the ratio in Eq. \thechapter .64 may be written
as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .66)
  -- -------- -- --------------------

where:

  -- -------- -- --------------------
     @xmath      ( \thechapter .67)
  -- -------- -- --------------------

This mapping (which we call the quantum RSM, or Q-RSM) is one in which
one simultaneously maps the displacements of the particles of each and
every replica of phase @xmath onto the corresponding replica of phase
@xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .68)
  -- -------- -- --------------------

All the simulations performed in this chapter were implemented via the
Q-RSM. The crucial feature of the Q-RSM is that since:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .69)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

the kinetic part of the action @xmath is the same in the two phases
under the operation of this mapping. This is a significant advantage of
this particular mapping since in the large P limit the kinetic action
@xmath dominates over the configurational action @xmath (see appendix
\thechapter ).

Following the development of earlier chapters one may proceed to define
a macrovariable which measures the energy cost of mapping the
configuration of the polymeric system associated with phase A onto that
of the polymeric system associated with phase B:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .70)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

By comparing Eq. \thechapter .66 to Eq. \thechapter .2 it is immediately
clear that the overlap identity (see Eq. \thechapter .21 ) will hold
even for our quantum system. If @xmath denotes the quantum sampling
distribution of phase @xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .71)
  -- -------- -- --------------------

then it immediately follows that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .72)
  -- -------- -- --------------------

From Eq. \thechapter .72 it clear that all the discussions of the
previous chapters (with the exception of chapter \thechapter ) also
apply to the problem of estimating the quantum mechanical FED’s. In
particular the vast array of estimators derived from Eq. \thechapter .21
and the various extended sampling strategies used to overcome the
overlap problem may also be used in the quantum case of Eq. \thechapter
.66 . Later on we will use the PS estimator in conjunction with the MH
extended sampling strategy to estimate @xmath for several different
values of P (see section 29.7 ). However before we do this we will
derive the quantum version of the FSM. Unlike the Q-RSM, the quantum FSM
(Q-FSM) is considerably different in appearance from its classical
counterpart.

#### 28.2 Quantum Fourier Space Mapping

It is a straightforward exercise to re-write the expression in Eq.
\thechapter .66 in terms of some effective configuration @xmath (see Eq.
\thechapter .5 ), corresponding to a PM which matches the fourier
coordinates of each and every replica:

  -- -------- -- --------------------
     @xmath      ( \thechapter .73)
  -- -------- -- --------------------

and

  -- -------- -- --------------------
     @xmath      ( \thechapter .74)
  -- -------- -- --------------------

or

  -- -------- -- --------------------
     @xmath      ( \thechapter .75)
  -- -------- -- --------------------

where this time:

  -- -------- -- --------------------
     @xmath      ( \thechapter .76)
  -- -------- -- --------------------

with:

  -- -------- -- --------------------
     @xmath      ( \thechapter .77)
  -- -------- -- --------------------

The relevant macrovariable which quantifies the cost of the mapping then
generalises to:

  -- -------- -- --------------------
     @xmath      ( \thechapter .78)
  -- -------- -- --------------------

Though perfectly valid, there are two problems with the PM as formulated
in Eq. \thechapter .76 . The first is that the kinetic action gets
modified under the corresponding mapping of configurations. That is even
though such a FSM matches the harmonic contribution to the
configurational energy of each and every replica, the fact that @xmath
gets modified means that on the transition to a large number of replicas
the cost of making a PM will become energetically expensive, thereby
reducing the overlap between the two phases. In this case even if the
quantum system becomes harmonic at very low temperatures, the guarantee
of curing the overlap problem in the harmonic limit will no longer
exist. In fact since the harmonic inter-replica interactions get
stronger for larger P and end up dominating the overall action (see
appendix \thechapter ), and since larger values of P will be needed at
lower temperatures, it follows that the cost of the PM, as measured by
Eq. \thechapter .78 , will become greater the lower the temperature.

An alternative formulation reveals itself when we notice that the
kinetic action in Eq. \thechapter .28 is a quadratic function of the
displacements @xmath . Therefore if the system only explores the
harmonic parts of the configurational energy @xmath then the overall
action @xmath will itself be a quadratic function of the displacements
@xmath . In this case it is possible to define a mapping with ensures
perfect overlap between the two systems. The construction of the
transformation follows a similar procedure to that used to derive the
classical transformation @xmath (see chapter \thechapter ). We start by
expanding the action in Eq. \thechapter .28 as a power series in the
displacements to yield:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .79)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

where @xmath is the dynamical matrix (see Eq. \thechapter .8 ). We may
then approximate the total action @xmath by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .80)
  -- -------- -- --------------------

where

  -- -------- --
     @xmath   
  -- -------- --

and @xmath where @xmath denotes the replica (assuming the values 1
through to P) and i denotes the component (taking the value 1 through to
3N). It is not hard to show from Eq. \thechapter .79 that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .81)
  -- -------- -- --------------------

Following the procedure employed in deriving the classical FSM (Eq.
\thechapter .16 ), we may immediately write down the transformation
matrix for the Q-FSM:

  -- -------- -- --------------------
     @xmath      ( \thechapter .82)
  -- -------- -- --------------------

where now @xmath and @xmath are the normalised 3NP component
eigenvectors of @xmath and @xmath respectively, @xmath are the
associated eigenvalues, and the summation @xmath is over the non-null
eigenvalues of @xmath , and where the indices i and j span the values
through from 1 to 3NP. In this formulation the ’global’ displacement
vector of one phase, @xmath say, is mapped onto that of the other phase
via the relation:

  -- -------- -- --------------------
     @xmath      ( \thechapter .83)
  -- -------- -- --------------------

such that the total actions of the two polymers are matched. This
transformation may be conceptualised as the following mapping:

  -- -------- -- --------------------
     @xmath      ( \thechapter .84)
  -- -------- -- --------------------

The partition function is now given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .85)
  -- -------- -- --------------------

where

  -- -------- -- --------------------
     @xmath      ( \thechapter .86)
  -- -------- -- --------------------

where

  -- -------- -- --------------------
     @xmath      ( \thechapter .87)
  -- -------- -- --------------------

and where @xmath and @xmath are related via Eq. \thechapter .83 . The
transformation @xmath ensures that the quantity in Eq. \thechapter .80
is identical for the two phases.

Unlike the classical FSM, this quantum version of the FSM will not
necessarily guarantee an improvement in the overlap as the temperature
is reduced. The reason for this is that the presence of zero point
motion means that the system may explore the anharmonic regions of the
configurational energy even at @xmath . However in cases where the
quantum effects (zero point motion) are not strong enough so as to force
the particles to visit the strongly anharmonic regions, then the quantum
FSM, as we have formulated here, might serve as a useful tool.

### 29 Implementation details and simulation results

#### 29.1 Motivation

An archetypal example of a solid in which quantum effects are important
is @xmath [ 168 ] . In low density solid helium, the atoms have a large
zero point motion due to the small atomic mass [ 213 ] . Since the
atomic interactomic configurational energy is relatively weak as
compared to the zero point motion, the lattice expands due to the
outward pressure arising from this zero point motion. The result of this
is that the solid is destabilised at much lower densities than would be
allowed classically [ 214 ] . In addition to this one finds that for
@xmath the result of this zero point motion is that the particles, on
average, sit at the relative maximum of a bimodal configurational energy
[ 213 ] , [ 215 ] , [ 216 ] , resulting in imaginary frequencies of the
dynamical matrix, rendering the harmonic description inaccurate. As the
density is increased, the configurational energy eventually comes to
dominate the zero point motion, and as a result the crystal develops a
single well configurational energy  which localises the particles to
their lattice sites, resulting in the harmonic description becoming
accurate [ 213 ] .

As noted before, in addition to zero point motion, a phenomena called
exchange arises in the case of indistinguishable particles. In solid
@xmath this has little effect since the fact that the atoms have no spin
to label them means that there is no direct consequence of exchange (see
[ 168 ] ). In the case of solid @xmath the fact that the atoms have
non-zero spin means that exchange plays an important role in determining
the magnetic properties (see [ 168 ] ) of the solid.

The phase diagram of @xmath is shown in figure 36 . @xmath may, at the
crudest level, be described by the Lennard-Jones (LJ) configurational
energy, which provides a reasonable model for the rare gases [ 218 ] , [
219 ] . However in order to accurately determine the phase diagram one
needs to employ more accurate configurational energies (see [ 220 ] - [
223 ] ). In the rest of this chapter we will concern ourselves with the
fcc/hcp regimes of the phase diagram (as modelled by the LJ
configurational energy). Our goal will not be to provide definitive
statements about the phase diagram but instead to investigate the
methodology developed in this chapter and to get a qualitative feel for
the effect of zero point motion on the relative stability of the fcc (A)
and hcp (B) crystalline structures. For detailed studies of the quantum
LJ solid, we refer the reader to [ 224 ] - [ 226 ] .

#### 29.2 The Model System

As was the case in the classical simulations, the reduced density @xmath
was set to be @xmath . In addition to the parameters @xmath and @xmath ,
which enter into the classical simulations (see section 10 ), one also
encounters the additional parameter @xmath in the case of the quantum
simulations. For our LJ systems this was fixed through the De Boer
parameter [ 227 ] , [ 228 ] @xmath given by Eq. \thechapter .37 . In
appendix \thechapter we clarify the way in which the de Boer parameter
enters into the calculations.

Initially simulations employing the same systems as those described in
section 10 were implemented in order to estimate the expectation of
thermodynamic variables such as the kinetic energy and the
configurational energy, with good accuracy being obtained with both the
PA (Eq. \thechapter .26 ) and the HOA (Eq. \thechapter .51 ) schemes.
However on attempting to estimate the FEDs, it was found that these
systems were too large for us to handle with the available computational
resources. This meant that we had to restrict our simulations to a
system size of N=96, which, from the way the fcc and hcp unit cells were
constructed (see figure 9 ), was the smallest system size that could be
used. A consequence of this system size was that in order to fulfil the
requirements imposed by the minimum image convention [ 58 ] particles
could only interact with their first nearest neighbour shell (comprising
of 12 particles), so that @xmath (see section 10 ).

#### 29.3 Sampling the polymer

In principle the simulation of a system whose hamiltonian @xmath is
described by Eq. \thechapter .26 or Eq. \thechapter .51 is a
straightforward task. One merely performs single particle perturbations,
and accordingly accept these moves with the acceptance probabilities
given in Eq. \thechapter .27 . In practice however this is not an
efficient way to sample this polymeric system. The origins of this lie
in the inter-replica coupling term @xmath . As the temperature is
lowered an increasing number of replicas will need to be employed in
order to keep the systematic errors controlled at some prescribed level
[ 229 ] . A consequence of this is that the harmonic inter-replica
coupling will get stiffer, resulting in the decrease of the average size
of an accepted move. Roughly speaking we see that since the kinetic
action is a gaussian like term with a prefactor which increases linearly
with the number of replicas, the mean square displacement should roughly
scale as @xmath . This leads to a critical slowing down [ 168 ] of the
simulation in this limit, and severely hampers the simulation.

In order to alleviate this problem, one must introduce an additional
move to the single particle moves already present in the classical case.
This move is a global polymer move [ 168 ] in which one moves a whole
polymer without changing its conformation. Such a move leaves unaffected
the kinetic action @xmath . As a consequence it is only the change in
@xmath which enters into the acceptance probability of such moves. Both
moves are important; on the one hand the global-polymer moves allow
faster exploration of the configurational energy @xmath , whilst the
single particle moves allow the different conformations of the polymer
to be explored. For the simulations considered here, it was found that
an implementation of the global polymer move for every P single particle
moves was optimal, in the sense that the correlation of the underlying
data was kept to a minimum.

#### 29.4 Testing the algorithm

In order to check that the algorithm was functioning correctly, two
separate checks were made on the simulation. In the first a harmonic
configurational energy was constructed, and the analytic results for the
mean total energy @xmath were compared to that obtained by the
simulation. In the second a LJ configurational energy was employed, and
the parameters were adjusted so as to get the simulation to explore a
region of the phase diagram in which the quantum effects were
non-negligible and in which only the harmonic regions of the
configurational energy were visited. In the latter case @xmath was also
estimated via a MH-PS simulation and compared to the corresponding
analytic result.

##### 29.4.1 Harmonic Potential

In order to test our algorithm, we considered a system interacting via
the interatomic configurational energy:

  -- -------- -- --------------------
     @xmath      ( \thechapter .88)
  -- -------- -- --------------------

where

  -- -------- -- --------------------
     @xmath      ( \thechapter .89)
  -- -------- -- --------------------

where @xmath denotes the set of particles which interact with particle
i. It is well known [ 112 ] that for a solid interacting via a harmonic
configurational energy, the mean total energy may be obtained exactly
from:

  -- -------- -- --------------------
     @xmath      ( \thechapter .90)
  -- -------- -- --------------------

where:

  -- -------- -- --------------------
     @xmath      ( \thechapter .91)
  -- -------- -- --------------------

where @xmath and where @xmath are the eigenvalues of the dynamical
matrix of @xmath , given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .92)
  -- -------- -- --------------------

where @xmath denotes a summation over the modes, and where @xmath
denotes the number of members in the set @xmath .

Figure 37 shows a graph comparing the estimates of the expectation of
the total energy as obtained by simulation with the theoretic values as
predicted by classical and quantum statistical mechanics. The curve for
the classical theory was obtained from the equipartition theorem [ 4 ] -
[ 6 ] which states that for a system of particles interacting via a
harmonic configurational energy, the expectation of the total excitation
energy is given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .93)
  -- -------- -- --------------------

where N is the number of particles (3N represents the number of degrees
of freedom) and @xmath is the number of constraints. For simulations
with periodic boundary conditions @xmath . The curve corresponding to
the theoretical predictions as made by quantum statistical mechanics was
obtained from Eq. \thechapter .90 .

The first thing that one notices is that the results of the simulation
are in complete agreement with the curve as extracted from Eq.
\thechapter .90 . Comparing the quantum and classical curves, we notice
from figure 37 that the quantum graph does, as expected, converge onto
the classical line on the transition to sufficiently high temperatures.
However at lower temperatures the situation is different. Here the mean
total excitation energy levels off and assumes a constant value,
corresponding to the term @xmath arising in Eq. \thechapter .90 . This
is the zero point energy of the system and arises from the inherent
motion of the particles, present even at 0 Kelvin. This is purely a
quantum phenomenon and arises from the Heisenberg uncertainty principle.
In contrast the mean total excitation energy vanishes in the classical
limit, and results in the departure of the quantum system from the
classical curve as seen in figure 37 .

##### 29.4.2 Lennard Jones System in harmonic regime

The test implemented in the previous section was also implemented on the
LJ hcp (B) and fcc system (phase A). The parameters @xmath and @xmath
were appropriately adjusted so as to ensure that only the harmonic parts
of the configurational energy were visited. The eigenvalues @xmath of
the dynamical matrix, obtained via numerical methods, were then used to
determine the analytic value for the mean total energy of the system,
via Eq. \thechapter .90 . This was then compared to the result obtained
by simulation and used to verify that the simulation was indeed visiting
only the harmonic regions of the LJ configurational energy. In the next
stage a check was made in order to verify that the simulation was indeed
estimating the FEDs correctly. The (quantum mechanical) analytic values
for @xmath were obtained via the relation [ 112 ] :

  -- -------- -- --------------------
     @xmath      ( \thechapter .94)
  -- -------- -- --------------------

where @xmath denotes the frequency of the i-th mode of phase @xmath .
These results were then compared to those obtained by a simulation
employing the HOA scheme (section 27.6 ) using the MH-PS method (section
18 ). The classical value was obtained from Eq. \thechapter .19 . The
results have been tabulated below:

The first column of table 1 verifies that the system of particles,
interacting via the LJ configurational energy, were indeed exploring
only the harmonic regions of the LJ configurational energy, since the
simulation results agree with the analytic values as predicted by
harmonic theory (Eq. \thechapter .94 ). The analytic values and the
simulation results for @xmath (see column 2 in table 1 ) clearly agree
to within the errors, and differ significantly from the classical value.
It is clear the effect of the increased amplitudes of vibration arising
from the zero point motion (see figure 38 ) act, in this regime of the
phase diagram, favourably towards the fcc (A) phase, making it more
probable (relative to the hcp (B) phase) than it would be in the
classical case. This is expected since in the classical case [ 35 ] the
increasing amplitudes of vibration obtained on increasing the
temperature also favours the fcc (A) phase, within the harmonic regimes
.

#### 29.5 Scaling of thermodynamic parameters with P

In practice a PIMC simulation necessarily involves a finite number of
replicas. Unlike in section 29.4 , where we had an analytic check so as
to allow us to determine whether a sufficient number of quantum replicas
had been employed, one will not have an idea as to the magnitude of the
systematic errors arising from the finite replica simulations in the
general case. As mentioned before the only information available to us
is the way the associated systematic errors scale with the number of
replicas. By plotting the desired expectation as an appropriate power of
the inverse of the number of replicas one may hope to obtain the
asymptotic value that the expectation assumes in the limit of an
infinite number of replicas.

Specifically we saw in sections 27.5 and 27.6 that the expectations of
an observable should have errors which scale as @xmath (for the PA) and
@xmath (for the HOA). In order to illustrate this scaling, simulations
of the harmonic system as described by the configurational energy given
in Eq. \thechapter .88 and Eq. \thechapter .89 were implemented via both
the PA and the HOA methods. The estimates of the asymptotic limits were
then extracted via the appropriate graphical extrapolation techniques,
and the results were then compared to the analytical value of the total
energy. Figure 39 and figure 40 illustrates that the scaling of the
expectation of the total energy does indeed follow that of Eq.
\thechapter .47 and Eq. \thechapter .56 for the PA and HOA methods
respectively, yielding the correct value (as obtained analytically) in
the asymptotic limit.

Given that both methods accurately determine the asymptotic value of the
mean total energy, the question now remains as to which yields a smaller
error (for a given computational resource). In addressing this issue, we
first note that the number of replicas chosen in figure 39 and figure 40
were the minimum needed in order to be in the appropriate scaling
regimes. Below this range the mean total energies no longer scaled as
@xmath and @xmath for the PA and HOA methods respectively. From the
graphs it is evident that half the number of replicas were needed in the
case of the HOA method than were required for the PA method. However it
was also found that the HOA method took roughly double the amount of
time to perform a given number of lattice sweeps, as compared to a PA
simulation employing the same number of replicas. In order to understand
this we note that since the higher order hamiltonian of Eq. \thechapter
.51 has the additional term @xmath to the primitive one (Eq. \thechapter
.26 ), and since this term has to be computed over the same number of
nearest neighbours for each particle as one would have to do when
calculating @xmath , the HOA method will require roughly double the time
to simulate. As a result we see that a 2P-replica PA simulation will
take the same time as a P replica HOA simulation, thus offsetting any
gains initially offered by the HOA method.

All that is left to compare between the two methods is the correlation
of the underlying data. Figure 41 shows the ratio of the error @xmath
obtained in a P replica HO simulation to the error @xmath obtained from
a 2P replica PA simulation, run for the same duration of time [ 231 ] .
Clearly figure 41 shows that in regards to correlations, the HO method
has a slight advantage over the PA method, and for this reason we
employed the HOA method in our attempts at estimating the FEDs. We also
note that the trend of the graph indicates that this advantage increases
as the number of replicas increases (for the systems studied here).

#### 29.6 Dependence of @xmath on P and T

As we have seen before, the statistics of the macrovariable @xmath
essentially contains all the information of the FED between the two
phases. Therefore it is instructive to examine the dependence of these
distributions on P and T. Figure 42 shows the quantum probability
distribution @xmath for different temperatures (and fixed replica
number); figure 43 shows the classical distributions @xmath for
different temperatures; and in figure 45 @xmath is plotted (at a given
temperature) for different numbers of replicas.

The first thing that is clear is that for the quantum case the behaviour
of @xmath is not monotonic with the variation of the relevant
parameters. In figure 42 the peak of the distributions initially move in
towards the origin with an accompanied decrease of variance. However
beyond a certain temperature ( @xmath ), the mean and variance of the
associated distributions start to increase as the temperature is raised.

The reason for this observed behaviour is as follows. At sufficiently
low temperatures the zero point motion of the particles force the system
to wander further away from the lattice sites (see figure 44 ) than
would be the case in the classical limit, resulting in the peak of the
distribution being further away from the origin than would be expected
in the classical case. This is clearly the case with @xmath (compare the
graphs in figures 42 and figure 43 ). As the temperature increases, the
contribution to the energy of the zero point motion remains constant ,
since the typical energy has not yet reached that of the phonon
excitation energies. From the definition given in Eq. \thechapter .70 it
immediately follows that @xmath must decrease . As the temperature is
increased even further the thermal excitation contributions to the
energy begin to become important. In this case the rate at which @xmath
decreases will itself reduce (since the decrease due to the division by
T will be offset by the increase in @xmath ) , until eventually @xmath
starts to increase. On further increase the system will become classical
and the (classical) thermal effects will mask the quantum zero point
motion, at which point the difference between the graphs in figure 42
and figure 43 , arising from the effect of the zero point motion, will
eventually be negligible.

Figure 45 shows the probability distribution @xmath for different
numbers of replicas. Initially as the number of replicas increases the
peak moves away from the origin (up to @xmath ). Further increase in the
number of replicas leads to the peak moving closer to the origin,
converging (by @xmath ) to the stationary distribution, which is
ultimately positioned further away from the origin than the
corresponding classical distribution. The important thing to note from
figures 42 and 45 is that as quantum effects become increasingly
important [ 232 ] , the peak of @xmath moves away from the origin
(relative to the classical distribution) and its width increases. This
means that in addition to the fact that the quantum simulation is
inherently more time consuming, additional time must also be spent
refining the sampling strategy (whether it be increased amounts of
multicanonicalisation in the case of MUCA simulations or increased
numbers of replicas being employed in the MH method) in order to
estimate the FEDs.

#### 29.7 FEDs

The primary motivation for developing the path integral machinery in the
preceding sections was to use it to estimate quantum FEDs. In fact by
formulating it in the way that was done in Eq. \thechapter .66 , we made
available to ourselves the vast array of tools discussed in the previous
chapters that are suitable for tackling this type of problem. In this
final section we discuss our attempts at estimating the quantum
mechanical FEDs. In estimating the FEDs, our aim was to investigate the
role of the zero point motion on the relative stability of the two
phases in a regime of the phase diagram in which both the quantum
effects and the anharmonic effects were significant. Figure 46 . shows
the estimates of the FEDs, obtained via the MH-PS method, in such a
regime.

It is clear from figure 46 that the quantum effects essentially act so
as to favour the hcp (B) phase (as compared to the classical case). This
can be understood in the context of results obtained in the classical
simulations. In [ 35 ] the classical LJ system was studied and it was
found that the increasing anharmonicity (obtained on increasing the
temperature) favoured the hcp (B) phase. This conclusion was arrived at
by comparing the simulation results to the harmonic calculations. The
same effect is likely to be the cause of the quantum effects favouring
the hcp (B) phase. That is the zero point motion pushes the particles
into regions further out from the minimum of the configurational
energy than they would typically explore in the classical case, making
the system more anharmonic. As is the case in the classical systems,
this anharmonicity acts in a way which favours the hcp (B) phase. This
is in sharp contrast to the harmonic regions of the quantum phase
diagram (see section 29.4.2 ), where the increased amplitudes of
vibration acts so as to favour the fcc (A) phase (as is also the case in
the classical regime, see [ 35 ] ).

### 30 Discussion

The quantum Lennard-Jones phase diagram has not been determined yet via
computational techniques and the work here represents a first step in
developing the necessary machinery for a move in that direction. The
factors limiting our investigation are the following:

1.  The slowing down associated with the simulation of a system of
    interacting polymers over that of a system of interacting particles.

2.  The increasing number (P) of replicas needed as the temperature is
    reduced. One not only has an increase in computational costs due to
    the fact that one has to simulate more replicas, but also a critical
    slowing down associated with the increasing strength of the
    inter-replica forces.

3.  The need to perform graphical extrapolation in order to obtain a
    single estimate of the expectation of an observable.

In order to accurately determine the phase diagram (using the same
number of processors that we used) at the temperature we chose, one
would need to employ a @xmath system. Since the time, associated with
keeping the error in the estimate of the FED at some prescribed level,
scales roughly as @xmath (for short ranged interactions) we see that the
simulation of a @xmath system would involve an increase in computational
requirement of approximately @xmath times. In accordance with Moore’s
Law, this sort of computational power will be available to us in about
13 years.

However a significant feature of the simulations the way that we have
done it (i.e. via the MH route) is the enormous scope for
parallelisation. This parallelisability does, in principle, allow us to
determine the phase diagram accurately even today, simply by
distributing the replicas amongst an increased set of processors [ 233 ]
. In our simulation we employed 256 processors. Therefore to perform the
above calculations one would require 1.5 million processors. With the
rapid expansion of parallel clusters (e.g. EPCC hpcx) the MH method
should make the task of determining the quantum phase diagram a
realistic project at a much earlier time than that predicted above.

## Chapter \thechapter Conclusion

In order to determine the location of a phase boundary between two
phases one must determine at which point in the phase diagram the FED of
the two phases is zero. The simplest approach is to tackle the problem
via computational techniques (Monte Carlo) whereby one determines the
weights of macrostates of one phase relative to those of the other. From
this one may then infer the ratio of the partition functions of the two
phases.

The problem with this approach is that generally a simulation initiated
in a given phase will not visit the regions of (absolute) configuration
space associated with the other phase, since the two phase will in
general be separated by a region of configuration space of intrinsically
low probability. As a consequence one will not be able to determine the
weights of macrostates of one phase relative to those of the other
phase. This is generally referred to as the overlap problem .

One way to circumvent this problem is to use the PM formalism [ 8 ] in
which one directly maps the configurations of one phase onto those of
the other phase. By choosing an ’intelligent’ PM one may generate
considerable overlap between the two phases. In constructing the PM
there are two issues which one must give consideration to. The first is
the choice of a reference configurations @xmath and the second is the
choice of coordinate systems ( @xmath ), or representation as we call
it, with which one parameterises the displacements of the particles from
the reference configuration. Since the PM matches the coordinates (
@xmath ), it is clear that the overlap is dependent upon both @xmath and
@xmath . The simplest and most straightforward choice of the
representation is that in which the coordinates are expressed in terms
of the displacements @xmath of the particles from the reference
configuration @xmath . We call the associated mapping the RSM. Another
possible choice is one in which the one parameterises the degrees of
freedom in terms of fourier coordinates of the system. This we call the
FSM. For the FSM one finds that, in the case of structurally ordered
phases, the overlap problem vanishes as the harmonic limit is approached
(see chapter \thechapter ), provided that the reference configurations
are chosen to be the ground state configuration (i.e. the lattice
sites).

Generally however the scope for refinement of the representation is
limited, and one finds that the overlap problem persists. The second
strategy that one naturally encounters is that of the estimator which
one uses to determine the FED. The choice of the optimal estimator
depends on the way in which the regions of (effective) configuration
space associated with the two phases overlaps. In the case where they
overlap in the manner shown in figure 18 (a) the EP estimator (Eq.
\thechapter .26 ), in which one performs a single simulation in phase A,
yields an estimate which is free of systematic errors. In the case where
they overlap as shown in figure 18 (b) then one must use estimators
which involve simulations in both phases. In this situation one may
either choose to use a phase constrained estimator in which one performs
two simulations, one in each phase, and in which the non-negligible
contributions come from the region of overlap (Eq. \thechapter .3 or Eq.
\thechapter .20 where G is appropriately chosen), or one may employ the
PS estimator (Eq. \thechapter .47 ) in which the sampling distribution
actually switches phases. However the validity of the estimates arising
from these estimators presupposes some form of overlap in the regions of
(effective) configuration space that the two phases explore. In the most
general case, however, there will not be any form of overlap and
therefore, like the choice of representation, the scope for refinement
of the estimator will be limit.

The final part of the FED problem is that of the sampling strategy . In
this case one refines the sampling distribution in order to engineer
overlap. Broadly there are three generic sampling strategies that one
may pursue. The first is the MUCA strategy, whereby one introduces
corrections to the Boltzmann weights appearing in the acceptance
probabilities so as to force the simulation to explore regions of
(effective) configuration space outside those it would normally explore
(using the canonical probability distribution). The second is the MH
strategy, whereby one simulates several systems independently. By
simulating a series of systems in such a way that they overlap in the
regions of (effective) configuration space that they explore and which,
taken together, connect the regions of configurations space associated
with one phase to those regions associated with the other phase, one is
able to determine the FED. The advantage of this method is that it is
highly parallelisable. The final strategy is the FG method, whereby one
performs non-equilibrium work on the system so as to force it from the
regions of (effective) configuration space associated with one phase to
those of the other. By ensuring that one performs work in a gradual [
234 ] , as opposed to abrupt, fashion, one may generate arbitrary
overlap between the two methods. The overall sampling strategy may also
involve combinations of these methods (see section 25 ).

The key components in tackling the FED problem have been summarised in
figure 47 .

## Chapter \thechapter Proof of the fluctuation theorem

In this section we set out to prove the fluctuation theorem as given by
Eq. \thechapter .101 . The original proof was given in [ 73 ] ; we
rederive it for the sake of mathematical clarity. The proof that is
given here is the particular case of that given in [ 73 ] in which
@xmath changes discontinuously from @xmath to @xmath at time @xmath ,
@xmath to @xmath at time @xmath , etc for the @xmath process (and the
reverse in the @xmath process).

## Appendix A Proof of the fluctuation theorem

We start by deriving a result which is central to the whole procedure.
Consider a simulation employing the Metropolis algorithm in which the
sequence of configurations @xmath is generated. Under the scheme of the
metropolis algorithm the probability of the system going from @xmath to
@xmath is given by @xmath (see Eq. \thechapter .20 ), where @xmath
satisfies the condition of detailed balance (Eq. \thechapter .19 ):

  -- -------- -- -------------------
     @xmath      ( \thechapter .1)
  -- -------- -- -------------------

where @xmath is the underlying sampling distribution. Eq. \thechapter .1
may be easily extended to the case of two non-consecutive
configurations:

  -- -------- -------- -------- -------- -------------------
     @xmath   @xmath   @xmath            ( \thechapter .2)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------------------

or:

  -- -------- -- -------------------
     @xmath      ( \thechapter .3)
  -- -------- -- -------------------

We may now proceed to prove the fluctuation theorem. Suppose that @xmath
denotes the probability of obtaining a path @xmath given an initial
configuration of @xmath in the @xmath process. Then since the initial
configuration @xmath is sampled from the distribution @xmath , it
follows that the distribution of the paths is given by:

  -- -------- -- -------------------
     @xmath      ( \thechapter .4)
  -- -------- -- -------------------

where:

  -- -------- -------- -------- -------- -------------------
     @xmath   @xmath   @xmath            ( \thechapter .5)
                                @xmath   
  -- -------- -------- -------- -------- -------------------

where @xmath denotes the probability of the system making a transition
from the configuration @xmath to @xmath , between times @xmath and
@xmath , under the sampling distribution @xmath . Similarly the path
@xmath for the @xmath process is sampled from the distribution @xmath
where:

  -- -------- -- -------------------
     @xmath      ( \thechapter .6)
  -- -------- -- -------------------

and where:

  -- -------- -------- -------- -------- -------------------
     @xmath   @xmath   @xmath            ( \thechapter .7)
                                @xmath   
  -- -------- -------- -------- -------- -------------------

Since from Eq. \thechapter .3 :

  -- -------- -- -------------------
     @xmath      ( \thechapter .8)
  -- -------- -- -------------------

it follows that:

  -- -------- -------- -------- -------- -------------------
     @xmath   @xmath   @xmath            ( \thechapter .9)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------------------

or:

  -- -------- -- --------------------
     @xmath      ( \thechapter .10)
  -- -------- -- --------------------

Defining:

  -- -------- -- --------------------
     @xmath      ( \thechapter .11)
  -- -------- -- --------------------

and similarly for @xmath . It follows from Eq. \thechapter .10 that:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .12)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

or:

  -- -------- -- --------------------
     @xmath      ( \thechapter .13)
  -- -------- -- --------------------

It is important to note that in this derivation we relied on Eq.
\thechapter .3 . As a consequence it is essential that the equilibration
time used to evolve @xmath to @xmath in the @xmath process is the same
as that used to equilibrate @xmath to @xmath in the @xmath process. This
is consistent with the interpretation of the @xmath process as being a
time-reversal of the @xmath process, in which the initial configurations
are sampled from the distribution @xmath instead of @xmath .

## Chapter \thechapter Fourier Space Mapping with periodic boundary
conditions

This section primarily deals with the modification that must be made to
the FSM (Eq. \thechapter .14 ) in the case where systems with periodic
boundary conditions are employed, and is relevant to the discussion of
section 12.1 .

Generally the employment of periodic boundary conditions (in conjunction
with a pairwise configurational energy) means that there will be three
eigenvectors of the dynamical matrix @xmath which will be of zero
eigenvalue. These eigenvectors correspond to translations of the system.
Clearly the fact that they are of zero eigenvalue means that they cannot
be incorporated into the framework of Eq. \thechapter .9 and Eq.
\thechapter .11 . Suppose that @xmath , @xmath , and @xmath correspond
to the null eigenvectors. In this case we may express the displacements
in terms of the fourier coordinates most generally as follows:

  -- -------- -- -------------------
     @xmath      ( \thechapter .1)
  -- -------- -- -------------------

where @xmath , @xmath , and @xmath are some arbitrary constants, which
are associated with the transformation (see Eq. \thechapter .5 ):

  -- -------- -- -------------------
     @xmath      ( \thechapter .2)
  -- -------- -- -------------------

The ratio of the partition functions may then be written as:

  -- -------- -- -------------------
     @xmath      ( \thechapter .3)
  -- -------- -- -------------------

where @xmath arises from the centre of mass contributions. Since these
should not contribute to the ratio of the partition functions @xmath we
set:

  -- -------- -- -------------------
     @xmath      ( \thechapter .4)
  -- -------- -- -------------------

The redundancy of the translational degrees of freedom means that one
may omit their consideration altogether in mapping the configurations of
one phase onto those the other. That is the transformation @xmath (Eq.
\thechapter .9 ) may be replaced by a 3N by 3N-3 column vector @xmath
which is given by:

  -- -------- -- -------------------
     @xmath      ( \thechapter .5)
  -- -------- -- -------------------

or

  -- -------- -- -------------------
     @xmath      ( \thechapter .6)
  -- -------- -- -------------------

The transformation @xmath now acts on the (3N-3) column vector @xmath
where the component @xmath is given by @xmath . The displacements @xmath
are then given by:

  -- -------- -- -------------------
     @xmath      ( \thechapter .7)
  -- -------- -- -------------------

or

  -- -------- -- -------------------
     @xmath      ( \thechapter .8)
  -- -------- -- -------------------

Likewise the inverse transformation @xmath [(3N-3 by 3N) transformation]
may be written as:

  -- -------- -- -------------------
     @xmath      ( \thechapter .9)
  -- -------- -- -------------------

or

  -- -------- -- --------------------
     @xmath      ( \thechapter .10)
  -- -------- -- --------------------

It then follows that the transformation @xmath (Eq. \thechapter .14 ),
which maps the configurations of one phase onto those of the other, may
be written as:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .11)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

From Eq. \thechapter .3 we see that the FED may then be written as Eq.
\thechapter .18 where now:

  -- -------- -- --------------------
     @xmath      ( \thechapter .12)
  -- -------- -- --------------------

and

  -- -------- -- --------------------
     @xmath      ( \thechapter .13)
  -- -------- -- --------------------

where

  -- -------- -- --------------------
     @xmath      ( \thechapter .14)
  -- -------- -- --------------------

Eq. \thechapter .12 is simply obtained from Eq. \thechapter .3 by noting
that in the harmonic limit @xmath , so that the configurational
integrals in the numerator and denominator exactly cancel out in this
limit.

The transformation @xmath in Eq. \thechapter .11 , through the mapping
in Eq. \thechapter .14 , ensures that the effective configuration @xmath
are preserved in mapping the displacements @xmath of one phase onto
those of the other [ 235 ] .

## Chapter \thechapter Perturbation theory for the Fourier Space Mapping

A probability distribution may be completely characterised by its
cumulants (eq. \thechapter .24 ). Therefore an alternative way to
investigate the dependence of the overlap on some generic parameter
(like the temperature T) is to find the dependence of the cumulants on
this parameter. In this appendix we will specifically focus on the FSM,
and we will find relations which determine the way the various cumulants
of @xmath scale with temperature. The primary conclusions of this
section will be that in the limit of @xmath all the cumulants vanish.
The discussion of this appendix is relevant to section 13.1 .

## Appendix B Preliminary Mathematical Properties of Gaussian Integrals

Let us define the harmonic average of a macrovariable @xmath :

  -- -------- -- -------------------
     @xmath      ( \thechapter .1)
  -- -------- -- -------------------

where the limits of integration are implicitly assumed to be from @xmath
to @xmath . Two results which we will frequently use in this appendix
and appendix \thechapter are the following:

  -- -------- -- -------------------
     @xmath      ( \thechapter .2)
  -- -------- -- -------------------

and

  -- -------- -- -------------------
     @xmath      ( \thechapter .3)
  -- -------- -- -------------------

where @xmath and where c is some constant.

For the sake of notational simplicity, we will, in the following
section, denote @xmath by @xmath when @xmath corresponds to fourier
coordinates of the system (defined by the relation in Eq. \thechapter .5
when @xmath is given by Eq. \thechapter .9 ) and @xmath by @xmath when
working with the RSM ( @xmath ).

## Appendix C Temperature scaling properties of @xmath

In the case of crystalline solids a physical motivation exists for the
separation of the harmonic contributions to the excitation energy from
the anharmonic ones. Let us start by first considering the Taylor
expansion of the excitation energy @xmath (Eq. \thechapter .9 ) in terms
of the fourier coordinates @xmath :

  -- -------- -------- -------- -------- -------------------
     @xmath   @xmath   @xmath            ( \thechapter .4)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------------------

where @xmath denotes the summation of all the terms of order n and where
@xmath corresponds to the harmonic contributions to the excitation
energy (see Eq. \thechapter .14 ). The expectation of a macrovariable M
with respect to the sampling distribution @xmath may then be written as:

  -- -------- -- -------------------
     @xmath      ( \thechapter .5)
  -- -------- -- -------------------

where the partition function @xmath may be expanded in the following
way:

  -- -------- -------- -------- -------- -------------------
     @xmath   @xmath   @xmath            ( \thechapter .6)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------------------

In Eq. \thechapter .6 we have used the fact (see Eq. \thechapter .3 )
that the integrals of integrands whose overall order of the fourier
coordinates @xmath is odd vanishes. The @xmath terms in Eq. \thechapter
.6 is what is left over on integrating the @xmath and @xmath terms. It
then follows that:

  -- -------- -- -------------------
     @xmath      ( \thechapter .7)
  -- -------- -- -------------------

Since @xmath and since we are only interested in the leading order terms
in the temperature in Eq. \thechapter .5 as @xmath , we see that one may
replace @xmath appearing in the expectations of Eq. \thechapter .5 with
@xmath in this limit. One may then rewrite Eq. \thechapter .5 as:

  -- -------- -- -------------------
     @xmath      ( \thechapter .8)
  -- -------- -- -------------------

where

  -- -------- -- -------------------
     @xmath      ( \thechapter .9)
  -- -------- -- -------------------

In order to analyse the cumulants of @xmath let us consider the
particular case when @xmath , which we denote by @xmath . From Eq.
\thechapter .4 we see that one may write:

  -- -------- -- --------------------
     @xmath      ( \thechapter .10)
  -- -------- -- --------------------

where

  -- -------- --
     @xmath   
  -- -------- --

The expectation of an arbitrary power of the overlap parameter may then
be written as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .11)
  -- -------- -- --------------------

In evaluating the expectation in Eq. \thechapter .11 one will obtain a
series of terms scaling in different ways with respect to the
temperature. Since we are examining the harmonic limit ( @xmath ), we
are only interested in the terms which are lowest order in T (i.e.
highest order in @xmath ). These originate from the integrals with the
lowest overall (even) order of v. This means that both @xmath and @xmath
need to be considered in evaluating the expectation of Eq. \thechapter
.11 , since, depending on whether n is even or not, it might be either
@xmath or @xmath which couple to the lowest order terms of @xmath so as
to yield the most slowly vanishing term.

Writing Eq. \thechapter .11 in full and retaining only the lowest order
terms, one finds that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .12)
  -- -------- -- --------------------

so that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .13)
  -- -------- -- --------------------

  -- -------- -- --------------------
     @xmath      ( \thechapter .14)
  -- -------- -- --------------------

From these relations we see that the mean and the variance of the
overlap parameter scale in the following way:

  -- -------- -- --------------------
     @xmath      ( \thechapter .15)
  -- -------- -- --------------------

and

  -- -------- -- --------------------
     @xmath      ( \thechapter .16)
  -- -------- -- --------------------

It is clear from Eq. \thechapter .13 and \thechapter .14 that @xmath
will scale as @xmath , since all other terms in the cumulants will
either scale with the same or higher power of T. Therefore we conclude
that for the FSM the cumulants of @xmath will scale in the following
way:

  -- -------- -- --------------------
     @xmath      ( \thechapter .17)
  -- -------- -- --------------------

  -- -------- -- --------------------
     @xmath      ( \thechapter .18)
  -- -------- -- --------------------

## Chapter \thechapter Perturbation theory for the Real Space Mapping

As in appendix \thechapter , we examine the temperature dependence of
the overlap, as engineered by the RSM, through an investigation of the
cumulants (Eq. \thechapter .24 ). We will derive exact expressions for
the mean and variances of @xmath , followed by a general argument to
show that @xmath tends to a constant non-zero value in the limit of
@xmath . The material in this appendix is relevant to the discussion of
section 13.2 .

## Appendix D Low temperature limit of @xmath

Let @xmath collectively denote the displacements of the particles of
phase @xmath from the reference configuration @xmath (which for the
systems employed here correspond to the lattice sites of the crystalline
solid, see section 10 ), and suppose that @xmath denotes the i-th
component of the fourier coordinates @xmath of phase @xmath . The RSM
ensures that:

  -- -------- -- -------------------
     @xmath      ( \thechapter .1)
  -- -------- -- -------------------

Using Eq. \thechapter .5 we see that this constraint imposes the
following relation between @xmath and @xmath :

  -- -------- -------- -------- -------- -------------------
     @xmath   @xmath   @xmath            ( \thechapter .2)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------------------

so that :

  -- -------- -- -------------------
     @xmath      ( \thechapter .3)
  -- -------- -- -------------------

From this we see that in the harmonic limit (where the excitation energy
is given by Eq. \thechapter .14 ) @xmath , which we write as @xmath to
signify the fact that we as using the RSM, may be written as:

  -- -------- -------- -------- -------- -------------------
     @xmath   @xmath   @xmath            ( \thechapter .4)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------------------

where the matrix elements of @xmath are given by:

  -- -------- -- -------------------
     @xmath      ( \thechapter .5)
  -- -------- -- -------------------

We will discard the subscript @xmath on the variable @xmath since we
will assume (for the rest of this section) that @xmath .

Because @xmath is symmetric (i.e. Hermitian), we may diagonalise it.
Typical diagonalization routines yield eigenvectors, which, in the case
of degenerate eigenvalues, may not be orthogonal. In this case one may
employ the Gram Schmidt orthogonalisation procedure to construct an
orthonormal set amongst these degenerate eigenvectors. Suppose that
@xmath has the eigenvalues @xmath and suppose that we write @xmath where
@xmath is diagonal. Since @xmath is an orthogonal transformation, we see
that @xmath and @xmath . Then:

  -- -------- -------- -------- -------- -------------------
     @xmath   @xmath   @xmath            ( \thechapter .6)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------------------

and therefore

  -- -------- -------- -------- -------- -------------------
     @xmath   @xmath   @xmath            ( \thechapter .7)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------------------

Alternatively, by directly appealing to Eq. \thechapter .3 and Eq.
\thechapter .4 , it is not hard to see that:

  -- -------- -- -------------------
     @xmath      ( \thechapter .8)
  -- -------- -- -------------------

where we have used the fact that @xmath (see Eq. \thechapter .2 ). From
Eq. \thechapter .8 it immediately follows the first cumulant for the RSM
may then be written as:

  -- -------- -- -------------------
     @xmath      ( \thechapter .9)
  -- -------- -- -------------------

## Appendix E Low temperature limit of @xmath

In a similar manner, the low temperature limit of the variance of @xmath
may be calculated. Using the fact that:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .10)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

we see that:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .11)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

Therefore we conclude that:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .12)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

Accordingly the second cumulant for the RSM may be written as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .13)
  -- -------- -- --------------------

## Appendix F Temperature scaling of @xmath

Even though in the preceding section we were able to calculate the exact
limiting form of @xmath and @xmath as @xmath , extending this to the
case of @xmath becomes tedious. Instead we will follow the presentation
in section C in order to derive the scaling relation of @xmath for the
general case. In calculating the temperature scaling properties of
@xmath we first note that for the RSM the harmonic terms @xmath are not
identical in the two phases. Therefore the perturbation expansion of
@xmath becomes (as compared with Eq. \thechapter .10 ):

  -- -------- -- --------------------
     @xmath      ( \thechapter .14)
  -- -------- -- --------------------

so that:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .15)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

It is immediately clear from Eq. \thechapter .15 that the temperature
scaling properties of @xmath will be governed by the leading order term
in eq. \thechapter .14 , @xmath , so that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .16)
  -- -------- -- --------------------

From this we may infer that the cumulants will, for sufficiently low
temperatures, be independent of the temperature:

  -- -------- -- --------------------
     @xmath      ( \thechapter .17)
  -- -------- -- --------------------

Therefore in the harmonic limit the distribution of @xmath , for the
RSM, assumes a stationary form which is not that of the limiting form
associated with perfect overlap (Eq. \thechapter .1 ).

## Chapter \thechapter Determining Statistical Errors

In this section we discuss the blocking method, which is a way to
determine the error associated with an estimate of the expectation of an
arbitrary macrovariable Q (see Eq. \thechapter .31 ) obtained from
correlated data. We also illustrate the way in which this blocking
method may be used to estimate the error in the FED estimate. For more
detailed information on the blocking method we refer the reader to [ 13
] , [ 236 ] , [ 237 ] .

## Appendix G Errors of averages

Suppose that we make a series of measurements @xmath (i=1,…,t), sampled
from a probability distribution of mean @xmath and variance @xmath .
Suppose that it is also our desire to obtain an unbiased estimate for
@xmath . This can be most simply obtained from the mean of the data set
@xmath :

  -- -------- -- -------------------
     @xmath      ( \thechapter .1)
  -- -------- -- -------------------

In the case where successive measurements @xmath are independent, one
finds [ 120 ] , [ 238 ] that the distribution of @xmath tends to a
Normal Distribution with mean @xmath and variance @xmath . That is
@xmath . This is simply a consequence of the central limit theorem.
Therefore in the particular case where the measurements are independent,
the “error” associated with the estimate of Eq. \thechapter .1 is simply
given by @xmath , where @xmath is an unbiased estimator of the variance
of @xmath , and is given by:

  -- -------- -- -------------------
     @xmath      ( \thechapter .2)
  -- -------- -- -------------------

In the case where the measurements @xmath are correlated, Eq.
\thechapter .1 still yields an unbiased estimate for the mean of the
underlying distribution of Q. However the associated error is now no
longer given by @xmath . One method for finding the associated error is
the so called blocking method. In this method, the set of data @xmath is
sectioned into M blocks each containing m data entries. That is block i
corresponds to the set @xmath . Then for each block an estimate @xmath
for the mean of the distribution of Q is made, and is given by:

  -- -------- -- -------------------
     @xmath      ( \thechapter .3)
  -- -------- -- -------------------

From this we obtain a set of block estimates for the mean of the
distribution:

  -- -------- --
     @xmath   
  -- -------- --

Since:

  -- -------- -- -------------------
     @xmath      ( \thechapter .4)
  -- -------- -- -------------------

it follows that the block estimates @xmath will themselves be
distributed with a mean given by @xmath and a variance given by @xmath ,
say. The key observation is that for sufficiently large blocksizes
successive @xmath will independent. In this case the error of the
average of the block estimates is simply given by @xmath . Since the
estimator @xmath is precisely this average (see Eq. \thechapter .4 ) it
follows that, for the (sufficiently large m) regimes where successive
block estimates are independent, the error in the estimate @xmath is
given by @xmath , where:

  -- -------- -------- -------- -------- -------------------
     @xmath   @xmath   @xmath            ( \thechapter .5)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------------------

and where @xmath is obtained from:

  -- -------- -- -------------------
     @xmath      ( \thechapter .6)
  -- -------- -- -------------------

In order to find the regime of blocksizes where successive block
estimates become independent, a simple graphical procedure may be used
to estimate the errors in @xmath . Since @xmath is constant in the
regime where the @xmath are independent (a result which must hold true
since @xmath is independent of the blocksize), we see that we may
determine the blocksizes m for which the @xmath are independent simply
by plotting a graph of @xmath versus m. As m increases the graph will
eventually plateau off, indicating that the block estimates are indeed
uncorrelated. From Eq. \thechapter .5 we see that one may then use the
value of the plateau (P) to determine the error in the estimate of
@xmath :

  -- -- -------- -------- -------- -------------------
        @xmath   @xmath            ( \thechapter .7)
                 @xmath   @xmath   
                 @xmath   @xmath   
  -- -- -------- -------- -------- -------------------

For a more mathematically rigorous treatment of the blocking procedure
we refer the reader to [ 236 ] .

## Appendix H Errors in the free energy difference

Though one may estimate the FED by taking an appropriate expectation
(Eq. \thechapter .26 ), the general expression for the FED will involve
the ratio of expectations (see for example Eq. \thechapter .8 , Eq.
\thechapter .89 , and Eq. \thechapter .105 ) . In this case the average
of the block estimates of @xmath is not the same as the estimate of
@xmath obtained from the whole data set. In this section we show that
the blocking method can also be used to estimate the error in @xmath .
As a specific example we use the PS estimator, in which the ratio of the
partition functions is estimated by determining the (unbiased) ratio of
the times spent in the two phases (see Eq. \thechapter .89 ). For
simplicity we consider the case where no weights are employed.

Suppose that we make a series of measurements @xmath (i=1,….,t) of the
“phase label” @xmath which can take on either of two values, A or B say,
during the course of PS simulation. Then the estimator for the
probability of being in phase A is given by:

  -- -------- -- -------------------
     @xmath      ( \thechapter .8)
  -- -------- -- -------------------

and the estimator for the probability of being in phase B is given by:

  -- -------- -- -------------------
     @xmath      ( \thechapter .9)
  -- -------- -- -------------------

It is clear from Eq. \thechapter .89 that @xmath (which is proportional
to the FED) may then be estimated by:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

If @xmath is deviated from its true value @xmath by a small amount
@xmath ( @xmath , then it is easy to show, using the approximation
@xmath valid for small x, that the error @xmath in @xmath is given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .11)
  -- -------- -- --------------------

Now let us consider dividing the data of t observations into M blocks,
each containing m data points. We may then make a block estimate of
@xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .12)
  -- -------- -- --------------------

where:

  -- -------- -- --------------------
     @xmath      ( \thechapter .13)
  -- -------- -- --------------------

The question that we now want to ask is how the block estimates @xmath
in Eq. \thechapter .12 may be used to determine the error in the
estimate given in Eq. LABEL:free_d1 . We note that strictly the blocking
procedure will only yield the error of the arithmetic average of the
block quantities. That is the blocking procedure will estimate the error
for the quantity @xmath , which is estimated by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .14)
  -- -------- -- --------------------

where @xmath also represents an estimator of @xmath , though it is not
an unbiased estimator. We will now show (a result which is expected)
that in the large t limit, the distinction between @xmath and @xmath
vanishes, so that we may estimate the error in Eq. LABEL:free_d1 simply
by using Eq. \thechapter .6 and Eq. \thechapter .7 in which @xmath and
in which @xmath is replaced by @xmath . To see this suppose that @xmath
fluctuates about the true value @xmath by an amount @xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .15)
  -- -------- -- --------------------

It is clear from Eq. \thechapter .8 that for sufficiently large
blocksizes , @xmath will be normally distributed with mean 0 and
variance @xmath say, @xmath . Then it follows that:

  -- -------- -------- -------- -------- --------------------
     @xmath   @xmath   @xmath            ( \thechapter .16)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------------------

Using the fact that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .17)
  -- -------- -- --------------------

and using the fact that for sufficiently large blocksizes m:

  -- -------- -- --------------------
     @xmath      ( \thechapter .18)
  -- -------- -- --------------------

we see that:

  -- -------- -- --------------------
     @xmath      ( \thechapter .19)
  -- -------- -- --------------------

so that the distinction between the two estimators vanishes for
sufficiently large t.

## Chapter \thechapter The overlap parameter and the Fermi function
estimator

In this appendix we bring out a relation that exists between the fermi
function (FF) method and the overlap parameter @xmath (Eq. \thechapter
.18 ). We start off by noting that in the case of arbitrary switching FG
the overlap parameter may be generalised to:

  -- -------- -------- -------- -------- -------------------
     @xmath   @xmath   @xmath            ( \thechapter .1)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------------------

or

  -- -------- -- -------------------
     @xmath      ( \thechapter .2)
  -- -------- -- -------------------

so that:

  -- -------- -- -------------------
     @xmath      ( \thechapter .3)
  -- -------- -- -------------------

and

  -- -------- -- -------------------
     @xmath      ( \thechapter .4)
  -- -------- -- -------------------

It is immediately apparent (from Eq. \thechapter .2 ) that knowledge of
the overlap @xmath translates to direct knowledge of @xmath [ 239 ] .
The point is that a-priori knowledge of @xmath (or @xmath ) is not at
hand so as to allow an estimation of @xmath via Eq. \thechapter .2 .
Consider the case where an equal number of independent samples are
obtained in each phase, so that @xmath . What the FF method does is to
start off with an estimate of @xmath , say @xmath (where @xmath is the
estimate of Eq. \thechapter .30 ), and use this to obtain separate
estimates of the overlap @xmath from simulations performed in one of the
two phases:

  -- -------- -------- -------- -------- -------------------
     @xmath   @xmath   @xmath            ( \thechapter .5)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------------------

and

  -- -------- -------- -------- -------- -------------------
     @xmath   @xmath   @xmath            ( \thechapter .6)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------------------

Only if @xmath is an unbiased estimator for @xmath will the two
estimates of @xmath and @xmath converge to the same value. Therefore
what Bennett’s recursive prescription (Eq. \thechapter .31 and Eq.
\thechapter .32 ) does is to vary ones estimate of @xmath (through
@xmath ) until the estimates of the overlap @xmath and @xmath have
converged to the same value. At this point one can be sure that the
estimate @xmath reflects the true value of @xmath since:

  -- -------- -------- -------- -------- -------------------
     @xmath   @xmath   @xmath            ( \thechapter .7)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------------------

## Chapter \thechapter Multihamiltonian method as a limiting form of the
Fast Growth method

The MH method can be viewed as a limiting form of the FG method. The key
insight is the observation that as the equilibration time @xmath
increases:

  -- -------- -- -------------------
     @xmath      ( \thechapter .1)
  -- -------- -- -------------------

where we recall that @xmath denotes the probability distribution of
@xmath at time @xmath , when the configurational energy has been
incremented from @xmath to @xmath and after the system has been
equilibrated with @xmath for a time @xmath . This stems from the fact
that if one perturbs the configurational energy from @xmath to @xmath ,
and then equilibrates the system for an infinite amount of time, to a
configuration @xmath , then the ensemble of configurations @xmath will
be Boltzmann distributed with distribution @xmath . In other words one
finds that in the case of adiabatic equilibration @xmath assumes the
simple form:

  -- -------- -- -------------------
     @xmath      ( \thechapter .2)
  -- -------- -- -------------------

This is exactly the sampling distribution of the MH method (see Eq.
\thechapter .15 ). Therefore the MH method can be viewed (for a given
@xmath ) as a limiting case of the FG method in which the equilibration
time is infinite (i.e. adiabatic equilibration). Figure 48 shows how the
distribution @xmath (of the FG method) tends to the limiting form of the
distribution of the MH method as the equilibration time ( @xmath ) is
increased, and clearly illustrates further the connection between the MH
and FG methods that we have just described.

In regards to systematic errors, it follows that if one uses the EP
estimator, then the systematic errors associated with the MH method will
be less than or equal to those of the FG method since adiabatic
equilibration translates to minimum systematic errors. However in the
case of the PS method this statement no longer holds if both methods
have sufficient overlap so as to ensure that the phase switches can take
place. In this case both methods have zero systematic errors since they
both visit all the important regions of (effective) configuration
space which contribute to the estimate of @xmath .

Apart from the issue of systematic errors, another difference of the two
methods is the way in which they are realised. In the FG method one
performs work on a single system as described in section 8.8 . In the MH
method, one makes use of the form of Eq. \thechapter .2 , which allows
one to realise the @xmath distribution by performing independent
simulations in parallel . This is a significant difference in that it
allows considerable speedup of the task of evaluating the FED since one
may parallelise the process. This will become especially apparent in
chapter \thechapter when we apply the method to the study of quantum
FEDs.

## Chapter \thechapter Details of the quantum simulations for the
Lennard-Jones potential

In this section we clarify the way the different parameters that enter
into the calculations of the hamiltonian of the polymeric systems for
the PA (Eq. \thechapter .26 ) and the HOA (Eq. \thechapter .51 ). For
simplicity we will work in the @xmath representation. We recap that for
the case of distinguishable quantum particles the PIMC simulation
involves the simulation of a system with a partition function given by:

  -- -------- -- -------------------
     @xmath      ( \thechapter .1)
  -- -------- -- -------------------

where:

  -- -------- -- -------------------
     @xmath      ( \thechapter .2)
  -- -------- -- -------------------

In the case of the Lennard-Jones configurational energy the distances
are measured as units of @xmath . Suppose that the superscript @xmath
over a variable denotes the fact that it is expressed in units of @xmath
, so that @xmath . Then we may conveniently express all quantities in
terms of these scaled variables. Suppose that @xmath denotes the k-th
coordinate of a particle, so that @xmath . If:

  -- -------- -- -------------------
     @xmath      ( \thechapter .3)
  -- -------- -- -------------------

and if @xmath denotes the contribution to the overall configurational
energy of the interaction between particle i and j, then it follows
that:

  -- -------- -------- -------- -------- -------------------
     @xmath   @xmath   @xmath            ( \thechapter .4)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------------------

where @xmath is a dimensionless number. If we denote by @xmath the
quantity @xmath , then it follows that in the case of the PA, Eq.
\thechapter .2 may be written as:

  -- -------- -- -------------------
     @xmath      ( \thechapter .5)
  -- -------- -- -------------------

where:

  -- -------- -- -------------------
     @xmath      ( \thechapter .6)
  -- -------- -- -------------------

and where @xmath denotes the distance between particles k and l in
replica i.

Furthermore:

  -- -------- -------- -------- -------- -------------------
     @xmath   @xmath   @xmath            ( \thechapter .7)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------------------

where @xmath is also a dimensionless function. Therefore if the HOA is
used then it follows that:

  -- -------- -- -------------------
     @xmath      ( \thechapter .8)
  -- -------- -- -------------------

where @xmath is given by:

  -- -------- -- -------------------
     @xmath      ( \thechapter .9)
  -- -------- -- -------------------

In using these quantities in the simulation, one must appropriately
modify these equations so as to take into account the fact that
particles only interact with only the first nearest neighbour shell.

## Chapter \thechapter Interplay between kinetic and configurational
actions

It is well known in the Path Integral Monte Carlo literature [ 168 ]
that on the transition to a large number of replicas, the kinetic action
@xmath dominates over the configurational action @xmath . Note however
this does not mean that the configurational action may be neglected on
the transition to large number of replicas, since @xmath essentially
determines where in configuration space the polymer resides in, where as
@xmath controls the magnitude of fluctuations between adjacent replicas
within this region of configuration space.

In this appendix we provide a simple numerical illustration of this for
the LJ systems employed in chapter \thechapter . Figure 49 shows the
dependence of @xmath and @xmath on the number of replicas P for a
simulation at a fixed temperature. For small number of replicas @xmath
starts off assuming a lower value than @xmath . As the number of
replicas increase both @xmath and @xmath increase, until eventually
@xmath comes to dominate over @xmath . Within this regime @xmath scales
linearly with P. @xmath on the other hand, had a positive gradient which
decreases as P increases, but never quite reaches zero. As a result
@xmath appears to plateau off, though the plateau is only reached in the
@xmath limit. The figure clearly illustrates the dominance of @xmath
over @xmath in the large P limit.
